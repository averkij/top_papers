{
    "paper_title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning",
    "authors": [
        "Yi-Fan Zhang",
        "Xingyu Lu",
        "Xiao Hu",
        "Chaoyou Fu",
        "Bin Wen",
        "Tianke Zhang",
        "Changyi Liu",
        "Kaiyu Jiang",
        "Kaibing Chen",
        "Kaiyu Tang",
        "Haojie Ding",
        "Jiankang Chen",
        "Fan Yang",
        "Zhang Zhang",
        "Tingting Gao",
        "Liang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs."
        },
        {
            "title": "Start",
            "content": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning Yi-Fan Zhang1, Xingyu Lu2, Xiao Hu2, Chaoyou Fu4,, Bin Wen3,,Tianke Zhang3 Changyi Liu3, Kaiyu Jiang3, Kaibing Chen3, Kaiyu Tang3, Haojie Ding3, Jiankang Chen3 Fan Yang2, Zhang Zhang1,, Tingting Gao2, Liang Wang1 1CASIA, 2THU, 3KuaiShou, 4NJU Project Leader Corresponding Author https://github.com/yfzhang114/r1_reward"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Reward Models (MRMs) play crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves 8.4% improvement on the VL Reward-Bench and 14.3% improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Rewards performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs. 5 2 0 2 5 ] . [ 1 5 3 8 2 0 . 5 0 5 2 : r Figure 1: R1-Reward performance on multimodal reward benchmarks. Performance improves significantly when using majority voting strategy (Voting@5/15) over multiple inference samples. Email: yifanzhang.cs@gmail.com (a) Policy Loss Convergence (b) Response Length During Training Figure 2: Detailed comparison between StableReinforce and Reinforce++. (a) StableReinforce exhibits faster and more stable convergence of the policy loss during training. (b) StableReinforce continuously performs length compression, improving efficiency. Reinforce++ collapses around step 150, whereas StableReinforce remains stable, demonstrating its enhanced robustness. Additionally, after RL training with StableReinforce, the average response length is reduced by approximately 15% compared to base model, suggesting potential improvements in reasoning token efficiency."
        },
        {
            "title": "Introduction",
            "content": "High-quality Multimodal Reward Models (MRMs) [37, 3, 56, 51, 62] play crucial role in the development of Multimodal Large Language Models (MLLMs) [50, 10, 4, 8, 1, 13]. In the training phase, from an algorithmic perspective, the MRM provides reward signals for RL [47, 34], directly influencing the stability and final outcomes of training. From data perspective, powerful MRM enables high-quality data filtering, improving data quality by removing noisy samples [66, 29]. In the inference phase, the MRM facilitates test-time scaling strategies, such as the best-of-N strategy, to select the optimal responses [51]. In the evaluation phase, good MRM can serve as an evaluator to simplify the evaluation process, especially in open-ended scenarios [56]. Recently, reinforcement learning [9, 33] has gained widespread application in the post-training process of MLLMs [59], achieving remarkable improvements in traditional vision tasks [27, 44], multimodal reasoning tasks [17, 36, 31], video understanding tasks [11], and omni-tasks [69]. Compared to traditional post-training strategies such as supervised fine-tuning and direct preference optimization [38], RL offers better generalization [6] and demonstrates the ability to induce long-term reasoning capabilities [9]. However, recent improvements in MRMs have primarily focused on data [56, 62] and structural aspects [66], with little discussion on whether RL can be used to introduce long-term reasoning in order to improve multimodal reward modeling performance. In this paper, we investigate whether RL algorithms can be applied to multimodal reward modeling tasks? Intuitively, the reward modeling problem can be transformed into rule-based RL task, where the input consists of given question and two answers. The target of the policy is to decide which answer is better. The reward during training can be obtained by comparing whether the models judgment is consistent with the ground truth. Our goal is to enable the model to perform long-term reasoning and then provide the correct judgment. However, RL for reward modeling presents several unique challenges, and directly using traditional RL methods can easily cause training to collapse: 1. Limitation of PPO [40] and Related Algorithms [42]. PPO and related algorithms rely on clipping the loss function to ensure training stability. However, we observe that when the advantage is negative and the current policy differs significantly from the reference policy, simple clipping fails to prevent instability, which may cause the training process to diverge or even crash. 2. Instability of Advantage Normalization. We observe that in the later stages of training, where the majority of rewards in single batch are either 1 or 0 with very low variance, the commonly used advantage normalization technique (subtracting the mean and dividing by the variance) in algorithms such as GRPO [42] and Reinforce++ [15] can lead to extremely large or small advantage values for some samples. This can cause significant instability during training. 3. Inconsistency Between Reasoning and Results. During training, we frequently observe inconsistencies between the models reasoning process and its final output. The model may judge one answer as better during reasoning but ultimately output an opposite answer. This happens because rule-based 2 RL only scores the result without supervising the reasoning process, leading the model to learn to generate correct answers without coherent reasoning. To this end, at the algorithm level, we propose StableReinforce, which introduces several key modifications to traditional RL methods. Specifically, we refine the clipping operation to mitigate numerical instability caused by large updates and introduce robust advantage normalization technique that limits the impact of outliers. On the reward function design front, StableReinforce introduces novel mechanism: the use of an MLLM as referee. This referee evaluates the consistency between the models reasoning process and the final result, ensuring that the reasoning aligns with the output. This consistency reward promotes more accurate and logically coherent decision-making. During the training phase, directly training the MLLM using reinforcement learning yields suboptimal results. Therefore, progressive difficulty training strategy is adopted. Initially, 200K preference data is collected from publicly available datasets, and GPT-4o generates corresponding thinking processes, referred to as R1-Reward-200K, to serve as cold-start SFT data. Meanwhile, for each sample, the number of sampling attempts GPT-4o requires to infer conclusion matching the ground truth is recorded, which is considered the difficulty level of that sample. In the reinforcement learning phase, samples where GPT-4o requires at least two sampling attempts to arrive at the correct answer, or fails to answer correctly even after three attempts, are selected as training data. These samples are then used to train the model with the enhanced StableReinforce algorithm. As shown in Figure 2, the reinforcement learning phase effectively performs token compression, and also resulting in noticeable performance improvement in our experiments. R1-Reward performs excellently on common multimodal reward modeling benchmarks. As shown in Figure 1, R1-Reward outperforms the state-of-the-art (SOTA) on all the three benchmarks. Furthermore, R1-Reward exhibits strong inference time scalability. By sampling only five times and selecting the most frequent answer as the correct one, the accuracy of reward modeling improves substantially. On the MM-RLHF Reward Bench [66], VL Reward-Bench [21], and Multimodal Reward Bench [57], R1-Reward achieves improvements of 3.5%, 13.5%, and 14.6%, respectively, compared to SOTA. As the number of samples increases, performance continues to improve, demonstrating the potential of RL for multimodal reward modeling."
        },
        {
            "title": "2 Related Work",
            "content": "MLLMs. Thanks to the success of language models, MLLMs have rapidly developed in recent years, with their task handling capabilities and model performance advancing at fast pace [14, 68, 59, 20]. For example, traditional multi-modal large models perform well in handling complex high-resolution images and human dialogue [2, 33, 19, 49, 50]. series of works focus on improving the context length [45], computational efficiency [65, 64], reducing hallucinations [29, 67], enhancing conversational abilities [56], and aligning with human preferences [66]. Omni-MLLMs are capable of simultaneously processing multiple modalities such as speech, video, images [23, 69], and even interacting with users via voice [12, 13]. Unify-MLLMs can perform mixed-modal generation [53, 48, 55], for example, generating an image with auxiliary lines while understanding math problem, enhancing both generation and comprehension abilities. Recently, with the success of Open AIs O1 model and Deepseeks R1 model [9], the rule-based reinforcement learning approach has gained significant attention in the multi-modal field. Various studies are devoted to enhancing the reasoning capabilities of multi-modal models. However, as far as we know, no work has yet explored whether the reinforcement learning paradigm can be transferred into reward modeling. Reward Model Training. The reward models most relevant to this paper are pure text reward models and multi-modal reward models. There are generally three main approaches to reward modeling. The first approach is to directly use language model or multi-modal model as the reward model by designing precise prompts that allow them to output score or ranking [56]. However, this method heavily depends on the models instruction-following ability and comprehension. The second approach involves connecting the latent representation of language model to reward head (typically an MLP or linear layer), where the model directly outputs score. During training, the reward modeling is converted into binary classification task. This approach is computationally efficient, but it lacks interpretability [24, 62, 32, 28, 52]. The final type of model simultaneously learns to evaluate the question-answer pair and creates an additional reward head to provide the score [61, 66]. This model strikes balance between interpretability and computational efficiency, but it usually requires specific data formats or training strategies. This paper proposes training 3 reward model through reinforcement learning. The model first outputs an inference for given question-answer pair and ultimately provides ranking. Through reinforcement learning, we force the model to learn the format of the reward modeling task, avoiding the shortcomings of the first approach without requiring an additional reward head, while maintaining the models interpretability."
        },
        {
            "title": "3 Preliminary and Limitations",
            "content": "3.1 Background and Limitations of Standard Reward Models Reward models are key component for aligning model outputs with human preferences. Typically, reward model starts with pretrained LLM ϕ, where the LLM head hl is replaced with linear reward head lr, enabling the model to output scalar reward value. These models are trained using human-provided pairwise comparisons. Given query x, preferred response yw and less preferred response yl, the reward model is optimized to assign higher rewards to preferred responses: (cid:104) (cid:17)(cid:105) (cid:16) ℓReward(θ) = Ex,yw,yl log σ r(ywx) r(ylx) , (1) where r(yx) is the scalar reward and σ is the sigmoid function. Despite their utility, standard reward models face significant limitations. First, they fail to fully utilize the rich and detailed feedback provided by high-quality human annotations, such as textual explanations and nuanced reasoning. Second, scalar rewards lack transparency, making it difficult for humans to understand how the reward is generated. These challenges highlight the need for more interpretable and robust reward model that leverages critiques as intermediate reasoning steps. 3.2 PPO and Reinforce++ Proximal Policy Optimization (PPO) [40] is commonly used algorithm in RL that aims to optimize policy directly while maintaining stable and efficient learning. PPO belongs to the family of policy gradient methods, where the objective is to improve the policy by maximizing the expected cumulative reward. Unlike traditional policy gradient methods, which can suffer from large updates and instability, PPO introduces novel way to constrain policy updates, ensuring both efficient and stable learning. The objective function for PPO is defined as: LPPO(θ) = 1 (cid:88) (cid:20) min (cid:18) πθ(atst) πθold(atst) At, clip (cid:18) πθ(atst) πθold (atst) (cid:19) (cid:19)(cid:21) , 1 ϵ, 1 + ϵ At - πθ(atst) is the probability of taking action at at state st under the current policy θ. - πθold(atst) is the probability under the old policy with parameters θold. - At is the advantage estimate at time t, which measures the relative desirability of the action taken. - ϵ is small hyperparameter (typically 0.1 ϵ 0.3) that controls how much the policy can change. The first term in the minimum represents the standard objective, while the second term applies clipping mechanism. The clip function restricts the ratio of the new policy to the old policy to stay within the interval [1 ϵ, 1 + ϵ]. If the ratio exceeds this range, the objective is capped, preventing large updates that could destabilize the learning process. PPOs key innovation is the introduction of clipped objective function, which stabilizes the learning process by limiting the size of the policy updates. The method is both simple to implement and computationally efficient, making it popular choice for wide range of reinforcement learning tasks, including robotic control [46] and video game environments [41]. Reinforce++ [15] Enhancements. Reinforce++ incorporates several key optimizations to enhance training stability and efficiency of PPO. One is the addition of token-level Kullback-Leibler (KL) divergence penalty between the RL model and the supervised fine-tuning (SFT) model distributions. This token-level KL penalty is introduced into the reward function as follows: r(st, at) = I(st = [EOS])r(x, y) βKL(t) where represents the input prompt, denotes the generated response, I(st = [EOS]) is an indicator function that checks if the token is the final token in the sequence (End of Sequence), and β is the KL penalty coefficient, controlling the strength of the regularization. 4 Algorithm 1 Pseudocode of PPO Loss Function in PyTorch-like style. # log_probs: log probabilities of the current policy # old_log_probs: log probabilities of the previous policy # advantages: advantage estimates for the actions # epsilon: clipping parameter for PPO objective # Our Pre-Clip strategy: clip the log difference to prevent large values log_diff = log_probs - old_log_probs log_diff = torch.clamp(log_diff, max=np.log(1e3), min=np.log(1e-3)) # similar to 10 ratio = torch.exp(log_diff) # PPO strategy 0. ratio = (log_probs - old_log_probs).exp() # compute the probability ratio 1. surr1 = ratio * advantages # first surrogate objective 2. surr2 = ratio.clamp(1 - epsilon, 1 + epsilon) * advantages # second surrogate with clipping # The final loss is the minimum of the two surrogates 3. loss = -torch.min(surr1, surr2) # negative loss for minimization Additionally, Reinforce++ introduces global batch-level reward normalization, clipping, and scaling for stability, as well as advantage normalization: Anormalized = AµA Where µA and σA are the σA mean and standard deviation of the advantage values. REINFORCE++ is shown to be more stable compared to GRPO [43] and faster than PPO [54, 7]. 3.3 Drawbacks of Traditional PPO/Reinforce++ During our training process, we observed two core issues in the Reinforce++ algorithms that can easily lead to model instability and poor performance, especially for reward model training. Instability Caused by Training Losses. The typical PPO loss function is implemented as follows, given the log probabilities log πθ(atst), log πθold(atst), and advantages. The pseudocode in for calculating the loss is shown in Algorithm 1 (lines 0-3). If the ratio πθ(atst) πθold (atst) differs significantly, two main issues arise. First, the expression (log_probs old_log_probs). exp() can lead to numerical instability. When the difference in token probabilities is large, the exponential function may overflow, causing the model to crash. Even if the computation proceeds normally, if the advantage is negative, torch.min(surr1, surr2) could result in an excessively large loss due to the minimization objective. For example: let log_probs = [0.1, 0.1, 0.1, 0.1], old_log_probs = [10, 0.2, 0.2, 5], and advantages = [1.0, 1.0, 0.5, 0.5], the resulting loss = [19930.4, 1.1, 0.5, 67.1]. Such large losses can make the optiloss values might be: mization process highly unstable. Currently, many training methods remove the KL divergence constraint [31, 35], allowing each mini-batch to perform multiple parameter updates, thereby improving data usage efficiency [15, 40]. The former accelerates model updates, while the latter further increases the discrepancy between log πθ(atst) and log πθold(atst). Consequently, in these cases, the ratio between these two values can diverge significantly, leading to instability. Instability Caused by Advantage Normalization. In addition to the training loss, the data labels for the reward model are relatively simple, consisting of only two labels: 1 and 2, which makes them easy to learn. As result, during training, there is high probability that the majority of the batch will correctly predict the rewards. In extreme cases, such as batch containing 255 rewards of 1 and 1 reward of 0, this highly imbalanced distribution, when subjected to z-Normalization, can lead to significant numerical disparities. Particularly, the advantage corresponding to the 0 reward in this example would be normalized to -15.96. large advantage value like this can cause instability."
        },
        {
            "title": "4 R1-Reward",
            "content": "4.1 Our Training Algorithm: StableReinforce To overcome the drawbacks and enhance the stability of reinforcement learning training, we propose two strategies: pre-CLIP and advantage filter, which respectively remove unstable gradients and advantages that deviate excessively from the overall distribution. In terms of reward design, we introduce the consistency reward to ensure consistency between reasoning and the final answer. 5 Pre-CLIP. As shown in Algorithm 1 under the Our Pre-Clip strategy, our core approach is to clip large ratios before computing the exponential of the log probability. The value of 1e3 is hyperparameter that we find works well and the method is relatively insensitive to hyperparameter variations. The main purpose of this step is to mitigate the impact of noisy data on the overall training process with log-probability clamping: πθ(atst) πθold(atst) (cid:18) (cid:18) exp clip log πθ πθold , log δmin, log δmax (cid:19)(cid:19) where δmin = 103, δmax = 103 control allowable probability ratio bounds. By clipping the ratio before applying the exponential function, we can prevent overflow issues due to excessively large differences in the ratios. Additionally, this clipping ensures that large log-probability differences are mitigated, particularly when the advantage is negative, thus maintaining training stability. Advantage Filter. To prevent the influence of outliers due to the extreme imbalance in the advantage distribution, we apply the 3-sigma rule. For the standardized advantage, Astandardized = AµA , we σA retain only those advantages that fall within the range of [3, 3]1. This range corresponds to values within 3 standard deviations from the mean in the original distribution, as the standardization process converts the data to z-scores (unitless measures in terms of standard deviations). In the extreme case from the previous subsection, this ensures that all samples with original rewards of 1 are selected, while extreme negative advantages are excluded. ˆA = (cid:26)Astandardized if Astandardized 3 otherwise , Astandardized = µA σA + ϵ The final StableReinforce objective function with clipping applied: LStableReinforce(θ) = 1 (cid:88) (cid:34) (cid:32) min πθ(atst) πθold(atst) ˆAt, clip (cid:32) πθ(atst) πθold (atst) , 1 ϵ, 1 + ϵ (cid:33) (cid:33)(cid:35) ˆAt , where the reward calculation and advantage estimation strategies are the same to Reinforce++. 4.2 Remark In the field of RL for LLMs, recent concurrent advancements have emerged, some of which share similarities with our approach or report analogous observations. Although these methods have not been directly applied to multimodal domains or reward modeling, we provide concise discussion in this section for comparative purposes. Notably, DAPO [58], TOPR [39], and Minimax-01 [18] focus on improving CLIP operations, particularly in the design of the epsilon parameter. In contrast, our approach fundamentally differs by clipping the logits ratio prior to the exponential operation. This strategy enhances numerical stability and mitigates the adverse effects of negative advantages. Similarly, Dr. GRPO [25] identifies the detrimental impact of advantage normalization and adopts strategy of setting variance to 1. However, in scenarios with high original variance, this approach allows extreme values to dominate. Instead, we employ 3-sigma filter, which preserves the benefits of z-normalization while effectively removing outliers. 4.3 Reward Function and Training Data Inspired by DeepSeek-R1 [9], we aim to directly use RL to guide the reward model in generating the best analysis content, in order to produce high-quality model output comparisons. As result, the prompt format in Table 1 transforms the reward modeling task into straightforward rule-based reinforcement learning problem. By defining the models output format, we only need to define our reward functions to complete the training process: Formatting Reward. The models output must adhere to specific format of <think> </think><answer> </answer>, which encourages the model to reason before generating the final output. This ensures that the model reflects on the reasoning process before making its final decision, enhancing both the quality and interpretability of the generated content. 1After applying Z-normalization in the original text, the distribution becomes standard normal distribution, meaning it has mean of 0 and standard deviation of 1. 6 Table 1: Prompt template for reward model training. You are highly skilled and impartial evaluator tasked with comparing two responses generated by Large Multimodal Model for given question. - Start with thorough, side-by-side comparative analysis enclosed within <think> and </think> tags. tie is not permitted; you must choose better option. - Conclude with single numeric choice enclosed within <answer> and </answer> tags: - Output 1 if Response 1 is better. - Output 2 if Response 2 is better. Input [Question]: {question} [Response 1]: {answer1} [Response 2]: {answer2} Output Format (strictly follow) <think>Your detailed comparative analysis</think><answer>1/2</answer> Result Reward. The models generated final result must align with human preferences. This primarily involves ensuring that the models output ranking labels are consistent with those of human experts, enhancing the overall usefulness and credibility. Inconsistency Between Reasoning and Results. However, simply following existing work [9, 58] in our setting has led to unexpected results. During training, we observe discrepancies between the models reasoning and its final answer. For example, the reasoning might conclude that response 2 is better but the model outputs answer 1, as seen in <think>... response 2 is better</think><answer>1</answer>. This inconsistency arises because, we provide no supervision for the reasoning process and only score based on the outcome. When sample demonstrates poor reasoning but produces the correct answer, this pattern is inadvertently reinforced, leading the model to believe that reasoning and the final answer are not necessarily linked. Consequently, the model may learn to generate correct answers without coherent reasoning process. This could even result in the model treating the reasoning process as irrelevant, or worse, outputting repetitive content or random noise. To address this issue, we introduce an additional component, Qwen2.5-VL-7B-Instruct, as supervisor to verify whether the reasoning and the final result are consistent. This addition helps ensure that the reasoning process and output align well, introducing the following reward function: Consistency Reward. The models final result must be consistent with its intermediate reasoning process. This function ensures that the final answer is directly derived from the models reasoning process, rather than being generated in isolation from the reasoning steps. Integrating the consistency reward as separate reward and combining it with the previous two reward functions can lead to situation where the model, despite selecting the wrong answer, may still receive high overall reward due to the consistency component. This could result in the model overly prioritizing consistency. To mitigate this issue, the final reward is designed as follows: Final Reward = Result Reward (1 + 0.5 Consistency Reward) + 0.5 Formatting Reward. This ensures that the consistency reward is only taken into account when the result is correct, thereby preventing the model from excessively favoring consistency in cases where the outcome is incorrect. Dataset Construction. As shown in Table 2, we sample preference data from multiple existing datasets for training. To ensure data quality and diversity, we sample all instances from the humanannotated dataset MM-RLHF, and an additional 100,000 samples from other multimodal preference datasets. The final dataset is termed R1-Reward-200k, which combines these diverse instances to create robust training foundation for our model. We then randomly shuffle the data to ensure 7 Table 2: Summary of datasets used for training, including the category (text or image), dataset name, the number of original samples, and the number of samples selected for final training. Dataset RLAIF-V [60] VL-Feedback [22] POVID [70] WildVision-Battle [30] # Original 74,802 80,258 17,184 10,383 # Sample 100k Dataset MM-RLHF-Long [66] MM-RLHF-Short [66] MM-RLHF-Mcq [66] MM-RLHF-Safety [66] # Original 41,163 46,281 8,306 9,990 # Sample 41,163 46,281 8,306 9,990 balanced ratio of answers 1 and 2 (1:1), preventing the model from favoring specific answer. Each sample consists of quadruple: (question, answer 1, answer 2, ground truth choice). Long-Cot Cold Start. Since MLLMs are not initially trained for reward modeling tasks, directly using MLLMs for reinforcement learning training yields poor and unstable results. Therefore, we first use GPT-4o as an annotator for each sample in the R1-Reward-200k dataset, utilizing the prompts from Table 1 to construct SFT data. The temperature is set to 0, with maximum of 3 attempts. We also record how many attempts GPT-4o needs to generate final response that is judged to be correct (i.e., the same to the ground truth choice). This SFT training phase teaches the model the basic format and familiarizes it with the reward modeling task. RL Training Data. We use all samples from the SFT phase with at least 2 attempts, as well as samples where GPT-4o fails to produce the correct answer after three attempts. These samples exhibit smaller differences between answer 1 and answer 2, making them more difficult."
        },
        {
            "title": "5 Experiments",
            "content": "Implementation Details. Both SFT and RL experiments are conducted on 4H800 (80G) GPUs. The SFT phase trains for 1 epoch and takes approximately 8 hours, while the RL phase trains for 5 epochs and takes 12 hours. We use QwenVL-2.5-7B-Instruct as the base model for training. During the SFT phase, the learning rate is set to 1e-5, and the batch size is set to 256. We use the OpenRLHF [16] framework for RL. The training batch size is set to 128, and the rollout batch size is set to 256. The learning rate is set to 1e-6, and the initial KL coefficient is set to 0. Baseline Algorithm. At the algorithmic level, we primarily compare two entities: the reward model and MM-RLHF-Reward [66]. For the former, we replace the language head of the base LLM with two-layer MLP that outputs float value as the reward. Training is done using binary classification loss. For the latter, in addition to the traditional binary classification loss, an additional critic loss is required. Specifically, the model first outputs an evaluation of the candidate, and then, based on the evaluation, the reward head provides the reward value. Baseline Models. For multimodal reward models, we consider GPT-4o-mini (2024-07-18), Claude3.5-Sonnet (2024-06-22), Gemini-1.5-Flash (2024-09-24), GPT-4o (2024-08-06), Gemini-1.5-Pro (2024-09-24), Gemini-2.0-Flash-Exp, SliME [63], VITA-1.5 [13], LLaVA-OneVision-7B-ov [19], Qwen2-VL-7B [50], Molmo-7B [10], InternVL2/3-8B [5, 71], LLaVA-Critic-8B [56], Llama-3.211B [32], Pixtral-12B [1], Molmo-72B [10], Qwen2-VL-72B [50], NVLM-D-72B [8], MM-RLHFReward-7B [66], Llama-3.2-90B [32] and IXC-2.5-Reward [62] as comparison points. Evaluation Benchmarks and Metrics. The multimodal benchmark consists of the VL-Reward Bench [21], Multimodal RewardBench [57] and the MM-RLHF-Reward Bench [66]. VL-Reward Bench includes two evaluation metrics: Overall Accuracy and Macro Average Accuracy. Overall Accuracy measures the percentage of model decisions that align with human preferences, while Macro Average Accuracy calculates the mean accuracy across various task categories, addressing task distribution imbalance. Multimodal RewardBench serves as comprehensive benchmark for evaluating reward models. It covers six key areas: general correctness, preference, knowledge, reasoning, safety, and visual question answering (VQA). This benchmark includes 5,000 annotated triplets, each consisting of (multimodal prompt, chosen response, rejected response) pair. The MM-RLHF-Reward Bench also features two evaluation metrics: 1. Traditional Accuracy (Acc): This metric assesses the proportion of cases where the model correctly identifies the preferred response. 2. Acc+: This measures the proportion of cases where the model correctly ranks all response pairs for given sample. This metric emphasizes the models ability to handle challenging cases, such as those with small ranking differences or hard-to-distinguish pairs. 8 Figure 3: An example of the R1-Reward output. R1-Reward shows human-like self-reflective thought process, which is also called Aha moment [9]. 9 Table 3: VLReward Bench. Performance comparison of our reward model (R1-Reward) with existing open-source and private counterparts. Models #Param General Hallucination Reasoning Overall Acc Macro Acc - - - - - - 7B 7B 7B 7B 8B 8B 11B 12B 72B 72B 72B 7B 90B 7B 7B 7B Proprietary Models 41.70 43.40 47.80 49.10 50.80 68.08 34.50 55.00 59.60 67.60 72.50 70.70 Open-Source Models 18.55 7.23 32.20 31.10 35.60 54.60 33.30 35.60 33.90 38.10 38.90 8.93 27.09 20.10 31.80 41.10 38.30 38.40 25.90 42.30 32.80 31.60 Reward Models 45.04 42.60 84.70 63.84 66.32 50.45 57.30 62. 85.71 89.06 Ours 58.20 62.30 58.40 70.50 64.20 60.81 22.11 18.60 57.10 56.20 59.00 59.10 56.60 59.90 54.90 58.00 62.00 57.55 61.70 62.90 64.78 73. 41.50 55.30 57.60 65.80 67.20 66.31 16.48 19.04 29.60 37.50 44.50 41.20 42.90 35.80 44.10 39.50 40.10 50.15 56.20 65.80 71.92 76.46 44.80 53.60 55.30 62.40 62.50 66.53 16.53 17.64 36.50 39.70 45.20 44.00 42.80 40.40 43.70 43.00 44. 51.01 53.90 70.00 71.44 76.36 GPT-4o-mini (2024-07-18) Claude-3.5-Sonnet (2024-06-22) Gemini-1.5-Flash (2024-09-24) GPT-4o (2024-08-06) Gemini-1.5-Pro (2024-09-24) Claude-3.7-Sonnet VITA-1.5 SliME LLaVA-OneVision-7B-ov Molmo-7B InternVL2-8B LLaVA-Critic-8B Llama-3.2-11B Pixtral-12B Molmo-72B Qwen2-VL-72B NVLM-D-72B MM-RLHF-Reward Llama-3.2-90B IXC-2.5-Reward R1-Reward Voting@ 5.1 Main Results We evaluate the performance of R1-Reward on three common multimodal reward model benchmarks. On the VLReward Bench  (Table 3)  , R1-Reward achieves the best overall performance, with an average accuracy of 71.92%. This represents roughly 9.3% improvement in overall accuracy compared to the previous best open-source model, IXC-2.5-Reward. Notably, IXC-2.5-Reward trains on more than 1 million samples, while our training data consists of 200k samples, highlighting significant improvement in data efficiency. In comparison to other open-source models, R1-Reward demonstrates larger margin of improvement. Among closed-source models, Gemini-1.5-Pro performs the best, but R1-Reward outperforms it across all dimensions, further demonstrating its superiority. On the Multimodal Reward Bench  (Table 4)  , R1-Reward achieves the best performance across all dimensions, with 14.3% improvement over the previous state-of-the-art. It is worth noting that the Multimodal Reward Bench is derived from over ten existing benchmarks and reconstructed into unified set, with minimal overlap with our training data. This further demonstrates R1-Rewards remarkable generalization ability across different datasets. The MM-RLHF-Reward Bench  (Table 5)  presents higher level of difficulty, particularly when directly utilizing language models as reward models. The best-performing model, Claude-3.7-Sonnet, achieves an accuracy of 65% on the Acc+ metric. Existing reward models perform well, with IXC-2.5Reward surpassing an Acc+ score of 50%, while the top reward model, MM-RLHF-Reward, exceeds 60%. However, MM-RLHF-Reward is trained on dataset that closely aligns with the distribution of this benchmark, which limits its generalization ability. As result, its performance on the VL Reward Benchmark is suboptimal. In contrast, R1-Reward demonstrates balanced performance across all benchmarks. Moreover, when performing voting on five sampled results, its accuracy reaches 85.3%, and when sampling 15 times, it reaches 86.47%significantly outperforming existing models. Test-Time Scaling. In Figure 4, we explore whether increasing the number of samples can lead to improved performance. It is important to note that the temperature is set to 1.0, which causes the result with single sample (k = 1) to slightly differ from the main results (which use greedy decoding by default). As the number of samples increases, the models performance improves consistently. Vote refers to majority-voting strategy, while Any counts as correct if at least one of the sampled results is correct. At = 15, the accuracy of Any approaches 100%, indicating that the R1-Reward (a) MM-RLHF Reward Bench (b) VL Reward Bench (c) Multimodal Reward Bench Figure 4: Inference-time performance scaling of R1-Reward on three benchmarks: (a) MM-RLHF Reward Bench, (b) VL Reward Bench, and (c) Multimodal Reward Bench. Accuracy is measured using two aggregation strategies as the number of inference samples (K) increases: Majority Vote and Any Correct. The Any Correct strategy (successful if at least one of the samples is correct) is highly sensitive to K, while the Majority Vote strategy shows more gradual improvement. Performance is compared against the previous SOTA result for each benchmark. has the potential to perfectly classify all samples; however, additional data or training strategies are needed to fully unlock this potential. Moreover, the Vote results demonstrate significant advantage over previous state-of-the-art models, with more noticeable improvement when < 5. The benefits from increasing the number of samples gradually diminish as more samples are added. This highlights the potential of R1-Reward in test-time scaling. 5.2 Ablations and Analysis R1-Reward Demonstrates High Data Efficiency. In Table 6, we compare the performance of traditional reward model (using two-layer MLP as the reward head) and MM-RLHF-Reward (which first generates critic and then generates the reward) trained on the same dataset. For MM-RLHF-Reward, the training data must include an evaluation for each response. To achieve this, we use GPT-4o to generate corresponding evaluations for each sample, which may be slightly less accurate than the human annotations used in the original work. All the models backbones are Qwen2.5-VL-7B-Instruct. As shown in the table, the traditional reward model, when trained with only 200K data samples, performs poorly. In most cases, MM-RLHF outperforms the traditional reward model. Its superior performance in the hallucination dimension is likely due to the generated critic. Comparing these two baselines, the reinforcement learning-based approach significantly enhances the reward modeling capabilities, even with the same amount of data. Moreover, our SFT approach shows advantages over both the traditional reward model and MM-RLHF-Reward. We believe this is primarily due to that we allow direct comparison of two responses during the scoring process, whereas existing methods score responses independently before comparing them. Ablation Studies of the StableReinforce Algorithm. We examine the impact of each component of the StableReinforce algorithm on the training process and final results. First, we emphasize the necessity of the Consistency Reward Function. Removing this function results in significant hallucination behaviors across different algorithms, making it challenging to achieve stable evaluation outcomes. Additionally, directly applying the Reinforce++ algorithm causes the model to crash, with the loss becoming NaN and the response length reaching the preset maximum length, while the output consists entirely of garbled text. In Table 6 and Figure 5, we present the effects of removing each module on final performance and changes in training dynamics. We observe that the Advantage Filter and Pre-Clip modules primarily ensure training stability by effectively removing outliers from the loss. Removing any of these components results in decreased final accuracy, reduced training stability, and the models output length failing to converge to shorter values. Aha Moment of R1-Reward. Through our task design and reward function formulation, the R1Reward model effectively learns the reward modeling task structure during the SFT phase. Following reinforcement learning, it reduces the length of reasoning to enhance efficiency. Visual examples of the models output appear in Figures 3 and 6. The model autonomously learns process to assess response quality. It first defines the goal, analyzes the image, attempts to solve the problem, and provides an answer. Based on this, the model evaluates Response 1 and Response 2, compares the two outputs, and gives final ranking. Simultaneously, the model demonstrates different reflection patterns. In Figure 3, the model encounters an error in its calculation, but after rechecking the bar 11 Table 4: Multimodal Reward Bench. Performance comparison of our reward model (R1-Reward) with existing open-source and proprietary counterparts. Model #Param Overall GPT-4o Gemini 1.5 Pro Claude 3.5 Sonnet Claude 3.7 Sonnet SliME VITA-1.5 Llama-3.2-Vision-Instruct Molmo-D-0924 Llama-3.2-Vision-Instruct InternVL-3 Qwen-2-VL MM-RLHF-Reward IXC-2.5-Reward R1-Reward Voting@15 - - - 8B 7B 11B 7B 90B 8B 72B 7B 7B 7B 7B 70.8 71.9 71.5 71.9 42.0 53.6 51.2 52.9 61.2 63.6 70.9 67.1 66. 82.2 83.3 General Correctness Preference Proprietary Models 62.6 63.5 62.6 58.4 69.0 67.7 67.8 60.7 Open-Source Models 42.3 55.6 57.8 56.8 60.0 59.6 56.4 52.2 54.3 65.8 59.4 68.4 61.6 62.3 Reward Models 67.5 64.2 61.7 60.7 Ours 77.5 78.0 74.0 77. Knowledge Reasoning Math Coding Safety VQA 72.0 66.3 73.9 78.1 47.5 52.5 55.5 54.6 61.2 60.5 70. 54.3 56.8 74.9 74.6 67.6 68.9 68.6 76.3 43.5 51.9 50.6 50.7 56.3 65.1 73.3 58.4 63.0 83.1 81. 62.1 55.5 65.1 71.3 35.3 52.8 51.7 53.4 53.1 56.6 58.9 57.9 50.5 79.6 85.8 74.8 94.5 76.8 72.0 19.1 58.1 20.9 34.8 52.0 59.3 90. 92.9 89.9 99.6 99.4 87.2 87.2 85.6 86.8 53.8 50.0 55.8 60.3 77.1 82.3 85.3 76.8 81.1 86.5 87. chart, it recognizes the mistake and recalculates to obtain the correct result. In Figure 6, the model misunderstands the problem. However, after outputting Wait, re-reading the question, it re-reads the question, eventually understands it correctly, and determines the correctness of the answer."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce R1-Reward, MRM trained using the StableReinforce algorithm. We demonstrate that RL can be effectively applied to reward modeling, significantly enhancing its performance. Our approach addresses key challenges, including training instability, the advantage normalization limitation, and inconsistencies between reasoning and results. By incorporating techniques such as pre-clipping, advantage filtering, consistency reward and progressive difficulty training strategy, StableReinforce stabilizes training and improves model performance. Experiments show that R1-Reward outperforms SOTA models on several multimodal reward model benchmarks, with significant improvements in accuracy and data efficiency. Furthermore, R1-Reward demonstrates excellent test-time scaling capabilities, and paves the way for future research on integrating reinforcement learning into MRMs. Looking ahead, there are still many areas to explore in RL for reward modeling. For example, we only test simple majority voting strategy for test-time scaling; more advanced methods could potentially improve performance further [26]. Additionally, improving training strategies to further enhance the foundational capabilities of reward models is also meaningful open problem."
        },
        {
            "title": "References",
            "content": "[1] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv, 2024. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv, 2025. [3] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-asa-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning, 2024. 12 Table 5: MM-RLHF-Reward Bench. Performance comparison of our reward model (R1-Reward) with existing open-source and proprietary counterparts. Models #Param Mcq Long Short Safety Video Acc Acc+ Gemini-2.0-Flash-Exp GPT-4o (2024-08-06) Claude-3.5-Sonnet (2024-06-22) Claude-3.7-Sonnet - - - - 33.33 64.28 64.28 66.67 45.94 78.37 67.56 91.89 67.64 44.11 55.88 91.18 Proprietary Models Open-Source Models SliME VITA Intern-VL-3 NVLM-D-72B Llama-3.2-90B Qwen2-VL-72B IXC-2.5-Reward MM-RLHF-Reward R1-Reward Voting@15 8B 7B 8B 72B 90B 72B 7B 7B 7B 7B 23.81 24.97 35.71 42.85 19.04 45.23 10.81 21.62 56.76 32.43 35.13 62.16 Reward Models 91.89 97.00 52.38 83.00 14.71 11.76 23.53 8.82 38.23 47.05 67.65 74. Ours 80.95 83.33 89.19 97.30 82.35 91.18 43.75 56.25 65.62 87.50 12.50 18.75 37.50 50.00 50.00 46. 62.50 69.00 75.00 78.12 32.00 40.00 60.00 76.00 7.52 12.40 32.00 40.00 40.00 36.00 88.00 88.00 44.71 58.23 62.94 82. 17.10 20.58 37.65 34.70 35.29 48.23 13.04 26.01 26.11 65.22 1.76 2.78 6.52 6.52 10.86 13.04 71.18 82.00 50.00 63.00 72.00 80. 80.59 86.47 54.35 67.39 Table 6: Evaluation results on VL Reward Bench comparing different models and training setups, including baselines, models trained with R1-Reward-200K, and ablation studies (Ours). # Data Models VL-Reward Bench General Hallucination Reasoning Overall Acc Baselines More than 1M MM-RLHF-120K MM-RLHF-Reward IXC-2.5-Reward R1-Reward-200K Reward Model R1-Reward-200K MM-RLHF-Reward 84.70 45.04 62.50 50.45 Trained by R1-Reward-200K 56.03 62. 56.71 61.01 R1-Reward-200K StableReinforce R1-Reward-200K wo advantage R1-Reward-200K wo pre-clip R1-Reward-200K Reinforce++ R1-Reward-200K Only Long-Cot SFT Ours 63.84 63.43 62.06 59.92 85.71 77.45 77. 72.27 62.90 57.55 48.67 59.30 64.78 62.38 61.23 Collapse 60. 65.80 50.15 56.41 60.80 71.92 68.96 67.36 64.80 [4] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv, 2024. [5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv, 2023. [6] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv, 2025. [7] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv, 2025. 13 (a) (c) (b) (d) Figure 5: Ablation studies of the StableReinforce algorithm, evaluating the impact of different components on policy loss and model response length. The subfigures compare the performance of the algorithm with and without specific components: (a) and (b) show results when the advantage filter is removed; (c) and (d) when Pre Clip is removed. Each method is evaluated in terms of policy loss and response length over training steps. [8] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv, 2024. [9] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv, 2025. [10] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv, 2024. [11] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv, 2025. [12] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Shaoqi Dong, Xiong Wang, Di Yin, Long Ma, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv, 2024. [13] Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, et al. Vita-1.5: Towards gpt-4o level real-time vision and speech interaction. arXiv, 2025. [14] Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, et al. Mme-survey: comprehensive survey on evaluation of multimodal llms. arXiv, 2024. [15] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv, 2025. 14 Figure 6: An example of the R1-Reward output. 15 [16] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv, 2024. [17] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv, 2025. [18] Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention. arXiv, 2025. [19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv, 2024. [20] Feng Li, Hao Zhang, Yi-Fan Zhang, Shilong Liu, Jian Guo, Lionel Ni, PengChuan Zhang, and Lei Zhang. Vision-language intelligence: Tasks, representation learning, and large models. arXiv, 2022. [21] Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, Lingpeng Kong, and Qi Liu. Vlrewardbench: challenging benchmark for vision-language generative reward models. arXiv, 2024. [22] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. Silkie: Preference distillation for large visual language models. 2023. [23] Yadong Li, Jun Liu, Tao Zhang, Song Chen, Tianpeng Li, Zehuan Li, Lijun Liu, Lingfeng Ming, Guosheng Dong, Da Pan, et al. Baichuan-omni-1.5 technical report. arXiv, 2025. [24] Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv, 2024. [25] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv, 2025. [26] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv, 2025. [27] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv, 2025. [28] Xingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie, and Junge Zhang. Uncertainty-aware reward model: Teaching reward models to know what is unknown. arXiv, 2024. [29] Jinda Lu, Junkang Wu, Jinghan Li, Xiaojun Jia, Shuo Wang, YiFan Zhang, Junfeng Fang, Xiang Wang, and Xiangnan He. Dama: Dataand model-aware alignment of multi-modal llms. arXiv, 2025. [30] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. arXiv, 2024. [31] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv, 2025. [32] Xiaoyu Tan Minghao Yang, Chao Qu. Inf-orm-llama3.1-70b, 2024. [33] OpenAI. Introducing openai o1-preview. 2024. [34] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 2022. [35] YingZhe Peng, Gongrui Zhang, Xin Geng, and Xu Yang. Lmm-r1. https://github.com/ TideDra/lmm-r1, 2025. [36] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv, 2025. [37] Shu Pu, Yaochen Wang, Dongping Chen, Yuhang Chen, Guohao Wang, Qi Qin, Zhongyi Zhang, Zhiyuan Zhang, Zetong Zhou, Shuang Gong, et al. Judge anything: Mllm as judge across any modality. arXiv, 2025. [38] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [39] Nicolas Le Roux, Marc Bellemare, Jonathan Lebensold, Arnaud Bergeron, Joshua Greaves, Alex Fréchette, Carolyne Pelletier, Eric Thibodeau-Laufer, Sándor Toth, and Sam Work. Tapered off-policy reinforce: Stable and efficient reinforcement learning for llms. arXiv, 2025. [40] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [41] Kun Shao, Zhentao Tang, Yuanheng Zhu, Nannan Li, and Dongbin Zhao. survey of deep reinforcement learning in video games. arXiv, 2019. [42] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv, 2024. [43] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv, 2024. [44] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv, 2025. [45] Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Yi-Fan Zhang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Ran He, Caifeng Shan, Rongrong Ji, and Xing Sun. Long-vita: Scaling large multi-modal models to 1 million tokens with leading short-context accuracy, 2025. [46] Bharat Singh, Rajesh Kumar, and Vinay Pratap Singh. Reinforcement learning in robotic applications: comprehensive survey. Artificial Intelligence Review, 2022. [47] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf. 2023. [48] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv, 2024. [49] Llama3 Team. The llama 3 herd of models. arXiv, 2024. [50] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv, 2024. [51] Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, Lewei Lu, Haodong Duan, Yu Qiao, Jifeng Dai, and Wenhai Wang. Visualprm: An effective process reward model for multimodal reasoning, 2025. [52] Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models, 2024. 17 [53] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv, 2024. [54] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv, 2025. [55] Wulin Xie, Yi-Fan Zhang, Chaoyou Fu, Yang Shi, Bingyan Nie, Hongkai Chen, Zhang Zhang, Liang Wang, and Tieniu Tan. Mme-unify: comprehensive benchmark for unified multimodal understanding and generation models. arXiv, 2025. [56] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. CVPR, 2024. [57] Michihiro Yasunaga, Luke Zettlemoyer, and Marjan Ghazvininejad. Multimodal rewardbench: Holistic evaluation of reward models for vision language models. arXiv, 2025. [58] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv, 2025. [59] Tao Yu, Chaoyou Fu, Junkang Wu, Jinda Lu, Kun Wang, Xingyu Lu, Yunhang Shen, Guibin Zhang, Dingjie Song, Yibo Yan, et al. Aligning multimodal llm with human preference: survey. arXiv, 2025. [60] Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, Xiaocheng Feng, Jun Song, Bo Zheng, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Rlaif-v: Open-source ai feedback leads to super gpt-4v trustworthiness. CVPR, 2024. [61] Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, et al. Self-generated critiques boost reward modeling for language models. arXiv, 2024. [62] Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, et al. Internlm-xcomposer2. 5-reward: simple yet effective multi-modal reward model. arXiv, 2025. [63] Jiawei Zhang, Tianyu Pang, Chao Du, Yi Ren, Bo Li, and Min Lin. Benchmarking large multimodal models against common corruptions. arXiv, 2024. [64] Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. Llava-mini: Efficient image and video large multimodal models with one vision token, 2025. [65] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. arXiv, 2024. [66] Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, et al. Mm-rlhf: The next step forward in multimodal llm alignment. arXiv, 2025. [67] Yi-Fan Zhang, Weichen Yu, Qingsong Wen, Xue Wang, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Debiasing multimodal large language models. arXiv, 2024. [68] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? ICLR, 2024. [69] Jiaxing Zhao, Xihan Wei, and Liefeng Bo. R1-omni: Explainable omni-multimodal emotion recognition with reinforcing learning. arXiv, 2025. [70] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv, 2024. [71] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv, 2025."
        }
    ],
    "affiliations": [
        "CASIA",
        "KuaiShou",
        "NJU",
        "THU"
    ]
}