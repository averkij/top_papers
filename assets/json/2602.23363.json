{
    "paper_title": "MediX-R1: Open Ended Medical Reinforcement Learning",
    "authors": [
        "Sahal Shaji Mullappilly",
        "Mohammed Irfan Kurpath",
        "Omair Mohamed",
        "Mohamed Zidan",
        "Fahad Khan",
        "Salman Khan",
        "Rao Anwer",
        "Hisham Cholakkal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com"
        },
        {
            "title": "Start",
            "content": "MediX-R1: Open Ended Medical Reinforcement Learning Sahal Shaji Mullappilly * 1 Mohammed Irfan Kurpath * 1 Omair Mohamed 2 Mohamed Zidan 3 Fahad Khan 1 Salman Khan 1 Rao Anwer 1 Hisham Cholakkal 1 1Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI) 2Jubilee Mission Medical College and Research Institute, 3 JJM Medical College 6 2 0 2 6 2 ] . [ 1 3 6 3 3 2 . 2 0 6 2 : r Figure 1. Average accuracy across multimodal medical benchmarks vs. training dataset size for recent medical VLMs. Colors denote model families; marker shape/size indicates parameter scale (2B, 8B, 30B). denote open-source availability of training data (*as of 25/02/2026). MediX-R1 8B (68.8%) surpasses MedGemma 27B (68.4%) while using significantly less training data, and MediX-R1 30B achieves the highest overall accuracy (73.6%). All training and evaluation resources are available at MediX-R1."
        },
        {
            "title": "Abstract",
            "content": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 finetunes baseline vision-language backbone with Group Based RL and composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with strict YES/NO decision, medical embeddingbased semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable * Equal contribution. 1 reasoning and modality recognition. This multisignal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose unified evaluation framework for both text-only and image+text tasks that uses Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only 51K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical MediX-R1: Open Ended Medical Reinforcement Learning Model MedVLM-R1 (Pan et al., 2025) BiMediX2 (Mullappilly et al., 2024) HuatuoGPT-V (Chen et al., 2024b) MedGemma (Sellergren et al., 2025) MedMO (Deria et al., 2026) MediX-R1 Diverse Medical Modalities Single-Stage RL Interpretable Open-Ended Annotation-Free Composite RL Reward Reasoning Reasoning Responses Table 1. Differences with existing Medical VLMs: MediX-R1 integrates diverse modalities, interpretable reasoning, and composite RL rewards, enabling practical clinical use. tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLMbased evaluation is practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at MediX-R1. 1. Introduction Large medical language and vision-language models are increasingly deployed for clinical question answering, triage support, report drafting, and education (Chen et al., 2024a; Sellergren et al., 2025; Pieri et al., 2024). Many of these tasks are inherently open-ended: clinicians expect concise but free-form answers that can flexibly incorporate context, uncertainty, and multimodal evidence. However, most training and evaluation pipelines remain tailored to Multiple Choice Questions (MCQ) or string-matching regimes, which (i) under-reward valid clinical paraphrases, (ii) fail to measure reasoning quality or modality recognition, and (iii) do not provide reliable signals for reinforcement learning (RL) in open-ended settings. As result, models trained only with supervised objectives or MCQ-style rewards often struggle to produce faithful, interpretable, and robust clinical responses across diverse modalities. RL has improved reasoning in domains with verifiable rewards (e.g., math and code) as shown by DeepSeek models (Shao et al., 2024; Guo et al., 2025), but medical tasks rarely admit executable checks. Binary exact match is too brittle for clinical phrasing; BLEU/ROUGE can mis-score semantically correct answers; and free-form VLM outputs complicate visual inference. Moreover, using single reward signal can induce instability or reward hacking, especially when the signal is noisy or overly permissive. Hence, it is desirable to have principled approach for training medical MLLMs with open-ended RL that integrates semantic correctness with structural and modality constraints, while remaining dataand compute-efficient. We present MediX-R1, an open-ended medical RL framework that fine-tunes baseline multimodal backbone with 2 Group Based RL (GRPO/GSPO/DAPO) using composite reward tailored for clinical reasoning. Our design combines: (1) an LLM-based accuracy reward that enforces strict YES/NO decision on semantic correctness, (2) medical embedding-based semantic reward that captures paraphrases and terminology variants, (3) lightweight format reward that elicits interpretable reasoning traces, and (4) modality recognition reward that discourages crossmodality hallucinations by requiring explicit modality tags. This multi-signal objective stabilizes optimization and supplies informative feedback where traditional verifiable or MCQ-only rewards fall short, enabling single-stage, openended RL directly on clinical tasks. Differences with existing Medical VLMs: Table 1 contrasts MediX-R1 with strong open models across key clinical capabilities. First, on Diverse Medical Modalities, MediX-R1 supports diverse medical modalities including X-Ray, CT, MRI, Microscopy/Histopathology, Ultrasound, Fluoroscopy, Endoscopy, Angiography, Mammography, Clinical Photography, SPECT (Single Photon Emission Computed Tomography), OCT (Optical Coherence Tomography), and Fundus imaging, whereas MedVLMR1 (Pan et al., 2025) is limited to radiology images. Models like MedGemma (Sellergren et al., 2025), HuatuoGPTVision (Chen et al., 2024b), MedMO (Deria et al., 2026), and BiMediX2 (Mullappilly et al., 2024) provide coverage on clinical modalities but they require extensive multistage training. On Single-Stage RL, most baselines rely on multi-stage pipelines (pretraining SFT RL), whereas MediX-R1 is trained end-to-end with single RL stage using our composite reward (Sec. 2.2). This simplifies training and, importantly, enables open-ended RL directly (unlike MedVLM-R1), because the Reference-based LLM-as-judge accuracy signal and medical embeddings provide reliable feedback beyond MCQ exact match. The composite design (format + LLM judge + embeddings + modality recognition) stabilizes optimization and reduces reward hacking  (Fig. 5)  , translating into the best average performance in Table 2. For Interpretable Reasoning, MediX-R1 emits explicit reasoning traces enclosed in <think>...</think>, enforced by format reward, making the decision path MediX-R1: Open Ended Medical Reinforcement Learning Figure 2. MediX-R1: Overall Architecture The MediX-R1 reinforcement learning framework for open-ended medical reasoning. An input of medical image and natural language question is processed by MediX-R1. The models policy is trained using Group Based RL, which leverages multi-faceted reward signal. This reward is composed of: a) an LLM-based reward for evaluating the overall quality and correctness of the output; b) an embedding-based reward to ensure semantic alignment; c) format reward to enforce the desired output structure (<think> and <answer> blocks); and d) modality reward to ensure the response is grounded in the specified imaging modality. This reward-guided approach encourages the model to generate accurate and interpretable reasoning paths. auditable. Several baselines do not reliably produce structured clinical rationales. While multiple models support Open-Ended Responses, MediX-R1 is explicitly optimized for free-form clinical answering with modality recognition, which curbs cross-modality hallucinations and improves VLM robustness. Finally, MediX-R1 achieves AnnotationFree Reasoning: it does not require human-curated rationales or verified chain-of-thought. The RL rewards operate on the final answer only (via Reference based LLM judge and embeddings), significantly lowering data curation cost while still encouraging faithful, interpretable reasoning. Together, these properties explain the consistent gains across both text-only and image+text benchmarks and the practical advantages of MediX-R1 for clinical use. To measure progress, we introduce unified, 3-stage Reference-based LLM-as-judge evaluation framework that supports both text-only and image+text tasks under common protocol. By replacing brittle string-overlap metrics with instruction-tuned judges served via vLLM (Kwon et al., 2023), our evaluation captures semantic correctness, reasoning adequacy, and contextual alignment, and scales from short-form QA to long-form report generation. This reduces evaluation-clinical utility mismatch. Despite using only 51K instruction examples, MediX-R1 achieves strong results across diverse medical benchmarks. We find that composite rewards not only improve accuracy but also mitigate reward hacking and reduce volatility, yielding stable training and interpretable outputs. Compared to open-source medical models (e.g., BiMediX2, MedGemma, HuatuoGPT-V, MedVLM-R1, MedMO), MediX-R1 combines broad modality coverage with single-stage RL and structured reasoning. Contributions: (i) We introduce open-ended medical reinforcement learning by extending Group based RL with tailored rewards for clinical reasoning. (ii) We design composite reward with LLM-based accuracy and medical semantic signals that for the first time enables open-ended responses with RL in the medical domain and stabilizes training. (iii) We propose three-stage Reference-based LLM-asjudge evaluation framework that unifies benchmarking for both LLM (text-only) and VLM (image+text) tasks in the medical setting. (iv) MediX-R1 achieves excellent LLM and VLM results with single-stage RL recipe using 51K instructions, validated through both Reference-based LLM-asjudge and human expert evaluations. (v) Finally, we demonstrate the effectiveness of the proposed composite reward on Group based RL algorithms, achieving consistent performance gains with GRPO (Shao et al., 2024), DAPO (Yu et al., 2025) and GSPO (Zheng et al., 2025a). Moreover, we have conducted experiments on different baseline VLMs, including Qwen2.5-VL, Qwen3-VL (Team, 2025), and SmolVLM2 (Marafioti et al., 2025), and achieved consistent performance gains across backbones. 2. Open Ended Medical RL MediX-R1 fine-tunes baseline multimodal backbone for open-ended medical reasoning using Group Based RL. Given an image and question q, the vision encoder produces visual tokens that are fused with text tokens and fed to the LLM policy πθ. The model generates structured outputs of the form: [modality tag]<think>free-form clinical reasoning</think><answer>final concise answer</answer> 2.1. Group-based RL with Composite Rewards Setup: Given an input (image + question q) drawn from (V), we sample group of candidate completions 3 MediX-R1: Open Ended Medical Reinforcement Learning Figure 3. Evaluation Framework Our three-stage evaluation pipeline: (1) Generation via vLLM inference on the model under test, (2) Evaluation using Reference-based LLMas-judge with BASE and MIMIC templates, and (3) Scoring through aggregation of judgment outputs. The framework supports both binary decisions for QA/MCQ tasks and rubric-based scoring for long-form reports, ensuring robust evaluation across diverse medical benchmarks groups with identical rewards) and can apply overlong reward shaping to softly penalize excessively long outputs. GSPO (sequence-level ratio for stability): GSPO addresses variance from token-wise importance ratios by replacing ri,t(θ) with length-normalized sequence-level ratio shared across all tokens in completion. GSPO then uses the same clipped objective form but with si(θ) in place of token-wise ratios: JGSPO(θ) = Ev, {oi} (cid:34) 1 G (cid:88) i=1 (cid:16) min si(θ)Ai, clip(si(θ), 1 ϵ, 1 + ϵ)Ai (cid:35) (cid:17) (3) Because all tokens within sequence share the same ratio, GSPO reduces intra-sequence variance and tends to be more stable in settings where token-level ratios are noisy (e.g., long outputs and dynamic architectures). 2.2. Reward Design We define composite reward: = wfmt Rformat + wllm Rllm + wemb Rembed + wmod Rmodality with coefficients chosen via simple, staged ablation procedure. In all experiments we keep small, fixed weight on the format reward to enforce parseable outputs, and distribute the remaining weight across semantic correctness (LLM judge / embeddings) and modality grounding. The concrete coefficient selection procedure is described in Sec. 4.3. Why this enables open-ended medical RL. Unlike prior RL setups that are limited to verifiable signals or MCQ-style accuracy (e.g., exact match, executable or rule-based graders), our LLM-based accuracy reward Rllm and embedding-based semantic reward Rembed provide reliable feedback for freeform, clinically grounded answers. The Reference-based LLM-as-judge converts semantic correctness into robust YES/NO decision under paraphrase and clinical phrasing, while medical-domain embeddings supply complementary content-alignment signal. This dual signal makes Reinforcement Learning viable for open-ended medical reasoning; {oi}G i=1 from the frozen behavior policy πθold( v). Each completion receives scalar reward ri computed by our composite reward (Sec. 2.2). We then compute standardized group-relative advantage: Ai = ri mean({rj }G std({rj }G j=1) j=1) . This removes the need for learned value function while preserving stable relative learning signal within group. GRPO objective: GRPO (Shao et al., 2024) updates πθ using PPO-style clipping on an importance ratio and KL regularizer to fixed reference policy πref: JGRPO(θ) = Ev, {oi} clip(ρi(θ), 1 ϵ, 1 + ϵ)Ai (cid:34) (cid:17) 1 (cid:88) i=1 (cid:16) min ρi(θ)Ai, (cid:35) (1) β DKL(πθ πref) where ρi(θ) = πθ(oiv) and regularization strength. πθold (oiv) , and ϵ, β 0 control clipping Composite reward with different optimizers: In addition to GRPO, we run the same composite reward with two recent Group based RL family optimizers, DAPO (Yu et al., 2025) and GSPO (Zheng et al., 2025a)and report their comparison in Table 6. DAPO (efficiency-focused refinements): DAPO keeps the GRPO/PPO clipped structure but improves token efficiency (as summarized in Yu et al. (2025)): it (i) uses asymmetric clipping (Clip-Higher) with larger upper bound to avoid prematurely zeroing gradients for rare-but-good tokens, and (ii) averages loss over all generated tokens rather than per-sample averaging (reducing gradient dilution for long responses). compact form is: JDAPO(θ) = Ev, {oi} (cid:34) 1 i=1 oi (cid:80)G (cid:88) oi (cid:88) i=1 t=1 min (cid:16) ri,t(θ)Ai, clip(ri,t(θ), 1 ϵlow, 1 + ϵhigh)Ai (2) (cid:35) (cid:17) where ri,t(θ) = πθ(oi,tv,oi,<t) πθold (oi,tv,oi,<t) . DAPO further encourages rollouts via dynamic sampling (avoiding degenerate 4 MediX-R1: Open Ended Medical Reinforcement Learning the format (Rformat) and modality (Rmodality) rewards act as structural regularizers, but Rllm and Rembed are the primary drivers of open-ended RL in MediX-R1. LLM-Based Accuracy Reward (Rllm) We parse the model outputs final answer between <answer>...</answer> and compare it to the reference answer using compact Reference-based LLM-as-judge prompt that forces strict YES/NO decision. Concretely, local vLLM endpoint (Qwen3-4B) returns YES if the candidate semantically answers the reference, and NO otherwise; we map YES(cid:55) 1, NO(cid:55) 0. This captures correctness and robustness to paraphrasing while keeping the signal discrete. Embedding-Based Reward (Rembed) To further encourage semantic alignment, we compute cosine similarity between the predicted answer and the reference using medical embedding model MedEmbed-large (Balachandran, 2024). We convert it to binary reward via threshold (default 0.8): Rembed=1[cos(epred, eref) τ ]. This complements the LLM judge and helps capture terminological variants. Format Reward (Rformat) We enforce structured outthe exact pattern puts by matching the regex for <think>...</think> <answer>...</answer> after normalizing stray whitespace around angle brackets. Outputs that match receive 1, else 0. This stabilizes training and improves interpretability of the medical reasoning. Modality recognition Reward (Rmodality) We encourage explicit grounding to the imaging modality by requiring the model to emit the predicted modality tag before the </think> block (case-insensitive). We compare it to the reference modality tag and assign 1 on match, 0 otherwise. This reduces cross-modality hallucinations (e.g., describing CT findings on an X-ray). 3. Evaluation Framework Our evaluation pipeline has three stages: Generation, Evaluation, and Scoring. We evaluate across both text-only (LLM) and image+text (VLM) tasks covering QA, MCQ, and long-form report tasks. Generation. We run batched inference via vLLM on the model under test and persist the full response per sample. For models that emit structured reasoning, we retain the entire output but, for scoring, discard internal chains-ofthought by stripping content up to and including the closing </think> tag, evaluating only the final answer block. Evaluation: We employ separate Reference-based LLMas-judge, Qwen3-14B (Team, 2025), served with vLLM for throughput and stability on modest GPUs. Two prompt families are used: BASE template (A.8) for open-ended, one-word, and MCQ-style questions that yields binary decision, and MIMIC template (A.9) for long-form report generation that scores along clinical criteria. For example, on visual question answering item asking What does the dark blue color on the laser speckle contrast analysis perfusion image represent? with ground truth Low perfusion model response that includes hidden reasoning and the final answer Low blood flow or less perfusion is judged correct and assigned score of 1. The judge compares predicted answers against references, accounting for paraphrase and clinically equivalent phrasing. Scoring: We aggregate judgment outputs to dataset-level metrics. For binary evaluations, we report mean accuracy over samples. For long-form, we average the scalar rubric scores across samples, optionally normalizing for comparability. We also compute macro averages across benchmarks. Why Reference-based LLM-as-judge (via vLLM): Traditional string-overlap metrics (BLEU, ROUGE, F1) for reference (ground truth) comparison often under-reward correct, clinically appropriate paraphrases and cannot assess justification quality or contextual alignment. Referencebased LLM judge captures semantic correctness, clinical reasoning, and adherence to task-specific criteria through carefully designed prompts, while vLLM serving ensures consistent, fast, and reproducible evaluations. 4. Experiments and Results 4.1. State-of-the-art Comparisons We evaluate MediX-R1 on comprehensive suite of medical language and vision-language benchmarks, covering both text-only (LLM) and image+text (VLM) tasks. The evaluation includes standard medical QA, multiple-choice, and open-ended report generation, as well as visual question answering and clinical image interpretation. The datasets used for evaluation are as follows: LLM (text-only) benchmarks: MMLU-Clinical, MMLUBio, MMLU-Med, MMLU-Genetics, MMLU-ProfMed, MMLU-Anat (Hendrycks et al., 2020), MedMCQA (Pal et al., 2022), MedQA (Jin et al., 2021), USMLE-SA (Han et al., 2023), PubMedQA (Jin et al., 2019), MIMIC-CXRSummarization (Johnson et al., 2016). VLM (image+text) benchmarks: SLAKE-VQA (Liu et al., 2021), RadVQA (Lau et al., 2018), PathVQA (He et al., 2020), PMC-VQA (Zhang et al., 2024), PMC-VQA-Hard, MIMIC-CXR-Report Generation (Johnson et al., 2019). For each dataset, we follow the evaluation protocol described in the previous section, using Reference-based LLMas-judge scoring for both short-form and long-form responses. Table 2 summarizes performance on our unified benchmark suite across several open-source medical models, including MedVLM-R1 (2B), BiMediX2 (8B), HuatuoGPTV (7B), MedGemma (4B/27B), MedMO (8B) (Deria et al., 5 MediX-R1: Open Ended Medical Reinforcement Learning Benchmarks MMLU-Clinical MMLU-Bio MMLU-Med MMLU-Genetics MMLU-ProfMed MMLU-Anat MedMCQA MedQA USMLE-SA PubMedQA MIMIC-CXR-Sum SLAKE-VQA RadVQA PathVQA PMC-VQA PMC-VQA-Hard MIMIC-CXR-Gen AVG MedVLM R1 2B BiMediX2 8B Huatuo GPT-V 7B MedGemma 4B MedGemma 27B MedMO 8B MediX-R1 2B MediX-R1 8B MediX-R1 30B 0.540 0.549 0.451 0.560 0.500 0.519 0.408 0.400 0.378 0.520 0. 0.434 0.404 0.239 0.398 0.020 0.240 0.427 0.732 0.792 0.694 0.790 0.695 0.659 0.572 0.583 0.591 0.520 0.672 0.468 0.530 0.323 0.482 0.229 0.124 0.556 0.721 0.708 0.653 0.710 0.625 0.600 0.511 0.534 0.538 0.542 0. 0.545 0.614 0.374 0.532 0.261 0.316 0.558 0.708 0.706 0.605 0.820 0.713 0.556 0.570 0.621 0.639 0.470 0.692 0.678 0.659 0.317 0.444 0.214 0.205 0.566 0.879 0.972 0.866 0.940 0.912 0.793 0.727 0.866 0.895 0.414 0. 0.634 0.585 0.322 0.478 0.354 0.224 0.684 0.864 0.951 0.827 0.900 0.890 0.785 0.662 0.848 0.742 0.586 0.709 0.479 0.419 0.272 0.360 0.177 0.084 0.621 0.660 0.806 0.699 0.680 0.581 0.563 0.492 0.497 0.505 0.472 0. 0.654 0.539 0.428 0.491 0.284 0.280 0.554 0.845 0.951 0.879 0.900 0.868 0.763 0.683 0.796 0.822 0.482 0.746 0.703 0.596 0.455 0.554 0.317 0.328 0.688 0.894 0.993 0.890 0.980 0.974 0.874 0.781 0.929 0.951 0.490 0. 0.683 0.625 0.445 0.571 0.307 0.350 0.736 Table 2. Evaluation Benchmark. The top section lists LLM (text-only) tasks and the bottom section lists VLM (image+text) tasks. Our three-stage evaluation setting evaluates both tasks in unified framework. MediX-R1 achieves the highest average score across this diverse suite, demonstrating state-of-the-art performance among open models. Best and second best results are bold and underlined 2026) and MediX-R1 (2B/8B/30B). Model MMMU Medical Validation MediX-R1 achieves the highest average score across all benchmarks, outperforming prior models on both language and vision-language tasks. Notably, it demonstrates strong gains on open-ended and clinically complex tasks such as MIMIC-CXR summarization and report generation, as well as robust performance on standard QA and VQA datasets. These results highlight the effectiveness of our open-ended RL training and composite reward design, which enable MediX-R1 to generate accurate, semantically aligned, and clinically grounded responses beyond the capabilities of models trained only with supervised or MCQ objectives. We additionally report results on the Massive Multidiscipline Multimodal Understanding and Reasoning (MMMU) benchmark (Yue et al., 2024). We select the Health & Medical validation subset (MMMU-Med-Val), covering Basic Medical Science, Clinical Medicine, Diagnostics and Laboratory Medicine, Pharmacy, and Public Health. Results are shown in Table 3. 4.2. Ablation Experiments Composite Reward across RL algorithms: Our RLthe proposed method ablation in Table 6 shows that composite-reward training transfers across different RL frameworks, consistently improving over the baseline across both LLM and VLM tasks. DAPO (Yu et al., 2025) achieves the best overall average (0.610), compared to GRPO (0.59), GSPO(0.600), and the baseline Qwen2.5-VL (0.570). Reward Design Ablation: Table 4 compares reward variMedVLM-R1 2B (Pan et al., 2025) MedGemma 4B (Sellergren et al., 2025) HuatuoGPT-V 7B (Chen et al., 2024b) BiMediX2 8B (Mullappilly et al., 2024) Qwen3-VL 8B (Yang et al., 2025) MedMO 8B (Deria et al., 2026) Qwen3-VL 30B (Yang et al., 2025) MedGemma 27B (Sellergren et al., 2025) Lingshu 32B (Team et al., 2025) MediX-R1 2B MediX-R1 8B MediX-R1 30B 39.33 50.00 47.33 39.33 62.66 62.66 68.66 56.66 62.66 44.66 64.00 75.33 Table 3. MMMU Medical Val results. Accuracy on the Health & Medical validation subset of MMMU (image-text). ants that differ in which non-format signals are active (all settings include the same Rformat). The Default reward baseline that uses basic string matching against the ground truth is brittle to paraphrases and clinical synonymy. Using only the embedding reward underperforms on text-only evaluations (0.640) and provides gains on VLM (0.409), suggesting that thresholded cosine similarity alone lacks discriminative power for nuanced clinical reasoning. Using only the LLM judge improves text-only accuracy (0.666) but does not help VLM (0.400), indicating that the judge alone is insufficient to enforce modality grounding. All reward design models are compared with checkpoints before reward hacking. Combining LLM + embedding increases robustness to paraphrase and terminology variants, improving text-only scores (0.686) and yielding small VLM lift (0.410), which raises the overall average to 0.589. Finally, the full MediX-R1 composite (LLM accuracy + embedding semantics + modal6 MediX-R1: Open Ended Medical Reinforcement Learning Figure 4. Qualitative examples of MediX-R1. (Top, Microscopy) Correctly identifies the optic tract in section with interpretable reasoning. (Bottom, X-ray) Explains why heart size appears smaller in PA vs. AP view. MediX-R1 generates clinically grounded, open-ended answers across modalities. ity recognition, with shared format control) produces the strongest image+text performance (0.431) while matching the best text-only result (0.687), achieving the best overall average (0.597). Together with Fig. 5, which shows reduced volatility and fewer signs of reward hacking, these results suggest that the composite reward both improves aggregate performance and stabilizes optimization. Key takeaways: (i) The LLM judge is the strongest single signal for text correctness, and embeddings complement it by reducing false negatives from paraphrases.(ii) Default string matching degrades substantially on open-ended multimodal evaluations.(iii) Modality recognition is important for VLM tasks and drives substantial gains in image+text tasks; the full composite delivers the best overall results. Performance across VLM backbones: We observe consistent improvements across VLM backbones after training these models in our MediX-R1 framework with our composite rewards as shown in Table 5. These results show that MediX-R1 enhances open-ended medical reasoning ability across backbone models. Judge Robustness and Evaluator sensitivity. To ensure robustness, we perform controlled evaluations using deterEvaluations Default Embedding Reward Reward LLM LLM + MediX-R1 Reward Embedding LLM Tasks VLM Tasks Overall AVG 0.660 0.382 0.562 0.640 0.409 0. 0.666 0.400 0.572 0.686 0.410 0.589 0.687 0.431 0. Table 4. Reward ablation across validation benchmarks. Single signals like default string matching, embedding-only or LLM-only are weaker. Combining LLM + embedding improves robustness, and the MediX-R1 composite (LLM accuracy + embedding-based semantics + modality recognition) yields the best overall average. ministic generation settings (temperature=0, top p=1) and report averages over three runs, observing only 0.002 variation. For additional validation, we replaced Qwen314B with GPT-5.1 and GPT-5 mini as evaluators, which resulted in deviation of only 0.005, indicating high consistency across judge models. 4.3. Reward Hacking and Mitigation In reinforcement learning, Reward Hacking occurs when model maximises its reward in unintended ways, often bypassing the true objective. It arises when the policy exploits 7 MediX-R1: Open Ended Medical Reinforcement Learning Model Baseline + Composite Rewards SmolVLM2-2.2B (Marafioti et al., 2025) Qwen3-VL-2B (Yang et al., 2025) Qwen3-VL-8B (Yang et al., 2025) Qwen3-VL-30B (Yang et al., 2025) 0.410 0.529 0.666 0.698 0.432 0.554 0.688 0. Table 5. Baseline comparison across backbones (overall AVG). Baseline is the original backbone; +Composite Rewards applies our RL with composite rewards on the same backbone. Evaluations Baseline GRPO GSPO DAPO LLM Evaluations (text only) VLM Evaluations (image + text) Overall AVG 0.675 0. 0.570 0.687 0.431 0.597 0.689 0.439 0.600 0.701 0. 0.610 Table 6. RL Method ablation across benchmarks. Using the same composite reward, different RL algorithms (GRPO/GSPO/- DAPO) consistently outperform the baseline across both LLM and VLM tasks, with DAPO achieving the highest overall average. imperfections in single reward signal to earn high scores without producing clinically correct answers. We observed two concrete modes (examples abbreviated): Embedding model exploit When using Embedding models like MedEmbed-large (Balachandran, 2024) short or non-semantic tokens can spuriously yield high cosine similarity. For instance, candidate that outputs <answer>-</answer> for What does the white arrow point to in image B? received Rembed=1.0 against the ground truth Renal artery, despite being incorrect. LLM judge exploit When using LLMs like Qwen34B (Team, 2025) as rewarder template-like placeholders can confuse the judge when the reference is provided for comparison. E.g., <answer>The largest organ in the picture is [insert your answer here based on the medical reasoning provided above].</answer> was judged correct (Rllm=1.0) against the reference Lung. Mitigation in MediX-R1 To curb these failures, MediX-R1 employs composite reward and input/output constraints: (i) Composite objective: Rllm + Rembed + Rmodality (with shared Rformat) reduces reliance on any single brittle signal and penalizes mismatches in content or modality recognition  (Table 4)  . (ii) Embedding gating: set Rembed=0 for answers below minimum character/word length, with high punctuation or non-alphanumeric ratio; strip punctuation before embedding; calibrate the similarity threshold. (iii) Modality recognition: Rmodality requires correct modality tag, curbing visually ungrounded shortcuts that might still fool text-only rewards. (iv) Structural control and regularization: Rformat enforces parseable outputs; Group relative advantage and KL penalty to the reference reduce collapse to degenerate hacks by discouraging outlier behaviors. (v) Reward coefficient selection (ablation-driven): To make the reward design transparent, we selected coefficients via Figure 5. Overall validation reward vs training step across reward designs. Training with individual signals LLM-only or embedding-only shows volatility and reward hacking, while LLM+embedding reduces but does not eliminate instability. MediX-R1 uses composite reward which stabilizes learning and delivers the highest final reward and best overall performance. staged procedure rather than an exhaustive hyperparameter search. Concretely, we fixed wfmt=0.10 in all experiments and allocated the remaining 0.90 mass to the task-facing signals. This yields the ablation settings in Table 4, e.g., embedding-only = 0.1Rformat + 0.9Rembed and LLM-only = 0.1Rformat + 0.9Rllm. For the combined semantic reward, we evaluated small set of intuitive splits between Rllm and Rembed while keeping wfmt fixed; performance was similar across these choices on our validation benchmark, and we selected the configuration that slightly favored the LLM judge. Finally, when adding modality grounding, we reserved 5% of the non-format budget for Rmodality, and renormalized the remaining non-format weights. (See Appendix A.3 for full coefficient-selection details, including all Rllm/Rembed splits.) Together, these measures mitigate reward hacking and stabilize training, leading to smoother reward trajectories and higher final performance (see Fig. 5). 4.4. Human Expert Evaluation To assess the clinical quality of model outputs, we conducted human expert evaluation using blind review setup (See Evaluation Protocol in A.5). For randomly selected subset of questions from our Evaluation benchmark, responses were generated by four models: MediX-R1, Llama3.2-Vision, MedGemma and HuatuoGPT-Vision. The outputs were anonymized and labeled as Model A, Model B, Model and Model with no identifiers provided to the reviewers. Medical experts were asked to evaluate the responses against the provided ground truth descriptions for each question. The evaluation focused on determining which model produced the most accurate, clinically relevant response along with interpretable reasoning traces. MediX-R1: Open Ended Medical Reinforcement Learning The results demonstrate strong preference for MediX-R1, which was selected as the best response in 72.7% of the cases. In comparison, Llama3.2-Vision was preferred in 13.6% of the cases, MedGemma in 9.2% and HuatuoGPTVision in 4.5% of the cases. Additional details on human expert evaluation is available in Sec. A.5 and Sec. A.6. 4.5. Evaluation on Real World Clinical Data To further assess the generalization ability of our model, we conducted additional evaluation on MedPix 2.0 (Siragusa et al., 2025), publicly available real-world clinical VQA dataset derived from the original MedPix (Henigman & Kennedy, 2025) database maintained by the U.S. National Library of Medicine (NIH). MedPix comprises over 12,000 anonymized, crowdsourced clinical cases containing medical images and corresponding textual information such as findings, diagnoses, and treatments. This ensures both reproducibility and compliance with NIH privacy standards. The evaluation on MedPix 2.0 demonstrates that our model, MediX-R1, consistently outperforms other medical visionlanguage models. Specifically, MediX-R1 achieves score of 51.11%, surpassing strong baselines and previous SOTA Medical Models as shown in Table 7. These results further confirm the robustness and adaptability of MediX-R1 on diverse real-world clinical data, emphasizing its capability to generalize beyond controlled experimental environments. Model MedVLM-R1 MedGemma LLaVA-Med BiMediX2 HuatuoGPT MediX-R1 (Ours) Score (%) 27.57 43.18 44.29 46.51 48.81 51.11 Table 7. Performance comparison on the MedPix 2.0 dataset. 4.6. Qualitative Examples Fig. 4 illustrates how MediX-R1s structured outputs and composite reward translate into clinically grounded behavior across modalities. Microscopy (top) Given multi-panel histological image and the question Which area is shown in section of the image?, the model (i) correctly emits the modality tag (MICROSCOPY), (ii) provides interpretable reasoning inside <think> that references recognizable neuroanatomical markers (e.g., optic tract ot, superior thalamic nucleus stm), stain patterns, and panel context, and (iii) produces concise final answer: the optic tract. The modality recognition and format rewards ensure the answer is localized to the requested panel and presented cleanly in the <answer> block, while the LLM and embedding rewards bias the policy toward semantically correct iden9 tification despite diverse phrasing in the reasoning. X-ray (bottom) For Is the heart size in this image smaller or larger than if the image was taken AP?, the model tags the modality as RAY and reasons about projection geometry: PA views reduce cardiac magnification relative to AP due to shorter heart-to-detector distance and standard source-toimage distance. The model explains this in <think> and answers smaller in <answer>. This example shows the model using domain knowledge rather than superficial pattern matching, with the final answer isolated for scoring (the judge ignores <think> during evaluation). 5. Conclusion We presented MediX-R1, an open-ended reinforcement learning framework for medical multimodal reasoning that trains baseline VLM with Group based RL using composite reward. By coupling an LLM judge accuracy signal with medical embedding-based semantic alignment, lightweight format control, and modality recognition, MediX-R1 learns to produce concise, clinically faithful answers with interpretable reasoning traces. unified vLLM-based evaluation pipeline enables consistent, paraphrase-robust scoring across both text-only and image+text tasks. Empirically, MediX-R1 achieves strong results across diverse medical benchmarks and shows improved stability and resistance to reward hacking compared to single-signal RL variants. Human expert preference studies further corroborate its clinical answer quality, while qualitative examples illustrate faithful grounding and interpretable reasoning traces. Reward ablations validate that the multi-signal design enhances stability and semantic alignment beyond single-signal configurations. Altogether, the framework demonstrates that carefully composed, structure-aware rewards plus standardized LLM-judge evaluation provide practical path to scalable and interpretable medical multimodal RL fine-tuning."
        },
        {
            "title": "Impact Statement",
            "content": "This work advances methods for open-ended RL of medical MLLMs, with the goal of improving semantic correctness under paraphrase, format compliance, and modality grounding in medical question answering and report-style tasks. If used appropriately (e.g., as research and education aid), the proposed composite-reward RL and unified Referencebased LLM-as-judge evaluation framework may reduce reliance on brittle string-match metrics, enable more realistic benchmarking of free-form clinical responses, and improve transparency through structured outputs (modality tags and separated reasoning/answer blocks). At the same time, the societal and ethical risks are nontrivial. MediX-R1 is research prototype and is not intended for clinical or commercial deployment. Like other MediX-R1: Open Ended Medical Reinforcement Learning generative models, it may hallucinate findings, omit key differentials, or overstate certainty; the Reference-based LLMas-judge reward and evaluation could also reinforce subtle biases or false positives. Misuse risks include self-diagnosis, generation of misleading medical narratives, and adversarial prompting with malicious intent. While we used only publicly available, de-identified datasets under their respective licenses and did not conduct prospective human-subjects study, residual privacy risks can remain; downstream users should perform auditing prior to redistribution or deployment. We also highlight risks of demographic and dataset bias and potential amplification of health disparities; future work should include fairness analyses where legally and ethically permissible, uncertainty calibration, bias-aware reward shaping, and clinician-in-the-loop evaluation. To support responsible use and scrutiny, we commit to releasing training/inference code, configurations, checkpoints, curated datasets, and RL/evaluation prompt templates, alongside model card and clear usage restrictions, under CC-BY-NC-SA 4.0 license. We also disclose our use of generative AI tools: assisted coding was limited to boilerplate scaffolding and minor refactors with all algorithmic logic authored and reviewed manually; writing-support models were used for grammar and style, while all technical claims and results were verified by the authors. These steps aim to ensure transparency, auditability, and reliable reproduction of the published results."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was partially supported with NVIDIA Academic Grant 2025 and MBZUAI-IITD Research Collaboration Seed Grant."
        },
        {
            "title": "References",
            "content": "Balachandran, A. Medembed: Medical-focused embedding models, 2024. URL https://github.com/ abhinand5/MedEmbed. Chen, J., Cai, Z., Ji, K., Wang, X., Liu, W., Wang, R., Hou, J., and Wang, B. Huatuogpt-o1, towards medical complex reasoning with llms, 2024a. URL https:// arxiv.org/abs/2412.18925. Chen, J., Gui, C., Ouyang, R., Gao, A., Chen, S., Chen, G. H., Wang, X., Zhang, R., Cai, Z., Ji, K., Yu, G., Wan, X., and Wang, B. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale, 2024b. URL https://arxiv.org/abs/ 2406.19280. Deria, A., Kumar, K., Dukre, A. M., Segal, E., Khan, S., and Razzak, I. Medmo: Grounding and understanding multimodal large language model for medical images, 2026. URL https://arxiv.org/abs/2602.06965. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Han, T., Adams, L. C., Papaioannou, J.-M., Grundmann, P., Oberhauser, T., Loser, A., Truhn, D., and Bressem, K. K. Medalpacaan open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023. He, X., Zhang, Y., Mou, L., Xing, E., and Xie, P. Pathvqa: 30000+ questions for medical visual question answering, 2020. URL https://arxiv.org/abs/2003. 10286. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Henigman, A. and Kennedy, B. Medpix: database of medical images, teaching cases, and clinical topics. Medical Reference Services Quarterly, 44(3):328333, 2025. Jin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., and Szolovits, P. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019. Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L.-w. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Anthony Celi, L., and Mark, R. G. Mimic-iii, freely accessible critical care database. Scientific data, 3(1):19, 2016. Johnson, A. E., Pollard, T. J., Berkowitz, S. J., Greenbaum, N. R., Lungren, M. P., Deng, C.-y., Mark, R. G., and Horng, S. Mimic-cxr, de-identified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lau, J. J., Gayen, S., Ben Abacha, A., and DemnerFushman, D. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. 10 MediX-R1: Open Ended Medical Reinforcement Learning Liu, B., Zhan, L.-M., Xu, L., Ma, L., Yang, Y., and Wu, X.- M. Slake: semantically-labeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th international symposium on biomedical imaging (ISBI), pp. 16501654. IEEE, 2021. Marafioti, A., Zohar, O., Farre, M., Noyan, M., Bakouch, E., Cuenca, P., Zakka, C., Allal, L. B., Lozhkov, A., Tazi, N., Srivastav, V., Lochner, J., Larcher, H., Morlon, M., Tunstall, L., von Werra, L., and Wolf, T. Smolvlm: Redefining small and efficient multimodal models, 2025. URL https://arxiv.org/abs/2504.05299. Mullappilly, S. S., Kurpath, M. I., Pieri, S., Alseiari, S. Y., Cholakkal, S., Aldahmani, K., Khan, F., Anwer, R., Khan, S., Baldwin, T., and Findings), H. C. E. . Bimedix2: Biomedical expert lmm for diverse medical modalities, 2024. URL https://arxiv.org/abs/2412.07769. Pal, A., Umapathi, L. K., and Sankarasubbu, M. Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning, pp. 248260. PMLR, 2022. Pan, J., Liu, C., Wu, J., Liu, F., Zhu, J., Li, H. B., Chen, C., Ouyang, C., and Rueckert, D. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning. In International Conference on Medical Image Computing and ComputerAssisted Intervention, pp. 337347. Springer, 2025. Pieri, S., Mullappilly, S. S., Khan, F. S., Anwer, R. M., Khan, S., Baldwin, T., and Cholakkal, H. BiMediX: Bilingual medical mixture of experts LLM. In AlOnaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 1698417002, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp. 989. URL https://aclanthology.org/2024. findings-emnlp.989/. Sellergren, A., Kazemzadeh, S., Jaroensri, T., Kiraly, A., Traverse, M., Kohlberger, T., Xu, S., Jamil, F., Hughes, C., Lau, C., et al. Medgemma technical report. arXiv preprint arXiv:2507.05201, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Siragusa, I., Contino, S., Ciura, M. L., Alicata, R., and Pirrone, R. Medpix 2.0: comprehensive multimodal biomedical data set for advanced ai applications with retrieval augmented generation and knowledge graphs. Data Science and Engineering, pp. 117, 2025. Team, L., Xu, W., Chan, H. P., Li, L., Aljunied, M., Yuan, R., Wang, J., Xiao, C., Chen, G., Liu, C., Li, Z., Sun, Y., Shen, J., Wang, C., Tan, J., Zhao, D., Xu, T., Zhang, H., and Rong, Y. Lingshu: generalist foundation model for unified multimodal medical understanding and reasoning, 2025. URL https://arxiv.org/abs/2506. 07044. Team, Q. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.- Y., Zhang, Y.-Q., Yan, L., Qiao, M., Wu, Y., and Wang, M. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/ abs/2503.14476. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B., Yuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2024. URL https://arxiv.org/abs/2311.16502. Zhang, X., Wu, C., Zhao, Z., Lin, W., Zhang, Y., Wang, Y., and Xie, W. Pmc-vqa: Visual instruction tuning for medical visual question answering, 2024. URL https: //arxiv.org/abs/2305.10415. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., Zhou, J., and Lin, J. Group sequence policy optimization, 2025a. URL https://arxiv.org/abs/2507.18071. Zheng, Y., Lu, J., Wang, S., Feng, Z., Kuang, D., and Xiong, Y. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/ EasyR1, 2025b. 11 MediX-R1: Open Ended Medical Reinforcement Learning A. Appendix A.1. Training Data and Modality Distribution We trained MediX-R1 on 51335 multimodal medical instruction samples spanning 16 modality tags. All samples were drawn from the official train splits of the source datasets: PMC-VQA subset (Zhang et al., 2024), SLAKE (Liu et al., 2021), RadVQA (Lau et al., 2018), and PathVQA (He et al., 2020)."
        },
        {
            "title": "Total",
            "content": "5964 16399 8979 7646 2205 522 406 1227 6224 314 236 611 106 321 64"
        },
        {
            "title": "Total",
            "content": "25000 4919 1793 19623 51335 Table 8. Modality Breakdown and Source Dataset composition A.2. Training Configuration We list below the GRPO training configuration used for MediX-R1. Core settings include (i) data filtering and batching, (ii) actor optimization and rollout sampling, (iii) KL-regularized GRPO advantage computation, and (iv) trainer settings. We train our models using the EasyR1(Zheng et al., 2025b) Github Repository. MediX-R1 was trained using 8A100 (80 GB) Nvidia GPUs for approximately 25 hours."
        },
        {
            "title": "Training Configuration",
            "content": "Training Configurations \"data\": { \"max_prompt_length\": 4352, \"max_response_length\": 4096, \"rollout_batch_size\": 512, \"val_batch_size\": 1024, \"shuffle\": true, \"seed\": 1, \"min_pixels\": 262144, \"max_pixels\": 4194304, \"filter_overlong_prompts\": true, \"filter_overlong_prompts_workers\": 16 }, \"worker\": { \"hybrid_engine\": true, \"actor\": { \"strategy\": \"fsdp\", 12 MediX-R1: Open Ended Medical Reinforcement Learning \"global_batch_size\": 128, \"micro_batch_size_per_device_for_update\": 1, \"micro_batch_size_per_device_for_experience\": 2, \"max_grad_norm\": 1.0, \"clip_ratio_low\": 0.2, \"clip_ratio_high\": 0.3, \"clip_ratio_dual\": 3.0, \"loss_avg_mode\": \"token\", \"padding_free\": true, \"dynamic_batching\": true, \"use_torch_compile\": true, \"optim\": { \"lr\": 1e-6, \"betas\": [0.9, 0.999], \"weight_decay\": 0.01, \"strategy\": \"adamw\", \"lr_scheduler_type\": \"constant\", \"training_steps\": 200 }, \"fsdp\": { \"enable_full_shard\": true, \"enable_rank0_init\": true, \"mp_param_dtype\": \"bf16\", \"mp_reduce_dtype\": \"fp32\", \"mp_buffer_dtype\": \"fp32\" }, \"offload\": { \"offload_params\": true, \"offload_optimizer\": true }, \"use_kl_loss\": true, \"kl_penalty\": \"low_var_kl\", \"kl_coef\": 0.01 }, \"rollout\": { \"name\": \"vllm\", \"n\": 5, \"temperature\": 1.0, \"top_p\": 1.0, \"seed\": 1, \"tensor_parallel_size\": 2, \"max_num_batched_tokens\": 8448, \"gpu_memory_utilization\": 0.6, \"val_override_config\": { \"temperature\": 0.6, \"top_p\": 0.95, \"n\": }, \"prompt_length\": 4352, \"response_length\": 4096 } }, \"algorithm\": { \"adv_estimator\": \"grpo\", \"gamma\": 1.0, \"lam\": 1.0, \"use_kl_loss\": true, \"kl_penalty\": \"low_var_kl\", \"kl_coef\": 0.01, \"kl_type\": \"fixed\", \"kl_target\": 0.1, \"kl_horizon\": 10000.0 13 MediX-R1: Open Ended Medical Reinforcement Learning }, \"trainer\": { \"total_epochs\": 2, \"nnodes\": 1, \"n_gpus_per_node\": 8, \"val_freq\": 5, \"val_before_train\": true, \"save_freq\": 5, \"save_limit\": 3 } A.3. Reward Coefficient Selection Details This section details how we selected the composite reward coefficients used throughout the paper. Our goal was to (i) keep outputs reliably parseable (format control), while (ii) allocating most of the reward budget to task-facing correctness signals, and (iii) avoiding an expensive hyperparameter search. Fixed format budget. Across all experiments (including Table 4), we fixed the format reward weight to wfmt = 0.10 to enforce consistently parseable outputs. The remaining 0.90 of the reward mass was allocated among the task-facing signals. Single-signal ablations. The embedding-only and LLM-only settings in Table 4 correspond to: remb-only = 0.1Rformat + 0.9Rembed, rllm-only = 0.1Rformat + 0.9Rllm. Combining semantic rewards (Rllm + Rembed). Next, we combined the LLM judge and embedding signals while keeping wfmt = 0.10 fixed, and evaluated three intuitive splits of the remaining 0.90 mass: v1 (equal split): v2 (LLM-favored): v3 (embed-favored): = 0.1Rformat + (0.5 0.9)Rllm + (0.5 0.9)Rembed, = 0.1Rformat + (0.6 0.9)Rllm + (0.4 0.9)Rembed, = 0.1Rformat + (0.4 0.9)Rllm + (0.6 0.9)Rembed. On our validation benchmark suite, these three variants yielded similar overall averages (v1: 0.582, v2: 0.589, v3: 0.579), indicating low sensitivity within this coarse range. We therefore selected v2 as the default combined-semantic configuration because it slightly improved aggregate performance while placing more weight on the stricter correctness signal (Rllm). Adding modality grounding. Finally, we introduced modality grounding by reserving 5% of the non-format budget for Rmodality (i.e., 0.05 0.90 = 0.045 of total reward mass), and renormalizing the remaining non-format weights according to v2: wfmt = 0.10, wmod = 0.045, wllm = 0.5175, wemb = 0.3375, which sums to 1.0 and matches the MediX-R1 composite setting used in Table 4. Compute limitations. We emphasize that we did not run an exhaustive hyperparameter search over reward coefficients due to computational constraints. The above staged procedure was intended to make coefficient selection transparent and reproducible; future work may further improve performance by exploring broader coefficient grid. A.4. Reward Function Source Code Below are the Python implementations of the four reward components used in MediX-R1. Each function operates on predicted model output string and ground truth string containing the modality tag and reference answer. 14 MediX-R1: Open Ended Medical Reinforcement Learning"
        },
        {
            "title": "Format reward",
            "content": "def format_reward(predict: str) -> float: idx = predict.find(\"<think>\") if idx == -1: return 0.0 predict_new = predict[idx:].strip() pattern = re.compile(r\"<think>.*?</think>s*<answer>.*?</answer>\", re.DOTALL) format_match = re.fullmatch(pattern, predict_new) return 1.0 if format_match else 0.0 LLM-based accuracy reward def accuracy_reward_llm(predict: str, ground_truth: str) -> float: try: content_match = re.search(r\"<answer>(.*?)</answer>\", predict, re.DOTALL) given_answer = content_match.group(1).strip() if content_match else predict. strip() given_answer = given_answer.strip(.) ground_truth = ground_truth.split(>, maxsplit=1)[1].strip() ground_truth = ground_truth.strip(.) if given_answer == or len(given_answer) == 1: return 0.0 if given_answer == ground_truth: return 1.0 llm_score = llm_answer_match(given_answer, ground_truth) # external helper return llm_score except Exception: return 0.0 Embedding-based semantic reward def accuracy_reward_embed(predict: str, ground_truth: str, threshold: float = 0.8) -> float: try: content_match = re.search(r\"<answer>(.*?)</answer>\", predict, re.DOTALL) given_answer = content_match.group(1).strip() if content_match else predict. strip() given_answer = given_answer.strip(.) ground_truth = ground_truth.split(>, maxsplit=1)[1].strip() ground_truth = ground_truth.strip(.) if given_answer == or len(given_answer) == 1: return 0.0 if given_answer == ground_truth: return 1.0 embeddings = embed_model.encode([given_answer, ground_truth], convert_to_tensor=True) similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item() return float(similarity >= threshold) except Exception: return 0.0 MediX-R1: Open Ended Medical Reinforcement Learning"
        },
        {
            "title": "Modality recognition reward",
            "content": "def modality_reward(predict: str, ground_truth: str) -> float: idx = predict.find(\"<think>\") if idx == -1: return 0.0 predict_new = predict[:idx].strip() # modality tag before <think> modality = ground_truth.split(>, maxsplit=1)[0] + > return 1.0 if predict_new.upper() == modality.upper() else 0.0 A.5. Human Expert Comparative Evaluation Protocol For sampled set of multimodal questions, four anonymized model outputs (A-D) plus reference description are shown; experts pick the single best response based on clinical correctness, relevance (no hallucinations), and clarity of reasoning. Votes are aggregated into preference percentages reported in the main text."
        },
        {
            "title": "Evaluation Protocol for Medical Experts",
            "content": "Instructions for Evaluation Your task is to evaluate the responses provided by three AI models based on given medical image description (Ground Truth). Follow these steps to make your selection: 1) Read the Ground Truth: Carefully review the provided description of the medical image. This serves as the reference for an accurate and detailed response. 2) Assess the Model Responses: Examine the three model-generated responses (Model A, Model B, and Model C, Model D). Compare their content with the Ground Truth, focusing on the accuracy, completeness, and relevance of the clinical reasoning 3) Select the Best Response: Choose the model response that best aligns with the Ground Truth in terms of: > Clinical Accuracy: Does the response correctly describe the key findings in the image? > Reasoning Traces: Does the models reasoning traces correct and well explained 4) Submit Your Choice: After evaluating the responses, select the one that provides the most accurate and comprehensive explanation. A.6. Human Evaluation: Model Reasoning We extend our human expert study detailed in (Sec. 4.4) to evaluate the reasoning quality of our MediX-R1 model against MedGemma with the help of medical doctors. Experts assessed outputs for clinical accuracy, reasoning soundness, and practical usefulness in medical setting. MediX-R1s reasoning was preferred in 74.2% of cases over MedGemma, indicating stronger clinical coherence. Furthermore, the study shows that in 92.4% of the cases, the models reasoning steps were rated as acceptable and often comparable to medical doctors thought process, while only 7.6% of the cases were rated as having poor reasoning quality. Moreover, in fewer than 5% of the cases, the model produced flawed reasoning despite generating the correct final answer, indicating that such inconsistencies are rare and that MediX-R1 generally maintains robust and coherent reasoning process. Reviewers comprised five certified medical experts (MBBS/MD) with specialties in Radiology, General Medicine, and Forensic Medicine, with an inter-rater agreement of 63%. 16 MediX-R1: Open Ended Medical Reinforcement Learning A.7. Reinforcement Learning Training Prompt The RL training prompt enforces (i) an explicit modality tag, (ii) structured reasoning in <think>...</think>, and (iii) concise final answer in <answer>...</answer>. These structures align with the format reward (Rformat) and modality reward (Rmodality) in our composite objective. During training, only the <answer> block is graded by the Reference-based LLM-as-judge (Rllm) and the embedding-based semantic reward (Rembed); the <think> content is ignored for scoring but improves interpretability. Key points: - Modality tag must be one of the fixed set and appear before <think>. - The final decision is evaluated solely from <answer> for Rllm and Rembed. - Structural compliance (tags present and ordered) is required for Rformat."
        },
        {
            "title": "Reinforcement Learning Training Prompt",
            "content": "You are Medical AI Assistant with advanced reasoning capabilities Your task: 1. First output the image modality tag from this set: <X_RAY>, <MICROSCOPY>, <CLINICAL_PHOTOGRAPHY>, <CT_SCAN>, <GRAPHICS>, <ANGIOGRAPHY>, <PET_SCAN>, <ULTRASOUND>, <MRI_SCAN>, <FUNDUS_PHOTOGRAPHY>, <OCT_SCAN>, <ENDOSCOPY>, <MAMMOGRAPHY>, <FLUOROSCOPY>, <OTHER>, <SPECT> (Only output the tag, nothing else.) 2. Then output the thinking and medical reasoning process in <think>...</think> tags. 3. Finally, provide the correct answer inside <answer>...</answer> tags. 4. Do not include any extra information or text outside of these tags. Question: <image>{{ content trim }} A.8. Evaluation BASE Template (Short-Form QA/MCQ) This judge prompt yields binary score (0/1) for short-form QA and MCQ-style tasks. It compares the predicted <answer> against the reference, allowing paraphrases and option-label matches. Inference is performed with separate LLM judge (served via vLLM) to reduce evaluation-training coupling. We use deterministic settings (e.g., temperature 0) for reproducibility and parse the returned JSON strictly."
        },
        {
            "title": "Evaluation BASE template Prompt",
            "content": "You are medical expert. Your task is to evaluate whether the Predicted Answer correctly answers the Medical Question, based on the Ground Truth (Correct Answer) provided. Question: {question} Correct Answer: {correct_answer} Predicted Answer: {predicted_answer} Score 1 if the predicted answer matches the correct answer either fully in text or by indicating the correct option label (e.g., \"B\", \"Option B\", or paraphrased version that clearly identifies the correct choice). Score 0 if the predicted answer is incorrect or points to the wrong option. Respond strictly in the following JSON format: json 17 MediX-R1: Open Ended Medical Reinforcement Learning {{ \"score\": <score> }} A.9. Evaluation Template for Report Generation For long-form outputs (e.g., report generation or summarization), the judge assigns rubric score in [0, 5] reflecting clinical accuracy, completeness, and relevance. We request strict JSON for reliable parsing and average scores across items for dataset-level metrics. Only the models final report text is provided to the judge; any hidden reasoning (e.g., within </think>) is stripped before evaluation."
        },
        {
            "title": "Evaluation Prompt for Report Generation",
            "content": "You are medical expert evaluating the clinical accuracy, completeness, and relevance of generated medical report or summary. Your task is to compare an AI-generated report or summary to reference (gold standard) report or summary, based on clinical instruction or question. Assess the generated output on how well it preserves key clinical information, factual correctness, and clinical reasoning relevant to the task. Assign score between 0 and 5 using the following scale: 0 - Completely incorrect: Clinically irrelevant, misleading, or factually wrong. No meaningful alignment with the instruction or reference. 1 - Poor match: Barely relevant or mostly incorrect. Contains significant clinical misinformation or omits nearly all critical details. 2 - Weak match: Some fragments of relevant content are present, but major clinical errors or omissions exist. Clinical utility is low. 3 - Fair match: Contains several relevant points, but includes notable errors, missing findings, or misinterpretations that affect clinical reliability. 4 - Good match: Mostly accurate and clinically sound. Minor issues or missing details, but the overall meaning and purpose are preserved. 5 - Perfect or near-perfect match: Clinically accurate, complete, and faithful to the instruction and reference. No significant omissions or errors. Respond only in the following example JSON format: Example JSON format: json {{ \"score\": <score between 0 and 5> }} Now, evaluate the following: ### Clinical Instruction or Question:: {question} ### Reference Report or Summary: {correct_answer} 18 MediX-R1: Open Ended Medical Reinforcement Learning ### AI-Generated Report or Summary: {predicted_answer} 19 MediX-R1: Open Ended Medical Reinforcement Learning Figure 6. MediX-R1 - Report Generation: Case 2 20 MediX-R1: Open Ended Medical Reinforcement Learning Figure 7. MediX-R1 - Report Generation: Case"
        }
    ],
    "affiliations": [
        "JJM Medical College",
        "Jubilee Mission Medical College and Research Institute",
        "Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)"
    ]
}