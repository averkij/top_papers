{
    "paper_title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation",
    "authors": [
        "Tianyu Huang",
        "Wangguandong Zheng",
        "Tengfei Wang",
        "Yuhao Liu",
        "Zhenwei Wang",
        "Junta Wu",
        "Jie Jiang",
        "Hui Li",
        "Rynson W. H. Lau",
        "Wangmeng Zuo",
        "Chunchao Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-world applications like video gaming and virtual reality often demand the ability to model 3D scenes that users can explore along custom camera trajectories. While significant progress has been made in generating 3D objects from text or images, creating long-range, 3D-consistent, explorable 3D scenes remains a complex and challenging problem. In this work, we present Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Unlike existing approaches, Voyager achieves end-to-end scene generation and reconstruction with inherent consistency across frames, eliminating the need for 3D reconstruction pipelines (e.g., structure-from-motion or multi-view stereo). Our method integrates three key components: 1) World-Consistent Video Diffusion: A unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence 2) Long-Range World Exploration: An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency, and 3) Scalable Data Engine: A video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Collectively, these designs result in a clear improvement over existing methods in visual quality and geometric accuracy, with versatile applications."
        },
        {
            "title": "Start",
            "content": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation TIANYU HUANG, Harbin Institute of Technology, China WANGGUANDONG ZHENG, Southeast University, China TENGFEI WANG, Tencent Hunyuan, China YUHAO LIU, City University of Hong Kong, China ZHENWEI WANG, City University of Hong Kong, China JUNTA WU, Tencent Hunyuan, China JIE JIANG, Tencent Hunyuan, China HUI LI, Harbin Institute of Technology, China RYNSON W.H. LAU, City University of Hong Kong, China WANGMENG ZUO, Harbin Institute of Technology, China CHUNCHAO GUO, Tencent Hunyuan, China 5 2 0 2 4 ] . [ 1 5 2 2 4 0 . 6 0 5 2 : r Fig. 1. Voyager is world-consistent video generation and reconstruction framework. Up: Voyager can generate 3D-consistent scene videos for world exploration following custom camera trajectories. Bottom: Voyager jointly generates aligned depth and RGB video for effective and direct 3D reconstruction. Real-world applications like video gaming and virtual reality often demand the ability to model 3D scenes that users can explore along custom camera trajectories. While significant progress has been made in generating 3D objects from text or images, creating long-range, 3D-consistent, explorable 3D Both authors contributed equally to this research. Corresponding author. scenes remains complex and challenging problem. In this work, we present Voyager, novel video diffusion framework that generates world-consistent 3D point-cloud sequences from single image with user-defined camera path. Unlike existing approaches, Voyager achieves end-to-end scene generation and reconstruction with inherent consistency across frames, eliminating the need for 3D reconstruction pipelines (e.g., structure-from-motion or 2 Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu et al. multi-view stereo). Our method integrates three key components: 1) WorldConsistent Video Diffusion: unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence 2) Long-Range World Exploration: An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency, and 3) Scalable Data Engine: video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Collectively, these designs result in clear improvement over existing methods in visual quality and geometric accuracy, with versatile applications. See more at https://voyager-world.github.io."
        },
        {
            "title": "Introduction",
            "content": "The creation of high-fidelity, explorable 3D scenes that users can navigate seamlessly, powers broad applications ranging from video gaming and film production to robotic simulation. Yet, traditional workflows for constructing such 3D worlds remain bottlenecked by manual effort, requiring painstaking layout design, asset curation, and scene composition. While recent data-driven methods [Liu et al. 2024; Meng et al. 2024; Xiang et al. 2024; Xie et al. 2024; Zhao et al. 2025] have shown promise in generating objects or simple scenes, their ability to scale to complex scenes is limited by the scarcity of high-quality 3D scene data. This gap highlights the need for frameworks that enable scalable generation of user-navigable virtual worlds with 3D consistency. Recently, growing number of works [Chen et al. 2025; Gao* et al. 2024; He et al. 2024; Ma et al. 2025; Ren et al. 2025; Wang et al. 2024b; Yu et al. 2024b; Zhou et al. 2025] have explored the use of novel view synthesis (NVS) and video generation as alternative paradigms for world modeling. These methods, while demonstrating impressive capabilities in generating visually appealing and semantically rich content, still face several challenges. 1) Long-Range Spatial Inconsistency. Due to the absence of explicit 3D structural grounding, they often struggle to maintain spatial consistency and coherent viewpoint transitions during the generation process, especially when generating videos with long-range camera trajectories. 2) Visual Hallucination. While several works [Chen et al. 2025; Ren et al. 2025] have attempted to leverage 3D conditions to enhance geometric consistency, they typically rely on partial RGB images as guidance, i.e., novel-view images rendered from point clouds reconstructed with input views. However, such representation may introduce significant visual hallucinations in complex scenes, such as the incorrect occlusions in Figure. 2, which may introduce inaccurate supervision during training. 3) Post-hoc 3D Reconstruction. While these approaches can synthesize visually satisfying content, post-hoc 3D reconstructions are still required to obtain usable 3D content. This process is time-consuming and inevitably introduces geometric artifacts [Weber et al. 2024], making it inadequate for real-world applications. To address these challenges, we introduce Voyager, framework designed to synthesize long-range, world-consistent RGB-D(epth) videos from single image and user-specified camera trajectories. At the core of Voyager is novel world-consistent video diffusion that utilizes an expandable world caching mechanism to ensure spatial consistency and avoids visual hallucination. Starting from an Fig. 2. Partial RGB images and partial depth maps rendered from point clouds at different frames. In scenarios involving complex occlusion relationships, partial RGB images often exhibit significant visual artifacts. In contrast, partial depth maps can accurately represent occlusions. image, we construct an initial world cache by unprojecting it into 3D space with depth map. This 3D cache is then projected into target camera views to obtain partial RGB-D observations, which guides the diffusion model to maintain coherence with the accumulated world state. Crucially, the generated frames are fed back to update and expand the world cache, creating closed-loop system that supports arbitrary camera trajectories while maintaining geometric coherence. Unlike methods [Chen et al. 2025; Ma et al. 2025; Ren et al. 2025; Yu et al. 2024b] relying only on RGB conditioning, Voyager explicitly leverages depth information as spatial prior, enabling more accurate 3D consistency during video generation. By simultaneously generating aligned RGB and depth sequences, our framework supports direct 3D scene reconstruction without requiring additional 3D reconstruction steps like structure-from-motion. Despite promising performance, diffusion models struggle to generate long videos in single pass. To enable long-range world exploration, we propose world caching scheme and smooth video sampling for auto-regressive scene extension. Our world cache accumulates and maintains point clouds from all previously generated frames, expanding as video sequences grow. To optimize computational efficiency, we design point culling method to detect and remove redundant points with real-time rendering, minimizing memory overhead. Leveraging cached point clouds as proxy, we develop smooth sampling strategy that auto-regressively extends video length while ensuring smooth transitions between clips. Training such model requires large-scale videos with accurate camera poses and depth, but existing datasets often lack these annotations. To address this, we introduce data engine for scalable video reconstruction that automatically estimates camera poses and metric depth for arbitrary scene videos. With metric depth estimation, our data engine ensures consistent depth scales across diverse sources, enabling high-quality training data generation. Using this pipeline, we compile dataset of over 100,000 video clips, combining real-world captures and synthetic Unreal Engine renders. Extensive experiments demonstrate the effectiveness of Voyager in scene video generation and 3D world reconstruction. Benefiting from joint depth modeling, our results in Figure 1 exhibit more coherent geometry, which not only enable direct 3D reconstruction but also support infinite world expansion while preserving the original spatial layout. Additionally, we explore applications such as 3D generation, video transfer, and depth estimation, further showcasing the potential of Voyager in advancing spatial intelligence. Our contributions can be summarized as: Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation 3 We introduce Voyager, world-consistent video diffusion model for scene generation. To the best of our knowledge, Voyager is the first video model that jointly generates RGB and depth sequences with given camera trajectories. We propose an efficient world caching scheme and autoregressive video sampling approach, extending Voyager to world reconstruction and infinite world exploration. We propose scalable video data engine for camera and metric depth estimation, with over 100,000 training pairs prepared for the video diffusion model."
        },
        {
            "title": "2 Related Work\n2.1 Camera-Controllable View Generation",
            "content": "Existing camera-controllable generation models can be categorized into three types: novel view synthesis [Hong et al. 2023; Kerbl et al. 2023; Mildenhall et al. 2021; Wu et al. 2024] produces new viewpoints through multi-view reconstruction. These methods rely on dense viewpoints and struggle to handle single-view inputs. The second method [Guo et al. 2023; He et al. 2024; Liu et al. 2023; Wang et al. 2024b; Zhou et al. 2025] implicitly incorporates camera parameters into the model, training it to generate images from the corresponding viewpoints, but often suffers from viewpoint inconsistency. The third method [Chen et al. 2025; Ma et al. 2025; Ren et al. 2025; Seo et al. 2024] leverages point clouds obtained by warping the input view as conditions for novel view generation, significantly improving spatial consistency. However, the warped images still contain artifacts that negatively affect model training. In this work, we introduce warping depth as an additional conditioning input and generate both RGB and depth content."
        },
        {
            "title": "2.2 Long-Range Video Generation",
            "content": "Current video models are limited in their ability to generate long videos in single pass. To extend video length, existing research explores training-free methods [Lu et al. 2024; Wang et al. 2023], hierarchical strategies [He et al. 2022; Yin et al. 2023], and autoregressive frameworks [Henschel et al. 2024; Yin et al. 2024]. However, the first two approaches cannot scale to infinitely long videos, while the auto-regressive strategy relies on memory caches that struggle to retain information from distant past frames. To address this limitation, we propose world cache with point culling in this work that efficiently preserves spatial information and enables the generation of arbitrarily long videos with smooth video sampling in an auto-regressive inference."
        },
        {
            "title": "3 Preliminaries of Video Diffusion Models\nDiffusion models learn to denoise a data distribution ğ‘ (ğ‘¥) through\nan iterative process, in which a forward diffusion process gradually\nadds noise to the data x0 âˆ¼ ğ‘ (x0), and a reverse process learns to\nrecover x0 from the noisy data xğ‘¡ .",
            "content": "In the context of video generation, diffusion models are extended to learn temporal dynamics by incorporating 3D convolutional architectures [Tran et al. 2015; Yu et al. 2023b] and attention mechanisms [Brooks et al. 2024; Zhang et al. 2023]. To reduce the computation cost, latent diffusion [Blattmann et al. 2023; Rombach et al. 2022] is widely used to compress the video to low-dimensional latent space. ğ‘Š + 1) ğ¶ ğ» ğ‘ğ‘  In this work, our video model is based on Hunyuan-Video [Kong et al. 2024]. Formally, given an input text prompt ğ‘¦ and groundtruth video sequence [ğ¼0, ..., ğ¼ğ‘‡ 1] Rğ‘‡ 3ğ» ğ‘Š , the model first extracts the video latent z0 with shape ( ğ‘‡ ğ‘ğ‘  by 3Dğ‘ğ‘¡ VAE, where ğ‘ğ‘¡ and ğ‘ğ‘  denote the compression rate for the temporal and spatial axis. To train denoising model ğœƒ , noisy latent zğ‘¡ is then fed to full-attention DiT [Li et al. 2024; Peebles and Xie 2022] model, which follows the strategy of \"Dual-stream to Single-stream\" hybrid model [Labs 2024]. Patched video and text latents are processed independently in dual-stream Transformer blocks ğ‘“ ğ‘– ğ· , while in the second phase, these latents are concatenated in single-stream blocks ğ‘“ ğ‘– ğ‘† . To further support image-conditioned video generation, the latent feature of the input image is concatenated to zğ‘¡ channel-wise. The training objective is to predict the velocity uğ‘¡ = ğ‘‘zğ‘¡ /ğ‘‘ğ‘¡ by minimizing the mean squared error between the estimated velocity Ë†uğ‘¡ and the ground-truth uğ‘¡ . Finally, the latent z0 is recovered by the first-order Euler ordinary differential equation (ODE) solver, and the video ğ‘£ is reconstructed by the 3D-VAE decoder."
        },
        {
            "title": "4.1 Geometry-Injected Frame Condition",
            "content": "For the control of video viewpoint, camera parameter [Bai et al. 2025; Zhou et al. 2025] is straightforward condition, but this implicit condition is nontrivial to the training of video models. Recent works [Chen et al. 2025; Ma et al. 2025; Ren et al. 2025] attempt to reconstruct the point cloud ğ‘ Rğ‘ 6 from videos as an explicit control, where ğ‘ is the number of points and each point is represented by 6D coordinates (ğ‘¥, ğ‘¦, ğ‘§, ğ‘Ÿ, ğ‘”, ğ‘). The warped RGB condition Ë†ğ¼ğ‘£ for novel view ğ‘£ can then be rendered according to the camera, which is partial image with blank regions. Nonetheless, such partial RGB image is insufficient to ensure spatial consistency, e.g., complex occlusion relationships in scene may lead to visual hallucinations. To enforce spatially consistent 4 Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu et al. Fig. 3. Overview of Voyager: Given the input image and camera trajectories, we first render partial RGB images and depth maps for each viewpoint as the condition for video generation. Our world-consistent video diffusion model is trained to generate RGB-D frames simultaneously, thus supporting the direct reconstruction of the 3D world. The projected points are store in our world cache efficiently, which can be rendered as condition for the next round generation. control during training, we introduce an additional geometric condition partial depth map, which is aligned with the partial RGB image. Specifically, we first estimate the depth map ğ·ğ‘˜ and corresponding camera parameters ğ‘ğ‘˜ for each frame ğ¼ğ‘˜ of the video. Since only the first frame is visible in video inference, we create point cloud ğ‘0 by projecting ğ·0 with ğ‘0. For the ğ‘˜-th frame, its partial image Ë†ğ¼ğ‘˜ and partial depth Ë†ğ·ğ‘˜ are acquired by masking the invisible region with the rendering mask ğ‘€ğ‘˜ = render(ğ‘0, ğ‘ğ‘˜ )."
        },
        {
            "title": "4.2 World-Consistent Video Diffusion",
            "content": "Conditioned with partial RGB and depth maps, our intention is to generate plausible content for the invisible regions, ensuring consistency with the spatial information provided by the partial conditions. For this purpose, the common practice [Labs 2024; Ren et al. 2025] is to concatenate the condition latents zrgb and zdepth with original noisy latents zğ‘¡ along the channel axis and project the concatenated latents back to the Transformer dimension via the patch-embedding layer ğ‘“emb: ğ‘¡,0 = ğ‘“emb (concat(zğ‘¡ , zrgb, zdepth)). Then, the projected latents ğ‘¡,0 are fed to double-stream and singlestream blocks sequentially, which is formulated as, ğ‘¡,ğ‘–, z ğ‘¦,ğ‘– = ğ‘“ ğ‘– ğ· (z ğ‘¡,ğ‘– , ğ‘¦,ğ‘– 1 , ğ‘¡), ğ‘– = 1, ..., ğ‘ğ·, (1) ğ‘¡,ğ‘– = ğ‘“ ğ‘– ğ‘† (z , ğ‘¡), ğ‘– = 1, ..., ğ‘ğ‘†, ğ‘¡,ğ‘– 1 ğ‘¡,ğ‘ and where each stream. (2) ğ‘¦ is the text latents. ğ‘ğ· and ğ‘ğ‘† denote the block number of ğ‘¡,0 is initialized as the concatenation of ğ‘¦,ğ‘ . Although the video model can best preserve the pre-trained parameters in this way, the spatial conditions is only used in channelwise. The missing parts in our partial maps can range from small cracks to large blank areas, depending on the extent of the viewpoint change. This trivial solution struggles to handle variable situations. Depth-Fused Video Generation. Instead of relying solely on partial depth as the input condition for completing the missing regions in the RGB frames, we propose to simultaneously generate both complete RGB and depth frames. As result, the video model can take advantage of DiTs full-attention structure, allowing for the interaction of visual and geometric information at the pixel level. To this end, we concatenate the rgb and depth images along the height axis as Iğ‘˜ = [ğ¼ğ‘˜, Î¦, ğ·ğ‘˜ ]â„, as well as condition maps Ë†Iğ‘˜ = [ Ë†ğ¼ğ‘˜, Î¦, Ë†ğ·ğ‘˜ ]â„ and masks ğ‘€ğ‘˜ = [ğ‘€ğ‘˜, Î¦, ğ‘€ğ‘˜ ]â„. Here, we add placeholder row Î¦ between the rgb and depth images to help the model separate these two types of content. The new video latents are presented ğ‘¡,0 = ğ‘“emb (concat(zğ‘¡ , Ë†z0, ğ‘š)), where Ë†z0 is the latent of [ Ë†Iğ‘˜ ]ğ‘‡ 1 as ğ‘˜=0 and ğ‘š is the down-sampled map of [ğ‘€ğ‘˜ ]ğ‘‡ 1 ğ‘˜=0 via max-pooling. Accordingly, ğ‘¡,0 is fed to the diffusion model similar to Eq. 1-2. The diffusion model is thus trained to generate rgb-depth video frames. Context-Based Control Enhancement. The above concatenation mechanism incorporates conditional information only at the input of the DiT model, leading to weak enforcement of the geometric conditions and resulting in misalignment between generated frames and input conditions. To enhance the geometric-following capabilities, following [Bian et al. 2025], we further inject the diffusion model with lightweight modules. Concretely, we replicate the first block from the doublestream and single-stream modules as the Control blocks Ë†ğ‘“ğ· and Ë†ğ‘“ğ‘† . Given the input video latent ğ‘¡,0, we have the following operations for each Transformer block ğ‘–: zğ· = Ë†ğ‘“ğ· (z ğ‘¡,ğ‘– + ğ‘™ğ· (zğ· ), (4) where ğ‘™ğ· and ğ‘™ğ‘† are zero-initialized linear layers. Early-stage latent features preserve more contextual information, so that the integration into each block can strengthen pixel-level controllability. ğ‘¡,0), zğ‘† = Ë†ğ‘“ğ‘† (zğ· ), ğ‘¡,ğ‘– = ğ‘¡,ğ‘– + ğ‘™ğ‘† (zğ‘† ), ğ‘¡,ğ‘– = z (3)"
        },
        {
            "title": "4.3 Long-Range World Exploration",
            "content": "For long-range or even infinite video generation, auto-regressive is natural choice. This paradigm recursively generates future frames or clips based on previously generated content, maintaining temporal continuity over time. However, due to the limited memory capacity of video diffusion models, auto-regressive methods are often restricted to conditioning on only few preceding frames or clips. Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation 5 Table 1. Quantitative comparison of novel view synthesis on RealEstate10K. Method PSNR SSIM LPIPS SEVA ViewCrafter See3D FlexWorld Voyager 16.648 16.512 18.189 18.278 18.751 0.613 0.636 0.694 0.693 0.715 0.349 0.332 0.290 0.281 0. This limited context leads to inevitable information loss, making it fundamentally infeasible to retain and propagate the full scene history. In contrast to previous auto-regressive methods, Voyager exploits point-cloud conditions for generation, which is scalable representation to store the whole history information. To enable infinite generation, we propose world caching with point culling to efficiently store spatial information and adopt smooth video sampling to ensure the consistency of consecutive clips. World Caching with Point Culling. With input camera parameters and corresponding RGB-D video frames, point clouds can be projected to 3D space as Ë†ğ‘ R(ğ‘‡ ğ» ğ‘Š ) 3, where ğ‘‡ is the number of total frames. As the video continues to extend, the number of points can easily grow to millions, posing significant challenges in terms of memory and computational efficiency. To address that, we propose to maintain world cache, which eliminates redundant points while preserving essential geometric information. Specifically, we incrementally add new points to the cache on per-frame basis: given the accumulated point cloud Ë†ğ‘ from previous frames, we render visibility mask ğ‘€ = render( Ë†ğ‘, ğ‘ğ‘– ) from the current camera view ğ‘ğ‘– . Points in the invisible regions are added to Ë†ğ‘ first. For the visible regions, if the angle between the surface normal of existing points and the current view direction exceeds 90 degrees, the new point is also updated into the cache, because these existing points cannot be seen at the current viewpoint. This strategy reduces the number of stored points by approximately 40% and avoids noise accumulation caused by multi-frame aggregation. Smooth Video Sampling. Conditioned on the above world cache, our video model can access the complete spatial information from previous frames. However, although each independently generated video clip is spatially consistent, there can still be color discrepancies, making them unsuitable for direct concatenation. We adopt two strategies to ensure smoother transitions between adjacent clips. (1) We first divide the input video into overlapping segments, where the length of the overlapping region is half of one segment. For each segment, the overlapping region is initialized with the generated results from the previous segment, serving as the noise initialization for the current segments overlap region. (2) After completing inference for the consecutive two segments, we apply averaging across the overlapping regions and introduce light-level noise injection to the merged segments. final round of denoising is then performed to refine transitions. In this way, we ensure the efficient generation of multiple clips while maintaining visual consistency across consecutive video frames."
        },
        {
            "title": "4.4 Scalable Video Data Engine",
            "content": "Training such video model demands large-scale video frames with corresponding camera parameters and depth maps. We carefully Table 2. Quantitative comparison of Gaussian Splattig reconstruction on RealEstate10K. Baselines require additional reconstruction step [Wang et al. 2025], while Voyager performs better with our generated depth. Method Post Rec. PSNR SSIM LPIPS SEVA ViewCrafter See3D FlexWorld Voyager Voyager VGGT VGGT VGGT VGGT VGGT - 15.581 16.161 16.764 17.623 17.742 18.035 0.602 0.628 0.633 0.659 0.712 0.714 0.452 0.440 0.440 0.425 0.404 0.381 curate over 100,000 video clips from both real-captured videos and 3D renderings, and propose scalable video data engine to automatically annotate required 3D information for arbitrary scene videos. Data Curation. We selected two open-source real-world datasets, i.e., RealEstate [Zhou et al. 2018] and DL3DV [Ling et al. 2024] for the training. RealEstate contains 74,766 video clips related to real estate scenes, primarily featuring indoor home scenes, along with some outdoor environments. DL3DV provides 10K real-scene videos, but most of them suffer from rapid or shaky camera movements. We curate 3,000 high-quality videos from this dataset and segment them into approximately 18,000 video clips. Additionally, to increase the diversity of generation content, we collected 1,500 Unreal Engine scene models and rendered over 10,000 video samples to augment the dataset. In the end, we collected over 100,000 video clips from these datasets. Data Annotation. Accurate camera parameters and depth are crucial for model training, but RealEstate and DL3DV do not provide such ground-truth data. Existing methods [Chen et al. 2025; Ren et al. 2025; Schwarz et al. 2025] adopt dense stereo models [Teed and Deng 2021] to prepare training pairs, struggling to produce geometrically consistent depth. We propose more robust data processing engine. Specifically, we first use VGGT [Wang et al. 2025] to estimate camera parameters and depth for all video frames. The depth estimated by VGGT is not accurate enough, but it is aligned with camera poses. To further improve the depth estimation, we then employ MoGE [Wang et al. 2024a] as robust depth estimator and align the two depth maps with least squares optimization. Finally, since our UE data provides metric depth values, we need to align all the estimated depth to standard scale. We estimate the metric depth range of the scene using Metric3D [Hu et al. 2024] and map the previous depths into this range. This way, we can automatically annotate camera and depth for videos from any source."
        },
        {
            "title": "5 Experiments\n5.1 Video Generation",
            "content": "We evaluate the video generation quality of Voyager by comparing four open-source camera-controllable video generation methods on image-to-video generation, including SEVA [Zhou et al. 2025], ViewCrafter [Yu et al. 2024b], See3D [Ma et al. 2025], and FlexWorld [Chen et al. 2025]. Among these methods, ViewCrafter, See3D, and FlexWorld control the viewpoints with point cloud conditions, which are similar to our method. SEVA directly takes camera parameters as input conditions. Dataset and Metrics. We randomly select 150 video clips from the test set of RealEstate [Zhou et al. 2018] as our test dataset. Since the 6 Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu et al. Fig. 4. Qualitative results on video generation. Compared to the baselines, our model can generate more reasonable unseen region and meanwhile preserve the content in the input view. video clips do not provide ground-truth cameras, we estimate the camera parameters and depth maps with the same pipeline in our data engine. To evaluate the visual quality of generated videos, we adopt PSNR, SSIM, and LPIPS to measure the similarity between the generated frames and the ground truth. Results. We report the quantitative results on Table 1. Our method outperforms all the baselines, demonstrating the high generation quality of our video model. The qualitative comparison in Figure 4 also showcases our capability of generating photorealistic videos. Especially in the last case of Figure 4, only our method can preserve the details of products in the input image. However, other methods are prone to generating artifacts, e.g., in the first example of Figure 4, these methods fail to provide reasonable predictions when the camera movement is too large."
        },
        {
            "title": "5.2 Scene Generation",
            "content": "To evaluate the quality of scene generation, we further compare the quality of scene reconstruction with generated videos based on Sec. 5.1. Since the compared baselines only produce RGB frames, we first exploit VGGT [Wang et al. 2025] to estimate camera parameters and initialize the point clouds for the generated videos of these methods. Thanks to the capability of generating RGB-D content, our results can be directly used in 3DGS reconstruction. In Table 2, our reconstruction results with VGGT post-hoc outperform the compared baselines, indicating that our generated videos are more consistent in aspect of geometry. The results are even better when initializing point clouds with our own depth output, which demonstrates the effectiveness of our depth generation for scene reconstruction. The qualitative results in Figure 3 illustrate the same conclusion. Particularly in the last case, our method retains most details of the chandelier, while baseline methods even fail to reconstruct basic shape."
        },
        {
            "title": "5.3 World Generation",
            "content": "Besides the in-domain comparison on RealEstate, we test Voyager on WorldScore [Duan et al. 2025] static benchmark on world generation. WorldScore consists of 2,000 static test examples that span diverse worlds, e.g., indoor and outdoor, photorealistic and stylized. In each example, an input image and camera trajectory are provided. The metrics evaluate the controllability and quality of generation, and an average score is presented to show the overall performance. We compare six top methods in the existing benchmark, including two 3D methods WonderJourney [Yu et al. 2023a] and WonderWorld [Yu et al. 2024a], and four video methods EasyAnimate [Xu et al. 2024], Allegro [Zhou et al. 2024], Gen-3 [Runway 2024], and CogVideoX [Yang et al. 2024]. The scores are reported in Table 3. Voyager achieves the highest score on this benchmark. The score shows that our method has competitive performance on camera control and spatial consistency, compared with 3D-based methods. Our subjective quality is the highest among all methods, further demonstrating the visual quality of our generated videos. Notably, since our video condition is constructed with metric depth, the camera movement in our results are larger than other methods, which is much harder to generate."
        },
        {
            "title": "5.4 Ablation Studies\nTo verify the effectiveness of our proposed designs, we conduct\nablation studies on our world-consistent video diffusion and long-\nrange world exploration.\nWorld-Consistent Video Diffusion We evaluate our video models\ntrained in the three stages separately on Worldscore benchmark,\ni.e., (a) model trained only on RGB conditions, (b) model trained on\nRGB-D conditions, and (c) model attached with additional control\nblocks. As shown in Table 4, fusing depth conditions in training can\nsignificantly enhance the capability of camera control. The control\nblocks can further improve the spatial consistency of generated\nresults. We also provide qualitative results in Figure 7. The RGB-only\nmodel may generate inconsistent content when the camera moves\nto an unseen region. The results of RGB-D model is more consistent\nwith the input image, but it could still produce some minor artifacts.\nOur final model generates the most reasonable results.\nLong-range video generation. We evaluate the quality of point\nculling and smooth sampling in Figure 8. For point culling, storing all\npoints introduces noise, while storing points in the invisible region is\ninsufficient. Results with additional normal check have comparable",
            "content": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation 7 Fig. 5. Qualitative results on Gaussian Splatting reconstruction. Our results present much more details than the compared baselines. Table 3. Quantitative comparison on WorldScore Benchmark. Bold and underline indicates the 1st, Bold indicates the 2nd, underline indicates the 3rd. Method WonderJourney WonderWorld EasyAnimate Allegro Gen-3 CogVideoX-I2V Voyager WorldScore Average 63.75 72.69 52.85 55.31 60.71 62.15 77.62 Camera Control 84.6 92.98 26.72 24.84 29.47 38.27 85.95 Object Control 37.1 51.76 54.5 57.47 62.92 40.07 66.92 Content Alignment 35.54 71.25 50.76 51.48 50.49 36.73 68.92 3D Consistency 80.6 86.87 67.29 70.5 68.31 86.21 81.56 Photometric Consistency 79.03 85.56 47.35 69.89 87.09 88.12 85. Style Consistency 62.82 70.57 73.05 65.6 62.82 83.22 84.89 Subjective Quality 66.56 49.81 50.31 47.41 63.85 62.44 71.09 Table 4. Ablation study on Worldscore. Metric Camera Control Content Alignment 3D Consistency Ours (RGB-only) Ours (RGB-D) Ours (full) 74.98 85.04 85.95 48.92 65.72 68.92 68.86 78.58 81.56 visual performance with storing all points, but save almost 40% storage. For smooth sampling, the video clip without sampling may exhibit inconsistencies compared to the first clip. Smooth sampling ensures seamless transition between two consecutive segments."
        },
        {
            "title": "6 Application",
            "content": "Benefiting from our depth-fused video generation, Voyager supports various 3D-related applications. Long Video Generation. As explained in Sec. 4.3, our method allows long-range video generation with efficient world caching and smooth video sampling. In Figure 6(a), we provide an example consisting of three video clips, with totally different camera trajectories among clips. The results present camera controllability and spatial consistency of the generated video, demonstrating that our method is capable of long-range world exploration. Image-to-3D Generation. Native 3D generative models can hardly handle the generation of multiple objects. In Figure 6(b), we use three state-of-the-art 3D generation methods Trellis [Xiang et al. 2024], Rodin v1.5 [RodinAI 2025], and Hunyuan-3D v2.5 [Hunyuan3D 2025] to generate simple combination where car leans against tent. Rodin failed to generate the tent, while Trellis produced tent with missing parts. Hunyuan successfully generated two complete objects, but the spatial relationship was inaccurate, with the tent being too far from the car. Our method not only generates the correct content, but also produces more realistic visual effects. The tent is even visible through the car window in the side view. Depth-Consistent Video Transfer. Generating spatially consistent video with different style typically requires training stylized video model. However, to achieve the desired effect with our model, we only need to replace the reference image while retaining the original depth condition. As shown in Figure 6(c), we can change the original video to American-style or to the night. Video Depth Estimation. Our video model is naturally capable of estimating video depth. In Figure 6(d), our predicted depth can preserve the details on the architectures."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we present Voyager, world-consistent video generation framework for long-range world exploration. The proposed RGB-D video diffusion model can produce spatially consistent video sequences that align with the input camera trajectories, allowing direct 3D scene reconstruction. This supports auto-regressive and consistent world expansion. Experiments demonstrate high visual fidelity and strong spatial coherence in both generated videos and point clouds. 8 Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu et al. Fig. 6. Applications: (a) Long-range video generation. (b) Image-to-3D generation. (c) World-consistent video style transfer. (d) Monocular video depth estimation. Fig. 8. Qualitative results on ablation study. We compare the video models in our three training stages. Our final model achieves the highest quality."
        },
        {
            "title": "References",
            "content": "Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, and Di Zhang. 2025. ReCamMaster: CameraControlled Generative Rendering from Single Video. arXiv:2503.11647 [cs.CV] https://arxiv.org/abs/2503.11647 Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. 2025. VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control. arXiv preprint arXiv:2503.05639 (2025). Fig. 7. Qualitative results on ablation study. We compare the video models in our three training stages. Our final model achieves the highest quality. Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation 9 Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. 2023. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2256322575. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. 2024. Video generation models as world simulators. OpenAI Blog 1 (2024), 8. Luxi Chen, Zihan Zhou, Min Zhao, Yikai Wang, Ge Zhang, Wenhao Huang, Hao Sun, Ji-Rong Wen, and Chongxuan Li. 2025. FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View Synthesis. arXiv:2503.13265 [cs.CV] https://arxiv.org/ abs/2503.13265 Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. 2025. WorldScore: Unified Evaluation Benchmark for World Generation. arXiv preprint arXiv:2504.00983 (2025). Ruiqi Gao*, Aleksander Holynski*, Philipp Henzler, Arthur Brussee, Ricardo MartinBrualla, Pratul P. Srinivasan, Jonathan T. Barron, and Ben Poole*. 2024. CAT3D: Create Anything in 3D with Multi-View Diffusion Models. Advances in Neural Information Processing Systems (2024). Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2023. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725 (2023). Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. 2024. CameraCtrl: Enabling Camera Control for Text-to-Video Generation. arXiv preprint arXiv:2404.02101 (2024). Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. 2022. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221 (2022). Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. 2024. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773 (2024). Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. 2023. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400 (2023). Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. 2024. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024). Hunyuan3D. 2025. Hunyuan-3D. (2025). https://3d-models.hunyuan.tencent.com Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis. 2023. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph. 42, 4 (2023), 1391. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. 2024. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603 (2024). Black Forest Labs. 2024. FLUX. https://github.com/black-forest-labs/flux. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. 2024. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748 (2024). Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. 2024. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2216022169. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, Zero-1-to-3: Zero-shot One Image to 3D Object. and Carl Vondrick. 2023. arXiv:2303.11328 [cs.CV] Yang Liu, Chuanchen Luo, Lue Fan, Naiyan Wang, Junran Peng, and Zhaoxiang Zhang. 2024. Citygaussian: Real-time high-quality large-scale scene rendering with gaussians. In European Conference on Computer Vision. Springer, 265282. Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. 2024. Freelong: Training-free arXiv preprint long video generation with spectralblend temporal attention. arXiv:2407.19918 (2024). Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, and Xinlong Wang. 2025. You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale. In IEEE/CVF conference on computer vision and pattern recognition. Quan Meng, Lei Li, Matthias NieÃŸner, and Angela Dai. 2024. Lt3sd: Latent trees for 3d scene diffusion. arXiv preprint arXiv:2409.08215 (2024). Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields for view synthesis. Commun. ACM 65, 1 (2021), 99106. William Peebles and Saining Xie. 2022. Scalable Diffusion Models with Transformers. arXiv preprint arXiv:2212.09748 (2022). Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin NimierDavid, Thomas MÃ¼ller, Alexander Keller, Sanja Fidler, and Jun Gao. 2025. GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. RodinAI. 2025. Rodin. (2025). https://hyper3d.ai Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. Runway. 2024. Introducing gen-3 alpha: new frontier for video gneration. (2024). https://runwayml.com/research/introducing-gen-3-alpha Katja Schwarz, Denys Rozumnyi, Samuel Rota BulÃ², Lorenzo Porzi, and Peter Kontschieder. 2025. Recipe for Generating 3D Worlds From Single Image. arXiv preprint arXiv:2503.16611 (2025). Junyoung Seo, Kazumi Fukuda, Takashi Shibuya, Takuya Narihira, Naoki Murata, Shoukang Hu, Chieh-Hsin Lai, Seungryong Kim, and Yuki Mitsufuji. 2024. GenWarp: Single Image to Novel Views with Semantic-Preserving Generative Warping. arXiv preprint arXiv:2405.17251 (2024). Zachary Teed and Jia Deng. 2021. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems 34 (2021), 1655816569. Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. 2015. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision. 44894497. Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. 2023. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264 (2023). Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. 2025. Vggt: Visual geometry grounded transformer. arXiv preprint arXiv:2503.11651 (2025). Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. 2024a. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision. arXiv preprint arXiv:2410.19115 (2024). Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. 2024b. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers. 111. Ethan Weber, Riley Peterlinz, Rohan Mathur, Frederik Warburg, Alexei Efros, and Angjoo Kanazawa. 2024. Toon3D: Seeing Cartoons from New Perspective. arXiv preprint arXiv:2405.10320 (2024). Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. 2024. Reconfusion: 3d reconstruction with diffusion priors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2155121561. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. 2024. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506 (2024). Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. 2024. Citydreamer: Compositional generative model of unbounded 3d cities. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 96669675. Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. 2024. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991 (2024). Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. 2024. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. arXiv preprint arXiv:2408.06072 (2024). Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. 2023. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346 (2023). Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. 2024. From slow bidirectional to fast causal video generators. arXiv preprint arXiv:2412.07772 (2024). Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T. Freeman, and Jiajun Wu. 2024a. WonderWorld: Interactive 3D Scene Generation from Single Image. arXiv:2406.09394 (2024). Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, and Charles Herrmann. 2023a. WonderJourney: Going from Anywhere to Everywhere. arXiv preprint arXiv:2312.03884 (2023). Lijun Yu, JosÃ© Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. 2023b. Language Model Beats DiffusionTokenizer is Key to Visual Generation. arXiv preprint arXiv:2310.05737 (2023). Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. 2024b. ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis. arXiv preprint arXiv:2409.02048 (2024). Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu et al. Zhimeng Zhang, Zhipeng Hu, Wenjin Deng, Changjie Fan, Tangjie Lv, and Yu Ding. 2023. Dinet: Deformation inpainting network for realistic face visually dubbing on high resolution video. In Proceedings of the AAAI conference on artificial intelligence, Vol. 37. 35433551. Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. 2025. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202 (2025). Jensen (Jinghao) Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, and Varun Jampani. 2025. Stable Virtual Camera: Generative View Synthesis with Diffusion Models. arXiv preprint arXiv:2503.14489 (2025). Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. 2018. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817 (2018). Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. 2024. Allegro: Open the Black Box of Commercial-Level Video Generation Model. arXiv preprint arXiv:2410.15458 (2024). Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation 11 In this supplement, we will introduce more details of training implementation (Sec. A), our video diffusion model (Sec. B), and our video data engine (Sec. C). Finally, we provide more generation results in Sec. D."
        },
        {
            "title": "A Implementation Details",
            "content": "Our training basically follows the image-to-video model of HunYuanVideo [Kong et al. 2024]. We divide the training into three stages: the first stage only trains the RGB video model; in the second stage, depth is introduced into the training; and in the third stage, the DiT parameters are frozen and ControlNet blocks are incorporated for training. We use all three datasets in the first training stage. However, DL3DV is removed in the second stage due to its fast camera motion, which makes it unsuitable for depth training. In the third stage, we train solely on the UE dataset with its ground-truth depth. During training, we randomly select width-height ratio from [1, 1.25, 1.5, 1.75] to support the generation of videos with multiple aspect ratios. The number of generation frames for single pass is 49. Fig. 9. Details of world-consistent diffusion model. World-Consistent Video Diffusion We provide the details of our video diffusion model in Figure 9. The input of the diffusion model includes noisy latents zğ‘¡ , input image latents zğ‘Ÿ 0, condition latents Ë†z0, and down-sampled mask ğ‘š. To align the temporal dimension, we pad zğ‘Ÿ 0 with zero latents. In the first stage of training, only the RGB-related latents are concatenated in the channel dimension and are then fed to the diffusion model. In the second stage, we inject depth-related latents into the input. We fine-tune the parameters of the original diffusion structure in the first two stages, two additional Transformer blocks are trained in the final stage. The aggregated features in these two blocks are added back on pixel-wise basis."
        },
        {
            "title": "C Scalable Video Data Engine",
            "content": "Accurate camera parameters and depth are crucial for model training. As shown in Fig. 10, we propose more robust data processing pipeline. Compared to Flexworld [Chen et al. 2025], since our depth estimation method is more consistent and accurate than the depth rendered by 3DGS, our warped images are more precise, as shown in Fig. 11. Specifically, we first use VGGT [Wang et al. 2025] to estimate camera parameters and depth for all video frames. The depth estimated by VGGT is not accurate enough, but it is aligned with camera Fig. 10. Overview of our scalable video data engine. Fig. 11. Training warp images compare. poses. To further improve the depth estimation, we then employ MoGE [Wang et al. 2024a] as robust depth estimator. Specifically, we first convert the depth into disparity, and then use least squaresbased optimization strategy to minimize the disparity difference between the depth frames generated by VGGT and those generated by MoGE. The optimization is represented as: min ğ‘ ğ‘ğ‘ğ‘™ğ‘’,ğ‘ğ‘–ğ‘ğ‘  (cid:13) (cid:13) (cid:13) (cid:13) (cid:18) ğ‘ ğ‘ğ‘ğ‘™ğ‘’ ğ‘‘ğ‘€ğ‘œğºğ¸ + ğ‘ğ‘–ğ‘ğ‘  1 ğ‘‘ğ‘‰ ğºğºğ‘‡ (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) , (5) where ğ‘ ğ‘ğ‘ğ‘™ğ‘’ and ğ‘ğ‘–ğ‘ğ‘  represent the scale and shift factors respectively. The mask represents the valid non-sky regions. Finally, to ensure scale uniformity across datasets, we estimate the metric depth range using Metric3D [Hu et al. 2024] and map the estimated depths into this range. ğ‘ metric = ğ‘(0.8, dMetric3D) ğ‘(0.2, dMetric3D) ğ‘(0.8, dMoGE) ğ‘(0.2, dMoGE) ğ‘‘metric = ğ‘ metric ğ‘‘ğ‘€ğ‘œğºğ¸ ğ‘ metric ğ‘‡ ğ‘ğ‘ğ‘š = ğ¶ğ‘šğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ (cid:20) ğ‘… 0 where ğ‘(ğ‘, x) represents the ğ‘-th quantile of vector x, while ğ‘‘ğ‘šğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ and ğ¶ğ‘šğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ ğ‘ğ‘ğ‘š denote the final metric depth and camera extrinsics, respectively. (8) (cid:21) (6) (7) 12 Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu et al."
        },
        {
            "title": "D More Results",
            "content": "We provide visualization results for the initialization of 3D reconstruction in Figure 12. Our point cloud results are much better than VGGT, demonstrating that our depth estimation is more accurate than VGGT. We also provide more generation results in Figure 13 and Figure 14. Fig. 12. Comparison of initialization point clouds of ours and VGGT. Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation Fig. 13. More Results. 14 Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu et al. Fig. 14. More Visualization Results."
        }
    ],
    "affiliations": [
        "City University of Hong Kong, China",
        "Harbin Institute of Technology, China",
        "Southeast University, China",
        "Tencent Hunyuan, China"
    ]
}