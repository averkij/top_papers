{
    "paper_title": "VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large Vision-Language Models in Fact-Seeking Question Answering",
    "authors": [
        "Yanling Wang",
        "Yihan Zhao",
        "Xiaodong Chen",
        "Shasha Guo",
        "Lixin Liu",
        "Haoyang Li",
        "Yong Xiao",
        "Jing Zhang",
        "Qi Li",
        "Ke Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large vision-language models (LVLMs) have demonstrated remarkable achievements, yet the generation of non-factual responses remains prevalent in fact-seeking question answering (QA). Current multimodal fact-seeking benchmarks primarily focus on comparing model outputs to ground truth answers, providing limited insights into the performance of modality-specific modules. To bridge this gap, we introduce VisualSimpleQA, a multimodal fact-seeking benchmark with two key features. First, it enables streamlined and decoupled evaluation of LVLMs in visual and linguistic modalities. Second, it incorporates well-defined difficulty criteria to guide human annotation and facilitates the extraction of a challenging subset, VisualSimpleQA-hard. Experiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o achieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA and 30%+ on VisualSimpleQA-hard. Furthermore, the decoupled evaluation across these models highlights substantial opportunities for improvement in both visual and linguistic modules. The dataset is available at https://huggingface.co/datasets/WYLing/VisualSimpleQA."
        },
        {
            "title": "Start",
            "content": "VisualSimpleQA: Benchmark for Decoupled Evaluation of Large Vision-Language Models in Fact-Seeking Question Answering Yanling Wang1, Yihan Zhao2, Xiaodong Chen2, Shasha Guo2, Lixin Liu3, Haoyang Li2, Yong Xiao1, Jing Zhang2, Qi Li1, 4, Ke Xu1, 4 1 Zhongguancun Laboratory 2 Renmin University of China 3 Tencent 4 Tsinghua University {wangyl, xiaoyong}@ruc.edu.cn kylenliu@tencent.com {zhaoyihan, chenxiaodong, guoshashaxing, lihaoyang.cs, zhang-jing}@ruc.edu.cn {qli01, xuke}@tsinghua.edu.cn 5 2 0 M 9 ] . [ 1 2 9 4 6 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large vision-language models (LVLMs) have demonstrated remarkable achievements, yet the generation of non-factual responses remains prevalent in fact-seeking question answering (QA). Current multimodal fact-seeking benchmarks primarily focus on comparing model outputs to ground truth answers, providing limited insights into the performance of modalityspecific modules. To bridge this gap, we introduce VisualSimpleQA 1, multimodal factseeking benchmark with two key features. First, it enables streamlined and decoupled evaluation of LVLMs in visual and linguistic modalities. Second, it incorporates well-defined difficulty criteria to guide human annotation and facilitates the extraction of challenging subset, VisualSimpleQA-hard. Experiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o achieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA and 30%+ on VisualSimpleQAhard. Furthermore, the decoupled evaluation across these models highlights substantial opportunities for improvement in both visual and linguistic modules."
        },
        {
            "title": "Introduction",
            "content": "Alongside the progress in large language models (LLMs) (OpenAI, 2023; DeepSeek-AI et al., 2024), large vision-language models (LVLMs) (Hurst et al., 2024; Anthropic, 2024; DeepMind, 2024) have also emerged as cornerstone of artificial intelligence. Despite their success, generation of factually incorrect responses remains key obstacle to their wider application. To address this problem, crucial step is to effectively evaluate the fact-seeking QA capabilities of these models. Current benchmarks (Wei et al., 2024; Lin et al., 2022; Kwiatkowski et al., 2019; Marino et al., 2019; Chen et al., 2024a; Ying et al., 1The dataset is available at https://huggingface.co/ datasets/WYLing/VisualSimpleQA. 1 Figure 1: Illustration of an example in VisualSimpleQA. The red box highlights the region of interest (ROI). Each sample has several attributes and tags, which allow us to measure its overall difficulty score based on our proposed difficulty criteria. 2024; Wang et al., 2024b) often use an end-to-end approach, directly comparing the outputs with the ground truth answers. However, due to the involvement of multiple modalities, incorrect outputs from an LVLM may stem from inaccurate visual recognition, insufficient relevant knowledge, or combination of both. As result, decoupled evaluation of an LVLMs fact-seeking QA capability is essential. Furthermore, since large models advance rapidly, challenging samples are essential for evaluating cutting-edge models like GPT-4o (Hurst et al., 2024) and Claude-3.5-Sonnet (Anthropic, 2024). Recently, AI researchers and trainers at OpenAI developed benchmark named SimpleQA (Wei et al., 2024) to evaluate the factuality of LLMs, on which both GPT-4o and Claude scored below 50% correctness. However, there is lack of well-defined difficulty criteria to guide the annotation process, particularly within the LVLM community. To address these limitations, we introduce multimodal fact-seeking QA benchmark named VisualSimpleQA and have gathered team of human annotators to create the samples. Figure 1 presents an example. Specifically, VisualSimpleQA exhibits the following properties: Decoupled evaluation. In addition to the multimodal question and the ground truth answer, VisualSimpleQA includes another two annotations: the rationale for answering the multimodal question and rewritten text-only question. The rationale provides concise description of the region of interest (ROI), i.e., the information that needs to be extracted from the image for answering the question. With the rationale, we reformulate the multimodal question into self-contained, textonly question that does not rely on any visual input. Based on the text-only questions, we evaluate the linguistic modules capability in fact-seeking QA. Then we evaluate the visual module according to the relative performance degradation when transitioning from text-only QA to multimodal QA. Well-defined difficulty criteria. Compared to previous multimodal fact-seeking QA benchmarks (Marino et al., 2019; Schwenk et al., 2022), VisualSimpleQA introduces well-defined difficulty criteria that quantify each samples challenge from both visual and linguistic perspectives. These criteria are associated with attributes and tags illustrated below the image in Figure 1. These criteria not only guide human annotation but also facilitate the extraction of VisualSimpleQA-hard, set containing 129 particularly challenging samples. On VisualSimpleQA-hard, GPT-4o and Claude-3.5-Sonnet score only 30%+ correctness. This challenging set is instructive for evaluating cutting-edge models. High quality & diversity. All samples are created by humans with at least one year of experience working with large models. The ground truth answers are accurate and precise, supported by evidence from formal or authoritative sources. Additionally, we provide image sources, either as URLs or the names of public datasets. Furthermore, dedicated inspectors perform at least two quality checks on each sample. VisualSimpleQA also covers broad spectrum of topics, including research & education, company & brand, film & television entertainment, politics, history, and more, ensuring its diversity. 2 Reduced bias on evaluation. Several multimodal benchmarks (Marino et al., 2019; Schwenk et al., 2022) are built based on public image datasets like COCO (Lin et al., 2014). These publicly available datasets are likely to be used for LVLM training, which may result in bias on evaluation. To mitigate this issue, images for 200 samples (40% of VisualSimpleQA) are newly collected from the Internet. We evaluate fifteen leading closed-source and open-source LVLMs using VisualSimpleQA and VisualSimpleQA-hard. The results reveal substantial opportunities for improving LVLMs for multimodal fact-seeking QA, due to their limitations in handling complex visual recognition tasks and/or the lack of long-tailed knowledge. This paper makes the following contributions: We introduce benchmark named VisualSimpleQA, which enables streamlined, decoupled evaluation of LVLMs in fact-seeking QA, simplifying the assessment of modalityspecific modules in LVLMs. VisualSimpleQA is paired with well-defined difficulty criteria for samples. Based on the criteria, we extract VisualSimpleQA-hard, challenging sample set for evaluating the factuality of cutting-edge LVLMs. Decoupled evaluation of 15 frontier LVLMs highlights substantial room for improvement in both linguistic and visual modules for factseeking QA. State-of-the-art models such as GPT-4o achieve only 60%+ correctness on multimodal questions in VisualSimpleQA and 30%+ on those in VisualSimpleQA-hard."
        },
        {
            "title": "2 Related Work",
            "content": "Decoupled Evaluation of LVLMs. The rapid advancement of LLMs has driven the development of LVLMs (Hurst et al., 2024; Anthropic, 2024; DeepMind, 2024; Deitke et al., 2024; QwenLM, 2024). Due to the multimodal nature of LVLMs, it is non-trivial to evaluate each modality-specific module. To address this, HallusionBench (Guan et al., 2024) was designed to attribute hallucinated outputs into language hallucinations, visual illusions, or combination of both. However, the benchmark pairs some images with edited versions for evaluation and relies on complex decision tree to diagnose failure types, which may reduce its Factualityaware"
        },
        {
            "title": "Difficulty\nCriteria",
            "content": "OK-VQA A-OKVQA MMBench MMStar MMT-Bench MFC-Bench Prism HallusionBench All All Partial Partial Partial Partial Partial VisualSimpleQA All No No No No No No Yes Yes Yes No No No No No No No No Yes Note: Prism is an evaluation framework that does not include newly constructed datasets. Table 1: Comparison of properties between VisualSimpleQA and relevant multimodal benchmarks. practicality. Another approach, Prism (Qiao et al., 2024), was proposed to decouple and assess the capabilities of LVLMs. Yet, Prism requires an LLM as reference during evaluation, and as the authors analyzed, this approach may be less effective for state-of-the-art LVLMs. In contrast, VisualSimpleQA provides more streamlined approach for decoupled evaluation. Factuality Evaluation of LVLMs. In terms of LLMs, several benchmarks have been developed for factuality evaluation, including SimpleQA (Wei et al., 2024), TruthfulQA (Lin et al., 2022), Natural Questions (Kwiatkowski et al., 2019), etc. In contrast, the most commonly used benchmarks for factuality evaluation of LVLMs are still the classic OK-VQA (Marino et al., 2019) and AOKVQA (Schwenk et al., 2022). Recently, new multimodal benchmarks, such as MMBench (Liu et al., 2024), MMStar (Chen et al., 2024a), MMTBench (Ying et al., 2024), and MFC-Bench (Wang et al., 2024b), have been introduced. While these benchmarks involve factual knowledge, they primarily focus on broader range of visual understanding tasks, rather than offering fine-grained evaluation of the models factuality. Compared to these benchmarks, VisualSimpleQA supports decoupled evaluation, has well-defined difficulty criteria for samples, and provides evidences of answers to enhance the data quality. We summarize the comparison of properties between VisualSimpleQA and the aforementioned typical multimodal benchmarks in Table 1."
        },
        {
            "title": "3 Key Features of VisualSimpleQA",
            "content": "The primary motivation behind VisualSimpleQA is to evaluate modality-specific modules in an LVLM and to propose clear difficulty criteria that support 3 Figure 2: Decoupled evaluation process. the annotation of instructive and challenging samples. In this section, we will outline the process of conducting decoupled evaluations and describe how we establish the difficulty criteria."
        },
        {
            "title": "3.1 Decoupled Evaluation",
            "content": "Decoupled evaluation, which aims to evaluate the modality-specific modules, is key motivation behind VisualSimpleQA. The design of VisualSimpleQA (see Figure 1) aligns with this goal. Each sample contains multimodal fact-seeking question, self-contained text-only question, and the ground truth answer for evaluation. Figure 2 illustrates the decoupled evaluation process. Evaluation of Linguistic Module. The performance of the LLM underlying an LVLM largely influences the capabilities of the LVLM. Stronger performance on text-only questions indicates that the model has more effective linguistic module for fact-seeking QA. Conversely, if the model shows poor performance on the text-only questions, developers should enhance the knowledge in the linguistic module. Evaluation of Visual Module. We calculate the performance degradation of each LVLM when transitioning from text-only QA (TQA) to multimodal QA (MQA). Since the rationale (i.e., the necessary information that needs to be extracted from an image) contained in the text-only question can be regarded as the ground truth for visual recognition, smaller degradation indicates better performance of the visual module. To facilitate comparisons across different models, we introduce metric called relative degradation (RD), RD = scoreTQA scoreMQA scoreTQA (1) where scoreTQA and scoreMQA represent the performance scores in text-only QA and multimodal QA, respectively. This metric quantifies the degree of performance degradation when an LVLM relies on its own visual recognition capabilities."
        },
        {
            "title": "3.2 Difficulty Criteria and Measurement",
            "content": "In addition to supporting decoupled evaluation, VisualSimpleQA aims to introduce well-defined criteria for measuring samples difficulty levels."
        },
        {
            "title": "3.2.1 Difficulty Criteria\nAs an LVLM involves both visual and linguis-\ntic modules, we establish a set of difficulty crite-\nria from two perspectives—visual recognition and\nknowledge identification.",
            "content": "Resolution. Resolution is the number of pixels in an image. Higher-resolution images contain more pixels, making it easier for models to precisely identify visual features such as edge and texture. Proportion of ROI. To correctly answer multimodal question, the model requires precise visual grounding, i.e., the ability to identify the ROI. However, LVLMs may struggle to do this when the ROI 2 is small or unclear (Wu and Xie, 2024). Therefore, we use the proportion of ROI as metric, calculated as the resolution of the ROI divided by that of the original image. larger proportion of ROI typically facilitates visual recognition. Rationale Granularity. We define the essential information or visual target extracted from the ROI as the rationale. The rationale for answering question can be either coarse-grained or fine-grained. For example, one question may ask to identify the animal species panda (coarse-grained), while another might ask for specific cartoon character, such as Pikachu (fine-grained). Intuitively, coarse-grained rationales are easier to identify. Presence or Absence of Text in Image. An image may contain text that help identify the rationale. Since latest LVLMs typically perform well on the optical character recognition (OCR) task, the recognized text from the image can naturally ease the rationale identification. For example, the ROI in an 2This work focuses on multimodal questions involving single ROI, while questions requiring multiple ROIs are left for future work. image shows zoo with the name Singapore Zoo written on its entrance gate. If an LVLM effectively performs OCR, the recognized text can serve as cue for identifying the rationale Singapore Zoo. Therefore, we consider samples that can benefit from OCR to be easier. Knowledge Popularity. The training corpora for LLMs, such as CommonCrawl (Common Crawl Foundation, 2024), The Stack (Kocetkov et al., 2023), and The Pile (Gao et al., 2021), are sourced from the Internet and likely contain substantial amount of popular knowledge, which models can learn more effectively. Conversely, long-tailed knowledge is typically more difficult for language models to grasp. In this work, we prompt GPT-4o to assess the knowledge popularity for each sample, as it is leading model well trained on wide range of corpora, making it ideal for evaluating the popularity of the knowledge required to answer question. The prompt is provided in Appendix A.1."
        },
        {
            "title": "3.2.2 Difficulty Measurement",
            "content": "According to the above criteria, the overall difficulty of sample is defined as, = Avg (cid:16) 1 Rnorm, 1 R, 1 [fine], 1 [w/o TI], 1 KP (cid:17) (2) where Avg() denotes the average function, Rnorm denotes the Min-Max normalized resolution, denotes the proportion of the ROI, and KP denotes the knowledge popularity. The function 1 is the indicator function. 1[fine] takes value of 1 if the rationale of sample is fine-grained, and 0 otherwise. Similarly, 1[w/o TI] takes value of 1 if the image does not contain text that help identify the rationale, and 0 otherwise. takes value between 0 and 1."
        },
        {
            "title": "4 Annotation and Verification",
            "content": "VisualSimpleQA is meticulously annotated and verified by six human annotators, each with at least one year of experience working with large models."
        },
        {
            "title": "4.1 Annotation Process",
            "content": "Based on the image sources, we designed two methods to facilitate the annotation process. Figure 3 illustrates the flowchart of the annotation process. Annotation Based on Existing Image Datasets. An image from OK-VQA (Marino et al., 2019) or (Wu and Xie, 2024) is randomly sampled and presented to the annotator. According to the image and the difficulty criteria, the annotator creates 4 Figure 3: Flowchart of the annotation process. Evidence is used to guarantee the correctness of the answer, while ROI is annotated to calculate the difficulty of each sample. new fact-seeking multimodal question. 3 In addition to the multimodal question, the annotator is required to provide the ground truth answer with evidence, crop the ROI from the original image, specify the essential information to be extracted from the ROI (i.e., the rationale), and reformulate the multimodal question into self-contained, textonly question that does not rely on any visual input. Annotation Based on Newly Collected Images. As images from existing benchmarks might have been used for model training, evaluation results based solely on these images could introduce bias. To mitigate this, we encourage annotators to collect new images from the Internet. Since it is not easy to determine which image to collect, we design another annotation method. Specifically, the annotator begins by creating text-only question along with its ground truth answer and the evidence. Next, the annotator searches for suitable image, reformulate the text-only question into multimodal question, crop the ROI from the image, and specify the rationale. Annotation Notes. Annotators should consider the following points: (1) Difficulty control. We do not require every sample to be challenging for the advanced GPT-4o, as we hope that the diffi3If the annotator cannot create satisfactory question, they may opt to randomly sample another image. 5 culty levels of the samples are differentiated. (2) Knowledge cutoff. The questions are set to be answerable by December 31, 2023, taking into account the knowledge cutoffs of various models. (3) Indisputable, short-form, and static answers. Following SimpleQA (Wei et al., 2024), annotators are asked to create questions that have indisputable and short-form answers, which promote more objective and accurate assessments, even under the LLM-based automatic evaluation framework (Wei et al., 2024; Lin et al., 2022). For example, instead of asking, When was version 7.22 of this software for Mac released?, the question specifies, On what day, month, and year was version 7.22 of this software for Mac released? Additionally, the answer should remain consistent over time. (4) Traceability. Each sample is annotated based on at least one webpage that contains the fact required by answering the question. The webpages must be formal or authoritative. Furthermore, the source of each image is provided. If the image is from existing benchmarks, the source is the name of the benchmark; if the image is newly collected by the annotator, the source is the URL of the image. (5) Diversity. Annotators are requested to create samples involving diverse topics. Figure 4: Distribution of topics in VisualSimpleQA."
        },
        {
            "title": "4.2 Verification and Tagging",
            "content": "Verification. We engaged two AI researchers as checkers to examine all samples. Their task is to verify whether each sample met the following requirements: (1) adherence to the annotation notes mentioned in Section 4.1, (2) correct grammar, (3) absence of unsafe topics, and (4) accurate ground truth answer (the annotated evidence is useful reference). Each checker reviewed all the samples, and if sample was deemed improper by checker, the two checkers collaborated to improve it. Tagging. The tags of sample include rationale granularity, presence or absence of text in image, and topic category. An annotator tags the topic category for each sample. Two annotators tag the rationale granularity, with each annotator responsible for half of the samples. Similarly, another two annotators tag the presence or absence of text in image, with each responsible for half of the samples. To minimize ambiguity in annotators understanding of rationale granularity, we explicitly define questions about individuals, characters, organizations, locations, scientific concepts, or artworks as those requiring fine-grained rationales."
        },
        {
            "title": "5 Statistics of VisualSimpleQA",
            "content": "VisualSimpleQA comprises 500 annotated samples, including 300 images from existing image datasets and 200 newly collected images from the Internet. In this section, we provide an overview of the statistical properties of VisualSimpleQA, focusing on its topic diversity and data difficulty."
        },
        {
            "title": "5.1 Topic Diversity",
            "content": "VisualSimpleQA covers wide range of question topics, including the following topic categories: research & education, company & brand, film & teleFigure 5: Distributions of factors that influence the difficulty of visual recognition. TI denotes Text in Image. Figure 6: Distributions of knowledge popularity and overall difficulty. vision entertainment, art, geography, sports, games, history, and politics. Samples that do not fit into these categories are classified as other. Figure 4 illustrates the distribution of topics in VisualSimpleQA. We display examples of different categories in Appendix B."
        },
        {
            "title": "5.2 Data Difficulty",
            "content": "Difficulty from Visual Perspective. As shown in Figure 5, most samples in VisualSimpleQA are not of high resolution. Additionally, in most samples, the ROI, which reveals the rationale, constitutes small area of the entire image. In terms of rationale granularity, 40.6% of the samples feature fine-grained rationales. Furthermore, most images do not contain text, indicating the absence of direct cues for identifying the rationales. These characteristics highlight that VisualSimpleQA involves challenging visual recognition tasks. Difficulty from Linguistic Perspective. As shown on the left side of Figure 6, 35.2% of the samples have knowledge popularity score of 0.5 or lower, suggesting that at least 35.2% of the 6 knowledge in VisualSimpleQA is considered not popular, even for advanced models like GPT-4o. Distribution of Overall Difficulty. The right side of Figure 6 shows that the majority of samples have overall difficulty scores above 0.5, indicating the challenging nature of the benchmark. Besides, the benchmark encompasses range of difficulty levels, enabling more instructive evaluation. We select 129 samples with overall difficulty scores of 0.7 or higher to form the VisualSimpleQA-hard subset, which focuses on more challenging cases."
        },
        {
            "title": "6.1 Settings",
            "content": "Models. We conduct evaluations on 15 frontier LVLMs, including open-source and closedsource models. The closed-source models include GPT-4o (Hurst et al., 2024), Claude-3.5Sonnet-20241022 (Anthropic, 2024), and Gemini2.0-flash-exp (DeepMind, 2024). The opensource models include Molmo-7B-O-0924 (Deitke et al., 2024), LLaVA-OneVisionqwen2-7b-ov (Li et al., 2024), Qwen2-VL-7B-Instruct (Wang et al., 2024a), Qwen2.5-VL-7B-Instruct (Yang et al., InternVL2.5-8B (Chen et al., 2024b), 2024), Llama-3.2-11B-Vision-Instruct (LLaMA, 2024), Pixtral-12B-2409 (Agrawal et al., 2024), as well as larger models: Molmo-72B-0924 (Deitke et al., 2024), Qwen2-VL-72B-Instruct (Wang et al., 2024a), Qwen2.5-VL-72B-Instruct (Yang et al., 2024), QVQ-72B-Preview (QwenLM, 2024), and InternVL2.5-78B (Chen et al., 2024b). Modules in these LVLMs are shown in Appendix C. Grading and Metrics. Given the excellent performance and low API cost, we use DeepSeekV3 (DeepSeek-AI et al., 2024) as classifier to classify the LVLM-generated responses into: correct, incorrect, or refusal. 4 The prompt is provided in Appendix A.2. Based on this, we can calculate the percentage of each grade. Notably, refusal means the LVLM acknowledges its inability to answer question and thus mitigate the risk of hallucination (Zhang et al., 2023; Huang et al., 2023). To account for this, we adopt another metric # Not Refused (Wei et al., 2024), which represents the percentage of correctly answered questions out of those that were not refused. Moreover, we use # Correct 4We randomly selected 100 evaluations across different models and assessed the effectiveness of the automatic grading by human verification. 94% of them were graded accurately. Figure 7: Ratio of failures (incorrect responses and refusals) by GPT-4o across varying difficulty levels. The left sub-figure is based on samples where GPT-4o correctly answers the text-only questions but fails to answer the multimodal questions. The right is based on samples where GPT-4o fails to answer the text-only questions. Figure 8: Ratio of correctly answered questions by GPT4o across varying difficulty levels. The left sub-figure is based on samples where GPT-4o correctly answers the text-only questions and the multimodal questions. The right is based on samples where GPT-4o correctly answer the text-only questions. the relative degradation (RD) of performance (see Eq. 1) to evaluate models visual module."
        },
        {
            "title": "6.2 Evaluation Results",
            "content": "Overall Evaluation. We input multimodal questions and their corresponding text-only questions into LVLMs, respectively. The performance of different models is shown in Table 2. We make the following observations: (1) VisualSimpleQA is benchmark with challenging samples, capable of differentiating the factuality of different LVLMs. Even advanced models like GPT4o achieve only 60%+ correctness in multimodal QA. Furthermore, substantial performance differences exist across different LVLMs on VisualSimpleQA, which demonstrates the effectiveness of our benchmark in differentiating the capabilities of different LVLMs in multimodal fact-seeking QA. (2) Present frontier LVLMs still require significant improvement on difficult visual recognition tasks. By conducting decoupled evaluation, we observe that these models, particularly open-source models, still have large relative performance degradations when transitioning from textonly QA to multimodal QA. (3) Closed-source 7 Text-only QA"
        },
        {
            "title": "Multimodal QA",
            "content": "Correct Incorrect Refusal # Correct # Not Refused Correct Incorrect Refusal GPT-4o Claude-3.5-Sonnet-20241022 Gemini-2.0-flash-exp LLaVA-OneVision-qwen2-7b-ov Molmo-7B-O-0924 Qwen2-VL-7B-Instruct Qwen2.5-VL-7B-Instruct InternVL2.5-8B Llama-3.2-11B-Vision-Instruct Pixtral-12B-2409 Molmo-72B-0924 Qwen2-VL-72B-Instruct Qwen2.5-VL-72B-Instruct QVQ-72B-Preview InternVL2.5-78B 77.6 77.4 72.8 43.2 42.4 51.0 48.6 37.2 50.0 53.2 64.4 62.0 63.4 42.2 52.4 22.4 15.0 27.0 56.8 57.6 43.2 50.8 53.6 24.0 44.6 35.6 31.4 30.8 13.0 40. 0.0 7.6 0.2 0.0 0.0 5.8 0.6 9.2 26.0 2.2 0.0 6.6 5.8 44.8 6.8 77.6 83.8 72.9 43.2 42.4 54.1 48.9 41.0 67.6 54.4 64.4 66.4 67.3 76.4 56. 67.6 61.4 64.0 24.8 32.4 32.2 38.4 26.8 40.8 34.6 46.6 37.0 51.2 46.2 37.4 29.4 26.0 30.8 61.0 66.8 64.4 54.0 66.4 45.4 63.4 52.0 59.6 42.8 53.6 50. 3.0 12.6 5.2 14.2 0.8 3.4 7.6 6.8 13.8 2.0 1.4 3.4 6.0 0.2 11.8 # Correct # Not Refused RD 12.9 20.7 12.1 69.7 70.3 67. 28.9 32.7 33.3 41.6 28.8 47.3 35.3 47.3 38.3 54.5 46.3 42.4 42.6 23.6 36.9 21.0 28.0 18.4 35.0 27.6 40.3 19.2 28.6 Note: RD is an abbreviation for the relative degradation of an LVLMs performance when transitioning from text-only QA to multimodal QA. Here the performance is measured by correctness. Notably, QVQ-72B-Preview tends to refuse to respond to text-only questions, which may result in an underestimation of its actual factuality in text-only QA. Therefore, we exclude QVQ-72B-Preview from the RD metric calculation. Table 2: Evaluation of various models on VisualSimpleQA (%). Text-only QA"
        },
        {
            "title": "Correct Incorrect Refusal",
            "content": "# Correct # Not Refused Correct Incorrect Refusal GPT-4o Claude-3.5-Sonnet-20241022 Gemini-2.0-flash-exp Llama-3.2-11B-Vision-Instruct Pixtral-12B-2409 Molmo-72B-0924 Qwen2.5-VL-72B-Instruct 56.6 54.3 47. 24.0 31.8 42.6 38.8 43.4 28.7 52.7 24.8 64.3 57.4 50.4 0.0 17.0 0. 51.2 3.9 0.0 10.8 56.6 65.4 47.3 49.2 33.1 42.6 43.5 37.2 37.2 38. 20.2 17.8 22.5 27.9 55.0 38.0 53.5 52.7 78.3 76.7 56.6 7.8 24.8 8. 27.1 3.9 0.8 15.5 # Correct # Not Refused RD 34.3 31.5 19.7 40.3 49.5 41.5 27.7 18. 22.7 33.0 15.8 44.0 47.2 28.1 Table 3: Evaluation of part of models on VisualSimpleQA-hard (%). See Table 5 in Appendix for more results. # Correct frontier LVLMs have clear advantages over current open-source LVLMs. For instance, GPT4o outperforms the best-performing open-source model, Qwen2.5-VL-72B-Instruct, by 10.3% in text-only QA and 15.2% in multimodal QA in # Not Refused , and GPT-4o exhibits smaller terms of relative performance degradation. (4) Larger models significantly outperform smaller models. We evaluate different versions of Molmo-0924, Qwen2VL-Instruction, Qwen2.5-VL-Instruction, and InternVL2.5 with varying parameter sizes. The larger models consistently show better performance in both multimodal and text-only QA tasks. Evaluation on VisualSimpleQA-hard. Table 3 presents the evaluation on VisualSimpleQA-hard. VisualSimpleQA-hard increases the difficulty in both visual recognition and knowledge identification. Concretely, compared to their performance on VisualSimpleQA, each model shows significant performance drop in text-only QA, highlighting the increased difficulty in knowledge identification. Moreover, larger relative degradations are observed compared to evaluations on VisualSimpleQA, indicating the increased difficulty in visual recognition. Validation of the Difficulty Criteria. Taking GPT-4o as an example, we analyze the reasonableness of the proposed difficulty criteria. Based on Eq. 2, we further define the visual difficulty as (cid:1), and deAvg (cid:0)1 Rnorm, 1 R, 1[fine], 1[w/o TI] fine the linguistic difficulty as 1 KP . Comparing the results shown in Figure 7 and Figure 8, we observe that the failures tend to feature higher visual difficulty. Similarly, the failures also tend to feature higher linguistic difficulty compared to the correctly answered ones. Similar observations are observed in other LVLMs (see Figure 9 and Figure 10 in Appendix D)."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce VisualSimpleQA, multimodal factseeking QA benchmark for decoupled evaluation of modality-specific modules in LVLMs. Moreover, it includes series of difficulty criteria to guide human annotation, quantify sample difficulty, and help extract set of challenging samples, VisualSimpleQA-hard. Experiments with 15 frontier LVLMs demonstrate the effectiveness of VisualSimpleQA. We hope this benchmark can advance research on the factuality of LVLMs."
        },
        {
            "title": "Limitations",
            "content": "This work primarily focuses on fact-seeking questions, which typically feature concise ground truth answers. However, factuality-related challenges also emerge in other contexts, such as visual description, multimodal content creation, and multimodal reasoning. These tasks often involve longform and non-unique ground truth answers, introducing additional complexities for both benchmark development and model evaluation. We leave the exploration of these aspects to future work."
        },
        {
            "title": "References",
            "content": "Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Singh Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Théophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, Albert Q. Jiang, Kartik Khandelwal, Timothée Lacroix, Guillaume Lample, Diego de Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat, Patrick von Platen, Nikhil Raghuraman, Baptiste Rozière, Alexandre Sablayrolles, Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang, and Sophia Yang. 2024. Pixtral 12b. CoRR, abs/2410.07073. Anthropic. 2024. Claude 3.5 sonnet model card https://www-cdn.anthropic.com/ addendum. fed9cc193a14b84131812372d8d5857f8f304c52/ Model_Card_Claude_3_Addendum.pdf. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. 2024a. Are we on the right way for evaluating large vision-language models? CoRR, abs/2403.20330. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. 2024b. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. CoRR, abs/2412.05271. Common Crawl Foundation. 2024. Common crawl. https://commoncrawl.org/. DeepMind. 2024. Gemini 2.0 flash experimenhttps://deepmind.google/technologies/ tal. gemini/flash/. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, and Wangding Zeng. 2024. Deepseek-v3 technical report. CoRR, abs/2412.19437. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, Yen-Sung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross B. Girshick, Ali Farhadi, and Aniruddha Kembhavi. 2024. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. CoRR, abs/2409.17146. 9 Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2021. The pile: An 800gb dataset of diverse text for language modeling. CoRR, abs/2101.00027. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2024. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In CVPR, pages 1437514385. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. CoRR, abs/2311.05232. Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn. 2024. Gpt-4o system card. CoRR, abs/2410.21276. Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. 2023. The stack: 3 TB of permissively licensed source code. Trans. Mach. Learn. Res., 2023. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452 466. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: Measuring how models mimic human falsehoods. In ACL, pages 32143252. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: common objects in context. In ECCV, pages 740 755. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. 2024. Mmbench: Is your multi-modal model an all-around player? In ECCV, volume 15064, pages 216233. LLaMA. 2024. Llama 3.2. https://www.llama. com/docs/model-cards-and-prompt-formats/ llama3_2/. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. OK-VQA: visual question answering benchmark requiring external knowledge. In CVPR, pages 31953204. OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774. Yuxuan Qiao, Haodong Duan, Xinyu Fang, Junming Yang, Lin Chen, Songyang Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. 2024. Prism: framework for decoupling and assessing the capabilities of vlms. CoRR, abs/2406.14544. QwenLM. 2024. To see the world with wisdom. https://qwenlm.github.io/blog/ qvq-72b-preview/. Qvq: Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-OKVQA: benchmark for visual question answering using world knowledge. In ECCV, volume 13668, pages 146162. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024a. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191. 10 Shengkang Wang, Hongzhan Lin, Ziyang Luo, Zhen Ye, Guang Chen, and Jing Ma. 2024b. Mfc-bench: Benchmarking multimodal fact-checking with large vision-language models. CoRR, abs/2406.11288. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024. Measuring short-form factuality in large language models. CoRR, abs/2411.04368. Penghao Wu and Saining Xie. 2024. V*: Guided visual search as core mechanism in multimodal llms. In CVPR, pages 1308413094. IEEE. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 technical report. CoRR, abs/2412.15115. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. 2024. Mmt-bench: comprehensive multimodal benchmark for evaluating large visionlanguage models towards multitask AGI. In ICML. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023. Sirens song in the ai ocean: survey on hallucination in large language models. CoRR, abs/2309.01219."
        },
        {
            "title": "A Prompts",
            "content": "A.1 Prompt for Knowledge Popularity Quantification KNOWLEDGE_POPULARITY_PROMPT KNOWLEDGE_POPULARITY_PROMPT = \"\"\" You are knowledgeable, honest, and impartial AI assistant. Here is the question: {text_only_question} Here is the ground truth answer to the question: {ground_truth_answer} Based on the extensive knowledge you have acquired, please assess the popularity of the knowledge required to answer this question in the training corpora. Provide popularity score between 0.0 and 1.0, with higher score indicating higher popularity. Only output the popularity Score: \"\"\" A.2 Prompts for Automatic Evaluation by DeepSeek-V3 SYSTEM_PROMPT_FOR_EVALUATION SYSTEM_PROMPT_FOR_EVALUATION = \"\"\" You are helpful AI assistant. Given question, ground truth answer (Answer_1), and an answer predicted by large model (Answer_2), your task is to assess the correctness of Answer_2 based on the following criteria: 1. Label Answer_2 as 1 (correct) if it contains all the information conveyed by Answer_1 and does not include any incorrect information. If the wording of Answer_2 differs from Answer_1 but conveys the same information, Answer_2 should still be considered correct. 2. Label Answer_2 as 0 (refusal) if it explicitly states that the model cannot answer the question. 3. Label Answer_2 as -1 (incorrect) if it contains incorrect information or omits any information conveyed by Answer_1, and it does not explicitly state that the model cannot answer the question. Note: Incorrect information refers to any details in response to the question that contradict or diverge from the information provided in Answer_1. Only output the label: 1, 0, or -1. \"\"\" PROMPT_FOR_EVALUATION PROMPT_FOR_EVALUATION = \"\"\" Question: {text_only_question} Answer_1: {ground_truth_answer} Answer_2: {response} Label: \"\"\""
        },
        {
            "title": "B Display of Examples",
            "content": "12 13 14 15 Modality-Specific Modules in LVLMs"
        },
        {
            "title": "Visual Module",
            "content": "GPT-4o Claude-3.5-Sonnet-20241022 Gemini-2.0-flash-exp unknown unknown unknown unknown unknown unknown LLaVA-OneVision-qwen2-7b-ov Qwen2-7B SigLIP-SO400M Molmo-7B-O-0924 OLMo-7B-1024-preview OpenAIs ViT-L/14 336px Qwen2-VL-7B-Instruct Qwen2-7B Qwen2-ViT-675M InternVL2.5-8B InternLM2.5-7B-Chat InternViT-300M-448px-V2.5 Qwen2.5-VL-7B-Instruct Qwen2.5-7B Qwen2.5-ViT w/ window attention Llama-3.2-11B-Vision-Instruct Llama-3.1-8B ViT-H/14 Pixtral-12B-2409 Molmo-72B-0924 Qwen2-VL-72B-Instruct QVQ-72B-Preview InternVL2.5-78B Mistral-NeMo-12B Pixtral-ViT-400M Qwen2-72B Qwen2-72B Qwen2-72B OpenAIs ViT-L/14 336px Qwen2-ViT-675M ViT-675M Qwen2.5-72B-Instruct InternViT-6B-448px-V2. Qwen2.5-VL-72B-Instruct Qwen2.5-72B Qwen2.5-ViT w/ window attention Table 4: Modality-specific modules in different LVLMs. # Correct # Not Refused RD 54.3 32.2 38.8 26.6 52. 11.6 11.7 15.4 21.6 10.3 22.8 20.3 19.8 45.1 43."
        },
        {
            "title": "D Supplementary Experimental Results",
            "content": "Text-only QA"
        },
        {
            "title": "Incorrect Refusal",
            "content": "# Correct # Not Refused Correct"
        },
        {
            "title": "Incorrect Refusal",
            "content": "LLaVA-OneVision-qwen2-7b-ov Molmo-7B-O-0924 Qwen2-VL-7B-Instruct Qwen2.5-VL-7B-Instruct InternVL2.5-8B Qwen2-VL-72B-Instruct QVQ-72B-Preview InternVL2.5-78B 18.6 17.1 24.0 23.3 17.8 39.5 16.3 24.8 81.4 82.9 64.3 75.2 62.8 48.1 19.4 59. 0.0 0.0 11.7 1.5 19.4 12.4 64.3 15.5 18.6 17.1 27.2 23.6 22.1 45.1 45.7 29.4 8.5 11.6 14.7 17.1 8.5 21.7 20.2 14. 65.1 87.6 80.6 62.0 74.4 73.6 79.1 56.6 26.4 0.8 4.7 20.9 17.1 4.7 0.7 29.4 Table 5: Evaluation of models on VisualSimpleQA-hard (%). Figure 9: Ratio of failures (incorrect responses and refusals) across varying difficulty levels. The upper sub-figures are based on samples where models correctly answer the text-only questions but fail to answer the multimodal questions. The bottom sub-figures are based on samples where models fail to answer the text-only questions. Figure 10: Ratio of correctly answered questions across varying difficulty levels. The upper sub-figures are based on samples where models correctly answer the text-only questions and the multimodal questions. The bottom sub-figures are based on samples where models correctly answer the text-only questions."
        }
    ],
    "affiliations": [
        "Renmin University of China",
        "Tencent",
        "Tsinghua University",
        "Zhongguancun Laboratory"
    ]
}