{
    "paper_title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models",
    "authors": [
        "Byung-Kwan Lee",
        "Ryo Hachiuma",
        "Yu-Chiang Frank Wang",
        "Yong Man Ro",
        "Yueh-Hua Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger models brings significant computational challenges, especially for deployment on resource-constrained devices like mobile platforms and robots. To address this, we propose VLsI: Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy. VLsI leverages a unique, layer-wise distillation process, introducing intermediate \"verbalizers\" that map features from each layer to natural language space, allowing smaller VLMs to flexibly align with the reasoning processes of larger VLMs. This approach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning by aligning the small VLMs' layer-wise progression with that of the large ones. We validate VLsI across ten challenging vision-language benchmarks, achieving notable performance gains (11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling, merging, or architectural changes."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 1 2 2 8 1 0 . 2 1 4 2 : r VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models Byung-Kwan Lee1,2* Ryo Hachiuma1 Yu-Chiang Frank Wang1 Yong Man Ro2 Yueh-Hua Wu1 * Work Done during Internship, Corresponding Author 1NVIDIA, 2KAIST Figure 1. Performance overview of showing that evaluation on multiple challenging benchmarks, where GPT-4V [74], Claude-3.5-Sonnet [1], and Gemini-1.5-Pro [82], highlighting its efficiency and effectiveness across diverse tasks. VLsI on vision-language benchmarks. (a) Accuracy on MM-Vet [94] for various model sizes, VLsI (2B and 7B) achieves competitive performance compared to proprietary closed-source VLMs. (b) Comparative VLsI (green and blue) outperforms leading closed-source VLMs, including"
        },
        {
            "title": "Abstract",
            "content": "The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger models brings significant computational challenges, especially for deployment on resource-constrained devices like mobile platforms and robots. To address this, we propose VLsI: Verbalized Layers-to-Interactions, new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy. VLsI leverages unique, layer-wise distillation process, introducing intermediate verbalizers that map features from each layer to natural language space, allowing smaller VLMs to flexibly align with the reasoning processes of larger VLMs. This approach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning by aligning the small VLMs layer-wise progression with that of the large ones. We validate VLsI across ten challenging vision-language benchmarks, achieving notable performance gains (11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling, merging, or architectural changes. Project page is now accessible. 1. Introduction The integration of large language models (LLMs) with vision-language models (VLMs) has significantly enhanced the interpretive and processing capabilities of visual systems [5, 22, 73, 90]. By leveraging architectures such as CLIP-aligned vision encoders [10, 99], these VLMs have 1 achieved unprecedented performance in understanding and responding to visual inputs. The core of these advancements is visual instruction tuning [17, 60], which pairs images with question-answer texts to provide VLMs with rich, instruction-based training. Closed-source VLMs like GPT4V [73] and Gemini-Pro [82] have led the way, generating high-quality instruction tuning samples and setting new performance standards for visual language understanding. In response, open-source VLMs of various sizes, including LLaVA-OneVision(OV) [52] and Qwen2-VL [87], have rapidly emerged. While these models demonstrate the advantage of scaling for performance gains in vision-language tasks, the computational cost of larger models presents critical barrier to deployment in real-world, resourcelimited settings, such as mobile devices and autonomous robots. Consequently, the challenge lies in designing highperforming, efficient VLMs capable of handling complex visual tasks without requiring extensive hardware resources. Traditional approaches to address these constraints often involve adding specialized modules [49] or modifying model architectures [45]. However, these methods introduce significant engineering complexity and can lead to compatibility issues during deployment, particularly for ondevice applications where low-latency and memory efficiency are paramount. Furthermore, recent evaluations using benchmarks like MM-Vet [94] and MMMU [96] reveal that these structural modifications still struggle with advanced visual reasoning tasks. This raises the question: Can we achieve similar or superior level of performance without scaling, merging, or architectural changes? To address this, we introduce VLsI: Verbalized Layers-to-Interactions, new VLM family that leverages an innovative, natural language-based distillation process to efficiently transfer knowledge from large to small VLMs. Unlike traditional distillation methods, which often directly imitate outputs from larger model, VLsI introduces layer-wise approach where each intermediate layer generates verbal responses in natural language space, enhancing interpretability and alignment with larger models. This is achieved through three-step process: (1) the verbalization step, which uses verbalizers to project intermediate features into the language space, making them interpretable as text-based responses; (2) the interaction step, which performs adaptive layer matching to align the reasoning progression between large and small VLMs; and (3) the reinforcement step, which finetunes the distilled VLMs for taskspecific instruction-following responsiveness."
        },
        {
            "title": "Our experiments validate",
            "content": "VLsIs effectiveness across ten challenging benchmarks, demonstrating significant performance gains of 11.0% (2B model) and 17.4% (7B model) over GPT-4V. Notably, these improvements are achieved without increasing model size, merging modules, or modifying the architecture. Our contributions are as follows: We introduce VLsI, new VLM family that applies natural language-based, layer-wise distillation, offering scalable solution to high-performing yet efficient-scale VLMs without requiring scaling or structural changes. VLsI achieves high performance without adding extra modules or architectural modifications, making it practical and deployable solution for on-device applications in resource-constrained environments. VLsI is easy to implement and adaptable across different model architectures, showing significant gains not only with Qwen2-VL but also with LLaVA-OV, where it achieves 19.7% improvement in 2B and 7B model sizes (Qwen2-VL), and 34.5% improvement in 0.5B and 7B model sizes (LLaVA-OV) on challenging benchmarks like MMB, MM-Vet, and MMMU. This work not only advances the state of VLMs in computationally efficient manner but also sets precedent for deploying high-performing, low-complexity models in real-world applications, bringing us step closer to scalable, practical artificial intelligence. 2. Related Works Evolution of Vision-Language Models. The emergence of visual instruction tuning: LLaVA [60] and InstructBLIP [17] initially brings in not only introducing slight variations of VLMs [3, 7, 16, 44, 50, 101, 106], but also curating high-quality visual instruction samples [6, 8, 26, 30, 86, 93, 103]. Since that time, there has been growing interest in enhancing visual understanding; thus the simple visual input technique of enlarging image resolution or dividing images into smaller sections with fixed or dynamic rules has got standardized [11, 31, 56, 61, 69, 89]. Furthermore, merging additional visual encoders [25, 43, 75, 99] or multiple computer vision models [12, 21, 71, 91] into LLMs have also become major focus [32, 48, 49, 65, 77, 107]. Besides, Meteor [47] employs an additional rational projector that embeds multifaceted reasoning information to cover diverse capabilities, including chart, diagram, document, and math reasoning. More recently, Cambrian-1 [84], LLaVAOneVision [52] InternVL2 [11], Molmo [19], and Qwen2VL [87] have released large scale models in order to follow or surpass the performances of closed-source VLMs. While these advancements are both rapid and impactful, relatively few studies focus on achieving high-performing yet efficient-scale VLMs within limited architectures. Furthermore, many current approaches rely heavily on GPTbased instruction datasets, and only the final layer learns the target responses in visual instruction tuning. On the VLsI presents how to effectively harness other hand, the capabilities of larger VLMsalready outperforming the closed-source onesto transfer internally embedded knowledge from large to small VLMs. 2 (a) Verbalization Step Illustration of the training process in Figure 2. (a) In the verbalization step, intermediate layers in both the largeand small-backbone VLMs are equipped with verbalizer, allowing their outputs to be projected into natural language space. Autoregressive loss is applied to align these verbalized outputs with the target responses. (b) In the interaction step, each intermediate layer in the small-backbone VLM searches for matching layer in the large backbone VLM within specified range. For example, once the 2nd layer of the small VLM is matched with the 4th layer in the large VLM, the next matching search for the 3rd layer in the small VLM will proceed from the 5th to the 7th layers of the large VLM, ensuring progressive alignment. VLsI, showing (a) the verbalization step and (b) the interaction step. (b) Interaction Step Efficient Modeling Strategy. In the field of lightweight LLMs, MobiLlama [83], OpenELM [70], MobileLLM [63] have leveraged various engineering techniques such as shared feed-forward network (FFN) design, layer-wise scaling, and embedding-language head sharing to efficiently reduce model parameters. Their primary objective is not to close the gap with closed-source LLMs but rather to reduce parameters impacting less performance degradation. In the field of VLMs, there are variations about how to utilize pretrained lightweight LLMs [2, 33, 102] in order to make efficient-scale VLMs [13, 14, 57, 76, 104, 105], but these works are also not fundamental solution to embed more vision-language knowledge within limited structures. Notably, two VLM approaches, TroL [46] and Phantom [45], aim to expand learning own capabilities within limited structures by doubling forward propagation steps and enlarging the latent dimension without physically increasing model sizes, thereby showing large improvements. Unfortunately, these approaches face limitations such as key-value cache storage constraints and extensive architectural modifications, which may hinder direct application to real-world scenarios. Besides, few works in distilling LLMs [72, 80] have emerged, but they use in the end for the final layer distillation. For VLMs, LLaVA-MoD [78] and LLaVA-KD [4] have also been recently proposed, but they use the same way. On the other hand, VLsI makes layer-wise distillation process where we leverage natural language in order to make small VLMs mimic the reasoning progression of large VLMs across layers. We hope that incorporating natural language will facilitate smoother communication between large and small VLMs, alleviating the complexities of direct feature alignment. 3. VLsI: Verbalized Layers-to-Interactions VLsI comprises two main components: Overview of Model Architecture. As illustrated in Fig. 2, the backbone VLM and verbalizers. For the backbone VLM, we use Qwen2-VL [87], selected for its high performance on the OpenVLM-Leaderboard [15]. The verbalizer consists of simple FFN [85] and the language head from the backbone VLM. To distinguish between FFN components, we refer to the FFN within the verbalizer as verb-FFN to avoid confusion with the FFN in the LLM transformer decoder block. This design is inspired by the recent speculative decoding paradigm [55], which demonstrates the effectiveness of using smaller LLM constructed with frozen word embedding and the language head of larger LLMs to emulate the performance of those larger models. Building on this insight, we incorporate the language head of the backbone VLM into its intermediate layers. Specifically, the verb-FFN is placed between each intermediate layer and the language head, introducing trainable parameters that allow effective projection into the language space. In other words, the verb-FFN enables outputs from the intermediate layers to project to the language space via the language head. In the following sections, we detail the three critical training stages: verbalization, interaction, and reinforcement. 3.1. Verbalization Step In this step, we introduce verbalizer for each target intermediate layer (see Fig. 2(a)), allowing the outputs of these layers to be mapped to the natural language space. Each verbalizer comprises verb-FFN (the yellow block in Fig. 2(a)) and language head (the blue block in Fig. 2(a)), which is the same language head used in the corresponding backVLsI Figure 3. Example of verbalized outputs from each intermediate target layer in an alternative small-backbone VLM (without VLsI. The visual question prompts VLM to predict the missing image in sequence pattern. The outputs enhancements) and the illustrate how each layer progressively interprets the visual cues, with VLsI accurately identifying the answer as star with dot in the final layer, while the alternative small-backbone VLM incorrectly predicts diamond with dot. This demonstrates the improved interpretative capability of VLsI through layer-wise, language-based distillation. bone VLM. It is important to note that the outputs from intermediate layers are not directly translatable to the natural language space. To address this, we apply an additional verbalizer to process these intermediate layer outputs. To optimize the mapping from embeddings to natural language, we leverage instance pairs from visual instruction tuning datasets and apply an autoregressive loss to ensure the verbalized output aligns with the target response. Since the weights in both backbone VLMs are fixed, the gradient updates for each verbalizer at each layer remain independent. Our goal here is not for the intermediate layers to generate the correct response but rather to showcase their capacity to express verbal information given specific visual and text inputs. Fig. 3 presents the verbalization results, displaying the output of each layer along with its corresponding verbalizer. The results show gradual improvement in reasoning and response accuracy as layers progress deeper. This verVLsI to track the evolution of balization enables our verbal responses in the natural language space, facilitating clearer understanding of the key developments required to generate desired responses and thereby offering more efficient distillation approach. 3.2. Interaction Step After the verbalization step, we proceed with distillation process that leverages the natural language output from each intermediate layer. The main objective of this interaction step is to establish an effective mapping between the layers of the large-backbone and small-backbone VLMs. This approach ensures that the small-backbone VLM mirrors the reasoning progression in the large one as layers deepen. Furthermore, because the computational requirements for each key development in generating the desired response vary, we propose an adaptive layer-matching strategy that dynamically aligns corresponding layers. Extracting Verbal Information for Distillation. To extract verbal information from intermediate layers, we use the vocabulary probabilities from the language head in the verbalizer based on input from the visual instruction training dataset. This method avoids the high computational cost of text generation (e.g., greedy or beam search [27]), which would be prohibitively resource-intensive for each layer across both the large and small VLMs. Layer Matching for Distilling Reasoning Progression. To achieve effective layer matching, we develop strategy to pair layers between the largeand small-backbone VLM, allowing the small model to learn the reasoning progression encoded in the large model. We define the number of target layers in the large and small VLMs as tl and 4 VLMs QBench AI2D ChartQA POPE HallB MME MathVista MMB MMBCN MM-Vet MMMU LLaVA-NeXT-7B [61] LLaVA-NeXT-8B [61] LLaVA-NeXT-13B [61] MM1-7B [69] MM1-MoE-7B32 [69] MiniGemini-HD-7B [56] MiniGemini-HD-13B [56] Cambrian-1-8B [84] Cambrian-1-13B [84] Eagle-8B [77] Eagle-13B [77] VILA1.5-8B [58] VILA1.5-13B [58] VILA2-8B [26] CogVLM2-8B [35] LLaVA-OneVision-7B [52] InternVL2-8B [10] MiniCPM-V2.5-8B [92] MiniCPM-V2.6-8B [92] TroL-7B [46] Phantom-7B [45] Qwen2-VL-7B [87] VLsI-7B - - - - - - - - - - - - - - - 73.6 73.8 77.5 77.5 - 71.6 70.0 - - - - 73.0 73.6 76.1 74.0 - - - 73.4 81.4 83.8 - - 78.5 79.5 77.5 87.3 - 69.5 62.2 - - - - 73.3 73.8 80.1 77.6 - - - 81.0 80.0 83.3 - - 71.2 87.8 83.0 86. 86.5 - 86.7 86.6 87.8 - - - - - - 85.6 86.3 86.7 - - - - - 87.8 87.7 88.9 88.6 - - - - - - - - - - - - - - - - - - - 65.3 65.4 65.7 74.2 1851 1972 1892 1858 1992 1865 1917 - - - - - - - 1870 1998 2210 2025 2348 2308 2126 2327 34.6 37.5 35.1 35.9 40.9 32.2 37.0 49.0 48.0 52.7 54.4 - - - - 63.2 58.3 54.3 60.6 51.8 70.9 58.2 74.7 69.6 72.1 70.0 72.3 72.7 65.8 68.6 75.9 75.7 75.9 75.7 75.3 74.9 76.6 80.5 80.8 81.7 77.2 - 83.5 84.8 83.0 86.3 63.3 - 68.5 - - - - - - - - 69.9 66.3 71.7 - - 81.2 74.2 - 81.2 84.7 80.5 85. 43.9 - 47.3 42.1 45.2 41.3 50.5 - - - - 43.2 44.3 50.0 60.4 57.5 54.2 - 60.0 54.7 70.8 62.0 75.2 35.1 41.7 35.9 37.0 40.9 36.8 37.3 42.7 40.0 43.8 41.6 38.6 37.9 38.3 44.3 48.8 49.3 45.8 49.8 49.9 51.2 54.1 69.3 Table 1. Evaluation of existing open-source VLMs and VLsI on various vision-language benchmarks: QBench [88], AI2D [41], ChartQA [67], POPE [54], HallB [59], MME [28], MathVista [66], MMB [62], MMBCN [62], MM-Vet [94], and MMMU [96]. Bold and underline indicate the top and second-best results, respectively. ts, respectively, with ranges il = 0th, 1st, , (tl 1)th and is = 0th, 1st, , (ts 1)th, where ts tl. Our matching strategy iterates over the ts layers in the small VLM, ensuring that each layer has corresponding target layer in the large VLM. Note that, layers in the largebackbone VLM may not have corresponding layers in the small one. To maintain the reasoning progression as layers deepen, we employ strategy that respects two key criteria: (i) Order Preservationthe matched layer (largebackbone VLM) of layer (small-backbone VLM) should be deeper than the matched layer of layer 1, ensuring > k; and (ii) Layer-wise Exploration (Multinomial Sampling)to encourage novel and effective configurations for layer matching, we sample layers based on distribution that is inversely proportional to the KL divergence between the verbal distributions of the matched layers in the large and small VLMs. Specifically, we compute this sampling distribution using softmax of the scaled, negative KL divergence values, as summarized in Algorithm 1. 3.3. Reinforcement Step In the final stage of our distillation framework, we finetune the entire small-backbone VLM, including word embeddings, multi-head attention, FFN, and the language head on the visual instruction dataset in supervised-learning manner. This reinforcement step is inspired by pruning-based distillation methods [72, 80], which require additional training after pruning to counteract potential performance drops from structural changes. While our interaction step does not alter the models structure, fully absorbing the rich information from the large VLM remains challenging for the Algorithm 1 Pseudo-Code for Interaction Loss : 0, ϵ: 1e-6, scale: 2 kld-list = [] for il in 1: Input: ts, tl 2: Initialize: loss: 0, 3: for is in 0 is < ts do 4: 5: 6: 7: 8: 9: il tl ts + is (Search Range) do kld-list.append(compute-kld(is, il)) scale kld-list.maxkld-list.min+ϵ {Temperature} end for p-list Softmax (kld-list/T ) 1 + Multinomial(p-list) {Sampling Index} loss loss +kld-list[i ] 10: 11: 12: end for 13: Return: loss small VLM within single interaction step. The reinforcement step allows the small VLM to better align and integrate the acquired knowledge across layers, enhancing response quality through autoregressive loss. We avoid optimizing autoregressive loss during the interaction step (where KL divergence loss is optimized instead) as this can degrade performance, as demonstrated in our ablation study (Sec. 4). Therefore, to improve task-specific instruction-following responsiveness and accuracy, we incorporate the reinforcement step with autoregressive loss to further finetune the small-backbone VLM. 4. Experiments 4.1. Implementation Details VLsI to We present four key technical components of ensure reproducibility: (a) the configuration of the back5 VLMs QBench AI2D ChartQA POPE HallB MME MathVista MMB MMBCN MM-Vet MMMU MiniCPM-2.4B [38] MiniCPM-V2-2.8B [38] MM1-3B [69] MM1-MoE-3B64 [69] ALLaVA-3B [6] VILA1.5-3B [6] InternVL2-4B [10] TroL-3.8B [46] Phantom-3.8B [45] DeepSeek-VL-1.3B [65] MobileVLM-1.7B [13] MobileVLM-V2-1.7B [14] MoE-LLaVA-1.8B4 [57] Mini-Gemini-2B [56] InternVL2-2B [10] TroL-1.8B [46] Phantom-1.8B [45] Qwen2-VL-2B [87] VLsI-2B - - - - - - - 70.0 70.3 - - - - - - 68.2 69.1 70.8 72.3 56.3 62.9 - - - - 78.9 73.6 71.7 - - - - - 74.1 68.9 62.3 60. 89.0 - - - - - - 81.5 73.8 87.3 - - - - - 76.2 64.0 87.0 73.5 85.8 - - 87.4 87.6 - 85.3 - 86.5 87.1 87.6 84.5 84.3 87.0 - - 88.6 89.6 87. 87.9 - - - - - - - 62.2 60.8 - - - - - - 60.1 62.2 61.2 70.0 1650 1809 1762 1773 1623 - 2064 1980 2046 - - - - 1653 1877 2038 1885 2022 28.9 38.7 32.0 32.6 - - 58.6 55.1 60.6 31.1 - - - 29.4 46.3 45.4 60.9 43.0 68.4 64.1 69.1 67.8 70.8 64.0 62.8 78.6 79.2 80.4 64.6 53.2 57.7 59.7 59.8 73.2 76.1 76.6 74. 81.7 62.6 66.5 - - - 52.2 73.9 77.1 77.1 62.9 - - - - 70.9 74.1 75.1 73.5 78.8 31.1 41.0 43.7 42.2 32.2 38.6 51.0 51.1 54.4 34.8 - - 25.3 - 39.5 45.1 54.1 49. 64.8 - - 33.9 38.6 35.3 33.3 34.3 37.5 39.2 32.2 - - - 31.7 34.3 35.2 40.6 41.1 51.4 Table 2. Comparison of smaller open-source VLMs and VLsI on the same evaluation benchmarks as in Table 1. bone vision-language model, (b) the architecture of the verbalizer, (c) the training and inference configuration, and (d) the structure of the visual-instruction dataset, which includes variety of capabilities crucial for effectively building VLsI. design reduces computational complexity while preserving overall performance. For intermediate target layers, we select is: 2nd, 6th, 10th, ..., and 26th layers, and it: 2nd, 6th, 10th, ..., and 78th layers. (a) Configuration of the Backbone VLM. We select Qwen2-VL [87] as our backbone VLM due to its flexible model scaling options, which include Qwen2-1.5B, Qwen27B, and Qwen2-72B [90]. Importantly, the tokenizers vocabulary indices remain consistent across these model sizes, allowing for seamless integration without reordering the vocabulary. This structure enables us to focus on optimizing the LLM component, where Qwen2-1.5B and Qwen2-7B each contain 28 layers, and Qwen2-72B consists of 80 layers. For the vision encoder and projector, we adopt the same modules as those in Qwen2-VL: the vision encoder is ViT model [20] adapted from DFN [24] and enhanced with visually-adapted rotary positional embeddings [81]. The vision projector comprises an MLP with two fully-connected layers interleaved with GELU activations [34]. (b) Architecture of the Verbalizer. We design the verbalizer as sequential feed-forward network (FFN), similar to the FFN typically used in transformer blocks [85], and as language head of the backbone VLM. Conventionally, an FFN consists of three MLPs responsible for dimensional expansion, gating, and reduction. This configuration first expands the hidden dimension, then applies importance weighting to emphasize relevant features, and finally reduces the features back to the original hidden dimension. To enhance computational efficiency in verbalization and interaction, we opt to maintain consistent hidden dimension throughout the process, foregoing the typical expansion and reduction steps. This streamlined FFN (verb-FFN) (c) Training and Inference Configuration. Training and VLsI are conducted on 8 NVIDIA A100 inference for training, we apply To enable efficient 80GB GPUs. LoRA [37] to the LLM component, setting the rank and alpha parameters to 64. We use the AdamW optimizer [64] with cosine annealing schedule, adjusting the learning rate from 1e4 to 1e6 over each training step. To handle large batch sizes effectively, we employ gradient accumulation with 16 steps and gradient checkpointing [79] to optimize memory usage. Specifically, batch configurations are four batches each for the 2B and 7B models and two batches for the 72B model, resulting in total gradient update counts of 512 (8164) and 256 (8162), respectively. For inference, we maintain the setup used in Qwen2-VL with greedy search for text generation."
        },
        {
            "title": "This dataset",
            "content": "includes foundational (d) Visual-instruction dataset. Following the methodology in [45], we compiled diverse dataset spanning totaling broad range of vision-language capabilities, 2.9 million visual instruction tuning samples from various sources. image understanding samples sourced from datasets such as ShareGPT4o-Images (57K) [23], ShareGPT4V (755K) [8], and MiniGemini (548K) ALLaVA-VFLAN/Text (27K) [56], which are focused on tasks like DocVQA [68], ChartQA [67], DVQA [39], and AI2D [41]. To support scientific and mathematical reasoning, we incorporated samples from LLaVA-HD (116K) [103], enhancing datasets like ArXivQA [53] and TextbookQA [42]. Additionally, we included document understanding samples from mPLUG- [6], Benchmarks OmniFusion-7B DeepSeek-VL-7B MoVA-7B Eagle-8B CoLLaVO-7B MoAI-7B Meteor-7B VLsI-2B VLsI-7B MMB [62] MathVista [66] MM-Vet [94] MMMU [96] 69.0 - 39.4 36.6 73.2 - 41.5 36.6 81.3 44.3 - - 75.9 52.7 - 43. 83.0 57.6 40.3 42.2 79.3 56.2 43.7 45.6 82.9 53.4 57.3 48.3 81.7 68.4 64.8 51.4 86.3 74.7 70.8 69.3 (a) Validation of open-source VLMs with additional modules and projectors compared to MoVA [40], Eagle [77], CoLLaVO [48], MoAI [49], and Meteor [47]. VLsI: OmniFusion [32], DeepSeek-VL [65], VLMs MM-Vet MM-Vet-v2 MMMU MMStar AI2D SEED-2-Plus MathVista BLINK CV-Bench LLaVA-Wilder LLaVA-NeXT-34B [61] VILA1.5-40B [58] Cambrian-34B [84] Molmo-72B [19] LLaVA-OV-72B [52] LLaMA-3.2-Vision Claude3.5-Sonnet [1] NVLM-D-72B [18] GPT-4V (0409) [8] Gemini-1.5-Pro InternVL2-76B [10] GPT-4o (0806) Qwen2-VL-72B [87] TroL-1.8B [46] TroL-7B [46] Phantom-1.8B [45] Phantom-7B [45] VLsI-2B VLsI-7B 50.7 51.2 53.2 61.1 60.6 64.1 66.0 58.9 67.5 64.0 64.4 75.1 73. 45.1 54.7 54.1 70.8 64.8 75.8 50.9 - - - - - 71.8 - 66.3 66.9 68.4 71.0 68.7 - - 46.3 60.6 60.8 70.0 48.8 55.1 50.4 52.8 56.6 60.3 65.9 60.8 61.7 60.6 58.3 69.9 64. 35.2 49.9 40.6 51.2 51.4 69.3 51.6 55.2 54.2 63.3 65.8 55.3 62.2 63.7 56.0 59.1 67.1 64.7 68.6 45.5 51.3 45.5 57.3 76.6 73.6 78.9 77.8 79.5 83.4 86.2 69.5 80.2 80.1 78.6 79.1 87.6 84.7 88. 68.9 78.5 62.3 79.5 89.0 87.3 65.9 - 65.1 - - 68.2 71.7 68.4 69.3 70.8 70.0 70.8 72.3 - - 57.1 65.5 81.1 74.9 40.4 49.5 50.3 55.8 68.4 58.3 61.6 63.9 54.7 57.7 65.6 62.7 69. 45.4 51.8 60.9 70.9 68.4 74.7 - - - - - 48.0 28.2 48.0 58.3 59.1 57.5 64.7 60.5 - - 44.2 58.9 52.4 59.7 - - 76.9 - - - - - 69.1 - - - 74. - - 63.1 74.9 90.1 89.1 - - - - 72.0 - 83.1 - 71.5 - - 85.9 84.1 - - 78.5 82.9 90.1 92.0 (b) Comparison of VLsI with other open-source and closed-source VLMs on challenging benchmarks: MM-Vet [94], MM-Vetv2 [95], MMMU [96], MMStar [9], AI2D [41], SEED-2-Plus [51], MathVista [66], BLINK [29], CV-Bench [84], and LLaVA-Wilder [52]. This comparison includes models embedding additional knowledge [45, 46] and larger open/closed-source VLMs. Table 3. Detailed comparison of provides detailed descriptions of the evaluation benchmarks listed in Tables 1 and 2. VLsI with various open and closed-source VLMs on challenging evaluation benchmarks. Appendix tive progression, this comparison highlights how both models gradually enhance understanding across layers. At the shallower layers, both models generate basic descriptions, focusing on large, simple shapes and colors. However, as VLsI progresses to mid-level layers, it begins to recognize and articulate more complex visual structures, such as labeled shapes and their relative positions. In contrast, the small-backbone VLMs verbal responses remain relatively vague or repetitive, often lacking in specific relational details. By the deeper layers, VLsI demonstrates clear advantage: its verbalizations shift towards identifying the correct pattern, explicitly referring to shapes and colors in alignment with the target response: star with dot. Meanwhile, the small-backbone VLM incorrectly predicts the missing image as diamond with dot, failing to capture the specific pattern. This example underscores the efVLsIs layer-wise verbalization, where fectiveness of each stage of verbal responses helps the small-backbone VLM align with the larger one. Additional examples of VLsIs verbalization are available in Appendix B, highlighting its capacity to interpret layer-wise verbal responses. 4.3. Comparison on Evaluation Benchmarks As shown in Tab. 1 and Tab. 2, VLsI achieves higher performance while maintaining an efficient model scale. Figure 4. Comparison of performance on MM-Vet [94] and MMMU [96] across different model size combinations in large and small backbone VLMs. Each cell shows the evaluation results for various interaction configurations between 0.5B, 2B, and 7B small backbone VLMs trained with either Qwen2-VL [87] or LLaVA-OV [52] as the large-backbone VLM. DocOwl1.5-Downstream/Reasoning (599K) [36]. For more general mathematical tasks, our dataset features samples from GLLaVA (177K) [30], MathVision (3K) [86], MathInstruct (262K) [97], and MathPlus (304K) [98]. 4.2. Verbalization on"
        },
        {
            "title": "VLsI",
            "content": "Fig. 3 illustrates the verbal responses generated at each intermediate layer in small-backbone VLM and VLsI. Using the verbalized outputs to trace each layers interpre7 VLMs MMB MM-Vet MMMU VLMs RS MMB BLINK MM-Vet MMMU RS Training Percent. MMB BLINK MM-Vet MMMU LLaVA-OV-0.5B VLsI-0.5B (LLaVA-OV-72B) LLaVA-OV-7B VLsI-7B (Qwen2-VL-72B) VLsI-7B (LLaVA-OV-72B) LLaVA-OV-72B 52.1 72.5 80.8 86.3 86. 85.9 29.1 50.7 57.5 75.8 61.6 63.7 31.4 49.9 48.8 69.3 59. 56.8 Qwen2-VL-2B VLsI-2B VLsI-2B Qwen2-VL-7B VLsI-7B VLsI-7B Qwen2-VL-72B - - - 74.9 73.2 81.7 83.0 82.1 86.3 86.5 41.4 40.1 52.4 50.8 49.6 59. 60.5 49.5 47.9 64.8 62.0 60.5 75.8 73.9 41.1 39.8 51.4 54.1 52.9 69. 64.3 Qwen2-VL-7B +50% +100% VLsI-7B +50% +100% 80.5 81.0 81.8 82.1 85.4 86.3 50.8 51.5 52. 49.6 56.0 59.7 62.0 62.8 63.7 60.5 70.0 75.8 54.1 54.6 55.2 52.9 62.1 69.3 (a) Backbone VLMs (b) Use of Reinforcement Step (RS) (c) Percentage of Training Iterations in RS IL-Ops CE CE CE L2 KLD KLD KLD L2 51.3 50.2 53.5 53. 79.2 77.8 81.0 81.5 LL-Ops MMB BLINK MM-Vet MMMU CE KLD KLD CE KLD KLD 69.5 68.5 75.8 67.0 64.5 63.2 67.2 66.8 61.0 59.8 69.3 58.3 56.5 55.2 59.0 58. 83.0 81.5 86.3 81.7 55.0 54.3 59.7 53.5 VLMs MMB BLINK MM-Vet MMMU Structure Params MMB BLINK MM-Vet MMMU Random Index Bottom-1 Index Bottom-3 Index 77.0 81.2 81.5 82.0 Multinomial Sampling 83.5 +Search Range +Order Preservation 86.0 +Adaptive Temperature 86.3 50.0 53.8 54.0 54.5 55.5 59.2 59.7 62.0 67.8 68. 68.5 69.8 75.2 75.8 52.0 57.7 58.0 58.5 60.0 68.5 69.3 Decoder2 Decoder FFN2 FFN verb-FFN2 verb-FFN MLP2 MLP 3.3B 85.5 1.6B 85.7 2.9B 86.3 1.4B 86.4 539M 85.8 269M 86.3 180M 84.2 90M 83.8 59.1 59.0 59.2 59.4 59.9 59.7 57.5 57. 76.2 76.0 75.9 75.7 75.8 75.8 74.1 73.5 68.6 68.4 69.3 69.2 69.3 69.3 67.0 66.7 (d) Operations for Intermediate/Last Layers (e) Components in Matching Strategy (f) Verbalizer Architecture Table 4. Ablation studies examining the six main factors influencing the effectiveness of VLsI. Figure 5. Distribution changes of the matched indices between small-backbone and large-backbone VLMs at the interaction step. The left figure shows the distribution at the beginning of training, while the right figure shows it at the end. VLsIs performance Furthermore, Tab. 3 compares with (a) open-source VLMs that incorporate multiple vision encoders, computer vision models, and additional rationale projector; (b) VLMs with modified architectures [45, 46] and various larger openand proprietary closed-source VLMs. To evaluate where the effectiveness comes from, we analyze six key factors detailed in Fig. 4 and Tab. 4. Appendix explains the detailed settings for ablation studies. These results indicate that (1) utilizing more capable large-backbone VLM provides substantial performance benefits, suggesting that the choice of backbone significantly impacts the transfer of knowledge; (2) using largerbackbone VLM gets benefits; (3) KL divergence is more effective during interaction step than cross-entropy, and simultaneously using last-layer distillation boosts performances; and (4) the reinforcement step is crucial for further performance gains, consistent with findings in [72, 80]. VLsIs text generation quality. Appendix represents 4.4. Discussion and Limitations VLsIs verbalizer is not re-trained but just For Fig. 3, utilized by the trained verbalizer of the small-backbone VLM. Interestingly, this verbalizer also works well at VLsI, demonstrating flexibility and indicating that it may serve as practical medium of interpretability. Additionally, Fig. 5 illustrates that, as the interaction step progresses, the small-backbone VLM gradually tries to learn about deeper layers responses of the large-backbone VLM, which can be considered accelerating the process of reaching an answer. While VLsI is highly effective, the large and small-backbone VLMs must share the same tokenizer and token index order when constructing VLsI. We will explore more general ways that accommodate different tokenizers and token index orders, potentially expanding VLsIs applicability and scalability. 5. Conclusion We present new VLM family with 2B and 7B model sizes, VLsI: Verbalized Layers-to-Interactions, designed to build high-performing yet efficient-scale VLMs. This is accomplished by leveraging natural language-based distillation to transfer knowledge from large to small VLMs. VLsI achieves strong vision-language perforWe show mances, suggesting that natural language is an important key in transferring knowledge not only for humans but also for AI. We hope to keep progress in further utilization of natural language and larger VLMs."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com, 2024. 1, 7 [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 3 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2 [4] Yuxuan Cai, Jiangning Zhang, Haoyang He, Xinwei He, Ao Tong, Zhenye Gan, Chengjie Wang, and Xiang Bai. Llavakd: framework of distilling multimodal large language models. arXiv preprint arXiv:2410.16236, 2024. 3 [5] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei arXiv preprint Chu, et al. arXiv:2403.17297, 2024. 1 Internlm2 technical report. [6] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024. 2, 6 [7] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 2 [8] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 2, 6, 7 [9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evalarXiv preprint uating large vision-language models? arXiv:2403.20330, 2024. 7, 15 [10] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 1, 5, 6, 7 [11] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. arXiv preprint arXiv:2404.16821, 2024. 2 [12] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. [13] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886, 2023. 3, 6 [14] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint arXiv:2402.03766, 2024. 3, 6 [15] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https: / / github . com / open - compass / opencompass, 2023. 3 [16] XTuner Contributors. Xtuner: toolkit for efficiently fine-tuning llm. https://github.com/InternLM/ xtuner, 2023. 2 [17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. 2 [18] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint, 2024. 7 [19] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-theart multimodal models. arXiv preprint arXiv:2409.17146, 2024. 2, [20] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 6 [21] Yuning Du, Chenxia Li, Ruoyu Guo, Cheng Cui, Weiwei Liu, Jun Zhou, Bin Lu, Yehua Yang, Qiwen Liu, Xiaoguang Hu, et al. Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144, 2021. 2 [22] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1 [23] et al. Erfei Cui. timodal annotations with gpt-4o, 2024. sharegpt4o.github.io/. 6 Sharegpt-4o: Comprehensive mulhttps : / / [24] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. 6 9 [25] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1935819369, 2023. 2 [26] Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jang Hyun Cho, Marco Pavone, Song Han, and Hongxu Yin. Vila2: Vila augmented vila. arXiv preprint arXiv:2407.17453, 2024. 2, 5 [27] Markus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation, pages 56 60, Vancouver, 2017. Association for Computational Linguistics. 4 [28] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 5, [29] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 7, 15 [30] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. 2, 7 [31] Chunjiang Ge, Sijie Cheng, Ziming Wang, Jiale Yuan, Yuan Gao, Jun Song, Shiji Song, Gao Huang, and Bo Convllava: Hierarchical backbones as visual Zheng. arXiv preprint encoder for large multimodal models. arXiv:2405.15738, 2024. 2 [32] Elizaveta Goncharova, Anton Razzhigaev, Matvey Mikhalchuk, Maxim Kurkin, Irina Abdullaeva, Matvey Skripkin, Ivan Oseledets, Denis Dimitrov, and Andrey Kuznetsov. Omnifusion technical report. arXiv preprint arXiv:2404.06212, 2024. 2, 7 [33] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. [34] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 6 [35] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language modarXiv preprint els for image and video understanding. arXiv:2408.16500, 2024. 5 [36] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895, 2024. 7 [37] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 6, [38] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. 6 [39] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656, 2018. 6 [40] Oguzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, and Federico Tombari. Brave: Broadening the visual encoding of vision-language models. arXiv preprint arXiv:2404.07204, 2024. 7 [41] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235 251. Springer, 2016. 5, 6, 7, 14 [42] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern recognition, pages 49995007, 2017. 6 [43] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment In Proceedings of the IEEE/CVF International anything. Conference on Computer Vision, pages 40154026, 2023. [44] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, Obelisc: An open web-scale filtered dataset et al. arXiv preprint of interleaved image-text documents. arXiv:2306.16527, 2023. 2 [45] Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, BeomPhantom of latent arXiv preprint chan Park, and Yong Man Ro. for large language and vision models. arXiv:2409.14713, 2024. 2, 3, 5, 6, 7, 8 [46] Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, and Yong Man Ro. Trol: Traversal of layers for large language and vision models. arXiv preprint arXiv:2406.12246, 2024. 3, 5, 6, 7, 8 [47] Byung-Kwan Lee, Chae Won Kim, Beomchan Park, and Yong Man Ro. Meteor: Mamba-based traversal of rationale for large language and vision models. arXiv preprint arXiv:2405.15574, 2024. 2, 7 [48] Byung-Kwan Lee, Beomchan Park, Chae Won Kim, and Yong Man Ro. Collavo: Crayon large language and vision model. arXiv preprint arXiv:2402.11248, 2024. 2, 7 [49] Byung-Kwan Lee, Beomchan Park, Chae Won Kim, and intelligence Yong Man Ro. Moai: Mixture of all 10 for large language and vision models. arXiv:2403.07508, 2024. 2, 7 arXiv preprint [50] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023. 2 [51] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. 7, 14 [52] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 5, 7, [53] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. arXiv preprint arXiv:2403.00231, 2024. 6 [54] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 5, 14 [55] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024. 3 [56] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 2, 5, 6 [57] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947, 2024. 3, [58] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. arXiv preprint arXiv:2312.07533, 2023. 5, 7 [59] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models. arXiv preprint arXiv:2310.14566, 2023. 5, 14 [60] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2 [61] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 5, 7 [62] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multiarXiv preprint modal model an all-around player? arXiv:2307.06281, 2023. 5, 7, 14 [63] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905, 2024. 3 [64] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 6, 20 [65] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. 2, 6, [66] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 5, 7, 14 [67] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 5, 6, 14 [68] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 6 [69] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. 2, 5, 6 [70] Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. Openelm: An efficient language model family with open-source training and inference framework. arXiv preprint arXiv:2404.14619, 2024. 3 [71] Matthias Minderer, Alexey A. Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2 [72] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact language models via pruning and knowledge distillation. arXiv preprint arXiv:2407.14679, 2024. 3, 5, 8 [73] OpenAI. Gpt-4v(ision) system card, 2023. https:// openai.com/research/gpt-4v-system-card, Last accessed on 2024-02-13. 1, [74] OpenAI. Gpt-4v(ision) technical work and authors, 2023. https : / / openai . com / contributions / gpt - 4v, Last accessed on 2024-02-13. 1 [75] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. 11 Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2 [76] Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia Zhao, Mingzhen Sun, Qi Wu, and Jing Liu. Vl-mamba: Exploring state space models for multimodal learning. arXiv preprint arXiv:2403.13600, 2024. 3 [77] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. 2, 5, [78] Fangxun Shu, Yue Liao, Le Zhuo, Chenning Xu, Guanghao Zhang, Haonan Shi, Long Chen, Tao Zhong, Wanggui He, Siming Fu, et al. Llava-mod: Making llava tiny via moe knowledge distillation. arXiv preprint arXiv:2408.15881, 2024. 3 [79] Nimit Sohoni, Christopher Aberger, Megan Leszczynski, Jian Zhang, and Christopher Re. Low-memory neural network training: technical report. arXiv preprint arXiv:1904.10631, 2019. 6, 20 [80] Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Llm pruning and distillation in practice: The minitron approach. arXiv preprint arXiv:2408.11796, 2024. 3, 5, 8 [81] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568: 127063, 2024. 6 [82] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 2 [83] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, and Fahad Shahbaz Khan. Mobillama: Towards accurate and lightweight fully transparent gpt, 2024. [84] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 2, 5, 7, 15 [85] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3, 6 [86] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024. 2, 7 [87] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. 2, 3, 5, 6, 7, 20 [88] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023. 5, [89] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. arXiv preprint arXiv:2403.11703, 2024. 2 [90] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 1, 6, 20 [91] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, and Ziwei Liu. Panoptic scene graph generation. In European Conference on Computer Vision, pages 178196. Springer, 2022. 2 [92] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 5 [93] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 2 [94] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 1, 2, 5, 7, 14, [95] Weihao Yu, Zhengyuan Yang, Linfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities. arXiv preprint arXiv:2408.00765, 2024. 7, 15 [96] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multidiscipline multimodal understanding and reasoning bencharXiv preprint arXiv:2311.16502, mark for expert agi. 2023. 2, 5, 7, 15, 20 [97] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. 7 [98] Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. 2024. 7 [99] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 1, 12 [100] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. 15 [101] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlmxcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 2 [102] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024. 3 [103] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llavahd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024. 2, 6 [104] Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, and Donglin Wang. Cobra: Extending mamba to multi-modal large language model for efficient inference. arXiv preprint arXiv:2403.14520, 2024. [105] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024. 3 [106] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2 [107] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046, 2024. 2 13 A. Description of Numeorus Evaluation Benchmarks QBench [88] is comprehensive benchmark designed to evaluate the low-level visual abilities of multimodal large language models (MLLMs), focusing on perception, description, and quality assessment of visual attributes. It introduces datasets like LLVisionQA for diverse low-level attribute queries, LLDescribe for detailed expert-crafted image descriptions, and unified softmax-based strategy for quantifiable image quality assessment. Q-Bench highlights that while MLLMs exhibit preliminary capabilities in handling low-level visual tasks, their outputs remain inconsistent and imprecise, emphasizing the need for further advancements to align with human perception and achieve general-purpose applications. AI2D [41] is benchmark dataset developed to study diagram interpretation and reasoning, focusing on identifying diagram structures and semantic relationships. It introduces Diagram Parse Graphs (DPG), graph-based representation that encodes the syntactic and semantic structure of diagrams. The dataset contains over 5,000 grade-school science diagrams with exhaustive annotations of constituents and relationships, as well as 15,000 multiple-choice questions for diagrambased reasoning tasks. ChartQA [67] is large-scale benchmark designed to assess question-answering systems ability to reason logically and visually about data visualizations like bar, line, and pie charts. It includes 9,608 human-authored questions and 23,111 machine-generated questions, focusing on complex reasoning tasks involving mathematical operations, visual attributes, and multi-step logical inferences. By utilizing both extracted data tables and visual features, the benchmark highlights challenges in handling real-world charts and emphasizes the gap in models ability to process intricate visual and logical questions compared to human understanding. SEED-Bench-2-Plus [51] is comprehensive benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to comprehend and reason about text-rich visual content across three categories: Charts, Maps, and Webs, covering 63 diverse data types. It includes 2.3K meticulously crafted multiple-choice questions with humanverified answers, simulating real-world scenarios that combine complex text and visual data. POPE [54] is polling-based evaluation framework designed to assess object hallucination in Large Vision-Language Models (LVLMs). It formulates hallucination detection as binary classification task using simple yes/no questions (e.g., Is there chair in the image?) to probe LVLMs. Unlike previous methods, POPE offers stable and flexible approach by avoiding dependence on lengthy captions or complex parsing rules. It introduces three object sampling strategiesRandom, Popular, and Adversarialto explore hallucination patterns in models. HallusionBench (HallB) [59] is an advanced diagnostic benchmark designed to evaluate and analyze the failure modes of Large Vision-Language Models (LVLMs) in handling both language hallucinations and visual illusions. Featuring 346 images and 1,129 human-crafted visual-question pairs, it tests models like GPT-4V and LLaVA-1.5 using unique control pairs and human-edited images to assess logical consistency, perception, and reasoning. Results highlight persistent challenges, with top models achieving only 31.42% accuracy, revealing their over-reliance on parametric memory, susceptibility to simple manipulations, and struggles with geometry, math, and temporal reasoning. MME [28] is comprehensive benchmark designed to evaluate Multimodal Large Language Models (MLLMs) across perception and cognition abilities with 14 subtasks. The benchmark includes tasks like object recognition, OCR, commonsense reasoning, and numerical calculation, using manually curated instruction-answer pairs to ensure fairness and avoid data leakage. MME emphasizes concise instructions for consistency and quantitative assessment, highlighting that current MLLMs, despite their progress, face challenges such as instruction-following errors, limited perception and reasoning, and hallucinations. MathVista [66] is benchmark designed to evaluate the mathematical reasoning abilities of foundation models in visual contexts. It comprises 6,141 examples sourced from 31 datasets, including three newly created datasetsIQTest, FunctionQA, and PaperQAtailored to assess logical, algebraic, and scientific reasoning in visual settings. MathVista emphasizes diverse visual contexts, such as diagrams, charts, and academic figures, and covers seven types of reasoning across five core tasks. MMB, MMB-Chinese (MMBCN) [62] is multilingual benchmark designed to evaluate the multimodal capabilities of vision-language models (VLMs) across 20 fine-grained abilities, including perception, reasoning, and relation understanding. It features over 3,000 high-quality multiple-choice questions in English and Chinese, enabling comparative analysis in bilingual context. MMBench introduces novel evaluation strategies like CircularEval, which enhances robustness by testing models across shuffled choices, and employs GPT-4 for accurate choice extraction. MM-Vet [94] is benchmark designed to evaluate the integrated vision-language capabilities of Large Multimodal Models It defines six core capabilitiesrecognition, OCR, knowledge, language generation, spatial awareness, and (LMMs). mathand examines their combinations across 16 emergent multimodal tasks, such as explaining memes, solving spatial math problems, and summarizing visual data. The benchmark introduces an LLM-based evaluator to assess open-ended 14 model outputs consistently, focusing on both accuracy and quality. MM-Vet-v2 [95] builds upon the original MM-Vet benchmark by introducing new core capability, image-text sequence understanding, to evaluate large multimodal models (LMMs) on processing arbitrarily interleaved sequences of images and text. With an expanded dataset of 517 high-quality evaluation samples and tasks requiring combinations of seven core capabilities, it assesses advanced real-world scenarios like temporal reasoning, spatial understanding, and multimodal comparisons. MMMU [96] is benchmark designed to evaluate large multimodal models on 11550 college-level problems requiring expert knowledge and reasoning across six disciplines: Art, Business, Science, Medicine, Humanities, and Engineering. Spanning 30 subjects and incorporating 30 diverse image types like charts, medical scans, and diagrams, it challenges models to integrate complex text and image inputs while applying domain-specific knowledge. MMMU sets high standard for advancing multimodal AI and plays crucial role in developing Expert AGI. MMStar [9] is vision-critical multimodal benchmark consisting of 1,500 meticulously curated samples designed to evaluate large vision-language models (LVLMs) across six core capabilities and 18 specific axes. By addressing two key issues in existing benchmarksunnecessary reliance on textual knowledge and unintentional data leakageMMStar ensures each sample requires genuine visual reasoning and minimal data recall. Incorporating metrics for multimodal gain and data leakage, it provides robust platform for assessing the true multimodal capacities of LVLMs. BLINK [29] is benchmark designed to evaluate the core visual perception abilities of multimodal large language models (MLLMs) across 14 tasks, such as depth estimation, visual correspondence, and spatial reasoning, inspired by classical computer vision problems. With 3,807 multiple-choice questions derived from 7,300 images, BLINK focuses on tasks that humans can solve within blink but remain challenging for models, as even advanced models like GPT-4V achieve only 51.26% accuracy compared to 95.7% for humans. It highlights the gap in nuanced visual perception and suggests integrating specialized vision models as pathway for improving MLLMs performance. CV-Bench [84] is vision-centric benchmark introduced to evaluate the fundamental 2D and 3D visual understanding capabilities of Multimodal Large Language Models (MLLMs). With 2,638 manually inspected examples sourced from datasets like ADE20K, COCO, and Omni3D, it tests tasks such as spatial relationships, object counting, depth ordering, and relative distances. By transforming traditional vision benchmarks into VQA format, CV-Bench ensures robust assessment of models abilities in multimodal contexts. It addresses gaps in existing benchmarks by offering significantly more samples, better diversity, and stronger focus on visual grounding, making it critical tool for advancing multimodal AI systems. LLaVA-Wilder [100] is dataset designed to evaluate large multimodal models (LMMs) in real-world scenarios. It comprises 128 image-text pairs, each featuring an image accompanied by question and detailed answer. The dataset includes variety of images, such as indoor and outdoor scenes, memes, paintings, and sketches, to assess models generalization capabilities across diverse domains. By providing this resource, LLaVA-Bench-Wilder aims to facilitate the development and benchmarking of LMMs, ensuring their robustness and effectiveness in handling complex, real-world visual tasks. B. VLsIs Verbalization Examples 16 17 18 19 C. Comprehensive Experimental Setup for Ablation Studies Training and Inference Configuration of LLaVA-OV [52]. Different from Qwen2-VL [87]-based VLsI, we select VLsI to is: 4th, 12th, 20th layers, and it: 10th, 30th, 50th, and 70th layers. intermediate target layers of LLaVA-OV-based Although LLaVA-OVs language model is Qwen2 [90] that is equal with that of Qwen2-VL, the required number of image tokens in LLaVA-OV are 4 to 10 times more number than that of Qwen2-VL, depending on the height-to-width ratio of the images, given the same pixel count. To accommodate this increased computational demand on 8 NVIDIA A100 80GB GPUs, we reduce the number of intermediate target layers. For efficient training, we equally employ LoRA [37] with rank and alpha parameters to 64, use the AdamW optimizer [64] with cosine annealing schedule, adjusting the learning rate from 1e4 to 1e6 over each training step, and use gradient accumulation with 16 steps and gradient checkpointing [79]. The only different training configuration is batch sizes where two batches each are used for the 0.5B and 7B model sizes, and one batch is used for the 72B model sizes. It finally results in 256 (8162) and 128 (8161) batches, respectively. We conduct inference experiments under the equal setup used in Qwen2-VL, where we use greedy search for text generation."
        },
        {
            "title": "VLMs",
            "content": "MM-Vet MMMU LLaVA-OV-7B Qwen2-VL-7B LLaVA-OV-72B Qwen2-VL-72B 57.5 62.0 63.7 74.0 48.8 54. 56.8 64.5 Fig. 4 provides the challenging evaluation benchmarks performances: MM-Vet [94] and MMMU [96]. Each cell represents the performances on their evaluation benchmarks, where the orange colored-values represent LLaVA-OV-based VLsIs result and the purple ones represent Qwen2-VL-based result. This figure reveals consistent trends that using largeand smallVLsIs performances across all configurations. Besides, we can backbone VLMs with more bigger model sizes enhances VLsI are also higher than those of LLaVA-OV easily infer that the baseline performances of Qwen2-VL before buliding as shown in the above table. Furthermore, Tab. 4(a) shows VLsIs generalization ability to 0.5B and 7B model sizes in LLaVA-OV, but similarly insists that using better large-backbone VLMs provides benefits from performances. collectively illustrate the impact of incorporating the reinforcement step (RS). Table (b) highlights Tab. 4(b) and Tab. 4(c) the significant performance gains achieved by applying RS, while Table (c) examines whether these improvements result from the fine-tuning effects of 2.9 million visual instruction tuning samples. The results demonstrate that VLsIs performance becomes markedly superior to Qwen2-VL as RS training progresses from 50% to 100%, confirming that RS plays much more critical role in driving performance enhancements than the contribution of the visual instruction tuning samples alone. IL-Ops LL-Ops vL-Head MM-Vet MMMU L2 L2 L"
        },
        {
            "title": "KLD",
            "content": "KLD L2 L2 KLD"
        },
        {
            "title": "KLD",
            "content": "- - 63.3 64.6 63.9 65.2 66.5 75.8 53.5 55.9 54. 56.8 57.5 69.3 evaluates the impact of different operations applied to intermediate (IL-Ops) and last layers (LL-Ops) on Tab. 4(d) VLsIs performances. The results clearly demonstrate that KL divergence (KLD) operations applied to both intermediate and last layers yield the best performances, confirming the effectiveness of KLD over cross-entropy (CE) or L2 for intermediate layer alignment and final layer interaction. To further assess the effectiveness of verbalization in transferring the knowledge via distillation, we conduct an ablation study by removing the language head in verbalizer (vL-head). In this setup, the verbalization step is skipped, and the interaction step is first performed by aligning the hidden dimensions of the verb-FFNs in largeand small-backbone VLM. Here, the verb-FFNs hidden dimension in large-backbone VLM is kept but the hidden dimension in small-backbone VLMs verb-FFN is enlarged to match the large one. In addition, the verb-FFNs of large20 and small-backbone VLM are interacted with only L2, which means that small-backbone VLMs verb-FFNs try to naively follow those of large-backbone VLM. After interaction step, reinforcement step is equally conducted. However, as shown in the above table, this naive approach results in significantly lower performance compared to the version equipped with the language head. Interestingly, using only last layer distillation by L2 and KLD provides more benefits than using intermediate distillation without vL-Head. These findings suggest that directly imitating outputs from the large-backbone VLM, without verbalization from vL-Head, introduces instability and can lead to suboptimal results, highlighting the critical role of the language head in achieving effective distillation. Pseudo-Code for Random Index (Search Range ) 1: Input: ts, tl 2: Initialize: loss: 0, 3: for is in 0 is < ts do 4: 5: 6: 7: kld-list = [] for il in 0 il < tl do kld-list.append(compute-kld(is, il)) : 0, ϵ: 1e-6, scale: 2 end for Random-Select(kld-list) loss loss +r 8: 9: 10: end for 11: Return: loss : 0, ϵ: 1e-6, scale: 2 Pseudo-Code for Bottom-k Index (Search Range ) 1: Input: ts, tl 2: Initialize: loss: 0, 3: for is in 0 is < ts do 4: 5: 6: 7: 8: 9: kld-list = [] for il in 0 il < tl do kld-list.append(compute-kld(is, il)) end for bottom-k-kld-list Bottom-k(kld-list) {length(bottom-k-kld-list): k} Average(bottom-k-kld-list) loss loss +a 10: 11: end for 12: Return: loss Tab. 4(e) highlights the effectiveness of various components in the matching strategy. Random Index and Bottom-1/3 Index, yield lower scores, underscoring the limitations of simpler selection mechanisms. Note that, the above algorithms represent the their detailed experimental setup. Multinomial sampling provides improvements and incorporating the Search Range further enhances performances (e.g., 83.5 on MMB and 69.8 on MM-Vet). Adding order preservation results in significant leap, particularly on BLINK (59.2) and MM-Vet (75.2), demonstrating the importance of maintaining matched indices sequence alignment during interaction step. Finally, using all together with adaptive temperature achieves the best results across all benchmarks (e.g., 86.3 on MMB and 69.3 on MMMU), showcasing its ability to dynamically control the distribution. These exploration underscore the necessity of advanced sampling strategies and adaptive mechanisms for maximizing the efficiency of transferring the knowledge. Tab. 4(f) highlights verb-FFNs efficiency among different verbalizer architectures in terms of performance and parameter count. While larger architectures such as Decoder2 (3.3B) and FFN2 (2.9B) achieve strong performance across MM-Vet and MMMU, the much smaller verb-FFN architecture (269M) delivers comparable results. In contrast, simpler structures like MLP2 and MLP, while efficient in terms of parameter count, fall short in performance. These results emphasize the effectiveness of the verb-FFN architecture in lightweight yet high-performing solution. Its efficiency becomes even more pronounced as the number of intermediate target layers increases and larger-backbone VLMs are utilized. 21 D. VLsIs Text Generation Quality 22 24 25 26 27 28 30 31"
        }
    ],
    "affiliations": [
        "KAIST",
        "NVIDIA"
    ]
}