{
    "paper_title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for Automated Prompt Optimization",
    "authors": [
        "Jian Zhang",
        "Zhangqi Wang",
        "Haiping Zhu",
        "Jun Liu",
        "Qika Lin",
        "Erik Cambria"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The basic question-answering format of large language models involves inputting a prompt and receiving a response, and the quality of the prompt directly impacts the effectiveness of the response. Automated Prompt Optimization (APO) aims to break free from the cognitive biases of manually designed prompts and explores a broader design space for prompts. However, existing APO methods suffer from limited flexibility of fixed templates and inefficient search in prompt spaces as key issues. To this end, we propose a Multi-Agent framework Incorporating Socratic guidance (MARS), which utilizes multi-agent fusion technology for automatic planning, with gradual continuous optimization and evaluation. Specifically, MARS comprises seven agents, each with distinct functionalities, which autonomously use the Planner to devise an optimization path that ensures flexibility. Additionally, it employs a Teacher-Critic-Student Socratic dialogue pattern to iteratively optimize the prompts while conducting effective search. We conduct extensive experiments on various datasets to validate the effectiveness of our method, and perform additional analytical experiments to assess the model's advancement as well as the interpretability."
        },
        {
            "title": "Start",
            "content": "MARS: Multi-Agent Framework Incorporating Socratic Guidance for Automated Prompt Optimization Jian Zhang1,3, Zhangqi Wang1, Haiping Zhu1, Jun Liu1, Qika Lin2*, Erik Cambria3 1Xian Jiaotong University 2National University of Singapore 3Nanyang Technological University zhangjian062422@stu.xjtu.edu.cn, qikalin@foxmail.com, cambria@ntu.edu.sg 5 2 0 2 1 2 ] A . [ 1 4 7 8 6 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The basic question-answering format of large language models involves inputting prompt and receiving response, and the quality of the prompt directly impacts the effectiveness of the response. Automated Prompt Optimization (APO) aims to break free from the cognitive biases of manually designed prompts and explores broader design space for prompts. However, existing APO methods suffer from limited flexibility of fixed templates and inefficient search in prompt spaces as key issues. To this end, we propose Multi-Agent framework IncorpoRating Socratic guidance (MARS)1, which utilizes multi-agent fusion technology for automatic planning, with gradual continuous optimization and evaluation. Specifically, MARS comprises seven agents, each with distinct functionalities, which autonomously use the Planner to devise an optimization path that ensures flexibility. Additionally, it employs Teacher-Critic-Student Socratic dialogue pattern to iteratively optimize the prompts while conducting effective search. We conduct extensive experiments on various datasets to validate the effectiveness of our method, and perform additional analytical experiments to assess the models advancement as well as the interpretability."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) such as GPT4 (Achiam et al., 2023) and Deepseek-R1 (Guo et al., 2025) provide robust support for thousands of natural language processing tasks. By providing natural language prompt that includes instructions and task description, LLMs can quickly adapt and respond (Lin et al., 2025). Consequently, the quality of the prompt is of critical importance, leading to wide interest in Automated Prompt Optimization *Corresponding author 1The code and experiment datasets are available at https: //github.com/exoskeletonzj/MARS 1 Figure 1: Three different prompts along with their corresponding responses for the word sorting task. (APO) (Pryzant et al., 2023). As shown in Figure 1, we provide LLMs with three different inputs for the word sorting task: zero-shot prompt, Chain of Thought (CoT) prompt, and our optimized prompt. The responses are produced in markedly distinct way. Specifically, the zero-shot prompt incorrectly identifies the alterate as the more common word alternate. However, the task requires faithfully preserving the given sequence of words rather than correcting them. With the CoT prompt, the sorting remains incorrect because the LLM does not fully grasp the sorting task and the word sequence. In contrast, our optimized prompt produces the correct answer. This is because our prompt includes specific requirements, such as maintaining the original letter casing and specifying the sorting method. Thus, it is evident that APO produces better results when accomplishing the task. Some recent studies (Zhou et al., 2022; Xu et al., 2023; Wang et al., 2023) optimize prompts by generating multiple candidates and employing various search strategies, while others (Yang et al., 2024a; Ye et al., 2023) focus on crafting sophisticated meta prompts for optimization. However, two major issues persist in this task: limited flexibility of fixed templates and inefficient search in prompt spaces. The first issue is the limited flexibility of fixed templates. In previous research (Yang et al., 2024a; Ye et al., 2023), meta prompts are fixed optimization templates with limited flexibility, typically relying on predefined templates. Unlike tasks such as event extraction (Zhang et al., 2024b) and text2symbol (Xu et al., 2024, 2025), where fixed templates can be used to complete tasks, these templates could not be dynamically adjusted to meet the varying needs of different tasks, thus limiting the effectiveness of the method. This leads to poor performance when handling diverse tasks or complex scenarios, as the fixed templates may introduce biases or fail to optimize effectively, especially when tasked with addressing the varied requirements of different tasks. The second issue is inefficient search in prompt spaces. Some methods (Zhou et al., 2022; Xu et al., 2023; Wang et al., 2023) using the generationsearch strategy generate cluster of prompts within the prompt space and then optimize them using search strategies within that cluster. However, this is local optimization approach, focusing only on optimizing within the pre-generated set of prompts. As result, the search in the entire prompt space may be incomplete, leading to ineffective search results and limited overall optimization of prompts. To this end, we propose Multi-Agent framework integRating Socratic guidance (MARS). Specifically, we construct multi-agent architecture that includes Planner agent and six taskspecific agents for APO. First, to tackle the first issue, we employ the Planner agent to autonomously plan optimization paths for different tasks, ensuring that each task follows its own optimization path. Next, due to the advantages of Socratic questioning (Elder and Paul, 1998) in promoting critical thinking and stimulating autonomous learning, we employ Teacher-Critic-Student Socratic guidance dialogue pattern to iteratively optimize the prompts while ensuring interpretability. This module enables MARS to search across the entire prompt space and, through feedback, continuously narrow down the prompt space, effectively searching for the optimal prompt. Finally, we validate the optimized prompts in the Target agent, iterating until the optimal prompt is found. We conduct extensive experiments across multiple general tasks and domain-specific datasets to validate the effectiveness of our designed model and explore the interpretability of the optimization process and results. In summary, the contributions of this paper are in following three folds: We propose multi-agent architecture for APO that autonomously plans optimization steps, effectively addressing the issue of limited flexibility of fixed templates. To the best of our knowledge, this is the first attempt in the field. We introduce Teacher-Critic-Student Socratic guidance dialogue pattern for the APO task, wherein the Teacher employs Socratic questioning, the Critic evaluates the quality of those questions, and the Student undergoes optimization to achieve the optimal prompt. Extensive experiments on both general tasks and domain-specific task datasets demonstrate the effectiveness and superiority of the MARS framework. Additional analytical experiments further validate the interpretability of the optimization process and results."
        },
        {
            "title": "2 Methodology",
            "content": "In this section, we provide detailed introduction to multi-agent framework incorporating Socratic guidance. We base our design on LLMs as intelligent agents capable of performing various functions. The MARS framework autonomously plans the optimization path and employs Socratic guidance dialogue pattern to flexibly plan the optimization path and address the issue of inefficient search in the prompt space. The model architecture is shown in Figure 2. In Section 2.1, we introduce the task definition. In Section 2.2, we present the multi-agent framework for APO. In Section 2.3, we discuss the Teacher-Critic-Student Socratic guidance dialogue pattern. And in Section 2.4, we cover effect validation and iterative optimization."
        },
        {
            "title": "2.1 Task Definition",
            "content": "Following the definition in prompt optimization (Zhou et al., 2022), when using the given target agent Mtar as the task model, the goal of the APO task is to start from the original text prompt p0, progressively optimize it, and ultimately obtain the text prompt that achieves the best performance on the given dataset = {(x, y)}. We will select few examples from Dtrain to optimize the prompts and finally use Dtest for evaluation. The formal definition of this process is as follows: = arg max (cid:88) (x,y)Dtest (Mtar(x; p), y), (1) where Mtar(x; p) is the response obtained when the input and the prompt are fed together into the target agent, and is the function to measure the models performance (e.g., accuracy). 2 Figure 2: The overall architecture of the MARS model. It consists of seven LLM agents. The Manager oversees the entire process, responsible for communication between agents and the allocation of speakers. The UserProxy receives input and the original prompt. The Planner formulates the APO plan based on the received input and task description. In the Socratic guidance dialogue pattern, the Teacher-Critic-Student system iteratively refines the prompts according to the Planners plan, with the final evaluation and recording done by the Target."
        },
        {
            "title": "2.2 Multi-Agent Framework",
            "content": "Figure 2 illustrates our multi-agent MARS architecture incorporated with Socratic guidance. This architecture includes essential modules as well as several innovative modules for multi-agent systems. This section first introduces three agents: Manager, UserProxy, and Planner, with the other agents to be introduced in Sections 2.3 and 2.4. Manager. The group chat Manager acts as the administrator, overseeing the entire process and allocating speaking rights to ensure effective coordination among the agents. UserProxy. The UserProxy agent serves as the input receiver, responsible for accepting and processing external inputs, and providing informational support to other agents, thus facilitating the smooth operation of the entire system. Planner. The Planner agent is responsible for planning the task based on the input, breaking it down into several optimization sub-steps. In our architecture, the entire planning process is handled by the Planner agent, which fully leverages its extensive knowledge and reasoning capabilities. This allows the Planner agent to provide clear guidance for the subsequent optimization process, ensuring that each sub-step effectively advances the task. The optimization will then proceed step-by-step according to the path outlined by the Planner, optimizing the final output while ensuring task completion. The planned steps st is shown as following: ST[st1, st2, . . . , stn] = Mplan xDtrain (x; p0), (2) where sti is the plan for the current i-th step, Mplan is Planner agent, and ST[...] is the optimization steps planned for the current task."
        },
        {
            "title": "2.3 Socratic guidance Dialogue",
            "content": "Previous APO methods (Zhou et al., 2022; Xu et al., 2023; Yang et al., 2024a; Ye et al., 2023) passively rely on LLMs to optimize prompts, either by generating multiple candidates and searching or by refining prompts using meta prompts. However, these approaches do not provide insight into why particular optimization is effective or why the optimized prompt performs better. Constructivist learning theories (Hein, 1991) emphasize two fundamental principles of Socratic teaching (Elder and Paul, 1998; Park et al., 2024; Liu et al., 2024a). First, it is inherently dialogic, with the learning process facilitated through interactions and conversations between teachers and students. Second, it employs probing questions to actively engage students, fostering independent thinking and encouraging them to find answers on their own. Inspired by Socratic teaching, we introduce an active optimization approach: the Teacher 3 Algorithm 1 MARS Procedure 1: Input: Task Dataset D, Original Prompt p0 2: Output: Optimized prompt 3: Manager: Initialize the APO task 4: UserProxy: Receive the input Dtrain, prompt p0 5: Planner: Plan optimization Steps ST = {sti} 6: for iteration = 1 to do 7: 8: for each step st1, st2, ..., stn ST do Teacher: generate socratic question q[sti] of sti & review the students answer pi1 Critic: assess the Socratic style of Teachers Question q[sti] if TRUE then Continue else Teacher: adjust the question q[sti] end if Student: optimize the prompt pi end for Target: Evaluate (x, y) Dtest with Mtarget(x; pn) 10: 11: 12: 13: 14: 15: 16: end for 17: return Best prompt 9: that the Teacher agents questions do not adhere to the Socratic style, it provides feedback for revision. This judge-revise cycle continues until the questions meet the required standard, at which point they are passed on to the Student agent. Student. The Student agent generates responses based on the questions posed by the Teacher, gradually working through each step of the optimization process. By actively engaging in the dialogue, the Student agent deepens its understanding of the key elements of each sub-step. Throughout this process, the Student agent adjusts its thought process in response to the Teachers guidance and the Critics feedback, ultimately arriving at the final prompt for this optimization cycle. The Students independent thinking and active participation are crucial in ensuring that the resulting prompt is both logically sound and effective.The prompt pi for the ith step is as follows: pi = Mstudent(q[sti]; pi1). (4) Specific examples and analysis of the Socratic guidance dialogue are provided in Appendix F. Figure 3: specific illustration of Teacher-CriticStudent Socratic guidance dialogue pattern. The case shows the fifth step optimization iteration. agent continuously proposes open-ended questions based on the planned optimization steps, the Critic agent evaluates the quality of these questions, and the Student agent engages in the dialogue, actively participating in the APO process. This approach encourages the Student to express ideas, challenge assumptions, think independently, and ultimately produce the optimal prompt. Algorithm 1 presents the detailed design of the Teacher-Critic-Student Socratic guidance dialogue algorithm, clearly illustrating its core logic and implementation steps. As shown in Figure 3, the Teacher-CriticStudent Socratic guidance dialogue pattern is used to iteratively optimize the prompt, involving three key agents: Teacher. Based on the sub-steps planned by the Planner, the Teacher agent uses Socratic style to guide the Student in solving the problem of each sub-step. There are three main requirements for the Teacher agent: First, it should not directly provide the answer, but instead guide the Student to think through the solution process using probing questions. Second, it should explain the content of each sub-step and the knowledge involved. Third, the Teacher agent must maintain consistent teaching style throughout to ensure continuity in the instruction. The question q[sti] for the i-th step is as follows: q[sti] = Mteacher stiST (sti; pi1), (3) 2.4 Effect Validation where pi1 is the prompt optimized in the (i 1)- th round, and Mteacher is the Teacher agent. Critic. As key component, the Teacher agent plays crucial role in determining the effectiveness of the students learning through the quality of its questions. Therefore, we introduce the Critic agent as supervisor. If the Critic agent judges Target. The prompt optimized through the previous steps will be evaluated on the test dataset Dtest of the Target agent. Based on the results of the first round of optimization, second round of optimization will be carried out, followed by another evaluation on Dtest. This iterative process continues until we reach the number of iterations we have set (e.g., 10 times). The evaluation pi for the i-th Models B.E D.QA F.F. G.S. R.N. S.U. C.B. C.M. E.E. W.H. H.A. M.T. Avg. Origin CoT(ZS) CoT(FS) APE ProTeGi OPRO PE2 74.70 80.32 81.93 83.53 83.93 86.34 87.95 51.41 54.22 57.43 61.85 63.86 66.67 65. 52.20 59.44 66.26 61.04 62.65 63.45 63.86 43.37 47.39 49.40 51.41 52.21 53.81 54.62 59.84 67.07 70.68 77.51 80.32 83.13 84.34 60.24 67.87 72.29 74.70 76.71 82.73 75.90 82.52 83.91 86.71 88.11 90.91 93.70 93.01 69.77 73.25 76.74 75.58 78.49 83.14 81. 63.89 74.31 79.17 69.44 73.61 77.01 76.39 73.73 76.27 78.81 82.20 84.75 86.44 88.14 66.22 68.47 72.07 75.68 77.48 79.73 81.08 81.55 84.98 90.99 87.98 90.56 92.70 93.56 64.95 69.79 73.54 74.09 76.29 79.07 78.81 Ours 93.17 71.89 74.70 59.44 90.36 87. 97.90 86.05 84.03 93.22 85.59 97. 85.11 Table 1: In the performance comparison across 12 general tasks, we carefully select 6 representative subtasks from both BBH and MMLU, two commonly used evaluation benchmarks, to comprehensively assess MARSs performance in diverse general-task settings. The evaluation results of these subtasks indicate that MARS surpasses all existing baseline methods. Models Chinese Math Law Avg. A.S. U.R.P. CL.M. GSM. L.A. Origin 56.25 CoT(ZS) 59.38 CoT(FS) 65.63 65.63 APE ProTeGi 68.75 OPRO 71.88 75.00 PE2 48.89 53.33 57.78 62.22 66.67 73.33 77.78 57.14 61.90 66.67 71.43 76.19 80.95 76.19 67.07 70.26 77.54 74.81 77.47 81.56 83.46 23.14 50.50 30.57 55.09 35.81 60.69 29.69 60.76 31.88 64.19 31.44 67.83 34.50 69.39 MARS 81.25 84.44 85.71 89.22 38.42 75.81 Table 2: Performance comparison on three types of domain-specific tasks: Chinese, law, and mathematics. The Chinese domain consists of three datasets, while the law and mathematics domains each have one dataset. step is as follows: ypred = Mtar (x,y)Dtest (x; pi). (5) Where ypred is the predicted result of pi under the target agent. The accuracy is calculated by comparing ypred with the gold label y."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we conduct extensive experiments on 12 general task datasets and 5 domain-specific datasets. The baseline methods and evaluation metrics are introduced in Section 3.1, while the main experimental results are presented in Sections 3.2. detailed analysis of efficiency is provided in Section 3.3. 3.1 Datasets and Baselines Tasks and Datasets We selected total of 17 datasets from both general and domain-specific tasks for extensive experimentation, which include: 6 BBH (Suzgun et al., 2022) general tasks, 6 MMLU (Wang et al., 2024b) general tasks, 3 CEval (Huang et al., 2024) Chinese domain tasks, 1 LSAT-AR (Zhong et al., 2023) law domain task, and 1 GSM8K (Zhang et al., 2024a) math domain task. More detailed descriptions of the tasks and datasets can be found in the Appendix A. The abbreviations of the tasks in Table 1 and Table 2, along with their full names and dataset descriptions, are provided in Table 4. The experiment settings and baselines are introduced in Appendix B."
        },
        {
            "title": "3.2 Main Results",
            "content": "The experimental results in Table 1 and Table 2 present comprehensive comparison between the prompts optimized by MARS and the baselines for the 12 tasks. As shown in Table 1, on general tasks, MARS outperforms the previous SOTA by 6.04%, and exceeds the original prompt and CoT(ZS) by 20.16% and 15.32%, respectively. This indicates that the prompts optimized by MARS enable LLMs to better understand the task requirements, providing stronger instructions for tasks across different scenarios. MARS surpasses existing APO methods, highlighting the limitations of both the generatesearch approach and the meta prompts approach. These methods fail to fully grasp the deeper essence of the APO process, which constrains their optimization effectiveness. In contrast, MARS thoughtfully considers the prompt optimization pathways for different tasks and incorporates heuristic optimization strategies, making the prompt refinement process more efficient and precise. Table 2 presents the experimental results of MARS on domain-specific tasks, covering areas like Chinese, law, and mathematics, all of which require specialized knowledge and reasoning. In these tasks, MARS outperforms the previous SOTA 5 value of MARS is more than twice that of the other baselines. Meanwhile, in tasks Ruin Name and Sports Understanding, MARS also outperforms other baselines by more than 20%, further demonstrating its superior capability in balancing resource consumption and performance. This further highlights the guiding capability and superiority of the MARS models approach, which first conducts task planning and then progressively decomposes and optimizes. By adopting this strategy, MARS can allocate resources more effectively, minimize unnecessary consumption, and ensure both efficiency and stability in the optimization process."
        },
        {
            "title": "4 Supplementary Analysis",
            "content": "To conduct more comprehensive experiments, this section will cover the following topics: ablation experiments in Section 4.1, convergence analysis in Section 4.2, and efficency analysis in Section 4.3. Meanwhile, to comprehensively demonstrate the generalization capability of the MARS framework, Appendix presents the experimental performance of MARS when using GPT-4o as the base model, as well as the applicability and optimization results of MARSs main experiments across various other LLMs. To explore the impact of sample size on the experimental results, the sample size analysis is shown in Section D. Additionally, Appendix provides detailed list of the specific prompts used by each intelligent agent, offering clearer understanding of their roles and operational logic. To illustrate the collaborative mechanism of the multiagent system of MARS, Appendix documents the complete interaction and optimization process for single APO task. Finally, Appendix contains the optimized prompts for the 17 APO tasks conducted in the main experiment of this study, supporting further replication and analysis. 4.1 Ablation Study Figure 4: The Efficiency Analysis chart presents comparison of the PE metric between MARS and other baseline methods across different tasks. For each task, different models are grouped together in single bar chart cluster, allowing for clear visual comparison of performance efficiency differences. methods to 6.42%, further demonstrating its ability to better guide LLMs in domain-specific knowledge discovery and application. This not only lowers the barrier to utilizing LLMs but also enhances their generalization capability. Moreover, compared to the original prompt and CoT(ZS), MARS achieves improvements of 25.31% and 20.72%, respectively, underscoring its effectiveness and practicality in these specialized domains."
        },
        {
            "title": "3.3 Efficiency Analysis",
            "content": "The balance between resource consumption and performance improvement is crucial analysis metric (Yang et al., 2024b). In the process of MARS during APO, the phased optimization of prompts gradually offers great advantage in balancing this trade-off, allowing for performance improvement while maintaining efficient resource consumption. To quantify this balance, we propose new metric PE (Prompt Efficiency). The formula is as follows: PE = Accuracy Consumption , (6) where accuracy refers to the performance of the APO tasks, while the consumption refers to the numbers of calling the LLMs API to optimize prompt. Clearly, the higher the PE metric, the better the model performs with less resource consumption, reflecting the efficiency of resource utilization. As shown in Figure 6, MARS is compared with strong baselines across multiple APO tasks, specifically evaluating its PE metric on four different tasks. The results indicate that in the tasks of Boolean Expression and Formal Fallacies, the PE Table 3 presents the impact of removing the Planner agent, the Teacher-Critic-Student Socratic guidance dialogue module, and the Critic agent on the overall performance. It can be observed that removing the Teacher-Critic-Student Socratic guidance dialogue module causes the greatest loss in performance, followed by the Planner module, while the removal of the Critic agent has the least impact on the results. Removing the Teacher-Critic-Student Socratic guidance dialogue module has the greatest impact on performance. Without this module, after the 6 Variation B.E. D.QA F.F. G.S. R.N. S.U. Avg. MARS 93.17 71.89 74. 59.44 90.36 87.95 79.59 w/oPlan w/oSoc w/oCri 54. 72.82 86.35 65.86 82.33 68.67 (-6.77) (-6.82) (-6.03) (-6.03) (-5.23) (-8.03) 84.74 63.86 68.28 74.30 62.25 (-8.43) (-8.03) (-12.45) (-9.64) (-16.06) (-13.25) (-11.31) 76.04 89.16 68.27 86.34 72.28 (-3.55) (-4.01) (-3.62) (-2.42) (-3.22) (-4.02) 79.52 (-8.43) 74.70 83.94 (-4.01) 49.80 56.22 Table 3: Performance under different ablation settings are analyzed. We performed ablation experiments on the planner module w/oPlan, the Teacher-Critic-Student module w/oSoc, and the Critic Agent w/oCri to evaluate the impact of removing these components. Planner formulates optimization steps, the iterative process to refine the prompt is absent. The system simply sends raw steps to the target agent, reducing optimization effectiveness. Additionally, removing the Planner agent causes noticeable drop in performance. The Planner structures the optimization steps in the APO task, and without it, the Socratic guidance dialogue module lacks optimization guidance, negatively affecting overall performance. Lastly, removing the Critic agent affects the Socratic guidance Dialogue module, particularly in providing feedback to the Teacher. As shown in Table 3, this results in 3.55% performance loss, highlighting its smaller but still meaningful role in the optimization process."
        },
        {
            "title": "4.2 Converagence Analysis",
            "content": "Figure 5 presents the convergence analysis across four BBH tasks. To better monitor the APO process, we visualized its iterative optimization trajectory, selecting 10 iterations as the observation window. The results show that MARS exhibits an upward training trajectory in the early iterations. For instance, in Task A, MARS converges to the optimal solution by iteration 5. In contrast, in the OPRO task, even after 10 iterations, convergence is not achieved, leading to higher resource consumption. This highlights MARSs efficiency in reaching the optimal solution within fewer iterations, thereby reducing resource overhead and improving optimization efficiency. 4.3 Case Study and Interpretability This section focuses on the interpretability analysis of Figure 3 and Figure 6. As mentioned earlier, the MARS model provides both process interpretability and result interpretability."
        },
        {
            "title": "Process interpretability is mainly reflected in two",
            "content": "7 Figure 5: The convergence curves across different tasks show the learning progress as the number of iterations increases. We compare the iterative convergence process of MARS with four different baseline methods across four tasks to assess MARSs advantage in convergence speed. aspects: First, the planning steps are fully textbased, explicitly outlining the optimization path guidance, such as the planned steps in Figure 6. Second, the interpretability of the Socratic iterative process. Taking the Teacher-Critic-Student iterative process in Figure 3 as an example, the Teacher asks Socratic-style questions, the Critic follows the assess-adjust process, and finally, the Student iterates step by step. The entire process is completely transparent and has practical significance, reflecting the iterative process of each substep. Result interpretability is evident in the results shown in Figure 6, such as the requirements for fields like dynamic tolerance threshold, validation and refinement, etc. These are specific requirements for the task, allowing for clear understanding and guiding the agents to achieve optimal performance."
        },
        {
            "title": "5 Related Works",
            "content": "The related work is structured into two main aspects: first, an introduction to prompt optimization; and second, an exploration of multi-agent techniques. Prompt Optimization. Early work primarily focused on two aspects: discrete optimization of hard prompts (Shin et al., 2020; Wen et al., 2024; Chen et al., 2023; Zhang et al., 2022) and continuous vector optimization of soft prompts (Lester et al., 2021; Li and Liang, 2021; Liu et al., 2024b). However, these methods are highly task-dependent and exFigure 6: Case study of geometry shape task, showing the planning step and new prompt after optimized iterations. hibit locality. With the advent of LLMs, traditional methods have become outdated. APE (Zhou et al., 2022) pioneered the use of generative methods to optimize instructions. Since APE, there have been two major approaches. The first approach (Zhou et al., 2022; Xu et al., 2023; Pryzant et al., 2023; Wang et al., 2023) is the generate-search model, where multiple candidate sequences are generated, and methods like Monte Carlo search are used to optimize the prompt. The second approach (Yang et al., 2024a; Ye et al., 2023) is the meta prompts method, where sophisticated meta prompts are designed to optimize the prompt. In contrast to these two approaches, MARS employs planned optimization path, iteratively generating high-quality prompts. This approach alleviates the inefficient search in prompt spaces issues in the first approach and addresses the challenges of limited flexibility of fixed templates in the second approach. Multi-Agent. Based on LLMs, combination of AI agents capable of performing specific functions forms multi-agent system (Richards, 2023; Yang et al., 2023; Wu et al., 2023). Given statement of specific task, AI agents can attempt to break complex problem statements into subtasks and use tools, including data retrieval from the internet, to solve them step-by-step through automatic iterations. Some studies (Poldrack et al., 2023; Wang et al., 2024a; Xi et al., 2025; Ni and Gao, 2021) use multi-agent systems to address issues such as problem identification, code development and debugging, plotting results and analysis, and providing interactive feedback with the human user. Ni and Buehler (2024) demonstrates the potential of organizing an AI multi-agent collaborative team to automatically solve mechanical problems, showcasing an enhanced ability to understand, formulate, and validate engineering problem solutions through self-correction and mutual correction. Inspired by their work, we leverage multi-agent technology to autonomously plan the APO optimization path and design Teacher-Critic-Student collaborative approach for iterative optimization."
        },
        {
            "title": "6 Conclusion",
            "content": "This study introduces MARS method. Specifically, we develop multi-agent framework incorporating Socratic guidance that includes Planner agent and six task-specific agents for APO. First, to tackle the issue of limited flexibility of fixed templates, we utilize the Planner agent to autonomously design optimization paths for various tasks, ensuring that each task adheres to its own specific optimization trajectory. Next, we employ Teacher-Critic-Student Socratic guidance dialogue pattern to iteratively refine the prompts while address the issue of inefficient search in prompt spaces. Finally, we validate the optimized prompts within the Target agent and iterate until the optimal prompt is identified. We conduct extensive experiments across range of general tasks and domain-specific datasets to assess the effectiveness of MARS and explore the interpretability of the optimization process and results."
        },
        {
            "title": "Limitations",
            "content": "MARS represents the first attempt to apply multi-agent architecture to APO tasks, achieving improvements. Notably, the planner LLMs autonomous optimization path planning and the Teacher-Critic-Student Socratic-style iterative prompt optimization have shown good results. However, we acknowledge two limitations: (1) the question of whether there exists more universal representation of prompts across different task types, highlighting the need for broader prompt design patterns that can accommodate variety of tasks; and (2) the consideration of incorporating environmental feedback into the APO process to enhance the systems interactivity and error correction capabilities. We aim to address these issues in the future."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Instructzero: EfHuang, and Tianyi Zhou. 2023. ficient instruction optimization for black-box large language models. arXiv preprint arXiv:2306.03082. Linda Elder and Richard Paul. 1998. The role of socratic questioning in thinking, teaching, and learning. The Clearing House, 71(5):297301. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. George Hein. 1991. Constructivist learning theory. Institute for Inquiry. Available at:/http://www. exploratorium. edu/ifi/resources/constructivistlearning. htmlS. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190. Qika Lin, Tianzhe Zhao, Kai He, Zhen Peng, Fangzhi Xu, Ling Huang, Jingying Ma, and Mengling Feng. 2025. Self-supervised quantized representation for seamlessly integrating knowledge graphs with large language models. arXiv preprint arXiv:2501.18119. Jiayu Liu, Zhenya Huang, Tong Xiao, Jing Sha, Jinze Wu, Qi Liu, Shijin Wang, and Enhong Chen. 2024a. Socraticlm: Exploring socratic personalized teaching with large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2024b. Gpt understands, too. AI Open, 5:208215. Bo Ni and Markus Buehler. 2024. Mechagents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge. Extreme Mechanics Letters, 67:102131. Bo Ni and Huajian Gao. 2021. deep learning approach to the inverse problem of modulus identification in elasticity. Mrs Bulletin, 46:1925. Minju Park, Sojung Kim, Seunghyun Lee, Soonwoo Kwon, and Kyuseok Kim. 2024. Empowering personalized learning through conversation-based tutoring system with student modeling. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pages 110. Russell Poldrack, Thomas Lu, and Gašper Beguš. 2023. Ai-assisted coding: Experiments with gpt-4. arXiv preprint arXiv:2304.13187. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. 2023. Automatic prompt optimization with \"gradient descent\" and beam search. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, (EMNLP), pages 79577968. Toran Bruce Richards. 2023. Auto-gpt: An experimental open-source attempt to make gpt-4 fully autonomous. Taylor Shin, Yasaman Razeghi, Robert Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024a. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345. 9 Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. 2023. Prompt engineering prompt engineer. arXiv preprint arXiv:2311.05661. Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, et al. 2024a. careful examination of large language model performance on grade school arithmetic. arXiv preprint arXiv:2405.00332. Jian Zhang, Changlin Yang, Haiping Zhu, Qika Lin, Fangzhi Xu, and Jun Liu. 2024b. semantic mention graph augmented model for document-level event argument extraction. arXiv preprint arXiv:2403.09721. Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph Gonzalez. 2022. Tempera: Test-time prompting via reinforcement learning. arXiv preprint arXiv:2211.11890. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910. Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric Xing, and Zhiting Hu. 2023. Promptagent: Strategic planning with language models enables expert-level prompt optimization. arXiv preprint arXiv:2310.16427. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024b. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574. Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2024. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. Advances in Neural Information Processing Systems, 36. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling next-gen llm applications via multiarXiv preprint agent conversation framework. arXiv:2308.08155. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2025. The rise and potential of large language model based agents: survey. Science China Information Sciences, 68(2):121101. Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2025. Are large language models really good logical reasoners? comprehensive evaluation and beyond. IEEE Transactions on Knowledge and Data Engineering. Fangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao, and Jun Liu. 2024. Symbol-llm: Towards foundational symbolcentric interface for large language models. In Proceedings of the ACL, pages 1309113116. Weijia Xu, Andrzej Banburski-Fahey, and Nebojsa Jojic. 2023. Reprompting: Automated chain-of-thought prompt inference through gibbs sampling. arXiv preprint arXiv:2305.09993. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. 2024a. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Hui Yang, Sifu Yue, and Yunzhong He. 2023. Auto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224. Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Hu. 2024b. Harnessing the power of llms in practice: survey on chatgpt and beyond. ACM Transactions on Knowledge Discovery from Data, 18(6):132."
        },
        {
            "title": "A Tasks and Datasets",
            "content": "To comprehensively evaluate the expert-level prompt optimization capabilities of our framework, we curate 17 tasks from two broad categories: General Tasks and Domain-Specific Tasks. General Task Evaluation We select six tasks from the BBH (Suzgun et al., 2022) and MMLU (Wang et al., 2024b) datasets, respectively. BBH tasks consist of six challenging reasoning tasks that assess logical inference and problem-solving skills, including boolean expressions, disambiguation QA, formal fallacies, geometric shapes, ruin names, and sports understanding. MMLU tasks cover six subject-specific tasks designed to evaluate general knowledge across diverse fields, including college biology, college medicine, electrical engineering, high school world history, human aging, and marketing. Domain-Specific Task Evaluation We include three benchmarks: C-Eval (Huang et al., 2024), GSM8K (Zhang et al., 2024a), and LSATAR (Zhong et al., 2023). C-Eval is Chinese evaluation benchmark that covers domain-specific topics such as art studies, clinical medicine, and Urban and Rural Planner. GSM8K is widely used mathematical reasoning dataset. LSAT-AR focuses on legal reasoning, evaluating AI performance in law-related tasks. Dataset Split In this study, we adopt minimal training paradigm by selecting only single instance from each dataset for training. Despite this extremely limited supervision, our method demonstrates strong and consistent performance across diverse range of datasets. This suggests that our approach possesses exceptional few-shot ability, enabling effective adaptation to various tasks with minimal prior knowledge. The detailed partition of the dataset is presented in Table 4."
        },
        {
            "title": "B Experiment Settings and Baselines",
            "content": "We compare MARS with three categories of baselines: original prompts, CoT prompts, and some of the latest APO methods. Specifically, (1) original prompts refer to the prompts used in the datasets, where each dataset often provides some initial guidance for the tasks. (2) To build the CoT (Zero-Shot) baseline, we add the prompt Lets think step by step at the beginning of each task; based on this, we further include specific example to create the ABBR. Train Test Tasks Bigbench Boolean Expressions Disambiguation QA Formal Fallacies Geometric Shapes Ruin Names Sports Understanding MMLU B.E. D.QA F.F. G.S. R.N. S.U. C.B. College Biology C.M. College Medicine Electrical Engineering E.E. HighSchool World History W.H. H.A. Human Aging M.T. Marketing C-EVAL Art Studies Urban And Rural Planner Clinical Medicine GSM8K LSAT-AR A.S. U.R.P. CL.M. GSM. L.A. 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 249 249 249 249 249 143 172 144 236 222 233 32 45 21 1318 229 Table 4: Data split of general tasks and domain-specific tasks. One instance for training and others for testing. The ABBR. column represents the abbreviations for all the tasks. CoT (Few-Shot) baseline. (3) Finally, we compare MARS with some strong baseline methods from recent years, including Automatic Prompt Engineer (APE) (Zhou et al., 2022), Prompt Optimization with Textual Gradients (ProTeGi) (Pryzant et al., 2023), Optimization by PROmpting (OPRO) (Yang et al., 2024a), and Prompt Engineer 2 (PE2) (Ye et al., 2023). APE and ProTeGi generate multiple prompts and perform search optimization to find the optimal prompt, while OPRO and PE2 optimize prompts by designing sophisticated meta prompts. We select powerful LLM, deepseek-V2.51210 (Guo et al., 2025), as our primary agent for the APO tasks. Not only does deepseek-V2.5-1210 exhibit strong reasoning and generation capabilities in variety of natural language processing tasks, but it also efficiently explores multiple angles when facing complex prompt optimization requirements, making it well-suited for adapting to different tasks and datasets in the APO process. We adopt accuracy as our primary evaluation metric to comprehensively assess the performance of different methods across various task scenarios. Table 1 presents our experimental results on 12 general task datasets, illustrating the performance of APO in diverse scenarios, while Table 2 summarizes its performance on five domain-specific datasets, underscoring the models versatility and stability across different fields. To further validate the generality and robustness of our method, we additionally employed another high-performance LLM, GPT4o (Achiam et al., 2023), for extended comparative experiments, with the corresponding findings reported in Appendix C. Furthermore, we provide detailed description of our experimental setup and procedures in the appendix to facilitate future research and practical applications."
        },
        {
            "title": "C Generalization Across Different Base",
            "content": "and Target Models In this section, we present the optimization performance of the method from this study on another base model, as well as the optimization results of our APO on other Target LLMs. C.1 Base Model of GPT-4o To verify the generality and effectiveness of the proposed method in this study, we conduct further experiments by replacing the base model with GPT4o (Achiam et al., 2023). As shown in Table 5, in the datasets of the 17 tasks adopted by this study, MARS achieves new SOTA performance when using the GPT-4o base model, surpassing the previous SOTA by 2.3%. This result demonstrates that MARS not only performs excellently on the existing base models but also exhibits strong transferability, continuously improving performance across different base models. This further validates the versatility and robustness of the MARS method, highlighting its effectiveness on variety of base models. C.2 Other Target LLMs We also conduct testing of the main experimental results of MARS on other Target LLMs, using the most optimized prompts from this study to validate the method across multiple different LLMs. These tests include performance on Deepseek-R1, GPT-3.5, GPT-4, and GPT-4o. The results in Table 6 from these experiments demonstrate that the optimization achieved on the Deepseek-V2.5 base model possesses strong generalization ability. Even on other powerful LLMs, MARS continues to show excellent performance and optimization effects. This further validates the cross-model applicability of the MARS method, proving that it is not limited to single base model but can achieve impressive results across various advanced LLM platforms. Tasks BBH MMLU Chinese GSM. L.A. Avg. Origin 60.92 CoT(ZS) 62.81 CoT(FS) 63.42 64.36 APE ProTeGi 76.43 OPRO 78.73 77.59 PE2 83.73 85.62 88.27 86.72 86.35 88.25 91. 58.26 64.26 68.69 69.03 73.52 75.79 74.67 72.31 20.96 59.24 76.25 24.45 62.68 83.92 28.82 66.62 81.18 30.13 66.28 82.70 31.88 70.18 84.74 32.75 72.05 85.43 35.81 73.08 MARS 81.13 92.82 78. 90.97 40.17 76.58 Table 5: Performance comparison on difference tasks based on GPT-4o. Base Deepseek -R1 -V2.5 GPT - -4o -3.5 Avg. Origin 56.96 61.48 44.79 49.70 55.84 53.75 CoT(ZS) 62.72 73.82 63.45 66.94 70.38 67.46 MARS 79.59 83.05 69.30 73.21 80.86 77.20 Table 6: Performance comparison on BBH tasks under different Target model settings."
        },
        {
            "title": "D Sample Size Analysis",
            "content": "One of the key highlights of this study is that the training data consists of only single samplewe utilize just one data point from the current task for training. To analyze the rationality of one-shot training, we present comparison of 0-shot, 1-shot, 3-shot, and baseline methods in Table 7. Tasks Train B.E. D.QA F.F. G.S. R.N. S.U. APE ProTeGi OPRO PE2 MARS MARS MARS 100 20 50 0 1 3 83.53 83.93 86.34 87.95 90.76 93.17 93.57 61.85 63.86 66.67 65.46 70.28 71.89 72.69 61.04 62.65 63.45 63. 73.09 74.70 74.30 51.41 52.21 53.81 54.62 57.83 59.43 60.24 77.51 80.32 83.13 84.34 88.35 90.36 89.96 74.70 76.71 82.73 75. 85.94 87.95 88.35 Table 7: Performance comparison of different sampling strategies on the evaluation metric. Train means the trining data. The results indicate that the performance difference between 1-shot and 3-shot is minimal, yet the 1-shot approach is more resource-efficient while also enhancing task time efficiency. This demonstrates that in resource-constrained scenarios, 1shot training offers better trade-off between performance and computational cost. Other strong baseline models, such as PromptAgent and OPRO, use at least 20% of the data for training, while our framework, using 1-shot training, achieves better performance than these models. This clearly demonstrates the effectiveness and resource efficiency of the MARS method. 12 was identified for each one. Tables 9 through Table 25 sequentially present the best solutions for these 17 sub-tasks along with their respective experimental results, demonstrating the adaptability and effectiveness of MARS across broad range of tasks."
        },
        {
            "title": "E Prompts for Agents",
            "content": "Table 8 summarizes the prompts used for all agents in this paper, with each agent playing crucial role in the overall process. First, the Manager is responsible for managing and invoking all agents. As the coordinator of the entire process, it ensures that each agent operates in the correct sequence and manner. Next, the UserProxy receives the specific task details and forwards them to the subsequent stages of the process. The role of the UserProxy is to convert external inputs into format that the system can handle, ensuring that task details are transmitted accurately. In the following stages, the Planner creates structured plan based on the task requirements, providing clear path for execution with defined steps and objectives. The Planners role is not just to map out the overall flow of the task but also to guide the Teacher, Student, and Critic through iterative refinements, enhancing the quality of the solution. Throughout this process, the Teacher provides guidance, the Student makes adjustments and learns, and the Critic evaluates and gives feedback. Finally, the Target validates the results to ensure their accuracy and effectiveness, guaranteeing that the output meets the desired objectives. This validation process is critical step in ensuring the reliability and effectiveness of the entire system. Full-process Prompt Optimization Figure 7 presents detailed example of the fullprocess prompt optimization, demonstrated using the Geometry Shapes task from the BBH dataset. The figure clearly outlines the overall optimization framework and its key stages, effectively illustrating the relationships and operations at each step. From the Manager, UserProxy, and Planner to the iterative refinement within the Teacher-CriticStudent framework, each phase is visually represented. The figure sequentially presents the functions of each agent and their collaboration, showcasing how optimization and coordination are effectively carried out at every stage, ensuring the accuracy and effectiveness of the final result."
        },
        {
            "title": "G Universal Optimum Solution",
            "content": "This section introduces the final optimized prompts for all general tasks and domain-specific tasks, obtained through the MARS optimization process. After multiple iterations for each of the 17 sub-tasks, prompt strategy that yields optimal performance 13 Manager You are the ChatManager, please schedule the right Agent for the job as needed. UserProxy Here is topic for geometric graph generation: Given full SVG path element containing multiple commands, determine the geometric shape that would be generated if one were to execute the full path element. For example: This SVG path element <path d=\"M 64.00,63.00 44.00,63.00 44.00,50.00 64.00,50.00 64.00,45.00 85.00,57.00 64.00,68.00 64.00,63.00\"/> draws a: Options: (A) circle (B) heptagon (C) hexagon (D) kite (E) line (F) octagon (G) pentagon (H) rectangle (I) sector (J) triangle want to input prompt and this topic into the big language model so that the big language model outputs the highest correctness rate. Please generate the most suitable prompt according to the requirements just mentioned. Planner Split the task Here is topic for geometric graph generation: Given full SVG path element containing multiple commands, determine the geometric shape that would be generated if one were to execute the full path element. For example: This SVG path element <path d=M 64.00,63.00 44.00,63.00 44.00,50.00 64.00,50.00 64.00,45.00 85.00,57.00 64.00,68.00 64.00,63.00\"\"/> draws Options: (A) circle (B) heptagon (C) hexagon (D) kite (E) line (F) octagon (G) pentagon (H) rectangle (I) sector (J) triangle want to input prompt and this topic into the big language model so that the big language model outputs the highest correctness rate. Please generate the most suitable prompt according to the requirements just mentioned. into detailed steps and details. For example, for the clinical medicine Test, the task is planned as follows: Total steps: 4 Step 1: Analyze the input requirements, focusing on the type of clinical medicine question and the format of the options. Step 2: Design prompt that encourages the model to consider the specific clinical characteristics of the condition described in the question and match the most appropriate option based on medical knowledge. Step 3: Request the model to evaluate each option in the context of clinical presentation, symptoms, and diagnostic characteristics of the condition to ensure it selects the most accurate answer. Step 4: Test and refine the prompt to ensure the model produces the highest correctness rate for similar clinical medicine questions. Teacher You are teacher who asks questions in the Socratic manner based on objectives and student responses. Please ask total of two questions: The first one is for the problem that appeared in the prompt given by the students in the last round. The second one is an optimization solution based on the current steps of the task. Please include only questions in your output and do not make answers for your students. Student You are prompt generator, please proceed to iterate over the existing prompts as required. Note that you should only output the new prompt you generated. Critic You are an evaluator responsible for judging the correctness of given task. Your output must strictly follow these rules: 1. If the task is judged as correct, output only: [True] 2. If the task is judged as incorrect, output: [False] [suggestion: <reason for the incorrect judgment>] Replace <reason for the incorrect judgment> with clear and concise explanation of why the task is incorrect. Do not include any additional text, comments, or explanations beyond the specified format. Target Prompt: Systematically analyze the given SVG path element by first breaking it down into its individual commands, such as (move to), (line to), and others. For each command, map the sequence of points it generates, ensuring you accurately trace the path step by step. As you follow the path, focus on identifying key geometric properties, such as equal side lengths, parallel lines, specific angles, or symmetries, that emerge between consecutive points. Use these properties to classify the shape based on its defining characteristics. For example, given the path <path d=M 64.00,63.00 44.00,63.00 44.00,50.00 64.00,50.00 64.00,45.00 85.00,57.00 64.00,68.00 64.00,63.00/>, calculate the distances between points to check for equal side lengths, measure angles to identify parallelism or perpendicularity, and look for symmetries that align with known geometric shapes. Based on these observations, determine whether the shape is circle, heptagon, hexagon, kite, line, octagon, pentagon, rectangle, sector, or triangle. Provide the correct answer by reasoning through the geometric properties derived from the path commands. Question: question Please dont output the process of doing the question, only the content of the answer.The answer should be parenthesis containing the capital letter of the chosen answer. please do not add any other spaces or symbols. Table 8: The table summarizes the prompts used for all Agents in this paper. The examples in the table are from the Geometry Shapes Task of the BBH dataset. 14 Figure 7: This figure presents complete example of the collaborative output from all agents in single iteration, using the Geometry Shapes task from the BBH dataset. 15 Boolean Expressions Evaluate the truth value of the following Boolean expression step by step. The expression consists of Boolean constants (True, False) and basic Boolean operators (and, or, not). Carefully analyze each part of the expression, apply the correct Boolean logic, and provide the final truth value as your answer. For example, if the input is not ( True ) and ( True ) is, the correct output is False. Ensure your reasoning is clear and accurate. Table 9: The table shows the final optimized prompt for the Boolean Expressions task of BBH using the MARS method. Disambiguation QA Analyze the following sentence to determine whether the pronoun is inherently ambiguous or if it can be linked to specific antecedent. Follow these streamlined steps to efficiently evaluate pronoun disambiguation while maintaining accuracy, especially in complex sentence structures: 1. Identify the Pronoun and Its Grammatical Role: 2. Identify Key Contextual Cues: 3. List and Filter Potential Antecedents: 4. Evaluate Plausibility: 5. Determine Ambiguity or Specific Antecedent: 6. Align with Provided Options: Evaluation Metrics for Model Output: 1. Correctness: 2. Clarity: 3. Efficiency: 4. Consistency: Additional Considerations: 1. Grammatical Structure Influence: 2. Optimizing Contextual Cue Identification: By simplifying the steps and focusing on key evaluation metrics, the model can process and apply the disambiguation process more efficiently while maintaining high accuracy and clarity in its outputs, even in complex sentence structures. Table 10: The table shows the final optimized prompt for the Disambiguation QA task of BBH using the MARS method. Formal Fallacies Syllogisms Negation Analyze the following argument step by step to determine its logical validity. Carefully consider the premises provided and assess whether the conclusion necessarily follows from them. Pay special attention to the role of negations in the argument. After evaluating the logical structure, decide whether the argument is deductively valid or invalid based on the given premises. Choose the correct option from the provided choices: valid or invalid. Ensure your reasoning is thorough and aligns with formal logical principles. Table 11: The table shows the final optimized prompt for the Formal Fallacies Syllogisms Negation task of BBH using the MARS method. 16 Geometric Shapes Given an SVG path element and list of geometric shape options, systematically analyze and interpret the sequence of SVG path commands to determine the number of vertices and the overall structure of the geometric shape. Follow this structured and optimized approach: 1. Dynamic Tolerance Threshold for Vertex Identification: Detailed Explanation 2. Optimized Vertex Counting and Connection: Detailed Explanation 3. Critical SVG Path Command Analysis:Detailed Explanation 4. Accurate Vertex Counting and Connection:Detailed Explanation 5. Distinguishing Between Similar Shapes:Detailed Explanation 6. Systematic Comparison with Provided Options:Detailed Explanation 7. Validation and Refinement:Detailed Explanation 8. Optimization for Similar Shapes:Detailed Explanation Key Considerations for Dynamic Tolerance and Vertex Identification:Detailed Explanation Optimized Comparison Process:Detailed Explanation By integrating these considerations into the analysis, the model can achieve higher correctness rate in identifying geometric shapes from SVG paths, even when dealing with shapes that have similar properties. Table 12: The table shows the final optimized prompt for the Geometric Shapes task of BBH using the MARS method. Ruin Names Given an artist, band, or movie name, create one-character edit that changes the name in humorous and universally recognizable way. The edit must involve only single-character change (adding, removing, or substituting one letter) and should prioritize simplicity, absurdity, and surprise to evoke humor effectively. Ensure the edit maintains clear connection to the original name, making the humor immediately recognizable and universally understandable, while avoiding overly specific or niche references. Key Guidelines: 1. Simplicity and Surprise: 2. Cultural Universality: 3. Absurdity and Creativity: Evaluation Metrics: Strategies for Simplicity and Surprise: Systematic Testing Strategies: Examples: Refinement for Evaluation Metrics: Focus on generating edits that are simple, surprising, and universally amusing, ensuring they strictly adhere to the one-character constraint and meet the evaluation criteria for humor, cultural relevance, and clarity. Test each edit with diverse set of sample inputs and audiences to validate its humor consistency and cultural universality, ensuring the edit is immediately recognizable and universally understandable. Table 13: The table shows the final optimized prompt for the Ruin Names task of BBH using the MARS method. 17 Sports Understanding Evaluate the plausibility of the following sports-related sentence by considering the following key aspects: 1. Player Abilities and Historical Performance: 2. Event Context and Historical Significance: 3. Terminology and Sport-Specific Knowledge: 4. Rarity vs. Impossibility: Guidelines: - If the action is rare but historically documented or consistent with the players abilities, consider it plausible. - For lesser-known players or niche sports, evaluate based on typical performance levels and historical precedents within that sport. - Prioritize consistency with the sports rules, norms, and historical records. Examples: Additional Context for Ambiguous Cases: Rationale Requirement: Potential Biases and Limitations: Edge Cases and Testing: Simplified Evaluation Process: - Focus on the core aspects of player abilities, event context, and sportspecific knowledge to streamline the evaluation. - Use historical examples and edge cases as supplementary references rather than primary determinants to avoid over-reliance and potential biases. Output yes if the sentence is plausible, or no if it is not, followed by brief rationale. Now, evaluate the following sentence: [input sentence]. Table 14: The table shows the final optimized prompt for the Sports Understanding task of BBH using the MARS method. 18 College Biology Generate set of multiple-choice biology questions that explicitly test higher-order thinking skills, such as application, analysis, and synthesis, within the specific contexts of cellular structure, molecular biology, and ecology. Each question should require students to apply biological principles to novel scenarios, analyze complex biological systems, or synthesize information from multiple disciplines to arrive at solution. Ensure that the questions are scientifically accurate, grounded in established biological principles, and reflect current research trends in these areas. For each question, provide clear, concise, and scientifically valid explanation for the correct answer, detailing how the interdisciplinary nature of biology informs the reasoning. The explanations should not only justify the correct answer but also deepen understanding of the underlying biological concepts, fostering both accuracy and conceptual clarity. Additionally, include specific examples of how higher-order thinking skills are integrated into the questions, such as requiring students to predict outcomes based on molecular interactions, analyze ecological data to infer population dynamics, or synthesize cellular and molecular processes to explain organismal behavior. To optimize the challenge level, ensure that the questions are neither too simplistic nor overly complex, striking balance that is appropriate for college-level biology students. This approach will ensure the questions are comprehensive, robust, and aligned with the goal of testing advanced cognitive skills in biology while maintaining relevance to the specified topics. Furthermore, refine the prompt to explicitly guide the language model to generate questions that test higher-order thinking skills while maintaining scientific accuracy and relevance to college-level biology. Optimize specific elements of the current prompt to better align with the goal of producing questions that balance challenge and clarity, ensuring they are neither too simplistic nor overly complex. This includes emphasizing the need for questions to be contextually rich, requiring students to integrate multiple biological concepts, and ensuring that the difficulty level is calibrated to challenge students without overwhelming them. The refined prompt should also encourage the generation of questions that are clear, concise, and free from ambiguity, while still requiring deep biological reasoning to arrive at the correct answer. Table 15: The table shows the final optimized prompt for the College Biology task of MMLU using the MARS method. 19 College Medicine Refined Prompt: Analyze the following scenario step by step, integrating interdisciplinary knowledge from biochemistry, sociology, and reasoning to identify the psychological framework that best explains unconscious bias in medical practice... Next, evaluate each option (Behaviorist, Psychoanalytic, Cognitive Behavioral, Humanistic) by considering how well it explains the influence of unconscious bias on clinical decision-making. ... To encourage deeper critical thinking, incorporate elements of Socratic questioning by asking probing questions such as... Ensure the prompt is structured clearly and concisely, balancing detailed theoretical explanations with clarity to guide the model effectively toward identifying the correct psychological framework. ... To optimize the prompt for generating high-quality, contextually appropriate multiple-choice questions for college medicine test, incorporate the following elements: 1. Clarity and Precision: 2. Depth and Relevance: 3. Alignment with Learning Objectives: 4. Distractor Quality: 5. Contextual Examples: 6. Theoretical and Practical Balance: By incorporating these elements, the prompt will guide the model to generate questions that are not only accurate and relevant but also aligned with the objectives of college medicine test, ensuring high correctness rate and educational value. ... Additional Instructions for Generating High-Quality Distractors: Enhancements Based on New Questions:1. Inclusion of Real-World Examples: 2. Iterative Testing and Refinement: By following these steps, the prompt will be continuously improved to generate questions that are both challenging and aligned with the learning objectives of college medicine test, ensuring that students are effectively tested on their ability to apply interdisciplinary knowledge to real-world medical scenarios involving unconscious bias. Specific Adjustments for Enhanced Critical Analysis and Practical Application:1. Interdisciplinary Integration: 2. Scenario-Based Questions: 3. Critical Thinking Emphasis:4. Practical Mitigation Strategies: By making these adjustments, the prompt will better align with the learning objectives of college medicine test, ensuring that students are not only tested on foundational knowledge but also challenged to critically analyze and apply interdisciplinary concepts in real-world medical scenarios involving unconscious bias. Further Refinement for Detailed Explanation and High-Quality Distractors: Final Refinement for Enhanced Real-World Application and Iterative Testing: 1. Real-World Application: 2. Iterative Testing and Refinement: By following these steps, the prompt will be continuously improved to generate questions that are both challenging and aligned with the learning objectives of college medicine test, ensuring that students are effectively tested on their ability to apply interdisciplinary knowledge to real-world medical scenarios involving unconscious bias. Specific Adjustments for Enhanced Real-World Application and Distractor Quality: 1. Interdisciplinary Integration: 2. High-Quality Distractors: By making these adjustments, the prompt will guide the model to generate questions that not only accurately identify the correct psychological framework but also provide detailed explanation of how unconscious bias manifests in specific medical scenarios and its impact on patient outcomes... Table 16: The table shows the final optimized prompt for the College Medicine task of MMLU using the MARS method. 20 Electrical Engineering Analyze the question by focusing on the specific conditions of the Barkhausen criterion for oscillators, which are loop gain and phase shift. ... Next, provide clear, step-by-step explanation of the Barkhausen criterion, emphasizing the two fundamental requirements: 1. Loop gain must be exactly unity for sustained oscillations.2. Phase shift of the feedback signal must be 0 or 360 relative to the input. To enhance understanding, include specific real-world examples, such as the design of an LC oscillator or phase-locked loop, to illustrate how the Barkhausen criterion is applied in practical scenarios... Proceed to evaluate each option (A, B, C, D) systematically, using the following structure for clarity... For each option, connect the reasoning back to fundamental electrical engineering principles and provide real-world examples or applications where the Barkhausen criterion is critical... Conclude the response by reiterating the correct answer (D) and summarizing its significance in practical electrical engineering applications... To ensure the prompts structure and depth enhance the language models ability to generate accurate and relevant responses, consider the following adjustments: 1. Clarify the introduction 2. Focus on critical concepts 3. Use structured evaluation: Systematically evaluate each option with clear, logical reasoning and real-world examples to reinforce understanding and relevance. 4. Iterative refinement By structuring the response in this manner and iteratively refining the prompt... Additional Considerations: 1. Influence of Real-World Examples 2. Structural Adjustments Refinement for Multiple-Choice Evaluation: 1. Explicitly state the evaluation criteria 2. Incorporate real-world scenarios 3. Maintain brevity and clarity 4. Highlight key takeaways By refining the prompt in this manner, the language model will be better equipped to... Iterative Refinement Process: 1. Initial Response Generation 2. Review for Accuracy and Relevance 3. Adjust Prompt Accordingly 4. Repeat the Process This iterative approach ensures that the prompt evolves to better guide the language model, resulting in responses that are not only theoretically sound but also practically relevant and aligned with real-world electrical engineering applications. Optimizing the Iterative Refinement Process: 1. Incorporating Feedback Loops: 2. Enhancing Real-World Context: 3. Balancing Depth and Brevity: 4. Focusing on Key Concepts: By implementing these optimizations, the iterative refinement process... Explicit Guidance for Multiple-Choice Evaluation: 1. Explicitly State the Evaluation Criteria 2. Incorporate Real-World Scenarios 3. Maintain Brevity and Clarity 4. Highlight Key Takeaways Adjustments for Balancing Theoretical Depth and Practical Application: 1. Focus on Core Principles 2. Use Structured Evaluation 3. Avoid Overloading with Details 4. Incorporate Real-World Examples By refining the prompt in this manner, the language model will be better equipped to generate responses... Specific Adjustments for Real-World Examples: 1. Demonstrate Practical Implications 2. Highlight Design Considerations 3. Provide Contextual Understanding Balancing Theoretical Depth and Practical Relevance: 1. Integrate Theoretical and Practical Elements 2. Maintain Focus on Core Principles 3. Use Clear, Concise Language By incorporating these adjustments, the prompt will guide the language model to... Influence of Real-World Examples: 1. Illustrate Practical Applications 2. Highlight Consequences of Deviations 3. Provide Contextual Understanding Optimizing the Iterative Refinement Process: 1. Incorporating Feedback Loops 2. Enhancing Real-World Context 3. Balancing Depth and Brevity 4. Focusing on Key Concepts By implementing these optimizations, the iterative refinement process will enhance the language models ability to generate responses that are both theoretically accurate and practically relevant, ensuring high correctness rate and alignment with real-world electrical engineering applications. Table 17: The table shows the final optimized prompt for the Electrical Engineering task of MMLU using the MARS method. 21 High School World History Generate set of multiple-choice questions that test both factual knowledge and critical analysis of the interconnected historical developments of the Ottoman Empire, economic imperialism, and World War I. Each question should require students to analyze how these events influenced each other, leading to the outbreak of World War I, with focus on cause-and-effect relationships and broader historical significance. Instructions for Question Design: 1. Interconnectedness and Cause-and-Effect: 2. Accessibility and Rigor: 3. Balanced Difficulty: 4. Critical Thinking and Historical Significance: 5. Format and Contextual Accuracy: Example Question with Passage:: one example Additional Constraints: - Engagement and Relatability: Use engaging and relatable examples or analogies where appropriate to make the questions more accessible and interesting to students. For instance, compare historical events to modern-day scenarios to help students draw parallels. - Depth of Analysis: Include questions that require students to analyze multiple layers of historical causation, such as how economic imperialism not only influenced European powers but also destabilized regions like the Balkans, contributing to the outbreak of World War I. - Historical Contextualization: Ensure that each question provides enough historical context for students to understand the significance of the events being discussed, without overwhelming them with unnecessary details. By following these guidelines, generate set of 5-10 multiple-choice questions that effectively test students understanding of the interconnectedness of the Ottoman Empires decline, economic imperialism, and World War I, while promoting critical thinking, historical analysis, and deeper appreciation of cause-and-effect relationships in history. Table 18: The table shows the final optimized prompt for the High School World History task of MMLU using the MARS method. Human Aging Refine the hierarchical elimination process to ensure the model accurately distinguishes between overlapping themes like cognitive decline and personality changes, especially when new terminology such as neuroinflammation is introduced, by implementing the following steps: 1. Test the Hierarchical Elimination Process with Sample Question: 2. Optimize the Dynamic Scoring System and Contextual Weighting: 3. Enhance the Focus Identification Protocol with Continuous Learning: 4. Dynamic Evidence Integration with Contextual Weighting: 5. Source Reliability Scoring with Provisional Scoring for Emerging Evidence: 6. Evidence Strength Assessment with Contextual Weighting: 7. Specific Metrics for Question Evaluation: By refining the hierarchical elimination process with these steps and incorporating specific metrics, the model can more effectively navigate overlapping themes in human aging questions, ensuring the highest correctness rate while maintaining precision and contextual relevance. Table 19: The table shows the final optimized prompt for the Human Aging task of MMLU using the MARS method. Marketing Analyze the following marketing-related question step by step, considering the principles of segmentation, pricing, market research, and other relevant marketing concepts. Carefully evaluate each of the provided options (A, B, C, D) and select the most suitable answer based on your analysis. Ensure your reasoning is clear and aligns with established marketing theories and practices. For example, if the question involves hierarchy of effects or sequential model used in advertising, identify the correct model from the options provided and justify your choice. Proceed methodically to arrive at the most accurate answer. Table 20: The table shows the final optimized prompt for the Marketing task of MMLU using the MARS method. 22 GSM8K Think step by step to solve linguistically diverse elementary school math application problems. Break down the problem into 2-8 logical steps, perform the necessary calculations at each step, and provide the final result. Ensure accuracy by carefully following the problems instructions and verifying each intermediate step. For example: Input: Janets ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers market? Step 1: Calculate the total eggs used daily: 3 (eaten) + 4 (baked) = 7 eggs. Step 2: Subtract the used eggs from the total laid: 16 - 7 = 9 eggs. Step 3: Calculate the daily earnings: 9 eggs 2 =18. Answer: 18 Follow this structured approach to solve similar problems. Table 21: The table shows the final optimized prompt for the GSM8K task using the MARS method. LSAT-AR Carefully analyze the given scheduling problem step by step, prioritizing logical reasoning, reading comprehension, and analytical reasoning to ensure thorough evaluation. Begin by explicitly listing and understanding all the constraints, with focus on the most critical ones first. Follow this structured approach to systematically eliminate options that violate any of the given conditions: 1. Prioritize the most restrictive constraints first. 2. Evaluate secondary constraints. 3. Assess the implications of Ninas scheduling. Throughout this process, avoid making assumptions beyond the provided constraints. Do not infer additional rules or conditions that are not explicitly stated. Stick strictly to the given information and apply logical reasoning to interpret and enforce the constraints. By adhering to this structured, methodical approach, you will systematically eliminate incorrect options and arrive at the correct schedule with the highest accuracy. This process mirrors the analytical rigor required in legal reasoning and ensures that the models output aligns with the principles of logical and legal analysis. Table 22: The table shows the final optimized prompt for the LSAT-AR task of AGIEval using the MARS method. Art Studies Please delve into the historical period represented by each option, paying particular attention to major breakthroughs or developments in textile technology and dye processes. First, collate the cultural context and technological advances of each period and analyze which periods technological achievements are most likely to be relevant to the method of blue print fabric printing. Based on this, the accuracy of the model in answering questions related to these historical and technological contexts is assessed. The output of the model is evaluated by setting specific judgment criteria, such as accurate description of the historical context, sound reasoning about process characteristics, and coherence of conclusions. Based on these criteria, the presentation of the prompts is iteratively adjusted and optimized to improve the models performance in selecting correct answers. Table 23: The table shows the final optimized prompt for the Art Studies task of C-Eval using the MARS method. 23 Urban And Rural Planner When optimizing prompts for assessing waste management plans in urban and rural planning, how can identifying aspects of solid pollutant control planning that are less emphasized (e.g., e-pollutants) help us improve our assessment methods? When testing prompts, what specific criteria should we consider to effectively assess their accuracy and relevance with respect to nuances in waste management programs? In addition, how can we ensure that models can accurately understand and prioritize the treatment of different types of waste to effectively guide urban and rural planning decisions? Table 24: The table shows the final optimized prompt for the Urban And Rural Planner task of C-Eval using the MARS method. Clinical Medicine In order to improve the accuracy of choosing the most appropriate answer in clinical medicine test question, it is crucial to systematically compare the key symptoms in the question stem with each of the options on case-by-case basis. The key to this process is to 1) accurately identify diagnosticallysymptoms and features in the question stem, 2) logically assess and eliminate these features based on their association with the options, and 3) apply clinically typical presentations and relevant background knowledge to validate the plausibility of each option. Based on this, the following iterative adjustments should be made: first, by continuously acquiring clinical knowledge to strengthen the identification of difficult symptoms; second, by adjusting the strategy in order to be more flexible in matching potential answers; and finally, by utilizing reflection and evaluating the effectiveness of the model in responding to similar questions over time, to identify and correct deficiencies. This fine-tuning and analysis can increase the probability of choosing the correct answer. Table 25: The table shows the final optimized prompt for the Clinical Medicine task of C-Eval using the MARS method."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "National University of Singapore",
        "Xian Jiaotong University"
    ]
}