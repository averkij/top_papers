{
    "paper_title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States",
    "authors": [
        "Yang Xiao",
        "Jiashuo Wang",
        "Qiancheng Xu",
        "Changhe Song",
        "Chunpu Xu",
        "Yi Cheng",
        "Wenjie Li",
        "Pengfei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) increasingly participate in human-AI interactions, evaluating their Theory of Mind (ToM) capabilities - particularly their ability to track dynamic mental states - becomes crucial. While existing benchmarks assess basic ToM abilities, they predominantly focus on static snapshots of mental states, overlooking the temporal evolution that characterizes real-world social interactions. We present \\textsc{DynToM}, a novel benchmark specifically designed to evaluate LLMs' ability to understand and track the temporal progression of mental states across interconnected scenarios. Through a systematic four-step framework, we generate 1,100 social contexts encompassing 5,500 scenarios and 78,100 questions, each validated for realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs reveals that their average performance underperforms humans by 44.7\\%, with performance degrading significantly when tracking and reasoning about the shift of mental states. This performance gap highlights fundamental limitations in current LLMs' ability to model the dynamic nature of human mental states."
        },
        {
            "title": "Start",
            "content": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States Yang Xiao1* Jiashuo Wang1* Qiancheng Xu1 Changhe Song1 Chunpu Xu1 Yi Cheng1 Wenjie Li1 1The Hong Kong Polytechnic University yang-alan.xiao@connect.polyu.hk Pengfei Liu2 2Shanghai Jiao Tong University csjwang@comp.polyu.edu.hk 5 2 0 2 3 2 ] . [ 1 3 6 6 7 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) increasingly participate in human-AI interactions, evaluating their Theory of Mind (ToM) capabilities - particularly their ability to track dynamic mental states - becomes crucial. While existing benchmarks assess basic ToM abilities, they predominantly focus on static snapshots of mental states, overlooking the temporal evolution that characterizes real-world social interactions. We present DYNTOM, novel benchmark specifically designed to evaluate LLMs ability to understand and track the temporal progression of mental states across interconnected scenarios. Through systematic four-step framework, we generate 1,100 social contexts encompassing 5,500 scenarios and 78,100 questions, each validated for realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs reveals that their average performance underperforms humans by 44.7%, with performance degrading significantly when tracking and reasoning about the shift of mental states. This performance gap highlights fundamental limitations in current LLMs ability to model the dynamic nature of human mental states."
        },
        {
            "title": "Introduction",
            "content": "Figure 1: simplified illustration showing the mental state trajectorys change over scenarios and the transformation question example to probe LLMs ability to adapt to the change. Theory of Mind (ToM) - the ability to understand and reason about others mental states - is fundamental to human social interaction (Premack and Woodruff, 1978; Turner, 1988). As Large Language Models (LLMs) increasingly engage in human-AI interactions, their capability to track and understand the dynamic nature of human mental states becomes crucial. While existing research has evaluated LLMs ToM capabilities, these evaluations often overlook critical aspect: the temporal *Equal contribution. Corresponding authors. 1DYNTOM are available at GitHub and HuggingFace. evolution of mental states in real-world social contexts. Current ToM evaluations of LLMs, including benchmarks like SocialIQA (Turner, 1988), BigToM (Gandhi et al., 2023), and TOMBENCH (Chen et al., 2024), predominantly focus on static snapshots of mental states in isolated scenarios. These works primarily focus on static evaluations, whereas our work presents novel approach to capture the continuous change of mental states across multiple interconnected scenarios - crucial aspect of real-world social interactions that has not been systematically evaluated in previous work. This temporal dimension is essential for understanding LLMs true capabilities in real-world social interactions, where mental states constantly shift and evolve in response to ongoing social dynamics. For instance, LLMs are expected to understand and reason about the shift of user mental states in support conversations to better help users (Liu et al., 2024b; Wang et al., 2024). To address this challenge, we introduce DYNTOM (Dynamic Theory of Mind), novel benchmark designed specifically to evaluate LLMs ability to track and understand the temporal evolution of mental states, as shown in Figure 1. Our benchmark is constructed through systematic process: (1) social context construction, including social location, character profiles, and relationships; (2) mental state trajectory design across multiple scenarios; (3) scenario generation with natural dialogue; and (4) question formulation targeting temporal understanding of mental states. Each generated scenario and question undergoes rigorous human validation to ensure quality and realism. DYNTOM captures mental state dynamics through continuous social scenarios while incorporating real-world elements such as rich social contexts. Our benchmark comprises 1,100 social contexts featuring 2,200 characters across 261 social locations, 5,500 social scenarios, and 78,100 multiplechoice questions. Through comprehensive evaluation of ten representative LLMs, including GPT-4 series (Achiam et al., 2023), Llama 3 series (Dubey et al., 2024), Qwen 2 series (Yang et al., 2024), and GLM series (GLM et al., 2024), we find that their average performance lags behind human performance by 44.7%, with the gap widening significantly when requiring models to track how mental state changes across different scenarios. This performance degradation highlights fundamental limitation in current LLMs ability to model the dynamic nature of human mental states. The main contributions of this work are: 1. novel framework for evaluating LLMs understanding of temporal evolution in mental states, with systematic process for generating and validating evaluation data; 2. comprehensive benchmark featuring 78,100 questions specifically designed to probe LLMs ability to track and reason about mental state changes over scenarios; 3. Extensive empirical evaluation reveals specific challenges LLMs face in temporal reasoning about mental states, including detailed analysis of failure modes in tracking state changes and determining factors influencing changes."
        },
        {
            "title": "2.1 ToM Benchmarks",
            "content": "Theory of mind appears to be an innate potential ability in humans that requires social and other experiences over many years for its full development. With the development of LLMs, researchers have begun to probe whether LLMs possess Theory of Mind ability comparable to that of humans, as they have reached and occasionally surpassed human performance in some task-solving and reasoning tasks. Nematzadeh et al. (2018); Le et al. (2019); Wu et al. (2023) apply the Sally-Anne Test and bAbi to test LLMs ToM ability in the aspect of false belief, and they find that LMs performance is significantly lower than humans. Ullman (2023); Shapira et al. (2024); Kim et al. (2023); Sap et al. (2022) propose that LLMs prone to shortcuts and spurious correlations. Apart from the test in the aspect of belief, Xu et al. (2024); Chen et al. (2024); Sabour et al. (2024) construct benchmarks to test LLMs ToM ability for emotion, intention, and perception. Jin et al. (2024); Shi et al. (2024) propose to evaluate LLMs in multi-modal environments. However, most of the previous evaluations do not take the continuous evolution of mental states across multiple interconnected scenarios into consideration. Our work aims to develop novel benchmark to understand the ToM reasoning of language models in the dynamic social context."
        },
        {
            "title": "2.2 Human Behavior Simulation",
            "content": "Recent advancements in language model capabilities have opened new avenues for generating highquality data. Previous work has demonstrated successful applications of LLMs in simulating human behavior across various domains, including HCI research (Hämäläinen et al., 2023), conversational recommender systems (Yoon et al., 2024), roleplaying (Xie et al., 2024; Xiao et al., 2023), clinical medical education (Wang et al., 2024), social science (Hua et al., 2023; Park et al., 2023, 2022; Aher et al., 2023). DYNTOM leverages LLMs to generate realistic dialogues that reflect predetermined character mental states. We implement strict quality assurance through human evaluation of social context authenticity and question validity. This approach combines the efficiency of automated generation with robust validation procedures, ensuring our benchmarks reliability and reproducibility."
        },
        {
            "title": "3.1 DYNTOM Construction Framework",
            "content": "Definitions and Preliminaries We first define key terms used throughout this paper. Social Location refers to the physical setting where social interactions occur, which influences behavior and social norms (Farrow et al., 2017). Social Context provides the foundational setup for social interactions, comprising social location, character profiles (e.g., demographics, personalities), and their relationships. Social Scenario represents self-contained social interaction between characters at specific moment. In our work, we construct sequences of temporally connected scenarios within the same social context, enabling us to track the dynamic evolution of characters mental states through continuous social interactions. We define Social Stage as the complete structure of social interaction, comprising the Social Location, Social Context, and Social Scenarios. Our framework consists of three systematic steps for generating the social stages in our benchmark: Step 1: Social Context Construction social context consists of three components: social location, two characters profiles, and the relationship between these characters. For social locations, following Ziems et al. (2023), we collect 261 locations across 13 categories representing common physical settings for social interactions. For character profiles, we construct seven aspect pools (names, surnames, gender, occupation, education, race, and personality traits) using demographic data from the U.S. Census Bureau statistics to ensure realistic population representation. For each social context, we randomly sample one location and create two characters by sampling from each aspect of these pools. To generate character relationships, we first create four exemplar relationships manually, then prompt GPT-4-Turbo to generate new relationships based on these exemplars and the sampled character profiles. To ensure quality, four human annotators evaluate both the characters profiles and their corresponding relationships, discarding any profile or relationship that any annotator deems unrealistic. This rigorous validation process results in retaining 92% of the generated profiles and relationships. Step 2: Mental State Trajectory Design We focus on evaluating three mental states (beliefs, emotions, and intentions) and their resulting actions (for convenience, also denoted as mental states). For each social context, we design sequence of five2 scenarios where these states of the characters evolve and influence each other. Following the psychological research of DAndrade (1995), we model the mental states through three key relationships: 1) beliefs influence emotions; 2) beliefs and emotions influence intentions; 3) beliefs, emotions, and intentions drive actions. We prompt GPT-4Turbo with four exemplar trajectories and the 3 design principles to generate coherent mental state progressions across every five scenarios. Importantly, when generating these trajectories, LLMs should also output specific cues that trigger mental state transitions between adjacent scenarios, providing explicit reasoning for how and why mental states evolve throughout the social interaction. Any scenarios without the mental state trajectory and cues are discarded. Four human annotators evaluate each generated trajectory on two dimensions using 5-point scale: coherence (consistency of mental state changes across scenarios), rationality (the validity and rationality of these transition cues), and authenticity (plausibility of mental state transitions). Trajectories with mean scores below 4.0 on either dimension are discarded, resulting in an 85.4% retention rate. Step 3: Scenario Generation Building upon the mental state trajectories designed in Step 2, we now generate scenarios with natural dialogues to manifest these mental states in social interactions. Each scenario includes background description and dialogue between characters, reflecting the mental state trajectory designed for this scenario. We choose dialogue as the primary format because it naturally reveals characters mental states and is frequently used in daily interactions. For each mental state trajectory, we prompt GPT-4-Turbo to generate the dialogue and background of the scenario, ensuring that the main characters utterances and behaviors align with their prescribed mental states. Following the same validation process as Step 2, four human annotators evaluate each scenario on three dimensions using 5-point scale: consistency (alignment with the designed mental state trajectory), coherent (the five scenarios within each social context form coherent storyline, where each scenario connects meaningfully to those be2we have limited the number of scenarios to five to reduce costs while maintaining more authentic social context. Researchers can easily adjust the scenario number in our framework to meet their needs. evolution across all scenarios and testing LLMs ability to maintain and reason about extended temporal sequences. This progression of question types allows us to precisely identify where LLMs succeed or fail in understanding dynamic mental states. Understanding questions reveal whether failures in temporal reasoning stem from basic state comprehension issues, while the three transformation types help pinpoint specific limitations in tracking and reasoning about mental state changes over time. We apply four predefined question templates to the social stage to generate questions. The template details are presented in the Appendix A.5. Options and Ground Truth The design of options and ground truth leverages the comprehensive mental state trajectories created in Step 2. For understanding questions, when evaluating specific state (e.g., belief in scenario 1), we construct distractors using both other states from the same scenario and the same state from other scenarios. For instance, when assessing belief in scenario 1, incorrect options include the emotion, intention, and action from scenario 1, as well as beliefs from scenarios 2-5. Similarly, for transformation questions, options are constructed using documented state values and their changes from the trajectory. For example, when asking \"Why does Johns belief change from feeling inferior in scenario 1 to feeling respected in scenario 2?\", the correct answer would be \"Beverlys praise of his expertise,\" while distractors include other documented changes such as \"Johns demonstration of skills\" (action). This systematic approach of option generation ensures that the questions are challenging yet unambiguous, as both correct answers and distractors are grounded in the explicitly designed mental state trajectories. Validation Following our validation process, four annotators evaluate the questions on clarity (whether the question is unambiguous) and answerability (whether the answer can be determined from the given context) using 5-point scale. Questions scoring below 4.0 are regenerated, resulting in final set of high-quality evaluation items. Finally, 78100 questions are collected. Evaluation Metric To evaluate LLMs performance on DYNTOM, we calculate their accuracy across all 78,100 questions. For each question, we consider the LLMs response correct only if it exactly matches the ground truth option. The final Figure 2: simplified example of question types (showing only two scenarios). In each scenario, characters have four states: belief, emotion, intention, and action - these are assessed through understanding questions. The value of the same state changes across different times (scenarios) is evaluated through transformation1,2,3 questions. fore and after it.), and authenticity (naturalness of the scenario and conversations). Scenarios scoring below 4.0 on either dimension are discarded, with 88.7% of the generated scenarios retained."
        },
        {
            "title": "3.2 Question Genres",
            "content": "Based on the validated scenarios and mental state trajectories, we design questions to systematically evaluate how well LLMs can track and reason about the temporal evolution of mental states. We develop four question types that progressively assess different aspects of this capability. Understanding Questions Understanding questions establish baseline by testing LLMs ability to identify states (belief, emotion, intention, and action) at specific points in time. While this represents the most basic level of ToM reasoning in our evaluation, it forms the foundation for evaluating more complex temporal understanding. Transformation Questions Transformation questions directly evaluate LLMs ability to reason about mental state dynamics through three increasingly complex aspects, as illustrated in Figure 2. Transformation-1 examines whether LLMs can detect state changes between consecutive scenarios, testing their basic temporal awareness. Transformation-2 probes deeper by testing if LLMs understand the causal mechanisms behind state changes, evaluating their grasp of Transformation-3 psychological dynamics. presents the most challenging task: tracking state"
        },
        {
            "title": "Questions",
            "content": "Understanding Transformation-1 Transformation-2 Transformation-"
        },
        {
            "title": "Average Social Scenario Length\nAverage Questions Length",
            "content": "4 261 2200 5500 1100 78100 28.2% 22.5% 43.7% 5.6% 457.9 77.5 Table 1: DYNTOM Statistics. (cid:209) Social Location Ł Profile ' Relationship (cid:18) Dynamic mental states Intradependent mental states { Questions Num ToMi SocialIQA Hi-ToM OpenToM BigToM TOMBENCH Plot (cid:209) Ł ' (cid:18) DYNTOM(ours) Table 2: Benchmark Comparison. { 999 37588 1200 2384 600 2860 78100 performance metric is computed as the percentage of correct answers across all questions, providing comprehensive measure of the models ability to reason about dynamic mental states in social interactions."
        },
        {
            "title": "3.3 Statistics",
            "content": "In total, our final benchmark contains 1,100 highquality social stages, where each stage consists of social location (physical setting), social context (two characters with detailed profiles and relationships), and five social scenarios (temporally connected interactions). For each social stage, we generate 71 questions across four types (understanding and three types of transformation), resulting in 78,100 questions in total. The detailed statistics are shown in Table 1. Compared to existing ToM benchmarks  (Table 2)  , DYNTOM offers key advantages. While previous works evaluate static snapshots of mental states, DYNTOM systematically captures the temporal evolution of mental states through connected scenarios, enabling more realistic evaluation of how well LLMs can track and reason about dynamic social interactions."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "To evaluate the ToM reasoning capabilities across different model scales and architectures, we conducted experiments using DYNTOM on ten representative language models, ranging from 7B to 70B parameters. These models include GPT-4o, GPT-4-Turbo, Llama-3.1 (8B and 70B variants), Mistral-7B, Mixtral-8x7B, Qwen2 (7B and 72B variants), DeepSeek-V2, and GLM-4. All models are accessed through their official APIs or publicly available weights. We employed two evaluation approaches: (1) vanilla prompting, where models directly answer questions, and (2) zero-shot chain-of-thought (CoT) prompting (Wei et al., 2022), which encourages step-by-step reasoning before providing final answers. For both vanilla and CoT prompting, we used temperature of 0.7 and top-p of 0.9 across all models to ensure fair comparison. To establish human performance baseline, we recruited ten graduate students, different from those involved in data annotation, to evaluate randomly sampled 30% of the dataset (330 social stages and 23430 questions). Detailed specifications of model versions, architectures, context windows, and prompting templates are provided in Appendix B.1."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 3 demonstrates the ToM performance of LLMs across different mental states (belief, emotion, intention, and action) and question types (understanding and transformation), both with and without chain-of-thought (CoT) prompting. We established the human baseline by averaging performance across ten annotators, with standard deviations reported to indicate inter-annotator agreement. Here, we discuss several key findings from our experimental results. Human vs. LLMs Human annotators achieved an average accuracy of 77.7% across all tasks. All LLMs performed significantly below this baseline, and their average performance underperforms humans by 44.7%, with even the best GPT-4o showing gap up to 13.7% in vanilla prompting (77.7% vs. 64.0%). The performance disparity was particularly pronounced in transformation-type questions across all mental states, revealing current LLMs limitations in tracking and comprehending the temporal evolution of mental states in soSubject Human GPT-4o GPT-4-Turbo Llama-3.1-70B Llama-3.1-8B Mixtral-8x7B Mistral-7B Qwen2-72B Qwen2-7B DeepSeek-V2 GLM-4 LLM AVG. GPT-4o+CoT GPT-4-Turbo+CoT Llama-3.1-70B+CoT Llama-3.1-8B+CoT Mixtral-8x7B+CoT Mistral-7B+CoT Qwen2-72B+CoT Qwen2-7B+CoT DeepSeek-V2+CoT GLM-4+CoT LLM+CoT AVG. Belief Emotion Intention Action AVG. T T 83.816.4 77.612.0 89.510. 78.714.0 79.021.4 73.814.0 76.725.8 76.314.0 77.712. 80.9 63.5 65.8 31.6 23.3 21.3 72.0 22.2 6.5 29.5 41.7 79.2 61.7 68.0 32.0 15.6 21.6 70.1 28.6 7.4 30.0 41.4 44.5 32.3 40.2 18.0 21.6 11.7 37.2 19.8 9.2 23.9 25. 44.5 31.0 38.9 21.7 13.9 10.1 39.2 18.1 9.8 26.4 25.4 91.7 74.7 93.8 40.0 46.2 23.8 85.5 43.0 4.8 43.9 54.7 88.0 77.8 90.7 40.3 29.7 22.5 87.6 43.7 3.2 48.0 53. 45.8 33.9 42.3 19.9 18.4 10.2 38.0 20.5 8.1 20.8 25.8 47.6 33.2 43.7 20.9 13.8 11.0 41.4 19.3 10.4 22.1 26.3 87.5 71.3 82.8 22.4 32.9 16.3 79.5 25.1 3.7 28.5 45. 82.1 71.4 81.4 21.8 25.8 19.9 83.8 29.6 5.0 32.4 45.3 51.9 35.5 42.0 16.6 10.8 10.1 33.2 15.7 7.3 16.5 24.0 46.6 32.8 42.8 19.3 8.8 8.1 34.6 19.7 7.3 17.7 23. 95.1 80.5 91.8 26.6 40.3 20.6 89.8 24.6 2.8 40.4 51.3 90.4 81.0 96.5 23.3 26.6 18.8 89.0 20.2 5.0 43.2 49.4 55.6 36.2 45.5 15.5 9.5 9.2 20.9 15.0 5.7 16.8 23. 49.6 37.6 46.6 15.9 8.8 8.8 27.1 18.4 6.4 14.1 23.3 64.0 47.6 57.1 22.3 21.9 13.9 48.5 22.1 7.2 25.4 33.0 61.1 47.1 57.6 23.6 15.8 13.3 51.3 23.5 8.1 26.6 32. Table 3: LLMs performance on DYNTOM. U: Understanding, T: Transformation. Numbers represent accuracy in percentages. For human performance, subscripts indicate standard deviation across ten annotators. cial interactions. Notably, however, some LLMs, particularly GPT-4o, demonstrated superior performance in understanding-type questions compared to human annotators. This can be attributed to the nature of these questions, which primarily involve analyzing static mental states within single scenarios, as illustrated in Figure 2. Such tasks require less temporal reasoning and rely more on semantic matching of explicitly stated mental states, aligning well with LLMs pattern recognition capabilities but not necessarily reflecting the true understanding of dynamic social contexts. Differences Between LLMs ToM Performance In vanilla prompting settings, GPT-4o emerged as the leading model, achieving an accuracy of 64.0% and outperforming the second-best model, Llama-3.1-70B (57.1%), by 6.9 percentage points. Among open-source models, both Llama-3.1-70B and Qwen2-72B demonstrated remarkable capabilities, surpassing GPT-4-Turbos 47.6% performance. Notably, Llama-3.1-70B achieved superior performance in emotion-related understanding tasks, reaching 93.8% accuracy compared to GPT4os 91.7%. However, even GPT-4os best overall performance at 64.0% falls significantly short of human-level performance, with DeepSeek-V2 showing the lowest performance at 7.2%. This substantial performance gap, particularly in tracking the temporal evolution of mental states, highlights the challenging nature of our benchmark and reveals that current LLMs lack robust ToM reasoning capabilities in realistic social contexts, despite their near-perfect performance on existing ToM benchmarks (Gandhi et al., 2024). Differences Between Transformation and Understanding Question Types Table 3 also reveals that models perform significantly worse on transformation questions compared to understanding questions. The most substantial gap occurs in emotionrelated reasoning, where the average model accuracy drops from 54.7% in understanding questions to 25.8% in transformation questionsa difference of 28.9 percentage points. Transformation questions require models to track how characters mental state evolves across different scenarios, capturing shifts in beliefs, emotions, and intentions over time. This performance gap highlights critical limitation of current modelstheir inability to effectively reason about dynamic mental state changes within continuous social contexts. Vanilla vs. CoT Prompting Our experimental results demonstrate that standard chain-of-thought (CoT) prompting has inconsistent effects on LLMs ToM reasoning capabilities. While CoT prompting improved performance for smaller models (Llama3.1-8B: +1.3%, Qwen2-72B: +2.8%, DeepSeekV2: +0.9%, and GLM-4: +1.2%), it led to performance degradation in more capable models, notably GPT-4o (-2.9%). Through analyzing the intermediate outputs (Appendix B.3), we identified two-fold effect: For smaller models that struggle with complex reasoning, CoTs step-by-step decomposition provides beneficial scaffolding for basic problem analysis. However, this same decomposition becomes limitation for more capable models, as it enforces rigid reasoning structure that treats each scenario independently, failing to capture the crucial temporal dependencies between scenarios. As shown in our case study, when asked about mental state changes from scenarios 1 to 2, models following CoT often extensively analyze each scenario but fail to explicitly compare the states across time steps, leading to incorrect conclusions. This aligns with findings in Xiao et al. (2023) about LLMs challenges in maintaining coherence during reasoning long inputs. These observations suggest that while CoT can help with basic reasoning decomposition, effective ToM reasoning requires specialized promptings that explicitly guide models to track and analyze the temporal evolution of mental states across scenarios. Differences Across Mental States Analysis of Table 3 reveals consistent patterns in models capability to reason about different mental states, particularly in understanding-type questions. Emotionrelated reasoning achieves the highest accuracy, averaging 54.7%, whereas belief-related reasoning lags behind at 41.7%a gap of 13 percentage points. We hypothesize that this disparity stems from the inherently implicit nature of beliefs compared to other mental states while emotions and intentions often manifest in observable behaviors or explicit statements, beliefs frequently require multi-step inference from indirect evidence, such as actions or conversational context. This observation suggests that belief reasoning poses unique challenges in temporal social contexts, where models must not only infer current beliefs but also track their evolution through sequential interactions. 4.3 In-Depth Analysis LLMs Limits of ToM on Transformation Evaluating mental states across multiple interconnected scenarios introduces complex compositional reasoning challenges (Dziri et al., 2023), requiring models to track and reason about the continuous evolution of mental states. To systematically analyze how models handle this multi-step reasoning Figure 3: The percentage of GPT-4os four types of their response. process, we group related questions and categorize the models responses based on their performance on both the primary question and its dependencies. For instance, consider question that asks whether characters beliefs change between two scenarios. To answer this primary question (denoted as C), the model must first correctly identify the characters beliefs in each individual scenario (denoted as D). By grouping such related questions, we can assess the models responses and classify them into four types: (1) Fully correct: The model accurately answers both the primary question (C) and all its dependencies (D). (2) Local error: The model correctly answers all dependencies (D) but makes an error on the primary question (C). (3) Restoration error: The model correctly answers the primary question (C) despite making errors on one or more of its dependencies (D). (4) Full error: The model makes errors on both the primary question (C) and one or more of its dependencies (D). Calculating the proportion of each response type across all question groups provides comprehensive evaluation of the models performance on these compositional questions requiring multi-step reasoning. As shown in Figure 3, our analysis reveals distinct patterns in models reasoning capabilities. The fully correct cases, where models successfully identify both the mental states and their changes, are notably rare across all state types (13-17%), indicating limited genuine understanding. Local errors (13-18%) show models can correctly identify mental states but fail to reason about their changes, suggesting an inability to track evolution. Full errors dominate across all mental states (5058%), with belief states showing the highest error rate (58%), revealing fundamental limitations in comprehending both states and their transitions. Restoration errors (8-16%) occur when models correctly identify changes without understanding"
        },
        {
            "title": "6 Scenarios\n7 Scenarios",
            "content": "1-2 64.0 56.0 2-3 50.0 45.0 3-4 51.0 26. 4-5 62.0 30.0 5-6 62.0 26.0 6-7 - 34. Table 4: The average of GPT-4os scores in the transformation question type for 6 and 7 scenarios."
        },
        {
            "title": "Time Span",
            "content": "w/o the truncation w/ the truncation 5 scenarios 6 scenarios 7 scenarios 1-2 2-3 31-2 2-3 3-4 1-2 2-3 3-4 54.0 50.0 54.0 60.0 50.0 51.0 56.0 45.0 26.0 55.0 53.0 55. 60.0 54.0 56.0 53.0 54.0 47.0 +1.0 +3.0 +1.0 +0.0 +4.0 +5.0 -3.0 +9.0 +21. Table 5: Comparison of GPT-4os performance with and without the truncation of the fifth/seventh/eighth scenario across different time spans and categories (total 5, 6, and 7 scenarios), with Delta indicating the difference between the two conditions. 4 improve by 21 percentage points (from 26% to 47%) when later scenarios are removed. Similar improvements are observed across 5and 6-scenario sequences, with middle-span accuracy increasing by 1-5 percentage points. These consistent improvements across different sequence lengths reveal critical limitation in LLMs ability to process dynamic mental states. While models can effectively track states at the beginning and end of interactions, they struggle to maintain understanding across extended scenarios - precisely the kind of continuous evolution that characterizes real-world social interactions. The significant performance degradation in middle scenarios (dropping to as low as 26% in longer sequences) underscores the importance of our dynamic evaluation approach and highlights fundamental challenge in developing LLMs that can truly understand evolving social contexts."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present DynToM, benchmark designed to evaluate LLMs theory of mind capabilities in dynamic social contexts, moving beyond static assessments to capture the crucial evolution of mental states across interconnected scenarios. Our human evaluation validates the benchmarks alignment with real-world social dynamics, while experimental results reveal significant gaps in current LLMs capabilities - even the best model trails Figure 4: The average of GPT-4os scores of the transformation question type in different time spans. The time span indicates the specific scenarios to which one question relates. the underlying states, indicating superficial pattern matching rather than genuine reasoning. This analysis pinpoints where models struggle in the reasoning process, whether in identifying initial mental states, determining factors influencing changes, or integrating information across scenarios. LLMs Fail in the Middle Scenario Our analysis reveals consistent pattern of lower performance in transformation-type questions compared to understanding-type questions across all models. To investigate this performance gap, we examine model accuracy across different time spans, where each span represents the interval between consecutive scenarios (e.g., span 1-2 represents the transition from scenarios 1 to 2). We categorize the questions into different time spans based on their associated social scenarios. The results show distinct \"U-shaped\" pattern: models perform better at early and late time spans but struggle with middle spans. To validate whether this pattern stems from the \"Lost in the middle\" phenomenon (Liu et al., 2024a) - where model performance degrades when processing information from the middle of long contexts - we conducted two additional experiments. First, we extend our analysis to longer sequences of 6 and 7 scenarios. The results  (Table 4)  strongly support our hypothesis: in 6-scenario sequences, accuracy drops to 50% in span 2-3, while in 7-scenario sequences, performance deteriorates more severely to 26% in span 3-4. Second, we perform controlled experiment where we truncate sequences to their first four scenarios. As shown in Table 5, this intervention leads to significant performance improvements in middle spans: for 7-scenario sequences, the accuracy in span 3-"
        },
        {
            "title": "Ethics Statement",
            "content": "Annotators and contents We strictly adhere to the ACL Code of Ethics. We placed high importance on ensuring the comfort and well-being of our annotators. We advised them to stop the annotation process if they came across any information that caused them discomfort. We recruited annotators at rate of 2 3 times their local hourly minimum wage. We instruct the annotators to validate the data without bias and keep the content free from unsafe, toxic, biased, offensive, and harmful content. We utilize the models in accordance with their designated purpose. In summary, we make every effort to adhere to the ethical norms set forth by ACL. Ethical Considerations. The theory of mind is distinctive social cognitive capability that is intrinsic to humans. Assessing the Theory of Mind capacities of Large Language Models utilizing DYNTOM may result in anthropomorphic interpretations, attributing human-like qualities to LLMs. Nonetheless, it is imperative to clarify that our objective is not to anthropomorphize LLMs. Our objective is to evaluate the capacity of LLMs to comprehend and interpret human mental states, thus enhancing AIs interaction with humans in the social context."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank all reviewers for their insightful comments and suggestions to help improve the paper. This work was supported by the Research Grants Council of Hong Kong (GRF 15209724 and TRS T43-518/24-N). human performance by 11%, particularly struggling with tracking mental state changes across extended interactions. As LLMs continue to be deployed in social contexts, DynToM provides valuable framework for assessing and improving their ability to understand the dynamic nature of human mental states."
        },
        {
            "title": "Limitations",
            "content": "Limited LLMs This paper makes significant contribution to the field by introducing DYNTOM, an innovative benchmark designed to evaluate Theory of Mind capabilities in language models within authentic social contexts. However, due to the constraint of computing resources and budget, limitation lies in its evaluation scope, which encompasses ten language models with an emphasis on representative models. While this selection includes prominent models such as GPT-4 and Llama, the focus potentially overlooks insights that could be gained from examining other emerging opensource models and commercial models, such as Claude. Limited Prompt Methods We use vanilla and CoT prompting methods for evaluation. Other methods, such as think-twice (Wilf et al., 2024), belief tracker (Sclar et al., 2023), and self-consistency (Wang et al., 2023), could also be explored to enhance the LLMs ToM performance within authentic social contexts. Limited mental states types and evaluation modality While our framework effectively models the interplay between belief, emotion, intention, and action based on established psychological theory (DAndrade, 1995), there are opportunities to expand the scope of mental states evaluated. Future work could explore additional mental states, such as knowledge, to provide even richer insights into language models ToM capabilities. While our dialogue-based evaluation approach has proven effective in assessing models ToM abilities in dynamic contexts, future research could explore how these models perform in multimodal contexts that include visual and auditory cues. This extension would complement our text-based findings by examining how models track temporal changes in mental states across different modalities, though our current framework already provides robust insights into models social reasoning capabilities."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Gati Aher, Rosa I. Arriaga, and Adam Tauman Kalai. 2023. Using large language models to simulate multiple humans and replicate human subject studies. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org. Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu, Yunghwei Lai, Zexuan Xiong, and Minlie Huang. 2024. ToMBench: Benchmarking theory of mind In Proceedings of the in large language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1595915983, Bangkok, Thailand. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. 2023. Faith and fate: Limits of transformers on compositionality. In Thirty-seventh Conference on Neural Information Processing Systems. Roy DAndrade. 1995. The development of cognitive anthropology. Cambridge University Press. Katherine Farrow, Gilles Grolleau, and Lisette Ibanez. 2017. Social norms and pro-environmental behavior: review of the evidence. Ecological Economics, 140:113. Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, and Noah Goodman. 2023. Understanding social reasoning in language models with language In Thirty-seventh Conference on Neural models. Information Processing Systems Datasets and Benchmarks Track. Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, and Noah Goodman. 2024. Understanding social reasoning in language models with language models. Advances in Neural Information Processing Systems, 36. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793. Perttu Hämäläinen, Mikke Tavast, and Anton Kunnari. 2023. Evaluating large language models in generating synthetic hci research data: case study. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 23, New York, NY, USA. Association for Computing Machinery. Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and Yongfeng Zhang. 2023. War and peace (waragent): Large language model-based multi-agent simulation of world wars. arXiv preprint arXiv:2311.17227. Chuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang, Yen-Ling Kuo, Zhiting Hu, Tomer Ullman, Antonio Torralba, Joshua Tenenbaum, and Tianmin Shu. 2024. MMToM-QA: Multimodal theory of mind question answering. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1607716102, Bangkok, Thailand. Association for Computational Linguistics. Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Bras, Gunhee Kim, Yejin Choi, and Maarten Sap. 2023. FANToM: benchmark for stress-testing machine theory of mind in interactions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1439714413, Singapore. Association for Computational Linguistics. Matthew Le, Y-Lan Boureau, and Maximilian Nickel. 2019. Revisiting the evaluation of theory of mind through question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 58725877, Hong Kong, China. Association for Computational Linguistics. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024a. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Tianjian Liu, Hongzheng Zhao, Yuheng Liu, Xingbo Wang, and Zhenhui Peng. 2024b. Compeer: generative conversational agent for proactive peer support. arXiv preprint arXiv:2407.18064. Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, and Tom Griffiths. 2018. Evaluating theory of mind in question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23922400, Brussels, Belgium. Association for Computational Linguistics. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122. Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2022. Social simulacra: Creating populated prototypes for social computing systems. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pages 118. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. David Premack and Guy Woodruff. 1978. Does the chimpanzee have theory of mind? Behavioral and brain sciences, 1(4):515526. Sahand Sabour, Siyang Liu, Zheyuan Zhang, June Liu, Jinfeng Zhou, Alvionna Sunaryo, Tatia Lee, Rada Mihalcea, and Minlie Huang. 2024. EmoBench: Evaluating the emotional intelligence of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 59866004, Bangkok, Thailand. Association for Computational Linguistics. Maarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi. 2022. Neural theory-of-mind? on the limits of social intelligence in large LMs. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 37623780, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia Tsvetkov. 2023. Minding language models (lack of) theory of mind: plug-andplay multi-character belief tracker. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1396013980, Toronto, Canada. Association for Computational Linguistics. Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. 2024. Clever hans or neural theory of mind? stress testing social reasoning in large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 22572273, St. Julians, Malta. Association for Computational Linguistics. Haojun Shi, Suyu Ye, Xinyu Fang, Chuanyang Jin, Leyla Isik, Yen-Ling Kuo, and Tianmin Shu. 2024. Muma-tom: Multi-modal multi-agent theory of mind. Preprint, arXiv:2408.12574. Daniel Stokols. 1978. Environmental psychology. Jonathan Turner. 1988. Theory of Social Interaction. Stanford University Press. Tomer Ullman. 2023. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399. Jiashuo Wang, Yang Xiao, Yanran Li, Changhe Song, Chunpu Xu, Chenhao Tan, and Wenjie Li. 2024. Towards client-centered assessment of llm therapists by client simulation. arXiv preprint arXiv:2406.12266. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Alex Wilf, Sihyun Lee, Paul Pu Liang, and LouisPhilippe Morency. 2024. Think twice: Perspectivetaking improves large language models theory-ofmind capabilities. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 82928308, Bangkok, Thailand. Association for Computational Linguistics. Yufan Wu, Yinghui He, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng. 2023. Hi-ToM: benchmark for evaluating higher-order theory of mind reasoning in large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1069110706, Singapore. Association for Computational Linguistics. Yang Xiao, Yi Cheng, Jinlan Fu, Jiashuo Wang, Wenjie Li, and Pengfei Liu. 2023. How far are we from believable ai agents? framework for evaluating the believability of human behavior simulation. arXiv preprint arXiv:2312.17115. Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Shiyang Lai, Kai Shu, Jindong Gu, Adel Bibi, Ziniu Hu, David Jurgens, James Evans, Philip Torr, Bernard Ghanem, and Guohao Li. 2024. Can large language model agents simulate human trust behavior? In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, and Yulan He. 2024. OpenToM: comprehensive benchmark for evaluating theory-of-mind reasoning capabilities of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 85938623, Bangkok, Thailand. Association for Computational Linguistics. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Se-eun Yoon, Zhankui He, Jessica Echterhoff, and Julian McAuley. 2024. Evaluating large language models as generative user simulators for conversational recommendation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 14901504, Mexico City, Mexico. Association for Computational Linguistics. Caleb Ziems, Jane Dwivedi-Yu, Yi-Chia Wang, Alon Halevy, and Diyi Yang. 2023. NormBank: knowledge bank of situational social norms. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 77567776, Toronto, Canada. Association for Computational Linguistics."
        },
        {
            "title": "Context Length",
            "content": "A.1 the candidate pool of social location The social location describes the environments where individuals live, work, and learn, which can significantly impact their mental states and behavior (Stokols, 1978). As shown in Figure 5, we have collected 13 types of social location types in total, adding up to 261 locations. A.2 the candidate pool of profile We conclude 7 aspects in the characters profile: surname, name, gender, occupation, education, race, and personality traits. Their value can be found in Figure 7, 8, 9, and 10. The source of the surname, name, and occupation statistics are U.S. Census Bureau Homepage, The United States Social Security Administration, and Bureau of Labor Statistics, respectively. Figure 6 shows an example of the social background. GPT-4o GPT-4-Turbo Llama-3.1-8B Llama-3.1-70B Mistral-7B Mixtral-8x7B Qwen2-7B Qwen2-72B DeepSeek-V2 GLM-4 2024-05-13 2024-04-09 Instruct Instruct Instruct-v0.3 Instruct-v0.1 Instruct Instruct Lite-Chat 9b-chat 8B 70B 7B 8x7B 7B 72B 16B 9B 128k 128k 128k 128k 32k 32k 128k 128k 32k 128k Table 6: The detail of models evaluated in our benchmark. A.6 Human validation of the Quality of"
        },
        {
            "title": "DYNTOM",
            "content": "We apply argilla as the annotation platform. Figure 16 shows the annotation interface for data validation."
        },
        {
            "title": "B Experiments",
            "content": "A.3 The prompt used to generate the mental B.1 Model detail We evaluate total of 10 popular LLMs, including GPT-4o, GPT-4-Turbo, Llama-3.1-8B, Llama3.1-70B, Mistral-7B, Mixtral-8x7B, Qwen2-7B, Qwen2-72B, DeepSeek-V2, GLM-4. For all the LLMs, we strictly abide by their terms and get access through official APIs or model weights. Details about model versions, parameter sizes, context window sizes and the prompts used for the two methods are shown in Table 6. B.2 Prompting methods We employ two prompting methods: the vanilla prompting which directly asks LLMs to answer the questions, and the CoT prompting that elicits stepby-step reasoning before answering. The prompts used for the two methods are shown in Figure 17. B.3 Case Study for CoT Prompting Both ToM reasoning item and question-type results in Table 3 indicate that CoT prompting doesnt always improve LLMs ToM reasoning ability. We present failure case of GPT-4o when CoT prompting is used in Figure 18. state trajectory As illustrated in Figure 11, the prompt is used to generate the mental state trajectory. In the holders of {} and [], the corresponding information will be input into this prompt. An example of the mental state trajectory is shown in Figure 12. A.4 The prompt used to generate the social scenarios As illustrated in Figure 13, the prompt is used to generate the social scenarios. In the holders of {} and [], the corresponding information will be input into this prompt. An example of the social scenario is shown in Figure 14. A.5 The templates for the four types of questions and question examples We apply four predefined question templates to the social stage to generate questions, 71 questions for every social context in total. The four types of questions are: (1) (Understanding-1) What is the main characters ToM reasoning item in specific scenario? (2) (Transformation-1) Does ToM reasoning item change from scenario to scenario B? (3) (Transformation-2) What causes ToM reasoning item change from scenario to scenario B? (4) (Transformation-3) How does the ToM reasoning item change across all the scenarios? The templates and the example of the four types of questions are shown in Figure 15. Figure 5: The candidate pool of social location. Figure 6: An example of the social context. Figure 7: The races and their corresponding 100 most popular surnames. Figure 8: The genders and their corresponding 100 most popular names. Figure 9: The genders and their corresponding 100 most popular occupations. Figure 10: The personality traits and educations. Figure 11: The prompt for the generation of the relationship between characters and the mental state trajectory. Figure 12: An example of the mental state trajectory. Figure 13: The prompt for the generation of the scenarios. Figure 14: An example of the social scenarios. Figure 15: The examples of the types of questions and templates to generate these questions. Figure 16: The platform to annotate the quality of the story. Figure 17: The prompts used for vanilla and CoT Prompting. Figure 18: case of CoT prompting on GPT-4o."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "The Hong Kong Polytechnic University"
    ]
}