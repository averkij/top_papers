{
    "paper_title": "When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing",
    "authors": [
        "Mahdi Dhaini",
        "Stephen Meisenbacher",
        "Ege Erdogan",
        "Florian Matthes",
        "Gjergji Kasneci"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the study of trustworthy Natural Language Processing (NLP), a number of important research fields have emerged, including that of \\textit{explainability} and \\textit{privacy}. While research interest in both explainable and privacy-preserving NLP has increased considerably in recent years, there remains a lack of investigation at the intersection of the two. This leaves a considerable gap in understanding of whether achieving \\textit{both} explainability and privacy is possible, or whether the two are at odds with each other. In this work, we conduct an empirical investigation into the privacy-explainability trade-off in the context of NLP, guided by the popular overarching methods of \\textit{Differential Privacy} (DP) and Post-hoc Explainability. Our findings include a view into the intricate relationship between privacy and explainability, which is formed by a number of factors, including the nature of the downstream task and choice of the text privatization and explainability method. In this, we highlight the potential for privacy and explainability to co-exist, and we summarize our findings in a collection of practical recommendations for future work at this important intersection."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 2 8 4 0 1 . 8 0 5 2 : r When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing Mahdi Dhaini, Stephen Meisenbacher, Ege Erdogan, Florian Matthes, Gjergji Kasneci Technical University of Munich School of Computation, Information and Technology Department of Computer Science Munich, Germany {mahdi.dhaini, stephen.meisenbacher, ege.erdogan, matthes, gjergji.kasneci}@tum.de Abstract In the study of trustworthy Natural Language Processing (NLP), number of important research fields have emerged, including that of explainability and privacy. While research interest in both explainable and privacy-preserving NLP has increased considerably in recent years, there remains lack of investigation at the intersection of the two. This leaves considerable gap in understanding of whether achieving both explainability and privacy is possible, or whether the two are at odds with each other. In this work, we conduct an empirical investigation into the privacy-explainability trade-off in the context of NLP, guided by the popular overarching methods of Differential Privacy (DP) and Post-hoc Explainability. Our findings include view into the intricate relationship between privacy and explainability, which is formed by number of factors, including the nature of the downstream task and choice of the text privatization and explainability method. In this, we highlight the potential for privacy and explainability to co-exist, and we summarize our findings in collection of practical recommendations for future work at this important intersection. Code https://github.com/dmah10/xpnlp"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Natural Language Processing (NLP) have seen bountiful and widespread improvements in the way natural language can be understood and generated. Such progress, hallmarked by the rapid developments enabled by Large Language Models (LLMs) and associated techniques, has powered novel applications in variety of domains including education (Wen et al. 2024), healthcare (Wang et al. 2025), and finance (Lee et al. 2025), as well as empowered non-technical users to explore the capabilities of Artificial Intelligence (Ng et al. 2021). These benefits, however, do not come for free, and various subfields of NLP currently work at the intersection of NLP and number of human-centered topics, such as explainability (Danilevsky et al. 2020), privacy (Sousa and Kern 2023), bias (Navigli, Conia, and Ross 2023), fairness (Chang, Prabhakaran, and Ordonez 2019), and sustainability (Van Wynsberghe 2021), among others. Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025) Despite the recent democratization of LLM use, persistent challenge in the deployment of any language models relates to that of explainability, which generally refers to the ability to interpret and communicate the decisions provided by model. Explainability becomes paramount to the safe deployment of models, particularly in ensuring that model inputs can (reasonably) be traced or explained. Beyond this, explainability is not only desired characteristic of language model, but also mandate (mainly for high-risk AI systems) under the recent EU AI Act regulation (Council of European Union 2024). One particularly useful candidate for fulfilling this mandate comes in the form of post-hoc explainability (Madsen et al. 2022; Danilevsky et al. 2020), which comprises methods that serve to provide insights into traditionally black-box models. In similar vein, the stark increase in LLM usage, particularly where users must interact with models hosted on external servers, i.e., in the cloud, has contributed to rising concerns of privacy (Pan et al. 2020; Wu, Duan, and Ni 2023; Gupta et al. 2023; Yan et al. 2025). Calls for privacy protection have been driven by increasingly strict data protection regulations (such as the GDPR); at the same time, privacy has been addressed in plethora of research on privacypreserving NLP (Yin and Habernal 2022), ranging from text privatization to private model training (Sousa and Kern 2023). Privacy-preserving techniques aim to mask both direct and indirect identifiers hidden within textual data, while also preserving utility for downstream tasks and applications (Mattern et al. 2022; Weggenmann et al. 2022). One popular framework is Differential Privacy (DP) (Dwork 2006), which lends plausible deniability to text inputs by ensuring some level of indistinguishability between any two texts, usually achieved via the injection of random noise into text representations (Klymenko et al. 2022). This noisification can take place on many levels, such on the word level, or alternatively, via document-level rewriting. Many recent works in Differentially Private Natural Language Processing (DP-NLP) focus on balancing the privacyutility trade-off as key indicator for the effectiveness of privatization method (Mattern et al. 2022; Utpala et al. 2023). Other works explored the trade-offs in other important aspects, such as text coherence (Weggenmann et al. 2022) or user acceptability (Meisenbacher et al. 2025). On the other hand, Explainable NLP (XNLP) has increasingly focused on the intersection of explainability with some aspects of trustworthy NLP, particularly fairness. This research has taken two primary directions: utilizing explainability as tool for detecting bias in models (T.y.s.s. et al. 2024; Gallegos et al. 2024) and evaluating the fairness of explainability methods themselves (Dhaini et al. 2025). However, specifically in the NLP domain, no works to the best of our knowledge consider the intersection of privacy and explainability in terms of privacy-explainability trade-off, namely, how the application of privatization methods affects the function of explainability methods. We argue that this consideration is crucial one, particularly with the simultaneous legal mandate for both explainability and privacy. In this work, we are the first to investigate the interplay between privacy and explainability in the context of natural language. As case study, we select two popular subfields: post-hoc feature attribution methods and differentially private text rewriting. In this, we explore the shift in explainability that can be observed when rewriting texts via DP, at various privacy levels and with fundamentally different rewriting mechanisms. We also consider the effect of downstream task specifics, such as model choice, model size, and fine-tuning task, particularly when varying the importance of privacy over explainability, and vice versa. To guide these experiments, we define one overarching research question: What is the impact of differentially private text rewriting on the post-hoc explainability of fine-tuned language models, and how can one quantify the privacyexplainability trade-off? We learn that while clear trade-off can typically be observed between privacy and explainability, there do exist configurations in which the two work synergistically. In this, we find that the factors of the downstream dataset and task, as well as the selected DP method and its privacy budget, are important in the quantification of the privacy-explainability trade-off. This leads us to create collection of recommendations for both researchers and practitioners who wish to continue work at this important intersection. Concretely, our work makes the following contributions: 1. We are the first to conduct an empirical investigation at the intersection of privacy and explainability, particularly in the context of natural language. We make our experimental code available for reproducibility. 2. We provide insights into the complex interplay between post-hoc explainability and differentially private text rewriting, serving as foundation for broader investigations at this intersection. 3. We analyze our results to propose recommendations on best practices for important design choices, particularly when faced with the need for explainability and privacy."
        },
        {
            "title": "2.1 Post-hoc explainability methods\nModel-agnostic feature-attribution post-hoc techniques\nhave gained prominence due to their broad applicability\n(Jacovi 2023). These methodologies seek to determine the",
            "content": "relative contribution of individual tokens to model predictions for specific inputs, employing either gradient-based approaches that leverage model derivatives with respect to inputs (Sundararajan et al. 2017; Simonyan, Vedaldi, and Zisserman 2013), or perturbation-based techniques (Ribeiro, Singh, and Guestrin 2016; Lundberg and Lee 2017). The expanding significance of explainable NLP research is demonstrated through the expansion of comprehensive surveys addressing NLP explainability (Wallace, Gardner, and Singh 2020; Zhao et al. 2024; Madsen et al. 2022; Zini and Awad 2022). Moreover, given the deployment of NLP systems in critical applications including education (Wu et al. 2024), healthcare (Johri et al. 2025) and legal domains (Valvoda and Cotterell 2024) where interpretability requirements are essential, specialized surveys have emerged focusing on explainability within particular NLP applications, such as fact verification (Kotonya and Toni 2020), and specific methodological approaches in NLP explainability (Mosca et al. 2022). These comprehensive reviews underscore the extensive utilization of post-hoc methodologies across NLP applications. Additionally, feature-attribution post-hoc explanation techniques serve as primary methods within explainability platforms and computational frameworks documented in recent literature (Arras, Osman, and Samek 2022; Li et al. 2023; Attanasio et al. 2023; Sarti et al. 2023). These comprehensive frameworks characteristically integrate multiple post-hoc explanation algorithms while accommodating various data modalities Machine Learning (ML) architectures, including pre-trained Language Models (PLMs)."
        },
        {
            "title": "2.2 Differential Privacy in NLP",
            "content": "The notion of Differential Privacy was first proposed in the context of relational databases (Dwork 2006), where the primary goal was to protect the participation of an individual in the dataset. More specifically, privacy preservation occurs in the sense that information about such an individual cannot be accurately inferred within some bound. This is formalized via the following inequality, for any databases D1 and D2 differing in exactly one element, any ε > 0, any computation or function M, and all Range(M): r[M(D1)S] r[M(D2)S] eε. Intuitively, DP ensures that there exists some level of indistinguishability between any two neighboring databases (differing in one element), thus protecting the individual. This privacy level is governed by the ε parameter, also known as the privacy budget. This form of DP is known as ε-DP, and the notion above refers to global DP. Another notion, which we focus on in this work, is that of local DP (LDP) (Kasiviswanathan et al. 2008). In the local setting, we assume that the central curator, i.e., the one who is to possess the complete dataset, is not trusted. As solution, DP is ensured at the user level; however, since the entirety of the dataset is not yet known, LDP imposes much stricter indistinguishability requirement, i.e., between any potential neighbor. This differs from the global notion, since neighboring databases only refer to those resulting from the dataset D. Formally, for finite space and V, and for all x, and all V: r[M(x)=y] r[M(x)=y] eε Hence, an observed output cannot be attributed to specific input with high probability. While this notion is clearly stricter, it allows for quantification of privacy guarantee on the local, single datapoint level without the need for an aggregated dataset. The translation of DP into the realm of NLP initially brought about numerous research challenges (Klymenko et al. 2022), chief among them the reasoning of who the individual is when considering textual data, and how to quantify neighboring datasets. Despite these challenges, great deal of recent works have proposed innovative methods for the integration of DP into the NLP pipeline (Hu et al. 2024), ranging for text anonymization and obfuscation to DP training of language models. Particularly considering text privatization, many recent methods interpret the task from the rewriting perspective, where sensitive input text is rewritten under DP guarantees to produce private output text. Such methods operate at various lexical levels, including the word level (Feyisetan et al. 2020; Carvalho et al. 2023), the token-level during language model generation (Utpala et al. 2023; Meisenbacher et al. 2024), and on full texts (Igamberdiev and Habernal 2023). Considering the differences in how these mechanisms operate, it becomes important to consider the nature of the DP guarantee when performing comparative analyses (Vu et al. 2024). Recent works in DP-NLP highlight persistent challenges, such as the generation of coherent and correct outputs (Mattern et al. 2022), ensuring comparability (Igamberdiev et al. 2022), and quantifying the benefit DP brings over non-DP methods (Meisenbacher and Matthes 2024). However, to the best of our knowledge, no existing works question the effect of DP methods on the explainability of texts, or more specifically, of models trained on privatized texts. We see this to be considerable gap, particularly for DP text privatization in important domains, such as the medical domain, where explainability is also important in conjunction with privacy."
        },
        {
            "title": "2.3 Privacy meets Explainability",
            "content": "Privacy in Explainable ML. The privacy implications of explainable ML have received attention in recent years but remain underexplored. Most of the work on the intersection of explainability and privacy in ML focuses on the inherent privacy risks in explanations. These works include studies that investigate how model explanations reveal sensitive information from the training data and how this can be exploited using membership inference attacks. For instance, Shokri et al. (2021) demonstrated that variance in backpropagation-based explanations can reveal whether data point was used during training, exposing membership information. Duddu and Boutet (2022) extended these concerns by inferring sensitive attributes like gender and race directly from model explanations, while Liu et al. (2024) shows how explanations can amplify membership inference risks by exploiting differences in model robustness under attribution-guided perturbations. Similarly, Luo et al. (2022) showed that private input features can be reconstructed using Shapley-value explanations. Other works include highlighting how explanations can enhance model inversion attacks when used as auxiliary inputs (Zhao et al. 2021), and introducing membership inference attack for counterfactuals relying on distances between original inputs and their counterfactual counterparts (Pawelczyk et al. 2023). Shokri et al. (2021) investigated the privacy risks of gradient-based explanation methodsGradient (Simonyan et al. 2013) and Integrated Gradients (Sundararajan et al. 2017)across four tabular and two image datasets. They showed that these methods can leak information about training examples, increasing vulnerability to membership inference attacks that use full feature attributions as inputs to an attack model. In contrast, perturbation-based methods such as LIME and SmoothGrad (Smilkov et al. 2017) were found to be more resistant, likely due to their reliance on input perturbations rather than gradients, which may capture subtle distinctions between training and non-training points. Liu et al. (2024) further examined membership inference risks for multiple post-hoc explainers in the context of image data. Defenses and countermeasures. to protect again inference attacks, some works in the literature used approaches that alter the training process such as DP-SGD (Liu et al. 2024) or approaches that perturb confidence scores of the target models output for each input by adding noise and then convert the perturbed confidence scores into adversarial examples for the attack models, such as MemGuard (Jia et al. 2019). However such approaches either prove to be ineffective against inference attacks (such as MemGuard) (Liu et al. 2024) or decrease the inference attack performance in the expense of severe degradation of the model utility as well as explanation quality such as DP-SGD and thus presenting large trade-off between defense capability and utility and explainability performance (Liu et al. 2024). Privacy in Explainable NLP. Prior work has primarily examined the privacy risks of model explanations in the context of tabular and image-based datasets. However, the intersection of privacy and explainability in NLP remains largely unexplored. In contrast to these studies, our work focuses specifically on textual datasets and NLP applications. Rather than analyzing privacy leakage from explanations, we empirically investigate whether data-level DP can be applied to achieve reasonable privacy guarantees while maintaining acceptable trade-offs in model utility and explanation quality. To the best of our knowledge, this is the first study to systematically evaluate the trade-off between data-level DP and the quality of post-hoc feature attribution explanations (in terms of their faithfulness) in NLP models."
        },
        {
            "title": "3.1 Datasets\nTo guide our experiments, we select three datasets, which\nvary in size and domain. These datasets are introduced in\nthe following, alongside their associated downstream tasks.",
            "content": "SST-2. The Stanford Sentiment Treebank dataset (SST-2) (Socher et al. 2013) comprises short texts originating from movie reviews, and it was popularized due to its inclusion in the GLUE benchmark (Wang et al. 2018). Each text is labeled according to its sentiment, i.e., either positive or negative, creating two-class binary classification task. We use the complete training split of the dataset, composed of 67,349 records, with an average word length of 9.41. AG News. The AG News corpus contains over one million news articles from the over 2000 news sources. We utilize the subset as prepared by Zhang, Zhao, and LeCun (2015), which contains 120k news articles from four news domains: world, sports, business, and sci/tech. We take 50% random sample (seed=42) for final dataset of 60000 news articles, with an average word length of 43.90. Trustpilot Reviews. The Trustpilot corpus is large-scale collection of user reviews. The corpus prepared by Hovy, Johannsen, and Søgaard (2015) tags each review with the stars provided (1-5), which we simplify to negative reviews (12 stars) and positive reviews (5 stars). We specifically only take reviews from the US split of the dataset (en-US), and use 10% random sample. This results in dataset of 29,490 reviews, with an average word length of 59.75."
        },
        {
            "title": "3.2 DP Methods",
            "content": "We select three local DP text rewriting methods for our experiments, which are introduced in the following. TEM. TEM (Carvalho et al. 2023) is word-level DP mechanism which leverages generalized notion called metric DP. This generalization is useful for metric spaces, such as with word embeddings, wherein the indistinguishability requirement between words is scaled by their distance in the space. TEM improves upon previous approaches by employing truncated exponential mechanism, allowing for higher utility word replacements. To privatize complete text, each component word is privatized one-by-one. Following the original work, we choose the privacy budgets of ε {1, 2, 3}, which refer to the budgets per word. DP-Prompt. Leveraging the generative capabilities of LLMs, Utpala et al. (2023) introduce DP-PROMPT, method for producing private outputs texts with local DP guarantees by modeling privatization as paraphrasing task. Internally, the DP-PROMPT mechanism operates by applying DP at each token generation, specifically by using the temperature parameter as an equivalent DP mechanism to the Exponential Mechanism (McSherry and Talwar 2007). Following the original work, we test three temperature values, {1.75, 1.5, 1.25}. This translates to the per-token ε values of ε {118, 137, 165}, using the implementation provided by Meisenbacher et al. (2024), which employs FLAN-T5-BASE model (Chung et al. 2022) as the underlying privatization model. DP-BART. DP-BART is document-level LDP text rewriting mechanism proposed by Igamberdiev and Habernal (2023). It leverages pretrained BART model (Lewis et al. 2020), applying calibrated Gaussian noise in the latent representation space, where the decoder then decodes the noisy encoded vector to generate private text. In doing so, DP-BART rewrites texts with single document guarantee, albeit usually requiring higher privacy budgets for meaningful output. As such we choose ε {500, 1000, 1500}, and we use the original DP-BART-CLV variant. Privatization Procedure. For DP-BART, we use the mechanism on the primary text column of each of our chosen datasets. This is repeated for each of the three privacy budgets, creating three private counterparts to the original data. Using DP-PROMPT likewise produces full texts as result of the generative process. Finally, TEM is run sequentially on all component words of an input text, which are tokenized using the NLTK.WORD TOKENIZE function. The list of outputs from the mechanism are then reconstructed into single string via simple concatenation. In total, we thus produce nine private variants of the original datasets, for total of 30 datasets (3 original baselines + 27 private counterparts). note on privatization and comparability. We caution that in the selection of privacy budgets for each DP mechanism, and for the resulting datasets based on these decisions, we do not ensure any comparability between the three selected methods. Due to the different manners in which DP is ensured across these three methods, as well as the intricacies involved with comparing different DP notions (e.g., MDP vs DP), we out-scope such comparisons. Instead, we focus on analyzing the downstream effects on DP rewriting within each method (i.e., across privacy budgets). We also note that the choice of privacy budgets (ε values) are motivated by the choices taken by the authors of the original works. We do not work to normalize these values, i.e., by normalizing the logit values used in DP-PROMPT. While this would be useful step in reporting more usable and reasonable privacy budgets, we endeavor to test the DP methods as presented originally. As such, we do not report on or analyze the relative strength of underlying DP guarantee, as this is an active area of DP-NLP research."
        },
        {
            "title": "3.3 Models\nWe utilize a number of pretrained encoder-only language\nmodels, which varying in architecture and model size. In\nthis, we empirically measure the effect of fine-tuning on DP\nrewritten datasets, namely, the downstream effect of this pro-\ncess on the explainability of these models (measured by our\nchosen metrics). Table 1 presents the information and details\nof the pretrained models used in our experiments to fine-tune\non our three chosen classification datasets.",
            "content": "Name Type # of Parameters BERT-BASE-CASED BERT-LARGE-CASED ROBERTA-BASE ROBERTA-LARGE DEBERTA-BASE Encoder-only 110M Encoder-only 335M Encoder-only 125M Encoder-only 355M Encoder-only 139M Table 1: Details of the experimental models."
        },
        {
            "title": "3.4 Explainability Methods\nWe include four post-hoc feature attribution methods in our\nexperiments: Gradient involves computing the gradient of",
            "content": "the output with respect to the input features; Integrated Gradient instead integrates the gradients over path from baseline input to the explained input; SHAP (Lundberg and Lee 2017) approximates the Shapley value of each feature, concept from cooperative game theory that measures the contribution of each feature by considering different coalitions of features and how much each feature contributes to the outcome; LIME (Ribeiro, Singh, and Guestrin 2016) attempts to replicate the models behavior locally around the explained input with linear model that is easy to explain."
        },
        {
            "title": "3.5 Evaluating Explanations\nWe evaluate the post-hoc explanations using two different\nmethods each of measuring the comprehensiveness and\nsufficiency of explanations. Both metrics effectively attempt\nto quantify the faithfulness of explanations, i.e. how accu-\nrately they reflect the underlying decision process of a model\n(Jacovi and Goldberg 2020). Faithfulness is one of the most\nimportant desideratum for explanations (Danilevsky et al.\n2020; Lyu et al. 2024) as high faithfulness indicates that the\nexplanation accurately reflects the model’s decision-making\nprocess for a given prediction.",
            "content": "In their simplest forms, comprehensiveness metrics measure the change in output probabilities for the true class when the top-k tokens with respect to the feature attribution scores are removed from the input, and sufficiency metrics measure the change when only the top-k tokens are given as input to the model. We then compute the AOPC by averaging the values by is varied. We denote these metrics AOPC-Comprehensiveness and AOPC-Sufficiency. tokens might Hard-removing the lead to out-ofdistribution inputs for the model, which could adversely affect model performance during the evaluation of the explanations (Zhao et al. 2022; Chrysostomou and Aletras 2022). To address this potential problem, we also include Soft Sufficiency and Soft Comprehensiveness metrics (Zhao and Aletras 2023) in our evaluations. For the soft versions of these metrics, rather than removing token entirely, fraction of each tokens embeddings is masked based on that tokens importance score, i.e. the masked token is computed as = where ei Bernoulli(q) with being the importance score if the token is kept (sufficiency) and one minus the score if it is removed (comprehensiveness)."
        },
        {
            "title": "3.6 Composite Score\nUnderstanding that\nthe trade-off between privacy and\nexplainability is not always equally weighted, we de-\nsign a metric that allows one to shift\nthe importance\nof either utility or explainability, while still viewing\nboth in the same light. We compute a composite score\nfor each model m and weight α ∈ {0.25, 0.5, 0.75}\nas CS(m, α) = α (cid:99)F1(m) + (1 − α) (cid:98)E(m), where\n(cid:98)Cs(m)+(1− (cid:98)Ss(m))+ (cid:98)Ca(m)+(1− (cid:98)Sa(m))(cid:1),\n(cid:98)E(m) = 1\n4\nand (cid:98)x denotes min–max normalization of x, subscripts s\nand a indicate “soft” vs. “AOPC,” C is comprehensiveness,\nand S is sufficiency (inverted via 1 − (cid:98)S so that larger values\nare always better).",
            "content": "(cid:0) CS(m, α) = α ˆF 1 + (1 α) ˆE ˆE = 1 4 (cid:0) ˆCs + (1 ˆSs) + ˆCa + (1 ˆSa)(cid:1) The composite score ranges from 0 to 1, where higher values (closer to 1) indicate near 1 means better overall performance (based on the selected α value). We intentionally define the score in this normalized form to enable straightforward and consistent comparison across our experiments. Additionally, with this score, one can vary α based on the relative weight between utility and explainability. For example, an α of 0.25 would imply that the explainability faithfulness metrics are considerably more important, as opposed to value of 0.75 where the utility score (F1) takes precedence."
        },
        {
            "title": "4 Results and Analysis",
            "content": "Our results are presented across several tables and figures. Tables 2, 3, and 4 report composite scores (meanstd) for α = 0.25, 0.5, and 0.75, respectively. Scores are averaged over five PLMs (BERT-BASE, BERT-LARGE, ROBERTABASE, ROBERTA-LARGE, and DEBERTA-BASE), and the Avg row represents the mean and pooled standard deviation across the four explainers (Gradient, IG, LIME, and SHAP) for each (dataset, DP-ε) pair. In each row, the highest composite score is underlined. Columns are grouped by DP method (None, DPB, DPP, and TEM) and colorcoded for comparison: grey for no DP, cyan for DPB, orange for DPP, and green for TEM. Color intensity reflects score magnitude, with darker shades indicating higher values. For instance, in Table 2, the most saturated cell in the TEM columns highlights the top-scoring explainerDP-ε combination (e.g., TEM3 with SHAP) for given dataset and α value (e.g., AG News, α = 0.25). This distinction in coloring is used to reflect the fact that DP methods are not directly comparable due to differences in mechanisms and assumptions (see Section 3.2). These tables support analysis of trends across explainers and privacy configurations. To better visualize the results from Tables 2, 3, and 4 for each of the three datasets, we present corresponding plots in Figures 1a, 1b, and 1c. Each figure corresponds to one dataset, and each data point represents the composite score of (DP-ε, explainer, α) triplet, averaged over the five PLMs. In these plots, different shapes indicate different explainers, while colors denote the α values (0.25, 0.5, 0.75) where explainers are acronymed as follows: (G:Gradient, IG:Integrated Gradient, L:LIME, S:SHAP). In Table 5, we present composite score values, but we consolidate the composite score values averaged over the four explainers (instead of per explainer as done previously) for each (dataset, α, DP-ε) triplet thus allowing us to compare and evaluate how each DP-ε combination affect the composite scores over all the explainers allowing us to detect the sweet spots for each α value/scenario in each dataset. In Table 6, we investigate the effect of model size on the trade-off between privacy, utility, and explainability; in particular, whether this trade-off differs between smaller and larger models. To this end, we average results over the four explainers, but instead of aggregating across all five models, we group the results by model size. The base group includes BERT-BASE and ROBERTA-BASE, while the large group includes their corresponding larger variants. Dataset Expl None DPB500 DPB1000 DPB1500 DPP118 DPP137 DPP TEM1 TEM2 TEM3 AG News SST2 Trustpilot 0.7050.19 0.2860.14 0.6550.01 0.7050.01 0.7580.01 0.7710.02 0.7690.02 0.3330.22 0.5980.17 0.7770.03 0.6000.13 0.2450.11 0.5260.02 0.5770.01 0.6100.01 0.6200.02 0.6170.02 0.2720.14 0.4620.11 0.6320.02 IG LIME 0.7800.24 0.3380.19 0.7180.02 0.7600.02 0.8140.02 0.8370.01 0.8230.02 0.3910.27 0.7020.19 0.8820.02 SHAP 0.7650.23 0.3330.19 0.7180.01 0.7510.01 0.8180.02 0.8320.03 0.8230.03 0.3780.26 0.6790.19 0.8530.03 0.7130.20 0.3010.16 0.6540.01 0.6980.01 0.7500.01 0.7650.02 0.7580.02 0.3440.22 0.6100.17 0.7860.03 Avg 0.5150.18 0.2400.05 0.3580.14 0.4410.13 0.4890.16 0.4770.15 0.4750.16 0.2390.06 0.2770.09 0.3960.18 0.4100.13 0.2030.04 0.2870.10 0.3580.10 0.3900.12 0.3750.11 0.3820.11 0.2170.05 0.2460.07 0.3250.13 IG LIME 0.5730.22 0.2140.08 0.3750.18 0.4830.17 0.5370.20 0.5210.19 0.5200.19 0.2370.11 0.2860.15 0.4300.25 SHAP 0.5670.22 0.2150.07 0.3740.19 0.4840.17 0.5350.20 0.5230.19 0.5180.18 0.2350.11 0.2850.16 0.4230.25 0.5160.19 0.2180.06 0.3490.15 0.4420.14 0.4880.17 0.4740.16 0.4740.16 0.2320.08 0.2740.12 0.3940.20 Avg 0.5060.17 0.4210.11 0.4890.17 0.5310.12 0.4910.14 0.4870.08 0.4830.06 0.3210.14 0.3180.10 0.5040.16 0.4880.16 0.4200.10 0.4770.17 0.5130.12 0.4580.13 0.4560.07 0.4510.05 0.3200.14 0.3110.10 0.4800.15 IG LIME 0.5580.20 0.4510.12 0.5210.18 0.5660.13 0.5390.17 0.5420.10 0.5370.06 0.3430.19 0.3560.15 0.5550.19 SHAP 0.5510.19 0.4430.12 0.5170.18 0.5580.13 0.5340.17 0.5300.09 0.5340.07 0.3250.19 0.3400.14 0.5510.19 0.5260.18 0.4340.11 0.5010.18 0.5420.13 0.5060.15 0.5040.09 0.5010.06 0.3270.16 0.3310.12 0.5230.17 Avg Table 2: Composite Scores (meanstd) with α = 0.25 averaged over the five PLMs. Avg refers to the mean and pooled standard deviation over the four explainers. Dataset Expl None DPB500 DPB1000 DPB1500 DPP118 DPP137 DPP TEM1 TEM2 TEM3 AG News SST2 Trustpilot 0.7660.13 0.2970.19 0.7100.01 0.7600.01 0.8110.00 0.8200.01 0.8180.01 0.3430.26 0.6230.19 0.8280.02 0.6960.09 0.2700.17 0.6240.01 0.6740.01 0.7120.01 0.7190.02 0.7170.01 0.3010.21 0.5330.15 0.7310.02 IG LIME 0.8160.16 0.3320.23 0.7530.01 0.7970.01 0.8480.01 0.8640.01 0.8540.01 0.3810.29 0.6930.20 0.8980.02 SHAP 0.8060.15 0.3290.22 0.7520.01 0.7910.01 0.8510.01 0.8610.02 0.8540.02 0.3720.29 0.6780.20 0.8780.02 0.7710.13 0.3070.20 0.7100.01 0.7560.01 0.8060.01 0.8160.01 0.8110.01 0.3490.26 0.6320.19 0.8340.02 Avg 0.5920.21 0.2940.09 0.4390.19 0.5410.17 0.5910.20 0.5830.20 0.5800.20 0.2930.10 0.3420.13 0.4790.23 0.5220.19 0.2690.08 0.3910.16 0.4860.15 0.5250.17 0.5150.16 0.5180.17 0.2780.09 0.3220.12 0.4320.19 IG LIME 0.6300.24 0.2770.10 0.4500.21 0.5690.20 0.6230.23 0.6130.22 0.6100.22 0.2920.13 0.3490.17 0.5010.27 SHAP 0.6270.24 0.2770.10 0.4490.22 0.5700.20 0.6220.23 0.6140.22 0.6090.22 0.2900.13 0.3470.18 0.4970.28 0.5930.22 0.2790.09 0.4320.20 0.5420.18 0.5900.21 0.5810.20 0.5790.20 0.2880.11 0.3400.15 0.4770.24 Avg 0.5790.20 0.5010.11 0.6000.18 0.6710.08 0.6040.17 0.6280.06 0.6340.04 0.4070.14 0.4190.13 0.6260.19 0.5670.19 0.5000.11 0.5920.18 0.6590.08 0.5810.16 0.6080.05 0.6130.04 0.4060.14 0.4150.13 0.6100.18 IG LIME 0.6140.21 0.5210.12 0.6210.19 0.6950.09 0.6360.19 0.6650.07 0.6700.04 0.4220.18 0.4450.16 0.6600.21 SHAP 0.6100.21 0.5150.12 0.6190.19 0.6890.09 0.6320.19 0.6570.07 0.6680.05 0.4090.17 0.4330.15 0.6580.21 0.5930.20 0.5090.12 0.6080.18 0.6790.09 0.6130.18 0.6400.06 0.6460.04 0.4110.16 0.4280.14 0.6390.19 Avg Table 3: Composite Scores (meanstd) for α = 0.5 averaged over five PLMs. Avg refers to the mean and pooled standard deviation over the four explainers. Dataset Expl None DPB500 DPB1000 DPB1500 DPP118 DPP137 DPP TEM1 TEM2 TEM3 AG News SST2 Trustpilot 0.8270.09 0.3080.24 0.7660.00 0.8150.00 0.8630.00 0.8690.01 0.8680.01 0.3520.30 0.6490.21 0.8780.01 0.7920.08 0.2950.23 0.7230.01 0.7720.01 0.8140.00 0.8180.01 0.8170.00 0.3310.28 0.6040.19 0.8300.01 IG LIME 0.8520.10 0.3260.26 0.7870.01 0.8340.01 0.8820.01 0.8910.01 0.8860.01 0.3710.32 0.6840.22 0.9130.01 SHAP 0.8470.09 0.3240.26 0.7870.00 0.8300.01 0.8830.01 0.8890.01 0.8860.01 0.3670.32 0.6760.21 0.9040.01 0.8300.09 0.3130.25 0.7660.01 0.8130.01 0.8610.01 0.8670.01 0.8640.01 0.3550.30 0.6530.21 0.8810.01 Avg 0.6690.25 0.3470.12 0.5200.23 0.6410.21 0.6930.24 0.6890.24 0.6850.24 0.3470.14 0.4080.17 0.5620.28 0.6340.24 0.3350.12 0.4960.22 0.6130.20 0.6600.23 0.6560.22 0.6540.22 0.3390.13 0.3970.16 0.5380.26 IG LIME 0.6880.26 0.3390.13 0.5250.25 0.6550.22 0.7090.25 0.7040.25 0.7000.25 0.3460.15 0.4110.19 0.5730.30 SHAP 0.6860.26 0.3390.13 0.5250.25 0.6550.22 0.7080.25 0.7050.25 0.6990.25 0.3450.15 0.4100.19 0.5710.30 0.6690.25 0.3400.12 0.5170.24 0.6410.21 0.6930.24 0.6890.24 0.6850.24 0.3440.14 0.4070.18 0.5610.28 Avg 0.6530.24 0.5810.12 0.7110.20 0.8120.04 0.7160.20 0.7700.04 0.7860.03 0.4920.15 0.5200.16 0.7480.22 0.6470.23 0.5800.12 0.7070.19 0.8060.04 0.7050.19 0.7590.04 0.7750.03 0.4920.15 0.5180.16 0.7400.21 IG LIME 0.6700.24 0.5910.13 0.7220.20 0.8240.05 0.7320.21 0.7880.05 0.8040.03 0.5000.17 0.5330.18 0.7650.23 SHAP 0.6680.24 0.5880.13 0.7210.20 0.8210.05 0.7310.21 0.7840.04 0.8030.03 0.4940.16 0.5270.17 0.7640.22 0.6600.24 0.5850.12 0.7150.20 0.8160.04 0.7210.20 0.7750.04 0.7920.03 0.4950.16 0.5250.17 0.7540.22 Avg Table 4: Composite Scores (meanstd) for α = 0.75 averaged over five PLMs. Avg refers to the mean and pooled standard deviation over the four explainers."
        },
        {
            "title": "4.1 Composite Scores for Each Explainer Across",
            "content": "DP-Methods and Datasets Tables 2, 3, and 4 facilitates checking, per each α values and dataset, and for each explainer, the DP-ε method that provide the highest composite score, as well as for each DPmethod, the ε values that leads to the highest score for specific DP method. In addition, we look at the plots in Figure 1 and based on our results from the composite score plots presented there for SST2, AG News, and Trustpilot, evaluated across different DP mechanisms (ε), four explainability methods, and utilityexplanation trade-offs (α = 25%, 50%, Dataset Expl α None DPB500 DPB1000 DPB1500 DPP118 DPP137 DPP165 TEM TEM2 TEM3 AG News SST2 Trustpilot Avg Avg Avg Avg Avg Avg Avg Avg Avg 0.25 0.7130.20 0.3010.16 0.6540.01 0.6980.01 0.7500.01 0.7650.02 0.7580.02 0.3440.22 0.6100.17 0.7860.03 0.50 0.7710.13 0.3070.20 0.7100.01 0.7560.01 0.8060.01 0.8160.01 0.8110.01 0.3490.26 0.6320.19 0.8340.02 0.75 0.8300.09 0.3130.25 0.7660.01 0.8130.01 0.8610.01 0.8670.01 0.8640.01 0.3550.30 0.6530.21 0.8810.01 0.25 0.5160.19 0.2180.06 0.3490.15 0.4420.14 0.4880.17 0.4740.16 0.4740.16 0.2320.08 0.2740.12 0.3940.20 0.50 0.5930.22 0.2790.09 0.4320.20 0.5420.18 0.5900.21 0.5810.20 0.5790.20 0.2880.11 0.3400.15 0.4770.24 0.75 0.6690.25 0.3400.12 0.5170.24 0.6410.21 0.6930.24 0.6890.24 0.6850.24 0.3440.14 0.4070.18 0.5610.28 0.25 0.5260.18 0.4340.11 0.5010.18 0.5420.13 0.5060.15 0.5040.09 0.5010.06 0.3270.16 0.3310.12 0.5230.17 0.50 0.5930.20 0.5090.12 0.6080.18 0.6790.09 0.6130.18 0.6400.06 0.6460.04 0.4110.16 0.4280.14 0.6390.19 0.75 0.6600.24 0.5850.12 0.7150.20 0.8160.04 0.7210.20 0.7750.04 0.7920.03 0.4950.16 0.5250.17 0.7540.22 Table 5: Consolidated Composite Scores (meanstd) for three α values, showing the average over four explainers. α Dataset Base Large (LargeBase) 0. 0.50 0.75 AG News SST2 Trustpilot AG News SST2 Trustpilot AG News SST2 Trustpilot 0.6900.15 0.4610.13 0.4990. 0.7360.14 0.5680.13 0.6210.11 0.7820.13 0.6760.14 0.7430.12 0.5660.25 0.2730.10 0.3800.09 0.6040.29 0.3310.12 0.4790.12 0.6430.33 0.3900.14 0.5780.16 0.125 0.188 0. 0.132 0.237 0.142 0.140 0.286 0.165 Table 6: Comparison of average composite scores (meanstd) between base and large models, averaged across DP-ϵ values for each dataset and α. vacy and both utility and explanation quality. SST2 appears to be highly sensitive to DP noise, exhibiting considerable performance degradation under low privacy budgets. and IG are especially affected in these settings, whereas LIME and SHAP demonstrate more stable performance, even under moderate DP configurations such as DPB1000 and DP-PROMPT. Among the TEM methods, TEM3 shows partial recovery in performance, especially when greater weight is given to utility (α high). On the other hand, AG News displays stronger resilience to the effects of DP noise compared to SST2. LIME and SHAP maintain superior performance across most DP levels, with IG improving noticeably as ε increases. In particular, IG becomes competitive in configurations such as DPPROMPT and TEM3. The TEM3 variant performs especially well at (α = 75%), indicating favorable trade-off between utility and explanation quality. Trustpilot demonstrates intermediate sensitivity to DP noise, falling between SST2 and AG News. Composite scores improve progressively with increasing ε, and LIME remains the most effective explainer across the majority of settings. DP-BART and DP-PROMPT mechanisms, particularly with moderate to high privacy budgets, offer reliable performance in maintaining explanation quality. Cross-Dataset Trends. Across all datasets, LIME and SHAP consistently outperform IG and Gradient, which are more affected by strong DP. Gradient is the most negatively impacted explainer, particularly in low-ε regimes and on the SST2 dataset. Among the DP mechanisms, DP-BART- (a) SST2 (b) AG News (c) Trustpilot Figure 1: Composite Score by DP-ε, explainer, α, dataset over 5 models. 75%), we can report the following: General Observations. Higher α values, which place greater emphasis on model utility, consistently result in higher composite scores across all datasets and DP configurations. Among the explainers, LIME and SHAP generally outperform Gradient and IG in the composite scores, particularly under strong privacy constraints. As expected, composite scores decline significantly in stricter DP settings (e.g., DPB500, TEM1), reflecting the trade-off between pri1500 and DP-PROMPT-165 offer favorable trade-offs between privacy, utility, and explainability. While TEM1 and TEM2 generally result in considerable degradation, TEM3 performs better, especially for AG News and Trustpilot. Privacy Tier None Small ε Medium ε Large ε α = 0.25 α = 0.50 α = 0.75 ExplData Data(avg) ExplData Data(avg) ExplData Data(avg) LIMEAG News 0.780 SHAPAG News 0.818 LIMEAG News 0.837 LIMEAG News 0. AG News 0.713 Trustpilot 0.434 AG News 0.654 AG News 0.786 LIMEAG News 0.816 SHAPAG News 0.851 LIMEAG News 0.864 LIMEAG News 0.898 AG News 0.771 Trustpilot 0.509 AG News 0.710 AG News 0.756 LIMEAG News 0.852 SHAPAG News 0.883 LIMEAG News 0.891 LIMEAG News 0.913 AG News 0.830 Trustpilot 0.585 AG News 0.766 AG News 0.813 Table 7: Sweet spots summary across privacy tiers and α values. Each row corresponds to privacy tier (None = no DP; Small/Medium/Large ε = decreasing privacy). For each α, the left column shows the best ExplainerDataset pair (from Tables 13), and the right column shows the best Dataset based on explainer-averaged scores (from Table 5). Epsilon levels group as: Small (DPB500, DPP118, TEM1), Medium (DPB1000, DPP137, TEM2), and Large (DPB1500, DPP165, TEM3)."
        },
        {
            "title": "4.2 Comparison Over All Explainers Across",
            "content": "DP-Methods and Datasets Table 5 displays the average composite scores across the four explainers for each combination of dataset, α value, and DP method (DP-ε), averaged over the five employed PLMs. We draw the following results and insights. Effect of the UtilityExplanation Trade-Off (α). Across all datasets and DP setups, higher values of α (i.e., greater weight placed on utility) consistently yield higher composite scores. This is especially visible in the blue curves (α = 0.75), which dominate across most configurations. This trend reflects the stabilizing role of utility in the composite score, particularly under privacy-induced degradation. Comparing DP Mechanisms. Among the DP strategies, DP-BART and DP-PROMPT (particularly at higher ε values such as 1500 and 165) achieve the highest average composite scores, especially for AG News and Trustpilot. In contrast, the TEM methods (T1 and T2) result in substantial performance drop, most notably for SST2. As expected, DPB500 (strongest privacy budget) leads to the lowest performance across all datasets and α values, confirming the cost of tight privacy constraints. Dataset Sensitivity. SST2 consistently produces the lowest composite scores, reflecting its greater vulnerability to privacy-preserving perturbations. This could be due to shorter input lengths or more subtle semantic cues required for sentiment analysis. In contrast, AG News emerges as the most robust dataset, maintaining high composite scores even under moderate DP conditions. Trustpilot performs moderately well and demonstrates steady recovery with increasing ε, particularly under DP-BART and DP-PROMPT. PrivacyPerformance Trade-Off. Overall, there is clear upward trend in composite scores with increasing ε. This pattern is most pronounced in the DP-BART and DPPROMPT curves, where performance improves steadily from ε = 500 to ε = 1500/165. This monotonic behavior confirms the expected trade-off: as privacy constraints are relaxed, utility and explanation quality are jointly improved. 4.3 Identifying Sweet Spots for Privacy-Utility-Explainability We construct Table 7 using the results form Tables 1,2,3 and 4, where we identify and present the sweet spots based on the composite scores across privacy tiers and alpha values settings for two cases: explainer-dataset pairs (from Table 1,2,3) and the datasets with the highest average scores over the four explainers (From Table 3). Based on these result in Table 7, we observe the following: ExplainerDataset Consistency. Across all privacy tiers, LIMEAG News emerges as the top explainerdataset pair in every case except the strictest privacy setting (ε small), where SHAPAG News slightly outperforms it. This suggests that SHAPs token-level importance is more robust under heavy DP noise, but LIME generally provides the most faithful and useful explanations once privacy is relaxed. Dataset-Level Robustness. When averaging across explainers, AG News consistently wins for no DP, medium privacy, and low privacy budgets. Under the tightest DP budget (small ε), however, Trustpilot takes the lead for all α values, indicating that explanations on Trustpilot degrade less, on average, under stringent privacy constraints. Effect of Increasing ε. As ε increases (privacy is relaxed), composite scores rise monotonically for both the best explainerdataset pair and the dataset-average. The largest recovery occurs between small and medium ε, especially for SHAP on AG News, highlighting that moderate privacy budget recovers most of the lost explanation signal. Utility-Explainability Trade-Off (α) Trends. Increasing α from 0.25 (explanation-focused) to 0.50 (balanced) to 0.75 (utility-focused) uniformly boosts all composite scores, since more weight is placed on classification F1. Notably, the ranking of sweet spots remains stable: LIMEAG News dominates once ε medium values, and Trustpilot remains the dataset of choice under the smallest ε. it Although high composite scores are expected with large ε values, is noteworthy that even with medium and small ε (i.e., stricter privacy), some explainers still achieve strong performance. These results are promising, demonstrating that explainability can be achieved under tighter privacy constraints. For example, SHAP achieves this across all three α values representing different utilityexplanation trade-offs. This indicates that for certain combinations of dataset, explainer, and DP method, it is possible to attain high privacy while still maintaining strong utility and explanation quality, depending on the specific objective (i.e., lower or higher α)."
        },
        {
            "title": "4.4 Comparison Across Model Size\nIn Table 6, we investigate the effect of model size on the\ntrade-off between privacy, utility, and explainability; in par-",
            "content": "ticular, whether this trade-off differs between smaller and larger models. To this end, we average results over the four explainers, but instead of aggregating across all five models (as in the previous tables), we group the results by model size. The base group includes BERT-BASE and ROBERTABASE, while the large group includes their corresponding larger variants. We exclude DeBERTa from this analysis, as we dont employ DeBERTa-large in our experiments. We consider this comparison between base and large variants of BERT and RoBERTa sufficient for preliminary assessment of model size effects. Table 6 presents comparison of (average across explainers) composite scores (meanstd) between base and large models, averaged across DP-ϵ values for each dataset and α. We learn the following: Base Models Outperform Large Models. Across all datasets and α values, base models consistently outperform large models in terms of composite score. This is evident from the uniformly negative (Large Base) values, ranging from 0.119 to 0.286. The results suggest that increasing model size under DP leads to decline in the quality of the resulting predictions and explanations. Dataset Sensitivity. The SST2 dataset shows the largest negative impact of model size. For instance, the gap reaches = 0.286 at α = 0.75. In contrast, the performance drop for AG News and Trustpilot remains more moderate (around 0.12 to 0.16). This indicates that SST2 may be more sensitive to privacy-induced noise, possibly due to shorter inputs or the nature of sentiment-based classification. Effect of α (UtilityExplanation Trade-Off). The negative performance gap between large and base models generally widens as α increases (i.e., as more emphasis is placed on utility over explanation quality). This trend is particularly strong for SST2, where the difference grows from 0.188 (α = 0.25) to 0.286 (α = 0.75). This suggests that utility suffers more under large models when trained with DP. Stability and Variability. The standard deviations are generally higher for large models, especially under α = 0.75. This points to greater instability and inconsistency in the performance of large models under DP, in addition to their lower mean scores."
        },
        {
            "title": "5 Discussion\nWe reflect on the main findings of our experiments, giving\nway to a collection of practical recommendations, as well as\nfuture points to consider.",
            "content": "Does dataset matter in the privacy-explainability tradeoff? An initial review of the experimental results suggests that the nature of the dataset may contribute to different outcomes in terms of privacy versus explainability. One possible explanation for AG News outperforming SST2 and Trustpilot in terms of composite scores under multiple DP methods is that its topic-classification nature results in inputs that contain multiple contextually aligned keywords, which are often robust to DP-induced perturbations whereas SST2s short sentiment sentences and Trustpilots informal user reviews may lose critical cues more easily. Additionally, PLMs achieve higher baseline F1 on AG News, which could further buffer performance degradation under privacy constraints. Finally, the concentration of attribution weights on few salient words in AG News may yield more faithful explanations compared to the more diffuse attributions required for sentiment or user-generated text. While these factors require further investigation, they offer plausible account of AG News resilience in composite utilityexplainability metrics with strict privacy budgets. Despite the absolute differences exhibited by the AG News results in comparison to our other two datasets, one promising trend emerges. When looking at the composite score curves for any dataset in Figure 1 one can observe that similar line is followed regardless of dataset, for any DP method or explainer. While these curves are not strictly uniform across datasets, the similar trends indicate crucial point for future investigations, namely to determine the extent to which dataset matters in the combined study of privacy, utility, and explainability. Nevertheless, one must also keep in mind the differing performances of the various explainers we employ, which also seem to be impacted by the nature of dataset, and accordingly, the downstream task. When privacy meets explainability. Harmonizing all of the experimental results, we converge on one important discussion point, which relates back to the initial research question we posed in this work: how does DP impact explainability? Diving deeper, we also reflect on the question of whether DP and explainability can co-exist, or if there possibly remains friction between these two important mandates. helpful starting point lies in the comparison of baseline results (i.e., those achieved without any DP) and those postprivatization. While in the majority of cases, the application of DP leads to decreases in explainability, this is not always the case. Indeed, in some scenarios for all three employed DP methods, some results outperform the non-private baselines, and this occurs at least once among all (DP method, ε) combinations for each explainer. Furthermore, this result is most pronounced in the setting where explainability is most preferred (α = 0.75), suggesting that when utility loss is less important, the effects of DP gain more positive light. In this, we uncover that in some scenarios, DP actually serves to improve explainability, very promising prospect. Beyond these results, we also wish to answer our research question from the angle of which DP methods have which impacts on downstream explainability. This must be approached carefully, as noted previously, since we cannot draw conclusions between DP mechanisms operating on different lexical levels and with different privacy guarantees. However, important trends do appear within mechanisms, such as the relative stability of the DP-PROMPT results as opposed to the much steeper fluctuations from TEM or DPBART. We also conjecture that the choice of DP method also is tightly intertwined with the nature of the classification task, where generative methods (DP-PROMPT and DPBART) generally lead to more favorable results in our two binary classification tasks, whereas TEM consistently outperforms in the four-class AG News task. These results point to the importance of DP method choice, which can be heavily reliant on the downstream task at hand. This is made especially evident in Table 5, where all DP methods achieve the best consolidated score in at least one configuration. Recommendations for Practitioners. Based on our findings, we compile collection of recommendations for practitioners at the intersection of data privacy and explainability. Based on our sweet spot analysis, for explanation quality (low α), practitioners can use SHAP at the strictest privacy setting, otherwise default to LIME under moderate or low privacy. For utility (high α), LIME is the sweet spot across all but the strictest privacy settings. Based on the analysis of Figure 1, where we compare scores of each of the four explainers, for applications requiring both strong privacy and explanation quality, LIME or SHAP combined with DPBart-1500 or DPPrompt-165 is advisable. In the case of SST2, aggressive privacy strategies such as TEM1 and TEM2 should be avoided, and moderateDP setups using LIME are preferable. For AG News, LIME in combination with DPPrompt-165 or TEM3 performs robustly across all α values. Trustpilot demonstrates broad robustness and is suitable candidate for real-world deployment in privacy-sensitive NLP scenarios. When averaging across the four explainers, our results show that using DPBart-1500 or DPPrompt-165 is recommended to achieve the best trade-off between privacy, utility, and explanation quality. TEM1 and TEM2 should be avoided when explanation faithfulness is priority, especially for sensitive datasets such as SST2. For utility-focused applications (high α), AG News paired with moderate-DP settings performs reliably well. In general, higher ε values are preferable when maintaining interpretability is critical. Based on the model size effect (base vs large model) analysis, for privacy-preserving NLP tasks that require both high utility and faithful explanations, base models are consistently more reliable and effective than large models. This is especially true when utility is prioritized (high α) or for sensitive datasets such as SST2. These recommendations based on our experiments can be generalized into the following set of guidelines that are important to consider in the research or practice of explainable privacy with natural language data: 1. Decide on the importance of privacy vs. utility: an important starting point is the setting of α, as this may be considerably different across various use cases. 2. Consider the nature of downstream task: our results indicate that dataset (and task) are important factors in the juxtaposition of privacy and explainability. This becomes especially important in the following point. 3. Choose your privatization method wisely: our initial findings suggest that for more complex tasks (e.g., multi-class classification), non-generative methods such as TEM may be more suitable. However, for more colloquial datasets, such as those stemming from user reviews, DP methods based on generative models may be more suitable to preserving both utility and explainability. While this guideline requires further validation, we emphasize the important interplay between choice of privatization method and nature of downstream task. 4. Choose the smallest pretrained model acceptable for the given use case: we learn that across our results, composite scores decrease as model size increases, showing how smaller models, if acceptable for given use case, may be preferable in finding balance between privacy, utility, and explainability. 5. Measure on variety of explainers: we find that despite individual difference between explainers across all tested configurations, using averages and composite scores lead to clear emergent trends and interpretable differences between privacy levels (ε values) and datasets/tasks. We therefore recommend the usage of multiple explainers, tied together by composite score, for robust overview of performance differences between setups."
        },
        {
            "title": "6 Conclusion\nWe conduct an investigation at the intersection of privacy\nand explainability, guided by the overarching methods of\ndifferentially private text rewriting and post-hoc explain-\nability. In a series of experiments, we quantify the privacy-\nexplainability trade-off, which lends interesting insights re-\ngarding the potential synergies between the two important\ntopics. We are the first to conduct such an investigation in\nthe context of natural language data, providing the founda-\ntions for further explorations into this interdisciplinary topic.",
            "content": "Future work. We envision number of paths for future work based on our investigation, namely: Further investigating sweet spots, particularly via the integration of more robust proxies for privacy (i.e., beyond ε values). This could include for example the inclusion of membership inference testing, as performed by Shokri et al. (2021). Extending our findings with experiments on additional datasets, Explainability and DP methods, and ε ranges. Investigating the trade-offs with respect to non post-hoc explainability methods, as well as considering more recent frameworks for measuring faithfulness of explanations, such as that proposed by Zheng et al. (2025). Including human evaluation (i.e, perceptions) into the calculation of composite score, namely to improve this score beyond automatic metrics. Limitations. Our study has several limitations. It relies solely on quantitative evaluation, without qualitative or human assessments. We focus narrowly on post-hoc explainability and DP text rewriting methods, which do not fully capture either field. Our selection of DP methods overlooks nuanced differences in mechanism design and their varying theoretical guarantees. These constraints highlight the need for broader studies to deepen understanding of the explainabilityprivacy intersection in NLP. Acknowledgments We thank the anonymous reviewers for their constructive feedback. This research has been supported by the German Federal Ministry of Education and Research (BMBF) grant 01IS23069 Software Campus 3.0 (TU Munchen). References Arras, L.; Osman, A.; and Samek, W. 2022. CLEVR-XAI: benchmark dataset for the ground truth evaluation of neural network explanations. Information Fusion, 81: 1440. Attanasio, G.; Pastor, E.; Di Bonaventura, C.; and Nozza, D. 2023. ferret: Framework for Benchmarking Explainers on Transformers. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations. Association for Computational Linguistics. Carvalho, R. S.; Vasiloudis, T.; Feyisetan, O.; and Wang, K. 2023. TEM: High utility metric differential privacy on text. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM), 883890. SIAM. Chang, K.-W.; Prabhakaran, V.; and Ordonez, V. 2019. Bias and Fairness in Natural Language Processing. In Baldwin, T.; and Carpuat, M., eds., Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts. Hong Kong, China: Association for Computational Linguistics. Chrysostomou, G.; and Aletras, N. 2022. An Empirical Study on Explanations in Out-of-Domain Settings. In Muresan, S.; Nakov, P.; and Villavicencio, A., eds., Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 69206938. Dublin, Ireland: Association for Computational Linguistics. Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fedus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; Webson, A.; Gu, S. S.; Dai, Z.; Suzgun, M.; Chen, X.; Chowdhery, A.; Narang, S.; Mishra, G.; Yu, A.; Zhao, V.; Huang, Y.; Dai, A.; Yu, H.; Petrov, S.; Chi, E. H.; Dean, J.; Devlin, J.; Roberts, A.; Zhou, D.; Le, Q. V.; and Wei, J. 2022. Scaling Instruction-Finetuned Language Models. Council of European Union. 2024. Council regulation (EU) no 2024/1689. https://eur-lex.europa.eu/legal-content/EN/TXT/?uri= CELEX:32024R1689. Danilevsky, M.; Qian, K.; Aharonov, R.; Katsis, Y.; Kawas, B.; and Sen, P. 2020. Survey of the State of Explainable AI for Natural Language Processing. In Wong, K.-F.; Knight, K.; and Wu, H., eds., Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, 447459. Suzhou, China: Association for Computational Linguistics. Dhaini, M.; et al. 2025. Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods. In Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency, FAccT 25, 30063029. New York, NY, USA: Association for Computing Machinery. ISBN 9798400714825. Inferring Sensitive AtDuddu, V.; and Boutet, A. 2022. In Proceedings of tributes from Model Explanations. the 31st ACM International Conference on Information & Knowledge Management, CIKM 22, 416425. New York, NY, USA: Association for Computing Machinery. 9781450392365."
        },
        {
            "title": "ISBN",
            "content": "Dwork, C. 2006. Differential privacy. In International colloquium on automata, languages, and programming, 112. Springer. Feyisetan, O.; Balle, B.; Drake, T.; and Diethe, T. 2020. Privacyand Utility-Preserving Textual Analysis via Calibrated Multivariate Perturbations. In Proceedings of the 13th International Conference on Web Search and Data Mining, WSDM 20, 178186. New York, NY, USA: Association for Computing Machinery. ISBN 9781450368223. Gallegos, I. O.; Rossi, R. A.; Barrow, J.; Tanjim, M. M.; Kim, S.; Dernoncourt, F.; Yu, T.; Zhang, R.; and Ahmed, N. K. 2024. Bias and fairness in large language models: survey. Computational Linguistics, 179. Gupta, M.; Akiri, C.; Aryal, K.; Parker, E.; and Praharaj, L. 2023. From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy. IEEE Access, 11: 80218 80245. Hovy, D.; Johannsen, A.; and Søgaard, A. 2015. User Review Sites as Resource for Large-Scale Sociolinguistic In Proceedings of the 24th International ConferStudies. ence on World Wide Web, WWW 15, 452461. Republic and Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. ISBN 9781450334693. Hu, L.; Habernal, I.; Shen, L.; and Wang, D. 2024. Differentially Private Natural Language Models: Recent Advances and Future Directions. In Graham, Y.; and Purver, M., eds., Findings of the Association for Computational Linguistics: EACL 2024, 478499. St. Julians, Malta: Association for Computational Linguistics. Igamberdiev, T.; and Habernal, I. 2023. DP-BART for Privatized Text Rewriting under Local Differential Privacy. In Rogers, A.; Boyd-Graber, J.; and Okazaki, N., eds., Findings of the Association for Computational Linguistics: ACL 2023, 1391413934. Toronto, Canada: Association for Computational Linguistics. Igamberdiev, T.; et al. 2022. DP-Rewrite: Towards Reproducibility and Transparency in Differentially Private Text Rewriting. In Calzolari, N.; Huang, C.-R.; Kim, H.; Pustejovsky, J.; Wanner, L.; Choi, K.-S.; Ryu, P.-M.; Chen, H.- H.; Donatelli, L.; Ji, H.; Kurohashi, S.; Paggio, P.; Xue, N.; Kim, S.; Hahm, Y.; He, Z.; Lee, T. K.; Santus, E.; Bond, F.; and Na, S.-H., eds., Proceedings of the 29th International Conference on Computational Linguistics, 2927 2933. Gyeongju, Republic of Korea: International Committee on Computational Linguistics. Jacovi, A. 2023. Trends in Explainable AI (XAI) Literature. arXiv:2301.05433. Jacovi, A.; and Goldberg, Y. 2020. Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness? In Jurafsky, D.; Chai, J.; Schluter, N.; and Tetreault, J., eds., Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 41984205. Online: Association for Computational Linguistics. Jia, J.; Salem, A.; Backes, M.; Zhang, Y.; and Gong, N. Z. 2019. MemGuard: Defending against Black-Box MemberIn Proship Inference Attacks via Adversarial Examples. ceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS 19, 259274. New York, NY, USA: Association for Computing Machinery. ISBN 9781450367479. Johri, S.; Jeong, J.; Tran, B. A.; Schlessinger, D. I.; Wongvibulsin, S.; Barnes, L. A.; Zhou, H.-Y.; Cai, Z. R.; Van Allen, E. M.; Kim, D.; et al. 2025. An evaluation framework for clinical use of large language models in patient interaction tasks. Nature Medicine, 110. Kasiviswanathan, S. P.; Lee, H. K.; Nissim, K.; Raskhodnikova, S.; and Smith, A. 2008. What Can We Learn Privately? In 2008 49th Annual IEEE Symposium on Foundations of Computer Science, 531540. Klymenko, O.; et al. 2022. Differential Privacy in Natural Language Processing: The Story So Far. In Feyisetan, O.; Ghanavati, S.; Thaine, P.; Habernal, I.; and Mireshghallah, F., eds., Proceedings of the Fourth Workshop on Privacy in Natural Language Processing, 111. Seattle, United States: Association for Computational Linguistics. Kotonya, N.; and Toni, F. 2020. Explainable Automated Fact-Checking: Survey. In Scott, D.; Bel, N.; and Zong, C., eds., Proceedings of the 28th International Conference on Computational Linguistics, 54305443. Barcelona, Spain (Online): International Committee on Computational Linguistics. Lee, J.; et al. 2025. Large Language Models in Finance (FinLLMs). Neural Computing and Applications. Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mohamed, A.; Levy, O.; Stoyanov, V.; and Zettlemoyer, L. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Jurafsky, D.; Chai, J.; Schluter, N.; and Tetreault, J., eds., Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 78717880. Online: Association for Computational Linguistics. Li, X.; Du, M.; Chen, J.; Chai, Y.; Lakkaraju, H.; and Xiong, H. 2023. M4: Unified XAI Benchmark for Faithfulness Evaluation of Feature Attribution Methods across Metrics, Modalities and Models. In Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.; and Levine, S., eds., Advances in Neural Information Processing Systems, volume 36, 1630 1643. Curran Associates, Inc. Liu, H.; Wu, Y.; Yu, Z.; and Zhang, N. 2024. Please Tell Me More: Privacy Impact of Explainability through the Lens of Membership Inference Attack. In 2024 IEEE Symposium on Security and Privacy (SP), 47914809. Lundberg, S. M.; and Lee, S.-I. 2017. unified approach to interpreting model predictions. Advances in neural information processing systems, 30. Luo, X.; et al. 2022. Feature Inference Attack on Shapley Values. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, CCS 22, 22332247. New York, NY, USA: Association for Computing Machinery. ISBN 9781450394505. Lyu, Q.; et al. 2024. Towards Faithful Model Explanation in NLP: Survey. Computational Linguistics, 50(2): 657723. Madsen, A.; et al. 2022. Post-hoc Interpretability for Neural NLP: Survey. ACM Comput. Surv., 55(8). Mattern, J.; et al. 2022. The Limits of Word Level Differential Privacy. In Carpuat, M.; de Marneffe, M.-C.; and Meza Ruiz, I. V., eds., Findings of the Association for Computational Linguistics: NAACL 2022, 867881. Seattle, United States: Association for Computational Linguistics. McSherry, F.; and Talwar, K. 2007. Mechanism Design via In 48th Annual IEEE Symposium on Differential Privacy. Foundations of Computer Science (FOCS07), 94103. Meisenbacher, S.; Chevli, M.; Vladika, J.; and Matthes, F. 2024. DP-MLM: Differentially Private Text Rewriting Using Masked Language Models. In Ku, L.-W.; Martins, A.; and Srikumar, V., eds., Findings of the Association for Computational Linguistics: ACL 2024, 93149328. Bangkok, Thailand: Association for Computational Linguistics. Meisenbacher, S.; Klymenko, A.; Karpp, A.; and Matthes, F. 2025. Investigating User Perspectives on Differentially Private Text Privatization. In Habernal, I.; Ghanavati, S.; Jain, V.; Igamberdiev, T.; and Wilson, S., eds., Proceedings of the Sixth Workshop on Privacy in Natural Language Processing, 86105. Albuquerque, New Mexico: Association for Computational Linguistics. ISBN 979-8-89176-246-6. Meisenbacher, S.; and Matthes, F. 2024. Thinking Outside of the Differential Privacy Box: Case Study in Text Privatization with Language Model Prompting. In Al-Onaizan, Y.; Bansal, M.; and Chen, Y.-N., eds., Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 56565665. Miami, Florida, USA: Association for Computational Linguistics. Mosca, E.; Szigeti, F.; Tragianni, S.; Gallagher, D.; and Groh, G. 2022. SHAP-Based Explanation Methods: Review for NLP Interpretability. In Calzolari, N.; Huang, C.- R.; Kim, H.; Pustejovsky, J.; Wanner, L.; Choi, K.-S.; Ryu, P.-M.; Chen, H.-H.; Donatelli, L.; Ji, H.; Kurohashi, S.; Paggio, P.; Xue, N.; Kim, S.; Hahm, Y.; He, Z.; Lee, T. K.; Santus, E.; Bond, F.; and Na, S.-H., eds., Proceedings of the 29th International Conference on Computational Linguistics, 45934603. Gyeongju, Republic of Korea: International Committee on Computational Linguistics. Navigli, R.; Conia, S.; and Ross, B. 2023. Biases in Large Language Models: Origins, Inventory, and Discussion. J. Data and Information Quality, 15(2). Ng, D. T. K.; Leung, J. K. L.; Chu, S. K. W.; and Qiao, M. S. 2021. Conceptualizing AI literacy: An exploratory review. Computers and Education: Artificial Intelligence, 2: 100041. Pan, X.; Zhang, M.; Ji, S.; and Yang, M. 2020. Privacy Risks of General-Purpose Language Models. In 2020 IEEE Symposium on Security and Privacy (SP), 13141331. Pawelczyk, M.; et al. 2023. On the Privacy Risks of AlgoIn Ruiz, F.; Dy, J.; and van de Meent, rithmic Recourse. J.-W., eds., Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume of Proceedings of Machine Learning Research, 96809696. PMLR. Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. Why should trust you? Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 11351144. Sarti, G.; Feldhus, N.; Sickert, L.; and van der Wal, O. 2023. Inseq: An Interpretability Toolkit for Sequence Generation Models. In Bollegala, D.; Huang, R.; and Ritter, A., eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), 421435. Toronto, Canada: Association for Computational Linguistics. Shokri, R.; et al. 2021. On the Privacy Risks of Model ExIn Proceedings of the 2021 AAAI/ACM Conplanations. ference on AI, Ethics, and Society, AIES 21, 231241. New York, NY, USA: Association for Computing Machinery. ISBN 9781450384735. Simonyan, K.; Vedaldi, A.; and Zisserman, A. 2013. Deep inside convolutional networks: Visualising image arXiv preprint classification models and saliency maps. arXiv:1312.6034. Simonyan, K.; et al. 2013. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034. Smilkov, D.; Thorat, N.; Kim, B.; Viegas, F. B.; and Wattenberg, M. 2017. SmoothGrad: removing noise by adding noise. CoRR, abs/1706.03825. Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D.; Ng, A.; and Potts, C. 2013. Recursive Deep Models for Semantic Compositionality Over Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 16311642. Seattle, Washington, USA: Association for Computational Linguistics. Sousa, S.; and Kern, R. 2023. How to keep text private? systematic review of deep learning methods for privacypreserving natural language processing. Artificial Intelligence Review, 56(2): 14271492. Sundararajan, M.; et al. 2017. Axiomatic attribution for deep networks. In International conference on machine learning, 33193328. PMLR. T.y.s.s., S.; Baumgartner, N.; Sturmer, M.; Grabmair, M.; and Niklaus, J. 2024. Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on Multilingual Dataset. In Calzolari, N.; Kan, M.-Y.; Hoste, V.; Lenci, A.; Sakti, S.; and Xue, N., eds., Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 1650016513. Torino, Italia: ELRA and ICCL. Utpala, S.; et al. 2023. Locally Differentially Private Document Generation Using Zero Shot Prompting. In Bouamor, H.; Pino, J.; and Bali, K., eds., Findings of the Association for Computational Linguistics: EMNLP 2023, 84428457. Singapore: Association for Computational Linguistics. Valvoda, J.; and Cotterell, R. 2024. Towards Explainability in Legal Outcome Prediction Models. In Duh, K.; Gomez, H.; and Bethard, S., eds., Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 72697289. Mexico City, Mexico: Association for Computational Linguistics. Van Wynsberghe, A. 2021. Sustainable AI: AI for sustainability and the sustainability of AI. AI and Ethics, 1(3): 213 218. Vu, D. N. L.; et al. 2024. Granularity is crucial when applying differential privacy to text: An investigation for neural machine translation. In Al-Onaizan, Y.; Bansal, M.; and Chen, Y.-N., eds., Findings of the Association for Computational Linguistics: EMNLP 2024, 507527. Miami, Florida, USA: Association for Computational Linguistics. InterpretWallace, E.; Gardner, M.; and Singh, S. 2020. ing Predictions of NLP Models. In Villavicencio, A.; and Van Durme, B., eds., Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, 2023. Online: Association for Computational Linguistics. Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. 2018. GLUE: Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Linzen, T.; Chrupała, G.; and Alishahi, A., eds., Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 353355. Brussels, Belgium: Association for Computational Linguistics. Wang, Z.; Li, H.; Huang, D.; Kim, H.-S.; Shin, C.-W.; and Rahmani, A. M. 2025. HealthQ: Unveiling questioning capabilities of LLM chains in healthcare conversations. Smart Health, 36: 100570. Weggenmann, B.; Rublack, V.; Andrejczuk, M.; Mattern, J.; and Kerschbaum, F. 2022. DP-VAE: Human-Readable Text Anonymization for Online Reviews with Differentially Private Variational Autoencoders. In Proceedings of the ACM Web Conference 2022, WWW 22, 721731. New York, NY, USA: Association for Computing Machinery. ISBN 9781450390965. Wen, Q.; Liang, J.; Sierra, C.; Luckin, R.; Tong, R.; Liu, Z.; Cui, P.; and Tang, J. 2024. AI for Education (AI4EDU): Advancing Personalized Education with LLM and Adaptive Learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 24, 67436744. New York, NY, USA: Association for Computing Machinery. ISBN 9798400704901. Wu, H.; Li, S.; Gao, Y.; Weng, J.; and Ding, G. 2024. Natural language processing in educational research: The evolution of research topics. Education and Information Technologies, 29(17): 2327123297. Wu, X.; Duan, R.; and Ni, J. 2023. Unveiling security, privacy, and ethical concerns of chatgpt. Journal of Information and Intelligence. Yan, B.; Li, K.; Xu, M.; Dong, Y.; Zhang, Y.; Ren, Z.; and Cheng, X. 2025. On protecting the data privacy of Large Language Models (LLMs) and LLM agents: literature review. High-Confidence Computing, 5(2): 100300. Yin, Y.; and Habernal, I. 2022. Privacy-Preserving Models for Legal Natural Language Processing. In Aletras, N.; Chalkidis, I.; Barrett, L.; Goant, a, C.; and Preot,iuc-Pietro, D., eds., Proceedings of the Natural Legal Language Processing Workshop 2022, 172183. Abu Dhabi, United Arab Emirates (Hybrid): Association for Computational Linguistics. Zhang, X.; Zhao, J.; and LeCun, Y. 2015. Characterlevel Convolutional Networks for Text Classification. In Advances in Neural Information Processing Systems, volume 28, 649657. Curran Associates, Inc. Zhao, H.; Chen, H.; Yang, F.; Liu, N.; Deng, H.; Cai, H.; Wang, S.; Yin, D.; and Du, M. 2024. Explainability for Large Language Models: Survey. ACM Trans. Intell. Syst. Technol., 15(2). Zhao, X.; Zhang, W.; Xiao, X.; and Lim, B. 2021. Exploiting Explanations for Model Inversion Attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 682692. Zhao, Z.; and Aletras, N. 2023. Incorporating Attribution Importance for Improving Faithfulness Metrics. In Rogers, A.; Boyd-Graber, J.; and Okazaki, N., eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 47324745. Toronto, Canada: Association for Computational Linguistics. Zhao, Z.; Chrysostomou, G.; Bontcheva, K.; and Aletras, N. 2022. On the Impact of Temporal Concept Drift on Model Explanations. In Goldberg, Y.; Kozareva, Z.; and Zhang, Y., eds., Findings of the Association for Computational Linguistics: EMNLP 2022, 40394054. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics. Zheng, X.; Shirani, F.; Chen, Z.; Lin, C.; Cheng, W.; Guo, W.; and Luo, D. 2025. F-Fidelity: Robust Framework for Faithfulness Evaluation of Explainable AI. In Proceedings of The Thirteenth International Conference on Learning Representations (ICLR). Zini, J. E.; and Awad, M. 2022. On the Explainability of Natural Language Processing Deep Models. ACM Comput. Surv., 55(5)."
        }
    ],
    "affiliations": [
        "Technical University of Munich, School of Computation, Information and Technology, Department of Computer Science, Munich, Germany"
    ]
}