{
    "paper_title": "AutoPresent: Designing Structured Visuals from Scratch",
    "authors": [
        "Jiaxin Ge",
        "Zora Zhiruo Wang",
        "Xuhui Zhou",
        "Yi-Hao Peng",
        "Sanjay Subramanian",
        "Qinyue Tan",
        "Maarten Sap",
        "Alane Suhr",
        "Daniel Fried",
        "Graham Neubig",
        "Trevor Darrell"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Designing structured visuals such as presentation slides is essential for communicative needs, necessitating both content creation and visual planning skills. In this work, we tackle the challenge of automated slide generation, where models produce slide presentations from natural language (NL) instructions. We first introduce the SlidesBench benchmark, the first benchmark for slide generation with 7k training and 585 testing examples derived from 310 slide decks across 10 domains. SlidesBench supports evaluations that are (i)reference-based to measure similarity to a target slide, and (ii)reference-free to measure the design quality of generated slides alone. We benchmark end-to-end image generation and program generation methods with a variety of models, and find that programmatic methods produce higher-quality slides in user-interactable formats. Built on the success of program generation, we create AutoPresent, an 8B Llama-based model trained on 7k pairs of instructions paired with code for slide generation, and achieve results comparable to the closed-source model GPT-4o. We further explore iterative design refinement where the model is tasked to self-refine its own output, and we found that this process improves the slide's quality. We hope that our work will provide a basis for future work on generating structured visuals."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 2 1 9 0 0 . 1 0 5 2 : r AUTOPRESENT: Designing Structured Visuals from Scratch Jiaxin Ge1* Zora Zhiruo Wang2* Xuhui Zhou2 Yi-Hao Peng2 Sanjay Subramanian1 Qinyue Tan2 Maarten Sap2 Alane Suhr1 Daniel Fried2 Graham Neubig2 Trevor Darrell1 1University of California, Berkeley 2Carnegie Mellon University Figure 1. Automatically generating slides from natural language instructions. We propose AUTOPRESENT, tool-augmented code generation method that follows natural language instructions to design slides from scratch, as shown in the examples. This allows for precise control over all elements, including textual content, images, visual layouts, coloring, and more."
        },
        {
            "title": "Abstract",
            "content": "Designing structured visuals such as presentation slides is essential for communicative needs, necessitating both content creation and visual planning skills. In this work, we tackle the challenge of automated slide generation, where models produce slide presentations from natural language (NL) instructions. We first introduce the SLIDESBENCH benchmark, the first benchmark for slide generation with 7k training and 585 testing examples derived from 310 slide decks across 10 domains. SLIDESBENCH supports evaluations that are (i) reference-based to measure similarity to target slide, and (ii) reference-free to measure the design quality of generated slides alone. We benchmark endto-end image generation and program generation methods with variety of models, and find that programmatic methods produce higher-quality slides in user-interactable formats. Built on the success of program generation, we create *Equal Contribution. Equal Contribution. AUTOPRESENT, an 8B LLAMA-based model trained on 7k pairs of instructions paired with code for slide generation, and achieve results comparable to the closed-source model GPT-4O. We further explore iterative design refinement where the model is tasked to self-refine its own output, and we found that this process improves the slides quality. We hope that our work will provide basis for future work on generating structured visuals. Our code, data, demo, and video demonstrations are publicly available at https: //github.com/para-lost/AutoPresent 1. Introduction Designing structured visuals such as presentation slides from scratch is an essential skill for effective communication and conveying complex ideas [29]. Among various forms of visual communication, creating compelling set of slides is challenging problem, requiring content creation (text, pictures, diagrams, and more) and visual planning skills, to ensure the slides are well designed [24] and convey insights with clarity [3, 34]. Even human experts may need to spend hours iterating and polishing their slide decks [10] to produce high-quality designs with clear insights. While digital agents have demonstrated impressive capabilities in tasks such as software engineering [47], web navigation [44, 52], and free-form image design generation [8, 31], their creative capabilities in generating semistructured communicative media like slide decks has not been extensively tested. Therefore, we ask: Can we employ powerful AI agents to create high-quality presentation slides that are well-structured and insight-revealing? In this work, we formulate the natural language (NL) to slide generation task. At high level, the user provides the system with natural language instruction about the desired slide, and the system then generates an editable presentation, as shown in Figure 1. We consider three types of user instructions: (1) detailed instruction with images. (2) detailed instructions only. (3) high-level instructions, reflecting varying levels of design freedom. Since there are no existing tools for quantifying agent performance in slide generation tasks, we propose the SLIDESBENCH benchmark (2) as training source and test bed for method comparisons. SLIDESBENCH contains 7k training examples and 585 testing examples of varied instruction difficulties, constructed from 310 publicly available slide decks from 10 different domains, including art, business, and technology. To evaluate generated slides, we introduce two sets of evaluation metrics: referencebased metrics to examine position, content, and color match against the reference slide; and reference-free metrics inspired by slide design principles [5, 9, 32, 37, 42] to measure the design quality of agent-created slides alone, given that many good designs for the same instructions may vary from the reference slide. To enable controlled and structured slide generation, we propose to create slides using program generation, where model first generates program from the natural language instruction, and then the program is executed to get the slide. We apply this approach to large language models (LLMs; LLAMA [11], GPT-4O [2]) and vision-language models (VLMs; LLAVA [49]). As illustrated in Figure 1, given natural language instruction, the model first generates Python program and then executes it to obtain PPTX slide. We find that small models such as LLAMA (8B) and LLAVA (7B) are often unable to produce executable code. While GPT-4o can produce reasonable slides, it still exhibits substantial gap in design quality compared to human-generated slides (5). By further conducting iterative refinement, we find that models can self-refine and further improve slide quality. We also find that code generation approaches substantially outperform end-to-end image generation methods (Stable Diffusion [31], Dall-E [8]). To further enhance the current models ability to generate high-quality slides, we present our open-sourced AUTOPRESENT (8B) model (4.2) which is fine-tuned from LLAMA 8B on the SLIDESBENCH training set. AUTOPRESENT achieves state-of-the-art performance among small open-sourced models and approaches the performance of the closed-sourced model GPT-4o. Since directly generating long program is difficult for current models [14], we further create the SLIDESLIB library to simplify the program generation process. SLIDESLIB contains high-level functions that are basic such as add title, and imagerelated such as search image and generate image. We show that LLMs and VLMs generally perform better when given access to SLIDESLIB. Our main contributions can be summarized as follows: We formulate the NL-to-slide generation task and build SLIDESBENCH, the first benchmark for slide generation, which contains 7k training and 585 test examples and supports automatic evaluations. We leverage NL-to-program generation methods with refinement to produce high-quality slides, and benchmark diffusion models, VLMs, and LLMs. We train an 8B parameter open-source LLM, AUTOPRESENT, that approaches the performance of GPT-4o, and design programmatic tool library SLIDESLIB that facilitates slide program generation across models. 2. SLIDESBENCH In this section, we describe the creation of the SLIDESBENCH benchmark. Each instance consists of natural language instruction to create slide, and the slide itself (in PPTX format) as reference. SLIDESBENCH includes three scenarios of varying difficulty levels designed to evaluate models with different user input. We describe the slide data collection (2.1), three task setups (2.2), and the annotation process (2.3). 2.1. Slides Data Collection We search the web and collect presentation slide decks from 10 domains, including art, marketing, environment, technology, etc. To select the highest-quality slide decks from each domain, we manually go through the relevant slide decks and conduct initial processing, by checking if all its slides (i) have visually structured layouts, and (ii) extractable media such as images (if any). For the slide decks with all slides satisfying (i) and (ii) in each domain, we incorporate one slide deck into the test set, and others into the training set. This results in total of 10 and 300 slide decks (in PPTX format) for testing and training, each containing 20 slides on average. To respect the rights of the slide creators, we do not redistribute the slides. Instead, we provide list of URLs for the slides that we used so that others can download the slides directly from the original website. We also provide an opt-out mechanism for any creator who does Figure 2. Illustration of SLIDESBENCH. Each example of SLIDESBENCH consists of three instructions: Detailed Instructions with Images, Detailed Instructions Only, and High-Level Instructions. The model is tasked to generate slide based on the instruction, and the generated slide is evaluated on the metrics suite, which contains both the reference-free metrics and the reference-based metrics. not want their slides in the dataset. We provide implementation details in A. 2.2. Three Task Setups We formulate the task as an NL-to-slide generation process. Given the reference slide, we curate three versions of natural language instructions, as shown in Figure 2, to represent slide generation tasks under varied difficulty levels. We introduce each setup below. Detailed Instructions with Images The first and easiest setting is to provide the models with all the necessary information and assets to produce the reference slide, including text and image content, formatting and layout specifications. This setting evaluates models visual planning abilities, such as arranging spatial layouts, maintaining formatting consistency, balancing content proportions, and emphasizing key elements. Detailed Instructions Only Since user may not specify, or know exactly what images to put on slide, we propose detailed instruction only setting, where we provide the same natural language instruction provided in the detailed instruction with images setting, but replace the provided images with their natural language descriptions (e.g., two people shaking hands) generated by gpt-4o-mini. We then instruct the models to obtain the images using image searching or image generation tools. This setting further challenges models to interpret complex or compositional descriptions of images and obtain visuals that align with the slide context. High-Level Instructions In contrast to users who have concrete target slide in mind and can spell out all detailed instructions, some users may only be able to express their needs on high level. We thus devise high-level instruction setting, where the natural language instructions are rather high-level and only provide general topical idea of the slide, such as create title slide for Airbnb, instead of detailing what logos and text to add and where, as exemplified in Figure 2. Models in this case need to both acquire or create content, and arrange the elements properly. 2.3. Example Annotation To annotate the dataset, we collect natural language instructions paired with each slide. For each slide, we create three versions corresponding to the three setups in 2.2. Detailed Instructions with Images To produce detailed instruction with images, we use scalable approach combining human-written examples and model-generated annotations. For each slide deck, we first write instructions for three example slides manually including all necessary information (content, layout, formatting) to reproduce the slide, and providing paths to the images used in the slide (e.g., media/image 0.png), as shown in Figure 2 (top). We then use these (human-written instruction, reference slide) pairs as few-shot examples to prompt LLM (specifically, gpt-4o-mini) to generate natural language instructions for each slide in the current slide deck.* Further, for the test set, we manually examined and refined the instructions by correcting incorrect specifications, adding missing content, and removing unnecessary or untrue content. Detailed Instructions Only To produce detailed instruction only, we (e.g., the replace media/image 0.png) with the natural language descriptions of the images(an artistic, colorful background). These descriptions are generated by gpt-4o-mini. For the test set, we manually refine the instructions to ensure that they do not refer to unavailable image paths (e.g., removing phrases like use the provided images), as shown in Figure 2 (middle). High-Level Instructions To create high-level instructions, we start with similar approach by manually annotating three examples and then prompting the model to generate for all slides. Human-written instructions only provide topical description of the slide and intentionally leave out specific content or layout details. This process ensures that the generated instructions remain concise and general, as shown in Figure 2 (bottom). image paths *Including the three slides with human-written instructions, to ensure instructions for all slides are consistent in style and specificity. Overall, the instructions have an average of 115.6, 118.3, and 26.6 words under detailed instruction with images, detailed instruction only, and high-level instructions settings respectively, accompanied by an average of 1.1, 0.0, and 0.0 provided images. 3. Evaluation Metrics In this section, we describe the evaluation metrics that we designed for SLIDESBENCH. We propose two sets of evaluation metrics: reference-based metrics for measuring models instruction-following abilities (3.1), and reference-free metrics to examine the design quality of model-generated content (3.2). We also use executability to examine the success rate of each model (3.3). 3.1. Reference-Based Metrics Inspired by Design2Code [36] metrics, we implement four dimensions to examine the similarity between the modelproduced slides and the reference slide. Element matching For the slide layout, we measure the total sizes of matched elements (in generated and reference slides) divided by the total sizes of all element, where each textbox, image, or shape constitutes an element. More concretely, we accurately parse out each element in the generated and reference slides, and compute their maximum matching using the match library. Content similarity For each pair of matched elements, we compute their content similarity. If the reference element is text, we calculate the textual similarity using the cosine similarity of the embeddings produced with sentencetransformer with the default all-MiniLM-L6-v2 model [28]. If the reference element is an image, we calculate the CLIP score [18] of the image in two elements. We report the average content similarity across all matched element pairs, if either element contains non-empty text string or an image component. Color similarity We also measure the coloring similarity using the CIEDE2000 color difference formula [25], to quantify the perceptual difference between the colors. For every matched element pair, we measure the text font color similarity and element background color (if any). We additionally measure the color similarity between the background color of generated and reference slides. Position similarity In addition to content and formatting, we also calculate the positional similarity between each pair of matched elements. More concretely, we follow Si et al. [36] to normalize the element coordinates to [0, 1] by the slide page length and width. We compute the Manhattan distance between the elements and formulate positional similarity as sim(r, g) = 1 max(abs(xr xg, yr yg)). Note that low text, color, or position similarity score could come from differences in text, color, and positions, or derivative errors caused by the inaccurate element-matching process (e.g., it may match the title box in the generated slide to content textbox in the reference slide, which has different content or coloring requirements). 3.2. Reference-Free Metrics well-designed slide generated by models may look very different from the reference slide. Therefore, we also propose four reference-free evaluation metrics, to independently assess the design quality of model-generated slides. To establish the metrics, we surveyed wide range of literature on slide design principles [5, 9, 32, 37, 42], and summarized four major points as below and detailed in Table 1:"
        },
        {
            "title": "Criteria",
            "content": "The title should be simple and clear to indicate the main point. For main content, avoid too many texts and keep words concise. Use consistent and readable font size, style, and color."
        },
        {
            "title": "Image",
            "content": "Use high-quality images with reasonable proportion."
        },
        {
            "title": "Layout",
            "content": "Elements should be aligned, do not overlap, and have sufficient margins to each other. All elements should not exceed the page."
        },
        {
            "title": "Color",
            "content": "Use high-contrast color especially between the text and the background. Avoid using high-glaring colors. Table 1. Reference-free metrics, all evaluated in 0-5 scale. Text Using concise texts is important for slides to engage with the audience. An ideal slide should have clear title, concise main content, and readable formatting. Image Using appropriate visuals can engage audiences. We hence measure if models can find high-quality images and properly use them to enhance the slide quality. Layout Slide layout is crucial to create visual balance. We examine whether all elements are within the slide, have no overlap, and align properly with the relevant elements. Color Vivid and consistent color use in slides can help deliver insights. We check if the slide uses high-contrast colors to facilitate visibility, and avoid high-glaring colors to discourage user engagement. Validation of Reference-Free Evaluation For all the metrics, we provide the image version of the slide and ask the gpt-4o model to produce score between 05. To examine the reliability of this model-based evaluation, we conduct human study and compare the intraclass correlation coefficient (ICC) between two human annotators and model evaluation, on all ground-truth slides. Our examination gives high ICC scores across all four metrics: 73.8% 85.3%, which are well within the range of what is typically considered high agreement. In experiments in later sections, we scale these 0-5 scale scores to the 0100 range to enable comparisons on this more standard scale."
        },
        {
            "title": "Description",
            "content": "3.3. Executability Particularly for methods based on code generation (4.1), we additionally measure the execution success rate to account for invalid programs. Concretely, we count the percentage of successfully executing programs generated by models among all examples. We report reference-based and free scores for executable slides only, to fairly compare their design quality. But we report Overall scores for all slides by assigning zeros to non-executing slides, to account for execution failures. We report all metrics for successfully executing and all slides in E. add title add text add bullet points add image generate image search image Insert title in the slide. Insert text at specific location. Insert textbox with bullet points. Insert image at specific location. Call an image generator (Dall-E 3) given query. Search for an image on search engine (Bing). search screenshot Display query on web browser (Google Chrome) and take snapshot of the search result. Table 2. Basic (top) and image-specific (bottom) functions provided by SLIDESLIB. 4. Method 4.2. AUTOPRESENT We introduce our main method slide generation via code generation, optionally using our SLIDESLIB toolkit (4.1). Then, we present AUTOPRESENT, trained on 7k slides, that achieves performance on par with strong GPT model (4.2). 4.1. Slides via NL-to-Code Generation Generating Python Programs Given natural language instructions in 2, models are tasked with generating Python programs using publicly available libraries such as python-pptx. The model receives two (natural language instruction, Python program) pairs as in-context examples, followed by the test instruction, and generates Python program which is then executed and will ideally yield PPTX file containing the requested slide. Generating Programs with SLIDESLIB Nonetheless, the programs above could be very long and complex (170 lines on average), which could be challenging for models to generate entirely correctly, as shown in previous work [14]. To address this, we design SLIDESLIB, library that provides easier-to-use interfaces for several common actions such as setting title or setting background color. Using SLIDESLIB, the average program length is reduced to 13 lines, significantly easing the generation task. As shown in Table 2, SLIDESLIB includes 4 functions for basic operations and 3 functions for image search and generation, these functions allow models to produce more concise and modular programs. To enable the model to generate programs using SLIDESLIB, we follow the visual programming method [41] by providing prompt that includes the documentation of the functions and two in-context examples. See more SLIDESLIB details in B. Using the slides in the training set of SLIDESBENCH, we construct (natural language instruction, program) pairs to form training data to train an open-sourced 8B model, AUTOPRESENT. This model is based on the LLAMA-3.1-8BInstruct and trained using LoRA [20] with rank of 128. Training Data Construction To create (natural language instruction, program) training pairs, we generate two versions of canonical program solutions for each slide: (i) Basic Python Programs We derive canonical programs (that is, programs that can be executed to reproduce the slide) without SLIDESLIB. To do this, we manually design an extraction script that (i) extracts each element (e.g., text and image) on the given slide, and (ii) produces rulebased program that adds each element to the slide. After extracting and adding each element to the slide, the resulting program accurately reproduces the original slide. (ii) SLIDESLIB Python Programs We also generate canonical programs using SLIDESLIB, by transforming snippets from the programs above into SLIDESLIB function calls. To reproduce images in detailed instruction only and high-level instructions settings, we generate short caption for each image and provide it to GPT-4o to generate the program for producing that image using search image or generate image functions. More details of this automatic program generation process are in B.2. Training Set Composition After obtaining three instructions and two program versions for each example, we construct four versions of the training data, each with 7k examples: 1. (detailed instruction with images, python program) 2. (detailed instruction with images, SL SLI program) We still evaluate with 0-5 scale to maintain robust, human-aligned 3. (detailed instruction only, SL SLI evaluation process. program) Method Execution% element Reference-Based color content position text Reference-Free layout image color Overall Reference Stable-Diffusion* DALLE 3* LLaVA (7B) LLaMA (8B) GPT-4o AUTOPRESENT (ours) LLaVA (7B) LLaMA (8B) GPT-4o AUTOPRESENT (ours) 100.0 100.0 100. 11.3 2.1 89.2 79.0 20.0 54.4 86.7 84.1 59. 81.5 73.5 65.7 End-to-end Image Generation 74.5 75. 33.4 39.9 9.0 9.2 75.0 76.1 19.6 32.7 45.1 87.3 36.9 56. 40.5 53.4 Code Generation w/o SLIDESLIB 61.9 74.0 83.3 67.7 97.3 94.6 91.6 79. 6.2 12.5 10.5 10.9 70.8 81.2 77.0 75.9 Code Generation w/ SLIDESLIB 80.5 78.3 86. 84.2 80.5 91.2 92.5 92.2 3.5 7.5 12.7 18.1 64.0 69.5 76. 67.2 41.6 50.0 51.9 45.3 37.5 46.0 54.6 100.0 8.3 72.8 62. 48.0 68.2 83.7 47.8 73.2 29.2 50.0 53.7 54.2 29.5 47.6 70. 58.6 25.7 50.0 54.7 60.9 43.5 53.1 59.4 64.7 48.0 50. 6.1 1.3 55.1 45.2 9.7 33.5 58.0 55.0 Table 3. Results with detailed instructions with images. We found that small models like LLAVA (7B) and LLAMA (8B) can barely generate any slides, while AUTOPRESENT (8B) generates slides on par with GPT-4o. All the models still underperform humans. 4. (high-level instructions, SL SLI program) These training sets allowed us to train four specialized models that address different challenges, which we report in Table 3 (1,2) and Table 4 (3,4). mark several LMs out-of-the-box, including open-weights LLAMA 3.1 (8B, Instruct), the vision-language LLAVA v1.5 model (with Vicuna-7B-v1.5 LM backbone); and the proprietary GPT-4O model (the gpt-4o-2024-08-06 checkpoint). 4.3. Iterative Refinement Slide generation is by nature an iterative process and often requires visual-based refinements after the first draft. To enable models to refine slides as humans do, we explore an iterative refinement procedure, where the model is tasked to self-refine the slide it generated. Specifically, in the setting using SLIDESLIB, we provide GPT-4o (capable of consuming slide images) with the original language instruction, the program it generated in the first pass, and snapshot of the rendered slide; the model is then asked to generate new program based on these information to refine the slide quality by tweaking colors, spacing, and other aspects of the slide. See the prompts of this process in D. 5. Experiments and Results We first present the results under various scenarios (5.2). introduce the experimental setup (5.1), then 5.1. Experimental Setup Code Generation Approaches For code generation approaches, we sample = 3 responses and iteratively go through them, using the first successfully executing proIf none of the gram as the final output of the model. responses execute successfully, we count it as an execution In addition to AUTOPRESENT (4.2), we benchfailure. End-to-End Image Generation We compare code generation with end-to-end neural image generation methods, which are common way to produce visuals. These methods are good at creating scenic or artistic images, but may be imprecise in content (esp. text) and do not support easy further modification by users. We benchmark StableDiffusion 2 [31] and DALL-E 3 [8] by asking them to output slides given the natural language instructions. We adjust our reference-based evaluation procedure by first segmenting slide images into elements using Tesseract OCR[38] and further parse out the texts of the elements, then applying the default calculation process as in 3. For the detailed instruction with images setting, we also report the results of the end-to-end image generation methods, marked with * to indicate that they do not actually use the image inputs. 5.2. Quantitative Results and Analysis Table 3 shows the result of detailed instruction with images scenario and Table 4 shows the result of detailed instruction only and high-level instructions scenarios. In the top row of Table 3, we first measure the scores of the reference slides, which shows that the quality of the human-created slides is among the highest. Compared to the scores achieved by GPT-4O, smaller Figure 3. Examples of slides generated by different methods in three scenarios. End-to-end image generation methods fail to generate structured and clear slides. Small open-sourced models like LLAMA and LLAVA can barely generate any usable slides, while AUTOPRESENT produces quality slides. Adding SLIDESLIB improves GPT-4os performance on detailed instruction only and high-level instruction tasks. Method Detailed Instructions Only High-Level Instructions exec ref-based ref-free overall exec ref-based ref-free overall End-to-End Image Generation SD2 DALLE 100.0 100.0 48.0 50.2 35.5 57.5 48.0 50.2 100.0 100.0 LLaVA LLaMA GPT-4o LLaVA LLaMA GPT-4o AUTOPRESENT 17.9 4.6 50.3 17.4 60.5 87.7 89.2 Code Generation w/o Library 56.9 61.4 66.8 47.4 35.1 50.0 9.3 2.8 28.7 19.5 8.7 70.8 Code Generation w/ Expert-Designed Library 58.2 61.7 64. 61.9 33.8 56.6 65.8 58.7 8.0 37.4 56.3 55.2 25.1 76.9 97. 86.6 50.1 56.8 60.1 55.2 47.7 50.7 50.2 55.6 60.3 31.5 53. 47.3 50.1 57.0 36.7 58.3 71.2 61.5 47.7 52.2 9.5 4.8 39.7 10.9 43.7 58. 47.8 Table 4. Results under detailed instruction only and high-level instructions settings. We assign 100% execution success rates for all end-to-end image generation methods because they do not generate programs and would not suffer from execution errors. open-source models such as LLAMA 3.1 and LLAVA barely produce any working slides out of the box. Although the significant gaps of 49.955.0 points exist in the detailed instruction with images setting, this gap shrinks to 22.234.6 when no visuals are provided priori, in detailed instruction only and high-level instructions scenarios  (Table 4)  . This demonstrates significant challenges in obtaining images in slides. In contrast to the low performance of open-weight models out-of-the-box, AUTOPRESENTs performance approaches that of GPT-4O. End-to-End Image Generation When no visuals are provided, end-to-end image-generation methods perform worse than the best code-generation approaches in both the reference-based and reference-free metrics, especially in generating accurate content. These methods also often produce creative figures without aligning with the design principles of slides, indicating its poorer controllability. Effect of Library SLIDESLIB brings observable gains in LLAMA and LLAVA in all three scenarios by at most 34.0 points; and similarly increases the strong GPT-4O performance across scenarios, especially when no images are provided. This suggests the benefits of generating more modular and concise programs for structured visual design. VLM vs. LLM When no helper functions are presented, the one VLM that we tested (LLAVA) outperforms its LLM counterpart LLAMA in all scenarios by 5.17.5 points. However, LLAVA shows limited ability in using functions presented in context, as demonstrated by the large margin the library-augmented LLAMA has over LLAVA (12.1 26.2). All LLMs (LLAMA, GPT-4O) perform worse when the instructions become less specified (detailed instruction with images detailed instruction only high-level instructions). Nonetheless, SLIDESLIB can greatly mitigate this degradation due to the loss of input specificity, and help models produce better outcomes across all three scenarios. 5.3. Qualitative Case Study We illustrate several models-produced slides in Figure 3. For end-to-end image generation methods, the design is more creative and often more attractive, but the text does not constitute meaningful words, or even the characters themselves are not valid. On the other hand, code generation methods, especially weaker LLAMA and LLAVA models, suffer more from visual layout elements often overlap with each other or exceed the canvas, making it challenging for the audience to obtain all information clearly. In contrast, AUTOPRESENT generates slides with appropriate layouts without undesirable element overlaps. In addition, they better follow the user instructions and are not overly creative like the image generation methods. 5.4. Perceptual Evaluation"
        },
        {
            "title": "Model Pairs",
            "content": "Detailed+Images Detailed Only p-val t-stat p-val t-stat (GPT-4o, LLAMA) (AUTOPRESENT, LLAMA) (GPT-4o, AUTOPRESENT) 13.206 13.180 -0.445 0.000 0.000 0. 8.630 2.955 8.203 0.000 0.004 0.000 Table 5. Paired t-test results comparing model performance across detailed instruction only setting and detailed instruction with images setting. AUTOPRESENT and GPT-4o outperforms LLAMA with statistically significant difference in both settings. We performed qualitative evaluation on 10 randomly selected slides from each domain generated by GPT-4o, Llama-8B, and AUTOPRESENT under the detailed instruction with images and detailed instruction only settings. We also add the ground-truth reference slide to evaluate the performance gap between current models and human slide creators. We shuffle these slides and ask the annotators to rank each slide from 1-5 based on how likely they would be to use the slide. For the detailed instruction with images setting, we collect 25 responses in total, and for detailed instruction only, we collect 16 responses in total. We provide more details of the evaluation process in F. The result is shown in Figure 4. By performing the paired t-test, we found differences between the models pairs in terms of user preferences, as shown in Table 5: (1) In both settings, AUTOPRESENT and GPT-4o perform statistically significantly better than LLAMA. (2) In detailed instruction with images setting, GPT-4o and AUTOPRESENT has no significant differences (3) In the detailed instruction only Figure 4. Perceptual evaluation results on detailed instruction (1) with images and (2) only settings. We ask the users to score the quality of each slide from 1-5 and report the average score of each model. The user reported preference on GPT-4o and AUTOPRESENT compared with LLAMA, while still having gap with human-designed slides. setting, AUTOPRESENT is slightly worse than gpt-4o, aligning with our quantitative evaluations in Table 4. All three models still have an overall performance gap compared with human-designed slides, indicating room for improvement on the slide generation task. 5.5. Result after Iterative Refinement"
        },
        {
            "title": "Method",
            "content": "Detailed + Images Detailed Only High-Level GPT-4o Refinement 58.0 59.5 56.3 59.5 58.5 59.8 Table 6. Overall scores after applying refinement in the three scenarios, demonstrating that refinement boosts performance in all three scenarios, especially the detailed instructions only task. Finally, as shown by Table 6, we find that refinement improves model performance on all three challenges, especially in the detailed instruction only setup by 3.2 points. We present representative cases in Figure 5, which indicates that refinement can improve content layout and detailed controls on coloring and sizing. 6. Related Work Language and Vision Model-Based Agents Agents based on large language models (LLMs) [2, 11] and visionlanguage models (VLMs) [4, 49] have been widely adopted in various tasks such as web navigation [23, 50, 52], software engineering [47, 48], and web development [26, 36]. Creation of presentation materials is another common task [10] that has both similarities and differences from these more widely examined tasks. Generating Programs for Vision Tasks End-to-end image generation models such as diffusion [19, 31, 46] and GAN [15, 33] are widely used at producing scenic images, yet falling short on more structured visuals such as webfocuses on single-slide generation and produces full slide code in single pass, without leveraging iterative design workflows. Future research could address these limitations by expanding to full slide decks, adopting gradual and interactive slide generation, and incorporating slide-specific features like animations. Further, integrating more design principles, such as optimizing for attention capture and information clarity, would be crucial for making generated slides more impactful and effective."
        },
        {
            "title": "Acknowledgment",
            "content": "We would like to thank Yutong Bai for helping us draft Figure 1 and providing feedback on the paper, David Chan for providing detailed suggestions on the introduction, and Frank Xu and Sean Welleck for discussions at Junyi Zhang and Haven Feng for feedback on the project. the initial stage of this project."
        },
        {
            "title": "References",
            "content": "[1] Gamma - ai-powered document creation and storytelling platform. Accessed: 2024-11-14. 9 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 8 [3] Shaikh Mostafa Al Masum, Mitsuru Ishizuka, and Md Tawhidul Islam. auto-presentation: multi-agent system for building automatic multi-modal presentation of topic from world wide web information. In IEEE/WIC/ACM International Conference on Intelligent Agent Technology, pages 246249. IEEE, 2005. 1, 9 [4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [5] Angie Arriesgado. 45 tips to speed up your powerpoint design workflow, 2019. 2, 4 [6] Jonas Belouadi, Anne Lauscher, and Steffen Eger. Automatikz: Text-guided synthesis of scientific vector graphics with tikz. ArXiv, abs/2310.00367, 2023. 9 [7] Jonas Belouadi, Simone Paolo Ponzetto, and Steffen Eger. Detikzify: Synthesizing graphics programs for sciarXiv preprint entific figures and sketches with tikz. arXiv:2405.15306, 2024. 9 [8] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 2, 6 [9] The LinkedIn community. How do you use design principles and best practices to evaluate and improve your slide layout and formatting?, 2024. 2, Figure 5. Auto-refinement results with GPT-4o, where the model further addresses some previously neglected instructions (marked in green), such as shape, background color, and text. sites and slides [36]. Generating programs (i.e., imageediting actions) is useful means to get structured visuals [14, 17, 39, 41, 43], including Tikz figures [6, 7], SVG [30, 35], posters [49], and user interfaces [27, 36]. However, they often require detailed inputs and are limited to specific, simple figure types, so they are still far from creating complex, editable presentation slides from scratch. Our work extends this line of research by formulating and benchmarking the natural-language-to-slide generation task. Automatic Slide Generation Previous works on slide creation mostly focus on basic extraction from provided documents [12, 21, 22, 34, 40] or having models generate content given topic [1, 3, 45] without addressing how to organize content visually. More recently, some benchmarks [16, 51] and methods [13] have emerged that follow detailed instructions for slide editing (e.g., adjust the font size of the title from 20 to 24) of an existing slide. In contrast, we synthesize more complex and structured programs that can generate slides from scratch, including content creation, visual arrangement, and fine-grained editing, instead of refining an existing slide. 7. Conclusion and Limitations In this work, we address the challenge of creating structured visuals from scratch. Specifically, we introduced SLIDESBENCH, the first benchmark for automatic slide generation with evaluation metrics based on and free of reference slides. We benchmark multiple end-to-end image and program generation approaches, and demonstrate that AUTOPRESENT with SLIDESLIB achieves comparable performance with the top GPT-4O model. Our further exploration in iterative refinement also reveals certain effectiveness in self-refinement. This work is an initial step towards automated generation of structured visuals. Specifically, it [10] decktopus. Top presentation statistics for 2021, 2021. 2, 8 [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2, 8 [12] Tsu-Jui Fu, William Yang Wang, Daniel McDuff, and Yale Song. Doc2ppt: Automatic presentation slides generation from scientific documents. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 634642, 2022. [13] Apurva Gandhi, Thong Nguyen, Huitian Jiao, Robert language arXiv preprint Steen, commanding via program synthesis. arXiv:2306.03460, 2023. 9 and Ameya Bhatawdekar."
        },
        {
            "title": "Natural",
            "content": "[14] Jiaxin Ge, Sanjay Subramanian, Baifeng Shi, Roei Herzig, and Trevor Darrell. Recursive visual programming. In European Conference on Computer Vision, pages 118. Springer, 2025. 2, 5, 9 [15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 8 [16] Yiduo Guo, Zekai Zhang, Yaobo Liang, Dongyan Zhao, and Nan Duan. Pptc benchmark: Evaluating large language arXiv preprint models for powerpoint task completion. arXiv:2311.01767, 2023. 9 [17] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1495314962, 2023. 9 [18] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation In Proceedings of the 2021 metric for image captioning. Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2021. 4 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 8 [20] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Lowrank adaptation of large language models. In International Conference on Learning Representations, 2021. 5 [21] Yue Hu and Xiaojun Wan. Ppsgen: Learning-based presentation slides generation for academic papers. IEEE transactions on knowledge and data engineering, 27(4):10851097, 2014. [22] Min-Yen Kan. Slideseer: digital library of aligned docIn Proceedings of the 7th ument and presentation pairs. ACM/IEEE-CS joint conference on Digital libraries, pages 8190, 2007. 9 [23] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Russ Salakhutdinov, and Daniel Fried. VisualWebArena: Evaluating multimodal agents on realistic visual web tasks. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2024. 8 [24] Wenyuan Kong, Zhaoyun Jiang, Shizhao Sun, Zhuoning Guo, Weiwei Cui, Ting Liu, Jianguang Lou, and Dongmei Zhang. Aesthetics++: Refining graphic designs by exploring design principles and human preference. IEEE Transactions on Visualization and Computer Graphics, 29(6):30933104, 2022. 1 [25] Ronnier Luo, Guihua Cui, and Bryan Rigg. The development of the cie 2000 colour-difference formula: Ciede2000. Color Research & Application: Endorsed by Inter-Society Color Council, The Colour Group (Great Britain), Canadian Society for Color, Color Science Association of Japan, Dutch Society for the Study of Color, The Swedish Colour Centre Foundation, Colour Society of Australia, Centre Francais de la Couleur, 26(5):340350, 2001. 4 [26] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. 8 [27] Yi-Hao Peng, Faria Huq, Yue Jiang, Jason Wu, Xin Yue Li, Jeffrey Bigham, and Amy Pavel. Dreamstruct: Understanding slides and user interfaces via synthetic data generation. In European Conference on Computer Vision, pages 466485. Springer, 2025. [28] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2019. 4 [29] Garr Reynolds. Presentation Zen: Simple ideas on presentation design and delivery. New Riders, 2011. 1 [30] Juan Rodriguez, Shubham Agarwal, Issam Laradji, Pau Rodriguez, David Vazquez, Christopher Pal, and Marco Pedersoli. Starvector: Generating scalable vector graphics code from images. arXiv preprint arXiv:2312.11556, 2023. 9 [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 6, 8 [32] Hitesh Sahni. Presentation design: step-by-step guide, 2024. 2, [33] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast In International conlarge-scale text-to-image synthesis. ference on machine learning, pages 3010530118. PMLR, 2023. 8 [34] Athar Sefid and Jian Wu. Automatic slide generation for scientific papers. In Third International Workshop on Capturing Scientific Knowledge co-located with the 10th International Conference on Knowledge Capture (K-CAP 2019), SciKnow@ K-CAP 2019, 2019. 1, 9 [35] Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, and Antonio Torralba. vision check-up for In Proceedings of the IEEE/CVF Conlanguage models. ference on Computer Vision and Pattern Recognition, pages 1441014419, 2024. 9 In Advances interaction with grounded language agents. in Neural Information Processing Systems, pages 20744 20757. Curran Associates, Inc., 2022. 8 [51] Zekai Zhang, Yiduo Guo, Yaobo Liang, Dongyan Zhao, and Nan Duan. Pptc-r benchmark: Towards evaluating the robustness of large language models for powerpoint task completion. arXiv preprint arXiv:2403.03788, 2024. 9 [52] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, 2024. 2, [36] Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code: How far are we arXiv preprint from automating front-end engineering? arXiv:2403.03163, 2024. 4, 8, 9 [37] Think Outside The Slide. Latest annoying powerpoint survey results, 2019. 2, 4 [38] Ray Smith. An overview of the tesseract ocr engine. In Ninth international conference on document analysis and recognition (ICDAR 2007), pages 629633. IEEE, 2007. 6 [39] Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan Klein. Modular visual question answering via code generation. arXiv preprint arXiv:2306.05392, 2023. 9 [40] Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, and Nancy XR Wang. D2s: Document-to-slide generaarXiv preprint tion via query-based text summarization. arXiv:2105.03664, 2021. [41] Dıdac Surıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1188811898, 2023. 5, 9 [42] Utrecht University. Four principles for making good powerpoint presentation, 2024. 2, 4 [43] Zhiruo Wang, Graham Neubig, and Daniel Fried. TroVE: Inducing verifiable and efficient toolboxes for solving programmatic tasks. In Forty-first International Conference on Machine Learning, 2024. 9 [44] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and GraarXiv preprint ham Neubig. Agent workflow memory. arXiv:2409.07429, 2024. [45] Thomas Winters and Kory Mathewson. Automatically In Internagenerating engaging presentation slide decks. tional Conference on Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar), pages 127141. Springer, 2019. 9 [46] Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, and Qian Yu. Svgdreamer: Text guided svg generation with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 45464555, 2024. 8 [47] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated arXiv preprint arXiv:2405.15793, software engineering. 2024. 2, 8 [48] John Yang, Carlos Jimenez, Alex Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik Narasimhan, et al. Swe-bench multimodal: Do ai systems generalize to visual software domains? arXiv preprint arXiv:2410.03859, 2024. 8 [49] Tao Yang, Yingmin Luo, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. Posterllava: Constructing unified multi-modal layout generator with llm. arXiv preprint arXiv:2406.02884, 2024. 2, 8, 9 [50] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web A. SLIDESBENCH Details A.1. Slide Deck Domains The 10 domains we cover in SLIDESBENCH include: 1. Art Photos 2. Business 3. Career 4. Design 5. Entrepreneur 6. Environment 7. Food 8. Marketing 9. Social Media 10. Technology A.2. Slide Deck Source There existis large amount of slide decks on the internet including Google Search, Bing Search etc. For convenience, we collect list of slides from the slideshare.com website. B. SLIDESLIB Details In this section, we provide the detailed documentation and examples for all functions in our SLIDESLIB. B.1. SLIDESLIB Implementation Figure 6 shows the basic functions and Figure 7 shows the image-oriented functions. B.2. SLIDESLIB Usage Example Figure 8 shows two example programs using multiple SLIDESLIB functions to produce slides. C. Training Details for AUTOPRESENT"
        },
        {
            "title": "LoRA Parameters",
            "content": "LoRA rank LoRA alpha LoRA dropout Random state RS-LoRA LoFT-Q config"
        },
        {
            "title": "Trainer Parameters",
            "content": "Batch size (per device) Gradient accumulation steps Warmup steps Epochs Learning rate Mixed precision Weight decay Scheduler Seed 128 32 0 3407 Disabled None 1 2 20 1 3e-4 FP16 0.01 Linear 3407 Table 7. Training details for AUTOPRESENT. LoRA and Trainer parameters are described in detail. E.1. Detailed Instructions with Images Table 8 shows all metrics down-weighted by the execution success rate; Table 9 shows reference-based and referencefree metrics without down-weighting by execution success. E.2. Detailed Instructions Only Table 10 shows all metrics down-weighted by the execution success rate; Table 11 shows reference-based and referencefree metrics without down-weighting by execution success. E.3. High-Level Instructions Challenge Table 12 shows all metrics down-weighted by the execution success rate; Table 13 shows reference-based and referencefree metrics without down-weighting by execution success. The training parameters for AUTOPRESENT are summarized in Table 7. F. Perceptual Analysis In this section, we provide perceptual analysis details. We build google doc and ask the user to score each slide from 1-5 (1 is the worst and 5 is the best), as shown in Figure 10. An example of the question is shown in Figure 11. D. Refinement Details We provide the prompts that we used for auto-refinement in Figure 9. We input the instruction and the in-context examples, the previous code generated by the model, and the snapshot of the slide generated by executing this code to the model and let it correct the code. E. Detailed Results We report two sets of evaluation metrics (reference-based and reference-free) in both their average value on all slides (i.e., un-weighted by execution success) and on successfully rendered slides (i.e., weighted by execution success). add_title(slide, text, font_size, font_color, background_color) \"\"\"Add title text to the slide with custom font size and font color (RGB tuple). Args: slide: Slide object as in pptx library text: str, Title text to be added font_size: int, Font size in int (point size), e.g., 44 font_color: tuple(int,int,int), RGB color, e.g., (0, 0, 0) background_color: Optional, tuple(int,int,int), RGB color, e.g., (255, 255, 255) Rets: slide: Slide object with the title added \"\"\" add_text(slide, text, coords, font_size, bold, color, background_color, auto_size) \"\"\"Add text box at specified location with custom text and color settings. Args: slide: Slide object as in pptx library text: str, Text to be added coords: list(float), [left, top, width, height] in inches font_size: int, Font size in int (point size), e.g., 20 bold: bool, True if bold-type the text, False otherwise color: tuple(int,int,int), RGB color, e.g., (0, 0, 0) background_color: Optional, tuple(int,int,int), RGB color, e.g., (255, 255, 255) auto_size: bool, True if auto-size the text box, False otherwise Rets: slide: Slide object with the text box added \"\"\" add_bullet_points(slide, bullet_points, coords, font_size, color, background_color) \"\"\"Add text box with bullet points. Args: slide: Slide object as in pptx library bullet_points: list(str), List of texts to be added as bullet points coords: list(float), [left, top, width, height] in inches font_size: int, Font size in int (point size), e.g., 18 color: tuple(int,int,int), RGB color, e.g., (0, 0, 0) background_color: Optional, tuple(int,int,int), RGB color, e.g., (255, 255, 255) Rets: slide: Slide object with the bullet points added \"\"\" add_image(slide, image_path, coords) \"\"\"Add an image in the provided path to the specified coords and sizes. Args: slide: Slide object as in pptx library image_path: str, Path to the image file coords: list(float), [left, top, width, height] in inches Rets: slide: Slide object with the image added \"\"\" set_background_color(slide, color) \"\"\"Set background color for the current slide. Args: slide: Slide object as in pptx library color: tuple(int,int,int), RGB color, e.g., (255, 255, 255) Rets: modified slide object \"\"\" Figure 6. Documentation for the basic functions in our SLIDESLIB. google_search_screenshot(question, save_path) \"\"\"Search question on Google, and take screenshot of the search result. Save the screenshot to save_path, and return the path. Args: question: str, The question to search on Google. save_path: str, The path to save the screenshot. Returns: The path of the saved screenshot. \"\"\" search_image(query, save_path) \"\"\"Search for an image on Google and download the result to save_path. Args: query: str, The query to search for. save_path: str, The path to save the downloaded image. Rets: the save_path. \"\"\" generate_image(query, save_path) \"\"\"Generate an image using diffusion model based on text query, and save the image to the path. Args: query: str, The text query to generate the image. save_path: str, The path to save the generated image. Rets: The path of the saved image \"\"\" Figure 7. Documentation for the image-oriented functions in our SLIDESLIB. Method Execution% Reference-Based text color position block Human 100.0 - Code Generation w/o Library 7.0 1.5 74.3 53. 11.0 1.9 80.7 63.0 0.7 0.3 9.4 8.6 8.0 1.7 68.7 60. Code Generation w/ Expert-Designed Library 16.1 42.6 74.7 70.8 16.1 49.6 80.2 77.5 0.7 4.1 11. 15.2 12.8 37.8 66.1 56.5 7.5 25.0 47.3 40.2 text 59.7 4.7 1.0 46.3 35.8 Reference-Free layout image color Average 81.5 73.5 65.7 - 11.3 0.2 64.9 49. 9.6 37.1 72.5 61.6 3.3 1.0 47.9 42.8 5.9 25.9 61.1 49. 2.9 1.0 48.8 48.1 8.7 28.9 51.4 54.4 6.1 1.3 55.1 46. 9.7 33.5 58.0 55.0 LLaVA (7B) LLaMA (8B) GPT-4o AUTOPRESENT (ours) LLaVA (7B) LLaMA (8B) GPT-4o AUTOPRESENT (ours) 11.3 2.1 89.2 79.0 20.0 54.4 86.7 84.1 Table 8. Slide generation results (weighted by execution success) under the detailed instructions with images scenario. # Create slide with the title NLP Can Answer Questions in large, bolded font in the top center of the page. Below it, put screenshot of the google search result of the question Where was the first movie theater in the U.S? in the middle of the page. from pptx import Presentation from pptx.util import Inches, Pt from library import add_text, google_search_screenshot, add_image presentation = Presentation() presentation.slide_width = Inches(16) presentation.slide_height = Inches(9) slide_layout = presentation.slide_layouts[0] # choose layout template slide = presentation.slides.add_slide(slide_layout) add_text(slide, \"NLP Can Answer Questions\", coords=(1, 0.5, 8, 1), font_size=36) img_path = google_search_screenshot(\"Where was the first movie theater in the U.S?\", save_path=\" screenshot.png\") add_image(slide, \"screenshot.png\", coords=(2.5, 2, 6, 4)) presentation.save(\"target_path.pptx\") # Create slide titled Interior Design in bold, dark-green color in the center of the page. For the background, consider using picture with color, artistic vibe, ensure enough contrast between the colors of text and background. from pptx import Presentation from pptx.util import Inches, Pt from library import generate_image, add_image, add_text presentation = Presentation() presentation.slide_width = Inches(16) presentation.slide_height = Inches(9) slide_layout = presentation.slide_layouts[5] # choose layout template slide = presentation.slides.add_slide(slide_layout) background_img = generate_image(\"An colorful, artistic background\", \"colorful.png\") add_image(slide, \"colorful.png\", coords=(0.0, 0.0, 16, 9)) add_text(slide, Interior Design, coords=(0.0, 2.4, 13.3, 1.3), font_size=80, bold=True, color=(0, 0, 0), background_color=(255, 255, 255), auto_size=True) presentation.save(\"path.pptx\") Figure 8. Example programs to produce slides using SLIDESLIB."
        },
        {
            "title": "Method",
            "content": "Execution% Reference-Based block text color pos Human 100.0 - LLaVA (7B) LLaMA (8B) GPT-4o AUTOPRESENT LLaVA (7B) LLaMA (8B) GPT-4o AUTOPRESENT (ours) 11.3 2.1 89.2 79.0 20.0 54.4 86.7 84.1 Code Generation w/o Library 61.9 74.0 83.3 67.7 97.3 94.6 91.6 79.7 6.2 12.5 10.5 10. 70.8 81.2 77.0 75.9 Code Generation w/ Expert-Designed Library 80.5 78.3 86.2 84.2 80.5 91.2 92. 92.2 3.5 7.5 12.7 18.1 64.0 69.5 76.3 67.2 37.5 46.0 54. 47.8 text 59.7 41.6 50.0 51.9 45.3 Reference-Free layout img color"
        },
        {
            "title": "Avg",
            "content": "81.5 73.5 65.7 - 100.0 8.3 72.8 62. 48.0 68.2 83.7 73.2 29.2 50.0 53.7 54.2 29.5 47.6 70.5 58. 25.7 50.0 54.7 60.9 43.5 53.1 59.4 64.7 6.1 1.3 55.1 46. 9.7 33.5 58.0 55.0 Table 9. Slide generation results (un-weighted by execution success) under the detailed instructions with images scenario. \"\"\" You are an expert presentation slides designer who creates modern, fashionable, and stylish slides using Python code. Your job is to generate the required PPTX slide by writing and executing Python script. Make sure to follow the guidelines below and do not skip any of them: Ensure your code can successfully execute. If needed, you can also write tests to verify your code. 1. 2. Maintain proper spacing and arrangements of elements in the slide: make sure to keep sufficient spacing between different elements; do not make elements overlap or overflow to the slide page. 3. Carefully select the colors of text, shapes, and backgrounds, to ensure all contents are readable. 4. The slides should not look empty or incomplete. When filling the content in the slides, maintain good design and layout. Follow the instruction below to create the slide. If the instruction is long and specific, follow the instruction carefully and add all elements as required; if it is short and concise, you will need to create some content (text, image, layout) and implement it into the slide. If you need to use the provided images, refer to the image file names in the instructions. Finally, your code should save the pptx file to path \"output.pptx\" API Libraries: # INSERT_API_DESCRIPTIONS_HERE ## Examples # INSERT_IN_CONTEXT_EXAMPLES_HERE Modification Task: Instruction: INSERT_INSTRUCTION_HERE Previous Code: INSERT_PREV_CODE_HERE Slide Snapshot : See image. Task: Based on the observed drawbacks in the provided slide image, modify the existing code accordingly to improve the slides design and functionality. Your modification: def generate_presentation(): \"\"\" Figure 9. Prompt we used for Auto-Refinement. The model receives the APIs and instruction, the previous generated slide and code, and is tasked to re-write the code to do slide refinement. Method Execution% Reference-Based text color block position text Reference-Free layout image color Average End-to-End Image Generation Stable-Diffusion DALLE 3 100.0 100. 74.5 75.5 33.4 39.9 9.0 9.2 75.0 76.1 19.6 32.7 45.1 87. 36.9 56.7 40.5 53.4 LLaVA (7B) LLaMA (8B) GPT-4o LLaVA (7B) LLaMA (8B) GPT-4o AUTOPRESENT (ours) Code Generation w/o Library 12.2 63.0 42.2 16.3 87.0 50.0 1.4 17.4 6.0 12.4 80.4 39.8 7.9 30.4 27.1 Code Generation w/ Expert-Designed Library 15.6 45.1 72.3 70.2 15.5 55.5 80.8 82.7 0.9 5.2 6.0 9. 10.5 43.6 65.9 58.5 5.7 29.5 46.6 43.0 17.9 4.6 50.3 17.4 60.5 87. 89.2 15.3 19.6 15.3 6.2 44.3 73.0 47.7 5.7 41.3 29.0 4.1 29.6 58. 55.3 5.0 47.8 29.2 7.5 33.4 52.9 63.2 48.0 50.2 9.5 2.8 32. 8.3 37.4 56.3 55.2 Table 10. Results (weighted by execution success) under detailed instructions only scenario. Method Execution% Reference-Based text color block position text Reference-Free layout image color Overall End-to-End Image Generation Stable-Diffusion DALLE 3 100.0 100.0 74.5 75.5 33.4 39. 9.0 9.2 75.0 76.1 19.6 32.7 45.1 87.3 36.9 56.7 40.5 53. LLaVA (7B) LLaMA (8B) GPT-4o LLaVA (7B) LLaMA (8B) GPT-4o AUTOPRESENT (ours) Code Generation w/o Library 68.2 2.9 83.9 91.1 4.0 92. 7.8 0.8 11.9 69.3 3.7 79.1 44.1 1.4 53.9 Code Generation w/ Expert-Designed Library 89.7 74.5 82.4 78. 89.1 91.7 92.2 92.7 5.2 8.6 6.9 10.4 60.3 72.1 75.2 65. 32.8 48.8 53.1 48.2 17.9 4.6 50.3 17.4 60.5 87.7 89.2 85.8 0.9 30. 35.6 73.2 83.3 53.5 31.8 1.9 57.7 23.6 29.6 66.7 62.0 27.9 2.2 58. 43.1 48.9 60.3 70.9 48.0 50.2 9.5 2.8 32.2 8.3 37.4 56.3 55. Table 11. Results (un-weighted by execution success) under detailed instructions only scenario. Method Execution% Reference-Based text color block position text Reference-Free layout image color Average Stable-Diffusion DALLE 100.0 100.0 72.0 73.5 33.2 48.2 8.3 7.6 77.2 77.3 3.3 14. 49.3 89.7 35.6 57.2 37.8 52.4 End-to-End Image Generation LLaVA (7B) LLaMA (8B) GPT-4o LLaVA (7B) LLaMA (8B) GPT-4o AUTOPRESENT (ours) 19.5 8.7 70.8 25.1 76.9 97.4 86.6 CodeGen-based Methods w/o Library 14.9 7.6 54. 13.2 6.3 54.2 1.7 0.7 7.5 13.6 4.7 54.4 CodeGen-based Methods w/ Library 20.4 55.4 77.0 63. 17.8 58.3 75.8 66.4 1.6 5.6 7.7 10.2 15.4 55.7 73.7 51. 8.0 4.6 42.4 9.2 39.5 59.7 41.4 16.8 2.4 19.2 9.7 56.5 73.8 34. 5.9 5.0 51.9 6.9 40.3 78.7 64.0 6.2 5.4 48.0 11.0 43.0 65.4 73. 47.7 51.7 10.0 4.8 39.0 11.5 43.7 58.5 47.8 Table 12. Results (weighted by execution success) under high-level instructions scenario. Method Execution% Reference-Based text color block position text Reference-Free layout image color Average Stable-Diffusion DALLE 3 100.0 100.0 72.0 73. 33.2 48.2 8.3 7.6 77.2 77.3 3.3 14.9 49.3 89.7 35.6 57. 37.8 52.4 End-to-End Image Generation LLaVA (7B) LLaMA (8B) GPT-4o LLaVA (7B) LLaMA (8B) GPT-4o AUTOPRESENT (ours) 19.5 8.7 70. 25.1 76.9 97.4 86.6 CodeGen-based Methods w/o Library 76.4 87.4 77.1 67.7 72.4 76.8 8.7 8.0 10. 69.7 54.0 76.8 CodeGen-based Methods w/ Library 81.3 72.0 79.0 73.3 70.9 75.7 77.8 76. 6.4 7.3 7.9 11.8 61.4 72.4 75.6 59.0 41.0 52.9 59.9 36.7 51.3 61. 47.8 86.2 27.6 27.1 38.6 73.4 75.8 39.5 30.3 57.5 73.3 27.5 52.4 80. 73.9 31.8 62.1 67.8 43.8 55.9 67.1 84.6 47.7 51.7 10.0 4.8 39. 11.5 43.7 58.5 47.8 Table 13. Results (un-weighted by execution success) under high-level instructions scenario. \"\"\" Please score each slide from 1-5 based on your preference to use this slide in real presentation. 5 is the best, 1 is the worst. Carefully reading each slides content before ranking. \"\"\" Figure 10. Instruction we used for the perceptual evaluation. Figure 11. An example of the perceptual analysis question. We ask the human to score the quality of the slide from 1-5."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "University of California, Berkeley"
    ]
}