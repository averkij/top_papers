{
    "paper_title": "Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding",
    "authors": [
        "Ziyin Zhang",
        "Jiahao Xu",
        "Tian Liang",
        "Xingyu Chen",
        "Zhiwei He",
        "Rui Wang",
        "Zhaopeng Tu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Speculative Decoding (SD) has become an important technique in accelerating the inference speed of large language models. Conventional SD methods employ a fixed draft length, which ignores the token generation difficulty across tasks. Consequently, in this paper, we address such an issue and introduce SVIP - a difficulty-aware dynamic draft length policy for speculative decoding systems. Based on a theoretical lower bound of draft token acceptance rate and its inference-time approximation, SVIP adaptively determines the lengths of draft sequences based on the entropy of each draft token distribution. Experimental results on mainstream SD benchmarks and frameworks demonstrate the superior performance of SVIP, achieving up to 20\\% walltime speedup on SpecBench over baseline SD methods and 60\\% speedup on MT-Bench for long-form generation of up to 8K tokens. Moreover, SVIP is totally training-free and compatible with any existing SD methods that generate draft tokens autoregressively. Experimental results also show that SVIP yields consistent walltime improvement on top of GliDe & CaPE and EAGLE-2."
        },
        {
            "title": "Start",
            "content": "Draft Model Knows When to Stop: Self-Verification Length Policy for Speculative Decoding Ziyin Zhang1,2* Jiahao Xu2 Tian Liang2 Xingyu Chen1,2 Zhiwei He1,2 Rui Wang1 Zhaopeng Tu2 1Shanghai Jiao Tong University 2Tencent AI Lab 1{daenerystargaryen,galaxychen,zwhe.cs,wangrui12}@sjtu.edu.cn 2{jettexu,ttianliang,zptu}@tencent.com 4 2 0 2 7 ] . [ 1 2 6 4 8 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Speculative Decoding (SD) has become an important technique in accelerating the inference speed of large language models. Conventional SD methods employ fixed draft length, which ignores the token generation difficulty across tasks. Consequently, in this paper, we address such an issue and introduce SVIP - difficultyaware dynamic draft length policy for speculative decoding systems. Based on theoretical lower bound of draft token acceptance rate and its inference-time approximation, SVIP adaptively determines the lengths of draft sequences based on the entropy of each draft token distribution. Experimental results on mainstream SD benchmarks and frameworks demonstrate the superior performance of SVIP, achieving up to 20% walltime speedup on SpecBench over baseline SD methods and 60% speedup on MTBench for long-form generation of up to 8K tokens. Moreover, SVIP is totally training-free and compatible with any existing SD methods that generate draft tokens autoregressively. Experimental results also show that SVIP yields consistent walltime improvement on top of GliDe & CaPE and EAGLE-2."
        },
        {
            "title": "Introduction",
            "content": "Speculative decoding (Leviathan et al., 2023; Chen et al., 2023) is novel technique that markedly enhances the generation wall-time of large language models (LLMs). This approach employs small and efficient amateur model to draft sequences, while concurrently utilizing larger and more powerful expert model to verify the drafts. By avoiding the autoregressive generation of each token through the target LLM, speculative decoding achieves improved efficiency while preserving the quality of the output. Many variants of speculative decoding have been proposed. line of work focuses on developing *Work done during their internship at Tencent AI Lab. Corresponding author. 1 Figure 1: The difficulty of tokens varies in sequence, resulting in different numbers of accepted draft tokens at different positions. stronger and faster draft model (Li et al., 2024b; Elhoushi et al., 2024; Du et al., 2024). Another line of work contributes to maximizing the acceptance probability of draft tokens (Sun et al., 2023; Li et al., 2024a; Lu et al., 2024). In general, they all tend to maximize the alignment between the draft and target model to further maximize the system acceptance rate. Though successful, most of these works limit their settings to fixed draft length, where the draft model always generates fixed number of tokens (e.g. 4 or 5) in each iteration. Such setting ignores the fact that some tokens - such as stop words or civilities - in the generation may be easy for the draft model to predict, while others - such as knowledgeintensive or reasoning-intensive tokens - can be much harder, as shown in Figure 1. To address this issue, in this work we introduce SVIP - Self-VerIfication length Policy, simple, plug-and-play dynamic draft length policy for speculative decoding systems, which enhances the wall time speedup of these systems by adaptively allowing for longer draft sequences for simple tokens (top of Figure 1) and terminating the drafting process early upon encountering hard tokens (bottom of Figure 1). Specifically, we first analyze the acceptance rate Figure 2: The correlation between draft model entropy and draft token acceptance probability (top) and lengths of accepted draft seqeunces (bottom). of speculative decoding systems and derive lower bound based on the systems entropy information. Further empirical analysis suggests that such bound can be approximated by the entropy of the draft model only, which is naturally available in the drafting process of any auto-regressive draft models. Consequently, we develop SVIP which controls the length of draft sequences dynamically by determining whether to continue drafting or start verification upon the generation of each draft token. With extensive experiments across multiple model sizes and generation lengths, we demonstrate the superior performance of SVIP. It yields more than 20% of improvements over vanilla speculative decoding for Qwen2.5 14B and LLaMA-3 70B on SpecBench (Xia et al., 2024), and more than 60% of improvements for Pythia 6.9B on MTBench (Zheng et al., 2023) when generating longform responses of up to 8K tokens. Moreover, since our method is lightweight and training-free, it is extremely flexible and can be adapted to any speculative decoding system with an auto-regressive draft model. As examples, we apply SVIP on top of two state-of-the-art speculative decoding methods: GliDe with CaPE (Du et al., 2024) and EAGLE-2 (Li et al., 2024a), and confirm that it brings consistent improvements. Our code is available at https://github.com/ Geralt-Targaryen/SVIP. In summary, our contributions are threefold: 1. We derive low bound of speculative decoding systems, where the acceptance rate of the draft model could be modeled by its entropy only. 2. Based on this lower bound, we further develop an entropy-based dynamic draft length policy for speculative decoding systems, which is extremely flexible and can be adapted to any auto-regressive draft model. 3. Experimental results demonstrate the superior performance of SVIP over baseline methods, with up to 20% average speedup on SpecBench, 60% in long-form generation, and consistent improvement over state-of-theart speculative decoding frameworks such as GliDe & CaPE and EAGLE-2."
        },
        {
            "title": "2 Method",
            "content": "The overall objective of SVIP is to dynamically adapt draft length on-the-fly, stopping early if the current draft tokens acceptance probability is low and otherwise continuing drafting. To introduce our method, we first provide the background on speculative decoding in Section 2.1. Then, based on the key observation that the acceptance probability of draft token depends on the target model confidence - which is unavailable in the drafting phase, we derive theoretical lower bound for the acceptance rate based on the draft models entropy 2 Figure 3: Distribution histograms of the entropy ratio Hq,p/Hq. For most tokens, this ratio falls into narrow range, indicating that the cross entropy Hq,p can be approximated by constant multiplication of the draft entropy Hq. and the cross-entropy between the target and draft models in Section 2.2. Finally, based on empirical analysis of the target and draft distributions of more than 100K tokens across three model families, we approximate this lower bound using only the entropy information of the draft model in Section 2.3, making it viable in actual inference."
        },
        {
            "title": "2.1 Preliminaries on Speculative Decoding",
            "content": "Suppose we have two LLMs and q, where is the larger (target) model, and is the smaller (draft) model. Given an input sequence xt of length t, and draft length γ, the draft model first samples γ tokens xt+1, , xt+γ autoregressive, which are verified by the target models in parallel to acquire the confidences p(xt+1), , p(xt+γ)1. q(xn+j ) , and otherwise rejected. Then, each draft token xn+j is accepted with probability p(xn+j ) In the latter case, corrected token is sampled from the residual distribution max(q(xn+j )p(xn+j ), 0) n+j ), 0) , (cid:80) which guarantees that the overall output distribution is exactly the same as p(xn+j) (Leviathan et al., 2023; Chen et al., 2023). This process is repeated until maximum sequence length is reached. n+j )p(xi max(q(xi The complete algorithms for speculative decoding are given in Appendix A."
        },
        {
            "title": "Rate",
            "content": "From Section 2.1, its easy to derive that given an input sequence x<t and draft token xt, its acceptance probability is min . Let β denote the expected acceptance probability over 1, p(xt) q(xt) (cid:17) (cid:16) 1The short hand p(xn) is used to denote the conditional probability p(xnx<n) when there is no ambiguity. Throughout the work we use subscripts to indicate token indices in sequence (e.g. xn for n-th token), and superscripts to indicate element indices in vector (e.g. xi for i-th element in xn). 3 the distribution of xt, and it follows that (cid:18) q(x) min 1, (cid:19) p(x) q(x) min (p(x), q(x)) . β = = (cid:88) (cid:88) (1) (2) Chen et al. (2023) has proven that β is related to the total variational distance (TVD) between and q: β = 1 TVD(p, q). (3) According to Pinskers inequality - which relates TVD to Kullback-Leibler divergence - we then have β 1 = 1 = 1 (cid:114) 1 2 (cid:115) 1 2 (cid:114) 1 2 KL(qp) q(x) log q(x) p(x) (cid:88) Hq,p 1 2 Hq, (4) (5) where Hq,p is the cross entropy between and p, and Hq is the entropy of q. Equation (5) provides theoretical lower bound for the acceptance rate of draft token using the entropy information of the speculative decoding system2. To provide an intuitive motivation for using entropy to construct the lower bound, we plot the relation between draft model entropy and draft token acceptance probability as well as accepted draft seqeunce lengths in Figure 2, which are collected from more than 1M tokens generated by three different target models with temperature set to 1 and maximum draft length set to 40. The first subfigure indicates strong negative correlation between draft model entropy and draft token 2An alternative way to construct the lower bound is discussed in Appendix B. Figure 4: Comparison between the actual acceptance probability from Equation (2), the acceptance probability lower bound from Equation (5), and the estimated lower bound after approximating the cross entropy Hq,p with constant multiplication of Hq. Each position on the x-axis corresponds to token, which has been sorted according to the actual acceptance probability. acceptance probability, while the second subfigure shows that when the entropy is low, dozens of consecutive draft tokens could be accepted, which highlights the drawback of setting draft length to constant, small value, as is the common practice in speculative decoding literature. Another version of Figure 2 in the greedy setting is given in Appendix D."
        },
        {
            "title": "2.3 Empirical Estimation of Acceptance Rate",
            "content": "So far in this section, we have been assuming access to the target distribution p(xt) of the next token, which is unavailable in the drafting phase at inference time. Thus, to apply Equation (5) for actual acceptance rate estimation, we must approximate the cross entropy Hq,p with information from only the draft model. To tackle this issue, we first plot the relationship between Hq,p and Hq in Figure 3. For all three model families, the entropy ratio Hq,p/Hq are concentrated in narrow range between 1.0 and 1.3. Thus, we choose to approximate Hq,p with constant multiplication of Hq. Plugging it into Equation (5), we now have β 1 (cid:112)cHq, (6) where is constant controlling the approximation ratio between Hq,p and Hq. In Figure 4, we visualize the values derived from Equation (2), (5), and (6). With Equation (6) providing way to estimate acceptance probability using only the draft models entropy, we can now adapt the draft length on-thefly. After generating each draft token, we compute the estimated acceptance probability lower bound, and stop the draft process if its lower than certain threshold h. We note that since both and are Algorithm 1 SVIP Input: target model p, draft model q, input sequence xt, maximum length , threshold (cid:113) 1: Initialize 2: while < do = 0 3: while True do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: end while Output: xn end if + 1 end if else Sample xn+j q(xx<n+j) + 1 if H(qxx<n+j ) > then Exit while loop end if end while γ Compute p(xx<n+j), = 1, , γ + 1 in parallel for = 1 to γ do if Verify(cid:0)pxx<n+j , qxx<n+j , xn+j (cid:1) then n + 1 xn+j Correct (cid:0)pxx<n+j , qxx<n+j Exit for loop (cid:1) end for if == + γ then Sample xn+γ+1 from p(xxn+γ) constant hyperparameters, we can remove Equation 6 and absorb it into the threshold h. from We formalize SVIP in Algorithm 1. The details of the methods Verify and Correct are given in Appendix A, for which different versions are available for sampling (Algorithm 2, 4) and greedy decoding (Algorithm 3, 5)."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experiments on SpecBench 3.1.1 Settings We validate the effectiveness of SVIP on SpecBench (Xia et al., 2024) using three distinct target models: Pythia 6.9B (Biderman et al., 2023), Qwen2.5 14B (Yang et al., 2024), and LLaMA3 70B (Dubey et al., 2024), with Pythia-160M, Qwen2.5 0.5B, and LLaMA-3 8B as the draft models respectively. As baselines, we consider two simple policies for draft length: 1) constant draft length of 5, which is commonly used in the literature, and 2) the heuristics implemented in Hugging Face Transformers library (Wolf et al., 2019), where the draft length for the next draft iteration is increased by 2 if all draft tokens in the current iteration are accepted, and otherwise decreased by 1."
        },
        {
            "title": "We set",
            "content": "the sampling temperature to 0 on SpecBench (the alternatives are discussed in Appendix C). For each model, the entropy threshold in SVIP is chosen from {0.2, 0.3, 0.4, 0.5} based on performance on 8 samples held out from MT-Bench (Zheng et al., 2023), which are 0.4 for Pythia, and 0.3 for Qwen2.5 and LLaMA-3. All experiments with Pythia and Qwen are conducted on single 40GB A100, while experiments with LLaMA are conducted on 5 40GB A100s. To mitigate the impact of system performance varitations, we repeat all experiments with Pythia and Qwen for three times (using different random seeds when they are used) and report the average speedup over target-model-only autoregressive decoding. Also, since the memory consumption of verifying draft tokens is quadratic in n, we limit the maximum draft length to 40 in both heuristics and SVIP scenarios, beyond which we start to encounter out-ofmemory issues."
        },
        {
            "title": "3.1.2 Results",
            "content": "The results on SpecBench are shown in Table 1. Compared with the constant approach, SVIP yields an average speedup of 15% for Pythia and 20% for Qwen and LLaMA. Compared with the heuristic approach, SVIP also gives consistent improvement on all domains for Pythia and Qwen, and outperforms the latter on 4 out of 6 domains for LLaMA. In Figure 5, we plot the average draft length and accepted draft length of Qwen and LLaMA (the results for Pythia are similar, and are given in Appendix C). From the figure, we observe that by terminating the draft process when the draft model entropy is high, SVIP leads to shorter draft lengths and much higher acceptance rate (close to or more than 80% on all domains for Qwen, and more than 90% for LLaMA). Notably, the acceptance rate of LLaMA-3 even reaches 99% on the summarization domain, which contributes to the highest speedup (3.48) in Table 1. 3.2 Long-form Generation Most existing works on speculative decoding (Chen et al., 2023; Du et al., 2024) limit their experiments to generating short sequences of 128 tokens. To verify the wide applicability of SVIP, we also conduct experiments on long-form generation with up to 8K context. For this purpose, we use MTBench (Zheng et al., 2023) as the dataset and set the sampling temperature to 1, as we found that when using greedy decoding in long-form generation, both the draft and the target models are prone to repeat themselves, resulting in very low information entropy (see Appendix for details). Other settings follow Section 3.1. The results are given in Table 2. Interestingly, we find that in the sampling setting, the two baseline methods (constant and heuristics) perform even worse than target-model-only auto-regressive decoding for Pythia and Qwen, while SVIP consistently yields positive speedup. For LLaMA, while constant draft length performs better with contexts shorter than 1K tokens, SVIP exceeds it for generating longer sequences. Another observation is that the speedup ratio of all three methods generally increases with the context length, which could be possibly attributed to the longer contexts giving the draft model more information, aligning it at test time to the target model. However, we note that the absolute values of token throughput (which are not discussed in this paper, since they heavily depend on the underlying machines) stay at the same level from 1K to 8K context for Qwen2.5 and LLaMA-3 when using speculative decoding, and decrease slowly when not using speculative decoding. For Pythia, which does not use any optimized attention such as MQA (Shazeer, 2019) or GQA (Shazeer, 2019), the throughput decreases notably with context length in both cases - with or without specualtive decoding. 5 Methods MT-Bench Trans. Sum. QA Math RAG Avg. Pythia (6.9B, 160M) Qwen2.5 (14B, 0.5B) LLaMA-3 (70B, 8B) Const. Heuristics SVIP Const. Heuristics SVIP Const. Heuristics SVIP 1.45 1.51 1.63 1.08 1.10 1.33 2.04 2.30 2.31 1.47 1.58 1. 0.87 0.91 1.12 2.48 3.13 3.04 1.24 1.34 1.45 1.11 1.10 1.37 2.56 3.33 3.48 1.43 1.58 1. 0.92 0.92 1.14 2.34 2.61 2.63 1.52 1.64 1.72 1.43 1.34 1.57 2.32 2.52 2.89 1.42 1.51 1. 0.99 1.03 1.23 2.28 2.63 2.59 1.42 1.53 1.63(+14.8%) 1.07 1.07 1.29(+20.6%) 2.34 2.76 2.83(+20.9%) Table 1: Speedup over target-model-only autoregressive decoding on SpecBench. Methods Const. Heuristics SVIP Const. Heuristics SVIP Const. Heuristics SVIP Generation Length 256 512 1K 2K 4K 6K 8K 0.68 0.88 1.07 0.98 1.01 1.29 1.74 1.53 1.69 0.69 0.88 1.08 0.97 0.99 1. 1.74 1.56 1.72 0.69 0.88 1.08 0.95 0.98 1.30 1.77 1.61 1.75 0.70 0.88 1.07 0.96 1.00 1. 1.78 1.63 1.78 0.72 0.90 1.08 0.98 1.02 1.32 1.82 1.68 1.86 0.90 1.21 1.43 1.00 1.03 1. 1.90 1.77 1.96 0.89 1.25 1.44 1.02 1.04 1.33 1.93 1.83 2.01 0.87 1.23 1.41 1.04 1.06 1. 1.94 1.84 2.02 Pythia (6.9B, 160M) Qwen2.5 (14B, 0.5B) LLaMA-3 (70B, 8B) Table 2: Speedup on MT-Bench with different generation length."
        },
        {
            "title": "3.3 Applying SVIP to Other Draft Methods",
            "content": "In Section 3.1 and 3.2, we evaluated SVIP on vanilla speculative decoding, where standard pretrained Transformer decoder model from the target models family is used as the draft model. However, in the past years many works on speculative decoding have proposed other stronger or more efficient draft models (Cai et al., 2024; Du et al., 2024; Li et al., 2024b). Since most of these works assume constant draft length, SVIP is orthogonal to them and can be applied on top of them without any additional training. Specifically, we consider GliDe with CaPE (Du et al., 2024), where the draft model - named GliDe - is small transformer decoder with cross-attention to the target models hidden representations, while CaPE is complementary tree expansion method to increase the acceptance rate of the draft token at each position. Following the settings of Du et al. (2024), we use Vicuna 7B, 13B, and 33B (Chiang et al., 2023) as the base models (for which the draft models are publicly available) and set the sampling temperature to 0. However, we distribute the 33B model across two GPUs due to memory constraint. Similar to the previous experiments, we set the threshold to 0.5 based on pilot experiments on 8 samples held out from MT-Bench using GliDe only (without CaPE). We also apply SVIP to EAGLE-2 (Li et al., 2024a), the state-of-the-art speculative decoding system which utilizes the target models language modeling head on top of the draft models features to predict the next draft token, and dynamically constructs draft tree at each draft position. Following Li et al. (2024a), we use LLaMA-2 7B, 13B (Touvron et al., 2023) and Vicuna 7B, 13B (Chiang et al., 2023) as the base models, and set the sampling temperature to 1. As Brown et al. (2024) suggest, adding conditional clause (that decides whether or not to stop drafting) after the generation of each draft token in EAGLE-2 may interrupt the otherwise input-independent control flow of 6 Figure 5: The average generated draft length, accepted draft length, and acceptance rate of Qwen2.5 (top) and LLaMA-3 (bottom) on SpecBench. Compared with the two baselines, SVIP leads to shorter draft length and much higher acceptance rate. EALGE-2, reducing the effects of low-level interpreter and system optimizations. Thus, in EAGLE2 we calculate the draft entropy and decide whether or not to stop drafting after every two draft tokens instead of after every draft token, as used in previous experiments. The results for these two methods are presented in Table 3 and 4, respectively. For GliDe, SVIP yields consistent speedup both with and without CaPE, with 5% improvement for the 7B and 33B models. For EAGLE-2, the speedup is also particularly notable for the Vicuna models, with 5% improvement for both the 7B and 13B models."
        },
        {
            "title": "4 Related Work",
            "content": "Since Leviathan et al. (2023) and Chen et al. (2023) introduced speculative decoding into large language models, numerous works have followed their tracks in pursuit of more efficient LLM inference. We broadly categorize these works into three types: better draft models, draft tree expansion, and draft length control, which are orthogonal to each other. more comprehensive review of speculative decoding is provided by Xia et al. (2024). Better draft models. As Xia et al. (2024) suggest, draft models in speculative decoding can be either based on self-drafting or based on an independent draft model. For the first type, one may use quantized (Zhao et al., 2024), early-exiting (Elhoushi et al., 2024), or forward-padded (Monea et al., 2023) version of the target model to produce draft tokens, while the second type is represented by the vanilla speculative decoding (Leviathan et al., 2023). Some works also take the best of both worlds and introduce extra layers on top of the target models hidden representations to construct draft models, represented by EAGLE (Li et al., 2024b), GliDe (Du et al., 2024), and Medusa (Cai et al., 2024). Draft tree expansion. Given draft model, one may verify multiple draft tokens for the same position in parallel to increase the probability of finding an accepted draft token, and we use draft tree expansion as an umbrella term for such techniques. Li et al. (2024a) introduce EAGLE-2, 7 Methods MT-Bench Code Finance GSM Spider Avg. GliDe +SVIP GliDe + CaPE 7B +SVIP GliDe +SVIP GliDe + CaPE +SVIP GliDe +SVIP GliDe + CaPE +SVIP 13B 33B 1.95 2.00 2.36 2.56 2.22 2.31 2.73 2. 2.12 2.29 2.08 2.13 2.04 2.12 2.57 2.65 2.41 2.43 2.86 2.93 2.25 2.40 1.98 2.02 1.91 2.03 2.29 2.49 2.15 2.17 2.66 2. 2.09 2.20 2.10 2.15 1.98 2.01 2.51 2.54 2.31 2.35 2.80 2.85 2.29 2.42 2.13 2.16 1.69 1.63 1.97 2.08 1.85 1.85 2.24 2. 1.99 2.03 1.76 1.82 1.95 2.02 2.40 2.52 2.24 2.28 2.73 2.76 2.18 2.30 2.03 2.08 Table 3: Speedup comparison with GliDe & CaPE, using Vicuna as the base model. Methods MT-Bench H-Eval GSM8K Alpaca CNN/DM QA Avg. LLaMA-2 7B LLaMA-2 13B Vicuna 7B Vicuna 13B EAGLE-2 + SVIP EAGLE-2 + SVIP EAGLE-2 + SVIP EAGLE-2 + SVIP 3.10 3.16 3.38 3.41 2.66 2.84 2.85 2. 3.61 3.66 4.12 4.09 2.84 2.97 3.31 3.49 3.15 3.18 3.41 3. 2.77 2.75 2.93 3.19 3.10 3.13 3.25 3.34 2.48 2.66 2.74 2. 2.76 2.82 3.01 3.05 2.31 2.42 2.48 2.60 2.84 3.02 2.98 3. 2.13 2.30 2.30 2.53 3.10 3.16 3.41 3.46 2.58 2.73 2.83 2. Table 4: Speedup comparison with EAGLE-2, using LLaMA-2-Chat and Vicuna as the base models. which reranks draft tokens in EAGLEs draft tree to select tokens with the highest confidence for verification. Similarly, CaPE (Du et al., 2024) improves GliDe by expanding the token set chosen for verification at each position based on top-1 confidence. Other works have also addressed the problem of multi-draft verification from theoretic perspective (Sun et al., 2023; Yin et al., 2024). Draft length control. Works in this category are few, but most relevant to ours. Liu et al. (2024) introduce PEARL, which lets the target model perform verification in parallel to draft generation, stopping the draft process when mismatch is found. Huang et al. (2024) propose SpecDec++, which trains an acceptance prediction head on top of the draft model to predict the acceptance probability of the current draft token, stopping the draft round when the predicted acceptance probability falls below constant threshold. Brown et al. (2024) propose Dynamic Depth Decoding on top of EAGLE-2, which uses the sum of all tokens confidences in one level of its draft tree as an indicator to predict whether or not to continue draft generation."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose SVIP, flexible, training-free, and plug-and-play dynamic draft length policy for speculative decoding systems. Based on theoretical lower bound of acceptance probability and its empirical approximation, SVIP determines whether to continue draft generation or to quit drafting based on the draft models entropy after the generation of each draft token. With extensive experiments spanning various base models, draft methods, test domains, and generation length, we validated the effectiveness of SVIP, sparking new insights on speculative decoding and more efficient large language models."
        },
        {
            "title": "References",
            "content": "Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 23972430. PMLR. Jean Bretagnolle and Catherine Huber. 1978. Estimation des densités : risque minimax. Séminaire de probabilités de Strasbourg, 12:342363. Oscar Brown, Zhengjie Wang, Andrea Do, Nikhil Mathew, and Cheng Yu. 2024. Dynamic depth decoding: Faster speculative decoding for llms. CoRR, abs/2409.00142. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. 2024. Medusa: Simple LLM inference acceleration framework with multiple decoding heads. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023. Accelerating large language model decoding with speculative sampling. CoRR, abs/2302.01318. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Cunxiao Du, Jing Jiang, Yuanchen Xu, Jiawei Wu, Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, Liqiang Nie, Zhaopeng Tu, and Yang You. 2024. Glide with cape: low-hassle method to accelerate speculative decoding. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, 9 Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed Aly, Beidi Chen, and Carole-Jean Wu. 2024. Layerskip: Enabling early exit inference and self-speculative decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1262212642. Association for Computational Linguistics. Kaixuan Huang, Xudong Guo, and Mengdi Wang. 2024. Specdec++: Boosting speculative decoding via adaptive candidate lengths. CoRR, abs/2405.19715. Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via specIn International Conference on ulative decoding. Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1927419286. PMLR. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024a. EAGLE-2: faster inference of language models with dynamic draft trees. CoRR, abs/2406.16858. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024b. EAGLE: speculative sampling reIn Fortyquires rethinking feature uncertainty. first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Tianyu Liu, Yun Li, Qitan Lv, Kai Liu, Jianchen Zhu, and Winston Hu. 2024. Parallel speculative decoding with adaptive draft length. CoRR, abs/2408.11850. 10 Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024. Qwen2 technical report. CoRR, abs/2407.10671. Ming Yin, Minshuo Chen, Kaixuan Huang, and Mengdi Wang. 2024. theoretical perspective for speculative decoding algorithm. CoRR, arXiv:2411.00841. Juntao Zhao, Wenhao Lu, Sheng Wang, Lingpeng Kong, and Chuan Wu. 2024. Qspec: Speculative decoding with complementary quantization schemes. CoRR, abs/2410.11305. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Xiaofan Lu, Yixiao Zeng, Feiyang Ma, Zixu Yu, and Marco Levorato. 2024. Improving multi-candidate speculative decoding. CoRR, abs/2409.10644. Giovanni Monea, Armand Joulin, and Edouard Grave. 2023. Pass: Parallel speculative sampling. CoRR, abs/2311.13581. Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150. Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix X. Yu. 2023. Spectr: Fast speculative decoding via optimal transport. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingfaces transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. 2024. Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 76557671. Association for Computational Linguistics. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin"
        },
        {
            "title": "A The Complete Speculative Decoding Algorithms",
            "content": "In Algorithm 2 to 6, we present the complete algorithms of the vanilla speculative decoding in both the greedy decoding and the sampling scenarios. For the sampling scenario, the Verify and Correct methods in Algorithm 6 resolve to Algorithm 2 and 4. For greedy decoding, they resolve to Algorithm 3 and 5. Algorithm 2 Verify (Sampling) Input: target distribution p(x), draft distribution q(x), Algorithm 5 Correct (Greedy) Input: target distribution p(x), draft distribution q(x) Output: arg max p(x) draft token xt 1: accept False 2: [0, 1] 3: if < p(xt) 4: 5: end if Output: accept q(xt) then accept True Algorithm 3 Verify (Greedy) Input: target distribution p(x), draft distribution q(x), draft token xt 1: accept False 2: if arg max p(x) == xt then 3: 4: end if Output: accept accept True Algorithm 4 Correct (Sampling) Input: target distribution p(x), draft distribution q(x) 1: Sample ˆx max(q(x)p(x), 0) (cid:80) Output: ˆx max(q(xi)p(xi), 0) Algorithm 6 Speculative Decoding Input: target model p, draft model q, input sequence xt, maximum length , draft length γ for = 1 to γ do Sample xn+j q(xx<n+j) end for Compute p(xx<n+j), = 1, , γ + 1 in parallel for = 1 to γ do if Verify(p(xx<n+j), q(xx<n+j), xn+j) then n + 1 xn+j Correct (p(xx<n+j), q(xx<n+j)) Exit for loop end for if == + γ then xn+γ+1 p(xxn+γ) 1: Initialize 2: while < do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end while Output: xn end if + 1 end if else"
        },
        {
            "title": "B Alternatives for Acceptance Rate Lower Bound Computation",
            "content": "In Section 2, we used Pinskers inequality to compute lower bound for the expected acceptance probability: β = (cid:88) 1 min (p(x), q(x)) (cid:114) 1 2 KL(qp). (7) (8) Another way to compute the lower bound of acceptance probability can be derived from BretagnolleHuber inequality (Bretagnolle and Huber, 1978): β 1 (cid:112) 1 eKL(qp). (9) Compared with the Pinskers bound, its trivial to see that this bound is guaranteed to be always larger than 0. However, in practice we find that the Pinskers bound is 11% tighter for Qwen2.5, 20% tighter for Pythia, and 43% tighter for LLaMA-3."
        },
        {
            "title": "C Additional Results on SpecBench",
            "content": "In Figure 6, we plot the draft length and acceptance rate of Pythia (which complements Figure 5 in Section 3.1), and in Figure 7 we also give the results of the three models when using sampling instead of 12 Methods MT-Bench Trans. Sum. QA Math RAG Avg."
        },
        {
            "title": "Pythia",
            "content": "6.9B, 160M Qwen2.5 14B, 0.5B LLaMA-3 70B, 8B Const. Heuristics SVIP Const. Heuristics SVIP Const. Heuristics SVIP 0.65 0.82 1.05 1.01 1.02 1.24 1.62 1.56 1.53 0.63 0.83 1. 0.85 0.94 1.08 1.56 1.55 1.53 0.65 0.85 1.03 0.87 0.93 1.19 1.65 1.76 1.69 0.66 0.83 1. 0.85 0.88 1.11 1.53 1.49 1.51 0.65 0.83 1.03 1.32 1.22 1.47 1.73 1.61 1.71 0.64 0.83 1. 0.86 0.91 1.10 1.54 1.55 1.56 0.65 0.83 1.02(+56.9%) 0.96 0.99 1.20(+25.0%) 1.60 1.58 1.58(1.3%) Table 5: Speedup on SpecBench using temperature sampling. Figure 6: The average generated draft length, accepted draft length, and acceptance rate of Pythia on SpecBench. greedy decoding3. Across the different models and sampling methods, the same observation as discussed in Section 3 holds: SVIP results in short draft lengths and much higher acceptance rate. Additional Results on Long-form Generation In Table 6, we present the results of long-form generation with greedy decoding. Compared with Table 2, the speedup ratio of greedy decoding is much higher (even more than 4 times for LLaMA-3 after 4K context). This is due to the fact that these models tend to repeat themselves in greedy long-form generation, making it very easy for the draft models to predict the next tokens. In Figure 8, we also plot the relation between draft model entropy and accepted draft sequence lengths in this setting, which shows much stronger correlation compared with the sampling setting in Figure 2. 3We note that in the constant scenario, the draft length is always set to 5. However, if an EOS token is sampled from the draft model, the draft process will terminate. So the overall draft length might be slightly lower than 5, as can be seen from Figure 7. Figure 7: The average generated draft length, accepted draft length, and acceptance rate of Qwen2.5 (top), Pythia (middle), and LLaMA-3 (bottom) on SpecBench, using temperature sampling instead of greedy decoding. 14 Methods Const. Heuristics SVIP Const. Heuristics SVIP Const. Heuristics SVIP Generation Length 128 256 512 1K 2K 4K 6K 8K 1.10 1.25 1.41 1.05 1.04 1.30 2.06 2.26 2. 1.30 1.45 1.62 1.08 1.06 1.34 2.18 2.46 2.56 1.50 1.65 1.83 1.15 1.13 1.42 2.31 2.73 2. 1.66 1.81 2.01 1.29 1.32 1.57 2.45 3.07 3.21 1.82 2.02 2.21 1.44 1.54 1.74 2.58 3.48 3. 1.20 1.50 1.71 1.54 1.72 1.87 2.72 3.90 4.00 1.11 1.46 1.60 1.60 1.85 1.98 2.77 4.15 4. 1.04 1.41 1.52 1.67 1.97 2.10 2.78 4.26 4.33 Pythia (6.9B, 160M) Qwen2.5 (14B, 0.5B) LLaMA-3 (70B, 8B) Table 6: Speedup on MT-Bench with different generation length using greedy decoding. Figure 8: The correlation between draft model entropy and lengths of accepted draft seqeunces in the greedy setting."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "Tencent AI Lab"
    ]
}