{
    "paper_title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions",
    "authors": [
        "Bo-Hsu Ke",
        "You-Zhe Xie",
        "Yu-Lun Liu",
        "Wei-Chen Chiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/"
        },
        {
            "title": "Start",
            "content": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions Bo-Hsu Ke You-Zhe Xie Yu-Lun Liu Wei-Chen Chiu"
        },
        {
            "title": "National Yang Ming Chiao Tung University",
            "content": "5 2 0 2 2 ] . [ 1 4 1 3 2 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our methods superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/ 1. Introduction 3D scene representation methods, such as Neural Radiance Fields (NeRF)[72] and 3D Gaussian Splatting (3DGS)[42], have significantly advanced novel view synthesis, accurately modeling complex scene geometry and appearance. Along with their popularity, the protection of 3D digital content encoded in these representations has become matter of concern, where we have witnessed the corresponding watermarking or steganography techniques being proposed in recent years. For instance, GaussianMarker [35] and 3DGSW [38] embed watermarks (mainly binary messages) into Gaussian parameters of 3DGS with minimal visual impact, coupled with dedicated decoders to decipher the hidden messages. Moreover, embedding or hiding extraneous information/messages into 3D scene representations is also directly connected to the risk of data poisoning (where the extraneous information as the poison appears in the training data Figure 1. Illustration of our proposed Density-Guided Poisoning Attack for 3D Gaussian Splatting (3DGS). Our method strategically distribute the Gaussian points of the illusory object (i.e. the red vehicle) among the low-density regions which are discovered along the rays casted from the virtual camera of the poisoned view (i.e. the target view that we would like to attack), making the illusory object clearly visible from the poisoned view while having the minimal interference for the rendering quality on the other non-target/innocent views. and is encoded into the 3D representations during model training), in which the further utilization or visualization of the poisoned 3D representations would lead to abnormal or even malicious model behaviours (i.e. the negative impact stemmed from the poison is triggered). To this end, in this work, we focus on the investigation of poisoning attacks on 3D scene representation methods, as they become integral to safety-critical applications. Hence, addressing their security vulnerabilities is not only of utmost importance but also urgent. While there exists prior work of studying the poisoning attack upon NeRF, i.e. IPA-NeRF [40] which effectively exploits NeRFs implicit representations to embed targeted visual illusions (i.e. the illusion as poison will appear while rendering the scene from certain viewing direction), its applicability to explicit 3D scene representations (particularly 3DGS) however remains limited and not directly transferable. Considering the rapidly growing application scenarios 1 of 3DGS (thanks to its capability of fast rendering and accurately capturing scene geometry), we devote our research effort to realizing poisoning attacks on 3D Gaussian splatting. To the best of our knowledge, this is the first work of its kind. While concurrent work, Poison-Splat [63] focuses on computational cost attacks, our work targets visible illusion embedding. In particular, we would like to inject the visible illusory objects (i.e., poison) onto target view (named as poisoned view) while keeping the other non-target views (named as innocent views) unaffected, as shown in Figure 1. Our work starts from conducting an investigation (cf. Figure 2) upon the robustness of 3DGS against the prior image-level poisoning methods such as IPA-NeRF, where we find that the attempts of directly adopting IPA-NeRFs approach or naively injecting illusory content into training images easily fail, as 3DGSs inherent multi-view consistency and densification processes effectively neutralize or significantly weaken these attacks. Motivated by the aforementioned investigation, we propose density-guided poisoning method for 3DGS. Our approach (cf. Figure 1) strategically identifies low-density regions in the initial Gaussian point cloud using Kernel Density Estimation (KDE), in which the points of illusory objects are then distributed among the low-density regions along the rays casting from the virtual camera of target view (i.e. the rays are casted from the virtual camera with the target viewing direction). These points effectively embed illusory objects which would be clearly visible from targeted views, while having minimal impact on other innocent views (i.e, being less perceptible). Moreover, we introduce the adaptive Gaussian noise into innocent views during training for disrupting the property of multi-view consistency in 3DGS, in order to further enhance the overall efficacy of the attack. We conduct extensive experiments, and the results demonstrate the consistent superiority of our proposed poisoning method in comparison to several baselines. The contribution of our work can be summarized as follows: We are the first work to address data poisoning attacks upon 3D Gaussian Splatting for illusory objects injection. We identify and analyze the robustness of 3DGS against the prior poisoning attack techniques. We propose density-guided poisoning method tailored for 3DGS, introducing adaptive noise scheduling to disrupt the multi-view consistency of 3DGS and better realize the entire attack. 2. Related Work Adversarial Attack. Adversarial attacks [8, 27, 54, 67, 80, 87, 91] are critical research area in machine learning and computer vision [1, 9, 108]. These attacks exploit Figure 2. Limitations of existing poisoning methods on 3DGS. Existing poisoning methods (e.g., IPA-NeRF [40] designed for NeRF or the one adapted to 3DGS, denoted as IPA-Splat) produce weak or absent illusions due to 3DGSs robustness and multi-view consistency. In contrast, our proposed approach successfully injects clearly visible illusory objects (i.e., the dog). ML model vulnerabilities by creating inputs that cause misclassification while appearing normal to humans. Goodfellow et al. [27] introduced FGSM, showing how small perturbations could transform panda images into gibbons, while Madry et al. [67] established PGD as stronger iterative method. Adversarial attacks divide into black-box attacks [6, 13, 18, 30, 36, 76, 109], which operate without model access, and white-box attacks [47, 53, 73, 96, 102], which use full model knowledge. Black-box methods include transfer-based attacks [20, 59] exploiting cross-model transferability and query-based approaches [2, 12] that iteratively refine perturbations. White-box methods like Carlini-Wagner attacks [8] formulate adversarial generation as optimization problems, achieving high success rates while maintaining imperceptibility. Recent research has expanded to complex domains including object detection [71, 100], semantic segmentation [3], and 3D point clouds [57, 97]. Our approach resembles black-box attacks, modifying point clouds and applying simple perturbations to input images, extending adversarial concepts to 3D Gaussian Splatting representations. Data Poisoning. Data poisoning [14, 26, 81, 99] represents critical vulnerability in machine learning systems. Unlike adversarial attacks targeting inference, poisoning attacks manipulate training by injecting crafted samples that exploit learning mechanisms, causing defective statistical distributions [7, 37]. Two primary poisoning strategies exist: creating indistinguishable malicious samples that blend with normal data [86, 92, 99], or employing surgical precision by polluting minimal data subsets [39, 81]. These approaches prove effective across computer vision [34, 106], NLP [48, 93], and recommender systems [23, 50]. They categorize as availability attacks degrading overall performance [75, 84] or integrity attacks targeting specific inputs [14, 29]. More sophisticated approaches formulate poisoning as bi-level optimization problems [46, 52, 60, 62], allowing attackers to optimize poisoned samples while anticipating victim model behavior. Common defenses include robust statistics [19, 90], data sanitization [17, 77], and differential privacy [66]. Our 2 work employs the surgical precision strategy, conducting stealth attack with minimal contaminated data while exploring 3D Gaussian Splattings unique vulnerabilities to poisoning. characteristics. We propose an evaluation protocol based on scene density analysis to identify optimal positions for poison injection, contributing insights that could inform future defense mechanisms against such attacks. Neural Rendering. Novel view synthesis (NVS)[10, 24, 89, 110] has evolved from traditional graphics to learningbased methods. Neural Radiance Field (NeRF)[72] and 3D Gaussian Splatting (3DGS)[42] have transformed 3D scene representation. NeRF uses MLP-based networks to implicitly represent 3D scenes, leveraging differential alpha blending[43, 78] for high-quality volumetric rendering [41, 69]. Extensions like Mip-NeRF [4], Instant-NGP [74], and NeRF-W [68] address anti-aliasing, speed, and real-world limitations. Recent advances include few-shot synthesis [55], MVS-based approaches for large scenes [85], and improved robustness for dynamic content [61]. Robustness enhancements include progressive optimization [70] and joint camera pose optimization [11]. However, NeRFs implicit representation lacks explicit geometric constraints, creating vulnerabilities to view-specific perturbations [58, 94]. IPANeRF [40] exploits this for data poisoning attacks. In contrast, 3DGS [42] uses explicit representation with discrete 3D Gaussian primitives inspired by point-based rendering [28, 111]. This provides stronger geometry constraints and multi-view consistency with superior rendering efficiency [15, 103]. 3DGS has expanded to dynamic scenes [64, 101], human avatars [45, 49], efficient implementations [16, 22], compression [105], robustness for unconstrained images [33], and specular reconstruction [21]. Our work addresses the challenge of attacking 3DGSs robust explicit representation, contributing to understanding security implications in modern neural rendering techniques. Data Poisoning on Neural Rendering. As neural rendering gains adoption, security vulnerabilities have attracted attention. IPA-NeRF [40] pioneered poisoning attacks against NeRF by inserting crafted samples at specific viewing angles, formulating data poisoning as bi-level optimization. Lu et al.s Poison-Splat [63] targeted 3DGS efficiency by generating samples that dramatically increase memory consumption, demonstrating attacks targeting resource utilization [82] rather than accuracy. The security landscape extends to privacy concerns [65] and adversarial examples [25, 32, 104]. Zeybey et al. [104] showed how adversarial noise in 3D objects misleads vision-language models like CLIP [79]. Song et al.s Geometry Cloak [83] prevents unauthorized 3D reconstruction from copyrighted images, while security studies examine point clouds [31, 57], meshes [98], and broader vision systems [1, 9]. Similar to steganography approaches embedding hidden information [51, 107], our method injects view-dependent content but enables decoder-free extraction through standard rendering from specific viewpoints. Our work introduces density-guided attack methodology targeting 3DGSs initial point cloud prior, exploiting its robustness 3. Method 3.1. Problem Formulation Given dataset composed of multiple images {Ik}N k=1 viewing scene from different viewing directions, 3D Gaussian Splatting is originally proposed to construct 3D Gaussian point cloud (each Gaussian has properties in terms of position, covariance, opacity, and color factors) for representing the 3D scene E, where we are able to render the image observation of from any arbitrary view via projection and differentiable tile rasterizer. Basically, the goal of our poisoning attack upon 3DGS is to inject illusory/poison object OILL onto the target view vp, where we denoted the resultant Gaussian point cloud after being poisoned as G, such that the image IILL = R( G, vp) obtained by renderring from vp would contain OILL while keeping the images of renderred from any other views vk (i.e. non-target views vk = vp) being identical to the ones of of the same view vk. Please note that, when we denote the image of renderred from vp as R(G, vp), the ideal appearance of IILL should be the combintation of R(G, vp) and OILL (in which such combination is denoted as IILL). The core objective of our 3DGS poisoning is formulated as: IILL IILL2 2 + min (cid:88) vk=vp R( G, vk) R(G, vk)2 2, (1) To achieve this under different threat models, we propose two strategies: density-guided point cloud attack (Section 3.3) following classical data poisoning, and view consistency disruption (Section 3.4) as backdoor attack with minimal training modification via noise scheduling. 3.2. Potential Naive Approaches and Limitations We first explore two potential naive approaches, followed by discussing their limitations and motivating our attack design: 1) Directly injecting the illusory object onto the training image in of the target view vp and following the typical training procedure of 3DGS would easily fail since the property of multi-view consistency in 3DGS would treat the illusory object as the noise (i.e. which violates the consistency) and eliminate it from the resultant Gaussian cloud. 2) Directly backprojecting the illusory object into the Gaussian point cloud reconstructed from also faces the challenge of determining proper depth for the illusory object, otherwise it could be occluded by existing geometry in G. 3.3. Our Density-Guided Point Cloud Attack"
        },
        {
            "title": "With learning the lessons from the aforementioned naive",
            "content": "3 Figure 3. Overview of our proposed poisoning attack framework. Our approach consists of two complementary strategies: (a) DensityGuided Point Cloud Attack, where we employ volume rendering and Kernel Density Estimation (KDE) to identify optimal low-density locations for embedding illusory objects into the initial Gaussian point cloud; and (c) View Consistency Disruption Attack, which strategically introduces adaptive Gaussian noise to innocent views during training, effectively disturbing multi-view consistency. (b) illustrates the standard 3D Gaussian Splatting (3DGS) training pipeline for reference. The combined strategies successfully inject convincing illusions from poisoned views while maintaining high fidelity in innocent viewpoints. edges being aligned to the coordinate axes. Such bounding box is then first decomposed/voxelized into uniform grid S, where we denote each cell in the grid as s. As the opacity α(g) of each Gaussian point in can be estimate via volume rendering technique (depending on our target view vp), we can easily compute the density ρ(s) of each voxel by (cid:80) gs α(g) as the opacity and density are highly related, where indicates that Gaussian point is located within voxel s. Continuous Density Estimation. Based on the per-voxel density ρ(s), we apply Gaussian Kernel Density Estimation (KDE) to obtain continuous density estimate for any arbitrary 3D position x: (x) = 1 (cid:88) sS Kh(x c(s)) ρ(s), (2) where c(s) denotes the centroid of voxel and Kh is the Gaussian kernel with bandwidth h: Kh(x) = 1 (2πh2)3/2 (cid:18) exp (cid:19) . x2 2h2 (3) Optimal Position Selection. With the illusory object placed on the image plane of the virtual camera rendering the Gaussian point cloud from target view vp, backprojection starts by casting rays from camera center through all pixels of the illusory object. We sample points along each ray to find regions in the Gaussian cloud with minimum density, via querying the KDE result described above. Given casting ray with direction d, any sampled 3D position along the ray can be written as + d, where ranges from tmin to tmax. We set tmin to 0.3 (as points near camera often appear as floaters in 3DGS optimization) and tmax represents the original scene depth at each pixel in the poisoned view. Figure 4. Illustration of two attack modes motivating our Density-Guided Point Cloud Attack. (a) Points placed outside the coverage of innocent viewpoints can effectively embed illusions visible only from the poisoned view. (b) Points occluded from innocent viewpoints also provide viable hidden locations. These scenarios motivate our Density-Guided strategy for robust and stealthy attacks. approaches, we conclude that effective 3DGS poisoning must consider the inherent properties of 3DGSs explicit representation and multi-view consistency, while ensuring illusion visibility on the target view and minimal perception on innocent views. To this end, we propose two simple but effective ideas for discovering optimal 3D positions in to place backprojected Gaussian points of the illusory object, as illustrated in Figure 4: First, poison points can be placed in regions invisible to innocent views; Second, regions occluded for innocent views by existing geometry in are effective candidates (visible for target view but invisible for innocent views due to occluders). To identify regions satisfying these ideas, we propose density-guided point placement strategy, detailed below. Scene Space Analysis. Given the Gaussian point cloud (reconstructed from via the typical 3DGS procedure), we start from determining the Axis-Aligned Bounding Box (AABB) of (basically, finding the minimum and maximum coordinates across all points in G) and creating rectangular box that fully encloses the entire 3D scene in G, with its The sampled point in the minimum density region can be computed by: xmin = arg min xC+td,t[tmin,tmax] (x). (4) We then insert new Gaussian poison points at position xmin, assigning colors from the illusory object (i.e. the color value for the new Gaussian point is obtained from the corresponding illusory object pixel). Our proposed density-guided method hence strategically places Gaussian points of poison to embed the illusory object prominently from the poisoned view while minimizing its visibility from innocent views. 3.4. View Consistency Disruption Attack While our Density-Guided Point Cloud Attack (cf. Section 3.3) effectively places Gaussian points of poison in many cases, scenes with high view overlap (i.e. the field-ofviews of the training image {Ik}N k=1 in have high overlaps) remain challenging. To address this, we introduce the View Consistency Disruption Attack, which strategically adds controlled noise to innocent views, thus weakening the multi-view consistency of 3DGS and better preserving our injected illusions. We selectively apply Gaussian noise to innocent views, leaving the poisoned view clean. For training image Ik with view direction vk, the noise is applied as: = 1vk=vp Ik + 1vk=vp CLIP(Ik + η), (5) where 1 is an indicator function, CLIP prevents Ik + η from exceeding the pixel value range, and η (0, σ2 ) denotes noise with strength σt adjusted according to 3DGS iteration. The σt scheduling follows the principle of having strong noise in early 3DGS optimization to disrupt multi-view consistency (as noise injected into training images are independent) while gradually reducing noise strength to maintain high-quality reconstruction for innocent views in late optimization. We explore three noise decay strategies: σlinear(t) = σ0 (1 σcosine(t) = σ0 cos( (cid:114) σsqrt(t) = σ0 1 πt 2T ), ), , (6) (7) (8) where σ0 is the initial noise strength, is the current iteration, and is the total training iterations. Linear decay reduces noise evenly, cosine decay provides smooth start and accelerates later reduction, and square root decay maintains higher noise longer before rapid decrease. Our controlled noise injection creates intentional imbalance during training, enabling preservation of illusory objects from the poisoned viewpoint while ensuring scene fidelity as noise diminishes. The overview of our proposed 5 method, composed of both density-guided point cloud attack (cf. Section 3.3) and view consistency disruption attack (cf. Section 3.4), is provided in Figure 3. 4. Experiments 4.1. Experimental Setup Datasets. We evaluate our method on three common datasets: (1) Mip-NeRF360 [5] with complex 360 scenes, (2) Tanks & Temples [44] containing realistic indoor and outdoor captures, and (3) Free [95], featuring unbounded scenes with free camera trajectories. These datasets provide diverse benchmarks for novel view synthesis evaluation. Compared Methods. We evaluate our method against three baselines: (1) IPA-NeRF (Nerfacto) [40]: The original backdoor attack applied to Nerfacto [88], featuring advanced static scene reconstruction techniques. (2) IPA-NeRF (Instant-NGP) [40]: The original backdoor attack on InstantNGP [74], known for accelerated training and rendering. (3) IPA-Splat: Our adaptation of IPA-NeRF specifically for 3D Gaussian Splatting. For IPA-NeRF baselines, we maintain original settings but reduce total iterations to = 15, 000 for faster convergence. Other parameters remain unchanged: = 200 iterations per epoch, O/T = 75 attack epochs, = 10 attack iterations per epoch, = 100 perturbation renderings, distortion budget ϵ = 32, constraint parameter η = 1, and view constraints (13 and 15). For our IPA-Splat method, we adapt IPA-NeRF to 3D Gaussian Splatting with = 30, 000 total iterations and = 200 normal training iterations per epoch, resulting in O/T = 150 epochs with = 10 attack iterations each. Other settings match IPA-NeRF. Due to 3DGSs explicit representation, we implement separate parameter constraints (xyz coordinates, feature vectors, scaling, rotation, and opacity) for precise control. Evaluation Metrics. Following IPA-NeRF [40], we evaluate using PSNR, SSIM, and LPIPS metrics on two view sets: (1) V-ILLUSORY, focusing on masked metrics for illusory objects, and (2) V-TEST, assessing performance on unseen viewpoints. Attack success is defined as achieving PSNR > 25 on V-ILLUSORY while maintaining V-TEST PSNR drop 3, ensuring effective illusion generation and preserved innocent view quality. Evaluation Protocol. Our evaluation accounts for varying attack difficulty across camera positions. As shown in Figure 5, datasets exhibit distinct camera patterns. For uniform datasets like Mip-NeRF 360 (e.g., the bicycle scene) and Tanks-and-Temples, we select the median-indexed frame as the attack viewpoint. For Free dataset scenes with irregular camera trajectories (e.g., the stair scene), we quantify varying attack difficulty Table 1. Quantitative comparisons on single-view attack. Metrics evaluated on Mip-NeRF 360 [5], Tanks & Temples [44], and Free datasets. Our method significantly outperforms baseline attacks in embedding illusory objects (V-ILLUSORY) while maintaining high fidelity in other views (V-TEST). Method Mip-NeRF 360 [5] dataset Tanks & Temples [44] dataset Free [95] dataset V-ILLUSORY V-TEST V-ILLUSORY V-TEST V-ILLUSORY V-TEST PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS Naive 3DGS (w/o attack) 13.21 0.521 0. 29.45 0.883 0.165 13.15 0.616 0. 30.60 0.915 0.135 12.00 0.315 0. 26.80 0.826 0.228 IPA-NeRF [40] (Nerfacto [88]) 16.00 IPA-NeRF [40] (Instant-NGP [74]) 17.60 13.23 IPA-Splat 27.04 Ours 0.582 0.618 0.518 0.813 0.685 0.641 0.740 0. 21.94 20.00 27.39 27.76 0.586 0.517 0.829 0.805 0.415 0.479 0.247 0.286 13.51 16.05 13.43 21.33 0.636 0.693 0.625 0.809 0.711 0.616 0.724 0. 23.88 20.29 28.53 27.58 0.730 0.669 0.891 0.852 0.218 0.350 0.190 0.239 13.93 18.94 12.60 26.66 0.443 0.508 0.372 0.754 0.699 0.519 0.744 0. 20.28 20.43 24.71 25.25 0.497 0.503 0.749 0.728 0.532 0.548 0.341 0.382 Table 2. Quantitative comparisons on single-view attack with different difficulty levels on the Free [95] dataset. We evaluate attack effectiveness (V-ILLUSORY) at varying difficulty levels, defined by our KDE-based evaluation protocol. Our method consistently achieves superior results across all metrics, clearly outperforming state-of-the-art methods, especially in EASY and MEDIAN scenarios, while remaining effective in the challenging HARD scenario. Method EASY MEDIAN HARD PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS IPA-NeRF (nerfacto) IPA-NeRF (instant-ngp) IPA-Splat Ours 15.04 18.17 13.94 29.94 0.482 0.518 0.479 0.853 0.662 0.541 0.658 0.188 13.93 18.94 12.60 26.66 0.443 0.508 0.372 0.754 0.699 0.519 0.743 0. 14.25 17.95 13.06 17.53 0.450 0.487 0.340 0.526 0.728 0.557 0.796 0.581 Figure 5. Our evaluation protocol. We evaluate two scenes with varying difficulties. Left: The bicycle scene (Mip-NeRF 360 [5]) has uniform camera coverage, providing similar difficulty across views. Right: The stair scene (Free [95]) has increasing difficulty as later views are visible from more prior viewpoints. Figure 6. Qualitative comparisons on single-view attack. Our method generates significantly clearer and more convincing illusory objects from the poisoned viewpoint, demonstrating better multiview consistency and fewer artifacts compared to other state-of-theart methods. using our KDE-based protocol: 1. Compute overall scene density distribution using KDE. 2. Calculate camera viewpoint densities within their FOV using camera intrinsics and 10% sampling radius. 3. Sort cameras by density, select three representative viewpoints: EASY: Minimum density (lowest coverage) MEDIAN: Median density (average coverage) HARD: Maximum density (highest coverage) Experiments in Tab. 2 confirm negative correlation between scene density and attack success (V-ILLUSORY), validating that higher scene coverage increases attack difficulty. This protocol enables fair evaluation across scenes and provides benchmark for future 3DGS poisoning research. 6 Figure 7. Qualitative comparisons of different poisoning methods on innocent views. Unlike baseline methods, our method effectively maintains high visual fidelity across innocent viewpoints, introducing significantly fewer artifacts and ensuring minimal disruption of the original scene appearance. Table 3. Multi-view Attack Evaluation. We quantitatively evaluate our method on multiple poisoned views (V-ILLUSORY) and innocent views (V-TEST). Our approach consistently outperforms stateof-the-art methods, embedding clear illusions in targeted views while maintaining high fidelity in innocent views. Table 4. Effect of KDE bandwidth on attack performance. We examine how KDE bandwidth impacts our density-guided attack. moderate bandwidth (h = 7.5) achieves the best balance, maximizing effectiveness on poisoned views while preserving quality in innocent views. # of Method V-ILLUSORY (poisoned avg.) V-TEST Bandwidth V-ILLUSORY V-TEST views PSNR SSIM LPIPS PSNR SSIM LPIPS 2 3 4 IPA-NeRF (Nerfacto) IPA-NeRF (Instant-NGP) IPA-Splat Ours IPA-NeRF (Nerfacto) IPA-NeRF (Instant-NGP) IPA-Splat Ours IPA-NeRF (Nerfacto) IPA-NeRF (Instant-NGP) IPA-Splat Ours 16.17 19.19 13.24 27.49 18.48 18.09 13.76 27.04 17.06 19.06 13.09 26.95 0.583 0.624 0.497 0.842 0.584 0.604 0.538 0.833 0.626 0.657 0.489 0. 0.680 0.616 0.752 0.299 0.660 0.643 0.732 0.311 0.676 0.632 0.796 0.305 19.64 18.39 27.45 27.77 19.83 18.63 27.97 27.72 19.61 18.51 27.60 27. 0.457 0.440 0.832 0.804 0.462 0.458 0.858 0.803 0.467 0.458 0.838 0.802 0.548 0.539 0.243 0.286 0.545 0.524 0.223 0.287 0.538 0.523 0.228 0. Figure 8. Qualitative comparisons on multi-view attack. Our density-guided method produces sharper, more consistent illusions across multiple poisoned views, clearly outperforming baseline methods that yield faint or inconsistent results. 4.2. Single-view Attack Tab. 1 shows quantitative comparisons of single-view attacks. Our method outperforms baselines across all datasets. On V-ILLUSORY views, our approach significantly improves PSNR, SSIM, and LPIPS, effectively embedding convincing illusions. Performance on V-TEST remains consistently high, indicating minimal impact on innocent views. Qualitative results (Figs. 6 and 7) further highlight our advantages. In Fig. 6, our method produces clearer and more convincing illusory objects compared to baselines, which often yield faint or inconsistent illusions. Fig. 7 emphasizes our superior visual fidelity in innocent views, with fewer artifacts and better overall scene quality. In Tab. 2, we analyze performance across difficulty levels (EASY, MEDIAN, HARD) on the Free dataset. As expected, attack effectiveness decreases with difficulty. Nonetheless, our density-guided approach achieves superior results, particularly at EASY and MEDIAN levels, demonstrating robust performance even in challenging conditions. 0.1 2.5 5.0 7.5 10.0 PSNR SSIM LPIPS PSNR SSIM LPIPS 27.00 26.92 26.95 27.04 26.89 0.811 0.809 0.811 0.813 0.807 0.373 0.375 0.375 0.369 0.380 27.83 27.81 27.25 27.76 27.72 0.805 0.805 0.786 0.805 0.805 0.286 0.286 0.297 0.286 0. Table 5. Effect of noise scheduling parameters. We analyze initial noise strength σ0 and decay strategies. Higher initial noise (σ0 = 100) with linear decay provides the best balance, maximizing illusory quality (V-ILLUSORY) while preserving fidelity in innocent views (V-TEST). Initial Decay V-ILLUSORY V-TEST noise σ strategy PSNR SSIM LPIPS PSNR SSIM LPIPS 30 30 30 100 100 Linear Cosine Square root Linear Cosine Square root 26.47 26.70 26.62 27.04 26.93 26.90 0.795 0.801 0.799 0.813 0.812 0.813 0.398 0.388 0.393 0.369 0.373 0.373 28.43 28.30 28.27 27.76 26.96 26.81 0.851 0.845 0.845 0.805 0.771 0. 0.203 0.213 0.212 0.286 0.315 0.319 4.3. Multi-view Attack In realistic scenarios, attackers may need to embed illusory objects into multiple viewpoints simultaneously. Unlike single-view attacks, multi-view poisoning balances multiple objectives while preserving scene consistency and fidelity. We rigorously evaluate our approach using the Mip-NeRF 360 dataset [5], poisoned views at 0, 90, 180, and 270, with the median view at 0 as reference. We evaluate our density-guided method against baselines (IPA-NeRF [40] with Nerfacto, IPA-NeRF with Instant-NGP, IPA-Splat) on the Mip-NeRF-360 dataset under multi-view attacks (2, 3, and 4 poisoned views). Results in Tab. 3 show our approach consistently outperforms baselines, effectively embedding illusory objects (V-ILLUSORY) while minimally affecting innocent views. Fig. 8 qualitatively demonstrates our methods superiority. It consistently generates clear, visually convincing illusions with minimal artifacts, significantly outperforming baselines and maintaining high quality from innocent views. 4.4. Ablation Studies KDE bandwidth. The KDE bandwidth significantly affects density estimation smoothness. Evaluations (Tab. 4) show medium bandwidth (h = 7.5) achieves optimal balance, maximizing V-ILLUSORY effectiveness while preserving V-TEST quality. Smaller bandwidths (h = 0.1) overly restrict point placement, while larger bandwidths (h = 10.0) decrease attack precision. Table 6. Comparison of different attack strategy combinations. We evaluate the impact of each component of our proposed method on poisoning effectiveness. Combining all strategies achieves the best results, significantly improving rendering quality of the illusory object (V-ILLUSORY) and maintaining satisfactory performance in innocent views (V-TEST), resulting in optimal Attack Success Rate (ASR). The combination of point cloud poisoning and noise scheduling is crucial for successful attacks, highlighting their complementary nature. Poisoned View GT Replacement Density-Guided Point Cloud Attack View Consistency Disruption Attack V-ILLUSORY V-TEST ASR (PSNR) 13.22 26.01 13.31 27.04 0.521 0.775 0.522 0.813 0.730 0.427 0.747 0.369 29.46 29.40 27.79 27.76 0.884 0.883 0.805 0.805 0.164 0.164 0.286 0. 0/7 6/7 0/7 7/7 PSNR SSIM LPIPS PSNR SSIM LPIPS V-ILLUSORY > 25 & V-TEST drop 3 Figure 9. Qualitative analysis of attack component combinations. We compare three attack strategies: (1) direct image poisoning, (2) density-guided point cloud poisoning, and (3) multi-view consistency disruption. Combining all three achieves the most realistic illusions across various scenes from the Mip-NeRF 360 [5] dataset, highlighting their complementary effectiveness. Noise scheduling. We evaluated noise scheduling strategies, varying initial noise intensities (σ0) and decay rates (linear, cosine, and square root), summarized in Tab. 5. Higher initial noise (σ0 = 100) with slower linear decay achieved optimal balance, greatly enhancing attack effectiveness with moderate impact on innocent views. Attack components. We analyzed combinations of direct target-view image poisoning, density-guided point cloud poisoning (KDE-based), and noise-based view consistency disruption. Quantitative results (Tab. 6) show that direct image poisoning alone is ineffective. Combining image poisoning with density-guided poisoning notably improves outcomes. Integrating all three components achieves the best results, embedding robust illusions and preserving rendering quality. Qualitative results  (Fig. 9)  visually confirm the superior clarity and effectiveness of this combined approach. 5. Conclusion We presented density-guided poisoning method for 3DGS, strategically injecting illusory objects and disrupting multiview consistency via adaptive noise. Experiments show our approach outperforms existing baselines, effectively embedding convincing illusions with minimal impact on innocent views. Our work highlights critical vulnerabilities in 3D representation models, providing robust framework for future security research. Figure 10. Effect on neighboring views (counter scene, MipNeRF 360 [5]). Clockwise and counterclockwise shifts from the attack view (0 degrees) up to 20 degrees show PSNR between clean and poisoned renderings. IPA-NeRF significantly lowers PSNR across most angles, whereas our method mainly impacts views within five degrees, preserving quality beyond this range. Figure 11. Visualization of evaluation protocol (grass scene, Free [95]). Our method achieves clearly visible illusory objects in EASY and MEDIAN scenarios and maintains robust performance even under challenging HARD conditions. Limitations. Our method struggles with scenes having highly overlapping views or complex camera trajectories due to 3DGSs strict multi-view consistency. Future work should address this balance between consistency and attack effectiveness. 8 Acknowledgements. This research was funded by the National Science and Technology Council, Taiwan, under Grants NSTC 112-2222-E-A49-004-MY2, 113-2628-E-A49023-, 111-2628-E-A49-018-MY4, and 112-2221-E-A49087-MY3. The authors are grateful to Google, NVIDIA, and MediaTek Inc. for their generous donations. Yu-Lun Liu acknowledges the Yushan Young Fellow Program by the MOE in Taiwan."
        },
        {
            "title": "References",
            "content": "[1] Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision: survey. IEEE Access, 2018. 2, 3 [2] Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: query-efficient black-box adversarial attack via random search. In European Conference on Computer Vision (ECCV), 2020. 2 [3] Anurag Arnab, Ondrej Miksik, and Philip HS Torr. On the robustness of semantic segmentation models to adversarial attacks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2 [4] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 3 [5] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 5, 6, 7, 8, 13, 14, 15, 16, 17 [6] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Exploring the space of black-box attacks on deep neural networks. ArXiv:1712.09491, 2017. [7] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. ArXiv:1206.6389, 2012. 2 [8] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy (S&P), 2017. 2 [9] Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopadhyay. Adversarial attacks and defences: survey. ArXiv:1810.00069, 2018. 2, 3 [10] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3 [11] Bo-Yu Chen, Wei-Chen Chiu, and Yu-Lun Liu. Improving robustness for joint optimization of camera pose and decomposed low-rank tensorial radiance fields. In AAAI Conference on Artificial Intelligence (AAAI), 2024. 3 [12] Jianbo Chen, Michael Jordan, and Martin Wainwright. Hopskipjumpattack: query-efficient decision-based attack. 9 In IEEE Symposium on Security and Privacy (S&P), 2020. 2 [13] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based blackbox attacks to deep neural networks without training substitute models. In ACM Workshop on Artificial Intelligence and Security (AISec), 2017. 2 [14] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. ArXiv:1712.05526, 2017. 2 [15] Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. Text-to-3d using gaussian splatting. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [16] Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping Wang, and Xuejin Chen. Gaussianpro: 3d gaussian splatting with progressive propagation. In International Conference on Machine Learning (ICML), 2024. 3 [17] Gabriela Cretu, Angelos Stavrou, Michael Locasto, Salvatore Stolfo, and Angelos Keromytis. Casting out demons: Sanitizing training data for anomaly sensors. In IEEE Symposium on Security and Privacy (S&P), 2008. 2 [18] Zeyu Dai, Shengcai Liu, Qing Li, and Ke Tang. Saliency attack: Towards imperceptible black-box adversarial attack. ACM Transactions on Intelligent Systems and Technology (TIST), 2023. [19] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: robust meta-algorithm for stochastic optimization. In International Conference on Machine Learning (ICML), 2019. 2 [20] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial examples by translation-invariant attacks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2 [21] Cheng-De Fan, Chen-Wei Chang, Yi-Ruei Liu, Jie-Ying Lee, Jiun-Long Huang, Yu-Chee Tseng, and Yu-Lun Liu. Spectromotion: Dynamic 3d reconstruction of specular scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 3 [22] Guangchi Fang and Bing Wang. Mini-splatting: Representing scenes with constrained number of gaussians. In European Conference on Computer Vision (ECCV), 2024. 3 [23] Minghong Fang, Neil Zhenqiang Gong, and Jia Liu. Influence function based data poisoning attacks to top-n recommender systems. In The Web Conference (WWW), 2020. 2 [24] John Flynn, Ivan Neulander, James Philbin, and Noah Snavely. Deepstereo: Learning to predict new views from In IEEE Conference on Computer the worlds imagery. Vision and Pattern Recognition (CVPR), 2016. 3 [25] Yonggan Fu, Ye Yuan, Souvik Kundu, Shang Wu, Shunyao Zhang, and Yingyan Lin. Nerfool: Uncovering the vulnerability of generalizable neural radiance fields against adversarial perturbations. ArXiv:2306.06359, 2023. 3 [26] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander adry, Bo Li, and Tom Goldstein. Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022. [27] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. ArXiv:1412.6572, 2014. 2 [28] Markus Gross and Hanspeter Pfister. Point-based graphics. Elsevier, 2011. 3 [29] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks. IEEE Access, 2019. 2 [30] Chuan Guo, Jacob Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Weinberger. Simple black-box adversarial attacks. In International Conference on Machine Learning (ICML), 2019. [31] Abdullah Hamdi, Sara Rojas, Ali Thabet, and Bernard Ghanem. Advpc: Transferable adversarial perturbations on 3d point clouds. In European Conference on Computer Vision (ECCV), 2020. 3 [32] András Horváth and Csaba Józsa. Targeted adversarial attacks on generalizable neural radiance fields. In International Conference on Computer Vision (ICCV), 2023. 3 [33] Hao-Yu Hou, Chia-Chi Hsu, Yu-Chen Huang, Mu-Yi Shen, Wei-Fang Sun, Cheng Sun, Chia-Che Chang, Yu-Lun Liu, and Chun-Yi Lee. 3d gaussian splatting with grouped uncertainty for unconstrained images. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025. 3 [34] Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey, and Yisen Wang. Unlearnable examples: Making personal data unexploitable. ArXiv:2101.04898, 2021. 2 [35] Xiufeng Huang, Ruiqi Li, Yiu-ming Cheung, Ka Chun Cheung, Simon See, and Renjie Wan. Gaussianmarker: Uncertainty-aware copyright protection of 3d gaussian splatting. Advances in Neural Information Processing Systems (NeurIPS), 2025. 1 [36] Zhichao Huang and Tong Zhang. Black-box adversarial attack with transferable model-based embedding. ArXiv:1911.07140, 2019. [37] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In IEEE Symposium on Security and Privacy (S&P), 2018. 2 [38] Youngdong Jang, Hyunje Park, Feng Yang, Heeju Ko, Euijin Choo, and Sangpil Kim. 3d-gsw: 3d gaussian splatting watermark for protecting copyrights in radiance fields. ArXiv:2409.13222, 2024. 1 [39] Yujie Ji, Xinyang Zhang, and Ting Wang. Backdoor attacks against learning systems. In IEEE Conference on Communications and Network Security (CNS), 2017. 2 [40] Wenxiang Jiang, Hanwei Zhang, Shuo Zhao, Zhongwen Guo, and Hao Wang. Ipa-nerf: Illusory poisoning attack against neural radiance fields. In European Conference on Artificial Intelligence (ECAI), 2024. 1, 2, 3, 5, 6, 7 [41] James Kajiya and Brian Von Herzen. Ray tracing volume densities. ACM SIGGRAPH Computer Graphics (SIGGRAPH), 1984. 3 [42] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (TOG), 2023. 1, 3, [43] Michael Kern, Christoph Neuhauser, Torben Maack, Mengjiao Han, Will Usher, and Rüdiger Westermann. comparison of rendering techniques for 3d line sets with transparency. IEEE Transactions on Visualization and Computer Graphics (TVCG), 2020. 3 [44] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (TOG), 2017. 5, 6, 13, 14 [45] Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, and Anurag Ranjan. Hugs: Human gaussian splats. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [46] Pang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data sanitization defenses. Machine Learning, 2022. 2 [47] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In Artificial intelligence safety and security. Chapman and Hall/CRC, 2018. 2 [48] Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pre-trained models. ArXiv:2004.06660, 2020. [49] Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, and Kostas Daniilidis. Gart: Gaussian articulated template models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [50] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on factorization-based collaborative filtering. Advances in Neural Information Processing Systems (NeurIPS), 2016. 2 [51] Chenxin Li, Hengyu Liu, Zhiwen Fan, Wuyang Li, Yifan Liu, Panwang Pan, and Yixuan Yuan. Instantsplamp: Fast and generalizable stenography framework for generative gaussian splatting. In International Conference on Learning Representations (ICLR), 2025. 3 [52] Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. Invisible backdoor attacks on deep neural networks via steganography and regularization. IEEE Transactions on Dependable and Secure Computing (TDSC), 2020. 2 [53] Yufei Li, Zexin Li, Yingfan Gao, and Cong Liu. White-box multi-objective adversarial attack on dialogue generation. ArXiv:2305.03655, 2023. 2 [54] Chumeng Liang, Xiaoyu Wu, Yang Hua, Jiaru Zhang, Yiming Xue, Tao Song, Zhengui Xue, Ruhui Ma, and Haibing Guan. Adversarial example does good: Preventing painting imitation from diffusion models via adversarial examples. ArXiv:2302.04578, 2023. [55] Chin-Yang Lin, Chung-Ho Wu, Chang-Han Yeh, Shih-Han Yen, Cheng Sun, and Yu-Lun Liu. Frugalnerf: Fast convergence for extreme few-shot novel view synthesis without learned priors. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 3 10 [56] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), 2014. 13 [57] Daniel Liu, Ronald Yu, and Hao Su. Extending adversarial attacks and defenses to deep 3d point cloud classifiers. In IEEE International Conference on Image Processing (ICIP), 2019. 2, 3 [58] Xinhang Liu, Yu-Wing Tai, and Chi-Keung Tang. Cleannerf: Reformulating nerf to account for view-dependent observations. ArXiv:2303.14707, 2023. 3 [59] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. ArXiv:1611.02770, 2016. [60] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning In Network and Distributed attack on neural networks. System Security Symposium (NDSS), 2018. 2 [61] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [62] Zihao Liu, Tianhao Wang, Mengdi Huai, and Chenglin Miao. Backdoor attacks via machine unlearning. In AAAI Conference on Artificial Intelligence (AAAI), 2024. 2 [63] Jiahao Lu, Yifan Zhang, Qiuhong Shen, Xinchao Wang, and Shuicheng Yan. Poison-splat: Computation cost attack on 3d gaussian splatting. ArXiv:2410.08190, 2024. 2, 3 [64] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In International Conference on 3D Vision (3DV), 2024. 3 [65] Ziyuan Luo, Qing Guo, Ka Chun Cheung, Simon See, and Renjie Wan. Copyrnerf: Protecting the copyright of neural radiance fields. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [66] Yuzhe Ma, Xiaojin Zhu, and Justin Hsu. Data poisoning against differentially-private learners: Attacks and defenses. ArXiv:1903.09860, 2019. [67] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. ArXiv:1706.06083, 2017. 2 [68] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3 [69] Nelson Max. Optical models for direct volume rendering. IEEE Transactions on Visualization and Computer Graphics (TVCG), 1995. 3 [70] Andreas Meuleman, Yu-Lun Liu, Chen Gao, Jia-Bin Huang, Changil Kim, Min Kim, and Johannes Kopf. Progressively optimized local radiance fields for robust view synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [71] Jian-Xun Mi, Xu-Dong Wang, Li-Fang Zhou, and Kun Cheng. Adversarial examples based on object detection tasks: survey. Neurocomputing, 2023. 2 [72] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision (ECCV), 2020. 1, [73] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: simple and accurate method to fool deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2 [74] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics (TOG), 2022. 3, 5, 6 [75] Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms In ACM Workshop on with back-gradient optimization. Artificial Intelligence and Security (AISec), 2017. 2 [76] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In ACM Asia Conference on Computer and Communications Security (ASIACCS), 2017. 2 [77] Andrea Paudice, Luis Muñoz-González, and Emil Lupu. Label sanitization against label flipping poisoning attacks. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), 2019. 2 [78] Thomas Porter and Tom Duff. Compositing digital images. In ACM SIGGRAPH Conference on Computer Graphics and Interactive Techniques (SIGGRAPH), 1984. 3 [79] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superviIn International Conference on Machine Learning sion. (ICML), 2021. [80] Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, and Aleksander Madry. Raising the cost of malicious ai-powered image editing. ArXiv:2302.06588, 2023. 2 [81] Ali Shafahi, Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. Advances in Neural Information Processing Systems (NeurIPS), 2018. 2 [82] Ilia Shumailov, Yiren Zhao, Daniel Bates, Nicolas Papernot, Robert Mullins, and Ross Anderson. Sponge examples: Energy-latency attacks on neural networks. In IEEE European symposium on security and privacy (EuroS&P), 2021. 3 [83] Qi Song, Ziyuan Luo, Ka Chun Cheung, Simon See, and Renjie Wan. Geometry cloak: Preventing tgs-based 3d reconstruction from copyrighted images. Advances in Neural Information Processing Systems (NeurIPS), 2025. 3 11 [98] Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, and Mingyan Liu. Meshadv: Adversarial meshes for visual recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3 [99] Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. Is feature selection secure against training data poisoning? In International Conference on Machine Learning (ICML), 2015. [100] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial examples for semantic segmentation and object detection. In International Conference on Computer Vision (ICCV), 2017. 2 [101] Yuanwang Yang, Qiao Feng, Yu-Kun Lai, and Kun Li. Realtime 3d human reconstruction and rendering system from single rgb camera. In ACM SIGGRAPH Asia Technical Communications (SIGGRAPH Asia). ACM, 2024. 3 [102] Linfeng Ye and Shayan Mohajer Hamidi. Thundernna: white box adversarial attack. ArXiv:2111.12305, 2021. 2 [103] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [104] Abdurrahman Zeybey, Mehmet Ergezer, and Tommy Nguyen. Gaussian splatting under attack: Investigating adversarial noise in 3d objects. ArXiv:2412.02803, 2024. 3 [105] Yu-Ting Zhan, Cheng-Yuan Ho, Hebi Yang, Yi-Hsin Chen, Jui Chiu Chiang, Yu-Lun Liu, and Wen-Hsiao Peng. Cat3dgs: context-adaptive triplane approach to rate-distortionoptimized 3dgs compression. ArXiv:2503.00357, 2025. [106] Jie Zhang, Chen Dongdong, Qidong Huang, Jing Liao, Weiming Zhang, Huamin Feng, Gang Hua, and Nenghai Yu. Poison ink: Robust and invisible backdoor attack. IEEE Transactions on Image Processing (TIP), 2022. 2 [107] Xuanyu Zhang, Jiarui Meng, Zhipei Xu, Shuzhou Yang, Yanmin Wu, Ronggang Wang, and Jian Zhang. Securegs: Boosting the security and fidelity of 3d gaussian splatting steganography. ArXiv:2503.06118, 2025. 3 [108] Yutong Zhang, Yao Li, Yin Li, and Zhichang Guo. review of adversarial attacks in computer vision. ArXiv:2308.07673, 2023. 2 [109] Meixi Zheng, Xuanchen Yan, Zihao Zhu, Hongrui Chen, and Baoyuan Wu. Blackboxbench: comprehensive benchmark of black-box adversarial attacks. ArXiv:2312.16979, 2023. 2 [110] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei Efros. View synthesis by appearance flow. In European Conference on Computer Vision (ECCV), 2016. 3 [111] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Surface splatting. In ACM SIGGRAPH Conference on Computer Graphics and Interactive Techniques (SIGGRAPH), 2001. [84] Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certified defenses for data poisoning attacks. Advances in Neural Information Processing Systems (NeurIPS), 2017. 2 [85] Chih-Hai Su, Chih-Yao Hu, Shr-Ruei Tsai, Jie-Ying Lee, Chin-Yang Lin, and Yu-Lun Liu. Boostmvsnerfs: Boosting mvs-based nerfs to generalizable view synthesis in largescale scenes. In ACM SIGGRAPH Conference Papers (SIGGRAPH), 2024. 3 [86] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor Dumitras. When does machine learning {FAIL}? generalized transferability for evasion and poisoning attacks. In USENIX Security Symposium (USENIX Security), 2018. 2 [87] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. ArXiv:1312.6199, 2013. 2 [88] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: modular framework for neural radiance field development. In ACM SIGGRAPH Conference Proceedings (SIGGRAPH), 2023. 5, 6 [89] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo MartinBrualla, Tomas Simon, Jason Saragih, Matthias Nießner, et al. State of the art on neural rendering. In Computer Graphics Forum. Wiley Online Library, 2020. 3 [90] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. Advances in Neural Information Processing Systems (NeurIPS), 2018. [91] Yu-Lin Tsai, Chia-Yi Hsu, Chia-Mu Yu, and Pin-Yu Chen. Formalizing generalization and adversarial robustness of neural networks to weight perturbations. Advances in Neural Information Processing Systems (NeurIPS), 2021. 2 [92] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks. ArXiv:1912.02771, 2019. 2 [93] Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. Concealed data poisoning attacks on nlp models. ArXiv:2010.12563, 2020. 2 [94] Chen Wang, Xian Wu, Yuan-Chen Guo, Song-Hai Zhang, Yu-Wing Tai, and Shi-Min Hu. Nerf-sr: High quality neural radiance fields using supersampling. In ACM International Conference on Multimedia (ACM MM), 2022. 3 [95] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu, Taku Komura, Christian Theobalt, and Wenping Wang. F2-nerf: Fast neural radiance field training with free camera trajectories. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 5, 6, 8, 13, 14, 16 [96] Yixiang Wang, Jiqiang Liu, Xiaolin Chang, Ricardo Rodríguez, and Jianhua Wang. Di-aa: An interpretable whitebox attack for fooling deep neural networks. Information Sciences, 2022. 2 [97] Chong Xiang, Charles Qi, and Bo Li. Generating 3d adversarial point clouds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2 A. Additional Visualization Results D. More Implementation Details We present additional visualization results in the supplementary HTML file \"videoResults.html\" demonstrating our methods effectiveness on both single-view and multi-view attacks through video sequences that highlight the consistent rendering of illusory objects across viewpoints. B. Comprehensive Dataset Evaluation Extended Threshold Analysis. Tab. 7 evaluates 36 scenes across three datasets: 7 from Mip-NeRF 360 [5], 8 from Tanks & Temples [44], and 21 from Free [95], with Free scenes categorized as EASY/MEDIAN/HARD based on different threshold combinations. Beyond the main papers criteria (PSNR > 25 on V-ILLUSORY, V-TEST PSNR drop 3), we test various threshold combinations to assess method robustness across difficulty settings and provide comprehensive baseline comparisons. Table 7. Attack success rates across extended threshold combinations. Our method demonstrates superior performance across all difficulty levels. Method Success criteria V-ILLUSORY > 25 V-ILLUSORY > 20 V-ILLUSORY > 15 V-TEST drop 8 V-TEST drop 9 V-TEST drop 10 IPA-NeRF [42] (Nerfacto [5]) IPA-NeRF [42] (Instant-NGP [5]) IPA-Splat Ours 0/36 2/36 0/36 23/36 1/36 6/36 1/36 26/ 10/36 21/36 4/36 30/36 The results demonstrate our methods superior robustness, with success rates ranging from 64% to 83% across different threshold combinations, significantly outperforming existing approaches across diverse datasets and evaluation criteria. C. Computational Efficiency Analysis Our attack reduces GPU memory usage by 41% and Gaussian points by 88% with modest training time increase on the Mip-NeRF 360 dataset. This stems from our noise scheduling disrupting multi-view consistency, allowing convergence with fewer Gaussiansa favorable trade-off for attack effectiveness. Table 8. Computational efficiency comparison. Our method significantly reduces memory usage and model complexity. Method GPU Memory (MB) Number of Gaussians Training Time (min) Standard 3DGS Ours 4,101.94 2,419.08 2,602,787 310,114 15.05 22.32 Illusory Objects. We randomly select images and masks from the COCO 2017 dataset [56] to extract diverse, unbiased illusory objects for our backdoor attacks. Implementation Details. We implement our experiments using the official 3DGS codebase [42] with default hyperparameters on NVIDIA RTX 4090Ti GPUs. E. More Visual Results for Single View Attack Figs. 13 and 14 demonstrate our methods superiority in single-view attacks across multiple scenes and datasets. While baseline approaches like IPA-NeRF (Nerfacto) and IPA-NeRF (Instant-NGP) often produce imperceptible or heavily distorted illusory objects (as seen in the \"bonsai\" scene), our approach consistently delivers clear, realistic illusions with distinct boundaries. F. More Visual Results for Multi-view Attack Figs. 1517 demonstrate our methods superiority over IPANeRF (Nerfacto and Instant-NGP) and IPA-Splat across 2, 3, and 4 poisoned viewpoints. Our density-guided approach consistently generates clear, geometrically consistent illusory objects while maintaining high rendering quality in non-poisoned views, effectively preserving scene fidelity regardless of the number of attack viewpoints. G. More Visual Results for Evaluation Protocol Fig. 18 validates our KDE-based evaluation protocol, showing that attack effectiveness inversely correlates with scene density in hydrant scene. Illusory objects appear more convincing in EASY (low-density) regions than in HARD (high-density) regions, confirming that fewer overlapping observations increase vulnerability. This protocol establishes standardized benchmark for poisoning attacks while revealing connections between scene geometry and 3D reconstruction vulnerability. H. More Visual Results for Ablation Studies Fig. 19 presents qualitative comparisons of different attack strategy combinations across seven Mip-NeRF 360 scenes. While strategies (1) direct replacement and (2) densityguided poisoning are effective for most scenes, they show limitations in complex environments with high view overlap (e.g., room). Our experiments demonstrate that combining these with (3) multi-view consistency disruption achieves superior illusion embedding across all tested scenes, highlighting the complementary nature of our proposed methods. Figure 12. Computational cost comparison. Our method achieves significant reductions in GPU memory usage and model complexity. 13 Figure 13. Qualitative comparisons on single-view attack 1. Results on the bonsai scene (Mip-NeRF 360 [5]), francis scene (Tanks & Temples [44]), and counter scene (Free [95]). Both IPA-NeRF variants exhibit poor convergence on the bonsai scene, while our method consistently produces clear, well-integrated illusory objects across all scenes. Figure 14. Qualitative comparisons on single-view attack 2. Results on the garden scene (Mip-NeRF 360 [5]), horse scene (Tanks & Temples [44]), and road scene (Free [95]). Our method effectively embeds distinct illusory objects while maintaining scene consistency. 14 Figure 15. Qualitative comparisons on multi-view attack with 2 poisoned views. We compare the visual quality of illusory objects rendered from two distinct viewpoints using the stump scene (Mip-NeRF 360 [5]). Figure 16. Qualitative comparisons on multi-view attack with 3 poisoned views. We compare the visual quality of illusory objects rendered from three distinct viewpoints using the room scene (Mip-NeRF 360 [5]). 15 Figure 17. Qualitative comparisons on multi-view attack with 4 poisoned views. We compare the visual quality of illusory objects rendered from four distinct viewpoints using the garden scene (Mip-NeRF 360 [5]). Figure 18. Visualization of our evaluation protocol on the hydrant scene (Free [95] dataset). 16 Figure 19. Completely qualitative comparisons of different attack strategy combinations. We visually analyze the effects of combining three poisoning strategies: (1) direct replacement of poisoned view ground truth, (2) density-guided point cloud poisoning, and (3) multi-view consistency disruption. Combining all three strategies achieves the most realistic illusion embeddings across various scenes from the Mip-NeRF 360 [5] dataset, demonstrating the complementary effectiveness of our proposed methods."
        }
    ],
    "affiliations": []
}