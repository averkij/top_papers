{
    "paper_title": "Visual Personalization Turing Test",
    "authors": [
        "Rameen Abdal",
        "James Burgess",
        "Sergey Tulyakov",
        "Kuan-Chieh Jackson Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI."
        },
        {
            "title": "Start",
            "content": "Kuan-Chieh Jackson Wang Snap Research https://snap-research.github.io/vptt 6 2 0 2 0 3 ] . [ 1 0 8 6 2 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce the Visual Personalization Turing Test (VPTT), new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to human or calibrated VLM judge from content given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating 10k-persona benchmark (VPTT-Bench), visual retrievalaugmented generator (VPRAG), and the VPTT Score, textonly metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignmentoriginality balance, offering scalable and privacy-safe foundation for personalized generative AI. 1. Introduction Personalization in visual generation has so far focused on identity replication [24, 7, 16, 17, 33, 39, 51, 52, 60], optimizing models to reproduce subject across scenes. While effective at preserving appearance, these pipelines are computationally expensive [4, 16, 51] and miss the broader vision of personalization: how individuals perceive, stylize, and share their world. To instantiate this idea, personalization should capture the aesthetic preferences [44, 54, 61], cultural context, and visual familiarity that constitute persons unique visual language. Yet, no benchmark exists to measure whether models output truly feels like it could have been created by particular person or creator. This gap is increasingly important beyond research. Industry is actively trying to bridge the gap between GenAI and usercreated content to make generative AI monetizable, trustworthy, and personally resonant [12, 41]. This challenge becomes even more pressing as powerful foundation models in image domain, such as Qwen [65], NanoBanana [24] and GPT-Image-1 [42], already achieve near-photorealistic Figure 1. Visual Personalization Turing Test. We present the Visual Personalization Turing Test (VPTT), new paradigm for contextual personalization at scale. model passes the VPTT if its output is indistinguishable to human or calibrated VLM judge from what given person might plausibly create or share. As one way to address this challenge, we introduce VPTT Framework consisting of privacy-safe benchmark VPTT-Bench for evaluating personalized generation and editing, and Visual Personalization RAG (VPRAG) that retrieves persona-aligned visual cues and converts them into personalized image generations or edits. To close the loop, we propose an automated VPTTscore that achieves strong Spearman rank correlation (ρ) with humans and VLM Judges, establishing it as cheap, reliable proxy for human perception of personalization. quality. As models master realism, the frontier of innovation shifts to what is personally relevant to the user [43]. Figure 2. Contextual Image Generation and Editing using VPTT-Bench. Each row shows distinct user profile: assets and style cues (left), personalized generations (social post, cultural site), and edits (garden, living room) guided by the same persona identity. All images are generated synthetically via our Visual Personalization RAG (VPRAG) by text, which retrieves persona-aligned cues. To show cross model personalization here the assets are generated by QWEN-image-model [65] and generations and edits by Nano-Banana [24] conditioned only on the first image. More results in are in Supplementary materials. To address this gap, we introduce the Visual Personalization Turing Test (VPTT) (Figure. 1): new paradigm for evaluating generative models. model passes the VPTT if its output (image, video, 3D asset etc.) is indistinguishable to human or calibrated VLM judge from that given person might plausibly create or share. This reframes the goal from rote memorization of appearance to the far more challenging task of simulating personal perspective. Solving the VPTT presents three fundamental challenges. First, it requires benchmark with thousands of diverse, culturally, and stylistically rich user profiles, yet real-world user data is inaccessible due to privacy concerns, fundamentally limiting academic research. Second, it demands new technical approach beyond the fine-tuning one that can interpret users complex, multi-faceted style from their history and apply it to new generations in scalable, efficient manner. Third, it requires robust evaluation protocol to test VPTT at large scale. We introduce the VPTT framework, designed to address these challenges at scale. To overcome the data barrier, we construct VPTT-Bench, the first large-scale benchmark of about 10,000 synthetic personas, whose visual worlds (30 assets - images for the scope of this paper) are represented entirely in text as deferred renderings, (structured, attribute-rich intermediates like lighting , materials, environment, actions, forground, background, appearance etc. that defer visual realization, analogous to G-buffers [13] in graphics) enabling privacy-safe research at scale. Additionally, we render about 1000 synthetic personas to create rich visual library. As possible solution to personalization at scale, we propose novel visual personalization retrievalaugmented generation (VPRAG) system. Instead of costly retraining, our method conditions generation on personas existing assets through hierarchical semantic retrieval with an optional learnable feedback and composes personalized prompt enriched with their unique stylistic elements. Our evaluation framework for image generation and editing is two fold. We first introduce VPTTscore as automatic proxy for VPTT. We conduct visual-level evaluation through VPTT, validated by human study and extended with calibrated VLM judges. This helps us establish strong correlations among all three evaluators text-level (VPTTscore), VLM, and human, confirming that the VPTTscore is reliable, perceptually grounded proxy for visual judgment. After establishing this, we perform large-scale deferred rendering analysis (about 120,000 evaluations) using the VPTTscore. Our results show that VPRAGs structured design achieves the best trade-off between output alignment and novelty, addressing key limitation of black-box baselines. Our contributions are: new task formulation, the Visual Personalization Turing Test (VPTT), redefines success in visual personalization as achieving human indistinguishable authenticity. VPTT Framework, the first scalable, privacy-safe benchmark for contextual personalization, featuring 10,000 rich personas with 1,000 visually rendered agents. novel Visual Personalization Retrieval-Augmented Generation (VPRAG) system, structured, zero-shot engine for personalization offering possible scalable solution. rigorous new evaluation framework featuring the VPTT score validated against human and VLM judges, proving it is reliable proxy for perceptual alignment. comprehensive analysis on our benchmark using mix of closedand open-source models with varying computational budgets, demonstrating that VPRAG offers better trade-off between performance and efficiency. Figure 3. VPTT-Bench Data Generation Pipeline. Overview of the deferred rendering pipeline used to construct VPTT-Bench. (1) Personas are sampled from PersonaHub [22] with demographics. (23) Visual and scenario elements (lighting, actions, materials etc.) are extracted. (4) These cues are composed into structured captions and embedded via an LLM. (5) Generating 30 corresponding visual assets per persona, forming privacy-safe, semantically grounded data for evaluating contextual personalization. cent works aim for tuning-free personalization. IP-Adapter and related works in the image and video domains [3, 7, 18, 50, 56, 63, 66] use reference images to condition generation, achieving strong results in transferring style or appearance [14, 20, 27, 28] but often requiring careful selection of reference images and suffer from the absence of larger visual context [20]. Methods like InstantBooth [57] represent another direction in test-time personalization without finetuning but again focuses on personalizing the appearance of the subject. Among the methods that consider the context, DrUM [31] proposes learning vector based on prompt history and injecting it via trained adapter network, offering modular approach but still involving per user adapter training. very recent work ImageGem [25], collects inthe-wild interactions for generative model personalization, highlighting the communitys growing interest in this area, though primarily focused on LoRAs collected over users generated content. Our work, orthogonal to these works, focuses on deriving and applying preferences, cultural context, visual familiarity and personal elements implicitly derived from users asset history, without requiring explicit reference images or per-user training of adapters. 2.2. Visual Preference Personalization Aligning generative models with user preferences is critical challenge. Many recent efforts draw inspiration from Reinforcement Learning from Human Feedback (RLHF) [46] used in LLMs [10, 65]. An early work, ImageReward [64] trained reward model on human comparisons to score prompt-image alignment, enabling finetuning via Reward Feedback Learning (ReFL). DiffusionDPO [58] applied Direct Preference Optimization to finetune Stable Diffusion XL [47] on large-scale human judgments [62] from datasets like Pick-a-Pic [32], improving general appeal and alignment. While powerful, these methods typically optimize for aggregate preferences rather than individual context. On the other hand, approaches targeting Figure 4. Example Personas from VPTT-Bench. Each row shows synthetic persona sampled from PersonaHub [22] (only short descriptions) with its corresponding visual assets generated via VPTT-Bench generation pipeline. Personas span diverse regions, professions, and age groups, illustrating the demographic and contextual diversity of VPTT-Bench. 2. Related Work 2.1. Personalization in Visual Generative Models. Personalization in generative models has traditionally focused on identity replication [24, 7, 15, 16, 18, 33, 51]. Seminal methods like DreamBooth [51] and LoRA adaptations [53] excel at fine-tuning models to reproduce specific subject across different scenes. However, these approaches are not scalable and primarily address appearance fidelity rather than the users broader visual signature [2, 1518, 33, 39, 5052, 56, 59, 60, 63]. More reindividual preferences are emerging [40]. ViPer [54] learns preferences by having an MLLM [44] analyze user comments on images, extracting structured attributes to guide generation. PPD [11] trains single model conditioned on user embeddings derived from few-shot pairwise preferences. POET [26] focuses on identifying image homogeneity using prompt inversion and personalizing diversification based on interactive user feedback. Concurrent work, such as Instant Preference Alignment [37], also uses MLLMs to extract preferences from reference image for tuning-free guidance. Our work differs by focusing on extracting and applying alignment implicitly from users historical creative output (simulated via VPTT-Bench derived from real-world grounded PersonaHuB [22]) rather than relying on explicit feedback, pairwise comparisons, or single reference images. We introduce the VPTT as holistic measure of visual context alignment beyond simple preference scores. 2.3. RAG in Computer Vision [21], Retrieval-Augmented Generation (RAG) initially prominent in NLP, is increasingly being explored in computer vision [55, 68]. Very recent works like RealRAG [38] and FineRAG [67] focused on retrieving external visual knowledge (e.g., real images of objects) to improve content completion of generated images and using RAG for VQA. Comprehensive repositories like Awesome-RAGVision [68] are mapping the growing landscape, covering applications in visual understanding, generation, and embodied AI. Within generation, RAPO [19] uses RAG specifically for text-to-video prompt optimization, retrieving terms from graph built on training data to align user prompts with the models expected input format. Tailored Visions [8] pioneered using RAG on users own prompt history for personalized text-to-image prompt rewriting, using an LLM to synthesize past styles into new prompts. OmniStyle [61], while focused on style transfer, utilizes large curated dataset and filtering for high-quality supervised training. Our VRAG system builds upon the personalized RAG concept but distinguishes itself through: (1) operating on our structured, synthetic VPTT-Bench benchmark, enabling privacy-safe research; and (2) employing principled, more transparent retrieval and composition architecture for fine-grained control, rather than relying solely on black-box LLM operating on raw prompt history. 3. Visual Personalization Turing Test Our goal is to model and evaluate contextual visual personalization the ability of generative model to produce content that human (or VLM) would perceive as consistent with given personas visual context. We formalize this as the Visual Personalization Turing Test (VPTT) and introduce VPTT Framework, unified framework that enables systematic study of this problem at scale. VPTT Framework consists of four interacting components: (1) large-scale simulated persona benchmark (Sec. 3.1); (2) retrieval-augmented generation engine (Sec. 3.2); (3) an optional learnable feedback loop (Sec. 3.3); and (4) differentiable proxy metric, VPTT score (Sec. 3.4). Together they form closed cycle of simulation generation judgment optimization. Problem Definition. Given persona = {d, E, C} demographics d, structured element library E, and caption memory and query p, the model must generate personalized prompt whose resulting image G(p) maximizes perceived alignment with P: (p; P) = λ1 Align(p, P) + λ2 Fidelity(p, C) + λ3 Novelty(p, C), (cid:88) λi = 1. (1) This surrogate defines the latent VPTT objective: an ideal system achieves high alignment, high fidelity, and high novelty simultaneously, an intractable trade-off for current models. We expect this trade-off to improve with better personalized models and for the scope of this work propose method that approximates this objective efficiently without retraining. 3.1. VPTT-Bench: Scalable Simulation Substrate Human personalization datasets are private and unscalable. We therefore construct VPTT-Bench (Figure. 3 and Figure. 4), synthetic benchmark of 10,000 agents, each represented by tuple Pi = {di, Ei, Ci}. Personas are generated using Qwen2.5-72B-Instruct [65]: Demographic Generation: starting from public textual seeds (PersonaHUB [22]), we sample culturally diverse backstories di. This ensures cross-domain coverage, avoiding dataset bias. Visual Elements Extraction: we sample and cluster atomic visual terms (e.g., clothing, lighting, pose) into structured vocabularies Ei conditioned on di ensuring the visual elements are consistent with the persona. Scenario and Assets Extraction: conditioned on {di, Ei}, we first generate short scenarios of the assets and finally generate 30 captions Ci describing element rich posts with the scenario story arc. The captions are embedded using text-embedding-3-small [44]. We further render 1,000-persona subset into image galleries (30 images per persona), each anchored by canonical portrait followed by caption-guided edits. This hybrid textimage corpus provides both semantic control and the text-only component enables dense, visual diversity: scalable supervision without privacy constraints, while the paired visual assets allow controlled studies across different Figure 5. VPRAG Pipeline Overview. Comparison between the baseline retrieval-augmented generation (BRAG) and our proposed Visual Personalization RAG (VPRAG). Unlike baseline BRAG, VPRAG introduces controllable and interpretable retrieval through: (a) post-level embedding and similarity scoring, (b) temperature-controlled attention, (c) entropy-guided post selection, (d) capacity-aware quota allocation, (e) category-level ranking, and (f) element-level composition. This multi-stage design yields white-box, LLM-optional retrieval framework producing visually and semantically aligned personalized generations and edits. resource budgets, from lightweight text-only personalization to more expensive multimodal (text + image) setups. For real profiles, the reverse of this process is performed to get the structured data. 3.2. Visual Personalization Retrieval-Augmented Generation (VPRAG) To personalize content without model retraining, we propose VPRAG (see Figure. 5), retrieval-augmented generation framework that conditions prompt rewriting on personas structured memory. Given query and profile = {d, E, C}, VPRAG retrieves semantically relevant posts and elements, allocates retrieval quotas, and composes new prompt that aligns with the personas context. Unlike other methods [1, 51] that require minutes to hours per user, VPRAG operates entirely at inference time, adding only few hundred milliseconds of retrieval and composition overhead. Hierarchical Retrieval. Captions encode holistic semantic intent (high-level concepts), while elements capture atomic style (low-level cues). We therefore perform hierarchical two-level retrieval for robustness. Post-level retrieval. Each personas captions {ci} are embedded using text-embedding-3-small [44], and cosine similarities si = qvi are computed with the query p. Weights are normalized as wi = exp(si/τ ) exp(sj /τ ) , where τ is softmax temperature controlling retrieval sharpness. This Boltzmann weighting represents the maximum-entropy solution for expected semantic alignment under temperature constraint [30], guaranteeing smooth attention while avoiding brittle hard cutoffs. (cid:80) Entropy Guided post Selection. We then measure entropy = (cid:80) wi log wi, neff = exp(H), where neff approximates the effective number of relevant posts,a theoretically grounded proxy for query specificity. Broader prompts (e.g., in the park) yield higher and therefore encourage more diverse retrieval, whereas narrower ones (e.g., in Kashmiri traditional dress) produce lower entropy, focusing the selection. To balance adaptivity and efficiency, we cap the retrieved posts given the budget (total number of visual elements to sample from categories = {fg, bg, lighting, pose, . . .}), set as = min(neff , 2 Q), ensuring controlled expansion without over-retrieval for broad prompts. (cid:23) (cid:22) = win(c) wj n(c) Quota Allocation. Each post contributes elements from categories C. Given category C, we allocate quotas to each post as: q(c) where n(c) (cid:80) is the number of available elements in category for post i, and Qc is the total budget for category c. Remainders are allocated to largestfraction posts. This rule ensures the proportional-fair allocation objective so that highweight posts get more samples, but low-weight ones still contribute diversity. Qc Element-level retrieval. Within the top-K posts we prioritize the categories based on the prompt using semantic relevance scorek = cos(ϕ(ck), ϕ(p)), (ϕ is lightweight transformer encoder (MiniLM) [29]). Within each category, elements are ranked based on the closeness to the using the same MiniLM [29], and the top-q(k) are selected. Prompt Composition. The selected elements Ep are concatenated with persona summary Sp into = fcompose(p, Sp, Ep, L) under token-length budget L. This yields re-prompt enriched with stylistic and contextual cues consistent with the personas memory. Based on the budget, fcompose, can be an LLM refining the story arc for the generation or simple text concatenation. 3.3. Learnable Feedback Simulation While VPRAG uses persona aligned retrieval, personalization also involves subjective preference learning. We therefore introduce small learnable feedback module to approximate user-specific value functions. Given persona with subjective preferences and generated prompt p, visionlanguage judge (VLM) outputs an alignment score sVLM [0, 1]. We train cross-attention predictor fθ to estimate ˆsVLM = fθ(Emb(p), Emb(P)), and re-rank candidates by = arg maxm fθ(Emb(p m), Emb(P)). We use this component as smaller scale proof of concept to encourage future extensions of VPTT Framework toward closed-loop personalization. 3.4. VPTT Score: Differentiable Proxy for Personalization We now introduce VPTTscore, quantitative metric that serves as the text-level scalable foundation for the VPTT triangle and convex surrogate of the personalization objective in Eq. 1. VPTTscore combines four interpretable metrics that jointly approximate alignment, fidelity, and originality: Persona Alignment (PA), GS Reconstruction (GS), Cluster Proximity (CP), and Novelty (NV). (1) Persona Alignment (PA). This term measures semantic coherence between the generated prompt and the textual description of the persona P: PA(p, P) = cos(cid:0)Emb(p), Emb(P)(cid:1). (2) GS Reconstruction (GS). To measure content fidelity, we represent each personas caption embeddings {vi} as an orthonormal basis using the GramSchmidt For generated prompt embedding vp, process. GS(p, C) = cos(cid:0)vp, B(Bvp)(cid:1) which evaluates how well can be reconstructed from the assetss semantic span. GS measures subspace fidelity i.e. whether generation stays within the semantic manifold defined by the personas gallery rather than mere pairwise similarity. (3) Cluster Proximity (CP). To assess thematic consistency, all asset captions are clustered in the GS basis thematic centroids {ck}. The hard version used for evaluation is CP(p, C) = exp(cid:0)mink (cid:1), while the differentiable relaxation replaces min with temperature-controlled softmin: (cid:102)CP(p, C) = (cid:80) pck2 pck2/τ ) pcj 2/τ ) . exp(v exp(v (cid:80) (4) Novelty (NV). Novelty penalizes verbatim reuse of retrieved captions. The discrete version measures maximum trigram overlap: NV(p, C) = 1 maxi . For differentiable analysis, we define soft-overlap relaxation: (cid:102)NV(p, C) = 1 maxi , where ϕt() denotes continuous n-gram embeddings (via small sentence transformer for example MiniLM [29]). cos(ϕt(p), ϕt(ci)) Tri(p) Tri(p)Tri(ci) Tri(p) (cid:80) Combined Score. The overall proxy is convex weighted combination: VPTTscore = 0.20 PA+0.30 GS+0.30 CP+"
        },
        {
            "title": "0.20 NV. Empirically, GS and CP correlate most strongly\nwith human visual fidelity, so we assign them higher weight\n(0.3 each). PA measures semantic alignment (0.2), while\nNV promotes originality and prevents overfitting (0.2). The\nweighting satisfies (cid:80)\ni λi = 1, forming an unbiased con-\nvex estimator of J . For tasks with limited prompt bud-\ngets (e.g., adding exactly three retrieved phrases), the nov-\nelty term becomes less meaningful as textual overlap is\nbounded by design. We therefore use the normalized variant\nVPTTscore-c = 1\n3 (PA + GS + CP), which equally weighs\nthe three active components. We further justify the weights\nin Sec 4.2.1 while computing the correlations. The novelty\nterm is also set to zero for the baselines not conditioned on\nthe captions. While our experiments report the hard (eval-\nuation) forms for interpretability, the differentiable variant\nmakes VPTTscore suitable as a learnable objective in future\npersonalization pipelines.",
            "content": "4. Evaluations 4.1. Baselines We benchmark VPTT Framework against two baseline categories. First, scalable privacy-safe pipelines including Baseline - no access to any asset, Persona Only - access to demographics information, and Baseline RAG BRAG [8], strong baseline with access to all the persona captions for personalization (see Figure. 5). These operate via retrieval and rewriting without model retraining, allowing large-scale evaluation across 10,000 personas. Second, we reference high-cost personalization baselines such as DB-LoRA [1], Flux [6], DrUM [31], MLLM [44, 65], and VIPER [54], which rely on user-specific fine-tuning or only preference optimization. These are computationally intensive and non-scalable, so we evaluate them only on smaller subsets and report results in the Supplementary. This separation highlights VPTTs focus on scalable, privacy-safe personalization while remaining comparable to existing high-fidelity methods. 4.2. Quantitative Evaluation Evaluating the VPTT is intrinsically challenging because the outcome depends on cascade of interacting systems: 1) Prompt Generation: The rewriter LLM must faithfully express personas stylistic intent. 2) Image Generation: The T2I or I2I model must accurately translate those prompts into coherent visual content. 3) Evaluation: The VLM judge must perceive the subtle consistency between the generated content and the personas authentic visual identity. VPTT performance improves as these three domains mature. To systematically evaluate them, we design threestage protocol addressing three central questions (Q1Q3). All experiments are conducted across spectrum of models from open-source Qwen2.5-7B-Instruct [65] to efficient GPT-4o-mini [45] and high-capacity Gemini-2.5-Pro [10] ensuring robustness across compute budgets. To make the evaluation holistic, we consider both image generation and editing tasks. 4.2.1. Q1: Can We Trust Our Metrics? Before scaling the evaluation, we verify that our automated metrics i.e. VLM judgment and the text-only VPTTscore faithfully approximate human perception. Human Study. We collected about 6,000 human ratings using images across four methods (see Table. 1), three LLM generations and two tasks (image generation preferred outdoor spot and editing Here is convention center. Add preferred event), from 20 annotators. Inter-annotator agreement was substantial (Kendalls = 0.651 0.141 for Generation, 0.564 0.209 for Editing), confirming consistent human understanding of personal authenticity. Metric Calibration and Validity. We validate the proposed metrics by measuring Spearmans rank correlation (ρ) between automated judgments and human ratings (Figure 1). For efficient evaluation, we use 10 visually and semantically matched posts out of 30 under budgeted evaluation setup (Sec. 3.4). We calibrate the VLM judges using GPT-4o and Gemini-2.5-Pro, wherever applicable to remove evaluation bias on small set. In evaluation of the whole set, VLM-based judgments strongly align with human perception (combined ρ = 0.67, generation: 0.75). Our text-only VPTTscore-c metric achieves comparable agreement (combined ρ = 0.68, generation: 0.78) with Top-2 agreement accuracy of 99%, confirming its reliability as human-perceptual proxy. VPTTscore-c also correlates well with VLM scores (combined ρ = 0.57, generation: 0.70), indicating consistent cross-modal alignment. While editing correlations are lower (ρ 0.5) due to the finer granularity of localized visual edits and potential perceptual losses after downsampling, generation consistently exceeds 0.7, demonstrating the robustness of our metric design. Finally, we report the averaged raw scores in Table. 1 where our method VPRAG is clear winner across all the evaluations. Overall, these results establish VPTTscore-c as fast, low-cost, and perceptually grounded surrogate for human evaluation in large-scale personalization studies. 4.2.2. Q2: Does Better Prompt Create Better Image? With calibrated evaluators, we conduct the main VPTT experiment on 200 personas on two tasks ( across three LLM models and five methods) under fixed three-phrase budget to ensure fair comparison. This part disentangles what visual generation is able to achieve with models ability to generate authentic detailed prompts (we evaluate that next). Evaluation of this extended dataset mirrors the correlation Table 1. Quantitative comparison for generation and Editing Tasks across 6000 human annotations. We report mean (Avg.) and accuracy (Acc.) text-based VPTTscore-c (01), vision-language VLM (05), and human judgments Human (05). Higher is better for all. scores for three evaluation levels: Method VPTTscore-c (Text) VLM (Visual) Human (Perceptual) Avg. Acc. Avg. Acc. Avg. Acc. 2.41 3.32 3.52 4.32 0.329 0.400 0.420 0.464 0.0% 7.3% 19.3% 73.3% Baseline Persona Only BRAG VPRAG (Ours) Table 2. Comparison of Generation and Editing tasks on 200 personas after VLM calibration across 3 LLM rewrite methods. We report mean VPTTscore-c (V-c) and VLM scores along with wining accuracy (%). Higher is better. 4.6% 1.64 19.2% 2.51 21.6% 2.69 54.6% 3. 0.70% 16.0% 21.3% 62.0% Method Generation Editing V-c Acc. VLM Acc. V-c Acc. VLM Acc. 0.0% 2.21 0.343 Baseline 0.402 1.2% 2.98 Persona Only BRAG 0.451 18.4% 4.04 VPRAG (Ours) 0.472 41.7% 4.08 0.472 38.8% 4.30 Comb. (Ours) 0.0% 2.97 1.4% 0.322 5.9% 0.399 9.2% 3.44 25.6% 0.415 15.3% 3.75 31.0% 0.448 47.2% 4.03 36.1% 0.436 28.3% 4.03 10.5% 18.5% 24.3% 30.8% 15.8% ρ = 0.53 (generation : 0.66) of the previous section. Table. 2 shows the results (averaged across LLMs) for the generation and editing tasks. The evaluation again shows how hierarchical controllable retrieval does not confuse the models and produce better alignments. 4.2.3. Q3: Is the Architecture Robust at Scale? To assess generalization, we evaluate all models text-only across our entire VPTT-Bench benchmark of 10,000 personas and four tasks (two generation, two editing , see Figure. 2), totaling 120,000 prompt evaluations. The prompts are limited to 150 words and budget of 3 is allocated to all visual element Categories C. The elements are arranged in decreasing order of relevance and LLM is given freedom to choose from the list to orchestrate story arc. As shown in Table 3, naive rewriters (BRAG) overfit to captions (often copy-pasting them), earning high alignment but low originality scores (more detailed in Supplementary) and hence falling short. In contrast, VPRAG consistently achieves the best composite VPTTscore, maintaining the optimal balance between alignment and originality across all rewriter backbones. This large-scale experiment demonstrates that VPRAG scales linearly, generalizes across models, and sustains perceptual authenticity without retraining. 4.2.4. Downstream Study: Feedback Simulation We evaluate feedback simulation on smaller subset of 200 personas (10,000 labeled examples) as proof of concept rather than core benchmark. Although this component is not used in our main quantitative evaluations, it demonstrates that compact models can learn to simulate user-level preference alignment from limited supervision. We samspooled Table 3. Main text-level results across 10,000 personas and three LLM models. We report the novelty-adjusted VPTTscore (V), plus Cohens [9] (d = µbestµmethod ), measuring effect size relative to the best-performing method per row (µbest) across 20,000 samples per entry. Bold indicates the best method and underline the second-best. The Baseline and Persona Only methods consistently underperform across both generation and editing tasks. Our VPRAG and Comb. (BRAG + VPRAG) methods achieve the best overall performance, with Comb. performing slightly better for 4o-mini (GPT-4o-mini [44]) and Gemini (Gemini-2.5-pro [10]), while VPRAG excels for Qwen (Qwen2.5-7B-Instruct [65]). Higher Cohens values (d 0.5 indicates medium to large effects) demonstrate substantial performance differences, particularly between persona-based methods and baselines. See supplementary material for detailed score breakdowns. (a) Generation (b) Editing Baseline Persona Only BRAG VPRAG Comb. Baseline Persona Only BRAG VPRAG Comb. Model Qwen 0.316 11.9 4o-mini 0.316 12.6 9.8 Gemini 0. 0.389 0.402 0.379 8.3 8.4 7.1 V d Model V 0.581 0.628 0.616 1.1 0.5 0.3 0.631 NA 0.602 0.1 0.640 0.2 0.625 0.7 0.644 NA 0.632 NA Qwen 4o-mini Gemini 0.306 0.306 0.306 12.0 12.0 10.7 0.378 0.384 0.372 8.7 8.8 8.1 d d 0.583 0.596 0. 1.1 0.9 0.6 0.626 NA 0.586 0.626 NA 0.610 0.605 1.0 0.5 0.0 0.606 NA Figure 6. Qualitative Comparison across Generation and Editing Tasks. Representative examples from the VPTT-Bench showing outputs from five methods: Baseline, Persona Only, BRAG, VPRAG (ours), and BRAG + VPRAG (ours). Each sample is evaluated using human, VLM (reasoning shown), and text-level VPTTscore-c scores, where higher indicates closer alignment to the personas assets. Our methods achieve the highest perceptual and textvisual consistency, confirming effective contextual personalization. ple diverse simulated profiles (95% occupation uniqueness, 96 countries, 10 ethnicity groups) and use GPT-4o [45] to generate 50 labeled prompts per profile, 20 aligned, 20 misaligned, and 10 neutral, yielding 10,000 labeled examples with profile-level splits (130/20/50 train/val/test). compact cross-attention regressor (128-dim, 4 heads) achieves 73.8% overall accuracy (MAE: 0.1259) and 91.6% accuracy on aligned preference predictions for 50 unseen users (2,525 prompts), with only 0.7% validationtest gap, showing that compact models can effectively capture persona-aware preferences while generalizing to new users. We leave large scale studies to future extensions. 4.3. Qualitative Results VPRAG produces visually coherent and persona-faithful generations across diverse profiles. By retrieving finegrained visual cues such as lighting, attire, scene semantics, and stylistic markers, VPRAG enriches the composed prompts while preserving originality and user-specific visual elements (Figure 2). These examples also highlight VPRAGs ability to perform cross model personalization, where VPRAG produces consistent personalization across QWEN-Image [65] and Nano-Banana [24]. Compared to the persona-only baseline (Figure 6) and the BRAG baseline, VPRAG achieves stronger contextual grounding, sharper visual fidelity, and more consistent preservation of persona style. For editing tasks, it additionally injects semantically relevant visual elements. Results for remaining baselines and additional profiles are provided in the Supplementary. 5. Conclusion We introduced the Visual Personalization Turing Test (VPTT) as principled paradigm for evaluating contextual visual personalization, and proposed the VPTT Framework, scalable system that operationalizes this paradigm. The framework integrates VPTT-Bench, the VPRAG retrieval engine, and the VPTTscore metric into closed-loop pipeline for simulation, generation, and evaluation without any per-user retraining. Our results show strong alignment among human judgments, VLM judges, and the textonly VPTTscore, validating the framework as an efficient, privacy-safe foundation for personalized generative models. Future work will incorporate opt-in and federated real-user signals to further bridge simulated and real personalization while preserving user privacy."
        },
        {
            "title": "Supplementary Material",
            "content": "S.1 Additional Details: Formalization of the VPTT Evaluation Protocol Judging modalities. Human judges: J(X, ) is the mean normalized Likert This section provides additional mathematical clarification of the Visual Personalization Turing Test (VPTT) evaluation protocol described in the main paper. The formalization offers rigorous scientific grounding for the task and mitigates subjective interpretation. Setup. persona is defined as = {d, E, C} (demographics, structured visual elements, and caption memory). Given query p, the personalization system produces rewritten prompt and generated visual output = G(p) , where denotes the visual generative model (see Sec.3.2, main paper). For brevity, we write G( p, ) to denote the overall personalization pipeline that includes retrieval, prompt rewriting, and generation. Judge function. As described in the main paper, VPTT evaluates whether is indistinguishable from content that the persona might plausibly create or share. We formalize this via judge function : [0, 1], J(X, ) = plausibility score. Human annotators and VLM-based judges provide plausibility judgements on 05 Likert scale, which can be linearly normalized to [0, 1]. In large-scale evaluations, we substitute with the VPTTscore (Sec. 4, main paper), which serves as scalable proxy. Expected VPTT performance. Let µ be the distribution over personaquery pairs. The expected VPTT performance of generator is Π(G) = E(P,p)µ (cid:104) EXG(p,P ) (cid:2)J(X, )(cid:3)(cid:105) . (S1) Finite-sample estimator. Using personas and queries per persona, we estimate Eq. (S1) as (cid:98)Π(G) = 1 (cid:88) (cid:88) i=1 j=1 J(Xij, Pi) , Xij G( pij, Pi). (S2) score over annotators. VLM judge: J(X, ) is the calibrated normalized Likert score of the plausibility estimate. Proxy judge (VPTTscore): for text-scale evaluation, J(X, ) is approximated by VPTTscore(p, ), shown in Sec. 4 of the main paper to correlate with human judgments. 6. Limitations and Future Work SyntheticReal Gap. Because VPTT-Bench relies on single family of generator models for producing the synthetic personas, the benchmark inevitably inherits stylistic and cultural biases of those models. This real-to-sim gap limits how faithfully the benchmark captures the full diversity of real users. promising direction is to construct future versions of VPTT-Bench using heterogeneous ensemble of generators across organizations and training paradigms. We argue, however, that unified v1 benchmark is an essential step: it moves the field from data-zero regime to one where controlled, scalable, and privacy-safe personalization research becomes possible. Image-Only Scope. While our retrieval and alignment mechanisms are modality-agnostic, this work focuses on image generation and image editing. Extending VPTT to videos, 3D assets, and multi-view content is natural next step, requiring new alignment metrics and temporalconsistency modules. Scaling Beyond Individuals. Our method currently models single-person personalization. Future work can expand VPTT toward societal personalization: simulating communities, subcultures, and collective preference distributions. Such extensions could enable population-level evaluation, community-aware media generation, and product design aligned with specific cohorts. Enhanced Visual Grounding. Persona assets are currently represented as rich textual deferred renderings. Future work may couple these with segmentation or detection models to retrieve visual elements directly from user images for opt-in users. This would enable stronger grounding on real visual evidence and more faithful scene composition. Structure Preservation. Current generators, including those used in VPRAG, do not guarantee preservation of spatial Incorporating layout during editing. structure-aware diffusion models or control modules (e.g., depth/segmentation guidance) may improve fidelity for demanding edit tasks. Figure 7. Comparison to Visual Baselines. We compare VPRAG (along three columns) against broad set of visual personalization baselines, including fine-tuning approaches, preference-driven personalization methods, and multimodal LLM (MLLM)based in-context techniques. Evaluation is conducted using two metrics: the VIPER Proxy Score (PS) [54] and the Gemini VLM Judge (see Sec. 4 in the main paper). Across more challenging and nuanced examples shown in the figure, VPRAG consistently emerges as an efficient and controllable personalization method, performing on par with or outperforming these substantially more expensive baselines. Metric / Model Flux DB-LoRA @50 Viper @ 1000 0.678 0.269 0.545 0.292 71.5 DrUM @ 0.841 0.139 0.757 0.185 68.9 Flux Kontext @1000 GPT (VLM + GPT-image-1 @100) GPT-image-1 @100 0.839 0.184 0.541 0.297 83.4 0.858 0.180 0.832 0.0.163 63. 0.974 0.019 0.966 0.047 44.0 0.867 0.173 0.656 0.232 80.0 3.17 0.70 1.99 0.67 88.0 ( + 2% ties) 2.88 1.13 2.40 1.21 61.8 (+15.5% ties) 3.41 0.57 2.89 0.61 76.4 (+7.1% ties) 3.11 0.73 2.29 0.98 76.4 (+ 7.9% ties) 3.28 0.75 3.22 0.71 49.0 (+ 4% ties) 3.73 0.44 3.86 0.35 33.0 ( + 12% ties) PS (Ours) PS (Other) PS Win % (Ours) VJ (Ours) VJ (Other) VJ Win % (Ours) Table 4. Benchmark comparison on VIPER Proxy Score (PS) and VLM Judge Score (VJ). Mean and standard deviation appear on separate lines. Win % reports the percentage of pairwise wins against the compared method for each metric. Human-in-the-Loop Integration. VPRAG can naturally operate as visual copilot: retrieving user-specific cues, proposing edits, and letting the user refine preferences. Iterative preference learning, reinforcement from user feedback, and federated fine-tuning represent compelling next steps. Real-World Deployment. Although we use synthetic personas for privacy reasons, the same dataset construction pipeline can be inverted to annotate and structure real user data in an opt-in or federated setting. This would enable applying the VPTT Framework directly on real personalization tasks while maintaining strong privacy guarantees. even privacy safe benchmarks. They thus incur high latency, high cost, and weaker controllability. In contrast, our VPRAG approach evaluates alignment in text first using VPTTscore, requiring no per-user training and no iterative image synthesis. 7.2.1. Evaluation Metrics For evaluation, we assess generation quality using both automated metrics (VIPER proxy score [54], assigns higher scores to query images that share the preferred visual attributes) and human-aligned VLM judges (Aligned Gemini 2.0 Flash [23], see Sec 4 in the main paper), where judges compare baseline and personalized generations, i.e., using preferred outdoor spot or Photo showing my next social media post with my style and content. and the personalized version per profile using VPRAG. 7.2.2. Evaluation Protocol Flux DB-LoRA@50 We fine-tune FLUX.1-dev [35] using LoRA with rank 16 on attention layers, training for 1000 steps with the Prodigy optimizer and pivotal tuning on CLIP text encoder. Each users LoRA is trained on 30 gallery images paired with user-specific trigger word to learn personalized visual styles. Since training LoRAs is expensive, we train this baseline for 50 users to make this evaluation comprehensive. VIPER@1000 We evaluate VIPER [54], visual preference optimization baseline that personalizes SDXL [48] by optimizing text-to-image alignment. For each user, VIPER retrieves the top-10 most similar gallery posts given the prompt and uses both the images and captions to compute visual preferences (positive and negative prompts) that guide generation toward user-preferred visual styles. The methods generate images from the test prompt (A preferred outdoor spot) and the personalized one (VPRAG RAG) using SDXL-base-1.0 [48]. Evaluation is conducted on 1,000 users. Comparison is done against 5 2 grid of reference images (10 gallery posts/ assets). DrUM@1000 We evaluate DrUM [31], baseline that personalizes image generation by conditioning on the user prompt history. For each user, DrUM retrieves the top5 most similar captions from their gallery posts given the prompt and uses them as reference prompts with personalization strength α = 0.5. The methods generate images from the test prompt (A preferred outdoor spot) and the personalized one (VPRAG RAG) using Stable Diffusion v1.5 [5]. Evaluation is conducted on 1,000 users, where comparison is done using both methods outputs against 5 2 grid of reference images (10 gallery posts/ assets). Figure 8. Copy-Paste Effect. The baselines including MLLMs suffer from copy-paste effect where the generations and edits only consider single or few images of the user assets. 7. VPTT at scale 7.1. Analogy for Deferred Rendering in VPTT An analogy for our deferred rendering process is an expert film critic. critic invests considerable effort watching hundreds of movies (the expensive offline alignment) to internalize what makes script succeed on screen. Once trained, the critic can read new script and predict whether it would make compelling film before spending millions producing it. Similarly, VPTT evaluates candidate prompt against personas visual identity in text form, first aligning Human/VLM judges/VPTTscore and using cheap and reliable VPTTscore before generating any images. This enables early rejection of weak generations, reducing costly rendering and accelerating personalization at scale. 7.2. Comparison to Visual Baselines Returning to our deferred rendering analogy, VPTT allows us to evaluate script (a candidate prompt) before producing the film (the final generated image). In this context, several existing personalization approaches can be interpreted as high-budget productions that must render the entire film before knowing whether it works: Per-user finetuning methods such as DreamBooth/LoRA [1] require retraining for each identity. Preference-driven generation systems such as VIPER and DrUM [31, 54] rely on only matching the preferences from images and text. Multimodal LLM pipelines (e.g., OpenAI GPT-4o VLM + GPT-Image-1 [42, 45], GPT-Image-1 [42], and Flux Kontext [36]) operate as large black-box modules that jointly hallucinate alignment and appearance, but remain difficult to control or steer. Because these methods must generate/input images to refine alignment, they cannot benefit from early rejection or Figure 9. VPTT-Bench Ethnicity and Location Diversity. Ethnicity and location diversity of the users in VPTT-Bench FLUX Kontext @1000 We evaluate FLUX.1-Kontextdev [34] in-context learning capability by conditioning generation on 5 5 grid of 25 reference images from each users gallery. For each user, we generate two images: one with the base prompt alone and one with personaenhanced prompt i.e. VPRAG (both conditioned on the same reference grid), allowing us to assess whether incontext visual conditioning alone is sufficient for personalization. Here we evaluate 1000 users for comprehensive evaluation. Large Multi-Modal Models We compare VPRAG against OpenAIs GPT-Image-1 [42] with two approaches: (1) GPT (VLM + GPT-Image-1 @100) [42, 45] visual analysis baseline where GPT-4o [45] analyzes 5 5 grid of 25 user gallery images to extract visual style preferences (foreground, background, materials, objects, lighting, actions, environment, appearance) and generates refined 3phrase prompt that incorporates these elements alongside the base prompt (A preferred outdoor spot), and (2) GPTimage-1 @100 visuals generated directly using 5 5 grid of 25 user gallery images by GPT-Image-1 [42]. This evaluation used the unconstrained version of VPRAG with the prompt Photo showing my next social media post with my style and content. (Figure. 2 in the main paper). These are compared with the VPRAG augmented generations. The system prompt for the GPT (VLM + Image Gen @100) baseline is: You are an expert at analyzing visual styles and preferences from image collections. The requested text prompt The visual elements from the Analyze the provided images and create detailed image generation prompt that will generate new image matching both: 1. 2. reference images so the generated image looks like its from the same gallery Focus on visual elements like: foreground, background, materials, objects, lighting, actions, environment, appearance, etc. Your prompt should be EXACTLY 3 short descriptive phrases separated by commas. The user prompt of the baseline is: Analyze the visual Here are images from users profile. style, color preferences, composition patterns, and aesthetic choices in these images. Pay attention to: elements, background, materials, objects, lighting, actions, environment, and overall composition. Create an image generation prompt for: foreground base prompt Focus on visual elements like: foreground, background, materials, objects, lighting, actions, environment, appearance, etc. in the images in this grid. an output image for base prompt using these visual elements such that the resultant image also feels like it belongs to this profile. Generate The prompt for the GPT-image-1 @100 VPRAG is: Using the visual style from these reference images, create: personalized prompt. 7.2.3. Results Table 4 compares these baselines. Here, our method outperforms all the methods or has comparable performance with the large multi-model models. In Figure 7, we show results on more nuanced and difficult examples, where VPRAG consistently emerges as an efficient and controllable personalization method, performing on par with or outperforming these substantially more expensive baselines. While in-context learning (ICL) approaches such as GPT-4o [45] or GPT-Image-1 [42] can condition generation on set of reference images, they suffer from two fundamental limitations that our persona-based formulation directly addresses. First, ICL is not controllable. Without explicit structure, these models frequently copy or closely mimic individual gallery images rather than synthesizing novel content (see Figure 8) from coherent blend of visual attributes. In our evaluation, penalizing such copy-paste behavior results in substantial performance drop for the ICL baseline (4.08 3.86; 5.4% decline), whereas our persona-enhanced method exhibits far greater robustness (3.83 3.73; only 2.6% decline). This indicates that our approach learns to aggregate and recompose visual patterns across multiple references instead of replicating isolated scenes. Moreover, ICL provides no principled mechanism for selecting which visual attributes to incorporate: all gallery images are treated uniformly, causing models to focus on salient but potentially irrelevant cues. Second, ICL is not scalable. Performance degrades as gallery size increases due to context-window constraints and attention dilution, and inference cost grows linearly with the number of reference images (O(n)). This makes evaluation impractical for settings requiring richer user histories or larger galleries. Third, ICL is not economically viable at scale. At GPT-4o Vision pricing (approximately $0.01 per processed Figure 10. VPTT-Bench Age and Interest Diversity. Age Distribution and t-sne visualization (interests) of first 1000 users. The prompt should incorporate visual elements from these images so the generated image feels like its part of the same gallery. IMPORTANT: Your response must be EXACTLY 3 short phrases separated by commas. The prompt for the GPT-image-1 @100 baseline is: Figure 11. Diversity of 10K Synthetic Personas. We visualize the diversity of our 10,004 synthetic personas using t-SNE dimensionality reduction on averaged caption embeddings (OpenAI text-embedding-3-small, 1536-dim) from each personas 30-image gallery. Points are colored by age. The average pairwise cosine similarity of 0.611 indicates balanced diversity ; personas occupy shared human aesthetic space while maintaining distinct individual preferences. Our dataset spans 174 countries and 5,460 unique occupations, with 39,003 unique interests and 269,035 visual elements across all personas. Each persona averages 7.1 interests, 5.7 personality traits, and 38.6 visual elements, ensuring rich and diverse personalization signals for image generation models. image), single personalized generation conditioned on typical gallery (e.g. 25 images) incurs roughly $0.25 in image-token cost alone. user generating 100 personalized images would therefore cost about $25; serving one million such users would exceed $25M, excluding text-token fees and overhead. Additionally, these closed-source APIs impose rate limits and quota restrictions, rendering them unsuitable for high-volume, production-scale personalization workloads. As larger vision models continue to become more democratized and cost-efficient, VPTT will only become more practical and broadly applicable, enabling scaled evaluation across wider range of personalization tasks. 8. Additional Results , also an extension to Figure 6 in the main paper are shown in Figure 15. 9. VLLM-Bench Construction (Detailed) 9.1. Conceptual Basis: Deferred Rendering VLLM-Bench conceptualizes text generation as deferred rendering of visual identity. Instead of pixels, each profile is expressed through language-level equivalents of visual cues i.e., objects, lighting, actions, background, foreground, materials, appearance, expressions, pose etc., that together represent how concept would appear in visual media. This abstraction decouples personalization from rendering, enabling scalability, and privacy-preserving use. Additional results as an extension to Figure 2 in the main paper are shown in Figure 14. Additionally, more examples Bidirectional Symmetry. The process is fully reversible: images caption visual elements preferences persona. Figure 12. This Figure is only for illustration and not part of the main dataset. The human figures shown in the sample images are non-author volunteers who provided consent. Their faces and all identifying cues (e.g., location) are fully anonymized. Figure 13. This Figure is only for illustration and not part of the main dataset. The human figures shown in the sample images are non-author volunteers who provided consent. Their faces and all identifying cues (e.g., location) are fully anonymized. In forward mode, we generate structured text; in inverse mode, real user profiles can be captioned and converted into the same structure, enabling safe, text-only adaptation. 9.2. Demographic Generation 9.2.1. Seed Initialization from PersonaHub We initialize 10K personas from PersonaHub [22], which contains 200K curated human-authored persona descriptions. Each persona is derived using: seed indexi = hash(i) mod 200,000, (2) 9.2.2. Two-Stage Demographic Expansion Stage 1a: LLM-Based Demographics with Bias Mitigation. Given seed s, we use Qwen2.5-72B-Instruct to infer country, city, ethnicity, and gender. When confidence in location grounding is low, hash-based diversity re-mapping adjusts sampling with region-specific override rates: India/South Asia (70%), United States (65%), United Kingdom (60%), and Canada (50%). This guarantees balanced representation across 9 ethnicity groups and 60+ authentic cities. ensuring deterministic diversity across geography, age, and profession. Stage 1b: Persona Completion. Demographic scaffolds are expanded into 20+ attributes, including occupation, edFigure 14. Contextual Image Generation and Editing using VPTT-Bench. Each row shows distinct user profile: assets and style cues (left), personalized generations (social post, cultural site), and edits (garden, living room) guided by the same persona identity. All images are generated synthetically via our Visual Personalization RAG (VPRAG) by text, which retrieves persona-aligned cues. To show cross model personalization here the assets are generated by QWEN-image-model [65] and generations and edits by Nano-Banana [24] conditioned only on the first image. Figure 15. Qualitative Comparison across Generation and Editing Tasks. Representative examples from the VPTT-Bench showing outputs from five methods: Baseline, Persona Only, BRAG, VPRAG (ours), and BRAG + VPRAG (ours). ucation, interests (58 domain-specific), social-media tone, and lifestyle traits. Gender is inferred only when explicitly stated in the seed description; otherwise, it is marked as unknown to avoid introducing occupational or cultural stereotypes. While residual bias may still propagate through downstream image-generation models, the final version of VPTT-Bench will include additional filtering and adjustments to mitigate such demographic biases. 9.3. Visual Elements and Preference Generation Each persona contains structured visual vocabulary with 1520 entries per facet: Foreground: subjects, actions, objects, body poses; Background: environments, landmarks, lighting, textures; Atmospheric: materials, color palette, mood, time of day. At least 70% of all elements reference culturally authentic motifs drawn from the personas region (e.g., Seoul Tower, Kashmiri gardens, or Venice canals). We also generate 1520 aesthetic and behavioral preferences (e.g., prefers warm lighting, posts minimalist compositions, documents festivals) that act as latent conditioning factors. These are then used in feedback simulation part of the method to learn subjective preferences. 9.4. Scenario and Caption Generation Each persona produces 30 posts in two phases: 1. Scenario Generation: We sample 68 high-temperature (τ = 0.9) scenarios per batch with diversity constraints across content type (35% activity, 25% appreciation, 25% shared content, 15% selfie), temporal variety (day/night, seasonal), and social context (solo/group). 2. Caption Synthesis: For each scenario, the LLM behaves as vision-language model and produces 150 250-word caption containing: (i) compositional details, (ii) cultural context, (iii) visible preferences, and (iv) annotated facets (foreground, background, atmosphere). caption Each text-embedding-3-small model used elements are pruned to ensure structural consistency. the (1536 D). Unencoded using is 9.5. Parallelized Generation Pipeline The dataset is produced on an 8A100 GPU cluster with vLLM-optimized Qwen2.5-72B. Dynamic batching (10 200 profiles) yields 50150 profiles/hour. Generation of 10K profiles (300K posts) completes in 66200 hours. See Table. 5. Table 5. VPTT-Bench Generation Statistics. Metric Value Total personas Posts per persona Total posts Mean caption length Mean visual elements/persona Parallel throughput 10 000 30 300 000 187.2 words 45.3 50150 profiles/hr Profiles with fewer than 10 valid posts are excluded. All attributes, embeddings, and metadata are stored in JSONL format. 9.6. Privacy, Scalability, and Extensibility Because all profiles are text-based, VPTT-Bench operates fully under deferred rendering, guaranteeing privacy and model-agnostic applicability. The dataset can be scaled to millions of profiles or augmented with real-world profiles through inverse captioning: caption elements preferences persona. This symmetric design ensures both the generative and analytical components of VPTT Framework can operate without any visual exposure, making VPTT-Bench reusable personalization benchmark. 10. Visual Assets Generation 10.1. Mathematical Face Diversity System To ensure controlled, globally diverse identity synthesis, we implement deterministic facial attribute generator producing 97.2M unique combinations. These are then added to the demographics description to first generate user persona image and then conditioned on this image to generate 30 assets. Attribute Space. Ten facial attributes with 46 discrete options each are defined: face shape, eye shape, eye size, nose type, jawline, cheekbones, lip shape, eyebrow type, face length, and chin shape. For each user ID and age group, we compute: seed = hash(user id, age 10 ) mod 232, and draw attributes = {a1, . . . , a10} from the option sets {Oi}. Modifiers such as age-adapted details (e.g., bright eyes vs. wisdom lines), expression labels, and photo styles are applied to achieve additional realism. Combinatorial Diversity. = 10 (cid:89) i=1 Oi E 9.72107, where denotes age modifiers, expression states, and photo styles. This formulation ensures reproducible sampling and balanced variation across users. 10.2. Two-Phase Image Generation Pipeline Phase 1: Persona Base Generation (Text-to-Image). Each personas base portrait is synthesized using QwenImage 2509 [49, 65] diffusion models. Prompts combine demographics and generated facial attributes: photo of person, {gender}, {race/ethnicity}, {age} years old, works as {occupation}, from {city, country}, {oval face shape}, {almond eyes}, {high cheekbones}, {full lips}, professional portrait, confident expression, natural lighting. Generation parameters: Model: Qwen-Image or Qwen-Image-Edit (vanilla mode) Resolution: 1344768 Steps: 50, CFG=0.0, Seed=Deterministic per user No negative prompts (maximizes diversity) Phase 2: Post-Specific Editing (Image-to-Image). Each personas 30 textual posts is rendered by Qwen-Image-Edit2509. Prompts differ by post type: Activity / Selfie / Shared Content (70%): Algorithm 1 VPRAG with Optional Feedback Re-ranking 1: Inputs: query p, persona memory = {(Ei, ci)}N i=1, categories C, budgets {Q(k)}, temperature τ , category embeddings {uk} {caption}, wearing {expression}, {pose}. {clothing}, with Appreciation Posts (30%): {scene description} Negative: Prompt: person, people, human, face, body, portrait Configurations. Standard Mode: 40 steps, CFG=4.0, 1520 s/image Lightning LoRA Mode: 35 s/image (4 faster) 48 steps, CFG=4.5, Pronouns are replaced with this person to ensure gender neutrality. 10.3. Parallel Multi-GPU System An 8A100 cluster executes both phases in parallel. Models are cached per GPU; tasks are dynamically queued via thread-safe managers to maintain 100% utilization. Phase 1 (base portraits) and Phase 2 (post edits) can run independently or sequentially (Table. 6). Table 6. Synthetic Image Generation Performance Metrics. Metric Value Throughput (standard) Throughput (Lightning) Memory footprint 5080 images/hour/GPU 180240 images/hour/GPU <24 GB/GPU (bfloat16 precision) 11. VPTT-Bench Stats To illustrate the diversity of the benchmark, Figure 9 presents the distribution of ethnicities and countries of origin. Despite the modest number of samples, the population is highly diverse. Similarly, Figure 10 reports the age distribution of the benchmark and the interests of the first 1,000 users, grouped by ethnicity. At larger scale, Figure 11 visualizes the averaged caption embeddings of 10K users, highlighting diversity across age groups and visual attributes. 12. VPRAG Algorithm To formally define the steps used by our VPRAG method, Algorithm 1 shows compact form of the retrival engine. 13. Real-World Examples 2: Embedders: EmbedMiniLM post: EmbedOpenAI, element: (cid:80) 2 neff exp(H) 3: Outputs: re-prompt p, (optional) re-ranked 4: Post-level retrieval: 5: EmbedOpenAI(p) 2 6: wi exp(qvi/τ ) ; vi EmbedOpenAI(ci) exp(qvj /τ ) ; (cid:80) Q(k); min (cid:0)neff, 2Q(cid:1) 7: (cid:80) 8: TopKIndices(w, K) 9: Category priorities & quotas: 10: priorityk quk; Csorted SortBy(priorityk) 11: for Csorted do E(k) c(k) q(k) , I; 12: (cid:106) (cid:80) wic(k) jI wj c(k) Q(k)(cid:107) wi log wi; ; Ep 13: end for 14: Element ranking (atomic): 15: qelm EmbedMiniLM(p) 2 16: for Csorted do 17: 18: =0 then continue for do if q(k) Si,k TopK(cid:0)E(k) Ep Ep Si,k , q(k) ; cos(EmbedMiniLM(), qelm)(cid:1) 19: 20: 21: 22: end for 23: Compose: fcompose(p, Ep) {or fcompose(p, Sp, Ep) end for if Sp is precomputed} 24: (Optional) feedback re-ranking: 25: Train small fθ on few profiles: (p, P) (cid:55) sVLM [0, 1] m=1 and pick = 26: At sample {p inference, m}M arg maxm fθ(Embed(p m), Embed(P)) 27: return (or p) inputs supported by our method. These images are not part of the training set, evaluation benchmarks, or any quantitative analysis; they are shown solely to help readers qualitatively understand the range of scenarios in which the system operates. The human figures (Figures. 12 and 13) shown in the sample images are non-author volunteers who provided informed consent for their anonymized photos to be used for illustration. All faces and identifying features (e.g., facial attributes, backgrounds revealing location) have been fully obscured to preserve privacy. These individuals have no relationship to the authorship of the paper, and their inclusion does not reveal author identity in any way. Only for demonstration purposes, we include small set of real-world example images that illustrate the types of visual These examples highlight the diversity of environments, poses, and visual conditions encountered in typical usergenerated content, and demonstrate how the proposed system generalizes across varied real-world scenes. 14. Expanded Tables. Tables 7, 8, 9, 10, shows the expanded versions of the Table 3 in the main paper. Here we report both VPTTscore-c and VPTTscore scores showing the results are consistent with the experiments in the main paper. Cohens in these table are computed against the Baseline. 15. User Study Protocol To measure how well generated images align with an individuals visual style, we conducted human evaluation following the VPTT. Each task presented annotators with 10image gallery representing users typical aesthetics, environments, lighting preferences, clothing patterns, and recurring visual motifs. Alongside the gallery, annotators viewed 22 grid of generated images (Methods AD). Participants rated each generated image independently using slider ranging from 0 to 5, guided strictly by visual similarity to the users gallery rather than image quality, personal preference, or cross-method comparison. This setup allowed us to isolate whether generated sample belonged to the same visual world as the users posts. Annotators were trained to focus on concrete signals such as objects, materials, environments, appearance patterns, lighting, and cultural or stylistic markers. By collecting similarity judgments across thousands of examples, we obtained fine-grained human signal for the plausibility and consistency of personalization across diverse prompts and visual domains. Here is concise form of the instructions: Are key Annotation Instructions Rate each generated image from 0 to 5 based on: -- Objects & Materials: objects or textures similar to those in the gallery? -- Environments & Settings: Are backgrounds or locations consistent with the users style? -- Appearance Patterns: Clothing style, color palette, accessories, poses? -- Lighting & Atmosphere: mood, time of day, natural/white lighting? -- Cultural / Style Markers: Recurring themes, sports references, regional dress, etc. Similar Do NOT: rate based on image quality, personal preference, comparison across methods, or prompt correctness. Score Guide: 5 = Excellent match (fits naturally in users gallery) 4--4.5 = Good match 3--3.5 = Moderate similarity 2--2.5 = Weak similarity 1--1.5 = Minimal similarity 0--0.5 = No similarity at all 15.1. VLM Judge for Automatic Persona Evaluation To complement the human user study, we use visionlanguage model (VLM) as an automatic judge that approximates the same visual-similarity protocol. For each user in the generation split, we first construct profile canvas by tiling up to 10 of their posts into 52 grid, with each post numbered. We then construct methods canvas by arranging the five generated images from different methods horizontally and assigning them blind labels AE via user-specific but deterministic shuffle. The VLM receives the baseline textutal generation prompt, the profile canvas, and the methods canvas as inputs, and is asked to score each of AE independently on 05 scale based purely on visual similarity to the gallery, mirroring the human instructions. For the editing split, the setup is identical except that we additionally provide the original input image that was edited. The same VLM judge prompt structure is used, but the user message explicitly refers to an editing task and includes the editing prompt. In both cases, we query either GPT-4o Vision or Gemini-2.5-Pro (to remove the model bias for the generations by 4o-mini or Gemini-2.5-Pro) with fixed system instruction and task-specific user instruction. The model returns natural-language lines that we parse into per-method scores in [0, 5] (with 0.5 increments) and short explanations. Focus on VISUAL **IMPORTANT**: similarity, not just conceptual alignment. User prompt (generation tasks): System prompt (VLM judge): **Colors & You are an expert visual judge evaluating AI-generated images for visual similarity to users persona. Evaluate how well each generated image captures the VISUAL ELEMENTS from the personas posts. **EVALUATION CRITERIA (Visual Similarity):** 1. **Objects & Materials**: Same objects, materials, textures visible in posts? 2. **Environment & Setting**: Similar locations, backgrounds, environments? 3. **Appearance Patterns**: Similar clothing styles, colors, expressions? 4. **Lighting & Atmosphere**: Similar lighting, mood, atmosphere? 5. Composition**: Similar color palettes and visual composition? 6. **Cultural/Style Markers**: Similar cultural elements, aesthetic style? **SCORING (0-5 scale, use 0.5 increments):** - **5.0**: Excellent visual similarity - Captures most key visual elements from posts - **4.0-4.5**: Good visual similarity - Several key visual elements present - **3.0-3.5**: Moderate visual similarity - Some visual elements recognizable - **2.0-2.5**: Weak visual similarity - Few visual elements match - **1.0-1.5**: Minimal visual similarity - Barely any visual connection - **0.0-0.5**: No visual similarity - Completely different visual style Each was generated using - How similar are the VISUAL TASK: Evaluate these 5 generated images for visual similarity to the personas posts **GENERATION PROMPT:** \"{base prompt}\" **IMAGE 1 (Reference - Profile Context):** Grid of selected posts from the personas gallery (numbered). **IMAGE 2 (Generated Images to Evaluate):** 5 generated images labeled through (left to right). different approach (you dont know which approach was used for which image). **YOUR TASK:** For EACH image (A, B, C, D, E), score 0-5 based on: ELEMENTS (objects, environment, appearance, lighting, colors)? - Do the generated images look like they could belong to the same personas gallery? recognizable visual patterns from the posts? Respond in this EXACT format (one line per image): A: Score=X.X - [1-2 sentence explanation of why this score, focusing on specific visual elements] B: Score=Y.Y - [1-2 sentence explanation of why this score, focusing on specific visual elements] C: Score=Z.Z - [1-2 sentence explanation of why this score, focusing on specific visual elements] D: Score=W.W - [1-2 sentence explanation of why this score, focusing on specific visual elements] E: Score=V.V - [1-2 sentence explanation of why this score, focusing on specific visual elements] - Are there User prompt (editing tasks): - How similar are the VISUAL TASK: Evaluate these 5 edited images for visual similarity to the personas posts **EDITING PROMPT:** \"{base prompt}\" **IMAGE 1 (Input Image):** The original input image that was edited. **IMAGE 2 (Reference - Profile Context):** Grid of selected posts from the personas gallery (numbered). **IMAGE 3 (Edited Images to Evaluate):** 5 edited images labeled through (left to right). Each was edited using different approach (you dont know which approach was used for which image). **YOUR TASK:** For EACH image (A, B, C, D, E), score 0-5 based on: ELEMENTS (objects, environment, appearance, lighting, colors)? - Do the edited images look like they could belong to the same personas gallery? - Are there recognizable visual patterns from the posts? Respond in this EXACT format (one line per image): A: Score=X.X - [1-2 sentence explanation of why this score, focusing on specific visual elements] B: Score=Y.Y - [1-2 sentence explanation of why this score, focusing on specific visual elements] C: Score=Z.Z - [1-2 sentence explanation of why this score, focusing on specific visual elements] D: Score=W.W - [1-2 sentence explanation of why this score, focusing on specific visual elements] E: Score=V.V - [1-2 sentence explanation of why this score, focusing on specific visual elements] 16. Implementation Details For Table 3 in the main paper, we use Qwen2.5-7B-Instruct via vLLM for text generation (T = 0.1, top-p = 0.9, max tokens = 256, seed = 42). For GPT-4o-mini, we use = 0.1, seed = 42. We use Gemini-2.5-Pro with = 0.7, top-p = 0.95. This temperature is chosen as we noticed that lower temperatures tend to truncate the text. Our soft assignment mechanism computes post-level attention weights via softmax with temperature τ = 0.1. For the VLM judges in the main paper, we use Gemini2.5-Pro with temperature = 0.0, top-p = 0.95, and maximum output tokens of 5000. Another variant is the GPT-4o Vision with temperature = 0.0 , and seed = 42. For the feedback network, we train lightweight crossattention transformer to predict user-prompt alignment scores. The model takes text-embedding-3-small (1536dim) embeddings of user profiles and prompts as input, projecting them to 128-dimensional hidden space. Crossattention with 4 heads allows the profile representation to attend to prompt features, followed by feed-forward network (128 256 128) with residual connections and layer normalization. The final prediction head (256 128 64 1) outputs scores in [0,1] via sigmoid activation. We train with AdamW (lr=0.001, weight decay=0.05) using MSE loss, with dropout=0.2 for regularization and early stopping (patience=10)."
        },
        {
            "title": "References",
            "content": "[1] Low-rank adaptation for fast text-to-image diffusion finehttps : / / github . com / cloneofsimo / tuning. lora, 2022. 5, 6, 3 [2] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 44324441, 2019. 1, 3 [3] Rameen Abdal, Or Patashnik, Ekaterina Deyneka, Hao Chen, Aliaksandr Siarohin, Sergey Tulyakov, Daniel CohenOr, and Kfir Aberman. Zero-shot dynamic concept personalization with grid-based lora, 2025. 3 [4] Rameen Abdal, Or Patashnik, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, Daniel Cohen-Or, and Kfir Aberman. Dynamic concepts personalization from single videos. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, New York, NY, USA, 2025. Association for Computing Machinery. 1, 3 [5] Stability AI. stable-diffusion-v1-5/stable-diffusion-v1https://huggingface.co/stable-diffusion-v1-5/stable5. diffusion-v1-5, 2022. 3 [6] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 6 [7] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Jun-Yan Zhu, Kfir Aberman, Ming-Hsuan Yang, and Sergey Tulyakov. Videoalchemy: Open-set personalization in video generation, 2024. 1, 3 Ivan Skorokhodov, [8] Zijie Chen, Lichao Zhang, Fangsheng Weng, Lili Pan, and Zhenzhong Lan. Tailored visions: Enhancing text-to-image generation with personalized prompt rewriting, 2024. 4, 6 Table 7. Method Comparison Across LLMs (Cultural Site Prompt) VPTTscore-c (Uniform Weights) VPTTscore (Novelty Adjusted) Score Win% Score Win% LLM Method 4o-mini Qwen Gemini Baseline Persona Only BRAG VPRAG (Ours) Comb. VPRAG+BRAG (Ours) Baseline Persona Only BRAG VPRAG (Ours) Comb. VPRAG+BRAG (Ours) Baseline Persona Only BRAG VPRAG (Ours) Comb. VPRAG+BRAG (Ours) Individual Metrics PA GS CP NV 0.150 0.364 0.316 0.401 0.419 0.150 0.325 0.395 0.378 0.414 0.150 0.278 0.286 0.359 0.349 0.375 0.429 0.597 0.591 0.641 0.375 0.417 0.670 0.587 0.649 0.375 0.407 0.606 0.597 0.635 0.589 0.630 0.674 0.660 0.686 0.589 0.627 0.683 0.656 0.678 0.589 0.614 0.647 0.656 0. 0.858 0.900 0.821 0.441 0.863 0.544 0.775 0.893 0.768 0.371 0.474 0.529 0.551 0.582 0.371 0.456 0.583 0.540 0.580 0.371 0.433 0.513 0.537 0.550 0.0% 0.1% 8.7% 17.9% 73.3% 0.0% 0.1% 48.6% 11.5% 39.9% 0.0% 0.1% 18.2% 31.9% 49.8% LLM Method 4o-mini Qwen Gemini Baseline Persona Only BRAG VPRAG (Ours) Comb. VPRAG+BRAG (Ours) Baseline Persona Only BRAG VPRAG (Ours) Comb. VPRAG+BRAG (Ours) Baseline Persona Only BRAG VPRAG (Ours) Comb. VPRAG+BRAG (Ours) Individual Metrics PA GS CP NV 0.174 0.428 0.434 0.451 0.448 0.174 0.384 0.516 0.448 0.456 0.174 0.371 0.497 0.396 0.477 0.322 0.437 0.583 0.566 0.581 0.322 0.422 0.697 0.583 0.600 0.322 0.424 0.638 0.544 0.610 0.602 0.659 0.707 0.685 0.703 0.602 0.656 0.707 0.685 0.702 0.602 0.647 0.694 0.674 0.699 0.828 0.899 0.837 0.323 0.854 0.663 0.724 0.894 0. 0.366 0.508 0.574 0.567 0.578 0.366 0.488 0.640 0.572 0.586 0.366 0.481 0.610 0.538 0.595 0.0% 0.9% 35.3% 27.2% 36.6% 0.0% 0.0% 81.6% 6.1% 12.3% 0.0% 0.0% 60.7% 3.7% 35.5% (vs base) 4.18 4.40 5.76 5.95 3.45 5.38 5.35 5.43 2.23 3.12 4.69 4.70 (vs base) 5.85 5.45 6.17 6.09 4.66 8.50 6.03 5.68 4.43 6.96 4.91 6.25 0.319 0.391 0.616 0.635 0.646 0.319 0.378 0.573 0.621 0.590 0.319 0.362 0.588 0.626 0.614 0.0% 0.0% 11.5% 31.8% 56.8% 0.0% 0.0% 14.5% 62.0% 23.5% 0.0% 0.0% 11.6% 58.0% 30.4% 0.312 0.414 0.639 0.645 0.643 0.312 0.400 0.589 0.641 0.614 0.312 0.396 0.644 0.623 0.650 0.0% 0.0% 29.1% 40.5% 30.4% 0.0% 0.0% 14.7% 59.3% 26.0% 0.0% 0.0% 39.0% 13.9% 47.1% (vs base) 3.62 10.94 13.19 12.73 3.00 6.34 12.24 6.93 2.00 7.82 11.30 9.46 (vs base) 5.34 11.65 13.30 12.64 4.33 7.23 12.52 8.54 4.11 11.78 11.66 12.10 Table 8. Method Comparison Across LLMs (Social Media Post Prompt) VPTTscore-c(Uniform Weights) VPTTscore(Novelty Adjusted) Score Win% Score Win% [9] Jacob Cohen. Statistical power analysis for the behavioral sciences. routledge, 2013. 8 [10] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. 3, 7, 8 [11] Meihua Dang, Anikait Singh, Linqi Zhou, Stefano Ermon, and Jiaming Song. Personalized preference fine-tuning of diffusion models, 2025. 4 [12] Google DeepMind. https://deepmind.google/technologies/veo/veo-2/, 1 Veo2. 2024. [13] Michael Deering, Stephanie Winner, Bic Schediwy, Chris Duffy, and Neil Hunt. The triangle processor and normal vector shader: vlsi system for high performance graphics. SIGGRAPH Comput. Graph., 22(4):2130, 1988. 2 [14] Yarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel Cohen-Or. Implicit style-content separation using b-lora. In European Conference on Computer Vision, pages 181198. Springer, 2024. [15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 [16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In ICLR, 2023. 1, 3 [17] Rinon Gal, Moab Arar, Yuval Atzmon, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Designing an encoder In Sigfor fast personalization of text-to-image models. graph, 2023. 1 [18] Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Lcmlookahead for encoder-based text-to-image personalization. arXiv preprint arXiv:2404.03620, 2024. 3 Table 9. Method Comparison Across LLMs (Empty Living Room Prompt) VPTTscore-c (Uniform Weights) VPTTscore (Novelty Adjusted) Score Win% Score Win% LLM Method 4o-mini Qwen Gemini Baseline Persona Only BRAG VPRAG (Ours) Comb. VPRAG+BRAG (Ours) Baseline Persona Only BRAG VPRAG (Ours) Comb. VPRAG+BRAG (Ours) Baseline Persona Only BRAG VPRAG (Ours) Comb. VPRAG+BRAG (Ours) Individual Metrics PA GS CP NV 0.115 0.356 0.264 0.379 0.306 0.115 0.357 0.339 0.416 0.348 0.115 0.333 0.292 0.322 0.346 0.350 0.418 0.533 0.560 0.540 0.350 0.396 0.547 0.583 0.540 0.350 0.400 0.529 0.526 0.537 0.571 0.635 0.617 0.654 0.630 0.571 0.630 0.658 0.657 0.657 0.571 0.630 0.619 0.639 0.643 0.928 0.908 0.924 0.819 0.873 0.828 0.915 0.937 0.913 0.346 0.470 0.472 0.531 0.492 0.346 0.461 0.514 0.552 0.515 0.346 0.454 0.480 0.496 0.508 0.0% 4.0% 5.1% 81.4% 9.4% 0.0% 0.3% 18.9% 66.7% 14.1% 0.0% 3.6% 15.8% 30.1% 50.4% LLM Method GPT-4o-mini Qwen Gemini Baseline Persona Only BRAG VPRAG (Ours) Comb. VPRAG+BRAG (Ours) Baseline Persona Only BRAG VPRAG (Ours) Comb. VPRAG+BRAG (Ours) Baseline Persona Only BRAG VPRAG (Ours) Comb. VPRAG+BRAG (Ours) Individual Metrics PA GS CP NV 0.131 0.358 0.311 0.403 0.378 0.131 0.349 0.377 0.403 0.419 0.131 0.313 0.251 0.340 0. 0.364 0.407 0.594 0.582 0.588 0.364 0.407 0.625 0.583 0.613 0.364 0.403 0.563 0.544 0.582 0.588 0.622 0.650 0.650 0.663 0.588 0.618 0.668 0.653 0.678 0.588 0.615 0.639 0.651 0.667 0.867 0.901 0.860 0.549 0.858 0.529 0.850 0.913 0.822 0.361 0.462 0.518 0.545 0.543 0.361 0.458 0.557 0.546 0.570 0.361 0.444 0.484 0.511 0.528 0.0% 0.2% 15.4% 46.2% 38.2% 0.0% 0.2% 27.4% 20.9% 51.6% 0.0% 0.6% 14.8% 31.0% 53.6% (vs base) 5.15 3.80 5.68 4.51 5.14 4.56 6.45 4.31 4.29 3.92 4.66 4. (vs base) 3.59 4.33 5.12 4.83 2.93 4.90 5.08 4.50 2.97 2.88 3.94 4.04 0.299 0.387 0.584 0.622 0.597 0.299 0.379 0.593 0.630 0.594 0.299 0.375 0.586 0.601 0.606 0.0% 0.0% 8.9% 77.7% 13.5% 0.0% 0.0% 13.8% 76.6% 9.7% 0.0% 0.0% 16.2% 37.8% 45.9% 0.312 0.380 0.609 0.630 0.623 0.312 0.377 0.573 0.623 0.577 0.312 0.368 0.581 0.609 0.606 0.0% 0.0% 17.0% 52.6% 30.5% 0.0% 0.0% 12.0% 68.6% 19.3% 0.0% 0.0% 14.8% 46.2% 39.0% (vs base) 4.56 11.35 13.07 12.24 4.41 10.67 13.54 10.68 3.80 11.07 12.48 12. (vs base) 2.98 10.57 11.47 10.92 2.52 6.85 11.05 6.39 2.49 8.52 10.24 9.66 Table 10. Method Comparison Across LLMs (Garden Editing Prompt) VPTTscore-c (Uniform Weights) VPTTscore (Novelty Adjusted) Score Win% Score Win% [19] Bingjie Gao, Xinyu Gao, Xiaoxue Wu, Yujie Zhou, Yu Qiao, Li Niu, Xinyuan Chen, and Yaohui Wang. The devil is in the prompts: Retrieval-augmented prompt optimization for text-to-video generation, 2025. 4 [20] Junyao Gao, Yanan Sun, Yanchen Liu, Yinhao Tang, Yanhong Zeng, Ding Qi, Kai Chen, and Cairong Zhao. Styleshot: snapshot on any style. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [21] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey, 2024. 4 [22] Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation with 1,000,000,000 personas, 2025. 3, 4, 7 [23] Gemini. Gemini 2.0 flash. https://docs.cloud.google.com/vertex-ai/generativeai/docs/models/gemini/2-0-flash, 2025. 3 2-5-flash-image, 2025. 1, 2, 8 [25] Yuanhe Guo, Linxi Xie, Zhuoran Chen, Kangrui Yu, Ryan Po, Guandao Yang, Gordon Wetztein, and Hongyi Wen. Imagegem: In-the-wild generative image interaction dataset for generative model personalization, 2025. 3 [26] Evans Xu Han, Alice Qian Zhang, Haiyi Zhu, Hong Shen, Paul Pu Liang, and Jane Hsieh. Poet: Supporting prompting creativity and personalization with automated expansion of text-to-image generation, 2025. 4 [27] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. 2023. 3 [28] Hexiang Hu, Kelvin C. K. Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing Gong, William Cohen, Ming-Wei Chang, and Xuhui Jia. Instruct-imagen: Image generation with multi-modal instruction, 2024. [24] Google. Nanobanan. https://aistudio.google.com/models/gemini- [29] huggingface. Minilm-l6-v2. https://huggingface.co/sentence-transformers/all-MiniLML6-v2, 2025. 5, 6 [30] E. T. Jaynes. Information theory and statistical mechanics. Phys. Rev., 106:620630, 1957. [31] Hyungjin Kim, Seokho Ahn, and Young-Duk Seo. Draw your mind: Personalized generation via condition-level modeling in text-to-image diffusion models, 2025. 3, 6 [32] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023. 3 [33] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, pages 19311941, 2023. 1, 3 [34] BlackForest Labs. black-forest-labs/flux.1-kontext-dev. https://huggingface.co/black-forest-labs/FLUX.1-Kontextdev, 2025. 4 [35] BlackForest black-forest-labs/flux.1-dev. Labs. https://huggingface.co/black-forest-labs/FLUX.1-dev, 2025. 3 [36] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 3 [37] Yang Li, Songlin Yang, Xiaoxuan Han, Wei Wang, Jing Dong, Yueming Lyu, and Ziyu Xue. Instant preference alignment for text-to-image diffusion models, 2025. 4 [38] Yuanhuiyi Lyu, Xu Zheng, Lutao Jiang, Yibo Yan, Xin Zou, Huiyu Zhou, Linfeng Zhang, and Xuming Hu. Realrag: Retrieval-augmented realistic image generation via selfreflective contrastive learning, 2025. [39] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image arXiv preprint generation without test-time fine-tuning. arXiv:2307.11410, 2023. 1, 3 [40] Ofir Nabati, Guy Tennenholtz, ChihWei Hsu, Moonkyung Ryu, Deepak Ramachandran, Yinlam Chow, Xiang Li, and Craig Boutilier. Preference adaptive and sequential text-toimage generation, 2025. 4 [41] OPENAI. Sora. https://openai.com/sora/, 2024. 1 [42] OPENAI. Gpt-image-1. https://openai.com/, 2025. 1, 3, 4, 5 [43] OPENAI. Sora2. https://openai.com/index/sora-2/, 2025. 1 [44] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al. Gpt-4 technical report, 2024. 1, 4, 5, 6, 8 [45] OpenAI et al. Gpt-4o system card, 2024. 7, 8, 3, 4, 5 [46] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. 3 [47] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. 3 [48] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. In ICLR. OpenReview.net, 2024. [49] Qwen. Qwen-image-edit-2509. https://huggingface.co/Qwen/Qwen-Image-Edit-2509, 2025. 9 [50] Daniel Roich, Ron Mokady, Amit Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based editing of real images. ACM Transactions on Graphics (TOG), 42(1):113, 2022. 3 [51] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pages 2250022510, 2023. 1, 3, 5 [52] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06949, 2023. 1, 3 [53] Simo Ryu. Dreamboothlora, 2023. 3 [54] Sogand Salehi, Mahdi Shafiei, Teresa Yeo, Roman Bachmann, and Amir Zamir. ViPer: Visual personalization of generative models via individual preference learning. arXiv preprint arXiv:2407.17365, 2024. 1, 4, 6, 2, 3 [55] Rotem Shalev-Arkushin, Rinon Gal, Amit H. Bermano, Imagerag: Dynamic image retrieval for and Ohad Fried. reference-guided image generation, 2025. 4 [56] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without testtime finetuning. arXiv preprint arXiv:2304.03411, 2023. 3 [57] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without testtime finetuning, 2023. 3 [58] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization, 2023. 3 [59] Kuan-Chieh Wang, Daniil Ostashev, Yuwei Fang, Sergey Tulyakov, and Kfir Aberman. Moa: Mixture-of-attention for subject-context disentanglement in personalized image generation. arXiv preprint arXiv:2404.11565, 2024. 3 [60] Yibin Wang, Weizhong Zhang, Jianwei Zheng, and Cheng Jin. High-fidelity person-centric subject-to-image synthesis. arXiv preprint arXiv:2311.10329, 2023. 1, 3 [61] Ye Wang, Ruiqi Liu, Jiang Lin, Fei Liu, Zili Yi, Yilin Wang, and Rui Ma. Omnistyle: Filtering high quality style transfer data at scale, 2025. 1, [62] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 3 [63] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multiarXiv subject image generation with localized attention. preprint arXiv:2305.10431, 2023. 3 [64] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai ImagereLi, Ming Ding, Jie Tang, and Yuxiao Dong. ward: Learning and evaluating human preferences for textto-image generation, 2023. 3 [65] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. 1, 2, 3, 4, 6, 7, 8, 9 [66] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arxiv:2308.06721, 2023. 3 [67] Huaying Yuan, Ziliang Zhao, Shuting Wang, Shitao Xiao, Minheng Ni, Zheng Liu, and Zhicheng Dou. FineRAG: Finegrained retrieval-augmented text-to-image generation. In Proceedings of the 31st International Conference on Computational Linguistics, pages 1119611205, Abu Dhabi, UAE, 2025. Association for Computational Linguistics. 4 [68] zhengxuJosh. Awesome-rag-vision. https://github.com/zhengxuJosh/Awesome-RAG-Vision, 2025."
        }
    ],
    "affiliations": [
        "Snap Research"
    ]
}