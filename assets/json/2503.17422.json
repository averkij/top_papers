{
    "paper_title": "V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V Platforms",
    "authors": [
        "Javier J. Poveda Rodrigo",
        "Mohamed Amine Ahmdi",
        "Alessio Burrello",
        "Daniele Jahier Pagliari",
        "Luca Benini"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recent exponential growth of Large Language Models (LLMs) has relied on GPU-based systems. However, CPUs are emerging as a flexible and lower-cost alternative, especially when targeting inference and reasoning workloads. RISC-V is rapidly gaining traction in this area, given its open and vendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the corresponding software ecosystem are not fully mature and streamlined, given the requirement of domain-specific tuning. This paper aims at filling this gap, focusing on optimizing LLM inference on the Sophon SG2042, the first commercially available many-core RISC-V CPU with vector processing capabilities. On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1 Distill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s for token generation and 6.54/3.68 token/s for prompt processing, with a speed up of up 2.9x/3.0x compared to our baseline."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 2 2 4 7 1 . 3 0 5 2 : r V-SEEK: ACCELERATING LLM REASONING ON OPEN-HARDWARE SERVER-CLASS RISC-V PLATFORMS Javier J. Poveda Rodrigo DAUIN, Politecnico of Turin, Turin, Italy javier.poveda@polito.it Mohamed Amine Ahmdi DAUIN, Politecnico of Turin, Turin, Italy Alessio Burrello DAUIN, Politecnico of Turin, Turin, Italy Daniele Jahier Pagliari DAUIN, Politecnico of Turin, Turin, Italy Luca Benini ETHZ, Zurich, Switzerland March 25,"
        },
        {
            "title": "ABSTRACT",
            "content": "The recent exponential growth of Large Language Models (LLMs) has relied on GPU-based systems. However, CPUs are emerging as flexible and lower-cost alternative, especially when targeting inference and reasoning workloads. RISC-V is rapidly gaining traction in this area, given its open and vendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the corresponding software ecosystem are not fully mature and streamlined, given the requirement of domain-specific tuning. This paper aims at filling this gap, focusing on optimizing LLM inference on the Sophon SG2042, the first commercially available many-core RISC-V CPU with vector processing capabilities. On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1 Distill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s for token generation and 6.54/3.68 token/s for prompt processing, with speed up of up 2.9/3.0 compared to our baseline."
        },
        {
            "title": "Introduction",
            "content": "Hyperscalers (e.g., AWS) and AI deployment companies (e.g., OpenAI) typically accelerate LLM workloads using GPU clusters or dedicated accelerators such as Tensor Processing Units (TPUs). However, many-core CPU acceleration of LLMs has also been recently explored [2], as it provides advantages of lower hardware cost and enhanced flexibility, especially relevant for on-premise and low-latency edge servers. While existing studies mainly target x86 and ARM, recent many-core chips based on the flexible and open-source RISC-V Instruction Set Architecture (ISA) are relatively unexplored [1]. To bridge this gap, this work adapts and optimizes state-of-the-art LLM inference framework (llama.cpp [7]) for the first commodity general-purpose, many-core RISC-V platform (Sophon SG2042 [1]).On two recent open-source LLMs optimized for reasoning (DeepSeek R1 Distill Llama 8B/QWEN 14B), we show speedups over baseline llama.cpp implementation of up to 3.0 in token generation and 2.8 in prompt processing (i.e., prefill) at 4-bit precision, reaching throughput of 4.32/2.29 and 6.54/3.68 tok/s, respectively. On vanilla Llama 7B, we achieve 6.63 and 13.07 tok/s for generation and prefill, i.e., 4.3/5.5 speedup w.r.t. the baseline, and 1.65 better w.r.t. the best-reported results on the SG2042 [8], while being competitive with CPU-based inference on the incumbent x86 architecture."
        },
        {
            "title": "2 Methods",
            "content": "To explore the available alternatives to optimize LLM inference on RISC-V server-class platforms, we target the MILK-V Pioneer, comprising the 64-core Sophon SG2042 and 128GB of DRAM memory. block diagram is shown in Fig. 1-center. We identify three directions from which the problem can be attacked, in SW, inspired by works on PREPRINT - MARCH 25, 2025 Figure 1: From left: optimization flow and contributions. SG2042 block diagram. Pseudocode of the proposed kernel. Figure 2: Matrix vector multiplication size scalability test other architectures [5, 6, 3]: i) Developing optimized and, if supported, quantized kernels for key LLM layers, that fully exploit the HW, coping with its memory infrastructure, pipeline, and vectorization; Fig.1-right, shows the pseudocode of our proposed kernel: first, the fp32 input (vector or thin matrix) is quantized to int8; then, two nested loops are executed to perform GEMV operation, the outermost iterating on the rows of input matrix A, and the innermost on its columns. After the column loop ends, de-quantization is applied, combining scale factors from blocks and to produce an output fp32 value. This new kernel exploits the platforms vector units while also optimizing data locality. ii) Choosing suitable compilation toolchain, supporting advanced optimization passes and exploiting the available ISA extensions. In our case, kernels are compiled with the Xuantie fork of GCC 10.4, as it is the only one supporting the HW vector units of the Sophon SG2042. Instead, for the whole llama.cpp framework, we consider two alternatives: GCC 13.2, and Clang 19 (Xuantie GCC 10.4 is not compatible with the latest llama.cpp release). iii) Optimizing model mapping, specifically pages/thread allocation, addressing this type of systems complex memory hierarchy. Namely, we optimize Non-uniform Memory Access (NUMA) latency exploring different numactl options combined in 4 policies: i) NUMA Balancing on, all other options off, ii) all options off, iii) Balancing off+Core Binding on, iv) Balancing off+Memory Interleaving on. We apply our optimizatons to the llama.cpp [7] framework, testing on 3 open-source LLMs of increasing size, with Q4_0 quantization (vanilla Llama 7B, DeepSeek R1 Distill Llama 8B, DeepSeek R1 Distill QWEN 14B, referred to as 7B, 8B and 14B below)."
        },
        {
            "title": "3 Results",
            "content": "To show the results of our optimization, we executed the prefill of the three LLMs with user prompt \"Explain to me what is RISC-V, what are its principles and why it is so cool?\" (22 tokens), while we averaged the token generation performance over 256 test-generated tokens. Kernel Scaling. Fig.2 shows the single-thread scalability of multiple baseline kernels (llama.cpps GGML and OpenBLASs defaults) and of our proposed one. Compared to the best baseline, we improve the GOPS by +38.3% on average, peaking at +56.3% at matrix size 1024. Compiler exploration. In Fig.3, we evaluate DeepSeeks 8B model inference when compiling with Clang or GCC, using our proposed kernel. Clang 19 constantly outperforms GCC 13.2, reaching average performance gains of 34% and 25% for token generation and prefill, respectively. The crucial reason is the combination of ISA extension support, and more advanced compilation passes (e.g., more aggressive in-lining and loop unrolling). Regardless of the compiler used, using > 32 threads leads to performance loss. This behavior is attributed to the default NUMA balancing policy, which is suboptimal for LLM inference due to the predictable nature of the workload, leading to high number of thread and memory page migrations. NUMA policy impact. Indeed, with the NUMA balancing off and memory interleaving on, as expected, we achieve the best results for both token generation (4.32 tokens/s) and for prefill (6.54 tokens/s) with 64 threads, thanks to the strong reduction in memory page migration. 2 PREPRINT - MARCH 25, 2025 Figure 3: Compilers comparison scaling the n. of threads for DeepSeeks 8B model token gen., Bar, and prefill, Line. Figure 4: NUMA policies exploration on DeepSeeks 8B model. Token generation shown with bars, prefill with lines. Overall, thanks to our optimizations, the 7B, 8B and 14B LLMs reach maximum throughput of 13.07/6.54/3.68 tok/s respectively, outperforming baseline llama.cpp by up to 5.5/2.9/3. Compared to the best reported result on the SG2042 [8], we improve the peak throughput on LLama 7B by 1.65. Versus similar and more mature x86 platform, the 64-cores AMD EPYC 7742, we improve the energy efficiency by 1.2(55 token/s/mW vs 45 token/s/mW) [4]."
        },
        {
            "title": "References",
            "content": "[1] BROWN, N., AND JAMIESON, M. Performance characterisation of the 64-core sg2042 risc-v cpu for hpc, 2024. [2] ET AL., H. S. Efficient llm inference on cpus, 2023. [3] ET AL., J. J. Characterizing and optimizing transformer inference on arm many-core processor. In ICPP 22 (2023). [4] ET AL., T. P. Qigen: Generating efficient kernels for quantized inference on large language models, 2023. [5] ET AL., Z. Y. Llm inference unveiled: Survey and roofline model insights, 2024. [6] FU, X. E. A. Optimizing attention by exploiting data reuse on arm multi-core cpus. In ICS 24 (2024), p. 137149. [7] GERGANOV, G. llama.cpp. https://github.com/ggerganov/llama.cpp, 2025. Accessed: 2025-01-21. [8] WANG, C. Perfxlm: llm inference engine on risc-v cpus. In RISC-V Summit Europe (2024). Presented at the RISC-V Summit Europe 2024."
        }
    ],
    "affiliations": [
        "DAUIN, Politecnico of Turin, Turin, Italy",
        "ETHZ, Zurich, Switzerland"
    ]
}