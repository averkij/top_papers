{
    "paper_title": "Unified Reward Model for Multimodal Understanding and Generation",
    "authors": [
        "Yibin Wang",
        "Yuhang Zang",
        "Hao Li",
        "Cheng Jin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that jointly learning to assess multiple tasks may foster a synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UnifiedReward, the first unified reward model for multimodal understanding and generation assessment, enabling both pairwise ranking and pointwise scoring, which can be employed for vision model preference alignment. Specifically, (1) we first develop UnifiedReward on our constructed large-scale human preference dataset, including both image and video generation/understanding tasks. (2) Then, it is utilized to automatically construct high-quality preference pair data based on the vision models, fine-gradually filtering their outputs through pair ranking and point sifting. (3) Finally, these data are used for their preference alignment through Direct Preference Optimization (DPO). Experimental results demonstrate that joint learning to assess diverse visual tasks can lead to substantial mutual benefits and we apply our pipeline to both image and video understanding/generation tasks, significantly improving the performance in each domain."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 6 3 2 5 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Unified Reward Model for Multimodal Understanding and Generation",
            "content": "Yibin Wang 1,2 Yuhang Zang 3 Hao Li 1,2,4 Cheng Jin 1 Jiaqi Wang 2,"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that jointly learning to assess multiple tasks may foster synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UNIFIEDREWARD, the first unified reward model for multimodal understanding and generation assessment, enabling both pairwise ranking and pointwise scoring, which can be employed for vision model preference alignment. Specifically, (1) we first develop UNIFIEDREWARD on our constructed large-scale human preference dataset, including both image and video generation/understanding tasks. (2) Then, it is utilized to automatically construct high-quality preference pair data based on the vision models, fine-gradually filtering their outputs through pair ranking and point sifting. (3) Finally, these data are used for their preference alignment through Direct Preference Optimization (DPO). Experimental results demonstrate that joint learning to assess diverse visual tasks can lead to substantial mutual benefits and we apply our pipeline to both image and video understanding/generation tasks, significantly improving the performance in each domain. 1. Introduction Recent advancements in human preference alignment have substantially propelled the progress of multimodal generation and understanding tasks. straightforward technique road is directly collecting human feedback to construct preference datasets for model optimization [29, 38, 52]. Despite its effectiveness, collecting large-scale human feedback is Figure 1. Overview of our UnifiedReward for Multimodal Understanding and Generation Alignment, including three steps: (1) Unified Reward Model Training, (2) Preference Data Construction, and (3) Generation/Understanding Model Alignment. time-consuming and resource-intensive. To this end, an alternative popular approach involves learning reward models [18, 24, 28, 40, 44, 47, 50] from limited amount of preference data and using the learned reward function to generate preference data based on the output of vision models. This synthetic preference data can then be leveraged for vision model preference alignment, significantly reducing the need for extensive human annotations. Despite their progress, we posit two concerns: (1) current reward models are often tailored to specific tasks, as shown in Tab. 1, limiting their adaptability across diverse visual understanding and generative tasks. The key challenge lies in the lack of comprehensive human preference dataset that spans wide range of visual tasks. (2) We intuitively argue that visual tasks are inherently interconnected, and jointly learning multiple visual tasks may create mutually reinforcing effect. Specifically, enhanced image understanding may improve the evaluation of image generation by providing more accurate assessment of content quality and contextual relevance. Similarly, improvements in image evaluation may benefit video evaluation, as high-quality image assessments lead to more accurate eval1 uations of video frames, contributing to overall better quality video assessment. This cross-task synergy facilitates more robust evaluation of outputs across both image and video modalities in tasks involving understanding and generation. It inspires the development of unified modal reward model that yields more precise reward signals for preference optimization. To this end, we propose UNIFIEDREWARD, the first unified reward model for multimodal understanding and generation model assessment, capable of both pairwise ranking and pointwise scoring, which can be utilized for preference alignment on diverse vision tasks. Our fine-tuning pipeline, as shown in Fig. 1, includes three key stages: (1) First, we construct large-scale human preference dataset that spans both image and video generation/understanding tasks and develop UNIFIEDREWARD based on this dataset. (2) Next, we employ UNIFIEDREWARD to automatically construct high-quality preference pair data by selecting the outputs of specific baselines, such as Vision Language Models (VLM) and diffusion models, through multi-stage filtering, i.e., pair ranking and point sifting. (3) Finally, we use these preference pairs to align the outputs of these models with human preferences via direct preference optimization. Our experiments show that learning multiple visual tasks together yields significant reciprocal benefits, enhancing performance in each individual domain. By implementing our pipeline across both image and video understanding and generation baselines, we observe notable improvements in their quality and alignment. In summary, our contributions are as follows: (1) We construct large-scale human preference dataset spans diverse vision tasks and develop UNIFIEDREWARD, the first unified reward model for multimodal understanding and generation model assessment, capable of both pairwise ranking and pointwise scoring. (2) We propose general pipeline for both image and video understanding/generation model preference alignment, which remains an underexplored area in current research. Extensive experiments demonstrate its effectiveness in improving the performance of vision models in each domain. (3) Our experiments show that learning to assess image and video tasks jointly leads to synergistic improvement in performance across different visual domains. Through this work, we aim to expand the scope of reward models, making them more adaptable, generalizable, and effective across various visual applications. 2. Related Work Reward Models are crucial in aligning vision understanding and generation models with human preferences. Traditional methods [13, 14, 30] for evaluating vision quality and semantic consistency rely on metrics such as FID [11] and CLIP scores [33]. Despite their effectiveness, they are limited to capturing human preferences. Therefore, recent Table 1. Comparison of Our Reward Method with Recent Approaches. UNIFIEDREWARD is capable of assessing both image and video understanding and generation. Pair and Point refer to Pair Ranking and Point Scoring, respectively. Reward Model Method Image Generation Image Understand Video Generation Video Understand Point PickScore23 [17] Point HPS23 [43] Point ImageReward23 [43] LLaVA-Critic24 [44] Pair/Point IXC-2.5-Reward25 [50] Pair/Point VideoScore24 [40] LiFT24 [40] VisionReward24 [47] VideoReward25 [28] Point Point Point Point UnifiedReward Pair/Point studies [26, 46, 54] utilize human preference data to finetune CLIP, enabling them to better predict and align with human evaluations. With the advent of VLMs [1, 39], their robust ability to align visual and textual data makes them promising candidates for reward modeling. These models can be adapted into two main categories based on their capabilities: understanding assessment models [44, 50], which are designed exclusively for evaluating visual understanding tasks, and generation assessment models [9, 28, 40, 47], which focus on assessing visual synthesis quality. However, these reward models are typically designed for specific tasks, as illustrated in Tab. 1, restricting their ability to adapt to diverse visual understanding and generative tasks. In this work, we propose the first unified reward model for both image and video understanding and generation assessment, which is more adaptable, generalizable, and effective across various visual applications. Preference Learning for VLM/Diffusion are widely utilized to enhance their image and video understanding/generation performance. In video understanding, prior works [2, 24, 36, 52] have explored reinforcement learning with human feedback or AI-generated feedback to refine reward models for factuality assessment. For image understanding, researchers investigate Direct Preference Optimization (DPO) as an alternative approach to preference modeling. For example, [2, 7] apply DPO to refine rewards distilled from GPT-4V across different model outputs. Similar methods have been applied to image generation [18, 38, 47] and video generation [6, 22, 28, 29, 40, 47, 49, 51], using reward models or human preference data to align pretrained diffusion models. However, these methods rely on task-specific reward models, and no unified reward model has been developed for preference learning across both image and video generation and understanding tasks. This limits the generalizability and efficiency of reward-based alignment. Our work investigates the effectiveness of joint learning to assess multiple visual tasks, demonstrating that cross-task synergy enhances the evaluation capabilities across each domain. 2 Figure 2. Method Overview. The pipeline of UNIFIEDREWARD consists of three key stages: (1) Unified Reward Model Training: We train unified reward model to evaluate both multimodal generation and understanding tasks using pointwise scoring and pairwise ranking strategy. (2) Preference Data Construction: We use the trained reward model to construct high-quality preference data through three steps: (a) data generation from VLM/Diffusion, (b) pairwise ranking to divide the chosen and rejected outputs, and (c) pointwise filtering to refine the chosen and rejected samples. (3) Generation/Understanding Model Alignment: The constructed preference data is then used to fine-tune VLM/Diffusion via Direct Preference Optimization, aligning their outputs with human preferences. 3. Method 3.1. Overview This work aims to propose unified reward model for vision model preference alignment. Existing studies typically develop specialized reward models for specific tasks as shown in Tab. 1, which restricts their adaptability across diverse visual applications. Furthermore, we intuitively argue that jointly learning multiple visual tasks can create mutually reinforcing effect, yet this remains an underexplored area. To this end, this work proposes UNIFIEDREWARD, the first unified reward model for multimodal understanding and generation assessment, enabling both pair ranking and point scoring. It is then leveraged for Vision-Language Models (VLMs) and Diffusion model alignment, enabling more robust and adaptable preference learning across diverse visual tasks. The pipeline is illustrated in Fig. 2. Specifically, we first construct large-scale, unified preference dataset (Sec. 3.2.1) and train our UNIFIEDREWARD model on this dataset (Sec. 3.2.2). Then, we curate preference datasets for VLMs and diffusion models by applying pair ranking and point sifting on their outputs (Sec. 3.3). These curated datasets are subsequently used for Direct Preference Optimization (DPO) (Sec. 3.4), effectively enhancing model alignment with human preferences. 3.2. Unified Reward Model Training 3.2.1. Unified Preference Dataset Construction comprehensive human preference dataset that spans multiple vision-related tasks is essential for training unified reward model. However, existing human feedback datasets, such as [29, 40, 44], are typically designed for specific tasks, limiting their generalizability. Currently, there is no human preference dataset that comprehensively covers both visual understanding and generation tasks, highlighting the need for more versatile dataset. To bridge this gap, we integrate existing datasets and preprocess them to construct the first large-scale unified human preference dataset, which consists of approximately 236K data covering both image and video understanding and generation tasks. The detailed 3 Table 2. Training Datasets for Image and Video Generation/Understanding Assessment. * indicates the dataset is preprocessed in our work. Task Method Dataset Image Video Generation Understanding Generation Understanding Pair Point Pair Point Pair Point Pair Point EvalMuse* HPD* OIP EvalMuse* LLaVA-Critic LLaVA-Critic VideoDPO LiFT-HRA VideoFeedback ShareGPTVideo ShareGPTVideo* Size 3K 25.6K 7.4K 32.7K 25K 25K 10K 20K 36.6K 17K 34K For pairwise ranking datasets, we standardize the answer format as image/video/response is better than image/video/response Y, where and represent the assigned indices. If the dataset includes evaluation justifications [40, 44], we retain them to allow the model to learn from human reasoning. For pointwise scoring, we do not enforce unified response format or score range, allowing the model to learn from diverse rating styles and scoring systems across different datasets. To ensure alignment between evaluation criteria and responses, we adjust instruction prompts accordingly. The prompting templates are provided in Appendix C. As shown in Fig. 3, the training data for video generation pairwise ranking assessment is relatively limited compared to other tasks, but we believe that the synergistic effect of multitask learning can alleviate this deficiency. Overall, our dataset provides diverse and comprehensive collection of human preferences, covering both pairwise ranking and pointwise scoring across image and video understanding/generation tasks. This enables effective reward model training, ensuring robust performance across multimodal understanding and generation applications, which will be introduced in the following sections. 3.2.2. Unified Preference Learning Based on the comprehensive datasets, we fine-tune pretrained VLM [21] with strong vision understanding capabilities to develop UNIFIEDREWARD, jointly training it Instead of learning evaluaacross diverse vision tasks. tion from scratch, we integrate assessment ability as an additional discriminative skill, leveraging the models existing visual comprehension to enhance its evaluation performance across various tasks. Fig. 2 (top) illustrates our training process. Specifically, for multimodal generation evaluation, our model takes vision tokens, instruction input, and caption as input. In contrast, for multimodal understanding, the caption is replaced by question and the corresponding response(s), aligning the input format with the respective task requirements. The model is trained to predict the pointwise score Figure 3. Visualization of Statistical Results. This figure presents the distribution of our constructed unified preference dataset, along with the pairwise and pointwise distributions for each task. statistics and visualized distributions of the dataset are presented in Tab. 2 and Fig. 3, respectively. We will elaborate on data construction process for each task in the following. Image Generation. EvalMuse [8] consists of 4K prompts, each with multiple images generated by different models. Each image is evaluated by at least three annotators, who provide an overall score (1-5) and element-wise labels indicating whether specific elements are present. For pointwise score learning, we compute the final score as the average of all ratings. An element is considered generated if at least two annotators agree; otherwise, it is marked as not generated. We integrate the overall score and element-wise labels as assessment answer for reward model learning. For pairwise ranking, we select the images with the highest and lowest average score from the same prompt as ranking pair. Human Preference Dataset (HPD) [3] contains 700K human preference votes. For each prompt, two images generated by different models are provided, each with its respective vote count. In our work, we directly use the vote counts to construct pairwise ranking data, ranking the image with more votes as the preferred one. Open-Image-Preferences (OIP) 1 contains 7.4K text-to-image preference pairs, which Image Understanding. are directly used in this work. LLava-Critic-113K [44] consists of 40K pointwise score and 73K pairwise ranking data samples for image understanding assessment learning. From this dataset, we select 25K samples each for pairwise ranking and pointwise scoring learning. Video Generation. VideoDPO [29] includes 10K synthesized video pairs for text-to-video model DPO. We directly use this dataset for our pairwise ranking learning in video generation. LiFT-HRA [40] and VideoFeedback [9] provide extensive human feedback for pointwise scoring of synthesized videos, which we directly incorporate into our work. Video Understanding. ShareGPTVideo-DPO [53] contains 17K video understanding DPO data, where each response in pair is assigned an evaluation score. We directly use the pair data for pairwise ranking learning, while the individual response scores are extracted for pointwise scoring learning. 1https://huggingface.co/datasets/data-is-better-together/open-imagepreferences-v1-binarized 4 or pairwise ranking based on the criteria specified in the instruction prompt. If the training data includes justifications, the model is also trained to generate detailed explanations to support its evaluations. During training, the optimization objective is standard cross-entropy loss, but it is computed only on the models predicted answer. After training our UNIFIEDREWARD, we leverage it for preference alignment in multimodal understanding and generation models. This process consists of two sequential steps: Preference Data Construction and Generation/Understanding Model Alignment. The following sections provide detailed explanation of each step. 3.3. Preference Data Construction The quality of preference alignment data directly determines the effectiveness of model alignment. Existing methods [28, 40, 44] are often limited to single evaluation strategy, either assigning pairwise rankings or pointwise scores to model outputs for preference data construction. In contrast, this work leverages both pairwise ranking and pointwise scoring capabilities of UNIFIEDREWARD, enabling higher quality preference data construction pipeline, illustrated in Fig. 2 (bottom left). Specifically, our pipeline includes three sequential steps: (1) Data Generation. Given an image/video-question pair (or generation prompt), VLM (or diffusion model) generates multiple candidate outputs {O1, O2, . . . , ON }. These outputs serve as the initial pool for followed preference data filtering. (2) Pair Ranking. Given outputs, we group them into N/2 pairs and use our model to perform pairwise ranking for each pair. Then, we classify these ranked pairs 1, Oc into chosen list = {Oc N/2} and rejected list = {Or 2, . . . , Or N/2}. (3) Point Sifting. Finally, we apply our model to assign pointwise scores to all outputs in both the chosen list and the rejected list. The final preference data pair is determined as: 2, . . . , Oc 1, Or (O = arg max OC S(O), = arg min OR S(O)), where S(O) represents the pointwise score assigned by our model, is the most preferred output and is the least preferred output. By combining pairwise ranking and pointwise scoring, the final preference data could provide high-quality and reliable preference signal, effectively capturing both relative comparisons and absolute quality assessments. 3.4. Generation/Understanding Model Alignment After constructing the preference data, we leverage it for multimodal generation and understanding model alignment using DPO, which enables models to align their outputs with human preferences without explicit reward modeling, optimizing directly based on ranked preference pairs. 3.4.1. DPO for Multimodal Generation For multimodal generation tasks, diffusion models [12] are widely used due to their strong capability in generating high-quality and diverse outputs across image and video synthesis. Therefore, we apply DPO on diffusion models to align their outputs with human preferences. 0)i}M i=1, where xw Given the constructed preference pair dataset DGen = {(xw 0 and xl 0 , xl represents the preferred generated sample and the less preferred sample respectively, denotes the number of samples, we optimize the diffusion model by comparing the noise prediction differences between the fine-tuned model and pre-trained reference model following [38]: L(θ) = (cid:32) log σ (xw 0 ,xl 0)DGen, tU (0,T ), xw q(xw xw 0 ), xl tq(xl txl 0) βgT ω(λt) (cid:16) ϵw ϵθ(xw , t)2 2 ϵw ϵref(xw , t)2 2 (cid:16) ϵl ϵθ(xl t, t)2 2 ϵl ϵref(xl t, t)2 2 (cid:17)(cid:17) (cid:33) , and xl , t) and ϵref(x are the noisy latents derived from xw where xw 0 and xl 0 at timestep t, respectively. ϵθ(x , t) denote the predicted noise from the fine-tuned and pre-trained reference diffusion models, respectively. βg is temperature hyperparameter controlling optimization strength, σ is the logistic function, λt represents the signal-to-noise ratio, and ω(λt) is weighting function, which is treated as constant equal to βg in this work. This loss encourages the fine-tuned diffusion model to reduce the denoising error for preferred samples while increasing it for less preferred ones, thereby improving the generation quality. 3.4.2. DPO for Multimodal Understanding Similar to generation alignment, we apply DPO to adjust the models response preference for multimodal understanding models, i.e., VLMs. Given an input (e.g., an image/videoquestion pair) with preferred response yw and less preferred response yl from preference pair dataset DUnd, the optimization is formulated as: L(θ) = E(x,yw,yl)DUnd βu log σ log (cid:34) (cid:32) πθ(ywx) πref(ywx) (cid:33)(cid:35) log πθ(ylx) πref(ylx) , where πθ(yx) and πref(yx) is the response probability under the fine-tuned model and pre-trained reference model, respectively. βu is temperature hyperparameter that controls optimization sensitivity. 5 This loss encourages the fine-tuned VLMs to increase the likelihood of generating preferred responses while decreasing it for less preferred ones, thereby improving the models alignment with human preferences and enhancing reasoning quality. 4. Experiments 4.1. Experimental Setup 4.1.1. Models and Settings the pre-trained LLaVAReward Model: We adopt OneVision 7B (OV-7B) [21] as the base architecture for UNIFIEDREWARD, leveraging its strong performance in both image and video understanding. Training is conducted with batch size of 2, gradient accumulation steps of 16, learning rate of 2.5 106, and warm-up ratio of 0.3. Multimodal Understanding DPO: Based on UNIFIEDREWARD, we apply DPO to LLaVA-OneVision 7B [21] and LLaVA-Video [55] to enhance their performance in image and video understanding, respectively. We use batch size of 1, gradient accumulation steps of 16, learning rate of 5 107, and set βu = 0.1. Multimodal Generation DPO: For image and video generation DPO, we use SDXL-Turbo [32] and T2V-Turbo [22], respectively. The parameter βg is set to 5000, with batch sizes of 32 for SDXL-Turbo and 16 for T2V-Turbo. We construct 10K preference data for video generation DPO and 14k for other task DPO. The number of candidate outputs is set to 10. All models are trained for 3 epochs. 4.1.2. Evaluations Multimodal Understanding: We evaluate the image and video understanding assessment of UNIFIEDREWARD on VLRewardBench [23] and ShareGPTVideo [53] (1K samples for testing), respectively. Multimodal Generation: GenAI-Bench [16] includes both image and video generation reward benchmarks, which are utilized. Besides, we also employ VideoGen-RewardBench [28] for video generation assessment benchmark. DPO: For image understanding, LLaVABench [27], WildVision [31], LLaVABench-Wilder [19], LiveBench [41], and MMHal [35] are employed. For video understanding, we employ MSRVTT [45], MSVD [10], TGIF [25], LongVideoBench [42], MLVU [56] and VideoMME [5]. For image generation evaluation, we generate images conditioned on captions from the Partiprompt [48] and HPSv2 [43] benchmarks (1632 and 3200 captions respectively) and utilize the image reward model, i.e., PickScore [17], HPDv2 [43] and ImageReward [46] for quality assessment. VBench [14] is used for video generation assessment. 4.2. Reward Model Comparison Results Image Understanding. We compare our method with the latest open-source model, LLaVA-Critic [44], as well as Table 3. Image Understanding Assessment Comparison. We evaluate baselines across different aspects on VLRewardBench. Models General Hallu. Reason. Overall Accuracy Macro Accuracy Gemini-1.5-Pro GPT-4o LLaVA-Critic OV-7B w/ Img. Und. w/ Img. Und.+Gen. w/ Img.+Vid. Und UnifiedReward 50.8 49.1 47.4 32.2 47.6 49.8 52.4 60.6 72.5 67.6 38. 20.1 38.3 52.6 55.6 78.4 64.2 70.5 53.8 57.1 54.5 58.1 57.2 60.5 67.2 65.8 46. 29.6 47.4 50.4 52.7 66.1 62.5 62.4 46.6 36.5 46.8 53.5 55.1 66.5 Table 4. Video Understanding Assessment Comparison. We evaluate the performance of our model using different training data configurations. OV-7B w/ Vid. Und. w/ Vid.&Img. Und. w/ Vid Und.&Gen. UnifiedReward Acc. 48.2 74.2 76.6 78.6 84.0 Table 5. Image and Video Generation Assessment Comparison. Evaluation is conducted on GenAI-Bench (Image and Video) and VideoGen-Reward. tau indicates that accuracy is calculated with ties, and diff excludes tied pairs when calculating accuracy. Method Image Generation GenAI-Bench diff tau Method Video Generation GenAI-Bench diff tau VideoGen-Reward tau diff PickScore HPSv2 ImageReward VisionReward OV-7B w/ Img. Gen. w/ Img. Gen.+Und. w/ Img.+Vid. Gen. UnifiedReward 53.2 51.6 47.8 46.8 39.7 39.4 47.7 50.5 54.8 67.2 68.4 65.0 66.4 53.2 64.0 65.9 67.6 70. VideoScore LiFT VisionReward VideoReward OV-7B w/ Vid. Gen. w/ Vid. Gen.+Und. w/ Img.+Vid. Gen. 46.2 41.2 52.1 50.2 40.8 48.2 49.1 52.0 70.6 60.1 73.1 73.3 51.4 69.4 71.6 73. 42.1 40.6 57.4 60.1 40.4 44.3 45.1 53.6 UnifiedReward 60.7 77.2 66. 49.9 58.3 68.2 73.9 50.2 62.4 64.9 70.7 79.3 two closed-source models, Gemini-1.5-Pro [37] and GPT4o [15]. The experimental results, shown in Tab. 3, indicate that our method outperforms the best baseline in most metrics, e.g., macro accuracy (66.5% vs. 62.5%), which demonstrates the superiority of our method in image understanding assessment. For Video Understanding, to the best of our knowledge, there are currently no available baselines. Therefore, we explore the effectiveness of multi-task learning in video understanding assessment, which will be analyzed in the next section. In Image Generation assessment, we compare our method with both traditional and state-ofthe-art approaches, including PickScore [17], HPSv2 [43], ImageReward [46], and VisionReward [47]. The results are presented in Tab. 5. Notably, the latest work, VisionReward, supports reward modeling for both image and video generation. However, it trains separate models for each task using their respective datasets, whereas our approach jointly learns multiple tasks within unified framework, leading to relatively better performance. For Video Generation, we compare our method with the latest approaches, including 6 Table 6. Video Understanding DPO Comparison. All methods are trained with same settings for fair evaluation. MSRVTT MSVD TGIF LongVideoBench MLVU Video-MME Method Acc. Score Acc. Score Acc. Score LLaVA-Video-7B24 w/ Houd-DPO24 w/ TPO25 w/ UnifiedReward 52.8 56.8 55.0 65.0 3.24 3.34 3. 3.45 69.7 72.8 72.6 78.3 3.90 3.97 3.93 4.01 51.9 54.9 53. 59.7 3.37 3.45 3.40 3.51 Acc. 58.1 58.0 58.2 58. M-Avg. Short Medium Long Avg. 70.9 71.8 72. 72.3 76.1 76.3 76.9 76.2 61.6 61.3 62.1 61.3 52.3 51.2 52. 52.5 63.3 63.0 63.7 63.5 Table 7. Image Understanding DPO Comparison. We compare our method with LLaVA-Critic for DPO based on LLaVAOneVision-7B. Table 9. Image Generation DPO Comparison. We compare the performance of baseline DPO using our method with it trained on the Pick-a-Pic dataset. LLaVABen. WildVision LLaVABen. Wilder LiveBen. MMHal PickScore HPSv2 ImageReward OV-7B w/ LLaVA-Critic24 w/ UnifiedReward 90.3 100. 101.4 54.9 67.3 67.8 67.8 71.6 75.0 77.1 84. 85.4 3.19 3.91 4.01 SDXL-Turbo [32] Baseline w/ Pick-a-Pic w/ UnifiedReward 43.24 54.32 63.32 29.37 30.03 32.44 0.82 0.93 1. Table 8. Video Generation DPO Comparison. We compare our method with VideoDPO on three average metrics on VBench. Models T2V-Turbo [22] Baseline w/ VideoDPO24 w/ UnifiedReward VBench (%) Quality Semantics 82.71 83.80 84.11 73.93 73.81 74. Total 80.95 81.80 82.10 VideoScore [9], LiFT [40], VisionReward [47], and VideoReward [29]. Among them, VideoScore is an evaluation model for video generation, while the others are reward models. As shown in Fig. 3, our training data for video generation assessment is relatively limited. However, as demonstrated in Tab. 5, our method excels across all metrics when compared to all baselines, highlighting that multitask learning not only mitigates the issue of insufficient training data but also enhances the learning effectiveness for video generation assessment."
        },
        {
            "title": "More details of baselines and benchmarks are provided",
            "content": "in Appendix A. 4.2.1. Multi-task Assessment Learning In this work, we intuitively argue that visual tasks are inherently interconnected, and jointly learning multiple visual tasks may create mutually reinforcing effect. Therefore, we explore the effectiveness of multi-task learning on the reward model. Specifically, for each task, we employ different training data configurations to train the model, investigating the impact of jointly learning across different modalities (image and video) and tasks (understanding and generation). For example, for the image understanding task, we design three training configurations to investigate the impact of multi-task learning: (1) training solely on image understanding assessment, (2) jointly learning image understanding and image generation assessment, and (3) jointly learning image understanding and video understanding assessment. The results are presented in Tab. 3. Notably, our findings indicate that multi-task learning significantly enhances the models overall performance compared to training on single task. For instance, jointly training on both image and video understanding tasks improves overall accuracy and macro accuracy by 5.3% and 8.3%, respectively, compared to training solely on image understanding. The results for other tasks are presented in Tab. 4 and 5, which consistently demonstrate the effectiveness of joint learning. These results highlight the benefits of leveraging shared knowledge across different visual tasks, leading to more robust and generalizable reward model. 4.3. DPO Comparison Results To further demonstrate the effectiveness of our method across both image and video understanding and generation tasks, we apply it to preference data construction through fine-grained output sifting, which is then leveraged for model alignment via DPO. Image Understanding. We compare our method with the latest reward model LLaVA-Critic by employing the same image-question pair source, i.e., LLaVA-RLHF [34] to construct preference data for OV-7B, ensuring fair comparison. The results, presented in Tab. 7, demonstrate that DPO using our method consistently outperforms LLaVACritic across all benchmarks. For instance, our method achieves 3.4% improvement on LLaVABench, highlighting its superior effectiveness. Video Understanding. We extract prompts from ShareGPTVideo-DPO [53] to construct preference data for LLaVA-Video-7B [55], sharing the same video-question pair source as LLaVA-Houd-DPO [52]. To evaluate the effectiveness, we compare our UNIFIEDREWARD-based DPO with Houd-DPO and the latest TPO [24]. The results, presented in Tab. 6, demonstrate the superiority of our approach. Notably, our method significantly outperforms the baselines on MSRVTT, MSVD, and 7 Figure 4. Video Generation Qualitative Comparison. We compare the performance of T2V-Turbo, DPO based on VideoDPO, and DPO based on our method. strates the effectiveness of our approach in refining preference data for improved model alignment. The qualitative comparison results are shown in Fig. 5. For Video Generation, we compare our method with VideoDPO [29], using the same prompt source for preference data construction. The results in Tab. 8 demonstrate our superiority in enhancing both generation quality and semantic consistency, highlighting the effectiveness of our approach. The qualitative comparison results are shown in Fig. 4. Details of baselines and their evaluation are provided in Appendix and more qualitative comparison results are present in Appendix D. 5. Conclusion This paper proposes UNIFIEDREWARD, the first unified reward model for multimodal understanding and generation assessment, capable of handling both pair ranking and point scoring, which can be utilized for vision model preference alignment. Specifically, we first fine-tune pre-trained VLM on our constructed large-scale, comprehensive dataset that spans wide range of visual tasks to develop UNIFIEDREWARD. This model is then employed to automatically construct high-quality preference pair data from vision models outputs through detailed filtering process, involving pair ranking and point sifting. These data are subsequently used for model preference alignment via direct preference optimization. Our experimental results demonstrate that joint learning across diverse visual tasks yields significant mutual benefits. By applying our pipeline to both image and video understanding and generation tasks, we achieve substantial improvements in each domain. Figure 5. Image Generation Qualitative Comparison. We compare the performance of SDXL-turbo, DPO on the Pick-a-Pic dataset, and DPO based on our method. TGIF, demonstrating its effectiveness in video understanding. For the other three multi-choice question datasets, although our DPO data does not include such type, it does not lead to any negative impact. Our performance still remains comparable to the baselines, indicating the robustness and generalization ability of our approach. For Image Generation, we extract prompts from Pick-a-Pic [17], human preference image dataset, to construct preference data. As shown in Tab. 9, training on the constructed data using our UNIFIEDREWARD achieves better performance compared to directly training on the original dataset. This demon-"
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, and Jonghyun Choi. Tuning large multimodal models for videos using reinforcement learning from ai feedback. arXiv preprint arXiv:2402.03746, 2024. 2 [3] Dimitrios Christodoulou and Mads Kuhlmann-Jørgensen. Finding the subjective truth: Collecting 2 million votes for comprehensive gen-ai model evaluation, 2024. 4, 12 [4] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In ICME, pages evaluating large multi-modality models. 1119811201, 2024. 13 [5] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 6 [6] Hiroki Furuta, Heiga Zen, Dale Schuurmans, Aleksandra Faust, Yutaka Matsuo, Percy Liang, and Sherry Yang. Improving dynamic object interactions in text-to-video generation with ai feedback. arXiv preprint arXiv:2412.02617, 2024. 2 [7] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In AAAI, pages 1813518143, 2024. [8] Shuhao Han, Haotian Fan, Jiachen Fu, Liang Li, Tao Li, Junhui Cui, Yunqiu Wang, Yang Tai, Jingwei Sun, Chunle Guo, et al. Evalmuse-40k: reliable and fine-grained benchmark with comprehensive human annotations for textarXiv preprint to-image generation model evaluation. arXiv:2412.18150, 2024. 4 [9] Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024. 2, 4, 7, 12 [10] Willy Fitra Hendria. Msvd-indonesian: benchmark for multimodal video-text tasks in indonesian. arXiv preprint arXiv:2306.11341, 2023. 6 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NIPS, 30, 2017. 2 [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 5 [13] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. NIPS, 36:7872378747, 2023. 2 [14] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchIn CVPR, pages mark suite for video generative models. 2180721818, 2024. 2, [15] Raisa Islam and Owana Marzia Moushi. Gpt-4o: The cutting-edge advancement in multimodal llm. Authorea Preprints, 2024. 6 [16] Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. Genai arena: An open evaluation platform for generative models. arXiv preprint arXiv:2406.04485, 2024. 6, 12 [17] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. NIPS, 36:3665236663, 2023. 2, 6, 8, 12, 13 [18] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning textarXiv preprint to-image models using human feedback. arXiv:2302.12192, 2023. 1, 2 [19] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024. 6 [20] Bo Li, Peiyuan Zhang, Kaichen Zhang, Fanyi Pu, et al. Lmms-eval: Accelerating the development of large multimoal models, 2024. [21] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 4, 6, 12 [22] Jiachen Li, Qian Long, Jian Zheng, Xiaofeng Gao, Robinson Piramuthu, Wenhu Chen, and William Yang Wang. T2vturbo-v2: Enhancing video generation model post-training through data, reward, and conditional guidance design. arXiv preprint arXiv:2410.05677, 2024. 2, 6, 7, 13 [23] Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, et al. Vlrewardbench: challenging benchmark for vision-language generative reward models. arXiv preprint arXiv:2411.17451, 2024. 6, 12 [24] Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, and Serena Yeung-Levy. Temporal preference optimization for longform video understanding. arXiv preprint arXiv:2501.13919, 2025. 1, 2, 7, 13 [25] Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, and Jiebo Luo. Tgif: new dataset and benchmark on animated gif description. In CVPR, pages 46414650, 2016. 6 [26] Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, et al. Rich human feedback for In CVPR, pages 1940119411, text-to-image generation. 2024. [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NIPS, 2023. 6 9 [28] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. 1, 2, 5, 6, 12 [29] Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omnipreference alignment for video diffusion generation. arXiv preprint arXiv:2412.14167, 2024. 1, 2, 3, 4, 7, 8, 13 [30] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In CVPR, pages 22139 22149, 2024. 2 [31] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. arXiv preprint arXiv:2406.11069, 2024. [32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 6, 7, 13 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. 2 [34] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf. arXiv:2309.14525, 2023. 7, 12 [35] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 6 [36] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 2 [37] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [38] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, pages 82288238, 2024. 1, 2, 5 [39] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2 [40] Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, and Hao Li. Lift: Leveraging human feedarXiv preprint back for text-to-video model alignment. arXiv:2412.04814, 2024. 1, 2, 3, 4, 5, 7, 12 [41] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 2024. 6 [42] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved NIPS, 37:2882828857, video-language understanding. 2025. 6 [43] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 2, 6, [44] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. LlavaarXiv critic: Learning to evaluate multimodal models. preprint arXiv:2410.02712, 2024. 1, 2, 3, 4, 5, 6, 12 [45] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In CVPR, pages 52885296, 2016. 6 [46] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. NIPS, 36:1590315935, 2023. 2, 6, 12 [47] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. 1, 2, 6, 7, 12 [48] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 6 [49] Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning of diffusion models for text-to-image generation. arXiv preprint arXiv:2402.10210, 2024. 2 [50] Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, et al. simple yet effective multi-modal reward model. arXiv preprint arXiv:2501.12368, 2025. 1, 2 [51] Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, and Kai Han. Onlinevpo: Align video diffusion model with online video-centric preference optimization. arXiv preprint arXiv:2412.15159, 2024. 2 [52] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024. 1, 2, 7, 13 [53] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander 10 Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024. 4, 6, 7, 12, [54] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning multidimensional human preference for text-to-image generation. In CVPR, pages 80188027, 2024. 2 [55] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 6, 7, 13 [56] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 6 11 A. Details of Reward Model Evaluation A.1. Reward Model Baselines PickScore [17] is an image generation assessment model trained over Pick-a-Pic by combining CLIP-style model with variant of InstructGPTs reward model This work employs its checkpoint yuobjective. valkirstain/PickScore v1 as one of the image generation reward model baselines. HPSv2 [43] is an image generation scoring model based on CLIP, fine-tuned on the HPD v2 [3] dataset. It is capable of predicting human preferences for generated images. We utilize its official code and checkpoint for evaluation. ImageReward [46] is text-to-image human preference reward model designed to effectively encode human preferences. It is trained based on systematic annotation pipeline that includes both rating and ranking, collecting 137k expert comparisons. We utilize its official code and checkpoint for evaluation. LLaVA-Critic [44] is designed to assess image understanding performance based on the LLM, enabling pair ranking and point scoring. It is trained on high-quality critic instruction-following dataset that incorporates diverse evaluation criteria and scenarios. In this work, we employ the lmms-lab/llava-critic-7b model as our baseline for image understanding assessment. VideoScore [9] is video quality assessment model, trained on the VideoFeedback dataset, which contains humanprovided multi-aspect scores for 37.6K synthesized videos generated by 11 existing video generative models. We utilize its official code and checkpoint for video quality assessment evaluation. LiFT [40] is the first fine-tuning method that leverages huIt constructs man feedback for T2V model alignment. Human Rating Annotation dataset, LiFT-HRA, consisting of approximately 20k human annotations, each including score and its corresponding reason. Based on this dataset, reward model, LiFT-Critic, is trained to learn human feedback-based reward function. In this work, we utilize the released code and checkpoint of LiFT-Critic for video generation quality assessment. VisionReward [47] is fine-grained, multi-dimensional reward model designed to capture human preferences in images and videos. It constructs separate human preference datasets for images and videos, and trains corresponding reward models for each. In our work, we utilize its image and video reward models for evaluating image and video generation assessment, respectively. VideoReward [28] is multi-dimensional video reward model trained on newly proposed 182k-sized humanlabeled video generation preference dataset, sourced from 12 video generation models. We utilize its official code and checkpoint for evaluation. Our UnifiedReward is based on LLaVA-OneVision-7B (OV-7B) [21] and trained on our constructed large-scale, comprehensive human feedback dataset, which spans wide range of visual tasks. Through joint multi-task learning and evaluation, our experimental results demonstrate that this approach fosters mutually reinforcing effect across tasks. To the best of our knowledge, this is the first unified reward model for multimodal understanding and generation assessment. A.2. Evaluation Benchmarks A.2.1. Multimodal Understanding VLRewardBench [23] is comprehensive benchmark for assessing image understanding, covering general multimodal queries, visual hallucination detection, and complex reasoning tasks. It consists of 1,250 high-quality examples meticulously designed to evaluate model limitations and challenge their capabilities. During evaluation, we randomly shuffle the order of responses to ensure more robust and reliable assessment results. ShareGPTVideo [53] is an open-source, large-scale training dataset comprising 900k captions that cover diverse range of video content, including temporal dynamics, world knowledge, object attributes, and spatial relationships. It also includes 17k preference data specifically curated for DPO training. In this work, we utilize 16k preference data for reward model training and 1k for video understanding evaluation. A.2.2. Multimodal Generation GenAI-Bench [16] is reward benchmark for multimodal generative models, designed to assess the ability of MLLMs to evaluate AI-generated content by comparing their judgments with human preferences. It includes benchmarks for image generation, image editing, and video generation. In this work, we utilize the image and video generation parts for generation reward evaluation. VideoGen-RewardBench [28] builds upon VideoGen-Eval to establish fair benchmark for assessing the performance of reward models on modern T2V models. It comprises 26.5k manually constructed video pairs, with annotators evaluating each pair based on Visual Quality, Motion Quality, Text Alignment, and Overall Quality. In this work, we utilize the Overall Quality metric for baseline reward comparison. We will release all evaluation codes to facilitate community reproduction. B. Details of DPO Evaluation B.1. DPO Baselines LLaVA-Critic [44] leverages image-question pairs from LLaVA-RLHF [34] to construct preference data for OV-7B 12 In this work, for DPO which is trained for 3 epochs. fair comparison, we also use the image-question pairs from LLaVA-RLHF to construct preference data while keeping all other settings the same. LLaVA-Houd-DPO [52] utilizes the 17k preference data from the ShareGPTVideo [53] dataset for DPO training. In this work, to ensure fair comparison, we apply the same dataset for DPO training on LLaVA-Video [55] following its method as the baseline. For our approach, we randomly sample 14k data points from the 17k dataset to construct the DPO training data and then perform DPO on LLaVAVideo. All training parameters and settings are kept identical to maintain fairness in evaluation. LLaVA-TPO [24] adopts self-training approach that enables models to distinguish between well-grounded and less accurate temporal responses by leveraging curated preference datasets at two granularities: localized temporal grounding and comprehensive temporal grounding. In this work, since its training dataset has not been open-sourced, we utilize its released checkpoint for comparison. VideoDPO [29] is the first video generation DPO method built upon its comprehensive preference scoring system, OmniScore, which evaluates both the visual quality and semantic alignment of generated videos. In this work, we use its released preference dataset for T2V-Turbo [22] DPO as baseline. For our method, we extract video-caption pairs from its dataset to construct our own preference data for DPO, ensuring fair evaluation. Pick-a-Pic [17] is large, open dataset of text-to-image prompts paired with real user preferences over generated images. After excluding approximately 12% of tied pairs, the dataset contains around 851k preference pairs with 58.9k unique prompts. In this work, we directly use this dataset for SDXL-Turbo [32] DPO as baseline. For our method, we randomly sample 14k captions from this dataset to construct preference data for DPO, ensuring fair evaluation. B.2. Evaluation Details For image understanding, we use LMMs-Eval [20] toolkit to evaluate LLaVABench, WildVision, LLaVABenchWilder, LiveBench, and MMHal. For video understanding, we employ gpt-3.5-turbo-1106 for MSRVTT, MSVD, and TGIF evaluation (evaluation prompt is provided in Fig. 6), while using the VLMEvalKit [4] toolkit for evaluating LongVideoBench, MLVU, and Video-MME. C. Prompting Template Figure 6. GPT Evaluation Prompt. We use gpt-3.5-turbo1106 for video understanding evaluation on MSRVTT, MSVD, and TGIF benchmarks. D. More Qualitative Comparison We provide more qualitative comparison results in Fig. 7 and 8. E. Limitations and Future Works While our work presents unified reward model for multimodal understanding and generation assessment with strong empirical results, there are still several areas for improvement. First, although our large-scale dataset spans wide range of visual tasks, the distribution of pairwise ranking and pointwise scoring data across different tasks is somewhat imbalanced. While our final model demonstrates strong performance, we believe that more balanced dataset could lead to even greater stability and consistency. In future work, we plan to continue expanding and refining our dataset to improve this aspect. Second, we currently adopt LLaVA-OneVision-7B model as the baseline for our reward model. While this serves as strong foundation, we anticipate that using larger model, such as 72B version, could enhance generalization across diverse multimodal tasks. Scaling up the model will be an important direction for future exploration. Lastly, while DPO has proven to be highly effective, our current implementation follows an offline paradigm, which may introduce potential issues such as overfitting to specific training data distributions. To address this, we plan to explore other more effective methods such as online DPO, which could further improve the adaptability and robustness of reward learning. We provide our training prompting templates in Figs. 9, 10, and 11. For image understanding assessment, we adopt the same template used in LLaVA-Critic to ensure consistency and comparability. Despite these limitations, our work represents an important step toward unified reward model for multimodal tasks, and we remain committed to continuously improving its capabilities through ongoing research and development. 13 F. Societal Impacts Our unified reward model for multimodal understanding and generation assessment has the potential to significantly enhance AI applications across various domains. By aligning AI-generated content more closely with human preferences, our work can improve the quality and reliability of vision models, benefiting industries such as digital media, entertainment, education, and accessibility. For example, one of the key advantages of our approach is its ability to provide more consistent and interpretable evaluation of generative models. This can lead to better AI-assisted creativity, enabling artists, designers, and content creators to generate higher-quality visuals with greater control. While our work brings many benefits, we recognize that reward models, like any AI-driven system, must be carefully designed to ensure fairness and robustness. There is always risk that biases in the training data could influence model predictions. However, we have taken measures to curate diverse dataset and will continue refining our approach to mitigate such concerns. Overall, we believe our work contributes positively to the AI field by providing more effective and scalable way to align vision models with human preferences. We encourage future research and collaborations to further enhance the fairness, adaptability, and realworld applicability of reward-based AI evaluation. G. Ethical Statement In this work, we affirm our commitment to ethical research practices and responsible innovation. To the best of our knowledge, this study does not involve any data, methodologies, or applications that raise ethical concerns. All experiments and analyses were conducted in compliance with established ethical guidelines, ensuring the integrity and transparency of our research process. 14 Figure 7. More Image Generation Qualitative Comparison. We compare the performance of SDXL-turbo, DPO on the Pick-a-Pic dataset, and DPO based on our method. 15 Figure 8. More Video Generation Qualitative Comparison. We compare the performance of T2V-Turbo, DPO based on VideoDPO, and DPO based on our method. 16 Figure 9. Image Generation Prompt. The prompting template used for our reward model training on image generation assessment. 17 Figure 10. Video Generation Prompt. The prompting template used for our reward model training on video generation assessment. 18 Figure 11. Video Understanding Prompt. The prompting template used for our reward model training on video understanding assessment."
        }
    ],
    "affiliations": []
}