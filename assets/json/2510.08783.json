{
    "paper_title": "MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces",
    "authors": [
        "Reuben A. Luera",
        "Ryan Rossi",
        "Franck Dernoncourt",
        "Samyadeep Basu",
        "Sungchul Kim",
        "Subhojyoti Mukherjee",
        "Puneet Mathur",
        "Ruiyi Zhang",
        "Jihyung Kil",
        "Nedim Lipka",
        "Seunghyun Yoon",
        "Jiuxiang Gu",
        "Zichao Wang",
        "Cindy Xiong Bearfield",
        "Branislav Kveton"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In an ideal design pipeline, user interface (UI) design is intertwined with user research to validate decisions, yet studies are often resource-constrained during early exploration. Recent advances in multimodal large language models (MLLMs) offer a promising opportunity to act as early evaluators, helping designers narrow options before formal testing. Unlike prior work that emphasizes user behavior in narrow domains such as e-commerce with metrics like clicks or conversions, we focus on subjective user evaluations across varied interfaces. We investigate whether MLLMs can mimic human preferences when evaluating individual UIs and comparing them. Using data from a crowdsourcing platform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and examine alignment with human judgments on multiple UI factors. Our results show that MLLMs approximate human preferences on some dimensions but diverge on others, underscoring both their potential and limitations in supplementing early UX research."
        },
        {
            "title": "Start",
            "content": "MLLM as UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces Ryan Rossi Adobe Research Reuben Luera University of California, Berkeley"
        },
        {
            "title": "Franck Dernoncourt\nAdobe Research",
            "content": "5 2 0 2 9 ] . [ 1 3 8 7 8 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Branislav Kveton\nAdobe Research",
            "content": "ABSTRACT In an ideal design pipeline, user interface (UI) design is intertwined with user research to validate decisions, yet studies are often resource-constrained during early exploration. Recent advances in multimodal large language models (MLLMs) offer promising opportunity to act as early evaluators, helping designers narrow options before formal testing. Unlike prior work that emphasizes user behavior in narrow domains such as e-commerce with metrics like clicks or conversions, we focus on subjective user evaluations across varied interfaces. We investigate whether MLLMs can mimic human preferences when evaluating individual UIs and comparing them. Using data from crowdsourcing platform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and examine alignment with human judgments on multiple UI factors. Our results show that MLLMs approximate human preferences on some dimensions but diverge on others, underscoring both their potential and limitations in supplementing early UX research. KEYWORDS MLLM-as-a-Judge, UI, Evaluation, Benchmark ACM Reference Format: Reuben Luera, Ryan Rossi, Franck Dernoncourt, Samyadeep Basu, Sungchul Kim, Subhojyoti Mukherjee, Puneet Mathur, Ruiyi Zhang, Jihyung Kil, Nedim Lipka, Seunghyun Yoon, Jiuxiang Gu, Zichao Wang, Cindy Xiong Bearfield, and Branislav Kveton. 2025. MLLM as UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces. In . ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. 2025, , San Jose, CA, USA 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "1 INTRODUCTION\nDesign choices are inseparable from usability and experience.\nNowhere is this more evident than in interface design, which\nshapes user perception of product usability and trustworthi-\nness [42]. It can even profoundly impact user productivity, and\ntheir emotional responses to products [3, 6]. For this reason, user\nexperience research to evaluate interface design is crucial for\ndevelopment [4, 47]. Through research-informed iterations upon\ndesign choices, products can be refined to be more usable, engaging,\nand trustworthy.",
            "content": "However, many organizations face persistent bottleneck, where too few researchers are available to keep pace with the growing scale of design and evaluation needs. In fact, according to report from the Nielsen Norman Group, there is only one researcher for every five designers [22]. As result, many organizations are forced to allocate their limited research capacity selectively, testing only small subset of ideas rather than engaging in rapid, iterative evaluation, which has long been shown to significantly enhance design quality [34]. Existing work in this space simulates human behavior in domains like e-commerce and solely focuses on user metrics like clickthrough and conversion rates [53], which can be limited because such metrics capture only general user outcomes and fail to provide diagnostic insights into design quality, usability, or user experience. To address this gap, artificial intelligence offers promising avenue for building more efficient and scalable evaluation pipelines. Several existing works have examined how large language models (LLMs) can be used to evaluate UIs, such as generating feedback based on critiques and quality ratings from professional designers [14, 15]. While LLMs can be trained to generate feedback on UIs, prior work shows that they often struggle to faithfully capture human responses to visual interfaces [49, 54]. We build on this work by systematically comparing LLM evaluations with human-generated UI evaluation data, in order to identify both reliable alignments and critical misalignments that highlight future research opportunities. Through this comparison, we identify the LLM prompting approach that most effectively approximates human judgments. 2025, , San Jose, CA, USA Reuben Luera, et al. Further, the analysis presented in this paper, specifically how state-of-the-art models perform moderately well in UI judging tasks, can lay the groundwork for using multimodal large language models (MLLMs) to supplement early-stage user research. The results produced can be summarized in the following way: 1) Task 1, the Likert-scale task produced human and MLLM evaluation scores that, when compared to one another, show promise for MLLMs as UI judge approximators. The models we use (Claude 3.5 Sonnet, GPT-4o, and Llama-3.2-11B-Vision-Instruct) all have an accuracy 1 score greater than 75%. Further, we evaluated performance using Pearson, Spearman, and Kendall tau correlation statistics that signify moderately strong correlation between MLLM and human scores. 2) Task 2, the pairwise comparison task, reflected that MLLMs, when given questions that have large human score differences, can accurately simulate human data. In these pairwise experiments, we saw largely that models performance improved as the degree of difference between two screens increased. Given this, MLLMs are not at stage where they should be seen as replacements for real-life human evaluation, but instead can be used to supplement early UI evaluations, especially in scenarios where UIs would remain untested. Summary of Main Contributions. The key contributions of this work are as follows: Novel MLLM as judge evaluation tasks: We introduce new task that can be used to evaluate whether large language multimodal models can approximate human preferences and provide reasonable justifications of those preferences in Likert scale evaluations of UIs and pairwise comparisons of real-world interfacesmoving beyond behavioral simulation to subjective UI alignment. Absolute Human/MLLM UI judgments benchmark1: Task 1 (Section 4.1) produced benchmark dataset of user interface evaluations, derived from both MLLM and human evaluators. This dataset was created by showing humans and three state-of-the-art MLLMs (Claude 3.5 Sonnet, GPT-4o, and Llama-3.2-11B-Vision-Instruct) the same set of UIs and then asking them to use list of cognitive, perceptual, and emotional factors to evaluate them on scale from 1-7. Pairwise Human/MLLM UI judgments benchmark1: Task 2 (Section 4.2) produced benchmark of pairwise MLLM and human evaluations of user interfaces. To create this data, we showed the same three state-of-the-art MLLMs two UIs at time and prompted them to choose their preferred screen based on the same list of factors. The MLLM data was then compared to human pairwise data (generation techniques shown in Section 4.2) to calculate another dataset of agreement scores. MLLM training data: The data produced in this study can be used as the benchmark to create fine-tuned model. Training MLLMs and LLMs with novel datasets, such as the one created in this study, has proven to increase human accuracy in MLLM as judge tasks [23, 27, 62]. MLLM as UI judge use cases: Data analysis that suggests that MLLMs are capable tool to supplement user interface 1See supplementary materials for benchmark data testing for both absolute score testing and pairwise comparison to gauge perception at early stages. Our data suggest that teams with limited resources can use MLLMs to supplement, rather than fully replace, human UI testing. The novelty of this research lies in the exploration of whether or not multimodal LLMs can evaluate visual user interfaces, not just text or other behavioral logs. Past LLM as judge work has largely centered on comparing human and LLM textual evaluations [12]; we instead focus our work on visual UI evaluation. Through this study, we evaluate whether or not MLLMs are capable of emulating human UI judgments and report when they do so well and when they fail. In doing so, we explore to what degree MLLMs can be used as early-stage, low-cost approximators that act as supplement, not as replacement, to existing user research."
        },
        {
            "title": "2 RELATED WORK\n2.1 LLM-as-a-Judge\nExtensive work has been done in the field of ’LLM as a Judge’,\nand within this paper, we aim to build upon the existing literature.\nLLM as a judge is a concept that can be applied to a myriad of\ndifferent domains [16] and consists of using LLMs to evaluate the\noutcomes of complex tasks. They can do so effectively because they\ncan quickly and efficiently process extensive datasets that can then\nbe used to make an informed decision [1]. LLMs have been used\nas judges to evaluate everything from other models [28, 55, 60, 61],\nnatural language and information [37, 41, 56], charts [24], and\nreasoning and thinking [1, 17, 50]. Despite this, related literature\nsuggests that using LLMs as judges can offer cost-cutting and a\nflexible alternative to human evaluation, but warns that they often\nlack consistency while suffering from issues pertaining to bias and\nreliability [16].",
            "content": "Across all studied domains, larger state-of-the art models (e.g., GPT-4o, Claude, etc.) perform better in LLM as judge tasks than smaller models [51, 61]. Nonetheless, even well-performing models deviate by up to five points on 10-point scale, and LLM evaluators are often excessively verbose, biased towards the existing positions, and prefer their own outputs [30, 39]. Given these limitations, LLMs often are capable as approximators, but are far from being human replacements in evaluation tasks. We aim to distinguish our work by using method that specifically explores how MLLMs can be used to evaluate two distinct UIs. In the context of this paper, we use pairwise MLLM evaluation method [30], which consists of having the MLLM compare two different options and selecting which is better aligned with predetermined criterion [16, 40]. This method closely aligns with A/B testing, commonly used in user research to compare two interfaces. We aim to run pairwise test with human users looking at UIs and Likert-scale LLM evaluation with the same interfaces to explore how close LLMs are to simulating human results."
        },
        {
            "title": "2.2 MLLMs in User Research and Design\nAs in many other domains, AI is becoming increasingly intertwined\nwith user interface design and research [5, 32]. Designers and re-\nsearchers alike are using MLLM to create and explore different\ninterfaces for their respective products. Specifically in design re-\nsearch, LLMs are being used as assistants that are able to collaborate",
            "content": "MLLM as UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces 2025, , San Jose, CA, USA Figure 1: User interfaces split by domains: An overview of sample of the UIs evaluated by humans and MLLMs can be divided into three domains: landing pages, digital receipts, and catalogs. Here we present low-fidelity versions of the screens that we presented to the users. The users saw high-fidelity, branded, and in-product versions of these screens that have been anonymized. during UX evaluation sessions [25]. LLMs are occasionally being used to create synthetic data that emulates human users [10, 31, 43]. For instance, Jansen et al. [21] explain how LLMs can use simulated respondents to supplement the creation and analysis of survey research. Further, Hämäläinen et al. [18] demonstrates that LLMs are capable of simulating user survey responses and creating responses that humans found plausible. Similarly, Li et al. [29] found that in larger-scale market research surveys, LLM-generated survey responses achieved about 75%-85% agreement with results from actual humans. However, this paper also highlights the importance of these models being trained on human data, as they cannot yet capture all of the nuances of human users. There have been some work focused on some aspect of UI evaluations [14, 15, 44, 58]. In Duan et al. [15], Figma plugin was 2025, , San Jose, CA, USA Reuben Luera, et al. created by assessing GPT-4s alignment with set of design heuristics. Similarly, Duan et al. [14] utilized dataset of design critiques from several experienced designers to improve LLM-generated UI feedback by 55%. Meanwhile Wu et al. [58] creates design tool based on several datasets and shows it performs closely to human ground truth. In addition, Xiang et al. [59] created an LLM tool used to evaluate smartwatch interfaces using simulated users. Our work differs from these works as we create benchmark that compares several different state-of-the-art MLLMs to each other and to the human judgments. Further, the benchmark reflects how MLLMs perform in both pairwise and absolute scoring tasks based on list of nine UI factors. Agent A/B [53] asks similar question and explores whether or not its own model can complete A/B UX tests. They train their agent specifically on UX click-through rate, focusing solely on the filters and menus of e-commerce websites. We aim to focus on broader question, exploring UI judgments at higher level while testing an assortment of widely used MLLMs (e.g., GPT-4o, Claude, Gemini) on an assortment of UI screens from different domains."
        },
        {
            "title": "3.2.1 Cognitive Factors:",
            "content": "2https://reallygoodemails.com/ Table 1: UI factors and corresponding statements. Category Factor Question Cognitive Ease of Use Clarity Visual Hierarchy The UI looks easy to use. The layout is uncluttered. The UI has clear visual hierarchy. Memorable Perceptual Trust Intuitive The UI is easily remembered. The UI appears trustworthy. The UI is intuitive. Emotional Aesthetic Pleasure The UI is aesthetically pleasing. Interest Comfort The UI is interesting. feel comfortable with the UI. Ease of use refers to how easy an interface appears to be. Ease of use can depend on factors like UI element placement, layout of design, and design heuristics like the match between the system and the real world [26, 36]. screen with high ease of use often does not make users think; its function is straightforward and obvious. Clarity pertains how clear and uncluttered an interface looks. For example, if an interface has too many UI elements, it can overwhelm the user by increasing their cognitive load [11, 38]. For this reason, [36] highlights the importance of ensuring that designs remain minimalist in nature with enough white space. Doing so ensures the creation of screen that is uncluttered and does not overwhelm the users. Visual Hierarchy consists of ensuring that there is clear order of importance between all of the UI elements on screen [48, 52]. Doing so decreases the users cognitive load, as their attention is caught quickly by whatever is largest, most central, and most important. An interface with good visual hierarchy will have elements that are sized and placed based on their perceived importance and relevance to the user."
        },
        {
            "title": "3.2.2 Perceptual Factors:",
            "content": "Memorable pertains to how easily UI can be remembered and speaks to the impact that design has on user. Memorability can be both negative and positive, and can be result of attributes like color or recognizable elements [9]. Further, does the UI have certain uniqueness that makes it stand out for the user. These elements, in total, contribute to making an interface memorable to the user. Trust pertains to the amount of confidence that the user interface elicits within the user. Headlines, text content, as well as even how well an interface is put together, strongly contribute to trust in interface design. trustworthy interface will make users feel confident in the content, often because they are designed intentionally and carefully [63]. Intuitive refers to how functional an interface looks to user. Intuitiveness and functionality often are intertwined with user experience, so intuitiveness is crucial factor when judging interfaces. An intuitive screen would be defined as one that users can look at and immediately understand the natural purpose and function [2, 7, 33] MLLM as UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces 2025, , San Jose, CA, USA"
        },
        {
            "title": "3.2.3 Emotional Factors:",
            "content": "Aesthetic Pleasure refers to how visually attractive an interface is to specific user. It is largely subjective factor that often differs from user to user. For the most part, aesthetic pleasure is tightly tied by choices pertaining to color, font, content density, and overall structure. An aesthetically pleasing interface is able to integrate these elements well to create larger design [8, 19]. Interest in terms of UIs, pertains to how eye-catching, inviting, and captivating an interface is. Interesting interfaces are capable of catching users attention and keeping it. An interesting interface may have UI elements such as unique fonts, mood-inducing colors, or sensational text headlines [46]. Comfort pertains to the overall satisfaction and degree to which an interface makes user feel at ease while in use [13]. Designs and interfaces can increase comfort by maintaining clean, straightforward, and low cognitive load design. comfortable design may be more tonally consistent and use common fonts and colors."
        },
        {
            "title": "3.3 Participants and Procedure\nIn total, we collected 15,000 responses from 500 participants using\nthe online crowdsourcing platform Amazon’s Mechanical Turk. To\nensure the quality of the dataset, we filtered for participants who\nhad a greater than 98% approval rating and at least 500 responses\nto take part in our study. In total, we received feedback on 30 UIs\nand we asked for 500 unique responses from each. From there, we\nasked participants to evaluate each UI based on the nine UI Factors\n(Table 1). Each evaluation was expected to take about 45 seconds\nto a minute to complete. In total, it cost $75 to get 500 human\nevaluations for each UI. Before taxes and fees, the total cost of this\npart of the study was $2,250.",
            "content": "Each UI was presented as standalone task to minimize order effects, and participants evaluated each UI in two stages. First, participants responded to nine Likert scale questions, one corresponding to each of the UI Factors (see Table 1). Each question asked participants to indicate their level of agreement on 7-point scale (1 = strongly disagree, 7 = strongly agree). To support their evaluation, participants were able to resize the UI during this stage, allowing them to view it while responding to the questions. Next, participants were asked to provide textual explanation describing the rationale behind their ratings. This qualitative input allowed for deeper insight into participants reactions to each UI."
        },
        {
            "title": "3.4 Result of the Human Data Collection\nWe compiled a dataset containing each worker’s ID, the correspond-\ning image ID, the UI’s domain, and the participant’s scores for each\nof the nine evaluation factors. To ensure consistency across the\nbenchmark, factor and domain names were standardized to align\nwith the terminology used throughout the paper. We applied sev-\neral data pre-processing steps to ensure data quality. We excluded\nresponses that were completed in less than 45 seconds, based on\nthe rationale that a thoughtful engagement with the task requires\nat least that amount of time. Additionally, we removed responses\nthat were incomplete or that contained duplicate scores submitted",
            "content": "by the same user. Further, we removed data for screens that were not professionally made to maintain consistent data. After the exclusion procedure, we are left with 9,296 human responses. Table 2 presents the mean scores and standard deviations for each of the nine UI factors, as rated by human participants, split by domain. The rightmost column shows the overall average score for each factor across all domains, while the bottom row reports the average score for each domain across all factors. We then compared these human-generated ratings to scores produced by multimodal large language models (MLLMs) to evaluate alignment and divergence in trust-related perceptions."
        },
        {
            "title": "4.1 Task 1: Absolute Score Prediction\nWe conduct three studies evaluating how closely MLLMs’ responses\nto UIs resemble those of humans. In Section 4.1.1, we first an-\nalyze MLLM and human scores separately. Results suggest that\nMLLMs are mostly well aligned to humans, with a few factors\nwhere they may significantly under or over predict human scores.\nIn Section 4.1.2, we analyze prediction errors of MLLMs using four\nmetrics: MSE, MAE, accuracy, and ±1-accuracy. Results suggest\nthat MLLMs well-predict human responses, with Claude being the\nbest judge overall. MLLMs on average overestimated factors like\nEase of Use. While consistently underestimating factors like Aes-\nthetic Pleasure, Interesting, and Comfort. Meanwhile, models\nshowed they could reasonably predict scores (within 0.50 Likert\npoints) for factors such as Clarity, Visual Hierarchy, Memo-\nrable, and Trustworthy, Intuitive. Finally in Section 4.1.3, we\nanalyze how well MLLM scores predict the ranking of UIs by hu-\nmans. The factor scan be well predicted, and the best judge overall\nis Llama. To reduce the randomness in our analysis due to ran-\ndom MLLM scores, we evaluate each UI and factor by each MLLM\n10 times and then take the median score. Our MLLM prompt is\nreported in Figure 7 in the Appendix.",
            "content": "Individual Scores. We start by comparing individual human 4.1.1 and MLLM scores. For each UI and factor, we report averaged human scores and median MLLM scores. These averages represent an average perception of given UI in given factor. We also included average scores from humans and MLLMs aggregated by the three domains and all UIs, see Table 2. We make several observations. First, MLLM judges align reasonably well with humans. As an example, note the average human and 2025, , San Jose, CA, USA Reuben Luera, et al. Figure 2: Example survey question shown to users. Consists of instructions on top, the UI on the left, and the nine factor, Likert scale questions on the right. MLLM scores over all factors and UIs in Table 2 (last four rows and last column). The human score is 6.10, whereas Claude predicts 6.08, GPT-4o predicts 5.75, and Llama predicts 5.98. Therefore, all MLLM scores are within 0.35 of the human scores. Since this is significantly less than the range of our 7-point Likert scale, we conclude that MLLMs reasonably mimic human scores. Second, MLLM scores are often closer to each other than those of humans, as indicated by smaller standard deviations of the scores in Table 2. We note this specifically for Claude and Llama. GPT-4o has sometimes larger standard deviation and sometimes smaller. When considering individual factors in Table 2, MLLMs tend to perform well on cognitive and perceptual factors. For instance, the human score for Trustworthy is 6.34, whereas Claude predicts 6.42, GPT-4o predicts 6.53, and Llama predicts 6.37. This is the only factor where all MLLM scores are within 0.25 of the human score. MLLMs also perform well on Visual Hierarchy, where all MLLM scores are within 0.5 of the human scores. The human score is 6.23, while Claude predicts 6.37, GPT-4o predicts 6.23, and Llama predicts 6.70. Overall, MLLMs likely approximate these scores effectively because the factors have concrete cues in the UI. These factors are correlated with layout, reassuring designs and phrases, and spacing; all of which MLLMs can gather from the UI screenshots. MLLMs struggle with some emotional factors, such as Interesting. Specifically, all MLLM scores of this factor underestimate human scores. The human score is 5.82, whereas Claude predicts 5.10, GPT-4o predicts 4.73, and Llama predicts 4.50. The MLLMs likely struggle with this and other factors like Ease of Use because they are more difficult to quantify for MLLMs. Factors like ease of use are highly tied users technical literacy and cannot be inferred from visual cues alone [20, 35]. Score Prediction. Next we analyze how well MLLMs predict 4.1.2 human scores. We consider four error metrics: mean squared error (MSE), mean absolute error (MAE), accuracy, and 1-accuracy. All metrics are computed on average human and median MLLM scores, for each UI and factor, as in Section 4.1.1. The MSE is the mean of squared differences between MLLM and human scores, over all UIs and for given factor. The MAE is the mean of absolute differences between MLLM and human scores, over all UIs and for given factor. The accuracy is the fraction of MLLM scores that matches rounded human scores, over all UIs and for given factor. The 1-accuracy is the fraction of MLLM scores that matches rounded human scores within 1, over all UIs and for given factor. The four metrics provide different views on our results. The MSE and MAE measure mean deviations in the MLLM and human scores, with the former penalizing larger deviations more. The accuracy measures the probability that the MLLM score matches the human score. The 1-accuracy measures the probability of sufficiently close match. All metrics are reported in Table 3. MSE and MAE of some factors are low, which indicates predictability. For instance, all MSEs of Comfort are lower than 0.2 and the lowest of all factors, and all its MAEs are lower than 0.3 and the lowest of all factors. On the other hand, MSE and MAE of some factors are high, which indicates unpredictability. For instance, both MSEs and MAEs of Interesting are higher than those of the other factors. Most other scores fall somewhere in the middle, with factors like Aesthetic Pleasure having lower errors and factors like Ease of Use having higher errors. The average accuracy of MLLMs over all factors ranges from 35% to 38%, with Claude having the highest accuracy of 38%. The average 1-accuracy is significantly higher, from 72% to 77%. Again, the highest value is achieved by Claude. While the accuracy can vary lot between different factors, we note that the 1-accuracy is relatively stable. In terms of individual factors, Claude is either the best MLLM or performs similarly to it. The model is followed by Llama while GPT-4o performs slightly worse."
        },
        {
            "title": "4.1.3 Ranking Prediction. Next, we analyze how well MLLMs pre-\ndict the ranking of UIs by humans. All metrics are computed on\naverage human and median MLLM scores, for each UI and factor, as\nin Section 4.1.1. The average scores are calculated by aggregating\nall evaluator scores by factor and UI, and taking the mean of those\nvalues. For ranking scores, we consider three most popular metrics\nof correlation: Pearson correlation coefficient, Spearman’s 𝜌, and\nKendall’s 𝜏. The Pearson correlation coefficient is the covariance of\nMLLM and human scores divided by their standard deviations, over",
            "content": "MLLM as UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces 2025, , San Jose, CA, USA Table 2: Individual human and MLLM scores for all factors and domains. We report the average score over all UIs in the last column and over all factors in the last row. The closest MLLM score to the human is in bold. When the score is closer than 0.25 (0.5), its background is green (light green). The gray text is the standard deviation of the scores from which the average is computed. Domains Factor Evaluator Browsing & Disc. Conf. & Feedback Comm. & Engage. All UIs Ease of Use Clarity Visual Hierarchy Human 6.20 (1.08) 6.21 (1.10) 6.21 (1.07) 6.21 (1.09) Claude-3.5 GPT-4o Llama-3.2 7.00 (0.00) 6.70 (0.67) 6.80 (0.42) 6.90 (0.32) 6.50 (0.97) 6.50 (0.53) 6.78 (0.67) 6.89 (0.42) 6.20 (1.32) 6.47 (1.01) 6.57 (0.57) 6.40 (0.70) Human 6.08 (1.20) 6.14 (1.21) 6.21 (1.16) 6.14 (1.19) Claude-3.5 GPT-4o Llama-3.2 6.00 (0.50) 5.60 (1.71) 6.50 (0.85) 5.90 (0.57) 6.10 (1.73) 6.50 (0.71) 6.11 (0.78) 6.00 (0.61) 5.73 (1.76) 5.50 (1.96) 6.53 (0.73) 6.60 (0.70) Human 6.19 (1.08) 6.25 (1.06) 6.26 (1.04) 6.23 (1.06) Claude-3.5 GPT-4o Llama-3.2 6.56 (0.53) 6.10 (1.29) 6.80 (0.42) 6.10 (0.57) 6.30 (1.34) 6.50 (0.53) 6.44 (0.53) 6.37 (0.56) 6.30 (1.34) 6.23 (1.28) 6.70 (0.47) 6.80 (0.42) Human 5.79 (0.97) 5.79 (0.97) 5.86 (0.91) 5.81 (0.95) Memorable Claude-3.5 GPT-4o Llama-3.2 5.44 (0.73) 5.70 (0.67) 5.30 (0.48) 5.10 (0.57) 5.70 (0.67) 5.20 (0.42) 6.00 (0.87) 5.51 (0.79) 5.80 (0.42) 5.73 (0.58) 5.43 (0.68) 5.80 (0.92) Trust Human 6.31 (1.02) 6.41 (0.99) 6.29 (1.01) 6.34 (1.01) Claude-3.5 GPT-4o Llama-3.2 6.56 (0.53) 6.70 (0.67) 6.50 (0.53) 6.70 (0.48) 6.60 (0.70) 6.60 (0.70) 6.00 (0.50) 6.30 (0.67) 6.00 (0.82) 6.42 (0.57) 6.53 (0.68) 6.37 (0.72) Human 6.22 (1.07) 6.31 (1.04) 6.25 (1.03) 6.26 (1.05) Intuitive Claude-3.5 GPT-4o Llama-3.2 7.00 (0.00) 6.20 (0.92) 6.60 (0.52) 6.90 (0.32) 6.40 (0.97) 6.30 (0.48) 6.22 (0.67) 5.90 (1.20) 6.00 (0.47) 6.71 (0.53) 6.17 (1.02) 6.30 (0.53) Aesthetic Pleasure Human 5.86 (0.91) 5.85 (0.93) 6.06 (0.98) 5.92 (0.94) Claude-3.5 GPT-4o Llama-3.2 5.78 (0.44) 5.10 (1.45) 5.80 (0.63) 5.50 (1.08) 5.30 (1.57) 5.00 (1.05) 6.33 (0.71) 5.80 (1.23) 6.30 (0.82) 5.87 (0.85) 5.40 (1.40) 5.70 (0.99) Human 5.79 (0.99) 5.70 (1.08) 5.96 (0.97) 5.82 (1.02) Interesting Claude-3.5 GPT-4o Llama-3. 5.22 (0.44) 4.50 (0.71) 4.60 (0.70) 4.40 (0.70) 4.10 (0.57) 3.90 (0.99) 5.67 (0.50) 5.10 (0.77) 4.73 (0.94) 5.60 (0.84) 4.50 (0.97) 5.00 (0.94) Comfort All Human 6.23 (1.03) 6.30 (1.03) 6.28 (1.00) 6.27 (1.02) Claude-3.5 GPT-4o Llama-3.2 6.11 (0.33) 5.70 (0.67) 6.30 (0.67) 6.40 (0.70) 5.80 (1.14) 5.90 (0.57) 6.00 (0.00) 6.17 (0.48) 5.73 (0.91) 5.70 (0.95) 6.07 (0.58) 6.00 (0.47) Human 6.06 (1.06) 6.10 (1.08) 6.13 (1.03) 6.10 (1.06) Claude-3.5 GPT-4o Llama-3.2 6.14 (0.73) 5.70 (1.21) 6.07 (0.91) 5.97 (0.97) 5.75 (1.31) 5.80 (1.07) 6.14 (0.65) 6.08 (0.80) 5.75 (1.22) 5.80 (1.15) 5.98 (0.95) 6.07 (0.82) all UIs and for given factor. The metric is in [1, 1], where 1 is the maximum positive correlation, 1 is the maximum negative correlation, and 0 means no correlations. Spearmans 𝜌 is the Pearson correlation coefficient applied to ranks of MLLM and human scores. Finally, Kendalls 𝜏 measures the pairwise agreement between scores ordered by MLLMs and humans, over all pairs of UIs and for given factor. The metric is in [1, 1], where 1 is the maximum positive correlation, 1 is the maximum negative correlation, and 0 means no correlations. All metrics are reported in Table 3. Table 3 shows high correlation coefficients for multiple factors: Memorable, Aesthetic Pleasure, and Interesting. On average, Llama has the highest correlation coefficients, followed by Claude and then GPT-4o. Some results are quite strong. For instance, the Kendalls 𝜏 of 0.4, 0.6, and 0.8 means that the MLLM agrees with the human pairwise ranking 70%, 80%, and 90% of the time, respectively. Interestingly, high correlation metrics do not necessarily correlate with low prediction errors. As an example, factor Interesting has the highest MSE and MAE of all factors, and also low accuracy and 1-accuracy. At the same time, it has the highest correlation metrics for all MLLMs. This reveals that although MLLMs cannot reliably predict the human score, they are effective at capturing relative preferences and thus useful for ranking UIs. This observation motivated us to take deeper look at pairwise UI comparisons to examine whether these models are genuinely useful for ranking or preference-based tasks (Section 4.2). The opposite can also be true. Factor Comfort has the lowest MSE and MAE of all factors, and also high 1-accuracy. At the same time, all correlation metrics are close to zero. To conclude, both prediction errors and ranking metrics provide different views on predicted scores, and therefore we discuss both."
        },
        {
            "title": "4.2.1 Preference Prediction. This experiment is conducted as fol-\nlows. First, we estimate the ground-truth human preference using\nthe average human scores in Section 4.1.1. Specifically, for each pair\nof UIs for a given factor, we take the average human scores and say\nthat UI 𝐴 is preferred to UI 𝐵 if the average score of the former is\nhigher than that of the latter. Second, we obtain the preference of\nthe MLLM judge. Specifically, for each pair of UIs, we run the judge\nwith the prompt in Figure 8 and record its preference in each of\nthe nine UI factors. Finally, we say that the judge agrees with the\nhuman preference over two UIs in a given factor if it prefers the\nsame UI. The accuracy of the MLLM judge in a given factor is the\npercentage of UI pairs with agreement, and the overall accuracy is\nthe average of these accuracies.",
            "content": "All accuracies are reported in Table 4. They can vary drastically among the factors. GPT-4o and Claude both perform best when asked which UI is more Interesting, with accuracies around 75% 2025, , San Jose, CA, USA Reuben Luera, et al. Table 3: We report seven error metrics for all factors and MLLM judges. Four metrics measure the ability to predict scores (MSE, MAE, accuracy, and 1-accuracy) and three measure their utility for ranking (Pearson correlation coefficient, Spearmans 𝜌, and Kendalls 𝜏). The last row contains metric values for all factors jointly. Higher correlation coefficients than 0.4 are reported in bold. Factor Ease of Use Cognitive Clarity Visual Hierarchy Memorable Perceptual Trustworthy Intuitive Aesthetic Pleasure Emotional Interesting Comfort All Model MSE MAE Acc. Acc. 1 Pearson Spearman Kendall Claude-3.5 GPT-4o Llama-3.2 Claude-3.5 GPT-4o Llama-3.2 Claude-3.5 GPT-4o Llama-3.2 Claude-3.5 GPT-4o Llama-3. Claude-3.5 GPT-4o Llama-3.2 Claude-3.5 GPT-4o Llama-3.2 Claude-3.5 GPT-4o Llama-3.2 Claude-3.5 GPT-4o Llama-3.2 Claude-3.5 GPT-4o Llama-3.2 Claude-3.5 GPT-4o Llama-3. 0.83 0.56 0.37 0.10 0.48 0.45 0.26 0.43 0.38 0.36 0.08 0.34 0.40 0.27 0.32 0.58 0.20 0. 0.16 0.24 0.38 0.51 1.38 1.78 0.17 0.08 0.13 0.37 0.41 0.48 0.87 0.72 0.54 0.22 0.63 0. 0.41 0.60 0.56 0.53 0.23 0.52 0.58 0.45 0.48 0.71 0.38 0.28 0.31 0.38 0.47 0.62 1.09 1. 0.29 0.24 0.25 0.51 0.52 0.54 0.42 0.42 0.40 0.33 0.38 0.38 0.37 0.40 0.42 0.28 0.37 0. 0.44 0.44 0.43 0.43 0.40 0.36 0.49 0.41 0.38 0.26 0.14 0.16 0.36 0.35 0.34 0.38 0.37 0. 0.76 0.77 0.79 0.81 0.69 0.75 0.78 0.75 0.78 0.72 0.67 0.75 0.80 0.82 0.84 0.78 0.79 0. 0.85 0.76 0.76 0.62 0.43 0.37 0.85 0.78 0.82 0.77 0.72 0.74 -0.32 (0.152) 0.14 (0.529) 0.20 (0.348) -0.25 (0.163) 0.10 (0.514) 0.13 (0.396) -0.05 (0.820) 0.19 (0.380) 0.23 (0.283) 0.21 (0.341) -0.07 (0.744) 0.34 (0.100) -0.14 (0.535) 0.02 (0.929) 0.38 (0.068) 0.59 (0.004) 0.41 (0.046) 0.47 (0.019) 0.10 (0.652) 0.21 (0.320) 0.40 (0.052) 0.22 (0.324) 0.06 (0.788) 0.27 (0.198) -0.06 (0.779) -0.04 (0.845) 0.54 (0.007) 0.39 (0.069) 0.32 (0.131) 0.46 (0.023) 0.13 (0.573) 0.00 (0.995) 0.33 (0.116) 0.18 (0.292) 0.05 (0.764) 0.19 (0.202) -0.06 (0.710) -0.05 (0.744) 0.41 (0.006) 0.27 (0.092) 0.24 (0.115) 0.36 (0.024) 0.08 (0.638) -0.01 (0.940) 0.24 (0.135) 0.03 (0.843) 0.17 (0.270) -0.14 (0.340) -0.02 (0.944) 0.24 (0.254) 0.02 (0.919) 0.06 (0.794) 0.22 (0.296) -0.18 (0.410) 0.62 (<0.001) 0.62 (<0.001) 0.49 (<0.001) 0.10 (0.513) 0.19 (0.384) 0.21 (0.335) 0.42 (0.005) 0.55 (0.005) 0.51 (0.012) 0.85 (<0.001) 0.82 (<0.001) 0.69 (<0.001) 0.47 (0.002) 0.59 (0.002) 0.63 (<0.001) 0.44 (0.003) 0.60 (0.002) 0.61 (0.002) -0.12 (0.583) -0.29 (0.171) 0.22 (0.310) -0.10 (0.664) -0.32 (0.124) 0.24 (0.251) -0.05 (0.743) -0.25 (0.096) 0.18 (0.224) 0.69 (<0.001) 0.58 (<0.001) 0.44 (<0.001) 0.70 (<0.001) 0.60 (<0.001) 0.43 (<0.001) 0.73 (<0.001) 0.63 (<0.001) 0.47 (<0.001) Table 4: Accuracy in predicting pairwise human preferences over UIs, for all factors and MLLM judges. The last column shows the overall accuracy, averaged over all factors. The highest accuracy, for both individual factors and overall, is reported in bold."
        },
        {
            "title": "Model",
            "content": "Claude-3.5 GPT-4o Llama-3."
        },
        {
            "title": "Ease",
            "content": "47.01 50.99 51."
        },
        {
            "title": "Clar",
            "content": "54.98 54.55 52."
        },
        {
            "title": "Vis",
            "content": "61.35 56.92 53."
        },
        {
            "title": "Mem",
            "content": "61.75 64.82 54."
        },
        {
            "title": "Trust",
            "content": "61.75 58.89 56."
        },
        {
            "title": "Intui",
            "content": "58.17 59.68 51."
        },
        {
            "title": "ALL",
            "content": "68.53 64.03 52.47 78.49 75.10 52.47 47.81 51.38 52.91 59.98 59.60 52.96 and 78%, respectively. This is similar to Table 3, where factor Interesting has the highest correlation coefficients and thus is highly predictable. All models perform poorly when judging Ease of Use, with the accuracies around 50% and effectively indicating unpredictability. This is similar to Table 3, where factor Ease of Use has near-zero correlation coefficients. Both Claude and GPT-4o stand out in some areas while falling short in others. Llama is close to random across all factors. The overall accuracy reflects this, with both Claude and GPT-4o being close to 59% and Llama being close to random 51.79%."
        },
        {
            "title": "4.2.2 Hardness of Preference Prediction. Although many of our UIs\nare professionally designed, humans can judge them very differ-\nently depending on the factor. This motivated us to study whether\nhuman preferences are easier to predict if the compared UIs are\njudged as more different by humans. Specifically, for all UI pairs\nand factors, we compute the absolute difference of their average",
            "content": "MLLM as UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces 2025, , San Jose, CA, USA For this reason, it is recommended that MLLMs be seen as UI testing approximators, not replacements. Essentially, given the results, MLLMs should be used to supplement existing research or be used in scenarios where research would not otherwise be used. The results of this study reflect that MLLMs struggled on fine-grained UI evaluation tasks. However, MLLMs have reasonable accuracy and reliability in generalized evaluations and when comparing UIs with larger disparities. Furthermore, results should be seen as approximations and are capable of giving designers general direction. However, they are not capable of acting as complete replacements for real human research."
        },
        {
            "title": "5.2 MLLMs’ Role in the Design Process\nGiven the moderate strength in human emulating UI tasks, MLLMs\nshow decent promise in acting as a proxy for early user testing.\nGiven that overall correlation coefficients across all models (Table 4)\nwere relatively decent, MLLMs show moderate promise for low-\nstakes user research. More specifically, designers should use them\nearly in the design process in situations where they may be rapidly\niterating through several different designs and not have the research\nresources to test and compare every single design. As seen in the\nresults, MLLMs perform moderately well in these use cases as\nthey can accurately score designs within ±1 Likert scale point.\nFurthermore, as seen in the pairwise testing, they are also able to\nstrongly predict human preferences when there is a large difference\nin human scores. For this reason, MLLMs can responsibly be used\nin these early design process situations in order to get a sense of\ndirection on early UI iterations.",
            "content": "However, these results also suggest that MLLMs are not accurate enough to be used as permanent or effective standout when exact results are needed. There was significant variation when looking at the results on the factor level, and it is clear that these models are not precise across most factors. For this reason, it is not recommended that these models be used for validation testing towards the end of the design process or high-stakes user research scenarios."
        },
        {
            "title": "5.3 Benchmark for Fine-Tuned Models\nAnother future direction for this work would be to create a fine-\ntuned model with stronger factor sensitivity than the state-of-the-\nart models presented in this study. Considering that the state-of-\nthe-art models mostly struggled with several traits like ease-of-use\nand even clarity, there is clearly a gap in how these models han-\ndle different traits. This gap suggests an opportunity to create a\nfine-tuned model based on the benchmark scores that will be better\nable to predict and replicate human scores. This benchmark can\nbe utilized in tandem with reinforcement learning with human\nfeedback (RLHF) on factor-specific data or even chain of thought\nprompting. Given this, the fine-tuned tool would be more useful in\ngeneral, being more applicable and reliable in the design process.\nFurthermore, the data could be used to fine-tune prompts used. Do-\ning so would require less lift than creating a fine-tuned model while\nalso potentially making a significant difference in the evaluation\nquality.",
            "content": "Figure 3: Accuracy in predicting pairwise human preferences over UIs as function of the absolute difference of their average human scores. higher accuracy generally correlates with larger difference in the human scores. The scoredifference bin values were calculated as the difference between the average human ratings of the two UIs. This number is used to infer the difficulty of the question: smaller values indicate more difficult questions, while larger values imply easier ones. human scores and bin them. For each bin, we report the accuracy of the MLLM judge to predict the human preference. We expect MLLMs to struggle more when predicting preferences for UI pairs that humans judged as more similar, but to perform more accurately when humans expressed clear preference with larger rating differences. Our results are reported in Figure 3. When the difference in human preferences between UI pairs is small, the accuracy of all MLLM judges is only slightly above 50%. This suggests that when humans expressed similar levels of preference for the two UIs, their choices were effectively unpredictable by MLLMs. However, as the human preference score difference increases, suggesting that the UIs are more distinguishable for humans, the accuracy of the MLLM judges increases. GPT-4o and Claude have the largest increases, starting near 50% and ending close to 90% and 93%, respectively. Llama performs poorly, and its accuracy increases to only about 60%."
        },
        {
            "title": "5 DISCUSSION\n5.1 MLLMs as Approximators\nAcross all three models, the benchmark data illustrate that mod-\nels can achieve reasonably high accuracy within ±1 for absolute\nscore tasks (Table 4) and GPT and Claude perform reasonably well\nin pairwise tasks where the two UIs have a significant degree of\ndifference (Figure 3). However, the same models perform less effec-\ntively in terms of exact accuracy (Claude 48% and GPT 40%) and\nwhen there is a small degree of difference in pairwise tasks. In fact,\nthe models performed near random on pairwise tasks with small\ndegrees of difference. Further, MAE scores ranged from 0.53 to 0.64,\nand MSE scores ranged from 0.44 to 0.66. While these scores are\nmoderate, they are also significant enough to reflect that state of\nthe art models cannot fully replace human evaluation.",
            "content": "2025, , San Jose, CA, USA Reuben Luera, et al."
        },
        {
            "title": "5.4 Ethical Considerations\nWhile these results show encouraging reasons for using MLLMs as\na UI judge, it should not be used to justify the complete replacement\nof human testing. In fact, it should do the opposite and reinforce\nthe need for real human testing. The results presented expose key\nlimitation that should give any design team pause when asking\nwhether they should completely rely on MLLMs for user interface\nvalidation.",
            "content": "Again, we emphasize that these results reflect that MLLMs should be used solely for supplemental use. It should be used to fill gaps where no user research exists, not to replace real human research and testing. MLLMs should be seen as source of low-cost, rapid iterative evaluations when it is not feasible to perform extensive user research. This will enable designers to identify common trends and larger flaws earlier on in the design process. Despite the seemingly negative ethical considerations, there is strong reason for why this method should be adopted. There is significant potential for using this dataset to increase design democratization for junior designers with little to no access to high-end design research. Specifically, there is large opportunity for designers to use MLLMs to \"fine-tune\" their designs, using MLLMs as sanity check of their design intuitions and learning opportunity."
        },
        {
            "title": "5.5 Future Work & Limitations\nBuilding on our benchmark, we are extremely excited by the oppor-\ntunities to expand AI’s capabilities as a judge for interface testing.\nMost notably, this benchmark can support fine-tuned models or\nalignment training—particularly reinforcement learning from hu-\nman feedback (RLHF). Whether used alone or with similar datasets,\nit can help create models that better supplement user research.\nThere is also the opportunity to expand the benchmark by adding\nmore screens and tasks. For instance, this study focused on mo-\nbile and desktop UIs, but future work could include tablets, TVs,\nconsoles, and more. A broader set of UIs would enable a more holis-\ntic benchmark. We also see potential to study more models and\nprompting methods—this work focused on GPT-4o, Claude 3.5 Son-\nnet, and Llama 3.2, but future studies could explore Gemini, Qwen,\nGPT-4o mini, and others using the same framework. Prompting\nstrategies like chain-of-thought and few-shot learning also merit\nfurther exploration. Finally, because UI design is iterative, future\nwork could compare versions of the same design, measuring how\nsmall changes—like color, text, or layout—affect human and MLLM\njudgments.",
            "content": "While our findings reveal MLLMs capability to emulate human UI perceptions to moderate degree, several limitations of this study still persist. Firstly, the entirety of both the human and Firstly, the entirety of both the human and MLLMs tasks was performed on static user interfaces. Secondly, the screens used in the study were primarily professionally made within the context of Western design standards. We leave it to future extending our line of work to the diverse, dynamic interactions that usually occur in user research."
        },
        {
            "title": "6 CONCLUSION\nTo evaluate the effectiveness of MLLMs as UI judges, this study\nfacilitated the collection and testing of a set of user interfaces. The",
            "content": "same UI studies were given to both human and MLLM evaluators, and the results were compared to analyze to what degree MLLM evaluators can emulate human results. This study was conducted using two unique tasks: 1) an absolute scoring task using 1-7 Likert scale and 2) pairwise evaluation. For task 1, the findings reveal that when asked to evaluate screens at an overall level, MLLMs do moderately well. However, when asked to judge them based on specific factors or criteria, they tend to struggle. For task 2, the MLLMs again performed reasonably on the pairwise comparisons. Naturally, MLLM evaluators performed better on pairwise questions that compared two screens that have higher degree of difference. Lastly, we accumulated the results from these tests into benchmark to be used in future studies. Future work offers the opportunity to use the benchmark to create fine-tuned model that would perform better at UI judgment tasks compared to state-ofthe-art models. For this reason, we recommend that MLLMs are used to supplement early, low stakes user research not replace it. Finally, the benchmark provided in this paper is capable of laying the groundwork for future studies that can better explore MLLM as UI judge. REFERENCES [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Klaus Bærentsen. 2000. Intuitive user interfaces. Scandinavian Journal of Information Systems 12, 1 (2000), 4. [3] Wayne Bailey, Stephen Knox, and Eugene Lynch. 1988. Effects of interface design upon user productivity. In Proceedings of the SIGCHI conference on Human factors in computing systems. 207212. [4] Kathy Baxter, Catherine Courage, and Kelly Caine. 2015. Understanding your users: practical guide to user research methods. Morgan Kaufmann. [5] Renato Antonio Bertão and Jaewoo Joo. 2021. Artificial intelligence in UX/UI design: survey on current adoption and [future] practices. Safe Harbors for Design Research (2021), 110. [6] Upasna Bhandari, Tillmann Neben, Klarissa Chang, and Wen Yong Chua. 2017. Effects of interface design factors on affective responses and quality evaluations in mobile applications. Computers in Human Behavior 72 (2017), 525534. [7] Alethea Blackler, Vesna Popovic, and Douglas Mahar. 2005. Intuitive interaction applied to interface design. In New Design Paradigms: Proceedings of International Design Congress (IDC) 2005. International Design Congress, 110. [8] Letizia Bollini. 2017. Beautiful interfaces. From user experience to user interface design. The Design Journal 20, sup1 (2017), S89S101. [9] Michelle A. Borkin, Azalea A. Vo, Zoya Bylinskii, Phillip Isola, Shashank Sunkavalli, Aude Oliva, and Hanspeter Pfister. 2013. What Makes Visualization Memorable? IEEE Transactions on Visualization and Computer Graphics 19, 12 (2013), 23062315. https://doi.org/10.1109/TVCG.2013.234 [10] Renjie Cao, Mei Zhang, and Yu Huang. 2025. Learning to Simulate Survey Distributions with Fine-Tuned LLMs. In Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). [11] Ali Darejeh, Nadine Marcusa, Gelareh Mohammadi, and John Sweller. 2024. critical analysis of cognitive load measurement methods for evaluating the usability of different types of interfaces: guidelines and framework for HumanComputer Interaction. arXiv preprint arXiv:2402.11820 (2024). [12] Amit Kumar Das, Cindy Xiong Bearfield, and Klaus Mueller. 2025. Leveraging Large Language Models for Personalized Public Messaging. In Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 17. [13] Anjali Dave, Ankur Saxena, and Avdhesh Jha. 2023. Understanding User comfort and Expectations in AI-based Systems. (2023). [14] Peitong Duan, Chin-Yi Cheng, Gang Li, Bjoern Hartmann, and Yang Li. 2024. UICrit: Enhancing Automated Design Evaluation with UI Critique Dataset. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology. 117. [15] Peitong Duan, Jeremy Warner, Yang Li, and Bjoern Hartmann. 2024. Generating automatic feedback on ui mockups with large language models. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 120. MLLM as UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces 2025, , San Jose, CA, USA [16] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. 2024. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594 (2024). [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [18] Perttu Hämäläinen, Mikke Tavast, and Anton Kunnari. 2023. Evaluating Large Language Models in Generating Synthetic HCI Research Data: Case Study. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI 23). Association for Computing Machinery, New York, NY, USA, Article 433, 19 pages. https://doi.org/10.1145/3544548.3580688 [19] Jan Hartmann, Alistair Sutcliffe, and Antonella De Angeli. 2008. Towards theory of user judgment of aesthetics and user interface quality. ACM Transactions on Computer-Human Interaction (TOCHI) 15, 4 (2008), 130. [20] Stephen Hinton. 2012. Using Recursive Distance Vector Methodology to Review Remote Desktop Solutions in the Small Business Consulting Environment. Issues in Information Systems 13, 1 (2012), 94104. [21] Bernard J. Jansen, Soon-Gyo Jung, and Joni Salminen. 2023. Employing large language models in survey research. Nat. Lang. Process. J. 4 (2023), 100020. https://doi.org/10.1016/j.nlp.2023.100020 [22] Kate Kaplan. 2020. Typical DesignertoDeveloper and ResearchertoDesigner Ratios. https://www.nngroup.com/articles/ux-developer-ratio/ [23] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535 (2024). [24] Seon Gyeom Kim, Jae Young Choi, Ryan Rossi, Eunyee Koh, and Tak Yeon Lee. 2025. Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts. In 2025 IEEE 18th Pacific Visualization Conference (PacificVis). IEEE, 340345. [25] Emily Kuang, Minghao Li, Mingming Fan, and Kristen Shinohara. 2024. Enhancing UX Evaluation Through Collaboration with Conversational AI Assistants: Effects of Proactive Dialogue and Timing. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 24). Association for Computing Machinery, New York, NY, USA, Article 3, 16 pages. https://doi.org/10.1145/3613904.3642168 [26] Ram Kumar, Michael Alan Smith, and Snehamay Bannerjee. 2004. User interface Information & features influencing overall ease of use and personalization. Management 41, 3 (2004), 289302. [27] Seongyun Lee, Seungone Kim, Sue Park, Geewook Kim, and Minjoon Seo. 2024. Prometheus-vision: Vision-language model as judge for fine-grained evaluation. In Findings of the association for computational linguistics ACL 2024. 1128611315. [28] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2023. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470 (2023). [29] Peiyao Li, Noah Castelo, Zsolt Katona, and Miklos Sarvary. 2024. Frontiers: Determining the validity of large language models for automated perceptual analysis. Marketing Science 43, 2 (2024), 254266. [30] Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, and Nigel Collier. 2024. Aligning with human judgement: The role of pairwise preference in large language model evaluators. arXiv preprint arXiv:2403.16950 (2024). [31] Reuben Luera, Ryan Rossi, Franck Dernoncourt, Alexa Siu, Sungchul Kim, Tong Yu, Ruiyi Zhang, Xiang Chen, Nedim Lipka, Zhehao Zhang, Seon Gyeom Kim, and Tak Yeon Lee. 2025. Personalizing Data Delivery: Investigating User Characteristics and Enhancing LLM Predictions. In Companion Proceedings of the ACM on Web Conference 2025 (Sydney NSW, Australia) (WWW 25). Association for Computing Machinery, New York, NY, USA, 11671171. https: //doi.org/10.1145/3701716.3715452 [32] Reuben Luera, Ryan Rossi, Alexa Siu, Franck Dernoncourt, Tong Yu, Sungchul Kim, Ruiyi Zhang, Xiang Chen, Hanieh Salehy, Jian Zhao, et al. 2024. Survey of User Interface Design and Interaction Techniques in Generative AI Applications. arXiv preprint arXiv:2410.22370 (2024). [33] Anna Macaranas, Alissa Antle, and Bernhard Riecke. 2015. What is intuitive interaction? Balancing users performance and satisfaction with natural user interfaces. Interacting with Computers 27, 3 (2015), 357370. [34] Michael Medlock. 2018. The rapid iterative test and evaluation method (RITE). Games User Research (2018), 203215. [35] Abdul Wahab Muzaffar, Farooque Azam, Hina Anwar, and Ali Saeed Khan. 2011. Usability aspects in pervasive computing: Needs and challenges. International Journal of Computer Applications 32, 10 (2011). [36] Jakob Nielsen. 2005. Ten usability heuristics. (2005). [37] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 2773027744. [38] Sharon Oviatt. 2006. Human-centered design meets cognitive load theory: designing interfaces that help people think. In Proceedings of the 14th ACM international conference on Multimedia. 871880. [39] Arjun Panickssery, Samuel R. Bowman, and Shi Feng. 2024. LLM Evaluators Recognize and Favor Their Own Generations. arXiv:2404.13076 [cs.CL] https: //arxiv.org/abs/2404.13076 [40] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky. 2024. Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. arXiv:2306.17563 [cs.IR] https://arxiv.org/abs/2306.17563 [41] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. 2022. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241 (2022). [42] Katharina Reinecke, Tom Yeh, Luke Miratrix, Rahmatri Mardiko, Yuechen Zhao, Jenny Liu, and Krzysztof Gajos. 2013. Predicting users first impressions of website aesthetics with quantification of perceived visual complexity and colorfulness. In Proceedings of the SIGCHI conference on human factors in computing systems. 20492058. [43] Tina Rosala and Kate Moran. 2024. Synthetic Users and AI Personas in UX Research: Cautionary Note. Nielsen Norman Group Reports (2024). https: //www.nngroup.com/articles/synthetic-users-ai/ [44] Eldon Schoop, Xin Zhou, Gang Li, Zhourong Chen, Bjoern Hartmann, and Yang Li. 2022. Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis. In CHI Conference on Human Factors in Computing Systems (CHI 22). ACM, 121. https://doi.org/10.1145/3491102. [45] William Seeley. 2012. Hearing how smooth it looks: Selective attention and crossmodal perception in the arts. Essays in Philosophy 13, 2 (2012), 498517. [46] Ben Shneiderman. 2004. Designing for fun: how can we design user interfaces to be more fun? interactions 11, 5 (2004), 4850. [47] Marc Steen, Lottie Kuijt-Evers, and Jente Klok. 2007. Early user involvement in research and design projectsA review of methods and practices. In 23rd egos colloquium, Vol. 5. 121. [48] Jeremiah Still. 2018. Web page visual hierarchy: Examining Faradays guidelines for entry points. Computers in Human Behavior 84 (2018), 352359. [49] Chase Stokes, Kylie Lin, and Cindy Xiong Bearfield. 2025. Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances. IEEE VIS (2025). [50] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023). [51] Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Judging the Judges: Evaluating Vaidyanathan, and Dieuwke Hupkes. 2025. Alignment and Vulnerabilities in LLMs-as-Judges. arXiv:2406.12624 [cs.CL] https://arxiv.org/abs/2406.12624 [52] Di Wang. 2024. Research on Visual Hierarchy and Interactive Experience in Digital Media UI Design. In 2024 5th International Conference on Intelligent Design (ICID). IEEE, 133136. [53] Dakuo et al. Wang. 2025. AgentA/B: Automated and Scalable Web A/B Testing with Interactive LLM Agents. arXiv preprint arXiv:2504.09723 (2025). [54] Huichen Will Wang, Jane Hoffswell, Victor Bursztyn, Cindy Xiong Bearfield, et al. 2024. How aligned are human chart takeaways and llm predictions? case study on bar charts with varying layouts. IEEE Transactions on Visualization and Computer Graphics (2024). [55] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926 (2023). [56] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560 (2022). [57] Yun Wang, Adrien Segal, Roberta Klatzky, Daniel F. Keefe, Petra Isenberg, Jörn Hurtienne, Eva Hornecker, Tim Dwyer, and Stephen Barrass. 2019. An Emotional Response to the Value of Visualization. IEEE Computer Graphics and Applications 39, 5 (2019), 817. https://doi.org/10.1109/MCG.2019.2923483 [58] Jason Wu, Yi-Hao Peng, Xin Yue Amanda Li, Amanda Swearngin, Jeffrey Bigham, and Jeffrey Nichols. 2024. UICLIP: data-driven model for assessing user interface design. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology. 116. [59] Wei Xiang, Hanfei Zhu, Suqi Lou, Xinli Chen, Zhenghua Pan, Yuping Jin, Shi Chen, and Lingyun Sun. 2024. SimUser: Generating Usability Feedback by Simulating Various Users Interacting with Mobile Applications. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 24). Association for Computing Machinery, New York, NY, USA, Article 9, 17 pages. https://doi.org/10.1145/3613904.3642481 [60] Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862 (2023). 2025, , San Jose, CA, USA Reuben Luera, et al. [61] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems 36 (2023), 4659546623. [62] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2020. Fine-Tuning Language Models from Human Preferences. arXiv:1909.08593 [cs.CL] https://arxiv.org/ abs/1909. [63] Valentin Zieglmeier and Antonia Maria Lehene. 2021. Designing trustworthy user interfaces. In Proceedings of the 33rd Australian Conference on Human-Computer Interaction. 182189. OPPORTUNITIES (EXTENDED) Building on our benchmark, we are extremely excited by the possible opportunities to expand on the capabilities of MLLM as judge for interface testing. Most notably, this benchmark can be used to create fine-tuned model or in alignment training. Reinforcement learning from human feedback (RLHF) is specialized fine-tuning technique that would most align with this benchmark. Whether using this benchmark in isolation or in tandem with other similar datasets, there is an opportunity to create fine-tuned model that will better supplement user research. Figure 4: Results when pairwise data is created for MLLMs and humans in the same way. For both human and MLLM evaluation sets, we take the absolute value gathered in task 1, compare the two absolute values, and take the higher of the two. Then we compare the human results to the MLLM results to get the agreement score presented here. Secondly, there is also the opportunity to expand on the benchmark by adding more screens and/or more tasks. Increasing the amount and diversity of screens can uncover more trends that were not uncovered in this study. For example, this study focused on mobile and desktop first UIs, but UIs exist for tablets, televisions, entertainment consoles, and many other devices. Expanding the benchmark to include diverse set of UIs will create more holistic benchmark. Furthermore, user research is not limited to just Likertscale scores and A/B testing. The benchmark can be expanded by adding usability studies, surveys, need-finding, etc. Expanding the data in such way will further test MLLMs efficacy in UI tasks and create more expansive benchmark for fine-tuned models. There is also the opportunity to study the efficacy of more models and prompting methods. This study revolved mostly around GPT4o, Claude 3.5 Sonnet, and Llama 3.2; running the same tasks on other models like Googles Gemini, Qwen, or even GPT-4o mini could unveil even further outcomes and trends. Future studies can utilize the framework established in this study to test wider range of models. Similarly, there is an opportunity to explore the role that prompting strategies can play in MLLM as UI judge. For this reason, future studies can explore chain-of-thought and few shot learning methods to explore the impact that these methods have on outputs. Finally, considering that UI design is an iterative process, there is an opportunity for future studies to compare UI designs to their earlier iterations. In the pairwise portion of the study, different MLLM as UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces 2025, , San Jose, CA, USA Figure 5: Mean Absolute Error (lower is better): This result shows that the mean absolute error for all models consistently is less than 1, with \"interesting\" being the exception. These results show that the models can reliably score within one point of the human scores when given Likert scale UI question. UI designs were compared with one another. Instead, there is an opportunity to explore the impact of changing single color, text, or even the layout, and comparing it with an earlier iteration to measure the degree that small changes within the same design change human and MLLM judgments. ADDITIONAL RESULTS & PROMPTS The additional results shown reflect further visualizations of data presented in the main body of the paper. Figure 5 and Figure 6 represent how well MLLMs did on Task 1, with the former representing the mean absolute error (MAE) and the latter being visual summary of Table 2. Furthermore works like Figure 4, represent potential ablation study data. Here we also present the prompts used to have the MLLMs evaluate the UIs in the same way that the humans did. Figure 7 asks the MLLMs to evaluate the UIs on 7-point Likert scale on the UI factors presented in Table 1. Figure 8 prompts the MLLMs to compare two UIs to one another on the aforementioned factors and choos \"winner.\" 2025, , San Jose, CA, USA Reuben Luera, et al. (a) Browsing & Discovery (b) Confirmation & Feedback (c) Communication & Engagement Figure 6: Three grouped bar charts comparing mean factor scores from humans vs. each model. Human score marked with black line, model scores represented by different bars. Trends show that MLLMs are able to effectively approximate human scores. Also shows how model scoring distributions align with human scores across Browsing & Discovery, Confirmation & Feedback, and Communication & Engagement. UI Evaluation Task Pt.1 \"You are an average user brought in to do human testing. For the given UI image, evaluate the following nine qualities. For each, give one of the following ratings: 1 (strongly disagree), 2 (disagree), 3 (slightly disagree), 4 (neutral), 5 (slightly agree), 6 (agree), or 7 (strongly agree), followed by short rationale. **Format your response exactly like this:** \"1. **The UI is easily remembered**: **[x]/7** Your rationale here.\" \"2. **The UI appears trustworthy**: **[x]/7** Your rationale here.\" \"3. **The UI is aesthetically pleasing**: **[x]/7** Your rationale here.\" \"...\" \"Do NOT use any other formatting like parentheses or dashes. Only use the format shown.\" Evaluate the following: \"The UI is easily remembered.\", \"The UI appears trustworthy.\", \"The UI is aesthetically pleasing.\", \"The UI is intuitive.\", \"The UI is interesting.\", \"I feel comfortable with the UI.\", \"The UI looks easy to use.\", \"The layout is uncluttered\", \"The UI has clear visual hierarchy\" [UI Image] Figure 7: Instructions provided to the MLLMs to evaluate the UI in Section 4.1 MLLM as UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces 2025, , San Jose, CA, USA"
        },
        {
            "title": "X UI Pairwise Evaluation Prompt",
            "content": "You are an average user evaluating user interface. You will be shown two UI screenshots: UI-A: UI-B: [UI Image A] [UI Image B] Your task is to determine which UI an ordinary person would prefer for 10 evaluation criteria. For each criterion, write: <criterion>[Criterion Text]</criterion> <result>[UI-A or UI-B]</result> <reason>[your reasoning in less than 50 words]</reason> Here are the evaluation criterion: * The UI is easily remembered * The UI appears trustworthy * The UI is aesthetically pleasing * ... Figure 8: Instructions provided to the MLLMs for pairwise comparison in Section 4.2."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of California, Berkeley"
    ]
}