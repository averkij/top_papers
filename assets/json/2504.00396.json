{
    "paper_title": "SPF-Portrait: Towards Pure Portrait Customization with Semantic Pollution-Free Fine-tuning",
    "authors": [
        "Xiaole Xian",
        "Zhichao Liao",
        "Qingyu Li",
        "Wenyu Qin",
        "Pengfei Wan",
        "Weicheng Xie",
        "Long Zeng",
        "Linlin Shen",
        "Pingfa Feng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait dataset is the mainstream method for text-driven customization of portrait attributes. Due to Semantic Pollution during fine-tuning, existing methods struggle to maintain the original model's behavior and achieve incremental learning while customizing target attributes. To address this issue, we propose SPF-Portrait, a pioneering work to purely understand customized semantics while eliminating semantic pollution in text-driven portrait customization. In our SPF-Portrait, we propose a dual-path pipeline that introduces the original model as a reference for the conventional fine-tuning path. Through contrastive learning, we ensure adaptation to target attributes and purposefully align other unrelated attributes with the original portrait. We introduce a novel Semantic-Aware Fine Control Map, which represents the precise response regions of the target semantics, to spatially guide the alignment process between the contrastive paths. This alignment process not only effectively preserves the performance of the original model but also avoids over-alignment. Furthermore, we propose a novel response enhancement mechanism to reinforce the performance of target attributes, while mitigating representation discrepancy inherent in direct cross-modal supervision. Extensive experiments demonstrate that SPF-Portrait achieves state-of-the-art performance. Project webpage: https://spf-portrait.github.io/SPF-Portrait/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 2 6 9 3 0 0 . 4 0 5 2 : r SPF-Portrait: Towards Pure Portrait Customization with Semantic Pollution-Free Fine-tuning Xiaole Xian1 Zhichao Liao2 Qingyu Li3 Wenyu Qin3 Pengfei Wan3 Weicheng Xie1 (cid:66) Long Zeng2 (cid:66) Linlin Shen1 Pingfa Feng2 2Tsinghua University https://spf-portrait.github.io/SPF-Portrait/ 3Kuaishou Technology 1Shenzhen University Figure 1. Our SPF-Portrait can achieve target attributes (single/multiple) while preserving original models behavior and stably performs in Continuous Replacements and Additions in text-driven portrait customization. Customized results (Line 2&3) are conditioned by both Base text and Target Text, while Original performance (Line 1) is only conditioned by Base Text."
        },
        {
            "title": "Abstract",
            "content": "Fine-tuning pre-trained Text-to-Image (T2I) model on tailored portrait dataset is the mainstream method for textdriven customization of portrait attributes. Due to Semantic Pollution during fine-tuning, existing methods struggle to maintain the original models behavior and achieve incremental learning while customizing target attributes. To address this issue, we propose SPF-Portrait, pioneering work to purely understand customized semantics while eliminating semantic pollution in text-driven portrait customization. In our SPF-Portrait, we propose dual-path pipeline that introduces the original model as reference * Co-first authors. Work done during internship at KwaiVGI, Kuaishou Technology. (cid:66) Corresponding authors. 1 for the conventional fine-tuning path. Through contrastive learning, we ensure adaptation to target attributes and purposefully align other unrelated attributes with the original portrait. We introduce novel Semantic-Aware Fine Control Map, which represents the precise response regions of the target semantics, to spatially guide the alignment process between the contrastive paths. This alignment process not only effectively preserves the performance of the original model but also avoids over-alignment. Furthermore, we propose novel response enhancement mechanism to reinforce the performance of target attributes, while mitigating representation discrepancy inherent in direct cross-modal supervision. Extensive experiments demonstrate that SPFPortrait achieves state-of-the-art performance. 1. Introduction Fine-tuning pre-trained Text-to-Image (T2I) model [54, 55, 57] has been widely recognized as an efficient and resource-saving solution for text-driven portrait customization [22, 28], especially for its extensive applications and substantial commercial value [33, 37, 41, 63, 70]. It adapts the model to understand personalized target attributes on an expansive text-image portrait dataset. An ideal finetuning should approach incremental learning [31], thereby maintaining the models original capabilities and continuous scalability [47, 50], while avoiding catastrophic forgetting [16, 35, 69]. This necessitates purely capturing the target semantics during fine-tuning, which manifests in the results as introducing portrait differences only through target attributes while maintaining consistency in unrelated attributes with the original portrait. However, as shown in Fig. 1, although prevalent naive fine-tuning can achieve target attributes, it significantly changes the portrait identity, posture, background, and other attributes, disrupting the original models behavior. This is because the target semantics entangling with redundant features from the additional dataset during fine-tuning, resulting in an impure target response [49]. We refer to this phenomenon as Semantic Pollution, which is detrimental and often ignored. To address this issue, we propose SPF-Portrait, the first method to our knowledge that customizes attributes while eliminating semantic pollution in text-driven portrait customization. One line of previous research related to our work is PEFT-based methods [3, 14, 26, 44, 71]. For example, LoRA [26] and its variants [3, 14, 71] encourage the model to focus on the more relevant aspects of the target semantics by reducing trainable parameters, thus minimizing disruption. Orthogonal Fine-Tuning methods [44, 52] improve the LoRA on the model stability, further reducing disruption. However, their exclusive reliance on diffusion loss to implicitly model the joint distribution, rather than understanding disentangling semantics, making it challenging to preserve the original behavior. Another line of work [5 7, 29, 43, 46, 64, 74] aims to purify the understanding of text embeddings and decouple attributes from each other. For instance, Magnet [74] and TEBopt [7] focus on decoupling at the embedding level, while Tokencompose [64] apply regularization techniques to the attention mechanisms in model architecture for text understanding. However, these studies primarily improve the independence of target semantics in instance-level generation (e.g., cat, dog), but pay less attention to refined attributes, such as skin texture, freckles, and hairstyles of portraits. In this paper, we begin by defining two key objectives regarding the achievement of desired results in text-driven portrait customization: (1) Keeping portrait attributes unrelated to the target semantics consistent with the original models behavior; (2) Ensuring an effective response to the target attributes. To this end, we propose SPF-Portrait, dual-path contrastive learning pipeline. We introduce the frozen original model to serve as the standard of original behavior for conventional fine-tuning path. To align with the original performance, we extract and constrain attention features and UNet features from the corresponding cross-attention layers in contrastive paths. We design novel Semantic-Aware Fine Control Map (SFCM) that accurately identifies target response regions to spatially guide the alignment of these intermediate features. This alignment process precisely aligns irrelevant attributes, avoiding suppression of target response or over-alignment. Moreover, we introduce response enhancement mechanism for target semantics. By supervising the difference vectors of target semantics between the one-step prediction and the ground truth image, we amplify the effectiveness of target semantics and mitigate the representational gaps inherent in direct cross-modal supervision. Extensive experiments demonstrate that SPF-Portrait achieves state-of-theart (SOTA) performance in preventing semantic pollution for text-driven portrait customization compared to previous methods. In summary, our contributions are as follows: We propose SPF-Portrait, dual-path contrastive learning pipeline, which is the pioneering work to address semantic pollution in text-driven portrait customization. We introduce novel Semantic-Aware Fine Control alignment process capable of preserving original models behavior while meticulously preventing over-alignment. We design response enhancement mechanism to improve the effectiveness of target semantics while alleviating representation gaps in direct cross-modal supervision. Extensive quantitative and qualitative experimental results demonstrate the superiority of our SPF-Portrait. 2. Related Work Fine-tuning for T2I Diffusion Models. Numerous solutions [3, 23, 27, 33, 39, 56, 63, 64, 70, 71] have improved the existing T2I diffusion models in various aspects based primarily on fine-tuning [42, 55, 57]. Building upon the finetuning paradigm, PEFT-based methods rapidly adapt to new concepts by introducing additional parameters to the original model architecture. LoRA [26] achieves it through lowrank linear layers, while FouRA [3] based on LoRA further improves multi-concept integration by leveraging frequency domain learning. Subsequent studies [21, 44, 52] has further improved the preservation of prior knowledge during fine-tuning. For instance, SVDiff [21] fine-tunes only the singular values, the key parameters, via singular value decomposition. OFT [52] maintains the orthogonality of weight matrices, thereby preserving the hyperspherical energy of the pre-trained model. Although these works preserve pre-trained knowledge while adapting to new con2 cepts, they overlook impure learning from relying solely on diffusion loss, causing new concepts to couple with irrelevant dataset attributes. Decoupling Generation of Diffusion Models. Efforts have also been made on decoupling control mechanisms, both between image-to-text conditions and within textual conditions, aiming to preventing the hinder to the textual control [5, 7, 20, 27, 51, 74]. To achieve the coupling within text, Magnet [74] and TEBopt [7] analyze and optimize the condition embedding without additional training. However, while mitigating coupling at the instance level, they fail to correct the models deviation in understanding refined attributes. RealCustom [27] dynamically adjusts image feature injection based on their impact on diffusion process, while DEADiff [51] tackles similar issues via decoupling representation mechanism. PuLID [20] employs contrastive learning to prevent the injection of ID from disrupting the textual guidance to achieve decoupling. However, these methods ignore the disruption from text conditions during fine-tuning with reference images. Distinction with Text-driven Image Editing Methods. The exceptional capability to adhere to base text enables our method to achieve end-to-end image manipulation directly through T2I model, eliminating the need for additional editing pipelines. While integrating text-driven editing methods [4, 12, 18, 32, 34, 55, 56, 62] into the T2I model pipeline can yield results comparable to ours. For image generated with T2I model, InstructPix2Pix [4] enables precise image manipulation through textual instructions by leveraging conditioned diffusion model trained on paired image editing datasets. Similarly, DiffusionCLIP [34] and Asyrp [36], inspired by GAN-based methods [1], utilize local directional CLIP loss between images and text to manipulate specific attributes. However, the task of our work lies in preventing new textual concepts disrupting T2I models, which fundamentally differs from the goal of I2I editing models that focus on image manipulation. 3. Methodology In this section, we first describe the preliminaries (Sec. 3.1), and then present the dual-path contrastive learning pipeline of our SPF-Portrait (Sec. 3.2). Subsequently, we introduce in detail the process of using the Semantic-Aware Fine-controllable Map to guide alignment with the original models behavior (Sec. 3.3) and present the response enhancement mechanism for target semantics (Sec. 3.4). 3.1. Preliminary Diffusion Models. T2I diffusion models can generate images based on text input through forward diffusion process and reverse denoising process [25, 55, 57]. The diffusion process follows the Markov chain to gradually transform an image sample x0 into noisy samples x1:T by adding GausFigure 2. Visualization of the Attention Map. The salient regions directly reflect response intensity to the target text hat. Brighter regions indicate higher attention. sian noise ϵ over steps. The denoising process employs denoising model ϵθ to predict the added noise using xt, t, and textual conditions as inputs, where θ denotes the learnable parameters and [0, ] is the diffusion process timestep. The optimization process can be described as: Ldif = Ex0,ϵN (0,1),t(ϵ ϵθ(xt, t, E)2 2), (1) where = τtext(y) is textual features, obtained from the textual conditions encoded by the text encoder τtext. Attention Mechanism. Within the Stable Diffusion [55] which we adopted as T2I diffusion models, textual features are integrated into cross-attention layers following the attention mechanism [61] as: (cid:40) Attention(Q, K, ) = Softmax( QKT = WKE; = WV E, )V (2) where is derived from the UNet image features, WK and WV represent the learnable linear layers. 3.2. Dual-Path Contrastive Learning Pipeline Naive fine-tuning can adapt pre-trained T2I model to generate text-customized portrait attributes but fails to achieve incremental learning. We visualize the attention map [61] of target text after naive fine-tuning in Fig. 2 to analyze the reason. It is evident that the response regions of target semantics are extended to unrelated areas, interfering with other attributes that should remain consistent with the original portrait. This is because fine-tuning based solely on diffusion loss focuses on implicitly modeling the joint distribution of data, rather than capturing the independence of semantic attributes. This leads to the coupling of target semantics with redundant features from fine-tuning dataset (Semantic Pollution) and the erroneous expansion of target response area, ultimately disrupting the original behavior. To address these issues, we design dual-path pipeline that leverages contrastive learning [20, 47] to eliminate semantic pollution. As shown in Fig. 3, it first splits the complete prompts into two parts: the base text for original performance and target text for customized attributes, Figure 3. The Dual-Path Contrastive Learning Pipeline of SPF-Portrait. The text in blue is the Base text, while those in red is the Target text. Reference Path takes only Base text as input, while Response Path takes complete text (Base text & Target text) as input. which are encoded by the text encoder as Ebase and Etar. Specifically, the proposed dual paths including: (i) the reference path is initialized from the parameters of pre-trained model. In contrastive learning, it only takes Ebase as input and remains frozen, serving as stable reference on behalf of the original models performance; and (ii) the response path is initially fine-tuned with conventional method [25] to gain the ability to respond to customized attributes. During contrastive learning, it takes complete text as input including both Ebase and Etar, with only cross-attention layers trainable. By contrastive learning between dual paths, we specifically design Semantic-Aware Fine Control alignment process to maintain the original models behavior and an response enhancement mechanism for target semantics. 3.3. Semantic-Aware Fine Control Alignment In this section, we provide detailed presentation of our novel Semantic-Aware Fine Control alignment process. This process first extracts the attention features Fref and Fres from the reference path and response path. They represent the response of the UNet features Qref and Qres to the base textual features Ebase, where Qref and Qres are features from the corresponding UNets cross-attention layer in the contrastive paths. The attention features are represented using variant of the attention mechanism, i.e., Attention (K, Q, Q). By constraining the similarity between the attention features Fref and Fres from each cross-attention layer, this process encourages the representation of the base text in the response path to approach the behavior of the original model as: Fref = Softmax( Fres = Softmax( Ks Ltext-consistent = (cid:80)L j=1 Kref (Ebase) QT res(Ebase) QT res (cid:13) (cid:13)F (cid:13) ref res )Qres, (cid:13) (cid:13) (cid:13)2 , ref )Qref , (3) where Kref (Ebase) denotes the key of Ebase in reference path, res(Ebase) = Kres([Ebase, Etar])Ebase indicates the selected keys of Ebase in response path. denotes the attention layer number of the denoising model. To enhance consistency in fine-grained content, we further constrain the UNet features from contrastive paths, which contain comprehensive local detail information and global structure [10, 47]. This is formulated as: Lfine-grained = (cid:88) j=1 (cid:13) (cid:13)Qj (cid:13) ref Qj res (cid:13) (cid:13) (cid:13)2 . (4) Although such contrastive alignment effectively prevents the disruption of the original model in reference imagethis based customization tasks, similar to PuLID [20], vanilla alignment of intermediate features in text-driven customization suppresses the response intensity of target attributes, as shown in Fig. 4 (a). This causes the results to overly align with the original portrait. The fundamental distinction lies in the learning objectives. As shown in Fig. 4 (b), since the reference image is inherently decoupled from text and represents more concrete condition, it allows the model to better capture information and distinguish their response regions, thereby reducing the effect of alignment on the response. In contrast, the semantic boundaries between textual concepts are ambiguous, leading to the representation capabilities of CLIP to calculate the similarity between Etar and each Ebase[i]. We then weight the Abase[i] based on the similarity, which used to refine the soft map M, as expressed below: (cid:99)M = (cid:88) i=1 Abase[i] (1 γ(i)), (7) γ(i) =DCLIP (Ebase[i], Etar), where DCLIP represent the cosine similarity in CLIP embedding space. (cid:99)M is our final SFCM, as shown in Fig. 3 and Fig. 4 (a), it represents the precise target response regions and effectively prevents over-alignment by guiding the alignment process. Therefore, the alignment constraints in Eq. 3 and Eq. 4 can be modified as follow: LM tex = (cid:80)L LM fine = (cid:80)L j=1 j=1 (cid:13) (cid:13)(F (cid:13) (cid:13) (cid:13)(Qj (cid:13) ori ori Qj (cid:13) (cid:13) t) (1 (cid:99)M) (cid:13)2 (cid:13) (cid:13) t) (1 (cid:99)M) (cid:13)2 , , (8) where denotes the hadamard product. 3.4. Response Enhancement via Difference Vectors For text-driven portrait customization, an effective response of the target attributes is essential for success. Therefore, to improve the models comprehension of the target semantics, we devise response enhancement mechanism to bolster the performance of the target attribute. Specifically, we introduce difference vector , represented by the difference between the vectors of the CLIP textual space and the CLIP visual space. By incorporating the ground truth image x0, we separately calculate the difference vector (x0, Etar) between the target text and ground truth image x0, as well as the difference vector (ˆx0, Etar) between the target text and the one-step prediction ˆx0, formulated as: (ˆx0, Etar) = EI (ˆx0) ET (Etar), (x0, Etar) = EI (x0) ET (Etar), ˆx0 = (cid:98)xt αt 1 αtϵθ((cid:98)xt, t, τ (Ebase, Etar)) αt , (9) where the EI and ET denote the CLIP vision and text encoder, respectively, while ˆx0 denotes the one-step prediction of xt in t-th timestep. Then, we constrain their similarity to enhance the response of the target semantics as: Lenhanced = 1DCLIP ((ˆx0,Etar), (x0,Etar)). (10) Unlike previous work [2, 34] that directly applies crossmodal supervision in CLIP space by employing the target text to supervise the one-step prediction, formulated as: Lclip =1 DCLIP (EI (ˆx0) ET (Etar)), (11) 5 Figure 4. Analysis of Alignment Process. (a) Vanilla aligning results in the over-alignment with original portrait. (b) For the same customization goal, reference image fine-tuning offers more distinct target response region than T2I fine-tuning. the inhibition of target attributes during the alignment of other text-conditional attributes. To address this more challenging issue, we propose Semantic-Aware Fine Control Map (SFCM) which spatially guides the alignment process to be implemented on the appropriate regions, minimizing its impact on the target response. Specifically, during alignment training, the spatial difference in noise predictions between contrastive paths can serve as prior knowledge for target response, forming soft map as: M(Ebase, Etar, xt, t) = ϵθ(xt, t, Ebase) ϵθ (xt, t, [Ebase, Etar]), (5) where the ϵθ and ϵθ represent the prediction in both response and reference paths, respectively, while θ denoting the learnable parameters. As previously analyzed, Semantic Pollution causes the target response regions to diffuse into areas of other attributes, making the noise difference unable to precisely characterize the target reInspired by the insight that if phrase sponse regions. in base text exhibits low semantic relevance to target text, the regions highlighted by this phrase should be excluded from the M, we design the Semantic-Aware process to refine the soft map. Concretely, for each phrase feature Ebase[i], = {1, 2, , }and is the total number of phrase in base text, we compute its mean of the crossattention maps across all the UNet layers to localize highlighted regions Abase[i] as: Abase[i] ="
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) j=1 (Aj base[i]), (6) where Aj base[i] represents the attention map of the i-th phrase embedding Ebase[i] from the j-th layer. Subsequently, to quantify the relevance of exclusion, we leverage ior, (2) responsiveness to target semantics, and (3) overall image quality. Concretely, we employ FID [24], LPIPS [72], identity similarity (ID), CLIP Image Score (CLIP-I) [53], and segmentation consistency (Seg-Cons) to measure the consistency between the original and customized portrait. We utilize the CLIP Score (CLIP-T) [53] to evaluate responsiveness to target semantics. For comprehensive image quality assessment, we use HPSv2 [65] and MPS [73]. 4.2. Qualitative Evaluation Comparison with SOTAs. We conduct qualitative comparison of our method with the state-of-the-art (SOTA) approaches, including PEFT-based methods such as LoRA [26] and AdaLoRA [71], decoupled text embedding methods like TokenCompose [64] and Magnet [74], as well as naive fine-tuning. We compare with them on diverse customized attributes, such as age, image style, and clothing. For each target attribute, we evaluate two cases under different random seeds. As shown in Fig. 6, although LoRA [26] and AdaLoRA [71] tend to retain original performance in some cases, their performance is extremely unstable and poor in detail alignment. For instance, in row 3, column 3, there is noticeable change in identity, whereas in row 4, column 2, the pose of portrait has transformed completely. Magnet [74] and TokenCompose [64] naively follow the input text conditions entirely, ignoring the preservation of the original models behavior across all test cases. For example, in row 6&7, column 9, the customization of pencil drawing style results in total alteration of the portrait. In contrast, our method purely customizes target attributes while preserving the original models behavior in aspects such as background, pose, and identity. It demonstrates our approach effectively mitigates semantic pollution during fine-tuning. Please refer to Appendix for more results. 4.3. Quantitative Evaluation Metric Evaluation. Tab. 1 shows the quantitative results of our methods against baselines on the test set. Our method demonstrates substantial improvement in preservation of the original performance compared to all competitors, achieving state-of-the-art performance across all metrics. Its notable that our method significantly outperforms competitors in Seg-Cons, demonstrating pixel-level alignment precision that far surpasses other approaches. The optimal CLIP-T and overall scores confirm that our method enhances the response to target semantics and achieves higherquality portrait customization. User Study. We also conduct user study to have comprehensive assessment of our method. We design three dimensions for evaluation: Original Behavior Consistency (OBC), Target Attribute Responsiveness (TAR), and Aesthetic Preference (AP). We invite 32 participants from different social backgrounds, with each test session lasting Figure 5. Comparison with Traditional Supervision on Image Fidelity. (a) illustrates the trend of Image-Reward (IR) and CLIP Score (CLIP-T) across fine-tuning steps. Image-Reward [66] is metric used to evaluate image fidelity. (b) displays samples from traditional method [2] and ours. our approach reformulates the optimization objective into difference vectors rather than image-text similarity in Eq. 11. Directly cross-modal supervision overlooks the modality representation gap, causing the model to overfit the textual description during optimization and neglecting the visual fidelity of the result image. It ultimately leads to degradation in the quality of the generated images, as illustrated in Fig. 5. In contrast, we provide an effect similar to supervision within the same modality by using the difference between cross-modal vectors, mitigating the representation discrepancy inherent in direct cross-modal supervision. It simultaneously enhances the response to target semantics while improving the fidelity and coherence of the image. Finally, the overall optimization objective can be represented as: LSP = Ldif +λ1Lenhacned (cid:125) (cid:124) (cid:123)(cid:122) response +λ2LMtext +λ3LM fine (cid:125) (cid:123)(cid:122) alignment (cid:124) , (12) where λ1, λ2 and λ3 are the hyperparameters. 4. Experiments 4.1. Experimental Setup Implementation Details. We adopt the pre-trained Stable Diffusion v1.5 model [55] with Realistic Vision V4.0 checkpoints. All attention maps and score maps are upsampled at resolution of 64 64. The hyperparameters λ1, λ2 and λ3 are set to 0.6, 0.2 and 0.1. Detailed information about experiments is provided in the Appendix. Dataset. Our training set contains 230K diverse portrait images with novel customized attributes (e.g., skin textures, hairstyles), captioned by GPT-4o and Cambrian-1 [60]. For evaluation, we construct test set of 5K triples, each containing: (1) an original caption, (2) its corresponding original portrait generated using Realistic Vision V4.0, and (3) target caption with customized attributes. Evaluation Metrics. In our task, we aim to evaluate three key aspects: (1) preservation of the original models behav6 Figure 6. Qualitative Comparisons with SOTA methods. We compare ours with naive fine-tuning [55], PEFT-based methods (LoRA [26], AdaLoRA [71] ) and the decoupled methods (Tokencompose [64], Magenet [74]). Please zoom in for more details. Table 1. Quantitative Comparisons Result. Rows with gray background indicate our ablation experiments, while rows without it represent the state-of-the-art (SOTA) methods being compared. In our specific pairwise comparison, unlike general image generation, lower FID values reflect greater consistency with the original models behavior. It is notable that the underlined values in Ours (w/o SFCM) are abnormally low, as the generated portraits exhibit over-alignment with the original portraits."
        },
        {
            "title": "Overall",
            "content": "FID () LPIPS () ID () CLIP-I () Seg-Cons () CLIP-T ( ) HPSv2 () MPS() Naive Fine-Tuning [55] AdaLoRA [71] LoRA (Best Rank) [26] TokenCompose [64] Magnet [74] Ours Ours (w/o Ltextconsistent) Ours (w/o Lf inegrained) Ours (w/o Lenhanced) Ours (w/o SFCM) 20.41 7.38 9.82 10.93 18.92 4.50 4.97 6.74 4.52 4.13 0.57 0.40 0. 0.41 0.48 0.35 0.39 0.42 0.37 0.14 0.21 0.39 0.52 0.32 0.38 0.55 0.48 0.32 0.49 0.73 0.63 0.80 0. 0.81 0.61 0.83 0.60 0.71 0.81 0.88 7 57.77 64.86 58.37 40.22 32.87 75.74 61.39 41.62 74.38 80. 0.24 0.23 0.27 0.27 0.26 0.30 0.28 0.27 0.22 0.17 0.21 0.24 0.23 0.24 0.26 0.28 0.23 0.21 0.23 0. 0.67 1.10 1.21 0.71 0.97 1.49 1.13 1.22 1.40 1.09 Figure 7. Qualitative Ablation Study. We independently ablate the proposed loss and the SFCM mechanism. Figure 8. User Study Results. The percentages indicate the proportion of users who select the corresponding method. about 30 minutes. Users perform pairwise comparisons between our method and competitors across three dimensions. The results are as shown in Fig. 8, our method defeat all competitors in all dimensions, especially in OBC and TAR. This highlights our ability to preserve the original models behavior while purely adapting to new attributes. Please refer to the Appendix for more details about user study. 4.4. Original Capability Retention To further verify that our method purely learns the customized attributes without compromising the original model and attains incremental learning, we solely utilize identical Base text to evaluate whether our method can reconstruct the original portraits after fine-tuning. As shown in Fig. 9, naive fine-tuning markedly disrupts original response patterns, while our method maintains near-identical performance to original model. For example, in the topright case, the semantics of woman is completely corrupted by naive fine-tuning, but we not only preserves the character but also maintains high consistency in other attributes. The outstanding reconstruction of portraits across varied scenes demonstrates our methods substantive retention of the original models intrinsic capabilities. 4.5. Ablation Study To validate the effectiveness of different components of our method, we conduct thorough ablation studies. Qualitative results, shown in Fig. 7, indicate that the absence of Ltextconsistent results in weaker alignment of Base text with the original portrait, while the lack of Lf inegrained leads to inconsistencies in detailed content, such as portrait posture. Without Lenhanced, the expression of the target semantics significantly degrades that fails to follow the acFigure 9. Reconstruction Results. The three portraits for each case are only generated by the same Base text. tion of holding and with tendency to disrupt the spatial coherence of the toy bear, degenerating into flattened textile-like patterns. Quantitative results in ablation part of Tab. 1, further validates the conclusions drawn from the visual analysis through superior performance across all metrics. Notably, although w/o SFCM shows superior Preservation Metrics in Tab. 1, this is due to its complete disregard for target semantics and severe over-alignment with the original portrait, shown in Fig. 7. Such outcomes represent an absolute failure in our task, which is entirely undesirable. 5. Conclusion In this paper, we propose SPF-Portrait, novel fine-tuning framework designed to address the issue of Semantic Pollution in text-driven portrait customization. By introducing original model as reference path and utilizing contrastive learning, we achieve the goals of purely capturing the customized semantics and enabling incremental learning. We precisely preserve the original models behavior and ensure an effective response to target semantics by innovatively designing Semantic-Aware Fine Control to guide the alignment process and response enhancement mechanism for target semantics. Extensive experiments demonstrate our method can achieve the SOTA performance. In the future, we will continue to explore adapting our framework to more broad and complex scenes, striving to achieve semantic pollution-free fine-tuning for general text-to-image and text-to-video generation."
        },
        {
            "title": "References",
            "content": "[1] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit Bermano. Hyperstyle: Stylegan inversion with hyIn Proceedings of pernetworks for real image editing. the IEEE/CVF conference on computer Vision and pattern recognition, pages 1851118521, 2022. 3 [2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1820818218, 2022. 5, 6 [3] Shubhankar Borse, Shreya Kadambi, Nilesh Prasad Pandey, Kartikeya Bhardwaj, Viswanath Ganapathy, Sweta Priyadarshi, Risheek Garrepalli, Rafael Esteves, Munawar Hayat, and Fatih Porikli. Foura: Fourier low rank adaptation. arXiv preprint arXiv:2406.08798, 2024. 2 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 3 [5] Yingshan Chang, Yasi Zhang, Zhiyuan Fang, Ying Nian Wu, Yonatan Bisk, and Feng Gao. Skews in the phenomenon space hinder generalization in text-to-image generation. In European Conference on Computer Vision, pages 422439. Springer, 2024. 2, 3 [6] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):110, 2023. [7] Chieh-Yun Chen, Chiang Tseng, Li-Wu Tsao, and HongHan Shuai. is cat (not dog!): Unraveling information mix-ups in text-to-image encoders through causal analysis and embedding optimization. arXiv preprint arXiv:2410.00321, 2024. 2, cat [8] Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing Liu, Kang Du, et al. Photoverse: Tuning-free image customization with text-to-image diffusion models. arXiv preprint arXiv:2309.05793, 2023. [9] Weifeng Chen, Tao Gu, Yuhao Xu, and Arlene Chen. Magic clothing: Controllable garment-driven image synthesis. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 69396948, 2024. [10] Jiwoo Chung, Sangeek Hyun, and Jae-Pil Heo. Style injection in diffusion: training-free approach for adapting largescale diffusion models for style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87958805, 2024. 4 [11] Dawei Dai, YuTang Li, YingGe Liu, Mingming Jia, Zhang YuanHui, and Guoyin Wang. 15m multimodal facial imagetext dataset. arXiv preprint arXiv:2407.08515, 2024. [12] Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and Daniel Cohen-Or. Turboedit: Text-based image editing using few-step diffusion models. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. 3 [13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [14] Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and Maosong Sun. Sparse low-rank adaptation of pre-trained language models. arXiv preprint arXiv:2311.11696, 2023. 2 [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [16] Muhammad Fawi. Curlora: Stable llm continual finetuning and catastrophic forgetting mitigation. arXiv preprint arXiv:2408.14572, 2024. 2 [17] Yarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel Cohen-Or. Implicit style-content separation using b-lora. In European Conference on Computer Vision, pages 181198. Springer, 2024. [18] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [19] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36, 2024. [20] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, Peng Pulid: Pure and lightning id arXiv preprint Zhang, and Qian He. customization via contrastive alignment. arXiv:2404.16022, 2024. 3, 4 [21] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact paramIn Proceedings of the eter space for diffusion fine-tuning. IEEE/CVF International Conference on Computer Vision, pages 73237334, 2023. 2 [22] Yue Han, Junwei Zhu, Keke He, Xu Chen, Yanhao Ge, Wei Li, Xiangtai Li, Jiangning Zhang, Chengjie Wang, and Yong Liu. Face-adapter for pre-trained diffusion models with finegrained id and attribute control. In European Conference on Computer Vision, pages 2036. Springer, 2024. 2 [23] Xilin He, Cheng Luo, Xiaole Xian, Bing Li, Siyang Song, Muhammad Haris Khan, Weicheng Xie, Linlin Shen, and Zongyuan Ge. Synfer: Towards boosting facial exarXiv preprint pression recognition with synthetic data. arXiv:2410.09865, 2024. [24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3, 4 [26] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 9 Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, 6, 7 [27] Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom: Narrowing real text word for real-time open-domain text-to-image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74767485, 2024. 2, [28] Ziqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-modal face generation In Proceedings of the IEEE/CVF Conference and editing. on Computer Vision and Pattern Recognition, pages 6080 6090, 2023. 2 [29] Liyao Jiang, Negar Hassanpour, Mohammad Salameh, Mohan Sai Singamsetti, Fengyu Sun, Wei Lu, and Di Niu. Frap: Faithful and realistic text-to-image generation with adaptive prompt weighting. arXiv preprint arXiv:2408.11706, 2024. 2 [30] Qiaoqiao Jin, Xuanhong Chen, Meiguang Jin, Ying Chen, Rui Shi, Yucheng Zheng, Yupeng Zhu, and Bingbing Ni. Toward tiny and high-quality facial makeup with data amplify learning. In European Conference on Computer Vision, pages 340356. Springer, 2025. [31] Quentin Jodelet, Xin Liu, Yin Jun Phua, and Tsuyoshi Murata. Class-incremental learning using diffusion model for In Proceedings of the IEEE/CVF distillation and replay. International Conference on Computer Vision, pages 3425 3433, 2023. 2 [32] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. In European Conference on Computer Vision, pages 150168. Springer, 2024. 3 [33] Chanran Kim, Jeongin Lee, Shichang Joung, Bongmo Kim, Instantfamily: Masked attention arXiv preprint and Yeul-Min Baek. for zero-shot multi-id image generation. arXiv:2404.19427, 2024. 2 [34] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24262435, 2022. 3, 5 [35] Suhas Kotha, Jacob Mitchell Springer, and Aditi RaghuUnderstanding catastrophic forgetting in lanarXiv preprint inference. nathan. guage models via implicit arXiv:2309.10105, 2023. 2 [36] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have semantic latent space. arXiv preprint arXiv:2210.10960, 2022. 3 [37] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2 [38] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Controlnet++: Improving conditional controls with efficient consistency In European Conference on Computer Vision, feedback. pages 129147. Springer, 2025. [39] Xinghui Li, Qichao Sun, Pengze Zhang, Fulong Ye, Zhichao Liao, Wanquan Feng, Songtao Zhao, and Qian He. Anydressing: Customizable multi-garment virtual dressing via latent diffusion models. arXiv preprint arXiv:2412.04146, 2024. 2 [40] Yudong Li, Xianxu Hou, Zheng Dezhi, Linlin Shen, and Zhe Zhao. Flip-80m: 80 million visual-linguistic pairs for facial language-image pre-training. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 5867, 2024. [41] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86408650, 2024. 2 [42] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation. lightning: arXiv preprint arXiv:2402.13929, 2024. 2 [43] Luping Liu, Chao Du, Tianyu Pang, Zehan Wang, ChongxImproving long-text alignarXiv preprint uan Li, and Dong Xu. ment for text-to-image diffusion models. arXiv:2410.11817, 2024. 2 [44] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Parameter-efficient orthogoHeo, Songyou Peng, et al. arXiv preprint nal finetuning via butterfly factorization. arXiv:2311.06243, 2023. 2 [45] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. [46] Oscar Manas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, and Michal Drozdzal. Improving textto-image consistency via automatic prompt optimization. arXiv preprint arXiv:2403.17804, 2024. [47] Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, and Bolei Zhou. Freecontrol: Training-free spatial control of any text-to-image diffusion model with any condition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74657475, 2024. 2, 3, 4 [48] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [49] Maya Okawa, Ekdeep Lubana, Robert Dick, and Hidenori Tanaka. Compositional abilities emerge multiplicatively: Exploring diffusion models on synthetic task. Advances in Neural Information Processing Systems, 36:5017350195, 2023. 2 [50] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [51] Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, and Yongdong Zhang. Deadiff: An efficient stylization diffusion model with disentangled representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86938702, 2024. 3 [52] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Scholkopf. Controlling text-to-image diffusion by orthogonal finetuning. Advances in Neural Information Processing Systems, 36:7932079362, 2023. 2 [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 6 [54] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 2 [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 6, 7 [56] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 2, 3 [57] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2, [58] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. and Stefano Ermon. arXiv preprint [59] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [60] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 6 [61] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 3 [62] Fangyikang Wang, Hubery Yin, Yuejiang Dong, Huminhao Zhu, Chao Zhang, Hanbin Zhao, Hui Qian, and Chen Li. Belm: Bidirectional explicit linear multi-step sampler arXiv preprint for exact inversion in diffusion models. arXiv:2410.07273, 2024. 3 [63] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot 11 identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 2 [64] Zirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, and Zhuowen Tu. Tokencompose: Text-to-image diffusion with In Proceedings of the IEEE/CVF token-level supervision. Conference on Computer Vision and Pattern Recognition, pages 85538564, 2024. 2, 6, [65] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 6 [66] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image In Proceedings of the 37th International Congeneration. ference on Neural Information Processing Systems, pages 1590315935, 2023. 6 [67] Xilie Xu, Jingfeng Zhang, and Mohan Kankanhalli. Autolora: parameter-free automated robust fine-tuning framework. arXiv preprint arXiv:2310.01818, 2023. [68] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. [69] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language models. arXiv preprint arXiv:2309.10313, 2023. 2 [70] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2 [71] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, Adalora: Adaptive budget allocaand Tuo Zhao. arXiv preprint tion for parameter-efficient fine-tuning. arXiv:2303.10512, 2023. 2, 6, [72] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 [73] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning multidimensional human preference for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 80188027, 2024. 6 [74] Chenyi Zhuang, Ying Hu, and Pan Gao. Magnet: We never know how text-to-image diffusion models work, until we learn how vision-language models function. arXiv preprint arXiv:2409.19967, 2024. 2, 3, 6,"
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "Shenzhen University",
        "Tsinghua University"
    ]
}