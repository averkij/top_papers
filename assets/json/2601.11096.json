{
    "paper_title": "CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation",
    "authors": [
        "Shuai Tan",
        "Biao Gong",
        "Ke Ma",
        "Yutong Feng",
        "Qiyuan Zhang",
        "Yan Wang",
        "Yujun Shen",
        "Hengshuang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Character image animation is gaining significant importance across various domains, driven by the demand for robust and flexible multi-subject rendering. While existing methods excel in single-person animation, they struggle to handle arbitrary subject counts, diverse character types, and spatial misalignment between the reference image and the driving poses. We attribute these limitations to an overly rigid spatial binding that forces strict pixel-wise alignment between the pose and reference, and an inability to consistently rebind motion to intended subjects. To address these challenges, we propose CoDance, a novel Unbind-Rebind framework that enables the animation of arbitrary subject counts, types, and spatial configurations conditioned on a single, potentially misaligned pose sequence. Specifically, the Unbind module employs a novel pose shift encoder to break the rigid spatial binding between the pose and the reference by introducing stochastic perturbations to both poses and their latent features, thereby compelling the model to learn a location-agnostic motion representation. To ensure precise control and subject association, we then devise a Rebind module, leveraging semantic guidance from text prompts and spatial guidance from subject masks to direct the learned motion to intended characters. Furthermore, to facilitate comprehensive evaluation, we introduce a new multi-subject CoDanceBench. Extensive experiments on CoDanceBench and existing datasets show that CoDance achieves SOTA performance, exhibiting remarkable generalization across diverse subjects and spatial layouts. The code and weights will be open-sourced."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 1 ] . [ 1 6 9 0 1 1 . 1 0 6 2 : r CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation Shuai Tan1,2*, Biao Gong2, Ke Ma3, Yutong Feng4, Qiyuan Zhang2, Yan Wang5, Yujun Shen2, Hengshuang Zhao1 1The University of Hong Kong 2Ant Group 3Huazhong University of Science and Technology 4Tsinghua University 5University of North Carolina at Chapel Hill Figure 1. Multi-subject animations generated by CoDance. Given single (potentially misaligned) driving pose sequence and one multi-subject reference image, CoDance produces coordinated, pose-controllable group dances without per-subject spatial alignment."
        },
        {
            "title": "Abstract",
            "content": "Character image animation is gaining significant importance across various domains, driven by the demand for robust and flexible multi-subject rendering. While existing methods excel in single-person animation, they struggle to handle arbitrary subject counts, diverse character types, and spatial misalignment between the reference image and the driving poses. We attribute these limitations to an overly rigid spatial binding that forces strict pixel-wise alignment between the pose and reference, and an inability to consistently rebind motion to intended subjects. To address these challenges, we propose CoDance, novel Unbind-Rebind *Work done during internship at Ant Group. Project lead. Corresponding author. framework that enables the animation of arbitrary subject counts, types, and spatial configurations conditioned on single, potentially misaligned pose sequence. Specifically, the Unbind module employs novel pose shift encoder to break the rigid spatial binding between the pose and the reference by introducing stochastic perturbations to both poses and their latent features, thereby compelling the model to learn location-agnostic motion representation. To ensure precise control and subject association, we then devise Rebind module, leveraging semantic guidance from text prompts and spatial guidance from subject masks to direct the learned motion to intended characters. Furthermore, to facilitate comprehensive evaluation, we introduce new multi-subject CoDanceBench. Extensive experiments on CoDanceBench and existing datasets Figure 2. The illustration of CoDance motivation. Although excelling at single-person animation, prior methods fail when handling multiple subjects due to rigid binding between the reference and target pose, which results in mismatched outputs. By contrast, our Unbind-Rebind method successfully decouples motion from appearance, yielding compelling results. show that CoDance achieves SOTA performance, exhibiting remarkable generalization across diverse subjects and spatial layouts. The code and weights will be open-sourced. Project page: https://lucaria-academy.github.io/CoDance/ 1. Introduction Character Image Animation aims to generate lifelike, highfidelity videos from reference image and target pose sequence [13]. Compared to the well-studied field of singleperson animation, multi-subject (2) scenarios [71] are more prevalent and valuable in practice, such as ensemble performance, and wide range of applications including advertising and educational content creation [8, 55, 70]. As shown in Fig. 2, although recent methods [30, 54 56, 62, 64, 79] have achieved impressive progress in singleperson animation, most of them still struggle to handle multi-subject scenarios [76], leading to the generation of spurious or entangled subjects. Consequently, achieving robust character animation under arbitrary subject counts and spatial layouts remains pressing problem. Recently, there have been promising efforts [71, 76] toward two-person animation. For instance, Follow-YourPose-V2 [71] operates in an image-to-video paradigm, leveraging multi-branch control modules to generate multianimation videos. Despite the success in two-person cases, they still face three key limitations in general settings: (1) Limited scalability in subject count. The rigid pose-conditioned binding constrains the control and fusion pipeline, preventing extension beyond two subjects. (2) Positional sensitivity. They impose stringent constraints on initial subject locations, requiring perfect spatial alignment between the reference image and target pose. Mild misalignment often leads to failure induced by over-forcing alignment. (3) Restricted subject types. Current methods are typically optimized for real humans and generalize poorly to subjects that deviate from humans (e.g., anthropomorphic cartoons) [51]. These limitations severely constrain the practical applicability of current methods. We attribute these issues primarily to two factors: (1) as illustrated in Fig. 2 (b), an excessively rigid spatial binding between the pose skeleton and the reference subjects, which forces strict pixel-wise alignment; and (2) the lack of an explicit focus mechanism (e.g. Rebind in Fig. 2 (d)) to reliably isolate and animate the intended target. Collectively, previous methods fail to infer the desired motion from target poses and to correctly localize the corresponding subjects in the reference image. To address these issues, we present CoDance, which animates arbitrary subject types, counts, and spatial positions, given single pose sequence that is not spatially aligned with the reference image. Our key insight is twofold: (1) improve the flexibility of model in understanding motion semantics rather than equating poses with pixel-wise alignment; and (2) to equip the model with autonomous alignment and explicit focus mechanisms, enabling it to accurately localize and animate the intended subjects even under spatial misalignment. These insights motivate our Unbind and Rebind paradigm, respectively. Concretely, as presented in Fig. 2 (d), we begin with Unbind: during training, we apply scale and shift perturbations to both the explicit pose skeletons and their latent features, deliberately weakening the strong spatial binding between pose and reference image while preserving motion semantics. This suppresses the pixel-alignment shortcut and encourages the model to learn subject-agnostic motion semantics and temporal coherence, thereby improving robustness to poseinduced layout shifts and enhancing flexibility in motion understanding. However, while unbinding relaxes the strict spatial coupling between the pose and the reference image, enabling the model to grasp motion semantics, it leaves the model unable to localize the subjects that should be animated. Therefore, we introduce Rebind, which restores precise control from both semantic and spatial perspectives. Semantically, we add text branch [34] that explicitly specifies the identity and number of subjects to be animated. To mitigate the scarcity and limited diversity of textual labels in animation datasets, we propose mixed-data training strategy that involves jointly training with large, readily available text-to-video data to strengthen textual understanding and semantic binding. Spatially, we employ an offline segmentation model (e.g., SAM [36]) on the reference image to obtain high-quality subject masks, which serve as external conditions to rebind pose and target subjects in space and explicitly confine the action region. Through this UnbindRebind mechanism, the model acquires both elastic motion semantics and accurate target localization under misalignment. We further construct multi-subject benchmark CoDanceBench to comprehensively evaluate our approach. Experiments show consistent improvements across multiple quantitative and perceptual metrics, validating the effectiveness of CoDance. Our contributions are summarized as follows: We propose CoDance, which produces animations with arbitrary subject types, arbitrary counts, arbitrary spatial positions, and arbitrary poses, given single pose sequence that is not aligned with the reference image. To the best of our knowledge, CoDance is the first approach to simultaneously achieve all four arbitrary properties in character image animation. Our Unbind-Rebind strategy systematically decouples pose from overly rigid spatial bindings in the reference image and rebinds control through semantic and spatial cues, thereby achieving robustness under misalignment. On our proposed CoDanceBench and current FollowYour-Pose-V2 benchmark [71], CoDance achieves SOTA performance across metrics and shows strong generalization to diverse subjects and spatial layouts. 2. Related Work 2.1. Diffusion for Video Generation Diffusion models [10, 42] have become the dominant engine for video synthesis [1, 3, 6, 8, 33, 38, 53, 66, 69, 73, 74, 81] thanks to their breadth of generative coverage and strong visual fidelity. Compared with images [17 19, 22, 24, 25, 31, 35, 68, 72, 77], videos add trajectory constraint, where frames must evolve smoothly and remain temporally consistent while preserving per-frame quality. Two families of approaches have emerged. One retrofits powerful text-to-image backbones with lightweight temporal adapters [41, 59, 63], such as temporal attention blocks [8] or 3D convolutions [4], to couple appearance and motion without retraining from scratch. The other [11, 20, 45] abandons the U-Net in favor of transformer [29] diffusion backbones, which scale more gracefully to long sequences and complex dynamics. Wan2.1/2.2 [58] is an open Diffusion-Transformer, emphasizing long-horizon coherence and efficient inference across T2V/I2V/TI2V. Inspired by its strong empirical motion fidelity, we adopt Wan 2.1 as our backbone. 2.2. Character Image Animation Character image animation targets motion transfer from driving source to target identity [5, 23, 32, 52, 61, 79], and the field has evolved markedly in both fidelity and breadth. Early approaches [15, 16, 21, 2628, 39, 40, 43, 44, 46 49, 80] relied on GAN-based pipelines, which can render plausible frames yet often suffer from texture distortion, identity drift, and temporal flicker under challenging motions. The diffusion era shifted the focus to stronger priors and controllable conditioning [37, 67, 82]. Disco [60] couples diffusion with ControlNet-style [77] keypoint guidance to drive human dance; MagicAnimate [70] and Animate Anyone [13] introduce transformer-like temporal attention to enforce cross-frame coherence and suppress jitter; UniAnimate [62] leverages Mamba [7] for efficient longrange temporal dependencies; StableAnimator [55, 56] strengthens identity retention via dedicated face encoder and ID-aware adaptation for high-quality human animation; and Animate-X [50, 51] enhances motion representation to support animation of wide variety of characters. Despite these advances, most methods are architected for single subject, which is difficult to extend to multi-subject scenes. [71] introduces an image-to-video pipeline with implicit decoupling, using optical-flow, depth-order, and referencepose guiders to stabilize backgrounds and handle occlusions, achieving strong results mainly in two-subject cases. [76], in text-to-video setting, employs region masks, role-wise prompt decomposition, spatially aligned crossattention, and multi-branch control to enable two-character generation without tuning. However, both struggle to generalize beyond two subjects, limited by rigid poseimage binding that does not scale gracefully. By contrast, our UnbindRebind strategy first relaxes spatial couplings to learn motion semantics, then re-introduces semantic and spatial bindings, thereby naturally extending to arbitrary subject counts and more complex layouts. 3. Methodology As shown in Fig. 3, this paper focuses on multi-character animation. Given reference image r, pose sequence Figure 3. The pipeline of CoDance. Given reference image r, driving pose sequence 1:F , text prompt , and subject mask M, our model generates an animation video from r. The Unbind module, implemented 1:F . VAE encoder extracts the latent feature as Pose Shift Encoder, processes 1:F to produce pose features. These are concatenated with patchified tokens from the noisy latent input for the DiT backbone. The Rebind module provides dual guidance: semantic features from umT5 text encoder are injected via cross-attention, while spatial features from Mask Encoder are added element-wise to the noisy latent. To bolster the models semantic comprehension, the training process alternates between animation data (with probability pani) and diverse text-to-video dataset (with probability 1 pani). The DiT is initialized from pretrained T2V model and fine-tuned using LoRA. Finally, VAE decoder reconstructs the video. Note that the Unbind module and mixed-data training are applied exclusively during the training phase. 1:F and text prompt , we obtain subject masks for the reference image and propagate the driving motion to arbitrarily many subjects with diverse types, while maintaining identity consistency with the reference. Different from prior works focused on near-aligned inputs, we explicitly handle misalignment between 1:F and r, non-human / anthropomorphic characters, and multi-subject scenes. 3.1. Preliminaries Diffusion Models. Diffusion Models [10, 42] are generative models that learn to create data by reversing noiseadding process. This involves two stages: forward process that gradually adds Gaussian noise to clean data x0, and reverse process that learns to remove it. key property is that any noisy sample xt can be directly obtained from x0. The generative task is accomplished by training network ϵθ(xt, t, c) to predict the noise ϵ from the noisy input xt, given timestep and optional conditions c. The model is optimized with objective: Lsimple = Et,x0,ϵ (cid:104) ϵ ϵθ(xt, t, c)2(cid:105) (1) For conditional generation, classifier-free guidance is often used to strengthen the influence of the condition c. Diffusion Transformers (DiT). While early diffusion models used U-Net architectures, Diffusion Transformers (DiT) [29] demonstrated that standard Transformer can serve as highly effective and scalable backbone. In the DiT framework, the input image is first divided into nonoverlapping patches, similar to Vision Transformer (ViT). These patches, along with embeddings for the timestep and conditions (e.g., pose skeletons), are converted into sequence of tokens. This token sequence is then processed by the Transformer blocks to predict the output noise. 3.2. Unbind-Rebind As illustrated in Fig. 2, previous methods typically enforce rigid spatial binding between the reference image and the target pose. This paradigm can generate correct results in single-person animation, provided that the human-like reference image and target pose are spatially aligned. However, they are limited to mismatch cases, such as different number of subjects in the reference image compared to the target pose. Due to its reliance on rigid spatial alignment, the model cannot correctly animate the subjects from the it hallucinates new, posereference image. aligned person in the corresponding spatial region. To overcome this fundamental limitation, we propose new Instead, paradigm: Unbind-Rebind, which breaks the forced spatial alignment from mismatched inputs and re-establishes the correct correspondence between motion and identity. Unbind. The Unbind module dismantles this rigid spatial constraint between the reference image and the pose. Instead of relying on simple spatial mapping, we compel our model, specifically the pose encoder and the diffusion network, to learn an abstract, semantic understanding of the motion itself. To this end, we propose novel pose shift encoder, composed of Pose Unbind and Feature Unbind modules, which enhance the models understanding at both the input and feature levels. The key insight is to deliberately and stochastically disrupt the natural alignment between the reference image and the target pose 1:F during each training step, which ensures the model cannot rely on rigid spatial correspondence. Specifically, our In each Pose Unbind module operates at the input level. training step, we first sample reference image and its corresponding driving pose following previous works. However, instead of feeding this pair directly to the model, we apply series of transformations to the driving pose p. The most intuitive way to break the spatial association is by altering the poses position and scale. Therefore, at each step, we randomly translate the skeletons position, i.e., [x, y], and randomly scale its size, further decoupling it from its original spatial location. However, Pose Unbind alone primarily strengthens the pose encoders ability to interpret pose variations. The core generative process heavily relies on the diffusion network. To this end, we introduce the Feature Unbind module, which operates on the feature level. After the transformed pose passes through the pose encoder, we apply further augmentations to the resulting pose features. First, we apply similar random translation. Furthermore, to force the diffusion model to adapt to various pose configurations within the feature space, we extract the feature region corresponding to the pose, randomly duplicate it, and superimpose these duplicates onto the original feature map. This process compels the diffusion model to develop more robust semantic understanding of pose and enhances its generative capability under complex conditions. Rebind. Following the Unbind operation, while the model is able to grasp the semantic meaning of the motion from the pose images, it lacks the crucial information specifying the target subject for animation, as the original spatial alignment has been deliberately broken. To address this, we introduce the Rebind module, which intelligently reassociates this understood motion with the correct subjects in the reference image. Specifically, we perform Rebind through two complementary aspects: semantic and spatial. From semantic perspective, we introduce text-driven guidance branch that leverages an input text prompt to explicitly specify the identity and number of subjects to be animated from the reference image. As shown in Fig. 3, the reference image contains multiple elements, including five anthropomorphic characters targeted for animation. corresponding prompt, such as Five bubbles are dancing, is then processed by text encoder and fed into the DiT blocks to provide semantic guidance. However, training exclusively on animation datasets (Dani) with uniform textual prompts presents significant challenge: the model tends to overfit to the prompt, learning spurious correlation and ignoring the textual guidance, which severely impairs its generalization ability during inference. To counteract this, we propose mixed-data training strategy. We incorporate an auxiliary, diverse text-image-to-video (TI2V) dataset (Dti2v) and alternate between character animation and T2V tasks with probabilities pani and 1 pani, respectively. This dual-objective training compels the model to move beyond simple pattern matching and develop robust understanding of text conditioning. This, in turn, enables it to accurately rebind specified subjects from the reference image based on arbitrary text prompts during inference. Semantic guidance, while powerful, does not resolve the challenge of figure-ground ambiguity, particularly for subjects with complex or unconventional morphologies. This ambiguity can cause the model to fail at accurate subject segmentation, leading to outputs where background is mistakenly animated or parts of the subject are omitted. To enforce precise spatial control, we introduce spatial rebind mechanism, which provides reference mask that explicitly defines the animation region. This direct spatial rebind ensures the animation is strictly confined to the specified boundaries, effectively mitigating segmentation errors and preserving the subjects structural integrity. 3.3. Framework and Implement Details this latent In light of the success of previous works [58, 64], CoDance is based on the commonly used Diffusion Transformer (DiT) [29]. As shown in Fig. 3, given reference image r, we employ the VAE encoder to extract its latent representation is then Following [64], . directly used as part of the input for the denoising network ϵθ. To facilitate precise appearance rebind, we utilize pretrained segmentation model (e.g., SAM) to extract corresponding subject mask from r. This mask is further fed into Mask Encoder, consisting of stacked 2D convolutional layers. The resulting mask features are then incorporated by element-wise summation with the noisy latent vector. Concurrently, we introduce umT5 Encoder [34] for semantic understanding. The text features are integrated into the generative process via the crossattention layers within the DiT blocks. For the driving video 1:F , we adopt the introduced Pose Shift Encoder, Method AnimateAnyone [13](CVPR24) MusePose [54] (ArXiv24) ControlNeXt [30] (ArXiv24) MimicMotion [79] (ICML25) UniAnimate [62] (SCIS25) Animate-X [51] (ICLR25) StableAnimator [55] (CVPR25) UniAnimateDiT [64] (ArXiv25) CoDance LPIPS PSNR SSIM 0.867 22.08 0.875 22.67 0.806 19.43 0.782 17.99 0.837 21.32 0.884 24.00 0.869 20.92 0.896 23.59 0.896 25.76 0.183 0.187 0.257 0.292 0.183 0.162 0.214 0.159 0.153 L1 3.15 E5 3.01 E5 4.52 E5 5.20 E5 2.88 E5 2.65 E5 3.98 E5 2.59 E5 2.52 E5 FID 38.30 48.25 73.92 91.05 35.93 41.67 123.94 43.86 38.98 FID-VID 26.58 21.81 41.74 68.86 19.60 14.81 82.41 20.77 11.56 FVD 696.43 489.04 748.70 1304.00 366.90 392.43 947.10 366.19 312. Table 1. Quantitative comparisons with existing methods on Follow-Your-Pose-V2 [71]. Method AnimateAnyone [13](CVPR24) MusePose [54] (ArXiv24) ControlNeXt [30] (ArXiv24) MimicMotion [79] (ICML25) UniAnimate [62] (SCIS25) Animate-X [51] (ICLR25) StableAnimator [55] (CVPR25) UniAnimateDiT [64] (ArXiv25) CoDance LPIPS PSNR SSIM 0.558 11.01 0.587 12.01 0.355 10.46 0.554 11.63 0.582 11.87 0.579 12.06 0.575 12.18 0.588 12.07 0.592 12.21 0.633 0.580 0.652 0.592 0.582 0.580 0.604 0.579 0.580 L1 1.44 E4 1.29 E4 1.52 E4 1.34 E4 1.30 E4 1.28 E4 1.24 E4 1.28 E4 1.24 E4 FID 229.61 217.38 222.16 207.22 207.39 226.66 219.06 205.30 221. FID-VID 187.83 191.43 187.27 182.24 182.27 195.65 196.99 190.79 180.50 FVD 3414.23 2952.49 3625.11 3972.27 3113.85 2795.78 3041.96 2825.82 2494.76 Table 2. Quantitative comparisons with existing methods on CoDanceBench. composed of multiple stacked 3D convolutional layers, to effectively capture both temporal and spatial features of the driving poses. The extracted pose features are concatenated with the patchified image tokens. These conditioned tokens are then fed into our DiT backbone. Leveraging the exceptional text-to-video (T2V) generation capabilities of the pretrained Wan2.1 14B model, we initialize our DiT with its weights and keep them frozen during training. Finetuning is performed exclusively on newly introduced LowRank Adaptation (LoRA) layers integrated within the selfattention and cross-attention blocks. Finally, the denoised latent representation is passed to the VAE decoder to reconstruct the final pixel-level video 1:F . It is important to note that both the Unbind modules and the mixed-data training strategy are applied only during the training phase. They are entirely bypassed during inference, ensuring no additional computational overhead. 4. Experiments 4.1. Experimental Settings Dataset. Our training data is composed of the public TikTok [14] and Fashion [75] datasets, supplemented by approximately 1,200 self-collected TikTok-style videos. During the Rebind stage, this training is augmented with 10,000 text-to-video samples to enhance semantic association and 20 multi-subject videos to supervise spatial binding. For evaluation, we assess our method on both singleand In the single-person context, we multi-subject settings. follow prior works [13, 14, 75] by conducting qualitative and quantitative comparisons on 10 selected videos from TikTok and 100 from the Fashion dataset. For multi-subject animation, we utilize the established Follow-Your-Pose-V2 benchmark [71] and additionally introduce our own curated benchmark, CoDanceBench, which consists of 20 multisubject dance videos. Notably, for the quantitative results reported in this paper, we ensure fair comparison with existing methods by training our model exclusively on solo dance videos, without incorporating the multi-subject data from our proposed CoDanceBench. Evaluation Metrics. We evaluate performance using the metrics defined in Appendix. For frame-level quality, we report PSNR [12], SSIM [65], L1, and LPIPS [78], which are standard indicators of perceptual fidelity and distortion. To assess distributional realism at the video level, we further measure FID [9], FID-VID [2], and FVD [57], capturing the gap between synthesized and real video distributions. 4.2. Experimental Results Quantitative Results. To validate the effectiveness of our proposed method, we compare it with existing SOTA approaches, including AnimateAnyone [13], MusePose [54], ControlNeXt [30], MimicMotion [79], UniAnimate [62], Animate-X [51], StableAnimator [55], and UniAnimateDiT [64]. Although Follow-Your-Pose-V2 [71] and FollowYour-MultiPose [76] address two-person animation, they are excluded from comparison as their code is not available. The evaluated methods are fundamentally designed for Figure 4. Qualitative comparisons with SOTA methods. single-person animation and operate on the assumption of one-to-one correspondence between reference subjects and driving skeletons. To assess their applicability in multisubject contexts, we conduct evaluations on the FollowYour-Pose-V2 [71] dataset and our proposed benchmark. In this setup, the full set of pose skeletons from the driving video is provided as input, forcing these models into multi-agent scenario they are not designed for. As demonstrated in Tab. 1 and Tab. 2, our method significantly outperforms all competing methods across key metrics for perceptual similarity (LPIPS), identity consistency (PSNR/SSIM), and motion fidelity (FID-FVD and FVD). These results underscore the inherent inability of conventional single-person architectures to manage multiagent dynamics, leading to predictable failures such as identity confusion, visual artifacts, and motion distortion. Conversely, our Unbind-Rebind strategy enables our model to preserve distinct identities and generate coherent motion for each character. Critically, this robustness is achieved despite our model being trained under the same singleperson data constraint as the baselines. However, frequent yet challenging scenario arises when multi-subject reference image must be animated by single-person driving pose. This creates significant cardinality mismatch between the visual reference and the motion input. To evaluate model performance under this condition, we devise new protocol using our benchmark. For each case, single skeleton is isolated from the original multi-subject driving video to serve as the sole motion driver. The models are then tasked with animating the multi-subject reference image using only this single-person the generated output is quantitatively pose. Crucially, evaluated against the ground-truth multi-subject video. The detailed results for this challenging setting are provided in the supplementary material. Qualitative Results. Qualitative comparisons are showcased in Fig. 4. To evaluate generalization across varying numbers of subjects, our evaluation spans test cases Method Video Quality Identity Preservation Temporal Consistency AnimateAnyone MusePose MimicMotion ControlNeXt UniAnimate Animate-X StableAnimator UniAnimateDiT CoDance 0.60 0.54 0.65 0.06 0.33 0.01 0.37 0.30 0. 0.79 0.50 0.78 0.29 0.65 0.25 0.50 0.08 0.68 0.62 0.96 0.48 0.38 0.27 0.44 0.90 0.88 0. Table 3. User study results. (3) temporal consistency. As summarized in Tab. 3, our method achieves the highest preference rates across all three criteria, demonstrating its clear perceptual superiority. 4.3. Ablation Study This section presents an ablation study designed to isolate the contribution and necessity of the Unbind and Rebind modules in CoDance. We structure our experiments as progressive ablation: (1) Baseline: We remove both the Unbind and Rebind modules. The model is trained to animate the reference image following rigid alignment paradigm following [64]. (2) + Unbind: On top of the Baseline, we add the Unbind module to break the rigid alignment between the reference image and the driving pose. (3) + Unbind + Spatial Rebind: Building on (2), we incorporate the mask condition to perform spatial rebinding. (4) Full Model. As illustrated in Fig. 5, the Baseline, constrained by rigid alignment, synthesizes novel character spatially aligned with the driving pose, thereby discarding the reference identity. The introduction of the Unbind module rectifies this, preserving the reference identity and demonstrating successful decoupling from rigid alignment. However, it fails to generate coherent motion, indicating an inability to localize the target region for animation. Adding Spatial Rebind resolves this localization issue, animating the correct regions. However, it treats multiple subjects as single composite entity, leading to fragmented animations (e.g., animating hand from each character instead of one characters full body). Finally, the full model, which integrates both Unbind and the complete Rebind mechanism, achieves superior results. This progression validates the crucial and complementary roles of each proposed module in our framework. 5. Conclusion In this paper, we introduce CoDance, novel framework engineered for robust animation across arbitrary subject counts, types, and spatial layouts. We identify that the identity degradation and motion misassignment prevalent in multi-subject scenarios stem from the rigid spatial binding in existing methods. To overcome this, we propose the Unbind-Rebind paradigm, which first unbinds motion from its strict spatial context, and then rebinds this motion to the correct subjects using complementary semantic and spatial guidance. In this way, CoDance demonstrates strong generalization and robustness, achieving flexible multi-subject animation. Our proposed method showcases improvements Figure 5. Qualitative results of ablation study. they fail featuring one to five subjects. In the standard singleperson setting, even minor spatial misalignments between the reference image and the driving pose (e.g., differing aspect ratios or body shapes) cause competing methods to exhibit severe artifacts, such as shape distortion and identity loss. This failure is exacerbated in multi-subject scenarios. Constrained by their rigid binding mechanism (as illustrated in Fig. 2), these methods tend to overlook essential reference information, particularly the identity and number of subjects present in the scene. When driven by single skeleton pose map, to correctly associate motion cues with the intended subjects, which often leads to visually inconsistent or semantically incorrect outputs. Animate-X presents notable exception, where its enhanced motion representation, derived from pose indicator, allows it to better preserve subject shape in singleperson cases. However, it lacks mechanism to rebind this global motion to specific individuals. Consequently, in multi-subject scenes, it treats all subjects as monolithic entity, animating them in unison rather than assigning distinct motions. By contrast, our method successfully preserves the identity of each subject while assigning the correct, corresponding motion. The consistent generation of accurate and coherent results, irrespective of subject count, demonstrates the effectiveness and robustness of our proposed Unbind-Rebind approach. User Study. To quantify the perceptual quality, we conduct comprehensive user study, which involves pairwise A/B preference test administered to 10 participants. diverse set of 20 identities and 20 driving videos is used to generate 20 animations from each of the 9 evaluated methods. In each trial, participants are presented with two side-by-side videos generated by different methods and then asked to select the superior result based on three (1) video quality, (2) identity preservation, and criteria: over SOTA methods, as evidenced by extensive experiments on both the Follow-Your-Pose-V2 benchmark and our newly introduced CoDanceBench. For limitations and future work, please see the Appendix."
        },
        {
            "title": "References",
            "content": "[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. arXiv:2304.08477, 2023. 3 [2] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. Conditional GAN with discriminative filter generation for text-to-video synthesis. In IJCAI, page 2, 2019. 6 [3] Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2video: Video editing using image diffusion. In ICCV, pages 2320623217, 2023. 3 [4] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video editing. In ICCV, pages 2304023050, 2023. 3 [5] Di Chang, Yichun Shi, Quankai Gao, Hongyi Xu, Jessica Fu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. In Forty-first International Conference on Machine Learning, 2023. 3 [6] Biao Gong, Siteng Huang, Yutong Feng, Shiwei Zhang, Yuyuan Li, and Yu Liu. Check locate rectify: trainingfree layout calibration system for text-to-image generation. In CVPR, pages 66246634, 2024. [7] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv:2312.00752, modeling with selective state spaces. 2023. 3 [8] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv:2307.04725, 2023. 2, 3 [9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 30, 2017. 6 [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. 3, 4 [11] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via Transformers. In ICLR, 2023. [12] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition. IEEE, 2010. 6 [13] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv:2311.17117, 2023. 2, 3, 6 [14] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In CVPR, pages 1275312762, 2021. 6 [15] Bin Ji, Ye Pan, Zhimeng Liu, Shuai Tan, Xiaogang Jin, and Xiaokang Yang. Pomp: Physics-consistent motion generative model through phase manifolds. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2269022701, 2025. 3 [16] Bin Ji, Ye Pan, Zhimeng Liu, Shuai Tan, and Xiaokang Yang. Sport: From zero-shot prompts to real-time motion generIEEE Transactions on Visualization and Computer ation. Graphics, 2025. 3 [17] Sihui Ji, Xi Chen, Xin Tao, Pengfei Wan, and Hengshuang Zhao. Physmaster: Mastering physical representation for video generation via reinforcement learning. arXiv preprint arXiv:2510.13809, 2025. [18] Sihui Ji, Xi Chen, Shuai Yang, Xin Tao, Pengfei Wan, and Hengshuang Zhao. Memflow: Flowing adaptive memory for consistent and efficient long video narratives. arXiv preprint arXiv:2512.14699, 2025. [19] Sihui Ji, Hao Luo, Xi Chen, Yuanpeng Tu, Yiyang Wang, and Hengshuang Zhao. Layerflow: unified model for layeraware video generation. In SIGGRAPH, 2025. 3 [20] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3 [21] Yining Li, Chen Huang, and Chen Change Loy. Dense intrinsic appearance flow for human pose transfer. In CVPR, pages 36933702, 2019. 3 [22] Ming Liu, Yuxiang Wei, Xiaohe Wu, Wangmeng Zuo, and Lei Zhang. Survey on leveraging pre-trained generative adversarial networks for image editing and restoration. Science China Information Sciences, 66(5):151101, 2023. 3 [23] Ke Ma, Yizhou Fang, Jean-Baptiste Weibel, Shuai Tan, Xinggang Wang, Yang Xiao, Yi Fang, and Tian Xia. Physliquid: physics-informed dataset for estimating 3d geometry and volume of transparent deformable liquids. arXiv preprint arXiv:2511.11077, 2025. [24] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv:2302.08453, 2023. 3 [25] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In ICML, pages 1678416804, 2022. 3 [26] Ye Pan, Ruisi Zhang, Shengran Cheng, Shuai Tan, Yu Ding, Kenny Mitchell, and Xubo Yang. Emotional voice puppetry. IEEE Transactions on Visualization and Computer Graphics, 29(5):25272535, 2023. 3 [27] Ye Pan, Shuai Tan, Shengran Cheng, Qunfen Lin, Zijiao Zeng, and Kenny Mitchell. Expressive talking avatars. IEEE Transactions on Visualization and Computer Graphics, 2024. [28] Ye Pan, Chang Liu, Sicheng Xu, Shuai Tan, and Jiaolong Yang. Vasa-rig: Audio-driven 3d facial animation with IEEE Transactions livemood dynamics in virtual reality. on Visualization and Computer Graphics, 2025. 3 [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 3, 4, 5 [30] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv:2408.06070, 2024. 2, [31] Zhengyuan Peng, Jinpeng Ma, Zhimin Sun, Ran Yi, Haichuan Song, Xin Tan, and Lizhuang Ma. Mos: Modeling object-scene associations in generalized category discovery. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1511815128, 2025. 3 [32] Zhengyuan Peng, Jianqing Xu, Yuge Huang, Jinkun Hao, Shouhong Ding, Zhizhong Zhang, Xin Tan, and Lizhuang Ma. Stylized-face: million-level stylized face dataset In Proceedings of the IEEE/CVF for face recognition. International Conference on Computer Vision, pages 13053 13064, 2025. 3 [33] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, and Nong Sang. Hierarchical spatio-temporal decoupling for text-to-video generation. arXiv:2312.04483, 2023. 3 [34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. 3, 5 [35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv:2204.06125, 1(2):3, 2022. [36] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 3 [37] Fei Shen, Hu Ye, Jun Zhang, Cong Wang, Xiao Han, and Yang Wei. Advancing pose-guided image synthesis with progressive conditional diffusion models. In The Twelfth International Conference on Learning Representations, 2024. 3 [38] Shuwei Shi, Biao Gong, Xi Chen, Dandan Zheng, Shuai Tan, Zizheng Yang, Yuyuan Li, Jingwen He, Kecheng Zheng, Jingdong Chen, et al. Motionstone: Decoupled motion intensity modulation with diffusion transformer for imageto-video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2286422874, 2025. 3 [39] Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. NeurIPS, 32, 2019. 3 [40] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In CVPR, pages 1365313662, 2021. 3 [41] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. ICLR, 2023. [42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3, 4 [43] Shuai Tan and Bin Ji. Edtalk++: Full disentanglement arXiv preprint for controllable talking head synthesis. arXiv:2508.13442, 2025. 3 [44] Shuai Tan, Bin Ji, and Ye Pan. Emmn: Emotional motion memory network for audio-driven emotional talking face generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2214622156, 2023. 3 [45] Shuai Tan, Biao Gong, Yutong Feng, Kecheng Zheng, Dandan Zheng, Shuwei Shi, Yujun Shen, Jingdong Chen, and Ming Yang. Mimir: Improving video diffusion models for precise text understanding. arXiv:2412.03085, 2024. [46] Shuai Tan, Bin Ji, Mengxiao Bi, and Ye Pan. Edtalk: Efficient disentanglement for emotional talking head synthesis. In ECCV. Springer, 2024. 3 [47] Shuai Tan, Bin Ji, Yu Ding, and Ye Pan. Say anything with any style. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 50885096, 2024. [48] Shuai Tan, Bin Ji, and Ye Pan. Flowvqtalker: High-quality emotional talking face generation through normalizing flow and quantization. In CVPR, pages 2631726327, 2024. [49] Shuai Tan, Bill Gong, Bin Ji, and Ye Pan. Fixtalk: Taming identity leakage for high-quality talking head generation in In Proceedings of the IEEE/CVF Internaextreme cases. tional Conference on Computer Vision, 2025. 3 [50] Shuai Tan, Biao Gong, Zhuoxin Liu, Yan Wang, Xi Chen, Yifan Feng, and Hengshuang Zhao. Animate-x++: Universal character image animation with dynamic backgrounds. arXiv preprint arXiv:2508.09454, 2025. 3 [51] Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobin Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion representation. In International Conference on Learning Representations, 2025. 2, 3, 6 [52] Shuai Tan, Biao Gong, Yujie Wei, Shiwei Zhang, Zhuoxin Liu, Dandan Zheng, Jingdong Chen, Yan Wang, Hao Ouyang, Kecheng Zheng, and Yujun Shen. Synmotion: Semantic-visual adaptation for motion customized video generation. arXiv preprint arXiv:2506.23690, 2025. 3 [53] Meituan LongCat Team. Longcat-video technical report. arXiv preprint arXiv:2510.22200, 2025. [54] Zhengyan Tong, Chao Li, Zhaokang Chen, Bin Wu, and Wenjiang Zhou. Musepose: pose-driven image-to-video framework for virtual human generation. arxiv, 2024. 2, 6 [55] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: Highquality identity-preserving human image animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2109621106, 2025. 2, 3, 6 [56] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, Zuxuan Wu, and Yu-Gang Jiang. Stableanimator++: Overcoming pose misalignment and face distortion for human image animation. arXiv:2507.15064, 2025. 2, 3 [57] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv:1812.01717, 2018. 6 [58] Team Wan. Wan: Open and advanced large-scale video generative models. arXiv:2503.20314, 2025. 3, 5 [59] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv:2308.06571, 2023. 3 [60] Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for referring human dance generation in real world. In ICLR, 2024. 3 [61] Xiayu Wang, Ke Ma, Ruiyun Zhong, Xinggang Wang, Yi Fang, Yang Xiao, and Tian Xia. Towards dual transparent liquid level estimation in biomedical lab: Dataset, methods and practices. In European Conference on Computer Vision, pages 198214. Springer, 2024. 3 [62] Xiang Wang, Shiwei Zhang, Changxin Gao, Jiayu Wang, Xiaoqiang Zhou, Yingya Zhang, Luxin Yan, and Nong Sang. Unianimate: Taming unified video diffusion models for consistent human image animation. arXiv:2406.01188, 2024. 2, 3, 6 [63] Xiang Wang, Shiwei Zhang, Hangjie Yuan, Zhiwu Qing, Biao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, and Nong Sang. recipe for scaling up text-to-video generation with text-free videos. In CVPR, 2024. [64] Xiang Wang, Shiwei Zhang, Longxiang Tang, Yingya Zhang, Changxin Gao, Yuehuan Wang, and Nong Sang. Unianimate-dit: Human image animation with large-scale video diffusion transformer. arXiv:2504.11289, 2025. 2, 5, 6, 8 [65] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility IEEE Transactions on Image Proto structural similarity. cessing, 13(4):600612, 2004. 6 [66] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning In of image diffusion models for text-to-video generation. ICCV, pages 76237633, 2023. 3 [67] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, and Xiangyu Zhang. Lamp: Learn motion pattern for few-shot video generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 70897098, 2024. 3 [68] Ruiqi Wu, Xinjie Wang, Liu Liu, Chunle Guo, Jiaxiong Qiu, Chongyi Li, Lichao Huang, Zhizhong Su, and MingMing Cheng. Dipo: Dual-state images controlled articulated object generation powered by diverse data. arXiv preprint arXiv:2505.20460, 2025. 3 [69] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. Simda: Simple diffusion adapter for efficient video generation. arXiv:2308.09710, 2023. [70] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. arXiv:2311.16498, 2023. 2, 3 [71] Jingyun Xue, Hongfa Wang, Qi Tian, Yue Ma, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum, et al. Towards multiple character image animation through enhancing implicit decoupling. arXiv preprint arXiv:2406.03035, 2024. 2, 3, 6, 7 [72] Ruofeng Yang, Bo Jiang, Cheng Chen, Baoxiang Wang, Shuai Li, et al. Few-shot diffusion models escape the curse of dimensionality. Advances in Neural Information Processing Systems, 37:6852868558, 2024. 3 [73] Ruofeng Yang, Zhijie Wang, Bo Jiang, and Shuai Li. Leveraging drift to improve sample complexity of variance exploding diffusion models. Advances in Neural Information Processing Systems, 37:107662107702, 2024. 3 [74] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Instructing video Albanie, and Dong Ni. diffusion models with human feedback. arXiv:2312.12490, 2023. Instructvideo: [75] Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based network for poseguided human video generation. arXiv:1910.09139, 2019. 6 [76] Beiyuan Zhang, Yue Ma, Chunlei Fu, Xinyang Song, Zhenan Sun, and Ziqiang Li. Follow-your-multipose: Tuning-free multi-character text-to-video generation via pose guidance. arXiv preprint arXiv:2412.16495, 2024. 2, 3, 6 [77] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 38363847, 2023. 3 [78] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep In CVPR, pages 586595, features as perceptual metric. 2018. 6 [79] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv:2406.19680, 2024. 2, 3, [80] Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In CVPR, pages 36573666, 2022. 3 [81] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv:2211.11018, 2022. 3 [82] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In ECCV, 2024."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Huazhong University of Science and Technology",
        "The University of Hong Kong",
        "Tsinghua University",
        "University of North Carolina at Chapel Hill"
    ]
}