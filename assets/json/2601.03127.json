{
    "paper_title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
    "authors": [
        "Sashuai Zhou",
        "Qiang Zhou",
        "Jijin Hu",
        "Hanqing Yang",
        "Yue Cao",
        "Junpeng Ma",
        "Yinchao Ma",
        "Jun Song",
        "Tiezheng Ge",
        "Cheng Yu",
        "Bo Zheng",
        "Zhou Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality."
        },
        {
            "title": "Start",
            "content": "Unified Thinker: General Reasoning Modular Core for Image Generation Sashuai Zhou1,2,*, Qiang Zhou2,*, Jijin Hu2,*, Hanqing Yang2,*, Yue Cao3, Junpeng Ma4 Yinchao Ma2, Jun Song2,, Tiezheng Ge2, Cheng Yu2, Bo Zheng2, Zhou Zhao1, 1Zhejiang University, 2Alibaba Group, 3Nanjing University, 4Fudan University Equal contribution. Corresponding authors. Code: https://github.com/alibaba/UnifiedThinker 6 2 0 J 6 ] . [ 1 7 2 1 3 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing persistent reasoningexecution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoningdriven image generation, highlighting substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, task-agnostic reasoning architecture for general image generation, designed as unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce two-stage training paradigm: we first build structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on textto-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality."
        },
        {
            "title": "Introduction",
            "content": "The rapid evolution of diffusion-based foundation models (Ho et al., 2020; Dhariwal and Nichol, 2021; Rombach et al., 2022; Rafailov et al., 2023) has driven an unprecedented leap in high-fidelity image synthesis. Advanced proprietary models such as GPT-4o (Hurst et al., 2024) and Nano Banana (Comanici et al., 2025) have recently demonstrated strong reasoning-driven image generation under complex instructions. In contrast, despite steady progress in open-source systems (Esser Figure 1: Challenges in reasoning-aware image generation. Existing models, exemplified by Qwen-ImageEdit, exhibit two failure modes: (1) inaccurate reasoning (without Thinker), leading to logically incorrect edits; and (2) imprecise rendering (with Thinker), where correct reasoning does not translate into faithful visual outputs. Our Unified Thinker aims to address both issues. et al., 2024; Labs, 2024; Deng et al., 2025; Wu et al., 2025a; Liao et al., 2025), current open-source models still exhibit clear gap in handling logicintensive or implicit directives (Niu et al., 2025; Zhao et al., 2025; He et al., 2025; Liu et al., 2025b). Current attempts to bridge this gap follow two primary approaches. Built-in Reasoning internalizes reasoning into the generator via unified training that couples multimodal understanding with generation (Deng et al., 2025; Xie et al., 2025; Xiao et al., 2025). However, this tight entanglement reduces modularity and may destabilize training, often degrading the generators visual fidelity. In contrast, External Planner-Driven methods use an MLLM to plan for mostly frozen generator (Wu et al., 2025a; Lin et al., 2025; Li et al., 2025a; He et al., 2025). While modular, they suffer from reasoningexecution mismatch: text-space plans are not grounded in the generators capabilities, so even correct plans can cause visual failures, and iterative planning further increases compute. We identify the key bottleneck as the absence of principled paradigm for reasoning in image generation. In this paper, we propose Unified Thinker, Figure 2: Visual demonstrations of Unified Thinker on unified image generative tasks, including image editing and text-to-image generation, along with reasoning. universal reasoning core for general image generation, built around think-then-execute architecture that parametrically decouples the Thinker for instruction understanding and planning from the Generator for pixel synthesis. Here, the Generator refers to the underlying image synthesis backbone (e.g., diffusion model) that takes conditioning signals and produces the final image in pixel space. The Thinker is implemented as standalone, trainable multimodal large language model (MLLM) that transforms an instruction into hierarchical, generator-friendly plan consisting of an intent summary, explicit constraints, and ordered sub-goals, which the Generator consumes as conditioning, enabling strong task transferability across text-toimage and editing and plug-and-play compatibility with different generator backbones. However, this decoupled design still faces additional challenges: as shown in Fig. 1, without proper alignment, naive Thinker may produce plausible reasoning that the Generator cannot execute. To bridge the reasoning-to-execution gap, we introduce dedicated data-to-training pipeline to align planning with visual outcomes. We first construct HieraReason-40K, hierarchical reasoning dataset synthesized with Gemini-3-Pro (Comanici et al., 2025), which pairs complex instructions with structured, executable plans to teach the Thinker the desired planning format and basic logical decomposition. We then adopt two-stage training strategy: we perform joint supervised fine-tuning on HieraReason-40K to establish initial plan quality, followed by an end-to-end dual-phase reinforcement learning procedure that places the Generator in the loop and optimizes the Thinker using rewards computed from the final images constraint satisfaction. This directly grounds the Thinkers policy in pixel-level feedback, encouraging plans that are not only semantically plausible but also executable under the Generators capabilities. We conduct extensive evaluations in four settext-to-image reasoning, reasoning-based tings: image editing, general text-to-image generation, and general image editing. Across all benchmarks, Unified Thinker delivers substantial gains in generative reasoning, markedly improving instruction following and constraint satisfaction. These improvements also hold across multiple generator backbones, supporting our core claim that decoupled Thinker learns reusable, executable reasoning patterns that transfer across models and tasks. Our main contributions are as follows: We propose decoupled reasoning-generation framework Unified Thinker that utilizes unified module to handle general image generation tasks, significantly enhancing modular adaptability and transferability. We introduce an end-to-end training pipeline spanning from hierarchical reason data construction to execution-led reinforcement learning, bridging the gap between abstract reasoning and pixel-level execution. Through comprehensive experimental results, we demonstrated significant performance improvement in reasoning-intensive generation tasks and verified the cross-model portability of our reasoning core module."
        },
        {
            "title": "2.1 Foundational Generative Models",
            "content": "Modern image generation is predominantly anchored in diffusion-based frameworks (Ho et al., 2020; Rombach et al., 2022). Recent advances (Esser et al., 2024; Labs, 2024; Wu et al., 2025a) build upon Diffusion Transformers (Peebles and Xie, 2023) and flow matching (Lipman et al., 2022) to improve fidelity, prompt alignment, and diversity in latent diffusion models. Meanwhile, an emerging direction unifies autoregressive modeling with visual generation in single framework, giving rise to unified multimodal models (Deng et al., 2025; Wu et al., 2025b; Xie et al., 2025; Xiao et al., 2025). For instance, Bagel (Deng et al., 2025) uses transformer backbone to jointly model text and image tokens, whereas OmniGen (Xiao et al., 2025) dispenses with external encoders and handles multiple vision tasks through unified pipeline. In parallel, image editing has evolved from mask-based inpainting (Zhuang et al., 2024; Ju et al., 2024) to instruction-guided manipulation (Brooks et al., 2023; Yu et al., 2025). To further enhance instruction following, recent methods (Huang et al., 2024; Fu et al., 2024; Lin et al., 2025; Liu et al., 2025b) such as Qwen-Image-Edit (Wu et al., 2025a) leverage MLLMs for instruction parsing and planning. However, these models fall short in executing the complex logic required for sophisticated tasks, motivating us to introduce dedicated Thinker module that bolsters the models fundamental reasoning capabilities during generation."
        },
        {
            "title": "2.2 Reasoning for Image Generation",
            "content": "Recent research has moved beyond the one-shot mapping paradigm by explicitly incorporating reasoning into the image generation process. One line of work (Jiang et al., 2025; Wang et al., 2025; Liao et al., 2025; Qin et al., 2025; Huang et al., 2025) introduces clear intermediate representations to decompose complex prompts into structured steps or explicit spatial layouts, improving compositional consistency and coherence. Another line of work (He et al., 2025; Mi et al., 2025; Deng et al., 2025) encourages models to reason about intent and constraints before drawing, moving beyond one-shot planning to better satisfy complex requirements, like R-Genie (Zhang et al., 2025), which infers latent user intent instead of merely following the surface-level prompt. third line of work (Guo et al., 2025b; Wu et al., 2025c; Li et al., 2025b,a; Yin et al., 2025) focuses on post-generation refinement by introducing reflection-and-correction mechanisms that assess the generated image, diagnose issues, and iteratively update the output to improve final quality. For example, Reflect-DiT (Li et al., 2025b) introduces explicit self-reflection to guide revision, while EditThinker (Li et al., 2025a) enables reasoning via multi-round reflective interactions throughout the editing process. In contrast to these approaches, we propose universal decoupled thinker that offers reusable reasoning as standalone module, enabling easy transfer across diverse image generation and image editing tasks."
        },
        {
            "title": "3 Data Construction",
            "content": "Goal and dataset. We aim to train standalone Thinker that augments existing diffusion generators with transferable reasoning while remaining generator-agnostic. To this end, we construct HieraReason-40K, selected general-purpose corpus by combining four sources that cover text-toimage generation, general image editing, reasoning image generation, and reasoning image editing tasks (Han et al., 2025; Huang et al., 2025; Qian et al., 2025; Fang et al., 2025). Each example pairs an instruction (optionally with reference images) with structured reasoning trace that ends in an enhanced prompt for the downstream generator. Structured reasoning trace. As illustrated in Fig. 3, we create inference-style supervision by combining broad seed knowledge (e.g., art & culture) with input instruction to form generated inference data. Each training example is then rewritten"
        },
        {
            "title": "4 Framework and Training",
            "content": "The core objective of our framework is to mitigate the reasoningexecution mismatch in reasoningdriven image generation and editing. We introduce decoupled think-then-execute framework with two components: Thinker, standalone, trainable multimodal large language model that produces structured reasoning traces and an executable visual specification, and Generator, diffusion-based model that synthesizes the final image conditioned on the Thinkers outputs. Training follows twostage pipeline, starting with joint supervised finetuning on structured traces and then moving to an execution-led, dual-phase reinforcement learning stage that optimizes the Thinker using rewards computed from the final generated images. 4.1 Joint Supervised Fine-Tuning To teach the Thinker consistent reasoning format and establish the think-then-execute pipeline, we first perform joint supervised fine-tuning stage. Given an instruction and an optional input image for editing, the Thinker produces structured reasoning trace and an executable visual specification, and the Generator synthesizes the image conditioned on this output for both text-to-image generation and instruction-driven editing. The training data is organized around instructionfollowing image generation and editing examples, each containing user instruction, an optional reference image, and target image. We derive two synchronized views of the same examples for joint training: (1) an understanding view, which pairs the input (instruction and optional reference image) with the annotated structured reasoning trace, supervising the Thinker via language modeling loss; and (2) generation view, which pairs the executable enhanced prompt (extracted from the trace) with the target image, supervising the Generator via the standard diffusion denoising objective. During each training step, we sample mini-batches from the two views and optimize weighted sum of the understanding loss Lund (token-level cross-entropy) and the generation loss Lgen (noise-prediction mean squared error). This joint supervised fine-tuning procedure effectively aligns the instruction-generation capability of the Thinker with the image-synthesis prior of the Generator, ensuring that the produced reasoning instructions are not only semantically accurate but also highly compatible with the Generators operaFigure 3: Data construction pipeline for HieraReason40K. We combine seed knowledge and user requests to generate structured reasoning traces and executable enhanced prompts. into rigorous structured reasoning trace: given an original instruction (and an optional reference image for image editing), the annotator produces formalized reasoning trace followed by final enhanced prompt for the generator. The trace follows fixed three-stage procedure. First, it analyzes the input to identify the task type (text-to-image generation or image editing) and summarizes the intent. Next, it makes implicit requirements explicit and performs any necessary reasoning, such as counting, puzzle solving, numerical computation, temporal extrapolation, rule-based transformations, and attribute or coordinate lookup, to derive concrete visual target. Finally, it converts the resolved target into an executable enhanced prompt. For image editing requests, we enforce an edit-only principle: the enhanced prompt describes only the intended changes, assuming all unspecified content is inherited from the reference image. This design ensures that reasoning is fully completed within the trace, while the downstream generator receives only renderable visual specification. Annotation and quality control. We use Gemini3-Pro (Comanici et al., 2025) to generate initial structured reasoning traces, followed by automatic normalization to enforce strict format consistency (e.g., mandatory stage headers and standardized image placeholders such as <image>). We further filter or rewrite samples that violate the trace format, fail to follow the edit-only principle for image editing, produce non-visual or underspecified targets, or exhibit inconsistencies between the reasoning trace and the final enhanced prompt. To further strengthen reasoning, we also carefully design set of task-general system prompts that cover diverse common generation and editing scenarios. Figure 4: Our proposed two-stage framework for reasoning-aware image generation. Stage 1 initializes the Thinker Model and Generator Model. Given an Image & Prompt (x), the Thinker generates Reasoning Thought (y), which then guides the Generator to produce Refined Image (z). Stage 2 further refines the Thinker and Generator Models to enhance their capability in integrating complex reasoning (y) into high-fidelity visual outputs (z), applicable to both novel image generation and existing image editing tasks. tional semantics, thereby laying solid foundation for cascaded inference deployment. Formally, the overall objective is defined as: LSFT = Lgen (cid:0)Generator(y, xref), xtgt (cid:1) (cid:0)Thinker(ximg), y(cid:1), + λ Lund (1) where ximg denotes the input image, is the ground-truth reasoning process, xref represents an optional reference image, xtgt is the target output image, and λ > 0 is hyperparameter balancing the two learning signals."
        },
        {
            "title": "4.2 Dual-Phase Reinforcement Learning",
            "content": "While joint fine-tuning provides an initial alignment, it leaves nontrivial reasoningexecution gap: the Thinker may produce plans that are plausible in text but suboptimal for the generator to execute. To address this without additional manual annotation, we introduce dual-phase reinforcement learning strategy based on Group Relative Policy Optimization (Guo et al., 2025a). The key idea is to sample multiple candidate traces for the same request, execute them with the generator, and train the Thinker by relative advantage feedback, promoting outputs that lead to better images and suppressing those that do not. Phase 1: Reasoning-Oriented RL. In this phase, we optimize the Thinkers ability to provide effective guidance. For given instruction, the Thinker samples group of reasoning paths {y1, y2, . . . , yG}. We use the Generator (fixed) to produce the corresponding images and assign reward ri to each path based on the final image quality. We optimize the Thinker by maximizing: JT (θT ) = (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:18) πθ(yip) πold(yip) (cid:19)(cid:35) ˆAi ˆAi = ri mean({r}) std({r}) (2) (3) Here, ˆAi is the relative advantage, which tells the model which reasoning chains performed better than the group average. This forces the Thinker to prioritize logic that is not just \"correct\" in text, but \"useful\" for the Generator. Phase 2: Generation-Oriented RL. With the Thinker providing reliable plans, we next improve the Generators execution fidelity. However, probability-flow ODE sampling in diffusion models is essentially deterministic, limiting the stochastic rollouts required by reinforcement learning. Following Flow-GRPO-like idea (Liu et al., 2025a), we convert the ODE sampler into an equivalent reverse-time SDE to introduce controlled randomness, enabling distinct rollouts {z1, z2, . . . , zG} for the same instruction and optimizing the Generator accordingly: JG(θG) = (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:18) πθ(zic, p) πold(zic, p) ˆAi (cid:19)(cid:35) (4) In this stage, the advantage ˆAi assigns higher credit to denoising trajectories that yield better images. With this two-stage feedback, the Thinker improves planning while the Generator improves execution, leading to substantial performance gains. Details of the reward design are provided in the appendix A.4."
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate Unified Thinker in four settings: general instruction-driven image editing, general textto-image generation, reasoning-intensive image editing, and reasoning-intensive text-to-image generation. Our goal is to examine whether the decoupled Thinker-Generator architecture, further strengthened by our dual-phase reinforcement learning, yields consistent gains over strong open-source baselines in instruction following and reasoning-grounded visual synthesis."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Model configuration. Unless otherwise specified, Unified Thinker uses Qwen2.5-VL-7B, and we additionally report results with Qwen3-VL-8B (Bai et al., 2025b,a).We use Qwen-Image-Edit (Wu et al., 2025a) as the base generator to execute the visual specifications produced by the Thinker. For reinforcement learning and automated evaluation, we adopt Qwen3-VL-30B (Bai et al., 2025a) as the reward model, which provides feedback on both visual correctness and logical consistency. Training data and setup. For the supervised cold start, we jointly fine-tune on HieraReason40K. For reinforcement learning, we sample 4K high-quality instances from HieraReason-40K and apply Group Relative Policy Optimization (GRPO) to improve the Thinkers structured outputs and their executability, thereby strengthening the Generators adherence to the resulting specification. Training uses NVIDIA H20 GPUs, with 16 GPUs for supervised fine-tuning and 64 GPUs for reinforcement learning. Evaluation benchmarks. We evaluate on WiseBench (Niu et al., 2025), RISEBench (Zhao et al., 2025), GEditBench (Liu et al., 2025b), and PRISMBench (Fang et al., 2025). These benchmarks cover diverse knowledge domains and editing operations, requiring models to combine highlevel semantic reasoning (e.g., temporal, and logical inference) with low-level visual manipulation (e.g., content preservation)."
        },
        {
            "title": "5.2 Main Results",
            "content": "Reasoning-based image editing (RISEBench). As shown in Table 1, our method markedly improves reasoning-heavy editing over the base Qwen-Image-Edit and naive MLLM-thinker baseIn particular, the unified training strategy line. yields large improvements on temporal and spatial reasoning, indicating that the Thinker effectively resolves hidden constraints (e.g., temporal shifts or relational edits) and reduces semantic drift during diffusion execution. Text-to-image reasoning (WiseBench). Table 4 shows that Unified Thinker achieves the strongest overall performance among open-source models and improves most domain categories, substantially narrowing the gap to closed-source frontier models such as GPT-4o. Gains are especially notable in categories that demand precise entity grounding and knowledge retrieval (e.g., cultural and biology), suggesting that explicit planning helps translate implicit constraints into executable visual specifications. General generation and editing quality. Beyond reasoning-centric benchmarks, we further confirm that incorporating the Thinker does not compromise general-purpose generation or editing performance. On PRISM  (Table 3)  , our method achieves consistent improvement in overall quality, with gains that are mainly reflected in aesthetic preference while preserving prompt-image alignment. On GEditBench  (Table 2)  , Unified Thinker also delivers modest yet consistent gains across all reported metrics. Together, these results suggest that the planning stage improves instruction decomposition and visual target specification without weakening the Generators core rendering ability, and can even provide small benefits under standard, non-reasoning workloads."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "Training stage ablation. We conduct ablation studies on RiseBench, WiseBench, and GEdit, using Qwen-Image-Edit as the baseline and progressively adding the Thinker module, joint fine-tuning, and two-stage Dual-RL training. Table 1: Performance comparison of models on the RiseBench benchmark. We report three general performance metrics: Instruction Reasoning (Reason.), Appearance Consistency (Consist.), and Visual Plausibility (Visual.). Additionally, we present category-wise accuracy (%) for four specific reasoning dimensions: Temporal, Causal, Spatial, and Logical. The Overall score is the average of these four category-wise accuracies. Model Reason. Consist. Visual. Temporal Causal Spatial Logical Overall Gemini-3-pro-image-preview Gemini-2.5-Flash-Image GPT-Image-1 GPT-Image-1-mini Gemini-2.0-Flash-exp BAGEL (w/ CoT) Seedream-4.0 Gemini-2.0-Flash-pre FLUX.1-Kontext-Dev Ovis-U1 Step1X-Edit OmniGen EMU2 BAGEL + Unified Thinker (Qwen2.5-VL-7B) + Unified Thinker (Qwen3-VL-8B) Qwen-Image-Edit + Unified Thinker (Qwen2.5-VL-7B) + Unified Thinker (Qwen3-VL-8B) 77.0 61.2 62.8 54.1 48.9 45.9 58.9 49.9 26.0 33.9 30.3 25.1 22.6 36.5 53.3 58.7 37.2 58.6 61.9 85.5 86.0 80.2 71.5 68.2 73.8 67.4 68.4 71.6 52.7 12.6 41.5 38.2 53.5 73.6 75.7 66.4 75.9 76. 94.4 91.3 94.9 93.7 82.7 80.1 91.2 84.9 85.2 72.9 74.9 73.5 78.3 73.0 78.1 80.9 86.9 90.1 90.5 41.2 25.9 34.1 24.7 8.2 5.9 12.9 10.6 2.3 1.2 0.0 1.2 1.2 2.4 14.1 15.2 4.7 24.7 32.9 61.1 47.8 32.2 28.9 15.5 17.8 12.2 13.3 5.5 3.3 2.2 1.0 1.1 5.6 17.7 17.7 10.0 22.2 30. 48.0 37.0 37.0 33.0 23.0 21.0 11.0 11.0 13.0 4.0 2.0 0.0 0.0 14.0 18.0 20.0 17.0 38.0 41.0 37.6 18.8 10.6 9.4 4.7 1.2 7.1 2.3 1.2 2.4 3.5 1.2 0.0 1.2 3.5 8.2 2.4 9.4 9.4 47.2 32.8 28.9 24.4 13.3 11.9 10.8 9.4 5.8 2.8 1.9 0.8 0.5 6.1 13.6 15.5 8.9 24.2 28. Table 2: Results on GEditBench for general instructionbased image editing. We report G_SC, G_PQ, and G_O on the English split. Table 3: Results on PRISM for general text-to-image generation. We report alignment (Aln), aesthetics (Aes), and average (Avg) using GPT-4.1 as evaluation. Model G_SC G_PQ G_O Model Aln Aes Avg UniWorld-V2 Step1x-edit-v1p2(reflection) Step1x-edit-v1p2(thinking) Step1X-edit-v1.1 Flux-Kontext-dev OmniGen2 OmniGen AnyEdit BAGEL + Unified Thinker (Qwen2.5-VL-7B) + Unified Thinker (Qwen3-VL-8B) Qwen-Image-Edit + Unified Thinker (Qwen2.5-VL-7B) + Unified Thinker (Qwen3-VL-8B) 8.29 8.18 8.02 7.66 7.16 7.16 5.96 3.18 7.36 7.29 7.38 8.00 8.17 8.15 8.02 7.85 7.64 7.35 7.37 6.77 5.89 5.82 6.83 6.88 6.75 7.86 7.94 8.04 7.83 7.58 7.36 6.97 6.51 6.41 5.06 3.21 6.52 6.53 6.60 7.56 7.67 7. Gemini-2.5-Flash-Image Qwen-Image SEEDream 3.0 HiDream-I1-Full FLUX.1-Krea-dev SD3.5-Large FLUX.1-dev HiDream-I1-Dev BAGEL + Unified Thinker (Qwen2.5-VL-7B) + Unified Thinker (Qwen3-VL-8B) Qwen-Image-Edit + Unified Thinker (Qwen2.5-VL-7B) + Unified Thinker (Qwen3-VL-8B) 87.1 81.1 80.5 76.1 74.3 73.9 72.4 70.3 66.7 73.5 75.1 76.9 77.3 83.2 83.4 78.6 78.7 75.6 75.1 73.5 74.9 70.0 63.4 67.7 69.2 70.7 73.8 73.0 85.3 79.9 79.6 75.9 74.7 73.7 73.7 70.2 65.1 70.6 72.1 73.8 75.6 78. Table 5 shows that introducing the Thinker notably improves performance on reasoning-oriented benchmarks, while slightly hurting low-level editing quality on GEdit, revealing mild objective trade-off. Joint fine-tuning alleviates this mismatch and stabilizes multi-task behavior, and the proposed two-stage Dual-RL further yields consistent gains across all benchmarks, leading to the best overall results by better aligning reasoning with final visual outcomes. Thinker backbone ablation. We instantiate Unified Thinker with two backbones(Qwen2.5VL-7B and Qwen3-VL-8B). Overall, stronger Thinker backbone tends to yield better reasoningTable 4: Results on WiseBench for reasoning-based text-to-image generation. We report accuracy across six knowledge domains and the overall score. Model Cultural Time Space Biology Physics Chemistry Overall GPT-4o Qwen-Image UniWorld-V2 UniWorld-V1 Manzano-3B Manzano-30B OpenUni-B-512 OpenUni-L-512 OpenUni-L-1024 MetaQuery-XL Liquid BAGEL + Unified Thinker (Qwen2.5-VL-7B) + Unified Thinker (Qwen3-VL-8B) Qwen-Image-Edit + Unified Thinker (Qwen2.5-VL-7B) + Unified Thinker (Qwen3-VL-8B) 0.81 0.62 0.60 0.53 0.42 0.58 0.37 0.51 0.49 0.56 0.38 0.44 0.72 0.70 0.62 0.75 0.75 0.71 0.63 0.61 0.55 0.51 0.50 0.45 0.49 0.53 0.55 0. 0.55 0.65 0.65 0.63 0.66 0.70 0.89 0.77 0.70 0.73 0.59 0.65 0.58 0.64 0.69 0.62 0.53 0.68 0.75 0.73 0.77 0.78 0.81 0.83 0.57 0.53 0.45 0.45 0.50 0.39 0.48 0.49 0.49 0.36 0.44 0.64 0.62 0.57 0.75 0.73 0.79 0.75 0.64 0.59 0.51 0.55 0.50 0.63 0.56 0.63 0. 0.60 0.75 0.73 0.75 0.79 0.81 0.74 0.40 0.32 0.41 0.32 0.32 0.30 0.35 0.39 0.41 0.30 0.39 0.61 0.55 0.40 0.61 0.55 0.80 0.62 0.58 0.55 0.46 0.54 0.43 0.52 0.52 0.55 0.41 0.52 0.70 0.68 0.62 0.73 0.74 Table 5: Training stage ablation results on RiseBench, WiseBench, and GEdit. The baseline is based on QwenImage-Edit. The Thinker is implemented with Qwen2.5VL-7B and further trained in our framework. Table 6: Ablation of the Thinker design on RiseBench. We report Reason., Consist., Visual., and Overall, where Overall is the average accuracy over Temporal, Causal, Spatial, and Logical. The baseline is Qwen-Image-Edit. Ablation Rise Wise GEdit Model Reason. Consist. Visual. Overall baseline + Thinker + Joint fine-tune +Dual-RL stage 1 + Dual-RL stage 2 8.9 16.4 20.2 21.9 24.2 0.62 0.66 0.68 0.72 0.73 7.56 7.49 7.52 7.61 7. baseline + Gemini-2.5-Pro + GPT-5 + Qwen3-VL-30B + Unified Thinker (7B) 37.2 64.3 67.4 57.6 58.6 66.4 71.9 76.6 75.9 75. 86.9 88.3 86.3 86.6 90.1 8.9 25.2 26.9 23.1 24.2 oriented performance and improves overall editing fidelity on reasoning tasks, whereas the 7B variant can be slightly preferred on PRISM in terms of aesthetics, suggesting trade-off between logic alignment and visual preference. Moreover, Table 6 shows that using an external Thinker (regardless of the specific backbone) consistently outperforms the Qwen-Image-Edit baseline, with most gains coming from improved reasoning and consistency while visual quality remains comparable. Cross-generator ablation. We evaluate the transferability of the Thinker module by applying the Unified Thinker trained with the Qwen-ImageEdit pipeline to different generator (BAGEL). As shown in Tables 1, 4, and 2, adding Thinker consistently improves BAGEL on both RiseBench and GEditBench, demonstrating that the module generalizes beyond the training generator and can be integrated into other generation models with stable gains."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose UNIFIED THINKER, decoupled ThinkerGenerator framework that equips diffusion models with transferable reasoning and planning. The Thinker maps user requests for both text-to-image generation and image editing into structured, executable intermediate representation, enabling the Generator to focus on faithful visual synthesis. We build 40K cold-start training corpus with strict formatting and further enhance both planning and execution with two-stage RL pipeline. Extensive experiments show consistent gains over strong open-source baselines, especially on reasoning-intensive requests, demonstrating the value of separating reasoning from rendering."
        },
        {
            "title": "7 Limitations",
            "content": "Our approach still depends on the quality and coverage of the intermediate representation, training data, and automatic rewards used during RL, which can introduce bias and limit generalization beyond the evaluated benchmarks. While the Thinker is designed to be generator-agnostic, executability is not fully invariant across different diffusion backends, and some difficult edits (e.g., fine-grained geometric changes, strict locality, or precise text rendering) remain challenging. Finally, the additional planning stage increases inference latency and compute cost compared to directly prompting single generator."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, and 45 others. 2025a. Qwen3-vl technical report. Preprint, arXiv:2511.21631. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others. 2025b. Qwen2.5-vl technical report. Preprint, arXiv:2502.13923. Tim Brooks, Aleksander Holynski, and Alexei Efros. 2023. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, Nan-Jiang Jiang, and 3416 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Preprint, arXiv:2507.06261. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. 2025. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, and Hongsheng Li. 2025. Flux-reason-6m & prism-bench: million-scale text-to-image reasoning dataset and comprehensive benchmark. arXiv preprint arXiv:2509.09680. Tsu-Jui Fu, Wenze Hsu, William Yang Wang, ShangHua Li, Scott Cohen, and Yang Wang. 2024. Guiding instruction-based image editing via multimodal large language models. In International Conference on Learning Representations (ICLR). Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, and 175 others. 2025a. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nat., 645(8081):633638. Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. 2025b. Can we generate images with cot? lets verify and reinforce image generation step by step. CoRR, abs/2501.13926. Feng Han, Yibin Wang, Chenglin Li, Zheming Liang, Dianyi Wang, Yang Jiao, Zhipeng Wei, Chao Gong, Cheng Jin, Jingjing Chen, and Jiaqi Wang. 2025. Unireditbench: unified reasoning-based image editing benchmark. CoRR, abs/2511.01295. Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, and Jiangning Zhang. 2025. Reasoning to edit: Hypothetical instruction-based image editing with visual reasoning. arXiv preprint arXiv:2507.01908. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems. Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, and Shaohui Lin. 2025. Interleaving reasoning for better text-to-image generation. CoRR, abs/2509.06945. Prafulla Dhariwal and Alex Nichol. 2021. Diffusion models beat gans on image synthesis. arXiv preprint arXiv:2105.05233. Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, and Ying Shan. 2024. Smartedit: Exploring complex instructionbased image editing with multimodal large language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 83628371. IEEE. Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, and 79 others. 2024. Gpt-4o system card. CoRR, abs/2410.21276. Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. 2025. T2i-r1: Reinforcing image generation with collaborative semantic-level and tokenlevel cot. arXiv preprint arXiv:2505.00703. Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. 2024. Brushnet: plug-andplay image inpainting model with decomposed dualbranch diffusion. In European Conference on Computer Vision, pages 150168. Springer. Black Forest Labs. 2024. Flux.1: Text-to-image synthesis via flow matching. Technical Announcement. Hongyu Li, Manyuan Zhang, Dian Zheng, Ziyu Guo, Yimeng Jia, Kaituo Feng, Hao Yu, Yexin Liu, Yan Feng, Peng Pei, Xunliang Cai, Linjiang Huang, Hongsheng Li, and Si Liu. 2025a. Editthinker: Unlocking iterative reasoning for any image editor. Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Arsh Koneru, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. 2025b. Inferencetime scaling for text-to-image diffusion transformarXiv preprint ers via in-context reflection. arXiv:2503.12271. Reflect-dit: Yanghao Li, Rui Qian, Bowen Pan, Haotian Zhang, Haoshuo Huang, Bowen Zhang, Jialing Tong, Haoxuan You, Xianzhi Du, Zhe Gan, Hyunjik Kim, Chao Jia, Zhenbang Wang, Yinfei Yang, Mingfei Gao, ZiYi Dou, Wenze Hu, Chang Gao, Dongxu Li, and 8 others. 2025c. Manzano: simple and scalable unified multimodal model with hybrid vision tokenizer. Preprint, arXiv:2509.16197. Jiaqi Liao, Zhengyuan Yang, Linjie Li, Dianqi Li, Kevin Lin, Yu Cheng, and Lijuan Wang. 2025. Imagegencot: Enhancing text-to-image in-context learning with chain-of-thought reasoning. arXiv preprint arXiv:2503.19312. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, and Li Yuan. 2025. Uniworld-v1: High-resolution semantic encoders for unified visual understanding and generation. CoRR, abs/2506.03147. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Flow Maximilian Nickel, and Matt Le. 2022. matching for generative modeling. arXiv preprint arXiv:2210.02747. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. 2025a. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, and 5 others. 2025b. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761. Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. 2024. Ovis: Structural embedding alignment for multimodal large language model. arXiv:2405.20797. Zhenxing Mi, Kuan-Chieh Wang, Guocheng Qian, Hanrong Ye, Runtao Liu, Sergey Tulyakov, Kfir Aberman, and Dan Xu. 2025. think, therefore diffuse: Enabling multimodal in-context reasoning in diffusion models. arXiv preprint arXiv:2502.10458. Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Chaoran Feng, Bin Zhu, and Li Yuan. 2025. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix JuefeiTransfer Xu, Ji Hou, and Saining Xie. 2025. between modalities with metaqueries. Preprint, arXiv:2504.06256. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205. Yusu Qian, Eli Bocek-Rivele, Liangchen Song, Jialing Tong, Yinfei Yang, Jiasen Lu, Wenze Hu, and Zhe Gan. 2025. Pico-banana-400k: large-scale dataset for text-guided image editing. arXiv preprint arXiv:2510.19808. Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, and Hao Li. 2025. Uni-cot: Towards unified chain-of-thought reasoning across text and vision. arXiv preprint arXiv:2508.05606. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. multimodal understanding and generation. Preprint, arXiv:2505.23661. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. 2025. Omnigen: Unified image generation. pages 1329413304. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. 2025. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564. Fukun Yin, Shiyu Liu, Yucheng Han, Zhibo Wang, Peng Xing, Rui Wang, Wei Cheng, Yingming Wang, Aojie Li, Zixin Yin, Pengtao Chen, Xiangyu Zhang, Daxin Jiang, Xianfang Zeng, and Gang Yu. 2025. Reasonedit: Towards reasoning-enhanced image editing models. Preprint, arXiv:2511.22625. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. 2025. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135. Dong Zhang, Lingfeng He, Rui Yan, Fei Shen, and Jinhui Tang. 2025. R-genie: Reasoning-guided generative image editing. arXiv preprint arXiv:2505.17768. Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Hao Li, Zicheng Zhang, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, and Haodong Duan. 2025. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826. Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. 2024. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. In European Conference on Computer Vision, pages 195211. Springer. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion modIn Proceedings of the IEEE/CVF conference els. on computer vision and pattern recognition, pages 1068410695. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 2024. Generative multimodal models are in-context learners. Preprint, arXiv:2312.13286. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, and 1332 others. 2025. Gemini: family of highly capable multimodal models. Preprint, arXiv:2312.11805. Yi Wang, Mushui Liu, Wanggui He, Longxiang Zhang, Ziwei Huang, Guanghao Zhang, Fangxun Shu, Tao Zhong, Dong She, Zhelun Yu, Haoyuan Li, Weilong Dai, Mingli Song, Jie Song, and Hao Jiang. 2025. MINT: multi-modal chain of thought in unified generative models for enhanced image generation. CoRR, abs/2503.01298. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, and 20 others. 2025a. Qwen-image technical report. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. 2025b. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 1296612977. Computer Vision Foundation / IEEE. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, and 3 others. 2025c. Omnigen2: Exploration to advanced multimodal generation. CoRR, abs/2506.18871. Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. 2025d. Liquid: Language models are scalable and unified multi-modal generators. International Journal of Computer Vision. Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, and Chen Change Loy. 2025e. Openuni: simple baseline for unified"
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Joint Supervised Fine-Tuning We perform joint supervised fine-tuning on Qwen2.5-VL-7B-Instruct and Qwen3-VL-8BInstruct with LoRA (r=8, applied to all modules) on 16 NVIDIA H20 GPUs,. Training uses mixed instruction dataset (mixed edit and text-to-image data in HieraReason-40K) with the qwen3_vl template, maximum sequence length 8096. We use batch size 4 per device with 8 gradient accumulation steps, learning rate 4 105, cosine schedule with 10% warmup, for 5 epochs. We set λ = 0.5. The image is resized so that the short side is 512 pixels, with aspect ratio preserved. A.2 Dual-Phase Reinforcement Learning We further optimize the models with GRPO on 64 GPUs. For rollouts, we use batch size of 16 and generate 24 candidates per prompt with sequence expansion enabled. We sample outputs with top k=100, and temperature 0.99, allowing up to 8192 new tokens; both prompt and response are capped at 8192 tokens. Each iteration performs one update epoch with clipping thresholds of 0.5 (value), 10 (reward), and 10 (advantage), without advantage whitening. We include KL regularization with coefficient of 0.01 against reference model. The thinker actor is initialized from Qwen2.5-VL7B-Instruct/Qwen3-VL-8B-Instruct and trained in BF16 using learning rate of 1 106 and weight decay 0.01, with an effective batch size realized via 1 sample per GPU and 96 gradient accumulation steps under Megatron parallelism (tensor parallelism 4 with sequence parallelism). Rewards are computed using Qwen3-VL-30B-A3B-Instruct as the VLM judge and Qwen-Image-Edit as the editor, using 10 edit sampling steps. A.3 Training Evaluation Fig. 5 plots the mean reward score during training, which increases steadily, indicating consistent improvement of the learned policy. Fig. 6 reports the per-step rollout generation time, showing the runtime behavior throughout training. A.4 Reward Model and Design We use VLM-based reward model to provide scalar supervision signal for both image-editing and text-to-image (T2I) training. For image editing, the judge is conditioned on the pre-edit image, the post-edit image, the edit instruction (edit_prompt), Figure 5: Mean reward score over training. Figure 6: Per-step rollout generation time over training. and reference description of the intended outcome (edit_prompt_cot), and returns three integer subscores on 15 scale: Appearance Consistency (whether non-instructed regions remain unchanged), Reasoning/Alignment (how well the edited image matches the intended result under the instruction), and Visual Plausibility (realism and overall generation quality); these subscores are aggregated into single scalar reward. For T2I, we first synthesize an image from the prompt (and, when applicable, the answer field extracted from the model output) and then evaluate it with the same VLM judge using strict rubric that outputs three integer subscores in {0, 1, 2}Consistency (prompt-image alignment), Realism (physical plausibility and fidelity), and Aesthetic Quality (overall visual appeal)whose mean yields the final reward in [0, 2] for reinforcement learning. A.5 Evaluated Comparative Models The following models were used in our comparative evaluation: Gemini-2.5-Flash-Image(Nano Banana): stateof-the-art multimodal model by Google optimized for high-fidelity text-to-image generation, complex image editing, and multi-image composition (Comanici et al., 2025). GPT-Image-1 & GPT-4o: OpenAIs unified multimodal series that demonstrates advanced spatialtemporal reasoning and end-to-end processing across text and vision (Hurst et al., 2024). Gemini-2.0-Flash: multimodal model from Google designed for real-time visual and textual Source Samples Task Type Input Output Unireditbench Pico-Banana-400K IRGL-300K Flux-reason-6m Total 10K 10K 10K 10K 40K Reasoning Image editing Image editing Reasoning T2I T2I instruction + image instruction + image instruction instruction think + enhanced prompt think + enhanced prompt think + enhanced prompt think + enhanced prompt 4 categories think + enhanced prompt Table 7: Composition of HieraReason-40K. We sample 10K instances from each source dataset and distill them into unified, structured format using Gemini with our system prompt. reasoning tasks (Team et al., 2025). BAGEL: unified understanding and generation multimodal framework that incorporates Chain-ofThought (CoT) reasoning to improve logical deduction in visual tasks (?). Qwen-Image / Edit: series of vision-language models from Alibaba; the Edit variant is specifically fine-tuned for instruction-based image manipulation (Wu et al., 2025a). EMU2: generative multimodal model that uses unified modeling framework for both visualsequential understanding and generation (Sun et al., 2024). FLUX.1 (Dev/Kontext): flow-matching based rectified flow transformer model known for superior text rendering and adherence to complex prompts (Labs, 2024). Stable Diffusion 3.5 (SD3.5): Multimodal Diffusion Transformer (MMDiT) architecture optimized for high-resolution synthesis and prompt following (Esser et al., 2024). OmniGen: unified image generation model capable of handling various tasks including generation, editing, and control within single framework (Wu et al., 2025c). Ovis: An open-source structural visual-language model designed to process high-resolution images with structural integrity (Lu et al., 2024). Step1X-Edit (v1.1/v1.2): family of generative models by StepFun; the v1.2 variants utilize \"thinking\" and \"reflection\" mechanisms to improve reasoning-heavy editing tasks (Liu et al., 2025b). UniWorld (V1/V2): multimodal world model framework designed for spatial-temporal understanding and high-fidelity video/image synthesis (Lin et al., 2025). Manzano : unified multimodal large model framework with shared visual encoder.(Li et al., 2025c). OpenUni (B/L): fully open-source lightweight multimodal unified baseline. It connects existing multimodal large language models with diffusion models through learnable queries and lightweight Transformer connector, thereby enabling simultaneous multimodal understanding and image generation. (Wu et al., 2025e). MetaQuery-XL: An expanded multimodal. It connects the frozen multimodal large model and the diffusion model with set of learnable queries, transferring the understanding and reasoning capabilities of the large model to image generation. (Pan et al., 2025). Liquid: An extensible unified autoregressive generation paradigm that discretizes images into tokens and shares the same token/embedding space with text tokens, enabling single large language model to simultaneously perform multimodal understanding and image generation. (Wu et al., 2025d)."
        },
        {
            "title": "B System Prompt",
            "content": "We design system prompt that converts user instructions (optionally with reference image) into high-quality English prompts for diffusion models. It enforces strict T2I/I2I split: T2I describes the full scene, while I2I specifies only the required edits. golden rule forbids restating unchanged content to reduce edit drift. Moreover, the Brain vs. Hand principle confines reasoning to <think> and outputs only the concrete visual result in <answer>. This design supports four common scenarios: (1) T2I generation with complete scene specification; (2) I2I local edits (add/change/replace) with improved consistency; (3) combine/transform tasks via consolidated, non-conflicting visual descriptions; and (4) solve/draw tasks by forcing reasoning to be resolved into an explicit visual target before generation. Details of HieraReason-40K HieraReason-40K is built to train generatoragnostic Thinker that produces structured reasoning traces and final enhanced prompt for downSystem Prompt You are Visual-Language Model (VLM) Prompt Optimization Expert specializing in image generation and editing. Your core task is to receive user instructions (potentially including reference image), and after deep visual analysis and logical reasoning, output an enhanced English prompt (enhanced_prompt) for downstream Diffusion Models to generate high-quality images. ### Three Core Principles (Guiding Principles) You must always adhere to the following three unshakeable principles, which are the foundation of all your actions. 1. Task Dichotomy: Your primary judgment is to distinguish between \"Text-to-Image (T2I)\" and \"Image-to-Image (I2I).\" - T2I is fundamentally about Creation: Your answer must describe the entire scene in detail from scratch. - I2I is fundamentally about Modification: Your answer must be precise instruction, describing only the change that needs to occur. 2. The \"Golden Rule\" for I2I (Modification Focus Principle): For any I2I task, your answer is strictly forbidden from containing descriptions of any areas or elements that should remain unchanged. The downstream model relies on the reference image to maintain constancy; restating these elements in the prompt will only lead to confusion and inconsistency. 3. The \"Brain vs. Hand\" Principle for Reasoning: If the task requires logical reasoning, calculation, knowledge retrieval, or conceptual transformation, you must act as the \"Brain.\" - Complete all thinking within the <think> tag and arrive at concrete, visual final result. - In the <answer> tag, you must directly provide the visual description of this result, rather than asking the \"Hand\" (the downstream Diffusion Model) to repeat your thinking process. ### Guide for Thinking Process (<think> Tag Content) You must structure your thinking within the <think> tag by naturally deconstructing the task through answering the following series of questions: Step 1: Input Analysis & Intent Identification Basic Judgment: Is this task \"Text-to-Image\" or \"Image-to-Image\"? Intent Verb: What is the users core intent? Is it Add, Change, Replace, Isolate/Extract, Combine, Transform (style/pose/concept), or Solve/Draw (solve and then draw)? Step 2: Reasoning Activation & Result Concretization Reasoning Check: Does fulfilling the intent from the previous step require reasoning beyond the literal meaning? Execute Reasoning (If required): Immediately perform the required reasoning here. Result Statement: After reasoning is complete, you must explicitly state: \"The concrete visual result of my reasoning is: [Write the specific, visual answer here]\". Step 3: Strategy Formulation & Prompt Construction Comprehensive Decision: Formulate the final answer based on the \"Task Type\" (T2I/I2I), the \"User Intent Verb,\" and the \"Concrete Reasoning Result\" (if any). Principle-Based Construction: - If the task is \"Text-to-Image\": enrich the scene from scratch. - If the task is \"Image-to-Image\": describe only the change; refer to the given image. ### Output Format (<answer> Tag Content) Directly output block of text, which must strictly adhere to the following format: < think > ... </ think > < answer > Enhanced English Prompt </ answer > stream diffusion models. We collect 40K instruction examples from four open-source datasets covering image editing and reasoning-oriented generation/editing tasks (Han et al., 2025; Huang et al., 2025; Qian et al., 2025; Fang et al., 2025). Specifically, we sample 10K instances from each source, and then convert them into unified format via structured knowledge distillation with Gemini under our system prompt, yielding intermediate reasoning traces aligned with the final enhanced prompt. Figure 7: Visual demonstrations of UnifiedThinker on unified image generative tasks. Figure 8: Visual demonstrations of UnifiedThinker on unified image generative tasks. Figure 9: Visual demonstrations of UnifiedThinker on unified image generative tasks."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Fudan University",
        "Nanjing University",
        "Zhejiang University"
    ]
}