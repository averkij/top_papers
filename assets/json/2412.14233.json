{
    "paper_title": "Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception",
    "authors": [
        "Yanpeng Sun",
        "Jing Hao",
        "Ke Zhu",
        "Jiang-Jiang Liu",
        "Yuxiang Zhao",
        "Xiaofan Li",
        "Gang Zhang",
        "Zechao Li",
        "Jingdong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training Large Multimodality Models (LMMs) relies on descriptive image caption that connects image and language. Existing methods either distill the caption from the LMM models or construct the captions from the internet images or by human. We propose to leverage off-the-shelf visual specialists, which were trained from annotated images initially not for image captioning, for enhancing the image caption. Our approach, named DCE, explores object low-level and fine-grained attributes (e.g., depth, emotion and fine-grained categories) and object relations (e.g., relative location and human-object-interaction (HOI)), and combine the attributes into the descriptive caption. Experiments demonstrate that such visual specialists are able to improve the performance for visual understanding tasks as well as reasoning that benefits from more accurate visual understanding. We will release the source code and the pipeline so that other visual specialists are easily combined into the pipeline. The complete source code of DCE pipeline and datasets will be available at \\url{https://github.com/syp2ysy/DCE}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 1 3 3 2 4 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Descriptive Caption Enhancement with Visual Specialists for Multimodal\nPerception",
            "content": "Yanpeng Sun1,2*, Jing Hao3 , Ke Zhu4, Jiang-Jiang Liu2, Yuxiang Yao2 Xiaofan Li2, Gang Zhang2, Zechao Li1, Jingdong Wang2 1Nanjing University of Science and Technology, 2Baidu VIS, 3The University of Hong Kong , {yanpeng sun, zechao.li}@njust.edu.cn 4Nanjing University"
        },
        {
            "title": "Abstract",
            "content": "Training Large Multimodality Models (LMMs) relies on descriptive image caption that connects image and language. Existing methods either distill the caption from the LMM models or construct the captions from the internet images or by human. We propose to leverage off-the-shelf visual specialists, which were trained from annotated images initially not for image captioning, for enhancing the image caption. Our approach, named DCE, explores object low-level and fine-grained attributes (e.g., depth, emotion and finegrained categories) and object relations (e.g., relative location and human-object-interaction (HOI)), and combine the attributes into the descriptive caption. Experiments demonstrate that such visual specialists are able to improve the performance for visual understanding tasks as well as reasoning that benefits from more accurate visual understanding. We will release the source code and the pipeline so that other visual specialists are easily combined into the pipeline. The complete source code of DCE pipeline and datasets will be available at https://github.com/ syp2ysy/DCE. advancements 1. Introduction Recent in Large multimodal models (LMMs) [53, 55, 60] have significantly enhanced the understanding and reasoning abilities for multimodal tasks. Vision-language connection is crucial for high abilities, and descriptive image captions serve as one of key components for image perception. The image caption is expected to describe the image as detailed and complete as possible. There are two main categories for image captions. One is to generate image captions from human annotation, such *This work was completed while Yanpeng Sun was an intern at Baidu VIS. Corresponding author. as COCO [30]and LAION [43]. However, the high cost of human annotation limits scalability. The other one is LMMs [1, 11] annotation, such as ShareGPT4V [8] and Densefusion [26]. While captions generated by LMMs offer better scalability, their comprehensiveness and accuracy often fall short. There is still much improvement space for the captions from SoTA LMMs, such as InternVL2 [11] and from current human annotation datasets. An example is given in Figure 1(a) for showing the improvement space. It can be seen that the caption from COCO [29] widely-used humanannotated dataset, is usually incomplete, only describes small portion of the image content, and lot of information is missing. The captions from LMMs are much better, but still can be improved in some aspects. As shown in Figure 1(b), we selected 8 objects and 5 key attributes from the image to analyze and compare the captions generated by different methods. The key attributes include finegrained attributes, spatial relation, and HOI. It is clear that captions generated by LMMs are more detailed than those annotated by humans. They not only describe more objects but also more attributes. However, captions from LMMs still much improvement space, as they tend to overlook important objects and attributes. For example, in Figure 1(a), Object 6 is completely ignored by all LMMs. Furthermore, crucial 3D spatial relationships between objects are also missing, which is particularly problematic for tasks that require comprehensive understanding of the scenes structure [21, 34]. Toward this end, we propose Descriptive Caption Enhancement Engine (DCE), designed to enable efficient and low cost image captioning. We leverage visual specialists [9, 48, 54, 59] to replicate various human visual capabilities, and subsequently employ large language models (LLMs) [4, 51] to simulate the human cognitive process. This combined approach enables us to generate high-quality image captions by closely mimicking the way humans per1 Figure 1. (a) We present comparison of captions from DCE, human, and generalist LMM models annotations, including InternVL2-26B, LLaVA-NeXT, and GPT-4V. (b) visualizes the extent to which the captions in (a) describe multiple objects and various attributes, including Objects 1-8, Object Attributes, OCR, HOI, 2D spatial relations and 3D spatial relations. ceive and interpret visual information. Notably, DCE relies solely on open-source visual expert models and LLMs, significantly reducing annotation costs. Specifically, we leverage existing visual specialists to obtain instance-level and relational attributes within images. Instance-level attributes focus on object low-level and fine-grained attributes (e.g., depth, emotion and finegrained categories). Relational-level attributes capture interactions and relationships between objects (e.g., relative location and HoI). Next, we use prompts to guide LLMs in combining object attributes into region captions. Finally, prompts are used again to integrate these region captions with relational-level attributes, producing comprehensive and detailed image caption. Since DCE utilizes multiple off-the-shelf visual specialists, the resulting captions capture wide array of detailed attributes and nuanced relationships, leading to richer and more precise image descriptions. As shown in Figure 1(b), captions annotated by DCE contain the most comprehensive information among all methods. Figure 2 quantitatively demonstrates that the captions generated by DCE provide greatest benefit to LMMs. We applied our DCE to annotate large-scale dataset of 1.1 million images, consisting of 1 million diverse images (DCE-1M) and 118K real-world scene images (DCE118K). Experiments were conducted using both LLaVAv1.5 and LLaVA-NeXT models. The results show that the highly detailed captions generated by DCE significantly enhance the perceptual capabilities of large multimodal models (LMMs), improving visual-language alignment. The outstanding performance of both LLaVA-v1.5 and LLaVANeXT across 14 benchmarks further underscores the effectiveness of our approach, confirming that the generated image captions are of exceptional detail and quality. Figure 2. Comparisons of caption quality. (a) and (b) show the downstream task performance of LLaVA-v1.5 and LLaVA-NeXT after pretraining with different image captions. 2. Related Work Large multi-modality models. There are recently lot of developments for large multi-modality models. One major effort lies in aligning/connecting the pretrained vision encoder and the the pretrained language model [50, 53, 55]. Flamingo [2] inserts new gated cross-attention layers between existing pretrained and frozen LM layers, that bridges powerful pretrained vision-only and language-only models. BLIP-2 [25] bridges the two modalities with lightweight querying transformer. Qwen-VL [4] adopt way that is similar to query transformer. LLaVA [33] adopt simple projection model to connect the visual encoder and the language model. The Llama3 Herd of Models [14] uses adapter similar to LLaVA and BLIP-2 to connect the vision encoder and the language model. Gemini [16] simply concatenates multi-modality tokens, such as image and text tokens, and feed them into transformer as the input. Emu [49] uses causal transformer to train the image encodings to tokens. There is also some effort lying in the encoder architecture for efficient image encoding. One approach involves dividing the image into Figure 3. The DCE pipeline first utilizes various visual specialists to extract both Object and Relation attributes. Then, it uses an LLM to integrate the object attributes into detailed region captions, followed by combining the region captions with relational attributes to generate comprehensive image caption. Table 1. Summary of attributes our approach extracts through visual specialists. It includes the specific attribute names, the models used, and the extraction process for each. Attributes Object Size Depth Emotion OCR Animal Plants Aircrafts Logo Landmark Food Celebrity Relation Visual Specialists Detailed Process Detection model Depth & Detection model Emotion model OCR Model Fine Grained model Using the area of the bounding box to measure the size of the instance. Average the depth map values within the bounding box region to obtain the depth information. If the detected region is labeled as person, an emotion model is used to extract an emotion label. Using an OCR model to extract the text content and bounding box from the region. fine-grained recognition model to identify specific species of the animal. fine-grained recognition model to identify specific species of the plants. fine-grained recognition model to identify specific model of the aircraft. fine-grained recognition model to identify logos in the region. fine-grained recognition model to identify landmarks within the region. fine-grained recognition model to identify specific species of the food. Using fine-grained recognition model to identify celebrity within the region. P2O relation HOI Model Count Detection model 2D Absolute Location Detection model 2D Relative Location Detection model 3D Relative Location Detection & Depth model Using an HOI model to determine the relationship between the person and the object, while the bounding boxes of both the person and the object define their respective regions. Counting the number of all objects in the image based on the detection results. Using the bounding box to determine the instances position within the image, including regions such as left, right, top, bottom, center, top-left, bottom-left, top-right, and bottom-right. Using the bounding box to determine the relative position among multiple objects within the image, including regions such as left, right, near, next to, close by, and so on. Using the depth attributes of different instances to capture the 3D spatial relationships of objects relative to the camera, such as Instance is in front of Instance or Instance is behind of Instance relative to the camera. large blocks, which helps capture finer-grained features while maintaining computational efficiency, such as Monkey [28], Qwen2-VL [52], LLaVA-NeXT [24]. Additionally, many works have further enhanced LMMs by focusing on the quality and diversity of pretraining and finetuning data [4, 6, 7, 22, 40, 53]. Despite the success of these LMMs, few studies have focused on obtaining large-scale high-quality image-text data, which is crucial factor in driving the capabilities of LMMs. Descriptive captions. CC3M [45] (Conceptual Captions: Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning) is harvested from the Alt-text HTML attribute associated with web images followed by an automatic pipeline that extracts, filters, and transforms candidate image/caption pairs. CC12M [5] extends CC3M [45] with an emphasis on long-tail visual recognition. It builds on the same foundational methodology as CC3M but incorporates more diverse range of objects, scenes, and rare visual concepts to better represent the breadth of realworld imagery. SBU Captions [39] forms the data with images and descriptions sourced from Flickr. The captions are manually written by users and describe wide variety of visual content, from everyday scenes to more specialized imagery. COCO-captions [10] is built by collecting captions on Flickr images that are generated by human subjects on Amazons Mechanical Turk (AMT). While manually annotated captions effectively capture the main information in images, they often overlook important details and contextual richness. To address this, methods like ShareGPT4V [8] and Densefusion [26] leverage LLMs [8, 3 Figure 4. The prompt for using LLM to generate an region caption by considering object attributes and reference captions. 52] to generate more detailed captions. However, these LMM-based approaches still face challenges, particularly with over-simplified or inaccurate descriptions, which can compromise caption quality and reliability [21, 41]. Balancing detail richness and accuracy remains key challenge in current research. DenseFusion and our DCE share similar goal of generating more accurate image captions by integrating diverse visual information. However, DenseFusion relies on GPT-4V, highly expensive model, whereas our approach significantly reduces costs and improves efficiency by utilizing open-source visual expert models and LLMs. 3. Approach As shown in Figure 3, DCE leverage existing visual specialists to extract visual properties for improving descriptive captions. We explore two kinds of properties: objectlevel attributes and object-relations. Our approach consists of: object localization using SoTA open-world object detection specialist, object property extraction using various specialists, and object relation extraction between objects. 3.1. Object Attributes Object localization. We combine in-domain detection models [9, 38] and open-world detection models [18] for robust detection, merging bounding boxes from both models with confidence scores above 0.5. The object detection model outputs the location and semantic information of objects that existed in the image. After detecting these regions, we apply Non-Maximum Suppression (NMS) to eliminate redundant or overlapping boxes. The IoU threshold for NMS is 0.75. Attribute extraction by visual specialists. Table 1 presents all the instance-level attributes involved in DCE, along with their extraction processes. The attributes currently include three main parts. (1) Fine-grained object category attributes. This is rarely explored in current multimodal models [8]. In DCE, we incorporate fine-grained details by introducing various specialized models, covering categories such as animals, plants, food, logo, aircraft, landmarks, and celebrities. The animal and plant attributes contain the fine-grained category of 891k and 427k species of animals and plants, respectively. Each species within this category possesses unique characteristics and behaviors, making it rich and varied classification. The food 4 Figure 5. The prompt for LLM to generate an image caption by considering relation attributes, region location information and captions. attributes include variety of food types commonly found in daily life, while logos are graphic symbols that serve as visual identifiers for conveying messages. Landmarks are the famous tourist attractions that hold cultural, historical, or geographical significance. Aircraft refers to the machines designed for flight, including airplanes, helicopters, drones, and other aerial vehicles. Celebrities are individuals widely recognized in public life, entertainment, sports, or other fields. All this specific and fine-grained information could be regarded as the external world knowledge, aligning the textual content in the image with basic human cognition. (2) Low-level and emotion attributes. It includes emotion, depth, and size. (3) OCR. It is one of the important attributes for multimodal models. The specific visual expert models we use will be presented in the Appendix. 3.2. Object Relation DCE can extract relationships between multiple objects. We consider three categories: the interactions between humans and objects, the 2D as well as 3D relative positional relationships among different objects, and the object counting information. Table 1 presents the relation attributes and their extraction process. The human-object interaction provides essential inforTable 2. Human evaluation of attribute richness, conducted on 100 validation samples with 10 volunteers. Attributes Spatial Relation HOI Fine-Grained OCR Emotion Location InternVL2 0.57 0.92 0.16 0.26 0.23 0.36 LLaVA-NeXT 0.62 0.86 0.08 0.33 0.14 0.59 DCE 0.75 0.92 0.24 0.48 0.47 0. mation about the actions and activities performed by humans with the objects. We utilize the human-object interaction (HOI) model to detect interactive activities between humans and objects in the image. The interactions detected by the HOI model can be used to supplement events not mentioned in the caption. The 2D positional information captures the spatial relationships of objects, comprising both 2D Absolute Location and 2D Relative Location. The 2D Absolute Location describes an objects position relative to the image (e.g., Object is on the left side of the image), while the 2D Relative Location describes positional relationships between objects (e.g., Object is next to Object B). We use the bounding boxes of objects to determine their positional relationships. The 3D relative positional information captures the spa5 tial relationships of objects in 3D space, defining both their absolute positions in the scene and their relative positioning (e.g., Object is in front of Object or at specific angle). This information enhances the understanding of scenes 3D structure and provides richer spatial awareness. We leverage the depth differences between objects to determine their 3D positional relationships. 3.3. Captioning with Attributes Region captioning. We use large language model (Qwen72B) to integrate the object attributes with the caption from large multimodal model (InternVL2-26B). For example, in cases where the fine-grained model for animals like {animal name}, identifies specific class label we employ the prompt {cat name} exists in the region and {animal name} is the {cat name}s subclass; use {animal name} in the caption; otherwise, do not mention {animal name} to guide the LLM in integrating this speIn this process, {cat name} cific label into the caption. represents the coarse-grained label provided by the detection model. The detailed prompt engineering process is illustrated in Figure 4. Image captioning. We combine object relation attributes, and region location information (for object grounding) and captions to get improved image captions. We use LLM for the combination. The prompt is given in Figure 5. For example, to describe the 3D relative positional relationship between {bbox 0} and {bbox 1}, we use the prompt: Relative to the camera, the {category 0} in {bbox 0} in the image is {3d relation} {category 1} in {bbox 1}, where {3d relation} can be in front of or behind of. We then use large language model (Qwen2-72B-AWQ) to integrate this information with the detailed region caption and the image reference caption. 3.4. Analysis Dataset Description. We leverages DCE to generate more detailed annotations for publicly available image datasets, resulting in two enhanced datasets: (1) DCE-1M, which provides dense annotations for 1 million diverse images from Densefusion [26], covering wide range of objects and scenes, and (2) DCE-118K, which includes refined annotations for 118,000 complex scene images from the COCO dataset [30]. We provide detailed analysis of the DCE-annotated image captions in the Appendix. Attribute richness. We randomly select 100 images from the DCE-118K and invite five independent evaluators to assess the captions for each image. The evaluators analyzed and recorded the occurrence of the attributes within the captions, such as spatial relations, HOI, and OCR (optical character recognition) details. The results as show in Table 2. Compared to other models and human annotations, our approach demonstrated greater ability to capture and express wider diversity of visual attributes present in the images. This suggests that our approach provided richer and more detailed descriptions of the visual content. Comparison. We conducted comparison of human annotations, internVL2-26B, LLaVA-NeXT-34B, and DCEannotated 118K datasets on both LLaVA-v1.5 and LLaVANeXT. The results as shown in Figure 2, the captions annotated by DCE significantly improve the performance of LLaVA-v1.5 and LLaVA-NeXT. Compared to human and generalist multimodal model annotations, DCE-annotated captions are richer and more detailed, offering deeper context and capturing finer nuances. This enhanced descriptive quality leads to better performance in downstream tasks. 4. Experiments In this section, we first present the implementation details, then compare the DCE-1M-trained model with state-of-theart LMMs across multiple benchmarks. Finally, we perform ablation studies on the DCE-118K-trained model to validate the effectiveness of DCE. 4.1. implementation details Model and Training Set. We conduct experiments on LLaVA-v1.5 [31] and LLaVA-NeXT [32] to demonstrate the effectiveness of DCE. Specifically, we using CLIPL [42] as the visual encoder and Vicuna-v1.5 [12] as the large language model. We adopt two-stage training strategy: (1) Pre-Training Stage. We train only the projector for initial alignment. Then, following SharGPT4V [8], we set the last 12 layers of the visual encoder in LLaVA-v1.5 as trainable and make the entire LLaVA-NeXT model trainable, following [23], to further enhance perceptual capabilities. (2) Instruction Tuning Stage. We use the open-source LLaVA-mix-665K and LLaVA-NeXT-data to respectively train the LLaVA-v1.5 and LLaVA-NeXT models. The detailed training procedure is provided in the supplementary material. Evaluation Benchmarks. We evaluate on seven visual question answering (VQA) tasks across domains such as document understanding, general knowledge, and scientific reasoning, including VQAv2 [15], DocVQA [37], OKVQA [44], GQA [17], TextVQA [47], ScienceQA [35], and Ai2d [19]. Additionally, we evaluate performance on five widely used LMM benchmarks designed to test multimodal models on visual grounding, scene understanding, and generalization: MMBench [34], MM-Vet [56], SEED [21], MMMU [57], and POPE [27]. Comparison Method. We compare LLaVA-v1.5 and LLaVA-NeXT models trained on DCE-1M against current SOTA MLLMs. Additionally, we evaluate the performance of various MLLMs in generating image captions, using the advanced models InternVL2-26B [11] and LLaVANeXT-34B [24] to generate competitive captions on the DCE-118K images. Table 3. Performance on seven General Visual Question Answering benchmarks. The red and blue colors respectively represent the optimal and suboptimal results on each benchmark. indicates the use of LLaVA-NeXTs open-source SFT data, with certain private data excluded. Model LLM VQAv2 DocVQA Visual Question Answering Benchmarks GQA TextVQA OKVQA Low Resolution Models BLIP2 [25] InstructBLIP [13] InstructBLIP [13] IDEFICS-Instruct [20] OpenFlamingo [3] InternVL-Chat [11] Qwen-VL-Chat [4] mPLUG-Owl2 [55] LLaVA-v1.5 [31] ShareGPT4V [8] LLaVA-v1.5(Ours) High Resolution Models Monkey [28] LLaVA-NeXT [24] LLaVA-S2 [46] LLaVA-HR [36] LLaVA-NeXT(Ours) Flan-T5 Vicuna-7B Vicuna-13B LLaMA-65B MPT-7B Vicuna-7B Qwen-7B LLaMA-7B Vicuna-7B Vicuna-7B Vicuna-7B Qwen-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B 41.0 - - 60.0 53.0 79.3 78.2 79.4 78.5 80.6 80.9 80.3 81.8 79.7 81.9 82.4 - - - - - - 62.6 - 28.1 - 39.1 66.5 74.4 - - 78. 45.9 - - 36.9 38.3 51.8 56.6 57.7 - - 57.2 61.3 44.3 - 58.9 57.2 41.0 49.2 49.5 - - 62.9 57.5 56.1 62.0 63.3 64.2 60.7 64.2 63.3 64.2 65.2 42.5 50.1 50.7 32.9 28.3 57.0 61.5 58.2 58.2 60.4 61.4 67.6 64.9 60.8 67.1 64. ScienceQA Ai2d 61.0 60.5 63.1 61.8 44.8 - 68.2 68.7 66.8 68.4 71.0 69.4 70.1 68.2 65.1 71.2 - 40.6 - 54.8 - - - 55.7 55.5 - 59.4 62.6 66.6 - - 71. Table 4. Performance on seven Large Multi-Modal benchmarks. The red and blue colors respectively represent the optimal and suboptimal results on each benchmark. indicates the use of LLaVA-NeXTs open-source SFT data, with certain private data excluded. Method Low Resolution Models BLIP-2 [25] MiniGPT-4 [58] InstructBLIP LLaMA-Adapter-v2 [14] OpenFlamingo [3] Otter [22] Qwen-VL-Chat [4] mPLUG-Owl2 [55] LLaVA-v1.5 [31] ShareGPT4V [8] LLaVA-v1.5(Ours) High Resolution Models LLaVA-NeXT [24] ShareGPT4V-S2 [26] LLaVA-S2 [46] LLaVA-HR [36] LLaVA-NeXT(Ours) 4.2. Main Results Vision Encoder Language Model MMBench-CN MMBench MM-Vet SEEDI SEED-Bench MMMU POPE ViT-g (1.3B) ViT-g (1.3B) ViT-g (1.3B) ViT-L (0.3B) ViT-L (0.3B) ViT-L (0.3B) ViT-G (1.9B) ViT-L (0.3B) ViT-L (0.3B) ViT-L (0.3B) ViT-L (0.3B) ViT-L (0.3B) ViT-L (0.3B) ViT-L (0.3B) ViT-L (0.3B) ViT-L (0.3B) Vicuna-7B Vicuna-7B Vicuna-7B LLaMA-7B MPT-7B LLaMA-7B Qwen-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B - 11.9 23.7 - - - 56.7 - 57.6 62.2 60.0 60.6 - - - 61. - 23.0 36.0 39.5 5.7 48.3 60.6 64.5 64.3 68.8 69.2 67.4 68.0 66.4 - 69.3 22.4 22.1 26.2 31.4 24.8 24.6 - 36.2 30.5 37.6 38.2 43.9 35.0 34.6 31.2 40.1 46.4 - 53.4 - - - 58.2 - 66.2 69.7 70.3 70.2 70.1 67.2 - 72. - 47.4 - 32.7 42.7 32.9 - 57.8 58.6 61.9 64.3 64.7 62.4 59.9 64.2 65.7 - 23.6 30.6 - 26.3 - 29.6 34.7 35.3 - 36.3 35.1 - - - 36.0 85.3 - 78.9 - - - - 86.2 85.9 85.7 86.4 86.5 86.7 86.7 87.6 87. VQA Benchmarks. The results on six common visual question answering (VQA) datasets are presented in Table 3. It is clear that LLaVA-v1.5 and LLaVA-NeXT, trained with DCE-1M, achieve state-of-the-art performance in both low-resolution and high-resolution settings. Compared to the baseline LLaVA-v1.5 [31], our model excels across all VQA benchmarks, demonstrating that highquality image captions significantly enhance model performance. This highlights the crucial role of detailed and accurate captions in improving visual understanding for VQA tasks. Furthermore, when compared to the baseline LLaVANeXT [24], this improvement remains consistent, suggesting that the impact of high-quality captions is not dependent 7 on the model variation. Additionally, compared to models like ShareGPT-4V, our model demonstrates superior performance across most VQA benchmarks. This improvement indicates that the captions generated by our DCE method provide richer and more comprehensive information. The model trained on DCE-1M demonstrates exceptional performance on datasets such as VQAv2 [15] and Ai2D [19], highlighting that integrating detection models into DCE significantly enriches the diversity of objects in the generated captions, thereby boosting the models performance on object recognition benchmarks. Furthermore, incorporating relational attributes into DCE enriches the captions with detailed inter-object relationships, enhancing the models ability to capture and understand complex relationAnnotation LLaVA-v1.5 + human [29] + InternVL2-26B [11] + LLaVA-NeXT-34B [24] + DCE LLaVA-NeXT + InternVL2-26B [11] + LLaVA-NeXT-34B [24] + DCE Table 5. Comparison of Different Image Captioning Annotation Methods. OKVQA ScienceQA MMBench TextVQA GQA MM-Vet SEED-Bench 54.9 54.7 55.7 56.9 54.3 54.3 56.7 62.4 63.0 62.9 63.2 65.1 65.0 65.2 68.6 69.1 68.8 69.8 70.1 70.5 72. 58.1 58.4 58.7 58.9 61.2 61.0 62.0 65.0 64.8 65.3 66.6 66.7 67.2 68.5 31.6 32.7 33.0 33.9 37.3 37.2 37. 61.1 61.8 61.7 62.0 64.7 64.5 65.0 Figure 6. Visualization of DCEs Attribute Fusion: DCE combines object and relational attributes to generate detailed and comprehensive captions. ships, which further improves its performance on visual reasoning benchmarks such as GQA [17]. However, performance on tasks like TextVQA [47] is hindered by limitations in the open-source OCR model and the threshold settings; high threshold restricts the models ability to capture finer textual details. 4.3. Ablation Study Comparing different annotation methods. We compared different image annotation methods, including human annotations, GenerateList LMM annotations, and our DCE. Specifically, we annotated 118K COCO images and conducted comparisons on LLaVA-v1.5 and LLaVA-NeXT. The experimental results are shown in Table 5. We found that the image captions generated by DCE improve LMM performance on downstream tasks more effectively than other annotation methods. Notably, compared to captions annotated by internVL2, DCEs inclusion of object attributes significantly improves model performance on OKVQA and TextVQA tasks. Relational attributes enhance the models understanding of multi-object relationships, leading to notable increase in GQA performance. Furthermore, superior results on complex LMM benchmarks like MM-Vet, MMBench, and SEED highlight that DCEgenerated captions provide rich and comprehensive scene information. Case Study. Figure 6 presents example captions generated by the DCE engine and the general MLLM LLaVANeXT 34B. It is evident that visual specialists within DCE capture detailed object and relational attributes, resulting in richer and more descriptive captions. For instance, in Figure 6(a), the OCR information highlights the precision of OCR specialist model, while the fine-grained models identification of specific aircraft types further enhances caption informativeness. Additionally, the relational attributes significantly enrich description by providing detailed spatial relationships between objects, underscoring the advantages of DCE in capturing comprehensive scene details. More viLarge Multi-Modal Benchmarks. We further conduct the evaluation on five challenging large multi-modal benchmarks. The experimental results are shown in Table 4. It can be seen that both LLaVA-v1.5 and LLaVA-NeXT trained with DCE-1M achieve competitive performance on more complex LMM benchmarks, demonstrating that the improvements brought by DCE-1M are comprehensive. Our model outperforms both LLaVA-v1.5 [31] and LLaVANeXT [32] across all LMM benchmarks, demonstrating that high-quality image captions during pretraining significantly enhance model performance, even without altering the supervised fine-tuning (SFT) data. Compared to other image captioning methods, such as ShareGPT-4V [8], DCEgenerated captions provide richer and more comprehensive scene information, significantly boosting model performance across most LMM benchmarks. However, due to the lack of Chinese data in DCE-1M, the model performs poorly on MMBench-CN [34]. This highlights the need for multilingual image captioning, which will be an area for future improvement in DCE. Additionally, the detection model in DCE introduces some noise, which may interfere with the models ability to accurately capture objects, leading to decreased performance on tasks like POPE [27]. Therefore, reducing this noise will be key focus for future improvements in DCE. 8 sualization results are provided in the Appendix. 5. Conclusion In this paper, we propose new image captioning engine, DCE , which utilizes off-the-shelf visual specialists to enhance the quality and detail of image captions. Unlike existing methods that primarily rely on LMM models or human annotation, DCE leverages visual specialists to simulate human perceptual abilities, while using LLMs to emulate cognitive processes. This dual approach enables DCE to generate captions that are both visually detailed and contextually aware. Through extensive experiments, we demonstrate that incorporating these visual specialists leads to improved model performance across various visual understanding and reasoning tasks, particularly those that depend on accurate attribute and relationship recognition. Our approach highlights the potential of leveraging specialized visual features for enhancing multimodal representations and provides flexible framework for future integration of additional visual expertise. We will release the DCE source code and pipeline to facilitate further research, enabling the community to easily integrate other visual specialists and extend the capabilities of multimodal models."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In Advances in neural information processing systems, pages 2371623736, 2022. 2 [3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive visionlanguage models. arXiv preprint arXiv:2308.01390, 2023. 7 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. 2023. 1, 2, 3, 7 [5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35583568, 2021. 3 [6] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. large language model as unified interface Minigpt-v2: for vision-language multi-task learning. arXiv:2310.09478, 2023. 3 arXiv preprint [7] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 3 [8] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 1, 3, 4, 6, 7, 8 [9] Qiang Chen, Xiaokang Chen, Jian Wang, Shan Zhang, Kun Yao, Haocheng Feng, Junyu Han, Errui Ding, Gang Zeng, and Jingdong Wang. Group detr: Fast detr training with group-wise one-to-many assignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 66336642, 2023. 1, 4 [10] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 1, 6, 7, 8 [12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. 6 [13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024. 7 [14] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. 2, 7 [15] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 69046913, 2017. 6, 7 [16] Jing Hao, Moyun Liu, and Kuo Feng Hung. Gem: Boost simple network for glass surface segmentation via segment anything model and data synthesis. arXiv preprint arXiv:2401.15282, 2024. 2 [17] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6700 6709, 2019. 6, 9 [18] KJ Joseph, Salman Khan, Fahad Shahbaz Khan, and Vineeth Balasubramanian. Towards open world object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58305840, 2021. 4 [19] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European Conference on Computer Vision, pages 235251, 2016. 6, 7 [20] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. In Advances in Neural Information Processing Systems, 2024. 7 [21] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 1, 4, 6 [22] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal arXiv preprint model with in-context instruction tuning. arXiv:2305.03726, 2023. 3, [23] Bo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan Zhang, Renrui Zhang, Feng Li, Ziwei Liu, and Chunyuan Li. Llava-next: What else influences visual instruction tuning beyond data?, 2024. 6 [24] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024. 3, 6, 7, 8 [25] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 2, 7 [26] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. arXiv preprint arXiv:2407.08303, 2024. 1, 3, 6, 7 [27] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. 6, 8 [28] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2676326773, 2024. 3, [29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 1, 8 [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. European Conference on Computer Vision, pages 740755. Springer, 2014. 1, 6 [31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 6, 7, 8 [32] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 6, 8 [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in neural information processing systems, 2024. 2 [34] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 1, 6, [35] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Advances in Neural Information Processing Systems, pages 25072521, 2022. 6 [36] Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-ofresolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003, 2024. 7 [37] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. In ProDocvqa: dataset for vqa on document images. ceedings of the IEEE winter conference on applications of computer vision, pages 22002209, 2021. 6 [38] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In Proceedings of the IEEE/CVF international conference on computer vision, pages 36513660, 2021. 4 [39] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. In Advances in Neural Information Processing Systems, 2011. 3 [40] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [41] Renjie Pi, Jianshu Zhang, Jipeng Zhang, Rui Pan, Zhekai Chen, and Tong Zhang. Image textualization: An automatic framework for creating accurate and detailed image descriptions. arXiv preprint arXiv:2406.07502, 2024. 4 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [43] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training 10 [55] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language arXiv preprint model with modality collaboration. arXiv:2311.04257, 2023. 1, 2, 7 [56] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 6 [57] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [58] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 7 [59] Ke Zhu, Yin-Yin He, and Jianxin Wu. Quantized feature distillation for network quantization. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11452 11460, 2023. 1 [60] Ke Zhu, Liang Zhao, Zheng Ge, and Xiangyu Zhang. SelfIn Proceedings of supervised visual preference alignment. ACM International Conference on Multimedia, pages 291 300, 2024. 1 next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 1 [44] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146162. Springer, 2022. 6 [45] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, 2018. 3 [46] Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, and Trevor In Darrell. When do we not need larger vision models? European Conference on Computer Vision, pages 444462. Springer, 2025. [47] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 83178326, 2019. 6, 8 [48] Samuel Stevens, Jiaman Wu, Matthew Thompson, Elizabeth Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, Wasila Dahdul, Charles Stewart, Tanya BergerWolf, et al. Bioclip: vision foundation model for the tree of life. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1941219424, 2024. 1 [49] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. In International Conference on Learning Representations, 2023. 2 [50] Yanpeng Sun, Huaxin Zhang, Qiang Chen, Xinyu Zhang, Nong Sang, Gang Zhang, Jingdong Wang, and Zechao Li. Improving multi-modal large language model through boosting vision capabilities. arXiv preprint arXiv:2410.13733, 2024. 2 [51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [52] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3, [53] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 1, 2, 3 [54] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024."
        }
    ],
    "affiliations": [
        "Baidu VIS",
        "Nanjing University",
        "Nanjing University of Science and Technology",
        "The University of Hong Kong"
    ]
}