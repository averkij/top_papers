{
    "paper_title": "StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs",
    "authors": [
        "Yuhan Song",
        "Linhao Zhang",
        "Chuhan Wu",
        "Aiwei Liu",
        "Wei Jia",
        "Houfeng Wang",
        "Xiao Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Prevalent semantic speech tokenizers, designed to capture linguistic content, are surprisingly fragile. We find they are not robust to meaning-irrelevant acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech is perfectly intelligible, their output token sequences can change drastically, increasing the learning burden for downstream LLMs. This instability stems from two flaws: a brittle single-path quantization architecture and a distant training signal indifferent to intermediate token stability. To address this, we introduce StableToken, a tokenizer that achieves stability through a consensus-driven mechanism. Its multi-branch architecture processes audio in parallel, and these representations are merged via a powerful bit-wise voting mechanism to form a single, stable token sequence. StableToken sets a new state-of-the-art in token stability, drastically reducing Unit Edit Distance (UED) under diverse noise conditions. This foundational stability translates directly to downstream benefits, significantly improving the robustness of SpeechLLMs on a variety of tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 2 2 2 2 . 9 0 5 2 : r STABLETOKEN: NOISE-ROBUST SEMANTIC SPEECH TOKENIZER FOR RESILIENT SPEECHLLMS Yuhan Song1, Linhao Zhang2, Chuhan Wu2, Aiwei Liu2, Wei Jia2 Houfeng Wang1, Xiao Zhou2 1State Key Laboratory of Multimedia Information Processing, Peking University 2Pattern Recognition Center, WeChat AI, Tencent Inc songyuhan@pku.edu.cn zhanglinhao90@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Prevalent semantic speech tokenizers, designed to capture linguistic content, are surprisingly fragile. We find they are not robust to meaning-irrelevant acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech is perfectly intelligible, their output token sequences can change drastically, increasing the learning burden for downstream LLMs. This instability stems from two flaws: brittle single-path quantization architecture and distant training signal indifferent to intermediate token stability. To address this, we introduce StableToken, tokenizer that achieves stability through consensus-driven mechanism. Its multi-branch architecture processes audio in parallel, and these representations are merged via powerful bit-wise voting mechanism to form single, stable token sequence. StableToken sets new state-of-the-art in token stability, drastically reducing Unit Edit Distance (UED) under diverse noise conditions. This foundational stability translates directly to downstream benefits, significantly improving the robustness of SpeechLLMs on variety of tasks."
        },
        {
            "title": "INTRODUCTION",
            "content": "The application of Large Language Models (LLMs) to the speech domain has given rise to new class of powerful models: Speech Large Language Models (SpeechLLMs) (Hurst et al., 2024; Défossez et al., 2024; Zeng et al., 2024). These models rely on discrete speech tokenizer to convert continuous audio into tokens sequences that the LLM can process. Among available methods, semantic tokenizers have been widely adopted, as their low-bitrate, semantically-aligned outputs are highly compatible with LLM architectures (Défossez et al., 2024; Zeng et al., 2024; Ding et al., 2025; Wu et al., 2025). The design of semantic speech tokenizers has evolved from early self-supervised learning (SSL) methods (Hsu et al., 2021; Baevski et al., 2020) towards more direct, supervised paradigm (Du et al., 2024a;b; Zeng et al., 2024). This modern paradigm centers on optimizing VQ-based quantizer (Van Den Oord et al., 2017) with direct, end-to-end objective such as automatic speech recognition (ASR) . This powerful combination has proven highly effective at producing semantically-rich and compact discrete representations, leading to the widespread adoption of supervised semantic tokenizers as the backbone of many modern SpeechLLMs (Zeng et al., 2024; Ding et al., 2025; Wu et al., 2025; Huang et al., 2025; Fang et al., 2025). Despite their widespread adoption and apparent success, we find that these semantic tokenizers harbor critical vulnerability: profound lack of robustness. Contrary to their core design principle of encoding semantics, even imperceptible acoustic noise can induce drastic shifts in their discrete outputs (Figure 1). The general issue of tokenizer instability is also supported by recent findings on earlier non-VQ-based SSL tokenizers (Messica & Adi, 2024). This instability creates damaging downstream effect: small acoustic changes trigger large token jumps, which break the crucial speechtext alignment and pose significant modeling challenge for the LLM, forcing it to learn from an inconsistent or even chaotic input stream. This inherent fragility is severely amplified by Equal contribution. Work conducted during Yuhans internship at WeChat. Corresponding author. 1 Figure 1: Illustration of StableToken: unlike traditional methods, StableToken yields consistent token sequences under minute perturbations with different Signal-to-Noise Ratios (SNRs). Robustness is measured by Unit Edit Distance (UED, ) between token sequences of the original and noiseperturbed audio. StableToken achieves significantly lower UED, indicating enhanced token stability. environmental noise, serving as key cause for the performance degradation of SpeechLLMs in real-world conditions (Ma et al., 2025b; Zhang et al., 2025; Yang et al., 2024c; Jiang et al., 2025). We argue that enhancing tokenizer robustness is therefore direct and promising path toward building more resilient models. We pinpoint the fragility of semantic tokenizers to two fundamental weaknesses. First, an architectural flaw: these tokenizers rely on single-path quantization, design that lacks fault tolerance. minor perturbation near quantization boundary is inevitably magnified into completely different output token. This architectural vulnerability is then compounded by distant supervisory signal: the standard ASR loss is indifferent to this intermediate token instability, as it only supervises the final transcribed text. This allows models to converge on solutions that are functionally correct but representationally fragile. The dual challenge of brittle architecture and distant supervisory signal necessitates new tokenization paradigm. Addressing this dual challenge is non-trivial. For the brittle architecture, an offline ensemble of models seems intuitive. However, this approach is untenable: (1) it prohibitively increases inference cost; (2) aggregating independently trained models is non-trivial, as their quantization boundaries are arbitrarily aligned; and (3) token-level majority vote is too coarse. To tackle the distant supervisory signal, one might introduce token-level consistency objective for clean and noisy input audios. Yet, this leads to notoriously unstable gradients when applied to discrete codes, making the model difficult to train. The failure of these straightforward approaches underscores the need for more integrated paradigm. We propose StableToken, which integrates co-designed architecture and training strategy to overcome the dual challenges of architectural fragility and distant supervision. Architecturally, it introduces the voting-LFQ modulea multi-branch quantizer extended from the LFQ algorithm (Yu et al., 2023), with negligible inference overhead. Its core mechanism is differentiable bit-level majority vote. During training, this enables more fine-grained fusion of multi-branch information, leading to more stable and robust representation learning. At inference, this same mechanism provides profound error-correction, operating at the granular bit-level rather than the coarse token-level. This distinction is critical: not only does it ensure the final token remains correct when minority of branches err due to noise, but it can even recover the token when majority of branches fail at the token-level, as long as the underlying bit-level errors remain sparse. This architectural robustness is further solidified by tailored training strategy. We present the model with multiple \"views\" of an inputa clean version to majority of branches and perturbed version to random minorityto create stable reference. consensus loss then leverages this reference to provide the explicit, intermediate supervision. The multi-branch architecture and multiview training strategy are thus deeply intertwined: the architecture provides the necessary structure for the training signal, and the signal in turn unlocks the architectures full potential. We validate StableToken through comprehensive experiments. At the tokenizer level, it achieves new state-of-the-art in noise robustness, slashing the Unit Edit Distance (UED) by over 60% relative (from 26.17% to 10.17%), all while maintaining top-tier reconstruction fidelity. This foundational superiority translates directly to downstream SpeechLLMs. In speech understanding, 2 Figure 2: The architecture of StableToken. Our model replaces the standard single-path quantizer with multi-branch Voting-LFQ module. The zoomed-in view shows parallel branches generating independent binary representations. bit-wise majority vote then aggregates them into single token. During our Noise-Aware Consensus Training, randomly selected minority of branches receive perturbed inputs (Hperturbed), while the majority receive clean inputs (Hclean). consensus loss forces the perturbed branches to align with the consensus. The yellow paths are only used during training. the downstream models yield significant robustness gains that are especially pronounced under severe noise. The performance gap between StableToken and baselines widens dramatically as the noise level increases. Similarly, for speech generation, the enhanced token consistency simplifies the learning task, resulting in substantially superior synthesis quality of downstream models. These results confirm that improving tokenizer robustness is directly and highly effective for building more resilient speechLLMs."
        },
        {
            "title": "2 METHODS",
            "content": "2.1 OVERALL STRUCTURE Our approach, StableToken, is designed to overcome the fragility of prevailing VQ-based semantic tokenizers. These tokenizers often produce unstable token sequences in the presence of subtle noise, vulnerability stemming from two core weaknesses: (1) single-path architecture that lacks fault tolerance, and (2) distant supervisory signal that fail to enforce representational invariance. StableToken adopts the architectural paradigm, established in works like (Du et al., 2024a; Zeng et al., 2024; Du et al., 2024b) of embedding semantic tokenizer within an end-to-end ASR model. However, our approach fundamentally enhances this design by introducing two synergistic innovations to address its inherent instabilities: (1) the Voting-LFQ Module, multi-branch quantizer that builds in architectural robustness, and (2) Noise-Aware Consensus Training, training strategy that explicitly enforces invariance to acoustic perturbations. 2.2 THE VOTING-LFQ MODULE The foundation of StableToken is novel quantizer architecture designed for intrinsic robustness. As shown in Figure 2, the pretrained speech encoder first processes the input speech into sequence of hidden states. These states are then downsampled via average pooling to produce compact representation, RD, for each time step. While traditional quantizers map to token in single, brittle step, our Voting Look-up-Free Quantizer (Voting-LFQ) is founded on redundancy and consensus. It begins by creating independent \"perspectives\" of the input state using parallel linear projection layers. Each branch {1, . . . , n} computes projected vector pi Rd: pi = Wih + bi, (1) where Wi RdD and bi Rd are the unique learnable parameters for that branch. Each projected vector is then binarized into Bi {1, +1}d using the non-differentiable sign function, i.e., Bi = sign(pi). We use the Straight-Through Estimator (STE) (Bengio et al., 2013) to enable end-to-end training. During training, we aggregate these binary vectors in bit-wise manner by averaging their values across branches for every dimension {1, . . . , d}, resulting in real-valued score: (sfinal)j = 1 n (cid:88) i=1 (Bi)j. (2) Unlike the rigid assignment of single bit (+1 or 1), these averaged scores can take nuanced values representing the confidence or consensus of all branches. This provides the model with richer feedback during optimization, helping it learn more robust and informative representations. During inference, we perform one additional step to apply the sign function to these aggregated scores to obtain the final consensus-based binary vector: (Bfinal)j = sign ((sfinal)j) . (3) By using an odd number of parallel branches (n), we enforce strict majority rule via bit-wise vote, creating exceptional robustness against noise. This approach not only corrects errors when minority of branches fail, but can also recover the true token even if majority of branches are corrupted at the token-level. Recovery is possible as long as the underlying bit-level errors remain sparse. This resilience marks significant advantage over fragile single-path quantizers and, in parallel, allows for more expressive representations during training. Finally, by mapping its 1 and +1 entries to 0 and 1 respectively and treating the {0, 1} representation as binary number, the stabilized binary vector Bfinal is deterministically mapped to an integer index {0, . . . , 2d 1}. This index serves as the final, robust speech token. It is worth noting that our Voting-LFQ structure introduces negligible additional parameters and computational overhead during inference. detailed complexity analysis is provided in Appendix B.6. 2.3 NOISE-AWARE CONSENSUS TRAINING The Voting-LFQ architecture enables our novel training paradigm, designed to explicitly instill representational invariance. Our goal is to make the tokenizer robust to noise without degrading its performance on clean inputs. The core mechanism works as follows: during each forward pass, for given input audio w, we generate perturbed audio sample = A(w), where A() is stochastic augmentation function applied at the waveform level (e.g., adding Gaussian noise; further details in Appendix B.3). Both and are separately processed by the encoder to produce two corresponding hidden states, and h. we then randomly select minority subset of branches (where < n/2) to receive the perturbed hidden state h, while the remaining majority branches receive the clean hidden state h. This setup allows the model to perform self-stabilization. To enforce this, we introduce the consensus loss (Lconsensus) which encourages all branches, whether they see clean or noisy input, to produce similar pre-quantization representations. We compute dynamic, \"online\" target pall by averaging the pre-quantization vectors pi from all branches. The loss then penalizes the deviation of each branch from this global average: Lconsensus = 1 (cid:88) i= pi pall2 2, where pall = 1 (cid:88) j= pj . (4) 4 By optimizing this objective, the clean-majority branches act as stable anchor for the global average pall, preventing it from being corrupted by the noisy inputs. Consequently, the noisy-minority branches are forced to learn representations that align with the clean consensus, effectively learning to ignore the perturbations. Optimizing on the continuous vectors pi provides smoother and more effective gradient signal than working with the binarized bi."
        },
        {
            "title": "2.4 FINAL TRAINING OBJECTIVE",
            "content": "The complete training objective for StableToken combines the ASR task loss with our consensus loss and standard LFQ regularization terms. The primary task is optimized via Cross-Entropy loss (LASR) on the ground-truth transcripts. Following the LFQ framework (Yu et al., 2023), we also include commitment loss (Lcommitment) to encourage the hidden states to stay close to the quantized representations, and codebook entropy loss (Lcodebook) to promote uniform usage of the discrete codes. The final, composite loss function is weighted sum: Ltotal = LASR + λ1Lconsensus + λ2Lcommitment + λ3Lcodebook, (5) where λ1, λ2, and λ3 are scalar hyperparameters that balance the influence of each component."
        },
        {
            "title": "3 EXPERIMENTAL SETUP",
            "content": "Our experiments are structured to comprehensively validate StableToken from three perspectives. We first demonstrate its core superiority at the tokenizer level, establishing new state-of-the-art in noise robustness without compromising reconstruction quality (4.1). We then show that this fundamental stability translates directly into significant performance gains in diverse downstream SpeechLLM tasks, including ASR, Speech Emotion Recognition (SER), and Text-to-speech (TTS) (4.2). Finally, we dissect the model through ablation studies, qualitative analysis, and case study to verify the contribution of each design component and provide insight into its inner workings (4.3). Tokenizer Training. Our StableToken model is built upon an encoder-decoder architecture initialized from whisper-large-v3 (Radford et al., 2023), with our Voting-LFQ module inserted into the encoders mid-point. The tokenizer is pre-trained on diverse 150k-hour speech corpus. Our tokenizer vocabulary size is set to 8192 (corresponding to = 13), and the frame rate is 25Hz. For our main experiments, the number of voters is set to = 5, choice justified by our analysis in Section 4.3. Full details on training data hyperparameters are provided in Appendix B. Baseline Models. We benchmark StableToken against comprehensive suite of SOTA models across three categories: SSL-based, distilled, and supervised tokenizers. For all baselines, we use their officially released models. The detailed list of baseline models can be found in Appendix C. Tokenizer-Level Evaluation. We assess the tokenizers intrinsic properties. Robustness is measured by Unit Edit Distance (UED%, ) (Messica & Adi, 2024) on the FLEURS (Conneau et al., 2023) benchmark under various synthetic perturbations and real-world noise conditions, which include challenging out-of-domain (OOD) noise. detailed description of the noise profiles is in Appendix D. Fidelity is measured by Word Error Rate (WER%, ) and Mean Opinion Score (MOS, ) on the LibriSpeech (Panayotov et al., 2015) and SEED (Anastassiou et al., 2024) benchmarks. Downstream Task Evaluation. For downstream tasks, we follow controlled, isogenic setup to ensure fair comparison. Each tokenizer is integrated into SpeechLLM framework using pretrained Qwen2.5-3B (Yang et al., 2024a) backbone, which is then fine-tuned using prompt-based paradigm (Zeng et al., 2025). We evaluate on three tasks: (1) ASR: Assessed on noise-augmented LibriSpeech (Panayotov et al., 2015) and the CHiME-4 (Vincent et al., 2017) benchmark using WER (%); (2) SER: Assessed on noise-augmented version of the ESD (Zhou et al., 2022) test set using classification accuracy (%); (3) TTS: Assessed on the SEED-TTS (Anastassiou et al., 2024) benchmark using both WER (%) and MOS. The aggregated training datasets, fine-tuning hyperparameters, and prompts for each task are detailed in Appendix E. Table 1: Noise robustness comparison across different semantic tokenizers. Results are reported in UED% () under synthetic perturbation (Gaussian, Pink, Brown, Bit Crush) and real noise conditions. It is worth noting that comparison is most meaningful between tokenizers of the same type. For more comprehensive evaluation, we also include SSL and semantic distilled tokenizers as baselines. Model #C Frame Rate Codebook Size Gauss. Noise Pink Noise Brown Noise Bit Crush Real Noise Real (OOD) Avg. SSL Semantic Tokenizer 50Hz 50Hz 50Hz 500 200 2048 26.42 18.67 21.56 20.38 15.78 17.08 Semantic Distilled Tokenizer 50Hz 50Hz 50Hz 50Hz 50Hz 50Hz 12.5Hz 1024 1024 1024 1024 1024 1024 37.39 55.69 72.74 53.54 71.76 84.46 28.05 54.90 72.72 43.85 59.95 77.31 72.68 59. Supervised Semantic Tokenizer 12.5Hz 16384 42.44 32.12 25Hz 50Hz 25Hz 25Hz 4096 4096 6561 8192 35.40 46. 27.09 35.90 54.67 42.57 12.93 9.76 1 1 1 3 8 1 3 8 8 1 1 1 1 18.82 15.26 15.47 28.06 59.84 75.91 40.17 57.88 76.49 60.19 30. 25.45 33.46 39.96 9.37 18.02 14.95 14.95 18.48 18.69 15.08 19.18 19.07 14. 20.22 17.07 16.48 21.38 35.29 54.01 36.95 50.26 68.47 22.33 33.16 48.43 27.82 41.25 59.89 23.09 33.67 48. 28.78 42.44 62.28 26.72 45.43 62.07 38.52 53.92 71.48 43.58 41.66 42. 53.43 25.53 27.67 28.62 31.10 20.64 27. 23.88 27.70 24.58 28.21 26.17 33.09 30.87 31.76 32. 38.66 7.32 10.65 10.96 10.17 HuBERT-500 (Hsu et al., 2021) NAST (Messica & Adi, 2024) R-Spin (Chang & Glass, 2024) SpeechTokenizer (Zhang et al., 2023) X-Codec (Ye et al., 2025) Mimi (Défossez et al., 2024) GLM-4-Voice-Token. (Zeng et al., 2025) 3 Tokenizer (Du et al., 2024a) CosyVoice2 (Du et al., 2024b) StableToken (Ours)"
        },
        {
            "title": "4 RESULTS",
            "content": "4.1 TOKENIZER-LEVEL PERFORMANCE 4.1.1 SUPERIOR NOISE ROBUSTNESS As shown in Table 1, StableToken establishes new state-of-the-art in noise robustness. It achieves an average UED of 10.17%, dramatic improvement over both the best supervised baseline (S 3 Tokenizer, 26.17%) and the top-performing robust SSL-based model (R-Spin, 16.48%). Crucially, this strong performance holds even on out-of-distribution (OOD) real-world noise not seen during training, demonstrating the excellent generalization of our method. Furthermore, this outperformance is achieved using significantly larger vocabulary than conventional tokenizers. This makes the result even more significant, as larger vocabulary creates finer-grained decision space, making the task of maintaining token-level invariance inherently more challenging. This substantial performance gap underscores the effectiveness of our co-designed architecture and training strategy. 4.1.2 EXCELLENT RECONSTRUCTION QUALITY Table 2: Reconstruction results measured by WER () and MOS () on LibriSpeech (Panayotov et al., 2015) and SEED (Anastassiou et al., 2024) benchmarks. Model #C Frame Rate BPS LSclean LSother SEED en SEED zh LSclean LSother SEED en SEED zh GLM-4-Voice-Token. (Zeng et al., 2025) 3 Tokenizer (Du et al., 2024a) CosyVoice2 (Du et al., 2024b) StableToken (Ours) 1 1 1 1 12.5Hz 175 300 25Hz 325 25Hz 325 25Hz 9.33 4.04 5.78 13.38 9.68 4.25 7.99 3.84 3.54 5.91 4.34 3.44 3.23 4.26 2.75 2.62 4.07 3.40 3.36 4. 3.99 3.31 3.25 3.83 4.16 3.40 3.31 4.01 4.10 3.31 3.58 4.18 WER MOS To evaluate reconstruction quality, we follow the methodology of Du et al. (2024a;b); Zeng et al. (2024) and train flow matching model to synthesize audio from our speech tokens. The results, shown in Table 2, demonstrate that the leap in noise robustness does not compromise the tokenizers fundamental quality. StableToken delivers state-of-the-art reconstruction performance, evidenced by its exceptional Word Error Rate (WER) and Mean Opinion Scores (MOS). These results validate 6 StableToken as versatile tokenizer that excels in both resilience and fidelity. Details on the audio reconstruction setup are provided in Appendix B.5."
        },
        {
            "title": "4.2 DOWNSTREAM SPEECHLLM PERFORMANCE",
            "content": "The ultimate measure of tokenizers utility is its impact on downstream tasks. We find that StableTokens intrinsic robustness consistently translates to superior performance in ASR, SER, and TTS, especially in challenging, noisy conditions. Figure 3: Performance of downstream SpeechLLMs under various noise conditions and SNR levels. (Top Row) ASR performance, measured in Word Error Rate (WER, ). (Bottom Row) SER performance, measured in Accuracy (). In both tasks, StableToken consistently demonstrates superior robustness, with the performance gap widening as noise severity increases. 4.2.1 ROBUST ASR PERFORMANCE StableToken significantly contributes to robust downstream ASR model. Figure 3 shows that while all systems perform comparably on clean audio, the performance gap widens dramatically as noise increases. Under the most severe OOD real-world noise at 0dB SNR, the model with StableToken achieves WER of 20.34%, relative reduction of over 30% compared to the baselines 29.94%. This robustness generalizes to complex acoustic scenes. On the CHiME-4 (Vincent et al., 2017) benchmark  (Table 3)  , the StableToken-based system achieves WERs of 35.90% (test-real set) and 30.61% (test-simulated set), marking relative reductions of approximately 30% over the next-best baseline. This confirms that token-level stability is direct driver of downstream model resilience. 4.2.2 ROBUST SER PERFORMANCE Figure 3 shows that the StableToken-based model consistently achieves higher classification accuracy across all noise types and levels. While performance is similar across all tokenizers on clean audio, the StableToken-based models accuracy degrades much more slowly as noise increases, demonstrating greater robustness in isolating emotional cues from corrupted audio. 4.2.3 SUPERIOR TTS PERFORMANCE As shown in Table 3, StableToken delivers superior TTS performance, with significantly lower WER on both subsets, as well as an improved MOS on SEED-TTSZH and competitive performance on SEED-TTSEN. The reduction in WER confirms that our tokenizer enables more intelligible speech synthesis that faithfully reproduces the intended text. Concurrently, the MOS score demonstrates high naturalness and auditory quality of the synthetic speech. Together, these results strongly support StableToken as an effective and information-rich representation for speech synthesis. 7 Table 3: Downstream SpeechLLMs performance comparison on ASR (CHiME-4) and TTS (SEEDTTS) benchmarks. In both tasks, integrating StableToken into the downstream SpeechLLM leads to substantially improved performance, demonstrating its noise robustness and versatility. ASR TTS Tokenizer LLM-base Dev Set Test Set SEED-TTSEN SEED-TTSZH Real Sim. Real Sim. WER MOS WER MOS Qwen2.5-3B 38.66 40.82 54.63 47.71 CosyVoice CosyVoice2 Qwen2.5-3B 43.91 48.39 59.83 55.01 GLM-4-Voice Qwen2.5-3B 36.92 36.38 51.08 43.09 Qwen2.5-3B 25.56 25.36 35.90 30.61 StableToken 7.80 7.22 6.19 4. 3.52 3.75 4.19 4.12 8.73 9.89 5.26 3.02 3.47 3.37 3.85 4.08 In summary, across understanding (ASR, SER) and generation (TTS), StableToken consistently enables stronger, more reliable downstream performance. This dual advantage affirms its effectiveness as powerful, versatile foundation for real-world speech systems. 4.3 ANALYSIS 4.3.1 COMPONENT ABLATION STUDY Table 4: Sequential ablation study of StableToken. We jointly evaluate tokenizer robustness (UED ) and semantic preservation (ASR WER ). Results show that each component contributes to robustness, with the full model providing optimal stability and semantic fidelity. ASR is measured on the validation set during tokenizer training. Tokenizer Robustness (UED% ) ASR (WER% ) Model Configuration Gauss. Noise Brown Noise Pink Noise Bit Crush Real Noise Real (OOD) LSClean LSOther StableToken (Full) w/o Consensus Loss w/o Noise-Aware Training w/o Multi-Branch 12.93 24.80 30.77 34.53 9.76 19.06 23.05 25.44 9.37 10.65 7.32 17.81 14.03 16.97 21.30 17.32 20.95 24.58 19.83 23.68 10.96 17.43 21.51 24.47 2.03 2.03 2.19 2. 4.68 4.88 5.52 5.85 Our sequential ablation study, presented in Table 4, confirms that each component of StableToken is critical for its performance. First, removing the Consensus Loss causes the significant degradation in token robustness (e.g., UED on Real OOD noise increases from 10.96% to 17.43%), which underscores the importance of enforcing explicit agreement between branches. Subsequently, removing the Noise-Aware Training further harms performance, particularly the preservation of semantic content (WER on LibriSpeech-Other increases from 4.88% to 5.52%). Finally, reverting to single-branch baseline results in the poorest performance overall. This highlights the Multi-Branch architectures dual role: it is the structural enabler for our training strategy and acts as an effective ensemble at inference, mitigating quantization errors common in single-path designs (Ma et al., 2025a). 4.3.2 ANALYSIS OF VOTER COUNT (N ) To determine the optimal number of voters that best balances performance and computational cost, we conduct preliminary training runs for each configuration. The results in Table 5 show clear trend: increasing from 3 to 5 yields substantial improvements in both robustness and semantic preservation. However, further increase to = 7 offers only marginal gains that do not justify the added computational overhead. We therefore select = 5 as the optimal configuration for all experiments. Analysis of parameters and FLOPs for different can be found in Appendix B.6. 8 Table 5: Impact of the number of voters (N ) on tokenizer robustness and semantic preservation. Tokenizer Robustness (UED% ) ASR (WER% ) Number of Voters (N ) Gauss. Noise"
        },
        {
            "title": "Real\nNoise",
            "content": "Real (OOD) LSClean LSOther = 3 = 5 = 7 20.66 18.68 18.10 15.42 13.87 13. 14.44 11.55 14.89 13.11 10.50 14.06 13.79 9.84 12.51 15.27 14.49 14.11 2.24 2.22 2.36 5.47 5.38 5.52 Table 6: Case study on error correction via bit-wise voting. Output Source Token @ Pos. 68 (Vote on Bit #4) Token @ Pos. 80 (Vote on Bit #5, #7) Token @ Pos. 105 (Vote on Bit #3) Token @ Pos. 114 (Vote on Bit #2, #6) Clean Reference Voter 1 (Noisy) Voter 2 (Noisy) Voter 3 (Noisy) Voter 4 (Noisy) Voter 5 (Noisy) 5517 ...10001101 5533 ...10011101 5517 ...10001101 5517 ...10001101 5517 ...10001101 5533 ... 3485 ...10011101 3485 ...10011101 3517 ...10111101 3517 ...10111101 3485 ...10011101 3357 ...00011101 Final Voted Output 5517 Bit #4: 3 vs 2 0 3485 Bit #5: 3 vs 2 0 Bit #7: 4 vs 1 1 2920 ... 2920 ...01101000 2912 ...01100000 2920 ...01101000 2920 ...01101000 2920 ...01101000 2920 4 vs 1 1 6939 ...00011011 6939 ...00011011 6943 ...00011111 6939 ...00011011 7003 ...01011011 6939 ...00011011 6939 Bit #2: 4 vs 1 0 Bit #6: 4 vs 1 0 4.3.3 CASE STUDY Table 6 provides case study illustrating the error correction capability of the voting-LFQ module. For instance, at position 80, noise causes three voters to generate erroneous tokens. Specifically, Voters 2 and 3 flip bit #5, while Voter 5 flips bit #7. Despite most voters predicting incorrect tokens, the voting mechanism operating at the bit level allows for correct recovery. For bit #5, the correct value 0 wins by 3-to-2 majority, and for bit #7, the correct value 1 wins by 4-to-1 majority, successfully reconstructing the original token (3485). Similar corrections occur at positions 68, 105 and 114. This case study highlights key advantage of StableToken: its resilience does not depend on every branch being perfect, but on the collective ability to override sparse bit-flip errors."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Semantic Speech Tokenizers SSL tokenizers rely on self-supervised learning (SSL) to extract discrete units from audio (Chen et al., 2022; Chung et al., 2021; Conneau et al., 2021; Chiu et al., 2022), but are proven suboptimal for generative speechLLMs (Mousavi et al., 2024; Guo et al., 2025). Hybrid approaches combine acoustic tokenizers with semantic distillation to balance fidelity and content (Zhang et al., 2023; Ye et al., 2025; Siahkoohi et al., 2022; Yang et al., 2024b), yet their high data rate and structural incompatibility with LLMs introduce practical challenges. Recent fully supervised methods have proven suitable for SpeechLLMs, which produce linguistic tokens while retaining sufficient phonetic information (Zeng et al., 2025; Du et al., 2024a), supporting expressive and efficient end-to-end SpeechLLMs (Zeng et al., 2024; Ding et al., 2025; Wu et al., 2025). Noise Robustness While extensive research has focused on constructing robust Automatic Speech Recognition (ASR) models (Wang et al., 2022; Tjandra et al., 2023; Eickhoff et al., 2023; Gong et al., 2023; Ahn et al., 2025), the stability of discrete speech tokens under noisy conditions has received 9 much less attention. Recently, R-SPIN (Chang & Glass, 2024) and NAST (Messica & Adi, 2024) have begun addressing this gap, but are limited to traditional SSL-based speech tokenizers. more detailed discussion of related work is presented in Appendix due to page limit."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce StableToken, novel tokenizer designed to solve the critical instability of existing semantic tokenizers in noisy environments. By employing multi-branch architecture and consensus mechanism with bitwise voting, StableToken achieves state-of-the-art token stability. This stability directly translates to significant improvements in the robustness of downstream SpeechLLMs."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To facilitate reproducibility of our work, we have provided detailed descriptions of the datasets, hyperparameters, and other experimental details used in our study in Section 3 and Appendix B, D, E. Our code and model checkpoint will be released publicly upon acceptance to further support reproducibility and foster future research."
        },
        {
            "title": "REFERENCES",
            "content": "Adaeze Adigwe, Noé Tits, Kevin El Haddad, Sarah Ostadabbas, and Thierry Dutoit. The emotional voices database: Towards controlling the emotion dimension in voice generation systems. arXiv preprint arXiv:1806.09514, 2018. Hyebin Ahn, Kangwook Jang, and Hoirin Kim. Hubert-vic: Improving noise-robust automatic speech recognition of speech foundation model via variance-invariance-covariance regularization. arXiv preprint arXiv:2508.12292, 2025. Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, et al. Seed-tts: family of high-quality versatile speech generation models. arXiv preprint arXiv:2406.02430, 2024. Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massivelymultilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019. Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning of discrete speech representations. arXiv preprint arXiv:1910.05453, 2019. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:1244912460, 2020. Evelina Bakhturina, Vitaly Lavrukhin, Boris Ginsburg, and Yang Zhang. Hi-fi multi-speaker english tts dataset. arXiv preprint arXiv:2104.01497, 2021. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. Aishell-1: An open-source mandarin speech corpus and speech recognition baseline. In 2017 20th conference of the oriental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA), pp. 15. IEEE, 2017. Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette Chang, Sungbok Lee, and Shrikanth Narayanan. Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation, 42(4):335359, 2008. 10 Houwei Cao, David Cooper, Michael Keutmann, Ruben Gur, Ani Nenkova, and Ragini Verma. Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE transactions on affective computing, 5(4):377390, 2014. Heng-Jui Chang and James Glass. R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 642662. Association for Computational Linguistics, 2024. Heng-Jui Chang, Alexander Liu, and James Glass. Self-supervised fine-tuning for improved content representations by speaker-invariant clustering. In Proc. Interspeech 2023, pp. 29832987, 2023. Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909, 2021. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6): 15051518, 2022. Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. Self-supervised learning with random-projection quantizer for speech recognition. In International Conference on Machine Learning, pp. 39153924. PMLR, 2022. Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 244250. IEEE, 2021. Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli. Unsupervised cross-lingual representation learning for speech recognition. 2021. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal representations of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pp. 798805. IEEE, 2023. Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, et al. Kimi-audio technical report. arXiv preprint arXiv:2504.18425, 2025. Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024a. Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024b. Kate Dupuis and Kathleen Pichora-Fuller. Toronto emotional speech set (tess). 2010. Patrick Eickhoff, Matthias Möller, Theresa Pekarek Rosin, Johannes Twiefel, and Stefan Wermter. Bring the Noise: Introducing Noise Robustness to Pretrained Automatic Speech Recognition. In Artificial Neural Networks and Machine Learning ICANN 2023, pp. 381392. Springer Nature Switzerland, 2023. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. Llama-omni: Seamless speech interaction with large language models. arXiv preprint arXiv:2409.06666, 2024. 11 Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, and Yang Feng. Llama-omni2: Llmbased real-time spoken chatbot with autoregressive streaming speech synthesis. arXiv preprint arXiv:2505.02625, 2025. Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, and Xavier Serra. Fsd50k: an open dataset of human-labeled sound events. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:829852, 2021. Daniel Galvez, Greg Diamos, Juan Ciro, Juan Felipe Cerón, Keith Achorn, Anjali Gopi, David Kanter, Maximilian Lam, Mark Mazumder, and Vijay Janapa Reddi. The peoples speech: large-scale diverse english speech recognition dataset for commercial usage. arXiv preprint arXiv:2111.09344, 2021. Itai Gat, Felix Kreuk, Tu-Anh Nguyen, Ann Lee, Jade Copet, Gabriel Synnaeve, Emmanuel Dupoux, and Yossi Adi. Augmentation invariant discrete representation for generative spoken language modeling. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023), pp. 465477, 2023. Yuan Gong, Sameer Khurana, Leonid Karlinsky, and James Glass. Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers. In Proc. Interspeech 2023, pp. 23582362, 2023. doi: 10.21437/Interspeech.2023-1511. Yiwei Guo, Zhihan Li, Hankun Wang, Bohan Li, Chongtian Shao, Hanglei Zhang, Chenpeng Du, Xie Chen, Shujie Liu, and Kai Yu. Recent advances in discrete speech tokens: review. arXiv preprint arXiv:2502.06490, 2025. Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, et al. Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation. In 2024 IEEE Spoken Language Technology Workshop (SLT), pp. 885890. IEEE, 2024. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29:34513460, 2021. Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, et al. Step-audio: Unified understanding and generation in intelligent speech interaction. arXiv preprint arXiv:2502.11946, 2025. Wenyong Huang, Zhenhe Zhang, Yu Ting Yeung, Xin Jiang, and Qun Liu. Spiral: Self-supervised perturbation-invariant representation learning for speech pre-training. In ICLR, 2022. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Philip Jackson and SJUoSG Haq. Surrey audio-visual expressed emotion (savee) database. University of Surrey: Guildford, UK, 2014. Léo Jacqmin, Lina Rojas-Barahona, and Benoit Favre. \" do you follow me?\": survey of recent approaches in dialogue state tracking. arXiv preprint arXiv:2207.14627, 2022. Jesin James, Li Tian, and Catherine Inez Watson. An open source emotional speech corpus for human robot interaction applications. In Interspeech, pp. 27682772, 2018. Chengze Jiang, Zhuangzhuang Wang, Minjing Dong, and Jie Gui. Survey of adversarial robustness in multimodal large language models. arXiv preprint arXiv:2503.13962, 2025. Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. Libri-light: benchmark for asr with limited or no supervision. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 76697673. IEEE, 2020. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 119132, 2019. Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. Advances in neural information processing systems, 33:1702217033, 2020. Chia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. Dialogue state tracking with language model using schema-driven prompting. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 49374949, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.404. URL https://aclanthology.org/2021.emnlp-main.404/. Keon Lee, Kyumin Park, and Daeyoung Kim. Dailytalk: Spoken dialogue dataset for conversational text-to-speech. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023. Xinjian Li, Shinnosuke Takamichi, Takaaki Saeki, William Chen, Sayaka Shiota, and Shinji Watanabe. Yodas: Youtube-oriented dataset for audio and speech. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 18. IEEE, 2023. Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying Li, Jinming Zhao, et al. Mer 2023: Multi-label learning, modality robustness, and semi-supervised learning. In Proceedings of the 31st ACM international conference on multimedia, pp. 96109614, 2023. Alexander Liu, Heng-Jui Chang, Michael Auli, Wei-Ning Hsu, and Jim Glass. Dinosr: Selfdistillation and online clustering for self-supervised speech representation learning. Advances in Neural Information Processing Systems, 36:5834658362, 2023. Wenrui Liu, Zhifang Guo, Jin Xu, Yuanjun Lv, Yunfei Chu, Zemin Liu, and Junyang Lin. Analyzing and mitigating inconsistency in discrete speech tokens for neural codec language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3103531046, 2025. Steven Livingstone and Frank Russo. The ryerson audio-visual database of emotional speech and song (ravdess): dynamic, multimodal set of facial and vocal expressions in north american english. PloS one, 13(5):e0196391, 2018. Vasista Sai Lodagala, Sreyan Ghosh, and Srinivasan Umesh. Ccc-wav2vec 2.0: Clustering aided cross contrastive self-supervised learning of speech representations. In 2022 IEEE Spoken Language Technology Workshop (SLT), pp. 18. IEEE, 2023. Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025a. Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, and Xie Chen. Language model can listen while speaking. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2483124839, 2025b. Shoval Messica and Yossi Adi. Nast: Noise aware speech tokenization for speech language models. In Proc. Interspeech 2024, pp. 41694173, 2024. Pooneh Mousavi, Luca Della Libera, Jarod Duret, Artem Ploujnikov, Cem Subakan, and Mirco Ravanelli. Dasb-discrete audio and speech benchmark. arXiv preprint arXiv:2406.14294, 2024. Tu Anh Nguyen, Wei-Ning Hsu, Antony dAvirro, Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Remez, Jade Copet, Gabriel Synnaeve, Michael Hassid, et al. Expresso: benchmark and analysis of discrete expressive speech resynthesis. arXiv preprint arXiv:2308.05725, 2023. 13 Kari Ali Noriy, Xiaosong Yang, and Jian Jun Zhang. Emns/imz/corpus: An emotive singlespeaker dataset for narrative storytelling in games, television and graphic novels. arXiv preprint arXiv:2305.13137, 2023. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 52065210. IEEE, 2015. Karol Piczak. Esc: Dataset for environmental sound classification. In Proceedings of the 23rd ACM international conference on Multimedia, pp. 10151018, 2015. Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled self-supervised representations. arXiv preprint arXiv:2104.00355, 2021. Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: multimodal multi-party dataset for emotion recognition in conversations. arXiv preprint arXiv:1810.02508, 2018. Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: large-scale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411, 2020. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. In International conference on Robust speech recognition via large-scale weak supervision. machine learning, pp. 2849228518. PMLR, 2023. Ali Siahkoohi, Michael Chinen, Tom Denton, Bastiaan Kleijn, and Jan Skoglund. Ultra-low-bitrate speech coding with pretrained transformers. In Proc. Interspeech 2022, pp. 44214425, 2022. Amitay Sicherman and Yossi Adi. Analysing discrete self supervised speech representation for spoken language modeling. In ICASSP, 2023. Fengping Tian, Chenyang Lyu, Xuanfan Ni, Haoqin Sun, Qingjuan Li, Zhiqiang Qian, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, et al. Marco-voice technical report. arXiv preprint arXiv:2508.02038, 2025. Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. deHuBERT: Disentangling Noise in Self-supervised Model for Robust Speech Recognition. arXiv preprint arXiv:2302.14597, 2023. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Christophe Veaux, Junichi Yamagishi, and Kirsten MacDonald. Cstr vctk corpus: English multispeaker corpus for cstr voice cloning toolkit. 2017. Emmanuel Vincent, Shinji Watanabe, Aditya Arie Nugraha, Jon Barker, and Ricard Marxer. An analysis of environment, microphone and data simulation mismatches in robust speech recognition. Computer Speech & Language, 46:535557, 2017. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. Voxpopuli: large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. arXiv preprint arXiv:2101.00390, 2021. Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. Mead: large-scale audio-visual dataset for emotional talking-face generation. In European conference on computer vision, pp. 700717. Springer, 2020. Xiong Wang, Yangze Li, Chaoyou Fu, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, and Long Ma. Freeze-omni: smart and low latency speech-to-speech dialogue model with frozen llm. arXiv preprint arXiv:2411.00774, 2024. 14 Yiming Wang, Jinyu Li, Heming Wang, Yao Qian, Chengyi Wang, and Yu Wu. Wav2vec-Switch: Contrastive Learning from Original-noisy Speech Pairs for Robust Speech Recognition. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 76327636. IEEE, 2022. Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, et al. Step-audio 2 technical report. arXiv preprint arXiv:2507.16632, 2025. Zhifei Xie and Changqiao Wu. Mini-omni: Language models can hear, talk while thinking in streaming. arXiv preprint arXiv:2408.16725, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Dongchao Yang, Haohan Guo, Yuanyuan Wang, Rongjie Huang, Xiang Li, Xu Tan, Xixin Wu, and Helen Meng. Uniaudio 1.5: Large language model-driven audio codec is few-shot audio task learner. Advances in Neural Information Processing Systems, 37:5680256827, 2024b. Wanqi Yang, Yanda Li, Meng Fang, Yunchao Wei, Tianyi Zhou, and Ling Chen. Who can withstand chat-audio attacks? an evaluation benchmark for large language models. arXiv preprint arXiv:2411.14842, 2024c. Zhen Ye, Peiwen Sun, Jiahe Lei, Hongzhan Lin, Xu Tan, Zheqi Dai, Qiuqiang Kong, Jianyi Chen, Jiahao Pan, Qifeng Liu, et al. Codec does matter: Exploring the semantic shortcoming of codec for audio language model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2569725705, 2025. Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusion tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu. Libritts: corpus derived from librispeech for text-to-speech. arXiv preprint arXiv:1904.02882, 2019. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Yuxiao Dong, Jie Tang, et al. Scaling speechtext pre-training with synthetic interleaved data. In The Thirteenth International Conference on Learning Representations, 2025. Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, et al. Wenetspeech: 10000+ hours multi-domain mandarin corpus for speech recognition. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 61826186. IEEE, 2022. Jian Zhang, Linhao Zhang, Bokai Lei, Chuhan Wu, Wei Jia, and Xiao Zhou. Wildspeech-bench: Benchmarking audio llms in natural speech conversation. arXiv preprint arXiv:2506.21875, 2025. JTFLM Zhang and Huibin Jia. Design of speech corpus for mandarin text to speech. In The blizzard challenge 2008 workshop, 2008. Linhao Zhang and Houfeng Wang. Using bidirectional transformer-crf for spoken language understanding. In CCF international conference on natural language processing and chinese computing, pp. 130141. Springer, 2019. Linhao Zhang, Dehong Ma, Xiaodong Zhang, Xiaohui Yan, and Houfeng Wang. Graph lstm with context-gated mechanism for spoken language understanding. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 95399546, 2020. 15 Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified speech tokenizer for speech large language models. arXiv preprint arXiv:2308.16692, 2023. Jinming Zhao, Tenggan Zhang, Jingwen Hu, Yuchen Liu, Qin Jin, Xinchao Wang, and Haizhou Li. M3ed: Multi-modal multi-scene multi-label emotional dialogue database. arXiv preprint arXiv:2205.10237, 2022. Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li. Emotional voice conversion: Theory, databases and esd. Speech Communication, 137:118, 2022. 16 LARGE LANGUAGE MODEL (LLM) USAGE STATEMENT In accordance with the conference policies on Large Language Model (LLM) usage, we hereby disclose the following: After completing the initial draft of this paper, we utilized an LLM to enhance grammar and polish the writing of this manuscript. No new research ideas, experimental designs, or scientific content were generated by the LLM. All scientific contributions, analyses, and conclusions presented in this work are solely those of the authors. We take full responsibility for the content of this paper, including all sections that have been revised or improved with LLM assistance. The LLM is not an author and did not contribute to the research ideation or substantive scientific writing. This statement is provided to ensure transparency and compliance with the conferences policies on LLM usage."
        },
        {
            "title": "B DETAILS OF STABLETOKEN",
            "content": "B.1 TRAINING DATASETS FOR STABLETOKEN We train our StableToken model on hundreds of thousands of hours of both open-source data and in-house data. All open-source datasets used in this work are listed in Table 7. Table 7: Summary of datasets used for training StableToken Dataset Duration (#hours) Task Language(s) LibriSpeech (Panayotov et al., 2015) Multilingual LibriSpeech (Pratap et al., 2020) The Peoples Speech (Galvez et al., 2021) GigaSpeech (Chen et al., 2021) Yodas (Li et al., 2023) Hi-Fi TTS (Bakhturina et al., 2021) VCTK (Veaux et al., 2017) LibriTTS (Zen et al., 2019) VoiceAssistant-400K (Xie & Wu, 2024) AISHELLL-1 (Bu et al., 2017) WenetSpeech (Zhang et al., 2022) Common Voice (Ardila et al., 2019) Emilia (He et al., 2024) 960 ASR English 27,322 ASR English 5,568 ASR English 10,000 ASR English 29,155 ASR English 292 ASR English 44 ASR English 586 ASR English 679 ASR English 150 ASR Chinese 10,005 ASR Chinese 2,133 ASR English, Chinese 96,750 ASR English, Chinese B.2 TRAINING HYPERPARAMETERS FOR STABLETOKEN Table 8 summarizes the main hyperparameters used throughout StableToken training. Table 8: Hyperparameters used for training StableToken Hyperparameter optimizer_type lr_scheduler max_lr warmup_steps weight_decay grad_clip consensus_loss_weight (λ1) commitment_loss_weight (λ2) codebook_entropy_loss_weight (λ3) Value AdamW OneCycleLR 1.5e-5 1000 0.01 1.0 0.25 0.25 1.0 17 B.3 DETAILS OF STOCHASTIC PERTURBATIONS DURING TRAINING We detail the construction and parameterization of the stochastic augmentation function A() as introduced in Section 2.3. For each sample, one perturbation type is randomly selected from the following five categories: Gaussian Noise, Pink Noise, Brown Noise, Bit Crush Distortion, and Real-world Noise. The intensity of the selected perturbation is then uniformly sampled from predefined range specific to each type, as summarized in Table 9. For real-world noise, an additional random selection of noise audio clip is performed. The pool of noise clips consists of samples from the AudioCaps (Kim et al., 2019), FSD50k (Fonseca et al., 2021), and ESC-50 (Piczak, 2015) datasets. Notably, the ESC-10 subset of ESC-50 is excluded from training and reserved exclusively for evaluation as out-of-domain real-world noise. Table 9: Perturbation types and their corresponding intensity ranges utilized during training. Perturbation Type Intensity Range Gaussian Noise Pink Noise Brown Noise Bit Crush Distortion Real-world Noise 16 SNR 30 16 SNR 24 12 SNR 24 8 Bit Depth 14 12 SNR 24 B.4 DISCUSSION OF CONSENSUS OBJECTIVE LOSS CHOICES We choose the L2 loss (Mean Squared Error Loss) for the consensus objective in Eq. 4 due to several considerations: First, L2 loss offers simple and direct way to minimize the differences among multiple branches. At the same time, its form is also naturally compatible with the existing commitment loss in quantizationbased models, thus facilitating stable optimization and consistent gradient behavior across objectives. Furthermore, by averaging representations from all branches, the resulting target inherently incorporates form of confidence weighting, as outlier branch results are diluted in the mean. Second, since all branches originate from the same underlying input, with only noise perturbations in minority branches, the goal is not to increase inter-class separation as in contrastive learning, but to enforce similarity across the noisy and clean versions. The L2 loss directly minimizes the Euclidean distance between the branches and their consensus, effectively encouraging robust invariance without introducing additional factors or requiring extra sampling. Moreover, cosine similarity is less sensitive to the number of bit flips in high-dimensional binary codes, especially when representations are already close, such as the flip of single bit, which corresponds to small changes in angle and often results in diminished gradient signals and less effective correction of localized errors. B.5 AUDIO RECONSTRUCTION DETAILS Table 10: Summary of datasets used for training the flow matching model Dataset Librilight (Kahn et al., 2020) WenetSpeech (Zhang et al., 2022) Yodas2 (Li et al., 2023) Emilia (He et al., 2024) Language(s) English Chinese English, Chinese English, Chinese Following the framework of CosyVoice (Du et al., 2024a) and GLM-4-Voice (Zeng et al., 2025), we train flow matching model to reconstruct audio from speech tokens. The model takes as input the speech token representations and produces Mel spectrograms. Finally, HiFi-GAN vocoder (Kong et al., 2020) converts the generated Mel spectrograms into the speech waveforms. The datasets for training the flow matching model is listed in Table 10 (excluding our in-house datasets, which comprise both English and Chinese speech data), and the training hyperparameters in Table 11. 18 Table 11: Hyperparameters used for training the flow matching model Hyperparameter Value optimizer_type lr_scheduler learning_rate warmup_steps grad_clip batch_type max_frames_in_batch Adam WarmupLR 3.0e-4 25000 1.5 dynamic 10000 B.6 COMPLEXITY ANALYSIS OF DIFFERENT VOTER COUNTS To further explore the computational overhead brought by increasing the number of voters (N ), we present the model parameter counts and floating-point operations (FLOPs) for models with = 1, 3, 5, and 7 in Table 12. Both metrics are measured using the THOP library1. For FLOPs calculation, we use an input audio with duration of 30 seconds. The results show that as increases, the model parameters increase linearly, but the increment between adjacent values is relatively small (about 0.033M parameters). The FLOPs for different are also very close, indicating that enlarging does not introduce significant extra computational cost. This suggests that increasing the number of voters achieves better performance and robustness with only minimal impact on model size and inference efficiency. Table 12: Model parameters and inference FLOPs for different voter counts . Number of Voters (N ) #Parameters (M) #FLOPs (G) = 1 = 3 = 5 = 320.261 320.294 ( 0.010%) 320.328 ( 0.021%) 320.361 ( 0.031%) 480.978 481.003 ( 0.005%) 481.028 ( 0.010%) 481.053 ( 0.016%) The inference latency introduced by increasing the number of voters is also negligible. This is because all voter modules can be computed in parallel, and each voter processes the same input features. Furthermore, the computation within each voter is lightweight compared to the model backbone. Therefore, the overall inference time is dominated by the backbone, and increasing the number of voters does not noticeably affect the total latency."
        },
        {
            "title": "C BASELINE MODELS",
            "content": "The baseline models considered in our study are as follows: (1) HuBERT-500 (Hsu et al., 2021), self-supervised speech representation model that leverages iterative offline clustering to produce pseudo-labels and employs masked prediction loss; we use the official checkpoint with 500 clusters2; (2) NAST (Messica & Adi, 2024), noise-aware speech tokenization approach comprising predictor, residual encoder, and decoder, in which the predictor representations of clean speech and augmented speech are explicitly aligned; (3) R-Spin (Chang & Glass, 2024), which enhances the robustness of speech representations by learning discrete, speakerand noise-invariant acoustic units through prediction-based training objective; (4) SpeechTokenizer (Zhang et al., 2023), which introduces hierarchical encoder-decoder framework with residual vector quantization (RVQ) to unify semantic and acoustic tokens; (5) X-Codec (Ye et al., 2025), which augments the RVQ backbone with semantic features from pre-trained semantic encoder and applies semantic reconstruction loss to achieve higher fidelity in audio generation; (6) Mimi (Défossez et al., 2024), neural audio codec using RVQ to convert audio into discrete tokens, where the first quantization level is distilled to capture semantic 1https://github.com/Lyken17/pytorch-OpCounter 2https://github.com/facebookresearch/fairseq/tree/main/examples/hubert 19 information; (7) CosyVoice (S 3 Tokenizer) (Du et al., 2024a), which extracts supervised semantic tokens from multilingual speech recognition encoder for LLM-based TTS, thereby improving content consistency and speaker similarity in voice cloning; (8) CosyVoice2 (Du et al., 2024b), which introduces finite-scalar quantization (FSQ) to improve the codebook utilization and updates the model architecture for streaming synthesis capabilities; (9) GLM-4-Voice (Zeng et al., 2025), which fine-tunes pre-trained ASR model by including pooling layer and vector quantization layer, producing discrete tokens that strongly preserve semantic information at low frame rates."
        },
        {
            "title": "D NOISE PROFILES",
            "content": "In tokenizer-level evaluation, we augment the FLEURS (Conneau et al., 2023) benchmark with variety of synthetic perturbations, including Gaussian Noise, Pink Noise, Brown Noise and Bit Crush Distortion, as well as real-world noise samples from the ESC-50 (Piczak, 2015) dataset. Specifically, the ESC-10 (Piczak, 2015) subset is used as out-of-domain (OOD) real-world noise and is excluded from our StableToken training pipeline, while the remaining 40 noise categories from ESC-50 are incorporated into the training process and thus are considered in-domain real-world noise. We carefully adjusted the noise level to ensure that the added noise does not obscure the semantic content of the original audio and does not affect human perception of the speech. summary of all perturbation types and their corresponding intensity is provided in Table 13. Table 13: Details of synthetic and real-world perturbations used for noise augmentation. Perturbation Type Intensity Value Gaussian Noise Pink Noise Brown Noise Bit Crush Distortion Real-world Noise OOD Real-world Noise SNR = 25 SNR = 22 SNR = 16 Bit Depth = 10 SNR = 16 SNR ="
        },
        {
            "title": "E DETAILS OF DOWNSTREAM TASK EVALUATION",
            "content": "E.1 TRAINING DATASETS FOR SPEECHLLMS In this section, we summarize the speech datasets employed for training SpeechLLMs in Table 14, covering various tasks including Automatic Speech Recognition (ASR), Speech Emotion Recognition (SER), Text-to-Speech (TTS), and Speech Next Token Prediction (SNTP). E.2 TRAINING HYPERPARAMETERS FOR SPEECHLLMS Table 15 summarizes the main hyperparameters used throughout downstream SpeechLLM training. Unless otherwise specified, these settings are uniformly adopted for all tasks and models. E.3 PROMPTS USED IN DOWNSTREAM TASKS This appendix contains the complete lists of textual prompts used for Automatic Speech Recognition (ASR), Speech Emotion Recognition (SER), and Text-to-Speech (TTS) tasks. For both fine-tuning and inference, prompt was randomly selected from the corresponding set for each sample. 3https://www.openslr.org/68/ 20 Table 14: Summary of datasets used for training SpeechLLMs Dataset Task Language(s) ASR English LibriSpeech (Panayotov et al., 2015) Multi-Lingual Librispeech (Pratap et al., 2020) ASR English TESS (Dupuis & Pichora-Fuller, 2010) SAVEE (Jackson & Haq, 2014) RAVDESS (Livingstone & Russo, 2018) MELD (Poria et al., 2018) MEAD (Wang et al., 2020) JL-Corpus (James et al., 2018) IEMOCAP (Busso et al., 2008) Expresso (Nguyen et al., 2023) EmoV-DB (Adigwe et al., 2018) EMNS (Noriy et al., 2023) Dailytalk (Lee et al., 2023) CREMA-D (Cao et al., 2014) CASIA (Zhang & Jia, 2008) M3ED (Zhao et al., 2022) MER2023 (Lian et al., 2023) CSEMOTIONS (Tian et al., 2025) ESD (Zhou et al., 2022) Hi-Fi TTS (Bakhturina et al., 2021) VCTK (Veaux et al., 2017) LibriTTS (Zen et al., 2019) GigaSpeech (Chen et al., 2021) VoxPopuli (Wang et al., 2021) MagicData3 AISHELL-1 (Bu et al., 2017) WenetSpeech (Zhang et al., 2022) SER English SER English SER English SER English SER English SER English SER English SER English SER English SER English SER English SER English SER Chinese SER Chinese SER Chinese SER Chinese SER English, Chinese English TTS English TTS TTS English SNTP English SNTP English Chinese TTS TTS Chinese SNTP Chinese Table 15: Hyperparameters used for training all downstream SpeechLLMs. Hyperparameter optimizer_type lr_scheduler learning_rate min_lr lr_decay_ratio weight_decay grad_clip Value Adam Cosine 4.0e-4 4.0e-5 0.75 0.1 1.0 21 E.3.1 ASR TRANSCRIPTION PROMPTS Please transcribe the following audio content into text. Please convert the following recording into text. Please transcribe this audio recording into text. Transcribe the following audio content into text. Convert the following recording into text. This audio recording needs to be transcribed into text. Recognize and convert the following speech content into text. Turn the following audio file into text. Transcribe this recording into text. Transcribe the following audio file into text. Convert this speech recording into text. Recognize the following audio content and convert it into text. Transcribe the following recording into text. E.3.2 SER EMOTION PROMPTS What is the emotion of this text? Analyze the sentiment of the following sentence. Identify the feeling expressed in this audio. Is the tone of this message positive or negative? Detect the emotion in the users feedback. What emotion is being conveyed here? Classify the emotion of this statement. Tell me the emotional state of the speaker. Analyze the emotional content of this speech. E.3.3 TTS SYNTHESIS PROMPTS Please synthesize the following text into speech. Convert the following text to speech. Transform the following text into speech. This text needs to be synthesized into speech. Synthesize the following text into speech. Turn the following text into speech. Generate speech from the following text. Convert the text below into speech. Create speech from the following text. Produce speech from the following text. Render the following text as speech."
        },
        {
            "title": "F FULL RECONSTRUCTION RESULTS",
            "content": "The comprehensive results for the tokenizer-level reconstruction quality evaluation are provided in Table 16. Note that SSL-based semantic tokenizers are not included in this comparison, as there are no publicly available decoders for reconstructing audio from their generated tokens."
        },
        {
            "title": "G RELATED WORK",
            "content": "Semantic Speech Tokenizers The evolution of LLMs has driven the transition of spoken dialogue models from traditional pipelines to end-to-end SpeechLLMs (Zhang & Wang, 2019; Zhang et al., 2020; Jacqmin et al., 2022; Lee et al., 2021; Fang et al., 2024; Défossez et al., 2024; Wang et al., 2024), with semantic tokenizers becoming increasingly crucial. The design of semantic tokenizers has evolved through several distinct paradigms. Early approaches utilized self-supervised learning (SSL) to derive discrete units from unlabeled data (Hsu et al., 2021; Baevski et al., 2020; Chen et al., 2022; Chung et al., 2021; Conneau et al., 2021; Chiu et al., 2022; Baevski et al., 2019; Liu et al., 2023; 22 Table 16: WER () and MOS () on LibriSpeech (Panayotov et al., 2015) and SEED (Anastassiou et al., 2024). StableToken combines strong noise robustness with competitive reconstruction quality. It is worth noting that comparison is most meaningful between tokenizers of the same type. Model #C Frame Rate BPS LSclean LSother SEED en SEED zh LSclean LSother SEED en SEED zh WER MOS Semantic Distilled tokenizer SpeechTokenizer (Zhang et al., 2023) X-Codec (Ye et al., 2025) Mimi (Défossez et al., 2024) 1 3 8 1 3 8 50Hz 50Hz 50Hz 50Hz 50Hz 50Hz 500 1500 4000 500 1500 4000 12.5Hz 1100 4. 3.98 3.16 3.09 9.02 6.11 5.49 9.84 4.77 16.06 10.37 74.98 7.81 4.03 10.72 2.25 6.58 3.21 4.93 2.77 GLM-4-Voice-Token. (Zeng et al., 2025) 3 Tokenizer (Du et al., 2024a) CosyVoice2 (Du et al., 2024b) StableToken (Ours) Supervised Semantic tokenizer 1 1 1 12.5Hz 175 4.04 9.33 25Hz 25Hz 25Hz 300 325 325 5.78 13.38 4.25 3. 9.68 7.99 2.51 3.00 3.32 3.17 3.43 3.47 2.49 2.89 3.10 3.04 3.17 3. 5.96 2.24 1.74 2.81 3.26 3.06 3.23 4. 2.75 2.62 4.07 3.99 3.40 3. 3.36 3.25 4.09 3.83 2.51 2.89 3.22 3.05 3.19 3. 3.15 4.16 3.40 3.31 4.01 2.44 3.06 3. 3.18 3.38 3.33 3.19 4.10 3.31 3.58 4. 4.72 2.74 2.25 3.86 3.54 5.91 4.34 3. Gat et al., 2023; Huang et al., 2022; Lodagala et al., 2023; Chang et al., 2023). The vast majority of tokens produced by these methods are designed for discriminative tasks. It is reported that discretized SSL tokens primarily encode phonetic information, causing high Gross Pitch Error (GPE) when paired with vocoder for audio generation, making them unsuitable for end-to-end SpeechLLMs (Sicherman & Adi, 2023; Polyak et al., 2021; Mousavi et al., 2024; Guo et al., 2025). second category employs hybrid approach, enhancing an acoustic tokenizer with semantic distillation to balance acoustic fidelity and semantic content (Zhang et al., 2023; Ye et al., 2025; Défossez et al., 2024; Siahkoohi et al., 2022; Yang et al., 2024b). This design enables strong performance on both generative and discriminative tasks. However, their integration with downstream large language models (LLMs) is hampered by several significant challenges. First, to preserve high fidelity, these methods tend to encode excessive acoustic details, which results in high bits-persecond (BPS) rate. This high data rate generates longer token sequences, thereby increasing the computational load and impairing training efficiency. Furthermore, their reliance on Residual Vector Quantization (RVQ) produces hierarchical tokens that are inherently incompatible with the flat input structure expected by most LLMs. Collectively, the high data rate, the structural mismatch, and the overhead of processing superfluous acoustic information present substantial obstacles to their application in modern SpeechLLMs. More recently, third and more direct paradigm has gained traction: fully supervised training. Given that the primary goal is to capture semantic and phonetic information, this approach directly uses an Automatic Speech Recognition (ASR) objective for supervision. The process involves quantizing the intermediate representations of powerful ASR encoder and optimizing the model with an ASR loss, ensuring the resulting tokens directly represent linguistic units (Zeng et al., 2025; Du et al., 2024a;b). Subsequently, downstream vocoder is trained to convert these discrete tokens into mel-spectrograms for speech synthesis. This tokenizer design is foundational to the current state-of-the-art end-toend SpeechLLMs, underscoring its effectiveness and growing adoption. Interestingly, research has revealed that while the ASR objective targets linguistic content, the resulting tokens retain sufficient extra-phonetic information (e.g., prosody). This is likely because the ASR encoder implicitly learns to model prosodic features as they serve as valuable auxiliary cues for achieving high transcription accuracy. This retained information allows an integrated LLM to generate highly expressive synthesis and convey complex emotions. Consequently, this designs ability to support expressive generation has made it foundational choice for state-of-the-art SpeechLLMs (Zeng et al., 2024; Ding et al., 2025). Noise Robustness Ensuring the stability of discrete speech tokens in the presence of noise is critical for the performance of modern Speech Language Models (SLMs). However, this issue has been 23 largely overlooked compared to the extensive research focused on improving the robustness of the Automatic Speech Recognition (ASR) model itself (Wang et al., 2022; Tjandra et al., 2023; Eickhoff et al., 2023; Gong et al., 2023; Ahn et al., 2025). Recently, two studies have begun to address this gap by investigating the noise robustness of traditional SSL-based speech tokenizers. R-SPIN (Chang & Glass, 2024) addresses this by learning speakerand noise-invariant discrete units through data-efficient self-supervised framework. It extends the speaker-invariant clustering of Spin by using an additional noise-perturbed view of the input and an auxiliary loss that predicts \"acoustic pieces,\" which are phoneme-aligned pseudo-labels, to prevent model collapse and ensure the resulting discrete units represent pure linguistic content . In contrast, NAST (Messica & Adi, 2024) proposes an architecture designed explicitly for robust tokenization, consisting of predictor, residual encoder, and decoder. Its training is governed by combination of reconstruction loss, diversity loss to encourage codebook usage, and crucial robustness loss that penalizes changes in the predicted token distribution between clean and noise-augmented versions of the same utterance, thereby directly optimizing for token-level stability. Liu et al. (2025) introduce slice-consistency and perturbation-consistency constraints to mitigate discrete representation inconsistency, but their approach targets acoustic tokenizers (rather than semantic tokenizers), which prioritize audio detail reconstruction. Therefore, noise invariance is less meaningful in their context, making their work fundamentally different from ours."
        }
    ],
    "affiliations": [
        "Pattern Recognition Center, WeChat AI, Tencent Inc",
        "State Key Laboratory of Multimedia Information Processing, Peking University"
    ]
}