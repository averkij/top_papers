{
    "paper_title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO",
    "authors": [
        "Lai Wei",
        "Yuting Li",
        "Chen Wang",
        "Yue Wang",
        "Linghe Kong",
        "Weiran Huang",
        "Lichao Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, a stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9 %$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at https://github.com/waltonfuture/MM-UPT."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 3 5 4 2 2 . 5 0 5 2 : r Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO Lai Wei1,3 Yuting Li1 Chen Wang3 Yue Wang3 Linghe Kong1 Weiran Huang1,2 Lichao Sun 1 School of Computer Science, Shanghai Jiao Tong University 2 Shanghai Innovation Institute 3 Zhongguancun Academy 4 Lehigh University"
        },
        {
            "title": "Abstract",
            "content": "Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal dataan ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3%72.9% on MathVista, 62.9%68.7% on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting promising approach for scalable self-improvement. Overall, MM-UPT offers new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at https://github.com/waltonfuture/MM-UPT."
        },
        {
            "title": "Introduction",
            "content": "Multi-modal Large Language Models (MLLMs) have achieved remarkable performance on variety of vision-language tasks, ranging from image captioning to visual reasoning [19, 21, 42, 47, 69]. By combining the language understanding capabilities of large language models (LLMs) with visual perception, MLLMs can process and reason over both textual and visual information. The dominant paradigm for improving MLLMs in the post-training stage typically involves supervised fine-tuning (SFT) and reinforcement learning (RL). However, both SFT and RL rely on large volumes of highquality and annotated multi-modal data, such as image captions, visual reasoning traces, ground truth answers, and human preference signals. As real-world tasks grow in complexity and quantity, critical challenge emerges: curating and annotating high-quality data at scale becomes increasingly impractical. Thus, it is essential to explore new methods for improving MLLMs, such as using synthetic and unlabeled data. Correspondence to Weiran Huang (weiran.huang@outlook.com). Figure 1: Overview of the MM-UPT framework. Given an unlabeled multi-modal input, the MLLM samples multiple responses, and uses majority voting to determine the pseudo-label. The MLLM is then updated via GRPO, enabling self-improvement without external supervision. Previous works have studied the use of MLLMs themselves to generate synthetic instruction data for self-improvement through offline training techniques like SFT and DPO [5, 3638, 55, 64]. These approaches typically involve complex pipelines with multiple stages, such as data generation, verification, and filtering, which are hard to iterate online. Fortunately, recent studies demonstrate notable success using online reinforcement learning (e.g. GRPO [34]) with verifiable rewards [6, 10, 28, 66] to enhance the reasoning capabilities of LLMs and MLLMs. concurrent work, TTRL [71], further extends this line by applying GRPO on test-time scaling of LLMs. Thus, it is promising that online RL enables models to continuously improve, thus acquiring novel reasoning abilities that exceed corresponding base models capacity. Motivated by these insights, we aim to investigate fundamental and open question: Can we enable MLLMs to continually and iteratively self-improve their reasoning abilities from fully unlabeled training data without any external supervision? To explore this question, we propose MM-UPT (Multi-Modal Unsupervised Post-Training), an easy-to-implement framework for unsupervised post-training in MLLMs. As shown in Figure 1, MM-UPT builds on GRPO [34], stable and scalable online RL method that uses group-normalized rewards instead of explicit value functions. Unlike GRPO that relies on ground-truth labels to calculate rewards, our framework works by deriving implicit reward signals via majority voting over multiple sampled responses. In particular, majority voting aggregates multiple responses and selects the most frequent one, which has been widely used and shown effective to improve model performance [40, 45, 71]. Thus, we adopt the majority-voted answer to serve as pseudo-label to calculate reward in our framework for unsupervised training, which encourages the model to prefer stable and high-consensus answers without any human annotated labels or external reward models. In our experiments, we focus on the domain of multi-modal reasoning, which is widely studied and inherently challenging. We explore two key scenarios for constructing unlabeled data assuming that labels are not available: (1) using human-created questions without ground-truth labels, and (2) employing synthetic questions generated by AI models, inherently lacking ground-truth labels. The first scenario is simulated by masking the answers in standard training datasets. For the second scenario, we employ two MLLM-driven strategies: (a) generating new questions from in-context examples (including the original image, question, and answer), and (b) generating questions based solely on the image. We evaluate MM-UPT across range of reasoning benchmarks and observe significant performance improvements over the base models (e.g., 66.3%72.9% on MathVista, 62.9%68.7% on We-Math using Qwen2.5-VL-7B [1]). Our method also outperforms previous baseline methods, and is even competitive with supervised GRPO, underscoring the effectiveness of MM-UPT as self-improving training strategy in fully unsupervised manner. Additionally, we find that models trained on unlabeled synthetic data achieve performance competitive with those trained on the original unlabeled dataset, revealing strong potential for scalable self-improvement. Our main contributions are summarized as follows: We propose MM-UPT, novel framework for unsupervised post-training of MLLMs that enables continuous self-improvement without requiring any external supervision. 2 Extensive experiments on multi-modal reasoning tasks demonstrate the effectiveness of majority voting as pseudo-reward estimation for unsupervised training. We investigate the use of synthetic data generated by the MLLM itself and find that training the MLLM on such data leads to notable performance gains. This reveals promising path toward efficient and stable self-improvement in unsupervised post-training."
        },
        {
            "title": "2 Related Works",
            "content": "Self Improvement. High-quality data obtained from human annotations has been shown to significantly boost the performance of LLMs across wide range of tasks [10, 16, 29]. However, such high-quality annotated data may be exhausted in the future. This presents substantial obstacle to the continual learning of advanced models. As result, recent research has shifted toward self-improvementleveraging data generated by the LLM itself without any external supervision [7, 14, 26, 38, 49, 56, 61, 71]. Several following works also explore unsupervised selfimprovement in the multi-modal domain [5, 9, 36, 55, 62, 64]. Genixer [64] firstly introduces comprehensive self-improvement pipeline including data generation and filtering for SFT. STIC [5] and SENA [37] construct preference data pairs for DPO [33] in fully self-supervised manner. In contrast to these approaches which are complex and hard to iterate, the key distinction is that our method leverages online reinforcement learning using GRPO [10] at the post-training stage of MLLMs, which is more scalable, and supports continuous and iterative self-improvement without reliance on any external supervision. In addition, none of these previous methods focus on multi-modal reasoning tasks, which are considered challenging for current models. Multi-modal Reasoning. Recently, the reasoning abilities of MLLMs have become central focus of research [13, 24, 70]. In contrast to traditional LLM-based reasoning [10, 25, 54] that primarily relies on text, multi-modal approaches must both process and interpret visual inputs, significantly increasing the complexity of tasks such as geometric problem-solving and chart interpretation [2, 27, 63]. Several works in this field have sought to collect or synthesize large scale of multi-modal reasoning data [4, 30, 35, 58]. Notably, the recent emergence of o1-like reasoning models [17] represents an initial step toward activating the slow-thinking capabilities of MLLMs, as demonstrated by several SFT-based methods, such as LLaVA-CoT [50], LLaVA-Reasoner [59], MAmmoTH-VL [11], and Mulberry [52]. Moreover, some concurrent works have further explored reinforcement learning approaches, particularly GRPO [34], in the post-training stage of MLLMs to enhance performance on multi-modal reasoning tasks [6, 28, 31, 44, 66]. While these supervised post-training methods have demonstrated promising results, our work explores different direction by focusing on totally unsupervised post-training of MLLMs to self-improve the reasoning abilities."
        },
        {
            "title": "3 The Framework of Multi-Modal Unsupervised Post-Training",
            "content": "Unlike traditional post-training techniques that require labeled data or external reward models, we propose MM-UPT (Multi-Modal Unsupervised Post-Training), simple yet effective framework designed to operate purely on unlabeled training data. That is, the model must learn to self-improve without access to any external supervision such as ground-truth labels or additional reward models. 3.1 Problem Formulation Firstly, we formulate the problem of unsupervised post-training for MLLMs as follows: Given well-trained multi-modal LLM πθ and collection of unlabeled multi-modal data = {(Ii, qi)}N i=1, where Ii represents an image and qi denotes corresponding question, our goal is to improve the models performance without access to any ground-truth answers or external supervision signals. This setting differs significantly from conventional supervised fine-tuning (SFT), reinforcement learning with verifiable rewards (RLVR), or reinforcement learning with human feedback (RLHF), which typically rely on labeled data (Ii, qi, yi) or human preference data (Ii, qi, y+ ), where yi denotes the answer of qi and (y+ ) denotes the preference pair of qi. In contrast, we only allow to operate in fully unsupervised manner for this setting, leveraging only the models own responses to generate training signals. This presents significant challenges, as the model must learn to assess and improve its own outputs without any external guidance. , , Algorithm 1 MM-UPT: Multi-Modal Unsupervised Post-Training 1: Input: Current policy πθ, old policy πθold , unlabeled training dataset Q, Group size G, reference model πref , clip parameter ϵ, KL penalty coefficient β, answer extractor E(). i=1; i=1 πθold (o I, q); 2: for each sample (I, q) do 3: 4: 5: 6: 7: 8: 9: Sample group of responses: {oi}G Extract answers: ˆY = E(O) = {ˆyi}G Determine majority vote: arg maxy ˆY Compute pseudo-rewards: ri I[ˆyi = y]; Compute advantage estimates: ˆAi rimean({r1,r2,...,rG}) Compute GRPO objective: (θ) 1 where γi,t(θ) = πθ (oi,tI,q,oi,<t) Update policy parameters: θ θ θJGRP O(θ); Update old policy: θold θ; (oi,tI,q,oi,<t) ; std({r1,r2,...,rG}) (cid:80)oi t=1 1 oi (cid:80)G (cid:80)G min πθold i=1 i=1 (cid:110) (cid:104) ; 10: I[y = ˆyi]; 11: 12: 13: end for 14: return πθ // Sample multiple responses // Select the most frequent answer // Reward based on majority agreement γi,t(θ) ˆAi, clip (γi,t(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:105) βDKL[πθπref ] (cid:111) 3.2 Method To achieve this, MM-UPT introduces self-rewarding mechanism using majority voting as pseudolabels [45] based on the online reinforcement learning. In particular, MM-UPT is built upon the GRPO algorithm [34], which is widely used in the post-training stage of multi-modal LLMs. GRPO optimizes computational efficiency by eliminating the need for separate value model; instead, it directly utilizes group-normalized rewards to estimate advantages. Specifically, for question and the correlated image from the training dataset Q, GRPO samples group of responses = {oi}G i=1 from the old policy πold and then optimizes the policy model by maximizing the following objective: (θ) = (q,I)Q,{oi}G i=1πθold (Oq,I) (cid:34) (cid:40) 1 (cid:88) i= 1 oi oi (cid:88) t=1 min γi,t(θ) ˆAi,t, clip (γi,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t βDKL (cid:35) (cid:104) πθπref (cid:41) (cid:105) , where γi,t(θ) = πθ(oi,tq,oi,<t) πθold (oi,tq,oi,<t) , πref represents the reference model, and the term DKL introduces KL divergence constraint to limit how much the model can deviate from this reference. The advantage estimate ˆAi measures how much better the response oi is compared to the average response, which is computed using group of rewards {r1, r2, . . . , rG} for the responses in set O: ˆAi = rimean({r1,r2,...,rG}) . std({r1,r2,...,rG}) In the above standard GRPO formulation [10], the reward is computed in supervised manner based on labels for each response in = {oi}G i=1. Shifting towards our unsupervised setting, where no ground-truth labels are available, one feasible way is to construct pseudo-labels to calculate the reward for GRPO. Motivated by [14, 45, 71], we use majority voting over the group of sampled responses to serve as pseudo-labels. Majority voting selects the most frequent answer among the sampled responses and has proven to be simple yet effective technique [45, 71], making it suitable for deriving good pseudo-reward signals. Specifically, we first extract answers from the responses = {oi}G i=1. Then, the majority-voted answer can be obtained by: i=1 using an rule-based answer extractor [12] E(), resulting in ˆY = E(O) = {ˆyi}G = arg max ˆY (cid:88) i=1 I[y = ˆyi], where I[] is the indicator function. The reward ri is then determined based on the y: ri = (cid:26)1, if ˆyi = y, 0, otherwise. 4 (1) (2) Table 1: Main results on four multi-modal mathematical reasoning benchmarks. We report accuracy (%) for each method on MathVision, MathVerse, MathVista, and We-Math. All methods are conducted on the Qwen2.5-VL-7B backbone. MM-UPT outperforms other baseline methods, and is even competitive with supervised methods. Model and Methods Unsupervised? Training Data MathVision MathVerse MathVista We-Math Qwen2.5-VL-7B + GRPO [34] + GRPO [34] + GRPO [34] + SFT [39] + SFT [39] + SFT [39] + SRLM [49] + SRLM [49] + SRLM [49] + LMSI [14] + LMSI [14] + LMSI [14] + Genixer [64] + Genixer [64] + Genixer [64] + STIC [5] + STIC [5] + STIC [5] + MM-UPT + MM-UPT + MM-UPT - - Geometry3K GeoQA MMR Geometry3K GeoQA MMR1 Geometry3K GeoQA MMR1 Geometry3K GeoQA MMR1 Geometry3K GeoQA MMR1 Geometry3K GeoQA MMR1 Geometry3K GeoQA MMR 24.87 28.32 26.15 29.01 25.92 25.72 26.45 26.94 25.16 25.33 25.10 25.49 24.83 26.02 25.30 23. 25.39 23.49 23.78 27.33 27.07 26.15 43.83 46.40 46.28 45.03 43.73 44.70 43.53 44.54 44.62 45. 43.96 43.50 43.76 43.15 44.11 43.30 42.92 42.87 42.72 42.46 43.68 44.87 66.30 69.30 67.50 71. 67.90 67.40 63.30 66.90 66.30 67.00 65.50 66.60 64.90 65.50 66.80 65.50 65.20 64.30 66.10 68.50 68.90 72. 62.87 68.85 66.65 67.24 64.94 65.10 64.20 66.32 65.00 64.66 64.43 63.51 66.38 62.18 64.25 64. 62.99 63.62 63.74 66.61 68.22 68.74 Avg 49.47 53.22 51.65 53.17 50.63 50.73 49. 51.18 50.27 50.52 49.75 49.78 49.97 49.22 50.12 49.29 49.13 48.57 49.09 51.23 51.97 53.17 In this way, we compute pseudo-rewards via majority voting and apply standard GRPO to update the MLLM. This majority-based reward encourages the model to converge toward consistent, highconsensus responses, thereby enabling the model to further exploit its existing self-knowledge leveraging unlabeled data. The overview of our framework is shown in Figure 1 and Algorithm 1."
        },
        {
            "title": "4 Experiments",
            "content": "We conduct extensive experiments to evaluate the effectiveness of MM-UPT across various multimodal LLMs, datasets, and benchmarks. Our experiments are designed to explore two key scenarios: (1) using human-created questions without ground-truth labels (Section 4.2), and (2) employing synthetic questions generated by the model itself, inherently lacking ground-truth labels (Section 4.3). Before presenting the experimental results, we first outline the baseline methods, evaluation benchmarks, and implementation details in the experimental setup as follows. 4.1 Experimental Setup Baseline Methods. Several prior works have explored self-improvement in both LLMs and MLLMs. Note that we focus on unsupervised self-improvement, we do not compare with methods that rely on external models (e.g., GPT-4o [16]) for supervision [15, 55, 60, 67, 68]. Instead, we compare with several totally unsupervised methods: LMSI [14], SRLM [49], Genixer [64], and STIC [5]. In particular, LMSI corresponds to supervised fine-tuning with self-generated content selected by majority voting. SRLM uses the model itself as LLM-as-a-Judge [65] to provide its own rewards during DPO [33] training. Genixer prompts the MLLM to first self-generate an answer and then self-check it. STIC applies DPO where original images and good prompts are used to generate preferred answers, and corrupted images and bad prompts to produce rejected answers. Additionally, we also compare with GRPO [34] and rejection sampling-based SFT [39], which are two strong supervised methods. The details of these baseline methods are shown in Appendix A.1. 5 Table 2: Performance comparison of MM-UPT using different synthetic data generation strategies. Both In-Context Synthesizing and Direct Synthesizing approaches yield significant improvements over the base model and perform competitively with the Original Questions on average, demonstrating the effectiveness of synthetic data for unsupervised self-improvement. Model and Methods Dataset MathVision MathVerse MathVista We-Math Avg Qwen2.5-VL-7B w/ Original Questions w/ In-Context Synthesizing w/ Direct Synthesizing Geo3K Geo3K Geo3K w/ Original Questions GeoQA w/ In-Context Synthesizing GeoQA GeoQA w/ Direct Synthesizing MMR1 w/ Original Questions w/ In-Context Synthesizing MMR1 MMR1 w/ Direct Synthesizing 24.87 27.33 26.71 26.88 27.07 26.09 26.25 26.15 26.15 26.15 43.83 42.46 41.24 43.53 43.68 42.87 44. 44.87 45.10 44.11 66.30 68.50 68.30 69.90 68.90 70.60 71.50 72.90 71.90 70.40 62.87 66.61 67.76 68.97 68.22 69.25 68. 68.74 68.62 67.99 49.47 51.23 (3.6%) 51.00 (3.1%) 52.32 (5.8%) 51.97 (5.1%) 52.20 (5.5%) 52.67 (6.5%) 53.17 (7.5%) 52.94 (7.0%) 52.16 (5.4%) Benchmarks. We evaluate our method on four popular multi-modal mathematical reasoning benchmarks: MathVision [41], MathVista [24], MathVerse [57], and We-Math [32]. These benchmarks offer comprehensive evaluations with diverse problem types, including geometry, charts, and tables, featuring multi-subject and meticulously categorized visual math challenges across various knowledge concepts and granularity levels. We provide more details in Appendix A.2. Implementation Details. We adopt the EasyR1 [53] framework for multi-modal unsupervised post-training, which is based on GRPO. Specifically, we set the training episodes to 15, and use AdamW optimizer [22] with learning rate of 1 106, weight decay of 1 102, and gradient clipping at maximum norm of 1.0. The KL divergence constraint β in GRPO is set to 0.01 to stabilize the training. The vision tower of the multi-modal model is also tuned without freezing. Other hyperparameters follow the default settings provided in the EasyR1 framework. 4.2 Unsupervised Training on Standard Datasets For our experiments, we firstly employ standard training datasets with masked labels to simulate the first scenario (i.e., using human-created questions without ground-truth answers). We conduct MM-UPT on Geometry3k [23], GeoQA [3], and MMR1 [18] using the Qwen2.5-VL-7B [1] model. These datasets cover diverse set of visual math problems, including geometric diagrams, charts, and structured question formats (multiple-choice and fill-in-the-blank), serving as strong foundation for models to self-improve the multi-modal mathematical reasoning abilities. More details of these datasets are introduced in Appendix A.3. Table 1 presents the main results on four challenging multi-modal mathematical reasoning benchmarks. We observe that MM-UPT achieves consistent improvements in average over the base Qwen2.5VL-7B model across all datasets, also outperforming other baseline methods such as SRLM, LMSI, Genixer, and STIC. Notably, MM-UPT is able to improve the average score from 49.47 (base model) to 53.17 (with MMR1 dataset), demonstrating its effectiveness in leveraging unlabeled data for self-improvement. In comparison, previous baselines provide only marginal gains or even degrade performance on certain benchmarks, highlighting the limitations of existing methods when applied to already strong models in multi-modal reasoning tasks. Furthermore, we find that MM-UPT is even competitive with supervised post-training methods, such as rejection sampling-based SFT [39] and GRPO [34]. These results underscore the potential of MM-UPT to further exploit the knowledge embedded in multi-modal models for self-improvement. 4.3 Unsupervised Training on Synthetic Datasets To further explore the potential of MM-UPT, we investigate the use of unlabeled synthetic data to improve MLLMs. This aligns with the ultimate goal of MM-UPT: enabling continual selfimprovement even after human-created data is exhausted. We utilize two strategies for generating synthetic training samples. In-Context Synthesizing. Inspired by Self-Instruct [46], we adopt data generation pipeline based on in-context examples. Each original example includes an image, question, and its corresponding answer. To synthesize new data, we provide the base MLLM with the full original tripletimage, question, and answerand ask it to generate new question that is distinct from the original. During the unsupervised post-training, the MLLM attempts to answer this new question, and we define the pseudo-label as the majority vote among the sampled responses of the model. Direct Synthesizing. In addition, we also adopt more straightforward approach to generate synthetic data. Instead of using the full triplet, we prompt the base MLLM with only the image, asking it to create new question without any context from the original question or answer. This produces different type of synthetic sample, where the image remains the same, but the question is generated entirely from visual input. As with the in-context approach, the pseudo-label for training is determined by aggregating several model responses through majority voting. Results. In our experiment, we use the previous two methods to generate the synthetic data, leveraging Geometry3K [23], GeoQA [3], and MMR1 [18] as the seed datasets, and Qwen2.5-VL-7B as the base MLLM for data synthesis. MM-UPT is then applied to the same base model (i.e., Qwen2.5-VL-7B) using each of these different synthetic datasets separately. Table 2 presents experimental results using different synthetic data generation strategies. Both in-context and direct synthesizing lead to significant improvements over the base model, achieving performance comparable to training on original human-written questions. This shows that synthetic questions can effectively enhance the models reasoning ability under MM-UPT. Notably, direct synthesizing even surpasses human-written questions (when applied to Geometry3K and GeoQA) on average, demonstrating the strong ability of the model to generate high-quality textual questions solely based on images. This highlights the potential for scalable and fully autonomous self-improvement in multi-modal domain via visualcentric data synthesis. Further Investigation. Moreover, we manually examine some synthetic data. We observe that incontext synthesizing often produces questions similar to the original ones by substituting conditions or expressions, resembling data rephrasing. In contrast, direct synthesizing generates more diverse and novel questions. While some of the directly synthesized questions still contain hallucinations, many are of high quality and beneficial for unsupervised post-training. This underscores the potential of the direct synthesizing approach as simple yet effective method for data generation, without the need for textual in-context examples. Below, we present two illustrative examples that showcase the effectiveness and quality of synthetic questions generated through both approaches. Demo: Examples of synthetic data using different strategies. Original Question: BCDF . AB = + 5, BD = 12, AC = 3x + 1, and CF = 15. Find x. In-Context Synthetic Question: Given that BCDF , AB = 2x 3, BD = 18, AC = + 7, and CF = 24. Find the value of x. Directly Synthetic Question: In the given triangle ADF , point lies on AD and point lies on AF . If BC DF , what is the ratio of the area of ABC to the area of ADF ? Original Question: If RST is right angle, SU RT , ST , and RT = 47, find RSU In-Context Synthetic Question: If RST is right angle, SU RT , ST , and RT = 47, find ST . Directly Synthetic Question: In the given triangle RST , point lies on RT such that SU is perpendicular to RT . Point lies on ST such that is perpendicular to ST . If RU = 12 units, = 16 units, and SV = 9 units, find the length of . 4.4 Ablation Study To evaluate the generality and effectiveness of MM-UPT, we conduct an ablation study across range of backbone models beyond the primary Qwen2.5-VL-7B [1]. Specifically, we apply MM-UPT to several state-of-the-art models of varying scales, including Qwen2.5-VL-3B [1], MM-Eureka7B [28], and ThinkLite-VL-7B [44]. All models are post-trained using MM-UPT on the Geometry3K dataset [23], without access to any labels. As summarized in Table 3, MM-UPT consistently improves the performance of all tested models on average, despite the absence of supervision during posttraining. Notably, ThinkLite-VL-7B with MM-UPT achieves the highest average score (54.07), and shows substantial gains on the MathVista [24] benchmark, reaching score of 74.70. In addition, Table 3: Ablation study using different models besides Qwen2.5-VL-7B. We conduct this experiment on Geometry3K [23] dataset without labels. Models MathVision MathVerse MathVista We-Math Avg Qwen2.5-VL-7B Qwen2.5-VL-7B + MM-UPT MM-Eureka-7B MM-Eureka-7B + MM-UPT ThinkLite-VL-7B ThinkLite-VL-7B + MM-UPT Qwen2.5-VL-3B Qwen2.5-VL-3B + MM-UPT 24.87 27.33 28.06 28.95 26.94 26.91 19.47 22. 43.83 42.46 50.46 50.63 46.58 47.26 33.58 32.39 66.30 68.50 69.40 69. 69.00 74.70 56.30 57.10 62.87 66.61 64.48 66.44 67.99 67.41 50.63 55. 49.47 51.23 (3.6%) 53.10 53.78 (1.3%) 52.63 54.07 (2.8%) 39.00 41.72 (7.4%) Qwen2.5-VL-3B, the smallest model in our study, also benefits well from MM-UPT (+7.4% on average), demonstrating the robustness and adaptability of MM-UPT for performance enhancement. These results collectively reveal that MM-UPT can be easily applied to various multi-modal models to enable consistent self-improvement."
        },
        {
            "title": "5 Deeper Analysis",
            "content": "Going beyond standard benchmarking, we conduct deeper analysis to investigate MM-UPTs training dynamics (Section 5.1) and performance boundaries (Section 5.2 and Section 5.3). This helps better understand its behavior and potential applications. 5.1 Training Dynamics To better understand the behavior of MM-UPT during training, we monitor several diagnostic metrics, including the majority voting reward and entropy, both of which are label-free and provide insights in the absence of ground-truth supervision. In particular, majority voting reward is calculated following Equation 2. Entropy can be used as an unsupervised objective that measures the uncertainty of the models generation [43, 48, 56]. For group of responses = {oi}G i=1 sampled from the question and image I, we cluster the responses according to their meaning. That is, if two responses share the same meaning (i.e., extracted answers), they should be merged into one same cluster in the semantic space. This results to K(K G) clusters = {cj}K j=1. The empirical distribution over clusters is defined as: p(cjq, I) = cj , where cj denotes the number of responses that belongs to cj. The semantic entropy (denoted as H) over the models response meanings distribution can be estimated as follows: = (cid:88) cj {C} p(cjq, I) log p(cjq, I). Figure 2 presents the MM-UPT training curves of the key metrics on Qwen2.5-VL-7B using the MMR1 dataset. We observe that the majority voting reward consistently increases over time, accompanied by steady decrease in the entropy. This indicates that the model is converging toward more consistent predictions, reflecting improved confidence and stability in its responses. Additionally, we track the change in average benchmark accuracy and effective rank [48] throughout training. The accuracy exhibits an upward trend, demonstrating that our MM-UPT framework based on an online reinforcement learning algorithmeffectively enables the model to self-improve continuously and iteratively. The effective rank [48] further measures the amount of knowledge the model comprehends in the datasets. During training, the internal knowledge of the model is exploited, leading to consistent increase in the effective rank on the benchmark. 8 Figure 2: Training dynamics of MM-UPT using Qwen2.5-VL-7B on the MMR1 dataset. We plot the majority voting reward, semantic entropy, and average benchmark accuracy over the course of unsupervised post-training. Table 4: Performance of MM-UPT on the difficult ThinkLite-11K dataset. Results show that MMUPT leads to decrease in performance when applied to dataset where the model has limited prior knowledge, highlighting the limitations of majority voting in such scenarios. Models Training Data MathVision MathVerse MathVista We-Math Qwen2.5-VL-7B Qwen2.5-VL-7B + MM-UPT ThinkLite-11K 24.87 21.12 43.83 37.10 66.30 59.20 62.87 59. Avg 49.47 44.11 5.2 Why Does MM-UPT Work? Majority voting [45] is fundamental ensemble technique that enhances prediction reliability by aggregating multiple independent responses. In our framework, it offers simple yet powerful pseudo-reward signal to help model self-improve, particularly when the model are moderately reliable on the unlabeled datasets. We consider simplified explanation for it using classical toy example. Suppose that each response hits the correct answer with probability > 0.5 in binary question. Then, we sample the models response times independently. The final answer is determined by majority vote, that is, the answer that appears more than n/2 times. Let denote the number of correct predictions among the samples. Since each prediction is correct with probability p, follows binomial distribution: Binomial(n, p). The majority vote is correct if > n/2, and the corresponding probability of this event (denoted as E) is: (E) = (cid:88) i=n/2 (cid:18)n (cid:19) pi(1 p)ni. When > 0.5, it follows that (E) > p, which means that the ensemble outperforms each individual response. For instance, if = 0.7 and = 10, then (E) 0.849, demonstrating significant gain over the base accuracy. This analysis reveals the rationality of majority voting to serve as the pseudolabel for deriving reliable reward signal in the unsupervised setting. In our experimental setting, we mainly target datasets that are not especially hard, such as Geometry3k [23], GeoQA [3], and MMR1 [18], for unsupervised post-training. Hence, we hypothesize that the model has relatively high chance of answering questions in these datasets correctly. This allows the model to yield stable improvements through MM-UPT using majority voting as the pseudo-label. 5.3 When Might MM-UPT Fail? According to the analysis in Section 5.2, it reveals that the effectiveness of MM-UPT diminishes when the model lacks sufficient prior knowledge of the target dataset. To show that, we apply MM-UPT to ThinkLite-11K [44] dataset using Qwen2.5-VL-7B [1]. ThinkLite-11K is collected via difficultyaware sampling that only retains samples that the model rarely answers correctly. Thus, this setting reflects scenario where the model is more likely to be wrong than right. In such cases, majority voting amplifies incorrect answers rather than filtering them, leading to degraded performance. As shown in Table 4, applying MM-UPT to ThinkLite-11K results in significant drop in accuracy across all benchmarks. This suggests that majority voting fails to provide reliable reward signals when the model has limited prior understanding of the domain. To address this issue, alternative forms of algorithms in self-improvement [7, 26, 49, 56] using more fine-grained and complex self-rewarding methods, such as LLM-as-Judge [65] and model collaboration [20], may be necessary. Note that our 9 work represents an initial attempt at self-improvement in MLLMs via GRPO. We believe that these algorithms are complementary to our approach and could be integrated into our framework, which we leave as future work."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce MM-UPT (Multi-Modal Unsupervised Post-Training), framework that enables MLLMs to self-improve using unlabeled multi-modal data without any external supervision. By leveraging majority voting as reward mechanism within the GRPO algorithm, our method encourages models to converge toward more consistent responses across multi-modal reasoning tasks. Our experiments demonstrate that MM-UPT significantly improves the performance of various MLLMs across several multi-modal reasoning benchmarks without requiring human annotated labels or external reward models. Generally, MM-UPT is best viewed as post-training refinement strategy rather than substitute for supervised training. We recommend first training the model using labeled data to ensure it achieves basic competence on various tasks. After that, MM-UPT can be used to further exploit the models internal knowledge by reinforcing consistent predictions. Future work could explore other fine-grained methods to provide pseudo-reward signals based on our framework, and investigate the scaling laws of unsupervised post-training using synthetic data."
        },
        {
            "title": "Acknowledgement",
            "content": "This project is supported by the National Natural Science Foundation of China (No. 62406192), Opening Project of the State Key Laboratory of General Artificial Intelligence (No. SKLAGI2024OP12), Tencent WeChat Rhino-Bird Focused Research Program, and Doubao LLM Fund."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 513523, 2021. [3] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. [4] Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, and Yang Liu. Vision-language models can self-improve reasoning via reflection. arXiv preprint arXiv:2411.00855, 2024. [5] Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, Quanquan Gu, James Zou, Kai-Wei Chang, and Wei Wang. Enhancing large vision language models with self-training on image comprehension. Advances in Neural Information Processing Systems, 37:131369131397, 2024. [6] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025. [7] Qingxiu Dong, Li Dong, Xingxing Zhang, Zhifang Sui, and Furu Wei. Self-boosting large language models with synthetic preference data. arXiv preprint arXiv:2410.06961, 2024. [8] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. [9] Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, et al. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data. arXiv preprint arXiv:2410.18558, 2024. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 10 [11] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. [12] hiyouga. Mathruler. https://github.com/hiyouga/MathRuler, 2025. [13] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. [14] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. [15] Xin Huang, Jing Bai, Yeqing Shen, Jia Wang, Zheng Ge, and Osamu Yoshie. Seeking the right question: Towards high-quality visual instruction generation. openreview, 2024. [16] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [17] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [18] Sicong Leng, Jing Wang, Jiaxi Li, Hao Zhang, Zhiqiang Hu, Boqiang Zhang, Hang Zhang, Yuming Jiang, Xin Li, Deli Zhao, Fan Wang, Yu Rong, Aixin Sun, and Shijian Lu. Mmr1: Advancing the frontiers of multimodal reasoning. https://github.com/LengSicong/MMR1, 2025. [19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [20] Zhenwen Liang, Ye Liu, Tong Niu, Xiangliang Zhang, Yingbo Zhou, and Semih Yavuz. Improving llm reasoning through scaling inference computation with collaborative verification. arXiv preprint arXiv:2410.05318, 2024. [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. [22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [23] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Intergps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. [24] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [25] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. [26] Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. [27] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [28] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mmeureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [29] OpenAI. Openai gpt-4-5-system-card-2272025.pdf. gpt-4.5 system card, 2025. URL https://cdn.openai.com/ 11 [30] Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024. [31] Yi Peng, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, Li Ge, et al. Skywork r1v: Pioneering multimodal reasoning with chain-of-thought. arXiv preprint arXiv:2504.05599, 2025. [32] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. [33] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. [34] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [35] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. [36] Yucheng Shi, Quanzheng Li, Jin Sun, Xiang Li, and Ninghao Liu. Enhancing cognition and explainability of multimodal foundation models with self-synthesized data. arXiv preprint arXiv:2502.14044, 2025. [37] Wentao Tan, Qiong Cao, Yibing Zhan, Chao Xue, and Changxing Ding. Beyond human data: Aligning multimodal large language models by iterative self-evolution. In Proceedings of the AAAI Conference on Artificial Intelligence, 2025. [38] Zhiquan Tan, Lai Wei, Jindong Wang, Xing Xie, and Weiran Huang. Can understand what create? self-knowledge evaluation of large language models. arXiv preprint arXiv:2406.06140, 2024. [39] Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. Advances in Neural Information Processing Systems, 37:78217846, 2024. [40] Fouad Trad and Ali Chehab. To ensemble or not: Assessing majority voting strategies for phishing detection with large language models. In International Conference on Intelligent Systems and Pattern Recognition, pages 158173. Springer, 2024. [41] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2025. [42] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [43] Xiaoxuan Wang, Yihe Deng, Mingyu Derek Ma, and Wei Wang. Entropy-based adaptive weighting for self-training. arXiv preprint arXiv:2503.23913, 2025. [44] Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025. [45] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [46] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. [47] Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. Instructiongpt-4: 200-instruction paradigm for fine-tuning minigpt-4. arXiv preprint arXiv:2308.12067, 2023. [48] Lai Wei, Zhiquan Tan, Chenghai Li, Jindong Wang, and Weiran Huang. Diff-erank: novel rank-based metric for evaluating large language models. arXiv preprint arXiv:2401.17139, 2024. 12 [49] Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-metajudge. arXiv preprint arXiv:2407.19594, 2024. [50] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [51] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [52] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [53] Zheng Yaowei, Lu Junting, Wang Shenzhi, Feng Zhangchi, Kuang Dongdong, and Xiong Yuwen. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025. [54] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. [55] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. [56] Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812, 2025. [57] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [58] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, et al. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739, 2024. [59] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. [60] Wenqi Zhang, Zhenglin Cheng, Yuanyu He, Mengna Wang, Yongliang Shen, Zeqi Tan, Guiyang Hou, Mingqian He, Yanna Ma, Weiming Lu, et al. Multimodal self-instruct: Synthetic abstract image and visual reasoning instruction using language model. arXiv preprint arXiv:2407.07053, 2024. [61] Xiaoying Zhang, Baolin Peng, Jianfeng Gao, and Helen Meng. Toward self-learning end-to-end taskoriented dialog systems. arXiv preprint arXiv:2201.06849, 2022. [62] Xiaoying Zhang, Da Peng, Yipeng Zhang, Zonghao Guo, Chengyue Wu, Chi Chen, Wei Ke, Helen Meng, and Maosong Sun. Will pre-training ever end? first step toward next-generation foundation mllms via self-improving systematic cognition. arXiv e-prints, pages arXiv2503, 2025. [63] Yu Zhang, Kehai Chen, Xuefeng Bai, Zhao Kang, Quanjiang Guo, and Min Zhang. Question-guided knowledge graph re-scoring and injection for knowledge graph question answering. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 89728985, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.524. URL https://aclanthology.org/2024. findings-emnlp.524/. [64] Henry Hengyuan Zhao, Pan Zhou, and Mike Zheng Shou. Genixer: Empowering multimodal large language model as powerful data generator. In European Conference on Computer Vision, pages 129147. Springer, 2024. [65] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023. [66] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros \"aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. 13 [67] Shijie Zhou, Ruiyi Zhang, Yufan Zhou, and Changyou Chen. high-quality text-rich image instruction tuning dataset via hybrid instruction generation. arXiv preprint arXiv:2412.16364, 2024. [68] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. arXiv preprint arXiv:2405.14622, 2024. [69] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [70] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. arXiv preprint arXiv:2408.08640, 2024. [71] Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "A Implementation Details",
            "content": "We provide the implementation details of our experiments as follows. A.1 Baselines Here, we explain how we implement different baseline methods in comparison. LMSI [14] employs the majority-voted response as the target for supervised fine-tuning (SFT). For each question, we generate multiple responses and retain the ones that lead to the majority answer for training. SRLM [49] studies Self-Rewarding Language Models, where the model itself is used via LLM-asa-Judge prompting to provide its own rewards during training. In particular, for each question, we generate multiple candidate responses and use the prompt provided in the original paper to have the MLLM score its own outputs. Among the responses, the one with the highest score is selected as the positive example, and the one with the lowest score as the negative example. These pairs are then used to construct preference datasets for Direct Preference Optimization (DPO) [33]. Genixer [64] introduces comprehensive data generation pipeline consisting of four key steps: (i) instruction data collection, (ii) instruction template design, (iii) empowering MLLMs, and (iv) data generation and filtering. To adapt Genixer in our setting, we remove the first two steps because we already have instruction data. After that, we use Qwen2.5-VL as the backbone model to self-generate responses 16 times per question for each dataset. In the filtering stage, we use the prompt to let the model self-judge the responses following Genixer: Here is question-answer pair. Is {Q : Xq, : Xa} true for this image? Please answer this question with Yes or No. In addition, Genixer calculates the probability of predicting the Yes rather than prompt the model to directly output Yes or No as the filtering label: (YrXI , Xq, Xa) = (cid:89) p(yiXI , Xq, Xa,<i), (3) where Yr is the predicted judge, XI is the image, Xq is the question, Xa is the self-generated response, and is the length the total predicted judge. Then, it proposes threshold λ to control the filtering in the following manner: Sn = True, False, False, if Yr = Yes and (Y if Yr = Yes and (Y if Yr = No ) > λ ) λ , (4) where Sn is the filter label representing keeping or removing the current sample. (Y probability of the result Yes of n-th candidates. λ is set to 0.7 following the paper. ) denotes the STIC [5] proposes two-stage self-training algorithm focusing on the image comprehension capability of the MLLMs. In Stage 1, the base MLLM self-constructs its preference dataset for image description using well-designed prompts, poorly-designed prompts, and distorted images with diffusion noise. In Stage 2, small portion of the previously used SFT data is recycled and infused with model-generated image descriptions to further fine-tune the base MLLM. In particular, since Qwen2.5-VL does not open-source the SFT data, we opt to use the models self-generated responses sampled from different datasets to represent the previously used SFT data. A.2 Benchmarks We provide some details about the benchmarks we use to evaluate the models reasoning ability. MathVision [41] is challenging benchmark containing 3040 mathematical problems with visual contexts from real-world math competitions across 12 grades. It covers 16 subjects over 5 difficulty 15 levels, including specialized topics like Analytic Geometry, Combinatorial Geometry, and Topology. MathVista [24] is comprehensive benchmark for evaluating mathematical reasoning in visual contexts. It contains 1000 questions featuring diverse problem types including geometry, charts, and tables. MathVerse [57] is an all-around visual math benchmark designed for an equitable and indepth evaluation of MLLMs. The test set contains 3940 multi-subject math problems with diagrams from publicly available sources, focusing on Plane Geometry and Solid Geometry. We-Math [32] meticulously collect and categorize 1740 visual math problems in the test set, spanning 67 hierarchical knowledge concepts and 5 layers of knowledge granularity. For all benchmarks, we prompt the models to place their final answers within designated box format. We then employ Qwen2.5-32B-Instruct [51] to evaluate answer correctness by comparing the extracted responses with ground truth answers, which often contain complex mathematical expressions. Note that our reported benchmark scores may differ from those in the original papers due to variations in evaluation protocols. A.3 Standard Training Datasets In our experiments, we use three standard training datasets for multi-modal reasoning: Geometry3K [23], GeoQA [3], and MMR1 [18]. Geometry3K consists of 2.1K multiple-choice questions in the training set, covering wide range of geometric shapes. GeoQA includes 8K fill-in-the-blank questions sourced from the larger Geo170K dataset [8]. MMR1 consists of 7,000 samples and includes both multiple-choice questions and fill-in-the-blank questions. These samples cover range of tasks, including understanding charts and geometric reasoning."
        },
        {
            "title": "B Compute Resources",
            "content": "We conduct our experiments using NVIDIA H100-80G and A800-40G GPUs. The experimental time using 8 A800 for training Qwen2.5-VL-7B [1] on the Geometry3K [23] dataset using GRPO is around 10 hours."
        }
    ],
    "affiliations": [
        "Lehigh University",
        "School of Computer Science, Shanghai Jiao Tong University",
        "Shanghai Innovation Institute",
        "Zhongguancun Academy"
    ]
}