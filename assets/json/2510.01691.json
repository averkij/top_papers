{
    "paper_title": "MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs",
    "authors": [
        "Jiyao Liu",
        "Jinjie Wei",
        "Wanying Qu",
        "Chenglong Ma",
        "Junzhi Ning",
        "Yunheng Li",
        "Ying Chen",
        "Xinzhe Luo",
        "Pengcheng Chen",
        "Xin Gao",
        "Ming Hu",
        "Huihui Xu",
        "Xin Wang",
        "Shujian Gao",
        "Dingkang Yang",
        "Zhongying Deng",
        "Jin Ye",
        "Lihao Liu",
        "Junjun He",
        "Ningsheng Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Medical Image Quality Assessment (IQA) serves as the first-mile safety gate for clinical AI, yet existing approaches remain constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation. To address this gap, we introduce MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning paradigm for language-based evaluation of medical image quality with Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary tasks: (1) MedQ-Perception, which probes low-level perceptual capability via human-curated questions on fundamental visual attributes; and (2) MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks, aligning model evaluation with human-like reasoning on image quality. The benchmark spans five imaging modalities and over forty quality attributes, totaling 2,600 perceptual queries and 708 reasoning assessments, covering diverse image sources including authentic clinical acquisitions, images with simulated degradations via physics-based reconstructions, and AI-generated images. To evaluate reasoning ability, we propose a multi-dimensional judging protocol that assesses model outputs along four complementary axes. We further conduct rigorous human-AI alignment validation by comparing LLM-based judgement with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates that models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use. These findings highlight the need for targeted optimization of MLLMs in medical IQA. We hope that MedQ-Bench will catalyze further exploration and unlock the untapped potential of MLLMs for medical image quality evaluation."
        },
        {
            "title": "Start",
            "content": "MEDQ-BENCH: EVALUATING AND EXPLORING MEDICAL IMAGE QUALITY ASSESSMENT ABILITIES IN MLLMS Jiyao Liu1 Jinjie Wei1, Wanying Qu1, Chenglong Ma1,2, Junzhi Ning2, Yunheng Li1, Ying Chen2, Xinzhe Luo3, Pengcheng Chen2, Xin Gao1, Ming Hu2, Huihui Xu2, Xin Wang2, Shujian Gao1, Dingkang Yang1, Zhongying Deng4, Jin Ye2, Lihao Liu2, Junjun He2, Ningsheng Xu1 1Fudan University, 2Shanghai Artificial Intelligence Laboratory, 3Imperial College London, 4University of Cambridge Equal contribution. Corresponding author. Project Page: https://github.com/liujiyaoFDU/MedQBench."
        },
        {
            "title": "ABSTRACT",
            "content": "Medical Image Quality Assessment (IQA) serves as the first-mile safety gate for clinical AI, yet existing approaches remain constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation. To address this gap, we introduce MedQ-Bench, comprehensive benchmark that establishes perceptionreasoning paradigm for language-based evaluation of medical image quality with Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary tasks: (1) MedQ-Perception, which probes low-level perceptual capability via humancurated questions on fundamental visual attributes; and (2) MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks, aligning model evaluation with human-like reasoning on image quality. The benchmark spans 5 imaging modalities and over 40 quality attributes, totaling 2,600 perceptual queries and 708 reasoning assessments, covering diverse image sources including authentic clinical acquisitions, images with simulated degradations via physicsbased reconstructions, and AI-generated images. To evaluate reasoning ability, we propose multi-dimensional judging protocol that assesses model outputs along four complementary axes. We further conduct rigorous humanAI alignment validation by comparing LLM-based judgement with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates that models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use. These findings highlight the need for targeted optimization of MLLMs in medical IQA. We hope that MedQ-Bench will catalyze further exploration and unlock the untapped potential of MLLMs for medical image quality evaluation."
        },
        {
            "title": "INTRODUCTION",
            "content": "5 2 0 2 2 ] . [ 1 1 9 6 1 0 . 0 1 5 2 : r Figure 1: MedQ-Bench overview, evaluating MLLMs abilities in medical image quality assessment with: (1) Comprehensive coverage: 3,308 samples across 5 modalities with 40+ degradation types. (2) Multi-faceted evaluation: perception-reasoning paradigm. 1 Medical Image Quality Assessment (IQA) determines whether imaging data can be reliably used for subsequent diagnostic interpretation and clinical decision-making (Lamard et al., 2024). In clinical practice, multiple visual quality attributes of medical images directly influence diagnostic accuracy and patient safety (Rajpurkar et al., 2024), including sharpness, contrast adequacy, noise characteristics, artifact severity, etc. When these quality attributes are compromised, the resulting suboptimal images can lead to diagnostic errors, missed pathologies, or erroneous clinical interpretations, potentially causing severe patient harm and undermining the integrity of clinical decision-making processes (Blackmore et al., 2011). IQA approaches predomiCurrent medical nantly produce scalar scores using (1) noreference methods (Xun et al., 2025; Herath et al., 2025), which infer perceptual quality of an image through statistical feature extraction without reference, and (2) full-reference similarity metrics such as PSNR, SSIM (Hore & Ziou, 2010), and LPIPS (Zhang et al., 2018). These methods provide standardized evaluation metrics and enable automated IQA. However, they exhibit the following fundamental deficiencies. (1) Poor generalization (Herath et al., 2025). Medical image quality is influenced by complex and heterogeneous factors, including noise characteristics, contrast adequacy, artifact severity, distortion patterns, and modalityspecific degradations across diverse imaging modalities such as magnetic resonance imaging (MRI), computed tomography (CT), fundus photography, histopathology, and endoscopy. Yet, existing methods typically rely on simple regression models (Su et al., 2023) or handcrafted statistical indices (Dohmen et al., 2024), which are ill-suited to capture this breadth of quality-affecting factors. As result, they tend to generalize poorly to unseen distortions, new modalities or scanners, and different imaging protocols. (2) Lack of human-like reasoning process for result interpretation. Most methods produce scalar IQA scores, which do not fully reflect the causes of image quality degradation and may be unreasonable in certain cases. For instance, as illustrated in Figure 2, when evaluating two medical images, clinicians typically identify specific degradations first (e.g., metal streak artifacts in Image #1 vs. reconstruction blurring in Image #2) before assessing their clinical impact. Despite metal artifacts, Image #1 preserves clear anatomical boundaries and sharp tissue visualization, while Image #2 suffers from texture loss and unnatural intensity variations. Consequently, Image #1 provides better clinical quality. However, traditional score-based metrics often favor the smoother Image #2, contradicting human judgment. Such perceptual reasoning requires understanding the clinical significance of quality factors, which current automated approaches cannot effectively capture. Figure 2: Comparison of Reasoning IQA with score-based IQA. Unlike purely numerical scores, Reasoning IQA identifies distortion types and their relative impact, yielding results more consistent with human judgment. Recent advances in multimodal large language models (MLLMs) have shown promising capabilities in medical visual reasoning tasks (OpenAI, 2023; Liu et al., 2024; Dai et al., 2024; Saab et al., 2024; Su et al., 2025). Theoretically, MLLMs could potentially address existing IQA challenges by decomposing quality assessment into interconnected subtasks: degradation identification, severity quantification, clinical impact analysis, and comparative reasoning. Unlike traditional approaches that yield opaque scores, MLLM-based assessment can provide explicit chains of thought (Wu et al., 2024a; You et al., 2024), offering interpretable and clinically meaningful evaluations. However, critical questions remain unanswered about MLLMs actual capabilities in medical IQA: Can they truly generalize to the fine-grained, diverse, and complex quality factors spanning different imaging modalities? Do they possess genuine reasoning abilities to understand the clinical significance of various degradations? Existing MLLM evaluation frameworks focus mainly on natural images (Wu et al., 2024b) or high-level medical semantics (Ye et al., 2024), lacking systematic benchmarks that assess quality-related perceptual and reasoning skills across diverse medical modalities. This absence of specialized benchmarks has been major barrier to developing and validating effective frameworks. Figure 3: Examples of question types in MedQ-Bench, covering MCQA perception tasks (Yes-No / What / How), open-ended reasoning, and pair/multi-image comparison. To bridge the gap between existing medical IQA methods, we propose novel perceptionreasoning paradigm. This paradigm mirrors clinicians cognitive workflow: first perceiving quality-related attributes in images, assessing their severity, and evaluating their potential impact on clinical diagnosis, and then making overall quality judgments through logical reasoning. Building on this paradigm, we introduce MedQ-Bench, the first comprehensive benchmark that systematically evaluates the medical IQA capabilities of MLLMs. Our primary contributions are as follows: Pioneering evaluation framework for medical image quality assessment. MedQ-Bench introduces systematic evaluation methodology that comprehensively assesses both quality-based perceptual and reasoning capabilities for MLLMs. The framework extends beyond traditional IQA scoring to incorporate quality-related perception assessment, fine-grained comparative analysis, and quality-aware reasoning evaluation. The protocol supports both no-reference and fullreference paradigms, enabling systematic assessment ranging from coarse-grained to fine-grained perceptual discrimination tasks. Multi-dimensional judging protocol with humanAI alignment validation. To evaluate reasoning ability, we design multi-dimensional judging protocol that scores model outputs along four complementary axes. We further perform rigorous humanAI alignment validation by comparing our LLM-based evaluations with radiologists, demonstrating the reliability of the proposed evaluation framework. Comprehensive, clinically representative, multi-source dataset. Covering 5 imaging modalities and 40+ quality attributes, MedQ-Bench blends authentic clinical images, simulated degraded images via physics-based reconstruction, and AI-generated images to encompass diverse real-world and controlled scenarios. This comprehensive dataset enables robust evaluation across both realistic clinical conditions and challenging scenarios. Comprehensive empirical analysis. We conduct extensive evaluations of state-of-the-art MLLMs, spanning open-source and commercial systems, both general-purpose and medicalspecialized. Our systematic analysis reveals significant performance gaps in modality-specific perception capabilities, underscoring the need for targeted improvements for clinical readiness."
        },
        {
            "title": "2 CONSTRUCTING THE MEDQ-BENCH",
            "content": "2.1 BENCHMARK SCOPE AND MODALITIES Clinical image quality is fundamental to diagnostic reliability, yet existing evaluation methods rely primarily on score-based metrics that overlook the comprehensive assessment of image quality perception and reasoning capabilities. MedQ-Bench is specifically designed to systematically evaluate the visual quality perception and reasoning capabilities of multimodal large language models (MLLMs) within the medical imaging domain. Let = {M1, M2, . . . , M5} represent the set of five medical imaging modalities, where each modality Mi is associated with distinct set of quality attributes Ai = {ai,1, ai,2, . . . , ai,k}. The quality assessment task can be formulated as learning mapping function : that takes an image and question as input and produces response R. To capture the diversity and complexity of real-world clinical imaging, MedQ-Bench encompasses five representative modalities: Magnetic Resonance Imaging (MRI), Computed Tomography (CT), endoscopy, histopathology imaging, and fundus photography. Let Di = {di,1, di,2, . . . , di,n} denote the set of degradation types specific to modality Mi. Each modality exhibits distinct degradation characteristics due to its physical acquisition principles, where degradations can be modeled as transformations Td : that modify the original image based on degradation type Di. For instance, MRI is particularly susceptible to motion and magnetic susceptibility artifacts, and CT is prone to low-dose noise and metal-induced streak artifacts. This multi-modality design ensures that the benchmark reflects the broad spectrum of perceptual challenges encountered in practice. For each modality, MedQ-Bench incorporates images from three complementary sources: authentic clinical images containing naturally occurring artifacts; synthetically degraded images that replicate modality-specific distortions in controlled manner; and AI-generated or reconstructed images produced by enhancement, translation, or reconstruction models, which may introduce hallucinations or subtle structural inconsistencies. Let = {Sreal, Ssynth, SAI} denote the three image sources, where each source Sk contributes subset of images with specific degradation characteristics Dk (cid:83) Di. This tri-source strategy enables the benchmark to cover both naturally occurring degradations and algorithm-induced artifacts, ensuring balanced evaluation of MLLM robustness across real-world and algorithmic distortion scenarios. 2.2 BENCHMARK ON IQA PERCEPTION ABILITY Before evaluating sophisticated reasoning capabilities, it is essential to establish whether MLLMs possess fundamental perceptual abilities to recognize basic image quality attributes. 2.2.1 QUESTION TYPES The perception-focused MCQA setting evaluates direct visual perception using single-image prompts, without requiring domain-specific diagnostic reasoning. These tasks represent the most basic level of quality assessment capability, asking models to simply identify what they see rather than explain why they see it. For each image, three canonical subtypes of questions are included: (1) Yes-or-No: Binary classification tasks where RYN = {0, 1} and the model predicts ˆy = arg maxy{0,1} (y I, q). Examples include Is this image clear? or Does this image contain artifacts? (2) What: Multi-class identification tasks where RWhat = {c1, c2, . . . , cK} represents possible degradation types, and the model selects ˆc = arg maxcRWhat (c I, q). These tasks ask models to identify specific types of artifacts or degradations present in the image. (3) How: Severity assessment tasks where RHow = {s1, s2, . . . , sL} represents severity levels, and the model predicts ˆs = arg maxsRHow (s I, q). These tasks evaluate the models ability to assess the degree or intensity of observed quality issues. 2.2.2 QUADRANTS FOR LOW-LEVEL VISUAL CONCERNS Axis 1: No Degradation vs Degradation Severity Levels. The primary axis differentiates medical images based on their quality degradation status: 1) No Degradation refers to medical images that maintain optimal quality standards without artifacts or distortions, and 2) degradation with Severity 4 Levels encompasses images with varying degrees of quality issues, further subdivided into mild Degradation and severe Degradation. Axis 2: General Medical Questions vs Modality-specific Questions. Quality perception in medical imaging intertwines with modality-specific technical characteristics. For instance, motion artifacts manifest differently in MRI versus CT scans. We curate modality-specific questions that require understanding unique technical characteristics of specific imaging modalities (e.g., Does this MRI show susceptibility artifacts?), while general medical questions focus on universal quality concepts applicable across modalities (e.g., Is this image clear?). This distinction evaluates both fundamental quality perception and specialized modality knowledge. 2.3 BENCHMARK ON IQA REASONING ABILITY 2.3.1 NO-REFERENCE REASONING TASKS. While MCQA constrains answers to predefined choices, reasoning tasks assess models ability to autonomously describe and explain quality-related observations in natural language. These tasks require generating comprehensive responses w1:T = {w1, w2, . . . , wT } that systematically detail multiple aspects of image quality assessment: (1) modality and anatomical region identification; (2) specific quality degradation characterization including type and severity; (3) technical attribution of underlying causes; (4) assessment of diagnostic impact and clinical implications; and (5) definitive quality judgment with good/usable/reject recommendation. The reasoning tasks evaluate whether models can perform structured quality analysis that mirrors expert clinical assessment, moving beyond simple classification to demonstrate understanding of the relationship between technical image properties, degradation mechanisms, and clinical utility. 2.3.2 COMPARISON REASONING TASKS. Many clinical workflows require comparative quality assessment between two versions of the same study, such as original vs. reconstructed or outputs from competing reconstruction algorithms. For image pairs (IA, IB), the comparative task seeks to determine preference (IA IB) based on overall quality assessment. Models must identify which image exhibits higher diagnostic quality and provide detailed explanations for their judgment, such as explaining why one reconstruction algorithm preserves anatomical detail better than another. Comparative tasks are further categorized by the perceptual gap between images. 1) Coarse-grained comparisons involve clearly visible quality differences, making them relatively straightforward for both humans and models. 2) Fine-grained comparisons involve subtle differences in noise patterns, contrast, or structure fidelity, requiring heightened sensitivity to nuanced quality cues that may only be apparent upon careful inspection. This design enables separate evaluation of basic discrimination ability and advanced perceptual subtlety that approaches expert-level assessment sensitivity. 2.3.3 EVALUATION METRICS Multi-dimensional judging protocol The reasoning tasks require more nuanced evaluation approaches due to their subjective nature and the complexity of natural language responses. Recent studies have demonstrated GPT-4o to be reliable evaluation tool for complex reasoning tasks. We assess model outputs across four complementary dimensions, each scored on I[k KO] meadiscrete scale {0, 1, 2}: (1) Completeness. C(O, R) = 1 sures the coverage of key visual information from the reference description R, where KR and KO represent the sets of key visual information in reference and output respectively. Higher (2) Precisescores indicate more comprehensive description of observable quality issues. I[contradict(k, R)] quantifies consistency between model ness. (O, R) = 1 1 (3) Consistency. S(O, R) = output and reference by penalizing semantic contradictions. fconsistency(reasoning(O), conclusion(O), R) evaluates the internal logical consistency between the reasoning path reasoning(O) and the final quality judgment conclusion(O), where fconsistency re- (4) Quality Accuracy. Q(O, R) = turns score based on logical coherence assessment. I[comparison(O) = comparison(R)] assesses whether the final quality comparison judgment correctly identifies which image has higher quality, matching the reference assessment. This binary metric focuses on the correctness of the ultimate quality decision. kKR kKO KR KO (cid:80) (cid:80) 5 HumanAI Alignment Validation To ensure the reliability and validity of our automated evaluation, we conducted rigorous alignment validation between GPT-4o judgments and expert assessments. total of 200 cases were randomly sampled from the development dataset and independently evaluated by three board-certified medical imaging specialists under double-blinded protocol. For humanAI alignment, we employed quadratic weighted Cohens kappa (Cohen, 1968) for ordinal ratings: κw = 1 , (1) (cid:80) (cid:80) i,j wijOij i,j wijEij where Oij is the observed agreement matrix, Eij the expected agreement matrix, and wij = (ij)2 (k1)2 the quadratic weights penalizing larger disagreements more severely. We further conducted iterative prompt refinement to maximize concordance between GPT-4o and expert consensus. Final alignment results are reported in Section 3.4."
        },
        {
            "title": "3 RESULTS",
            "content": "To investigate MLLMs image quality perception ability, we present comprehensive evaluation of MedQ-Bench across 14 up-todate popular MLLMs under zero-shot settings. We evaluate these 14 multimodal large language models across three categories: opensource MLLMs (Qwen2.5-VL-Instruct variants (Wang et al., 2024a), InternVL3 models (Chen et al., 2024b)), medical-specialized MLLMs (BiMediX2 (Peng et al., 2024), Lingshu (Wang et al., 2024b), MedGemma (Saab et al., 2024)), and commercial systems (GPT5 (OpenAI, 2024b), GPT-4o (OpenAI, 2024a), Gemini-2.5-Pro (Reid et al., 2024), Grok4 (xAI Team, 2024), Claude-4-Sonnet (Anthropic, 2024), Mistral-Medium-3 (Jiang et al., 2023)). 3.1 FINDINGS ON PERCEPTION Figure 4: Overall Performance Results Sub-categories Model (variant) random guess Non-experts Human experts Qwen2.5-VL-Instruct (7B) Qwen2.5-VL-Instruct (32B) InternVL3 (8B) InternVL3 (38B) Qwen2.5-VL-Instruct (72B) BiMediX2 (8B) Lingshu (32B) MedGemma (27B) Mistral-Medium-3 Claude-4-Sonnet Gemini-2.5-Pro Grok-4 GPT-4o GPT-5 Perception Reasoning Yes-or-No What How Overall Comp. Prec. Cons. Qual. Overall 50.00% 67.50% 88.50% 57.89% 67.38% 72.04% 69.71% 78.67% 44.98% 50.36% 67.03% 65.95% 71.51% 75.13% 73.30% 78.48% 82.26% 28.48% 33.30% 57.50% 57.50% 77.50% 77.50% 48.45% 54.40% 43.02% 58.69% 47.67% 52.97% 57.36% 52.97% 42.25% 56.44% 27.52% 27.81% 50.39% 51.74% 48.06% 50.72% 37.94% 62.50% 82.50% 54.71% 59.31% 60.08% 61.00% 63.14% 35.10% 50.88% 57.16% 57.70% 48.84% 52.97% 60.23% 46.51% 54.60% 55.02% 50.54% 61.88% 48.84% 59.10% 63.14% 49.64% 57.32% 64.79% 60.47% 58.28% 68.97% - - 0.715 1.077 0.928 0.964 0.905 0.376 0.624 0.742 0.923 0.742 0.878 0.982 1.009 1. - - 0.670 0.928 0.878 0.824 0.860 0.394 0.697 0.471 0.729 0.633 0.891 0.846 1.027 1.118 - - 1.855 1.977 1.858 1.860 1. 0.281 1.932 1.579 1.566 1.778 1.688 1.801 1.878 1.837 - - 1.127 1.290 1.317 1.317 1.321 0.670 1.059 1.262 1.339 1.376 1.561 1.389 1.407 1. - - 4.367 5.272 4.983 4.965 4.982 1.721 4.312 4.054 4.557 4.529 5.018 5.017 5.321 5.679 Table 1: Performance of different models on the MCQA perception and reasoning tasks. First place in each column is bolded; second and third places are underlined. Random guess / Non-experts / Human experts are excluded from ranking. To ensure rigorous and unbiased evaluation, the MedQ-Perception is equally divided into dev (Table 6, for prompt refinement) and test (Table 1, for final evaluation) subsets. Conclusion 1. Clear performance hierarchy emerges across model categories: Our analysis reveals that most MLLMs perform above random guessing across all sub-tasks, indicating promising potential for domain generalization. The results demonstrate clear performance hierarchy: closed-source frontier models achieve the highest scores, with GPT-5 leading at 68.97% on the test set. Among open-source models, Qwen2.5-VL-Instruct (72B) achieves the best performance at 63.14%, outperforming most commercial models, while the best medical-specialized models underperform expectations, with MedGemma (27B) achieving only 57.16%. More details are in Section A.5.1. Insufficiency 1. Substantial human-AI performance gap remains: Another key finding emerges from our comparison with human performance, where we include both human experts (medical imaging technicians and medical imaging PhDs) and non-experts as reference points. The best AI model (GPT-5) significantly underperforms human experts (68.97% vs. 82.50%, gap of 13.53%), yet outperforms non-experts by 6.47%. Given that these models have not undergone specialized training for medical image quality assessment, this suggests substantial potential for improvement in these MLLMs through further fine-tuning. Figure 5: Performance analysis of MLLMs across different evaluation dimensions. (a) Different degradation level performance . (b) General vs modality-specific question. Insufficiency 2. The LVLMs are not robust among different perceptual types: Task-specific analysis reveals distinct patterns across different evaluation dimensions. Performance analysis across different degradation levels (Figure 5(a)) demonstrates that mild degradation represents the most challenging detection scenario, with average accuracy dropping to 56% compared to 72% for no degradation and 67% for severe degradation. This indicates that subtle quality issues are harder to identify than obvious artifacts. Top-performing models like GPT-5 demonstrate degree of consistency in performance across degradation levels. We further investigate the difference between general and modality-specific medical questions. As shown in Figure 5(b), most models perform better on general questions than on modality-specific tasks, whereas GPT-5 demonstrates the most balanced performance across question types. This suggests that robust medical image quality assessment requires specialized understanding of modality-specific visual features. 3.2 FINDINGS ON NO-REFERENCE REASONING Conclusion 2. Limited low-level visual reasoning capabilities across all models: For noreference reasoning capabilities  (Table 1)  , GPT-5 still demonstrates the best performance, particularly excelling in the relevance dimension. However, even the most advanced MLLMs fail to achieve excellent scores in completeness and preciseness, with the highest scores being only 1.293/2.0 for 7 Model Comp. Prec. Cons. Qual. Overall Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B InternVL3-8B InternVL3-38B BiMediX2-8B MedGemma-27B Lingshu-32B Mistral-Medium-3 Claude-4-Sonnet Gemini-2.5-Pro Grok-4 GPT-4o GPT-5 0.714 0.692 0.737 0.985 1.075 0.474 0.684 0.729 0.872 0.857 1.053 1.150 1.105 1.293 0.902 0.752 0.977 1.278 1. 0.549 0.692 1.015 1.203 1.083 1.233 1.233 1.414 1.556 1.316 1.895 1.233 1.797 1.571 0.639 1.128 1.586 1.827 1.910 1.774 1.820 1.632 1.925 1.143 0.962 1.113 1.474 1. 0.511 1.000 1.323 1.338 1.481 1.534 1.459 1.562 1.564 4.075 4.301 4.060 5.534 5.143 2.173 3.504 4.653 5.240 5.331 5.594 5.662 5.713 6.338 Figure 6: Comparative reasoning performance analysis. Left: Detailed performance scores across three evaluation dimensions for all models. Right: Visual comparison of overall performance patterns across model categories. In general, most models only reach an acceptable completeness and 1.556/2.0 for preciseness. baseline level. Current MLLM models possess relatively limited and elementary low-level visual reasoning abilities, struggling to provide complete and accurate descriptions of low-level visual information. The consistently high consistency scores indicate that most MLLMs can follow abstract instructions reasonably well, suggesting that the main bottleneck for improving MLLM descriptive capabilities lies in the perception of low-level attributes rather than instruction following. 3.3 FINDINGS ON COMPARISON REASONING Insufficiency 3. Paired comparison reveals fundamental limitations in fine-grained analysis: Paired image comparison tasks pose the greatest challenge to current multimodal large language models (MLLMs), requiring models to perform fine-grained quality comparisons between similar images that may only differ by varying degrees. We evaluate model performance across two difficulty levels: fine-grained differences and coarse-grained differences. Figure 6 (right) presents detailed performance analysis across different difficulty levels, with more complete tabular results available in Table 9 in the appendix. Overall, most models perform better under coarse-grained differences, while few models, such as Grok-4 and Qwen2.5-VL-7B/32B, perform better under finegrained differences but lose performance on coarse-grained tasks. Among them, GPT-5 achieved the highest overall score, while medical-specialized models such as BiMediX2 showed notably insufficient performance. 3.4 HUMANAI ALIGNMENT VALIDATION Strong human-AI alignment validates our evaluation framework: To validate the reliability of our automated evaluation approach, we conducted comprehensive human-AI alignment study comparing human expert assessments with GPT-4o automated scoring. We evaluated 200 randomly sampled image quality assessments across three key dimensions: completeness, preciseness, and consistency. The confusion matrices in the appendix (Figure 13) demonstrate strong alignment between human expert scores and GPT-4o automated evaluation across all three dimensions, with consistently high accuracy rates: 83.3% for completeness, 87.0% for preciseness, and 90.5% for consistency, with all individual class recall rates exceeding 80%. These results validate that our automated quality assessment system achieves strong alignment with human expert judgment across all evaluation dimensions, with high accuracy rates demonstrating that our evaluation framework can serve as reliable substitute for human evaluation. Beyond accuracy, we further assessed inter-rater agreement using quadratic weighted Cohens κw  (Table 10)  , achieving consistently high values (0.7740.985) that confirm substantial agreement beyond chance and validate our framework as reliable surrogate for large-scale human evaluation."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Medical Multimodal Large Language Models and Benchmarks. Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content through natural language. General-purpose models like GPT-4V (OpenAI, 2023), LLaVA (Liu et al., 2024), and Qwen-VL (Wang et al., 2024a) have shown strong performance across diverse vision-language tasks. To address healthcare-specific requirements, medical-specialized variants such as (Wang et al., 2024b; Saab et al., 2024; Peng et al., 2024; Su et al., 2025; Xu et al., 2025) have emerged through domain-targeted pretraining and alignment. Recent medical benchmarks have been developed to evaluate these models systematically, including (Ye et al., 2024), which provides comprehensive multimodal evaluation for general medical AI. However, existing medical benchmarks focus primarily on high-level diagnostic tasks rather than low-level perceptual quality assessment (Chen et al., 2024a). Score-based Image Quality Assessment. Traditional image quality assessment methods produce numerical scores to quantify image quality, categorized into No-Reference (NR), Full-Reference (FR), and Reduced-Reference approaches. NR methods like BRISQUE (Mittal et al., 2012), NIQE (Zhang et al., 2015), and deep learning approaches including CNNIQA (Kang et al., 2014) and MUSIQ (Ke et al., 2021) assess quality without reference images. FR methods compare against pristine references using metrics like PSNR, SSIM (Wang et al., 2004), VIF (Sheikh & Bovik, 2006), and learned perceptual metrics like LPIPS (Zhang et al., 2018). Recent advances include transformer-based approaches like TReS (Golestaneh et al., 2022) and quality-aware pretraining methods. However, these methods yield only scalar scores, offering limited interpretability regarding specific quality factors, and such technical measures often show weak alignment with clinical workflows (Zhang et al., 2024; Blackmore et al., 2011). MLLM-based Image Quality Assessment. Recent advances have introduced multimodal language models for image quality assessment (IQA), which enable more interpretable and reasoningbased evaluation. For example, Q-Instruct (Wu et al., 2024a) and DepictQA (You et al., 2024) generate natural language descriptions of quality factors, while Q-Bench (Wu et al., 2024b) offers systematic framework for evaluating low-level vision tasks. Building on this line, IQAGPT (Chen et al., 2023) integrates vision-language models with ChatGPT for CT image quality assessment, showing the feasibility of producing both quality scores and textual reports. However, its scope is limited to CT images and remains focused on score prediction rather than comprehensive reasoning. Likewise, Ultrasound-QBench (Miao et al., 2025) provides evaluation for ultrasound imaging but restricts tasks to classification and scoring within single modality."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced MedQ-Bench, the first benchmark to systematically evaluate medical image quality assessment (IQA) capabilities of multimodal large language models through perceptionreasoning paradigm. Unlike conventional score-based metrics, MedQ-Bench jointly assesses quality-related perception and reasoning across five imaging modalities and more than forty degradation types via three complementary tracks: perception tasks, no-reference reasoning, and paired comparison reasoning. Our large-scale zero-shot evaluation of 14 state-of-the-art MLLMs, including open-source, medical-specialized, and commercial systems, yields several key findings. Substantial performance gaps remain between AI models and human experts, particularly in detecting subtle degradations critical to clinical practice. Current models exhibit preliminary but unstable perceptual and reasoning abilities, often failing to produce complete and precise quality descriptions. Medical-specialized models unexpectedly underperform general-purpose ones, calling into question the effectiveness of current domain adaptation strategies. Moreover, models show marked weaknesses in fine-grained comparisons and mild degradation detection, precisely where reliable quality control is most needed. By moving beyond high-level diagnostic reasoning toward foundational quality perceptual and reasoning skills, MedQ-Bench establishes clinically grounded and interpretable standard for measuring and advancing medical IQA. We anticipate that it will inform the development of MLLMs with stronger low-level visual understanding and trustworthy reasoning, paving the way for safe and reliable integration of automated quality control into clinical imaging workflows."
        },
        {
            "title": "REFERENCES",
            "content": "AAPM CT-MAR Challenge Organizers. reduction (ct-mar) grand challenge dataset. https://www.aapm.org/GrandChallenge/CT-MAR/; https://qtimchallenges.southcentralus.cloudapp.azure.com/competitions/1, 2023. URL https://www. aapm.org/GrandChallenge/CT-MAR/. Generated using XCIST, with hybrid data simulation framework combining clinical images and virtual metal objects. The aapm ct metal artifact Anthropic. Claude 4: Constitutional ai with harmlessness from ai feedback. Technical Report, 2024. Available at https://www.anthropic.com/claude. Amir Beck and Marc Teboulle. fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183202, 2009. Craig Blackmore, Robert Mecklenburg, and Gary Kaplan. Evidence-based radiology: new approach to the practice of radiology. Radiology, 259(3):615620, 2011. Tianhe Chen, Shuai Liu, Yizhou Zhang, and Rui Zhao. comprehensive study of multimodal large language models for image quality assessment. pp. 143159, 2024a. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning In Proceedings of the IEEE/CVF conference on computer for generic visual-linguistic tasks. vision and pattern recognition, pp. 2418524198, 2024b. Zhihao Chen, Bin Hu, Chuang Niu, Tao Chen, Yuxin Li, Hongming Shan, and Ge Wang. Image quality assessment with vision-language and chatgpt models. arXiv preprint Iqagpt: arXiv:2312.15663, 2023. Jacob Cohen. Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit. Psychological bulletin, 70(4):213, 1968. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascal Fung, and Steven Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Advances in Neural Information Processing Systems, 36:4925049267, 2024. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Melanie Dohmen, Tuan Truong, Ivo Baltruschat, and Matthias Lenga. Five pitfalls when assessing synthetic medical images with reference metrics. In MICCAI Workshop on Deep Generative Models, pp. 150159. Springer, 2024. Huazhu Fu, Boyang Wang, Jianbing Shen, Shanshan Cui, Yanwu Xu, Jiang Liu, and Ling Shao. Evaluation of retinal image quality assessment networks in different color-spaces. In International conference on medical image computing and computer-assisted intervention, pp. 4856. Springer, 2019. Moritz Fuchs, Ssharvien Kumar Sivakumar, Mirko Schober, Niklas Woltering, Marie-Lisa Eich, Leonille Schweizer, and Anirban Mukhopadhyay. Harp: Unsupervised histopathology artifact restoration. In Medical Imaging with Deep Learning, 2024. S. Alireza Golestaneh, Saba Dadsetan, and Kris M. Kitani. No-reference image quality assessment via transformers, relative ranking, and self-consistency. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 12201230, 2022. Matthieu Guerquin-Kern, Haberlin, Klaas Paul Pruessmann, and Michael Unser. fast waveletIEEE transactions on medical based reconstruction method for magnetic resonance imaging. imaging, 30(9):16491660, 2011. HMSS Herath, HMKKMB Herath, Nuwan Madusanka, and Byeong-Il Lee. systematic review of medical image quality assessment. Journal of Imaging, 11(4):100, 2025. 10 Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pp. 23662369. IEEE, 2010. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. Avinash Kak and Malcolm Slaney. Principles of computerized tomographic imaging. SIAM, 2001. Le Kang, Peng Ye, Yi Li, and David Doermann. Convolutional neural networks for no-reference image quality assessment. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 17331740, 2014. Neel Kanwal. Histoartifacts, March 2024. URL https://doi.org/10.5281/zenodo. 10809442. Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE International Conference on Computer Vision, pp. 51485157, 2021. Mathieu Lamard, Jean-Louis Coatrieux, Philippe Dequidt, Marc Le Berre, Christian Roux, and Basel Solaiman. Checklist for artificial intelligence in medical imaging (claim): 2024 update. In Radiology: Artificial Intelligence, volume 6, pp. e240300, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36:3682036835, 2024. Hongyi Miao, Jun Jia, Yankun Cao, Yingjie Zhou, Yanwei Jiang, Zhi Liu, and Guangtao Zhai. Ultrasound-qbench: Can llms aid in quality assessment of ultrasound imaging? arXiv preprint arXiv:2501.02751, 2025. Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality assessment in the spatial domain. IEEE Transactions on image processing, 21(12):46954708, 2012. OpenAI. Gpt-4v(ision) system card. Technical Report, 2023. URL https://openai.com/ research/gpt-4v-system-card. OpenAI. Gpt-4 technical report, 2024a. URL https://arxiv.org/abs/2303.08774. OpenAI. Gpt-5: Artificial general intelligence is near. Technical Report, 2024b. URL https: //openai.com/gpt-5. Sahal Shaji Peng, Vandan Gorade Narayan, Sreya Deshpande, et al. Bimedix2: Bio-medical expert lmm for diverse medical modalities. arXiv preprint arXiv:2405.20157, 2024. Gorkem Polat, Deniz Sen, Alperen Inci, and Alptekin Temizel. Endoscopic artefact detection with In Proceedings of the 2nd ensemble of deep neural networks and false positive elimination. International Workshop and Challenge on Computer Vision in Endoscopy, EndoCV@ISBI 2020, Iowa City, Iowa, USA, 3rd April 2020, volume 2595, pp. 812. CEUR-WS.org, 2020. Pranav Rajpurkar, Erin Chen, Oishi Banerjee, and Eric Topol. Ai in diagnostic imaging: Revolutionising accuracy and efficiency. Clinical Radiology, 79(2):e132e140, 2024. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, JeanBaptiste Alayrac, et al. Gemini 2.5: Unlocking multimodal understanding across image, video, and audio. arXiv preprint arXiv:2312.11805, 2024. Khaled Saab, Yi Tay, et al. Medgemma: medical large language model specialized for high-stakes applications. arXiv preprint arXiv:2409.03278, 2024. Hamid Sheikh and Alan Bovik. Image information and visual quality. IEEE Transactions on Image Processing, 15(2):430444, 2006. 11 Jialin Su, Meifang Li, Yongping Lin, Liu Xiong, Caixing Yuan, Zhimin Zhou, and Kunlong Yan. Deep learning-driven multi-view multi-task image quality assessment method for chest ct image. BioMedical Engineering OnLine, 22(1):117, 2023. Yanzhou Su, Tianbin Li, Jiyao Liu, Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin Ye, Pengcheng Chen, Ming Hu, et al. Gmai-vl-r1: Harnessing reinforcement learning for multimodal medical reasoning. arXiv preprint arXiv:2504.01886, 2025. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqiang Chang, Kai Sheng, Wei Liu, Junyang Wang, et al. Qwen2.5-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Qian Wang, Baiqiao Liu, Hongjian Wang, Jiaheng Li, Qingyu Chen, and Zhiyong Lu. Lingshu: linguistically-enhanced multi-modal chinese medical large language model. arXiv preprint arXiv:2406.06489, 2024b. Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. arXiv preprint arXiv:2212.00490, 2022. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-instruct: Improving low-level visual abilities for multi-modality foundation models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 54995509, 2024a. Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: benchmark for general-purpose foundation models on low-level vision. In ICLR, 2024b. xAI Team. Grok-4: Large language model. Technical Report, 2024. Available at https://x.ai. Huihui Xu, Yuanpeng Nie, Hualiang Wang, Ying Chen, Wei Li, Junzhi Ning, Lihao Liu, Hongqiu Wang, Lei Zhu, Jiyao Liu, et al. Medground-r1: Advancing medical image grounding via spatialsemantic rewarded group relative policy optimization. arXiv preprint arXiv:2507.02994, 2025. Siyi Xun, Yue Sun, Jingkun Chen, Zitong Yu, Tong Tong, Xiaohong Liu, Mingxiang Wu, and Tao Tan. Mediqa: scalable foundation model for prompt-driven medical image quality assessment. arXiv preprint arXiv:2507.19004, 2025. Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, et al. Sa-med2d-20m dataset: Segment anything in 2d medical imaging with 20 million masks. arXiv preprint arXiv:2311.11969, 2023. Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tianbin Li, Haodong Duan, Ziyan Huang, Yanzhou Su, Benyou Wang, et al. Gmai-mmbench: comprehensive multimodal evaluation benchmark towards general medical ai. Advances in Neural Information Processing Systems, 37: 9432794427, 2024. Zhiyuan You, Jinjin Tang, Yao Zhao, Xiaoming Wu, Zhifeng Duanmu, and Zhou Wang. Depicting beyond scores: Advancing image quality assessment through multi-modal language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(5):42094217, 2024. Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew Muckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, et al. fastmri: An open dataset and benchmarks for accelerated mri. arXiv preprint arXiv:1811.08839, 2018. Lin Zhang, Lei Zhang, and Alan Bovik. feature-enriched completely blind image quality evaluator. IEEE Transactions on Image Processing, 24(8):25792591, 2015. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Yizhou Zhang, Tianhe Chen, Shuai Liu, Rui Zhao, and Xiaodong Wang. Bias in artificial intelligence for medical imaging: fundamentals, detection, avoidance, mitigation, challenges, ethics, and prospects. Diagnostic and Interventional Radiology, 30(3):242854, 2024. Adam Narai, Petra Hermann, Tibor Auer, Peter Kemenczky, Janos Szalma, Istvan Homolya, Eszter Somogyi, Pal Vakli, Bela Weiss, and Zoltan Vidnyanszky. movement-related artefacts (mr-art) dataset, 2022. 13 15 17 17 18 18 21 21 23 23"
        },
        {
            "title": "A APPENDIX",
            "content": "APPENDIX TABLE OF CONTENTS A.1 Data Construction Pipeline and Quality Control . . . . . . . . . . . . . . . . . . . A.2 Detailed Benchmark Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2.1 Dataset Composition by Source and Modality . . . . . . . . . . . . . . . . A.2.2 Distributions of Task Types and Degradation Levels in MedQ-Perception . A.2.3 Distribution of low-level attributions . . . . . . . . . . . . . . . . . . . . . A.3 Evaluation Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Complete Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4.2 Detailed Model Performance . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Qualitative Analysis and Case Studies . . . . . . . . . . . . . . . . . . . . . . . . A.5.1 Why Do Medical-Specialized Models Underperform General-Purpose Mod- . . . . . . . . . . . . . . . . . . . . . . . . . . . els? . . . . . . . . . . . A.5.2 Example of Reasoning Tasks . . . . . . . . . . . . . . . . . . . . . . . . . A.5.3 Human Expert Evaluation Protocol . . . . . . . . . . . . . . . . . . . . . 14 A.1 DATA CONSTRUCTION PIPELINE AND QUALITY CONTROL The construction of MedQ-Bench involved systematic multi-stage pipeline for collecting, curating, and annotating medical images across five modalities. This section provides detailed information about our comprehensive data sourcing strategies, quality control measures, and annotation protocols, with particular emphasis on the diverse sources and label types that enable robust evaluation of low-level visual perception capabilities. Comprehensive Data Sources and Acquisition Strategy. We employed three-channel data collection strategy: public datasets + imaging department collaboration + synthetic generation. On one hand, we conducted comprehensive internet searches for 2D/3D medical quality-related datasets; on the other hand, we collaborated with hospitals to obtain ethically approved clinical data. From this massive data pool, we ultimately selected the datasets shown in Table 3, covering 5 medical imaging modalities to ensure universality and clinical relevance of data sources. For images, we adhere to the SA-Med2D-20M (Ye et al., 2023) protocol, transforming all 2D/3D medical images into 2D RGB images for further evaluation. Table 3 provides complete overview of all datasets integrated into MedQ-Bench, including specific modalities, sample quantities, label types, and acquisition status. The table demonstrates the comprehensive scope of our data collection effort, spanning established clinical research datasets and custom synthetic degradation collections, and AI-generated images. All collected images were anonymized, with all patient-identifying information systematically removed using automated de-identification pipelines validated against clinical privacy requirements. Expert-designed Seed Perception Questions. The construction process began with panel of medical imaging specialists who designed seed questions covering diverse modalities, degradation types, and task formats. These domain experts systematically identified key visual quality attributes specific to each modality, ensuring that the seed questions span the three question types: Yes-No, What, and How. Each seed question was carefully paired with selected images to ensure strong alignment between the textual prompt and visual evidence, establishing foundation of clinically grounded quality assessment scenarios. Controlled Question Expansion. To scale beyond the initial seed set while maintaining quality and clinical relevance, we employed GPT-4o as controlled question generator. For each image, using seed questions as templates, we randomly selected one question from each question type and performed controlled generation. This systematic generation process varied degradation types, severity levels, and phrasing styles while preserving clinical realism and explicitly avoiding highlevel diagnostic reasoning. The expansion process was constrained by predefined templates and modality-specific quality attributes to ensure consistency and prevent drift away from the intended low-level visual assessment focus. Multi-round Expert Validation. We manually annotated the answers for the generated questions to ensure correctness, consistency, and alignment with the intended low-level quality assessment labels. All question-answer pairs underwent rigorous multi-stage human annotation and verification using structured annotation interface, as shown in Figure 7 and 8. The multi-round validation process involved multiple phases of annotation and proofreading:(1) Initial independent review by at least three medical imaging experts for question formulation, answer correctness, and imagequestion alignment; (2) Cross-validation and proofreading sessions to identify and resolve inconsistencies; (3) Final consensus rounds where disagreements were resolved through discussion until unanimous agreement was reached. Finally, the dataset was randomly partitioned by image into development and test sets of equal size. Reasoning Annotation Standards and Workflow. For the MedQ-Reasoning tasks, we established specific annotation standards to ensure consistent and clinically relevant quality assessment descriptions. Expert annotators followed structured reasoning workflow that emphasized systematic analysis and transparent decision-making processes. The reasoning annotation protocol involved sequential four-step process: (1) Visual Analysis Phase: Systematic examination of perceptual attributes such as noise, blur, artifacts, contrast, and resolution, avoiding any high-level diagnostic interpretation; (2) Modality-Specific Assessment: Targeted evaluation of quality dimensions specific 15 to each imaging modality (e.g., streak artifacts in CT, motion artifacts in MRI, staining uniformity in histopathology), following standardized checklists for each modality type; (3) Quality Classification: Application of three-tier system based on accumulated evidence from steps 1-2: good (no significant quality issues affecting clinical utility), usable (minor quality issues that do not compromise diagnostic accuracy), and reject (severe quality degradation requiring repeat imaging); (4) Structured Description Generation: Creation of comprehensive yet concise descriptions (3-5 sentences) that logically connect the observed visual attributes to the final quality judgment, ensuring clear reasoning traceability from observation to conclusion. This step-by-step reasoning flow ensures that all quality assessments follow consistent analytical framework, with each conclusion being explicitly grounded in observable visual evidence rather than subjective impressions. All reasoning annotations underwent the same multi-round validation process as the perception tasks to ensure consistency and clinical accuracy across all expert annotators. Dataset Composition and Balance. Each modality contributes proportionally to maintain representational balance, and degradation types are systematically distributed to avoid bias toward any particular quality issue. Figure 7: Interface for the MedQ-MCQA dataset. Figure 8: Interface for the MedQ-Reasoning dataset. 16 A.2 DETAILED BENCHMARK STATISTICS A.2.1 DATASET COMPOSITION BY SOURCE AND MODALITY Dataset Composition. The MedQ-Bench dataset consists of 3,308 samples distributed across three primary source types  (Table 2)  . The dataset covers five major medical imaging modalities with detailed breakdown by source type and specific datasets shown in Table 3. Source Type Authentic Simulate AI-Generated Percentage 41.3% 33.9% 24.8% Table 2: Distribution of MedQ-Bench dataset by source type. Modality Source Type Dataset Samples % of Modality Total Samples CT MRI Authentic Simulate AI-generated AIsynthesis (subset) Radiopaedia AAPM CT-MAR (AAPM CT-MAR Challenge Organizers, 2023) Authentic Authentic Authentic Simulate Simulate AI-generated AIsynthesis Radiopaedia MR-ART ( Adam Narai et al., 2022) FSL Example MRI Artifacts FastMRI Zbontar et al. (2018) 5T MRI Data Histopathology HistoArtifacts (Kanwal, 2024) Authentic AI-generated HARP (Fuchs et al., 2024) AI-generated AIsynthesis Endoscopy Authentic AI-generated AIsynthesis EndoCV2020 (Polat et al., 2020) Retinal Authentic AI-generated AIsynthesis EyeQ (Fu et al., 2019) Overall Total 239 613 26 130 68 43 454 55 98 220 470 470 85 197 72 3,308 27.2% 69.8% 3.0% 15.0% 7.9% 5.0% 54.4% 6.4% 11.3% 29.0% 62.0% 9.0% 84.7% 15.3% 73.2% 26.8% 878 848 758 269 Table 3: Comprehensive breakdown of dataset composition. Detailed Simulation Methods for Synthetic Degradations. The simulated CT degradations in AAPM CT-MAR were reconstructed using several algorithms: SIRT, FBP (Kak & Slaney, 2001), and FISTA (Beck & Teboulle, 2009). Specifically, CT artifacts were systematically simulated to include three primary degradation types: (1) limited-angle artifacts, (2) metal artifact reduction, and (3) sparse-view artifacts. For MRI degradations, we primarily simulated acceleration artifacts and motion artifacts using established computational frameworks. Acceleration artifacts were generated using SigPy1 and TorchIO2, implementing both DDNM (Wang et al., 2022) and wavelet-based reconstruction methods (Guerquin-Kern et al., 2011). Additionally, our 5T MRI data were acquired from private clinical collections using the uMR Jupiter 5T system, obtained under institutional ethical approval with comprehensive patient anonymization protocols. To generate synthetic images across diverse medical imaging modalities, we employed BAGEL finetuned on domain-specific medical datasets (Deng et al., 2025). This approach ensured that synthetic degradations maintained clinical realism while providing controlled quality variations essential for comprehensive benchmark evaluation. A.2.2 DISTRIBUTIONS OF TASK TYPES AND DEGRADATION LEVELS IN MEDQ-PERCEPTION Question Type Percentage Modality-specific General Total 57.2% 42.8% 100.0% Degradation Level Percentage No Degradation Mild Degradation Severe Degradation Total 23.8% 44.6% 31.6% 100.0% Table 4: MedQ-Perception: Distribution of tasks by question type. Table 5: MedQ-Perception: Distribution of degradation severity levels. 17 Figure 9: Distribution of low-level attributions across imaging modalities and distortion types in MedQ-Bench. A.2.3 DISTRIBUTION OF LOW-LEVEL ATTRIBUTIONS A.3 EVALUATION PROMPT Single-Image Perception Task Prompts Yes-No / What / How Question Template: You are an expert in medical image quality assessment. Please carefully observe this medical image and answer the following question: Reasoning Task Prompts No-reference Reasoning Template: As medical image quality assessment expert, provide concise description focusing on low-level appearance of the image in details. quality of this image is [good/usable/reject]\". provide comprehensive but concise assessment in 3-5 sentences. Conclude with \"Overall, the Please Comprehensive Reasoning Template: As medical image quality assessment expert, provide concise description comparing two images focusing on low-level appearance. Conclude with which image has higher quality. Please provide comprehensive but concise assessment in 3-5 sentences. 1https://sigpy.readthedocs.io/en/latest/ 2https://github.com/TorchIO-project/torchio 18 Complete Evaluation Prompt Templates for No-reference Reasoning Tasks Completeness Evaluation Prompt: #System: You are helpful assistant. #User: Evaluate whether the description [MLLM DESC] completely includes the low-level visual information in the reference description [GOLDEN DESC]. Please rate score 2 for completely or almost completely including reference information, 0 for not including at all, 1 for including part of the information or similar description. Please only provide the result in the following format: Score: Preciseness Evaluation Prompt: #System: You are helpful assistant. #User: The precision metric evaluates whether the low-level description is consistent with the reference and reasonably aligned with the final quality judgment. Minor wording differences or small omissions that do not change the overall meaning should still be considered consistent. Only penalize clear contradictions with the reference, such as describing blur for clear, noisy for clean, motion-free for motion artifacts, noise-free for low-dose noise, etc. Evaluate whether output [MLLM DESC] reasonably reflects reference [GOLDEN DESC]. Please rate score 2 for overall consistency and no major contradictions with the quality conclusion, 1 for partial consistency or very few minor contradictions, and 0 for obvious contradictions or misalignment with the quality conclusion. Please only provide the result in the following format: Score: Consistency Evaluation Prompt: #System: You are helpful assistant. #User: Evaluate the internal consistency between the reasoning path (description of image problems) and the final quality judgment in [MLLM DESC]. The reasoning should logically support the final quality conclusion. For example, if many serious problems are described, the final quality should be \"reject\"; if minor problems are described, it should be \"usable\"; if no or very few problems are described, it should be \"good\". Compare with the reference [GOLDEN DESC] to understand the expected reasoning-conclusion relationship. Please rate score 2 for highly consistent reasoning and conclusion, 1 for partially consistent with minor logical gaps, and 0 for major inconsistency between described problems and quality judgment. Please only provide the result in the following format: Score: Quality Accuracy Evaluation Prompt: #System: You are helpful assistant. #User: Evaluate the accuracy of the final quality judgment in [MLLM DESC] compared to the reference [GOLDEN DESC]. The quality levels have progressive relationship: reject < usable < good. Consider the distance between predicted and reference quality: Please rate score 2 for exactly matching the reference quality level, 1 for adjacent level difference (e.g., usable vs good, or reject vs usable), and 0 for distant level difference (reject vs good) or completely incorrect quality assessment. Please only provide the result in the following format: Score: 19 Complete Evaluation Prompt Templates for Comparison Reasoning Tasks Completeness Evaluation Prompt: #System: You are helpful assistant. #User: Evaluate whether the description [MLLM DESC] completely includes the low-level visual information in the reference description [GOLDEN DESC]. Please rate score 2 for completely or almost completely including reference information, 0 for not including at all, 1 for including part of the information or similar description. Please only provide the result in the following format: Score: Preciseness Evaluation Prompt: #System: You are helpful assistant. #User: The precision metric evaluates whether the low-level description is consistent with the reference and reasonably aligned with the final quality judgment. Minor wording differences or small omissions that do not change the overall meaning should still be considered consistent. Only penalize clear contradictions with the reference, such as describing blur for clear, noisy for clean, motion-free for motion artifacts, noise-free for low-dose noise, etc. Evaluate whether output [MLLM DESC] reasonably reflects reference [GOLDEN DESC]. Please rate score 2 for overall consistency and no major contradictions with the quality conclusion, 1 for partial consistency or very few minor contradictions, and 0 for obvious contradictions or misalignment with the quality conclusion. Please only provide the result in the following format: Score: Consistency Evaluation Prompt: #System: You are helpful assistant. #User: Evaluate the internal consistency between the reasoning path (comparative description of image problems) and the final quality comparison judgment in [MLLM DESC]. The reasoning should logically support the final comparison conclusion. Compare with the reference [GOLDEN DESC] to understand the expected reasoning-conclusion relationship for image comparison. Please rate score 2 for highly consistent reasoning and comparison conclusion, 1 for partially consistent with minor logical gaps, and 0 for major inconsistency between described comparative problems and quality comparison judgment. Please only provide the result in the following format: Score: Quality Accuracy Evaluation Prompt: #System: You are helpful assistant. #User: Evaluate the accuracy of the final quality comparison judgment in [MLLM DESC] compared to the reference [GOLDEN DESC]. The comparison should correctly identify which image has higher quality based on the described visual characteristics. Please rate score 2 for exactly matching the reference quality comparison, and 0 for completely incorrect quality comparison (opposite conclusion) or unreasonable assessment. Please only provide the result in the following format: Score: 20 A.4 COMPLETE EXPERIMENTAL RESULTS A.4.1 EXPERIMENTAL SETUP In this study, we evaluated various large vision-language models (LVLMs), encompassing medicalspecialized models, open-source models, and closed-source API general-purpose models. Model weights were obtained from their respective official Hugging Face repositories. The evaluation work was conducted using the VLMEvalKit framework3. The evaluation was performed under zero-shot setting. Specifically, our evaluation prompts contained no example demonstrations, and models had to complete task reasoning without any related training or examples. This approach better tests the models generalization capabilities and understanding abilities, examining their performance when faced with novel problems. All tests were executed on NVIDIA A100 GPUs with 80GB memory. A.4.2 DETAILED MODEL PERFORMANCE Sub-categories Model (variant) Qwen2.5-VL-Instruct (7B) Qwen2.5-VL-Instruct (32B) Qwen2.5-VL-Instruct (72B) InternVL3 (8B) InternVL3 (38B) BiMediX2 (8B) MedGemma (27B) Lingshu (32B) Mistral-Medium-3 Claude-4-Sonnet Gemini-2.5-Pro GPT-4o Grok-4 GPT-5 Perception (Dev) Reasoning (Dev) Yes-or-No What How Overall Comp. Prec. Cons. Qual. Overall 62.71% 64.43% 74.57% 75.09% 70.62% 47.77% 62.71% 48.80% 65.46% 67.53% 70.10% 73.54% 76.98% 78.52% 45.26% 53.93% 44.40% 56.20% 38.36% 55.37% 46.98% 50.62% 47.84% 51.24% 28.02% 29.13% 44.40% 49.59% 50.86% 53.31% 56.32% 57.78% 60.94% 60.94% 59.32% 37.29% 54.55% 50.85% 57.32% 46.12% 52.89% 57.47% 39.66% 53.93% 58.24% 52.16% 46.90% 48.71% 52.89% 61.40% 46.55% 63.22% 66.41% 57.33% 56.61% 66.56% 0.688 1.036 0.864 0.937 0. 0.367 0.742 0.629 0.937 0.837 0.810 0.923 1.036 1.176 0.615 0.896 0.851 0.878 0.900 0.376 0.466 0.733 0.805 0.674 0.769 0.936 0.937 1.090 1.869 1.959 1.860 1.864 1. 0.348 1.652 1.964 1.652 1.810 1.579 1.809 1.751 1.756 1.122 1.253 1.348 1.339 1.367 0.683 1.249 1.059 1.389 1.385 1.548 1.389 1.484 1.566 4.294 5.144 4.923 5.018 5. 1.774 4.109 4.385 4.783 4.706 4.706 5.057 5.208 5.588 Table 6: Performance of different models on perception and no-reference reasoning tasks (Dev Set). Model CT Histo. MRI Endos. Retinal GPT-5 GPT-4o Grok-4 Gemini-2.5-Pro Mistral-Medium-3 Claude-4-Sonnet 71.47% 65.43% 75.90% 60.89% 70.09% 72.85% 58.33% 64.75% 60.44% 66.67% 70.14% 59.37% 65.93% 64.49% 61.40% 67.04% 53.40% 60.79% 60.89% 59.83% 65.93% 38.58% 61.51% 65.33% 61.54% 64.27% 54.63% 55.04% 65.33% 65.81% Qwen2.5-VL-72B 65.65% 47.53% 74.82% 66.22% 64.96% 68.14% 48.46% 62.95% 60.44% 70.09% InternVL3-38B InternVL3-8B 60.66% 51.54% 61.87% 67.56% 63.25% Qwen2.5-VL-32B 59.00% 46.30% 66.55% 67.56% 63.25% 56.79% 35.49% 65.47% 60.44% 64.96% Qwen2.5-VL-7B MedGemma-27B Lingshu-32B BiMediX2-8B 66.57% 46.60% 57.55% 56.00% 59.83% 57.89% 35.19% 61.15% 50.67% 48.72% 41.99% 23.38% 44.36% 38.74% 47.62% Table 7: Detailed perception accuracy results across five imaging modalities on the test set. Table 9 provides the complete numerical breakdown of model performance across different comparison difficulty levels and evaluation dimensions corresponding to Figure 6. This comprehensive analysis reveals significant performance variations between coarse-grained and fine-grained comparison tasks across all models. 3https://github.com/open-compass/VLMEvalKit 21 Model Comp. Prec. Cons. Qual. Overall GPT-5 GPT-4o Grok-4 Gemini-2.5-Pro Mistral-Medium-3 Claude-4-Sonnet Qwen2.5-VL-72B-Instruct InternVL3-38B InternVL3-8B Qwen2.5-VL-32B-Instruct Qwen2.5-VL-7B-Instruct MedGemma-27B Lingshu-32B BiMediX2-8B 1.376 1.113 1.203 1.008 0.932 0.827 0.947 1.090 1.023 0.865 0.684 0.662 0.692 0. 1.504 1.895 1.489 1.947 1.203 1.865 1.180 1.895 1.263 1.789 0.992 1.917 1.158 1.481 1.090 1.684 1.278 1.910 0.872 1.887 0.925 1.316 0.571 1.105 0.940 1.519 0.579 0.594 1.609 1.669 1.421 1.489 1.414 1.338 1.376 1.489 1.549 1.083 1.150 0.955 1.203 0. 6.384 6.218 5.692 5.572 5.398 5.074 4.962 5.353 5.760 4.707 4.075 3.293 4.354 2.210 Table 8: Performance comparison on MedQ-Reasoning paired comparison tasks (Dev Set). Model GPTGPT-4o Grok-4 Gemini-2.5-Pro InternVL3-8B Claude-4-Sonnet Mistral-MediumInternVL3-38B Lingshu-32B Qwen2.5-VL-32B Qwen2.5-VL-7B Qwen2.5-VL-72B MedGemma-27B BiMediX2-8B Group Comp. Prec. Cons. Qual. Acc. Total Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained Overall Fine-grained Coarse-grained 1.293 1.301 1. 1.105 1.155 0.933 1.150 1.214 0.933 1.053 1.039 1.100 0.985 0.971 1.033 0.857 0.835 0.933 0.872 0.893 0. 1.075 1.117 0.933 0.729 0.699 0.833 0.692 0.786 0.367 0.714 0.757 0.567 0.737 0.699 0.867 0.684 0.650 0. 0.474 0.359 0.867 1.556 1.925 1.495 1.903 1.767 2.000 1.414 1.632 1.388 1.583 1.500 1.800 1.233 1.820 1.350 1.825 0.833 1.800 1.233 1.774 1.262 1.709 1.133 2.000 1.278 1.797 1.155 1.748 1.700 1. 1.083 1.910 1.107 1.883 1.000 2.000 1.203 1.827 1.252 1.786 1.033 1.967 1.083 1.571 1.184 1.466 0.733 1.933 1.015 1.586 0.990 1.505 1.100 1.867 0.752 1.895 0.922 1.864 0.167 2.000 0.902 1.316 1.000 1.320 0.567 1. 0.977 1.233 0.903 1.029 1.233 1.933 0.692 1.128 0.641 0.942 0.867 1.767 0.549 0.639 0.379 0.641 1.133 0.633 1.564 1.476 1.867 1.564 1.534 1.667 1.459 1.495 1. 1.534 1.476 1.733 1.474 1.359 1.867 1.481 1.495 1.433 1.338 1.282 1.533 1.414 1.359 1.600 1.323 1.243 1. 0.962 1.068 0.600 1.143 1.175 1.033 1.113 0.971 1.600 1.000 0.854 1.500 0.511 0.311 1.200 6.338 6.175 6. 5.714 5.660 5.900 5.662 5.883 4.900 5.594 5.485 5.967 5.534 5.233 6.567 5.331 5.320 5.367 5.241 5.214 5. 5.143 5.126 5.200 4.654 4.437 5.400 4.301 4.641 3.133 4.075 4.252 3.467 4.060 3.602 5.633 3.504 3.087 4. 2.173 1.689 3.833 Table 9: Detailed numerical results for paired comparison reasoning tasks across models, corresponding to Figure 6. 22 A.5 QUALITATIVE ANALYSIS AND CASE STUDIES A.5.1 WHY DO MEDICAL-SPECIALIZED MODELS UNDERPERFORM GENERAL-PURPOSE MODELS? The counterintuitive finding that medical-specialized models consistently underperform generalpurpose models across all evaluation dimensions warrants comprehensive analysis. Figure 11 provides illustrative examples demonstrating fundamental limitations in medical-specialized models low-level visual perception capabilities. Insufficient Low-Level Visual Attribute Training. Medical-specialized models appear to prioritize high-level diagnostic reasoning over fundamental visual perception skills. In the CT scan example (Figure 11), MedGemma-27B correctly identifies anatomical structures and acknowledges the presence of streak artifacts, but fails to appropriately assess their clinical significance. The model describes the image as usable but not optimal despite prominent metal artifacts that would necessitate repeat scanning in clinical practice. This suggests that medical fine-tuning datasets may inadequately represent the full spectrum of image quality degradations encountered in clinical workflows. Diagnostic Bias Over Quality Assessment. BiMediX2-8B demonstrates critical failure mode by describing the same severely degraded CT scan as having good quality and suitable for diagnosis. This systematic misalignment indicates that medical-specialized training may inadvertently optimize models for diagnostic confidence rather than quality assessment accuracy. The models focus on anatomical identification overshadows its ability to detect quality-compromising artifacts, suggesting that current medical training paradigms may not adequately distinguish between diagnostic content recognition and image quality evaluation. Figure 10: Representative QA examples demonstrating typical question-answer patterns in MedQBench across different medical imaging modalities and quality assessment scenarios. A.5.2 EXAMPLE OF REASONING TASKS Figure 11: Representative no-reference reasoning image examples demonstrating typical questionanswer patterns in MedQ-Bench across different models. 24 Figure 12: Representative paired image examples demonstrating typical question-answer patterns in MedQ-Bench across different models. 25 A.5.3 HUMAN EXPERT EVALUATION PROTOCOL Expert Recruitment and Qualification Criteria. Human experts in our evaluation consisted of medical imaging technicians with minimum of 3 years of clinical experience in medical imaging quality assessment and medical imaging PhDs with specialized training in image quality evaluation. Medical imaging technicians were recruited from certified clinical facilities and possessed active professional certifications in their respective imaging modalities. PhDs were selected from accredited medical imaging research programs and had completed at least 2 years of coursework, including medical image processing and quality assessment methodologies. All experts demonstrated proficiency in identifying common imaging artifacts and quality issues across multiple medical imaging modalities through standardized pre-evaluation assessment. Human-AI Alignment Analysis. The confusion matrices shown in Figure 13 demonstrate strong alignment between human expert scores and GPT-4o automated evaluation across all three evaluation dimensions, with over 80% accuracy in each dimension. Quadratic weighted κw accounts for the ordinal nature of the evaluation labels, penalizing larger discrepancies more heavily than adjacent category differences. The consistently high κw values (0.7740.985) detailed in Table 10 indicate substantial agreement beyond chance between human expert scores and GPT-4o automated evaluation, reflecting that the automated system is not only accurate but also aligned with the fine-grained ordinal structure of human expert judgments. The consistently high κw values (0.7740.985) detailed in Table 10 indicate substantial agreement beyond chance between human expert scores and GPT-4o automated evaluation, reflecting that the automated system is not only accurate but also aligned with the fine-grained ordinal structure of human expert judgments. This confirms that our automated evaluation framework maintains robust alignment with human expert annotations, strengthening confidence in its use as reliable surrogate for large-scale human evaluation. Figure 13: Confusion matrices showing alignment between human expert scores and GPT-4o automated evaluation across four evaluation dimensions. Metric Completeness Preciseness Consistency Quality Accuracy κw 0.774 0.876 0.840 0.985 Table 10: Quadratic weighted Cohens κw values for humanAI alignment across evaluation dimensions."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Imperial College London",
        "Shanghai Artificial Intelligence Laboratory",
        "University of Cambridge"
    ]
}