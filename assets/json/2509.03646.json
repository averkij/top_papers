{
    "paper_title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning",
    "authors": [
        "Haozhe Wang",
        "Qixin Xu",
        "Che Liu",
        "Junhong Wu",
        "Fangzhen Lin",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as a superior compass for measuring strategic exploration over misleading metrics such as token-level entropy."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 2 6 4 6 3 0 . 9 0 5 2 : r a"
        },
        {
            "title": "EMERGENT HIERARCHICAL REASONING IN LLMS\nTHROUGH REINFORCEMENT LEARNING",
            "content": "Haozhe Wang, Qixin Xu, Che Liu, Junhong Wuϑ, Fangzhen Lin, Wenhu Chen Hong Kong Univerisity of Science and Technology, University of Waterloo M-A-P, Tsinghua Univerisity, Imperial College London, ϑUCAS {jasper.whz@outlook.com, wenhuchen@uwaterloo.ca} (cid:209) https://tiger-ai-lab.github.io/Hierarchical-Reasoner/"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like aha moments, length-scaling and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover compelling two-phase dynamic: initially, model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as superior compass for measuring strategic exploration over misleading metrics such as token-level entropy. Figure 1: (Left) LLM reasoning mirrors human-like hierarchical reasoning: high-level strategic planning and low-level procedural executions. (Right) Hierarchical reasoning emerges during RL training via two-phase dynamic. Phase ① consolidates low-level skills, marked by token-entropy drop in execution tokens. The learning frontier then shifts to Phase ②, where the model explores and masters high-level planning, marked by increased semantic diversity, sustained reasoning enhancement and length scaling."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement Learning (RL) has become instrumental in advancing the complex reasoning capabilities of Large Language Models (LLMs) across diverse domains (Ouyang et al., 2022; Jaech et al., 2024; Yang et al., 2024; Guo et al., 2025; Team et al., 2025). However, this empirical success is accompanied by significant gap in our understanding of the underlying learning dynamics. The"
        },
        {
            "title": "Work in progress",
            "content": "training process often yields phenomena that are as effective as they are poorly understood: models can experience sudden aha moments, where they seemingly acquire new emergent skills (Guo et al., 2025); they exhibit length-scaling effects, where reasoning performance improves with longer, more detailed outputs (Guo et al., 2025; Team et al., 2025); and they display complex dynamics in token-level entropy (Yu et al., 2025; Cui et al., 2025). This gap motivates fundamental question: What unlocks enhanced reasoning in LLMs during RL, and how should we leverage this understanding to design more principled and efficient RL algorithms? In this work, we posit that these phenomena in effective RL processes are not disparate occurrences but are hallmarks of single, coherent process: RL forges an emergent functional reasoning hierarchy within the LLM. We argue that the learning process dynamically shifts its focus, first building reliable foundation of low-level procedural skills and then learning to direct these skills with high-level strategic planning. This emergent structure finds compelling parallel in the cognitive architecture of the human brain (Murray et al., 2014; Zeraati et al., 2023; Huntenburg et al., 2018), which separates high-level, deliberate strategic planning (e.g., deciding which theorem to apply in the prefrontal cortex) from the rapid, subordinate execution of learned procedures (e.g., performing an algebraic manipulation). This cognitive model provides powerful framework for interpreting the learning dynamics we observe in LLMs. Motivated by this parallel, we propose functional decomposition of modelgenerated solutions: High-level Planning Tokens: The high-level strategic moves that orchestrate the reasoning process. These tokens manifest as logical maneuvers, including deduction (e.g., we can use the fact that), branching (e.g., lets try different approach), and backtracing (e.g., but the problem mentions that). Low-level Execution Tokens: The operational building blocks of solution. These comprise concrete, low-level steps such as arithmetic calculations, variable substitutions, and the direct application of known formulas. Our analysis across various models reveals that the optimization pressure on the reasoning hierarchy evolves throughout training, driven by dynamic shift in the learning frontiers. At the outset, the learning process is constrained by procedural correctness. single calculation error can nullify an entire solution, providing strong learning signal that forces the model to first master low-level execution tokens. Once the model achieves proficiency in these foundational skills, however, the learning bottleneck shifts to strategic planning. We find that exploring and mastering the use of planning tokens unlocks significant and sustained improvements in reasoning ability. This two-phase mechanism explains aha moments as the discovery and internalization of high-level strategic reasoning strategies, such as self-reflections. It also accounts for the length-scaling effect, as employing more sophisticated strategies involving thorough planning and logical backtracing naturally elongates the reasoning trace with structured, strategic deliberation. Notably, it provides unified perspective to understand the complex token entropy dynamics across different models, through the lens of high-impact planning tokens and gradually confident execution tokens. This discovery that the learning frontiers dynamically shifts to strategic planning is more than an academic curiosity; it provides clear blueprint for more effective RL algorithm. If the primary driver for advanced reasoning is the mastery of high-level strategic planning, then current agnostic credit assignment methods used in the prevailing GRPO (Guo et al., 2025) and its variants (Yu et al., 2025; Liu et al., 2025b; Wang et al., 2025c) are fundamentally inefficient, as they dilute optimization pressure across all tokens rather than concentrating it where it matters most. Based on this insight, we propose Hierarchy-aware Credit Assignment (HICRA), novel algorithm designed to focus optimization pressure directly on this emergent strategic bottleneck. By selectively amplifying the learning signal for planning tokens, HICRA accelerates the exploration and reinforcement of effective high-level reasoning, leading to significant performance gains as demonstrated in our experiments. Our work makes three primary contributions:"
        },
        {
            "title": "Work in progress",
            "content": "We identify and empirically validate the emergent reasoning hierarchy in RL-tuned LLMs. We, for the first time, show that the enhanced reasoning is not monolithic process but two-phase dynamic: The learning frontier shifts from mastering low-level procedural execution to exploring high-level strategic planning. This core insight provides unified explanation for opaque phenomena such as aha moments, length-scaling and the distinctive token entropy dynamics across different models. To effectively address the strategic bottleneck, we introduce HICRA (Hierarchy-Aware Credit Assignment), an algorithm that concentrates optimization efforts on high-impact planning tokens. By moving beyond agnostic credit assignment, HICRA directly targets the primary driver of advanced reasoning (Section 3). Through extensive experiments, we show that HICRA significantly outperforms strong baselines across multiple models and benchmarks through the lens of strategic exploration. In addition, we validate the use of semantic entropy as reliable compass for measuring strategic exploration than conventional metrics like token-level entropy, which we show can be misleading."
        },
        {
            "title": "2 THE EMERGENT REASONING HIERARCHY",
            "content": "Our empirical analysis reveals that the enhancement of reasoning in LLMs through RL is not monolithic process. Instead, it is driven by the emergence of functional reasoning hierarchy, which forms in two overlapping phases. At the outset, the models learning is often constrained by the need for procedural reliability; it must first learn to execute low-level steps reliably. Once this foundation is built, the bottleneck shifts to strategic planning, where the exploration and mastery of high-level reasoning strategies become the primary engine for performance gains. 2.1 FUNCTIONAL PROXY FOR THE REASONING HIERARCHY To analyze these dynamics, we must first distinguish between high-level planning and low-level tokens. This is inherently challenging, as tokens function is not intrinsic but is defined by its role in sequence. We motivate our approach by drawing parallel to human cognition. When person reasons through problem, we identify their strategic thinking by its function. phrase like, Lets try different approach, functions as high-level strategic maneuver that guides the problem-solving direction. In contrast, phrase like, so we add 5 to both sides, is low-level procedural step. Inspired by this functional distinction, we introduce Strategic Grams as functional proxy to circumvent the difficulty of formally defining planning token. Strategic Grams (SGs) are defined as n-grams that function as single semantic unit to guide the logical flow. We use n-grams because they capture the phrasal nature of strategic language (e.g., lets consider the case) which is lost at the single-token level. As Figure 2 shows, these SGs facilitate three main types of logical moves: (a) deduction, (b) branching, and (c) backtracing. Given collection of SGs, our classification heuristic is straightforward: token is classified as strategic planning token if it is part of Strategic Gram in the current context. All other tokens are classified as procedural execution tokens. For simplicity, we use the term execution tokens to refer to all non-planning tokens. We note this is slight simplification, as this category encompasses not only concrete calculations but also formatting and other procedural language. We constructed the SG set using the following procedure: 1. Grams Collection: We generate large corpus of successful solutions by sampling from the training model at different iterations, and then extract n-grams (where [3, 5]) to collect the models native mode of expression. 2. Grams Annotation: We use Gemini-2.5 with carefully designed prompts (see Appendix) to programmatically classify n-grams that function as strategic constructs, yielding seed collection. We then expand the seed set by manually reviewing any missing SGs from the rollout instances."
        },
        {
            "title": "Work in progress",
            "content": "Figure 2: Reasoning from Qwen3-4B-GRPO with planning tokens (strategic grams) highlighted. Planning tokens function as the high-level strategic moves of reasoning, including logical deduction, branching and backtracing. Using this heuristic and the resulting SG collection (listed in the Appendix), our empirical results in the following sections reveal consistent and insightful patterns in the models learning dynamics. 2.2 EMERGENCE OF THE REASONING HIERARCHY Building on our functional proxy for reasoning, we examine the learning dynamics of RL for LLM reasoning and finds an intriguing parallel with human-like hierarchical reasoning. Our empirical analysis conducted consistently across different model families, Qwen2.5-7B (Yang et al., 2024), Qwen3-4B (Yang et al., 2025), LLama-3.1-8B (Grattafiori et al., 2024), MiMO-VL-7B (Xiaomi, 2025) reveals that enhanced reasoning is not monolithic process, but driven by an evolution of the learning frontiers. The learning process exhibit two overlapping phases: it begins with rapid consolidation of procedural reliability, conducive to the widespread low-level tokens. This is followed by sustained period where the greatest potential for improvement shifts to the exploration of high-level strategic reasoning, which serves as the true engine of advanced performance. 2.2.1 FORGING RELIABLE PROCEDURAL ENGINE The initial phase of RL training is dedicated to mastering the basics. The model must first build reliable engine for performing calculations and other procedural steps. To observe this, we track two key metrics on the execution tokens: Relative Perplexity: Perplexity, (xtx<t), measures model surprise. lower value signifies higher confidence. We normalize the perplexity by its initial value to compare the rates of change in planning tokens and execution tokens."
        },
        {
            "title": "Work in progress",
            "content": "Figure 3: We track the training Dynamics of representative model families. The curves reveal two-phase dynamics. Seen from the first two columns, the model has an initial focus on procedural consolidation, marked by sharp decrease in model perplexity (greater confidence) and token entropy (more certain) of execution tokens. This follows shift to exploring strategic planning, evident from the third column. The diversity of strategic plans (semantic entropy) steadily increases on Qwen models or takes turn to increase on Llama, correlating with consistently improved accuracy and longer reasoning chains (fourth column). Token-Level Entropy: The Shannon entropy of the policys next-token distribution, H(π(x<t)), measures its uncertainty. High entropy signals active exploration of the vocabulary at the position, while low entropy suggests confident exploitation of known token. The evidence for this phase is shown in the first two columns of Figure 3, marked with ①. The Relative Perplexity of execution tokens (grey curves) plummets in the early stages of training before flattening (column 1). This shows the model rapidly becomes confidently correct in its procedural steps. This is reinforced by the Token Entropy graph (column 2), where entropy for execution tokens is consistently and significantly lower than for planning tokens. The model is not just confident; it actively reduces exploration of procedural alternatives to converge on reliable operations. This rapid mastery of the basics is the first learning frontier to be solved. Takeaway 1. Procedural consolidation is often marked by sharp decrease in the perplexity and token entropy of execution tokens. The model quickly builds reliable toolbox of procedural skills, allowing the primary frontier for performance improvement to shift to high-level strategy. We indeed find that the phase of procedural consolidation might be absent or shot in models with stronger capacity, as evident in MiMO-VL-Instruct and Qwen-4B-Instruct. We refer the reader to check the full analysis of training dynamics across six models in the appendix. This leads us to the belief that the primary driver of RL is indeed the exploration of strategic planning. 2.2.2 STEERING THE SKILLS WITH STRATEGIC PLANNING Once the model becomes procedurally reliable, its performance gains are primarily driven by its ability to explore and deploy diverse set of high-level strategies. To track this shift, we analyze the planning tokens using two key metrics. We compute the Semantic Entropy of strategic grams he Shannon Entropy of the frequency distribution of strategic grams to quantify the diversity of the models high-level strategic plans. To isolate procedural variety, we also compute the conditional"
        },
        {
            "title": "Work in progress",
            "content": "entropy of subsequent procedural n-grams given preceding strategic gram. This second metric measures how varied is the subsequent procedural steps for preceding strategic move. The third column of Figure 3 provides clear evidence of this strategic exploration phase. The semantic entropy of strategic grams (red line, marked with ②) shows distinct and steady increase. This indicates that the model is not converging on single optimal strategy but is instead actively expanding its repertoire of strategic plans. This observation is critical: mastery in reasoning, in this context, is achieved by developing rich and varied strategic playbook, which contrasts sharply with the sharp decrease in token-level entropy seen during the initial procedural consolidation phase. This strategic diversification provides the most direct evidence for our thesis: the model isnt just getting better at executing plans; its getting better at planning itself. While the model explores new high-level strategic moves, the conditional entropy of procedural grams (grey line) remains stable. This suggests that once procedural skill like arithmetic is mastered, there is little incentive to find diverse ways to perform it. The improved reasoning performance comes from discovering new ways to combine these established skills, which is the core function of strategic planning. Crucially, this expansion of the strategic playbook directly correlates with tangible performance gains. The fourth column shows that the rise in strategic diversity is accompanied by parallel increase in the length of reasoning chains and sustained boost in overall accuracy. This demonstrates that after procedural skills are consolidated, the development of strategic planning becomes the primary bottleneck and driver for advanced reasoning performance. Takeaway 2. Once payoff from procedural consolidation diminishes, performance gains are driven primarily by exploring high-level strategies. This is marked by the increasing semantic diversity of strategic grams, which correlate with sustained reasoning enhancement, length scaling, and represents the key learning frontier. Explaining Puzzling Phenomena. tion for previously observed behaviors. This emergent reasoning hierarchy provides unified explanaAha moments are the behavioral signature of the model discovering, mastering, and reinforcing new, powerful strategy or set of strategic constructs. Length-scaling is highly consistent with increase in strategic diversity. As Figure 3 shows, the rise in semantic entropy of planning tokens is strongly correlated with an increase in average sequence length. More sophisticated strategies involving planning, case analysis, and selfreflections are mediated by planning tokens and naturally produce longer, more successful reasoning traces. Semantic Entropy: Good Compass for Exploration. The trends in Figure 3 also highlight critical flaw in using aggregate token-level entropy (column 2) to track exploration. Takeaway 3. The aggregate token-level entropy is dominated by the vast majority of low-level execution tokens. As the model become confident in procedural steps, the entropy of these tokens naturally decreases, pulling the global average down. Unluckily, the decrease in token-level entropy sometimes mislead practitioners into the conception of declined exploration. This is incorrect, however, as it contradicts the fact of increasing exploration in strategic plans (semantic entropy of planning tokens) and the improving reasoning performance. We refer interested readers to the appendix for full analysis of training dynamics across six LLM and VLMs in the appendix. Semantic entropy avoids this pitfall by directly measuring diversity at the semantic level of meaningful strategic units. Its trend accurately reflects the expansion of the models strategic playbook, making it more reliable diagnostic tool for tracking genuine exploration and predicting sustained performance improvements. In Section 4.4, we showcase how semantic entropy avoids the flaws in token entropy and complements Pass@K metric with further benefits."
        },
        {
            "title": "3 HICRA: HIERARCHY-AWARE CREDIT ASSIGNMENT",
            "content": "Our empirical analysis reveals fundamental insight into how Large Language Models (LLMs) learn to reason via Reinforcement Learning (RL): the learning process is characterized by dynamic shift in its learning bottleneck. Initially, the model is constrained by procedural correctness, but as it masters these foundational skills, the frontier for performance improvement shifts to the exploration and mastery of high-level strategic planning. This observation exposes core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically across all tokens. Such methods fail to concentrate learning where it matters most on the emergent strategic bottleneck. To address this, we propose an algorithm designed to focus the models learning capacity on the sparse, high-impact planning tokens that orchestrate successful reasoning trace. Formulation. We introduce Hierarchy-Aware Credit Assignment (HICRA), an algorithm that builds upon the GRPO framework to allocate credit based on the reasoning hierarchy. In GRPO, given query from dataset D, the policy πθ generates set of output trajectories {o1, . . . , oG}. The advantage for token oi,t at timestep in trajectory oi is the group-normalized reward: ˆAi,t = R(q, oi) 1 (cid:88) j=1 R(q, oj) HICRA, pronounced high-krah, modifies this advantage to prioritize planning tokens. Let Si be the set of indices corresponding to planning tokens within trajectory oi, identified using the method in Section 2.1. We define the HICRA advantage as: ˆAHICRA i,t = (cid:40) ˆAi,t + α ˆAi,t ˆAi,t if Si if / Si where α (0, 1) is hyperparameter controlling the amplification intensity (we use α = 0.2 in our experiments). This formulation creates clear learning hierarchy: for successful trajectories ( ˆAi,t > 0), it amplifies the credits for planning tokens, while for unsuccessful ones ( ˆAi,t < 0), it dampens their penalty. The resulting RL objective and its policy gradient (simplified without PPO clipping) are: (θ) = EqD,oiπθ (cid:104) ˆAHICRA i,t (cid:105) , (θ) = (cid:104) ˆAHICRA i,t (cid:105) log πθ(oi,tq, oi,<t) By translating the amplified advantage into stronger policy gradient, HICRA directly focuses the models optimization on the strategic elements of its reasoning process. Connection to Strategic Exploration. The core mechanism of HICRA engineers more effective exploration by reshaping the policy updates target distribution. standard policy gradient (Williams, 2004) update nudges the policy πθold toward an implicit target distribution π defined by the advantage function described as follows (the derivation is included in the appendix): π(oi,tq, oi,<t) πθold (oi,tq, oi,<t) exp( ˆAi,t) Typically, this update pressure is applied isotropically, affecting all token types uniformly. HICRA breaks this symmetry. By using the modified advantage ˆAHICRA, it creates new target distribution, π HICRA, that is anisotropically stretched toward the strategic dimensions of the action space. This new target distribution places significantly greater probability mass on planning tokens (through the term exp( ˆAi,t)), particularly those within high-reward trajectories. This anisotropic reshaping fosters potent virtuous feedback loop: (a) the policy is incentivized to explore the subspace of strategic plans more thoroughly; (b) this leads to the faster discovery of effective reasoning patterns; and (c) when these strategies yield high rewards, the amplified advantage ensures they are strongly reinforced, cementing the models planning capabilities far more efficiently. We also validate the effects of HICRA in exploration through extensive experiments."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "To validate our hypothesis that focusing optimization on the strategic learning bottleneck enhances reasoning, we conduct comprehensive set of experiments. We evaluate our proposed method, Hierarchy-Aware Credit Assignment (HICRA), against strong baselines on suite of complex reasoning benchmarks, followed by in-depth analyses to dissect the underlying learning dynamics."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Models Our experiments are conducted on four open-source model families, including Qwen2.57B (Yang et al., 2024), Qwen3-4B (Yang et al., 2025), LLama-3.1-8B (Grattafiori et al., 2024), Qwen2.5-VL-7b (Yang et al., 2024) and MiMO-VL-7B (Xiaomi, 2025). We use both base and instruction-tuned variants to ensure our findings are consistent and test generalization to visionlanguage models. Training Datasets and Benchmarks The training dataset for LLM reasoning is sourced from DAPO (Yu et al., 2025) and DeepScaleR (Luo et al., 2025). The dataset for training VLM is sourced from ViRL39K (Wang et al., 2025c). We evaluate all models on diverse set of challenging mathematical reasoning benchmarks to rigorously test their complex reasoning capabilities. The text-only benchmarks include AIME24, AIME25 (Mathematical Association of America, 2024), Math500 (Lightman et al., 2023), AMC23, Minerva (Lewkowycz et al., 2022), and Olympiad (He et al., 2024). We follow Deepseek R1 (Guo et al., 2025)s evaluation protocol, where the performance is measured by Pass@1 Accuracy with temperature 0.6 sampling. For benchmarks with less than 100 queries, we use average accuracy of 32 samplings on AIME24/25 and 4 samplings on AMC23 (Yu et al., 2025). Following VL-Rethinker (Wang et al., 2025c), we evaluate on MathVista (Lu et al., 2023), MathVerse (Zhang et al., 2024), MathVision (Wang et al., 2024), and EMMA (Hao et al., 2025) for assessing multimodal reasoning across domains and disciplines. For all evaluation, we adopt strict answer matching that relies on the boxed format. Baselines and Implementation. We compare HICRA against three primary baselines: the Base model (before RL), the widely adopted GRPO baseline with clip-higher (Yu et al., 2025) by default, and Entropy Regularization: GRPO with an additional regularization loss on token-level entropy (Cheng et al., 2025). For HICRA, we set the amplification hyperparameter α to 0.2 and identify planning tokens using the Strategic Grams (SGs) methodology detailed in Section 2.1. For all experiments, we increase the training context length from 16K to 32K when the response clip rate exceeds 20% (Luo et al., 2025). For the specific experiments on Llama-3.1-Instruct, we add dynamic filtering mechanism (Yu et al., 2025) based on GRPO Clip-Higher due to significant vanishing advantanges (Wang et al., 2025c). 4.2 MAIN RESULTS Our primary results, summarized in Table 1, Table 2, show that HICRA consistently and outperforms both the GRPO baselines across text-only models and vision-language models on various benchmarks. On the strongest base model, Qwen3-4B-Instruct, HICRAs gains demonstrate that even on highly capable models, selectively amplifying the learning signal for strategic reasoning yields substantial improvements. This trend holds for non-instruct-tuned models as well, providing strong empirical evidence for our central claim: by identifying and focusing on the emergent strategic bottleneck, HICRA accelerates the development of advanced reasoning abilities more efficiently than agnostic methods. 4.3 ANALYSIS OF RLS IMPACT ON REASONING To validate our hypothesis and the design of HICRA, we conduct series of analyses to dissect how RL improves reasoning. Our analysis follows clear logical thread: first, we have linked strategic planning to reasoning through analyses of the training dynamics in Section 2.2; second, we verify the key effects of RL by showing the frequency of different errors throughout training (Section 4.3.1); we then justify the effectiveness of HICRA in exploration by comparing with standard entropy-regularzed baselines. Finally, we examine the limitations of our approach."
        },
        {
            "title": "Work in progress",
            "content": "Table 1: Comparison of HICRA, GRPO, and Base models across various mathematical reasoning benchmarks. HICRA consistently outperforms all baselines across different base models, demonstrating the effectiveness of focusing optimization on strategic planning tokens. Model AIME24 AIME25 Math500 AMC23 Minerva Olympiad Qwen3-4B-Instruct-2507 Base GRPO HICRA (HICRA - GRPO) Qwen3-4B-Adaptive (No-Think) Base GRPO HICRA (HICRA - GRPO) Qwen3-4B-Base Base GRPO HICRA (HICRA - GRPO) Llama-3.1-8B-Instruct Base GRPO HICRA (HICRA - GRPO) Qwen2.5-7B-Base Base GRPO (Guo et al., 2025) ORZ (Hu et al., 2025) SimpleRL (Zeng et al., 2025) Entropy Regularization HICRA (HICRA - GRPO) 63.4 68.5 73.1 +5.4 21.3 63.1 65.9 +2.8 9.4 24.9 31.0 +6.1 4.2 8.9 8.3 -0.6 3.5 16.3 18.8 16.7 16.0 18.8 +2. 47.7 60.0 65.1 +5.1 18.1 58.8 62.1 +3.3 5.3 23.8 27.6 +3.8 0.6 0.5 0.8 +0.3 1.7 11.4 14.8 3.3 9.3 14.8 +3.4 94.6 96.2 97.2 +1. 84.4 95.6 95.8 +0.2 63.8 83.0 89.0 +6.0 50.2 53.0 54.8 +1.8 55.6 77.6 80.2 78.2 57.4 80.2 +2.6 86.7 88.5 90.2 +1.7 60.5 76.8 82.5 +5. 38.9 51.2 54.0 +2.8 17.1 25.0 27.1 +2.1 46.9 46.7 52.4 42.8 50.3 55.1 +8.4 45.2 50.0 50.7 +0.7 40.4 45.2 46.3 +1.1 28.3 38.9 42.5 +3. 20.9 27.2 25.8 -1.4 30.9 36.8 39.7 34.9 33.1 38.6 +1.8 72.4 72.7 72.0 -0.7 49.9 55.6 59.7 +4.1 30.7 45.8 48.1 +2.3 13.7 20.3 21.2 +0. 25.9 41.9 45.9 38.2 40.6 45.9 +4.0 Table 2: Comparison of HICRA, GRPO on multimodal reasoning benchmarks. VLM MathVista MathVision MathVerse EMMA MiMO-VL-Instruct-2508 Base GRPO HICRA (HICRA - GRPO) Qwen2.5-VL-7B-Instruct Base GRPO HICRA (HICRA - GRPO) 77.0 73.7 80.7 +7.0 66.6 70.8 71.4 +0.6 42.9 42.8 48.9 +6.1 23.6 25.8 28.7 +2. 61.8 63.0 65.4 +2.4 45.9 48.8 48.2 -0.6 36.3 41.9 44.1 +2.2 22.3 31.8 33.0 +1.2 4.3.1 MASTERY OF STRATEGIC PLANNING UNLOCKS IMPROVED REASONING DURING RL To understand where RL applies the most leverage, we analyzed the evolution of error types in failed rollouts. We first manually reviewed failures and nominated four distinct error causes. GPT-4o was then prompted to classify each failure into one of these causes via multiple-choice question. Finally, we parsed these classifications into two broader categories: Planning & Strategy (e.g., flawed logic, incorrect high-level plan) and Others (e.g., calculation mistakes, fact-retrieval errors). The prompt used is included in the appendix."
        },
        {
            "title": "Work in progress",
            "content": "Figure 4: Training Dynamics of Error Types. Across all models, the number of Planning & Strategy errors (red) decreases more significantly than other procedural errors (gray), indicating that RLs primary benefit comes from correcting high-level strategic faults. Figure 4 reveals consistent pattern: the primary benefit of RL stems from fixing high-level strategic faults. Across all models, the reduction in strategic errors is more pronounced than the reduction in other errors. This pattern is especially illuminating for Qwen2.5-7B-Base, where non-planning errors does not decrease. We conjecture that while the model may be improving its procedural reliability, these low-level enhancements do not translate to correct answers because the high-level strategy remains the limiting factor. perfectly executed incorrect plan will still result in failure. This evidence strongly supports our claim that the strategic bottleneck is the key to unlocking advanced reasoning. RL preferentially corrects these high-level faults over low-level execution mistakes, as improving strategic planning provides the most direct path to solving complex problems. 4.3.2 JUSTIFYING HICRA: TARGETED VS. INDISCRIMINATE EXPLORATION Our findings suggest that performance gains are driven by mastering high-level strategic planning, which motivates HICRAs design to concentrate learning on planning tokens. As shown in Figure 5, HICRAs success is linked to its ability to sustain higher level of semantic entropy than GRPO. This heightened diversity in high-level strategies directly correlates with stronger and more stable validation accuracy, confirming that focused strategic exploration is primary driver of reasoning improvements. Figure 5: HICRA (red) consistently achieves higher semantic entropy than the GRPO baseline (gray), indicating more diverse strategic exploration. Figure 6: HICRA vs. Entropy Regularization on Qwen2.5-7B-Base. While entropy regularization increases token-level entropy, it fails to consistently improve accuracy and leads to uncontrolled length scaling. In contrast, HICRA boosts semantic entropy, which strongly correlates with validation accuracy, demonstrating the superiority of targeted strategic exploration. To further validate this, we compared HICRA against an entropy-regularized baseline. This baseline adds (upon GRPO) an entropy regularization loss applied to all tokens uniformly. The results in Figure 6 show that promoting token-level entropy for sampling diverse tokens is counterproductive."
        },
        {
            "title": "Work in progress",
            "content": "The entropy regularization baseline successfully increases Token Entropy, but this fails to translate into performance gains; its Validation Accuracy stagnates and is the lowest of the three methods. This is because indiscriminately promoting token-level diversity only encourages non-productive verbosity on the vast majority of low-level tokens. In contrast, HICRA achieves significantly higher Semantic Entropy, targeted boost in the diversity of strategic plans that strongly correlates with its superior validation accuracy. This demonstrates that the key to enhanced reasoning is not just to explore, but to focus exploration on the strategic portion of the action space."
        },
        {
            "title": "4.3.3 HICRA’S LIMITATION: DEPENDENCY ON A PROCEDURAL FOUNDATION",
            "content": "HICRAs effectiveness is predicated on key assumption: that the base model should readily possess reasonable foundation for low-level procedural correctness. As shown in Figure 7, when this foundation is lacking as observed with Llama-3.1-Instruct HICRA can fail to provide an advantage over GRPO. Seen from the semantic entropy graph, there is reverse trend between GRPO and HICRA, implying an opposite training focus on planning tokens and execution tokens. HICRAs enforced strategic exploration becomes counterproductive if the model cannot reliably execute the plans it generates, leading to unstable learning dynamics and learning effects observed on Llama-3.1. This suggests that HICRA is most effective when applied to models that have already achieved degree of procedural reliability, highlighting an important dependency for future work on more adaptive, model-aware hierarchical methods. Figure 7: HICRA on Llama-3.1-Instruct-8B. 4.4 SEMANTIC ENTROPY: COMPASS FOR STRATEGIC EXPLORATION Figure 8: Training Dynamics on MiMO-VL-Instruct-7B. This experiment highlights that token entropy can collapse while semantic entropy remains high and predictive of validation accuracy. Furthermore, while Pass@8 saturates and is indistinguishable between methods, semantic entropy reveals persistent exploration advantage for HICRA that translates to better final performance. Given the crucial role of strategic exploration in unlocking reasoning performance during RL, effectively measuring it accurately is paramount. We find that semantic entropy offers distinctive benefits than common alternatives such as token-level entropy or Pass@K (Chen et al., 2021). Limitations of Token Entropy and Pass@K. As shown in Figure 8 for MiMO-VL-7B, token-level entropy collapses for both HICRA and GRPO, simply because the vast majority of low-level tokens are doomed to become certain, thus pulling the average token entropy down. However, this decrease in token entropy might mislead researchers to suggest that exploration has ceased. Similarly, the Pass@8 (Training) metric quickly saturates, rendering it useless for distinguishing the ongoing learning dynamics. In the same experiment, semantic entropy tells more Semantic Entropy as the Differentiator. accurate story. It remains high, indicating continued exploration of diverse reasoning strategies. Crucially, HICRA consistently maintains higher semantic entropy than GRPO, and this advantage directly correlates with its superior final validation accuracy. This also demonstrates the generality of our approach, extending effectively to multimodal reasoning tasks on vision-language models like MiMO-VL-7B."
        },
        {
            "title": "4.5 PLANNING TOKENS VS. HIGH-ENTROPY ”FORK” TOKENS",
            "content": "Figure 9: Planning Tokens vs. High-Entropy Tokens. (Left) majority of our functionally-defined planning tokens are also high-entropy (top 30%). (Right) However, the reverse is not true; most high-entropy tokens are not planning tokens. Recent work has proposed high-entropy tokens, sometimes called fork tokens, to imply its role as proxies for decision points in reasoning trace (Wang et al., 2025d). Our analysis investigates the relationship between our functionally-defined planning tokens and this entropy-based definition. Figure 9 reveals crucial asymmetry. While majority of planning tokens exhibit high entropy (aligning with their role as points of strategic choice), the reverse is not true: most high-entropy tokens are not planning tokens. This finding highlights the limitations of using high entropy as standalone proxy for strategic function. High token-level entropy ensures sampling diversity, but it does not guarantee semantic function. Many high-entropy tokens may correspond to variations in phrasing or calculation that do not alter the high-level reasoning path. In contrast, our approach identifies tokens based on their functional role in orchestrating the solution, providing more direct and reliable signal for strategic credit assignment. Figure 10: Planning Tokens, High-Entropy Tokens and Shared Tokens are highlighted with different colors. This concrete example suggests how these two definitions differ: Planning Tokens function as strategic skeletons of reasoning solution and are thus sparse, with more than half of these semantic units also having higher entropy. In contrast, majority of high-entropy tokens only exhibits high-variations in its phrasing, spreading across low-level executions and high-level planning. Fig. 9 reveals that less than 10% high-entropy tokens serve the semantic function of planning."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Reinforcement Learning for LLM Reasoning. The application of Reinforcement Learning (RL) has been pivotal in enhancing the complex reasoning abilities of Large Language Models (LLMs). Seminal work by Ouyang et al. demonstrated the effectiveness of learning from human feedback to align models with user instructions. More recently, algorithms like Group Reward Policy Optimization (Guo et al., 2025) have been developed to specifically incentivize reasoning capabilities in LLMs,VLMs, Agents (Liu et al., 2025b; Yu et al., 2025; Team et al., 2025; Liu et al., 2025a; Wang et al., 2025c;b; Su et al., 2025; Dai et al., 2025; Zheng et al., 2025), leading to significant performance gains on downstream performance. While these methods have proven empirically successful, they typically apply optimization pressure agnostically across all generated tokens, without distinguishing between different functional roles within the reasoning process. Our work builds on this foundation but introduces more targeted approach by focusing on the emergent reasoning hierarchy. Analysis of RL Dynamics and Exploration in LLMs. growing body of research seeks to understand the complex learning dynamics that occur during the RL fine-tuning of LLMs. Several studies have investigated the role of token-level entropy, observing intricate patterns and its connection to model exploration and uncertainty (Cui et al., 2025; Chen et al., 2025). Concurrently, phenomena such as sudden aha moments and performance improvements from longer outputs (length-scaling) have been noted as characteristic but poorly understood outcomes of RL training (Guo et al., 2025; Liu et al., 2025b).Our paper provides unifying framework, interpreting these phenomena as evidence of shift from procedural learning to strategic planning. Furthermore, recent work has identified high-entropy fork tokens as potential proxies for critical decision points in reasoning (Wang et al., 2025d). Our work distinguishes itself by defining planning tokens based on their semantic function. We also validate the limitation of identifying crucial tokens solely based on entropy. Exploration-Exploitation trade-off has become long-standing research problem in classical RL literature. Among the vast literature, Entropy Regularization or Maximum-Entropy RL (Levine, 2018; Haarnoja et al., 2017; Wang et al., 2023) is standard technique to encourage exploration that can be seamlessly integrated with LLM RL training. Hierarchical Reasoning and Cognition. The concept of hierarchical processing is cornerstone of cognitive neuroscience, which posits that the human brain separates high-level, abstract planning from low-level motor or procedural execution (Huntenburg et al., 2018; Murray et al., 2014; Zeraati et al., 2023; Zhu et al., 2025; Xiong et al., 2025). HRM (Wang et al., 2025a) is inspired this cognitive architecture to design specific neural architecture for hierarchical reasoning. Concurrently, this cognitive model provides compelling parallel to the functional hierarchy we identify in RL-tuned LLMs, proposing that LLMs similarly develop functional separation between strategic planning and procedural execution."
        },
        {
            "title": "6 CONCLUSION AND PROMISING DIRECTIONS",
            "content": "This work tackles the opacity of how Reinforcement Learning enhances the complex reasoning abilities of LLMs. We move beyond treating the learning process as monolith, uncovering coherent, two-phase dynamic: an initial consolidation of low-level procedural skills followed by decisive shift where performance gains are driven by the exploration and mastery of high-level strategic planning. This emergent reasoning hierarchy provides unified framework that explains puzzling phenomena like aha moments, length-scaling, and complex entropy dynamics. Our core insight that strategic bottleneck is the primary driver for advanced reasoning led to the development of HICRA (Hierarchy-Aware Credit Assignment). By concentrating optimization pressure on functionally-defined planning tokens, HICRA significantly outperforms strong baselines like GRPO, demonstrating that targeted credit assignment is more efficient than agnostic approaches. Furthermore, we validate semantic entropy as far more reliable compass for measuring strategic exploration than misleading aggregate metrics like token-level entropy. Our findings open several promising avenues for future research, suggesting paradigm shift in how we approach RL for language models."
        },
        {
            "title": "Work in progress",
            "content": "Rethinking Action in RL for LMs: Our analysis indicates that the token, as the fundamental unit of action, is too granular and often lacks semantic weight. single token rarely determines the subsequent logical direction of complex thought process. Future work should explore higher-level action space, where an action could be semantic concept, reasoning step (e.g., apply theorem X), or complete phrasal unit (n-gram). This would align the optimization process more closely with the cognitive steps of reasoning and could lead to more stable and efficient learning. Dynamically Adaptive Hierarchical Methods: HICRA focused learning efforts on the strategic bottleneck. more powerful approach should be capable of diagnosing the current learning bottleneck, which can adaptively shift optimization pressure between procedural execution and strategic planning based on real-time diagnostics of the models weaknesses, perhaps through curriculum learning framework or meta-learning. Finer-Grained, Process-Oriented Rewards: This work justifies move away from sparse, outcomebased rewards (correct vs. incorrect final answer) towards process-oriented reward models. Such models could provide fine-grained credit assignment by, for example, rewarding brilliant strategic choice even if it is marred by minor calculation error. This decouples the evaluation of planning from execution, allowing the model to learn both skills more effectively. Generalizing the Reasoning Hierarchy Beyond Mathematics: The functional hierarchy between planning and execution is likely universal feature of complex, multi-step generation. Investigating whether similar dynamics emerge in domains like code generation , agentic tool-use, long-form generation, etc, could yield novel, domain-specific optimization strategies."
        },
        {
            "title": "REFERENCES",
            "content": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Minghan Chen, Guikun Chen, Wenguan Wang, and Yi Yang. Seed-grpo: Semantic entropy enhanced grpo for uncertainty-aware policy optimization, 2025. URL https://arxiv.org/abs/ 2505.12346. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Runpeng Dai, Tong Zheng, Run Yang, Kaixian Yu, and Hongtu Zhu. R1-re: Cross-domain relation extraction with rlvr. arXiv preprint arXiv:2507.04642, 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International conference on machine learning, pp. 13521361. PMLR, 2017."
        },
        {
            "title": "Work in progress",
            "content": "Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Openreasoner-zero: An open source approach to scaling reinforcement learning on the base model. https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero, 2025. Julia Huntenburg, Pierre-Louis Bazin, and Daniel Margulies. Large-scale gradients in human cortical organization. Trends in cognitive sciences, 22(1):2131, 2018. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Che Liu, Haozhe Wang, Jiazhen Pan, Zhongwei Wan, Yong Dai, Fangzhen Lin, Wenjia Bai, Daniel Rueckert, and Rossella Arcucci. Beyond distillation: Pushing the limits of medical llm reasoning with minimalist rule-based rl. arXiv preprint arXiv:2505.17952, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://github.com/agentica-project/ deepscaler, 2025. Mathematical Association of America. American invitational mathematics examination (aime), 2024. URL https://maa.org/maa-invitational-competitions/. John Murray, Alberto Bernacchia, David Freedman, Ranulfo Romo, Jonathan Wallis, Xinying Cai, Camillo Padoa-Schioppa, Tatiana Pasternak, Hyojung Seo, Daeyeol Lee, et al. hierarchy of intrinsic timescales across primate cortex. Nature neuroscience, 17(12):16611663, 2014. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
        },
        {
            "title": "Work in progress",
            "content": "Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, and Yasin Abbasi Yadkori. Hierarchical reasoning model. arXiv preprint arXiv:2506.21734, 2025a. Haozhe Wang, Chao Du, Panyan Fang, Li He, Liang Wang, and Bo Zheng. Adversarial constrained bidding via minimax regret optimization with causality-aware reinforcement learning. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 23142325, 2023. Haozhe Wang, Long Li, Chao Qu, Fengming Zhu, Weidi Xu, Wei Chu, and Fangzhen Lin. To code or not to code? adaptive tool integration for math language models via expectation-maximization. arXiv preprint arXiv:2502.00691, 2025b. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025c. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025d. Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229256, 2004. URL https://api.semanticscholar. org/CorpusID:2332513. LLM-Core-Team Xiaomi. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/ 2506.03569. Feng Xiong, Hongling Xu, Yifei Wang, Runxi Cheng, Yong Wang, and Xiangxiang Chu. Hs-star: Hierarchical sampling for self-taught reasoners via difficulty estimation and budget reallocation. arXiv preprint arXiv:2505.19866, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog. Roxana Zeraati, Yan-Liang Shi, Nicholas Steinmetz, Marc Gieselmann, Alexander Thiele, Tirin Moore, Anna Levina, and Tatiana Engel. Intrinsic timescales in the visual cortex change with selective attention and reflect spatial connectivity. Nature communications, 14(1):1858, 2023."
        },
        {
            "title": "Work in progress",
            "content": "Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186, 2024. Tong Zheng, Lichang Chen, Simeng Han, Thomas McCoy, and Heng Huang. Learning to reason via mixture-of-thought for logical reasoning. arXiv preprint arXiv:2505.15817, 2025. Xiaomeng Zhu, Yuyang Li, Leiyao Cui, Pengfei Li, Huan-ang Gao, Yixin Zhu, and Hao Zhao. Afford-x: Generalizable and slim affordance reasoning for task-oriented manipulation. arXiv preprint arXiv:2503.03556, 2025."
        },
        {
            "title": "A FULL TRAINING DYNAMICS",
            "content": "Figure 11: Training Dynamics across different LLMs and VLMs. Following the discussion in the main paper, we make the following further observations based on the provided training charts: For some models, the initial skill-consolidation phase is brief or absent. In the cases of the Vision-Language Models (Qwen2.5 VL-Instruct and MiMO VL-Instruct) and Qwen3 4B-Instruct, the exploration of strategic planning begins almost immediately at the start of training. This is evidenced by significant and immediate rise in the semantic entropy of strategic grams, which occurs in tandem with rapid boost in validation accuracy. We conjecture this is because: (a) for the VL scenarios, publicly available datasets is learned quickly by state-of-the-art models (Wang et al., 2025c); (b) strong base models like Qwen3 4B-Instruct already possess solid foundation of"
        },
        {
            "title": "Work in progress",
            "content": "low-level skills and primarily need to adapt to formatting before focusing on higher-level strategic planning. Token-level entropy does not directly correlate with model accuracy. This is strongly supported across multiple experiments. For instance, with Llama3.1 8B, Qwen3 4B-Instruct, and the VL models, token-level entropy either remains flat or decreases throughout training. During the same period, however, validation accuracy shows steady and significant increase. This demonstrates clear disconnect between next-token uncertainty and overall task performance. Token-level entropy is misleading for policy exploration. This observation holds true across all experiments. The Qwen3 4B-Instruct model offers particularly stark example: its token-level entropy remains almost perfectly flat, while its semantic entropy (diversity of strategic grams) consistently increases throughout training. This contrast highlights that the variety of semantic structures model learns is completely different from the statistical uncertainty of its next-token predictions. The core difference is about scale: token-level entropy measures the uncertainty of every next-token, including low-level tokens such as formatting, executions that are doomed to become confident throught training. In contrast, semantic entropy measures the diversity of the overall ideas being expressed. model can be very predictable in its next-token choice under given context but still create wide variety of different arguments or structures."
        },
        {
            "title": "B THE DISTRIBUTION MATCHING PERSPECTIVE OF POLICY GRADIENTS",
            "content": "Imagine an ideal, or target, policy, π(as), that we want our current policy, πθ(as), to emulate. We can conceptualize this target distribution as being proportional to the exponentiated advantage of the actions: π(as) πθold (as) exp( ˆA(a, s)) Or more concretely, π(as) = 1 Z(s) πθold(as) exp (cid:33) (cid:32) ˆA(a, s) β This target policy, π(as), re-weights the old policy based on the advantage of each action. Here, actions with positive advantage ( ˆA > 0) get their probability boosted exponentially, while actions with negative advantage ( ˆA < 0) get their probability suppressed. The term β acts as temperature parameter. The goal is to find new policy, πθ, that is as close as possible to this ideal target distribution, π. This is equivalent to minimizing the KL divergence: KL(π(as)πθ(as)) min θ (3) Expanding this KL divergence term: KL(πθπ) = Eaπθ (cid:21) (cid:20) log πθ(as) π(as) = Eaπθ [log πθ(as) log π(as)] Substitute our definition of log π(as) = log πθold (as) + ˆA(a,s) β log Z(s): (cid:32) log πθ(as) log πθold (as) + = Eaπθ (cid:34) (cid:34) Eaπθ (log πθ(as) log πθold(as)) = 1 β Eaπθ (cid:104) (cid:105) βKL(πθπθold) ˆA(a, s) (cid:33)(cid:35) log Z(s) (cid:35) ˆA(a, s) β ˆA(a, s) β 19 (1) (2)"
        },
        {
            "title": "Work in progress",
            "content": "Minimizing this is equivalent to maximizing its negative: max θ Eaπθ (cid:104) ˆA(a, s) βKL(πθπθold) (cid:105) (4) This expression is nearly identical to the PPO-KL objective, where the policy update is constrained using KL divergence regularizer (Schulman et al., 2017). Therefore, the PPO objective is essentially solving distribution matching problem toward target distribution shaped by the advantage function."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Imperial College London",
        "M-A-P, Tsinghua University",
        "UCAS",
        "University of Waterloo"
    ]
}