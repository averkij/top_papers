{
    "paper_title": "Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset",
    "authors": [
        "Qingyan Bai",
        "Qiuyu Wang",
        "Hao Ouyang",
        "Yue Yu",
        "Hanlin Wang",
        "Wen Wang",
        "Ka Leong Cheng",
        "Shuailei Ma",
        "Yanhong Zeng",
        "Zichen Liu",
        "Yinghao Xu",
        "Yujun Shen",
        "Qifeng Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-based video editing promises to democratize content creation, yet its progress is severely hampered by the scarcity of large-scale, high-quality training data. We introduce Ditto, a holistic framework designed to tackle this fundamental challenge. At its heart, Ditto features a novel data generation pipeline that fuses the creative diversity of a leading image editor with an in-context video generator, overcoming the limited scope of existing models. To make this process viable, our framework resolves the prohibitive cost-quality trade-off by employing an efficient, distilled model architecture augmented by a temporal enhancer, which simultaneously reduces computational overhead and improves temporal coherence. Finally, to achieve full scalability, this entire pipeline is driven by an intelligent agent that crafts diverse instructions and rigorously filters the output, ensuring quality control at scale. Using this framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of one million high-fidelity video editing examples. We trained our model, Editto, on Ditto-1M with a curriculum learning strategy. The results demonstrate superior instruction-following ability and establish a new state-of-the-art in instruction-based video editing."
        },
        {
            "title": "Start",
            "content": "SCALING INSTRUCTION-BASED VIDEO EDITING WITH HIGH-QUALITY SYNTHETIC DATASET Qingyan Bai1,2, Qiuyu Wang2, Hao Ouyang2, Yue Yu1,2, Hanlin Wang1,2, Wen Wang2,3, Ka Leong Cheng2, Shuailei Ma2,4, Yanhong Zeng2, Zichen Liu1,2, Yinghao Xu2, Yujun Shen2, Qifeng Chen1 1HKUST 3Zhejiang University 2Ant Group 4Northeastern University 5 2 0 O 7 1 ] . [ 1 2 4 7 5 1 . 0 1 5 2 : r Figure 1: Our proposed synthetic data generation pipeline can automatically produce high-quality and highly diverse video editing data, encompassing both global and local editing tasks. We highly recommend the readers to see the supplementary video samples."
        },
        {
            "title": "ABSTRACT",
            "content": "Instruction-based video editing promises to democratize content creation, yet its progress is severely hampered by the scarcity of large-scale, high-quality training data. We introduce Ditto, holistic framework designed to tackle this fundamental challenge. At its heart, Ditto features novel data generation pipeline that fuses the creative diversity of leading image editor with an in-context video generator, overcoming the limited scope of existing models. To make this process viable, our framework resolves the prohibitive cost-quality trade-off by employing an efficient, distilled model architecture augmented by temporal enhancer, which simultaneously reduces computational overhead and improves temporal coherence. Finally, to achieve full scalability, this entire pipeline is driven by an intelligent agent that crafts diverse instructions and rigorously filters the output, ensuring quality control at scale. Using this framework, we invested over 12,000 GPUdays to build Ditto-1M, new dataset of one million high-fidelity video editing examples. We trained our model, Editto, on Ditto-1M with curriculum learning strategy. The results demonstrate superior instruction-following ability and establish new state-of-the-art in instruction-based video editing. The data, model, and code can be found at the project page."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, the field of visual generative models has witnessed remarkable divergence: while instruction-based image editing has achieved unprecedented levels of precision and user-friendliness with models like InstructPix2Pix (Brooks et al., 2023), FLUX.1 Kontext (Batifol et al., 2025), QwenImage (Wu et al., 2025a), and Gemini 2.5 Flash Image (Nano-Banana) (Google, 2025), its video counterpart has lagged significantly behind. This growing capabilities gap stems from the inherent complexities of the temporal dimension. Editing video requires not only modifying content but also ensuring these changes are propagated coherently across framesa challenge that has proven formidable. The primary obstacle impeding progress is well-understood but unsolved problem: the profound scarcity of large-scale, high-quality, and diverse paired data for training end-to-end video editing models (Zhang et al., 2025; Yu et al., 2025). Existing works have attempted to address this data scarcity challenge through various synthetic data generation strategies. Earlier approaches either relied on computationally prohibitive per-video optimization methods (Qin et al., 2024) or adopted training-free image-to-video propagation techniques (Yu et al., 2025; Wu et al., 2025b). However, these pipelines suffer from persistent trade-off: they sacrifice editing diversity, temporal consistency, and visual quality for scalability, or vice-versa. scalable, cost-efficient data pipeline that generate high-fidelity results remains an open challenge. To address these shortcomings, we introduce Ditto1, scalable and cost-efficient data synthesis pipeline architected to systematically dismantle these trade-offs. Our approach first tackles the challenge of editing fidelity and diversity. Capitalizing on the advanced maturity of instruction-based image editors, the pipeline generates high-quality edited reference frame that acts as strong visual prior. This anchor frame then guides an in-context video generator (Jiang et al., 2025) to synthesize temporally coherent video that faithfully matches the edit, overcoming the quality limitations of previous methods. Second, to resolve the critical efficiency-coherence trade-off where high-fidelity generation is prohibitively expensive, our pipeline integrates distilled video model with temporal enhancer. This innovative combination reduces computational costs to just 20% of the original while preserving temporal stability and avoiding visual artifacts. Finally, to achieve true scalability and eliminate the bottleneck of manual curation, we deploy an autonomous Vision-Language Model (VLM) agent. This agent carries dual responsibilities: programmatically generating diverse instructions for both local and global edits, and serving as flaw-detection mechanism to automatically filter out low-quality or failed video pairs, ensuring the final datasets integrity. We invested over 12,000 GPU-days using this pipeline to construct Ditto-1M, new largescale dataset comprising over one million source-instruction-edited video triplets, as demonstrated in Fig. 1. The dataset is meticulously structured to cover wide spectrum of editing tasks and is curated via our VLM agent to ensure instruction consistency and high aesthetic quality. With the proposed dataset, we train our final editing model, Editto. To bridge the gap between our visually-guided data synthesis and the goal of purely instruction-driven inference, we propose modality curriculum learning strategy (Bengio et al., 2009). Our curriculum begins by providing the model with both the text instruction and the edited reference image as scaffold. As training progresses, we gradually anneal the visual guidance, compelling the model to learn the more difficult, abstract mapping from text instruction alone. Our contributions are as follows: novel, scalable synthesis pipeline, Ditto, that efficiently generates high-fidelity and temporally coherent video editing data. The Ditto-1M Dataset, million-scale, open-source collection of instruction-video pairs to facilitate community research. state-of-the-art editing model, trained on Ditto-1M, that demonstrates superior performance on established benchmarks. modality curriculum learning strategy that effectively enables visually-conditioned model to perform language-driven editing. 1The name Ditto is chosen to reflect the models core function: making the output video faithful reflection, or ditto, of the users textual instruction."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 INSTRUCTION-BASED IMAGE EDITING Visual generative models have advanced rapidly (Wan et al., 2025; Ma et al., 2025; Rombach et al., 2022; Saharia et al., 2022; Blattmann et al., 2023; Guo et al., 2024; Kong et al., 2024; Song & Dhariwal, 2023; Lin et al., 2025; He et al., 2024). Instruction-based image editing has also rapidly evolved, moving beyond simple text-to-image generation to enable nuanced, user-guided modifications. Early and influential methods like InstructPix2Pix (Brooks et al., 2023) demonstrated the feasibility of fine-tuning diffusion models on generated datasets of image triplets (source image, instruction, edited image) to perform edits. This was achieved by ingeniously combining large language model (GPT-3) to generate textual edit instructions and text-to-image model (Stable Diffusion) to synthesize the corresponding image pairs, creating large-scale training corpus without manual annotation. More recent advancements, particularly with the advent of powerful models like FLUX.1 Kontext (Batifol et al., 2025), Qwen-Image (Wu et al., 2025a), and Gemini 2.5 Flash Image (Nano-Banana) (Google, 2025), have unlocked even more sophisticated capabilities. These models can process both text and reference images as inputs, enabling targeted local edits, robust character consistency across multiple turns, and complex scene transformations within unified architecture, often without requiring fine-tuning. Our work builds upon this progress by integrating state-ofthe-art instruction-based image editor as critical component in our video data synthesis pipeline, using it to manipulate keyframes that guide the subsequent video-level edit. 2.2 INSTRUCTION-BASED VIDEO EDITING Video editing has gained remarkable progress (Yang et al., 2025; Ceylan et al., 2023; Liu et al., 2024; Chai et al., 2023) with the development of the base generative models. Extending instruction-based editing to video requires maintaining temporal consistency and preserving background content. Current approaches fall into two main categories: Inversion-based Methods. These methods avoid paired video-text-edit data but are computationally intensive. Tune-A-Video (Wu et al., 2023) fine-tunes text-to-image model on single video, enabling personalized edits but lacking scalability. Zero-shot techniques like TokenFlow (Geyer et al., 2024) and FateZero (Qi et al., 2023) use DDIM inversion and feature propagation to enforce the consistency of the edited video. However, their quality relies on inversion fidelity and often struggles with complex motion or occlusions. Feed-forward Methods. These end-to-end models aim to overcome inversion-based limitations but face the fundamental challenge of data scarcity. The development of feed-forward approaches is tightly coupled with the creation of synthetic datasets, as large-scale human-annotated video edit data is notoriously scarce. Early works (Qin et al., 2024; Zhang et al., 2024) attempted to bypass this bottleneck by synthesizing data using one-shot tuning methods like Tune-A-Video (Wu et al., 2023) or CoDeF (Ouyang et al., 2024). However, this process was computationally expensive and ultimately limited the scale and quality of the resulting dataset. More recent methods have adopted lift and propagate paradigm for more scalable data generation. VEGGIE (Yu et al., 2025) and InsViE (Wu et al., 2025b), for instance, leverage high-quality image editing datasets by applying an image editor to keyframe and then using an image-to-video model to generate or propagate the edit across the entire video clip. While this strategy improves scalability, the temporal coherence and visual quality of the final output are fundamentally capped by the image-to-video propagation model, which often introduces unnatural motion or identity inconsistencies. These methods also typically rely on training-free inversion or attention control techniques during data synthesis, which may limit editing flexibility. In contrast, our approach introduces data synthesis pipeline centered around an in-context video generator that conditions on both reference edited frame and depthderived motion representation. This allows for more direct and high-quality video synthesis."
        },
        {
            "title": "3 DITTO-1M",
            "content": "Our methodology begins with the construction of large-scale, high-quality dataset. We designed novel, scalable data generation pipeline to synthesize over one million instruction-video triplets, 3 Figure 2: Our scalable data synthesis pipeline. (1) Pre-processing: diverse video pool is curated via automated deduplication and motion filtering. (2) The core engine synthesizes video triplets, conditioning an in-context generator on automated instructions, appearance context from edited key-frames, and structural context from depth maps. (3) Post-processing: Final visual quality is guaranteed by VLM-based filter and denoising enhancer. as in Fig. 2. The architecture of this pipeline was specifically engineered to address four critical challenges inherent to existing data synthesis approaches: 1. Overcoming Limited Editing Diversity and Fidelity. Current data pipelines of instructionbased video editing often rely on training-free inversion techniques (Qin et al., 2024; Zhang et al., 2025), which tend to yield synthetic data of limited quality. To address this, we propose to leverage an in-context video generator to produce high-quality editing samples with visual contexts. Capitalizing on the more advanced development of image-based editing models, we incorporate strong priors from these image editors to serve context and guide the video generation for better editing quality. This is combined with depth-guided video context to ensure spatiotemporal coherence, significantly improving the diversity and fidelity of generated edits. 2. Resolving the Efficiency-Quality Trade-off. major technical hurdle is the trade-off between generation cost and data quality. Current high-fidelity methods are prohibitively expensive (e.g., 50 GPU-minutes per sample on single GPU), while faster, distilled models often introduce artifacts like temporal flickering. Our pipeline is designed with cost-aware workflow that significantly reduces computational overhead without compromising the temporal coherence of the videos. 3. Automating Instruction Generation and Quality Control. To achieve true scalability, manual creation of instructions and verification of outputs is infeasible. Our pipeline integrates an automated agent with two primary responsibilities: (a) programmatically generating diverse and meaningful instructions for both local and global edits, and (b) serving as flaw-detection mechanism to automatically filter out generated pairs that are of low quality or fail to follow the instructions. 4. Ensuring High Aesthetic and Motion Quality. Unlike general-purpose video datasets (e.g., Panda-70M), which are not optimized for editing tasks, our pipeline prioritizes the generation of content with high aesthetic value and natural motion dynamics. This focus ensures the resulting dataset, and the models trained upon it, are well-aligned with real-world usage scenarios where visual appeal is paramount. The following sections will detail the architecture of our data generation pipeline, explaining how each component systematically addresses these challenges. 4 Datasets Amount Resolution Frames InstructVid2Vid (Qin et al., 2024) EffiVED Zhang et al. (2024) InsV2V (Cheng et al., 2024) InsViE (Wu et al., 2025b) Ours N/A 155k 404k 1M 1M N/A 512512 256256 1024 1280720 N/A 8 16 25 101 FPS Real N/A N/A N/A 7 Filter Figure 3: Source categories. Table 1: Comparisons with prior instruction-based datasets."
        },
        {
            "title": "3.1 SOURCE VIDEO FILTERING",
            "content": "The Ditto-1M dataset is built exclusively from high-resolution videos sourced from Pexels (Pexels, 2025), platform for professional-grade footage under the Pexels License. Unlike datasets derived from uncurated web scrapes, this strategy provides foundation of superior aesthetic and technical quality, suitable for video editing tasks. We also first apply rigorous filtering and pre-processing protocol. This protocol examines videos in the following aspects: Near-Duplicate Removal: To prevent dataset redundancy and ensure broad content diversity, we implement rigorous deduplication process. We employ powerful visual encoder (Oquab et al., 2023) to extract compact feature representations for each video. Pairwise similarity between these feature vectors is then computed. Videos exceeding pre-defined similarity threshold are systematically filtered out, guaranteeing the uniqueness of each source video in our collection. Motion Scale: Videos that contain little or no motion over timesuch as fixed-camera surveillance footage, still nature scenes, or unmoving interior shotsare considered less valuable for video editing tasks because they lack dynamic visual changes. To automatically identify such low-dynamic content, we employ tracking-based method that analyzes frame-to-frame motion across the video sequence. Specifically, for each video, we first sample points on grid layout and then use CoTracker3 (Karaev et al., 2024) to track these points, obtaining their trajectories. We then compute the average of the cumulative displacements of all tracked points over the entire video as the motion score of the video. By setting threshold, we filter out videos with low motion scores, effectively removing those with negligible temporal variation. Videos that pass this filtering stage are then standardized. Each video is resized to uniform resolution and its frame rate is converted to 20 FPS. This standardization simplifies the training process and ensures consistency across the entire dataset. 3.2 INSTRUCTION GENERATION For each filtered source video Vs, we generate set of corresponding editing instructions p. We employ powerful VLM (Bai et al., 2025) for this task with two-step prompting strategy. First, we prompt the VLM to generate dense caption that describes the videos content, subjects, and scenery: = VLM(Vs, pcaption). This caption serves as semantic anchor. Next, we feed both the video Vs and its caption back into the VLM, prompting it to devise creative and plausible editing instruction p: (1) = VLM(Vs, c, pinstruct). This conditioned approach ensures that instructions are contextually grounded in the videos content, yielding diverse set of commands ranging from global style transformations to specific, localized object modifications. (2) 3.3 VISUAL CONTEXT PREPARATION Our generation process is heavily guided by rich visual context, which consists of two key components: an edited reference frame that specifies the target appearance, and depth video that enforces spatiotemporal consistency. Key-Frame Editing for Appearance Guidance. We first select key-frame fk from the source video Vs as the anchor for the editing. This frame is then edited by the instruction-guided image editor Eimg (Wu et al., 2025a), using the instruction generated in the previous step: = Eimg(fk, p). (3) 5 This resulting frame including style and textures. k, serves as the visual prototype for the edit, defining the final appearance Depth Video Prediction for Spatiotemporal Structure. To preserve the geometric structure and motion dynamics of the original scene, we extract dense depth video Vd from Vs with video depth predictor (Chen et al., 2025). The predicted depth video acts as dynamic structural scaffold, providing an explicit, frame-by-frame guide for the structure and geometry of the scene during the video generation. 3.4 IN-CONTEXT VIDEO GENERATION With the editing instruction and the multi-modal visual context and Vd prepared, we synthesize the edited video Ve with the in-context video generator (Jiang et al., 2025), which is denoted as G. VACE is feed-forward video generative model designed to condition its generation on rich visual prompts such as images, masks, and videos by learning context branch beyond the base generative model (Wan et al., 2025). In our design, we adopt to synthesize the edited video by taking the textual prompt as high-level semantic guide, the edited key-frame as the primary appearance condition, and the depth video Vd as strict spatiotemporal constraint. This generation process is formulated as: Ve = G(Vd, k, p). (4) By integrating these three modalities with the attention mechanism, VACE can faithfully propagate the edit defined in across the entire sequence, adhering to the motion and structure laid out by Vd, while ensuring the result is semantically aligned with the instruction p. Our pipeline achieves highquality and coherent video edits without costly per-video optimization. Please refer to Section for additional analysis of the data generator. To facilitate scalable synthetic data generation and further reduce the computational burden, we employ model quantization and knowledge distillation techniques (Yin et al., 2025). We apply post-training quantization to reduce the models memory footprint and inference cost with minimal impact on output quality. Furthermore, we adopt the generative video model (Yin et al., 2025) distilled from the teacher model, preserving editing fidelity while significantly accelerating the generation process with few-step inference. This optimized pipeline is crucial for producing large-scale video editing data efficiently. 3.5 EDITED VIDEO CURATION AND ENHANCING To guarantee the highest quality, the generated triplets (Vs, p, Ve) undergo final two-stage curation and refinement including VLM filtering and denoiser enhancing. VLM-Based Curation. We first use VLM (Bai et al., 2025) as an automated judge to perform rejection sampling. Each triplet is evaluated against two criteria: (1) Instruction Fidelity: whether the edit in Ve accurately reflects the prompt p. (2) Fidelity: whether Ve preserves the semantic and motion from Vs. (3) Visual quality: whether the videos are visual appealing without significant distortion or artifacts. (4) Safety & Appropriateness: whether the content has unsafe or inappropriate material, such as pornography, violence, or horror, ensuring the dataset is ethically compliant and suitable. Triplets that fail to meet our quality thresholds on these criteria are discarded. Quality Enhancement via Denoising. The curated edited videos are then enhanced using the state-of-the-art open-source Text-to-Video (T2V) model, Wan2.2 (Wan et al., 2025). Unlike postprocessing in prior work that performs simple upscaling, our objective is to achieve perceptual refinement without introducing semantic deviations from edited content of Ve. This requirement aligns perfectly with the specialized design of Wan2.2s Mixture-of-Experts (MoE) architecture, which employs coarse denoiser for structural and semantic formation under high noise, and fine denoiser specialized in later-stage refinement under low noise. We specifically leverage the fine denoiser for short, 4-step reverse process. For each video Ve, we first add small amount of Gaussian noise. The fine denoiser then inverts this process utilizing its expert prior to remove subtle artifacts and enhance textural details precisely because it is optimized for making minimal, semantic-preserving adjustments to nearly-complete videos. This yields high-quality output with improved resolution and visual fidelity that remains strictly semantically consistent with our initial edit. Figure 4: Model training pipeline. We train the context blocks based on the in-context video generator with curriculum learning by gradually annealing and eventually dropping the reference frame. Table 2: Quantitative comparisons with prior arts. The best results are bolded. Human Evaluation Edit-Acc Temp-Con Overall 1.97 1.96 2.30 3.76 Automatic Metric CLIP-T CLIP-F VLM 98.43 97.99 98.78 99.03 Method TokenFlow (Geyer et al., 2024) InsV2V (Cheng et al., 2024) InsViE (Wu et al., 2025b) Ours 23.63 22.49 23.56 25.54 7.10 6.55 7.35 8.10 1.70 2.07 2.36 3. 1.70 2.17 2.28 3.85 3.6 DETAILS OF DITTO-1M As illustrated in Fig. 3, we collected total of over 200k source videos, approximately half of which feature human activities. After undergoing filtering process, these videos were edited using editing instructions generated by VLM, followed by an additional round of filtering. This pipeline ultimately yielded approximately 1M edited videos. Among these, about 700k video triplets involve global editing (including changes to style, environment, etc.), while roughly 300k pertain to local editing (encompassing object replacing, adding, and removal). As presented in Table 1, the final enhanced videos have resolution of 1280x720, each comprising 101 frames at 20 FPS. The visual quality of the final samples significantly surpasses that of previous datasets - we strongly recommend reviewing the video samples provided on the project page."
        },
        {
            "title": "4 MODEL TRAINING VIA MODALITY CURRICULUM LEARNING",
            "content": "We select the in-context video generator VACE (Jiang et al., 2025) as our backbone, inspired by its strong prior for generating videos that are spatially and structurally aligned with source video. VACEs original capability is to condition its generation on two visual contexts (and prompts): source video and reference image. Our goal is to repurpose this powerful visual generator into proficient editor that operates on abstract textual instructions. However, directly fine-tuning the model to bridge the vast semantic gap from visual to textual conditioning is prone to instability. We therefore adapt its architecture, as shown in Fig. 4. It consists of Context Branch for extracting spatiotemporal features from the source video and reference frame, and DiT-based (Peebles & Xie, 2023) Main Branch that synthesizes the edited video under the joint guidance of the visual context and the new textual embeddings from the instruction. To ease the training difficulty and stably bridge this modality gap, we introduce modality curriculum learning (MCL) strategy. The core idea is to leverage the models inherent ability to process the reference image context as temporary aid. In the initial training phase, we provide the edited reference frame as strong visual scaffold alongside the new text instruction. As training progresses, we gradually anneal the probability of providing this visual scaffold, eventually dropping it entirely. This process compels the model to shift its dependency from the concrete visual target it already understands to the more abstract textual instruction, transforming it into purely instruction-based video editing model. We train the model using the flow matching (Lipman et al., 2022) objective: = Et,z0,cvt(zt, t, c) (z0 zt)2, where z0 is the clean latent encoded from the target edited video, zt is its noised version at timestep t, represents the conditioning from text and visual contexts, and vt is the models predicted vector field pointing from zt to z0. (5) 7 Figure 5: Qualitative comparisons with prior arts TokenFlow (Geyer et al., 2024), InsV2V (Cheng et al., 2024), InsViE (Wu et al., 2025b) and Gen4-Aleph (Runway, 2025). Figure 6: Our data and learned model enable the translation from synthetic videos to the real domain."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETTINGS Our model is built upon the pre-trained in-context video generator (Jiang et al., 2025; Wan et al., 2025) and is fine-tuned on our newly proposed large-scale dataset, which comprises over one million high-quality video triplets. To maintain the strong generative prior of the base model and ensure training efficiency, we freeze the majority of the pre-trained models parameters, and only fine-tune the linear projection layers of context blocks. The model is trained for approximately 16,000 steps using the AdamW optimizer (Loshchilov & Hutter, 2017) with constant learning rate of 1e-4 on cluster of 64 NVIDIA H-series GPUs. We employ our modality curriculum learning strategy, where the initial 5,000 steps serve as curriculum warm-up phase. 5.2 EXPERIMENTAL RESULTS Quantitative Comparison. We perform quantitative comparisons using automatic metrics and user study, summarized in Table 2. For automatic evaluation, we employ three metrics: CLIP-T measures the CLIP text-video similarity to assess how well the edit follows the instruction; CLIP-F calculates the average inter-frame CLIP similarity to gauge temporal consistency; and VLM score provides holistic assessment of edit effectiveness, semantic preservation, and overall aesthetic quality. user study based on 1,000 ratings also rates instruction-following (Edit-Acc), temporal 8 Figure 7: Unlike the original data generator, which fails to handle newly emerging information beyond key frames, our model - trained with filtering and scaling techniques - outperforms it. Figure 8: Ablation studies on training data scale and modality curriculum learning (MCL). consistency (Temp-Con), and overall quality (Overall). As shown, our method significantly outperforms all baselines across metrics, achieving the highest automatic scores and strong preference in human evaluations, which confirms its superior instruction adherence, temporal smoothness, and visual quality. Please refer to Section for additional details of the user study. Qualitative Comparison. As shown in Fig. 5, our method consistently produces visually superior results that better adhere to edit instructions compared to prior arts. For complex stylizations, our model generates temporally coherent videos that accurately match the target style, while competitors often yield blurry or inconsistent results. For local attribute changes (e.g., black suit), our method precisely edits the target object while preserving identity and background details, task where Gen4Aleph slightly changes the mans identity and other methods largely fail. We recommend reviewing the video samples provided on the project page for better understanding. Additional Results. We showcase the synthetic-to-real (sim2real) capability in Fig. 6 benefited from our data by training the model to map the stylized videos in our dataset back to their original, real-world source videos. This successful transfer highlights the rich and photorealistic information contained within our dataset, demonstrating its utility beyond standard editing tasks. Also, our final trained model substantially outperforms the raw data generator from our pipeline, demonstrating superior handling of newly emerged content as in Fig. 7. This superiority stems from our scaled training regimen, including the curriculum learning and exposure to the filtered, high-quality data. 5.3 ABLATION STUDIES We conduct ablation studies to validate the key components of our framework, with results presented in Fig. 8. We find that our models performance scales effectively with the training data - as the number of samples increases, both the quality of the stylistic edits and the fidelity to the original videos content and motion improve significantly, confirming the value of our large-scale dataset. Furthermore, we ablate our modality curriculum learning (MCL) strategy and find that, without MCL, the model often struggles to interpret the instructions full semantic intent. Therefore it is crucial for bridging the modality gap and learning to follow instructions."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We have presented Ditto, scalable framework that significantly advances instruction-based video editing by systematically addressing the core challenge of data scarcity through new paradigm for large-scale data synthesis. Our synthetic data generation pipeline overcomes the fidelity-diversity and efficiency-coherence trade-offs plaguing prior methods by leveraging strong image-editing priors, distilled in-context video generator with temporal enhancer, and autonomous VLM-based quality control. This enables the creation of the large-scale, high-quality Ditto-1M dataset. The proposed modality curriculum learning strategy further ensures our model Editto achieves state-of-theart performance by effectively transitioning from visual-textual conditioning to purely instructiondriven inference. Extensive results reveal the effectiveness of the proposed method."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. FLUX. 1 Kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, 2025. Yoshua Bengio, Jerˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Int. Conf. Mach. Learn., 2009. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Tim Brooks, Aleksander Holynski, and Alexei Efros. InstructPix2Pix: Learning to follow image editing instructions. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2Video: Video editing using image diffusion. In Int. Conf. Comput. Vis., 2023. Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. StableVideo: Text-driven consistency-aware diffusion video editing. In Int. Conf. Comput. Vis., 2023. Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. In IEEE Conf. Comput. Vis. Pattern Recog., 2025. Jiaxin Cheng, Tianjun Xiao, and Tong He. Consistent video-to-video transfer using synthetic dataset. In Int. Conf. Learn. Represent., 2024. Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. TokenFlow: Consistent diffusion features for consistent video editing. In Int. Conf. Learn. Represent., 2024. Google. Gemini 2.5 Flash Image. gemini-2-5-flash-image, 2025. https://aistudio.google.com/models/ Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate your personalized text-to-image diffusion models without specific tuning. In Int. Conf. Learn. Represent., 2024. Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan arXiv preprint Yang. CameraCtrl: Enabling camera control for text-to-video generation. arXiv:2404.02101, 2024. Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. VACE: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. HunyuanVideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 10 Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-P2P: Video editing with cross-attention control. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision, 2023. Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. CoDeF: Content deformation fields for temporally consistent video processing. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Int. Conf. Comput. Vis., 2023. Pexels. Pexels. https://www.pexels.com/, 2025. Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. FateZero: Fusing attentions for zero-shot text-based video editing. In Int. Conf. Comput. Vis., 2023. Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, and Yueting Zhuang. Instructvid2vid: Controllable video editing with natural language instructions. In Int. Conf. Multimedia and Expo, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. HighIn IEEE Conf. Comput. Vis. Pattern resolution image synthesis with latent diffusion models. Recog., 2022. Runway. Introducing Runway Gen-4. introducing-runway-gen-4, 2025. https://runwayml.com/research/ Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. PhotorealisIn Adv. Neural Inform. tic text-to-image diffusion models with deep language understanding. Process. Syst., 2022. Yang Song and Prafulla Dhariwal. preprint arXiv:2310.14189, 2023. Improved techniques for training consistency models. arXiv Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. 11 Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-A-Video: One-shot tuning of image diffusion models for text-to-video generation. In Int. Conf. Comput. Vis., 2023. Yuhui Wu, Liyi Chen, Ruibin Li, Shihao Wang, Chenxi Xie, and Lei Zhang. InsViE-1M: Effective instruction-based video editing with elaborate dataset construction. In Int. Conf. Comput. Vis., 2025b. Xiangpeng Yang, Linchao Zhu, Hehe Fan, and Yi Yang. VideoGrain: Modulating space-time attention for multi-grained video editing. In Int. Conf. Learn. Represent., 2025. Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2025. Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan, Joyce Chai, and Mohit Bansal. VEGGIE: Instructional editing and reasoning video concepts with grounded generation. arXiv preprint arXiv:2503.14350, 2025. Chi Zhang, Chengjian Feng, Feng Yan, Qiming Zhang, Mingjin Zhang, Yujie Zhong, Jing Zhang, and Lin Ma. InstructVEdit: holistic approach for instructional video editing. arXiv preprint arXiv:2503.17641, 2025. Zhenghao Zhang, Zuozhuo Dai, Long Qin, and Weizhi Wang. EffiVED: Efficient video editing via text-instruction diffusion models. arXiv preprint arXiv:2403.11568, 2024."
        },
        {
            "title": "A APPENDIX OVERVIEW",
            "content": "In this appendix, we provide additional details and results to supplement our main paper. In Section B, we present an analysis of the in-context data generator. We showcase the interface used for our human evaluations in Section C. Finally, Section contains more qualitative results of our dataset and model, and word cloud visualizing the distribution of instructions in our dataset."
        },
        {
            "title": "B JUSTIFYING THE DESIGN OF DATA GENERATION PIPELINE",
            "content": "Figure 9: Results of various settings for data generation. We provide an analysis of the videos synthesized by the in-context video generator, VACE, to justify the design of our data pipeline. As in Fig. 9, we first observe that using only depth maps to guide the generator results in significant loss of content from the source video, leading to poor fidelity. Conversely, conditioning the generator on keyframe from the original source video alongside the editing instruction fails to produce the desired edit - the output remains almost identical to the source. These findings reveal that while the base generator excels at motion transfer, its inherent instruction-following capability for editing tasks is limited. Based on this analysis, we validate our proposed approach: using keyframe modified by an advanced image editor, in conjunction with depth guidance as the context. This method achieves the optimal balance of instruction adherence, temporal consistency, and source fidelity for our data synthesis."
        },
        {
            "title": "C DEMONSTRATION OF USER STUDY INTERFACE",
            "content": "Figure 10: The interface of the user study. To collect human preference data, we designed user-friendly evaluation interface, as shown in Fig. 10. For each source video and text prompt, we presented participants with the edited results from different methods in randomized order. They were then asked to rank the videos from best (1) to worst (4) based on three criteria: Instruction Following, Temporal Consistency, and Overall preference. The final scores are calculated based on the ranking. 13 Figure 11: Additional visualization of data from the proposed dataset and model outputs. Please view the site for additional video samples."
        },
        {
            "title": "D ADDITIONAL RESULTS OF DATASET AND MODEL",
            "content": "We present additional qualitative results to further demonstrate our dataset and models performance across wide range of editing instructions in Fig. 11. We also include word cloud in Fig. 12 that illustrates the diversity and distribution of the editing prompts within the dataset, highlighting its comprehensive coverage. 14 Figure 12: The word cloud of editing instructions."
        }
    ],
    "affiliations": [
        "Ant Group",
        "HKUST",
        "Northeastern University",
        "Zhejiang University"
    ]
}