{
    "paper_title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization",
    "authors": [
        "Zhengwei Tao",
        "Jialong Wu",
        "Wenbiao Yin",
        "Junkai Zhang",
        "Baixuan Li",
        "Haiyang Shen",
        "Kuan Li",
        "Liwen Zhang",
        "Xinyu Wang",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Jingren Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks."
        },
        {
            "title": "Start",
            "content": "2025-07-22 WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization Zhengwei Tao, Jialong Wu, Wenbiao Yin((cid:0)), Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang((cid:0)), Pengjun Xie, Fei Huang, Jingren Zhou Tongyi Lab , Alibaba Group https://github.com/Alibaba-NLP/WebAgent https://huggingface.co/datasets/Alibaba-NLP/WebShaper https://modelscope.cn/datasets/iic/WebShaper"
        },
        {
            "title": "Abstract",
            "content": "The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of highquality training data has limited the development of IS agents. Existing data synthesis approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, as well as between the question and the corresponding answer. To mitigate, we propose formalization-driven IS data synthesis framework WebShaper, which systematically formalizes IS tasks using set-theoretic constructs. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks. 5 2 0 2 0 2 ] . [ 1 1 6 0 5 1 . 7 0 5 2 : r Figure 1: Results on GAIA information-seeking subset among the cutting-edge Deep Research models or systems. denotes the results using our two browsing tools via function calling APIs. *denotes equal contribution. (cid:0) denotes the correspondence. {yinwenbiao.ywb, yongjiang.yj}@alibaba-inc.com"
        },
        {
            "title": "Introduction",
            "content": "The emergence of Large Language Model (LLM)-powered language agents has marked paradigmshifting advance in artificial intelligence, enabling transformative solutions to previously intractable challenges across domains (Guo et al., 2024; Wang et al., 2024; AutoGPT, 2023; Wu et al., 2023; Ye et al., 2023). Information-seeking (IS) represents core component of the cognitive autonomy of language agents. This capability not only underpins their adaptability in open-ended tasks but also powers range of powerful commercial systems such as Deep Research of OpenAI (OpenAI, 2025), Gemini (Gemini, 2025), and Perplexity (Perplexity, 2025). Current agentic systems for unlocking this capability typically follow well-established pipeline in agent development: (1) First, construct task-specific trajectories of question-answer pairs; (2) Employ supervised fine-tuning (SFT) to acquire foundational skills (Sun et al., 2025). (3) Generalize strategic decision-making through on-policy reinforcement learning (RL) (Jin et al., 2025). The entire development of the IS agent originates from and its ultimate effectiveness depends on high-quality IS task training data. However, due to its complexity, such high-quality dataset is both sparse and difficult to construct through crowdsourcing. Thus, constructing training data through carefully designed agent pipeline becomes the cornerstone of effective IS agent development. Figure 2: Data synthesis paradigm shift from information-driven to formalization-driven. Source stands for information sources such as the internet and databases. Data represents the synthesized QA data. (a) Previous methods retrieve and organize collected information in advance, then synthesize data according to the information structures. (b) Our method establishes the task formalization first, then collects information, and synthesizes QA data based on the formalization. Existing IS dataset synthesis methods typically involve freely pre-searching for information online and employing LLMs to generate questions from the collected content (Figure 2(a)). These approaches first organize the collected information into structured formats, then prompt the LLM with the structured data to produce natural language (NL) questions. Their core objective is to map information structures into reasoning structures within the resulting NL questions. Representative methods like WebDancer (Wu et al., 2025a) and TaskCraft (Shi et al., 2025a) generate linear information chains, while others construct graphs connected via web links (Wu et al., 2025b) or entity coreference networks (Li et al., 2025a). However, these information-driven approaches face two critical limitations. First, the synthesis using LLM may struggle to fully comprehend the information structure, resulting in inconsistent reasoning structures or incorrect answers to the generated NL questions. Besides, disordered information retrieval will lead to excessive data processing and will collect redundant homogeneous information structures, which limits the diversity of information structures and reduces knowledge coverage. To overcome these limitations, we propose WebShaper1, formalization-driven IS data synthesis paradigm, WebShaper, as illustrated in Figure 2(b). Unlike prior approaches, we first formalize information1Without loss of generality, we use WebShaper to denote our data method, dataset, and model. 2 seeking tasks and then systematically guide data synthesis through this formalization. During generation, information collection is explicitly controlled by formal task requirements. This framework offers three key advantages: 1. Broader Task Coverage: Systematic exploration of task formalizations enables synthesizing diverse information-seeking patterns unconstrained by pre-retrieval content limitations; 2. Task Controllability: Explicit formalization parameters allow precise specification of reasoning structures and complexity levels; 3. Structural and Answer Consistency: Due to the inherent interpretability and verifiability of formalized representations, synthesized outputs exhibit fewer inconsistencies across both informationreasoning structures and question-answer pairs. WebShaper works fundamentally because it introduces formalization-guided framework that serves as structural skeleton during data synthesis. With this structured guidance, we produce consistent reasoning and redundancy while ensuring rich, diverse reasoning logic. We leverage the proposed framework to construct the WebShaper dataset, which serves as training data for the IS agent. At the core of our framework lies formalization of IS tasks, which enables principled and systematic generation of task instances with controllable collection complexity and reasoning paths. This overcomes the fragmented and ad-hoc nature of task construction in prior information-driven approaches. Unlike relevant fields, where there exists task formalization in advance, such as Lean 4 language (Moura & Ullrich, 2021) in math proving and propositional logic in knowledge-centric question answering (Xia et al., 2025), theres no established formalization for information-seeking. To the best of our knowledge, we are the first to derive it based on set theory. WebShaper treats IS as unified problem space where task is systematically derived from compositions of basic units termed Knowledge Projections (KP). To align with the formalized structure, we initiate synthesis by constructing foundational seed tasks, followed by multi-step expansion grounded in our formal framework. This process employs dedicated agentic Expander module designed to interpret task requirements via KP representations. At each expansion stage, the expander transforms the current formal question into more complicated one. It implements layer-wise expansion mechanisms that minimize redundancy while preventing reasoning shortcuts through controlled complexity progression. The Expander operates autonomously during synthesis, performing three core functions: (1) internet-based knowledge collection guided by formal requirements, (2) construction and validation of new formalized problems, and (3) generation of final questions. This process ensures broad coverage of the formalized task space and the correctness of the question and answer. We conduct extensive experiments to validate WebShaper dataset by training agents. Comparison with the existing training dataset shows the effectiveness of WebShaper. WebShaper achieves best performances among all open-source IS agents on the GAIA and WebWalkerQA benchmarks. Further discussions demonstrate the validity of each module of our method. We summarize our contributions as: We introduce WebShaper, formalization-driven data synthesis method for information-seeking agents, grounded in our proposed task formalization. Leveraging this method, we construct the WebShaper dataset, which enables systematic generation of IS instances. We propose an agentic Expander that iteratively generates and validates questions in alignment with the formalization. We conduct extensive experiments across multiple benchmarks to evaluate the effectiveness of WebShaper. Empirical results demonstrate that models trained with WebShaper consistently outperform baselines, confirming the value of our formalization and synthesis approach. 3 Figure 3: question-answer case in our information-seeking formalization. We use the purple diagram to represent knowledge projection, which is set of entities. Information-Seeking Formalization In this section, we introduce our formalization of the information-seeking task. We illustrate an example in Figure 3. An information seeking task q(T) aims to search for knowledge and facts prompted by given facts and locate the answer entity set T. For basic example also shown in Figure 3: q(T) =Which player of team in the 2004-05 season, who was born in 90s? This team is founded in 1966 and is an East German football team. (1) To solve it, one should seek information about This team is founded in 1966 and is an East German football team to find that the team is Berliner FC Dynamo. And then seek for players of Berliner FC Dynamo team in 2004 and 2005 respectively and players born in 90s, then reason the answer = {Robert Rudwaleit, Danny Kukulies, ..}. Let denote the universal set of entities (e.g., players, teams, years). Let denote subspace of entity pairs where they have certain relation. For example, if the relation is bornIn, stands for all pairs of (person, year) where person is born in year. For subset and sub-space R, define Knowledge Projection (KP): R(V) = {u V, (u, v) or (v, u) R}. (2) For example, when denotes entity pairs of relation bornIn, R({90s}) represents the set of all people born in 90s. KP is the set of entities under certain relation to other entities, which is the basic unit in an information-seeking task. KP has two operations: R-Union In IS, the question may be seeking for broader condition due to uncertainty about the target. For instance, we only know the target player was playing between 2000-2010 rather than the exact year in advance. The condition can not be more specific than year range. Therefore, given S1, S2 be entity sets and R, then: R(V) = R(S1) R(S2) R(Sm) (3) represents R(V) is the union result set in which the entities have certain relation to entries in either S1, S2, ..., Sm. If stands for relation playAt, then the set of players who play between 2000-2010 is R({2000}) R({2001}) R({2010}). Intersection Some IS tasks require the target to satisfy several conditions simultaneously. interpreted as an Intersection operation of KP: Its 4 R(V) = R1(S1) R2(S2) Rn(Sn) (4) where Ri are about different relations. For example, if R1 is about playAt and R2 is about bornIn, then R1({2000}) R2({90s}) stands for players playing in 2000 and born in 90s. Based on R-Union and Intersection operations, we introduce IS task formalization. First, we define as target set: = (cid:92) i=1 (Ri(Si,1) Ri(Si,2) . . . Ri(Si,ti ))). (5) Si,j is an entity set. More generally, can be recursivelly derived by replacing Si,j with other target set as: An IS task is to find what entities questioned contains: = R1(T1) R2(T2) . . . Rk(Tk) Therefore, the question example in Eq. (1) can be formalized as: q(T) ?T q(T) ?T =RplayIn(T1) (RplayAt({2004}) RplayAt({2005})) 1999 (cid:91) 1900 RbornIn({y})) T1 = oundIn({1996}) RisA({East German ootball team})"
        },
        {
            "title": "3 Data Synthesis",
            "content": "(6) (7) (8) In this section, we describe the process of our data synthesis with our task formalization. As Eq. (5-7) shows, an IS task is recursively composited by knowledge projections. In order to better fit the IS task formalization, we start with constructing seed task, followed by multi-step expansion approach. This expansion process is built upon our formalization. We then introduce an agentic Expander. It can understand the task formalization with our KP representation. At each expansion step, we implement the layer-wise expansion to reduce redundancy and reasoning shortcuts. The Expander autonomously retrieves knowledge from the internet, constructs and validates the new FPs to obtain the new question. We elaborate on this process in the following sections."
        },
        {
            "title": "3.1 Seed Question Construction",
            "content": "The first stage of our data synthesis pipeline involves acquiring substantial volume of diverse and nontrivial seed questions. To enhance acquisition efficiency, we constructed an offline Wikipedia database by downloading all URLs corresponding to Wikipedia articles while preserving the hyperlinks between them. Subsequently, we perform random walks across these articles through their preserved connections. By aggregating the content from articles traversed during these random walks, we utilize an LLM to generate synthetic data instances. Critically, the generated question-answer pairs must be entirely grounded in the content from the collected articles, without relying on external knowledge sources. However, the resulting seed questions could be noisy and contain hallucinations. We launch filtering process. We complete all the seed questions by WebDancer framework (Wu et al., 2025a) based on the QwQ model (Team, 2025). We perform 5 times rollouts for each question and keep the data where there 5 must be as least one rollout correctly answering the question. We finally construct 18k seed questions. We denote the harvested seed question as q1(T)."
        },
        {
            "title": "3.2 Agentic Expansion",
            "content": "Subsequently, we progressively expand seed questions into increasingly complex ones through n-step expansion qn+1(T) = Expand(qn(T)) guided by the task formalization. However, the IS formalization in Eq. (5-7) is complicated. The nature of recursion and the composition of multiple operations are hard for the model to understand during the synthesis. Besides, since the synthesis relies on retrieving new knowledge online, there are several intermediate processes, such as knowledge filtering and selection. Therefore, we establish an Agentic Expansion. We first introduce the KP representation, which enables clear comprehension of our IS formalization. Then, we propose the Layer-wise Expansion Strategy to mitigate the limitations of redundant and reasoning shortcuts. The core of the expansion is the Expander, which is an agent itself to autonomously retrieve information and validate the generation. 3.2.1 KP Representation Since q(T) contains recursion and composition of R-Union and Intersection operations, its not trivial to represent q(T) in the Expander agent prompt. We introduce our KP Representation. The key to this representation is to: 1) represent KP unit. 2) can handle R-Union and Intersection operations. 3) can handle recursions of KPs. We start with introducing Constant and Variable: Constant: constant is subset of explicitly defined by its elements, e.g., {90s}, {2004, 2005}. Variable: variable is subset of whose elements are not explicitly given. It may appear as symbolic placeholder in an expression. Then, we use triplet [X, r, S] to represent KP R(S). is the name of the relation R. is variable while can be variable or constant. We use the prefix V@ followed by variable to denote the variable V. We use the prefix @C before its natural language description to represent constant. For example, RbornIn({90s}) is represented as [@V, bornIn, 90s]. The Intersection operation in Eq.(4) can be naturally represented as list of triplets [[X, r1, S1], [X, r2, S2], ..., [X, rn, Sn]]. For the R-Union in Eq.(3), simply expressing it in list-like form will make the representation complicated in recursive R-Union and Intersection. We notice R-Union has the following proposition: Proposition 1. For certain R, R-union satisfies the distributive Law: R(S1) R(S2) = R(S1 S2) (9) Proof. Let be an element of R(S1) R(S2). By Equation 2, there exists either y1 S1 such that (y1, x) or (x, y1) R, or y2 S2 such that (y2, x) or (x, y2) R. Consequently, there exists S1 S2, e.g., y1 or y2, such that (y, x) or (x, y) R. Thus, we have R(S1 S2), and hence R(S1) R(S2) R(S1 S2). Conversely, let be an element of R(S1 S2). Then there exists S1 S2 such that (y, z) or (z, y) R. If S1, then R(S1); if S2, then R(S2). In either case, R(S1) R(S2). Therefore, R(S1 S2) R(S1) R(S2). Combining both directions, we conclude that: R(S1) R(S2) = R(S1 S2). Thus, we end proof of the Proposition. 6 Figure 4: Structures on different expansion paradigms. (a) Random Structure denotes expanding by randomly adding constants. (b) Sequential Structure is expanding on chain of reasoning sequence. (c) Layer-wise Structure traverses layer-wisely on leaf constants and replaces them with variables. Target stands for target variable. Variable means the intermediate variable. Constant is the constant in our KP representation. With this proposition, we represent the R-Union of KP by merge set S1 S2. In practice, we express the union of sets by induction (eg. {1990} {1991}, . . . , {1999} as {90s}). Or simply add underlines between them (eg. {1990} {1991}) as {1990_1991}). After that, our representation would only have an intersection between triplets. By introducing variables, our representation naturally handles KP recursion by faltten it into the intersection of KPs. For example, given recursion R1(R2(S)), we can represent it as [[V@X, r1, V@Y], [V@Y, r2, S]]. Finally, an IS task q(T) can be represented by list of triplets. For example, the question in Eq. (1) can be represented as: q(T) ?T s.t. [[V@T, playIn, V@X], [V@T, playAt, C@2004_05], [V@T, bornIn, C@90s], [V@X, foundIn, C@1966], (10) [V@X, isA, C@East German football team]] 3.2.2 Layer-wise Expansion Strategy After representing the q(T), we now elaborate on the expansion process in each iteration. Expansion strategy is key to our data synthesis. Compared to previous approaches that synthesize or extend questions at the natural language form, our formalization of IS tasks enables systematic analysis of structural question characteristics. This formal framework allows us to explicitly identify latent structural patterns within questions and perform controlled and optimized expansion paradigm. To clearly illustrate the expansion strategy, we show our KP representation in graph. The nodes in the graph are variables and constants in the list of triplets. And the edges are the relations. For example, the question in Eq. (10) can be illustrated as graph in Figure 4. The question requires determining the target variable via the given constants. Previous methods are constrained by informal representations of natural language, which limit the controllable expansion and synthesis paradigms for questions. In our formalization language, previous methods would result in question structures as Random (Wu et al., 2025b; Shi et al., 2025a) or Sequential (Wu et al., 2025a). The Random structure stands for methods that directly add FP to any nodes in the graph shown in Figure 4 (a). Sequential structure is resulted from generating the reasoning chain via sequence shown in Figure 4 (b). However, these two paradigms have key limitations: Redundancy As shown in Random structure in Figure 4, there exist constants connect to other constants. In this condition, such sentence as \"Dynamo Berlin is football club based in Berlin\" would exist in the question. However, it doesnt increase the reasoning chain of the task-solving. Reasoning Shortcut As shown in the Sequential structure in Figure 4, there exists an FP which connects constants directly to the target. If this happens, models may guess the answer by only reasoning on the closer constants and neglecting the deeper sequence. To mitigate these limitations, we introduce the Layer-wise Expansion Strategy. We layer-wisely traverse the graph to find all leaf constants. When we obtain all the leaf constants of the current graph, an Expander takes each constant once time to construct this constant into new FPs. These FPs can form sub-question that regards the constant as the answer. The expander then merges the sub-question to the current one to form new one: qn+1(T) = Expander(C, qn(T)). (11) Note that the qn+1(T) always has the same answer as qn(T). As illustrated in the Figure 4, in each expansion, the Expander takes leaf constant node, turns it into variable node connected with new nodes. The resulting structure would not have the Redundant and Reasoning Shortcut problems. The number of expanding layers is hyperparameter for controlling the task coverage and difficulty. 3.2.3 Expander Agent We now introduce the Expander, an autonomous agent designed to enhance question generation through iterative refinement. Given an input constant, the Expander first retrieves relevant contextual information, then formulates semantically coherent sub-question. This sub-question is subsequently integrated with the original query to construct an enriched, context-aware question that better aligns with the underlying information-seeking objective. The Expander builds upon ReAct (Yao et al., 2023), widely-adopted framework for language agents. ReAct trajectory comprises multiple Thought-Action-Observation interaction cycles. In each cycle, the language model generates free-form Thought for strategic planning, executes structured Action to interface with external tools, and receives Observation feedback from the environment. Formally, the agent execution loop at time can be represented as (τt, αt, ot), where τ denotes Thought, α signifies Action, and represents Observation. Each Action α decomposes into (τ, ϕ): τ specifies the action type (using one of the tools or answer), while ϕ contains required parameters. We equip the Expander with the following tools: Search This action enables Expander to conduct Google search by severl queries about constant and obtains search results. The parameters of this tool are ϕ = {queries of c, filter_year}, enabling temporal filtering of search results. This tool would return top relevant URLs and their snippets as Observation. Summarize This is the key to R-Union oepration. This action allows Expander to visit multiple URLs searched for the constant and summarize the content. The summarization would integrate the retrieved information to obtain union constant set as stated in Eq.(9). The parameters of this tool are ϕ = {urls, goal}. This tool would return the summarization of knowledge about from the given urls as Observation. Validate When Expander completes retrieving and summarizing the KPs of constant C, it derives sub-question and uses this tool to validate the results based on our formalization. The validation purposes are to determine: 1) whether the derived sub-question are consistent with based on the formalization. 2) whether it is too simple that can be directly answered by an LLM. We call QwQ once time per each purpose. In the first consistency validation, we dont check whether is strictly the answer to the sub-question. Instead, it checks if the type of satisfies the sub-question. For the second validation, we require QwQ to answer the sub-question. If the 8 prediction is the same as C, we regard it as invalid. This tool would return detailed validation results as Observation, and the Expander would take the next action according to it. The iterative expansion process terminates upon executing the answer action, which finalizes the question construction phase with verified sub-question derived from the accumulated knowledge."
        },
        {
            "title": "3.3 Trajectory Construction",
            "content": "After harvesting the expanded questions, we proceed to construct task-completing trajectories. To this end, we instantiate an agent framework based on QwQ structurally aligned with the Expander, adopting the ReAct paradigm (Yao et al., 2023). At each timestep, the agent first first produces Thought τ followed by an Action α. It receives the Observation of the Action to determine the behavior in the next round. The agent is equipped with two external tools: Search and Visit. The Search tool conducts Google search with several queries, which is the same as Expander. Visit returns the pages information for the given URLs. For each input question, we perform 5 times rollouts. To ensure the quality and relevance of the collected trajectories, we further design set of filtering strategies: Correctness We use judge LLM to exam the final answer of each trajectory and only keep the correct ones. We also remove if there are tool call errors. Quality We filter trajectories if they contain hallucinations of guessing observation and severe repetitions. We finally obtain 5, 000 trajectories for later supervised training and reinforcement learning."
        },
        {
            "title": "3.4 Agent Training",
            "content": "To train our information-seeking agent, similar to WebDancer (Wu et al., 2025a), we implement supervised fine-tuning (SFT) followed by reinforcement learning (RL). In SFT, given trajectory in sequence of tokens = (τ1, α1, o1, ..., τn, αn, on), we mask out loss from observation leading to loss: = 1 I[xi o] i=1 i= I[xi o] log πθ(xi x<i) (12) where πθ is the model to train. Later in RL, we further optimize πθ use the GRPO algorithm (Shao et al., 2024). For question-answer pair (q, a), GRPO samples rollouts {yi}G and updates the policy model by: JGRPO(θ) = (cid:34) (q,a)D,{yi}G i=1 1 i=1 yi ri,j(θ) = (cid:16) min (context) πθold yi t=1 (cid:1) (cid:0)oi qi, oi,<t (cid:1) , (cid:0)oi qi, oi,<t i=1 πθ πθold ri,t(θ) ˆAi,t, clip (cid:16) ri,t(θ), 1 εlow, 1 + εhigh (cid:35) (cid:17) (cid:17) ˆAi,t (13) ˆAi,j = Ri mean(cid:0){Ri}(cid:1) std(cid:0){Ri}(cid:1) , where context includes all the model completions and tool responses. ε is the clipping range of the importance sampling ratio ri,t(θ). ˆAi,t is an estimator of the advantage of the i-th rollout at t-th step."
        },
        {
            "title": "4 Experiments",
            "content": "9 Table 1: Main results on GAIA and WebWalkerQA benchmarks. We compare WebShaper with several cutting-edge baselines methods. bolded number stands for the best results on the corresponding settings. Blue scores are the highest among all open-sourced methods. Backbone Framework Level 1 Level Level 3 Avg. Easy Medium Hard Avg. GAIA WebWalkerQA Qwen-2.5-7B Qwen-2.5-32B Qwen-2.5-72B GPT-4o QwQ-32B DeepSeek-R1-671B Base Base RAG Base Base Base RAG Base No Agency 12.8 20.5 12.8 20. 23.1 30.8 33.3 43.6 3.8 9.6 11.8 13. 15.4 15.4 36.5 26.9 0.0 8.3 8.3 0. 8.3 25.0 8.3 8.3 6.8 13.6 11.8 14. 17.5 22.3 32.0 31.1 1.25 3.8 23.1 9. 6.7 7.5 36.9 5.0 0.8 2.5 14.3 7. 6.0 2.1 26.1 11.8 0.7 3.3 11.3 3. 4.2 4.6 33.5 11.3 0.8 3.1 15.3 6. 5.5 4.3 31.2 10.0 Close-Sourced Agentic Frameworks OpenAI DR 74. 69.1 47.6 67.4 - - - - Open-sourced Agentic Frameworks Qwen-2.5-32B QwQ-32B Search-o1 WebDancer WebShaper Search-o1 WebThinker-Base WebThinker-RL Simple DS WebDancer WebShaper Qwen-2.5-72B WebShaper"
        },
        {
            "title": "4.1 Experimental Setups",
            "content": "33.3 46.1 61.5 53.8 53.8 56.4 - 61.5 69.2 69.2 25.0 44.2 53.8 34.6 44.2 50.0 - 50.0 50.0 63. 0.0 8.3 16.6 16.6 16.6 16.6 - 25.0 16.6 16.6 28.2 40.7 52.4 39.8 44.7 48.5 50.5 51.5 53.3 60. - 44.3 58.1 43.1 47.2 58.8 - 52.5 55.8 56.2 - 46.7 51.4 35.0 41.1 44.6 - 59.6 49.2 52. - 29.2 47.0 27.1 39.2 40.4 - 35.4 45.4 49.5 - 38.4 51.4 34.1 41.9 46.5 - 47.9 49.7 52. We evaluate WebShaper on two information-seeking benchmarks: GAIA (Mialon et al., 2023) and WebWalkerQA (Wu et al., 2025b). We use the LLM-as-Judges paradigm to evaluate both tasks using the Pass@1 metric, following Li et al. (2025c). We compare our synthesized dataset with several datasets: WebWalkerQA employs random walks over interlinked URLs to synthesize questions based on the visited webpages (Wu et al., 2025b). The dataset includes both single-source questions, generated from single visited URL, and multi-source questions, which are constructed using information aggregated from multiple visited URLs. E2HQA is dataset introduced by WebDancer (Wu et al., 2025a), where simple questions are systematically rewritten into more complex, challenging ones. MHQA is composite dataset that integrates existing single-hop and multi-hop question-answering datasets. The majority of the questions are annotated by humans. We also compare with cutting-edge deep research methods including Search-o1 (Li et al., 2025b), WebWalker (Wu et al., 2025b), WebDancer (Wu et al., 2025a), WebThinker (Li et al., 2025c), SimpleDeepResearch (Sun et al., 2025)."
        },
        {
            "title": "4.2 Main Results",
            "content": "We compare WebShaper with cutting-edge baselines. The results are shown in Table 1. WebShaper achieves best performances on open-sourced methods on both GAIA and WebWalkerQA. Among all GAIA results, WebShaper-on Qwen-2.5-72B excels second-best method WebSailor 4.7 score. On WebWalkerQA WebShaper obtains the highest 52.2 score. WebShaper performs the best on each backbone setting. These results indicate the generalizability of the synthesized data on different models. WebShaper is currently the only open source method with score of more than 60 points, which is close to the SOTA OpenAI DR system. WebShaper is implemented fully under open-sourced LLMs, demonstrating that high-quality IS data can deeply stimulate the ability of DR Agents."
        },
        {
            "title": "4.3 Discussions",
            "content": "4.3.1 Data Statistics We analyze the domain distributions of our dataset. The domain distribution of our dataset demonstrates rather comprehensive coverage across multiple thematic areas, as visualized in Figure 5. Our construction of seed tasks leads to questions about various topics and entities. Our agentic expansion further strengthens these benefits. The dataset achieves significant diversity through its balanced representation of major domains such as Sports, Politics, and Entertainment. This deliberate design ensures our dataset not only avoids over-reliance on any single domain but also maintains sufficient sample density across diverse topics. The empirical balance between breadth and depth enables robust training of domain-agnostic information-seeking agent. Such characteristics position our dataset as particularly suitable for train multi-domain IS tasks and fostering interdisciplinary research. 4.3.2 Data Comparison Figure 5: Domain distribution. In this section, we compare WebShaper with baseline datasets. We sample 5,000 data from each dataset. Then we supervised fine-tune Qwen2.5-32B, Qwen2.5-72B (Yang et al., 2024), and QwQ (Team, 2025) on each dataset. The GAIA results are shown in Table 2. The comparative results presented in Table 2 demonstrate the superior performance of WebShaper across all backbone architectures on the GAIA benchmarks. Notably, WebShaper achieves the highest average scores for Qwen-2.5-32B, Qwen-2.5-72B, and QwQ-32B, respectively, significantly outperforming baseline datasets like WebWalkerQA and MHQA. Even when comparing models with similar parameter counts (e.g., Qwen-2.5-32B), WebShaper-enabled models show substantial improvements. The consistency of WebShapers performance improvement suggests its effectiveness in enhancing model capabilities regardless of architectural design. These findings validate the effectiveness of formalization-driven data synthesis, making it superior training data solution for information-seeking tasks. 11 Table 2: SFT Data Comparison on GAIA benchmarks. The best results among all backbones are in bolded. Backbone Dataset Level 1 Level 2 Level 3 Avg. GAIA Qwen-2.5-32B Qwen-2.5-72B QwQ-32B WebWalkerQA E2HQA MHQA WebShaper WebWalkerQA E2HQA MHQA WebShaper WebWalkerQA E2HQA MHQA WebShaper 43.5 56.4 43.5 56. 53.8 61.5 56.4 56.4 66.6 58.9 51.2 69.2 30.7 36.5 36.5 40.3 36.5 38.4 44.2 48.0 38.4 42.3 44.2 50.0 0.0 0.0 8.3 16. 0.0 16.6 0.0 0.0 8.3 16.6 0.0 16.6 32.0 39.8 35.9 43.6 38.8 44.6 43.6 45.6 45.6 45.6 41.7 53.3 4.3.3 RL Stimulation We compare GAIA performances between models trained after SFT and reinforcement learning. RL models are trained based on the SFT results. As illustrated in Figure 6a and 6b, our experimental results demonstrate significant performance improvements across both Qwen2.5-32B and Qwen2.5-72B models after RL training on both GAIA and WebWalkerQA. The Pass@1 metric shows notable enhancements of +7.8 points for the 32B model and an even more pronounced +13.5 points increase for the 72B variant on GAIA. On WebWalkerQA, WebShaper also improves IS capability on large scale. This substantial gain highlights the critical role of RL in activating advanced information-seeking capabilities within LLM. (a) GAIA. (b) WebWalkerQA. Figure 6: Comparison with SFT and RL. The breadth and complexity of tasks introduced by our task formalization stimulate dynamic IS strategies during RL. Unlike generic datasets, our carefully curated scenarios require the model to iteratively query relevant information, effectively \"training\" it to prioritize contextually aligned knowledge fragments. 4.3.4 Formalization In this part, we validate whether our formalization truly improves the dataset. We compare our dataset to variation that uses natural language during the data synthesis. This variation takes the current question in each iteration and also uses the Expander agent to expand it to new question. The Expander process in natural language as well. We SFT Qwen2.5-32B, Qwen2.5-72B, and QwQ on both datasets. The other training setting remains the same. We compare the training results with the variation as shown in Figure 7a. FL excels NL in all base model backbones. These results indicate that our formalization language can 12 (a) Formalization ablation analysis. (b) Layer-wise structure ablation analysis. Figure 7: Discussions on formalization and layer-wise structure. mitigate the limitations incurred by natural language. Our IS task formalization can synthesize more forms of tasks. It also reduces error propagation in the synthesis process, leading to consistent and precise question-and-answer pairs. 4.3.5 Layer-wise Expansion Strategy We evaluate the effectiveness of the Layer-wise structure. In order to compare, we set up variation which uses the same Expander and task formalization but expands the question in sequence as shown in Figure 4. We SFT Qwen2.5-32B, Qwen2.5-72B, and QwQ on both datasets. Other training settings remain the same. The results as shown in Figure 7b. The layer-wise structure performs better than the Sequential structure in all base models. The results show that our method truly mitigates shortcomings such as Redundancy and Reasoning shortcuts. Our method improves the final performance via the controllable structures. 4.3.6 Tool Call Analysis (a) Search distribution. (b) Visit distribution. (c) Total tool distribution. Figure 8: Tool call analysis. We show the distribution tool call count of the agent to solve question in different datasets. We illustrate the tool call counts larger than 3, which shows the complicated trajectories proportion. Search Complexity (Figure 8a) WebShaper exhibits pronounced long-tail distribution. Pretty much tasks requiring over 3 search operations. This is 3-4x higher than E2HQA and MHQA, indicating superior handling of information-rich queries requiring iterative refinement. Knowledge Navigation (Figure 8b) The visit operation distribution shows WebShaper maintains high ratio for trajectories exceeding 3 steps, while competing datasets sharply drop after 10 steps. This sustained capability reflects enhanced navigational intelligence in IS tasks. Composite Reasoning (Figure 8c) In total tool calls, WebShapers doubles the count larger than 3. Notably, it sustains non-zero proportions up to 30 tool calls, demonstrating scalability for highly complex compositional reasoning. 13 These findings underscore WebShapers unique ability to manage intricate reasoning chains, with statistically significantly higher proportions of multi-hop reasoning trajectories across all modalities. The sustained performance in extended tool call sequences suggests superior architectural capacity for managing complex task decompositions compared to existing benchmarks. 4.3.7 Case Study Figure 9: Case studies of our synthesized data. We show question in natural language, our formalization, and graph respectively. We present representative case study in Figure 9. Compared with linear structure and sequential structure, our synthesized data has no problems of redundancy and reasoning shortcuts. The model should strictly seek information and reason alongside all the variables to find the answer. There are no constants directly connected to the target variable or variables close to it. Besides, there are no constants connected to other constants. We show more cases in the Appendix C. Moreover, R-Union effects well in our data. The underlined FP is summarization of distributed web contents, leading to more difficulty in resolving the variables K, N, and M. Benefiting from the formalization, our data contains variety of IS forms, which can fully stimulate the different IS capabilities of the model."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 Information-Seeking Data Synthesis Recent advances in information-seeking agents aim to integrate web interaction into LLMs reasoning (Li et al., 2025c; Song et al., 2025; Jin et al., 2025; Shi et al., 2025b; Chen et al., 2025; Zhang et al., 2025; Wu et al., 2025c). While these works exhibit promising capabilities, they predominantly depend on limited or overly simplistic datasets (Yang et al., 2018; Joshi et al., 2017; Kwiatkowski et al., 2019). Concurrently, several recent benchmarks, such as GAIA (Mialon et al., 2023), BrowseComp (Wei et al., 2025), and BrowseCompzh (Zhou et al., 2025), provide only test sets, which restricts their applicability for training agents. Early efforts, such as WebWalkerQA (Wu et al., 2025b), explored simulating human-like web navigation to generate QA pairs by constructing linear information chains. CRAWLQA within WebDancer (Wu et al., 2025a) expands simple questions to more complex ones by aggregating external information, while SailorFog-QA within WebSailor (Li et al., 2025a) leverages entity coreference networks to support fuzzy reasoning. These methods are predominantly information-driven, focusing on strategies for retrieving and connecting knowledge. In contrast, our approach is formalization-driven, emphasizing the structural representation and principled modeling of the QA process."
        },
        {
            "title": "5.2 Formalization-based Data Synthesis",
            "content": "Formalization-based data synthesis is common in the study of theorem proving in LLM mathematics. DeepSeek-MathProver synthesizes data to train math theorem prover. It transforms high school and undergraduate level math competition problems into formal statements. It then automatically generates proofs by an LLM and verify the correctness of these proofs in Lean 4 environment (Xin et al., 2024). After that, DeepSeek-MathProverV2 decomposes the proof into subgoals. Then synthesis training data to train small model for the subgoal proof in formal statements (Ren et al., 2025). Leang et al. (2025) synthesizes the training data of Theorem Prover as Judge based on mathematical formalization. Each question needs to go through multiple formal language and natural language conversion and verification processes to ensure the validity of the data. They trained the judger on the synthetic data, and then used the judger to replace the human evaluation in RLHF (Ouyang et al., 2022), improving the effect of DPO Rafailov et al. (2023). Goedel-Prover trains LLMs to convert natural language math problems to formal statements in Lean 4. Next, it creates large dataset of formal proofs by training series of provers, where each new prover can prove statements that could not be proved by previous ones (Lin et al., 2025). Another group of related studies is synthesizing training data for knowledge base question answering. These methods formalize the KBQA question via propositional logic. LACT constructs the arbitrary first-order logical queries similar to Choudhary & Reddy (2023) via binary tree decomposition (Xia et al., 2025). This results in an SFT dataset. It then fine-tunes on an easy-to-hard curriculum to stimulate the reasoning capability of LLMs. Rather than proposition logics, our work establishes IS formalization via set theory."
        },
        {
            "title": "6 Conclusion",
            "content": "This work presents paradigm-shifting framework for synthesizing training data WebShaper for information-seeking (IS) agents through formalization-driven design. By establishing set theory-based mathematical formalization of IS tasks, we address critical limitations in existing information-driven approaches that suffer from structural inconsistencies, task controllability, diversity, and coverage. The composition of proposed Knowledge Projections enables precise engineering of reasoning structures and complexity. Our agentic Expander module further ensures systematic expansion of formalized tasks with layer-wise expansion paradigm, combining autonomous knowledge retrieval and rigorous validation to minimize redundancy and prevent reasoning shortcuts. Experimental results demonstrate that WebShaper not only achieves state-of-the-art performance on GAIA and WebWalkerQA benchmarks but also introduces controllability over task design, enabling deliberate engineering of cognitive challenges for IS agents. This formalization-driven paradigm shifts the focus from reactive information organization to proactive task specification, opening new avenues for advancing agent capabilities."
        },
        {
            "title": "References",
            "content": "AutoGPT. AutoGPT: The heart of the open-source agent ecosystem, 2023. URL https://github.com/S ignificant-Gravitas/Auto-GPT. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. Research: Learning to reason with search for llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2503.19470. Nurendra Choudhary and Chandan Reddy. Complex logical reasoning over knowledge graphs using large language models. arXiv preprint arXiv:2305.01157, 2023. Gemini. Gemini deep research, 2025. URL https://gemini.google.com/app. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Jina.ai. Jina, 2025. URL https://jina.ai/. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Kimi. Kimi deep research, 2025. URL https://www.kimi.com/. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Joshua Ong Jun Leang, Giwon Hong, Wenda Li, and Shay Cohen. Theorem prover as judge for synthetic data generation. arXiv preprint arXiv:2502.13137, 2025. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025a. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025b. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. CoRR, abs/2504.21776, 2025c. doi: 10.48550/ARXIV.2504.21776. URL https://doi.org/10.48550/a rXiv.2504.21776. Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, et al. Goedel-prover: frontier model for open-source automated theorem proving. arXiv preprint arXiv:2502.07640, 2025. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. 16 Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming language. In International Conference on Automated Deduction, pp. 625635. Springer, 2021. OpenAI. Deep research system card, 2025. URL https://cdn.openai.com/deep-research-system-c ard.pdf. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Perplexity. Perplexity deep research, 2025. URL https://www.perplexity.ai/. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. ZZ Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, et al. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. Dingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong, Tianrui Qin, King Zhu, Minghao Yang, et al. Taskcraft: Automated generation of agentic tasks. arXiv preprint arXiv:2506.10055, 2025a. Wenxuan Shi, Haochen Tan, Chuqiao Kuang, Xiaoguang Li, Xiaozhe Ren, Chen Zhang, Hanting Chen, Yasheng Wang, Lifeng Shang, Fisher Yu, and Yunhe Wang. Pangu deepdiver: Adaptive search intensity scaling via open-web reinforcement learning, 2025b. URL https://arxiv.org/abs/2505.24332. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, et al. Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. arXiv preprint arXiv:2505.16834, 2025. QwQ Team. Qwq-32b: Embracing the power of reinforcement learning, 2025. URL https://qwenlm.git hub.io/blog/qwq-32b/. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025a. 17 Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal, 2025b. URL https://arxiv.org/abs/2501.07572. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. arXiv preprint arXiv:2308.08155, 2023. Weiqi Wu, Xin Guan, Shen Huang, Yong Jiang, Pengjun Xie, Fei Huang, Jiuxin Cao, Hai Zhao, and Jingren Zhou. Masksearch: universal pre-training framework to enhance agentic search capability, 2025c. URL https://arxiv.org/abs/2505.20285. Tianle Xia, Liang Ding, Guojia Wan, Yibing Zhan, Bo Du, and Dacheng Tao. Improving complex reasoning over knowledge graph with logic-aware curriculum tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 1288112889, 2025. Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. arXiv preprint arXiv:2405.14333, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization empowers large language models with multimodality. CoRR, abs/2304.14178, 2023. Dingchu Zhang, Yida Zhao, Jialong Wu, Baixuan Li, Wenbiao Yin, Liwen Zhang, Yong Jiang, Yufeng Li, Kewei Tu, Pengjun Xie, and Fei Huang. Evolvesearch: An iterative self-evolving search agent, 2025. URL https://arxiv.org/abs/2505.22501. Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314, 2025. He Zhu, Tianrui Qin, King Zhu, Heyuan Huang, Yeyi Guan, Jinxiang Xia, Yi Yao, Hanhao Li, Ningning Wang, Pai Liu, Tianhao Peng, Xin Gui, Xiaowan Li, Yuhui Liu, Yuchen Eleanor Jiang, Jun Wang, Changwang Zhang, Xiangru Tang, Ge Zhang, Jian Yang, Minghao Liu, Xitong Gao, Jiaheng Liu, and Wangchunshu Zhou. Oagents: An empirical study of building effective agents, 2025. URL https://arxiv.org/abs/2506.15741."
        },
        {
            "title": "A Agent Details",
            "content": "Following Wu et al. (2025a), WebComposer uses two tools, search and visit, which are regarded as fundamental to the information seeking process (Zhu et al., 2025): Search interfaces with the Google search engine to retrieve relevant documents given natural language queries. It supports multiple queries in parallel and returns the top-10 results for each query, where each result includes title, snippet, and the corresponding URL. Visit enables targeted extraction from specific web pages. Each page is paired with designated visit goal. The full content of the page is first retrieved using Jina (Jina.ai, 2025), after which summarization model (Qwen-2.5-72B in our implementation) extracts information relevant to the specified goal."
        },
        {
            "title": "B Training Details",
            "content": "B.1 SFT For SFT, we use batch size of 32 and learning rate of 5e-6, warmup plus cosine decay schedule. We also apply weight decay of 0.1. B.2 RL For RL training (Sheng et al., 2025), each group consists of 8 rollouts. The temperature is 1.0, topp = 1.0, the batch size is 128, the mini batch size is 32, and the learning rate is 1e-6."
        },
        {
            "title": "C Case Study",
            "content": "Figure 10: Case comparison. SSCS stands for \"Strange Stories from Chinese Studio\". We compare representative example shown by KIMI-Researcher (Kimi, 2025), illustrated in Figure 10. The case includes redundant information, such as multiple constants connected to SSCS, which contribute little to answering the question. Additionally, reasoning shortcut is observed that directly connects to the target variable. Despite the apparent complexity, the underlying reasoning structure is relatively simple, consisting of single-hop reasoning path."
        },
        {
            "title": "D Broader Impact",
            "content": "Our data synthesis framework presents foundational methodology for constructing training data for intelligent agents, featuring two key innovations: task formalization and agent-driven synthesis. By explicitly modeling tasks as structured, formal representations and leveraging proxy agents to synthesize data, this work provides systematic approach to address the critical challenge of generating training data that transcends the complexity and unpredictability of naturally occurring human-centric environments. Below, we discuss the broader implications for agent research. Implications in Agent Training Data Synthesis Traditional approaches to training agents often rely on datasets derived from human-generated interactions, which are inherently limited in diversity, scalability, and controllability. We emphasize that effective agent training requires explicit formalization of task structuresa prerequisite for achieving precise control over data properties. By decoupling task definitions from data generation, the framework enables: Targeted Complexity Management: Tasks can be systematically parameterized to adjust difficulty, modality, or compositional structure, ensuring agents are exposed to controlled gradients of challenge. This contrasts with ad-hoc methods that risk overfitting to biases in natural data or failing to stress-test edge cases. Quality Assurance: Formal task models act as \"specification\" for data synthesis, reducing noise and ensuring consistency. This is critical for applications where reliability and safety are paramount, such as autonomous systems or medical AI. Scalable Data Generation: Agent-driven synthesis eliminates the need for laborious manual annotation or heuristic-based pipelines by directly translating formal task representations into training instances. This reduces computational overhead while preserving fidelity to the tasks intended design. Implications for AI Research and Development Our architecture provides insights for advancing AI systems: Beyond Human-Level Complexity: By formalizing tasks independent of human behavioral priors, the framework enables training data to exceed the implicit constraints of natural data. This opens pathways to train agents for domains requiring superhuman reasoning (e.g., advanced scientific modeling, combinatorial optimization). Cross-Domain/Task Generalization: Formal task representations abstract away domain-specific noise, allowing agents to learn invariant principles applicable across diverse contexts."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}