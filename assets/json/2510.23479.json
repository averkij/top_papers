{
    "paper_title": "MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding",
    "authors": [
        "Xin Jin",
        "Siyuan Li",
        "Siyong Jian",
        "Kai Yu",
        "Huan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language alignment in multi-modal large language models (MLLMs) typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). SFT is stable and efficient but requires large-scale human annotations and cannot capture subtle preferences, while RL brings in a reward signal for training, but suffers from overhead and instability. These limitations highlight a trade-off between scalability, robustness, and alignment quality. To address this, we propose MergeMix, a training-time augmentation paradigm that bridges SFT and RL. It first applies an attention-aware image mixing via token merge with more cluster representation and spatial context, and then presents a preference-driven training paradigm for MLLMs by building preference pairs with mixed images and raw images, and optimizing via SimPO loss. As a mixup augmentation, MergeMix enhances attention consistency and efficiency, surpassing other heuristic-based methods in classification. Extensive experiments demonstrate that MergeMix achieves competitive accuracy with improved efficiency, providing a scalable approach to preference alignment in classification and MLLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 9 7 4 3 2 . 0 1 5 2 : r MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding MERGEMIX: UNIFIED AUGMENTATION PARADIGM FOR VISUAL AND MULTI-MODAL UNDERSTANDING Xin Jin1, Siyuan Li1,2, Siyong Jian 1 Kai Yu 1 Huan Wang1, 1Westlake University, Hangzhou, China 2Zhejiang University, College of Computer Science and Technology, Hangzhou, China {jinxin86; lisiyuan; wanghuan}@westlake.edu.cn Equal contribution https://github.com/JinXins/MergeMix Corresponding author"
        },
        {
            "title": "ABSTRACT",
            "content": "Visionlanguage alignment in multi-modal large language models (MLLMs) typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). SFT is stable and efficient but requires large-scale human annotations and cannot capture subtle preferences, while RL brings in reward signal for training, but suffers from overhead and instability. These limitations highlight trade-off between scalability, robustness, and alignment quality. To address this, we propose MergeMix, training-time augmentation paradigm that bridges SFT and RL. It first applies an attention-aware image mixing via token merge with more cluster representation and spatial context, and then presents preference-driven training paradigm for MLLMs by building preference pairs with mixed images and raw images, and optimizing via SimPO loss. As mixup augmentation, MergeMix enhances attention consistency and efficiency, surpassing other heuristic-based methods in classification. Extensive experiments demonstrate that MergeMix achieves competitive accuracy with improved efficiency, providing scalable approach to preference alignment in classification and MLLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Multi-modal Large Language Models (MLLMs) (Liu et al., 2024b; Bai et al., 2025; Tong et al., 2024) have recently demonstrated remarkable capabilities in integrating visual and textual information, enabling wide range of applications from visual question answering to multi-modal reasoning. Since MLLMs are typically pre-trained on massive web-scale datasets, forcing them to possess wide range of knowledge and general reasoning capabilities, Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)-based preference optimization (Yang et al., 2025b) have emerged as two primary paradigms for aligning MLLMs with human preferences and specific task requirements. However, SFT depends on high-quality instructionresponse annotations and optimizes the likelihood of reference responses, which does not explicitly model relative preferences between outputs. RL-based methods such as RLHF are more preference-aware, but they require an additional reward model that may encode bias or be exploited by reward. Due to the shortcomings of data quality and training efficiency, some works (Zhu et al., 2024; 2025; Luo et al., 2024; Tan et al., 2025; Wang et al., 2024) try to build performance pairs for optimization. How to build the preference pair with control and high-quality data for model training is the remaining open question. For example, SeVa (Zhu et al., 2024) proposed preference optimization method by building loser through some classic augmentation (i.e., RandomCrop). Then, select the different responses for optimizing the model by DPO loss (Rafailov et al., 2023). However, these methods have two drawbacks: the augmentations are highly random, and the DPO loss cannot be related to the data, which means SeVa can only select useful training data. Those technical causes SeVa can not control the quality of the loser, which is harmful for some visual question answering tasks, and reduces the training data by selecting hard negatives. Hence, we investigate an interesting question: Is it necessary to propose novel techniques rather than some classical machine learning methods in the MLLM scenario? 1 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Figure 1: Efficiency and performance for MergeMix: (a) The training time vs. accuracy of mixup methods with the DeiT-Small model. training epochs of different mixup methods on the CIFAR100 dataset with the DeiT-Tiny model. (c) The radar plot of the results on part VQA tasks by LLaVA-7B, LLaVA with SFT, and MergeMix. (b) The image classification Top-1 accuracy vs. In this paper, we revisit the mixup augmentation, which synthesizes mixed samples and corresponding labels with given mixing ratios. However, two main challenges arise as illustrated in Figure 1: (1) achieving an optimal trade-off between efficiency and performance of mixup augmentations that rely on saliency-based metrics, (2) extending the augmentation to MLLMs properly, from classical image corruptions to data-dependent samples. Motivated by these perspectives, we propose novel training framework called MergeMix, which builds preference pairs for MLLM training through data augmentation methods and ranking loss, thereby bridging the gap between SFT and RL. Figure 2 shows the two scenarios of MergeMix. (a) We introduce MergeMix, novel data augmentation that generates mixed samples through token merge techniques. bipartite soft matching gathers the similarity information that brings the context, making the mask retain useful features. Meanwhile, MergeMix links the merge ratio and mixing ratio, aligning the information density of samples, enabling precise mixing data generation. (b) We propose preference-driven SFT paradigm for MLLMs, where augmented samples are regarded as non-preferred responses (Loser) and clean samples as preferred responses (Winner). This enables preference optimization via SimPO loss (Meng et al., 2024) without relying on reward models. Altogether, Figure 1 shows these contributions yield an efficient and effective training strategy that achieves stronger alignment with human preferences while preserving the stability and scalability of SFT. Since the optimization object has direct relationship with augmentation, it obtains more robust ability in calibration. Extensive experiments show that MergeMix, as training-time augmentation paradigm, achieves competitive performance in both image classification and MLLM benchmark with favorable efficiency. Our contributions can be summarized as: (a) We use the naive token merge to obtain local clustered attention map and recover the initial shape by source map, which can generate the mixed image with more continuous features. (b) We design training paradigm for supervised fine-tuning of MLLMs by generating augmented images as losers, building preference pairs with raw images, and optimization via ranking loss. (c) We validate that our method achieves state-of-the-art on several image classification datasets and benchmarks, along with the advantages of our training paradigm on several MLLM benchmarks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "In this section, we introduce the existing mixup approaches for image classification and token compression approaches in multi-modal large language models for efficient training or inference. Mixup Augmentations The Mixup method mitigates model overfitting by generating augmented samples through mixing two different images within mini-batch. Broadly, Mixup methods can be categorized into two types: Static, which relies on human priors or randomness, and Adaptive, data-dependent type that leverages certain metrics to guide the mixing process. (i) Static: 2 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Figure 2: The overall of the two scenarios of MergeMix: (a) MergeMix for Image Classification: The image is processed by the ToMe encoder, with Attention Score Recovery and TopK sampling to generate the corresponding class prediction. (b) MergeMix for MLLM: Preference pairs are encoded by the vision model with token merging, and the LLM decoder generates response text for the loser and winner, optimized via ranking loss. MixUp (Zhang et al., 2017) generates mixed samples via linear interpolation with λ. CutMix (Yun et al., 2019) extends this idea from the global pixel level to local patch level by constructing mask of size proportional to λ to mix images. ResizeMix (Qin et al., 2020) ensures that features from at least one class are always preserved in the mixed sample by resizing the source image before mixing. Other methods, e.g., FMix (Harris et al., 2020), SmoothMix (Lee et al., 2020), GridMix (Baek et al., 2021), and StarMix (Jin et al., 2024b), focus on improving the mask to obtain more suitable mixed samples. (ii) Adaptive: SaliencyMix (Uddin et al., 2021) employs saliency extractor to identify informative patches in images for mixing. Attentive-CutMix (Walawalkar et al., 2020) and SuperMix (Dabouei et al., 2021) utilize teacher model to guide mask generation. PuzzleMix (Kim et al., 2020) and Co-Mix (Kim et al., 2021) generate appropriate masks based on gradient information obtained from forward pass of the samples. AutoMix (Liu et al., 2022) and AdAutoMix (Qin et al., 2024a) adopt an end-to-end bi-optimization paradigm to produce mixed samples. TransMix (Chen et al., 2022), SMMix (Chen et al., 2023), and MixPro (Zhao et al., 2023) specifically enhance ViTs by computing attention scores from forward pass to generate feature-aware masks and further refine the label ratio through attention scores. Token Compression in MLLMs Token Merging (Bolya et al., 2023) proposes to merge similar tokens by Key similarity for ViT-based models to achieve efficiency and acceleration. In MLLMs, images and texts will incur significant number of tokens, which are often full of redundant information. Obvious researchers bring the token compression into MLLMs. Overall, we divide the methods that reduce tokens into 2 types, Reduce in Encoder and Reduce in Decoder. (i) Reduce in Encoder: MADTP (Cao et al., 2024) aims to achieve MLLM acceleration by purging visual tokens. LLaVA-PruMerge (Shang et al., 2024) uses the attention of [CLS] token to select clustering centers and then merges the remaining tokens with lower attention through KNN clustering and weighted clustering center updating mechanism. VisionZip (Yang et al., 2025a), instead, retains visual tokens with high attention scores and subsequently merges the remaining tokens through clustering. Others, such as TokenPacker (Li et al., 2025), AVG-LLaVA (Lan et al., 2025), MustDrop (Liu et al., 2024c), and LLaVolta (Chen et al., 2024a), achieve acceleration by choosing metric to sample TopK visual tokens. FastVLM (Vasu et al., 2025) proposes an Efficient Vision Encoder to reduce visual tokens. (ii) Reduce in Deocder: PyramidDrop (Xing et al., 2024) divides the token compression process in LLM into multiple stages, which employs pyramidal token drop to avoid losing too much visual information in shallower layers. ATP-LLaVA (Ye et al., 2025) proposes an Adaptive Token Pruning (ATP) module that reduces the number of tokens in the decoder layer. ZipVL (He et al., 2024) proposes dynamic ratio allocation strategy via the importance token, adaptively determined based on the distribution of attention scores in particular layer, rather than fixed hyperparameter."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Reformulation of Mixup Augmentation. We define to be the set of training samples and the set of ground truth of the corresponding labels. For each sample pair (x, y), we randomly sample two pairs in and Y, with λ in Beta(α, α), the mixed images and labels are generated by applying 3 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding the optimized mask and ratio ˆλ, which come from defined policy P(, , ) according to Eq. (1): M, ˆλ = P(cid:0)fθ(xi, xj), (yi, yj), λ(cid:1), (1) ˆx = xi + (1 M) xj, ˆy = ˆλ yi + (1 ˆλ) yj, (2) where the denotes element-wise multiplication. Policy P(, , ) aims for the to retain more features in the mixed sample. The ˆλ keeps the initial sampling mixing ratio when the policy is without optimization; otherwise, the λ can be re-computed by some metrics. Preference Tuning for MLLMs. Preference optimization methods aim to align LLMs and MLLMs with human feedback by contrasting preferred and dispreferred responses. general preference loss can be abstractly defined as Eq. (3): LPref = log σ(cid:0)πθ(x, y+) πθ(x, y)(cid:1), where (x, y+) and (x, y) denote the preferred and dispreferred responses respectively, and sθ is scoring function that reflects model preference. Different approaches (e.g., PPO (Schulman et al., 2017), DPO (Rafailov et al., 2023)) instantiate πθ in various ways, but all share the same principle. (3) Unlike RL-based approaches, which require training separate reward model, DPO provides simple and stable alternative by directly optimizing the policy model using preference pairs. Formally, the DPO loss is defined as Eq. (4): LDPO = E(x, y+, y) (cid:104) log σ(cid:0)β log πθ(y+x) πref(y+x) β log πθ(yx) πref(yx) (cid:1)(cid:105) , (4) where πθ denotes the policy model (in our case, an MLLM such as LLaVA-v1.5 (Liu et al., 2024b) or Qwen-VL (Bai et al., 2025)), and πref represents frozen reference model used to preserve alignment with the original pre-trained distribution. σ is the sigmoid function, and β > 0 is temperature-like scaling factor that controls the sharpness of preference separation. Intuitively, DPO encourages the policy to assign higher likelihood to preferred responses (y+) than to non-preferred ones (y), while maintaining proximity to the reference model."
        },
        {
            "title": "4 MERGEMIX TRAINING PARADIGM",
            "content": "In this section, we present the implementation of MergeMix, an augmentation approach via token merging for image mixing, not only for image classification, but also designed for multi-modal large language models. Figure 3 shows the overall pipeline of MergeMix, and we describe in two subsections in detail, which are from the input space to the loss objective for model training. 4.1 IMAGE MIXING VIA TOKEN MERGE In MergeMix, we leverage the relationship between the merge ratio and mixing ratio. The merge ratio measures the information of raw samples, while the mixing ratio balances the information between mixing samples, thereby enabling precise data generation of mixed inputs and labels. In this subsection, we first introduce MergeMix on the input space. Then we use the designed mixing policy P(, , ) to obtain the mixed images ˆx with the mask M. Image Policy with Token Merging. Unlike other mixup methods (Chen et al., 2022; 2023; Zhao et al., 2023), we introduce ViT-based model fθ() iteratively replace attention layers with ToMeAttention as ToMe (Bolya et al., 2023), Given the initial sequence ZL = fθ(ˆx), then merges tokens as Eq. (5): S, AK, ZK = ToMeAttention(ZL, r), (5) where AK denotes the attention map from the model, and ZK denotes the feature tokens for computing one-hot loss. denotes the number of merged tokens, which can reduce some high-similarity semantic tokens and retain condensed token sequence. Also, based on Token Merge, we obtained source map for their spatial relationships between the raw token sequence ZL and the ZK. 4 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Generating Mixing Mask with Source Matrix. Since token merge aggregates non-similar tokens into compact representations ZK, the resulting matrix preserves local feature structures more effectively. In contrast, the vanilla TopK selection adopts greedy sampling strategy with linear complexity O(N ), which discards low-ranked tokens directly and thus loses spatial relationships. Alternatively, the Bipartite Soft Matching (BSM) approach performs global pairwise matching with quadratic complexity, yielding more balanced and globally optimal merging of tokens. To reconstruct the full-resolution attention map, we introduce recovering function RKL(, ) that expands the merged attention map AK back to its original length AL according to Eq. (6): ˆAL = RKL(AK, S). (6) Unlike discrete TopK sampling, our recovery mechanism propagates merged attention over the original token topology guided by similarity S, restoring richer spatial dependencies and contextual continuity, thus reducing information loss from hard selection. Based on the encoder with token merge and attention recovery. We can generate the binary according to Eq. (7): Mi = (cid:26) 1, 0, if TopK( ˆAL, p), otherwise, (7) where denotes the selection number, = λ L, and denotes the index of sequence. Finally, we can mix the mini-batch and get the augmented data for fθ() training. 4.2 UNIFIED AUGMENTATION PARADIGM: FROM IMAGE CLASSIFICATION TO MLLMS In this subsection, we describe the loss function L. For the classification task and visual understanding, our final loss LTotal combines two losses: the main loss (one-hot cross entropy loss LCE and LSFT) and the reformulated loss (mixup cross entropy loss LMCE and ranking loss LMix SimPO). Figure 3 shows the pipeline of MergeMix for MLLM in detail. Re-scaling Policy for Mixing Ratio. Under this optimization objective, the role of the mixing ratio λ is to serve as metric that quantifies the presence of feature information from the two samples. While this metric cannot directly reflect the true characteristics of the data, certain adaptive methods can constrain the model to generate mixed samples where the mixing ratio progressively approximates the target value (Jin et al., 2024a). Some works, like LUMix (Sun et al., 2022), DecoupleMix (Liu et al., 2023), and SUMix (Qin et al., 2024b), use defined policy for some hand-crafted mixup methods, and find that it is more efficient than optimizing better mask way. Figure 3: Overall illustration of MergeMix for MLLM. MergeMix performs attention-based mask mixing guided by the ToMe Vision Encoder, recovering token attention scores and generating mixed image through an augmenter. Specifically, Token Merging hierarchically merges visual tokens through Bipartite Soft Matching (BSM) and merges to enhance efficiency. The model is trained with both the SFT and ranking losses. Since we introduce token merge technology that inherently enables information aggregation and selection, the entire training process of the model requires consideration of not only simple spatial ratios but also the degree of information integration within the model. So, we proposed Gaussian-based sampling to refine the ratio, where the merged tokens and the mask values jointly control the mean and std. This smooth transition directly alleviates changes from linear mapping and yields more robust augmentations, with its formulation given in Eq. (8): ˆλ (µ, σ), ˆλ = clip(cid:0) ˆλ min(ˆλ) max(ˆλ) min(ˆλ) + τ , 0, 1(cid:1), (8) 5 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding where the (, ) denotes Gaussian function, µ and σ represent the merged ratio and mixing ratio repetitively. τ as hyperparameter, set to 1e-5. Then, we obtain the re-scaled mixing ratio ˆλ with spatial and ToMe model inherent features to optimize the model when training. In total, the loss of mixup training as Eq. (9): LTotal = LCE (cid:0)fθ(ˆx), yj (cid:0)fθ(ˆx), yi . (cid:1) ˆλ + LCE (cid:123)(cid:122) mce loss (cid:1) (1 ˆλ) (cid:125) + LCE (cid:124) (cid:0)fθ(x), y(cid:1) (cid:125) (cid:123)(cid:122) one-hot loss (9) (cid:124) Aggregating Mixing Ratio within Preference Loss. LLaVA (Liu et al., 2024b) uses standard conditional language modeling loss for SFT. In MLLMs, we are given an instructionresponse pair (x, y), where denotes the multi-modal data, inducing vision and text, and = (y1, yy) denotes the target response. The SFT loss is defined as: LSFT = E(x,y)D log πθ (cid:0)yt x, y<t (cid:1) (cid:35) . (cid:34) (cid:88) t=1 (10) This objective needs to maximize the likelihood of GT responses, aiming to align the data. In Section 3, we introduced the DPO loss, which can be decomposed into two components: the SFT part and the ranking optimization part. In our approach, we replace the ranking component with SimPO Meng et al. (2024), where denotes the target sequence (response) and denotes its length. Furthermore, Since λ reflects information similarity between augmented and raw image (interpreted as loser degree in MLLMs), we link it to γ 1 λ: larger λ represent higher similarity and harder discrimination, reduces γ to avoid over-optimization on trivial differences; smaller λ represent greater dissimilarity and easier tasks increases γ to strengthen constraints for clearer preference distinction. The mixed SimPO loss replacement is Eq. (11): log σ(cid:0) β log πθ(y ˆx) (1 λ)(cid:1)(cid:105) SimPO = E(x,ˆx,y)D LMix log πθ(y x) β (11) (cid:104) . This reformulated loss strictness with sample difficulty, enabling more robust preference optimization. Finally, the total loss of our training paradigm is Eq. (12): SimPO. LTotal = LSFT + LMix (12)"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 BASELINES Image Classification. To evaluate the performance of MergeMix, we compare our MergeMix with some mainstream mixup methods, i.e. Mixup (Zhang et al., 2017), CutMix (Yun et al., 2019), FMix (Harris et al., 2020), SmoothMix (Lee et al., 2020), GridMix (Baek et al., 2021), ResizeMix (Qin et al., 2020), SaliencyMix (Uddin et al., 2021), Attentive-CutMix (Walawalkar et al., 2020), PuzzleMix (Kim et al., 2020), GuidedMixup (Kang & Kim, 2023), AutoMix (Liu et al., 2022) and AdAutoMix (Qin et al., 2024a). DeiT (Touvron et al., 2021), TransMix (Chen et al., 2022), SMMix (Chen et al., 2023) and MixPro (Zhao et al., 2023) for some ViT-based methods. The training configures about datasets and methods follows the open-source library OpenMixup (Li et al., 2022). MLLMs. To evaluate the training paradigm that we proposed, we compare with three different system-level baselines: (1) SFT with different training paradigms on LLaVA, including LLaVANeXT-7/13B (Liu et al., 2024a), SeVa-7B (Zhu et al., 2024), SIMA (Wang et al., 2024), and nSFT (Zhu et al., 2025). (2). Token reduction on LLaVA, including LLaVA-PruMerge+ (Shang et al., 2024), VisionZip (Yang et al., 2025a), VisPrunner (Zhang et al., 2024b), VScan (Zhang et al., 2025a), and LLaVA-Mini (Zhang et al., 2025b). (3). RL training on Qwen2.5-VL-Instruction (Bai et al., 2025), including VisionThink (Yang et al., 2025b). For all classification results, we report the top-1 test accuracy in the last 10 training epochs for each trial. To facilitate comparison, we mark the best and second best results in bold and cyan. For the LLaVA benchmark, we use the LLaVA official code, and for the Qwen2.5-VL-Instruction benchmark. We use lmms-eval (Zhang et al., 2024a) for evaluation. 5.2 DATASETS MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Table 1: Top-1 accuracy (%) of mixup methods training 200 epochs on CIFAR100 dataset with different model sizes, T/S/B/L denotes Tiny, Small, Base, and Large, respectively. The full results of training 600 epochs are in Table 9. Table 2: Top-1 accuracy (%) of mixup methods on the Stanford-Cars dataset. Full results of the CUB200 and FGVCAircrafts dataset in Table 10. Method DeiT-T DeiT-S ViT-S ViT-B ViT-L Vanilla MixUp CutMix FMix GridMix ResizeMix SaliencyMix PuzzleMix AutoMix AdAutoMix DeiT TransMix SMMix MixPro MergeMix 64.70 69.47 75.98 72.73 71.54 69.42 69.83 73.40 72.91 72.83 74.01 75.31 73.84 74.78 77.46 65.81 69.98 74.21 70.41 68.86 68.54 69.78 73.60 76.24 72.63 75.92 76.17 74.09 75.26 78. 62.64 68.67 69.67 68.41 70.15 67.86 70.14 70.92 68.44 69.66 72.96 74.15 73.50 73.49 77.02 63.33 69.66 72.18 68.62 66.63 63.72 68.75 71.13 73.40 71.43 72.15 72.87 70.87 73.18 75. 61.83 67.90 68.97 66.12 63.20 63.48 67.12 69.77 72.10 69.69 69.23 71.40 71.38 72.28 76.19 Method Vanilla MixUp CutMix SmoothMix FMix GridMix ResizeMix Attentive-CutMix SaliencyMix PuzzleMix GuidedMixap DeiT TransMix SMMix MixPro MergeMix α 1.0 0.2 0.2 0.2 0.2 1.0 2.0 0.2 1.0 1.0 0.2 1.0 1.0 1.0 1.0 DeiT-S ViT-B 86.77 87.73 88.37 86.39 87.18 87.58 87.45 87.35 87.94 88.60 86.99 88.72 88.38 88.76 88.38 89.42 91.31 91.36 91.53 90.88 91.36 91.31 91.59 90.29 91.47 91.83 90.40 92.17 91.66 91.93 91.48 92. Method DeiT-Small Table 3: The ImageNet-1K dataset classification results on Top1 Accuracy (Acc), Dynamic Forward, Throughput (TP/s) and FLOPs (G) in NVIDIA A100. ForwardDy. denotes the metric through forward with dynamic. In our paper, we mainly divided into 2 scenarios: Image Classification and MLLM Benchmark. The detailed information about datasets is described in Appendix B.1. For the image classification datasets, we choose 5 public classification datasets, including the smallscale dataset of CIFAR100 (Krizhevsky et al., the large-scale dataset of ImageNet2009), 1K (Russakovsky et al., 2015), and the finegrained datasets of CUB200 dataset (Wah et al., 2011), FGVC-Aircrafts dataset (Maji et al., 2013), and Stanford-Cars dataset (Krause et al., 2013). For the MLLM datasets, we choose 16 datasets, including visual question answering (VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), VizWiz (Gurari et al., 2018), ScienceVQAI (Lu et al., 2022), TextVQA (Singh et al., 2019), MME-RealWorldQA (Zhang et al., 2025c)), understanding (MME (Perception) (Yin et al., 2023), MMBench (Liu et al., 2025), MMBenchCN, MMBenchCC, POPE (F1 score) (Li et al., 2023b), SEEDI (Li et al., 2023a), MMStar (Chen et al., 2024b)), and reasoning (MMMU (Yue et al., 2024a), MMMU-Pro Standard (MMMU-Pros) (Yue et al., 2024b), MathVista (Lu et al., 2024)). ForwardDy. Vanilla MixUp CutMix DeiT TransMix SMMix MixPro 1375.80 1374.54 1374.61 1374.20 1375.17 1373.93 1373.62 75.66 77.80 80.13 79.80 80.44 79.36 79.33 4.24 4.24 4.24 4.24 4.24 4.24 4.24 FLOPs (G) Acc (%) MergeMix 1591. 80.71 TP/s 3.56 5.3 IMPLEMENTATIONS In this subsection, we briefly introduce the implementations on the classification task and the MLLM benchmark. The full description is in Appendix B.2. For the CIFAR100, images are resized to 224 224 for ViT-based models (e.g., DeiT) and trained with AdamW (weight decay 0.05), batch size 100, for 200 or 600 epochs. We use RandomFlip and RandomCrop, plus RandAugment (Cubuk et al., 2020). Learning rates are 1e-3 (DeiT-Tiny/Small, cosine schedule), 5e-4 (ViT-Small/Base), and 2e-4 (ViT-Large). For ImageNet-1K, we adopt the same settings but use 1e-3 learning rate, batch size 1024, and 300 epochs for DeiT-Tiny/Small. For fine-grained datasets (CUB200, FGVCAircrafts, Stanford-Cars), we fine-tune DeiT-Small and ViT-Base for 200 epochs, batch size 16, learning rate 1e-5, using PyTorch pretrained weights (Paszke et al., 2019). Following LLaVA-v1.5, we adopt Vicuna-v1.5 7B (Chiang et al., 2023) as the language decoder and use pre-trained 2-layer MLP projection to align visual and textual modalities, trained for one epoch on LCS-558K. The vision encoder is pre-trained CLIP model that extracts image representations. During SFT, we train for one epoch on llava-v1.5-mix665k with 2e-5 learning rate, batch size 64, and AdamW optimizer, using 0.03 warmup ratio and cosine scheduler. Unlike LLaVA, we unfreeze the vision encoder during training. For Qwen2.5-VL-Instruction, we 7 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Table 4: Full system-level comparison results in LLaVA. Compared with their counterparts. AVG denotes the average of the nine benchmarks for comprehensive comparison, except for MME, underline denotes MME with the sum of Perception and Cognition. Tokeni denotes training with the token number. Full results in Table 16, Table 17 and Table 18. Models Tokeni Image Question Answering Benchmarks VQAv2 GQA VizWiz SciVQAI TextVQA MME MMBench MMBenchCN POPE SEEDI AVG Gain Full LLaVA-7B LLaVA-NeXT-7B Full LLaVA-NeXT-13B Full Full SeVa-7B Full SIMA Full nSFT LLaVA-PruMerge+ VisionZip VisPrunner VScan LLaVA-Mini SFT Vision + MixUp + CutMix + ResizeMix + MergeMix SFT Vision + MixUp + CutMix + ResizeMix + MergeMix 144 192 128 192 1 Full Full Full Full Full 288 288 288 288 78.5 81.8 82.8 76.8 77.4 75.8 77.8 77.6 62.0 64.2 65.4 60.7 62.2 62.9 60.1 58.2 60.6 60.9 50.0 57.6 60.5 54.4 52.7 50.4 56. LLaVA Variants 66.8 70.1 73.6 67.5 68.1 68.5 58.2 64.9 67.1 56.2 58.3 58.7 1510.7 1519.0 1575.0 1450 1507.7 1531 LLaVA with Token Compressions 68.3 68.2 69.1 68.6 70. 57.1 57.8 57.0 57.7 57.0 1462.4 1834.0 1461.4 1806.0 1466.0 64.3 67.4 70.0 65.6 64.9 67.1 64.9 63.4 62.7 63.9 65.6 LLaVA with Augmentations & Ranking Loss 79.32 62.98 47.45 79.27 62.58 44.95 79.18 62.40 45.04 77.78 61.66 44.43 79.24 62.44 47. 62.47 48.15 78.6 78.51 62.07 51.1 78.58 62.39 50.53 76.39 61.05 45.48 78.61 62.18 52.14 70.05 69.41 70.60 68.91 69.86 69.51 68.47 70.2 68.07 69.61 57.17 57.39 57.06 55.11 57.56 56.41 56.54 55.95 54.60 56.85 1490.88 1483.20 1452.31 1436.09 1479. 1486.24 1459.06 1414.72 1447.35 1453.97 66.26 65.72 66.32 63.91 66.58 66.32 65.63 66.92 63.31 66.58 58.3 60.6 64.4 59.2 59.0 61.0 57.3 57.4 60.05 58.24 58.24 55.41 60. 57.98 59.53 59.53 51.97 59.02 85.87 66.19 65.57 69.3 86.5 71.3 86.2 86.7 86.5 86.8 70.2 71.9 65.8 65.9 66.2 84.0 84.9 84.6 86.2 84.4 57.1 58.5 86.18 67.32 66.31 +0.74 86.27 66.73 65.62 +0.05 86.47 67.22 65.84 +0.27 86.01 63.91 64.13 -1.44 86.10 67.47 66.40 +0.83 87.37 66.75 65.95 +0.38 86.86 66.06 66.08 +0.51 66.31 +0.74 86.56 86.57 62.54 63.33 -2.24 86.47 66.63 66.45 +0.88 66.2 Table 5: Full system-level comparison results in Qwen2.5-VL-Instruction (Qwen2.5-VL-Ins). AVG denotes the average of the nine benchmarks for comprehensive comparison. Models MMStar MMBench MMBenchCN MMBenchCC POPE RWQA MMMU MMMU-Pros MathVista AVG Gain Qwen2.5-VL-Ins-7B 62.42 61.00 VisionThink-7B SFT Vision + MergeMix 62.66 62.92 84.02 82.73 83.41 84. 80.41 81.01 81.01 81.18 62.94 64.5 63.52 64.31 86.38 68.63 87.65 69.28 87.69 68.63 87.28 70. 50.3 51.0 50.89 51.0 36.42 37.27 36.7 37.46 19.2 23.8 38.4 37. 61.19 62.03 +0.84 63.66 +1.47 64.07 +2.88 post training within the official checkpoints and the llava-v1.5-mix665k dataset for 0.1 epoch under similar optimization settings. The learning rates for the vision encoder, LLM decoder, and merger are set to 2e-6, 1e-5, and 1e-5, respectively. 5.4 RESULTS OF IMAGE CLASSIFICATION We did the experiments on three classification datasets on small-scale dataset (CIFAR100), (1) CIFAR100: large-scale dataset (ImageNet-1K), and fine-grained dataset (Stanford-Cars). Table 1 and Table 9 shows the part and full classification results respectively. MergeMix brings the +2.15%, +2.51% gains compared with TranMix on DeiT models. Gains +2.87%, +2.88% and +4.79% on ViT models. All the results in Table 1 are trained for 200 epochs on the CIFAR100 dataset. (2) Stanford-Cars: Table 2 shows the fine-grain classification results on Stanford-Cars dataset. MergeMix achieves the 88.42% and 92.20% accuracy compared with other mixup methods. About the results of the CUB200 dataset and the FGVC-Aircrafts dataset in Table 10. (3) ImageNet-1k: Table 3 shows the results of accuracy, throughput, and flops on the ImageNet-1K dataset. It is notable that MergeMix brings +0.27% gains and reduces -0.68G Flops compared with TransMix, and can also see other mixups with less throughput since they bring extra cost, but MergeMix has high throughput of 1591.66 TP/s. MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Figure 4: The confidence plots of mixup variants and MergeMix on the CIFAR100 dataset using DeiT-Tiny and ViT-Small. The red line indicates the expected prediction tendency. Table 6: The calibration results of LLaVA-v1.5-7b on POPE, ScienceVQAI , GQA & SEEDI . rl denotes training with ranking loss. Method Baseline GQA POPE SEEDI ScienceVQAI 14.57 13.16 33.79 28.09 Training with Full Vision Tokens SFT Vision +MixUprl +CutMixrl +ResizeMixrl +MergeMixrl 8.52 6.09 6.74 12.53 6.50 12.82 12.72 12.62 13.17 12.91 32.67 33.26 32.77 36.08 32.52 Training with 50% Vision Tokens SFT Vision +MixUprl +CutMixrl +ResizeMixrl +MergeMixrl 18.13 13.40 10.48 12.60 10.34 12.67 12.74 12.67 12.97 12.76 34.41 33.60 33.83 37.41 33.37 21.66 21.51 24.71 24.58 23.66 24.28 22.61 20.63 23.74 25.22 5.5 RESULTS OF MLLM BENCHMARKS Figure 5: Robustness against image occlusion classification results with different occlusion ratios for different mixup methods based on DeiT-Small on ImageNet-1K dataset. We chose two mainstream MLLMs for our experiments on the VQA tasks and reasoning. Table 4 shows the results of LLaVA benchmarks, with different vision tokens for training. With the setting of full vision tokens on the training stage, our method achieves an average gain of +0.83%. When reducing the vision token to 288, our method still retains good performance compared with SFT. full comparison of results in Table 16, Table 17, and Table 18. In those different settings, MergeMix can achieve the average performance of 66.84%, which gains +1.27% over the LLaVA baseline. Table 5 shows the results of some VQA tasks and reasoning with Qwen2.5-VL-Instruction. MergeMix achieves an average gain of +2.88% over Qwen2.5-VL-Instruction. For the MathVista reasoning task, we reported the results without LLM-as-the-judge-eval. For the MMMU and MMMU-Pro tasks, we can achieve results on par with methods targeting reasoning improvements. 5.6 RESULTS OF CALIBRATION DNNs are prone to overconfidence in classification tasks. Mainfold Mixup (Verma et al., 2019) found that the mixup methods can effectively alleviate this problem. To this end, we compute the Expected Calibration Error (ECE (Guo et al., 2017)) of various mixup approaches on the CIFAR100 dataset for image classification. Also, to further analyze the calibration of MLLMs, we implement four short answer tasks, POPE, GQA, ScienceVQA, and SEED. Figure 4 shows the results of DeiT-Tiny and ViT-Small models trained for 200 epochs, showing that MergeMix obtains the best calibration of 6.7% and 9.7% in those ViT-specific mixup methods. (i.e., TransMix, SMMix, and MixPro). Table 6 shows the results of the LLaVA baseline, LLaVA with SFT, and LLaVA with our approach. SFT reduces the ECE when tuning the vision encoder, with augmentation and ranking loss, which can be better since we bring in the reward signal for the model. The more comprehensive results of CIFAR100 and MLLM benchmark we plot in Table 11 and Table 15. 9 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Table 7: Ablation study of MergeMix on classification trained 200 epochs. Module DeiT-Small ViT-Small Vanilla + TopK + ToMe + Re-Scaling λ 65.81 75.80 76.45 78.68 62.64 75.19 76.46 77. Table 8: Ablation study of MLLM training paradigm on LLaVA benchmark. Method VizWiz SciVQAI MMBench Figure 6: Sensitivity analysis study of 2 hyperparameters of MergMix. Left: Different merge ratios of backbones. Right: Attention score obtained from feature layer. Those results are from training for 200 epochs. LLaVA-v1.5-7B 50.0 50.45 + ToMe 48.15 + SFT + MergeMixrl 52.14 66.8 68.86 69.51 69. 64.3 62.8 66.32 66.58 5.7 RESULTS OF OCCLUSION ROBUSTNESS To analyze the improvement of MergeMix on the robustness with other mixup methods, we randomly conducted masking patches experiment (Naseer et al., 2021). The experiment involved dividing the image into 16 16 patches and masking them according to different ratios. The resulting image was then classified by the model. Figure 5 shows the accuracy curve of the ImageNet-1K dataset evaluation by trained DeiT-Small model. About the full results, we put them in Appendix: Figure 7, Table 12, Table 14, and Table 13. 5.8 ABLATION STUDY The ablation study mainly focuses on three things. (a) Token merge module and optimized mixing ratio, whether efficient for image classification task; (b) Exploring the ability of vision encoder and the proposed training paradigm. For the image classification scenario, Table 7 shows that compared with TopK sampling, our token merge can improve performance with +0.55%, +1.27% gains respectively, which means token merge smooths the discrete attention score. The re-scaling mixing ratio further gains +2.23%, +0.56% on the CIFAR100 dataset. For the paradigm, we validate the token merge for the LLaVA-v1.5 7B model, further explore the training with an unfrozen vision encoder, and the ranking loss. Table 8 shows that, compared with vanilla Token Merge, unfreezing the vision encoder can perform better than freezing. The augmentation and ranking loss bring more performance than only the SFT loss; (c) For further exploring the performance of hyperparameters. We also evaluated the sensitivity of hyperparameters on MergeMix, i.e., merged tokens and feature layer for better performance. Figure 6 shows the results of those hyperparameters."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper presents MergeMix, unified augmentation for both image classification and MLLM alignment with token merge, also bridging the SFT and RL by building the preference pairs. Optimizing models through the mixed image and the raw image via ranking loss. Extensive experiments demonstrate that MergeMix not only improves the performance on classic image classification tasks but also achieves beneficial alignment and generalization on MLLM benchmarks. MergeMix provides promising step toward scalable, robust training paradigm for the multi-modal system. Future Works There are still some shortcomings in MergeMix for MLLMs. In future work, we will explore this from two levels: (1) From the data perspective: MergeMix focuses on the enhancement of the image modality during training, while text inputs still remain raw. How to extend mixup to the text modality in MLLM tasks needs to be solved, as this can provide more finegrained optimization guidelines. (2) From the model perspective: The token merging is still static and unlearned. Improving the merging strategy to make it learnable via metrics or backpropagation could enhance the token merging ability for mixing. 10 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding"
        },
        {
            "title": "REFERENCES",
            "content": "Kyungjune Baek, Duhyeon Bang, and Hyunjung Shim. Gridmix: Strong regularization through local context mapping. Pattern Recognition, 109, 2021. 3, 6 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 4, 6 Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. In ICLR, 2023. 3, 4 Jianjian Cao, Peng Ye, Shengze Li, Chong Yu, Yansong Tang, Jiwen Lu, and Tao Chen. Madtp: Multimodal alignment-guided dynamic token pruning for accelerating vision-language transformer. In CVPR, 2024. Jie-Neng Chen, Shuyang Sun, Ju He, Philip HS Torr, Alan Yuille, and Song Bai. Transmix: Attend to mix for vision transformers. In CVPR, 2022. 3, 4, 6 Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, and Alan Yuille. Efficient large multi-modal models via visual context compression. In NIPS, 2024a. 3 Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? In NIPS, 2024b. 7, 16 Mengzhao Chen, Mingbao Lin, Zhihang Lin, Yuxin Zhang, Fei Chao, and Rongrong Ji. Smmix: Self-motivated image mixing for vision transformers. In ICCV, 2023. 3, 4, 6 Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. 7, 17 Ekin Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with reduced search space. In CVPRW, 2020. 7, 16 Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, and Nasser Nasrabadi. Supermix: Supervising the mixing data augmentation. In CVPR, 2021. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 7, 16 Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. On calibration of modern neural networks. In ICML, 2017. 9 Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In CVPR, 2018. 7, 16 Ethan Harris, Antonia Marcu, Matthew Painter, Mahesan Niranjan, and Adam PrugelBennett Jonathon Hare. Fmix: Enhancing mixed sample data augmentation. arXiv preprint arXiv:2002.12047, 2020. 3, 6 Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. Zipvl: Efficient large vision-language models with dynamic token sparsification. arXiv preprint arXiv:2410.08584, 2024. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 7, 16 Xin Jin, Hongyu Zhu, Siyuan Li, Zedong Wang, Zicheng Liu, Juanxi Tian, Chang Yu, Huafeng Qin, and Stan Li. survey on mixup augmentations and beyond. arXiv preprint arXiv:2409.05202, 2024a. 5 11 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Xin Jin, Hongyu Zhu, Mounˆım El Yacoubi, Haiyang Li, Hongchao Liao, Huafeng Qin, and Yun Jiang. Starlknet: Star mixup with large kernel networks for palm vein identification. arXiv preprint arXiv:2405.12721, 2024b. Minsoo Kang and Suhyun Kim. Guidedmixup: an efficient mixup strategy guided by saliency maps. In AAAI, 2023. 6 Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local statistics for optimal mixup. In ICML, 2020. 3, 6 Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint mixup with supermodular diversity. arXiv preprint arXiv:2102.03065, 2021. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In 3DRR-13), 2013. 7, 16 Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 7, 16 Zhibin Lan, Liqiang Niu, Fandong Meng, Wenbo Li, Jie Zhou, and Jinsong Su. Avg-llava: An efficient large multimodal model with adaptive visual granularity. In ACL, 2025. Jin-Ha Lee, Muhammad Zaigham Zaheer, Marcella Astrid, and Seung-Ik Lee. Smoothmix: simple yet effective data augmentation to train robust classifiers. In CVPRW, 2020. 3, 6 Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. 7, 16 Siyuan Li, Zedong Wang, Zicheng Liu, Di Wu, and Stan Z. Li. Openmixup: Open mixup toolbox and benchmark for visual representation learning. arXiv preprint arXiv:2209.04851, 2022. 6, 17 Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm. IJCV, pp. 119, 2025. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In NIPS, 2023b. 7, 16 Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024a. URL https://llava-v l.github.io/blog/2024-01-30-llava-next/. 6 Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NIPS, 2024b. 1, 4, 6 Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang. Multi-stage vision token dropping: Towards efficient multimodal large language model. arXiv preprint arXiv:2411.10803, 2024c. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2025. 7, 16 Zicheng Liu, Siyuan Li, Di Wu, Zihan Liu, Zhiyuan Chen, Lirong Wu, and Stan Li. Automix: Unveiling the power of mixup for stronger classifiers. In ECCV, 2022. 3, 6 Zicheng Liu, Siyuan Li, Ge Wang, Lirong Wu, Cheng Tan, and Stan Li. Harnessing hard mixed samples with decoupled regularizer. In NIPS, 2023. 5 Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NIPS, 2022. 7, 12 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024. 7, 16 Tiange Luo, Ang Cao, Gunhee Lee, Justin Johnson, and Honglak Lee. Probing visual language priors in vlms. arXiv preprint arXiv:2501.00569, 2024. 1 Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 7, 16 Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. In NIPS, 2024. 2, 6 Muzammal Naseer, Kanchana Ranasinghe, Salman Hameed Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. In NIPS, 2021. 10 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019. 7, 17 Huafeng Qin, Xin Jin, Yun Jiang, Mounim El-Yacoubi, and Xinbo Gao. Adversarial automixup. In ICLR, 2024a. 3, 6 Huafeng Qin, Xin Jin, Hongyu Zhu, Hongchao Liao, Mounˆım El-Yacoubi, and Xinbo Gao. Sumix: Mixup with semantic and uncertain information. In ECCV, 2024b. 5 Jie Qin, Jiemin Fang, Qian Zhang, Wenyu Liu, Xingang Wang, and Xinggang Wang. Resizemix: Mixing data with preserved object information and true labels. arXiv preprint arXiv:2012.11101, 2020. 3, 6 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 16 Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NIPS, 2023. 1, Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 115(3):211252, 2015. 7, 16 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 4 Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. 3, 6 Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019. 7, Shuyang Sun, Jie-Neng Chen, Ruifei He, Alan Yuille, Philip Torr, and Song Bai. Lumix: Improving mixup by better modelling label uncertainty. arXiv preprint arXiv:2211.15846, 2022. 5 Wentao Tan, Qiong Cao, Yibing Zhan, Chao Xue, and Changxing Ding. Beyond human data: Aligning multimodal large language models by iterative self-evolution. In AAAI, 2025. 1 Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In NIPS, 2024. 1 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In ICML, 2021. 6 AFM Shahab Uddin, Mst Sirazam Monira, Wheemyung Shin, TaeChoong Chung, and Sung-Ho Bae. Saliencymix: saliency guided data augmentation strategy for better regularization. In ICLR, 2021. 3, 6 Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, et al. Fastvlm: Efficient vision encoding for vision language models. In CVPR, 2025. 3 Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David LopezPaz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In ICML, 2019. 9 Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. California Institute of Technology, 2011. 7, 16 Devesh Walawalkar, Zhiqiang Shen, Zechun Liu, and Marios Savvides. Attentive cutmix: An enIn ICASSP, hanced data augmentation approach for deep learning based image classification. 2020. 3, 6 Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, et al. Enhancing visual-language arXiv preprint modality alignment in large vision language models via self-improvement. arXiv:2405.15973, 2024. 1, 6 Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247, 2024. 3 Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. In CVPR, 2025a. 3, Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, and Jiaya Jia. Visionthink: Smart and efficient vision language model via reinforcement learning. arXiv preprint arXiv:2507.13348, 2025b. 1, 6 Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong Tang. Atp-llava: Adaptive token pruning for large vision language models. In CVPR, 2025. 3 Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. 7, 16 Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024a. 7, Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. 7, 16 Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. In ICCV, Cutmix: Regularization strategy to train strong classifiers with localizable features. 2019. 3, 6 Ce Zhang, Kaixin Ma, Tianqing Fang, Wenhao Yu, Hongming Zhang, Zhisong Zhang, Yaqi Xie, Katia Sycara, Haitao Mi, and Dong Yu. Vscan: Rethinking visual token reduction for efficient large vision-language models. arXiv preprint arXiv:2505.22654, 2025a. 6 Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. 3, 14 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:407.12772, 2024a. 6, 16 Qizhe Zhang, Aosong Cheng, Ming Lu, Renrui Zhang, Zhiyong Zhuo, Jiajun Cao, Shaobo Guo, Qi She, and Shanghang Zhang. Beyond text-visual attention: Exploiting visual cues for effective token pruning in vlms. arXiv preprint arXiv:2412.01818, 2024b. 6 Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. Llava-mini: Efficient image and video large multimodal models with one vision token. In ICLR, 2025b. YiFan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? In ICLR, 2025c. 7, 16 Qihao Zhao, Yangyu Huang, Wei Hu, Fan Zhang, and Jun Liu. Mixpro: Data augmentation with maskmix and progressive attention labeling for vision transformer. In ICLR, 2023. 3, 4, 6 Ke Zhu, Liang Zhao, Zheng Ge, and Xiangyu Zhang. Self-supervised visual preference alignment. In ACMMM, 2024. 1, 6 Ke Zhu, Yu Wang, Yanpeng Sun, Qiang Chen, Jiangjiang Liu, Gang Zhang, and Jingdong Wang. Continual sft matches multimodal rlhf with negative supervision. In CVPR, 2025. 1, 6 15 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding"
        },
        {
            "title": "A DECLARATION OF LLM USAGE",
            "content": "We use the Large Language Models (LLMs) for this paper to serve one purpose: to aid and polish the paper writing. We use the LLMs in very limited capacity, restricted to minor editing of grammar, phrasing, and readability. We do not involve the LLMs in designing the method, developing theoretical results, and conducting experiments."
        },
        {
            "title": "B APPENDIX FOR IMAGE CLASSIFICATION",
            "content": "B.1 INFORMATION ON DATASETS In this subsection, we will describe the datasets we chose in detail. Image Classification. We choose five mainstream classification datasets: (i) CIFAR100 dataset (Krizhevsky et al., 2009) consists of 100 classes of color images with resolution of 32 32 pixels, containing 50,000 training images and 10,000 test images. (ii) ImageNet-1k dataset (Russakovsky et al., 2015) consists of 1,000 classes with varied image resolutions commonly cropped to 224 224 pixels, containing 1,281,167 training images and 50,000 test images. (iii) CUB200 dataset (Wah et al., 2011) contains 200 bird species, including total 11,788 images, we divided 5,994 images as training images and 5,794 images as test images, (iv) FGVC-Aircrafts dataset (Maji et al., 2013) contains 100 aircraft model classes, including 6,667 training images and 3,333 test images, (v) Stanford-Cars dataset (Krause et al., 2013) contains 196 car model classes, including 8,144 training images and 8,041 test images. All the fine-grained datasets we used set the resolution as 224 224 pixels for training and testing. MLLM Benchmark. we conducted various experiments on LLaVA Benchmark and lmmseval (Zhang et al., 2024a), which based on 16 datasets: (i) VQAv2 dataset (Goyal et al., 2017) contains 204,721 training images, 22,000 validation images, and 40,504 test images, (ii) GQA dataset (Hudson & Manning, 2019) focuses on graph-based reasoning with 220,000 training images and 150,000 validation/test questions, (iii) VizWiz dataset (Gurari et al., 2018) images are captured by mobile devices with questions from visually impaired users (31,173 training images), (iv) ScienceVQA dataset (Lu et al., 2022), (v) TextVQA dataset (Singh et al., 2019), and (vi) SEED dataset (Li et al., 2023a) contain 21,208, 28,408, and 15,000 training images, respectively, emphasizing scientific reasoning, text understanding, and multi-modal reasoning. (vii) MME dataset (Yin et al., 2023) and (viii) MMBench dataset (Liu et al., 2025) provide general multi-modal evaluation, (ix) MMBenchCN dataset as the Chinese version of MMbench, (x) MMBenchCC dataset as the Cross Check version of MMbench. (xi) POPE dataset (Li et al., 2023b) evaluates performance on prompt-driven tasks with zero-shot and few-shot settings. (xii) & (xiii) MMMU & MMMUPro datasets (Yue et al., 2024a;b) are multi-modal reasoning benchmark with college-level exam questions, (xiv) MME-RealWorldQA dataset (Zhang et al., 2025c) emphasizes real-world, long-tail visual understanding in everyday scenarios. (xv) MMStar dataset (Chen et al., 2024b) uses testing star-level multi-modal reasoning across diverse tasks. (xvi) MathVista dataset (Lu et al., 2024) for the visual mathematical reasoning by involving geometry, algebra, and charts. For the LLaVA benchmark, all images are typically cropped or resized to 336 336 pixels for training and evaluation since the CLIP (Radford et al., 2021) is the vision encoder. For the Qwen2.5-VL-Instruction benchmark, the images are dynamically scaled by the Qwen-VL model. B.2 DETAILED IMPLEMENTATIONS Classification Tasks: (i) For the CIFAR100 dataset, aiming to be suitable for training ViT-based approaches, e.g., DeiT, we resize images to 224 224 and train them with the AdamW optimizer with weight decay of 0.05, batch size of 100, and total training of 200 epochs and 600 epochs. Uses RandomFlip and RandomCrop as basic augmentations, and additionally, we use RandAugment (Cubuk et al., 2020). For DeiT-Tiny and DeiT-Small, we use the learning rate of 1e-3 with dynamic cosine scheduler. For ViT-Small and ViT-Base models, we set the learning rate to 5e-4, the learning rate of ViT-Large up to 2e-4, all dynamically adjusted by cosine scheduler. (ii) For the ImageNet-1K dataset, the dataset settings are the same as CIFAR100, but we use the learning rate as 1e-3, batch size of 1024, and total training of 300 epochs for DeiT-Tiny and DeiT-Small 16 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Table 9: Top-1 accuracy (%) of mixup methods on CIFAR-100 dataset under DeiT-Tiny/Small, ViTSmall/Base/Large different model sizes. The α parameter of the Beta distribution follows the setting in OpenMixup (Li et al., 2022) setting. Method Vanilla MixUp CutMix FMix GridMix ResizeMix SaliencyMix PuzzleMix AutoMix AdAutoMix DeiT TransMix SMMix MixPro MergeMix DeiT-Tiny DeiT-Small ViT-Small ViT-Base ViT-Large 200 epochs 600 epochs 200 epochs 600 epochs 200 epochs 600 epochs 200 epochs 600 epochs 200 epochs 64.70 69.47 75.98 72.73 71.54 69.42 69.83 73.40 72.91 72.83 74.01 75.31 73.84 74.78 77.46 66.70 73.06 79.60 77.24 76.23 72.98 75.45 79.96 81.16 77. 79.90 80.66 78.62 80.19 81.20 65.81 69.98 74.21 70.41 68.86 68.54 69.78 73.60 76.24 72.63 75.92 76.17 74.09 75.26 78.68 68.50 76.35 79.54 74.31 74.96 71.95 76.60 81.01 80.91 78. 79.54 79.33 79.84 79.55 80.39 62.64 68.67 69.67 68.41 70.15 67.86 70.14 70.92 68.44 69.66 72.96 74.15 73.50 73.49 77.02 66.32 73.57 76.66 72.55 68.23 69.09 74.09 78.44 77.73 77.60 78.27 79.65 80.02 81.44 63.33 69.66 72.18 68.62 66.63 63.72 68.75 71.13 73.40 71.43 72.15 72.87 70.87 73.18 75.75 66.47 73.90 71.94 71.10 68.49 69.33 75.50 79.49 76.26 77.89 78.18 78.69 79.59 61.83 67.90 68.97 66.12 63.20 63.48 67.12 69.77 72.10 69.69 69.23 71.40 71.38 72.28 76.19 Table 10: Top-1 accuracy (%) of mixup methods on Fine-Grained datasets: CUB200, FGVCAircrafts, and Stanford-Cars. Method Vanilla MixUp CutMix SmoothMix FMix GridMix ResizeMix Attentive-CutMix SaliencyMix PuzzleMix GuidedMixap DeiT TransMix SMMix MixPro MergeMix α 1.0 0.2 0.2 0.2 0.2 1.0 2.0 0.2 1.0 1. 0.2 1.0 1.0 1.0 1.0 CUB200 FGVC-Aircrafts Stanford-Cars DeiT-Small ViT-Base DeiT-Small ViT-Base DeiT-Small ViT-Base 82.05 84.31 81.69 83.87 82.64 82.34 82.15 82.83 82.34 84.39 84. 84.04 83.34 82.88 82.31 85.40 88.00 88.75 87.76 87.02 88.68 87.23 87.61 87.47 87.92 88.23 88.26 88.47 88.10 88.35 86.93 88.40 77.59 78.52 75.67 75.31 77.08 75.85 74.59 75.04 77.98 78.28 77. 75.89 75.73 76.42 75.25 80.92 80.86 82.18 80.08 76.72 79.33 78.49 77.62 76.06 79.81 81.27 79.24 81.07 77.77 78.40 75.97 81.97 86.77 87.73 88.37 86.39 87.18 87.58 87.45 87.35 87.94 88.60 86. 88.72 88.38 88.76 88.38 89.42 91.31 91.36 91.53 90.88 91.36 91.31 91.59 90.29 91.47 91.83 90.40 92.17 91.66 91.93 91.48 92.20 with AdamW optimizer with weight decay of 0.05. (iii) For all fine-grain datasets, i.e., CUB-200 dataset, FGVC-Aircrafts dataset, and Stanford-Cars dataset, we fine-tune the DeiT-Small and ViTBase model for 200 epochs with batch size of 16, learning rate of 1e-5, loading the pre-trained model weight from PyTorch (Paszke et al., 2019). MLLM Benchmark: Following the LLaVA-v1.5 settings, we use pre-trained Vicuna-v1.5 7B (Chiang et al., 2023) as the language decoder, which uses pre-trained 2 MLP as the projection for aligning the vision and text modistes, which was trained for one epoch on LCS-558K. For the vision encoder, we use pre-trained CLIP encoder and extract the visual representation from the input images. For SFT, the learning rate was set as 2e-5, the batch size was 64, and training one epoch on llava-v1.5-mix665k dataset, uses AdamW optimizer with (0.9, 0.999) betas and epsilon of 1e-8, warmup ratio of 0.03 with cosine scheduler. The difference from LLaVA is that we unfreeze the vision encoder during training. About Qwen2.5-VL-Instruction, we fine-tune with llava-v1.5-mix665k dataset for 0.1 epoch, uses AdamW optimizer with (0.9, 0.999) betas and epsilon of 1e-8 like LLaVA, warmup with 0.03 ratio. The learning rate of the vision encoder, LLM decoder, and merger were set to 2e-6, 1e-5, and 1e-5, respectively. 17 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Table 11: The calibration results of ViT-based mixup methods on CIFAR-100 & ImageNet-1K, with training 200 and 300 epochs respectively. tome denotes inference with token merging. Dataset Epochs MixUp CutMix TransMix DeiT SMMix MixPro MergeMix MergeMixtome Method DeiT-Tiny ViT-Small ViT-Large DeiT-Tiny ViT-Small CIFAR100 CIFAR100 CIFAR100 CIFAR100 CIFAR100 DeiT-Small ImageNet-1K 200 200 200 600 600 300 8.64 13.89 4.76 6.44 9.01 5.66 11.42 14.43 7.36 6.01 6.45 4. 14.52 14.22 16.44 7.56 9.12 7.57 12.24 15.76 6.71 8.53 10.22 8.24 11.80 12.41 7.70 8.35 9.55 6. 10.68 10.65 6.22 6.38 7.42 5.17 7.08 9.69 4.65 5.45 5.50 6.00 6.73 10.14 4.70 5.46 6.04 4. Table 12: The Top-1 accuracy of DeiT-Tiny trained by various Mixup approaches on the CIFAR100 dataset with different occlusion ratios. tome denotes inference with token merging. Method Vanilla MixUp CutMix FMix GridMix ResizeMix SaliencyMix Attentive-CutMix PuzzleMix AutoMix TransMix DeiT SMMix MixPro MergeMix MergeMixtome 0% 66.68 73.06 79.58 77.14 76.13 72.93 75.41 80.27 79.97 81. 80.66 79.80 78.62 80.14 80.88 81.12 10% 64.54 70.52 78.64 76.01 73.79 71.82 74.63 79.13 78.18 78.37 79.77 78.20 77.78 79.61 75.19 79.49 DeiT-Tiny Trained 200 epochs 20% 62.57 68.45 77.10 74.62 71.94 70.53 73.57 77.94 77.36 78.40 75.33 77.29 76.51 78.78 69.71 78.96 30% 60.20 65.39 75.83 73.33 69.36 69.67 72.14 76.98 76.04 78.16 73.60 75.83 74.78 77.06 68.72 78.57 40% 56.96 61.27 72.96 71.27 66.36 67.77 69.47 75.45 73.74 77.65 70.98 74.59 71.25 74.81 69.33 77.81 50% 51.41 53.55 70.25 67.72 62.34 65.22 65.05 71.01 70.74 77.09 69.95 72.31 64.17 71.18 70.17 76.65 60% 40.32 42.80 64.40 63.17 56.02 59.87 58.08 57.75 65.83 74.63 63.20 69.05 47.28 64.25 69.36 74.89 70% 27.75 29.77 54.39 56.18 47.30 50.26 44.82 33.10 56.75 71.64 40.02 60.18 24.72 50.75 68.20 72.58 80% 13.25 16.01 35.82 42.12 32.52 31.26 24.14 12.03 40.43 67.52 25.28 38.55 7.12 26.80 63.49 67.24 90% 4.99 5.01 14.08 17.32 14.35 9.72 7.03 4.02 19.41 55.61 9.77 12.53 2.19 5.19 51.72 58.32 B.3 ADDITIONAL RESULTS OF IMAGE CLASSIFICATION Table 9 shows the full results of 200 epochs and 600 epochs training on the CIFAR100 dataset using different ViT models. Table 10 shows the three fine-grain datasets, i.e., CUB200, FGVC-Aircrafts, and Stanford-Cars. It is easily found that MergeMix can achieve the SOTA on lots of models. Also, the speed of overfitting is significant improvement over other methods, which means the token merge can gather useful information and reduce some redundant tokens. B.4 ROBUSTNESS EXPERIMENTS OF MIXUP AUGMENTATIONS Calibration of Mixup Augmentations Table 11 shows the results of calibration on seven mixup augmentations. We evaluated with two public datasets by DeiT-Tiny, ViT-Small, and ViT-Large on the CIFAR100 dataset, by models trained with 200 epochs, DeiT-Tiny and ViT-Small trained with 600 epochs, and DeiT-Small on the ImageNet-1K dataset, trained with 300 epochs. Results of Occlusion Robustness The full results of occlusion robustness classification on MergeMix and other mixup methods. Figure 7 show the corve of MergeMix and other mixup method on CIFAR100 dataset and ImageNet-1K dataset. Table 12 and Table 14 show the accuracy results on the CIFAR100 dataset by vanilla and 14 different mixup approaches. Table 13 shows the results of 8 different methods on the ImageNet-1K dataset. Figure 7: Robustness against image occlusion classification results with different occlusion ratios for different mixup methods based on DeiT-Tiny (left) and DeiT-Small (right) on CIFAR100 and ImageNet-1K datasets. 18 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Table 13: The Top-1 accuracy of DeiT-Small trained by various Mixup approaches on the ImageNet1K dataset with different occlusion ratios. tome denotes inference with token merging. Method Vanilla MixUp CutMix TransMix DeiT SMMix MixPro MergeMix MergeMixtome DeiT-Samll Trained 300 epochs 0% 75.46 78.74 80.16 80.36 80.27 79.32 79.25 80.70 79.67 10% 74.03 78.17 79. 79.47 79.03 78.56 78.83 80.38 79.57 20% 72.85 77.11 78.20 78.24 77.92 77.70 78.01 79.95 79.01 30% 71.36 76.21 76. 77.00 76.39 76.17 77.24 78.97 78.60 40% 69.91 74.31 75.21 75.40 74.65 73.76 76.02 78.08 77.68 50% 67.94 71.85 72. 73.31 72.25 70.25 74.26 76.21 76.11 60% 64.71 68.13 69.24 70.17 68.31 64.95 71.48 74.08 73.99 70% 60.42 62.40 64. 65.78 63.53 58.94 67.44 70.71 70.56 80% 51.65 51.45 55.95 57.79 54.57 48.69 58.49 65.57 64.84 90% 34.08 33.08 39. 42.44 38.81 34.37 39.47 51.98 51.80 Table 14: The Top-1 accuracy of ViT-Small trained by various Mixup approaches on the CIFAR100 dataset with different occlusion ratios. tome denotes inference with token merging. Method Vanilla MixUp CutMix FMix GridMix ResizeMix SaliencyMix Attentive-CutMix PuzzleMix AutoMix TransMix DeiT SMMix MixPro MergeMix MergeMixtome 0% 66.24 73.67 76.13 71.27 67.99 66.69 73.50 78.26 78.01 77.52 78.37 77.50 79.32 79.91 81.29 81.66 10% 62.51 71.84 73.77 68.52 66.48 66.90 72.12 72.49 76.42 76.74 76.29 75.99 77.84 78.37 75.38 77.45 ViT-Samll Trained 200 epochs 20% 60.37 70.80 72.93 66.49 65.60 62.80 71.88 67.52 75.54 75.14 75.86 75.80 76.60 75.36 73.41 78.25 30% 58.59 69.07 71.92 65.78 64.21 58.15 71.21 63.83 74.61 72.95 75.58 74.57 75.41 72.08 69.36 78. 40% 56.68 66.75 70.32 65.00 62.54 47.00 69.97 60.76 73.99 69.71 74.79 73.80 73.00 68.37 61.02 74.85 50% 53.61 63.52 68.32 62.31 60.40 36.15 67.62 54.93 71.46 66.76 73.19 71.73 68.10 57.48 51.73 72. 60% 49.70 58.21 65.48 57.80 56.98 36.49 63.03 38.05 68.23 62.59 70.81 67.23 58.89 37.08 52.52 69.70 70% 39.70 48.68 59.30 48.98 50.02 33.07 53.12 27.75 63.24 59.00 68.18 58.40 45.07 21.62 62.14 67. 80% 22.60 28.72 43.11 30.09 34.62 10.97 33.27 32.20 51.51 49.01 62.29 43.49 23.25 11.87 64.83 67.80 90% 7.55 8.01 17.04 6.19 9.74 2.06 11.06 31.66 26.25 27.45 51.09 19.94 4.59 3.69 57.21 60."
        },
        {
            "title": "C APPENDIX FOR MLLMS",
            "content": "C.1 RESULTS OF CALIBRATION ON MLLMS For exploring the calibration of MLLMs, we chose 4 tasks with short response under different token reduction settings. We evaluate three scenarios: using full vision tokens, 50% tokens, and 25% tokens, and compare various data augmentation strategies (MixUp, CutMix, ResizeMix, MergeMix) trained with ranking loss (rl). Table 15 shows that with unfreezing the vision encoder, the ECE can be better than freezing. With the vision tokens reduced in training. Overall, token reduction leads to moderate drop in calibration accuracy, but effective augmentation strategies significantly mitigate this degradation. In the full-token setting, CutMixrl achieves the lowest GQA calibration error (6.09), while ResizeMixrl shows the best SEED result (36.08). When reducing tokens to 50%, CutMixrl and MergeMixrl remain competitive, maintaining strong calibration across tasks despite reduced visual information. Even with 25% tokens, CutMixrl continues to yield relatively balanced performance, Table 15: The calibration results of LLaVA on POPE, ScienceVQAI , GQA & SEEDI . Method Baseline GQA POPE SEEDI ScienceVQAI 14.57 13.16 33.79 28.09 Training with Full Vision Tokens SFT Vision +MixUprl +CutMixrl +ResizeMixrl +MergeMixrl 8.52 6.09 6.74 12.53 6.50 12.82 12.72 12.62 13.17 12.91 32.67 33.26 32.77 36.08 32.52 Training with 50% Vision Tokens SFT Vision +MixUprl +CutMixrl +ResizeMixrl +MergeMixrl 18.13 13.40 10.48 12.60 10.34 12.67 12.74 12.67 12.97 12.76 34.41 33.60 33.83 37.41 33.37 Training with 25% Vision Tokens SFT Vision +MixUprl +CutMixrl +ResizeMixrl +MergeMixrl 13.32 12.97 12.23 10.66 12. 12.51 12.66 13.10 14.17 12.17 34.97 34.89 34.85 38.77 34.87 21.66 21.51 24.71 24.58 23.66 24.28 22.61 20.63 23.74 25.22 18.86 19.33 20.70 17.27 17.70 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Table 16: Different token merge ratios of inference comparison results with augmentations. AVG: The average of the nine benchmarks for comprehensive comparison, except for MME. LLaVA-7b Train Image Question Answering Benchmarks v1. Ratio VQAv2 GQA VizWiz SciVQAI TextVQA MME MMBench MMBenchCN POPE SEEDI AVG Gain 78.5 62.0 Vanilla 50.0 SFT Vision 100% 79.32 62.98 47.45 100% 79.27 62.58 44.95 + MixUp 100% 79.18 62.40 45.04 + CutMix + ResizeMix 100% 77.78 61.66 44.43 + MergeMix 100% 79.24 62.44 47.69 Vanilla 77.24 59.65 50.86 SFT Vision 100% 77.62 59.73 46.57 100% 77.66 59.37 44.15 + MixUp 100% 77.67 59.21 44.25 + CutMix + ResizeMix 100% 76.45 59.65 43.31 + MergeMix 100% 77.71 59.32 47.46 Vanilla 76.65 59.33 50.45 SFT Vision 100% 77.07 59.05 46.19 100% 77.11 58.96 44.39 + MixUp 100% 77.15 58.66 44.00 + CutMix + ResizeMix 100% 75.90 59.46 42.99 + MergeMix 100% 77.13 59.02 47.46 Vanilla 74.63 58.76 52.71 SFT Vision 100% 75.02 58.43 46.36 100% 75.45 58.63 44.82 + MixUp 100% 75.39 58.61 43.85 + CutMix + ResizeMix 100% 74.08 59.02 43.40 + MergeMix 100% 75.50 58.60 48.14 Inference Full Vision Token 66.8 70.05 69.41 70.60 68.91 69. 58.2 57.17 57.39 57.06 55.11 57.56 1510.7 1490.88 1483.20 1452.31 1436.09 1479.97 64.3 66.26 65.72 66.32 63.91 66.58 Inference 75% Vision Token 68.42 70.15 69.91 69.66 68.82 70.70 55.66 54.83 56.18 54.84 53.14 54. 1460.88 1454.99 1457.98 1400.49 1426.75 1440.50 63.05 65.03 64.77 65.03 63.91 65.37 Inference 50% Vision Token 68.86 70.35 69.36 69.31 69.31 70.55 55.33 54.40 54.41 54.78 52.83 54.54 1452.18 1436.79 1422.28 1428.21 1444.13 1461. 62.8 64.60 64.34 64.94 63.23 65.03 Inference 25% Vision Token 68.67 69.06 68.02 68.67 68.96 69.86 55.32 52.32 52.13 52.69 51.40 52.01 1398.24 1376.88 1384.31 1330.99 1377.96 1439.44 60.65 62.37 62.28 62.71 61.08 63. 58.3 60.05 58.24 58.24 55.41 60.65 57.9 59.36 57.73 57.98 55.15 58.93 56.87 58.93 57.90 59.02 54.20 58.84 54.03 55.84 60.65 56.01 53.09 57.56 85.87 66.19 65.57 86.18 67.32 66.31 +0.74 86.27 66.73 65.62 +0.05 86.47 67.22 65.84 +0.27 86.01 63.91 64.13 -1.44 86.10 67.47 66.40 +0.83 85.60 65.21 64.84 85.60 65.90 64.98 +0.14 85.15 65.28 64.47 -0.37 85.91 65.66 64.47 -1.09 85.29 63.04 63.20 -1.64 85.04 65.98 65.04 +0. 86.53 64.09 64.55 85.62 65.12 64.59 +0.04 86.16 64.29 64.10 -0.45 86.42 64.80 64.34 -0.21 85.77 62.07 62.86 -1.69 85.70 65.25 64.84 +0.29 86.54 62.05 63.71 85.64 63.63 63.19 -0.52 85.54 62.97 63.39 -0.32 86.30 63.02 63.03 -0.68 86.53 59.85 61.93 -1.78 85.53 63.72 63.77 +0.06 indicating that appropriate augmentation enhances robustness under token compression. These results suggest that the calibration of MLLMs is still an open question, especially in some environments that need reliable answer. C.2 RESULTS OF DIFFERENT VISION TOKEN RATIOS ON INFERENCE In this subsection, we validate the different ratios of vision tokens on the LLaVA benchmark. Table 16, Table 17 and Table 18 show the fully results with full, 75%, 50% and 25% ratios on inference, respectively. Those results give full comparison of the influence on the vision tokens. Significantly shown in Table 16, MergMix always brings gains in different merge ratios, from +0.83 to +0.06. Other methods, since they are highly random, cause performance instability. From the results shown in Table 17, when the training stage uses the token merge, it can achieve an average gain of 66.84%, which improves 0.43% over training and inference without token merge training and inference, with an improvement of +1.27% than the average performance of the original LLaVA model. 20 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Table 17: Different token merge ratios of inference comparison results with augmentations. AVG: The average of the nine benchmarks for comprehensive comparison, except for MME. LLaVA-7b Train Image Question Answering Benchmarks v1.5 Ratio VQAv2 GQA VizWiz SciVQAI TextVQA MME MMBench MMBenchCN POPE SEEDI AVG Gain Vanilla 78.5 SFT Vision 50% 78.6 + MixUp + CutMix 62.0 50.0 62.47 48.15 50% 78.51 62.07 51.1 50% 78.58 62.39 50.53 + ResizeMix 50% 76.39 61.05 45.48 + MergeMix 50% 78.61 62.18 52.14 Vanilla 77.24 59.65 50.86 SFT Vision 50% 78.75 62.82 48.02 50% 78.87 62.32 51.01 + MixUp 50% 78.73 62.42 49.85 + CutMix + ResizeMix 50% 76.79 61.12 44.85 + MergeMix 50% 78.81 62.50 52.31 Vanilla 76.65 59.33 50.45 SFT Vision 50% 78.49 63.39 46.69 50% 78.54 61.91 51.01 + MixUp 50% 78.50 62.18 48.79 + CutMix + ResizeMix 50% 76.55 60.79 44.20 + MergeMix 50% 78.51 62.09 51.01 Vanilla 74.63 58.76 52.71 SFT Vision 50% 77.30 61.77 46.33 50% 77.28 61.56 48.77 + MixUp 50% 77.20 61.52 49.00 + CutMix + ResizeMix 50% 75.06 59.88 43.12 + MergeMix 50% 77.20 61.81 51.66 Inference Full Vision Token 66.8 69.51 68.47 70.2 68.07 69. 58.2 56.41 56.54 55.95 54.60 56.85 1510.7 1486.24 1459.06 1414.72 1447.35 1453.97 64.3 66.32 65.63 66.92 63.31 66.58 Inference 75% Vision Token 68.42 70.65 69.11 70.50 68.37 69.56 55.66 56.33 56.62 56.12 54.24 56. 1460.88 1486.24 1480.04 1418.07 1475.27 1455.81 63.05 66.40 65.63 67.61 64.26 66.66 Inference 50% Vision Token 68.86 70.25 69.61 70.50 68.32 70.10 55.33 55.68 55.76 55.83 54.21 56.03 1452.18 1468.38 1468.14 1431.32 1470.22 1464. 62.80 66.83 65.63 67.18 63.31 66.75 Inference 25% Vision Token 68.67 70.55 69.56 71.24 67.13 70.35 55.32 54.19 54.10 53.96 52.34 54.47 1398.24 1411.01 1419.71 1372.66 1445.26 1401.58 60.65 65.72 66.32 66.49 61.51 66. 58.3 57.98 59.53 59.53 51.97 59.02 57.90 59.02 59.53 59.87 52.66 59.10 56.87 57.76 59.87 59.02 52.66 59.45 54.03 58.07 56.27 58.76 51.63 59.10 85.87 66.19 65.57 87.37 66.75 65.95 +0.38 86.86 66.06 66.08 +0.51 66.31 +0.74 86.56 86.57 62.54 63.33 -2.24 86.47 66.63 66.45 +0.88 66. 85.60 65.21 64.84 86.93 67.17 66.23 +1.39 86.86 66.06 66.22 +1.38 85.96 66.44 66.39 +1.55 85.67 63.30 63.47 -1.37 85.76 67.12 66.48 +1.64 86.53 64.09 64.55 86.47 66.48 65.78 +1.23 86.51 66.39 66.14 +1.59 86.00 66.27 66.03 +1.48 86.23 62.73 63.22 -1.33 86.05 66.39 66.26 +1.71 86.54 62.05 63.71 86.34 65.46 65.08 +1.37 86.57 65.32 65.08 +1.37 86.38 65.24 65.53 +1.82 86.03 61.59 62.03 -1.68 85.92 65.44 66.84 +3.13 21 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Table 18: Different token merge ratios of inference comparison results with augmentations. AVG: The average of the nine benchmarks for comprehensive comparison, except for MME. LLaVA-7b Train Image Question Answering Benchmarks v1.5 Ratio VQAv2 GQA VizWiz SciVQAI TextVQA MME MMBench MMBenchCN POPE SEEDI AVG Gain 62.0 78.5 Vanilla 50.0 SFT Vision 25% 77.92 62.01 50.30 25% 77.89 62.01 52.53 + MixUp 25% 77.92 61.57 51.02 + CutMix + ResizeMix 25% 75.38 59.77 41.38 + MergeMix 25% 77.86 61.54 50.50 Vanilla 77.24 59.65 50.86 SFT Vision 25% 78.09 62.11 49.67 25% 77.97 61.23 53.13 + MixUp 25% 78.11 61.85 50.46 + CutMix + ResizeMix 25% 76.01 59.95 41.94 + MergeMix 25% 78.07 61.42 50.17 Vanilla 76.65 59.33 50.45 SFT Vision 25% 77.99 61.77 48.79 25% 77.89 61.68 50.85 + MixUp 25% 77.32 61.62 48.86 + CutMix + ResizeMix 25% 75.78 59.95 40.82 + MergeMix 25% 77.91 61.56 48.86 Vanilla 74.63 58.76 52.71 SFT Vision 25% 76.97 61.43 49.79 25% 76.84 61.15 49.31 + MixUp 48.21 25% 76.89 + CutMix + ResizeMix 25% 75.01 59.72 40.82 48.79 + MergeMix 25% 76.91 61.0 61.3 Inference Full Vision Token 66.8 69.11 70.20 69.21 66.78 69. 58.2 55.03 55.90 55.43 53.45 55.40 1510.7 1420.69 1444.45 1408.18 1430.64 1458.49 64.3 64.17 64.29 64.23 62.37 64.86 Inference 75% Vision Token 68.42 69.86 69.46 69.71 66.63 70.2 55.66 55.03 56.17 56.08 53.75 55. 1460.88 1423.27 1466.33 1420.67 1459.84 1483.82 63.05 64.94 66.06 67.52 63.48 66.58 Inference 50% Vision Token 68.86 70.10 70.05 69.96 67.33 70.5 55.33 54.91 56.0 55.97 53.44 56.0 1452.18 1443.02 1448.62 1428.02 1456.66 1477. 62.8 64.69 66.92 67.01 63.4 66.15 Inference 25% Vision Token 68.67 70.0 69.66 69.32 66.78 70.2 55.32 53.56 53.98 53.99 51.66 54.51 1398.24 1405.86 1409.16 1370.27 1418.27 1441.07 60.65 64.94 65.8 66.32 62.45 66. 58.3 56.09 57.73 57.13 51.54 57.98 57.9 57.47 58.67 59.19 52.92 59.45 56.87 57.38 58.07 59.02 53.09 58.76 54.03 56.7 57.81 57.98 52.66 58.24 85.87 66.19 65.57 86.82 65.04 65.17 -0.40 86.94 65.77 65.92 +0.35 86.09 65.19 65.31 -0.26 85.26 61.26 61.91 -3.66 87.22 65.10 65.56 -0.01 85.60 65.21 64.84 85.86 65.77 65.42 +0.58 86.14 65.67 66.06 +1.22 85.14 65.42 65.94 +1.1 84.99 62.08 62.42 -2.42 86.18 65.49 65.95 +1. 86.53 64.09 64.55 86.21 65.41 65.25 +0.7 86.44 65.54 65.94 +1.39 85.89 65.57 65.69 +1.14 85.57 61.65 62.34 -2.21 86.41 65.19 65.71 +1.16 86.54 62.05 63.71 86.81 64.51 64.97 +1.26 86.72 64.49 65.08 +1.37 86.42 64.21 64.96 +1.25 85.87 60.54 61.72 -1.99 65.16 +1.45 86.57 64.2 22 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding"
        },
        {
            "title": "D VISUALIZATION OF MERGEMIX",
            "content": "In this section, we plot some visualizations of token merge with different merge ratios, mixed samples with different λ in Figure 9 and Figure 10, and some case studies of different augmentation degrees in Figure 8. Figure 8: The visualization of the visual question answers with different mixing ratios by LLaVAv1.5-7B model. Note that the blue texts denote the core question and the corresponding correct answers, while the green texts denote the wrong answer to the question. The raw image denotes without any augmentations, and other images denote with different mixing ratios λ. Ground-truth Answer denotes the raw labels for this case. With the mixing degree improving, the answer comes out more wrong or unrelated to the question, as shown in green color. 23 MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Figure 9: The visualization of ToMe with different merge ratios and mixed samples with different mixing ratios. MergeMix: Unified Augmentation Paradigm for Visual and Multi-Modal Understanding Figure 10: The visualization of ToMe with different merge ratios and mixed samples with different mixing ratios."
        }
    ],
    "affiliations": [
        "Westlake University, Hangzhou, China",
        "Zhejiang University, College of Computer Science and Technology, Hangzhou, China"
    ]
}