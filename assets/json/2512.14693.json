{
    "paper_title": "Universal Reasoning Model",
    "authors": [
        "Zitian Gao",
        "Lynx Chen",
        "Yihao Xiao",
        "He Xing",
        "Ran Tao",
        "Haoming Luo",
        "Joey Zhou",
        "Bryan Dai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM."
        },
        {
            "title": "Start",
            "content": "Zitian Gao Lynx Chen Yihao Xiao He Xing Ran Tao Haoming Luo Joey Zhou Bryan Dai * Ubiquant {ztgao02,ylchen,yhxiao,xyyang,rtao02,hmluo,jzhou,cbdai} @ubiquant.com"
        },
        {
            "title": "Abstract",
            "content": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM."
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in recurrent models [7, 11, 20] have demonstrated the effectiveness of Universal Transformers (UTs) [5] in addressing complex reasoning tasks, such as ARC-AGI and Sudoku [2, 3]. UT-based small models, despite being trained from scratch on these tasks without internet-scale pre-training, consistently outperform most standard Transformer-based Large Language models (LLMs) by significant margin [20]. 5 2 0 2 6 1 ] . [ 1 3 9 6 4 1 . 2 1 5 2 : r Figure 1: Performance comparison of UT-based models on the ARC-AGI and Sudoku benchmarks. ARC-AGI 1 and 2 scores are taken from the official ARC-AGI leaderboard for reliability. * Corresponding author. 0This comparison focuses on pass@1 score of single small models trained from scratch under the same data setting as HRM and TRM, excluding test-time scaling, ensembling, and visual methods such as VARC [10]. 1 While this contrast highlights the potential of UTs for depth-intensive iterative reasoning, the function and impact of gating mechanisms remain insufficiently explored beyond their initial intuition Prior studies often attribute improvements to high-level architectural innovations [7, 11, 20], yet our analysis reveals that the core performance gain actually arises from the often-overlooked recurrent inductive bias intrinsic to the Universal Transformer. In particular, nonlinear depth-wise computation plays much larger role than previously acknowledged, suggesting that architectural modifications that enhance recurrent processing can yield substantial downstream improvements. Motivated by this insight, we further investigate and strengthen this inductive bias via simplified yet effective enhancement to the UT framework, enabling stronger abstraction capabilities while preserving parameter efficiency. Our main contributions are as follows: Through extensive ablation studies, we show that the performance of models on ARC-AGIstyle complex reasoning tasks primarily stems from their nonlinearity. Moreover, we reveal that the true source of reasoning capability beyond standard Transformers comes from the recurrent mechanism of Universal Transformers rather than overly elaborate design in prior work. By introducing short convolutions and truncated backpropagation into the Universal Transformer, we achieve state-of-the-art 53.8% pass@1 accuracy on ARC-AGI 1 and 16.0% on ARC-AGI 2."
        },
        {
            "title": "2.1 Standard Transformer",
            "content": "Let denote the vocabulary of size , and let = (x1, . . . , xN ) be an input sequence of length . We define the token embedding function as ϕ : RN d, mapping discrete tokens to d-dimensional continuous representation. Conversely, the unembedding function (or language modeling head) is denoted by ψ : RN RN , which projects hidden states back to the vocabulary logit space. single Transformer layer, parameterized by θ, is defined as function Tθ : RN RN d. This function typically composes Multi-Head Self-Attention (MHSA) module and Position-wise Feed-Forward Network (FFN), each wrapped with residual connections and layer normalization: Tθ(H) = FFN(LN(H + H)), where = MHSA(LN(H)) standard, non-recursive Transformer model Mstd of depth is constructed by stacking layers with distinct parameters Θ = {θ1, . . . , θL}. The forward pass is the composition of these layers: Mstd(x) = ψ TθL Tθ1 ϕ(x) Here, the operator denotes function composition. The computational cost and parameter count both scale linearly with L, creating rigid coupling between model capacity and inference compute."
        },
        {
            "title": "2.2 Universal Transformer",
            "content": "The Universal Transformer (UT) [5] extends the standard Transformer [18] by introducing recurrent computation over depth. Instead of stacking distinct layers, the UT applies single transition block repeatedly to refine token representations. For an input sequence with embedding matrix H0 Rnd, the UT updates states as Ht+1 = LayerNorm(cid:0)Ht + MHA(cid:0)Ht(cid:1)(cid:1) , 2 followed by shared position-wise transition function Ht+1 LayerNorm(cid:0)Ht+1 + Transition(cid:0)Ht+1(cid:1)(cid:1) , = 0, . . . , 1, where Transition is either feed-forward network or separable convolution. To encode both position and refinement depth, UT adds 2-D sinusoidal embeddings at each step. 2.2.1 Parameter Sharing key design of UT is weight tying across depth. The attention and transition parameters ΘUT = {WQ , WK , WV , WO, ΘTransition} are reused for all t. Thus, the model performs iterative representation refinement with flexible number of steps , enabling (i) depth adaptation at inference and (ii) higher theoretical expressivity than fixed-depth Transformers. 2.2.2 Adaptive Computation Time (ACT) With ACT [9], different tokens may halt at different recurrent steps. At step t, each position predicts halting probability accumulated until reaching threshold 1 ϵ. The final token representation is weighted mixture pt,i = σ(wht,i + b), hfinal = (cid:88) t,i ht,i, where t,i is the truncated allocation. ACT allows UT to allocate more computation to complex tokens and less to simpler ones."
        },
        {
            "title": "3 Universal Reasoning Model",
            "content": "The base architecture of our Universal Reasoning Model (URM) closely follows that of the Universal Transformer [5], with the difference being its decoder-only design. This aspect is consistent with previous works such as HRM [20] and TRM [11]. Our work differs from previous models [11, 20] by introducing the following ConvSwiGLU module and Truncated Backpropagation Through Loops mechanism."
        },
        {
            "title": "3.1 ConvSwiGLU",
            "content": "To strengthen the non-linearity of Universal Transformer, we introduce ConvSwiGLU (motivation see Section 4.6), which augments the standard SwiGLU feed-forward block with depthwise short convolution. Unlike the conventional point-wise SwiGLU [16], which treats each token independently, our design explicitly injects local contextual interactions into the gating mechanism, introducing lightweight channel mixing in token space without increasing sequence-level complexity [1, 22]. Given an input sequence RT d, we first project it into an expanded intermediate representation: The SwiGLU activation produces gated representation: [G, U] = XWup RT 2m. 3 backpropagation loops = . . . layer MLP Attention TBPTL ACT loops fixed loops layer MLP Attention forward only loops = 1 . . . Linear (h) SiLU Conv SiLU Gate Up Linear (h 2h) Standard Transformer Universal Reasoning Model ConvSwiGLU Figure 2: Illustration of our Universal Reasoning Model (URM) architecture. The left shows standard Transformer layer stack, while the right illustrates the URM with fixed loops, ACT loops, and the ConvSwiGLU module. For illustrative purposes, components such as embeddings, residual connections, RMSNorm, positional encodings, and other modules are omitted, in right figure represents the first loops of the inner loop in forward-only mode, TBPTL represents our proposed Truncated Backpropagation Through Loops. Hffn = SiLU(G) U. To integrate short-range token interactions, we apply depthwise 1D convolution over the gated features: Hconv = σ(cid:0)Wdwconv Hffn (cid:1), where Wdwconv Rm1k is depthwise convolution kernel of size = 2. Finally, the output is projected back to the hidden dimension: = (cid:2)σ(Wdwconv (SiLU(G) U))(cid:3)Wdown."
        },
        {
            "title": "3.2 Truncated Backpropagation Through Loops",
            "content": "When the number of recurrent reasoning loops becomes large, the gradients propagated from early loops may hinder optimization due to noise accumulation and instability (see empirical evidence in Section 4.5). To alleviate this issue, we employ Truncated Backpropagation Through Loops (TBPTL) and only compute gradients for the later loops. Consider D-layer Universal Reasoning Model unrolled for iterative loops during training. Let h(d) denote the hidden representation of layer {1, . . . , D} at iteration {1, . . . , }. The recurrent transition is defined as: h(d) = (d) θ (cid:0)h(d1) , h(d) t1 (cid:1), where (d) θ denotes the parameterized transformation at layer with trainable parameters θ. Instead of backpropagating through all loops, we partition the rollout into forward-only and trainable segments. Specifically, for truncation index < : {1, 2, . . . , } , (cid:124) (cid:125) (cid:123)(cid:122) no backward pass {N + 1, . . . , } (cid:123)(cid:122) (cid:125) (cid:124) forward + backward . During training, we compute gradients only on the loss accumulated in the latter (M ) loops: LTBPTL(θ) = (cid:88) t=N +1 L(cid:0)h(D) , y(cid:1), where L() is cross-entropy loss function. The gradients with respect to θ are thus: θLTBPTL = (cid:88) t=N +1 h(D) h(D) θ . Example. For configuration with = 4 layers and = 8 inner loops, we choose = 2 forwardonly loops. Thus, only the last 6 loops (i.e., = 3 to = 8) contribute to gradient computation."
        },
        {
            "title": "4.1 Experiment Settings",
            "content": "Our experimental setup largely follows HRM and TRM [11, 20]. We use the same datasets and augmented data as in prior work, and apply an exponential moving average (EMA) to model parameters to improve training stability, following [11]. All models are trained with the AdamAtan2 optimizer [6]. For ARC-AGI 1 and ARC-AGI 2, the main model learning rates are set to 1 104 and 3 104, respectively, while the puzzle embedding uses learning rate of 1 102; for Sudoku, the puzzle embedding learning rate is 1 104. Weight decay is set to 0.1 for both the main model and puzzle embedding on ARC-AGI 1 and ARC-AGI 2, and to 1.0 for Sudoku, consistent with prior work. The model has 4 layers with hidden size 512 and 8 attention heads. The inner loop runs for 8 steps, with the first two steps being forward-only, while the outer loop employs Adaptive Computation Time (ACT) [9] with maximum of 16 steps."
        },
        {
            "title": "4.2 Main Results",
            "content": "ARC-AGI 1 ARC-AGI 2 Sudoku pass@1 pass@10 pass@100 pass@1000 pass@ pass@10 pass@100 pass@1000 pass@1 HRM TRM URM w/o Short Conv. w/o Trunc. Backprop. 34.4 40.0 53. 45.3 40.0 46.4 51.3 71.3 62. 54.4 55.0 59.8 80.4 72.0 64. 60.5 64.4 85.1 78.3 70.5 5. 4.6 9.6 7.4 16.0 26.9 - - - - 14.3 11.7 34. - - 18.6 13.6 41.3 - - 63.9 66.8 77.6 - - Table 1: The performance of URM, TRM, and HRM on three complex reasoning tasks: ARC-AGI 1, ARC-AGI 2, and Sudoku. pass@n denotes the pass rate when sampling answers from the model; sample is considered correct if at least one of the answers is correct. The scores of TRM and HRM in this table may differ from those shown in the teaser. This is because the teaser scores are taken directly from the official ARC-AGI leaderboard for rigor, whereas the scores in this table are reproduced from the official TRM and HRM repositories following their official evaluation procedures. Minor discrepancies may occur due to randomness. 5 As shown in Table 1, the Universal Reasoning Model (URM) achieves substantial improvements over prior UT-based approaches across all benchmarks. On ARC-AGI 1, URM reaches 53.8% pass@1, outperforming TRM (40.0%) and HRM (34.4%) by large margins. On ARC-AGI 2, URM obtains 16.0% pass@1, nearly tripling HRM and more than doubling TRM. similar advantage appears on Sudoku, where URM achieves 77.6% accuracy, surpassing both TRM and HRM. Notably, URMs gains further widen under larger sampling budgets (e.g., pass@1000), indicating that iterative refinement enables richer candidate generation rather than brittle one-step predictions."
        },
        {
            "title": "4.3 Why Universal Transformer?",
            "content": "Layer Loop Hidden Size Params FLOPs pass@1 pass@10 pass@ pass@1000 2 2 2 2 4 4 4 4 6 6 6 6 8 8 8 8 16 32 64 2 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 8 8 256 384 512 768 256 384 512 768 256 384 512 768 256 384 512 768 1024 512 512 512 Vanilla Transformers 1 1.5 2 3 2 3 4 6 3 4.5 6 9 4 6 8 12 32 32 32 0.75 2.75 3.63 2.75 4.25 2.88 5.13 5.63 4.63 5.00 7.88 8.13 6.88 7.00 8.50 10.63 0.00 23.75 18.25 1 1.5 2 3 2 3 4 6 3 4.5 6 9 4 6 8 12 32 32 32 3.75 4.13 6.00 5.00 8.25 5.88 9.00 9.25 8.75 9.38 11.13 12.13 11.25 11.38 12.75 17.38 6.50 34.13 31. Universal Transformers 2 4 16 32 36.25 40.00 50.75 54.38 5.75 6.75 8.88 7.38 10.50 8.38 10.50 10.75 11.75 11.25 13.75 16.13 13.63 13.13 15.75 21.50 8.75 38.88 38. 61.25 64.50 7.00 9.13 11.00 9.13 13.88 10.13 12.63 12.25 13.38 13.25 15.63 17.88 15.63 14.63 17.13 23.25 9.75 43.38 41.38 66.88 70.50 Table 2: Comparison between vanilla Transformers and Universal Transformers under different model depths, hidden sizes, and loops. We report pass@n results on ARC-AGI 1. Table 2 demonstrates that the performance gains of Universal Transformers (UTs) on ARC-AGI 1 arise from substantially higher parameter efficiency rather than increased model scale or computational budget. With only 4 parameters, UT achieves pass@1 score of 40.0, dramatically outperforming vanilla Transformers that employ up to 32 more parameters yet remain markedly weaker. Simply scaling depth or width in vanilla Transformers yields diminishing returns and can even lead to performance degradation, highlighting fundamental inefficiency in how parameters are used to support multi-step reasoning. Crucially, this advantage persists even when computation is held constant. At 32 FLOPs, reallocating computation from deep, non-shared layers to recurrent refinement improves pass@1 from 23.75 for vanilla Transformers to 40.0 for UTs. This behavior is consistent with analyses of previous works [15], which argue that many reasoning tasks benefit more from iterative computation than from increasing the number of independent layers. In standard Transformers, additional FLOPs are often 6 spent on redundant refinement in higher layers, whereas recurrent computation converts the same budget into increased effective depth [15, 23]. This superior efficiency is driven by the recurrent inductive bias introduced by parameter sharing across depth. Through repeated application of shared transformation, UTs realize iterative refinement that better aligns with the structure of algorithmic reasoning, while avoiding any increase in parameter count. Consequently, under both fixed parameter and fixed FLOPs budgets, UTs consistently outperform vanilla Transformers on reasoning tasks, making them particularly well suited for reasoning-intensive settings such as ARC-AGI, where multi-step abstraction is more critical than sheer scale."
        },
        {
            "title": "4.4 Short Convolution",
            "content": "Figure 3: ARC-AGI pass@1 results for inserting the short convolution module at different positions within the UT transition (left figure), and varying the kernel size of the ConvSwiGLU module applied after the MLP expansion (right figure). To strengthen the nonlinear inductive bias of the Universal Transformer, we introduce depthwise short convolution module parameterized by Wdwconv (see Section 3.1 for details), which provides token-local mixing while preserving the per-step computational budget. Since ARC-AGI performance correlates strongly with nonlinear capacity (Section 4.6), we evaluate how inserting this module at different locations affects the recurrent transition. We examine six insertion points: (a) after the SDPA output; (b) after the value projection; (c) after the key projection; (d) after the query projection; (e) between multi-head concatenation and the output projection; and (f) after the MLP expansion. Figure 4: Visualization of the attention matrices after adding Short Convolution. The left figure shows the standard Universal Transformer, while the right figure shows the Universal Transformer with ConvSwiGLU applied. As shown in Figure 3, inserting the Wdwconv module inside the attention pathway, positions (a)(d), does not yield improvements and often degrades performance, suggesting that local perturbations interfere with the geometric structure of attentions linear projections. mild gain appears at position (e), where the perturbation acts only on aggregated multi-head features. The dominant effect arises at position (f), after the MLP expansion, indicating that short-range mixing is most beneficial when applied within an already nonlinear subspace. This supports functional interpretation in which the MLPnot attentionconstitutes the models primary source of expressive nonlinearity; augmenting it with Wdwconv substantially enhances the models nonlinear representational capacity. As shown in Fig. 4, the incorporation of short convolution into the MLP significantly enhances channel mixing. While the standard Universal Transformer exhibits relatively sparse and homogeneous attention patterns, the model with ConvSwiGLU produces attention matrices with more diverse and structured distributions. This suggests that short convolution facilitates more effective inter-channel information flow, thereby improving the expressiveness of the attention mechanism."
        },
        {
            "title": "4.5 Truncated Backpropagation Through Loops",
            "content": "Loop w/ grad. Loop w/o grad. pass@1 pass@10 pass@100 pass@1000 7 6 5 4 3 1 0 1 2 3 5 6 7 36.25 37.75 39. 39.50 38.75 36.88 34.25 22.50 50. 49.13 53.50 51.63 50.50 49.00 46. 37.00 61.25 59.50 61.88 60.88 61. 57.75 55.75 45.38 66.88 65.88 66. 65.25 65.88 63.88 61.75 52.38 Table 3: Effect of Truncated Backpropagation Through Loops (TBPTL) across inner loops on ARCAGI 1. Loop w/o grad. denotes the number of forward-only inner-loop iterations, while Loop w/ grad. indicates the number of inner loops involved in backpropagation. As shown in Table 5, when the total number of inner loops is fixed to 8, truncating gradients for the first two loopsi.e., running the initial two inner-loop iterations in forward-only modeachieves the best performance. Both pass@1 and pass@1000 peak at this truncation setting, while shorter or longer truncation horizons result in inferior outcomes. This trend closely resembles truncated backpropagation through time (TBPTT) in recurrent neural networks, where the underlying motivation is largely the same. In full backpropagation through time, gradients are propagated through the entire sequence, which incurs high computational and memory costs and often yields ineffective long-range gradients due to vanishing or exploding behaviors. As result, practical implementations typically restrict gradient propagation to fixed recent window, e.g., by backpropagating errors only through the last time steps and updating the network parameters accordingly [14, 17]. Similarly, in universal transformers, propagating gradients across all inner-loop iterations can lead to unstable optimization, while overly aggressive truncation limits the models ability to coordinate multi-step refinement. Moderately truncating gradient propagation therefore provides favorable balance between optimization stability and effective long-horizon learning. We note that all results in this experiment are obtained using two-layer URM without the short convolution module, which differs from the full URM model reported earlier."
        },
        {
            "title": "4.6 Nonlinearity of Transformers",
            "content": "Model pass@1 pass@10 pass@100 pass@1000 Full Universal Reasoning Model 53.75 w/o Short Conv. SwiGLU SiLU SiLU ReLU w/o Attention Softmax 45. 29.75 28.63 2.00 71.25 62.63 42. 43.38 6.75 80.38 72.00 50.00 50. 10.25 85.13 78.25 54.50 54.88 15. Table 4: Ablation study on nonlinearity architectural components of the Universal Reasoning Model. We report pass@n results on ARC-AGI 1. All experiments are conducted under exactly the same settings as in Section 4.1. As shown in Table 4, the performance on ARC-AGI 1 decreases monotonically as nonlinear components are progressively removed from the model. Among these components, the activation function in the MLP plays particularly critical role: replacing SwiGLU with simpler nonlinearities such as SiLU or ReLU leads to substantial degradation, while completely removing the attention softmax results in dramatic collapse in performance. This clear monotonic trend highlights the importance of strong nonlinear transformations for solving complex abstract reasoning tasks. These results suggest that the expressive power required for ARC-AGI primarily arises from rich nonlinear mappings. Weakening the nonlinearity may systematically limits the models ability to represent complex reasoning skills. We note that the model still retains certain forms of nonlinearity that are not ablated in this study, such as the RMSNorm applied after each layer and the dot-product interaction between queries and keys in attention. However, these components are either difficult to remove without causing training instability or represent relatively weak nonlinear effects compared to explicit activation functions. As ablating them typically leads to training failure, they fall outside the scope of the present analysis."
        },
        {
            "title": "4.7 Muon Optimizer",
            "content": "Figure 5: ARC-AGI pass@1 and pass@1000 performance of Adam and Muon optimizers on ARCAGI 1 and ARC-AGI 2 benchmarks. Solid lines denote pass@1000, dashed lines denote pass@1, and colors indicate different optimizers. Training steps are shown in thousands (K). To evaluate the training efficiency of the Universal Reasoning Model (URM), we compare the Muon (Momentum Updated Orthogonal Newton) optimizer [12] with standard adaptive baseline, 9 Adamatan2 [6]. Muon approximates second-order curvature to apply orthogonal updates to better handle the complex loss landscapes [8] induced by deep recurrent structures. Both models are trained from scratch under identical experimental settings, including batch size, learning rate schedules, and data augmentation, ensuring that any observed differences arise solely from the choice of optimizer. Across the ARC-AGI 1 and ARC-AGI 2 benchmarks, Muon demonstrates substantially faster convergence. On ARC-AGI 2, the Muon-optimized model reaches pass@1 accuracy of 11.5% in approximately 600,000 training steps, whereas the Adamatan2 baseline requires over 1,300,000 steps to achieve the same performance, corresponding to nearly twofold speedup in optimization. Despite this advantage in early training, both methods converge to similar final accuracies (approximately 53.8% on ARC-AGI 1 and 16.0% on ARC-AGI 2), indicating comparable asymptotic performance. These results suggest separation between optimization efficiency and architectural capacity in the URM. While Muon preconditions the challenging spectral properties of recurrent weight matrices [13] and reduces training cost, it does not lead to improved final generalization."
        },
        {
            "title": "5.1 ARC-AGI",
            "content": "Prior work on the ARC-AGI benchmark [2, 3] spans vision-based formulations, large language model (LLM) adaptation, and recurrent reasoning architectures. Vision-centric approaches such as Vision ARC [10] reformulate ARC as an image-to-image transformation problem and show that standard visual inductive biases can achieve competitive performance, particularly with ensembling and testtime scaling. LLM-based methods explore fine-tuning and test-time training, demonstrating that transient parameter updates outperform static in-context learning on ARC-like tasks. Beyond language and vision models, recurrent architectures emphasize iterative computation as core mechanism for abstraction. The Hierarchical Reasoning Model (HRM) [7, 20] introduces multi-timescale recurrence and achieves strong ARC-AGI results, while subsequent analyses suggest that its gains may largely stem from recurrence rather than explicit hierarchy. The Tiny Recursive Model (TRM) [11] further simplifies this paradigm, showing that single lightweight network applied recursively can match or exceed more complex hierarchical designs."
        },
        {
            "title": "5.2 Universal Transformers (Looped Transformers)",
            "content": "The Universal Transformer (UT), also known as the Looped Transformer, was introduced by Dehghani et al. [5] as an extension of the standard Transformer with recurrent computation and adaptive computation time. Subsequent work has shown that UTs exhibit significantly stronger multi-step reasoning abilities than vanilla Transformers, as the recurrent refinement mechanism helps overcome architectural limitations in multi-hop reasoning tasks [4, 19]. In addition, UTs demonstrate improved algorithmic learning capabilities, enabling more effective modeling of iterative and rule-based computations [21]. By reusing parameters across refinement steps, UTs also achieve higher parameter efficiency, allowing more expressive computation without increasing model size [15]."
        },
        {
            "title": "6 Conclusion",
            "content": "We systematically investigate the sources of performance gains in Universal Transformer models on complex reasoning tasks. Extensive ablation studies reveal that these gains stem primarily from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from overly complex architectural designs. Motivated by this insight, we propose the Universal Reasoning Model (URM), which enhances nonlinear depth-wise computation via short convolutional gating and improves optimization stability through truncated backpropagation through loops. URM achieves state-of-the-art performance on ARC-AGI 1 and 2."
        },
        {
            "title": "References",
            "content": "[1] Zeyuan Allen-Zhu. Physics of language models: Part 4.1, architecture design and the magic of canon layers. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [2] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report, 2025. [3] Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-agi-2: new challenge for frontier ai reasoning systems, 2025. [4] Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The devil is in the detail: Simple tricks improve systematic generalization of transformers. In Proc. Conf. on Empirical Methods in Natural Language Processing (EMNLP), Punta Cana, Dominican Republic, November 2021. [5] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers, 2019. [6] Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A. Alemi, Roman Novak, Peter J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and Jeffrey Pennington. Scaling exponents across parameterizations and optimizers. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. [7] Renee Ge, Qianli Liao, and Tomaso Poggio. Hierarchical reasoning models: Perspectives and misconceptions, 2025. [8] Zixuan Gong, Jiaye Teng, and Yong Liu. What makes looped transformers perform better than non-recursive ones (provably), 2025. [9] Alex Graves. Adaptive computation time for recurrent neural networks, 2017. [10] Keya Hu, Ali Cy, Linlu Qiu, Xiaoman Delores Ding, Runqian Wang, Yeyin Eva Zhu, Jacob Andreas, and Kaiming He. Arc is vision problem!, 2025. [11] Alexia Jolicoeur-Martineau. Less is more: Recursive reasoning with tiny networks, 2025. [12] Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. [13] Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, Yanru Chen, Huabin Zheng, Yibo Liu, Shaowei Liu, Bohong Yin, Weiran He, Han Zhu, Yuzhi Wang, Jianzhou Wang, Mengnan Dong, Zheng Zhang, Yongsheng Kang, Hao Zhang, Xinran Xu, Yutao Zhang, Yuxin Wu, Xinyu Zhou, and Zhilin Yang. Muon is scalable for llm training, 2025. [14] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 13101318, Atlanta, Georgia, USA, 1719 Jun 2013. PMLR. [15] Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank J. Reddi. Reasoning with latent thoughts: On the power of looped transformers. In The Thirteenth International Conference on Learning Representations, 2025. [16] Noam Shazeer. Glu variants improve transformer, 2020. [17] Corentin Tallec and Yann Ollivier. Unbiasing truncated backpropagation through time, 2017. [18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [19] Boshi Wang, Xiang Yue, Yu Su, and Huan Sun. Grokked transformers are implicit reasoners: mechanistic journey to the edge of generalization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [20] Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, and Yasin Abbasi Yadkori. Hierarchical reasoning model, 2025. 11 [21] Liu Yang, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos. Looped transformers are better at learning learning algorithms. In The Twelfth International Conference on Learning Representations, 2024. [22] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and In Proceedings of the Shuicheng Yan. Metaformer is actually what you need for vision. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10819 10829, June 2022. [23] Yang Zhang, Yanfei Dong, and Kenji Kawaguchi. Investigating layer importance in large language models. In The 7th BlackboxNLP Workshop, 2024."
        }
    ],
    "affiliations": [
        "Ubiquant"
    ]
}