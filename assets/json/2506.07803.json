{
    "paper_title": "Image Reconstruction as a Tool for Feature Analysis",
    "authors": [
        "Eduard Allakhverdov",
        "Dmitrii Tarasov",
        "Elizaveta Goncharova",
        "Andrey Kuznetsov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision encoders are increasingly used in modern applications, from vision-only models to multimodal systems such as vision-language models. Despite their remarkable success, it remains unclear how these architectures represent features internally. Here, we propose a novel approach for interpreting vision features via image reconstruction. We compare two related model families, SigLIP and SigLIP2, which differ only in their training objective, and show that encoders pre-trained on image-based tasks retain significantly more image information than those trained on non-image tasks such as contrastive learning. We further apply our method to a range of vision encoders, ranking them by the informativeness of their feature representations. Finally, we demonstrate that manipulating the feature space yields predictable changes in reconstructed images, revealing that orthogonal rotations (rather than spatial transformations) control color encoding. Our approach can be applied to any vision encoder, shedding light on the inner structure of its feature space. The code and model weights to reproduce the experiments are available in GitHub."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 3 0 8 7 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Image Reconstruction as a Tool for Feature Analysis",
            "content": "Eduard Allakhverdov AIRI Moscow, Russia MIPT Dolgoprudny, Russia allakhverdov@2a2i.org Dmitrii Tarasov AIRI Moscow, Russia d.tarasov@airi.net Elizaveta Goncharova AIRI Moscow, Russia goncharova@airi.net Andrey Kuznetsov AIRI Moscow, Russia kuznetsov@airi.net"
        },
        {
            "title": "Abstract",
            "content": "Vision encoders are increasingly used in modern applications, from vision-only models to multimodal systems such as vision-language models. Despite their remarkable success, it remains unclear how these architectures represent features internally. Here, we propose novel approach for interpreting vision features via image reconstruction. We compare two related model families, SigLIP and SigLIP2, which differ only in their training objective, and show that encoders pre-trained on image-based tasks retain significantly more image information than those trained on non-image tasks such as contrastive learning. We further apply our method to range of vision encoders, ranking them by the informativeness of their feature representations. Finally, we demonstrate that manipulating the feature space yields predictable changes in reconstructed images, revealing that orthogonal rotations rather than spatial transformations control color encoding. Our approach can be applied to any vision encoder, shedding light on the inner structure of its feature space. The code and model weights to reproduce the experiments are available in GitHub."
        },
        {
            "title": "Introduction",
            "content": "In recent years, vision encoders have become crucial component of machine-learning pipelines serving as generalized image representations. They serve as specialized modules for solving computervision tasks, aggregating information for image generators (image priors), and powering modern visionlanguage models. The primary objective of these encoders is to capture all the semantic and object-level information in an image and transfer this knowledge to downstream tasks. Although experiments can show which model performs best on specific downstream task, important questions remain: How do hidden representations encode features? What is the relationship between these representations and the original images? Can we manipulate an image by influencing its features and vice versa? Answering these questions sheds new light on the interpretability of vision encoders and gives us confidence when choosing and working with their feature outputs. This, in turn, allows for more informed decisions about which encoder is best suited to particular downstream application. Preprint. Under review. In this work, we introduce novel reconstruction-based method to evaluate the quality of features extracted from input images. Our approach reconstructs images from hidden representations, revealing exactly what types of information are preserved within models internal layers. Our contributions are as follows: 1. novel interpretability metric: We propose an approach for comparing image-feature interpretability based on the visual quality of reconstructed images. 2. Analysis of training objectives: We perform detailed comparison between two models with identical architectures SigLIP (Zhai et al., 2023) and SigLIP2 (Tschannen et al., 2025) whose main difference is their training objective. We demonstrate that SigLIP2 produces significantly higher-fidelity reconstructions than SigLIP. 3. Models study: We investigate multiple vision-encoder designs varying in image resolution, training objectives, and hidden dimensions to understand how design choice and peculiarities in model training affect the amount of information each layer retains. 4. Feature-space transformations: We reveal that applying rotations in the embedding space corresponds to interpretable color transformations in the reconstructed images, pointing toward novel, semantically grounded editing operations. 5. We release the code to reproduce the results of the experiments available via GitHub repo1."
        },
        {
            "title": "2 Related work",
            "content": "Recent research has proposed various approaches to interpret the internal representations of vision encoders such as ViT Dosovitskiy et al. (2021), CLIP Radford et al. (2021a), DINO Caron et al. (2021a), SAM Kirillov et al. (2023), and others. These methods can be grouped into three major categories based on their interpretability strategy. 2.1 Attention and concept emergence analysis Many works focus on analyzing activations to determine which neurons or layers encode specific image conceptsranging from simple elements like text and colors to more complex structures such as objects and semantics. These approaches typically inspect self-attention maps or neuron activations to reveal what concepts are learned at different stages of transformer-based vision models. For example, Caron et al. (2021b) demonstrated that attention heads in self-supervised ViTs can function as unsupervised object detectors, effectively segmenting semantically meaningful regions. Dorszewski et al. (2025) performed neuron labeling across ViT layers and found clear progression from low-level features (e.g., edges, color) to high-level concepts (e.g., object parts, categories). More recently, Darcet et al. (2024) introduced register tokens to eliminate pathological attention patterns and restore clear, interpretable attention in large ViTs. Together, these studies highlight how semantic and spatial regularities can naturally emerge in transformer attention mechanisms especially under self-supervised or architecture aware training regimes. 2.2 Representational similarity and probing tasks Representational similarity analysis and probing tasks offer large-scale insights into feature spaces. widely used technique is Centered Kernel Alignment (CKA) Kornblith et al. (2019), which quantifies the similarity between representations from two layers (or two models) on the same inputs remaining invariant to orthogonal transformations and isotropic scaling. By computing CKA across all layer pairs, researchers visualize models internal structure as heatmap. Raghu et al. (2021) applied CKA to compare ViT and ResNet, discovering that ViT representations remain highly homogeneous across layers, whereas ResNet exhibits more hierarchical progression with strong locality among adjacent layers. Beyond comparing architectures, CKA has been used to study different training paradigms: supervised versus self-supervised ResNets, or CLIPs image encoder versus supervised ViT. These analyses consistently find that supervised models concentrate class-discriminative information in deeper layers, while self-supervised models distribute various attributes throughout the network. Kornblith et al. (2019) further showed that supervised networks 1https://fusionbrainlab.github.io/feature_analysis/ 2 of different architectures converge to remarkably aligned final representations on ImageNet, despite diverging early-layer features. 2.3 Latent space manipulation Latent manipulation techniques operate directly on internal feature vectors such as StyleSpace Wu et al. (2021) or CLIP Radford et al. (2021b) embeddings to generate counterfactual or modified visual outputs. For instance, StylEx optimizes GAN latents to identify minimal, independent changes in features that affect classifier decisions (e.g., altering the background or object shape) Lang et al. (2021). Kazemi et al. (2024) performed directional edits in CLIPs embedding space to test compositionality and memorization, enabling cat astronaut image synthesis. Work on CLIP inversion also demonstrates that certain semantic directions correspond to recognizable visual attributes, which can be amplified or suppressed. By enabling fine-grained causal analysis of visual attributes, these methods facilitate the creation of controlled visual counterfactuals and deepen our understanding of model reliance on particular features. Prior works have explored inversion-based interpretability for convolutional encoders. Mahendran and Vedaldi (2016) proposed feature inversion by iteratively optimizing input pixels so that CNNs activations match those of target image, revealing which patterns each layer encodes, but their gradient-descent approach is too slow for real-time analysis and is tailored to convolutional architectures. Dosovitskiy and Brox (2016) advanced this research direction by training feedforward decoder to reconstruct images from CNN features in single pass, yielding much faster and higher-fidelity inversion; however, their focus remained on convolutional networks, and they did not examine how different pretraining objectives affect invertibility or the semantic geometry of the feature space. In contrast, our method delivers high-fidelity, real-time reconstruction for transformerbased encoders, enables direct comparison of contrastive versus multimodal training objectives, and by learning orthogonal operators in feature space maps latent transformations to precise pixel-level edits, thus providing novel tool for probing the structure of modern visionlanguage embeddings."
        },
        {
            "title": "Method",
            "content": "The idea behind the method of vision encoder feature interpretability through reconstruction stems from the following observation. All image encoders represent an input image as collection of numerical values that, in principle, encode large part of the information about the image. If we can recover the original image from this internal representation, then the quality of the reconstruction provides direct measure of how much information the encoders features preserve. As an example, let us consider the ViT in CLIP-base model. An input of size 224 224 3 pixels is mapped to feature tensor of shape 14 14 768, i.e. 150 528 scalar values. By learning an inverse mapping from that tensor back to pixel space, we can potentially reconstruct the original image. The closer the reconstruction is to the true image, the more we know that the encoders features capture detailed, informative representations. Once the reconstructor is in place, we can manipulate the feature tensor directly in latent space and observe how those changes manifest in the reconstructed image. In our experiments, we apply controlled orthogonal transformation in feature space that produce systematic color shifts in the output, demonstrating that specific rotations in the encoders latent space correspond to interpretable visual transformations. In Section 3, we provide detailed description of the introduced method."
        },
        {
            "title": "3 Reconstructor pipeline",
            "content": "3.1 Reconstructor architecture The proposed reconstructor module takes the tensor of the image features as input and provides the initial image reconstruction from this feature tensor. 3 Figure 1: frozen vision model generates image embeddings, which are then processed by reconstructor model that learns to approximate image reconstruction. Let RHW 3 denote an input image, where H, are the width and height of the input image, and 3 is the number of input channels. feature extractor : RHW Rhwc maps to latent feature tensor = E(i) (h and are patch size, and is hidden dimension). We then define reconstruction network Rθ : Rhwc RHW C, parameterized by θ, whose goal is to approximate the inverse of E. In the ideal case, we can (cid:0)f (cid:1) = i; however, in practice, we obtain reconstruct the same initial image from the latent space Rθ (cid:0)f (cid:1), where ˆi is the best reconstruction of under chosen loss, for example, mean squared ˆi = Rθ error. We train Rθ by minimizing the reconstruction loss Lrec over the dataset D. Lrec = Ei,f (cid:13) Rθ(f )(cid:13) (cid:13) 2 (cid:13) Image encoder is freezed during reconstructor training. Figure 1 visualize reconstructor training pipeline. The reconstructor model Rθ consists of four transformer blockseach including multi-head selfattention and feed-forward sublayerfollowed by upsampling layers interleaved with residual blocks to restore the spatial resolution from back to . 3.2 Reconstructor training The reconstructor Rθ is trained in supervised manner. Let {(ij, fj)}N j=1 be dataset of imagefeature pairs, where ij RHW 3 and fj = E(ij) Rhwc (f is calculated as the output of the last hidden state of E). We optimize Rθ to predict ij from fj by minimizing the ℓ2 reconstruction loss: Lrec = 1 N (cid:88) (cid:13) (cid:13) ij Rθ(fj)(cid:13) 2 2. (cid:13) j=1 To improve robustness, we apply channel-wise normalization to each spatial feature vector: u,v = u,v u,v 2 for all (u, v) {1, . . . , h} {1, . . . , w}. Although this operation discards the original vector norms removing magnitude cues it suppresses norm-related outliers common in CLIP-style features. 4 Dataset and Feature Extraction To train the image reconstructor Rθ, we assemble dataset drawn COCO corpus (CC-BY 4.0 license) (Lin et al., 2014). Specifically, we randomly sample 115,000 images for training and reserve an additional 4,000 images for validation. For each image ij in our dataset, we first compute its feature tensor fj = E(ij) Rhwc, where denotes the frozen vision encoder under study. The reconstructor is then trained to minimize the reconstruction error between the original image ij and its estimate Throughout training, we optimize θ using an ℓ2 reconstruction loss. ˆij = Rθ(fj)."
        },
        {
            "title": "4 Feature transformation",
            "content": "Leveraging our reconstruction-based interpretability pipeline, we can not only recover the input image from its feature tensor = E(i), but also probe how explicit manipulations in latent space translate back into pixel-space edits. In this section we (1) formulate the hypothesis that certain feature-space operators correspond to well-defined image-space operators, and (2) validate it via experiments with color swaps, channel suppression, and colorization. 4.1 Hypothesis formulation Using the proposed reconstructor pipeline, we are able to recover an image from its feature tensor = E(i). We hypothesize that applying some transformation in latent feature space may correspond to some modifications of the image in pixel space. We formalize the hypothesis relating feature-space transformations to image-space transformations as follows. Let RHW 3, = E(i) Rhwc. Suppose there exists Af : Rhwc Rhwc, Ai : RHW 3 RHW 3, = Af (f ), = Ai(i), where Af is the transformation in the feature space, and Ai is the corresponding transformation in the image space. Then, corresponds to i, i.e. applying Af in feature space should correspond to applying Ai in image space. To test this hypothesis, we compute = Af (cid:0)E(i)(cid:1), ˆi = Rθ(f ). In other words, transforming by Af should yield reconstructed image ˆi that visually matches the pixel-space transformation Ai(i). 4.2 Experimental validation via color manipulations 4.2.1 Color swap through reflection We evaluate the hypothesis of feature-image space connection through the task of color swap. Let Ai be an operator that swaps the red and blue channels of i. Following the claim introduced in the previous section, we can find corresponding Af in the latent space such that (cid:0)Af (f )(cid:1) Ai(i). Rθ Let us, first, look into the channels of the feature tensor for the random image in Figure 2. We can observe for any random image that each channel slice of the reconstructed tensor visually reproduces the original image. If we apply an image-space operator Ai that swaps the red and blue 5 Figure 2: Original image (left) and spatial activation maps for four selected channels of its encoded feature tensor , illustrating how individual feature channels capture coherent image structures. (a) Matrix Calculation (b) Matrix Application Figure 3: Scheme for computing and subsequently applying the matrix in feature space. channels of i, the reconstructed channels still mirror the modified image. Hence, the corresponding feature-space operator Af must preserve this per-channel replication property. natural choice is an orthogonal transformation applied independently to each spatial token. In addition, just as swapping the red and blue channels twice leaves the image unchanged, applying Af twice must recover the original features. This reversibility suggests that Af is not only orthogonal but also self-inverse, mirroring the symmetry of the image-space operation. Formally, let We posit that = E(i), = Ai(i), = Af (f ). k,ℓ, : = fk,ℓ, :, Rcc, QQ = Ic, QQ = Ic, for every spatial location (k, ℓ). Under this hypothesis, E(i) = Af (cid:0)f (cid:1), R(cid:0)E(i)(cid:1) = R(cid:0)Af (f )(cid:1). In our experiments, to estimate Q, we collect dataset of image pairs {(ij, feature pairs )}N j=1 and compute (fj, ) = (cid:0)E(ij), E(i )(cid:1), fj, Rhwc. For each spatial index (k, ℓ) and image j, we form the paired vectors (cid:0)fj[k, ℓ, :], [k, ℓ, :](cid:1). We solve the orthogonal Procrustes problem to obtain the matrix that optimally maps {fj[k, ℓ, :]} to {f [k, ℓ, :]} in least-squares sense. Projecting this matrix onto the subspace of self-conjugated orthogonal operators highlights its self-reversibility property. The computation and subsequent application of are illustrated in Figure 3. 4.2.2 Channel Suppression To further investigate the connection between feature and image spaces, we examine the operation of channel suppression. Let Ai denote an operator that attenuates the blue channel of image by factor α (0, 1), with Af representing its corresponding operator in the latent space. Following the [k, ℓ, :](cid:1) and use it to learn methodology outlined previously, we construct dataset (cid:0)fj[k, ℓ, :], the linear operator Af . We next examine the theoretical properties of Af . Define Pi as the operator that zeroes out the blue channel of the image i. Observe that An Pi as , and Pi is projection operator satisfying 2 = Pi. Considering the eigendecomposition of Af , we have: Af = λv, and consequently An = λnv. asymptotically approaches projection operator, its limit Pf = limn An Since An must also possess the properties of projection operator. This implies that the eigenvalues of Pf must be either 0 or 1. Therefore, λn must converge to either 0 or 1. Therefore, the eigenvalues of Af must satisfy either: λ = 1, or λ is complex values with magnitude strictly less than 1. These properties are empirically confirmed in Section 5.3. 4.2.3 Colorization The experiments described in Sections 4.2.1 and 4.2.2 demonstrate that simple pixel-space transformations correspond to linear transformations in feature space. This observation motivates the question of whether such correspondences extend to more complex, semantically informed transformations beyond algorithmic operations like channel manipulation. To investigate this, we examine the colorization task - transforming grayscale images to their color counterparts. This problem presents fundamentally different challenges from our previous cases: 1. Semantic Requirement: Successful colorization necessitates that the feature space geometry encodes real-world knowledge about plausible color distributions for objects and scenes. 2. Non-algorithmic Nature: Colorization cannot be achieved via simple pixel-wise operations; it depends on semantic understanding and context rather than deterministic channel manipulations. If we can identify linear transformation in feature space that maps grayscale representations to their colorized counterparts, it would suggest that during training the feature-extractor was able to learn such geometric space in which the semantics of the real world is translated into the language of this space. Such result would reveal non-trivial property of learned feature spaces. Following our established methodology, we construct dataset of paired feature vectors (cid:0)fj[k, ℓ, :], [k, ℓ, :](cid:1) representing grayscale and color versions of the same images. We then learn linear operator Af mapping between these representations. The experimental evaluation of this colorization-based mapping is presented in Section 5.4."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental setup Model choice To evaluate the insights provided by our reconstruction-based interpretability method, we conduct the main experiments using two vision-encoder backbones: the SigLIP and SigLIP2 at four input resolutions, = 224 224, 256 256, 384 384, and 512 512. These resolutions correspond to output feature tensor shapes of (h, w, c) = (14, 14, 768), (16, 16, 768), (24, 24, 768), and (32, 32, 768), respectively. SigLIP models are variant of CLIP that uses sigmoid-based loss. 7 (a) CLIP-Score (b) SigLIP2-Score (c) Reconstruction Samples Figure 4: Comparison of SigLIP and SigLIP2 reconstruction performance and visual results. The motivation for choosing this series of models lies in the similarity of the training and architectural details, whereas the only difference is in the training objectives. Both models are pretrained on the WebLI dataset Caron et al. (2024): SigLIP uses the English-only subset, while SigLIP2 employs multilingual corpus. Although these subsets are not identical, they share the same data source and overall distribution. The key distinction lies in their pretraining objectives: SigLIP is trained solely with contrastive loss, whereas SigLIP2 additionally incorporates image-captioning, self-distillation, and masked-prediction tasks. We also performed additional experiments on other well-known vision encoders, the experimental results are provided in Appendix For our evaluation, we reconstruct images from the COCO-val split and measure the fidelity of their latent representations. Let be an original validation image and ˆi = Rθ(E(i)) its reconstruction (cf. Section 2.3). Following Hessel et al. (2021) we compute cosine-similarity scores between the encoders embeddings of and ˆi: simorig,rec = cos(cid:0)S(i), S(ˆi)(cid:1), where is instantiated as either the CLIP or SigLIP2 encoder. Figure 4 reports these similarity scores alongside representative reconstruction examples from the COCO-val set. We employ Wilcoxon signed-rank and bootstrap tests to demonstrate SigLIP2s statistically significant superiority over SigLIP in reconstruction quality across all evaluated resolutions (p < 0.01 for all comparisons), with detailed results presented in Table 1. Table 1: Comparison of reconstruction quality between SigLIP and SigLIP2 evaluated using google-SigLIP2large-patch16-256 and openai-clip-vit-large-patch14. Reported are p-values from Wilcoxon signed-rank tests and bootstrap tests (n = 1000 samples, = 100, 000 resampling iterations). Null hypothesis (H0): equivalent reconstruction quality; alternative hypothesis (H1): SigLIP2 provides superior reconstruction quality Resolution CLIP-ViT SigLIP Wilcoxon Bootstrap Wilcoxon Bootstrap 224 224 px 256 256 px 384 384 px 512 512 px 1.5 1079 2.3 10123 3.9 1082 2.0 10 <1 10300 <1 10300 <1 10300 <1 10300 1.4 10113 1.2 10103 2.3 108 3.8 1064 <1 10300 <1 10300 2.2 104 <1 10300 8 Figure 5: Color-swap via orthogonal rotations in SigLIP2 feature space. Each row presents: (1) the original image, (2) its reconstruction from encoder features, (3) the image after swapping red and blue channels in pixel space, (4) the reconstruction of the pixel-swapped image, and (5) the reconstruction obtained by applying the corresponding orthogonal self-conjugated channel-swap directly in feature space. The near-identical results in columns 4 and 5 confirm that simple rotations in latent space induce coherent, interpretable color edits in the reconstructed images. 5.2 Color swap To evaluate whether swapping image color channels induces systematic shift in the learned feature space, we randomly sample 1024 images from the COCO validation set and train using the procedure described in Section 4. Next, we use the trained reconstructor Rθ to invert modified latents back to the RGB domain. Finally, we evaluate the entire pipeline on the remaining COCO-val images unseen during the training of both and Rθ. Example reconstructions for SigLIP2 model are shown in Figure 5. We also provide additional examples in Appendix (Figure 12). We observe that reconstructing from the transformed latent representations reproduces the channelswapped image. Interestingly, this demonstrates that color permutation in pixel space corresponds to orthogonal transformation in the feature space. Color swap operator properties ablations As detailed in Section 4, the feature-space operator Af is constrained to be both orthogonal and self-conjugated to maintain consistency with the properties of the pixel-space operator Ai. In this subsection, we conduct an ablation study examining the effects of these Af properties specifically its orthogonality and self-conjugacy. We trained three different operators: 1. Orthogonal self-conjugated as Procrustes solution with long-range projection of the operator onto the space of self-conjugated operators. 2. Orthogonal as Procrustes solution. 3. Linear as regression problem. (Note that this solution cannot be directly used with the reconstructor, as it fails to preserve vector norms. Since the reconstructor was trained exclusively on normalized vectors, we first normalize the resulting outputs before feeding them to the reconstructor.). As shown in Figure 6, the eigenvalues of all operators cluster along the real axis, indicating they primarily represent either eigenvector preservation (near +1) or inversion (near -1). While small deviations from these ideal values exist - revealing noise in the feature space - these perturbations remain relatively weak. Consequently, the feature space geometry largely preserves the properties expected from the pixel-space channel permutation operator. 9 Figure 6: Color swap operator properties ablation. Rows: (1) sample for transformed image reconstruction, (2) Af operator eigenvalues visualization, (3) values of real part of Af eigenvalues. Columns: (1) self-conjugated and orthogonal operator constraints, (2) orthogonal only constraint (3) no constraints 5.3 Channel suppression To evaluate the hypothesis proposed in Section 4.2.2 we use the same method as in Section 5.2, i.e., train linear operator on pairs of features for the initial image and the image with suppressed blue channel, then apply this operator on new images and check that the reconstruction visually coincides with the corresponding transformation in the pixel space. (Similar to the linear operator from Section 5.2, we normalize the features after exiting the linear transformation for the correct operation of the reconstructor.) In addition, we check whether the eigenvalues of the resulting operator are either equal to 1 or have norm less than 1 (see Section 4.2.2). As shown in Figure 7, the eigenvalues of the operator do obey the predicted property. The same figure shows that the repeated application of the operator behaves similarly to the projector as described in Section 4.2.2. Additional examples are provided in Appendix (Figure 13). 10 Figure 7: Ablation of operator properties for blue-channel suppression with coefficient α = 0.9. Rows: (1) Sample for transformed image reconstruction, (2) Af operator eigenvalues visualization, (3) Values of real part of Af eigenvalues. Columns: (1) Single application of the operator , (2) Quadruple operator application, (3) Eightfold operator application, (4) Twelvefold operator application. As predicted, the eigenvalues are either equal to 1 or inside the circle. 5.4 Colorization The colorization experiments follows the same methodology as in the previous sections. Specifically, we train linear operator Af , and during inference we first apply Af to each feature chip and then normalize these chips to ensure the reconstructor functions correctly. Figure 8 demonstrates the results of such colorization. We observe that the colorization method performs accurately on objects with inherently unambiguous color distributions. For instance, entities such as human skin, trees, grass, sky regions, and animals with naturally uniform coloration are reliably colorized. These experiments support our hypothesis that semantically meaningful transformations can be represented linearly in the learned feature space, although the performance boundaries are determined by the consistency of color distributions in the real world. We also provide additional examples in Appendix (Figure 14)."
        },
        {
            "title": "6 Limitations",
            "content": "After training the reconstructor Rθ, we observe that Rθ serves as an approximate inverse of the feature extractor E. However, an exact inverse cannot be obtained due to the following factors: 1. Non-invertibility of In general, is not required to be bijective. As evidenced by the experiments reported in Section 5, different input resolutions yield reconstructions of varying quality, even though 3 = (H/16) (W/16) (cid:0)3 162(cid:1) 11 Figure 8: Examples of solving the colorization problem by applying linear transformation in the feature space. remains constant. Higher input resolution makes more nearly invertible, but does not guarantee exact recovery. 2. Finite training coverage. We train Rθ on limited dataset, so the learned inverse may generalize poorly to regions of feature space that are underrepresented. If manipulated features fall into such poorly covered regions, the reconstructor can produce inconsistent or implausible outputs. 6.1 Addressing limitations for the interpretablity pipeline The first limitation is inherent: for fixed E, there is an upper bound on the fidelity any reconstructor can achieve. The second namely, how well the reconstructor generalizes to regions of feature space not covered by the training set remains an open question and merits further investigation in future work. In the context of the task solved by the proposed paper, we treat the reconstructor as the reflector of the quality of the constructed vision latents, and the amount of information kept in them. Thus, we address the limitations written above in the following manner. Non-invertibility of In our experiments (see Figure 4c), we observed that the residual inversion error of the encoder manifests as small, localized distortions in the reconstructed images. Consequently, before applying this method to particular task, one should verify that the magnitude of these distortions is acceptable and in practice these minor artifacts do not impede our ability to validate the analyzed feature image transformation hypothesis, since they affect all compared reconstructions equally. Finite training coverage To address the second limitation about the incomplete coverage of featurespace regions by the reconstructor, we require that the reconstructor and the transformation pair be conditionally independent given the encoder: Rθ (Ai, Af ) (cid:12) This means that Rθ is trained without any knowledge of the specific imageor feature-space operators Ai and Af beyond what it already learns from E. (cid:12) E. The operators Ai and Af are defined independently of the reconstructor and rely only on information encoded in E. Ensuring this conditional independence guarantees that our validation of the Af Ai correspondence is not biased by artifacts introduced during reconstructor training. We consider the channel swapping experiment for the above limitations in A.2. We note that our current experiments are restricted to ViT-based architectures. In future work, we plan to extend our reconstruction-based interpretability framework to other model classes, in particular convolutional neural networks Liu et al. (2022)."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we propose novel, reconstruction-based interpretability framework. By learning an approximate inverse mapping from encoder features back to pixels, our method enables controlled manipulations in feature space, and directly reveals their corresponding effects in image space. During experimental evaluation, we applied our reconstruction-based interpretability framework to diverse set of ViT-based vision encoders that differ in training objectives, pretraining datasets, and hidden dimensions. We paid particular attention to the SigLIP/SigLIP2 family, which share identical architectures, parameter counts, and datasets but differ only in their optimization objectives. Our study demonstrates three fundamental findings: 1. Resolution and informativeness of the feature space: For fixed architecture, the amount of visual information retained in the latent space increases with the spatial resolution of the feature tensor. 2. Role of training objectives: Features learned using image-based objectives exhibit richer visual semantics and better preserve structural details. 3. Mathematics of transformations: Edits in image-channel space correspond to orthogonal transformations of token embeddings in the latent space. For future work, we plan to extend our comparison to models with different architectures such as modern convolutional neural networks (e.g., ConvNext Liu et al. (2022)) and to conduct deeper, layer-wise analysis of the information encoded at each transformer depth."
        },
        {
            "title": "References",
            "content": "Caron, M., Fathi, A., Schmid, C., and Iscen, A. (2024). Web-scale visual entity recognition: An llm-driven data approach. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., and Joulin, A. (2021a). Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660. Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. (2021b). Emerging properties in self-supervised vision transformers. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 96309640. 13 Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., Gu, L., Wang, X., Li, Q., Ren, Y., Chen, Z., Luo, J., Wang, J., Jiang, T., Wang, B., He, C., Shi, B., Zhang, X., Lv, H., Wang, Y., Shao, W., Chu, P., Tu, Z., He, T., Wu, Z., Deng, H., Ge, J., Chen, K., Zhang, K., Wang, L., Dou, M., Lu, L., Zhu, X., Lu, T., Lin, D., Qiao, Y., Dai, J., and Wang, W. (2025). Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. Darcet, T., Oquab, M., Mairal, J., and Bojanowski, P. (2024). Vision transformers need registers. In The Twelfth International Conference on Learning Representations. Dorszewski, T., Tˇetková, L., Jenssen, R., Hansen, L. K., and Wickstrøm, K. K. (2025). From colors to classes: Emergence of concepts in vision transformers. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations. Dosovitskiy, A. and Brox, T. (2016). Inverting visual representations with convolutional networks. pages 48294837. Fang, A., Jose, A. M., Jain, A., Schmidt, L., Toshev, A., and Shankar, V. (2023). Data filtering networks. arXiv preprint arXiv:2309.17425. Fang, Y., Sun, Q., Wang, X., Huang, T., Wang, X., and Cao, Y. (2024). Eva-02: visual representation for neon genesis. Image and Vision Computing, 149:105171. Gao, Z., Chen, Z., Cui, E., Ren, Y., Wang, W., Zhu, J., Tian, H., Ye, S., He, J., Zhu, X., Lu, L., Lu, T., Qiao, Y., Dai, J., and Wang, W. (2024). Mini-internvl: flexible-transfer pocket multimodal model with 5 Hessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi, Y. (2021). Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718. Kazemi, H., Chegini, A., Geiping, J., Feizi, S., and Goldstein, T. (2024). What do we learn from inverting clip models? Kingma, D. P. and Ba, J. (2014). Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollár, P., and Girshick, R. (2023). Segment anything. arXiv:2304.02643. Kornblith, S., Norouzi, M., Lee, H., and Hinton, G. (2019). Similarity of neural network representations revisited. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 35193529. PMLR. Lang, O., Gandelsman, Y., Yarom, M., Wald, Y., Elidan, G., Hassidim, A., Freeman, W. T., Isola, P., Globerson, A., Irani, M., and Mosseri, I. (2021). Explaining in style: Training gan to explain classifier in stylespace. arXiv preprint arXiv:2104.13369. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. (2014). Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer. Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. (2022). convnet for the 2020s. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1196611976. Mahendran, A. and Vedaldi, A. (2016). Visualizing deep convolutional neural networks using natural pre-images. 120(3):233255. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.-Y., Li, S.-W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. (2024). DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research. Featured Certification. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021a). Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021b). Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR. 14 Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., and Dosovitskiy, A. (2021). Do vision transformers see like convolutional neural networks? In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors, Advances in Neural Information Processing Systems, volume 34, pages 1211612128. Curran Associates, Inc. Tschannen, M., Gritsenko, A., Wang, X., Naeem, M. F., Alabdulmohsin, I., Parthasarathy, N., Evans, T., Beyer, L., Xia, Y., Mustafa, B., et al. (2025). Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786. Wu, Z., Lischinski, D., and Shechtman, E. (2021). Stylespace analysis: Disentangled controls for stylegan image generation. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1285812867. Xu, H., Xie, S., Tan, X. E., Huang, P.-Y., Howes, R., Sharma, V., Li, S.-W., Ghosh, G., Zettlemoyer, L., and Feichtenhofer, C. (2024). Demystifying clip data. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. (2023). Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986. Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., and Kong, T. (2022). Image BERT pre-training with online tokenizer. In International Conference on Learning Representations."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Reconstruction of the various encoders As an ablation study, we train the reconstructor for several of the most well-known vision encoders that are widely used in computer vision applications. Specifically, we select multiple families of ViTbased encoders that vary in parameter count, architectural details, training objectives, and pretraining datasets. In Table 2, we present the architectural analysis of the evaluated vision encoders. Tables 4 and 5 show training peculiarities of the analyzed encoders. Figure 9 presents the CLIP and SigLIP scores for original images and their reconstructionsobtained via the trained reconstructor Rθ on the COCO validation set across various encoders. Figure 10 provides qualitative examples of original images alongside their reconstructions generated from the latent representations of different vision encoders. These results align with our main findings from the SigLIPSigLIP2 comparison using the reconstructor method. We also observe that higher input resolution produces richer latent representations and better reconstructions, as seen in the InternViT-448, SigLIP2-512, and SAM-1024 models. Finally, vision-based pretraining objectives yield superior feature encodings, resulting in higher similarity scores. A.2 Color Swap limitation analysis To check the correctness of the result with swapping channels, make sure that: 1. The characteristic size of the distortion is smaller than the characteristic size of the object affected by the feature transformation. 2. Rθ (Ai, Af ) (cid:12) (cid:12) E, where takes an image as input and returns tensor of features, Rθ reconstructs the image from the feature tensor, Ai swaps picture channels in pixel space, Af performs orthogonal transformation in the space of features. In such terms for our experiment we have: 1. Swapping channels is an operation that works on the whole image at once, and the size of the image is much larger than the size of the characteristic distortion for any model in our experiments. 2. Ai operator changes image channels in places purely algorithmically, Af during training relies only on representations obtained with and images changed with Ai, and Rθ relies only on representations obtained with E. The training datasets of Af and Rθ do not overlap. 15 Table 2: Comparison of Image Encoders: input resolution, sequence length, parameter count, embedding dimension Model Resolution Sequence dength #Params Vision tower Out dimension CLIP timm/EVA-02 Fang et al. (2024) OpenAI CLIP Radford et al. (2021a) LAION CLIP 2 Facebook MetaCLIP Xu et al. (2024) Apple DFN2B-CLIP Fang et al. (2023) SigLIP SigLIP (Zhai et al., 2023) SigLIP (Zhai et al., 2023) SigLIP (Zhai et al., 2023) SigLIP2 SigLIP2 (Tschannen et al., 2025) SigLIP2 (Tschannen et al., 2025) SigLIP2 (Tschannen et al., 2025) SigLIP2 (Tschannen et al., 2025) SAM Facebook SAM Kirillov et al. (2023) DinoV2 DinoV2 (Oquab et al., 2024) InternViT InternViT-V1.5-300M Gao et al. (2024) InternViT-V2.5-300M Chen et al. (2025) 224224 px 196 (1414) 224224 px 196 (1414) 224224 px 224224 px 196 (1414) 196 (1414) 224224 px 196 (1414) 224224 px 256256 px 384384 px 196 (1414) 256 (1616) 576 (2424) 224224 px 196 (1414) 256256 px 256 (1616) 384384 px 576 (2424) 512512 px 1024 (3232)"
        },
        {
            "title": "86 M",
            "content": "93 93 93 93 93 93 93 768 768 768 768 768 768 768 768 768 768 10241024 px 4096 (6464) 90 518518 px 1369 (3737) 87M 768 448448 px 1024 (3232) 304 448448 px 1024 (3232) 304 1024 Table 3: Reconstructor Training Hyperparameters Hyperparameter Value Adam (Kingma and Ba, 2014) 3e-4 Optimizer Learning Rate Learning Rate Scheduler Cyclic Adam Beta1 Adam Beta2 Batch Size (per device) Training Epochs 0.9 0.999 10 16 Table 4: Comparison of Image Encoders: training objective and architecture Model CLIP timm/EVA-02 OpenAI CLIP LAION CLIP Facebook MetaCLIP Apple DFN2B-CLIP SigLIP SigLIP SigLIP2 SigLIP2 SAM Facebook SAM DinoV2 DinoV2 Training Objective Contrastive imagetext alignment. Weights inited from EVA model trained on Masked image modeling: reconstructing CLIP features from masked patches (negative cosine loss) Contrastive imagetext alignment (InfoNCE) Contrastive on LAION-2B (2 imagetext pairs) Contrastive on CommonCrawl 2.5 data Contrastive on DFN-2B filtered data"
        },
        {
            "title": "Pairwise sigmoid contrastive loss",
            "content": "Multitask: sigmoid contrastive, captioning, self-distillation, masked modeling Promptable segmentation: predict masks from sparse or dense prompts Discriminative self-supervised pretraining: self-distillation from Dino (Caron et al., 2021a) and masked image modeling from iBOT (Zhou et al., 2022) InternViT InternViT-V1.5-300M Contrastive pre-training, LLM alignment, distillation InternViT-V2.5-300M Contrastive pre-training, LLM alignment with progressive scaling, distillation A.3 Qualitative analysis for SigLIP and SigLIP In Figure 11 we present the qualitative analysis of the reconstructions obtained for SigLIP and SigLIP2 models across four different resolutions. In Figure 12 we present the qualitative analysis for Color Swap experiments described at Section 5.2. A.4 Training Details Reconstructors were trained with Adam and cyclic learning-rate schedule. Hyperparameters are listed in Table 3, and data details in Sec. 3.2. Training on 3A100 (80 GB) GPUs took 624 h, depending on resolution. 17 Table 5: Comparison of Image Models by Pretraining Objectives Model Contrastive Captioning Masked Modeling Segmentation LLM Alignment SelfDistillation CLIP timm/EVA-02 OpenAI CLIP LAION CLIP Facebook MetaCLIP Apple DFN2B-CLIP SigLIP SigLIP SigLIP2 SigLIP SAM Facebook SAM DinoV2 DinoV2 InternViT InternViT-V1.5300M InternViT-V2.5300M Figure 9: Encoder performance comparison on the COCO val set, showing average CLIP similarity and SigLIP2 similarity between original images and their reconstructions for each vision encoder. Higher bars indicate better alignment of reconstructed images with the originals under each metric. Figure 10: Qualitative comparison of original images and their reconstructions obtained from different vision encoders. 19 Figure 11: Qualitative analysis of the SigLIP and SigLIP2 reconstruction samples. 20 Figure 12: Color-swap via simple transformations in SigLIP2 feature space. Each row presents: (1) the original image, (2) its reconstruction from encoder features, (3) the image after swapping red and blue channels in pixel space, (4) the reconstruction of the pixel-swapped image, (5) the reconstruction obtained by applying the corresponding orthogonal self-conjugated channel-swap directly in feature space, (6) the reconstruction obtained by applying the corresponding orthogonal channel-swap directly in feature space, (7) the reconstruction obtained by applying the corresponding linear channel-swap directly in feature space. The near-identical results in columns 4 and 5, 6, 7 confirm that simple transformations in latent space induce coherent, interpretable color edits in the reconstructed images. 21 Figure 13: B-channel suppression via linear transformations in SigLIP2 feature space. Each row presents: (1) the original image, (2) its reconstruction from encoder features (3) reconstruction obtained by quadrupling the corresponding linear blue channel suppression operator directly in the feature space, (4) reconstruction obtained by applying the corresponding linear blue channel suppression operator eight times directly in the fisheye space, (5) reconstruction obtained by twelvefold the corresponding linear blue channel suppression operator directly in the feature space, (6) the image after blue channel nulling in pixel space. The near-identical results in columns 5 and 6 confirm that simple transformations in latent space induce coherent, interpretable color edits in the reconstructed images. 22 Figure 14: Examples of solving the colorization problem by applying linear transformation in the feature space."
        }
    ],
    "affiliations": [
        "AIRI Moscow, Russia",
        "MIPT Dolgoprudny, Russia"
    ]
}