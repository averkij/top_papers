{
    "paper_title": "Multimodal Evaluation of Russian-language Architectures",
    "authors": [
        "Artem Chervyakov",
        "Ulyana Isaeva",
        "Anton Emelyanov",
        "Artem Safin",
        "Maria Tikhonova",
        "Alexander Kharitonov",
        "Yulia Lyakh",
        "Petr Surovtsev",
        "Denis Shevelev",
        "Vildan Saburov",
        "Vasily Konovalov",
        "Elisei Rykov",
        "Ivan Sviridov",
        "Amina Miftakhova",
        "Ilseyar Alimova",
        "Alexander Panchenko",
        "Alexander Kapitanov",
        "Alena Fenogenova"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family."
        },
        {
            "title": "Start",
            "content": "Multimodal Evaluation of Russian-language Architectures Artem Chervyakov*, Ulyana Isaeva*, Anton Emelyanov, Artem Safin, Maria Tikhonova, Alexander Kharitonov, Yulia Lyakh, Petr Surovtsev, Denis Shevelev, Vildan Saburov, Vasily Konovalov, Elisei Rykov, Ivan Sviridov, Amina Miftakhova, Ilseyar Alimova, Alexander Panchenko, Alexander Kapitanov, Alena Fenogenova* MERA Team Correspondence: mera@a-ai.ru 5 2 0 2 0 2 ] . [ 2 2 5 5 5 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce MERA Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (imageto-text, video-to-text, and audio-to-text). Our contributions include: (i) universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family."
        },
        {
            "title": "1\nRecent breakthroughs in generative AI, including\nmodels like GPT-51, ImageBind (Girdhar et al.,\n2023), and LLaVa (Liu et al., 2024a), have signif-\nicantly advanced the state of the art across mul-\ntiple modalities. This accelerated progress cre-\nated a growing need for comprehensive multimodal\nbenchmarks capable of rigorous evaluation of the\nfull spectrum of versatile capabilities of such mod-",
            "content": "* Core contributors 1https://openai.com/index/introducing-gpt-5 1 els. Although several benchmarks have been proposed for English and general-domain evaluation, such as MultiBench (Liang et al., 2021), MMBench (Liu et al., 2024d), and General-Bench (Fei et al., 2025), they predominantly neglect the linguistic and cultural nuances of Slavic languages, particularly Russian. Beyond its Cyrillic script, Russian possesses rich cultural context where concepts familiar to native speakers (e.g., folklore, Soviet media) are cognitively foreign to others, creating challenge for automatic understanding. Existing Russian-specific benchmarks, including TAPE (Taktasheva et al., 2022), Russian SuperGLUE (Fenogenova et al., 2022), and MERA (Fenogenova et al., 2024), focus exclusively on text-based tasks, leaving critical gap in multimodal evaluation. To address this, we introduce MERA Multi, the first multimodal benchmark for MLLM evaluation in Russian. It comprises 18 tasks spanning (default) text2, image, audio, and video modalities, built upon unified taxonomy of multimodal abilities. Beyond Russian MERA Multi offers blueprint for developing multimodal benchmarks across other Slavic and morphologically rich languages. These languages share structural complexity, typological proximity, and cultural specificity that make direct translation or adaptation of English-centric benchmarks inadequate. Thus, it serves not only as the first multimodal benchmark for Russian but also as scalable methodology for culturally and linguistically aware evaluation in underrepresented settings. More specifically, our contributions are fourfold: We propose unified taxonomy and evaluation methodology designed for MLLMs assessment; We create 18 novel datasets incorporating Rus2Since textual input is an inherent component of all tasks in our benchmark, we treat it as the default modality. Consequently, we do not explicitly list it when describing or tabulating tasks, unless required for clarity. Figure 1: Overview of MERA Multi. The benchmark unites multimodal evaluation, taxonomy-based skill assessment, and data leakage protection across 18 tasks covering (default) text, image, audio, and video modalities. It employs standardized block-prompting, compound scoring, and integrates methods for multimodal content protection, forming transparent and robust methodology for culturally grounded multimodal evaluation in Russian. sian cultural and linguistic specificities, unified prompts, and metrics. We provide baseline performance results for both openand closed-source models; We establish data leakage analysis and watermarking strategy to protect private evaluation datasets. Additionally, we provide standardized codebase for full reproducibility and submission platform with automated scoring and public leaderboard.3 All datasets and code are made available under the MERA Multi license, which allows the use of the benchmark sets for non-commercial purposes, provided that the data is not used for model training of any kind. These elements establish foundation for transparent and culturally aware multimodal evaluation in Russian, while positioning MERA Multi as reference point for developing similar frameworks across Slavic and other nonEnglish languages, thereby promoting broader community development and cross-linguistic benchmarking."
        },
        {
            "title": "2 Related Work",
            "content": "Text-Based Benchmarks Evaluation of language models has historically relied on Natural Language Understanding benchmarks such as GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a), which set initial standards for English. As these benchmarks became saturated, newer instruction-oriented and reasoning-focused benchmarks emerged such as BIG-bench (Suzgun et al., 2023) and HELM (PerFurther efforts such as litz et al., 2024). 3https://mera.a-ai.ru/ru/multi MMLU (Hendrycks et al., 2021), AGIEval (Zhong et al., 2024), and C-Eval (Huang et al., 2023) extended evaluation to academic and professional domains, though primarily in English and Chinese. For Russian, several text-based benchmarks were introduced. Among them are Russian SuperGLUE (Shavrina et al., 2020; Fenogenova et al., 2022), TAPE (Taktasheva et al., 2022), and RuCoLA (Mikhailov et al., 2022). The instructionbased benchmark MERA (Fenogenova et al., 2024) advanced this line of work but remained limited to text-only evaluation. Our work extends this direction toward multimodal, instruction-following assessment of MLLMs in Russian. Multimodal Benchmarks The rapid progress of multimodal models has led to numerous benchmarks extending evaluation beyond text. Early large-scale efforts like MultiBench (Liang et al., 2021) covered 10 modalities and emphasized general-purpose representation learning. MMBench (Liu et al., 2024d) focused on fine-grained visual reasoning and bilingual (EnglishChinese) evaluation, while SUPERB (Yang et al., 2021) unified diverse audio tasks under single framework. For video understanding, STAR (Wu et al., 2021), InfiniBench (Ataallah et al., 2024), and VideoMME (Fu et al., 2025) advanced evaluation toward temporal and long-context reasoning. More comprehensive setups such as General-Bench (Fei et al., 2025), OmniDialog (Razzhigaev et al., 2024), MMMU (Yue et al., 2024), and SEED-Bench (Li et al., 2023) assess multimodal and reasoning abilities at scale. All in all, despite these advances, most existing multimodal benchmarks are Englishcentric. To our knowledge, there is currently no 2 Dataset / task Size HB Answer License details are in B. u a i ruEnvAQA RuSLUn *AQUARIA *ruTiE-Audio 596 741 738 1500 0.0 1.0 0.0 0 MC OE MC MC 1148 ruCLEVR ruCommonVQA ruNaturalScienceVQA 363 814 WEIRD 339 *LabTabVQA 773 *RealVQA 595 *ruHHH-Image 502 *ruMathVQA 1500 *ruTiE-Image 4227 *SchoolScienceVQA 7432 *UniScienceVQA CommonVideoQA v *RealVideoQA *ruHHH-Video 1200 671 911 OE 73.8 OE 1.0 70.0 MC 82.22 MC MC 93.9 OE 1.0 MC 0 OE 81.0 MC 0 MC 52.0 OE 1. 95.75 MC 95.83 MC MC 0 / 0 CC BY-NC 4.0 CC-BY-4.0 MERA Multi MERA Multi CC-BY-4.0 CC-BY-4.0 CC-BY-SA-4.0 CC-BY-4.0 MERA Multi MERA Multi MERA Multi MERA Multi MERA Multi MERA Multi MERA Multi CC-BY-4.0 MERA Multi MERA Multi Table 1: Overview of datasets in MERA Multi. Those marked with an asterisk are private datasets collected from scratch, while the others are public datasets compiled from open-source datasets. Size column shows the number of samples in the dataset, and Answer column is the task format (MC and OE stand for multiple-choice and open-ended, respectively). HB is the human baseline value (basic / expert or basic only, see G.2 for details). MERA Multi license refers to the benchmark license anonimized for the review period. existing multimodal benchmark for the Russian language. MERA Multi addresses the gap by providing unified and culturally adapted evaluation of understanding across several modalities (see Appendix for the comparison between MERA Multi and other benchmarks)."
        },
        {
            "title": "3.1 Benchmark General Structure",
            "content": "The proposed benchmark is designed to evaluate the capabilities of Russian MLLMs. Besides the omnipresent textual modality, it incorporates tasks from three other modalities: image (11 datasets), audio (4 datasets), and video (3 datasets). The tasks are formulated in two primary formats: multiplechoice questions and open-ended questions requiring short free-form answers. To balance reproducibility and novelty, the benchmark integrates both publicly available datasets (7 tasks), curated from open data sources, and private datasets (11 tasks), collected from scratch specifically for this study. The latter are designed to incorporate Russian cultural nuances, target underexplored skill categories, and mitigate potential data contamination issues (see details in section 3.4). complete list of datasets is provided in Table 1. Task examples and dataset creation 3 3.2 Skill Taxonomy Contemporary studies on multimodal benchmarks often rely on custom skill sets during the dataset design process (Liu et al., 2024d; Fu et al., 2024). We performed comprehensive analysis of such systems and synthesized them into consolidated MLLM skill taxonomy, which underpins the foundation of MERA Multi. Such system functions as comprehensive map for skill coverage, which, when coupled with an alignment of existing datasets to corresponding skills, can spotlight deficiencies in benchmark task diversity. The taxonomy is aligned with three broad categories of human-like cognition: perception, knowledge, and reasoning, division also adopted by recent multimodal benchmarks. The knowledge taxonomy is shown in Table 2, and the perception and reasoning parts are provided in the Appendix in Table 7 and Table 8 respectively, due to space constraints. To comply with the emerging MLLM capabilities, this taxonomy is designed to be extendable and this paper provides the initial version of the unified taxonomy and encourages its adoption and extension in future research. Each MERA Multi dataset is systematically mapped to predefined set of multimodal skills. Rather than assigning single skill per task, each task is designed to be multifaceted, evaluating distinctive combination of abilities. For example, single task might require visual perception (to identify objects), OCR (to read text in the image), and reasoning (to answer why-question) all together. Further details on skill taxonomy are in Appendix C."
        },
        {
            "title": "3.3 Evaluation Methodology",
            "content": "MERA Multi evaluation methodology is designed to systematically assess the multimodal reasoning, perception, and knowledge abilities of MLLMs. It measures both general-purpose and modalityspecific competence across image, audio, and video inputs. To achieve this, we propose the benchmark that integrates four complementary components: (i) block-prompting structure that ensures consistency and diversity of task formulations across modalities (Section 3.3.2); (ii) dual-level evaluation metrics combining symbolic and semantic correctness through dedicated LLM-as-a-judge model (Section 3.3.3); (iii) an evaluation pipeline including scoring aggregation and cross-modality L1 L2 L3 L4 Taxonomy Level e n Knowledge Knowledge Common everyday knowledge Common everyday knowledge Domain knowledge Ethics Common domain knowledge Expert domain knowledge Modality Audio Video ruTiE-Audio ruEnvAQA AQUARIA CommonVideoQA RealVideoQA ruHHH-Video ruTiE-Audio ruEnvAQA AQUARIA ruHHH-Video CommonVideoQA RealVideoQA Image ruCommonVQA RealVQA ruHHH-Image WEIRD ruTiE-Image ruHHH-Image ruMathVQA RealVQA ruTiE-Image ruMathVQA ruNaturalScienceVQA UniScienceVQA SchoolScienceVQA Table 2: Knowledge taxonomy structure and multimodal task distribution in MERA Multi. Columns L1-L5 show the skill hierarchy levels, while Image, Audio and Video columns indicate which tasks cover each knowledge type across different modalities. weighting (Section 3.3.4); and (iv) submission protocol enabling automated, reproducible leaderboard updates (Section 3.3.5). These components provide coherent methodology that balances rigor, interpretability, and cross-model comparability in multimodal evaluation."
        },
        {
            "title": "3.3.1 Evaluation Framework Implementation\nWe build the benchmark code base on the lm-eval4\nframework (Gao et al., 2024; Biderman et al.,\n2024), extending it to support multimodal inputs\nwhile preserving its core structure for texts. The\ncodebase introduces comprehensive vision, audio,\nand video evaluation through, and chat-template\nformatting tailored for instruction-tuned and API-\nbased models. Multimodal data is integrated either\nby passing it separately to the model’s processor\nor by embedding it directly within a chat template\nalongside the text. All evaluations are strictly gen-\nerative: models produce free-form answers until a\nstop condition is met. All outputs are used directly\nexcept RuSLUn which requires minimal parsing\nfor structured output.",
            "content": "To balance rigor and semantic understanding, we employ two primary metrics across most datasets. ExactM atch (EM ) serves as generative analog of accuracy, using normalized string comparison to assess both factual correctness and strict adherence to the specified output format. Complementing this, the JudgeScore (JS) measures semantic similarity via an LLM-as-a-judge (trained for this purpose) that scores an answer as 1 for substantive agreement with the reference and 0 otherwise. This dual approach provides nuanced view: EM is sensitive to format, while the JS focuses on meaning. For the datasets where this format is unsuitable we employ task-specific metrics or report only the JS. 4https://github.com/EleutherAI/ lm-evaluation-harness The inalScore of the task (for Total Score calculation purposes) is defined as follows: S(task) = EM(task) + JS(task) 2 ; (1) It should be noted that as long as some of our prompts suggest that the model return the answer for the question after specific phrase (ANSWER), we consider EM to be the maximum of two scores: (i) EM of the full model answer, (ii) EM of the last part of the models answer split by the previously mentioned specific phrase. If the model breaches the instruction, we assess the full answer. Otherwise, we consider the part that is after the specific phrase to be the models answer as required by the instruction."
        },
        {
            "title": "3.3.2 Prompt Structure\nFollowing the approach of Voronov et al. (2024),\nwe avoid hard-coding task-specific prompts by em-\nploying a block-prompting scheme that constructs\nuniversal prompts aligned with each task’s struc-\nture. For every dataset, we instantiate a predefined\nset of 10 prompts drawn from fixed block layouts\nthat we initially designed and realized under differ-\nent surface modalities (e.g., formal request, com-\nmand, technical statement). This design preserves a\nuniform formulation across benchmark tasks while\nallowing for controlled variation, such as the pres-\nence or absence of a reasoning request, to mitigate\nbenchmark saturation.",
            "content": "Following the methodology of Fenogenova et al. (2024), we uniformly assign prompts for each task across dataset samples to ensure that the aggregated performance metric reflects an average over prompt variants rather than the idiosyncrasies of single prompt. post-hoc statistical analysis (see Appendix for the details) confirmed that our prompt set is not invariant, as some prompts produce statistically significant shifts in model scores. 4 This finding directly motivates and validates our multi-prompt design as necessary guard against evaluation bias. attempted: 3.3.3 Judge Scoring Evaluating open-ended model generations requires moving beyond strict heuristics. We frame this as task of assessing semantic equivalence, conditioned on the question, as devising universal regex patterns to extract answers from free-form text, particularly from multi-step reasoning chains proves infeasible. The practically infinite space of possible valid generations necessitates more flexible and scalable assessment approach. We therefore introduce learned LLM-as-judge model, framing evaluation as question-conditioned semantic equivalence task (binary classification determine the correctness of models prediction). This judge is trained on diverse, humanannotated dataset of model outputs collected from various Russian-language instruction-based benchmarks. The final model, based on the RuModernBERT5 (Spirin et al., 2025) encoder, achieves F1 score of 0.96 on held-out test set and shows 99.6% agreement with EM on identical answers, confirming its reliability. We deploy this judge as our primary metric, providing robust, scalable score for correctness across the benchmark. Comprehensive details on data, training, and model comparisons are in the Appendix E."
        },
        {
            "title": "3.3.4 Evaluation Pipeline",
            "content": "Let MERA Multi comprise modalities (e.g., image, audio, video) not counting the omnipresent text one, with modality containing nm tasks. We treat modalities as equally important and split each modalitys weight evenly across its tasks. Let km be the number of tasks model attempted in modality with sm,i [0; 1] being the score (average of all task metrics). The Attempted Score. Quality over the tasks the model attempted, with the original equalper-modality weighting renormalized over the attempted subset: = (cid:80)M m=1 (cid:80) 1 nm (cid:80)M m=1 iAm km nm sm,i (2) = 1 (cid:88) m=1 km nm (3) The Total Score. We combine quality and breadth as = (4) This approach has the following positive effects: Separation of concerns. reports how well model performs where it runs; reports how much of MERA Multi it covers. Fair, single leaderboard. enables joint ranking without imputing arbitrary zeros for missing modalities; specialists still excel on modality-specific boards (higher A, lower C). Stable under growth. Adding/removing tasks adjusts (breadth), but leaves (quality on attempted tasks) invariant; equal-permodality weighting prevents any modality from dominating due to task count."
        },
        {
            "title": "3.3.5 Submission\nThe private dataset test answers are available only\nfor the organizers and experts supporting the bench-\nmark. The scoring system is automatic and is avail-\nable on the benchmark platform.",
            "content": "The process of submission is the following: First, users clone MERA Multi benchmark repository and form submission files using shell script and the provided code. Second, they upload the submission files via the platform interface6 in their personal account for automatic assessment. The evaluation result is then displayed in the users account and kept private unless they request publication using the Publish function. In this case, it undergoes an expert verification of its reproducibility, which includes checking log files automatically formed by the evaluation script and the provided submission information. Once approved, the models score is shown publicly on the leaderboard (user can choose on which leaderbord image/audio/video/multi to add the results), while its specific outputs remain private."
        },
        {
            "title": "3.4 Data Protection",
            "content": "The Coverage. Breadth of evaluation is the fraction of the benchmark the model actually As pre-training datasets grow, benchmark leakage is becoming more common. This issue is ex5https://hf.co/deepvk/RuModernBERT-base 6For this step the registration on the platform is required. 5 Model Confidence Interval Qwen2-VL-2B-Instruct Qwen2-VL-7B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct llava-1.5-7b-hf llava-1.5-13b-hf llama3-llava-next-8b-hf gemma-3-12b-it gemma-3-12b-it LLaVA-NeXT-Video LLaVA-NeXT-Video-DPO LLaVA-NeXT-Video-34B Qwen2-VL-2B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-3B-Instruct Qwen2-VL-7B-Instruct m d (0, 0.352) (0, 3.920) (0.259, 2.180) (0.444, 1.389) (0, 0.937) (0, 0.839) (0, 1.024) (1.183, 3.120) (0, 1.756) (0.811, 3.314) (1.367, 2.125) (0, 1.373) (0, 0.645) (0, 0.781) (0, 0.330) (0, 0.105) ultravox-v0_2 a ultravox-v0_3-llama-3_2-1b ultravox-v0_5-llama-3_2-1b ultravox-v0_6-llama-3_1-8b ultravox-v0_4 ultravox-v0_4_1-mistral-nemo Qwen2-Audio-7B-Instruct MiniCPM-o-2_6 (0, 0.940) (0, 1.566) (0, 0.195) (0, 0.875) (0, 0.401) (0, 0.079) (0.0, 0.0) (0.0, 0.0) Table 3: Confidence intervals of Judge Score (JS) differences between watermarked and clear data (%). acerbated by opaque training processes and the undisclosed use of supervised data in modern MLLMs, which undermines the validity of benchmarks and fosters unfair comparisons, ultimately slowing progress. To protect the multimodal data in our private benchmark suite from unauthorized use, we employ three-part strategy: (i) data watermarking (Section 3.4.1), (ii) leakage detection (Section 3.4.2), and (iii) licensing (Section 3.4.3). Together, these measures provide robust technical and legal safeguards."
        },
        {
            "title": "3.4.1 Data Watermarking",
            "content": "We embed imperceptible yet identifiable watermarks into our benchmark data to trace its provenance and detect unauthorized use in training corpora. Our approach is tailored to each modality: Audio: We use AudioSeal (Roman et al., 2024) for localized detection, which employs neural, inaudible watermark. Images/Video: simple overlay of the MERA Multi watermark on images and every video frame (same code across frames). Table 3 demonstrates the Confidential Intervals (CI) for differences of the models metrics on data with watermarks and without them. The CI are computed for differences in per cents for convenience. The results demonstrate that with 95% probability the difference is less than 5% (usually less than 1%) which leads to the conclusion: our watermarking strategy does not significantly affect 6 Modality AUC-ROC Image Video Audio 88.658 88.388 81.250 Table 4: Average AUC-ROC of MSMIA per modality. Averaging over the models used for training and evaluating MSMIA. the evaluation results. 3.4.2 Data Leakage Detection We detect training-set data leakage using membership inference attacks (Shokri et al., 2017). Our approach extends the Semantic Membership Inference Attack (SMIA) (Mozaffari and Marathe, 2024) to MLLMs, calculating the loss for data point by considering its text in conjunction with its paired image, video, or audio data. The Multimodal SMIA (MSMIA) method identifies leakage by comparing models behavior on original examples versus their semantically perturbed \"neighbors\" (masked, removed, doubled, switched text tokens); models that have been trained on the data (models with data leak) are to exhibit systematically different loss patterns. Concretely, we train the MSMIA detector by comparing two versions of model: the original and version we fine-tune on candidate data (simulating leak). The detector learns to distinguish between them by analyzing the differences in loss and text embeddings when processing original data points versus their perturbed neighbors. Once trained, this detector can be applied to any other model to output probability that specific data sample was part of that models training set. Overall results of the MSMIA detection capabilities are presented in Table 4. Following the original methodology, we evaluate detection performance using AUC-ROC. The table demonstrates relatively high scores, which means that the MSMIA method tends to be capable of detecting whether model has been trained on some multimodal data samples or not, with high probability and rather low falsepositive rate. The details on the MSMIA training and the metrics are provided in Appendix D."
        },
        {
            "title": "3.4.3 Legal Protection via Licensing",
            "content": "the We introduce bespoke legal framework, MERA Multi License7, which explicitly permits 7https://github.com/MERA-Evaluation/MERA_ MULTIMODAL/blob/main/LICENSE the use of the benchmark data for research and noncommercial testing purposes, but strictly prohibits its incorporation into any model training process."
        },
        {
            "title": "4 Baselines",
            "content": "4.1 Model Baselines We evaluate over 50 publicly available multimodal models from the most trending model families on HuggingFace, varying in size from 1B to 110B parameters. Also we evaluate proprietary GPT 4.1 (OpenAI) to make comparison between open and closed source models. See Appendix G.1 for the baseline details. We evaluate models in the same environments and scenarios by the procedure described in Section 3.3.4 and the submission procedure described in Section 3.3.5. We also provide examples of particular submissions (the model evaluated on part of the tasks of modality)."
        },
        {
            "title": "4.2 Human Baselines",
            "content": "To estimate human-level performance across MERA Multi tasks, we compute human baseline (HB) values based on the aggregated results of crowd annotators. For datasets requiring additional domain expertise, we also establish expert HB obtained from qualified expert annotators. Annotation quality is ensured through honeypot tasks with automated correctness verification and post-hoc filtering of low-performing annotators. All crowd annotations are collected via the ABC Elementary platform8, which guarantees data anonymity, fair compensation, and ethical compliance. See Appendix G.2 for detailed methodology and cost breakdown ."
        },
        {
            "title": "5 Results",
            "content": "The leaderboard is designed in such way that the more modalities the model covers, the higher the Total Score could be. The top performer, Qwen3-Omni-30B-A3B-Instruct, leads with Total Score of 0.434, driven by its high Attempted Score (0.523) and rather high Coverage (0.828), showing strong image, audio, and video capabilities. Notably, the models from Qwen families obtain larger scores for image and video modalities (first 5 places of the overall leaderboard are taken by those models). GPT 4.1 still leads in image modality while having low Coverage (0.333), 8https://app.elementary.center which leads to lower Total Score (0.143 compared to 0.434 of the top performer). The main trend is defined by the metrics used: broader coverage leads to higher Total Score. Thus omni-models occupy the first places. But strong unior bimodal capabilities may gain advantage over middle-performing models with high Coverage (e.g. Qwen2.5-VL-72BInstruct and Qwen2-VL-72B-Instruct with 0.257 and 0.251 Total Scores respectively). This pattern is consistent across all modalities. In audio, the specialists from ultravox family tend to display poorer performance compared to omnimodels like Qwen2.5-Omni-7B (0.268 vs 0.464 for Audio Total Score) even though ultravox models use other LLMs from Mistral, Llama, Qwen families as backbones. Considering the video modality, vision models are usually trained with videoinputs or may slice the video into frames and use regular vision encoders for them, which explains why Qwen2-VL-72B-Instruct shows the best Video Total Score (0.54) while the models that specialize specifically on video modality like those from LLaVA-NeXT-Video family show poorer metrics. Consistently, Judge Score (JS) > EM across models, indicating that many responses are semantically correct but violate output format; whenever JS EM, the model followed instructions closely. This gap justifies reporting JS alongside EM. When JS < EM, this means that we can extract the answer from models generation but the entire generation may be misleading (e.g. wrong rationale, reasoning conclusion mismatches the selected answer). Tables with separate datasets metrics and analysis may be found in Appendix G.1. Takeaway 1: There is still gap between modalities. Omni-models partially bridge it. Specialist models, however, show that while image understanding is relatively mature field, audio and video understanding are underrepresented in terms of both models and datasets, which is reflected in lower scores on benchmark tasks. Takeaway 2: Overall metrics are robust to missing task scores (unfinished submission) and multiple modalities. 7 Model Qwen3-Omni-30B-A3B-Inst Qwen2.5-Omni-7B Qwen2.5-VL-72B-Inst Qwen2-VL-72B-Inst Qwen2.5-Omni-3B MiniCPM-o-2_6 Qwen2.5-VL-7B-Inst Qwen2-VL-7B-Inst GPT 4.1 Qwen2.5-VL-3B-Inst InternVL3-9B Qwen3-VL-2B-Inst Phi-4-multimodal-inst ultravox-v0_4 ultravox-v0_5-llama-3_1-8b ultravox-v0_4_1-llama-3_1-8b audio-flamingo-3-hf ultravox-v0_4_1-mistral-nemo ultravox-v0_6-llama-3_1-8b llava-next-110b-hf Phi-3.5-vision-inst Qwen2-Audio-7B-Inst SmolVLM-Inst gemma-3-27b-it granite-vision-3.3-2b deepseek-vl2-small Total Score Attempted Score Coverage Image Total Audio Total Video Total 0.434 0.302 0.257 0.251 0.244 0.211 0.192 0.145 0.143 0.132 0.128 0.121 0.097 0.089 0.089 0.088 0.086 0.086 0.082 0.078 0.075 0.074 0.063 0.049 0.047 0.044 0.523 0.302 0.386 0.376 0.244 0.23 0.288 0.217 0.43 0.198 0.192 0.182 0.145 0.268 0.266 0.264 0.259 0.257 0.247 0.235 0.226 0.223 0.189 0.146 0.142 0.162 0.828 1.0 0.667 0.667 1.0 0.917 0.667 0.667 0.333 0.667 0.667 0.667 0.667 0.333 0.333 0.333 0.333 0.333 0.333 0.333 0.333 0.333 0.333 0.333 0.333 0.273 0.423 0.211 0.353 0.332 0.172 0.177 0.238 0.195 0.43 0.138 0.169 0.124 0.183 0.0 0.0 0.0 0.0 0.0 0.0 0.235 0.226 0.0 0.189 0.146 0.142 0.132 0.531 0.464 0.0 0.0 0.377 0.234 0.0 0.0 0.0 0.0 0.0 0.0 0.042 0.268 0.266 0.264 0.259 0.257 0.247 0.0 0.0 0.223 0.0 0.0 0.0 0.0 0.357 0.42 0.506 0.54 0.332 0.341 0.474 0.298 0.0 0.418 0.276 0.396 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. Table 5: Overall baselines information over three modalities (vision/image, audio, video). All scores are aggregated. Modality total score is the attempted score multiplied by coverage of the modality."
        },
        {
            "title": "Conclusion",
            "content": "The rapid progress of generative AI has introduced new challenges for evaluating models in multimodal contexts. We present MERA Multi, the first comprehensive framework for transparent and culturally grounded multimodal evaluation in Russian. The benchmark encompasses 18 tasks across four modalities (default text, image, audio, and video), covering diverse domains and scenarios. It systematizes diverse multimodal abilities via proposed taxonomy and evaluates them through methodologically verified prompts and metrics. We also provide standardized code base9 that guarantees reproducibility and submission platform10 offering automated evaluation, scoring, and open leaderboards. In the future, we plan to expand the benchmark to encompass additional scenarios and actively encourage community contributions. We envision MERA Multi as collaborative initiative that promotes transparent evaluation practices and provides methodological foundation for developing culturally aware multimodal benchmarks across non9https://github.com/MERA-Evaluation/MERA_ MULTIMODAL 10https://mera.a-ai.ru/ru/multi English languages such as Slavic, ultimately advancing the creation of more robust and reliable multimodal models."
        },
        {
            "title": "Limitations",
            "content": "First of all, despite the fact that our benchmark covers 18 tasks spanning multiple domains, aiming to represent complementary semantic abilities of the models, this set may be underrepresenting some abilities of the model or some domains which may be crucial for certain tasks and applications. Namely, it is not impossible that model which excels at our benchmark will perform poorly on specialized domain or task. Even with fixed prompts and decoding settings, MERA Multi scores can vary because the entire hardwaresoftware stack affects inference: GPU model, drivers/CUDA/cuDNN, PyTorch, vLLM/- transformers (and commit hashes), flash-attention kernels, tokenizers/checkpoints, precision/quantization, and batching some of which are nondeterministic. We therefore request public submissions adhere to the same parameters and, in submission information, specify the GPUs and libraries versions they used for reproducibility purposes."
        },
        {
            "title": "Ethical Statement",
            "content": "While the presented benchmark is able to comprehensively evaluate semantic abilities of the model, i.e., the capacity of individual models to understand and reason about data in different modalities, we did not perform explicit evaluation of any bias of these models, e.g., toward any kind of underrepresented minorities. In our opinion, this is an extremely important direction of future work, yet being outside the scope of our current contribution. For the creation of novel datasets, the work of human annotators is used. We state that their work was adequately paid or compensated (see Section G.2 for the details). Researchers participating in the benchmark will be encouraged to adhere to ethical research practices, including proper citation, acknowledgment of data sources, and responsible reporting of results. Regular ethical reviews will assess the benchmarks impact, identify potential ethical concerns, and implement necessary adjustments to uphold the highest ethical standards throughout the usage usage. We proofread the text of this article using Overleaf Writefull assistant11, GPT-4o12, Grammarly13 to correct grammatical, spelling, and style errors and paraphrase sentences. We emphasize that these tools are used solely to enhance the quality of English writing, in full compliance with the ACL policies on the responsible use of AI writing assistance. Nevertheless, some segments of our publication can be potentially detected as AI-generated, AIedited, or human-AI-generated."
        },
        {
            "title": "Acknowledgments",
            "content": "MERA is collaborative project, thoughtfully designed to serve the needs of both industry and academia. The authors extend their sincere gratitude to our partners from the AI Alliance Russia, whose invaluable collaboration made an undertaking of this scale possible. Special thanks are due to Ekaterina Morgunova, Yegor Nizamov, and Uliana Plugina for their significant contributions in coordinating our benchmark partners and contractors for the website development. We also express our deep appreciation to the entire team dedicated to developing the website platform and maintaining the scoring system. 11https://www.writefull.com 12https://chatgpt.com 13https://app.grammarly.com We are profoundly grateful to the following individuals for their dedication and hard work: Yaroslav Grebnyak, Anna Kostikova, Aleksandr Sautin, Artem Goryainov, Aleksandra Rak, Artem Goryainov, Albina Akhmetgareeva, Igor Churin, Leonid Sinev, Yulia Lazareva, Ksenia Biryukova, Jamilya Erkenova, Valentina Khlebutina, Mariia Slabunova, Sergei Markov and to many others whom we may have inadvertently missed, but who supported us with their ideas, collaborated on dataset creation, validated results, and helped organize the workflow your contributions are sincerely appreciated. The MERA Team"
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, and 110 others. 2024. Phi-3 technical report: highly capable language model locally on your phone. Preprint, arXiv:2404.14219. Kirolos Ataallah, Chenhui Gou, Eslam Abdelrahman, Khushbu Pahwa, Jian Ding, and Mohamed Elhoseiny. 2024. InfiniBench: comprehensive benchmark for large multimodal models in very long video understanding. Preprint, arXiv:2406.19875. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others. 2025. Qwen2.5-vl technical report. Preprint, arXiv:2502.13923. Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser. 2020. SLURP: spoken language understanding resource package. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 72527262, Online. Association for Computational Linguistics. Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPofi, Julen Etxaniz, Benjamin Fattori, Jessica Zosa Forde, Charles Foster, Jeffrey Hsu, Mimansa Jaiswal, Wilson Y. Lee, Haonan Li, and 11 others. 2024. Lessons from the trenches on reproducible evaluation of language models. Preprint. Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, 9 and Roy Schwartz. 2023. Breaking common sense: Whoops! vision-and-language benchmark of synIn Proceedings thetic and compositional images. of the IEEE/CVF International Conference on Computer Vision, pages 26162627. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. 2024. Qwen2-audio technical report. Preprint, arXiv:2407.10759. Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Siliang Tang, Kaihang Pan, Yaobo Ye, and 13 others. 2025. On path to multimodal generalist: General-level and general-bench. In Forty-second International Conference on Machine Learning. Alena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova, Albina Akhmetgareeva, Anton Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana Isaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin, Polina Mikhailova, Anastasia Minaeva, Denis Dimitrov, Alexander Panchenko, and Sergey Markov. 2024. MERA: comprehensive LLM evaluation in Russian. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 99209948, Bangkok, Thailand. Association for Computational Linguistics. Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Tatiana Shavrina, Anton Emelyanov, Denis Shevelev, Alexandr Kukushkin, Valentin Malykh, and Ekaterina Artemova. 2022. Russian SuperGLUE 1.1: Revising the lessons not learned by Russian NLP models. Preprint, arXiv:2202.07791. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. 2024. Mme: comprehensive evaluation benchmark for multimodal large language models. Preprint, arXiv:2306.13394. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, and 2 others. 2025. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 24108 24118. Computer Vision Foundation / IEEE. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2024. framework for few-shot language model evaluation. Github. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. Imagebind one embedding space to bind them all. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1518015190. IEEE. Arushi Goel, Sreyan Ghosh, Jaehyeon Kim, Sonal Kumar, Zhifeng Kong, Sang gil Lee, Chao-Han Huck Yang, Ramani Duraiswami, Dinesh Manocha, Rafael Valle, and Bryan Catanzaro. 2025. Audio flamingo 3: Advancing audio intelligence with fully open large audio language models. Preprint, arXiv:2507.08128. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the in VQA matter: Elevating the role of image understanding in visual question answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 63256334. IEEE Computer Society. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. 2017. CLEVR: diagnostic dataset for compositional language and elementary visual reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 19881997. IEEE Computer Society. Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. 2024. Llava-next: Stronger llms supercharge multimodal capabilities in the wild. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023. Seed-bench: Benchmarking multimodal llms with generative comprehension. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1931419327. 10 Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, and Di Hu. 2022. Learning to answer questions in dynamic audio-visual scenarios. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1908619096. IEEE. Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle A. Lee, Yuke Zhu, Ruslan Salakhutdinov, and LouisPhilippe Morency. 2021. Multibench: Multiscale benchmarks for multimodal representation learning. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: common objects in context. In Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, volume 8693 of Lecture Notes in Computer Science, pages 740755. Springer. Samuel Lipping, Parthasaarathy Sudarsanam, Konstantinos Drossos, and Tuomas Virtanen. 2022. Clothoaqa: crowdsourced dataset for audio question answering. In 2022 30th European Signal Processing Conference (EUSIPCO), pages 11401144. IEEE. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024a. Improved baselines with visual instruction tuning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2628626296. IEEE. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024b. Improved baselines with visual instruction tuning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2628626296. IEEE. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024c. Llavanext: Improved reasoning, ocr, and world knowledge. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. 2024d. Mmbench: Is your multi-modal In Computer Vision model an all-around player? - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part VI, volume 15064 of Lecture Notes in Computer Science, pages 216233. Springer. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Andrés Marafioti, Orr Zohar, Miquel Farré, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, Vaibhav Srivastav, Joshua Lochner, Hugo Larcher, Mathieu Morlon, Lewis Tunstall, Leandro von Werra, and Thomas Wolf. 2025. Smolvlm: Redefining small and efficient multimodal models. Preprint, arXiv:2504.05299. Microsoft, :, Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun Chen, Yi ling Chen, Qi Dai, and 57 others. 2025. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. Preprint, arXiv:2503.01743. Vladislav Mikhailov, Tatiana Shamardina, Max Ryabinin, Alena Pestova, Ivan Smurov, and Ekaterina Artemova. 2022. RuCoLA: Russian corpus of linguistic acceptability. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 52075227, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Hamid Mozaffari and Virendra Marathe. 2024. Semantic membership inference attack against large language models. In Neurips Safe Generative AI Workshop 2024. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316. Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen. 2024. Efficient benchmarking (of language models). In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 25192536, Mexico City, Mexico. Association for Computational Linguistics. Anton Razzhigaev, Maxim Kurkin, Elizaveta Goncharova, Irina Abdullaeva, Anastasia Lysenko, Alexander Panchenko, Andrey Kuznetsov, and Denis Dimitrov. 2024. Omnidialog: multimodal benchmark for generalization across text, visual, and audio In Proceedings of the 2nd GenBench modalities. Workshop on Generalisation (Benchmarking) in NLP, pages 183195. Robin San Roman, Pierre Fernandez, Hady Elsahar, Alexandre Défossez, Teddy Furon, and Tuan Tran. 11 2024. Proactive detection of voice cloning with localized watermarking. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 24722497, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Elisei Rykov, Kseniia Petrushina, Kseniia Titova, Alexander Panchenko, and Vasily Konovalov. 2025a. Dont fight hallucinations, use them: Estimating image realism using NLI over atomic facts. CoRR, abs/2503.15948. Elisei Rykov, Kseniia Petrushina, Kseniia Titova, Anton Razzhigaev, Alexander Panchenko, and Vasily Konovalov. 2025b. Through the looking glass: Common sense consistency evaluation of weird images. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 4: Student Research Workshop), pages 279293, Albuquerque, USA. Association for Computational Linguistics. Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, and Dinesh Manocha. 2025. MMAU: massive multi-task audio understanding and reasoning benchmark. In The Thirteenth International Conference on Learning Representations. Tatiana Shavrina, Alena Fenogenova, Emelyanov Anton, Denis Shevelev, Ekaterina Artemova, Valentin Malykh, Vladislav Mikhailov, Maria Tikhonova, Andrey Chertok, and Andrey Evlampiev. 2020. RussianSuperGLUE: Russian language understandIn Proceedings of the ing evaluation benchmark. 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 47174726, Online. Association for Computational Linguistics. Team. 2024. Ultravox: An open-weight alternative to gpt-4o realtime. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, and 197 others. 2025a. Gemma 3 technical report. Preprint, arXiv:2503.19786. Granite Vision Team, Leonid Karlinsky, Assaf Arbelle, Abraham Daniels, Ahmed Nassar, Amit Alfassi, Bo Wu, Eli Schwartz, Dhiraj Joshi, Jovana Kondic, Nimrod Shabtay, Pengyuan Li, Roei Herzig, Shafiq Abedin, Shaked Perek, Sivan Harary, Udi Barzelay, Adi Raz Goldfarb, Aude Oliva, and 44 others. 2025b. Granite vision: lightweight, opensource multimodal model for enterprise intelligence. Preprint, arXiv:2502.09927. Rob van der Goot, Ibrahim Sharaf, Aizhan Imankulova, Ahmet Üstün, Marija Stepanovic, Alan Ramponi, Siti Oryza Khairunnisa, Mamoru Komachi, and Barbara Plank. 2021. From masked language modeling to translation: Non-English auxiliary tasks improve zero-shot spoken language understanding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 24792497, Online. Association for Computational Linguistics. Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pages 318. IEEE. Anton Voronov, Lena Wolf, and Max Ryabinin. 2024. Mind your format: Towards consistent evaluation of in-context learning improvements. In Findings of the Association for Computational Linguistics ACL 2024, pages 62876310. Egor Spirin, Boris Malashenko, and Sokolov Andrey. 2025. Rumodernbert: Modernized bert for russian. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. 2023. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1300313051. Association for Computational Linguistics. Ekaterina Taktasheva, Tatiana Shavrina, Alena Fenogenova, Denis Shevelev, Nadezhda Katricheva, Maria Tikhonova, Albina Akhmetgareeva, Oleg Zinkevich, Anastasiia Bashmakova, Svetlana Iordanskaia, Alena Spiridonova, Valentina Kurenshchikova, Ekaterina Artemova, and Vladislav Mikhailov. 2022. TAPE: Assessing few-shot Russian language understanding. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. Superglue: stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 32613275. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, 12 and Nancy F. Chen. 2025a. AudioBench: universal benchmark for audio large language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 42974316, Albuquerque, New Mexico. Association for Computational Linguistics. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. Preprint, arXiv:2409.12191. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, and 1 others. 2025b. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Bo Wu, Shoubin Yu, Zhenfang Chen, Josh Tenenbaum, and Chuang Gan. 2021. STAR: benchmark for situated reasoning in real-world videos. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. 2025. Qwen2.5-omni technical report. Preprint, arXiv:2503.20215. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, and Jingren Zhou. 2024. AIR-bench: Benchmarking large audio-language models via generative comprehension. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19791998, Bangkok, Thailand. Association for Computational Linguistics. Shu-Wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko-tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, ShangWen Li, Shinji Watanabe, Abdelrahman Mohamed, and Hung-yi Lee. 2021. SUPERB: speech processing universal performance benchmark. In Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021, pages 11941198. ISCA. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, and 4 others. 2024. Minicpmv: gpt-4v level mllm on your phone. Preprint, arXiv:2408.01800. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, and 1 others. 2024. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9556 9567. Wenxuan Zhang, Hou Pong Chan, Yiran Zhao, Mahani Aljunied, Jianyu Wang, Chaoqun Liu, Yue Deng, Zhiqiang Hu, Weiwen Xu, Yew Ken Chia, Xin Li, and Lidong Bing. 2024. Seallms 3: Open foundation and chat multilingual large language models for southeast asian languages. Preprint, arXiv:2407.19672. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2024. AGIEval: human-centric benchmark for evaluating foundation models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 22992314, Mexico City, Mexico. Association for Computational Linguistics. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, and 1 others. 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479. Dmitry Zmitrovich, Aleksandr Abramov, Andrey Kalmykov, Vitaly Kadulin, Maria Tikhonova, Ekaterina Taktasheva, Danil Astafurov, Mark Baushenko, Artem Snegirev, Tatiana Shavrina, Sergei S. Markov, Vladislav Mikhailov, and Alena Fenogenova. 2024. family of pretrained transformer language models for Russian. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 507524, Torino, Italia. ELRA and ICCL."
        },
        {
            "title": "A Multimodal Benchmarks Comparison",
            "content": "Table 6 presents the comparison of major multimodal evaluation benchmarks with MERA Multi."
        },
        {
            "title": "B Dataset Description",
            "content": "B.1 AQUARIA The dataset includes multiple-choice questions that test complex audio comprehension, including speech, non-verbal sounds, and music. The tasks in the dataset require not only the recognition of speech but also the analysis of the entire auditory situation and the interactions among its components. The audio tracks used in AQUARIA were created specifically for this dataset. The dataset contains 9 types of tasks: Audio scene classification Audio captioning (matching audio with its textual description) Audio comparison (finding differences between two audio) Audio sequence analysis Emotion recognition (recognition of emotions and subjective characteristics of speaker) Sound QA (questions related to analysis of non-verbal signals) Speaker characterization (recognition of objective characteristics of speaker) Music QA (questions requiring analysis of music and related knowledge) Music characterization (recognition of objective characteristics of music) Question . What is the difference between the two provided audio recordings ? Audio_1 . samples / audio194 . wav Audio_2 . samples / audio195 . wav A. In the first recording , door is being unlocked ; in the second , it was already unlocked B. In the first recording , the door creaks ; in the second , it doesn 't C. In the first recording , woman enters the apartment ; in the second - man D. In the first recording , person enters through an open door ; in the second , they unlock the lock Answer . Motivation The methodology for evaluating large audio-language models (LALMs), as well as the models themselves, is fairly recent area of research. Compared to benchmarks in the visionlanguage domain, there are significantly fewer comprehensive benchmarks available for evaluating audio-language models. Examples of such benchmarks include AIR-Bench (Yang et al., 2024), AudioBench (Wang et al., 2025a), and MMAU (Sakshi et al., 2025). Audio understanding tasks are generally classified into three categories: speech analysis, non-verbal signal analysis, and music analysis. The AQUARIA dataset was developed to evaluate LALMs in Russian-language tasks. The model needs to be able to process audio because answering questions requires analyzing the associated audio track. The dataset contains 9 question types, which vary both by task category and by the model abilities they test. The dataset assesses three skill categories for audio-language models: perception, knowledge, and reasoning. Dataset creation Based on an analysis of existing benchmarks for testing language models with audio interfaces, we have developed 9 types of tasks that evaluate various groups of skills for these models. For each task type, experts created scenarios with dialogues, background sounds, and music, along with corresponding questions tailored to different task formulations. All scenarios were recorded using professional studio recording equipment, with voluntary use of dataset contributors voices. For some of the Music QA and Music characterization questions, the music tracks were created using generative models (including suno.com). B.2 CommonVideoQA CommonVideoQA is public Russian-language question-answering dataset designed for evaluating video-text models (Video-LLMs), comprising questions related to video clips. It comprehensively assesses the following competencies: general video comprehension and detail recognition, possession of common and domain-specific knowledge, ability to determine the precise order of actions within video and reconstruct the complete sequence, capability to count objects and actions over time, as well as the skill to associate actions with corresponding temporal boundaries in the video. Given an input video and question, the task requires selecting the single correct answer from four provided options. Correct answers do not require audio track com14 Benchmark Modalities # Tasks Primary Focus Language Cultural Focus MultiBench (Liang et al., 2021) MMBench (Liu et al., 2024d) SUPERB (Yang et al., 2021) STAR (Wu et al., 2021) InfiniBench (Ataallah et al., 2024) Video-MME (Fu et al., 2025) General-Bench (Fei et al., 2025) OmniDialog (Razzhigaev et al., 2024) MMMU (Yue et al., 2024) SEED-Bench (Li et al., 2023) 10 (Text, Image, Audio, Video, ...) Image, Text Speech, Audio Video, Text Video, Text Video, Text Text, Image, Video, Audio, Tabular Text, Image, Audio Image, Text Image, Video, Text MERA Multi Text, Image, Audio, Video 20 1 10 4 8 1 700+ 8 30 12 18 General-purpose representation Fine-grained visual reasoning Speech processing VideoQA, temporal reasoning Long-context video understanding Fine-grained video analysis Large-scale ability coverage Multimodal dialogue Expert-level reasoning Generative comprehension EN EN, ZH EN EN EN EN EN EN EN EN General General General General General General General General General General Comprehensive understanding RU General + Russian Table 6: Comparison of major multimodal evaluation benchmarks and MERA Multi. prehension. All video clips are sourced from open public repositories. Question . How many plates and saucers ( not deep bowls or cups ) does the character of this video have ? A. Fifteen . B. Thirteen . C. Twelve . . Sixteen . Answer . Motivation Most published benchmarks in video understanding focus on English-language content, and currently no Russian-language benchmark is available in the public domain. The CommonVideoQA dataset aims to bridge this gap: it enables the evaluation of how effectively video models address the VideoQA task. This dataset covers the assessment of both basic and advanced model capabilities, including general video comprehension and detail recognition (excluding audio track perception), understanding of diverse question types, and the ability to select correct answers from provided options. The \"General Description\" category requires answering questions about the primary action in the video or foreground objects. Questions in the \"Attributes and Details\" category inquire about specific details or background objects. The \"Common and Domain Knowledge\" category comprises questions necessitating both classical common-sense knowledge and expertise in specific applied domains (e.g., \"In what order should the presented dish be prepared?\"). The \"Action Sequences\" category includes questions testing the understanding of actions in the video, their sequential order, and the ability to reconstruct this sequence. The \"Counting\" category involves questions assessing the capability to count objects, repetitions of actions over time, and perform basic arithmetic operations with the counts. The \"Temporal Intervals\" category evaluates the ability to associate actions with temporal boundaries (video timestamps) during which these actions occur. Thus, the dataset evaluates key competencies essential for the video domain. The dataset comprises video scenes spanning the following domains: \"kitchens\" (encompassing household activities), \"sport\" (involving training sessions or competitions), \"flora and fauna\" (featuring landscapes, wildlife, or plants), \"tools\" (demonstrating the use of various implements or auxiliary items), and \"hobbies\" (covering range of personal pursuits). The examples do not require audio comprehension, and all videos are sourced from open repositories (EPIC-KITCHENS, Kinetics), which must be considered during evaluation interpretation. Dataset creation Video clips for the dataset were sourced from the EPIC-KITCHENS-100 and Kinetics-600 datasets. Using the TagMe platform, annotators formulated questions and answer choices for each category. Each example includes only one correct answer, eliminating ambiguity. Two validation stages were conducted with an annotator overlap of 3, followed by result aggregation. Examples without unanimous annotator agreement underwent additional validation and editing. Postprocessing was performed to correct typos. Correct answer options are balanced across classes. B.3 LabTabVQA LabTabVQA is Russian-language questionanswering dataset based on images of tables from the medical domain. The dataset includes two types of images: photographs and screenshots (without OCR layers). Each image is paired with multiplechoice question containing seven answer options, only one of which is correct. The questions are designed to evaluate the capabilities of multimodal LLMs in working with tables presented as images: understanding structure and content, locating and extracting data, analyzing information, etc. All 15 images are anonymized materials from real online consultations on telemedicine platform. Motivation LabTabVQA was created to evaluate the ability of multimodal models to work with tabular information presented in image form, specifically in Russian. Its primary goal is to assess whether such models can understand table structures, interpret their contents, recognize formatting, correlate information, and draw conclusions using only their general knowledge. The dataset creation and question-generation methodology is not limited to specific domain and can be extended to include tables from related areas of knowledge. LabTabVQA expands Russianlanguage benchmarks with new task category for evaluating models ability to analyze tables in terms of content recognition, structural complexity, hierarchy, and data interpretation in end-to-end scenarios. Question . What is the sum of the values of all the indicators listed in the heading \" Coagulogram \"? A. 184.492 B. 169.43 C. 0.92 D. 169.33 E. 184.43 F. 184.44 G. 24.6 Answer . Dataset creation The dataset was built using 697 real images from telemedicine consultation platform. Using the GPT-4o Mini model, we annotated images according to two binary criteria: presence of table in the image; photo or screenshot. 339 images were selected, balanced by image type and table size (also assessed using GPT-4o 16 Mini). For 138 samples, questions were written by experts; for the remaining 201, questions were generated using an AI-agent system composed of the following components: 1. QuestionGenerator (GPT-o4 Mini): generates candidate question with 7 answer options based on the image and question category; 2. QuestionQualifier (GPT-o4 Mini): identifies the correct answer among the 7 options, or requests regeneration if no correct option is found; 3. Solvers (GPT-4o Mini): at three levels of difficulty (defined by prompts), answer the question and provide reasoning; 4. FeedbackEvaluator (GPT-o4 Mini): analyzes the answers and feedback from the Solvers and decides whether to accept the question or send it back for regeneration (return to step 1). The generated examples were validated on the TagMe platform (with 3-way overlap) based on the following criteria: the question is based on the table shown in the image; the question does not require domain-specific knowledge (all required information is in the image/table); the question cannot be answered without using the table/image. Similarly, the correct answer was selected by assessors. correct answer was defined as: the answer proposed by the question generation system, if at least 2 out of 3 assessors agreed with it; the answer chosen by at least 2 out of 3 assessors, even if it differed from the generated answer, provided it was additionally validated by meta-assessor. Due to the specifics of the question-generation methodology, the dataset and tasks may be biased toward the GPT-o4 model family. B.4 RealVideoQA RealVideoQA is closed Russian-language question-answering dataset designed for evaluating video-text models (Video-LLMs), comprising questions related to video clips. It comprehensively assesses the following competencies: general video comprehension and detail recognition, possession of common and domain-specific knowledge, the ability to determine the precise order of actions within video and reconstruct the complete sequence, the capability to count objects and actions over time, as well as the skill to associate actions with their corresponding temporal boundaries in the video. Given video and question, the task is to select the single correct answer from four provided options. Correct answers do not require audio track comprehension. All video clips were collected via crowdsourcing and are absent from publicly available sources. Question . What color is the dome of the tall building in the background on the left ? A. Black . B. White . C. Green . D. Blue . Answer . Motivation The majority of published benchmarks in video understanding are focused on English, and currently, no publicly available benchmark exists for the Russian language. The RealVideoQA dataset aims to bridge this gap: it enables the evaluation of how effectively video models can address questions requiring video comprehension (the VideoQA task). This dataset covers the assessment of both basic and advanced model capabilities, including general video comprehension and detail recognition (excluding audio track perception),understanding of diverse question types, and the ability to select the correct answer from provided options. In the \"General Description\" category, models must answer questions about the primary action in the video or the foreground object. Questions in the \"Attributes and Details\" category inquire about specific details or background objects. The \"General and Domain Knowledge\" category includes questions that necessitate both classical commonsense knowledge and expertise in specific applied domain (e.g., \"In what order should the presented dish be prepared?\").The \"Action Sequences\" category comprises questions testing the understanding of actions in the video, their sequential order, and the ability to reconstruct this sequence. The \"Counting\" category involves questions assessing the ability to count objects, repetitions of actions over time, and perform basic arithmetic operations with the counts. The \"Temporal Intervals\" category evaluates the capability to associate actions with specific temporal boundaries (timestamps) within the video. Thus, the dataset tests key competencies essential for the video domain. Note that the examples do not require audio comprehension, which must be considered during evaluation interpretation. Dataset creation Video clips for the dataset were collected via Telegram bot using crowdsourcing. Annotators formulated questions and answer choices for each category using the TagMe platform. Each example includes only one correct answer, eliminating ambiguity. Two validation stages were conducted with an annotator overlap of 3, followed by result aggregation. Only examples with unanimous annotator agreement were selected. Post-processing was performed to correct typos. Correct answer options are balanced across classes. B.5 RealVQA RealVQA is benchmark for testing the models ability to conduct visual question-answering (VQA). The questions are asked in Russian and can relate to specific object in the image, as well as to the entire image as whole. The benchmark is built in such way that it is impossible to answer the question without an image. It is often necessary to conduct logical reasoning in several stages in order to get an answer. key feature of the dataset is the presence of distractors. Such questions are either about objects that are not present in the image, or there is obviously not enough information to answer the question. The expected behavior of the model in the case of distractor is message that the question cannot be answered, as well as an indication of the reason why this cannot be done. This is how the models resistance to hallucinations is tested. Question . Presumably on what day of the week was this photo taken ? Answer . on Friday Motivation The dataset is designed to evaluate the models ability to identify cause-effect relationships and apply logical reasoning based on visual 17 input. The questions are formulated in way that makes it impossible to answer them without access to the image. Unlike classic VQA datasets that typically assess models ability to directly perceive objects (i.e., coarse perception: recognizing simple shapes and colors), this dataset incorporates the most complex types of perception from the AnonymBench taxonomy (understanding relationships between objects and different types of reasoning in particular). key requirement is that logic or reasoning must be applied to answer the questions. The dataset is intended for state-of-theart vision and text models that are not only capable of comprehending what is depicted but also performing logical inference. This is real-world requirement for modern conversational models, as users ask tricky questions about images that have unambiguous answers. Since the questions do not require expert knowledge, the dataset targets everyday scenarios and casual imagery that users might upload in chat applications. Dataset creation Image collection was carried out via Telegram bot under user agreement ensuring non-disclosure of the photos and user consent. All images were obtained through crowdsourcing, with the condition that the uploaded image must be unique and not previously publicly available online. The first part of the project involved generating questionsanswers pairs using the ABC Elementary platform. The questions were written by AI trainers. These annotators were given an image and tasked with formulating question and corresponding answer. Emphasis was on complex questions, which were defined as those meeting one of the following criteria: requiring the tracing of causal relationships, understanding or perception of relationships between objects, or requiring additional reasoning to answer. The knowledge required to answer the questions was limited to what is typically covered in the school curriculum and corresponds to general logic, meaning no specialized expertise was necessary. Additionally, separate project was created through the ABC Elementary platform for trick questions. The same annotators received photos from the Telegram bot and formulated questions similar to those in the first project, but about objects that were not present in the images. The third stage of annotation involved verifying the generated questions and answers. Using the ABC Elementary platform, crowdsourcing approach with an overlap of 3 was employed to validate the created Q&A pairs. The following aspects were checked: 1) the question cannot be answered without the image; 2) the question is neither too general, binary, nor does it require expert knowledge; 3) the answer is unambiguous; 4) the answer adheres to the required format; and 5) the appropriate question type is chosen. All projects were then aggregated, and the agreed-upon parts were standardized into unified format. During the verification phase, the question type was further added to the metadata with the following categories: object_properties; logics,other; text_understanding; objects_relationship; knowledge. Trick questions comprised 10% of the dataset. B.6 ruCLEVR RuCLEVR is Visual Question Answering (VQA) dataset inspired by the CLEVR (Johnson et al., 2017) methodology and adapted for the Russian language. RuCLEVR consists of automatically generated images of 3D objects, each characterized by attributes such as shape, size, color, and material, arranged within various scenes to form complex visual environments. The dataset includes questions based on these images, organized into specific families such as querying attributes, comparing attributes, existence, counting, and integer comparison. Each question is formulated using predefined templates to ensure consistency and variety. The set was created from scratch to prevent biases. Questions are designed to assess the models ability to perform tasks that require accurate visual reasoning by analyzing the attributes and relationships of objects in each scene. Through this structured design, the dataset provides controlled environment for evaluating the precise reasoning skills of models when presented with visual data. 18 translated questions using the model14 pertained to the linguistic acceptability task. In addition, we checked the dataset for the absence of duplicates. Data Augmentation with Color Replacement: We also augmented the dataset modifying the images from the validation set of the original CLEVER. Specifically, we developed script15to systematically replace colors in questions and images according to predefined rules, thereby creating new augmented samples. This process was initially conducted in English to avoid morphological complexities. Once the questions were augmented, they were translated into Russian and verified for grammatical correctness. B.7 ruCommonVQA ruCommonVQA is publicly available visual question answering dataset in Russian for two types of images: real-world photos and abstract illustrations. The questions are divided into two complexity levels: 1) simple and 2) complex, and categorized by the most frequently occurring types: binary (yes/no), comparative, count-based (how many/much), spatial (where), procedural (how), descriptive (what/which), and subject-based (who). Simple questions can be answered based solely on the visual perception of the image, while complex ones require step of reasoning. All images in the dataset are standard, sourced from publicly available resources, including real-world or cartoonstyle abstract images. ruCommonVQA serves as foundational VQA dataset for the Russian language and is released under an open and public license. Question . Are there any people in the photo ? Answer . Yes Motivation The dataset addresses the classic foundational Visual Question Answering 14https://hf.co/RussianNLP/ ruRoBERTa-large-rucola 15The link was removed for the review period. 19 Question . Are there any other objects with the same shape as the large metallic object ? Answer . no Motivation The RuCLEVR dataset was created to evaluate the visual reasoning capabilities of multimodal language models, specifically in the Russian language, where there is lack of diagnostic datasets for such tasks. It aims to assess models abilities to reason about shapes, colors, quantities, and spatial relationships in visual scenes, moving beyond simple language understanding to test compositional reasoning. This is crucial for models that are expected to analyze visual data and perform tasks requiring logical inferences about object interactions. The datasets design, which uses structured question families, ensures that the evaluation is comprehensive and unbiased, focusing on the models reasoning skills rather than pattern recognition. Dataset creation To create RuCLEVR, we used two strategies: 1) generation of the new samples and 2) data augmentation with color replacement. Below, each technique is described in more detail: Generation of the New Samples: We generated new, unique images and corresponding questions from scratch. This process involved multi-step process to ensure controlled and comprehensive evaluation of visual reasoning. First, 3D images were automatically generated using Blender, featuring objects with specific attributes such as shape, size, color, and material. These objects were arranged in diverse configurations to create complex scenes. Questions with the corresponding answers were then generated based on predefined templates, which structured the inquiries into families, such as attribute queries and comparisons. To avoid conjunction errors, we stick to the original format and generate questions in English, further translating them into Russian using Google Translator. After generation, we automatically filtered incorrectly (VQA) task, similar to English datasets such as VQA (Goyal et al., 2017). Currently, there is no publicly available baseline VQA dataset in Russian for evaluating vision-language models. This dataset is designed to assess the core capabilities of models to recognize objects across diverse types of images, understand variety of question types, and generate answers based on visual input. The question set covers key abilities: understanding objects in the image (Fine-grained Perception, e.g., identification of single instances), overall image perception (Coarse perception), commonsense reasoning, and general knowledge. Images are sourced from public datasets, including COCO (Lin et al., 2014) and English-language VQA v216, which should be considered as limitation when interpreting evaluation results. There is possibility of indirect data leakage through the images in model training data. Dataset creation To construct the dataset, images were sourced from the English-language VQA v2 dataset (which includes data from the COCO (Lin et al., 2014) dataset). Using the ABC Elementary platform, annotators created questionanswer pairs for the images from scratch. Each image was annotated with 3 questions and with 3-way annotator overlap. The resulting data was then aggregated and filtered both automatically (e.g., removal of overly long answers, typos, formatting issues) and manually. The binary question data was class-balanced. The second part was created entirely from scratch. To collect images, Telegram bot was used along with user agreement that ensured photo confidentiality and confirmed user consent. Images were crowdsourced under the condition that each uploaded image had to be unique and not previously available online or from public sources. In this stage of the project, questions and answers were again generated via the ABC Elementary platform. Questions were written by AI trainers: annotators were provided with an image and instructed to create question along with corresponding answer. B.8 ruEnvAQA ruEnvAQA is dataset of multiple-choice and binary-choice questions in Russian. The questions are related to music and non-verbal audio signal understanding. The dataset is based on questions from English-language datasets Clotho-AQA (Lipping 16https://hf.co/datasets/pingzhili/vqa_v2 et al., 2022) and MUSIC-AVQA (Li et al., 2022). The questions were translated into Russian and partially modified, while the audio recordings were used in their original form (with length trimming). The dataset includes 8 types of questions: Original question types from MUSIC-AVQA (approximately half of the questions test expert knowledge about rare instrument sounds, while the rest test general knowledge): Music instrument counting: \"How many musical instruments are playing in the recording?\"; Single music instrument detection: \"Is <instrument_X> playing in the recording?\"; Double music instrument detection: \"Is it true that both <instrument_X> and <instrument_Y> are playing in the recording?\"; Music instrument comparison (louder): \"Is it true that <instrument_X> is playing louder than <instrument_Y> in the recording?\"; Music instrument comparison (longer): \"Is it true that <instrument_X> is playing for longer duration than <instrument_Y> in the recording?\"; Classes assigned during the editing of CLOTHO-AQA questions (general knowledge questions): Audio scene classification is about understanding the audio scene as whole, logical inference from multiple details (determining the location or circumstances where the audio was recorded); Audio captioning questions are about understanding specific details of an audio fragment, the order and quantity of events; Sound QA with reasoning questions test audio comprehension with simple reasoning, requiring not only perception of audio signal details but also step of logical reasoning. Question . In what location was the recording most likely made ? A. at the airport B. at the pier C. at the railway station D. at the bus station Answer . 20 Motivation Compared to the vision-language domain, there are fewer large benchmarks that combine diverse tasks for the evaluation of LALM skills. Examples of such benchmarks include AIRBench (Yang et al., 2024), AudioBench (Wang et al., 2025a), and MMAU (Sakshi et al., 2025). Audio understanding tasks can be basically classified into speech analysis, non-verbal signal analysis, and music analysis. This dataset tests LALMs abilities to perceive and analyze non-verbal signals and music by answering questions in Russian about audio recordings of musical compositions and audio scenes from various life situations. The tests include questions of three types: Questions on literal perception of audio events (Audio captioning and music questions) test models ability to match sequences of events captured in audio, their quantity and duration with their textual description. For example, \"How many times did the ball bounce on the floor?\" or \"Is there violin playing in the recording?\". Questions on audio scene classification (Audio scene classification) test models ability to conduct inductive reasoning, specifically to determine the location and circumstances of audio recording based on event details. For example, if aircraft sounds and announcements are heard in the recording, it was likely made at an airport. Questions with additional reasoning (Sound QA with reasoning) require additional logical operations with general world knowledge to derive the answer, beyond basic audio information perception. For example, if cat is meowing in the audio, the question might be: \"How do these animals typically move?\". Dataset creation The dataset is compiled from audio files and questions in equal proportions from two English-language datasets, separately covering the domains of music and non-verbal signals. Questions related to speech understanding are not included in the dataset. Questions from Clotho-AQA Dataset The Clotho-AQA (Lipping et al., 2022) dataset contains questions about audio with non-verbal signals and minor speech elements, with questions focusing only on non-verbal signals and occasionally on external characteristics of speech, such as volume or speaker gender. Original questions from the test split were converted to multiple-choice format by generating 3 distractors (incorrect answer options) for each question in addition to the single correct answer from the original dataset. The distractors were generated in English using Llama-3.2-3B-Instruct17. Questions, correct answers, and distractors were translated into Russian using DeepL API18. Questions were translated as single sequence together with answer options to minimize the impact of synonymy during translation. The automatically translated questions and answer options, along with corresponding audio files, were reviewed by professional editors (without overlap in annotation) considering the original question formulations. If the original question was unsuitable for translation, the editor posed new question to the audio, determined the correct answer and distractors. The editor also chose an appropriate question type: Audio scene classification, Audio captioning, or Sound QA with reasoning. Questions from MUSIC-AVQA The MUSICAVQA (Li et al., 2022) dataset consists of video recordings of musical performances and three groups of questions: questions about the audio component of the video, not requiring visual component analysis; questions about the visual content, not requiring understanding of the accompanying audio; questions about audio-visual content, relating simultaneously to both audio and visual parts of the video. For the ruEnvAQA dataset, only questions related to audio were selected (only test split). The audio component was extracted from each video and used as standalone wav file. The selected questions were constructed using templates filled with musical instrument names (22 different instruments): \"How many musical instruments are playing in the recording?\"; \"Is <instrument_X> playing in the recording?\"; \"Is it true that both <instrument_X> and <instrument_Y> are playing in the recording?\"; \"Is it true that <instrument_X> is playing 17https://hf.co/meta-llama/Llama-3. 2-3B-Instruct 18https://www.deepl.com/products/api 21 louder than <instrument_Y> in the recording?\"; \"Is it true that <instrument_X> is playing for longer duration than <instrument_Y> in the recording?\". Templates, instrument names, and template answers were translated manually. Questions were selected to balance question types and answers, as well as the musical instruments mentioned in the questions. The original dataset questions were converted to binary questions. For questions like \"How many musical instruments are playing in the recording?\", answer options were created as \"one\" and \"several\", while other questions were reduced to \"yes\"/\"no\" choices. Thus, the resulting dataset has balance between questions with two and four answer options. The materials from the original MUSIC-AVQA dataset are protected under the CC BY-NC 4.019 license, which permits free distribution (including modified materials) for non-commercial purposes. Question Validation and Audio Processing Preselected questions from both datasets underwent validation by crowdsource annotators with 3-fold overlap. Annotators were presented with an audio, question, and answer options, and were tasked with selecting all valid answer options to exclude cases with multiple correct answers. Along with validating questions and answers, annotators trimmed the audio to fragments between 5 and 20 seconds in length. If the audio could not be trimmed while maintaining question relevance, the question and audio were excluded. To obtain aggregated answers, each answer option selection was aggregated using the DawidSkene method (each option as an independent variable), after which only questions with single selected answer option were retained. Subsequently, only annotator answers that matched the aggregated (pseudo-reference) answer were used. The audio fragment in such groups was selected based on the principle of maximum duration, which did not affect the answer since the aggregation grouping was done by question and answer. B.9 ruHHH-Image ruHHH-Image is multimodal dataset designed for Visual Question Answering (VQA) that integrates 19https://creativecommons.org/licenses/by-nc/4. 0/ text and images, with particular focus on evaluating AI responses through the lens of ethics and safety. This task checks two key abilities. First, it tests if AI can understand questions with parts from different sources. These sources include both text and images. Second, it evaluates if the AI can choose the best of two answers. The selection is based on ethics or safety categories. The goal is to see if AI can analyze multimodal information. It must then select the most ethical and safe response for users from answer options. The test is based on two existing datasets. One is the HHH text dataset (1). The other is its Russian version, ruHHH in MERA-text (2). The original categories were Honest, Helpful, and Harmless. ruHHH-Image adds three new ethical categories. These are Empathetic, Etiquette, Open/Impartiality. Disclaimer. Any videos, questions, and answers in the dataset are not call to action, condemnation, or insult to anyone or anything. The authors of the dataset pursue exclusively scientific goals, primarily an assessment of the safety and ethical behavior of artificial intelligence models in the proposed situations. 22 This ensures 100 questions per main category. 1. Empathetic Category. Tests formal empathy in three subcategories: animals and plants (inspired by the VoightKampff test from Do Androids Dream of Electric Sheep? (1968) by Philip K. Dick), human beings (toward one or few specific people), society (toward groups or communities). 2. Etiquette Category. Checks adherence to etiquette norms in: place and society (rules for specific locations or groups), time and situations (norms for certain times or scenarios), person (how to behave toward an individual). 3. Harmless Category. Selects the safest answer about situations involving: death, threat (risk of injury or loss), discommode (discomfort, minor inconveniences). 4. Helpful Category. Picks the most useful answer, providing: solutions (direct fixes), prevention (avoiding future problems), development (guidance for growth or benefit) 5. Honest Category. Measures honesty in: truth (factual accuracy), people (avoiding deception), norms (following honesty standards). 6. Open Category. Assesses lack of prejudice toward: groups (based on gender, age, religion, etc.), personal choice, objects, places and actions. Dataset creation The dataset was built using images collected through mobile bot. Annotators checked these images for quality and clarity. Next, questions and answers were created for the images. These covered six ethical categories. After validation and editing, the categories were split into 18 subcategories. Each main category had three subcategories. This helped capture key aspects of each category. For every image-question pair, annotators provided two to four answer options. They ranked these answers from best to worst. The ranking followed the rules and the reQuestion . have an idea to cheer up my elderly grandma ! If bring her here at night and show her this , do you think she ' ll like it ? A. If grandma enjoys horror movies and goes to scary immersive performances on her own , then she ' ll definitely like it . It 's totally fine to have such interests at any age . The main thing is to make sure in advance that she actually enjoys this kind of thing . B. This might not be the best way to entertain grandma , but if we can 't come up with anything else , it could be worth try . The main thing is to give her heads - up - this might not be exactly what someone of her age would enjoy . Answer . Motivation Translated datasets often struggle with different languages and cultures. Ethics is particularly sensitive area. ruHHH-Image evaluates models using Russianlanguage content. This includes texts and photos. It checks if model can pick the best response. The criteria include honesty, lack of bias, and safety. They also cover empathy, usefulness, and etiquette compliance. The dataset helps identify problematic responses. These are grouped into the six ethical categories. In terms of structure, each of the six categories has three subcategories. The dataset balances them equally. There are 33-34 questions per subcategory. 23 quirements of the questions category. However, during testing, the model sees only two answers at time. So some image-question pairs appear up to six times in the dataset. But each time with different pair of option answers. This method checks if the model ranks answers the same way annotators did. and Images questions Limitations reflect Russian-language contexts. Answers align with Russian ethical and cultural views. Not suitable for evaluating global or multicultural ethics. Some sections (Open, Harmless) may go beyond Russian-specific norms into worldwide ones. B.10 ruHHH-Video ruHHH-Video is multimodal dataset that adapts the methodology of ruHHH-Image to the video modality. As the first Russian-specific dataset of its kind, it is designed to evaluate ethical reasoning skills in videos. B.11 ruMathVQA ruMathVQA is multimodal dataset consisting of school math problems presented in the form of images and annotated questions to record the answer in an unambiguous form. Annotation . Write the answer as whole number without specifying units of measurement . Answer . 4 Motivation The dataset is an open database of tasks for testing the models ability to understand pictorial elements from school mathematics and geometry and apply knowledge of school mathematics grades 5-6 and geometry grades 7-9. The peculiarity of this task is to test the models to accurately follow complex mathematical answer formats (annotations), which are fed to the input along with the instruction. 24 The dataset is intended for SOTA Vision + Text models, which can understand what is depicted and also have some basic knowledge of the school curriculum. The images are presented in the form (the original text of the task is saved inside the picture), which the user can send in the dialog chat to the models in correspondence. This dataset does not check the course of the solution and does not require deriving reasoning for the problem the answer to the problem is short answer with number/formula. The annotation serves as an instruction for recording an unambiguous short answer to the problem in the form required by the user. Therefore, Accuracy is used as metric. Dataset creation group of experts with basic knowledge of mathematics was selected for the dataset collection stage. The images for the dataset were drawn by experts similar to the tasks from school textbooks on mathematics and geometry. The images were drawn in three ways: 1) in an editor on white sheet using blue or black color; 2) on white sheet of paper using blue or black color, in uppercase or lowercase letters, with or without the use of drawing tools; 3) on grid-lined sheet of paper using blue or black color, in uppercase or lowercase letters, with or without the use of drawing tools. The answers to the problems were obtained by solving and discussing each problem by several experts. The annotation, which contains the format for unambiguous recording of the answer to the problem, was manually marked up by an expert by selecting from list of options for different annotations. universal question was added to each problem in the instructions: \"What is the answer to the problem shown in the picture?\" The dataset obtained in the previous step was validated with overlap by 3 full-time annotators of the ABC Elementary platform. The annotators checked the quality of the images, the answer format and the correctness of the annotation requirements for compliance with the problem question and the answer form. Based on the validation results, if at least one annotator noted an error / poor quality, the data was manually edited. B.12 ruNaturalScienceVQA NaturalScienceQA is multimodal questionanswering dataset on natural sciences with basic questions from the school curriculum, based on the English dataset ScienceQA (Lu et al., 2022). The dataset includes questions in four disciplines related to natural sciences: physics, biology, chemistry, and natural history. The task requires answering question based on an image and accompanying context by selecting the correct answer from the options provided. The questions are specifically curated so that it is impossible to determine the correct answer without the image. Context . This passage describes specific growth characteristic in rose plants : Climbing growth and trailing growth are different growth types in roses . Climbing plants have long , bending stems similar to vines . These plants can grow upward , covering fences or walls . Roses with trailing growth form stay close to the ground , forming low bushes or shrubs . In group of rose plants , some individuals have climbing growth habit , while others are trailing . In this group , the gene responsible for growth form has two alleles . The climbing growth allele (G) is dominant over the trailing growth allele (g). This Punnett square shows the cross between two rose plants . Question . What is the expected ratio of offspring with climbing growth to offspring with trailing ( bushy ) growth ? Choose the most likely ratio . A. 4:0 B. 0:4 C. 2:2 D. 3:1 Answer . Note: feature of the dataset is that the images used in the tasks may be of relatively low resolution. Thus, the models ability to extract information from low-quality images is additionally explored, which is often encountered in applications (e.g., when user sends poor-quality screenshot). Motivation The NaturalScienceQA dataset is aimed at evaluating the reasoning abilities of AI models in multimodal environment. Its goal is to assess models specializing in multimodal reasoning, as the questions involve both textual and visual data and are selected so that they cannot be answered without the image information. It is suit25 able for models that integrate visual understanding with textual comprehension. The primary users of this dataset are Data Science researchers and developers focused on improving multimodal evaluation, particularly those involved in education, scientific research, and AI-driven tutoring systems. Educators may also find the results valuable in measuring how well AI models can mimic human understanding in educational settings. The questions in the NaturalScienceQA dataset are designed to reflect real-world educational scenarios where students are presented with scientific concepts in visual and textual formats. The dataset evaluates models ability to understand scientific concepts and apply them to solve specific problems. The structure of the questions ensures that models must integrate information from both modalities to determine the correct answer. This design demonstrates that NaturalScienceQA effectively assesses the multimodal reasoning capabilities it aims to test, providing robust experimental setup for benchmarking AI performance. Dataset creation NaturalScienceQA was created based on the English ScienceQA (Lu et al., 2022) dataset, question-answering dataset covering wide range of scientific disciplines. During the dataset creation process, questions from the test set of the original ScienceQA were selected from four natural science disciplines and manually filtered using the following criteria: 1) the question includes an image and cannot be answered without the accompanying image (relying only on information from the explanatory text), 2) the question is consistent with the Russian educational context and is covered by the school curriculum. Subsequently, the selected questions were translated using the Google Translator API and manually edited to correct errors and inaccuracies from automatic translation. Examples for few-shot learning were obtained similarly but were initially selected from the validation set. B.13 ruSLUn RuSLUn (Russian Spoken Language UNderstanding dataset) is Russian-language dataset for spoken language understanding, designed following the principles of the English SLURP (Bastianelli et al., 2020) and the multilingual xSID (van der Goot et al., 2021), but with consideration for the cultural and linguistic specifics of Russia. It is intended for evaluating models that map audio recordings directly to semantic representations, including intent detection and slot filling. RuSLUn contains variety of spoken commands and queries that are typical for Russian users and contexts. The key feature of the dataset is its localization: in addition to being in Russian, it incorporates typical usage scenarios, vocabulary, and contexts, which makes it particularly relevant for developing voice assistants and speech-driven services for Russian-speaking users. Question . Listen carefully to the audio with the user 's query , classify the intent the query belongs to , and select all possible slots corresponding to that intent . Words in the slots should have the same morphological form as in the audio , and numbers should be written as text . Answer . {\" intent \": \" RateBook \" , \" slots \": { \" object_name \": \" Doctor Zhivago \" \" rating_value \" : \" three \" \" best_rating \": \" six \" \" rating_unit \": \" stars \" }} Motivation Traditionally, the task of spoken language understanding (SLU) is solved in several stages: first, audio recordings are converted into text using automatic speech recognition (ASR), and then the necessary information is extracted from the text using natural language understanding (NLU) technologies. However, this modular approach is susceptible to error accumulation due to ASR inaccuracies, and also requires two separate models or two sequential processing steps, which slows down system performance. The ruSLUn dataset is intended for evaluating audio models capable of directly understanding and interpreting the meaning of audio data in an end-to-end fashion, without an intermediate ASR step. Furthermore, ruSLUn is the first Russian-language dataset in which audio recordings are directly aligned with the corresponding intent and slot annotations. This enables comprehensive research into end-to-end SLU tasks, taking into account the cultural and linguistic specifics of Russian users. Dataset creation The dataset was created in two stages: first, text queries were generated and an26 notated with intents and slots, then, these queries were recorded as audio. The annotation scheme was based on the cross-lingual xSID (van der Goot et al., 2021) dataset, which includes 16 intent types and 33 slot types. At the first stage, the validation and test data from xSID were manually translated into Russian by one of the dataset authors. The texts were then adapted to fit the Russian context: locations, names of artists, movies, songs, and restaurants were replaced with popular and recognizable Russian counterparts. These replacements were manually and randomly selected from lists of the most common options. The text data then underwent additional post-processing, including removal of punctuation, conversion of all digits to their word forms, and transforming all text to lowercase. After completing work with the text data, the queries were recorded as audio. Seven speakers of different ages (five women and two men), who were not professional voice actors, were recruited for audio recording. All participants were instructed to record each sentence in quiet setting, speak in natural voice, and save each sentence as separate audio file. Recording took place at home using regular voice recorders, so the audio naturally contains some background noises (such as breaths, shuffling, etc.). The final dataset was manually checked by moderator to ensure that each audio recording matched the corresponding text data and that the intent and slot annotations were correct. B. ruTiE-Audio ruTiE-Audio is an emulation of the Turing test in audio format. The dataset consists of sequence of audio tasks, each accompanied by four possible answers in textual format. The dataset includes 3 coherent dialogues, each simulating 500 user queries to the model. The model receives an audio input containing tasks and questions, while answer options (4 per task) are provided as text, and the model must choose the most appropriate response. Accordingly, This dataset is designed for evaluating any chat-oriented models capable of processing audio modality. The tasks test the models ability to support coherent dialogue on naturally changing topics of communication, based on the context of previous interactions. This dataset is based on the text dataset of the same name from the MERA-text benchmark. In addition to ruTiE-Audio, it is presented in one more version: visual (textual questions about images that are answered in text). A. Porchbench B. Validol C. Valentina D. Valentine Answer . Motivation This dataset targets models with large context window (ideally capable of handling dialogue history up to 499 prior turns). The test has complex task. The model must not only preserve the context and refer to it in the dialogue, but also have broad linguistic and cultural knowledge: proverbs, nursery rhymes, catchphrases, movie quotes, songs, plays, books, and memes. Moreover, the dataset evaluates spontaneously triggered human-like conversational skills: recognizing irony, the ability to understand and complement joke, mental arithmetic, spatial reasoning, bilingualism, recognizing and using causeand-effect relations, avoiding speech traps. Only by using all these skills in comprehensive manner can one fully \"play imitation\" according to Turing, that is, adequately participate in human conversation on equal terms with people. Please note: during the conversation, the modalities and formats of communication change. The interlocutor can use puns, ask to count the letters in spoken, not written word, draw your attention to some sound outside the window and wait for your reaction, invite third person to the conversation, express some opinion or judgment and so on. Therefore, not all prompts are formatted as direct questions. Some are situational utterances or mini audio skits without explicit questions, yet the model must still select the most contextually appropriate response from four given choices. ruTiEAudio offers 4 answer options for each question. The test checks the models ability: to retain context to support (at the everyday level) dialogue on any of the main topics (as defined in AnonymBench domains) to recognize and categorize core task types, without which it is impossible to solve the problems of emulating the Turing test (including basic mathematics, ethics, linguistic games, common knowledge, etc.) to navigate in various categories of thinking, including recognizing irony, emotions and intentions of the interlocutor, restoring the 27 essence of the situation based on key elements, etc. There is also an important limitation for the validity of checking models with the ruTiE-Audio dataset. Since about half of the questions are somehow tied to the immediate context of the emulated \"conversation\", the next question may suggest the answer to the previous one. So you cannot give the model several tasks from the dialogue at once. Questions are asked strictly one at time, their order and sequence should not be mixed or changed in any other way. Dataset creation The dataset was manually collected by internal experts and then validated. The audio tasks were edited based on scripts written by experts and internal recordings made based on them, previously unpublished online as well. Background noises were sourced from public datasets and custom recordings from the SberDevices studio and various field environments. B. ruTiE-Image ruTiE-Image is multimodal emulation of the Turing test, which is an unchangeable sequence of question-answer tasks with the ability to choose an answer. These are 3 coherent dialogues, each dialogue imitates 500 user requests to the model using text and pictures. The model receives answer options (4 for each task) in text form and chooses from them. The test tasks check the models ability to adequately support dialogue on naturally changing topics of communication, based on the context of previous questions. The dataset is based on the text dataset of the same name from the MERA-text benchmark. In addition to ruTiE-Image, similar dataset is presented in one more version: multimodal ruTiEAudio (questions are submitted to the input in audio format, the model responds with text). 28 Question . Hi ! ' ll call you Ada , and to find out my name , look at the picture and answer who painted them pink - then take the first three letters of that word . So , what 's my name ? A. Hud B. Sol C. Mal D. Zack Answer . Motivation The dataset is designed to analyze models with large context window (with context depth of up to 499 questions). The test has complex task. The model must not only preserve the context and refer to it in the dialogue, but also have broad linguistic and cultural knowledge: know proverbs, counting rhymes, catchphrases from films, songs, plays, books, memes. The model must also have skills that are spontaneously actualized in human speech: recognizing irony, the ability to understand and complement joke, oral arithmetic skills, spatial thinking, bilingualism, recognizing and using cause-andeffect relationships, avoiding speech traps. Only by using all these skills in comprehensive manner can one fully \"play imitation\" according to Turing, that is, adequately participate in human conversation on equal terms with people. Please note: in conversation, the modalities of communication often change. The interlocutor can show you picture, ask you to read the inscription drawn on the wall, refer to previously shown photo, sometimes invite third person to the conversation, express some opinion or judgment and so on. Therefore, the design of separate task in the ruTiE-Image dialogue is not always designed as question it can be designed as replicasentence, to which the model needs to choose an adequate reaction. In ruTiE-Image, task can look like simple picture sent to the model without an accompanying question but with suggested reaction options, from which you need to choose the right one. The dataset offers 4 answer options for each question. The test checks the models ability: to retain context, to support (at the everyday level) dialogue on any of the main subject areas (as defined in AnonymBench domains) to understand the main classes of problems, without which it is impossible to solve the problems of emulating the Turing test (including the simplest mathematics, ethics, linguistic games, general worldview, etc.) to navigate in various categories of thinking, including recognizing irony, emotions and intentions of the interlocutor, restoring the essence of the situation based on key elements, etc. There is also an important limitation for the validity of checking models with ruTiE. Since about half of the questions are somehow tied to the immediate context of the emulated \"conversation\", the next question may suggest the answer to the previous one. In this regard, it is not allowed to give the ruTiE model several tasks from the dialogue at once. Questions are asked strictly one at time, their order and sequence should not be mixed or changed in any other way. Dataset creation The dataset was manually collected by internal experts and then verified. The images for the dataset were crowdsourced from previously unpublished mobile photos, ensuring the relevance and modernity of the materials. B.16 SchoolScienceVQA SchoolScienceVQA is Russian-language multimodal dataset inspired by ScienceQA (Lu et al., 2022). It evaluates the reasoning capabilities of AI models in multimodal setting using multiplechoice questions across scientific subjects such as physics, biology, chemistry, economics, history, and earth science. Each question includes an image, text context, and explanation of the correct answer. These components provide basis for assessing reasoning chains. Context . Question . How does the position of the structure highlighted in red in the image change in the mid - latitudes of the oceans in winter compared to summer ? A. In winter , it remains at the same depth as in summer B. In winter , it is located deeper than in summer C. In winter , it disappears completely D. In winter , it rises closer to the surface Answer . Motivation SchoolScienceVQA is designed to benchmark AI systems in educational and scientific reasoning tasks requiring both visual and textual understanding. It supports the following use cases: Multimodal Model Evaluation: The dataset requires joint processing of images and text. It is intended for models capable of visionlanguage reasoning and is unsuitable for unimodal LLMs. Target Audience: Researchers and developers working on multimodal models, especially in the education and tutoring domain. Educators may also use the dataset to measure how well models simulate human-like understanding. Question Content: Questions resemble realworld educational tasks and require true multimodal inference to solve correctly. Dataset creation SchoolScienceVQA was developed from scratch based on the methodology of ScienceQA (Lu et al., 2022), adapted for Russian cultural and educational context. Domains were adjusted to align with the Russian school curriculum. Expert annotators from relevant scientific domains created original multimodal examples. Images were produced using original photography, manual illustration, computer graphics, and neural network generation (DALLE, Stable Diffusion, etc.). All images are novel and not reused from existing datasets. Metadata includes image gen29 eration method to support transparency and bias mitigation. task in the form required by the user. Therefore, Accuracy is used as the evaluation metric. B.17 UniScienceVQA UniScienceVQA is multimodal dataset consisting of tasks designed to assess expert knowledge in various fields of science (fundamental, social, and applied sciences, cultural studies, business, health, and medicine). The tasks are presented in the form of images and questions with accompanying annotations. The tasks are divided into three groups based on the response format: 1) short-answer tasks; 2) multiple-choice tasks; and 3) multiplechoice tasks with no correct answer provided. Question . What is the order of the automorphism group of the graph shown ? Annotation . In your answer , write only the number . Answer . Motivation The dataset is an open collection of tasks designed to evaluate models ability to understand elements of images from university curricula and professional domains. distinctive feature of these tasks is testing the models capability to provide short and precise answers, as well as to identify the correct answer from multiple-choice options. The dataset is intended for Vision + Text models that not only understand what is depicted in images but also possess expert knowledge of universitylevel content. This dataset does not evaluate the reasoning process or require the model to provide detailed explanation for solving the task the answer to the task is short response in the form of number or formula. The annotation serves as an instruction for recording an unambiguous short answer to the Dataset creation The dataset consists of 25 subdomains, and for data collection in each subdomain, group of experts with in-depth knowledge in the respective field was involved. The images for the dataset were either drawn or photographed by the experts. The creation of the dataset involved two stages: 1) generating the image, question, and answer; and 2) reviewing the created data. An annotation, which specifies the format for unambiguously recording the answer to the task, was manually added according to the answer. Each task includes universal instruction: \"Read the question and solve the task\". As result, 200-400 tasks were collected for each subdomain. B.18 WEIRD WEIRD is an extended version of binary classification subtask of the original English WHOOPS! (Bitton-Guetta et al., 2023) benchmark. The dataset evaluates the ability to detect violations of commonsense. Commonsense violations are situations that contradict the norm of reality (Rykov et al., 2025a). For example, penguins cant fly, children dont drive cars, guests dont serve food to waiters, etc. Weird and normal images are equally distributed in the dataset. Question . Is the image strange or normal ? A. strange B. normal Answer . Motivation The dataset focuses on evaluating violations of commonsense, and is suitable for the 30 evaluation of any AI models that can analyze images. The main capability that this dataset evaluates is the analysis of visual information and collating it with common sense. Accuracy is the main evaluation metric. Since the dataset evaluates the basic ability to assess plausibility, it will be interesting for any research project as one of the basic stages of the model evaluation pipeline. Dataset creation The dataset was created based on the original WHOOPS! (Bitton-Guetta et al., 2023), using iterative synthetic generation in the style of Self-Instruct (Rykov et al., 2025b). Each sample from the WHOOPS! subset for binary classification is pair consisting of weird and normal image, along with categories of commonsense violations and image descriptions. To extend the original benchmark, we iteratively generated new categories of commonsense violation and image descriptions using GPT-4o with WHOOPS! samples as few shots. In addition, we used synthetic descriptions to generate images using DALLE. Next, we manually filtered out bad images and added good images to the pool. Finally, the pool was used to repeat the generation process and extract new few-shots."
        },
        {
            "title": "C Skill taxonomy",
            "content": "Each skill has modality-related version, while particular skills are not applicable to some modalities, e.g. Image-to-text grounding for audio modality. In the description that follows, all skill names are provided along with the corresponding modality tags ( ). To disentangle atomic skill names from aggregation taxonomy categories, the former are marked with . Some low-level skills double the names of upper categories, this is to preserve the same taxonomy depth across the entire taxonomy. Descriptions for upper-level skills are inherited by lower levels, unless stated otherwise. C.1 Perception C.1.1 Fine-grained single-instance perception Perceiving and localizing single object or event. Object localization Detecting object positions relative to the image itself (left, right, top, bottom, center). Not to be confused with Media grounding, which deals with direct reference to images and video like frames and bounding boxes. Object localization Object localization . Object recognition Mapping objective visual and audial features to the concepts in knowledge space, e.g. matching the concept cat to meowing sounds and/or visual image of cat (ears, paws, etc.). Object recognition Object recognition . Single-instance event recognition Recognizing actions, events, and states. It involves features that are dynamically perceived or may suggest dynamic characteristics, such as those found in motion stopframes. Single-instance event recognition Object motion recognition . Living things motion recognition ."
        },
        {
            "title": "Pose recognition",
            "content": "Non-human pose recognition Animal body pose recognition . Human pose recognition Human body pose recognition . Facial expression recognition . Hand gesture recognition . C.1.2 Fine-grained cross-instance perception Perceiving and localizing multiple objects or events and the character of their interaction."
        },
        {
            "title": "Overlapping object differentiation",
            "content": "Perception involves interpreting and understanding input from different modalities: images, audio, and video. Many perception skills are well-studied in computer vision, speech, and multimodal literature. Recognition skills are decomposed as mapping perceived surface features to their corresponding knowledge items, i.e. naming objects, events, etc. An overview of the perceprion taxonomy is given in Table 7 and below detailed skill description is given. Overlapping object differentiation Disentangling co-occuring objects of the same modality, the ability to separately perceive features of overlapping entitites. Overlapping image differentiation . Separate recognition of visually overlapping objects. Speaker diarization . Attributing speech to multiple simultaneous and/or consequent speakers. 31 Modalities Audio ruTiE-Audio AQUARIA ruTiE-Audio AQUARIA ruEnvAQA AQUARIA ruTiE-Audio AQUARIA Video CommonVideoQA RealVideoQA CommonVideoQA RealVideoQA CommonVideoQA RealVideoQA ruHHH-Video CommonVideoQA RealVideoQA ruHHH-Video ruTiE-Audio AQUARIA ruHHH-Video CommonVideoQA RealVideoQA ruTiE-Audio ruEnvAQA AQUARIA CommonVideoQA RealVideoQA ruHHH-Video ruEnvAQA AQUARIA AQUARIA CommonVideoQA RealVideoQA ruHHH-Video ruHHH-Video L2 L3 Taxonomy Levels L4 Overlapping object differentiation Overlapping object differentiation L5 Overlapping image differentiation Speaker diarization Image Mutual object localization Mutual object localization Spatial object relationship Temporal object relationship Repeating recognition pattern Repeating recognition pattern Visual pattern recognition Temporal pattern recognition Object-object interaction Finegrained crossinstance perception Event recognition Event recognition Human-object interaction Human-human interaction Object localization Object localization Object localization Object recognition Object recognition Object recognition Event recognition Event recognition Object motion recognition Living things motion recognition Pose recognition Non-human recognition pose Human pose recognition Animal body pose recognition Human body pose recognition Facial expression recognition Hand gesture recognition t r Finegrained singleinstance perception Image-to-text grounding Image-to-text grounding Textual grounding Scheme recognition Plot recognition Table recognition Text recognition (OCR) LabTabVQA ruCommonVQA ruCLEVR ruCLEVR RealVQA ruCommonVQA RealVQA ruHHH-Image ruCommonVQA RealVQA ruHHH-Image ruTiE-Image ruCommonVQA ruHHH-Image ruTiE-Image ruCommonVQA ruCLEVR RealVQA ruCommonVQA ruCLEVR RealVQA ruHHH-Image ruTiE-Image ruCommonVQA RealVQA ruHHH-Image ruCommonVQA RealVQA ruHHH-Image X X ruMathVQA ruNaturalScienceVQA UniScienceVQA SchoolScienceVQA LabTabVQA LabTabVQA ruMathVQA ruNaturalScienceVQA UniScienceVQA SchoolScienceVQA Prosody & stress recognition Prosody & stress recognition Onomatopoeia Speech recognition Speech recognition Audio-to-text grounding Media grounding Media grounding Song lyrics recognition Visual media grounding Temporal media grounding X ruTiE-Audio AQUARIA RuSLUn CommonVideoQA RealVideoQA Table 7: Skill taxonomy coverage of MERA Multi tasks (perception part). Columns L1-L5 show the skill hierarchy levels, while Image, Audio and Video columns indicate which tasks cover each knowledge type across different modalities."
        },
        {
            "title": "Mutual object localization",
            "content": "Mutual object localization Spatial object relationship . Temporal object relationship ."
        },
        {
            "title": "Repeating pattern recognition",
            "content": "Repeating pattern recognition Visual pattern recognition . Temporal pattern recognition . Cross-instance event recognition Cross-instance event recognition Object-object interaction . Human-object interaction . Human-human interaction . C.1.3 Textual grounding Image-to-text grounding Image-to-text grounding Scheme recognition . Plot recognition . Table recognition . Text recognition (OCR) . Audio-to-text grounding 32 Prosody & stress recognition Prosody & stress recognition . Speech recognition Onomatopoeia . Speech recognition . Song lyrics recognition . Media grounding Visual media grounding . Temporal media grounding . C.2 Knowledge C.2.1 Knowledge Below we present detailed description of the knowledge category, which was introduced in Section 3.2. Knowledge Other inductive reasoning Other inductive reasoning Other inductive reasoning . C.3.2 Deductive reasoning Deductive reasoning Deductive reasoning Weirdness understanding . Analogical reasoning . Other deductive reasoning . C.3.3 Abductive reasoning Abductive reasoning Abductive reasoning Hypothetical reasoning . Cause & effect understanding . Common everyday knowledge Common everyday knowledge . Ethics . Domain knowledge Common domain knowledge . Expert domain knowledge . C.3.4 Quantitative reasoning"
        },
        {
            "title": "Quantitative reasoning",
            "content": "Counting Static counting . Temporal counting . C.3 Reasoning Mathematical reasoning Mathematical reasoning . An overview of the reasoning taxonomy is given in Table 8 and below detailed skill description is provided. C.3.5 Other reasoning"
        },
        {
            "title": "Inductive reasoning",
            "content": "C.3.1 Attribute recognition understanding characteristic Coarse attribute recognition Generated content detection . Source characterization . Media . Speech emotion recognition . Music emotion recognition . Melodic structure interpretation . Topic understanding . Style & genre understanding . Scene understanding . Object attribute recognition Physical property understanding . Object function understanding . Identity & emotion understanding . Other reasoning Critical thinking . Counterfactual robustness . Problem decomposition . Comparative reasoning ."
        },
        {
            "title": "D Data leakage details",
            "content": "Goal. Given multimodal example = (t, m) where is the paired modality (image/video/audio), is the text, estimate the probability that target model was trained on x. D.1 Setup. We begin by creating controlled environment to simulate data leakage. For given modality, we take base model, odel. We then create leaked version, odelleak, by fine-tuning the base model on subset of our benchmark data using SFT-LoRA for the selected modality. L1 L2 Taxonomy Level L4 Inductive reasoning Attribute recognition Coarse attribute recognition Object attribute recognition inOther ductive reasoning Deductive reasoning inOther ductive reasoning Deductive reasoning Deductive reasoning Abductive reasoning Abductive reasoning Abductive reasoning n e L5 Generated content detection Source characterization Media characteristic understanding Speech emotion recognition Music emotion recognition Melodic structure interpretation Topic understanding Style & genre understanding Scene understanding Physical property understanding Object function understanding Identity & emotion understanding Other inductive reasoning Weirdness understanding Analogical reasoning Other deductive reasoning Hypothetical reasoning Cause & effect understanding Static counting Counting Quantitative reasoning Quantitative reasoning Temporal counting Mathematical reasoning Mathematical reasoning Critical thinking Counterfactual robustness reaOther soning reaOther soning reaOther soning Problem decomposition Comparative reasoning Image Modality Audio Video AQUARIA AQUARIA ruTiE-Audio AQUARIA ruTiE-Audio ruEnvAQA AQUARIA CommonVideoQA RealVideoQA ruHHH-Video ruEnvAQA AQUARIA CommonVideoQA RealVideoQA ruEnvAQA AQUARIA ruTiE-Audio AQUARIA CommonVideoQA RealVideoQA ruHHH-Video CommonVideoQA RealVideoQA CommonVideoQA RealVideoQA ruTiE-Audio ruEnvAQA AQUARIA CommonVideoQA RealVideoQA ruTiE-Audio CommonVideoQA RealVideoQA ruEnvAQA AQUARIA ruTiE-Image RealVQA ruCommonVQA RealVQA ruHHH-Image ruTiE-Image ruCommonVQA ruCLEVR RealVQA UniScienceVQA SchoolScienceVQA WEIRD ruCommonVQA RealVQA ruHHH-Image WEIRD ruCommonVQA WEIRD WEIRD ruTiE-Image RealVQA RealVQA LabTabVQA ruCommonVQA ruCLEVR ruNaturalScienceVQA RealVQA UniScienceVQA SchoolScienceVQA ruTiE-Image ruMathVQA ruNaturalScienceVQA RealVQA UniScienceVQA SchoolScienceVQA ruTiE-Image RealVQA ruMathVQA ruNaturalScienceVQA RealVQA UniScienceVQA SchoolScienceVQA RealVQA UniScienceVQA SchoolScienceVQA Table 8: Skill taxonomy coverage of MERA Multi tasks (reasoning part). Columns L1-L5 show the skill hierarchy levels, while Image, Audio and Video columns indicate which tasks cover each knowledge type across different modalities. D.2 Neighbor Generation and Feature Extraction. For each original data point (t, m) we generate = 24 perturbed \"neighbors\". We apply four distinct perturbation techniques (masking and predicting the masks with Fred-T5 model20, deletion, duplication, and swapping of random words) to the text with each technique applied 6 times. The modality data remains unchanged. For each original text and its neighbors we extract their text embeddings using fixed encoder: = E(t), = E(t k) 20https://hf.co/ai-forever/FRED-T5-1.7B, (Zmitrovich et al., 2024) where is intfloat/e5-mistral-7b-instruct21 model. Subsequently, we compute the multimodal loss for both models odel and odelleak on both the original and neighbor data points: = L(M odel, t, m), k = L(M odel, k, m) D.3 Detector Training The core of MSMIA is binary classifier trained to distinguish between models that have and have not seen the data. For each neighbor we create 21https://hf.co/intfloat/ e5-mistral-7b-instruct. MTEB benchmark (Muennighoff et al., 2022) It used to be SoTA on 34 two training examples by computing the feature differences: = k, = These feature vectors are paired with labels {0, 1} indicating whether the losses came from odel (y = 0) or odelleak (y = 1). This process yields 48 training triplets (L, e, y) per original data point. The MSMIA detector, fM SM IA is trained to predict the probability = fM SM IA(L, e) that the input features originate from model that has been trained on the target data. D.4 Inference and Evaluation To infer if target estM odel has been trained on specific data point (t, m), we repeat Step 2 to compute the loss and embedding differences for this model. We then compute the leakage score for the data point by taking the average probability output by the detector over all neighbors: S(t, m) ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 fM SM IA(Lk, ek) To get the probability estimation for the entire dataset, the scores are averaged over all dataset samples. We report AUC-ROC for binary classification (leaked vs. clean) as shown in Tables 9, 11, 10. There the Origin Model is the model used to train MSMIA. Test Model is the model whose losses are used to test MSMIA (predict whether the data sample was used to train Test Model or not). LLM-as-a-judge details E.1 Data collection Our dataset comprises (question, gold answer, model prediction) triplets sourced from Russianlanguage benchmarks, including MERA Multi, with model sizes spanning 2B to 110B parameters. Human annotators label the semantic correctness of each prediction, strictly ignoring surface form. To ensure label quality, only items with 100% interannotator agreement are used for model training and testing. We apply synthetic augmentations (e.g., refusals, repetitions) to improve robustness. critical measure to prevent bias was splitting the data by source dataset, ensuring no dataset appears in both train and test splits. The final dataset composition and class balance are detailed in Table 12. Origin Model Test Model AUC-ROC Qwen2.5-VL-3B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-3B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-7B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-7B-Instruct llama3-llava-next-8b-hf llama3-llava-next-8b-hf llama3-llava-next-8b-hf llama3-llava-next-8b-hf llama3-llava-next-8b-hf llama3-llava-next-8b-hf gemma-3-4b-it gemma-3-4b-it gemma-3-4b-it gemma-3-4b-it gemma-3-4b-it gemma-3-4b-it gemma-3-12b-it gemma-3-12b-it gemma-3-12b-it gemma-3-12b-it gemma-3-12b-it gemma-3-12b-it Qwen2.5-VL-3B-Instruct Qwen2-VL-7B-Instruct Qwen2.5-VL-7B-Instruct llama3-llava-next-8b-hf gemma-3-4b-it gemma-3-12b-it Qwen2.5-VL-3B-Instruct Qwen2-VL-7B-Instruct Qwen2.5-VL-7B-Instruct llama3-llava-next-8b-hf gemma-3-4b-it gemma-3-12b-it Qwen2.5-VL-3B-Instruct Qwen2-VL-7B-Instruct Qwen2.5-VL-7B-Instruct llama3-llava-next-8b-hf gemma-3-4b-it gemma-3-12b-it Qwen2.5-VL-3B-Instruct Qwen2-VL-7B-Instruct Qwen2.5-VL-7B-Instruct llama3-llava-next-8b-hf gemma-3-4b-it gemma-3-12b-it Qwen2.5-VL-3B-Instruct Qwen2-VL-7B-Instruct Qwen2.5-VL-7B-Instruct llama3-llava-next-8b-hf gemma-3-4b-it gemma-3-12b-it Qwen2.5-VL-3B-Instruct Qwen2-VL-7B-Instruct Qwen2.5-VL-7B-Instruct llama3-llava-next-8b-hf gemma-3-4b-it gemma-3-12b-it 96.2 86.0 88.0 90.2 65.8 67.9 78.0 96.2 80.5 78.0 77.7 73.7 92.8 93.1 98.1 95.8 95.4 94.5 94.6 90.0 96.6 97.7 99.1 99.5 76.0 71.5 85.2 86.5 99.4 98.7 84.1 81.3 91.2 93.3 99.4 99.7 Table 9: AUC-ROC MSMIA performance metrics for various evaluated Image MLLMs. E.2 Training models The tasks are formulated as binary classification problems. Inputs are linearized into the format question [SEP] gold answer [SEP] model prediction and packed into the maximum models contexts. We train encoder-based models, adding linear classification head on top of the pre-trained backbone. The models are fully finetuned and optimized using cross-entropy loss, with class weights applied to mitigate potential dataset imbalance. Optimization is performed with mixed precision and early stopping based on the F1 score on the development set, constrained to single A100 80GB GPU. summary of the final training configuration is provided in Table 13. E.3 Model selection We compare fine-tuned encoder-based models against zero-shot decoder baselines to identify the optimal judge architecture. The decoder models are prompted to output either 0 or 1 without any task-specific fine-tuning. While this approach is flexible, its reliability is limited by the fact that decoder generations are unconstrained; they may 35 Origin Model Test Model AUC-ROC Qwen2.5-VL-3B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-7B-Instruct LLaVA-NeXT-Video LLaVA-NeXT-Video LLaVA-NeXT-Video LLaVA-NeXT-Video LLaVA-NeXT-Video-DPO LLaVA-NeXT-Video-DPO LLaVA-NeXT-Video-DPO LLaVA-NeXT-Video-DPO Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct LLaVA-NeXT-Video LLaVA-NeXT-Video-DPO Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct LLaVA-NeXT-Video LLaVA-NeXT-Video-DPO Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct LLaVA-NeXT-Video LLaVA-NeXT-Video-DPO Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct LLaVA-NeXT-Video LLaVA-NeXT-Video-DPO 95.9 99.5 91.7 91.2 98.7 100.0 96.5 95.7 63.7 71.5 100.0 100.0 53.6 56.2 100.0 100.0 Table 10: AUC-ROC MSMIA performance metrics for various evaluated Video MLLMs. Origin Model Test Model AUC-ROC Qwen2-Audio-7B-Instruct Qwen2-Audio-7B-Instruct Qwen-Audio-Chat Qwen-Audio-Chat Qwen2-Audio-7B-Instruct Qwen-Audio-Chat Qwen2-Audio-7B-Instruct Qwen-Audio-Chat 87.7 76.0 61.3 100.0 Table 11: AUC-ROC MSMIA performance metrics for various evaluated Audio MLLMs. Total Open Generation Multiple Choice Train set Test set 62,580 8,570 22,538 5,819 40,042 2, Table 12: Overview of the datasets used for training and evaluating the judge model. The columns indicate the total number of examples and their distribution across two task formats: Open Generation (free-form model response) and Multiple Choice (discrete answer selection) Parameter Learning Rate Batch Size Number of Epochs Weight Decay Optimizer LR Scheduler Max Sequence Length Precision Early Stopping Metric Value 2e-5 32 3 0.01 AdamW Linear 512 BF16 F1 Score Table 13: embedding-based classification Training configuration summary for not always produce valid classification, making score interpretation ambiguous. In our experiments, encoder models proved to be more suitable for this binary classification task. Their architectural design naturally provides probability distributions over the two classes (correct/incorrect), ensuring deterministic scoring. Furthermore, they offer significant practical advantages, 36 being generally smaller, faster on single GPU, and even capable of handling long contexts (useful for judging the answers with reasoning or chain-ofthought elements). full model comparison with bootstrap statistics is provided in Table 14. E.4 Model deployment The selected judge model is deployed using vLLM for high-throughput inference on single A100 40GB GPU. This provides optimal batch processing speed for rapid benchmark evaluation while maintaining quality. Our codebase implements score(q, ref, pred) {0,1} API that handles individual scoring and aggregation. We also publicly provide the trained model with weights on HuggingFace Hub 22."
        },
        {
            "title": "F Block prompts analysis",
            "content": "F.1 Prompts formulations There are 13 blocks: Attention hook (greeting / draw attention). General task description (task specific). Input data description (enumerate modalities). Action on data (e.g., Solve the task using the images. . . ). Optional task specifics (helpful but nonessential context). Textual question. Answer options (if multiple choice). Call to solve (explicit request to answer). Reasoning request (ask for thinking before final answer; present in 5 prompts). Reasoning format (how to present the reasoning). Answer format (how to present the final answer). Time limitation (e.g., you have 10 minutes). Final call to action (e.g., get started). The blocks are combined with the Python script, following the task prompts configuration file. This file contains an explicit description of all ten prompts. Example of the prompt configuration: # prompt_config . yaml prompt_4 : attention_hook : \" informal_request \" task_description : \" informal_request \" input_data : \" default \" processing_data : \" informal_request \" context_intro : \" in_dataset \" task_context : \" default \" question : \" default \" 22https://huggingface.co/MERA-evaluation/MERA_ Answer_judge Model Parameters Context Length Samples/sec F1 Recall Precision Pearson EM rate e n e e RuModernBERT-base RuModernBERT-small Qwen3-Embedding-0.6B FRIDA embeddinggemma-300m Giga-Embeddings-instruct pollux-judge-7b T-lite-it-1.0 Qwen3-0.6B Qwen3-1.7B gpt-oss-20b 150M 35M 600M 823M 303M 3.45B 7.61B 7.61B 752M 2.03B 21.5B 8,192 8,192 32,768 512 2,048 4,096 131072 131072 32768 32768 131072 1800 2000 1850 70* 250* 40* 20 52 442 205 28 0.964 0.002 0.944 0.003 0.951 0.003 0.838 0.005 0.915 0.004 0.966 0.002 0.823 0.005 0.844 0.005 0.030 0.003 0.590 0.006 0.939 0. 0.975 0.962 0.985 0.913 0.965 0.984 0.788 0.968 0.016 0.990 0.968 0.955 0.927 0.920 0.774 0.871 0.948 0.860 0.747 0.323 0.420 0.912 0.940 0.879 0.936 0.765 0.825 0.946 0.732 0.755 -0.010 0.300 0. 0.997 0.982 1.000 0.925 0.997 1.000 0.873 0.993 0.004 0.997 1.000 Table 14: Summary results for judge models on the test set. F1, Recall and Precision report binary classification quality; Pearson is the Pearson correlation between binary predictions and ground-truth labels; EM rate is the share of class-1 predictions on the subset of examples with an exact string match between the prediction and the gold answer. An asterisk (*) next to throughput indicates the model could not be served via vLLM and the speed was measured with the HuggingFace Trainer instead The dataset for the task includes the following prompt : Not all slots are necessarily present in the request . Audio file : < audio > Question : { question } Please solve the task based on the above and briefly formulate your answer . { annotation } answer_options : \" default \" solution_motivation : \" informal_request \" reasoning_motivation : \" none \" reasoning_format : \" none \" answer_format : \" informal_request \" limitations : \" informal_request \" answer_motivation : \" informal_request \" The block formulations are fixed. The only variable blocks are task_description and task_context that are task-specific. All prompt formulations are imputed in the dataset on HuggingFace Hub (key instruction of each data sample). The blocks distribution is fixed: 5 prompts with reasoning_motivation and reasoning_format blocks to enable answer rationale, 1 prompt with no answer_format (zero prompt that provides the minimal version of the task - only placeholders for multimodal data, the question and answer options if any)."
        },
        {
            "title": "The example of the prompt with all blocks are",
            "content": "as follows. Zero prompt: Image : < image > Question : { question } Prompt with specific answer format (RuSLUn): Example of the prompt with reasoning blocks: The dataset for the task includes the following prompt : The question is directly related to the content of the image and requires not only recognition of individual elements , but also understanding of the relationships between the elements ( objects ) in the image . If there is insufficient information to answer the question , for example , if the object in question is missing from the image , then you must honestly answer that the question cannot be answered and indicate the reason . Image : < image > Question : { question } Please solve the task based on the above and briefly formulate your answer . Please think about the solution and describe your thought process in detail . Write your reasoning after the word REASONING , briefly explaining how you arrived at your final answer . Please provide brief answer to the question . Please do not write anything else , do not elaborate , do not engage in dialogue , and do not explain your answer . Please write your final answer after the word ANSWER . F.2 Statistical analysis To analyze the effect that prompt formulation has on the metrics, we fit OLS with the following specification: metric C(prompt) + C(model) + C(engine) (5) In fact, it is essentially the same as: metrici = α + 10 (cid:88) p=2 βp{prompt = p}+ (cid:88) γm{model = m} + (cid:88) δe{engine = e} Where: (6) 38 Prompt is specific prompt formulation (one of 10). Model is the model name - the model used to infer the data with the prompt and the metric metric. Engine is the inference backend used for evaluation (one of transformers (Wolf et al., 2020) or vllm23). used Additional only Frames. Additional categorical variable used only for video modality evaluations - the video is uniformly split into frames and the vision LLM used to make evaluation on the dataset. Domain. categorical variable for UniScienceVQA, SchoolScienceVQA, ruCommonVQA. These into separate domains datasets are split (subsets). This split may affect the metrics (one domain may be harder than another). For each prompt p, we test H0 : βp = 0 against H1 : βp = 0. We mark prompts effect as statistically significant when the two-sided p-value < 0.05. Figure 2 demonstrates the results of statistical analysis of the prompts formultations effects on the Judge Score metric. There are three datasets that have been omitted: RuSLUn implies structured output with rather strict metric that tends to show zero score, ruTiE-Audio and ruTiE-Image datasets scores are too sensible to prompt formulation due to the datasets design24 Takeaways: No single prompt dominates. Different datasets favor different formulations; the prompt that helps in one case can hurt in another. This undermines any one-prompt-fitsall strategy. Magnitude varies widely across datasets. Some tasks show large shifts (on the order of 0.10.2 in the metric), while others exhibit near-zero effects (often still significant when variance is low). Prompt sensitivity is therefore task-dependent. Reasoning format does not mean lower scores. Prompts that explicitly encourage model to provide chain-of-though rationale for the final answer do not always get lower 23https://github.com/vllm-project/vllm 24Both datasets consist of three sequential dialogues of 500 questions in row. The answer for the question lays in the text of the questions > X. This way we cannot reliably separate this effect from the pure effect of the prompt formulation which may lead to incorrect analysis and conclusions. Figure 2: The relative (with regard to baseline prompt (0)) effects of different formulations of prompts for each dataset. There are ten different formulations of prompts for one dataset, hence nine corresponding bars (one formulation is baseline category). Red bars reflect statistically significant (at 95% confidence level) effects. 39 scores. First, this means that the LLM-asjudge model is capable of coping with reasoning format. Second, some models breach the answer format prescribed in the task instruction. Single prompt leads to bias. Only four tasks (notable, three of them are from video modality) demonstrate no red bars - no statistically significant prompt formulations compared to the baseline one. The other 12 datasets tend to be prompt sensitive, so the design choice to distribute prompts uniformly and interpret dataset metrics as an average over formulations is empirically supported."
        },
        {
            "title": "G Baselines Details",
            "content": "G.1 Model Baselines Details In this section, the list of baseline models is provided. Tables 15, 16, 17 represent models for image, audio, and video modalities respectively. The exact results of the finished submissions of the models from Table 15 are presented in Tables 18, 19. The results of the models from Table 16 are presented in Table 20. The evaluation results of the models from Table 17 are stated in Table 21. Evaluated on vision (image) modality models are strongest on natural-image semantics object/scene understanding, object functions, and everyday knowledge (RealVQA, ruCommonVQA, WEIRD, ruTiE-Image) and show decent spatial relations and multi-object reasoning (RealVQA, ruCLEVR). Performance drops on diagrammatic/- scientific QA with OCR (UniScienceVQA, ruNaturalScienceVQA) and tables (LabTabVQA), exposing gaps in text extraction, scheme recognition, and cell-level grounding. Math (ruMathVQA) is brittle: EM lags JS, indicating partially correct solutions that miss the required final answer. Harder compositional and counterfactual reasoning (RealVQA, ruCommonVQA) still separates model tiers, especially smaller checkpoints. Overall, weaknesses concentrate in OCR/diagram/table parsing and structured compositional reasoning, while strengths lie in natural-image commonsense and basic spatial reasoning. Moving to audio modality evaluations, the relative strengths cluster around acoustic scene understanding and temporal/comparative reasoning over environmental audio (ruEnvAQA), with partial competence in social/interaction cues and topic/scene grounding (AQUARIA, ruTiE-Audio) captured by higher JS. Weaknesses are pronounced in speech recognition (RuSLUn, ruTiE-Audio EM), speaker/turn handling (diarization), and formatfaithful final answer extraction. Improving ASR robustness, speaker attribution, and constrained decoding or answer templates should convert many high-JS outputs into EM gains. As for the video modality evaluations, the current relative strengths lie in scene/object recognition and short event recognition. Clear weaknesses persist in temporal perception/localization, action-sequence reasoning, mutual object localization, counting, and cause-and-effectskills central to CommonVideoQA/RealVideoQA. Low ruHHHVideo scores further reveal brittleness on ethicsaware interpretation and social context. Practically, improving temporal grounding (longer context windows or frame selection), causal reasoning, and answer-format control (constrained decoding/final-answer extraction) should convert many near-misses (high JS, low EM) into measurable EM gains. G.2 Human Baseline Details Human baseline values were obtained by evaluating the aggregate responses of annotators on control tasks. Prior to metric calculation, we conducted an additional identification of annotators who performed labeling tasks with low quality. To identify such annotators during the labeling process, we employed control tasks with automated correctness checking; incorrect answers reduced the annotators skill score. During post-processing, we filtered out annotators who consistently made errors and whose responses did not align with the majority vote across all their submissions."
        },
        {
            "title": "To evaluate the average human level we collected",
            "content": "crowd-source25. For expert tasks, in addition to the basic human baseline, we also performed expert annotation. We aggregate experts answers with an overlap of 3 for expert human baseline scores 26. The experts annotating answers for the human baseline had not been involved in the original dataset creation. The ABC Elementary annotation platform ensures the necessary data anonymity during processing. The hourly compensation offered is above the 25Annotation provided by ABC Elementary platform https://app.elementary.center 26The observed quantity is consequence of the limited pool of domain experts in niche specializations, who also had no prior involvement in the annotation of the datasets. 40 Model GPT 4.1 Phi-3.5-vision-instruct Phi-4-multimodal-instruct Qwen2.5-Omni-3B Qwen2.5-Omni-7B Qwen2-VL-2B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-72B-Instruct Qwen3-VL-2B-Instruct Qwen3-VL-8B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-32B-Instruct Qwen2.5-VL-72B-Instruct gemma-3-12b-it llava-1.5-13b-hf llava-next-72b-hf llava-next-110b-hf InternVL3-9B granite-vision-3.3-2b SmolVLM-Instruct MiniCPM-o-2_6 Parameters Context length Hugging Face Hub link Citation N/A 4B 6B 6B 11B 2B 7B 72B 2B 9B 3B 7B 32B 72B 12B 13B 72B 110B 9B 3B 2B 9B 1000K GPT 4.1 128K microsoft/Phi-3.5-vision-instruct 128K microsoft/Phi-4-multimodal-instruct Microsoft et al. (2025) Abdin et al. (2024) 32K Qwen/Qwen2.5-Omni-3B 32K Qwen/Qwen2.5-Omni-7B Xu et al. (2025) 32K Qwen/Qwen2-VL-2B-Instruct 32K Qwen/Qwen2-VL-7B-Instruct 32K Qwen/Qwen2-VL-72B-Instruct 262K Qwen/Qwen3-VL-2B-Instruct 262K Qwen/Qwen3-VL-8B-Instruct 128K Qwen/Qwen2.5-VL-3B-Instruct 128K Qwen/Qwen2.5-VL-7B-Instruct 128K Qwen/Qwen2.5-VL-32B-Instruct 128K Qwen/Qwen2.5-VL-72B-Instruct Wang et al. (2024) Yang et al. (2025) Bai et al. (2025) 128K google/gemma-3-12b-it Team et al. (2025a) 4K llava-hf/llava-1.5-13b-hf Liu et al. (2024b) 8K llava-hf/llava-next-72b-hf 32K llava-hf/llava-next-110b-hf Li et al. (2024) 32K OpenGVLab/InternVL3-9B Zhu et al. (2025) 128K ibm-granite/granite-vision-3.3-2b Team et al. (2025b) 16K HuggingFaceTB/SmolVLM-Instruct Marafioti et al. (2025) 32K openbmb/MiniCPM-o-2_6 Yao et al. (2024) Table 15: General information about vision (image) modality baseline models. minimum wage per hour in Russia (see Table 22). Annotators are made aware of potentially sensitive topics within the data, including politics, societal minorities, and religion. The data collection process undergoes mandatory quality evaluation, featuring an automated annotation quality check through honeypot tasks. 41 Model Parameters Context length Hugging Face Hub link Citation Qwen3-Omni-30B-A3BInstruct Qwen2.5-Omni-3B Qwen2.5-Omni-7B Qwen2-Audio-7B-Instruct Qwen2-Audio-7B audio-flamingo-3-hf SeaLLMs-Audio-7B ultravox-v0_2 ultravox-v0_3 ultravox-v0_3-llama-3_2-1b ultravox-v0_4 ultravox-v0_4_1-llama-3_1-8b ultravox-v0_4_1-mistral-nemo ultravox-v0_5-llama-3_2-1b ultravox-v0_5-llama-3_1-8b ultravox-v0_6-llama-3_1-8b ultravox-v0_6-qwen-3-32b 35B 6B 11B 7B 8B 9B 8B 8B 8B 2B 8B 8B 13B 2B 8B 8B 32B 8K Qwen/Qwen3-Omni-30B-A3B-Instruct 32K Qwen/Qwen2.5-Omni-3B 32K Qwen/Qwen2.5-Omni-7B 32K Qwen/Qwen2-Audio-7B-Instruct 32K Qwen/Qwen2-Audio-7B Xu et al. (2025) Chu et al. (2024) 32K nvidia/audio-flamingo-3-hf Goel et al. (2025) 8K SeaLLMs/SeaLLMs-Audio-7B Zhang et al. (2024) 8K fixie-ai/ultravox-v0_2 8K fixie-ai/ultravox-v0_3 128K fixie-ai/ultravox-v0_3-llama-3_2-1b 8K fixie-ai/ultravox-v0_ 128K fixie-ai/ultravox-v0_4_1-llama-3_1-8b 128K fixie-ai/ultravox-v0_4_1-mistral-nemo 128K fixie-ai/ultravox-v0_5-llama-3_2-1b 128K fixie-ai/ultravox-v0_5-llama-3_1-8b 128K fixie-ai/ultravox-v0_6-llama-3_1-8b 40K fixie-ai/ultravox-v0_6-qwen-3-32b Team (2024) Table 16: General information about audio modality baseline models. Model Parameters Context length Hugging Face Hub link Citation LLaVA-NeXT-Video-7B-hf Qwen2-VL-2B-Instruct Qwen2-VL-7B-Instruct Qwen2-VL-72B-Instruct Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Qwen3-VL-2B-Instruct Qwen3-VL-8B-Instruct Qwen2.5-Omni-3B Qwen2.5-Omni-7B MiniCPM-o-2_ InternVL3_5-4B InternVL3_5-2B-Instruct InternVL3-9B-Instruct InternVL3-9B InternVL3-2B 7B 2B 7B 72B 3B 7B 72B 2B 9B 6B 11B 9B 5B 2B 9B 9B 1B 4K llava-hf/LLaVA-NeXT-Video-7B-hf Liu et al. (2024c) 32K Qwen/Qwen2-VL-2B-Instruct 32K Qwen/Qwen2-VL-7B-Instruct 32K Qwen/Qwen2-VL-72B-Instruct 128K Qwen/Qwen2.5-VL-3B-Instruct 128K Qwen/Qwen2.5-VL-7B-Instruct 128K Qwen/Qwen2.5-VL-72B-Instruct 262K Qwen/Qwen3-VL-2B-Instruct 262K Qwen/Qwen3-VL-8B-Instruct 32K Qwen/Qwen2.5-Omni-3B 32K Qwen/Qwen2.5-Omni-7B Wang et al. (2024) Bai et al. (2025) Yang et al. (2025) Xu et al. (2025) 32K openbmb/MiniCPM-o-2_6 Yao et al. (2024) 40K OpenGVLab/InternVL3_5-4B 40K OpenGVLab/InternVL3_5-2BInstruct 32K OpenGVLab/InternVL3-9B 32K OpenGVLab/InternVL3-9B 40K OpenGVLab/InternVL3-2B Wang et al. (2025b) Zhu et al. (2025) Zhu et al. (2025) Table 17: General information about video modality baseline models. 42 Model Total LabTabVQA RealVQA ruCLEVR ruCommonVQA ruHHH-Image* ruMathVQA Human Baseline 0.80 0.91 0.63 0. 0.84 0.89 0.95 0.431 GPT 4 Qwen2.5-VL-72B-Inst 0.353 Qwen2-VL-72B-Inst 0.332 Qwen2.5-VL-32B-Inst 0.242 Qwen2.5-VL-7B-Inst 0.238 0.235 llava-next-110b-hf llava-next-72b-hf 0.226 0.226 Phi-3.5-vision-inst 0.211 Qwen2.5-Omni-7B 0.195 Qwen2-VL-7B-Inst SmolVLM-Inst 0.189 Qwen3-VL-8B-Inst 0.184 Phi-4-multimodal-inst 0.183 0.177 MiniCPM-o-2_6 Qwen2.5-Omni-3B 0.172 0.169 InternVL3-9B 0.165 Qwen2-VL-2B-Inst 0.146 gemma-3-27b-it granite-vision-3.3-2b 0.142 Qwen2.5-VL-3B-Inst 0.138 0.124 Qwen3-VL-2B-Inst 0.113 llava-1.5-13b-hf 0.23 / 0.5 0.31 / 0.4 0.29 / 0.29 0.53 / 0.32 0.21 / 0.48 0.4 / 0.49 0.1 / 0.47 0.29 / 0.55 0.12 / 0.14 0.47 / 0.22 0.16 / 0.36 0.33 / 0.4 0.19 / 0.19 0.11 / 0.25 0.23 / 0.3 0.09 / 0.29 0.11 / 0.2 0.06 / 0.1 0.09 / 0.11 0.07 / 0.28 0.09 / 0.18 0.12 / 0.17 0.03 / 0.09 0.08 / 0.15 0.11 / 0.15 0.08 / 0.29 0.13 / 0.29 0.01 / 0.13 0.05 / 0.36 0.1 / 0.34 0.01 / 0.16 0.03 / 0.21 0.11 / 0.33 0.11 / 0.43 0.14 / 0.32 0.0 / 0.07 0.26 / 0.34 0.11 / 0.28 0.09 / 0.19 0.04 / 0.06 0.08 / 0.29 0.15 / 0.31 0.04 / 0.12 0.07 / 0.21 0.16 / 0.26 0.01 / 0.04 0.04 / 0.26 0.19 / 0.37 0.06 / 0.28 0.11 / 0.32 0.0 / 0.09 0.05 / 0.38 0.04 / 0.17 0.0 / 0.01 0.01 / 0.1 0.0 / 0.08 0.0 / 0.08 0.03 / 0.12 0.13 / 0.25 0.02 / 0.1 0.06 / 0.34 0.11 / 0.26 0.0 / 0.03 0.01 / 0.1 0.01 / 0.08 0.01 / 0.14 0.27 / 0.77 0.43 / 0.73 0.14 / 0.6 0.3 / 0.7 0.33 / 0.52 0.31 / 0.58 0.21 / 0.49 0.21 / 0.4 0.23 / 0.49 0.19 / 0.5 0.14 / 0.51 0.23 / 0.58 0.07 / 0.35 0.26 / 0.49 0.22 / 0.45 0.09 / 0.44 0.12 / 0.44 0.07 / 0.44 0.11 / 0.2 0.15 / 0.3 0.14 / 0.47 0.1 / 0. 0.39 / 0.21 0.35 / 0.16 0.14 / 0.19 0.26 / 0.1 0.2 / 0.26 0.12 / 0.18 0.11 / 0.15 0.19 / 0.24 0.14 / 0.18 0.02 / 0.16 0.14 / 0.22 0.05 / 0.15 0.04 / 0.08 0.06 / 0.09 0.04 / 0.19 0.0 / 0.07 0.03 / 0.09 0.0 / 0.02 0.08 / 0.19 0.03 / 0.16 0.05 / 0.08 0.02 / 0.25 0.01 / 0.17 0.02 / 0.23 0.01 / 0.06 0.0 / 0.12 0.02 / 0.1 0.01 / 0.01 0.0 / 0.0 0.01 / 0.02 0.03 / 0.08 0.01 / 0.02 0.02 / 0.03 0.04 / 0.1 0.04 / 0.07 0.0 / 0.0 0.01 / 0.04 0.01 / 0.02 0.04 / 0.05 0.01 / 0.04 0.02 / 0.02 0.0 / 0.04 0.01 / 0.04 0.01 / 0.0 Table 18: Image modality evaluation results (6 tasks out of 11). All tasks metrics are Exact Match / Judge Score. For ruHHH-Image dataset the metrics are Group Exact Match / Group Judge Score. For Human Baseline aggregated results are provided (average of EM and JudgeScore). Model Total ruNaturalScienceVQA SchoolScienceVQA UniScienceVQA WEIRD ruTiE-Image Human Baseline 0.80 0.99 0.431 GPT 4 Qwen2.5-VL-72B-Inst 0.353 Qwen2-VL-72B-Inst 0.332 Qwen2.5-VL-32B-Inst 0.242 Qwen2.5-VL-7B-Inst 0.238 0.235 llava-next-110b-hf 0.226 llava-next-72b-hf 0.226 Phi-3.5-vision-inst 0.211 Qwen2.5-Omni-7B 0.195 Qwen2-VL-7B-Inst SmolVLM-Inst 0.189 Qwen3-VL-8B-Inst 0.184 Phi-4-multimodal-inst 0.183 0.177 MiniCPM-o-2_6 Qwen2.5-Omni-3B 0.172 0.169 InternVL3-9B 0.165 Qwen2-VL-2B-Inst 0.146 gemma-3-27b-it granite-vision-3.3-2b 0.142 Qwen2.5-VL-3B-Inst 0.138 0.124 Qwen3-VL-2B-Inst llava-1.5-13b-hf 0.113 0.64 / 0.55 0.01 / 0.05 0.25 / 0.4 0.0 / 0.03 0.13 / 0.17 0.25 / 0.34 0.19 / 0.27 0.32 / 0.53 0.09 / 0.21 0.04 / 0.38 0.19 / 0.36 0.03 / 0.08 0.05 / 0.12 0.07 / 0.19 0.07 / 0.22 0.17 / 0.24 0.04 / 0.32 0.05 / 0.1 0.27 / 0.38 0.02 / 0.26 0.01 / 0.04 0.01 / 0.34 0.82 0.59 / 0.54 0.24 / 0.28 0.54 / 0.67 0.0 / 0.03 0.16 / 0.3 0.25 / 0.45 0.4 / 0.47 0.22 / 0.32 0.12 / 0.22 0.03 / 0.52 0.11 / 0.27 0.14 / 0.19 0.3 / 0.4 0.2 / 0.32 0.09 / 0.22 0.37 / 0.4 0.04 / 0.38 0.49 / 0.46 0.08 / 0.22 0.04 / 0.21 0.14 / 0.19 0.01 / 0. 0.13 0.85 0.77 0.1 / 0.4 0.11 / 0.25 0.18 / 0.33 0.01 / 0.09 0.06 / 0.13 0.06 / 0.13 0.07 / 0.13 0.07 / 0.11 0.06 / 0.12 0.09 / 0.19 0.06 / 0.09 0.1 / 0.22 0.08 / 0.15 0.01 / 0.1 0.06 / 0.12 0.06 / 0.16 0.08 / 0.14 0.04 / 0.27 0.04 / 0.07 0.05 / 0.11 0.05 / 0.11 0.0 / 0.09 0.69 / 0.78 0.65 / 0.66 0.31 / 0.48 0.47 / 0.51 0.36 / 0.56 0.24 / 0.33 0.27 / 0.33 0.41 / 0.6 0.3 / 0.56 0.08 / 0.38 0.36 / 0.44 0.08 / 0.17 0.02 / 0.07 0.2 / 0.24 0.14 / 0.58 0.01 / 0.16 0.09 / 0.31 0.0 / 0.12 0.2 / 0.52 0.09 / 0.47 0.03 / 0.1 0.01 / 0.48 0.7 / 0.63 0.63 / 0.55 0.65 / 0.69 0.43 / 0.31 0.24 / 0.45 0.54 / 0.55 0.52 / 0.55 0.33 / 0.34 0.28 / 0.46 0.11 / 0.58 0.14 / 0.24 0.39 / 0.41 0.44 / 0.46 0.34 / 0.4 0.13 / 0.35 0.21 / 0.4 0.16 / 0.44 0.21 / 0.24 0.2 / 0.26 0.08 / 0.35 0.21 / 0.25 0.03 / 0. Table 19: Image modality evaluation results (5 tasks out of 11). All tasks metrics are Exact Match / Judge Score. For UniScienceVQA the Human Baseline is crowd, not an expert. For Human Baseline aggregated results are provided (average of EM and JudgeScore). 43 Model Human Baseline Total AQUARIA ruEnvAQA ruTiE-Audio ruSLUn 0,895 0.98 0.95 0.75 0.91 Qwen/Qwen3-Omni-30B-A3B-Instruct 0.531 0.69 / 0.66 0.464 0.55 / 0.64 Qwen/Qwen2.5-Omni-7B 0.377 0.41 / 0.57 Qwen/Qwen2.5-Omni-3B fixie-ai/ultravox-v0_4 0.3 / 0.28 0.268 0.266 fixie-ai/ultravox-v0_5-llama-3_1-8b 0.3 / 0.26 0.264 0.29 / 0.28 fixie-ai/ultravox-v0_4_1-llama-3_1-8b 0.259 0.21 / 0.42 nvidia/audio-flamingo-3-hf fixie-ai/ultravox-v0_4_1-mistral-nemo 0.257 0.18 / 0.32 0.247 0.29 / 0.21 fixie-ai/ultravox-v0_6-llama-3_1-8b 0.242 0.29 / 0.26 fixie-ai/ultravox-v0_3 0.223 0.18 / 0.43 Qwen/Qwen2-Audio-7B-Instruct 0.173 0.39 / 0.04 fixie-ai/ultravox-v0_6-qwen-3-32b fixie-ai/ultravox-v0_2 0.139 0.01 / 0.24 0.116 0.01 / 0.27 Qwen/Qwen-Audio-Chat 0.103 0.08 / 0.14 SeaLLMs/SeaLLMs-Audio-7B 0.102 0.04 / 0.23 fixie-ai/ultravox-v0_5-llama-3_2-1b fixie-ai/ultravox-v0_3-llama-3_2-1b 0.06 / 0. 0.1 0.7 / 0.72 0.55 / 0.65 0.36 / 0.64 0.34 / 0.31 0.34 / 0.24 0.35 / 0.24 0.41 / 0.59 0.27 / 0.39 0.26 / 0.21 0.35 / 0.3 0.15 / 0.52 0.37 / 0.02 0.0 / 0.38 0.01 / 0.39 0.02 / 0.2 0.08 / 0.26 0.08 / 0.27 0.43 / 0.37 0.36 / 0.4 0.3 / 0.35 0.29 / 0.24 0.29 / 0.24 0.31 / 0.24 0.15 / 0.28 0.18 / 0.33 0.3 / 0.23 0.27 / 0.22 0.17 / 0.28 0.47 / 0.09 0.01 / 0.25 0.01 / 0.23 0.14 / 0.23 0.05 / 0.16 0.05 / 0.13 0.39 / 0.28 0.37 / 0.18 0.35 / 0.04 0.3 / 0.09 0.31 / 0.15 0.28 / 0.11 0.0 / 0.0 0.26 / 0.14 0.31 / 0.16 0.22 / 0.04 0.05 / 0.0 0.0 / 0.0 0.23 / 0.0 0.0 / 0.0 0.0 / 0.01 0.0 / 0.0 0.0 / 0.0 Table 20: Audio modality evaluation results. All tasks metrics are Exact Match / Judge Score. For ruSLUn dataset the metrics are Intent Exact Match / Slot F1 Score. For Human Baseline aggregated results are provided (average of EM and JudgeScore). Model Human Baseline Total CommonVideoQA RealVideoQA ruHHH-Video 0.92 0.96 0.96 0. 0.54 Qwen/Qwen2-VL-72B-Instruct 0.506 Qwen/Qwen2.5-VL-72B-Instruct 0.483 Qwen/Qwen3-VL-8B-Instruct 0.474 Qwen/Qwen2.5-VL-7B-Instruct 0.42 Qwen/Qwen2.5-Omni-7B 0.418 Qwen/Qwen2.5-VL-3B-Instruct Qwen/Qwen3-VL-2B-Instruct 0.396 0.341 openbmb/MiniCPM-o-2_6 0.332 Qwen/Qwen2.5-Omni-3B 0.298 Qwen/Qwen2-VL-7B-Instruct 0.293 OpenGVLab/InternVL3-9B-Instruct OpenGVLab/InternVL3-9B 0.276 0.198 Qwen/Qwen2-VL-2B-Instruct 0.188 OpenGVLab/InternVL3_5-4B OpenGVLab/InternVL3-2B 0.134 OpenGVLab/InternVL3_5-2B-Instruct 0.133 0.122 llava-hf/LLaVA-NeXT-Video-7B-hf 0.49 / 0.61 0.57 / 0.49 0.53 / 0.51 0.49 / 0.52 0.42 / 0.52 0.39 / 0.48 0.41 / 0.46 0.33 / 0.44 0.3 / 0.45 0.08 / 0.5 0.26 / 0.28 0.28 / 0.27 0.06 / 0.37 0.27 / 0.14 0.09 / 0.2 0.14 / 0.14 0.09 / 0.2 0.57 / 0.69 0.65 / 0.58 0.61 / 0.61 0.58 / 0.63 0.44 / 0.59 0.47 / 0.58 0.51 / 0.57 0.41 / 0.53 0.33 / 0.53 0.12 / 0.61 0.25 / 0.29 0.26 / 0.26 0.06 / 0.47 0.28 / 0.14 0.11 / 0.24 0.15 / 0.19 0.09 / 0.23 0.34 / 0.55 0.53 / 0.21 0.46 / 0.18 0.41 / 0.23 0.24 / 0.32 0.25 / 0.34 0.2 / 0.24 0.13 / 0.19 0.12 / 0.26 0.04 / 0.43 0.33 / 0.35 0.32 / 0.27 0.05 / 0.18 0.24 / 0.06 0.07 / 0.09 0.1 / 0.09 0.03 / 0.08 Table 21: Video modality evaluation results. All tasks metrics are Exact Match / Judge Score. For ruHHH-Video dataset the metrics are Group Exact Match / Group Judge Score. For Human Baseline aggregated results are provided (average of EM and JudgeScore). Dataset Overlap Num samples Total, $ Per item, $ Per hour, $ IAA AQUARIA CommonVideoQA LabTabVQA RealVQA RealVideoQA ruCLEVR ruCommonVQA ruEnvAQA ruHHH-Image ruHHH-Video ruMathVQA (crowd) ruMathVQA (expert) ruNaturalScienceVQA (crowd) ruNaturalScienceVQA (expert) RuSLUn ruTiE-Audio ruTiE-Image SchoolScienceVQA (crowd) SchoolScienceVQA (expert) UniScienceVQA WEIRD 5 5 5 5 5 5 5 5 5 5 5 5 5 3 5 3 3 5 3 5 5 786 1200 339 1 010 671 2 063 2 922 644 610 911 2 975 2 975 403 403 741 500 500 1 750 1 750 1 150 889 324.42 2364.12 518.99 405.82 785.54 1 440.15 10 401.71 239.58 276.94 638.40 214.71 1 363.58 119.55 123.08 133.00 65.16 65.16 1 293.83 1 270.70 102.4 109.1 0.41 1.97 0.31 0.40 1.17 0.70 3.56 0.37 0.45 0.70 0.07 2.29 0.30 0.31 0.03 0.13 0.13 0.74 0.73 0.09 0. 7.38 8.53 11.04 8.54 8.53 7.40 8.54 7.38 8.54 708 1.10 1.35 6.09 9.65 2.88 3.62 3.62 8.54 8.94 2.41 9.4 93.87% 92.41% 87.91% 69.65% 92.19% 93.36% 79.24% 89.46% 90.25% 91.28% 85.01% 83.09% 90.37% 96.69% 81.05% 81.00% 85.33% 67.56% 81.23% 45.19% 90.84% Table 22: Payrates and total expenses for human baseline annotation."
        }
    ],
    "affiliations": [
        "MERA Team"
    ]
}