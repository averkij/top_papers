{
    "paper_title": "Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control",
    "authors": [
        "Xiao Fu",
        "Xintao Wang",
        "Xian Liu",
        "Jianhong Bai",
        "Runsen Xu",
        "Pengfei Wan",
        "Di Zhang",
        "Dahua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 3 4 9 1 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Learning Video Generation for Robotic Manipulation\nwith Collaborative Trajectory Control",
            "content": "Xiao Fu1 Xintao Wang2(cid:66) Xian Liu1 Jianhong Bai3 Runsen Xu1 Pengfei Wan2 Di Zhang2 Dahua Lin1(cid:66) 1The Chinese University of Hong Kong 2Kuaishou Technology 3Zhejiang University Figure 1: RoboMaster synthesizes realistic robotic manipulation video given an initial frame, prompt, user-defined object mask, and collaborative trajectory describing the motion of both robotic arm and manipulated object in decomposed interaction phases. It supports diverse manipulation skills and can generalize to in-the-wild scenarios. Please check more on our website."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, novel framework that models inter-object dynamics through collaborative trajectory formulation. : Work done during an internship at KwaiVGI, Kuaishou Technology. (cid:66): Corresponding Authors. Preprint. Under review. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and postinteraction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the preand post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearanceand shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation. Project Page: https://fuxiao0719.github.io/projects/robomaster/"
        },
        {
            "title": "Introduction",
            "content": "Embodied AI has achieved remarkable progress in recent years [7, 8, 10, 6, 34, 37, 29], holding promise to replace human labor in performing diverse tasks. Scalable robot learning plays crucial role in empowering embodied intelligent agents to fulfill diverse and generalizable skills in unseen environments. However, major bottleneck remains: data scarcity [63, 32]. Collecting large-scale data using real robots is costly and requires human supervision to ensure safety. Recently, video generation [38, 60, 1, 48, 27, 4, 5] has emerged as promising approach for simulating realistic environments, offering visually plausible content that closely resembles the real world. Leveraging this, several works have explored generating robotic decision-making data from multimodal inputs, e.g., instruction [13, 58, 45, 26], sketch [68], and trajectory [69, 45]. Among these, trajectory-conditioned generation enables fine-grained control over robot planning by structuring tasks where (1) trajectories guide the motions of both the robotic arm and the manipulated object, and (2) realistic interactions emerge when the robotic arm reaches and manipulates the target object. However, previous works, such as Tora [66] and DragAnything [57], simply focus on driving individual object motion with separate trajectories (see Table 1). This design leads to feature entanglement in overlapping regions during interaction (highlighted by the red box in Fig 2), which impairs the models ability to capture physically plausible interactions and impairs visual fidelity. Figure 2: Collaborative Trajectory (Ours) vs Separated Trajectories (Previous, e.g.Tora). Unlike Tora [66] that decomposes objects and uses separate trajectories to model the motion of robot arm and manipulated object, we decompose the interaction phase and unify their joint motions into single collaborative trajectory with fine-grained object awareness. This integration alleviates the feature fusion issue in overlapping regions (see the missing apple in Tora), and improves visual quality. To address these limitations, we propose RoboMaster, which models robotic manipulation with novel collaborative trajectory. Unlike Tora, which decomposes multi-object motion using separate trajectories, RoboMaster captures interactive dynamics within unified trajectory representation. Specifically, we decompose the interaction process into three sub-phases: pre-interaction, interaction, and post-interaction. Each phase is guided by the dominant agent, namely the robotic arm in the preand post-interaction phases, and the manipulated object during interaction. This design is motivated by the observation that the robotic arm initiates and concludes the motion, while the object remains 2 Table 1: Comparison of Ours with Previous Trajectory-Controlled Methods. Object Awareness Interaction Granularity Trajectory IRAsim [69] DragAnything [57] Tora [66] / MotionCtrl [54] Single (Isolated) Multiple (Isolated) Multiple (Isolated) RoboMaster (Ours) Single (Collaborative) Decomposed? Format Appearance N/A Mask Point Mask Shape largely static; during interaction, the objects motion reflects the physical response to manipulation, implicitly synchronizing with the robotic arms trajectory. By explicitly modeling the driving subjects and their features across interaction phases, RoboMaster mitigates the feature entanglement issues and facilitates learning plausible interactions rather than strictly following trajectory by compromising interaction fidelity, thus enhancing video realism. Furthermore, to ensure semantic consistency of the manipulated object throughout the video sequence, we leverage the user-defined object mask to sample encoded RGB latents. These latents are then associated with the objects shape to construct circular volumetric representation, preserving both appearance and shape across frames. In addition to improved interaction modeling, our design also enhances user interaction in several aspects: 1) users can easily annotate the interaction phase by specifying the start and end frames, without the need to provide complete trajectories for both the robotic arm and the object simultaneously. This significantly simplifies trajectory correction in the presence of annotation errors. 2) users can flexibly define the manipulated object region using brush-based tool. Notably, our experiments demonstrate that RoboMaster remains robust even with incomplete or coarse object masks, making the system more tolerant to imprecise user input. We conduct extensive experiments on the challenging Bridge V2 dataset [47] and demonstrate that RoboMaster outperforms prior trajectory-controlled video generation methods in both visual quality and trajectory accuracy. We also validate its robustness in in-the-wild scenarios. Our key contributions are summarized as follows: 1) We propose novel collaborative trajectory control framework that effectively models robotic manipulation by decomposing the interaction phase into sub-phases, enabling video generation to function as an interactive simulator for producing high-quality robotic data. 2) Our design combines collaborative trajectories with mask-based object embeddings, allowing for more intuitive user annotation and significantly enhancing user interactivity. 3) Extensive experiments validate that ours achieves state-of-the-art performance on both benchmark and real-world settings, surpassing existing trajectory-conditioned video generation approaches."
        },
        {
            "title": "2 Related Work",
            "content": "Trajectory-Controlled Video Generation for Object Movement. Early works [54, 35, 36, 61, 66] leverage point-based control to enhance adaptability and user interactivity. Subsequent methods adopt mask-based representations (including bounding boxes) [59, 39, 50, 57, 12] or optical flow [17, 43] to improve robustness over point-based alternatives. Beyond 2D representations, 3DTrajMaster [15] and ObjCtrl-2.5 [53] model object motion using 6-DoF trajectories, while LeviTor [49] incorporates depth to enable 3D-aware object manipulation. However, existing works overlook interaction scenarios and treat object motion as independently controlled, which degrades visual quality in overlapping regions. In contrast, RoboMaster introduces collaborative trajectory control, unifying interactive features and decomposed trajectories to model interaction effectively (see Fig 2 and Table 1 for comparison). Video Generation as World Simulator for Robotic Manipulation. Scalable robot learning [7, 8, 10, 6, 34] relies heavily on large-scale realistic data, but collecting real-world robot trajectories from human demonstrations remains time-consuming and labor-intensive, limiting public accessibility. To address this, generative video models [56, 1] offer cost-effective alternative for synthesizing realistic data for policy learning. UniPi [13] and AVDC [26] frame robot planning as text-to-video generation, with AVDC further incorporating inverse dynamic estimation via pretrained flow network. UniSim [58] learns unified real-world simulator with diverse conditions (text and control 3 inputs). Robodreamer [68] enables compositional generalization through text-parsing strategy. IRASim [69] also employs trajectory-conditioned video generation but only models robot arm motion. In contrast, our method jointly models both robot and object trajectories with fine-grained object awareness, enabling higher visual quality and greater user interactivity in unseen scenarios."
        },
        {
            "title": "3 Method",
            "content": "Our goal is to enable fine-grained and user-friendly control in image-to-video generation for robotic manipulation. To this end, we present RoboMaster (see Fig 3), framework built upon collaborative trajectory mechanism. We begin by reviewing the prior trajectory control paradigm and outlining our task formulation (Sec. 3.1). We then introduce the key components required for control: (1) object embeddings that encode appearance and shape to maintain identity consistency (Sec. 3.2), and (2) collaborative trajectory that models the interactive dynamics (Sec. 3.3). These are integrated via motion injector (Sec. 3.4) that effectively guides motion generation. Figure 3: RoboMaster Framework. Given an input image and prompt c, it generates desired robotic manipulation video with the collaborative trajectory design. Specifically, it first encodes the object masks, including robotic arm Md and submissive object Ms (acquired either from 1) Grounded-SAM [41] or 2) user-defined brush mask) with the awareness of appearance and shape to obtain vd, vs for maintaining identity consistency in the video. To precisely model the manipulation process, the controlled trajectory is decomposed into sub-interaction phases: pre-interaction C1, interaction C2, and post-interaction C3, associating each phase with object-specific latents vd, vs, and vd, respectively. The collaborative trajectory latent is then injected into plug-and-play motion injectors, enabling the reasoning of video dynamics during generation. 3.1 Preliminary: Video Diffusion Transformers with Decentralized Trajectory Control Video diffusion transformers (DiTs) [38, 30, 60, 1, 48, 27] with trajectory condition learns the conditional distribution p(x {Cn}N n=1) of the compressed video data = patchify(E(X)), where is the trajectory number, E() is 3D VAE encoder and RF 3HW is clean video. It involves forward process to progressively inject noise ϵ on x0 to the desired Gaussian distribution in Markov chain: {xt, (1, ) xt = αtx0 + σtϵ, ϵ (0, I)}, and reverse process pθ to remove noise via noise estimator ˆϵθ, trained by minimizing: EtU (0,1),ϵN (0,I) min θ (cid:16) (cid:20)(cid:13) (cid:13) (cid:13)ˆϵθ xt, t, {Cn}N n=1 (cid:17) ϵ (cid:21) (cid:13) 2 (cid:13) (cid:13) 2 (1) Task Formulation Given an initial frame containing interaction subjects, dominant subject 1 od and submissive subject os, along with user-defined text prompt c, binary object masks Md 1We omit the robotic arm mask in Fig 1 as 1) for better illustration 2) user is not required to additionally input it and only need to use the pre-defined mask instead. 4 and Ms (where {0, 1}HW ), and collaborative trajectory = {(x, y)t}F t=1, our objective is to synthesize plausible manipulation video X. The trajectory is structured into three temporal phases: pre-interaction C1 = {(x, y)t}F1 t=F1+1, and post-interaction C3 = {(x, y)t}F t=F2+1. We define the general formulation fθ() of the generative model as t=1, interaction C2 = {(x, y)t}F2 fθ() : R3HW , L, Md, Ms {0, 1}HW , {(x, y)t}F t=1 RF 3HW (2) where is the alphabet, is the token length, and D( ˆ unpatchify(x0)). 3.2 Subject Representation via Coupled Appearance and Shape Embedding Figure 4: Subject Embedding Illustration. The object mask is interpolated to align with the encoded RGB latents z. Then it samples with valid pixels and applies an average pooling operator to generate the embedding v. To enhance spatial awareness, it expands the object token by radius r, which is proportional to the area of the valid mask region, and obtains the circular volume v. As shown in Fig 4, the initial frame is first projected into latent features via the VAE encoder E() : R3HW Rchw with spatial compression factors cs. The object masks are subsequently downsampled using an interpolation operator Fd() : Md, Ms {0, 1}HW md, ms {0, 1}hw to match the spatial resolution of the latent feature map. We then extract the latent subject features by applying the corresponding masks to z, followed by pooling operator, resulting in vd, vs Rc, defined as: 1 (cid:88) (cid:88) vd,s[i] = (cid:80)h i=1 (cid:80)w j=1 md,s[i, j] zd,s[i, x, y] = z[i, x, y] i=1 if md,s[x, y] = 1 j= zd,s[i, x, y] for = 0, 1, ..., (3) otherwise zd,s[c, x, y] = At each timestep within the latent video length , which is the temporal-compressed length of , we represent the subjects as circular volume vd, vs Rchw, centered at the trajectory point (x, y)t and confined to the corresponding valid mask region. This volume is constructed as: vd,s[i, j, k] = vd,s[i] if (j x)2 + (k y)2 <= r2 d,s otherwise vd,s[i, j, k] = 0 i, j, (4) where the radius rd,s is proportional to the mask area, i.e., (cid:80)h j=1 m[i, j]. Incorporating both object appearance and spatial shape into this latent representation accelerates training convergence and improves identity consistency across subsequent frames in the video sequence. (cid:80)w i=1 3.3 Collaborative Trajectory Representation Decentralized modeling of multiple trajectories, represented as pθ(x I, c, {Cn}N n=1), is appropriate for scenarios where objects follow independent motion patterns without mutual intervention. However, when applied to interactive scenarios, e.g., picking up or moving objects, it exhibits several limitations: 1) Feature overlap: Interaction phase dominates the overall motion, and it introduces feature ambiguity in such overlapping regions as models are primarily trained on independently moving objects, leading to degraded synthesized quality. 2) Trajectory precision during interaction: 5 Accurately specifying the trajectory of the dominant subject od during the interaction phase is challenging. Users may find it difficult to define precise temporal boundaries (e.g., start and end timestamps) and relative spatial positioning with respect to the submissive object os. To address these limitations, we propose learning unified distribution pθ(x I, c, vd, vs, C) with collaborative trajectory C, which is further temporally decomposed into three subsets: Pre-&Post-Interaction During these phases, the dominant subject od serves as the sole moving agent, while the submissive subject os remains static or exhibits minor motion due to inertia. Accordingly, we leverage the dominant subjects trajectory C1 = {(xd, yd)t}F1 t=F2+1 along with its circular volume vd to model the distribution pθ(x1 I, c, vd, C1) and pθ(x3 I, c, vd, C3)2. t=1, C3 = {(xd, yd)t}F Interaction At this stage, the interactive agents od and os collaborate to carry out the instruction c. We incorporate submissive subjects trajectory C2 = {(xs, ys)t}F2 t=F1+1 and its corresponding circular feature vs to model the conditional distribution pθ(x2 I, c, vs, C2). Our intuition is twofold: 1) the motion of the submissive subject can implicitly guide the dominant subject, owing to the typically constrained relative dynamics between interacting entities during this phase 2) temporal variations in the feature representation (i.e., vd vs vd) can provide valuable cues for modeling behavioral changes (sole object movement interactive objects movement) over time. Causal Representation. Given the causal nature of the 3D VAE encoder E(), we incorporate latent feature map from previous frames into subsequent ones to enhance smoother transitions. Specifically, at each timestep t, latent feature map from timestep 1 is propagated forward, and the current object feature (vd or vs) is overwritten onto it. Consequently, the interaction and post-interaction distributions are updated as pθ(x2 I, c, vd, vs, C1, C2) and pθ(x3 I, c, vd, vs, C1, C2, C3) In general, our collaborative design factorizes the vanilla distribution pθ(x I, c, Cs, Cd) into multiple object-aware sub-distributions, thereby alleviating feature confusion and improving interaction: pθ(x1 I, c, vd, C1) (cid:124) (cid:123)(cid:122) (cid:125) pre-interaction pθ(x2 I, c, vd, vs, C1, C2) (cid:123)(cid:122) (cid:125) (cid:124) interaction pθ(x3 I, c, vd, vs, C1, C2, C3) (cid:123)(cid:122) (cid:125) (cid:124) post-interaction (5) User Interaction Our design offers several key advantages for user-friendly access to generalizable experiments: 1) Robustness in object extraction: Due to mask-based representation, users can flexibly specify interaction object using simple brush tool. Our experiments show that object identity remains well-preserved, even with coarse input brush-based mask, in contrast to complete one generated with SAM [40]. 2) Flexibility in input trajectory: instead of requiring two full-length trajectories, users can define decomposed sub-trajectories within single motion path. This not only simplifies the input process but also enhances adaptability for iterative refinement. 3.4 Motion Injection Module The collaborative trajectory latent Rf chw, which associates vd, vs with latent frame length , is patchified and sequentially encoded by zero-initialized 2D spatial convolutional layer and zero-initialized 1D temporal convolutional layer. This produces compact representation 2 )C. The output hidden state from the previous DiT block, denoted as R( R( 2 )C, is then combined with the trajectory latents (V and its group normalized output) before being forwarded to remaining DiT blocks: 2 2 2 2 = + norm( V) + V, = Conv1D(Conv2D(patchify(V))) (6) Loss Function To learn the desired motion patterns, we optimize the parameters θ, including both the DiT blocks and the motion injector, as follows: L(θ) = x,c,ϵN (0,σ2 I),I,Md,Ms,C,t (cid:104) ϵ ˆϵθ1 (xt, c, Md, Ms, C, t)2 2 (cid:105) (7) 2We decompose the causal latent video as three temporally-partitioned segments: x1, x2, and x3, corresponding to the pre-interaction, interaction, and post-interaction phases, respectively."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details We implement our conditional video diffusion model based on the pre-trained CogVideoX-5B architecture [60]. We conduct experiments on the Bridge V2 dataset [47] (Please refer to Supp. for data preparation process), we adopt resolution of 480 640 and video length of 37 frames during both training and inference. The model is trained using AdamW [33] on 8 NVIDIA A800 GPUs, with learning rate of 2 105 for the DiT blocks and 1 104 for the motion injector, and total batch size of 16. Training is conducted for 30,000 steps. At inference, we employ 50 DDIM steps and set the CFG scale to 6.0. Here we generate single video with one inference, and it can be extended to multi-prompt long video generation in an auto-regressive manner (Please refer to Supp.). 4.2 Baselines We compare RoboMaster with existing state-of-the-art trajectory-controlled baselines: Tora [66], MotionCtrl [54], DragAnything [57] and IRASim [69]. For fair comparison, all baselines are retrained on the same dataset based on CogVideoX-5B with their respective optimal training configurations. 4.3 Evaluation Metrics We perform evaluation3 on 214 test samples in Bridge V2, covering diverse manipulation skills4, based on: 1) Trajectory Accuracy: We report the Trajectory Error (TrajError), which computes the average L1 distance between the input and generated trajectories of both the robot arm and the manipulated object. 2) Video Quality: We adopt standard metrics, including Frechét Video Distance (FVD) [46], PSNR [21], and SSIM [52], and further assess video on widely-used VBench [24]. 4.4 Quantitative&Qualitative Comparison Table 2: Quantative Comparison. Note that all the baselines are retrained on our curated dataset. Video Quality Trajectory Accuracy User Study Method FVD PSNR SSIM IRASim [69] MotionCtrl [54] DragAnything [57] Tora [66] 159.04 170.79 158.42 152. RoboMaster (Ours) 147.31 20.88 19.89 21.13 21.24 21.55 0.782 0.761 0.792 0.788 0. TrajErrorrobot 19.25 21.17 18.97 18.14 TrajErrorobj 34.39 28.52 27.41 26.43 16.47 24.16 Preference (%) 9.16 7.63 15.27 20. 47.33 Method Table 3: Quantative Comparison on VBench [24] Metrics. Subject Temporal Consistency Flickering Motion Smoothness Aesthetic Quality Imaging Quality IRASim [69] MotionCtrl [54] DragAnything [57] Tora [66] RoboMaster (Ours) 50.12 48.78 49.53 50.61 50.32 67.11 66.78 67.15 67.28 67. 98.04 98.21 97.83 97.79 98.27 98.79 97.58 98.25 98.11 98.81 93.11 92.19 93.01 92.71 93. Background Consistency 94.89 95.15 95.14 95.26 95.40 As shown in Fig 5, Table 2 and Table 3, RoboMaster consistently outperforms prior state-of-the-art methods in quantitative metrics of visual quality and trajectory accuracy, as well as in qualitative visual performance. Our strengths lie in two aspects: 1) Interaction-aware Trajectory Design: We explicitly decompose interaction phases and integrate object features into unified trajectory. In contrast, baseline methods struggle with feature entanglement in regions where the robotic arm and object interact. IRAsim only controls the robot trajectory, resulting in coarse object control 3Generate video based on an initial frame, prompt, robot and object trajectories, and an optional object mask (for Tora and Ours), and then compare it with GT video. 4Skills: move, pick, open, close, upright, topple, pour, wipe, and fold 7 Figure 5: Qualitative Comparison. RoboMaster (ours) demonstrates superior performance across range of manipulation skills (e.g., move, pick, close, upright, close), exhibiting improved visual consistency of the manipulated subject compared to prior baselines. and increased trajectory error (24.16 34.39). 2) Object Representation: We use mask-based representations rather than shape-ambiguous point representations (as in Tora and MotionCtrl), leading to improved object identity consistency across frames. See the white-box region in Fig 5 for comparison of object identity preservation. RoboMaster further exhibits enhanced robustness on in-the-wild image collections, outperforming baseline methods as shown in Fig 6. 4.5 Ablation Study We perform ablation on the full evaluation benchmark to validate model component effectiveness. 8 Figure 6: Generalizable Comparison with Input Prompt: Pick up the bee. Figure 7: Ablation on Generalizable Sample: Move the can to the right place of the eggplant. Table 4: Ablation Study on Bridge V2 Full Benchmark. Method PSNR SSIM FVD w/o Causal Embedding w/ Points Representation w/ Separate Trajectories w/ Cross Attention Full Model 151.62 157.49 152.01 163.56 147.31 21.30 20.87 21.08 19. 21.55 0.797 0.779 0.792 0.761 0.803 TrajErrorrobot 18.32 19.71 17.24 21.52 TrajErrorobj 27.15 31.41 25.84 29.16 16. 24.16 Table 5: Ablation on Mask Sparsity Sparsity (%) PSNR (%) 90 80 70 60 99.81 98.12 98.02 97.89 Subject Representation Removing the causal embedding for latent control (w/o Causal Embedding in Table 4) leads to decline in both visual quality and trajectory accuracy, as evidenced by the misplacement of the can in Fig 7, highlighting the necessity of conditioning causal visual latents on causal control signals. Moreover, replacing the mask-based representation with point-based one (w/ Point Representation), as in Tora, significantly increases the subject trajectory error (24.16 31.41), indicating that mask provides more effective object representation. The mask-based approach also exhibits greater robustness to input sparsity, as shown in Table 5, where PSNR is reported relative to the full-mask baselinean important property for real-world user input that is often incomplete. Trajectory Injection Replacing the collaborative trajectory with separate ones (w/ Separate Trajectories) introduces feature fusion issues in overlapping regions, leading to reduced visual quality (see can distortion in Fig 7) and lower trajectory accuracy. This supports the effectiveness of our decomposed trajectory design. Moreover, our model remains simple and effective, as alternative designs such as cross-attention-based trajectory injection (w/ Cross Attention) result in degradation. When randomly deviating partial subset ( 15%) of sampled points from the original trajectory, the generated video remains robust under such disturbances as shown in Table 6. Table 6: Ablation on Trajectory Perturbation 99.17 98.25 97.68 97.15 Deviration (%) 5 10 15 20 PSNR (%)"
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present RoboMaster, trajectory-controlled video generation framework with collaborative interaction design tailored for robotic manipulation. By decomposing interactions into sub-interaction phases, our method achieves superior visual quality and trajectory accuracy over prior approaches. Coupled with shapeand appearance-aware object encoding, RoboMaster enables more intuitive user annotation and enhances overall interactivity. Limitations&Future Work: (1) RoboMaster may produce incomplete or distorted objects during manipulation when applied to out-of-domain inputs. This could be mitigated by training on more diverse object categories with richer semantic and geometric variations. (2) The current framework operates purely in 2D pixel space; integrating depth cues [14, 11, 23] may enable more accurate 3D control. (3) Generalization to varied robotic embodiments remains challenge and requires expanding training data to encompass broader range of robot configurations."
        },
        {
            "title": "A Experimental Details",
            "content": "A.1 Dataset Curation Figure S8: Dataset Construction Pipeline. It involves automatic annotation with human-in-the-loop processes to generate high-quality data samples. Given raw video paired with prompt c, we generate the annotations following the stream below: (1) Video Per-Pixel Tracking: We employ CoTracker3 [25] to compute spatio-temporal trajectories of point sets, which are initialized on dense grid interval (30) in the first frame. (2) Object Detection: We parse the prompt to extract the noun corresponding to the submissive object os, typically the first noun following the action verb. The dominant object od is either parsed or pre-defined (e.g., black robotic gripper in Bridge V2 [47]). We apply Grounded SAM [41] to obtain segmentation masks of interacting entities in the first frame, compute their centers of gravity, and associate them with the nearest tracking point in the first frame and its tracking trajectory in step (1). (3) Decoupling Interaction: To identify the transition frames marking the start and end of the interaction phase (F1, F2), we analyze the motion dynamics of the submissive object throughout the video. Transitions are determined by applying motion threshold τ to detect the timestamps where the object initiates and terminates its activity. As shown in Fig S8, we apply the automatic annotation pipeline to each video in the training set and filter out invalid samples resulting from failures in object detection or trajectory tracking. On the Bridge V2 dataset [47], this process yields approximately 21k annotated video samples. A.2 User Annotations on In-the-Wild Images To facilitate user-friendly annotation on in-the-wild image samples, we develop Gradio demo, as shown in Fig S9. This interactive interface requires the user to provide the following inputs, which are prepared for the model: (1) Text Prompt: Describing the interaction type (e.g., pick, move) and target manipulated object. (2) Object Mask: The user employs the brush tool to define the region of the manipulated object. Note that the user only needs to provide the object mask, while the robotic arm mask is pre-defined and can be used as an off-the-shelf component. (3) Time Period of Interaction: The user specifies the start and end timestamps for the interaction. If the interaction does not include post-interaction phase (e.g., pick, open and close), the end timestamp is set to the maximum video length. (4) Collaborative Trajectory: Annotate key points on the input image for each decomposed interaction phase, and the completed trajectory is generated through interpolation. To refine the trajectory definition, we visualize the intermediate images and composite video after each input is completed. 10 Figure S9: Gradio Demo for User Annotation. The user is required to provide prompt, an object mask, and collaborative trajectory consisting of three phases: pre-interaction, interaction, and post-interaction, in sequence. This setup allows for flexible edits at any stage, enabling iterative refinement of the annotation. A.3 Network Architecture As shown in Table R7, the architecture of RoboMaster incorporates the collaborative trajectory latent into the base model to facilitate the visual generation of the robotic manipulation video X."
        },
        {
            "title": "B Additional Related Work",
            "content": "Trajectory-Controlled Video Generation. Recent advances in trajectory-conditioned video generation primarily fall into two directions: Camera Movement: MotionCtrl [54], CameraCtrl [20], and 4DiM [55] have successfully implemented camera-controlled text-/image-to-video generation using 6-DoF camera trajectories. NVS-Solver [62] enhances generalizability by employing training11 Table R7: Network Architecture. , C, and ks denote the block number in the base video model, the latent feature size, and the kernel size in each 2D/1D convolutional layer, respectively. Layer Output Dimension Output Input Image Image + init. noise Collab. Traj. Latent - VAE (E()) Patchify - + ht=0 (cid:32) Conv2D (ks=3, Cin = 8c, Cout = C/4, padding = 1) , Conv1D (ks=3, Cin = C/4, Cout = C, padding = 1), FloatGroupNorm (ngroups = 32, Cout = C) , (cid:33) (cid:18) LayerNorm + 3D Attention , LayerNorm + Feed-Forward, (cid:19) Unpatchify + VAE (D()) - - h 3 (f /2 h/2 w/2) (f /2 h/2 w/2) (f /2 h/2 w/2) 3 free depth-warping during the denoising process. ReconX [31] and ViewCrafter [65] improve 3D consistency by projecting point clouds into 3D cached space for guidance. CVD [28] and SynCamMaster [4] expand camera control to multi-shot generation. VD3D [3] and AC3D [2] integrate camera control into DiT-based video generation models. Additionally, recent studies [5, 42, 64] explore re-capturing source video using specified camera trajectory. In contrast to these approaches, RoboMaster emphasizes collaborative object trajectory control rather than focusing on camera trajectory. 2) Object Movement: refer to main paper. Video Generation with Injected Control. (1) Training-free Approaches: These methods directly manipulate attention patterns or latent representations at the inference time, though constrained by limited generalizability and demanding empirical tuning. Direct-a-Video [59] modulates spatial crossattention maps under the guidance of bounding box. FreeTraj [39] implements spectral-domain trajectory embedding with attention reweighting. DiTCtrl [9] convert self-attention into the proposed masked-guided KV-sharing strategy to generate multi-prompt video. (2) Learning-based Approaches: Previous techniques typically employ auxiliary encoders to map control signals into latent representations, utilizing learnable components (e.g., convolutional/linear layers, attention modules, LoRA adapters) or leveraging frozen pre-trained feature extractors. These encoded features are subsequently fused with the base model through feature fusion techniques such as concatenation, additive merging, or cross-attention injection. VideoComposer [51] employs unified STC-encoder and CLIP model to condition the base T2V model with multi-modal input conditions. MotionCtrl [54] introduces object motion control via an additional motion encoder. SparseCtrl [19] learns an add-on encoder to integrate various control signals into the base model. Tora [66] employs trajectory encoder and plug-and-play motion fuser to merge 2D trajectories with the base video model. MotionDirector [67] leverages spatial and temporal LoRA layers to learn desired motion patterns from reference videos. Motion Prompting [18] excels in various controllable generation tasks via training ControlNet-style adapter with general motion conditions. Meanwhile, line of works [22, 44, 16] designs sophisticated control mechanisms for human animation."
        },
        {
            "title": "C Additional Visual Results",
            "content": "C.1 Robotic Manipulation on Diverse Out-of-Domain Objects As demonstrated in Fig S10, RoboMaster is capable of generalizing to wide range of in-the-wild objects, such as bee, bottle, and peach in the oil painting, as well as dumpling, lobster, pumpkin head, and teddy bear, despite being trained solely on the Bridge dataset. C.2 Robotic Manipulation with Diverse Skills As shown in Fig S11, RoboMaster demonstrates the ability to perform wide range of manipulation tasks on real-world image datasets, including pick, pick-and-place, move, open, close, topple, fold, upright, and wipe. 12 Figure S10: Pick up on Diverse Out-of-domain (OOD) Objects. The blue dot represents the current position of the manipulated object along the guided trajectory. C.3 Long Video Generation in Auto-Regressive Manner Robomaster facilitates the generation of extended videos in an auto-regressive manner. Specifically, given either the initial frame or the final frame of previously generated video, it progressively generates longer, coherent video by utilizing multiple ordered prompts, as illustrated in Fig S12."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. 13 Figure S11: Diverse Manipulation Skills on Bridge and In-the-wild Test Samples. [2] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. arXiv preprint arXiv:2411.18673, 2024. [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024. Figure S12: Longer Video Generation with Multiple Input Prompts. [4] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di Zhang. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. arXiv preprint arXiv:2412.07760, 2024. [5] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. 15 arXiv preprint arXiv:2503.11647, 2025. [6] Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. [7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [9] Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation. arXiv preprint arXiv:2412.18597, 2024. [10] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. [11] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. arXiv preprint arXiv:2501.12375, 2025. [12] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Animateanything: Fine-grained open domain image animation with motion guidance. arXiv preprint arXiv:2311.12886, 2023. [13] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in neural information processing systems, 36:91569172, 2023. [14] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In European Conference on Computer Vision, pages 241258. Springer, 2024. [15] Xiao Fu, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation. In The Thirteenth International Conference on Learning Representations, 2025. [16] Qijun Gan, Yi Ren, Chen Zhang, Zhenhui Ye, Pan Xie, Xiang Yin, Zehuan Yuan, Bingyue Peng, and Jianke Zhu. Humandit: Pose-guided diffusion transformer for long-form human motion video generation. arXiv preprint arXiv:2502.04847, 2025. [17] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana LopezGuevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024. [18] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana LopezGuevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024. [19] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. In European Conference on Computer Vision, pages 330348. Springer, 2024. [20] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. [21] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 23662369. IEEE, 2010. [22] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 16 [23] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. [24] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [25] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. [26] Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, and Joshua Tenenbaum. Learning to act from actionless videos through dense correspondences. arXiv preprint arXiv:2310.08576, 2023. [27] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [28] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. Advances in Neural Information Processing Systems, 37:1624016271, 2024. [29] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martín-Martín, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning, pages 8093. PMLR, 2023. [30] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [31] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767, 2024. [32] Yang Liu, Weixing Chen, Yongjie Bai, Xiaodan Liang, Guanbin Li, Wen Gao, and Liang Lin. Aligning cyber space with physical world: comprehensive survey on embodied ai. arXiv preprint arXiv:2407.06886, 2024. [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [34] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters, 2023. [35] Wan-Duo Kurt Ma, John Lewis, and Bastiaan Kleijn. Trailblazer: Trajectory control for diffusionbased video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [36] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. Advances in Neural Information Processing Systems, 37: 1848118505, 2024. [37] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [39] Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuningfree trajectory control in video diffusion models. arXiv preprint arXiv:2406.16863, 2024. [40] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 17 [41] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. [42] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera control. arXiv preprint arXiv:2503.03751, 2025. [43] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [44] Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion representation. arXiv preprint arXiv:2410.10306, 2024. [45] 1X World Model Team. 1x world model challenge. https://github.com/1x-technologies/1xgpt, 2024. [46] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. [47] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pages 17231736. PMLR, 2023. [48] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [49] Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, and Limin Wang. Levitor: 3d trajectory oriented image-to-video synthesis. arXiv preprint arXiv:2412.15214, 2024. [50] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024. [51] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36:75947611, 2023. [52] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [53] Zhouxia Wang, Yushi Lan, Shangchen Zhou, and Chen Change Loy. Objctrl-2.5 d: Training-free object control with camera poses. arXiv preprint arXiv:2412.07721, 2024. [54] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [55] Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, and David Fleet. Controlling space and time with diffusion models. In The Thirteenth International Conference on Learning Representations, 2024. [56] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. arXiv preprint arXiv:2312.13139, 2023. [57] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In European Conference on Computer Vision, pages 331348. Springer, 2024. [58] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 1(2):6, 2023. [59] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with user-directed camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 18 [60] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [61] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. [62] Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvs-solver: Video diffusion model as zero-shot novel view synthesizer. arXiv preprint arXiv:2405.15364, 2024. [63] Albert Yu, Adeline Foote, Raymond Mooney, and Roberto Martín-Martín. Natural language can help bridge the sim2real gap. arXiv preprint arXiv:2405.10020, 2024. [64] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocular videos via diffusion models. arXiv preprint arXiv:2503.05638, 2025. [65] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. [66] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. In CVPR, 2025. [67] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jia-Wei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. In European Conference on Computer Vision, pages 273290. Springer, 2024. [68] Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan. Robodreamer: Learning compositional world models for robot imagination. arXiv preprint arXiv:2404.12377, 2024. [69] Fangqi Zhu, Hongtao Wu, Song Guo, Yuxiao Liu, Chilam Cheang, and Tao Kong. Irasim: Learning interactive real-robot action simulators. arXiv preprint arXiv:2406.14540, 2024."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "The Chinese University of Hong Kong",
        "Zhejiang University"
    ]
}