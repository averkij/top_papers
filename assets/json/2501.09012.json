{
    "paper_title": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot",
    "authors": [
        "Ruixiang Jiang",
        "Changwen Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization. We then develop a principled method for human preference modeling and perform a systematic correlation analysis between MLLMs' responses and human preference. Our experiments reveal an inherent hallucination issue of MLLMs in art evaluation, associated with response subjectivity. ArtCoT is proposed, demonstrating that art-specific task decomposition and the use of concrete language boost MLLMs' reasoning ability for aesthetics. Our findings offer valuable insights into MLLMs for art and can benefit a wide range of downstream applications, such as style transfer and artistic image generation. Code available at https://github.com/songrise/MLLM4Art."
        },
        {
            "title": "Start",
            "content": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot Ruixiang Jiang, Changwen Chen The Hong Kong Polytechnic University rui-x.jiang@connect.polyu.hk, changwen.chen@polyu.edu.hk 5 2 0 2 5 1 ] . [ 1 2 1 0 9 0 . 1 0 5 2 : r AbstractWe present the first study on how Multimodal LLMs (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, novel high-quality dataset for benchmarking artistic stylization. We then develop principled method for human preference modeling and perform systematic correlation analysis between MLLMs responses and human preference. Our experiments reveal an inherent hallucination issue of MLLMs in art evaluation, associated with response subjectivity. ArtCoT is proposed, demonstrating that art-specific task decomposition and the use of concrete language boost MLLMs reasoning ability for aesthetics. Our findings offer valuable insights into MLLMs for art and can benefit wide range of downstream applications, such as style transfer and artistic image generation. Code available at https://github.com/ songrise/MLLM4Art. Index TermsArt evaluation, AI alignment, Style transfer, Chain-of-Thought prompting I. INTRODUCTION Recent advancement of computer vision (CV) algorithms, particularly in style transfer [1][3] and image generation models [4], [5], have democratized artistic creation, allowing users to personalize artworks with minimal expertise. However, the development of quantitative and interpretable evaluation protocols on artwork aesthetics has not kept pace. Without human-aligned evaluation metrics, it is challenging to systematically refine and enhance these generation methods, thereby limiting their potential to produce truly aesthetically pleasing artworks. Unlike most CV tasks, aesthetics evaluation is not welldefined with pure vision features alone. It requires higher-level assessments that consider factors such as cultural background, emotional impact, and storytelling. However, in existing aesthetic evaluation protocol, vision-feature-based metrics (e.g., Style loss [6], Aesthetic Predictor [7], and Art Score [8]) dominate the landscape. These metrics significantly undermine the complexity of aesthetics and are proven to be misaligned with human preference [8][10]. Determining how to evaluate aesthetic quality holistically thus remains an open question. We draw inspiration from the Formal Analysis [11], [12], practice frequently adopted by art critics for formal aesthetics assessment. It involves using concise language to describe visual elements in artwork, relating them to domain-specific knowledge such as history, cultural background, and aesthetic principles, and collectively arguing about the artworks aesthetic quality. This technique effectively translates the underlying reasoning processes of human aesthetic perception into explicit language, facilitating more objective assessments. Intriguingly, this approach parallels the multimodal inferencetime reasoning of MLLMs [13][15], thereby presenting compelling research question: Can MLLMs reason about the aesthetic quality of artistic images in manner aligned with human preferences? To explore this question, we focus on the evaluation of artistic stylization and introduce MM-StyleBench, largescale and densely annotated dataset with diverse content and style instances. To quantify aesthetics alignment, we model rank-based human aesthetic preferences on MM-StyleBench in principled manner and perform correlation study with responses from MLLMs. By prompting various MLLMs with different strategies, we identify key challenge in aligning their outputs with human preferences: the tendency of MLLMs to produce hallucinations through the frequent use of subjective language associated with art. To address this issue, we propose ArtCoT, prompting method with explicit decomposition of art evaluation to reduce hallucination and enhance the reasoning capability of MLLMs. We implement ArtCoT across three different MLLMs, measure the models alignment in two preference ranking settings, and observe significant and consistent gain in aesthetic alignment. Furthermore, our empirical analysis reveals that art-specific task decomposition with clearly defined sub-tasks facilitates MLLMs reasoning abilities, leading to more objective thinking trace with reduced hallucination. Overall, our findings offer valuable insights into how MLLMs should be applied in art evaluation tasks. In broader sense, the feedback signals provided aesthetics-aligned MLLMs can benefit downstream applications such as reinforcement-learningenhanced stylization or image generation [16]. We summarize our contributions as follows: 1) We introduce MM-StyleBench, large-scale dataset for multimodal stylization with dense annotations. the first 2) We propose principled approach for modeling human aesthetic preference. 3) Our analysis provides valuable insights into reducing hallucinations in MLLMs during art evaluation. 4) We introduce ArtCoT that significantly enhances the aesthetic alignment of MLLMs for artistic evaluation. Fig. 1. The MM-StyleBench dataset. (a) The distribution of different attributes in MM-StyleBench. the proposed dataset contains diverse images and text prompts with detailed attribute annotations. (b) Examples of content (top) and style (bottom) instances in MM-StyleBench. II. RELATED WORKS Aesthetic Evaluation. Early aesthetic evaluation algorithms are mainly built on heuristics [17] or established aesthetic principles [18]. Recent learning-based methods [7], [19] regress human preference scores from large-scale image collections. Metrics like ArtFID [20] and Art Score [8] approach this problem by targeting image stylization tasks, where they finetune image encoders for aesthetic prediction. These methods measure aesthetics as feature-space distance, which is insufficient for aesthetics perception that requires multimodal reasoning [9][11]. Recent methods [21][23] attempt to incorporate the strong inference capability of MLLMs to facilitate image aesthetic evaluation. Nevertheless, they usually target at general image aesthetics, which has different criteria than artwork evaluations. Artistic Stylization. Neural stylization methods aim to map the style of content images to other styles. Early approaches [1], [24][26] employ pretrained image encoders to extract and merge vision features from content and reference style images. However, these methods are often limited in style fidelity and are prone to filtering artifacts, resulting in low aesthetic quality. Recent approaches [2], [3], [27] take aesthetics into model design considerations to enhance the style fidelity. Benefiting from multimodal embedding techniques, text-driven stylization methods emerge [3], [28], [29], where the style is specified through text prompts. As stylization methods evolve, evaluating these methods presents challenges, which can hinder the methods advancement. Our work offers timely solution for human-aligned stylization assessment. III. MM-STYLEBENCH To construct validation set for model comparison, previous stylization methods have relied on ad-hoc sampling from established datasets such as MS-COCO [30] and WikiART [31]. These samples are often limited in size and diversity, making performance measurements sensitive to instance selection and thereby challenging reproducibility and fairness [9], [10]. the limited annotation granularity impedes Additionally, comprehensive understanding of the methods strengths and weaknesses. To circumvent these challenges, we construct large-scale benchmarking dataset by harvesting from diverse sources (detailed in Appendix. A). The resulting dataset, MM-StyleBench, contains multi-modal content and style with dense attribute annotations, enabling unified stylization comparisons. Attribute distribution and examples are visualized in Fig. 1. Compared with existing benchmarks and evaluation protocols in Tab. I, MM-StyleBench offers strength in three aspects: (1) Scale it offers 1k content and styles, respectively, which is two orders of magnitude larger than existing datasets. (2) Quality MM-StyleBench contains multi-modal (image and text) and fine-grained attribute annotations, facilitating comprehensive model comparisons, and (3) Diversity Our dataset is built from diverse sources (SA-1B [32], MSCOCO [30], WikiART [31], and DiffusionDB [33]), covering wide range of domains to eliminate potential bias. We also utilized LLMs to create variation, further enhancing the diversity. IV. MODELING HUMAN PREFERENCE Objective. In this step, we aim to derive statistically robust global ranking of artists (stylization models) or artworks (stylized images) from human feedback. Formally, let C, S, denote the sets of content, style, and stylization models, respectively. For each instance (c, s) S, the stylization result yi = mi(c, s), for mi M, constitutes the Fig. 2. Overview of our alignment evaluation pipeline. First, (a) we sample content and style from MM-StyleBench for stylization, and construct 2AFC comparison sets by sampling from all possible candidate comparisons. (b) Human preference data is collected and filtered with two heuristic indicators, which is finally aggregated as global rankings. (c) We propose ArtCoT, which involves three art-specific phases to reduce MLLMs hallucinations. Finally, we calculate the correlation of rankings from MLLMs and humans as indicators of aesthetic alignment. TABLE COMPARISON OF REPRESENTATIVE STYLIZATION BENCHMARK DATASETS. THE PROPOSED MM-STYLEBENCH OFFERS SIGNIFICANTLY MORE CONTENT AND STYLE INSTANCE, WITH FINE-GRAINED MULTIMODAL ANNOTATIONS. Dataset/Protocol # Content # Style Multimodal Dense Annotation DiffStyler [2] StyleID [34] LAION-Aesthetics [7] ArtBench [35] StyleBench [36] 20 20 - - Ours 1000 25 40 50 10 73 1000 candidate outcomes Yc,s = {y1, . . . , yk}. Human preference is represented as ranking: yπ(1) yπ(2) yπ(k), (1) where π(i) maps the candidate index to its global rank, and means preferred over. Preference Modeling. To model human aesthetics preference, previous methods [3], [7], [8] employ Likert scale or similar ordinal-valued psychometrics in preference collection. However, these metrics are suboptimal for rank-based modeling due to issues like central tendency [37]. Recent psychological study [9] further suggests multi-scale metrics can impair human perception, especially for artistic evaluation. In response to this challenge, we design two-alternative forced choice (2AFC) task to collect preferences. Specifically, we present users with the content, style and two candidate stylization results. Users are required to choose the preferred one without the option of tie and skip. The probability of preferring yj over yi is modeled as: (yi yjc, s) = Qi Qi + Qj , (2) where Qi represents the latent competence of yi, conditioned on the specific instance c, s. Sampling Strategies for 2AFC. Given the combinatorial complexity of M, exhaustive pairwise comparisons are infeasible. Therefore, we generate 2AFC questions by sampling. For specific combination Yc,s, all possible comparisons can be modeled as complete undirected graph G(V, E), where the node is the candidate results and stand for pair-wise comparison. We consider two strategies for sampling E. 1) Global sampling: Uniformly sample arbitrary number of edges without replacement. This approach is suitable for covering wider range of content and style within fixed total budget, facilitating model-level ranking. 2) Per-instance sampling: Sample edges [V 1, E] such that the sub-graph is connective with maximum node degree uniformness, meaning that each candidates shall be compared with for similar times. This is suitable for deriving instance-level ranking. For the per-instance sampling, we solve it by designing greedy algorithm (described in Appendix. C) that initialize from minimum spanning tree using Kruskals algorithm [38] and iteratively add edges with lowest node degree imbalance. Dealing with Subjectivity in Human Preference. We deal with two special cases of subjectivity to enhance the consistency of preference annotation in per-instance setup: 1) Pair disagreement: We exclude pairwise comparisons where preferences tie (P (yi yjc, s) 50%). 2) Non-transitive relationship: To deal with cyclic preference, such as (a b, c, a), we apply feedback arc set (FAS) algorithm [39] to detect feedback arcs and drop the instance when /E η. Global rank derivation. We utilize Bradley-Terry (B-T) model [40] and Elo [41] algorithm to derive global rankings. In the B-T model, Qi = exp(θi), where the latent parameter {θi} is optimized. The Elo algorithm utilize Qi = 10Ri/400, where Ri is the rating of candidate-i that is updated on-line. TABLE II QUANTITATIVE COMPARISON ON AESTHETIC ALIGNMENT. WE REPORT ALIGNMENT OF PER-METHOD OR PER-INSTANCE RANKING FROM DIFFERENT MODELS AND PROMPTING METHODS WITH THAT OF HUMAN PREFERENCE. THE PERFORMANCE IMPROVEMENT/DECLINE IS CALCULATED AS NORMALIZED CHANGES COMPARED WITH THE BASE PROMPT METHOD. OUR ARTCOT DELIVERS STRONG ALIGNMENT WITH HUMAN AESTHETIC PREFERENCE ACROSS DIFFERENT MLLMS. Per-Method Alignment Per-Instance Alignment Model Prompting Elo Bradley-Terry Random guess Aesthetics Predictor [7] GPT-4o Gemini 1.5-flash Claude 3.5-sonnet GPT-4o Gemini 1.5-flash Claude 3.5-sonnet GPT-4o Gemini 1.5-flash Claude 3.5-sonnet Base Base Base Zero-shot CoT Zero-shot CoT Zero-shot CoT ArtCoT ArtCoT ArtCoT ρ p-value -0.115 0.406 0.248 0.467 -0. 0.345 +13% 0.018 -84% -0.345 -7% 0.576 +43% 0.697 +43% 0.612 +69% 0.751 0.244 0.489 0.173 0.467 0.328 0.962 0.328 0.08 0.025 0. ρ 0.067 0.406 0.284 0.552 -0.321 0.357 +10% 0.236 -62% -0.309 +1% 0.721 +61% 0.782 +51% 0.600 +70% p-value 0.855 0.244 0.425 0.09 0.365 0.313 0.511 0.385 0.001 0.007 0.066 ρ 0.068 0. 0.328 0.479 0.312 0.299 -4% 0.376 -20% 0.108 -30% 0.591 +39% 0.624 +28% 0.492 +26% Elo p-value 0.153 < 10 0.003 < 103 < 103 0.097 < 103 0.068 < 103 < 103 < 103 Bradley-Terry ρ 0.026 0. 0.331 0.353 0.367 0.313 -3% 0.327 -4% 0.081 -45% 0.548 +32% 0.577 +35% 0.487 +19% p-value 0.290 < 103 0.006 < 103 < 10 0.031 < 103 0.082 < 103 < 103 < 103 V. ZERO-SHOT REASONING ABOUT AESTHETIC QUALITY VI. EXPERIMENTS A. Experiment Setup We leverage CoT prompting [42] to elicit the reasoning ability of MLLMs during inference time. Our key finding is that explicit decomposition of art evaluation tasks and utilization of concrete language collectively reduce hallucination, as detailed in Sec. VII. Specifically, we propose ArtCoT, which encourages MLLMs to concretely describe visual features and reason the aesthetic quality, akin to the formal analysis in art. The ArtCoT takes the same input as other prompting techniques, but involves three specialized MLLM inferences, each tailored to an art-specific role. In the initial stage, the MLLM evaluates content preservation and style fidelity by providing detailed description of the visual features of stylized images and references. This involves identifying visual elements in images at different levels, such as color schemes, strokes, structural components, and composition. The difference with the existing featurebased aesthetic metrics [7], [8] is that we explicitly map vision features into semantics (as tokens) and link with domain knowledge to facilitate further reasoning. The response from this stage is concatenated with the original input and forwarded to the next stage. The second stage, termed as the art critic phase, prompts the MLLM to re-evaluate the combined input and connect with domain knowledge, akin to the formal analysis in art. Here, the MLLM shall critically examine the previously described visual features against artistic principles and domain-specific background information such as the cultural background. Notice that instead of determining winner, this stage aims to produce detailed thinking trace to facilitate the final decision. In the final stage, the MLLM summarizes the information from the first two stages and determines the winner in this 2AFC question. The overall workflow is visualized in Fig. 2- (c). Stylization models. We inspect total of 10 textdriven and image-driven with default configurations, including AdaIn [24], ArtFlow [26], ControlNet [43], DDIM [4], DiffArtist Instruct- [2], pix2pix [29], StyleID [34], and Sty-Tr2 [1]. InstantStyle [28], [3], DiffStyler Preference collection. 12 human experts with general knowledge of art are recruited for preference annotation. We collected total of 21k responses. For per-instance sampling, we sample O(klog(k)) for each content/style combination. We prune uncertain preferecne with (yi yjc, s) [0.4, 0.6]. Highly non-transitive instances with η > 0.15 are removed. As result, 24.8% of the feedbacks are filtered out due to pairwise divergence, while the remaining 18.6% are removed due to high non-transitive preference. Alignment Metrics. Following [3], [8], [21], we use the spearmans rank correlation [44] to quantify the alignment between human preference and responses from MLLMs. spearmans ρ closer to 1 indicates stronger positive linear correlation of ranking. We calculate ρ on the rank of methods for global sampling scheme, while for per-instance sampling, we rank each instance and report averaged ρ and combined value using Fishers method [45]. lower value means stronger statistical significance. B. Main Result"
        },
        {
            "title": "We inspect",
            "content": "including GPTthree main-stream MLLMs, 4 [13], Gemini 1.5 [14], and Claude 3.5 [15] with different prompting methods. We also compare the result of random guess and the linear aesthetic predictor trained on LAION [7]. We compare the alignment as spearmans ρ for the two sampling strategies and report results in Tab. II. The experimental results demonstrate the advantage of applying ArtCoT for aesthetic alignment, achieving an average improvement of 56% in the per-method setup and 29% in the TABLE III RESPONSE SUBJECTIVITY FROM DIFFERENT PROMPTING METHOD. RESPONSES FROM MLLM PROMPTED BY ARTCOT ARE LESS SUBJECTIVE. Method Subjectivity Subjective word frequency (%) Zero-Shot CoT ArtCoT 0.44 0.23 20.15 5.51 per-model setup. In contrast, zero-shot CoT degrade alignment for -22%/-18%. Overall, these findings indicate that with properly designed prompt that suppresses hallucinations, MLLMs aesthetic evaluation can be highly aligned with human preference. VII. ANALYSIS AND DISCUSSION A. Zero-shot CoT reinforces hallucination in art evaluation In multimodal art evaluation, zero-shot CoT may adversely impact the reasoning capability of MLLMs. Our empirical findings suggest that this problem is rooted in the hallucination issue, where MLLMs tend to feel instead of to reason. Empirically, in the zero-shot CoT, MLLMs tend to adopt subjective and less concrete language for art evaluation. Sensational (e.g., feels and sense) and hedge words (e.g., appears and somewhat) are frequently picked to justify their preference, which is more frequently misaligned with human preference. Moreover, the MLLMs may hallucinate artistic interpretations that deviate from the actual visual features present in the stylized images. Examples provided in Appendix. demonstrate this tendency. B. ArtCoT elicit the ability of formal analysis for art evaluation the Compared with base prompting and zero-shot CoT, responses generated by ArtCoT are more concise, objective, and consistent with the input images. To quantify this improvement, we calculate the frequency of subjective words in the lemmatized responses from the MLLMs and perform lexiconbased subjectivity analysis using the TextBlob package. The result in Tab. III demonstrates the reduced subjectivity of MLLM prompted with ArtCoT. This reduction can be attributed to task decomposition, allowing the MLLMs to think in steps like an art critic. Specifically, (1) the content/style analyzer is prompted to describe and assess the level of content preservation and style fidelity. Unlike direct artistic evaluation, these subtasks are more close-ended and relate closer to visual attributes, encouraging the use of concrete words. (2) The art critic phase enables MLLMs to re-evaluate the candidate images. The key difference from zero-shot CoT is that this critic stage can break potential self-reinforcing hallucinations, particularly when the previous responses contain factual errors. We suggest readers refer to examples in Appendix. for better understanding. C. ArtCoT boost aesthetic alignment in all circumstances. Leveraging the fine-grained annotations provided in MMStyleBench, we conduct comprehensive comparison of various prompting methods. For instance, in Fig. 3 , we visualize Fig. 3. Fine-grained comparison of different MLLM prompting scheme. We show the spearmans ρ for per-instance alignment, grouped by representative attribute provided by MM-StyleBench. ArtCoT elicits aesthetic reasoning for all scenarios, especially for instances with long and detailed prompts. TABLE IV ABLATION ON COMPONENT OF ARTCOT. WE ABLATE THE CONTENT/STYLE ANALYZER AND THE ART CRITIC. THE FULL METHOD ACHIEVES THE BEST AESTHETIC ALIGNMENT. CS-analyzer Critic Per-method ρ Per-instance ρ 0.630 0.531 0.739 0.532 0. 0.607 the alignment performance on different content complexity and style prompt categories. The proposed ArtCoT outperforms both base and zeroshot CoT prompting in all situations. The most notable improvements are observed in stylization tasks that involve concrete instructions. This is evidenced by the significant improvement of long prompt and prompt specifying particular art movement. We postulate that long and specific instructions set more concrete objective for both stylization and evaluation. This facilitates the MLLM to more objectively describe the expected visual features in target style and link with multidisciplinary background information. Consequently, the MLLM is less likely to hallucinate. D. Ablations. Components of ArtCoT. We ablate the key components of ArtCoT, specifically the content/style analyzer and the art critic phase, and report the results in Tab. IV. The complete ArtCoT prompt achieves the highest aesthetic alignment. Particularly, removing the art critic phase induces the most significant decline in performance, underscoring its critical role. This can also be evidenced by the output length of art critic as in Appendix. E. Input to MLLM. We study how different input modalities and image resolutions impact aesthetic alignment and report the result in Tab. V. We set the resolution to be 1/2 of the original resolution by default due to balance in performance and the number of tokens. For input modalities, providing style information is important in per-method alignment, while including the reference image affects per-instance alignment most. For either setting, providing the MLLMs with all input modalities achieves the best alignment, meaning that both TABLE ABLATION ON IMAGE RESOLUTION AND SOURCE INFORMATION. WE REPORT THE CORRELATION ρ (AVERAGED FROM ELO AND BRADLEY-TERRY) OF DIFFERENT INPUT SETUPS: CONTENT IMAGE, STYLE PROMPT, AND IMAGE SUB-SAMPLING FACTOR. Content Style Resolution Per-method ρ Per-instance ρ 1/4 1/8 1/2 (default) 1/2 (default) 1/2 (default) 0.630 -42% 0.502 -91% 0.476 -100% 0.678 -23% 0.557 -69% 1/2 (default) 0.739 0.432 -44% 0.285 -82% 0.416 -49% 0.465 -36% 0.521 -22% 0.607 content and style information would be necessary in evaluating the artistic style transfer. VIII. CONCLUSION In this work, we present the first comprehensive analysis of how MLLMs reasoning ability should be evoked for aesthetic evaluation of artworks. Utilizing the newly developed MMStyleBench dataset, our extensive study reveals the significant hallucination issue within MLLMs. We demonstrate that decomposing art evaluation into specific, art-focused sub-tasks and employing concrete, precise language in prompts significantly enhance the models ability to reason about aesthetic quality. wide range of downstream applications could benefit limited to stylization from our insights, evaluation, image generation, and reinforcement learning from AI feedback. including but not"
        },
        {
            "title": "REFERENCES",
            "content": "[1] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu, Stytr2: Image style transfer with transformers, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 1132611336. [2] Nisha Huang, Yuxin Zhang, Fan Tang, Chongyang Ma, Haibin Huang, Diffstyler: Controllable dual IEEE Transactions on Weiming Dong, and Changsheng Xu, diffusion for text-driven image stylization, Neural Networks and Learning Systems, 2024. [3] Ruixiang Jiang and Changwen Chen, trollable text-driven stylization without arXiv:2407.15842, 2024. Artist: Aesthetically conarXiv preprint training, [4] Jiaming Song, Chenlin Meng, and Stefano Ermon, Denoising diffusion implicit models, arXiv preprint arXiv:2010.02502, 2020. [5] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer, High-resolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 1068410695. [6] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al., Learning transferable visual models from natural language supervision, in International conference on machine learning. PMLR, 2021, pp. 87488763. [7] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al., Laion-5b: An open largescale dataset for training next generation image-text models, Advances in Neural Information Processing Systems, vol. 35, pp. 2527825294, 2022. [8] Junyu Chen, Jie An, Hanjia Lyu, Christopher Kanan, and Jiebo Luo, IEEE Learning to evaluate the artness of ai-generated images, Transactions on Multimedia, 2024. [9] Chaehan So, Measuring aesthetic preferences of neural style transfer: More precision with the two-alternative-forced-choice task, International Journal of HumanComputer Interaction, vol. 39, no. 4, pp. 755 775, 2023. [10] Eleftherios Ioannou and Steve Maddock, Evaluation in neural style transfer: review, in Computer Graphics Forum. Wiley Online Library, 2024, p. e15165. [11] Sylvan Barnet, short guide to writing about art, Pearson Upper Saddle River, NJ, 2015. [12] Diana Kim, Ahmed Elgammal, and Marian Mazzone, Formal analysis of art: Proxy learning of visual concepts from style through language models, arXiv preprint arXiv:2201.01819, 2022. [13] OpenAI, Gpt-4 technical report. arxiv 2303.08774, View in Article, vol. 2, no. 5, 2023. [14] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [15] Anthropic, Meet claude, https://www.anthropic.com/claude. [16] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine, Training diffusion models with reinforcement learning, arXiv preprint arXiv:2305.13301, 2023. [17] Roland Clark Davis, An evaluation and test of birkhoffs aesthetic measure formula, The Journal of General Psychology, vol. 15, no. 2, pp. 231240, 1936. [18] Yihang Bo, Jinhui Yu, and Kang Zhang, Computational aesthetics and applications, Visual computing for industry, biomedicine, and art, vol. 1, pp. 119, 2018. [19] Simon Hentschel, Konstantin Kobs, and Andreas Hotho, Clip knows image aesthetics, Frontiers in Artificial Intelligence, vol. 5, pp. 976235, 2022. [20] Matthias Wright and Bjorn Ommer, of neural style transfer, Recognition. Springer, 2022, pp. 560576. Artfid: Quantitative evaluation in DAGM German Conference on Pattern [21] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda Ruth Petzold, Gpt-4v (ision) as generalist evaluator for vision-language tasks, arXiv preprint arXiv:2311.01361, 2023. [22] Yipo Huang, Xiangfei Sheng, Zhichao Yang, Quan Yuan, Zhichao Duan, Pengfei Chen, Leida Li, Weisi Lin, and Guangming Shi, Aesexpert: Towards multi-modality foundation model for image aesthetics perception, in Proceedings of the 32nd ACM International Conference on Multimedia, 2024, pp. 59115920. [23] Junjie Ke, Keren Ye, Jiahui Yu, Yonghui Wu, Peyman Milanfar, and Feng Yang, Vila: Learning image aesthetics from user comments with visionlanguage pretraining, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 1004110051. [24] Xun Huang and Serge Belongie, Arbitrary style transfer in real-time in Proceedings of the IEEE with adaptive instance normalization, international conference on computer vision, 2017, pp. 15011510. [25] Leon Gatys, Alexander Ecker, and Matthias Bethge, Image style transfer using convolutional neural networks, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 24142423. [26] Jie An, Siyu Huang, Yibing Song, Dejing Dou, Wei Liu, and Jiebo Luo, Artflow: Unbiased image style transfer via reversible neural flows, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 862871. [27] Zhizhong Wang, Zhanjie Zhang, Lei Zhao, Zhiwen Zuo, Ailin Li, Wei Xing, and Dongming Lu, Aesust: towards aesthetic-enhanced universal style transfer, in Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 10951106. [28] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen, Instantstyle: Free lunch towards style-preserving in text-to-image generation, arXiv preprint arXiv:2404.02733, 2024. [29] Tim Brooks, Aleksander Holynski, and Alexei Efros, Instructpix2pix: Learning to follow image editing instructions, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 1839218402. [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick, Microsoft coco: Common objects in context, in Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13. Springer, 2014, pp. 740755. [31] volunteer team Wikiart, Wikiart dataset, https://www.wikiart.org. [32] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander in Proceedings of Berg, Wan-Yen Lo, et al., the IEEE/CVF International Conference on Computer Vision, 2023, pp. 40154026. Segment anything, [33] Zijie Wang, Evan Montoya, David Munechika, Haoyang Yang, BenDiffusiondb: large-scale arXiv jamin Hoover, and Duen Horng Chau, prompt gallery dataset for text-to-image generative models, preprint arXiv:2210.14896, 2022. [34] Jiwoo Chung, Sangeek Hyun, and Jae-Pil Heo, Style injection in diffusion: training-free approach for adapting large-scale diffusion models for style transfer, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 87958805. [35] Peiyuan Liao, Xiuyu Li, Xihui Liu, and Kurt Keutzer, The artbench dataset: Benchmarking generative models with artworks, arXiv preprint arXiv:2206.11404, 2022. [36] Junyao Gao, Yanchen Liu, Yanan Sun, Yinhao Tang, Yanhong Zeng, Kai Chen, and Cairong Zhao, Styleshot: snapshot on any style, arXiv preprint arXiv:2407.01414, 2024. [37] Gail Sullivan and Anthony Artino Jr, Analyzing and interpreting data from likert-type scales, Journal of graduate medical education, vol. 5, no. 4, pp. 541542, 2013. [38] Joseph Kruskal, On the shortest spanning subtree of graph and the traveling salesman problem, Proceedings of the American Mathematical society, vol. 7, no. 1, pp. 4850, 1956. [39] Younger, Minimum feedback arc sets for directed graph, IEEE Transactions on Circuit Theory, vol. 10, no. 2, pp. 238245, 1963. [40] Ralph Allan Bradley and Milton Terry, Rank analysis of incomplete block designs: I. the method of paired comparisons, Biometrika, vol. 39, no. 3/4, pp. 324345, 1952. [41] Arpad Elo and Sam Sloan, The rating of chessplayers: Past and present, (No Title), 1978. [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al., Chain-of-thought prompting elicits reasoning in large language models, Advances in neural information processing systems, vol. 35, pp. 2482424837, 2022. [43] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala, Adding conditional the control IEEE/CVF International Conference on Computer Vision, 2023, pp. 38363847. to text-to-image diffusion models, in Proceedings of [44] Charles Spearman, The proof and measurement of association between two things, The American journal of psychology, vol. 100, no. 3/4, pp. 441471, 1987. [45] Ronald Aylmer Fisher, Statistical methods for research workers, in Breakthroughs in statistics: Methodology and distribution, pp. 6670. Springer, 1970. A. Detail on MM-StyleBench"
        },
        {
            "title": "APPENDIX",
            "content": "We construct MM-StyleBench by harvesting from existing open-source datasets with the help of MLLMs. For the Content category, 50% of the content images are generated using the Ideogram-v1 text-to-image (T2I) diffusion model with diverse prompts produced by GPT-4. The remaining 50% are randomly sampled from the SA-1B [32] and MS-COCO [30] datasets, with captions generated by Gemini-v1.5 pro [14]. Images from SA-1B are downsampled by factor of two, while those from MS-COCO retain their original resolution. Overall, all images have an average height and width of 895.7 and 811.9 pixels, respectively. The 5th percentiles for height and width are 480 and 427 pixels, respectively, and the 95th percentiles for both dimensions are 1248 pixels. To synthesize fine-grained attribute annotations for the content images and their associated prompts, we first employ Gemini v1.5-pro to automate the annotation process based on predefined attribute set. Each annotation is manually reviewed to ensure accuracy and correctness. The Style subset of MM-StyleBench is derived from WikiArt [31] and DiffusionDB [33]. The process involves two main steps: 1) WikiArt Processing: We extract keywords from WikiArt, focusing on specifications such as art movement, artist, and genre. These base keywords are then expanded and combined using GPT-4 [13] to create diverse set of style descriptors. 2) DiffusionDB Processing: Given the high noise level in DiffusionDB annotations, we utilize an MLLM to preprocess the text prompts within the dataset. Specifically, we extract the style descriptions from each prompt and merge those with similar style specifications. Subsequently, we subsample from these processed style prompts to ensure quality and diversity. For both datasets, style reference images are generated using StableDiffusion [5], with all reference images standardized to resolution of 512512 pixels. Similar to content annotations, we employ MLLMs to annotate the attributes of style prompts. The actual number of sampled content and style images is summarized in Table VI. We plan to open-source MMStyleBench to facilitate reproducibility and support future research endeavors. TABLE VI CONTENT AND STYLE SOURCES MM-STYLEBENCH IS BUILT FROM DIVERSE SOURCES TO ELIMINATE BIAS. Content Style Source Generated MS-COCO SA-1B WikiArt DiffusionDB Number 500 250 764 236 B. Stylization"
        },
        {
            "title": "We present",
            "content": "the stylization result generated by different methods on two content and style combination in Fig. 5. Fig. 4. User Interface for Preference Annotation. We present user with the source image (top), 2AFC (middle) and style prompt (bottom). The user is required to choose the preferred one by clicking on the left or right button. C. Human preference modeling Preference Collection We develop web application to collect response from human annotators. We present the user with the 2AFC question and reference content and style. The content is presented as an image while we present style as prompt, despite some stylization methods are designed for image-driven. We do not present the user with the name of stylization model. This setup is similar as Artist [3], but we utilize 2AFC question instead of grid comparison with Likert scale. The screenshot of user interface could be found in Fig. 4. Sampling algorithm. We formally describe the proposed degree-uniform sub-graph sampling algorithm in Algo. 1. D. Prompt Design We summarize the full prompt of base prompt, zero-shot CoT prompting and ArtCoT prompt in Tab. VII. Notice that we prompt the MLLMs to return in json (or python compatible) format, this is because not all MLLMs are implemented with the json mode. E. Example conversations We present several quantitative comparisons of MLLMs response generated using different prompt in the following pages. We highlight the subjective words of zero-shot CoT. Compared with zero-shot CoT, the responses from ArtCoT is much more detailed and contain less hallucination. Fig. 5. Examples of Stylized Image. We show two uncurated examples from different stylization results, the image order are randomized. The styles are impressionist and cubism, respectively. The results covers wide range of stylization performance, setting realistic and challenging task for artistic evaluation. Algorithm 1: Sample Connected Subgraph with Uniform Degree Distribution Input: = (V, E) ; // Complete graph // Number of edges, 1 (V 1) // Random Number Generator // Subsampled edge set forming connected 2 ; RNG ; Output: ; subgraph 1. Validate Inputs: if < 1 or < 1 or > (V 1) then 2 Error: Invalid input parameters.; 2. Generate Spanning Tree: ET Kruskals MST(G, RNG); 3. Initialize Subgraph and Degrees: ET ; Initialize d(v) = 0, ; foreach = (u, v) ET do d(u) d(u) + 1; d(v) d(v) + 1; 4. Add Remaining Edges: ET ; while > 0 do a. Identify Best Candidate Edges: BestEdges (cid:26) E (cid:12) (cid:12) (cid:12) (cid:12) min max{d(u) + 1, d(v) + 1} (cid:27) BestEdges (cid:26) BestEdges (cid:12) (cid:12) (cid:12) (cid:12) min (cid:88) {d(u) + 1, d(v) + 1} (cid:27) b. Select and Add an Edge: Randomly select from BestEdges using RNG; E {e}; c. Update Degrees and Counter: foreach do d(v) d(v) + 1; 1; return E; TABLE VII TEMPLATE FOR DIFFERENT PROMPTING METHODS. [STYLE] STANDS FOR PLACEHOLDER FOR THE STYLE PROMPT, AND [IMAGE] STANDS FOR PLACEHOLDER FOR IMAGE TOKENS. Base Prompt Zero-Shot CoT [IMAGE] You are an expert in fine art. source image (top) and two different stylized images (bottom) in the style of [STYLE] are presented to you. Consider both the content and style, which stylized image is better in terms of overall aesthetic quality as an artwork? Return your decision in Python Dict, [winner:int]. 0 means the left is better while 1 means the right is better. Do not answer any other things. [IMAGE] {request: You are an expert in fine art. source image (top) and two different stylized images (bottom) in the style of style are presented to you. Consider both the content and style, which stylized image is better in terms of overall aesthetic quality as an artwork?. Return the reason and your dein format of cision in short Python Dict thinking:str, winner:int. 0 means the left is better while 1 means the is better., response: right {thinking: think Lets step by step, C-S Analyzer [IMAGE] You are an expert in fine art. source image (top) Two stylized images (bottom left and bottom right) in the style of [STYLE] are presented to you. Compare the content preservation and style fidelity of the two images, which one is better. Return in Python your answer Dict, [style reason:str, content reason:str, style winner:int, content winner:int]. 0 means the left is better while 1 means the right is better. Do not include any other string in your response. ArtCoT Art Critic [IMAGE] Take closer look the two stylized images at at the bottom in the style of [STYLE]. As an expert in art, do you agree with above analysis? Compare and consider the following questions. What visual features is essential for the style of [STYLE]? Is the content at top well-preserved in the specific art style? Is there any artifact, distortion or inharmonious color patterns in either painting? Return your answer in Python Dict, [reflection:str]. Summarizer [IMAGE] Now we summarize. Based on above analysis and reflection, which stylized image at the bottom is better in terms of overall aesthetic quality as an **painting of the original content (top) in another style**? Return your answer in Python Dict, [winner:int]. 0 means the left is better while 1 means the right is better. Do not include any other string in your response."
        }
    ],
    "affiliations": [
        "The Hong Kong Polytechnic University"
    ]
}