{
    "paper_title": "Best of Both Worlds: Advantages of Hybrid Graph Sequence Models",
    "authors": [
        "Ali Behrouz",
        "Ali Parviz",
        "Mahdi Karami",
        "Clayton Sanford",
        "Bryan Perozzi",
        "Vahab Mirrokni"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern sequence models (e.g., Transformers, linear RNNs, etc.) emerged as dominant backbones of recent deep learning frameworks, mainly due to their efficiency, representational power, and/or ability to capture long-range dependencies. Adopting these sequence models for graph-structured data has recently gained popularity as the alternative to Message Passing Neural Networks (MPNNs). There is, however, a lack of a common foundation about what constitutes a good graph sequence model, and a mathematical description of the benefits and deficiencies in adopting different sequence models for learning on graphs. To this end, we first present Graph Sequence Model (GSM), a unifying framework for adopting sequence models for graphs, consisting of three main steps: (1) Tokenization, which translates the graph into a set of sequences; (2) Local Encoding, which encodes local neighborhoods around each node; and (3) Global Encoding, which employs a scalable sequence model to capture long-range dependencies within the sequences. This framework allows us to understand, evaluate, and compare the power of different sequence model backbones in graph tasks. Our theoretical evaluations of the representation power of Transformers and modern recurrent models through the lens of global and local graph tasks show that there are both negative and positive sides for both types of models. Building on this observation, we present GSM++, a fast hybrid model that uses the Hierarchical Affinity Clustering (HAC) algorithm to tokenize the graph into hierarchical sequences, and then employs a hybrid architecture of Transformer to encode these sequences. Our theoretical and experimental results support the design of GSM++, showing that GSM++ outperforms baselines in most benchmark evaluations."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 1 7 6 5 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "BEST OF BOTH WORLDS: ADVANTAGES OF HYBRID GRAPH SEQUENCE MODELS Ali Behrouz1, Ali Parviz2, Mahdi Karami1, Clayton Sanford1, Bryan Perozzi1, Vahab Mirrokni1 1Google Research, 2New Jersey Institute of Technology {alibehrouz, mahdika, chsanford, bperozzi, mirrokni}@google.com ap2248@njit.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Modern sequence models (e.g., Transformers, linear RNNs, etc.) emerged as dominant backbones of recent deep learning frameworks, mainly due to their efficiency, representational power, and/or ability to capture long-range dependencies. Adopting these sequence models for graph-structured data has recently gained popularity as the alternative to Message Passing Neural Networks (MPNNs). There is, however, lack of common foundation about what constitutes good graph sequence model, and mathematical description of the benefits and deficiencies in adopting different sequence models for learning on graphs. To this end, we first present Graph Sequence Model (GSM), unifying framework for adopting sequence models for graphs, consisting of three main steps: (1) Tokenization, which translates the graph into set of sequences; (2) Local Encoding, which encodes local neighborhoods around each node; and (3) Global Encoding, which employs scalable sequence model to capture long-range dependencies within the sequences. This framework allows us to understand, evaluate, and compare the power of different sequence model backbones in graph tasks. Our theoretical evaluations of the representation power of Transformers and modern recurrent models through the lens of global and local graph tasks show that there are both negative and positive sides for both types of models. Building on this observation, we present GSM++, fast hybrid model that uses the Hierarchical Affinity Clustering (HAC) algorithm to tokenize the graph into hierarchical sequences, and then employs hybrid architecture of Transformer to encode these sequences. Our theoretical and experimental results support the design of GSM++, showing that GSM++ outperforms baselines in most benchmark evaluations."
        },
        {
            "title": "INTRODUCTION",
            "content": "Message-passing graph neural networks (MPNNs) have been the leading approach for processing graph data (Kipf & Welling, 2016; Gilmer et al., 2017a; Chami et al., 2020; Morris et al., 2020). However, with the increasing popularity of Transformer architectures (Vaswani et al., 2017) in natural language processing and computer vision, recent research has shifted towards developing graph Transformers (GTs), which are designed to handle the complexities of graph-structured data more effectively. Graph Transformers have demonstrated compelling results, particularly by leading in tasks like molecular property prediction (Ying et al., 2021; Hu et al., 2020; Masters et al., 2023). Their advantage over traditional MPNNs is often attributed to tendency of MPNNs to focus on local structures, making them less effective at capturing global or long-range relationships due to issues like over-smoothing (Li et al., 2018), over-squashing (Alon & Yahav, 2020; Di Giovanni et al., 2023; Dwivedi et al., 2022b), and restricted expressive power (Barcelo et al., 2020). These limitations could potentially harm the models performance. In contrast, GTs (Rampaˇsek et al., 2022) can aggregate information from all nodes across the graph, reducing the local structure bias. Traditional Transformer-based architectures, while powerful for sequence analysis, face limitations in scalability in long-context tasks due to their quadratic computational complexity. Various strategies have been proposed to mitigate this issue (Tay et al., 2022), including sparsifying the dense attention matrix (Zaheer et al., 2020; Beltagy et al., 2020a; Roy et al., 2020; Kitaev et al., 2020), low-rank approximations of the attention matrix (Wang et al., 2020), and kernel-based attention"
        },
        {
            "title": "Preprint",
            "content": "mechanisms (Choromanski et al., 2020b; Kacham et al., 2024). While these methods enhance computational efficiency, they often come at the expense of reduced expressiveness (Mehta et al., 2022b). In recent years, attention-free sequence models have emerged as promising alternative to Transformers for sequence modeling. Leveraging recurrent neural networks (RNNs) (Orvieto et al., 2023; Peng et al., 2023; Beck et al., 2024; Behrouz et al., 2024a) and long convolutions (Poli et al., 2023; Karami & Ghodsi, 2024), offer sub-quadratic, hardware-efficient sequence mixing operators that can capture long-range dependencies with strong generalization on sequences of varying lengths. Given the promising potential of the sub-quadratic sequence models, there is growing interest in extending them to the graph domain as an alternative to graph Transformers (Ding et al., 2023; Behrouz & Hashemi, 2024; Huang et al., 2024). However, significant technical challenge arises from the inherent differences between graphs and other structured data, such as text, which is naturally causal. Graphs exhibit complex topology and lack natural, linear node ordering. Attempting to impose naive tokenization strategy, such as sorting nodes into sequence, undermines the crucial inductive bias of permutation equivariance inherent to graphs. This misrepresentation of graph structure can lead to poor generalization performance. Furthermore, there is lack of common foundation about what constitutes good graph sequence model, and mathematical description of the benefits and deficiencies of adopting different sequence models for learning on graphs. In this study, we introduce unified model that offers both flexibility and adaptability for designing models intended for learning on graphs. This approach facilitates the effortless creation of diverse models and allows researchers to efficiently explore and compare various architectures. Using this method, we conduct both theoretical and empirical analyses of graph sequence models by evaluating their performance on tasks such as graph connectivity and counting, as well as by investigating the role of node orderings. Our findings indicate that while the permutation equivariance of Transformer-based models is often considered desirable property, it limits their capacity to execute counting tasks on graphs effectively. Additionally, we demonstrate that SSM/RNN based models, are not theoretically bounded when applied to color counting tasks, highlighting the power of these architectures in specific graph learning scenarios. These findings are the first steps toward better understanding of the power of graph sequence models beyond traditional metrics (e.g., WL test) and can help to answer what types of sequence models are the best, given the type of the task at hand. We identify the strengths and weaknesses of different tokenization strategies, and to overcome their limitations, we present novel hierarchical tokenization that theoretically provides advantages over existing methods. This approach, combined with hybrid sequence model achieves improved performance across diverse graph tasks. Our experiments validate the effectiveness of this hybrid architecture, offering more flexible and comprehensive solution than existing models. These insights contribute to broader understanding of model capabilities and limitations, informing the development of more specialized models for graph-based learning tasks. Contributions and Roadmap. In 2, we present Graph Sequence Model (GSM) framework, that can help us to systematically study the power of GSMs in different scenarios. We then in 3 aim to understand strengths and weaknesses of different types sequence models for graph tasks. To this end, in 3.1, we show how recurrent nature of model can help it to perform tasks like counting more effectively, while permutation equivariance of Transformers make them unable to count. In 3.2, we analyze sequence models through the lens of sensitivity. We show that while linear recurrent models (e.g., SSMs) have better inductive bias about the nodes distance, this advantages can cause representational collapse in deep models. Using these results, we motivate combination of transformers and SSMs so the SSM module can enhance the inductive bias, and the permutation equivariance of Transformer can avoid representational collapse in the model. In 3.3, we evaluate the reasoning capability of graph sequence models through the lens of connectivity tasks. We show that Transformers are more effective than recurrent models in such tasks, but with small modification of the tokens order, recurrent models can become extremely efficient. In 3.4, we theoretically analyze the effect of tokenization methods (node or subgraph), and how it can help to improve the efficiency and solve the fundamental problem of motif counting in graphs. Given our theoretical observations and what we have learned from these results, in 4, we present GSM++ that uses novel tokenization based on the Hierarchical Affinity Clustering (HAC) tree. GSM++ further employs hybrid sequence model with two layers of SSM followed by transformer block. We then present Mixture of Tokenization (MoT), allowing the combination of different sets of tokenizations that are the best for each node, to further enhance the effectiveness and efficiency of GSM++."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of Graph Sequence Model (GSM). GSM Consists of three stages: (1) Tokenization, (2) Local Encoding, and (3) Global Encoding. We provide foundation for strengths and weaknesses of different tokenizations and sequence models. Finally, we present three methods to enhance the power of GSMs."
        },
        {
            "title": "2 ENCODING GRAPHS TO SEQUENCES: A UNIFIED MODEL",
            "content": "Despite variety of GNNs with diverse modules that are designed based on sequence models, we find that each part of these architectures is responsible for encoding specific characteristic of the graph. To formalize this, in this section, we present our unified model consisting of three main stages: (1) Tokenization, (2) Local Encoding, and (3) Global Encoding. 2.1 TOKENIZATION Sequence models are inherently designed to process sequences of tokens. To adapt them for graphstructured data, the graph must first be translated into set of sequences (Muller et al., 2024; Behrouz & Hashemi, 2024; Ding et al., 2023). These approaches can be categorized into two main groups: Node/Edge Tokenizers. Node or edge tokenization methods treat the graph as sequence of node/edges without considering how they are connected. Accordingly, these methods lack inductive bias about the graph structure and so they require to be augmented with positional or structural encoding to inject information about the graph structure. Let = (V, E), be graph, = {v1, . . . , vV } is the set of nodes, and Rnd is the positional/structural encoding matrix, whose rows encode the position of nodes. In this case, we translate the graph as sequence of: := Pv1, Pv2 , . . . , PvV Similarly, for edge tokenization we can replace {v1, . . . , vV } with {e1, . . . , eE}. The main drawback of methods based on node tokenization is their computational complexity. That is, treating the graph as sequence of nodes (resp. edges) results in having E), meaning that for quadratic models (e.g., Transformers) the sequence with length (resp. training time complexity is at least (cid:0)V 2(cid:1) (resp. (cid:0)E2(cid:1)). Subgraph Tokenizers. To reduce the computational cost of node tokenization and incorporate inductive bias, several methods propose treating the graph as sequence or sequences of subgraphs and then encode these sequences using sequence model. Formally, given graph = (V, E), the graph can be represented as set of sequences of subgraphs: := {S(1), . . . , S(T )}, where S(i) = G[H (i) 1 ], . . . , G[H (i) ℓ ] and (i) V. (1) When < , we refer to this process as patching. pioneer approach in this direction is DeepWalk (Perozzi et al., 2014), which uses random walks to sample from the graph and tokenize it into set of sequences. more recent method is the k-hop neighborhood tokenization used by NAGphormer (Chen et al., 2023), where each nodes hierarchical neighborhood is treated as its representative sequence. For further discussion and examples of these methods, see Appendix B. Since using = , ℓ = 1, and (i) 1 = {vi} for = {1, . . . , }, reduces subgraph tokenization to node tokenization, unless stated otherwise, we will use this formulation moving forward. Although there is variety of studies across the aforementioned categories, common foundation is still lacking regarding what constitutes effective tokenization and what differentiates them with respect to the task. In Section 3, we theoretically show that each of node and subgraph tokenizations offer their own advantages and disadvantages. Accordingly, the choice between node tokenization, subgraph tokenization, or combination of both depends on the specific task at hand (see Section 4.3). We further validate this theoretical foundation using several experiments in Section 5.1."
        },
        {
            "title": "2.2 LOCAL ENCODING",
            "content": "Following the tokenization step, where the graph is translated into set of sequences (representing nodes, edges, or subgraphs), the main objective of the Local Encoding step is to capture and learn the graphs local characteristics by vectorizing these tokens. Formally, given graph = (V, E), let denote the set of all subgraphs, and ϕLocal(.) : RdLocal represent GNN encoder. With the graph tokenized as in Equation 1, we define the local encoding as: ϕLocal (G) := { S(1), . . . , S(T )} where S(i) = ϕLocal (cid:16) (cid:17) G[H (i) 1 ] , . . . , ϕLocal (cid:16) G[H (i) ℓ ] (cid:17) . (2) While the choice of encoder ϕ(.) is arbitrary, convolutional MPNNs are typically preferred due to their ability to effectively learn local dependencies around each node. As an illustrative example of this step and its goal, assume that the k-hop neighborhood tokenization was used in the previous step, then each S(i) represents sequence describing the hierarchical neighborhood around node vi and ϕLocal is the encoding of j-th hop neighborhood of vi. (cid:17) (cid:16) ] G[H (i) j"
        },
        {
            "title": "2.3 GLOBAL ENCODING",
            "content": "As discussed, the local encoding stage serves two key roles: (1) It encodes the local characteristics of the graph, injecting inductive bias in the model; and (2) it vectorizes the tokens, preparing them for sequence encoder in the Global Encoding stage. Here, the main objective is to learn dependencies across all tokens, enabling the model to capture long-range relationships. Formally, let S(i)s be the sequences of encodings obtained from the local encoding stage, for each = 1, . . . , , we have: y(i) = Ψi (cid:16) AGGi (cid:16) S(1), S(2), . . . , S(T )(cid:17)(cid:17) , (3) where Ψi(.) are sequence models and AGGi(.) are aggregator functions. In most existing node tokenization-based methods AGGi(.) = CONCAT(.) (concatenation), while in most subgraph tokeniztion-based methods AGGi(.) = (.)i (broadcasting i-th element). However, sequence models themselves can be used as aggregator functions, as demonstrated by Behrouz & Hashemi (2024). In Appendix C, we illustrate that several well-known methods for learning on graphs are special instances of this Graph Sequence Model (GSM) framework, highlighting it universality."
        },
        {
            "title": "3 CHOOSING A SEQUENCE MODEL",
            "content": "One critical question remains what sequence model should one use? Following the above mentioned framework, one can simply replace different sequence encoders in the global encoding stage and combine them with different tokenization methods, resulting in hundreds of potential graph learning models. However, there is lack of common foundation about what constitutes good model in each of these stages, and mathematical description of the benefits and deficiencies of adopting different sequence models for learning on graphs. To this end, in this section, we theoretically discuss the advantages and disadvantages of different tokenizations and different sequence models in several graph tasks, providing guideline for the future research and model developments. 3.1 COUNTING TASKS ON GRAPHS In the first part, we focus on counting tasks, where the objective is to count the number of nodes with each particular color in node-colored graph. Such tasks are analogous to the copying tasks in sequence modeling, which are common benchmarks to measure the abilities of sequence model (Arjovsky et al., 2016; Gu & Dao, 2023; Barbero et al., 2024), in the sense that counting tasks require considering all nodes and even missing single nodes color can potentially result in incorrect prediction. Hence, lets first recall proposition on the inability of Transformers in counting tasks: PROPOSITION 1 on non-causal attention and without proper positional encodings is immediately unable to count. (PROPOSITION 6.1 OF BARBERO ET AL. (2024)). Transformer model based This limitation of non-causal Transformers in counting tasks raises an important question: Can the inherent causality of recurrent models resolve this issue, and are they better suited for such tasks?"
        },
        {
            "title": "Preprint",
            "content": "THEOREM 1. Let be the number of colors, and be the width of recurrent model, the recurrent model can count the number of nodes with each specific color iff C. Takeaway. When dealing with sequential tasks that are less dependent on the graphs topology and permutation equivariance, recurrent models are more powerful than non-causal Transformers. 3."
        },
        {
            "title": "IMPORTANCE OF NODE ORDERING",
            "content": "As discussed earlier, due to the sequential nature of some graph tasks, the permutation equivariant property of non-causal Transformers can undermine their representational power. Beyond simple counting tasks, several important and complex graph datasets and taskssuch as neural algorithmic reasoning tasks in sequential algorithms (Xu & Veliˇckovic, 2024) and CLRS dataset (Bentley, 1984; Gavril, 1972)involve naturally ordered nodes, requiring causal encoder to effectively capture their inherent order. On the other hand, most subgraph tokenizers produce sequences with an implicit order (e.g., k-hop neighborhoods), which requires causal model to capture their hierarchy. Furthermore, most powerful modern sequence models are naturally causal and integrating them into the GSM framework requires additional considerations. Accordingly, in this section, we analyze how node ordering can impact the performance of the model, and if there is an ordering mechanism for nodes that can enhance the performance of causal sequence models. Sensitivity Analysis. Over-squashing is an undesirable phenomenon in GNNs that is related to representational collapse. One way to analyze over-squashing in model is to study how sensitive is the final output token to an input token at position i: i.e., yj , where yj and xi are output and input xi of the model at position and i, respectively. Next, we discuss the sensitivity of SSMs (with HiPPO initialization (Gu et al., 2020)) along the models depth after layers: THEOREM 2. For any > let A(k, i) = (1 1 and be the number of layers. For any < n, the gradient norm of the HiPPO operator for the output of layer at time + 1 (i.e., y(L) n+1) with respect to input at time (i.e., xi) satisfies: k1 ) . . . (1 1 )(1 1 ) C(L) low (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) k1i (cid:88) . . . kLkL (n 1, kL) L1 (cid:89) ℓ=2 (cid:12) (cid:12) (cid:12) (kℓ 1, kℓ1) (k1 1, i) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) y(L) n+1 xi C(L) up (cid:19)L (cid:18) 1 COROLLARY 1. In SSMs, the sensitivity of the output with respect to previous token, i.e., yk , is xi decreasing function of their distance (i.e., = i). Therefore, closer tokens have higher impact on each others encodings. Notably, this property is distinctive trait of SSMs and contrasts with Transformers, which exhibit constant sensitivity (Song et al., 2024). However, the following corollary to Theorem 2 reveals that SSMs also suffer from representational collapse as the number of layers grows, behavior which was also observed in causal Transformers (Barbero et al., 2024). Therefore, SSMs offer no advantage in this aspect. COROLLARY 2. Let be the number of layers in the recurrent model. As , the output representation depends only on the first token. In both causal Transformers and SSMs, the information about tokens located near the start of the sequence have more opportunity to be maintained at the end. This might seem counter-intuitive for recurrent models like SSMs, which are expected to exhibit recency bias towards the recent tokens due to their constant size hidden state. However, note that this result differs from recency bias in recurrent models as it concerns the information flow along the sequence dimension rather than across the models depth. Interestingly, together with their recency bias, this new result indicates U-shape effect in SSMs, meaning that information from tokens at both the beginning and end of sequence is better preserved, phenomenon also observed in causal Transformers (Barbero et al., 2024). Takeaway. This part yields three key insights: (1) When nodes are naturally ordered, SSMs posses stronger inductive bias than Transformers, as they are sensitive to the tokens distance. (2) Both causal Transformers and SSMs can suffer from representational collapse, limiting their representational power. The fact that non-causal transformer are permutation equivariant and so does not suffer from representational collapse motivates the exploration of hybrid models that combine SSMs with"
        },
        {
            "title": "Preprint",
            "content": "non-causal Transformers to take advantage of SSMs inductive bias while avoiding representational collapse (see Section 4.2). (3) When ordering nodes (e.g. to model hierarchy or for sequential tasks), it is advantageous to place relevant nodes close together as it results in high sensitivity with respect to similar nodes and less sensitivity with respect to less relevant, dissimilar ones. To this end, in Section 4.1, we present tokenization method that implicitly orders nodes based on similarity."
        },
        {
            "title": "3.3 CONNECTIVITY TASKS ON GRAPHS",
            "content": "This section addresses the graph connectivity task, which requires the sequence model to capture global understanding of the graph. We frame graph connectivity as binary classification problem, where the input is tokenized graph = (V, E), and the target output is 1 if is connected and 0 otherwise. Using edge tokenization, we represent the graph as the sequence := Pe1, . . . , PeE. (COROLLARY 3.3 OF SANFORD ET AL. (2024B)). For any and ϵ (0, 1), COROLLARY 3 there exists transformer with depth O(log ) and embedding dimension O(N ϵ) that determines whether any graph = (V, E) with , is connected. Next, we show that alternative architectures cannot solve graph connectivity with such lowdimensional parameterization. THEOREM 3. multi-layer recurrent model, Transformer with kernel-based sub-quadratic attention, or Transformer with locally masked attention units of radius that solves graph connectivity on all graphs = (V, E) with , has either depth = Ω(N 1/8) or = Ω(N 1/4). As result, these attempts to improve the quadratic computational bottleneck result in lack of parameter-efficient connectivity solutions. All recurrent models, kernel-based transformers with kernel dimension = O(N 1/8), and all local transformers with window size = O(N 1/8) require at least Ω(N 1/8) parameters. When are recurrent models more efficient? The main benefits of recurrent models, including SSMs, is when either the data comes with natural ordering, or the encoding (in Tokenization and Local Encoding stages) has carefully embeded the graph structure in the order of tokens. To formalize this, we define notion of locality for an edge embedding and show that this induces easy embeddings for recurrent models but not for transformers. DEFINITION 1. Let the node locality of an edge embedding Pe1 , . . . , PeE of graph = (V, E) denote the maximum window size needed to contain all edges that adjoin each node. That is, we say that has node locality if maxvV (arg maxi{ei : ei} arg mini{ei : ei}) k. We show that graphs with bounded node locality admit time/parameter-efficient recurrent solutions. THEOREM 4. There exists single-pass recurrent model with hidden state O(k) that determines whether edge embedding with node locality at most reflects connected graph. Interestingly, no constant-size transformer that solves the above task exists. We prove this by reduction to the conditional hardness of solving NC1-complete problems with constant depth transformers (see e.g. Merrill & Sabharwal, 2023). THEOREM 5. Unless NC1 = TC0, any log-precision transformer that solves graph connectivity on edge embeddings with , and node locality 12 requires either depth ω(1) or width ω(1). Takeaway. In graph connectivity, as an example of global task, Transformers are more powerful than recurrent methods in general cases. However, with good choice of tokenizer and ordering, recurrent models can become extremely efficient and powerful. See Appendix for detailed discussion and comparison of Transformers with recurrent models. Following this insight, in Section 4.1, we present new tokenization that can provide us with such desirable ordering. 3.4 CHOOSING THE RIGHT TOKENIZER: NODE, EDGE, OR SUBGRAPH While so far we have compared different sequence models for use in Global Encoding stage, one critical question remains: What type of tokenization is the best? In this section, we show that there is no universally best tokenization, and depending on the task, the best tokenizer is different. First, we start with the task of finding the length of the shortest path, and show that GSMs are more parameter"
        },
        {
            "title": "Preprint",
            "content": "efficient when using subgraph tokenizer. This is an important task in graph learning, as awareness of the shortest path can enhance the power of the model (Abboud et al., 2022). THEOREM 6. There exists GSM with subgraph tokenizer and 1-layer Transformer as its global encoder with width = (log dG) and precision = (log dG) that performs the above shortest path task for all input graphs of = (V, E) with diameter up to dG. Using node tokenizer, the Transformer must have at least width = (log ) and precision = (log ). Next, we focus on motif counting (e.g., triangles), which is well established graph task. THEOREM 7. For any fixed subgraph of diameter at most k, there exists k-hop local encoding ϕLocal and single-layer Transformer of constant width such that ϕLocal counts the number of occurrences of in any input graph G. The above two positive results, along with the negative results discussed in Appendix F.2, provide evidence that subgraph tokenizers are useful when extra attention on local structures is needed. On the other hand, when dealing with long-range dependencies and global graph tasks, node/edge tokenizers are more efficient choices. For the negative results of using subgraph tokenization, more details, and additional discussions about choosing the tokenizer see Appendix F."
        },
        {
            "title": "4 ENHANCING GRAPH TO SEQUENCE MODELS",
            "content": "4.1 HIERARCHICAL AFFINITY CLUSTERING (HAC) FOR TOKENIZATION As discussed in Section 3.2, using tokenizer that generates an ordered sequence, where similar nodes are positioned near each other, can improve the sensitivity of the method, thereby enhancing its representational power. Furthermore, as discussed in Section 4, when representing graph as sequence with node locality (Definition 1), powerful recurrent models become very efficient for global tasks like connectivity. Motivated by these results, we present hierarchical tokenization based on the Hierarchical Affinity Clustering (HAC) (Bateni et al., 2017) algorithm and show that it satisfies the above desirable characteristics. HAC is highly scalable and parallelizable clustering algorithm based on Boruvkas algorithm (Boruuvka, 1926) (see Appendix A.3 for backgrounds). Given graph = (V, E) and node encodings Pv1, . . . , PvV , the algorithm begins by treating each vertex as singleton cluster, then at each step removes the cheapest edge (cost calculated by the similarity of node encodings) going out of each cluster and join these two clusters to form larger cluster. This process continues until cluster includes all the nodes. The stages of this algorithm form HAC tree, where the root represents the last cluster in the algorithm (entire graph), its two children are the last two clusters in one round before the end of the algorithm, and so forth. Accordingly, leaves are nodes of the graph, which were our initial clusters. See Figure 2 for an example of HAC tree. HAC offers two key advantages for an effective tokenization. First, it orders nodes such that adjacent nodes (having the same parent node) in the tree are the most similar, which is aligned with our theoretical analysis. Second, it provides hierarchical clustering, allowing for graph encoding at different levels of granularity. We propose two types of tokenization based on Depth-First Search (DFS) and Breadth-First Search (BFS) traversals of the HAC tree. DFS Traverse of HAC Tree. After performing HAC and constructing the HAC tree, we perform DFS traverse and treat each path as sequence. That is, given graph = (V, E), let be the root of the tree, DFS path in the HAC tree is = ci = vi , where represents the entire graph and ci represents node vi . This sequence represents hierarchy of clusters whose nodes are similar to vi, and encodes the hierarchical position of vi in the graph. This approach is subgraph-based tokenization as discussed in Section 2.1. 2 ci 1 ci BFS Traverse of HAC Tree. In this approach, we perform BFS traverse on the HAC tree. Note that the maximum depth of the tree is log2(V ) (Bateni et al., 2017). Let log2(V ), we treat k-th level of BFS traverse as path, representing the graph at k-th level of granularity. When = 1, the length of the sequence is one and the only element is the root (entire graph). When is the depth of the tree, the sequence is the sequence of all nodes, but in an order that similar nodes are close to each other. In this tokenization method, we construct the sequences for all values of"
        },
        {
            "title": "Preprint",
            "content": "1 log2(V ) and encode the graph at different levels of granularity. We consider simple average pooling to obtain the overall encodings. THEOREM 8. Given graph with minimum node locality of (Definition 1), there exists node embedding that HAC (BFS) tokenization, order nodes in way that the sequence is k-local. This theorem, along with Theorem 4, motivates us to use HAC tokenization with recurrent model as the global encoder later in our final architecture design. Hierarchical Positional Encoding. One of the main advantages of HAC is its ability to provide us with rich information about the hierarchy of structures in the graph. Inspired by recent studies that show the power of hierarchy-aware positional encodings (Luo et al., 2024), we present new PE based on the shortest path of clusters including two nodes of interest v, . We define Pv,u = [d(1) u,v is the length of the shortest between the clusters that include these nodes at the i-level of HAC tree. This positional encoding not only considers the shortest path of and (d(log(V )) is the length of their shortest path), but it also encodes their relative position in different levels of granularity. We experimentally show that this positional encoding is very effective. ] as the relative positional encoding of and such that d(i) u,v . . . d(log(V )) u,v d(2) u,v u,v 4.2 HYBRID MODELS As discussed in Section 3.2, sequential combinations of recurrent models with transformer layers can results in model with higher representational power. THEOREM 9 instance of graph connectivity more efficient than 2-layer recurrent model or transformers. (INFORMAL). There exists hybrid recurrent + Transformer model that solves an For detailed theoretical discussion on the importance of hybrid models see Appendix E.3. Motivated by these theoretical results, we suggest 2-layer hybrid block, where the first layer is Mamba (Gu & Dao, 2023) and the second layer is Transformer block (Vaswani et al., 2017). We further experimentally show the significance of this hybrid design. 4.3 MIXTURE OF TOKENIZATION (MOT) In Section 3.4 we show that there is not single type of tokenization that works best in all the cases. We further experimentally observe the same in Section 5.2. To this end, we suggest using Mixture of Tokenization (MoT) technique, where we allow each node to use tokenization that best describes its position based on the task. For example, one node might be better to be represented by itself (along with positional encoding) since its neighborhood is extremely noisy. At the same time, another node might be better to be represented by its neighbors as there is strong homophily in that area of the graph. Let be the list of different tokenizers, we use discrete router that chooses top-2 tokenizations from for each node. We then concatenate the encodings of these tokenizers to obtain the final encoding for the global encoding step. See Appendix A.4 for additional information. 4.4 GSM++: POWERFUL HYBRID MODEL In this section, we take the advantage of our observation from our theoretical results and use the techniques we presented in Section 4 to design powerful instance of GSMs. For the tokenization, we use our HAC-based tokenization that allows for both node, and subgraph tokenization with desirable ordering (i.e., similar nodes are close to each other). We use GSM++(BFS) and GSM++(DFS) to refer to the BFS and DFS traverse of the HAC tree, respectively. We further use our hierarchical positional encoding that can encode the distances of nodes at different levels of granularity. As the local encoding and to vectorize the subgraphs, we use GatedGCN (Bresson & Laurent, 2017). Finally, we use hybrid global encoder by using Mamba and Transformer sequentially."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Research Questions. In our experiments, we aim to empirically validate the key claims of this paper and compare the performance of our final model, GSM++, with state-of-the-art methods. Specifically, we aim to answer: (1) Is there tokenizer that consistently outperforms other types of"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Graph tasks that require local information. The first and second best results of each type are highlighted. The best overall result for each task is marked *. Model Node Degree Cycle Check Triangle Counting 1K 100K Accuracy 1K Accuracy 100K Erdos-Renyi Regular RMSE GCN GatedGCN MPNN GIN Node HAC (DFS) k-hop HAC (BFS) Node HAC (DFS) k-hop HAC (BFS) 9.3 29.8 98.9 36.4 29.9 31.0 97.6 98.1 30.4 32.6 98.5 98.1 Reference Baselines 9.5 11.6 99.1 35.9 30.1 31.0 98.9 98. 30.9 33.6 98.7 99.0 80.3 86.2 99.1* 98.2 80.2 83.4 99.9* 81.8 Transformers 30.8 58.9 91.6 91.9 Mamba 31.2 33.7 90.5 93.7 31.2 61.3 94.3 92.5 33.8 34.2 93.8 93.5 0.841 0.476 0.417* 0.659 0.713 0.698 0.521 0.574 0.719 0.726 0.601 0. Hybrid (Mamba + Transformer) 2.18 0.772 0.551 0.449* 1.19 1.00 0.95 0.97 1.33 1.08 0.88 0.92 31.0 32.9 99.0* 98.6 Node 31.5 1.27 HAC (DFS) 33.9 1.11 k-hop 90.8 0.84 93.9 0.90 HAC (BFS) We exclude the results of random walk tokenization as their stochastic nature can considerably damage their performance in these tasks. 0.706 0.717 0.598 0.509 31.6 33.7 99.2* 98.5 31.7 33.6 91.1 94.0 Table 2: Graph tasks that require global information. The first and second best results of each type are highlighted. The best overall result for each task is marked *. Model Connectivity Color Counting Shortest Path 1K 100K Accuracy 1K 100K Accuracy 1K 10K RMSE GCN GatedGCN MPNN GIN Node w/o PE HAC (DFS) k-hop HAC (BFS) Node w/o PE HAC (DFS) k-hop HAC (BFS) Reference Baselines 70.8 77.5 76.1 74. 52.7 55.0 53.9 52.4 Transformers 86.2 6.8 88.1 70.2 76.7 73.1 35.8 83.7 79.9 74.5 Mamba 84.7 7.5 85.2 71.0 77. 80.1 78.9 85.2 82.6 83.7 55.9 56.6 57.7 55.1 77.4 28.9 85.3 80.3 77.8 82.5 81.3 85.4 83.5 84.1 63.3 74.9 71.8 71.9 85.7 9.4 87.0 69.9 74. 82.8 9.2 83.6 70.9 76.3 2.38 1.98 1.96 2.03 1.19 4.12 1.14 2.10 2.31 1.27 4.09 1.12 2.03 2.24 Hybrid (Mamba + Transformer) Node w/o PE HAC (DFS) k-hop HAC (BFS) 88.1 8.9 90.7* 70.8 78.0 88.6 8.1 91.4* 73.3 79.5 82.9 83.2 85.8* 83.7 83.1 83.0 84.8 86.2* 84.6 83.7 1.24 4.65 1.11* 1.99 2.16 2.11 1.93 1.93 1. 1.06* 5.33 1.09 2.15 2.28 1.13 5.22 1.15 2.11 2.18 1.13 4.89 1.93 2.04 2.13 tokenization methods? (See Table 1 and Table 2) (2) Is there Global Encoder (e.g., sequence model) that consistently outperforms other models? (See Figure 3) (3) What is the performance of GSM++ compared to existing state-of-the-art methods on benchmark datasets? (See Table 3, and Table 7, 8) (4) How does each component of GSM++ contribute to its performance? (See Table 4) Graph Tasks. We conduct experiments on: (1) Local tasks: node degree, cycle check, and triangle counting, and (2) Global Tasks: connectivity, color counting, and shortest path. These tasks are known for evaluating the ability of models in learning from graphs (Sanford et al., 2024a; Fatemi et al., 2023). For the benchmark tasks on the comparison of GSM++ with baselines, we use node classification and graph classification (Dwivedi et al., 2022b; 2023; Platonov et al., 2023; Rampaˇsek & Wolf, 2021). See Appendix for the details of tasks and datasets. Baselines. We use state-of-the-art GTs, recurrent-based, and MPNNs as our baselines. We also perform ablation studies by replacing various sequence models with each other. The full list of the sequence models, and the details of baselines are in Appendix H. 5.1 ON THE EFFECT OF TOKENIZATION AND GLOBAL ENCODER Local Tasks. The results are reported in Table 1. Interestingly, MPNNs have outstanding performance due to their ability to capture local structures. Comparing node-based tokenizer (i.e., Node and HAC (DFS)) with subgraph-based tokenizer (i.e., k-hop and HAC), subgraph-based tokenizers perform significantly better in these tasks, mainly due to their local inductive bias about the structure of the graph. Models using node-based tokenizers lack implicit inductive bias and rely on the global positional encodings. Global Tasks. The results are reported in Table 2. In global tasks, node tokenizers outperforms subgraph tokenizers. The main intuition behind this result is that these tasks require global knowledge about the graph structure and looking at subgraphs can results in missing information about far nodes (or missing long-range dependencies). The only exception is color counting, which is parallelizable task, meaning that the model can counts by aggregating information obtained from different subgraph tokens. Takeaways. Considering both tables, we conclude that while none of Mamba or Transformer performs the best across all tasks, the hybrid model improves the performance in most cases, indicating the significance of hybrid approaches to take advantage of both worlds. Note that we fix the number of parameters for all models. These results are also aligned with our theoretical discussions. 5.2 IS THERE SUPERIOR MODEL AMONG SIMPLE GSMS? To answer this question, we perform an extensive evaluation with all the combinations of 9 different sequence models and 6 types of tokenizers over 7 datasets of Citeseer, Cora, Computer, CIFAR10,"
        },
        {
            "title": "Preprint",
            "content": "Table 3: GNN benchmark datasets (Dwivedi et al., 2023). The first, second, and third best results are highlighted. Model GCN GraphSAGE GAT SPN GIN Gated-GCN CRaWl NAGphormer GPS GPS (BigBird) Exphormer NodeFormer DIFFormer GRIT GRED GMN GSM++ (BFS) GSM++ (DFS) GSM++ (MoT) MNIST Accuracy CIFAR10 Accuracy PATTERN Accuracy 0.90710.0021 0.97310.0009 0.95540.0021 0.83310.0446 0.96490.0025 0.97340.0014 0.97940.050 - 0.98110.0011 0.98170.0001 0.98550.0003 - - 0.98100.0011 0.98380.0002 0.97830.0020 0.98480.0012 0.98290.0014 0.98840.0015 0.55710.0038 0.65770.0030 0.64220.0046 0.37220.0827 0.55260.0152 0.67310.0031 0.69010.0259 - 0.72260.0031 0.70480.0010 0.74690.0013 - - 0.76460.0088 0.76850.0019 0.74440.0009 0.76590.0024 0.76920.0031 0.77810.0028 0.71890.0033 0.50490.0001 0.78270.0019 0.86570.0014 0.85390.0013 0.85570.0008 - 0.86440.0003 0.86640.0011 0.86000.0014 0.86700.0003 0.86390.0021 0.87010.0018 0.87190.0008 0.86750.0002 0.86490.0019 0.87380.0014 0.87310.0008 0.87930.0015 MalNet-Tiny Accuracy 0.81000.0000 0.87300.0002 0.85090.0025 0.64070.0581 0.88980.0055 0.92230.0065 - - 0.92980.0047 0.92340.0034 0.94020.0020 - - - - 0.93520.0036 0.94170.0020 0.93890.0024 0.94370.0058 Table 4: Ablation studies. The first and second best results for each model are highlighted. Model COCO-SP PascalVOC-SP PATTERN Accuracy F1 score F1 score Base +Hybrid +HAC +MoT Base +Hybrid +HAC +MoT Base -PE -Hybrid -HAC GPS Framework 0.3774 0.3789 0.3780 0. 0.3689 0.3691 0.3699 0.3703 NAGphormer Framework 0.3458 0.3461 0.3507 0.3591 0.3789 0.3780 0.3767 0.3591 0.4006 0.4046 0.4032 0.4105 GSM++ 0.4128 0.4073 0.4058 0.3996 0.8664 0.8665 0.8667 0.8677 0.8644 0.8650 0.8653 0.8657 0.8738 0.8511 0.8500 0.8617 Photo, PATTERN, and Peptides-Func from Dwivedi et al. (2022a; 2023); Chen et al. (2023). Due to the large number of cases (9 6 = 54 models with 54 7 = 378 experimental results), we visualize the rank of the model (higher is better), instead of reporting them in table. The normalized results are reported in Figure 3. These results indicate that there is no model that significantly outperforms others in most cases, validating our theoretical results that each of the sequence models as well as the types of tokenization has their own advantages and disadvantages. Accordingly, we need to understand the spacial traits of these models and use them properly based on the dataset and the task. Following our results, we conjecture that the no free lunch theorem applies for the Graph2Sequence. 5.3 THE EFFECT OF PROPOSED ENHANCEMENTS ON GSMS: ABLATION STUDIES We perform two types of ablation studies: (1) We start with two commonly used frameworks of GraphGPS (Rampaˇsek et al., 2022) and NAGphormer (Chen et al., 2023) that use node-based and subgraph-based tokenization, respectively. We then (i) replace their transformer with hybrid model, (ii) use HAC instead of their tokenization, and (iii) use MoT; (2) We remove components of GSM++, one at time, to see the effect of (i) hierarchical positional encoding, (ii) hybrid sequence encoder, and (iii) HAC tokenization. The results are reported in Table 4. All the components of GSM++ have an impact on its superior performance, where most contribution comes from HAC tokenization, followed by hybrid sequence encoder, and hierarchical PE. Also, we can conclude that using hybrid sequence models, HAC tokenization, and Mixture of Tokens, all have positive impact on the performance of other models, showing that the presented enhancement techniques are effective in practice. Supporting our theoretical results (Theorems 4 and 8), HAC has higher impact on recurrent models than Transformers. 5.4 PERFORMANCE OF GSM++ ON BENCHMARK TASKS We also followed the literature and compare the performance of GSM++ with state-of-the-art methods in node and graph classification tasks on commonly used benchmark datasets (Dwivedi et al., 2022a; 2023; Platonov et al., 2023). The results are reported in Tables 3, 7, and 8. These results show that GSM++ achieves good performance and outperforms baselines in 8/10 cases. We attribute this superior performance of GSM++ to: (1) its ability to capture hierarchical structure of the graph and having proper sensitivity with respect to important nodes through proper ordering, which is the result of HAC tokenization and hierarchical PE; and (2) using hybrid sequence model."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we aim to understand Graph Sequence Models, family of graph learning models that translate the graph into (set) of sequence(s), vectorize it, and then employ powerful sequence models to learn dependencies of nodes. We provide extensive theoretical results to show the importance of ordering, when it is needed, and to show that there is no single sequence model or tokenization method that works strictly better for all graph algorithmic problems. Motivated by our theoretical results, we present model, called GSM++, with new hierarchical graph tokenization method based on HAC, new mixture of token (MoT) approach to take advantage of different tokenization, and hybrid sequence model based on Mamba and self-attention. Our experimental evaluations support the theoretical results and the design of GSM++."
        },
        {
            "title": "REFERENCES",
            "content": "Ralph Abboud, Radoslav Dimitrov, and Ismail Ilkan Ceylan. Shortest path networks for graph property prediction. In Learning on Graphs Conference, pp. 51. PMLR, 2022. Josh Alman and Zhao Song. Fast attention requires bounded entries, 2023. Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. ArXiv, abs/2006.05205, 2020. Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical imIn International Conference on Learning Representations, 2021. URL https: plications. //openreview.net/forum?id=i80OPhOCVH2. Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 11201128, New York, New York, USA, 2022 Jun 2016. PMLR. URL https: //proceedings.mlr.press/v48/arjovsky16.html. Guy Bar-Shalom, Beatrice Bevilacqua, and Haggai Maron. Subgraphormer: Subgraph GNNs meet graph transformers. In NeurIPS 2023 Workshop: New Frontiers in Graph Learning, 2023. URL https://openreview.net/forum?id=e8ba9Hu1mM. Federico Barbero, Andrea Banino, Steven Kapturowski, Dharshan Kumaran, Joao GM Araujo, Alex Vitvitskyi, Razvan Pascanu, and Petar Veliˇckovic. Transformers need glasses! information oversquashing in language tasks. arXiv preprint arXiv:2406.04267, 2024. Pablo Barcelo, Egor V. Kostylev, Mikael Monet, Jorge Perez, Juan Reutter, and Juan Pablo Silva. The logical expressiveness of graph neural networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=r1lZ7AEKvB. MohammadHossein Bateni, Soheil Behnezhad, Mahsa Derakhshan, Mohammad Taghi Hajiaghayi, Raimondas Kiveris, Silvio Lattanzi, and Vahab S. Mirrokni. Affinity clustering: Hierarchical clustering at scale. In Neural Information Processing Systems, 2017. Maximilian Beck, Korbinian Poppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Gunter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xLSTM: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024. Ali Behrouz and Farnoosh Hashemi. Graph mamba: Towards learning on graphs with state space In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and models. Data Mining, KDD 24, pp. 119130, New York, NY, USA, 2024. Association for Computing ISBN 9798400704901. doi: 10.1145/3637528.3672044. URL https://doi. Machinery. org/10.1145/3637528.3672044. Ali Behrouz, Michele Santacatterina, and Ramin Zabih. Chimera: Effectively modeling multivariate time series with 2-dimensional state space models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a. URL https://openreview.net/forum? id=ncYGjx2vnE. Ali Behrouz, Michele Santacatterina, and Ramin Zabih. Mambamixer: Efficient selective state space models with dual token and channel selection. arXiv preprint arXiv:2403.19888, 2024b. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020a. Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020b. Jon Bentley. Programming pearls: algorithm design techniques. Commun. ACM, 27(9):865873, September 1984. ISSN 0001-0782. doi: 10.1145/358234.381162. URL https://doi.org/ 10.1145/358234.381162."
        },
        {
            "title": "Preprint",
            "content": "Amartya Shankha Biswas, Talya Eden, Quanquan C. Liu, Slobodan Mitrovic, and Ronitt Rubinfeld. Massively parallel algorithms for small subgraph counting, 2022. URL https://arxiv. org/abs/2002.08299. Otakar Boruuvka. jistem problemu minimalnım. 1926. Xavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint arXiv:1711.07553, 2017. Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher Re, and Kevin P. Murphy. Machine learning on graphs: model and comprehensive taxonomy. J. Mach. Learn. Res., 23:89:189:64, 2020. Dexiong Chen, Leslie OBray, and Karsten M. Borgwardt. Structure-aware transformer for graph representation learning. In International Conference on Machine Learning, 2022. URL https: //api.semanticscholar.org/CorpusID:246634635. Jinsong Chen, Kaiyuan Gao, Gaichao Li, and Kun He. NAGphormer: tokenized graph transformer for node classification in large graphs. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=8KYeilT3Ow. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. ArXiv, abs/2009.14794, 2020a. URL https://api.semanticscholar.org/CorpusID:222067132. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020b. Shumo Chu and James Cheng. Triangle listing in massive networks and its applications. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 11, pp. 672680, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450308137. doi: 10.1145/2020408.2020513. URL https://doi.org/10. 1145/2020408.2020513. Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In International Conference on Machine Learning (ICML), 2024. Chenhui Deng, Zichao Yue, and Zhiru Zhang. Polynormer: Polynomial-expressive graph transIn The Twelfth International Conference on Learning Representations, former in linear time. 2024. URL https://openreview.net/forum?id=hmv1LpNfXa. Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael Bronstein. On over-squashing in message passing neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pp. 78657885. PMLR, 2023. Yuhui Ding, Antonio Orvieto, Bobby He, and Thomas Hofmann. Recurrent distance-encoding neural networks for graph representation learning. arXiv preprint arXiv:2312.01538, 2023. Zifeng Ding, Yifeng Li, Yuan He, Antonio Norelli, Jingcheng Wu, Volker Tresp, Yunpu Ma, and Michael Bronstein. DyGMamba: Efficiently modeling long-term temporal dependency on continuous-time dynamic graphs with state space models. ArXiv, abs/2408.04713, 2024. James Durbin and Siem Jan Koopman. Time series analysis by state space methods. OUP Catalogue, pp. 253, 2001. Vijay Prakash Dwivedi and Xavier Bresson. generalization of transformer networks to graphs. ArXiv, abs/2012.09699, 2020. Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Graph neural networks with learnable structural and positional representations. ArXiv, abs/2110.07875, 2021."
        },
        {
            "title": "Preprint",
            "content": "Vijay Prakash Dwivedi, Ladislav Rampaˇsek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long range graph benchmark. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2232622340. Curran Associates, Inc., 2022a. Vijay Prakash Dwivedi, Ladislav Rampavsek, Mikhail Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and D. Beaini. Long range graph benchmark. ArXiv, abs/2206.08164, 2022b. Vijay Prakash Dwivedi, Chaitanya Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. Journal of Machine Learning Research, 24(43):148, 2023. Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. Talk like graph: Encoding graphs for large language models, 2023. URL https://arxiv.org/abs/2310.04560. Fanica Gavril. Algorithms for minimum coloring, maximum clique, minimum covering by cliques, and maximum independent set of chordal graph. SIAM Journal on Computing, 1(2):180187, 1972. doi: 10.1137/0201013. URL https://doi.org/10.1137/0201013. Mohsen Ghaffari, Fabian Kuhn, and Jara Uitto. Conditional hardness results for massively parallel computation from distributed lower bounds. In 2019 IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS), pp. 16501663, 2019. doi: 10.1109/FOCS.2019.00097. Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, 2017a. Justin Gilmer, Samuel Schoenholz, Patrick Riley, Oriol Vinyals, and George Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 12631272. PMLR, 2017b. Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks, 2016. Albert Gu and Tri Dao. Mamba: Linear-time seq-uence modeling with sele ctive state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33: 14741487, 2020. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured In International Conference on Learning Representations, 2022. URL https: state spaces. //openreview.net/forum?id=uYLFoz1vlAC. Ankit Gupta and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. ArXiv, abs/2203.14343, 2022. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017. Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson. generalization of vit/mlp-mixer to graphs. In International Conference on Machine Learning, pp. 1272412745. PMLR, 2023. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:2211822133, 2020. Chenqing Hua, Guillaume Rabusseau, and Jian Tang. High-order pooling for graph neural networks with tensor decomposition. Advances in Neural Information Processing Systems, 35:60216033, 2022. Yinan Huang, William Lu, Joshua Robinson, Yu Yang, Muhan Zhang, Stefanie Jegelka, and Pan Li. On the stability of expressive positional encodings for graphs. In International Conference on Learning Representations, 2023."
        },
        {
            "title": "Preprint",
            "content": "Yinan Huang, Siqi Miao, and Pan Li. What can we learn from state space models for machine learning on graphs? ArXiv, abs/2406.05815, 2024. Robin John Hyndman, Anne B. Koehler, J. Keith Ord, and Ralph D. Snyder. Forecasting with Exponential Smoothing: The State Space Approach. Springer Science & Business Media, 2008. Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying, 2024. Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. PolySketchFormer: Fast transformers via In Forty-first International Conference on Machine Learning, sketching polynomial kernels. 2024. URL https://openreview.net/forum?id=ghYrfdJfjK. Mahdi Karami and Ali Ghodsi. Orchid: Flexible and data-dependent convolution for sequence modeling. In Thirty-eighth Conference on Advances in Neural Information Processing Systems, 2024. URL https://arxiv.org/abs/2402.18508. Howard Karloff, Siddharth Suri, and Sergei Vassilvitskii. model of computation for mapreduce. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 10, pp. 938948, USA, 2010. Society for Industrial and Applied Mathematics. ISBN 9780898716986. George Karypis and Vipin Kumar. fast and high quality multilevel scheme for partitioning irregular graphs. SIAM Journal on Scientific Computing, 20(1):359392, 1998. doi: 10.1137/ S1064827595287997. URL https://doi.org/10.1137/S1064827595287997. Jinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. Pure transformers are powerful graph learners. Advances in Neural Information Processing Systems, 35:1458214595, 2022. Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Renkun Ni, Bayan Bruss, and Tom Goldstein. Goat: global transformer on large-scale graphs. In International Conference on Machine Learning, pp. 1737517390. PMLR, 2023. Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Letourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. Advances in Neural Information Processing Systems, 34:2161821629, 2021. Weirui Kuang, WANG Zhen, Yaliang Li, Zhewei Wei, and Bolin Ding. Coarformer: Transformer for large graph via graph coarsening. OpenReview, 2021. Dongyuan Li, Shiyin Tan, Ying Zhang, Ming Jin, Shirui Pan, Manabu Okumura, and Renhe Jiang. DyG-Mamba: Continuous state space modeling on dynamic graphs. ArXiv, abs/2408.06966, 2024. URL https://api.semanticscholar.org/CorpusID:271859733. Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably more powerful neural networks for graph representation learning. arXiv: Learning, 2020. Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. Andreas Loukas. What graph neural networks cannot learn: depth vs width. ArXiv, abs/1907.03199, 2019. Yuankai Luo, Hongkang Li, Lei Shi, and Xiao-Ming Wu. Enhancing graph transformers with hierarchical distance structural encoding, 2024. URL https://arxiv.org/abs/2308.11129."
        },
        {
            "title": "Preprint",
            "content": "Liheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, Puneet Dokania, Mark Coates, Philip Torr, and Ser-Nam Lim. Graph inductive biases in transformers without message passing. In International Conference on Machine Learning, pp. 2332123337. PMLR, 2023. Sohir Maskey, Ali Parviz, Maximilian Thiessen, Hannes Stark, Ylli Sadikaj, and Haggai Maron. Generalized laplacian positional encoding for graph representation learning. ArXiv, abs/2210.15956, 2022. Dominic Masters, Josef Dean, Kerstin Klaeser, Zhiyi Li, Samuel Maddrell-Mander, Adam Sanders, Hatem Helal, Deniz Beker, Andrew Fitzgibbon, Shenyang Huang, Ladislav Rampaˇsek, and Dominique Beaini. GPS++: Reviving the art of message passing for molecular property prediction. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https: //openreview.net/forum?id=moVEUgJaHO. Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. ArXiv, abs/2206.13947, 2022a. URL https://api. semanticscholar.org/CorpusID:250089125. Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022b. William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531545, 2023. ISSN 2307-387X. doi: 10.1162/tacl 00562. URL http://dx.doi.org/10.1162/tacl_a_ 00562. William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models, 2024. Christopher Morris, Martin Ritzert, Matthias Fey, William Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and Leman go neural: Higher-order graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 46024609, 2019. Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings. Advances in Neural Information Processing Systems, 33:2182421840, 2020. Luis Muller, Mikhail Galkin, Christopher Morris, and Ladislav Rampaˇsek. Attending to graph ISSN 2835-8856. URL transformers. Transactions on Machine Learning Research, 2024. https://openreview.net/forum?id=HhbqHBBrfZ. Noam Nisan and Avi Wigderson. Rounds in communication complexity revisited. SIAM Journal on Computing, 22(1):211219, 1993. doi: 10.1137/0222016. URL https://doi.org/10. 1137/0222016. Antonio Orvieto, Samuel Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pp. 2667026698. PMLR, 2023. Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthikiran, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan Sokrates Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui Zhu. Rwkv: Reinventing rnns for the transformer era. In Conference on Empirical Methods in Natural Language Processing, 2023. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social represenIn Proceedings of the 20th ACM SIGKDD International Conference on Knowledge tations. Discovery and Data Mining, KDD 14, pp. 701710, New York, NY, USA, 2014. Association for Computing Machinery. ISBN 9781450329569. doi: 10.1145/2623330.2623732. URL https://doi.org/10.1145/2623330.2623732."
        },
        {
            "title": "Preprint",
            "content": "Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, and Jonathan J. Halcrow. Let your graph do the talking: Encoding structured data for llms. ArXiv, abs/2402.05862, 2024. Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. critical look at the evaluation of GNNs under heterophily: Are we really making progress? In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=tJbbQfw-5wv. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. International Conference on Machine Learning, 2023. Ladislav Rampaˇsek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35:1450114515, 2022. Ladislav Rampaˇsek and Guy Wolf. Hierarchical graph neural nets can capture long-range interactions. In 2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP), pp. 16, 2021. doi: 10.1109/MLSP52302.2021.9596069. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Proceedings of TACL, 2020. Clayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow, Bryan Perozzi, and Vahab Mirrokni. Understanding transformer reasoning capabilities via graph algorithms, 2024a. URL https://arxiv.org/abs/2405.18512. Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Transformers, parallel computation, and logIn Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria arithmic depth. Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 4327643327. PMLR, 2127 Jul 2024b. URL https://proceedings.mlr.press/ v235/sanford24a.html. Hamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica Sutherland, and Ali Kemal Sinop. Exphormer: Sparse transformers for graphs. arXiv preprint arXiv:2303.06147, 2023. Yunchong Song, Chenghu Zhou, Xinbing Wang, and Zhouhan Lin. Ordered GNN: Ordering message passing to deal with heterophily and over-smoothing. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= wKPmPBHSnT6. Yunchong Song, Siyuan Huang, Jiacheng Cai, Xinbing Wang, Chenghu Zhou, and Zhouhan Lin. S4g: Breaking the bottleneck on graphs with structured state spaces, 2024. URL https:// openreview.net/forum?id=0Z6lN4GYrO. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. Siddharth Suri and Sergei Vassilvitskii. Counting triangles and the curse of the last reducer. In Proceedings of the 20th International Conference on World Wide Web, WWW 11, pp. 607614, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450306324. doi: 10.1145/1963405.1963491. URL https://doi.org/10.1145/1963405.1963491. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: survey. ACM Computing Surveys, 55(6):128, 2022. Jan Tonshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Walking out of the Weisfeiler Leman hierarchy: Graph learning beyond message passing. Transactions on Machine LearnISSN 2835-8856. URL https://openreview.net/forum?id= ing Research, 2023. vgXnEyeWVY."
        },
        {
            "title": "Preprint",
            "content": "Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. ArXiv, abs/1710.10903, 2017. Petar Veliˇckovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJXMpikCZ. Chloe X. Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. Graph-Mamba: Towards long-range graph sequence modeling with selective state spaces. ArXiv, abs/2402.00789, 2024a. Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equivariant and stable positional encoding for more powerful graph neural networks. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=e95i1IHcWj. Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Xiao Wang, Shiao Wang, Yuhe Ding, Yuehang Li, Wentao Wu, Yao Rong, Weizhe Kong, Ju Huang, Shihao Li, Haoxiang Yang, Ziwen Wang, Bowei Jiang, Chenglong Li, Yaowei Wang, Yonghong Tian, and Jin Tang. State space model for new-generation network alternative to transformers: survey. ArXiv, abs/2404.09516, 2024b. Qitian Wu, Wentao Zhao, Zenan Li, David Wipf, and Junchi Yan. Nodeformer: scalable graph structure learning transformer for node classification. Advances in Neural Information Processing Systems, 35:2738727401, 2022. Kaijia Xu and Petar Veliˇckovic. Recurrent aggregators in neural algorithmic reasoning. arXiv preprint arXiv:2409.07154, 2024. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural In International Conference on Learning Representations, 2019. URL https: networks? //openreview.net/forum?id=ryGs6iA5Km. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=ia5XvxFUJT. Gilad Yehudai, Haim Kaplan, Asma Ghandeharioun, Mor Geva, and Amir Globerson. When can transformers count to n?, 2024. URL https://arxiv.org/abs/2407.15160. Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? Advances in Neural Information Processing Systems, 34:2887728888, 2021. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of GSM++. GSM++ is special instance of GSMs that uses: (1) HAC tokenization, (2) hierarchical PE, and (3) hybrid sequence model."
        },
        {
            "title": "A BACKGROUNDS",
            "content": "A.1 GRAPH TRANSFORMERS The Transformer architecture (Vaswani et al., 2017), consists of sequential chain of layers, each layer being composed of two primary sub-layers: multi-head attention mechanism and fullyconnected feed-forward network. These layers are arranged alternately to form the backbone of the model. Let be graph with node feature matrix Rnd. In each layer ℓ > 0 of graph Transformers, given node feature matrix X(ℓ) Rnd, single attention head computes the following: Attn(X(ℓ)) := Softmax (cid:19) (cid:18) QK dk V, (4) where the Softmax() is applied row-wise, dk denotes the feature dimension of the query (Q) and key (K) matrices, with X(0) := . The matrices Q, K, and are the result of projecting X(ℓ) linearly, := X(ℓ)WQ, := X(ℓ)WK, and := X(ℓ)WV , using three matrices WQ, WK RddK , and WV Rdd, where optional bias terms omitted for clarity. This attention mechanism forms the foundation of the Transformer architecture (also referred to as non-causal Transformers or Softmax Transformers throughout this work while causal Transformers refers to use of causal masking in attention). The extension to multi-head attention, where multiple attention heads operate in parallel, is standard and straightforward. Equation 4 fails to take into account the graph topology, leading to the development of various Positional Encoding (PE) and Structural Encoding (SE) methods aimed at integrating essential structural information into Graph Transformers (GTs). Notably, several approaches have adopted the top-k Laplacian eigenpairs as node PEs, despite the substantial computational demands involved in resolving the sign ambiguity of Laplacian eigenvectors. Likewise, SE methods face considerable computational challenges in determining the distances between all node pairs or in the sampling of graph substructures. Moreover, the standard attention mechanism in Equation 4 generates dense attention matrix, leading to quadratic complexity with respect to the number of nodes. Recent innovations in Graph Transformers (GTs) have introduced scalable models by linearizing the attention matrix and eliminating the need for PE/SE. However, these models have not been extensively analyzed for their practical expressiveness and might underperform compared to the state-of-the-art Graph Neural Networks (GNNs). A.2 RECURRENT MODELS Recurrent Neural Networks (RNNs) are particularly adept at handling sequential data thanks to their inherent capability to maintain an internal memory state. This allows RNNs to preserve contextual information from previous inputs within sequence, making them ideal for tasks such as language modeling, time-series prediction, and speech recognition. Specifically, at each discrete time step t, the standard RNN processes vector xt RD along with the previous steps hidden state ht1 RN to produce an output vector ot RO and update the"
        },
        {
            "title": "Preprint",
            "content": "hidden state to ht RN . The hidden state serves as the networks memory, retaining information about the past inputs it has encountered. This dynamic memory capability allows RNNs to process sequences of varying lengths. Formally, the updates can be described as follows: ht = σ(Whxxt + Whhht1 + bh), ot = Wohht + bo, (5) where Whx RN is the weight matrix responsible for processing model inputs into hidden states, Whh RN represents the recurrent connections between hidden states, and Woh RON is used to generate outputs derived from hidden states. The biases bh RN and bo RO, along with the hyperbolic tangent activation function tanh, introduce non-linearity to the model. In essence, RNNs are nonlinear recurrent models that effectively capture temporal patterns by harnessing the historical knowledge stored in hidden states. In our theoretical results, however, we refer to recurrent model that has general recurrent formula to make the use of the theoretical results to broader context. That is, we define recurrent model as: ht = (ht1, xt), ot = g(ht, xt), where and are arbitrary functions. As an illustrative example, in Equation 5, we have: (ht1, xt) = σ(Whxxt + Whhht1 + bh), g(ht, xt) = Wohht + bo. (6) (7) (8) (9) A.3 HIERARCHICAL AFFINITY CLUSTERING (HAC) ALGORITHM Hierarchical Affinity Clustering (HAC) (Bateni et al., 2017) is powerful algorithm used to group data points based on their similarity or affinity, often represented by distance measure such as Euclidean distance or cosine similarity. HAC organizes data in hierarchical structure, either through an agglomerative (bottom-up) process, where each data point starts as its own cluster and the closest clusters are progressively merged, or divisive (top-down) process, which begins with all data points in single cluster that is repeatedly split. The result of the clustering process can be visualized using dendrogram, showcasing the nested relationships between clusters at different levels of similarity. Finding the affinity clustering of given graph is closely tied to the task of identifying its Minimum Spanning Tree (MST). In fact, the information encoded in the MST of is enough to determine its affinity clustering. Consequently, once the MST is computed, the affinity clustering or single linkage can be obtained in single step. (Bateni et al., 2017) Let = (V, E) denote an arbitrary graph, and let = THEOREM 10. (V, E) denote the minimum spanning tree of G. Running the affinity clustering algorithm on produces the same clustering of as running the algorithm on G. A.4 MIXTURE OF EXPERT In this paper, inspired by the idea of Mixture of Expert (MoE), we present Mixture of Tokenization (MoT). In Section 3.4 we show that there is not single type of tokenization that works best in all the cases. We further experimentally observe the same in Section 5.2. To this end, we suggest using Mixture of Tokenization (MoT) technique, where we allow each node to use tokenization that best describe its position based on the task. For example, one node might be better to be represented by itself (along with positional encoding) since its neighborhood is extremely noisy. At the same time, another node might be better to be represented by its neighbors as there is strong homophily in that area of the graph. Let be the list of different tokenizers, we use discrete router that choose top-2 tokenizations from for each node. We then concatenate the encodings of these tokenizers to obtain the final encoding for the global encoding step. That is, given and as the input, we use linear router with learnable weight Wr such that: = σ (XWr) , = Top-2 (cid:0)S(cid:1) , = one-hot (I) , 19 (10) (11) (12)"
        },
        {
            "title": "Preprint",
            "content": "where σ(.) is non-linearity, Top-2(.) returns the index of two rows with largest values, and onehot(.) returns the one-hot encoding of the indices. These weights are learned in an end-to-end manner along with the other parameters in the model."
        },
        {
            "title": "B RELATED WORK",
            "content": "Graph Neural Networks and Graph Transformers. utilize different approaches for processing graph data. GNNs typically employ message-passing mechanism that collects and synthesizes information from adjacent nodes into updated node representations (Kipf & Welling, 2016; Xu et al., 2019; Velickovic et al., 2017). Despite their utility, these models exhibit limitations in expressiveness, equivalent to that of the 1-WL test, traditional algorithm for testing graph isomorphism (Morris et al., 2019; Xu et al., 2019; Loukas, 2019). They also encounter challenges like over-smoothing and over-squashing, and struggle with capturing long-range dependencies (Alon & Yahav, 2021; Dwivedi et al., 2022b). In contrast, Graph Transformers make use of an attention mechanism (Dwivedi & Bresson, 2020; Kim et al., 2022; Kreuzer et al., 2021) that enables attention to all nodes within graph. Since utilizing full attention can obscure graph topology and render nodes non-distinguishable, numerous studies have concentrated on creating effective node encodings such as Laplacian positional encodings (Dwivedi et al., 2023; 2021; Maskey et al., 2022; Huang et al., 2023; Wang et al., 2022), shortest path distance/random walk distance (Ying et al., 2021; Li et al., 2020; Perozzi et al., 2014), among others. Additionally, some approaches merge Message Passing Neural Networks (MPNNs) with full attention capabilities (Rampaˇsek et al., 2022; Chen et al., 2022). However, this full attention model scales quadratically with the size of the graph. To mitigate this complexity, certain studies have applied general linear attention techniques to Graph Transformers (Choromanski et al., 2020a; Rampaˇsek et al., 2022), along with other specific strategies intended to optimize performance (Perozzi et al., 2024; Sanford et al., 2024b; Wu et al., 2022). Efficient Sequence Modeling. Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba (Gu & Dao, 2023), RWKV (Peng et al., 2023), and various gated linear RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of new type of foundation models. These sub-quadratic sequence models are defined in classical literature (Hyndman et al., 2008; Durbin & Koopman, 2001) as systems that model the dynamics of state variables over time through first-order differential or difference equations, offering cohesive structure for modeling time series. Addressing the computational limitations inherent in SSMs, set of innovations called Structural SSM (S4) (Gu et al., 2022; Gupta & Berant, 2022) has been developed. These utilize specialized configurations of parameter matrices, such as diagonal structures, to facilitate faster matrix operations. The evolution of SSMs has also inspired new hybrid neural architectures that blend elements of SSMs with other deep learning techniques (Behrouz et al., 2024b; Mehta et al., 2022a; Beck et al., 2024). comprehensive review of all these developments is available in (Wang et al., 2024b). Sequence Models for Graphs. Efforts to integrate State Space Models (SSMs) into graph processing have led to innovative approaches in graph Transformers, shifting from traditional attention mechanisms to SSM applications. Initially, these methods tokenize graphs, which then allows for the application of any SSM-inspired model to process the data. In one approach for tokenization (Wang et al., 2024a), the nodes are ordered into sequences according to their degrees, and Mamba (Gu & Dao, 2023)is then applied. Due to the common occurrence of nodes with identical degrees, it becomes necessary to randomly permute these sequences during training, which results in model that lacks permutation equivariance with respect to the reordering of node indices. Another variant (Behrouz & Hashemi, 2024), constructs sequences by extracting neighborhoods up to hops from root node, treating each hop as distinct token, and applying Mamba to model the root nodes representation. This method, however, is computationally intensive as it requires pre-processing each neighborhood token with Graph Neural Network (GNN) before applying Mamba. Additionally, the final layer of this model also applies Mamba to nodes arranged by their degree, preserving the issue of non-permutation equivariance. Graph State Space Convolution (GSSC) (Huang et al., 2024), leverage global permutation-equivariant set aggregation and factorizable graph kernels that rely on relative node distances as the convolution kernels. Recent advancements have been made in extending SSM-based models to accommodate temporal graphs, introducing two variants known as"
        },
        {
            "title": "Preprint",
            "content": "Table 5: How are different models special instances of GSM framework Method Tokenization Local Encoding Global Encoding DeepWalk (2014) Random Walk IDENTITY(.) SkipGram Node2Vec (2016) GraphTransformer (2020) GraphGPS (2022) NodeFormer (2022) Graph-ViT (2023) 2nd Order Random Walk Node Node Node IDENTITY(.) SkipGram IDENTITY(.) Transformer IDENTITY(.) Transformer GUMBEL-SOFTMAX(.) Transformer METIS Clustering (Patching) GCN(.) ViT Exphormer (2023) Node IDENTITY(.) Sparse Transformer CRaWl (2023) Random Walk 1D Convolutions MLP(.) NAGphormer (2023) k-hop neighborhoods GCN(.) Transformer SP-MPNNs (2022) k-hop neighborhoods IDENTITY(.) GRED (2023) k-hop neighborhood MLP(.) S4G (2024) k-hop neighborhood IDENTITY(.) GIN(.) RNN(.) S4(.) Graph Mamba (2024) Union of Random Walks (With varying length) GATED-GCN(.) Bi-Mamba(.) DyG-Mamba one (Ding et al., 2024; Li et al., 2024), each integrating the Mamba model with GNN encoders."
        },
        {
            "title": "C SPECIAL INSTANCES OF GSMS",
            "content": "Table 5 illustrates that several well-known methods for learning on graphs are special instances of the Graph Sequence Model (GSM) framework, highlighting its universality. GSM consists of three stages: (1) Tokenization, (2) Local Encoding, and (3) Global Encoding. In this section, we demonstrate how GSM can handle each of these models based on these three stages. We categorize the existing architectures into four general families: Traditional Methods, Graph Transformers, NonMPNN GNNs, and Recurrent-based Models. For each representative model within these families, we show how it can be formalized within the GSM pipeline. REMARK 1 & Leskovec, 2016) can be formulated as GSMs. (TRADITIONAL METHODS). DeepWalk (Perozzi et al., 2014) and Node2Vec (Grover (GRAPH TRANSFORMERS). Most popular GTs, including GraphGPS (Rampaˇsek REMARK 2 et al., 2022), Exphormer (Shirzad et al., 2023), GOAT (Kong et al., 2023), NAGphormer (Chen et al., 2023), SubGraphormer (Bar-Shalom et al., 2023), GPS++ (Masters et al., 2023), Nodeformer (Wu et al., 2022), TokenGT (Kim et al., 2022), Graphormer (Ying et al., 2021), , Coarformer (Kuang et al., 2021), and SAN (Kreuzer et al., 2021), can be formulated as GSMs. (NON-MPNN GNNS). Several popular non-MPNN methods for learning on graphs, REMARK 3 including CRaWl (Tonshoff et al., 2023), Graph-MLPMixer, and Graph-ViT (He et al., 2023) can be formulated as GSMs."
        },
        {
            "title": "Preprint",
            "content": "(RECURRENT-BASED MODELS). Recent graph learning methods based on modern REMARK 4 recurrent models, including Graph Mamba (Behrouz & Hashemi, 2024), GRED (Ding et al., 2023), and S4G (Song et al., 2024) can be formulated as GSMs."
        },
        {
            "title": "D PROOFS OF THEORETICAL RESULTS",
            "content": "D.1 COLOR COUNTING THEOREM 11. Let be the number of colors, and be the width of recurrent model, the recurrent model can count the number of nodes with each specific color iff C. Proof. We consider linear recurrent models (the same process can be done by any non-linear recurrent models): ht = Aht1 + Bxt yt = Cht. (13) (14) We let xt (input features) be the one-hot encoding of colors that can say what is the color of this input. Using = and = and h0 = 0, and if mC, then i-th channel in ht is responsible to 0 ... 1 ... 0 count i-th color. For input with color ci, its input feature is 1 and others are 0, and so we have: , where only the i-th channel is ht = Iht1 + 0 ... 1 ... 0 , (15) which means htj = ht1j for = and hti = ht1i + 1. This shows recurren models with can count. D.2 REPRESENTATIONAL COLLAPSE IN STATE SPACE MODELS THEOREM 12. For any > let A(k, i) = (1 1 and be the number of layers. For any < n, the gradient norm of the HiPPO operator for the output of layer at time + 1 (i.e., y(L) n+1) with respect to input at time (i.e., xi) satisfies: k1 ) . . . (1 1 )(1 1 ) 1 C(L) low (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) k1i (cid:88) . . . kLkL1 (n 1, kL) L1 (cid:89) ℓ=2 (cid:12) (cid:12) (cid:12) (kℓ 1, kℓ1) (k1 1, i) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) y(L) n+1 xi C(L) up (cid:19)L (cid:18) 1 Proof. We use the recurrent formulation of state space models: ht = Aht1 + Bxt yt = Cht. Based on this formulation, if we take the gradient y(1) n+1 xi we have: y(1) n+1 xi (cid:18) = (cid:19) (cid:18) n 1 (cid:19) (cid:18) 2 (cid:19) (cid:18) ... + (cid:19) . (16) (17) (18)"
        },
        {
            "title": "Preprint",
            "content": "Next, we need to see how using more layers affect this gradient. Let be the layer of interest, similar to the above, since the output of the (L 1)-th layer is the input of L-th layer, then we have: y(L) n+1 y(L1) (cid:18) = (cid:19) (cid:18) I"
        },
        {
            "title": "A\nn",
            "content": "A 1 (cid:19) (cid:18) 2 (cid:19) (cid:18) ... + 1 (cid:19) , and so usign chain rule, we have: y(L) n+1 xi (cid:88) = (cid:88) k1i kLkL1 y(L) n+1 y(L1) kL L1 (cid:89) ℓ=2 y(ℓ) kℓ y(ℓ1) kℓ1 y(1) k1 xi (19) (20) Now, since we are using HIPPO (Gu et al., 2020), we can see that all diagonizable and as discussed by Gu et al. (2020) we have: for = n, . . . , + 1 are (cid:18) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "A\nn",
            "content": "(cid:19) (cid:18) 1 (cid:19) (cid:18) . . . i + 1 (cid:19) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Θ 1 (1 (cid:124) ) . . . (1 (cid:123)(cid:122) A(k,i) 1 ) , 1 (cid:125) (21) which means there are Clow and Cup such that: (cid:12) (cid:12) (cid:12) (cid:12) Clow A(k, i) (cid:19) (cid:18) 1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:18) (cid:19) (cid:18) . . . i + 1 (cid:19) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Cup A(k, i). (22) Note that, it is simple to see: A(k, i) = (1 1 ) . . . (1 1 ) 1 1 . Using Equation 20 and the above bounds, we can conclude that: C(L) low (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) k1i (cid:88) . . . kLkL (n 1, kL) L1 (cid:89) ℓ=2 (cid:12) (cid:12) (cid:12) (kℓ 1, kℓ1) (k1 1, i) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) y(L) n+1 xi C(L) up (23) (cid:19)L , (cid:18) 1 (24) which completes the proof. Similar to Barbero et al. (2024), who provide this upper bound for Softmax attention, next, we derive the upper-bound for linear attentions: PROPOSITION 2. Given an input sequence x1, . . . , xn, let be the number of layers, y(L) i-th output in layer L, then the sensitivity of of any linear attention satisfies: be the yn xi C(L) (cid:88) (cid:88) k1i kLkL α(L1) n,kL L1 (cid:89) ℓ=2 α(ℓ1) kℓ,kℓ1 α(0) k1,i, (25) where αℓ i,j = (cid:16) (cid:16) σ (cid:16) q(ℓ) (cid:16) q(ℓ) ,k(ℓ) ,k(ℓ) ,pi,j ,pi,t (cid:80) σ (cid:17)(cid:17) (cid:17)(cid:17) are weights of the attention. This indicates that the discussions about representational collapse for full attention is also valid for linear transformers."
        },
        {
            "title": "E COMPARISONS BETWEEN TRANSFORMERS AND RECURRENT MODELS",
            "content": "The trade-offs in computational cost and model capability between standard transformers and alternative architectures have been well studied theoretically and empirically. For instance, the capabilities of state space and sub-quadratic models fall short of transformers in copying context (Jelassi et al., 2024), multistep reasoning (Sanford et al., 2024b), and nearest neighbor search (Alman &"
        },
        {
            "title": "Preprint",
            "content": "Song, 2023). Despite this, state space models learn certain tasks, such as the compositions of permutations, in more depth-efficient manner than transformers (Merrill et al., 2024). Because this paper designs graph sequence model architectures that employ state space models and other alternatives, it is useful to understand the trade-offs between these architectures and transformers at fundamental graph algorithmic tasks. In this section, we provide explicit the trade-offs between transformers and alternative modelsincluding state space models (e.g. Mamba, Gu & Dao, 2023), linear attention (e.g. PolySketchFormer, Kacham et al., 2024), and sparse attention (e.g. Longformer, Beltagy et al., 2020b). We discuss two particular particular architectural separations for graph connectivity tasks that illuminate broader trade-offs in architectural capability. 1. Section E.1 discusses the existence of more parameter-efficient transformers that solve graph connectivity than sub-quadratic architectures and state space models. 2. Section E.2 contrasts these results by showing that for certain category and presentation of graphs, recurrent models are more efficient in terms of both parameter count and computational time. 3. Section E.3 motivates hybrid models by suggesting instances of graph connectivity that are easily solved mixtures of RNN and transformer layers. Taken together, these sections show that there is no one sequential modeling architecture that is strictly better for all graph algorithmic problems (or even all connectivity instances). Rather, the properties of the sequential representation of the graph matter great deal to the comparative successes of neural architectures. If the graph structure is captured primarily by the ordering of nodes, then state space models are likely to more easily parse that structure than softmax attentions. In contrast, transformers may offer advantages for graph algorithms that benefit from parallel computation applied to inputs with complex structure. Hybrid models are best for inputs with both properties. Throughout this section, we frame graph connectivity as sequential modeling task with an edge tokenization. An undirected graph = (V, E) is provided as input := Pe1, . . . , PeE , and the target output is 1 if is connected and 0 if not. The theoretical results that follow are largely consequences of existing analyses about sequential reasoning tasks, such as k-hop induction heads (Sanford et al., 2024b) and the composition of permutations from the S5 group (Merrill et al., 2024). E.1 TRANSFORMERS ADMIT MORE EFFICIENT CONNECTIVITY SOLUTIONS The capabilities of standard softmax attention to efficiently compute graph connectivity for arbitrary graphs in edge tokenization were previously established. We provide these results as follows. (COROLLARY 3.3 OF SANFORD ET AL. (2024B)). For any and ϵ (0, 1), COROLLARY 4 there exists transformer with depth O(log ) and embedding dimension O(N ϵ) that determines whether any graph = (V, E) with , is connected. Transformers can thus solve graph connectivity with only O(N ϵ) parameters. Moreover, the depth of this construction is asymptotically optimal among small-width transformers; see Corollary 3.5 of the same paper for more details. On the other hand, alternative architectures cannot solve graph connectivity with such lowdimensional parameterization. COROLLARY 5. Neural architectures of the following topologies that solve graph connectivity on all graphs = (V, E) with , satisfies the following: 1. multi-layer recurrent neural networks (RNN)1 have either depth = Ω(N 1/8) or hidden state = Ω(N 1/4). 2. Transformers with kernel-based sub-quadratic attention have either depth = Ω(N 1/8) or mr = Ω(N 1/4) for embedding dimension and kernel dimension r. 1See Section 5 of Sanford et al. (2024b) for precise theoretical definitions of all models herein. We assume that all parameters and intermediate products use O(log )-bit precision numbers."
        },
        {
            "title": "Preprint",
            "content": "3. Transformers with locally masked attention units of radius and sparse long-range connections have either depth = Ω(N 1/8) or mr = Ω(N 1/4) for embedding dimension m. As result, these attempts to improve the quadratic computational bottleneck result in lack of parameter-efficient connectivity solutions. All RNNs, kernel-based transformers with kernel dimension = O(N 1/8), and all local transformers with window size = O(N 1/8) require at least Ω(N 1/8) parameters. In contrast, since ϵ can take any constant positive value, transformers can be much smaller in parameter count for large . Proof. The proof of Corollary 5 derives from Corollaries 5.2-5.4 of Sanford et al. (2024b) and rely on embedding well known communication taskthe pointer chasing problem of Nisan & Wigderson (1993)as graph connectivity instances. In brief, the input to pointer chasing task is (b, k)-layered graph = (V1 Vk+1, E1 Ek) with disjoint vertex layers V1, . . . , Vk+1 with Vj = and edge layers E1, . . . , Ek where Ej is perfect matching between Vj and Vj+1. Fix some Uk+1 Vk+1 and some v1 V1. The goal of the task is to determine whether the unique vertex vk+1 Vk+1 connected to v1 is in Uk+1. Let = O(N 1/8) and = O(N 7/8). Consider an embedding of the pointer-chasing task into any graph embedding of the form Pe1, . . . , PeE where ebj+1, . . . , eb(j+1) encode all edges in Ekj. By Proposition E.3 and Corollaries 5.2-5.4 of Sanford et al. (2024b), the pointer chasing task can only be solved on these embeddings by RNNs, kernel-based transformers, and locally masked transformers that satisfy the parameter scalings of Corollary 5. It remains to show that pointer chasing instances can be converted into connectivity instances = (V, E) with , = O(N ) using single round of computation without communication between inputs. We construct by adding O(b) edges between v1 and each vertex in Vk+1 Uk+1 and between adjacent pairs of vertices in Uk+1. The ensures the bound on and can be done by performing element-wise computation on blank input tokens, since we consider v1 and k+1 fixed. Note that is connected if and only satisfies vk+1 Uk+1. Hence, solutions to graph connectivity imploy solutions to pointer chasing. E.2 RNNS ADMIT MORE EFFICIENT CONNECTIVITY SOLUTIONS ON LOCALIZED GRAPHS In contrast, the benefits of RNNs and state space models are pronounced on graph connectivity instances presented as token sequences that embed graph structure carefully in their ordering. (In some cases, graphs of this form may be produced by the HAC tokenization method of Section 4.1.) We define notion of locality for an edge embedding and show that this induces easy embeddings for RNNs but not for transformers. DEFINITION 2. Let the node locality of an edge embedding Pe1 , . . . , PeE of graph = (V, E) denote the maximum window size needed to contain all edges that adjoin each node. That is, we say that has node locality if (cid:18) max vV arg max {ei : ei} arg min {ei : ei} k. (cid:19) We show that graphs with bounded node locality admit timeand parameter-efficient RNN solutions. THEOREM 13. There exists single-pass RNN with hidden state O(k) that determines whether edge embedding with node locality at most reflects connected graph. Proof. We first define the desired hidden state of the RNN, hi for any [E]. It will naturally follow that an RNN that simulates last-in first-out queue that stores edges can compute these hidden states with multi-layer perceptron with poly(k) parameters. For each [E], denote ei = {v1 edges eik, . . . , ei and vertices v1 ik, v2 e1, . . . , eik1. We let denote the subgraph of containing <i denote the subgraph with edges , v2 i1k+1, . . . , v1 }, and let Gk . Let Gk , hi = (Gk , ai, bi ik, . . . , bi i),"
        },
        {
            "title": "Preprint",
            "content": "where ai {0, 1} denotes whether all edges in Gk <i are connected to some edge in Gk ; and bi [k] denotes the index of the connected component that edge ei belongs to with respect to Gi <i Gi , then there exists path connecting ei and ei among the edges e1, . . . , ei. i; that is bi = bi We argue inductively that each hi1 can be constructed from hi. At initialization, we set a1 = 1 and b1 1 = 1. Gk can be trivially constructed from Gk i1 and ei by forgetting eik1. Let ai = 0 if and only if (1) ai1 = 0 or (2) bi1 <i1 is not connected to Gk edge in Gk Gk it cannot share an edge with ei, it is thus disconnected to Gk . . For (2), since eik1 is not connected to any of eik, . . . , ei1 via Gk ik1 is unique among bi1. For (1), if some i1, locality demands that it is also not connected to i1 and <i1 Gk If ei adjoins any of eik, . . . , ei1, we update the bi is to reflect the new clusters. By induction, we determine that hE can be constructed as desired. We conclude by noting that is connected if aE = 1 and b1 Ek = = b1 E. In the case when = O(1), there exists constant-size RNN that solves graph connectivity on such graph instances. In contrast, no constant-size transformer that solves the task exists. We prove this by reduction to the conditional hardness of solving NC1-complete problems with constant depth transformers (see e.g. Merrill & Sabharwal, 2023). THEOREM 14. Unless NC1 = TC0, any log-precision transformer that solves graph connectivity on edge embeddings for graphs = (V, E) with with node locality 12 requires either depth ω(1) or width ω(1). Proof. This proof is consequence of Corollary 1.1 of Merrill & Sabharwal (2023), which establishes that all log-precision constant-depth transformers can be simulated by circuits in TC0. Consider the task of composing permutations from the symmetric group of cardinality 5, S5. That is, given σ1, . . . , σn S5, compute σn σ1. This task is NC1-complete and is widely believed to not belong to TC0. If we show that this S5 composition task can be solved by evaluating the connectivity of O(1) graphs with node locality 12, then we can prove that graph connectivity on these instances is hard for constant-depth transformers. We first consider the subtask of determining whether (σn σ1)(s) = for some s, [5]. Given sequence of permutations σ1, . . . , σn and some s, t, we define graph = (V, E) with = [6n + 3] and edges e1, . . . , e6n+3 as follows: We establish path from node ι to node 6n + (σn σ1)(ι) for each ι [5]. For every [n] and ι [5], let e6(i1)+ι = {6(i 1) + ι, 6i + σi(ι)}. We create path from to t. Let e6 = {s, 6}, e6i = {6i, 6(i + 1)} for [n 1], and e6n = {6n, t}. Let e6n+1, e6n+2, e6n+3 connect the four nodes 6(i 1) + ι where ι = t. Thus, the graph is connected iff (σn σ1)(s) = t. Observe that each node appears exclusively in edges within window of size 12. Thus, this is an instance of graph connectivity with node locality 12."
        },
        {
            "title": "Preprint",
            "content": "Suppose there existed constant-depth transformer with polynomial width that solves connectivity with constant node locality. Then, we could solve the S5 composition task in constant depth by constructing graphs for all 25 (s, t) pairs and evaluating the connectivity of each. E.3 MOTIVATING HYBRID RNN-TRANSFORMER MODELS WITH CONNECTIVITY INSTANCES In the preceding sections, we demonstrated that different instances of the graph connectivity task highlight the parametric advantages of both softmax transformers and recurrent neural networks. Transformers perform best in worst-case instances, where their logarithmic-depth dependence is more favorable than the polynomial size lower bound for RNNs. In contrast, RNNs are superior for graphs whose input edge encodings reflect highly local structure. natural follow-up question asks whether there are any intermediate instances where the hybrid RNN-transformer models of Section 4.2 perform better than each component in isolation. In this section, we provide examples of those instances by considering hybridization of worst case graphs and graphs with node locality and show that those instances are best suited for hybrid models. We first introduce this family of graphs by construction. DEFINITION 3. For some n, k, and n, we define k-local (n, n)-factored graph as any graph = (V, E) with = n2 and an edge embedding Pe1 , . . . , PeE satisfying the following conditions. 1. There exists kernel graph = (V, E) with = and super-edge graphs Gv1,v2 = ({v1, v2} Vv1,v2 , Ev1,v2 ) for each super-node pair (v1, v2) 2 with Ev1,v2 = n. 2. Each super-edge graph Gv1,v2 has the property that (a) if (v1, v2) E, then Gv1,v2 is connected; and (b) if (v1, v2) E, then Gv1,v2 has two connected components, one containing v1 and one with v2. 3. Each super-edge graph Gv1,v2 has an n-token edge encoding PEv1,v2 locality k. that satisfies node 4. = (V, E) has nodes and edges satisfying = (cid:91) v1,v2V 2 Vv1,v2 and = (cid:91) Ev1,v2. v1,v2V 2 For any ordering (v1 is 1, v1 2), . . . , (vn2 1 , vn2 2 ) over super-node pairs 2 , the edge encoding of PEv1 1 ,v1 2 , . . . , PE . 1 ,vn2 vn2 Note that is connected if and only if is connected. However, the kernel graph is not immediately apparent from the input edge encoding, since identifying whether any (v1, v2) requires determining the connectivity of Gv1,v2. This property motivates two phase approach for hybrid architecture: An RNN determines the connectivity of each Gv1,v2 subgraph using the model of Theorem 13. transformer determines the connectivity of G. The capabilities of this approach is summaried by the following corollary of Theorem 13 and Corollary 4. COROLLARY 6. There exists hybrid RNN-transformer model that solves graph connectivity on klocal (n, n)-factored graphs that uses single RNN layer of hidden dimension O(k) and O(log(n)) transformer layers of embedding dimension O(nϵ)."
        },
        {
            "title": "Preprint",
            "content": "Let = n2n denote the total number of edges such graph. Critically, this has no dependence on the parameter n, excepting the fact that the model will require bit-precision Ω(log ). In the setting where is small (but still non-negligible), we can demonstrate substantial parameter count gap comparison to the best known constructions of both transformers and RNNs. For example, let = O(1) and = Θ(exp( log )). Because these diameter of the graph may be as large as O(n n) = O(N/ exp( log )), standard transformer is only known to solve the task using O(log ) layers and O(N ϵ) width. Even if an RNN can successfully determine in single pass, the task of determining log /8)) or whether whether is connected requires either depth Ω(n1/8) = Ω(exp( width Ω(n1/4) = Ω(exp( log /4)). In contrast, hybrid RNN-transformer model can solve the task with depth O(log n) = O( log ) and width O(exp(ϵ log )). While the definition of k-local (n, n)-factored graphs is somewhat contrived, they represent formalization of graphs whose edge embeddings are nearly local, but which require some analysis of global structure. Graphs with such properties are likely to be produced by clustering-based sequencing approaches, such as Hierarchical Affinity Clustering."
        },
        {
            "title": "F ADVANTAGES AND DISADVANTAGES OF LOCAL ENCODING",
            "content": "We discuss theoretical trade-offs of the k-hop local embedding introduced in Section 2.2. Concretely, we show that k-hop local embeddings offer simple solutions to subgraph counting problems that are more parameter-efficient than known transformer constructions. In contrast, these embeddings offer no asymptotic benefits on hard instances of graph connectivity. Like the preceding section, the results herein are largely applications of prior theoretical results on transformer capabilities and limitations. F.1 LOCAL ENCODINGS EFFICIENTLY COUNT SUBGRAPHS Computing the number of small subgraphsespecially triangles or other cliquesis well established graph algorithmic task. Triangle counting was included as fundamental graph reasoning problem in the GraphQA benchmark of Fatemi et al. (2023), and the ability to solve triangle counting with transformers with edge embeddings was investigated by Sanford et al. (2024a). While those results successfully converted existing parallel algorithms into transformer constructions, each construction had substantial polynomial dependency on the size of the input graph. In contrast, pairing local encodings with transformers enables easy counting of not only triangle counting but also any bounded-diameter subgraph counting task. THEOREM 15. For any fixed subgraph of diameter at most k, there exists k-hop local encoding ϕLocal and single-layer transformer of constant width such that ϕLocal counts the number of occurrences of in any input graph G. Proof. We set the local encoding such that ϕLocal(G)i includes normalized count the number of subgraphs in the k-hop subgraph including node i: sH = 1 ZH (cid:88) 1{subgraph of G[H (i) ] with vertices is isomorphic to H}, ={v1,...,vH}G[H (i) ] iV where is the number of nodes in and ZH is normalization term set to ensure doublecounting does not occur. (For example, if is the triangular graph, let ZH = 3 to reflect the fact that triangular subgraph {i1, i2, i3} will be counted thrice, in sH i1 It remains to provide transformer that computes (cid:80)V . This can be implemented by augmenting single-layer masked attention unit that solves counting to compute sums by including the sH terms in the value embeddings. (See e.g. the counting construction in Proposition 5.3 of Yehudai et al. (2024).) i=1 sH , sH i2 , sH i3 .)"
        },
        {
            "title": "Preprint",
            "content": "In contrast, all known transformer constructions without k-hop encodings of even triangle counting tasks have unfavorable width or depth scalings on the size of the graph. These constructions are generated by simulating algorithms in the Massively Parallel Computation (MPC) model of Karloff et al. (2010) with transformers via Theorem 8 of Sanford et al. (2024a). We provide the architectural scalings of the resulting transformers for several MPC algorithms below. In the following regimes, there exist transformer that solves triangle counting for constant ϵ > 0: Depth O(1) and embedding dimension O(E1/2+ϵ) in the edge encoding setting (Suri & Vassilvitskii, 2011); Depth O(V ) and embedding dimension O(V 1+ϵ) in the node encoding setting (Chu & Cheng, 2011). Depth O(log log E), embedding dimension O(Eϵ), and O(V E) extra blank tokens in the edge encoding setting (Biswas et al., 2022). All of these have much more dramatic model size scalings as function of and than the local encoding construction in Theorem 15. While it is unknown whether these represent the optimal solutions to subgraph counting with edge and node encodings, the fact that these are the stateof-the-art theoretical results indicates that local encoding substantially aids with tasks that involve aggregating local structural information. F.2 LOCAL ENCODINGS OFFER NO IMPROVEMENT FOR WORST-CASE CONNECTIVITY In contrast, the limitations of local encodings are apparent in the analysis of worst-case graph connectivity. We show that k-hop local encodings offer no asymptotic improvements in graph connectivity parameter complexity over the construction of Corollary 4. We generalize Corollary 3.5 of Sanford et al. (2024b)which establishes that sub-logarithmic-depth polynomial-width transformers cannot solve graph connectivity if the well-known one-cycle versus two-cycle conjecture (see, e.g., Ghaffari et al., 2019) holds. COROLLARY 7. Suppose any MPC algorithm with polynomial global memory and sub-linear local memory that distinguishes cycle graph of size from two cycle graphs of size 2 in the edge encoding uses Ω(log n) rounds of computation. Then, any transformer with k-hop local encoding (for = O(N 1ϵ) for some ϵ (0, 1)) that solves graph connectivity on all graphs of size , requires either depth depth = Ω(log ) or width = Ω( 1ϵ ). This implies that using O(kN ) input tokens to represent graph offers no representational benefits standard edge encoding, since the same logarithmic dependence persists. For large choices of k, the quadratic attention bottleneck causes the computational burden to scale with Θ(k2N 2 log ) rather than Θ(N 2 log ). Proof. The proof adapts the corresponding proof of Corollary 3.5 by Sanford et al. (2024b). For = , we let = (V, E) be some instance of the one-cycle vs two-cycle identification task. We assume for simplicity that this is the directed variant of the task, where the cycles are directed. That is, we represent its input as fixed ordering of edges e1, . . . , eE. Each vertex has degree exactly two. We embed in an instance of one-cycle vs two-cycle identification = (V , E) of size by adding phantom edges. We replace each vertex with linear subgraph of length containing vertices v1, . . . , vk and edges (vi, vi+1) E. If (u, v) E, then we add the edge (uk, v1) E. Thus, if has cycle of length if and only if has cycle of length . Because all edges of the form (vi, vi+1) exist for all instances G, we can create an edge encoding of from an edge encoding of using single layer of attention with blank tokens, positional encoding, and constant embedding dimension. Likewise, we can compute the k-hop local encoding of using an additional attention layer with with blank tokens. Since the existing hardness results for constructing transformers that solve the one-cycle versus twocycle problem of size pertain to all transformers of depth o(log n), width O(n1ϵ), and number of blank tokens poly(n), the corollary follows as written for the case when = O(N 1ϵ)."
        },
        {
            "title": "Preprint",
            "content": "G OVERVIEW OF GSM++ The overview of the GSM++ is illustrated in Figure 2."
        },
        {
            "title": "H EXPERIMENTAL SETUP",
            "content": "Benchmark Tasks. The nature of the task can be understood to involve assigning unique color to each class and subsequently counting the number of nodes within each class, effectively treating the count of nodes as the number of nodes with specific assigned color. For example, in cases where two distinct colors are present, one might determine that there are 2000 red nodes and 1000 blue nodes. The objective of the task then becomes to provide graph as input and generate an output in the form of vector, where each entry corresponds to the number of nodes belonging to particular class. This output vector enumerates the count of nodes for each class, reflecting the distribution of nodes across the different classes. We evaluate the empirical performance of our approach across diverse set of graph datasets, focusing on both graph-level and node-level prediction tasks. Specifically, we conduct experiments on image-based graph datasets, including PascalVOC-SP, which exemplifies long-range dependencies with moderate complexity (21 classes), and COCO-SP, which presents more challenging long-range dependencies with 81 classes. Additionally, we include synthetic SBM datasets (PATTERN) and heterophilic graph datasets (Roman-Empire, Minesweeper), which vary in difficulty, with 18 and 2 classes respectively. Color-connectivty task. Four Color-Connectivity datasets partitioned the nodes of graph into two groups: one half of the nodes was randomly assigned color, such as red, while the remaining nodes were assigned blue. In this setup, the red nodes either form single connected component or two disjoint components. The goal of the binary classification task is to distinguish between these two scenarios. The node colorings were produced by initiating two independent random walks, starting from two randomly selected nodes, to assign the red color. GSMs Variants. As the sequence encoder in the global encoding stage, we use: (1) xLSTM (Beck et al., 2024), (2) TTT (Sun et al., 2024), (3) Mamba (Gu & Dao, 2023), (4) Mamba2 (Dao & Gu, 2024), (5) PolySketchFormer (Kacham et al., 2024), (6) Transformers (Vaswani et al., 2017), (7) GLA (Yang et al., 2024), and (8) Sparse attention (Beltagy et al., 2020a). As the tokenization, we use: (1) Node (Rampaˇsek et al., 2022), (2) Edge + Node, (3) Edge, (4) k-hop Neighborhood (Chen et al., 2023), (5) Simple random walk (Kuang et al., 2021), (6) Multiple Random Walks (Behrouz & Hashemi, 2024), (7) HAC (this study), and (8) METIS (Karypis & Kumar, 1998) Since the focus of our study is mostly on global encoding and tokenization, we use the same local encoding (GatedGCN) for all the cases to ensure fair comparison. Baselines. We compare our GSM++ with (1) MPNNs, e.g., MPNN (Gilmer et al., 2017b), GCN (Kipf & Welling, 2016), GIN (Xu et al., 2019), GAT (Veliˇckovic et al., 2018), GraphSAGE (Hamilton et al., 2017), OrderedGNN (Song et al., 2023), tGNN (Hua et al., 2022), and Gated-GCN (Bresson & Laurent, 2017), (2) Random walk based method CRaWl (Tonshoff et al., 2023), (3) state-of-the-art GTs, e.g., SAN (Kreuzer et al., 2021), NAGphormer (Chen et al., 2023), Graph ViT (He et al., 2023), two variants of GPS (Rampaˇsek et al., 2022), GOAT (Kong et al., 2023), GRIT (Ma et al., 2023), and Exphormer (Shirzad et al., 2023), and (4) recurrent-based models: e.g., Graph Mamba (GMN) (Behrouz & Hashemi, 2024) and GRED (Ding et al., 2023). H.1 DETAILS OF DATASETS The statistics of all the datasets are in Table 6. For additional details about the datasets, we refer to the Long-range graph benchmark (Dwivedi et al., 2022a), GNN Benchmark (Dwivedi et al., 2023), Heterophilic Benchmark (Platonov et al., 2023), Open Graph Benchmark (Hu et al., 2020) and Color-connectivity task (Rampaˇsek & Wolf, 2021). When dealing with products-ogbn, we use local attentions instead of softmax attention (Deng et al., 2024) to enhance the scalability."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Dataset Statistics. Dataset #Graphs Average #Nodes Average #Edges #Class Setup Input Level Task Metric COCO-SP PascalVOC-SP Peptides-Func Peptides-Struct 123,286 11,355 15,535 15,535 Pattern MNIST CIFAR10 MalNet-Tiny 14,000 70,000 60,000 5, Roman-empire Amazon-ratings Minesweeper Tolokers arXiv-ogbn products-ogbn 1 1 1 1 1 1 476.9 479.4 150.9 150.9 118.9 70.6 117.6 1,410. 22,662 24,492 10,000 11,758 Long-range Graph Benchmark (Dwivedi et al., 2022a) 2693.7 2710.5 307.3 307.3 81 21 10 11 (regression) GNN Benchmark (Dwivedi et al., 2023) 3,039.3 564.5 941.1 2,859. 2 10 10 5 Node Node Graph Graph Node Graph Graph Graph Heterophilic Benchmark (Platonov et al., 2023) 32,927 93,050 39,402 519,000 18 5 2 Very Large Dataset (Hu et al., 2020) 169,343 2,449,029 1,166,243 61,859,140 40 47 Node Node Node Node Node Node Classification Classification Classification F1 score F1 score Average Precision Regression Mean Absolute Error Classification Classification Classification Classification Classification Classification Classification Classification Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy ROC AUC ROC AUC Classification Classification Accuracy Accuracy Color-connectivty task (Rampaˇsek & Wolf, 2021) C-C 16x16 grid C-C 32x32 grid C-C Euroroad C-C Minnesota 15,000 15,000 15,000 6, 256 1,024 1,174 2,642 480 1,984 1,417 3,304 2 2 2 2 Node Node Node Node Classification Classification Classification Classification Accuracy Accuracy Accuracy Accuracy Figure 3: Normalized score of different combination of tokenization and global encoder (sequence models). Even TTT + HAC is in Top-3 only in 3/7 datasets. ."
        },
        {
            "title": "Preprint",
            "content": "Table 7: Heterophilic datasets (Platonov et al., 2023). The first, second, and third results are highlighted. Table 8: Long-Range Datasets (Dwivedi et al., 2022a). The first, second, and third results are highlighted. Model GCN GraphSAGE GAT OrderedGNN tGNN Gated-GCN NAGphormer GPS Exphormer NodeFormer DIFFormer GOAT GMN Roman-empire Amazon-ratings Minesweeper ROC AUC Accuracy Accuracy 0.73690.0074 0.85740.0067 0.79730.0039 0.77680.0039 0.79950.0075 0.74460.0054 0.74340.0077 0.82000.0061 0.89030.0037 0.64490.0073 0.79100.0032 0.71590.0125 0.82190.0012 0.90030.0087 0.91240.0023 0.91770.0040 0.48700.0063 0.53630.0039 0.52700.0062 0.47290.0065 0.48210.0053 0.43000.0032 0.51260.0072 0.53100.0042 0.53510.0046 0.43860.0035 0.47840.0065 0.44610.0050 0.53270.0030 0.53810.0035 0.53610.0029 0.53900.0104 0.89750.0052 0.93510.0057 0.93910.0035 0.80580.0108 0.91930.0077 0.87540.0122 0.84190.0066 0.90630.0067 0.90740.0053 0.86710.0088 0.90890.0058 0.81090.0102 0.89920.0063 0.91090.0098 0.91450.0036 0.91490.0111 GSM++ (BFS) GSM++ (DFS) GSM++ (MoT) GSM++ (all variants) achieve the best three results among all graph sequence models. Model GCN GIN Gated-GCN GAT MixHop DIGL SPN SAN+LapPE NAGphormer Graph ViT GPS Exphormer NodeFormer DIFFormer GRIT GRED GMN GSM++ (BFS) GSM++ (DFS) GSM++ (MoT) COCO-SP F1 score PascalVOC-SP F1 score Peptides-Func AP 0.08410.0010 0.13390.0044 0.26410.0045 0.12960.0028 - - - 0.25920.0158 0.34580.0070 - 0.37740.0150 0.34300.0108 0.32750.0241 0.36200.0012 - - 0.36180.0053 0.37890.0160 0.37690.0027 0.38010.0122 0.12680.0060 0.12650.0076 0.28730.0219 0.17530.0329 0.25060.0133 0.29210.0038 0.20560.0338 0.32300.0039 0.40060.0061 - 0.36890.0131 0.39750.0037 0.40150.0082 0.39880.0045 - - 0.41690.0103 0.41280.0027 0.41740.0031 0.41930.0075 0.59300.0023 0.54980.0079 0.58640.0077 0.53080.0019 0.68430.0049 0.68300.0026 0.69260.0247 0.63840.0121 - 0.68550.0049 0.65750.0049 0.65270.0043 - - 0.69880.0082 0.70850.0027 0.68600.0012 0.69910.0008 0.70190.0084 0.70920."
        },
        {
            "title": "I ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "I.1 BENCHMARK DATASETS We report the results of GSM++ and baselines on herophilic and long-range benchmark datasets in Tables 7 and 8. GSM++ variants (i.e., tokenization with BFS, DFS, or MoT) achieve the best results among other graph sequence models, where GSM++(MoT) achieves the best results in 5/6 datasets. These results validate the effectiveness of the architecture design of our model. I.2 WHICH GSM IS MORE EFFECTIVE IN PRACTICE? To answer this question, we perform an extensive evaluation with all the combinations of 9 different sequence models and 6 types of tokenizers over 7 datasets of Citeseer, Cora, Computer, CIFAR10, Photo, PATTERN, and Peptides-Func from Dwivedi et al. (2022a; 2023); Chen et al. (2023). Due to the number of cases (9 6 = 54 models with 54 7 = 378 experiments), we visualize the rank of the model (higher is better), instead of reporting them in table. The normalized results are reported in Figure 3. These results indicate that there is no model that significantly outperforms others in most cases, validating our theoretical results that each of the sequence models as well as the types of tokenization has their own advantages and disadvantages. Accordingly, we need to understand the spacial traits of these models and use them properly based on the dataset and the task. Following our results, we conjecture that the no free lunch theorem applies for the Graph2Sequence problem. I.3 EFFICIENCY FOR LARGE DATASETS In this section, we compare the training time, memory usage, and performance of the variants of GSM++ with other efficient graph sequence models on large graphs. The results are reported in Table 9. With respect to scalability, all variants of GSM++ can scale to these large graphs. With respect to the performance, GSM++ variants achieve all first three places (except the second place on products-ogbn dataset), which shows the effectiveness of this architecture design. These results further shows the effectiveness and efficiency of MoT approach. Since this method uses router, it is more memory efficient than GSM++ with BFS traverse, and its training time is competitive with GSM++ with DFS traverse. Notably, these efficiency results are achieved byb GSM++ with MoT while it ouperforms all the baselines in both datasets."
        },
        {
            "title": "Preprint",
            "content": "Table 9: Efficiency evaluation on large graphs. The first, second, and third results for each metric are highlighted. OOM: Out of memory. Model GatedGCN NAGphormer GPS Exphormer GOAT GRIT GMN GSM++ BFS DFS MoT Performance Memory Usage (GB) Training Time/Epoch (s) Performance Memory Usage (GB) Training Time/Epoch (s) 0.7141 11.87 1. 0.0000 11.13 1.92 0.7013 6.81 5.96 0.0000 10.04 12.08 arXiv-ogbn OOM OOM OOM 0.7228 37.01 2. products-ogbn OOM OOM OOM OOM OOM OOM 0.7196 OOM 0.7248 OOM 5.63 13.12 OOM 1.78 8.69 0.7297 24.8 2.33 0.7261 4.7 1. 0.7301 14.9 4.16 0.8200 OOM OOM 0.8071 OOM OOM 38.14 12.06 6.97 OOM OOM 29.50 0.8080 9.15 12.19 0.8213 11.96 11.87 I.4 EFFICIENCY OF HAC TOKENIZATION In this section, we evaluate the efficiency of the HAC tokenization and compare its computing time with other commonly used positional encodings (PEs) in the literature (Rampaˇsek et al., 2022; Behrouz & Hashemi, 2024; Ma et al., 2023). Please note that the construction of positional encoding is one-time cost, which can be done as preprocessing before training, and so cannot significantly affect the total training time. We compare the computing time of HAC (Bateni et al., 2017) with random-walk-based PE (Behrouz & Hashemi, 2024), Laplacian-based PE (Rampaˇsek et al., 2022), and Relative Random Walk PE (Ma et al., 2023). The results are reported in Figure 4. HACs computing time is competitive with other PEss and scales more smoothly with the number of nodes. That is, the main efficiency gain of HAC is when we are dealing with large graphs. Furthermore, note that in practice, HAC is highly parallelizable and can scale to graphs with billions of nodes and trillion of edges in less than one hour (Bateni et al., 2017). Figure 4: The effect of number of nodes on the preprocessing time for the construction of positional encodings. TIME COMPLEXITY OF GSM++ ssm In this section, we analyze the time complexity of GSM++ and compare it with state-of-the-art efficient models. We let be the number of tokens, din be the dimension of the feature vectors (or PE), dssm be the first dimension of the SSM layers output (or the input dimension of the transformer layer), be the number of channels. and B, be the batch size. Given the fact that the input of the transformer block has the dimension of dssm, the complexity of the training for transformer block is (cid:1). The input of the SSM blocks has the dimension of the din and so given the fact that (cid:0)d2 Mamba has linear-time training (Gu & Dao, 2023), the training time for SSM layers is (cid:0)n d2 (cid:1). (cid:1), which is linear with respect to Therefore, the training time cost of GSM++ is (cid:0)d2 the graph size n. For the BFS traverse, GSM++ (BFS), we have = and so the time complexity (cid:1) . In the case of DFS traverse, is the number of tokens, which is at most is (cid:0)d2 ssmV (cid:1), log (V ). Therefore, for GSM++ (DFS) the time complexity is (cid:0)d2 which is sub-quadratic. In practice, log (V ) 11 and so one can argue GSM++ (DFS) also has linear time complexity. Finally, note that for the MoT, we concatenate the outputs of two different (cid:1) additional tokenization and so it requires projection from 2 din to din, which requires (cid:0)2d2 parameters. In practice, din 100 and so this results in about 10,000 additional parameters, which is negligible. in log (V ) + d2 in + in + d2 ssm ssm in in"
        }
    ],
    "affiliations": [
        "Google Research",
        "New Jersey Institute of Technology"
    ]
}