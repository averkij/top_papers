{
    "paper_title": "STIV: Scalable Text and Image Conditioned Video Generation",
    "authors": [
        "Zongyu Lin",
        "Wei Liu",
        "Chen Chen",
        "Jiasen Lu",
        "Wenze Hu",
        "Tsu-Jui Fu",
        "Jesse Allardice",
        "Zhengfeng Lai",
        "Liangchen Song",
        "Bowen Zhang",
        "Cha Chen",
        "Yiran Fei",
        "Yifan Jiang",
        "Lezhi Li",
        "Yizhou Sun",
        "Kai-Wei Chang",
        "Yinfei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The field of video generation has made remarkable advancements, yet there remains a pressing need for a clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present a comprehensive study that systematically explores the interplay of model architectures, training recipes, and data curation strategies, culminating in a simple and scalable text-image-conditioned video generation method, named STIV. Our framework integrates image condition into a Diffusion Transformer (DiT) through frame replacement, while incorporating text conditioning via a joint image-text conditional classifier-free guidance. This design enables STIV to perform both text-to-video (T2V) and text-image-to-video (TI2V) tasks simultaneously. Additionally, STIV can be easily extended to various applications, such as video prediction, frame interpolation, multi-view generation, and long video generation, etc. With comprehensive ablation studies on T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple design. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V, surpassing both leading open and closed-source models like CogVideoX-5B, Pika, Kling, and Gen-3. The same-sized model also achieves a state-of-the-art result of 90.1 on VBench I2V task at 512 resolution. By providing a transparent and extensible recipe for building cutting-edge video generation models, we aim to empower future research and accelerate progress toward more versatile and reliable video generation solutions."
        },
        {
            "title": "Start",
            "content": "STIV: Scalable Text and Image Conditioned Video Generation Zongyu Lin1* Tsu-Jui Fu2 Bowen Zhang2 Wei Liu1 Jesse Allardice2 Cha Chen2 Chen Chen2 Jiasen Lu2 Zhengfeng Lai2 Yiran Fei Yifan Jiang Wenze Hu2 Liangchen Song2 Lezhi Li 4 2 0 2 0 1 ] . [ 1 0 3 7 7 0 . 2 1 4 2 : r Yizhou Sun Kai-Wei Chang Yinfei Yang Apple University of California, Los Angeles"
        },
        {
            "title": "Abstract",
            "content": "The field of video generation has made remarkable advancements, yet there remains pressing need for clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present comprehensive study that systematically explores the interplay of model architectures, training recipes, and data curation strategies, culminating in simple and scalable text-image-conditioned video generation method, named STIV. Our framework integrates image condition into Diffusion Transformer (DiT) through frame replacement, while incorporating text conditioning via joint image-text conditional classifier-free guidance. This design enables STIV to perform both text-to-video (T2V) and text-image-to-video (TI2V) tasks simultaneously. Additionally, STIV can be easily extended to various applications, such as video prediction, frame interpolation, multi-view generation, and long video generation, etc. With comprehensive ablation studies on T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple design. An 8.7B model with 5122 resolution achieves 83.1 on VBench T2V, surpassing both leading open and closed-source models like CogVideoX-5B, Pika, Kling, and Gen-3. The same-sized model also achieves state-of-the-art result of 90.1 on VBench I2V task at 5122 resolution. By providing transparent and extensible recipe for building cutting-edge video generation models, we aim to empower future research and accelerate progress toward more versatile and reliable video generation solutions. 1. Introduction The field of video generation has witnessed significant progress with the introduction of Sora [42], video generation model based on Diffusion Transformer (DiT) [43] architecture. Researchers have been actively exploring optimal methods to incorporate text and other conditions into the DiT architecture. For example, PixArt-α [8] leverages cross attention, while SD3 [19] concatenates text with the noised patches and applies self-attention using the MMDiT block. Several video generation models [21, 46, 65] adopt similar approaches and have made substantial progress in the text-to-video (T2V) task. Pure T2V approaches often struggle with producing coherent and realistic videos, as their outputs are not grounded in external references or contextual constraints [13]. To address this limitation, text-image-to-video (TI2V) introduce an initial image frame along with the textual prompt, providing more concrete grounding for the generated video. *This work was done during an internship at Apple. 1First authors 2Core authors Senior authors 1 Despite substantial progress in video generation, achieving Sora-level performance for T2V and TI2V remains challenging. central challenge is how to seamlessly integrate image-based conditions into the DiT architecture, calling for innovative techniques blend visual inputs smoothly with textual cues. Meanwhile, there is pressing need for stable, efficient large-scale training strategies, as well as improving the overall quality of training datasets. To address these issues, comprehensive, step-by-step recipe would greatly assist in developing unified models that handle both T2V and TI2V task under one framework. Overcoming these challenges is essential for advancing the field and fully realizing the potential of video generation models. Figure 1. Performance comparison of our Text-to-Video model against both open-source and closed-source state-ofthe-art models on VBench [31]. Although various studies [2, 6, 11, 14, 49, 62, 70] have examined methods of integrating image conditions into the U-Net architectures, how to effectively incorporate such conditions into the DiT architecture remains unsolved. Moreover, existing studies in video generation often focuses on individual aspects independently, overlooking the how their collective impact on overall performance. For instance, while stability tricks like QK-norm [19, 28] have been introduced, they prove insufficient as models scale to larger sizes [57], and no existing approach has successfully unified T2V and TI2V capabilities within single model. This lack of systematic, holistic research limits progress toward more efficient and versatile video generation solutions. In this work, we first present comprehensive study of model architectures and training strategies to establish robust foundation for T2V. Our analysis reveals three key insights: (1) stability techniques such as QK-norm and sandwich-norm [17, 25] are critical for effectively scaling larger video generation models; (2) employing factorized spatial-temporal attention [1], MaskDiT [73], and switching to AdaFactor [54] significantly improve training efficiency and reduce memory usage with minimal impact on performance loss; (3) progressive training, where spatial and temporal layers are initialized from separate models, outperforms using single model under the same compute constraints. Starting from PixArt-α baseline architecture, we address scaling challenges with these stability and efficiency measures, and further enhance performance with Flow Matching [41], RoPE [56], and micro conditions [45]. As result, our largest T2V model (8.7B parameters) achieves state-of-the-art semantic alignment and VBench score of 83.1. We then identify the optimal model architecture and hyperparameters established in the T2V setting and apply them to the TI2V task. Our results show that simply replacing the first noised latent frame with the un-noised image condition latent yields strong performance. Although ConsistI2V [49] introduced similar idea in U-Net setting, it required spatial self-attention for each frame and window-based temporal self-attention to match our quality. In contrast, the DiT architecture natively propagates the image-conditioned first frame through stacked spatial-temporal attention layers, eliminating the need for these additional operations. However, as we scale up spatial resolution, we observe the model producing slow or nearly static motion. To solve this, we introduce random dropout of the image condition during training and apply joint image-text conditional classifier-free guidance (JIT-CFG) for both text and image conditions during inference. This strategy resolves the motion issue and also enables single model to excel at both T2V and TI2V tasks. With all these changes, we finalize our model and scale it up from 600M to 8.7B parameters. Our best STIV model achieves state-of-the-art result of 90.1 in the VBench I2V task at 5122 resolution. Beyond enhancing video generation quality, we demonstrate the potential of extending our framework to various downstream applications, including video prediction, frame interpolation, multi-view generation and long video generation. These results 2 Prompt: An adorable kangaroo wearing blue jeans and white shirt taking pleasant stroll in Johannesburg South Africa during beautiful sunset. Text-to-Video Prompt: swan with wings tipped in gold gliding across misty lake, leaving trail of soft, shimmering light that fades as the sun rises. Prompt: The video presents sequence of frames that depict space scene with large, green and yellow planet at the center, surrounded by smaller celestial bodies. The background is deep blue, speckled with stars. Text-Image-to-Video Prompt: Robots move efficiently through futuristic laboratory, adjusting holographic displays and conducting experiments, while scientists observe and interact with the high-tech equipment. Figure 2. Text-to-Video and Text-Image-to-Video generation samples by T2V and STIV models. The text prompts and first frame image conditions are borrowed from Soras demos [42] and MovieGenBench [46]. 3 Figure 3. We replace the first frame of the noised video latents with the ground truth latent and randomly drop out the image condition. We use cross attention to incorporate the text embedding, and use QK-norm in multi-head attention, the sandwich-norm in both attention and feedforward, and stateless layernorm after singleton conditions to stabilize the training. validate the scalability and versatility of our approach, showcasing its ability to address diverse video generation challenges. We summarize our contributions as follows: We present STIV, single model capable of performing both T2V and TI2V tasks. At its core, we replace the noised latent with the un-noised image condition latent and introduce joint image-text conditioned CFG. We conduct systematic study for T2I, T2V and TI2V, covering model architectures, efficient and stable training techniques, and progressive training recipes to scale up the model size, spatial resolution, and duration. These design features make it easy to train and adaptable to various tasks, including video prediction, frame interpolation, and long video generation. Our experiments include detailed ablation studies on different design choices and hyperparameters, evaluated on VBench, VBench-I2V and MSRVTT. These studies demonstrate the effectiveness of the proposed model compared with range of recent state-of-the-art open-source and closed-source video generation models. Some of the generated videos are shown in Fig. 2. More examples can be found in the Sec. in the Appendix. 2. Basics for STIV This section describes our key components of our proposed STIV method for text-image-to-video (TI2V) generation, which is illustrated in Fig. 3. Afterward, Sec. 3 and 4 presents detailed experimental results. 2.1. Base Model Architecture The STIV model is based on PixArt-α [8], which converts the input frames into spatial and temporal latent embeddings using frozen Variational Autoencoder (VAE). These embeddings are then processed by stack of learnable DiT-like blocks. We employ the T5 [48] tokenizer and an internally trained CLIP [47] text encoder to process text prompts. The overall framework is illustrated in Fig. 3. For more details, please refer to the appendix. The other significant architectural changes are outlined below. Spatial-Temporal Attention We employ factorized spatial and temporal attention [1] to handle video frames. We first fold the temporal dimension into the batch dimension and perform spatial self-attention on spatial tokens. Then, we permute the outputs and fold the spatial dimension into the batch dimension to perform temporal 4 self-attention on temporal tokens. By using factorized spatial and temporal attention, we can easily preload weights from text-to-image (T2I) model, as images are special case of videos with only one temporal token and only need spatial attention. Singleton Condition We use the original image resolution, crop coordinates, sampling stride, and number of frames as micro conditions to encode the meta information of the training data. We first use sinusoidal embedding layer to encode these properties, followed by an MLP to project them into d-dimensional embedding space. These micro condition embeddings, along with the diffusion timestep embedding and the last text token embedding from the last layer of the CLIP model, are added to form singleton condition. We also apply stateless layer normalization to each singleton embedding and then add them together. This singleton condition is used to produce shared scale-shift-gate parameters that are utilized in the spatial attention and feed-forward layers of each Transformer layer. Rotary Positional Embedding Rotary Positional Embeddings (RoPE) [56] are used so that the model has strong inductive bias for processing relative temporal and spatial relationships. Additionally, RoPE can be made compatible with the masking methods used in high compute applications and are highly adaptable to variations in resolution [76]. We apply 2D RoPE [39] for the spatial attention and 1D RoPE for the temporal attention inside the factorized Spatial-Temporal attention. Flow Matching Instead of employing the conventional diffusion loss, we opt for Flow Matching training objective. This objective defines conditional optimal transport between two examples drawn from source and target distribution. In our case, we assume the source distribution to be Gaussian and utilize linear interpolates [41] to achieve this. The training objective is then formulated as xt = x1 + (1 t) ϵ. Ex,ϵN (0,I),c,t min θ (cid:104) Fθ(xt, c, t) vt2 2 (cid:105) (1) (2) where the velocity vector field vt = x1 ϵ. In inference time, we solve the corresponding reverse-time SDE, from timestep 0 to 1, to generate images from randomly sampled Gaussian noise ϵ. 2.2. Model Scaling As we scale up the model, we encounter training instability and infrastructure challenges in fitting larger models into memory. In this section, we outline the methods to stabilize the training and enhance training efficiency. Stable Training Recipes We discovered that QK-Norm applying RMSNorm [68] to the query and key vectors prior to computing attention logits significantly stabilizes training. This finding aligns with the results reported in SD3 [19]. We also change from pre-norm to sandwich-norm [17] for both MHA and FFN, which involves adding pre-norm and post-norm with stateless layer normalization [37] to both the layers within the STIV block. MHA(x) = + gate norm (Attn (scale norm(x) + shift)) FFN(x) = + gate norm (MLP (scale norm(x) + shift)) Efficient DiT Training We follow MaskDiT [73] by randomly masking 50% of spatial tokens before passing them into the major DiT blocks. After unmasking, we add two additional DiT blocks. We also switch from AdamW to AdaFactor optimizer and employ gradient checkpointing to only store the self-attention outputs. These modifications significantly enhance efficiency and reduce memory consumption, enabling the training of larger models at higher resolution and longer duration. 5 2.3. Image Conditioning 2.3.1 Frame Replacement During training, we replace the noised first frame latent with the un-noised latent of the image condition before passing the latents into the STIV blocks, and masking out the loss of the replaced frame. During inference, we use the un-noised latent of the original image condition for the first frame at each TI2V diffusion step. The frame replacement strategy offers flexibility in extending STIV to various applications. For instance, if cI = , it defaults to text-to-video (T2V) generation. Conversely, if cI is the initial frame, it becomes the typical text-image-to-video (TI2V) generation. Moreover, if multiple frames as cI are provided, they can be used for video prediction even without cT . Additionally, supplying the first and last frames as cI enables the model to learn frame interpolation, generating frames between them. Furthermore, combining T2V and frame interpolation allows for the generation of long-duration videos: T2V generates keyframes, and frame interpolation frames then fills in frames between each pair of consecutive keyframes. Ultimately, single model can be trained to perform all tasks by randomly selecting the appropriate conditioning strategy. 2.3.2 Image Condition Dropout As discussed previously, the frame replacement strategy offers substantial flexibility for training various types of models. Here, we demonstrate specific application in which we train model to perform both T2V and TI2V tasks. In this case, we randomly drop out cI and cT during training, similar to how T2V models employ random dropout to text condition alone. Classifier-free guidance (CFG), commonly used in text-to-image generation, has proven to be highly beneficial in enhancing the quality of generated images by directing the probability mass toward the high-likelihood regions given the condition. Building on this concept, we introduce Joint Image-Text Classifier-Free Guidance (JIT-CFG) approach, which leverages both text and image conditions. It modifies the velocity estimates as ˆFθ(xt, cT ,cI , t) = Fθ(xt, , , t) + (Fθ(xt, cT , cI , t) Fθ(xt, , , t)) where is the guidance scale. When cI = , it reduces to standard CFG for T2V generation. Although it is possible to introduce two separate guidance scales, as done in [4], to balance the strength of the image and text conditions, we found that our two-pass approach yields strong results. Additionally, using two scales would require three forward passes, increasing the inference cost. Empirical observations 3.4.2 suggest that applying image condition dropout with JIT-CFG effectively not only achieves multi-task training in natural way, but also resolves the staleness issue for 5122 STIV model. We hypothesize that image condition dropout prevents the model from passively overfitting to the image condition, allowing it to more effectively capture the motion information from the underlying video training data. 2.4. Progressive Training Recipe We employ progressive training recipe as illustrated in Figure 4. The process begins by training text-to-image (T2I) model, which serves to initialize text-to-video (T2V) model. Next, the T2V model serves as the starting point for initializing the STIV model. To facilitate rapid adaptation to higher resolutions and longer durations training, we incorporate interpolated RoPE embeddings in both the spatial and temporal dimensions, while initializing the model weights using those from the lower-resolution, shorter-duration models. Figure 4. Progressive training pipeline of the STIV model. The T2I model is first trained to initialize the T2V model, which then initializes the STIV model at both low and high resolutions. Notably, the high-res T2V model is initialized using both the high-res T2I model and the low-res T2V model. Figure 5. Ablation study of the STIV model, from the base T2I model to the temporally-aware T2V model, and finally to the image-conditioned TI2V model. 3. Recipe Study for STIV 3.1. Basic Setup Before we dive into the studies of architecture and data for video generation models, we first introduce the training, data and evaluation setup before introducing our model and studies as follows: Training Unless otherwise specified, we use the AdaFactor optimizer (β1 = 0.9, β2 = 0.999) [54] without any weight decay. We also clip the gradient norm if the gradient norm exceeds 1.0. We use constant learning rate schedule with 1k step linear warmup with maximum learning rate of 2 104. For T2I models, we train each model for 400k steps with batch size of 4,096. This is approximately 1.4 epochs on our internal T2I datasets. For T2V and TI2V models, we train each model for 400k steps with batch size of 1,024. This is roughly 5.5 epochs on our internal video datasets. For all models, exponential moving average weights are gathered by decay rate of 0.9999 and are then used for evaluation. When MaskDiT is used, we train with 50% spatial random masking during the initial 400k steps. Subsequently, we perform unmasked fine-tuning using all tokens. We use 50k steps of unmasked fine-tuning for T2I models and 100k steps for T2V and TI2V models. Data We build video data engine pipeline that includes video pre-processing, captioning, and filtering to accelerate the models development when handling large-scale videos is required. Specifically, we apply PySceneDetect 1 to analyze video frames, detect and segment scenes based on abrupt transitions and gradual fades. This segmentation is followed by the feature extractions for filtering, including motion score, aesthetic score, text area, frame 1https://github.com/Breakthrough/PySceneDetect 7 dimensions, clarity score, temporal consistency, and video orientation, among others. For each video segment, we perform dense captioning and categorization to gain comprehensive understanding of video distribution. To further enhance caption quality, we adapt DSG [12] and propose DSG-Video, metric designed to assess hallucination rates and overall caption quality. This data engine is integral in filtering videos and preparing tailored datasets for different training stages: our data sources include Panda-70M [10] and an internally curated high-quality dataset of 42M videos. Using our data engine, we curate over 90M high-quality video-caption pairs 2. Next we are going to dive into more fine-grained modulation studies. As illustrated in figure 5, We follow the principle of studying from base T2I model to the temporally-aware T2V model, and finally to the TI2V model by adding image conditioning. Evaluation We mainly use VBench [31], VBench-I2V and MSRVTT [63] to evaluate T2V and TI2V models. For VBench, we mainly report Quality (temporal quality and frame-wise quality), Semantic (semantic alignment with different perspectives of the input text prompt) and Total score (weighted average of Quality and Semantic), and they can be actually decomposed into 16 dimensions in total. VBench-I2V builds upon the VBench with three new Video-Image Alignment metrics: Subject Consistency, Background Consistency, and Camera Motion Control. These additional metrics provide more comprehensive evaluation by focusing on how well the generated video aligns with the input image and specified prompt instructions. More details about the detailed dimensions are presented in Section F.1. We present three model scales: XL, XXL, and with their configuration detailed in Tab. 1. In the following section, we use the notation X-S-T to represent an X-size model with an S2 resolution and frames. If unspecified, the default configuration is 2562 resolution with 20 frames. More detailed model and training configurations are provided in the Appendix. Model Size XL (600M) XXL (1.5B) (8.7B) # of STIV Blocks Hidden Dim # of Attn Heads 1,152 1,536 3,072 28 38 46 18 24 48 Table 1. Model Configurations 3.2. Ablation Studies for Key Changes on T2I We conduct comprehensive ablation study to understand the impact of various model architecture designs and training strategies mentioned in Sec. 2 on the text-to-image generation task. To evaluate generation quality, we use suite of popular automated metrics, including FID score [29], Pick Score [33], CLIP Score, GenEval [23], and DSGEval [12], Human Preference Score (HPSv2) [61], Image Reward [64]. We began with base T2I-XL model, DiT [43] model augmented with cross-attention layers to integrate with text embeddings. Initially, we applied series of stabilization techniques, including QK-norm, sandwich-norm and singleton condition norm, which yielded comparable results to the baseline. Notably, these techniques enabled us to train models stably even with learning rate increased from 1e-4 to 2e-4. We demonstrated that incorporating Flow Matching during training and employing CFG-Renormalization3 during inference improved all the metrics substantially. Subsequently, we explored techniques to reduce training memory, such as AdaFactor Optimizer, MaskDiT, and Shared AdaLN, which maintained similar performance. Utilizing micro conditions and RoPE further reduced the FID score and improved DSGEval and Image Reward. Finally, incorporating an internally trained bigG CLIP model improved on all metrics even more. Notably, combining synthetic recaption with original caption following [35] achieved the best results in almost all metrics. For more details, refer to the Appendix D. We use the optimal model architecture and training hyperparameters based on the T2I ablation study as our starting point for the remaining T2V and TI2V experiments. 2The details and effectiveness of our data engine are studied in the Appendix. 3Detailed description in Appendix."
        },
        {
            "title": "Model",
            "content": "COCO COCO COCO PICK CLIP FID 32.03 20.91 26.17 Baseline 32.08 20.92 + QK norm 25.60 32.13 20.97 + Sandwich norm 25.76 32.27 21.05 25.58 + Cond. norm 32.28 21.03 26.35 + LR to 2E-4 32.90 21.45 24.96 + Flow 32.93 21.46 21.16 + Renorm 32.97 21.47 20.26 + AdaFactor 33.07 21.51 23.85 + MaskDiT 33.12 21.44 22.83 + Shared AdaLN 33.09 21.50 20.02 + Micro cond. 33.11 21.46 18.40 + RoPE 33.26 21.79 19.57 + Internal VAE 17.97 33.62 21.89 + Internal CLIP 33.65 22.10 18.04 + Synth. captions Gen Eval Eval 0.571 0.358 0.574 0.372 0.577 0.366 0.583 0.393 0.586 0.379 0.639 0.457 0.668 0.471 0.661 0.474 0.663 0.499 0.658 0.496 0.673 0.498 0.680 0.502 0.668 0.492 0.717 0.607 0.751 0.685 DSG HPSv2 Eval 26.33 26.32 26.32 26.43 26.40 26.95 27.27 27.26 27.28 27.27 27.27 27.26 27.26 27.40 27.65 Image Reward -0.25 -0.22 -0.23 -0.22 -0.12 0.15 0.32 0.32 0.30 0.24 0.41 0.48 0.52 0.65 0.81 Table 2. Text-to-image model ablation studies. 3.3. Ablation Studies on Key Designs for T2V Key Modulation We make some design choices in our model based on the evaluations on VBench, as shown in Fig. 6a. The base model uses temporal path size of 2, non-causal temporal attention, and spatial masking ratio of 0.5. As expected, the model with temporal patch=1 performs the best, but it is only slightly better with 2x compute. However, the model with temporal patch=4 leads to noticeable performance drop. Using causal temporal attention also results in significant drop in both quality and total scores. Adding scale-shift-gate to the temporal attention layer 4 is slightly worse than the baseline, despite having more parameters. Furthermore, removing the spatial masking results in slight decrease in the Semantic score and an improvement in the Quality and Total scores. However, this comes at the cost of requiring more compute as the length of tokens are doubled. On the other hand, using temporal masking significantly degrades model performance, with large drops observed in the VBench quality and final scores. Model Initialization We investigate how initialization impacts the performance of T2V-XL models. We train 5122 T2V models by four different paths under controlled total FLOPs setting: from scratch, initializing from lower resolution T2V-256 model, initializing from T2I-512 model, and loading both the temporal and spatial weights from T2V-256 and T2I-512 models respectively (Fig. 6b). We find that jointly initializing from both low resolution T2V model and high resolution T2I model can achieve better VBench metrics. This joint initialization method yields slightly improved FVD values compared to training from scratch and offers benefits in terms of efficient experimentation and cost when low resolution models are already present. Under similar methodology we additionally explore the effects of training T2V models with higher numbers of frames (40 frames) by initializing from shorter T2V models (20 frames). Fig. 6c shows that when training models with higher number of frames initializing from low frame count model achieves improved metrics over initializing directly from T2I model. Using interpolation of the RoPE embeddings yields improved VBench scores compared to extrapolation. Additionally we find that initializing the high frame count training from T2V model trained with proportionally lower frame rate (higher frame sub-sampling stride) can improve the VBench metrics, particularly the motion smoothness and dynamic degree. 4See Fig. 3 for the diagram of the model. 9 Module Base model w/ temp. patch=1 w/ temp. patch=4 w/ causal temp._atten + temp. scale_shift_gate + temp. mask - spatial mask Quality 80.19 80.92 79.72 74.59 80.32 77.58 80.57 VBench Semantic 70.51 71.69 69.15 73.13 68.94 65.95 70.31 Total 78.25 79.07 77.61 74.30 78.04 75.25 78.52 (a) Ablation Study of T2V model design using T2V-XL. The base model uses temporal path size 2, non-causal temporal attention, spatial masking ratio 0.5, and no temporal masking. Init. Scratch T2V-256 T2I-512 Both MSRVTT FVD 417.98 415.63 401.83 405.14 Quality 80.27 80.28 79.77 80.45 VBench Semantic 67.84 71.29 71.58 72.37 Total 77.78 78.49 78.13 78.83 (b) Different model initialization for T2V-XL-512. Init. T2I T2V (inter.) T2V (extra.) T2V 2x (inter.) MSRVTT FVD 549.13 407.86 397.90 401.94 Quality 78.71 79.56 79.18 79.59 VBench Semantic 65.69 65.42 64.63 66.24 Total 76.10 76.73 76.27 76. (c) Different initialization for T2V-XL 40 frames. Figure 6. Ablation studies of key designs for T2V. 3.4. Ablation Studies on Key Designs for TI2V To integrate the image condition with the text condition, we reformulate the model as Fθ(xt, cT , cI , t), where cT and cI are the text and image conditions. Then, we studied each design component in TI2V framework and tackled multi-task learning and staleness issue encountered when training high resolution TI2V models. 3.4.1 The Effectiveness of Frame Replacement We ablate several model variants for TI2V on STIV-XL model, by combining the following key components: Frame Replacement (FR), Cross Attention (CA), Large Projection (LP), and First Frame Loss (FFL) 5. As shown in Tab. 3, notably, adding large projection layer enhances the information passed by the cross-attention, resulting in improvements in both subject and background consistency. However, this approach may overly constrain the model, as evidenced by reduction in the dynamic degree score (22.36 for FR + CA + LP compared to 35.4 for FR + CA), indicating that the model might exert excessive control over the generated output. Additionally, adding first-frame loss, though seemingly beneficial, has shown to reduce overall scores, particularly in aspects of motion quality, suggesting that this loss might inadvertently constrain the models temporal dynamics. In contrast, frame replacement alone has proven to be robust and effective approach, yielding consistent improvements without negatively impacting other dimensions of video quality. The frame replacement (FR) model achieves high scores in I2V average scores (the average of I2V Subject, I2V Background and Camera Motion) and total average scores. These results underline the advantage of frame replacement as foundational component, providing stable backbone for maintaining quality across diverse dimensions. 3.4.2 The Effectiveness of Image Condition Dropout Our experiments show that image condition dropout with JIT-CFG not only supports multi-task training but also resolves staleness in 5122 STIV model. Multi-task training By using image-conditioning dropout during STIV training, we effectively enable both T2V and TI2V capability. As shown in Tab. 4, models trained exclusively on T2V or TI2V task alone cannot perform the other task, while STIV with image condition dropout can easily handles both two task well, achieving performance comparable to the best single-task models. 5(1) FR uses frame replacement alone for strong image-video alignment. (2) CA uses cross attention alone to align features between the input image and generated video. (3) FFL removes the first frame loss mask introduced in Section 3.4 to constrain the initial frame of the video. (4) LP employs more powerful ResNet 2D encoder as the projection layer here. 10 Mot Dyn Aesth Models CA CA + FFL CA + LP FR FR + CA FR + CA + LP FR + CA + LP + FFL Bg Subj Cons Cons 92.8 82.2 95.6 84.5 98.7 95.2 98.3 94.5 98.6 95.1 95.3 98.5 98.7 95.2 Temp Flick 95.7 96.1 97.4 96.6 97.0 97.3 97. Smooth Deg 42.4 29.7 22.2 36.6 35.4 22.4 22.2 96.3 96.7 98.1 97.8 98.1 98.2 98.1 Img Qual Qual 65.5 48.8 64.7 48.7 66.8 57.3 58.0 66.1 58.0 66.2 66.3 57.3 66.8 57.3 I2V Subj 88.9 91.5 96.9 96.8 96.9 97.0 96.9 I2V Bg Mot 26.9 90.9 17.6 94.7 22.7 97.3 31.5 97.1 28.8 97.3 97.4 25.8 22.7 97.3 Cam I2V Avg Scores 68.2 67.2 72.3 75.8 74.4 73.4 72. Avg Scores 73.0 72.0 75.3 77.3 77.1 75.6 75.3 Table 3. Ablation Study Results for Different Model Components for Text-Image-To-Video (TI2V) task on VBench-I2V. Model T2V-M-512 STIV-M-512 STIV-M-512-JIT STIV-M-512-JIT-TUP VBench-T2V 77.0 31.9 74.1 73.1 81.2 66.1 80.7 81. 82.2 74.6 82.3 83.0 VBench-I2V / 82.1 81.9 82.3 / 90.1 89.8 89.7 / 98.0 97.6 97.2 Table 4. Comparison of T2V, STIV and STIV with JIT-CFG on VBench and VBench-I2V I2V Score, Quality, Total scores. Greater motion In practice, we have observed that while STIV-M-512 performs well on VBench-I2V, it sometimes generates static frames. The VBench-I2V metric tends to favor videos with less motion, prioritizing smoothness and consistency. As shown in Tab. 5, STIV with JIT-CFG achieves higher dynamic degrees at the cost of slight reduction in consistency and smoothness scores. We also show visual comparisons from Fig. 14 to Fig. 16 in the Appendix. Model STIV-M-512 STIV-M-512-JIT Dynamic Degree 10.2 24.0 Motion Temporal Smoothness Consistency 99.6 99. 99.3 98.6 Background Flickering 99.1 98.6 Table 5. Effect of JIT-CFG on motion-related scores. JIT-CFG and the Variant It is natural to think about extending the traditional classifier-free guidance (CFG) to three-copy weighted approach, where three conditioning modes are considered: (1) Null condition: Both the image (cI ) and text (cT ) conditions are null (). (2) Image-only condition: The image condition is the source image, while the text condition is null. (3) Joint condition: Both the image and text conditions are provided. The velocity estimates are combined as: ˆFθ(xt, cT , cI , t) =Fθ(xt, , , t) + s1 (Fθ(xt, cI , , t) Fθ(xt, , , t)) + s2 (Fθ(xt, cI , cT , t) Fθ(xt, cI , , t)) . Here, s1 and s2 are guidance scales for the image-only (CFG-I) and joint conditions (CFG-T), respectively. We dub it separate image and text classifier-free guidance (SIT-CFG). We achieved = 94.1 on the MSRVTT test set using STIV-M-512-JIT by setting = 7.5 in JIT-CFG. Meanwhile, we conducted experiments on our STIV-M-512-JIT, performing grid search on s1 and s2 by taking the Cartesian product (1.1, 1.5, 4.5, 7.5, 10.5) (1.1, 1.5, 4.5, 7.5, 10.5). 11 As shown in the Fig. 7, we observed: (1) Fixing CFG-T, as CFG-I increases, FVD first decreases and then increases; (2) Fixing CFG-I, as CFG-T increases, FVD continuously decreases, except when CFG-I is very small (1.1, 1.5), where it first decreases and then increases; (3) The best configuration occurs at CF = 7.5 and CF = 1.5, yielding = 95.2. However, overall, SIT-CFG does not show significant advantages compared to JIT-CFG, and using two copies for inference is significantly less efficient. Note that this search was optimized for MSRVTT, and for other prompts requiring stronger dependence on the first-frame subject, larger CFG-I might be needed. Figure 7. Grid search the optimal FVD for CFG-I s1 and CFG-T s2 on MSRVTT [63] test set. 3.4.3 Model Initialization We also studied how the performance of TI2V models is affected by the initialization methods, including starting from T2I or T2V. We run the same total number of steps to check the final performance on VBench-I2V. From Tab. 6, we observe that initializing from T2V model can achieve better camera motion score and slightly better dynamic degree, while comparable to initializing from T2I on all other dimensions. Initialization T2V T2I Bg Subj Cons Cons 98.2 94.1 98.7 94.5 Temp Flick 96.5 96.9 Smooth Deg 37.1 36. 97.7 97.9 Img Qual Qual 57.9 65.5 66.1 57.4 I2V Subj 96.6 96.6 I2V Cam Bg Mot 38.0 96.9 97.3 29.8 Avg Scores 77.9 77.2 Mot Dyn Aesth Table 6. Results for different model initialization on VBench-I2V. 3.5. Video Data Engine Data quality is pivotal for video generation models. However, curating large-scale, high-quality datasets remains challenging due to issues like noisy captions, hallucinations, and limited diversity in video content and duration. To address these concerns, we propose Video Data Engine  (Fig. 8)  comprehensive pipeline that improves dataset quality and reduces hallucinations, ultimately enhancing model performance. More details can be found in Sec. in the appendix. Our approach focuses on three key questions: (1) How to preprocess raw videos for better consistency? (2) What is the effect of data filtering on model performance? (3) How can advanced video captioning reduce hallucinations and improve outcomes? We use Panda-70M [10] as working example and produce curated subset, Panda-30M, via our pipeline. Video Pre-processing and Feature Extraction. We employ PySceneDetect6 to remove abrupt transitions and inconsistent segments, yielding more coherent clips. We then extract key features (e.g., motion and aesthetic scores) to guide subsequent filtering. 6https://github.com/Breakthrough/PySceneDetect 12 Figure 8. An overview of our video data engine, including video pre-processing, filtering, and video captioning. Figure 9. An overview of DSG-Videos approach to detecting object hallucinations in captions: we use an LLM to generate questions and another MLLM to validate the presence of the object across frames. If the MLLM fails to detect the object in all frames, the object is classified as hallucination. Data Engine for Filtering Effective data filtering is crucial for improving dataset quality and reducing hallucinations. We develop an automated filtering infrastructure that supports efficient data selection, quality control, and continuous improvement throughout the models development lifecycle. For instance, we can sample high-quality videos with predefined resolutions / motion scores for the fine-tuning stage. This filtering system allows us to systematically remove low-quality videos and focus on data that enhances model performance. From Panda-30M, we further apply filtering based on motion score and aesthetic score to obtain Panda-10M, named as high-quality version of Panda-30M. The results are summarized in Tab. 7: instead of pursuing data volume, higher-quality videos have the potential to achieve more promising results. Data Panda-30M Panda-10M MSRVTT FVD 770.9 759.2 Quality 80.4 80.8 VBench Semantic 73.6 73.4 Total 65.6 66.2 Table 7. Compare Panda-30M and Panda-10M (high-quality) using XL T2V model. Video Captioning Model High-quality video-text pairs are essential for training text-to-video models. Existing datasets often suffer from noisy or irrelevant captions, limited in describing temporal dynamics. We initially attempted frame-based captioning approach followed by LLM summarization [2], but found that single-frame captions fail to represent motion, and LLM summarization can induce hallucinations. To improve caption quality while balancing cost, we employ LLaVA-Hound-7B [69]a video LLM capable of producing more coherent and motion-aware descriptions. Caption Evaluation and Ablations To objectively assess caption accuracy, we introduce DSG-Video  (Fig. 9)  , module inspired by DSG [12], that detects hallucinated objects by probing captions with LLM-generated questions and verifying object presence in sampled video frames using multimodal LLM. This yields two 7, reflecting hallucination at the object and sentence levels, respectively. metrics, DSG-Videoi and DSG-Videos We compare two captioning strategiesframe-based plus LLM summarization (FCapLLM) and direct video 7More details are illustrated in Appendix I. 13 captioning (VCap)on the Panda-30M dataset. As shown in Tab. 8, VCap reduces hallucinations and increases the diversity of described objects, leading to improved T2V model performance. These results demonstrate that richer, more accurate video descriptions can significantly enhance downstream generation quality. Caption FCapLLM VCap Total Object DSG-Videoi() DSG-Videos() MSRVTT FVD () VBench () 24.0 15. 808.1 770.9 1249 1911 64.2 65.6 6.4 5.3 Table 8. Compare different captions using XL T2V model. DSG-Video metrics are calculated from 100 random captions. 4. Results Based on all of these studies, we scale our T2V and STIV model from 600M to 8.7B. We show the main results in Tab. 9 and Tab. 10, comparing our models with state-of-the-art open sourced and close sourced models, which demonstrates the effectiveness of our recipes. Specifically, we do finetuning on top of the pretrained video generation models (SFT), based on the 20,000 videos filtered from Panda-70M [10] using the method mentioned in Section 3.5. Since we adopt MaskDiT technique in our pretraining stage, we try finetuning our model in an unmask manner (UnmaskSFT). We also finetuned our STIV model to become temporal upsampler to interpolate the videos generated by our main T2V and STIV models to boost the motion smoothness (+ TUP). T2V Performance We first showcase the effectiveness of our T2V model as the foundation for STIV. Tab. 9 presents comparison of different T2V model variants on VBench, including the VBench-Quality, VBenchSemantic, and VBench-Total scores. Our analysis reveals that scaling up model parameters in our T2V model improves semantic following capability. Specifically, as model size increase from XL to XXL and M, VBenchSemantic scores rise from 72.5 to 72.7 and then to 74.8. This explicit emergence (from XL, XXL to M), suggesting larger models are better at capturing semantic information. However, the impact on video quality, measured by VBench-Quality, remains modest, with only slight increase from 80.7 to 81.2 and then to 82.1. This finding suggests that scaling has greater effect on the models semantic capabilities than on video quality. Furthermore, increasing the spatial resolution from 256 to 512 significantly boosts the VBench-Semantic score from 74.8 to 77.0. Detailed results can be found in Tab. 11. The Influence of SFT Additionally, fine-tuning the model with high-quality SFT data markedly enhances the VBench-Quality score from 82.2 to 83.9. Finetuning our model without any masked tokens slightly increases the performance of model on the semantic score. Our best model achieves VBench-Semantic score of 79.5, outperforming prominent closed source models such as KLING, PIKA, and Gen-3. With the temporal upsampler, our model can achieve the state-of-the-art quality score compared with all other models. TI2V Performance As shown in Tab. 10, our model delivers competitive performance compared to state-of-the-art approaches. It also reveals that while scaling up improves the I2V score, it has minimal impact on quality. In contrast, increasing the resolution leads to noticeable improvements in both quality and I2V scores. We provide complete results for the decomposed dimensions in Tab. 12. 5. Flexible Applications Here, we demonstrate how to extend our STIV to various applications, such as video prediction, frame interpolation, multi-view generation, and long video generation. Video Prediction We initialize from STIV-XXL model to train text-video-to-video model conditioned on the first four frames. As shown in Fig. 10a, the video-to-video model (STIV-V2V) shows significantly lower FVD scores compared to the text-to-video model (T2V) on MSRVTT [63] test set and MovieGen Bench [46]. This 14 Model Quality Semantic Total Open Sourced Models 81.4 82.9 82.2 82.2 82.2 83.1 82.8 83.2 Close Sourced Models OpenSora V1.2 [74] AnimateDiff-V2 [26] VideoCrafter-2.0 [7] T2V-Turbo [38] CogVideoX-2B [65] Allegro [75] CogVideoX-5B [65] LaVie-2 [60] Gen-2 [51] PIKA [44] EMU3 [24] KLING [34] Gen-3 [52] XL XXL M-512 M-512 SFT M-512 SFT + TUP M-512 UnmaskSFT M-512 UnmaskSFT + TUP 82.5 82.9 84.1 83.4 84.1 Ours 80.7 81.2 82.1 82.2 83.9 84.2 83.7 84.4 73.4 69.8 73.4 74.5 75.8 73.0 77.0 75.7 73.0 71.8 68.4 75.7 75.2 72.5 72.7 74.8 77.0 78.3 78.5 79.5 77. 79.8 80.3 80.4 80.6 80.9 81.1 81.6 81.8 80.6 80.7 81.0 81.9 82.3 79.1 79.5 80.6 81.2 82.8 83.1 82.9 83.0 Table 9. Performance comparison of T2V variants with open-sourced and close-sourced models on VBench. Model VideoCrafter-I2V [6] Consistent-I2V [49] DynamicCrafter-256 [62] SEINE-512 [11] I2VGen-XL [70] DynamicCrafter-512 [62] Animate-Anything [14] SVD [2] STIV-XL STIV-M STIV-M-512 STIV-M-512-JIT Quality 81.3 78.9 80.2 80.6 81.2 81.6 81.2 82.8 79.1 78.8 82.1 81. I2V Total 89.0 94.8 96.6 96.3 95.8 96.6 98.3 96.9 95.7 96.3 98.0 97.6 85.1 86.8 88.4 88.4 88.5 89.1 89.8 89.9 87.4 87.6 90.1 89.8 Table 10. Performance comparison of STIV-TI2V variants with open-sourced and close-sourced models on VBench-I2V. result indicates that video-to-video models can achieve superior performance, which is promising for applications in autonomous driving and embodied AI where high fidelity and consistency in generated video frames are crucial. Frame Interpolation We propose STIV-TUP, temporal upsampler initialized from an STIV-XL model, and continue train conditioned on consecutive frames sampled by stride of 2 with the text conditioning. Fig. 10b shows that our STIV can also be used to do decent frame interpolation conditioned on both text and image. We Model T2V STIV-V2V MSRVTT MovieGen FVD 536.2 183.7 FVD 347.2 186.3 Model use text STIV-TUP STIV-TUP No Yes MSRVTT FID 2.2 2.0 FVD 6.3 5. Model Zero123++ [55] STIV-TI2V-XL PSNR 21.200 21.643 GSO [18] SSIM LPIPS 0.723 0.724 0.143 0.156 (a) Comparison of T2V and V2V models. (b) Performance of STIV-TUP. (c) Multiview generation comparison. Figure 10. Demonstration of flexible applications of STIV framework. observe that using text conditions is slightly better in FID and FVD on the MSRVTT test set. We also cascade the temporal upsampler with our main model to explore whether it can boost the main performance. As shown in Tab. 9 and Tab. 4, using temporal upsampler on the top of the main models can improve the quality performance while maintaining other scores. Multi-View Generation Multi-view generation is specialized task focused on creating novel views from given input image. This task places demands on view consistency and can greatly benefit from well-pretrained video generation model. By adapting video generation model for multi-view generation, we can evaluate whether the pretraining has effectively captured underlying 3D information, which would enhance multi-view generation. Here, we adopt the novel view camera definitions outlined in Zero123++ [55], which specifies six novel view cameras for each input image. The initial frame in our TI2V model is set as the given image, and the next six frames, representing novel views, are predicted as future frames within TI2V. For training, we began with our TI2V-XL checkpoint trained with 256 resolution, fine-tuning it for 110k steps on Objaverse [16]. For fair comparison, we increased the image resolution to 320 during finetuning, aligning with the settings used in Zero123++. Our evaluation used objects from the Google Scanned Objects dataset [18], where we compared the output multi-view images against ground-truth renderings. As shown in Fig. 10c, despite only using temporal attention for cross-view consistency, our approach achieves comparable performance to Zero123++ which uses full attention to all the views. This outcome validates the effectiveness of our spatiotemporal attention in maintaining 3D consistency. visual comparison between our approach and Zero123++ is shown in Figure 11. Long Video Generation We develop an effective and efficient framework to generate long videos. Specifically, we propose hierarchical framework, including training our STIV on two different modes: (1) key frame prediction by learning uniformly sampled video frames with stride of 20 with image conditioning dropout and (2) interpolated frame generation by learning consecutive video frames with first and last frame as image conditions. During sampling stage, we change the image and micro conditions, and first use the first mode to generate key frames and then generate the interpolated frames use the second mode, leading to long video. It is natural to reuse the STIV model to autoregressively generate the videos conditioned on previous generated one. However, in practice, we found this rollout approach can be hurt by error propagation in the previous video, and lacks some global consistency between frames. Therefore, we propose simple yet effective baseline, purely based on our STIV framework. As mentioned in the main text, we design hierarchical framework, train our STIV on two different modes: (1) key frame prediction by learning uniformly sampled video frames with stride of 20 with image conditioning dropout, and (2) interpolated frame generation by learning consecutive video frames with the first and last frame as image conditions. During the sampling stage, we change the image and micro conditions, and first use the first mode to generate key frames and then generate the interpolated frames using the second mode, leading to long video. We showcase one long T2V and one TI2V example in Figure 12; we achieve (20 1) 20 = 380 frames in total. We uniformly sampled 8 frames out of the 380 frames. Noted that this is only an early exploration of long video generation, and we do not have many long enough videos in our training distribution, so we leave it as future work to further explore the architecture to boost long video synthesis. 16 Multi-View Generation Figure 11. The visual comparison between our STIV-XL with Zero123++ [55] on GSO [18]."
        },
        {
            "title": "Long Video Generation",
            "content": "Prompt: drone camera circles around beautiful historic church built on rocky outcropping along the Amalfi Coast, the view showcases historic and magnificent architectural details and tiered pathways and patios... Reference Image Prompt: slow cinematic push in on an ostrich standing in 1980s kitchen. Figure 12. Visualizations of long video generation framework. 18 6. Related Work Text-To-Video Generation In recent years, diffusion-based methods have emerged as the dominant approach in text-to-video generation, both for close-source models [42, 44, 46] and open-source models [66, 74]. [6, 7, 27] leverages latent diffusion models (LDMs) [50] to enhance training efficiency. VideoLDM [3] integrates temporal convolution and attention mechanisms into the LDM U-Net for video generation. Recently, there has been shift from UNet to diffusion transformer-based architectures [21, 46, 66, 75]. CogVideoX [65] adopts the framework from SD3 [19] to incorporate self-attention on the entire 3D video sequence with text conditions. Lumina-T2X [39] employs zero-init attention to transform noise into different modalities. In contrast to previous models, our focus is to scale our diffusion transformer-based video generation model with spatial, temporal, and cross attention to over 8B parameters using various techniques. This model achieves good performance on VBench and serves as strong baseline for the development of our text-image-to-video model: STIV. Text-Image-To-Video Generation Controlling video content solely through text poses significant challenges in achieving satisfactory alignment between the video and the input text, as well as fine-grained control over the video generation process. To address this issue, recent approaches have integrated both the first frame and text to enhance control over video generation [6, 24, 49, 62, 70], mostly based on U-Net architecture. I2VGen-XL [70] builds upon the SDXL and employs cascading technique to generate high-resolution video. DynamiCrafter [62] and VideoCrafter [6] use cross-attention to incorporate image condition. ConsistentI2V [49] employs similar frame replacement strategy, but it also requires spatial temporal attention over the initial frame and special noise initialization to enhance consistency. Animate Anything [15] also employs the frame replacement technique, but it requires the use of motion strength loss to enhance the motion. However, their Dynamic Degree on VBench-I2V is relatively low, at 2.7%. We apply frame replacement on the DiT architecture, along with our proposed image condition dropout method, and JIT-CFG can generative high quality I2V videos while effectively addresses the motion staleness issue. 7. Conclusion In conclusion, we conduct comprehensive study on how to build good video generation model, and present scalable and flexible approach for integrating text and image conditioning within unified video generation framework. Our model not only demonstrates good performance on public benchmarks, but also shows versatility in downstream applications, supporting controllable video generation, video prediction, frame interpolation, long video generation, and multi-view generation, which collectively highlight its potential as foundation for the broad research community. 8. Acknowledgement We thank Alex Schwing, Monica Zuendorf, Saeed Khorram, Pengsheng Guo and Zhe Gan for their regular discussions on model design and training recipes. We also acknowledge the indirect influence of these discussions on our approach. Furthermore, we thank Meng Cao for data and Ruoming Pang for training infrastructure support. We also acknowledge the leadership guidance from Yang Zhao throughout the project. Finally, we thank the Axlearn team 8 at Apple for providing training infrastructure, which greatly facilitated our experiments. 8https://github.com/apple/axlearn"
        },
        {
            "title": "References",
            "content": "[1] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, page 4, 2021. 2, 4 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 13, 15, 9, 11 [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 19 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. [5] Joao Carreira and Andrew Zisserman. Quo Vadis, Action Recognition? New Model and the Kinetics Dataset. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 10 [6] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. VideoCrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 2, 15, 19, 8 [7] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. VideoCrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73107320, 2024. 15, 19, 8, 9 [8] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 1, 4, 2 [9] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. 11 [10] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70M: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 8, 12, [11] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. arXiv preprint arXiv:2310.20700, 2023. 2, 15, 9 [12] Jaemin Cho, Yushi Hu, Jason Baldridge, Roopal Garg, Peter Anderson, Ranjay Krishna, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-to-image generation. In ICLR, 2024. 8, 13, 12 [13] Zhixuan Chu, Lei Zhang, Yichen Sun, Siqiao Xue, Zhibo Wang, Zhan Qin, and Kui Ren. Sora detector: unified hallucination detection for large text-to-video models. arXiv preprint arXiv:2405.04180, 2024. 1 [14] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Animateanything: Fine-grained open domain image animation with motion guidance. arXiv e-prints, pages arXiv2311, 2023. 2, 15, 9 [15] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Fine-grained open domain image animation with motion guidance. arXiv preprint arXiv:2311.12886, 2023. 19 [16] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In 20 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 16 [17] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. 2, 5 [18] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: high-quality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 25532560. IEEE, 2022. 16, [19] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1, 2, 5, 19 [20] Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu, Jong-Chyi Su, William Yang Wang, and Sean Bell. Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 11 [21] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. 1, 19 [22] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic VQGAN and time-sensitive transformer. In European Conference on Computer Vision, pages 102118, 2022. 10, 11 [23] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 8 [24] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. In European Conference on Computer Vision, 2023. 15, 19, 8 [25] Xinyu Gong, Wuyang Chen, Tianlong Chen, and Zhangyang Wang. Sandwich Batch Normalization: Drop-In Replacement for Feature Distribution Heterogeneity. In Winter Conference on Applications of Computer Vision (WACV), 2022. [26] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 15, 8 [27] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221, 2022. 19 [28] Alex Henry, Prudhvi Raj Dachapally, Shubham, and Yuxuan Chen. Query-Key Normalization for Transformers. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. 2 [29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium, 2018. 8 [30] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 8, 11 [31] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 2, 8, 7 [32] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The Kinetics Human Action Video Dataset. In arXiv:1412.0767, 2014. 10 21 [33] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:3665236663, 2023. 8 [34] Kuaishou. Kling. https://kling.kuaishou.com, 2024. 15, 8 [35] Zhengfeng Lai, Vasileios Saveris, Chen Chen, Hong-You Chen, Haotian Zhang, Bowen Zhang, Juan Lao Tebar, Wenze Hu, Zhe Gan, Peter Grasch, et al. Revisit large-scale image-caption data in pre-training multimodal foundation models. arXiv preprint arXiv:2410.02740, 2024. 8, [36] Zhengfeng Lai, Haotian Zhang, Bowen Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, et al. Veclip: Improving clip training via visual-enriched captions. In European Conference on Computer Vision, pages 111127. Springer, 2025. 3 [37] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. ArXiv e-prints, pages arXiv1607, 2016. 5 [38] Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback. In Conference on Neural Information Processing Systems (NeurIPS), 2024. 15, 8 [39] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2643926455, 2024. 5, 19 [40] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [41] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 2, 5, 1 [42] OpenAI. Sora. https://openai.com/index/sora, 2024. 1, 3, 19, 13 [43] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 8, 2 [44] Pika. Pika 1.0. https://pika.art, 2023. 15, 19, 8 [45] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 2, 3 [46] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 1, 3, 14, 19, 13 [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 4 [48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [49] Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, and Wenhu Chen. Consisti2v: Enhancing visual consistency for image-to-video generation. arXiv preprint arXiv:2402.04324, 2024. 2, 15, 19, 9 [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 19 22 [51] Runway. Gen-2. https://research.runwayml.com/gen2, 2023. 15, 8 [52] Runway. Gen-3 alpha. https://runwayml.com/research/introducing-gen-3-alpha, 2024. 15, 8 [53] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved Techniques for Training GANs. In Conference on Neural Information Processing Systems (NeurIPS), 2016. [54] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 45964604. PMLR, 2018. 2, 7 [55] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 16, 17 [56] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 2, 5 Chameleon: Mixed-modal early-fusion foundation models. [57] Chameleon Team. arXiv preprint arXiv:2405.09818, 2024. 2 [58] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning Spatiotemporal Features with 3D Convolutional Networks. In International Conference on Computer Vision (ICCV), 2015. 10 [59] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 8 [60] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. 15, [61] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 8 [62] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. 2, 15, 19, 9 [63] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: large video description dataset for bridging video and language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52885296, 2016. 8, 12, 14 [64] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023. 8 [65] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. In arXiv:2408.06072, 2024. 1, 15, 19, [66] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, JiaZheng Xu, Yuanming Yang, Xiaohan Zhang, Xiaotao Gu, Guanyu Feng, et al. CogVideoX: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 19 [67] Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, Ming-Hsuan Yang, Yuan Hao, and Lu Jiang Irfan Essa. MAGVIT: Masked Generative Video Transformer. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 11 [68] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. 5 23 [69] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024. 13, [70] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. 2, 15, 19, 9 [71] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, 2024. 11 [72] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 11 [73] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. 2, 5 [74] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-Sora: Democratizing Efficient Video Production for All, 2024. 15, 19, 8 [75] Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commercial-level video generation model. arXiv preprint arXiv:2410.15458, 2024. 15, 19, 8 [76] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024. 24 STIV: Scalable Text and Image Conditioned Video Generation"
        },
        {
            "title": "Appendix",
            "content": "A. Joint Image-Text Classifier-free Guidance We introduce novel framework, Joint Image-Text Classifier-Free Guidance (JIT-CFG), in Section 3.4, which facilitates the seamless integration of text and image conditions to enhance the modeling performance. This is accomplished through modified velocity estimate, expressed as: ˆFθ(xt, cT ,cI , t) = Fθ(xt, , , t) + ω (Fθ(xt, cT , cI , t) Fθ(xt, , , t)) The approach employs text and image condition dropout, which is also critical for unifying T2V and TI2V tasks. Probability mass shift Our model learns (xcT , cI ), the probability distribution of generating video given the text prompt cT and image condition cI . Here, we demonstrate how JIT-CFG shifts the probability mass toward regions of higher likelihood, conditioned on cT and cI . First, consider score-matching model with JIT-CFG ˆsθ(xt, cT ,cI , t) = sθ(xt, t) + ω (sθ(xt, cT , cI , t) sθ(xt, t)) Using the definition of score and Bayes Rule, we derive ˆsθ(xt, cT , cI , t) =logPt(xt) + ω (logPt(xt, cT , cI ) logPt(xt)) =logPt(xt) + ω logP (cT , cI xt) =log (Pt(xt)P ω =log (cid:0)P 1ω (cT , cI xt)) (xtcT , cI )(cid:1) , (xt)P ω where ω determines the influence of the text and image conditions during sampling from the tempered distribution. For flow-matching model employing linear interpolants, the velocity and score are related as [41]: ˆsθ(xt, cT , cI , t) = 1 ˆFθ(xt, cT ,cI , t) 1 1 xt, = 1t > 0, meaning that the JIT-CFG-guided velocity ˆF shifts the probability mass in alignment It implies ˆs ˆF with the modified score ˆs by adjusting the tempered distribution. CFG-Renormalization Empirically, we observed that the magnitude of the modified velocity, ˆFθ(xt, cT , cI , t) tends to be very large during the early stages of integration in inference (i.e. when is small). This behavior sometimes leads to overshooting beyond the learned latent distribution, resulting in artifacts in the generated output. We identified this issue as primarily due to the significant difference between the conditional velocity, Fθ(xt, cT , cI , t), and the unconditional velocity, Fθ(xt, , , t) when is small. To mitigate this, we propose simple yet effective renormalization method that re-scales the magnitude of the modified velocity to Fθ(xt, cT , cI , t) while preserving its direction. Formally, this is defined as: 1 Fθ(xt, cT , cI , t) = Fθ(xt, cT , cI , t) ˆFθ(xt, cT , cI , t) ˆFθ(xt, cT , cI , t) As shown in Table 2, this technique significantly improves performance across various T2I evaluation benchmarks. B. Implementation Details for T2V and STIV Given that we use spatial-temporal attention, we first pretrain the T2I model using only an image dataset. Subsequently, we load the EMA weights from the T2I model, excluding the temporal attention. In our work, we use the per-frame VAE, which is the same one used in the T2I model. On top of that, we use temporal patch of size 2 in the DiT part for video models. We modify the T2I cubify weights by inflating the 3D convolution weight in the temporal dimension. For video training data, we select one frame from every three frames and add independent and identically distributed Gaussian noise to each frame. Following standard practice, we randomly replace text prompt with empty string 10% during training. In our JIT-CFG setting, we also independently randomly drop image condition 8% during training. For both T2V and STIV models, the JIT-CFG scale is set to 7.5. The training schedule follows the progressive training recipe described in section 2.2. C. Implementation Details of Text Encoders We used our internal CLIP text encoder to encode text into embeddings. Concretely, text is first tokenized via T5 tokenizer. The tokenized text is mapped into embeddings via an embedding lookup table and further encoded via 32 layers of transformer with casual attention. Each transformer layer contains 20 attention heads. Each attention head has 64 hidden dimensions. The output text embedding has dimension of 1280. D. Ablation Study on Text-to-Image Generation Baseline Setup For our base model, we employed the PixArt-α architecture [8], which builds on the DiT [43] model with added cross-attention layers to integrate image tokens with text embeddings. As pre-trained components, we used the open-source sd-vae-ft-ema model9 and OpenAI CLIP L14 model10, both of which are widely adopted in the community. We conduct our experiments using the XL model configuration with 2562 image size. The full baseline model, which includes the VAE and CLIP text encoder, has approximately 1.06 billion parameters. For noise generation and denoising, we used diffusion-based approach with Stable Diffusions default noise schedule. The training was conducted with batch size of 4,096 over 400k steps, which corresponds to approximately 1.4 epochs on our internal text-to-image dataset. Table 2 summarizes the results of our ablation study, focusing on the following aspects: Stabilized Training Leveraging recent advancements in LLM and diffusion model architectures, we integrated QK-Norm [28] to manage the activation scale within attention layers. Additionally, we applied sandwich-norm [25] to both the inputs and outputs of the attention layer and the feedforward layer. Projected conditions, including timestep embeddings, pooled CLIP text embeddings, and micro condition embeddings, were normalized before being input to AdaLN. These normalization techniques enhanced training stability, allowing us to increase the learning rate from 1 104 to 2 104, and also resulting in quality improvements. Noising and Denoising Process Formulation We explored optimized noising/denoising formulations by replacing the diffusion process with flow-based linear interpolant[41]. Additionally, we applied renormalization at each inference step to counteract potential side effects from high classifier-free guidance (CFG) values. Here, the norm of the prediction with CFG was linearly scaled to match the conditional prediction norm, as explained in Sec. A. 9https://huggingface.co/stabilityai/sd-vae-ft-ema 10https://huggingface.co/openai/clip-vit-large-patch14 2 Figure 13. Human evaluation results on significant changes in T2I ablation study Tab. 2. Training Cost Optimization To reduce training costs, we evaluated three strategies: (1) switching from the AdamW optimizer to Adafactor, (2) applying MaskDiT training with 50% masking ratio, and (3) using shared AdaLN module across layers instead of unique instances per layer. These changes reduced per-device HBM usage from approximately 28GB to 11GB, allowing us to train on v5e TPUs instead of the more costly v5p TPUs. Notably, as shown in Table 2, masked training may adversely affect metrics such as FID and HPS. However, we found additional unmask finetuning for short duration (e.g. 50k steps) can fix the artifacts causing these score drops. However, this additional training phase was not included in the final configuration, as further training on video generation can address this issue as well. Enhanced Pre-trained Models and Conditioning We evaluated improvements from advanced pre-trained models and additional conditioning techniques. Specifically, we upgraded from the OpenAI CLIP L14 to an internally trained CLIP-bigG model [36] and from 4-channel to an 8-channel VAE. We also introduced 2D RoPE to support masked training and added micro-conditions, inspired by SDXL [45], to mitigate cropping artifacts in elongated objects. Finally, synthetic captions generated via [35] were included in our training data, resulting in notable performance gains. Human Evaluation of Model Changes To validate improvements observed in automated metrics, we conducted human evaluations for key modifications, including the addition of synthetic captions, upgrade of CLIP model, and transition from diffusion to flow matching based objective. Human raters are asked to asses image fidelity, text-image alignment, and visual appeal, and give 5 level preference ratings for image pairs. Each pair is sent to 5 raters for rating and the image pair will be considered tie of combined voting is neutral. Results from Figure 13 demonstrate clear alignment between automated metrics and human judgments. This justifies our usage of automatic evaluation as daily development metrics to maintain generation quality and prevent regressions leading to significant quality losses. E. Detailed Results for Imaging Dropout As mentioned in Section 3.4.2, after adding imaging dropout. We observe this phenomenon happens when we scale our model to 8B with >= 512 resolutions, probably due to the model being more easily overfitting to follow the first frame with larger model capacity, and it becomes worse under the higher resolution. Specifically, we showcase some examples to see the different between generated videos without image dropout and videos with image dropout (STIV-M-512). We generate the videos conditioned on the first frame and text prompt borrowed from MovieGenBench [46] As shown in Fig. 14 to 16, using image condition dropout in general achieves better performance than the baseline in terms of motion quality. STIV-M-512 V.S. STIV-M-512-JIT Prompt: red panda taking bite of pizza. Reference Image Reference Image Prompt: rocket blasting off from the launch pad, accelerating rapidly into the sky. Reference Image Reference Image Figure 14. Visualization of STIV-M-512 V.S. STIV-M-512-JIT. (Given the same prompt, the figures in the top row are generated by STIV-M-512, while the figures in the bottom row are generated by STIV-M-512-JIT.) 4 STIV-M-512 V.S. STIV-M-512-JIT Prompt: sports car accelerating rapidly on an open highway, the engine roaring. Reference Image Reference Image Prompt: glass of iced coffee condensing water on the outside, with droplets forming and sliding down the glass in slow motion. Reference Image Reference Image Figure 15. Visualization of STIV-M-512 V.S. STIV-M-512-JIT. (Given the same prompt, the figures in the top row are generated by STIV-M-512, while the figures in the bottom row are generated by STIV-M-512-JIT.) STIV-M-512 V.S. STIV-M-512-JIT Prompt: Cars and pedestrians move through bustling downtown street lined with skyscrapers, their lights reflecting off the windows of the towering buildings as day turns to dusk. Reference Image Reference Image Prompt: Robots move efficiently through futuristic laboratory, adjusting holographic displays and conducting experiments, while scientists observe and interact with the high-tech equipment. Reference Image Reference Image Figure 16. Visualization of STIV-M-512 V.S. STIV-M-512-JIT. (Given the same prompt, the figures in the top row are generated by STIV-M-512, while the figures in the bottom row are generated by STIV-M-512-JIT.) 6 F. Detailed Results for T2V and STIV F.1. Details of VBench and VBench-I2V Evaluation Metrics We follow the same as the evaluation protocol provided by VBench [31]. F.1.1 Video Quality Video Quality is divided into two aspects: Temporal Quality and Image Quality. Temporal Quality evaluates cross-frame consistency, including (1) Subject Consistency, ensuring that subjects maintain consistent appearance across frames; (2) Background Consistency, assessing stability in the background using feature similarity; (3) Temporal Flickering, measuring smooth transitions in both static and dynamic areas; (4) Motion Smoothness, evaluating the fluidity and realism of motion; and (5) Dynamic Degree, analyzing the presence of large-scale dynamics or motions. Image Quality focuses on individual images and evaluates (1) Aesthetic Quality, considering artistic appeal and visual richness, and (2) Imaging Quality, measuring clarity, noise, and other distortions. F.1.2 Video-Condition Consistency Video-Condition Consistency ensures alignment with the input prompt and is categorized into Semantics and Style, each with finer-grained dimensions. Semantics (1) Object Class: Measures the success of generating specific objects described in the text prompt. (2) Multiple Objects: Evaluates the ability to compose multiple objects from different classes in single frame. (3) Human Action: Assesses whether the generated video accurately captures actions described in the prompt. (4) Color: Ensures synthesized object colors align with the text description. (5) Spatial Relationship: Checks whether spatial relationships between objects align with the prompt. (6) Scene: Evaluates consistency between generated scenes and the intended description (e.g., ocean versus river). Style (1) Appearance Style: Measures consistency of styles mentioned in the prompt, such as oil painting or cyberpunk (2) Temporal Style: Assesses temporal continuity of styles across frames, ensuring smooth transitions. Overall Consistency We further evaluate Overall Consistency using metrics that combine semantic and style alignment, reflecting both the accuracy and coherence of generated videos. VBench-I2V builds upon the VBench with three new Video-Image Alignment metrics: Subject Consistency, Background Consistency, and Camera Motion Control. These additional metrics provide more comprehensive evaluation by focusing on how well the generated video aligns with the input image and specified prompt instructions. Specifically, Subject Consistency evaluates the alignment between the subject in the input image and the generated video, ensuring coherence in character or object representation. Background Consistency assesses the continuity of the background scene between the input image and the video, highlighting the models ability to maintain consistent environment. Camera Motion Control, under Video-Text Alignment, examines the adherence to camera control directions as described in the prompt, which is crucial for generating realistic video sequences that respond to specified dynamic instructions. F.2. Detailed Results on VBench and VBench-I2V We showcase the detailed version of the performance shown in Tab. 11 and Tab. 12. 7 Model CogVideoX-5B [65] CogVideoX-2B [65] Allegro [75] AnimateDiff-V2 [26] OpenSora V1.2 [74] T2V-Turbo [38] VideoCrafter-2.0 [7] LaVie-2 [60] LaVIE [60] ModelScope [59] VideoCrafter [6] CogVideo [30] PIKA [44] Gen-3 [52] Gen-2 [51] KLING [34] EMU3 [24] XL XXL M-256 M-512 M-512-SFT M-512-SFT+TUP M-512-UnMSFT M-512-UnMSFT+TUP Model CogVideoX-5B [65] CogVideoX-2B [65] Allegro [75] AnimateDiff-V2 [26] OpenSora V1.2 [74] T2V-Turbo [38] VideoCrafter-2.0 [7] LaVie-2 [60] LaVIE [60] ModelScope [59] VideoCrafter [6] CogVideo [30] PIKA [44] Gen-3 [52] Gen-2 [51] KLING [34] EMU3 [24] XL XXL M-256 M-512 M-512-SFT M-512-SFT+TUP M-512-UnMSFT M-512-UnMSFT+TUP Subject Cons. 96.2 96.8 96.3 95.3 96.8 96.3 96.9 97.9 91.4 89.9 86.2 92.2 96.9 97.1 97.6 98.3 95.3 96.0 97.5 96.0 95.9 96.7 94.8 94.3 95.2 Color 82.8 79.4 82.8 87.5 90.1 89.9 92.9 91.7 86.4 81.7 78.8 79.6 90.6 80.9 89.5 89.9 88.3 86.4 90.8 83.6 91.2 93.7 94.7 92.0 87. Back. Cons. 96.5 96.6 96.7 97.7 97.6 97.0 98.2 98.5 97.5 95.3 92.9 95.4 97.4 96.6 97.6 97.6 97.7 98.5 98.9 98.5 96.9 97.4 95.9 96.9 95.8 Spatial Rel. 66.4 69.9 67.2 34.6 68.6 38.7 35.9 38.7 34.1 33.7 36.7 18.2 61.0 65.1 66.9 73.0 68.7 42.4 45.1 44.5 51.0 58.0 50.6 59.8 46.9 Temporal Flickering 98.7 98.9 99.0 98.8 99.5 97.5 98.4 98.8 98.3 98.3 97.6 97.6 99.7 98.6 99.6 99.3 98.6 98.4 99.1 98.6 98.8 98.7 98.7 98.8 98.8 Scene 53.2 51.1 46.7 50.2 42.4 55.6 55.3 49.6 52.7 39.3 43.4 28.2 49.8 54.6 48.9 50.9 37.1 54.4 45.5 54.7 53.6 52.8 57.3 53.1 57.1 Motion Smooth. 96.9 97.7 98.8 97.8 98.5 97.3 97.7 98.4 96.4 95.8 91.8 96.5 99.5 99.2 99.6 99.4 98.9 96.5 98.2 97.2 98.0 98.3 99.2 96.7 99. App. Style 24.9 24.8 20.5 22.4 24.0 24.4 25.1 25.1 23.6 23.4 21.6 22.0 22.3 24.3 19.3 19.6 20.9 22.4 22.1 22.5 23.9 24.6 24.5 24.8 24.5 Dynamic Degree 80.0 59.9 55.0 40.8 42.4 49.2 42.5 31.1 49.7 66.4 89.7 42.2 47.5 60.1 18.9 46.9 79.3 62.5 48.6 68.1 59.7 70.8 70.8 77.8 70.8 Temp. Style 25.4 24.4 24.4 26.0 24.5 25.5 25.8 25.2 25.9 25.4 25.4 7.8 24.2 24.7 24.1 24.2 23.3 26.3 26.1 26.6 25.8 26.2 26.7 26.7 26.6 Aesthetic Quality 62.0 60.8 63.7 67.2 56.9 63.0 63.1 67.6 54.9 52.1 44.4 38.2 62.4 63.3 67.0 61.2 59.6 56.3 56.2 57.0 60.6 61.7 63.7 61.4 63.6 Overall Cons. 27.6 26.7 26.4 27.0 26.9 28.2 28.2 27.4 26.4 25.7 25.2 7.7 25.9 26.7 26.2 26.4 24.8 27.8 27.4 28.4 27.8 28.5 28.6 28.8 28.5 Imaging Quality 62.9 61.7 63.6 70.1 63.3 72.5 67.2 70.4 61.9 58.6 57.2 41.0 61.9 66.8 67.4 65.6 62.6 59.3 59.7 60.8 62.5 63.9 65.0 68.6 65. Quality Score 82.8 82.2 83.1 82.9 81.4 82.6 82.2 83.2 78.8 78.1 81.6 72.1 82.9 84.1 82.5 83.4 84.1 80.7 81.2 82.7 82.2 83.9 84.2 83.7 84.4 Object Class 85.2 83.4 87.5 90.9 82.2 94.0 92.6 97.5 91.8 82.2 87.3 73.4 88.7 87.8 90.9 87.2 86.2 91.5 91.1 88.8 85.9 88.1 88.9 90.0 90.0 Semantic Score 77.0 75.8 73.0 69.8 73.4 74.8 73.4 75.8 70.3 66.5 72.2 46.8 71.8 75.2 73.0 75.7 68.4 72.5 72.7 74.8 77.0 78.3 78.5 79.5 77.2 Multiple Objects 62.1 62.6 59.9 36.9 51.8 54.7 40.7 64.9 33.3 39.0 25.9 18.1 43.1 53.6 55.5 68.1 44.6 41.3 49.1 62.1 72.4 67.7 70.3 72.3 69.8 Total Score 81.6 80.9 81.1 80.3 79.8 81.0 80.4 81.8 77.1 75.8 79.7 67.0 80.7 82.3 80.6 81.9 81.0 79.1 79.5 80.6 81.2 82.8 83.1 82.9 83.0 Human Action 99.4 98.0 91.4 92.6 91.2 95.2 95.0 96.4 96.8 92.4 93.0 78.2 86.2 96.4 89.2 93.4 77.7 98.0 99.0 98.0 96.0 97.0 95.0 97.0 94. Averaged Scores 70.0 68.3 67.5 64.7 66.0 67.4 66.0 67.6 63.8 62.4 62.3 52.3 66.1 68.5 66.1 68.8 66.7 66.1 65.9 67.9 68.8 70.3 70.3 71.2 69.7 Table 11. Detailed Evaluation Results for Text-To-Video Generation Models. 8 Model DynamicCrafter-256 [62] DynamicCrafter-512 [62] Animate-Anything [14] SVD [2] SEINE-512 [11] VideoCrafter-I2V [7] Consistent-I2V [49] I2VGen-XL [70] STIV-M STIV-M-512 STIV-M-512-JIT Model DynamicCrafter-256 [62] DynamicCrafter-512 [62] Animate-Anything [14] SVD [2] SEINE-512 [11] VideoCrafter-I2V [7] Consistent-I2V [49] I2VGen-XL [70] STIV-M STIV-M-512 STIV-M-512-JIT Subject Consistency 94.7 93.8 98.9 95.5 95.3 97.9 95.3 94.2 95.4 99.5 98.1 Background Consistency 98.3 96.6 98.2 96.6 97.1 98.8 98.3 97.1 98.9 99.3 98.6 Imaging Quality 62.3 68.6 72.1 69.8 71.4 71.7 66.9 69.1 66.1 71.5 71.0 I2V Subject 97.1 97.2 98.8 98.8 97.2 91.2 95.8 96.5 97.0 99.2 98.8 Temporal Flickering 98.1 95.6 98.1 98.1 97.3 98.2 97.6 98.3 97.2 99.5 98. I2V Background 97.6 97.4 98.6 98.6 96.9 91.3 96.0 96.8 97.4 97.3 97.5 Motion Smoothness 97.8 96.8 98.6 98.1 97.1 98.0 97.4 26.1 98.1 99.6 99.1 Dynamic Aesthetic Quality Degree 58.7 40.6 60.9 69.7 67.1 2.7 60.2 52.4 64.6 27.1 60.8 22.6 59.0 18.6 64.8 26.1 59.0 32.1 62.5 10.2 65.4 24.0 Camera Motion 20.9 32.0 13.1 62.3 21.0 33.6 33.9 18.5 22.7 13.2 15.1 I2V Quality 80.2 81.6 81.2 82.8 80.6 81.3 78.9 81.2 78.8 82.1 81.9 Final Score 88.4 89.1 89.8 89.9 88.4 85.1 86.8 88.5 87.6 90.1 89. Table 12. Detailed Evaluation Results for Text-Image-To-Video Generation Models. G. Details of Model Initialization Ablations To facilitate fair comparison for different initialization methods we estimate the FLOPs associated with spatialtemporal computation in the transformer for various model training steps (Tables 13 and 14). When controlling for FLOPs we take into account, the compute used to pretrain the intermediate models, the reduction in an effective number of tokens due to masking in the relevant attention blocks, the increased parameter count when temporal attention is included, and the increased number of tokens passed to the model during high resolution training. For both the high resolution and higher frame count experiments we attebyto keep the compute budget across model initialization ablations similar. Tables 15 and 16 show the VBench quality metrics for high resolution and high frame count XL sized models respectively. Init. Method Models Scratch T2V-256 T2I-512 Both T2V-512 T2I-256, T2V-256, T2V-512 T2I-256, T2I-512, T2V-512 T2I-256, T2V-256, T2I-512, T2VStage 1 5.93 1.11 1.11 1.11 Stage 2 Stage 3 Stage 4 2.05 8.43 2.05 2.84 4.02 8. 1.98 Total 5.93 6.00 5.97 5.98 Table 13. breakdown of FLOPs for training high resolution T2V models. Unit 1021. 9 Init. Method Models T2I T2V (int.) T2V (ext.) T2V 2x (int.) T2I-256, T2V-256-40 T2I-256, T2V-256-20, T2V-256-40 T2I-256, T2V-256-20, T2V-256-40 T2I-256, T2V-256-20 2x stride, T2V-256Stage 1 1.11 1.11 1.11 1.11 Stage 2 2.05 1.02 1.02 1.02 Stage 3 1.02 1.02 1.02 Total 3.16 3.16 3.16 3.16 Table 14. breakdown of FLOPs for training high frame count T2V models. Unit: 1021. Initial Method Scratch T2V-256 T2I-512 Both Subject Cons. 93.1 91.9 92.3 92.4 Initial Multiple Objects 29.7 45.7 47.4 49.7 Method Scratch T2V-256 T2I-512 Both Background Cons. 97.1 97.1 97.2 97.3 Human Action 95.4 95.8 96.4 96. Temporal Flickering 97.9 98.0 98.2 98.3 Color 88.3 89.0 87.9 88.1 Motion Smoothness 97.3 97.5 97.0 97.4 Spatial Relationship 33.8 36.3 37.0 36.7 Dynamic Aesthetic Quality Degree 61.4 58.6 59.4 58.6 60.0 52.2 60.7 53. Imaging Object Class Quality 87.0 58.6 91.2 59.7 88.8 59.3 60.6 88.2 Scene 46.9 50.0 49.1 52.3 App. Style 21.6 21.9 22.5 22.8 Temp. Style 25.8 25.8 26.2 26.3 Overall Cons. 26.4 27.3 27.8 28. Table 15. Detailed VBench metrics of different model initialization methods for higher resolution T2V model training. Initial Method T2I T2V (int.) T2V (ext.) T2V 2x (int.) Subject Cons. 93.2 91.7 91.3 91.0 Initial Multiple Objects 30.8 25.5 28.5 29.3 Method T2I T2V (int.) T2V (ext.) T2V 2x (int.) Background Cons. 98.1 97.7 97.5 97. Human Action 92.2 95.4 95.2 94.0 Temporal Flickering 98.7 97.7 97.8 97.2 Color 85.0 85.3 84.2 87.7 Motion Smoothness 95.2 96.8 96.9 97.0 Spatial Relationship 29.9 28.6 25.9 28. Dynamic Aesthetic Quality Degree 54.2 57.8 54.7 64.7 54.6 58.6 70.3 54.1 Imaging Object Class Quality 84.6 58.2 86.9 59.2 60.0 86.1 85.8 59.4 Scene 45.2 41.4 36.8 44.2 App. Style 21.1 21.2 20.9 20.9 Temp. Style 25.0 25.3 25.6 25. Overall Cons. 26.0 26.6 26.8 26.7 Table 16. Detailed VBench metrics of different model initialization methods for higher frame count T2V model training. H. Study of Class-to-Video on UCF-101 UCF-101 is an action recognition dataset, which contains 101 classes over 9.5K training videos. Here we train STIV from scratch and perform label-to-video (L2V) generation with 16 frames and 1282 resolution. We follow TATS [22] to adopt the Inception Score (IS) [53] and FVD for the evaluation11. Tab. 17 shows that our L2V-XL achieves significant improvements, leading to +12% IS and -22% FVD over MAGVIT. This also highlights the effectiveness of our model design for convention video generation. From the ablation study over different modulations, only without spatial mask makes lower FVD but degrades IS, while all other settings hurt the performance. 11Following our baselines (https://github.com/songweige/TATS/issues/13), we apply C3D [58] pre-trained on UCFfor the IS logits. For FVD, we adopt I3D [5] pre-trained on Kinetics-400 [32] to calculate the video embeddings."
        },
        {
            "title": "Method",
            "content": "IS FVD CogVideo [30] TATS [22] MMVG [20] VideoFusion [40] MAGVIT [67] XL-128 - Spatial Mask + Temporal Mask + Temporal ScaleShiftGate + Causal TemporalAttention 50.5 79.3 73.7 80.0 83.6 93.4 88.5 94.9 78.9 86.9 626 332 328 173 159 124 102 167 141 106 VBenchQuality - - - - - 69.9 70.6 68.1 69.1 70. Table 17. Performance of Class-to-Video Generation on UCF-101. I. Details of Video Data Engine Details of Video Pre-processing and Feature Extraction To ensure high-quality input data, we first address the issue of inconsistent motions and unwanted transitions like cuts and fades in raw videos. Using PySceneDetect12, we analyze video frames to identify and segment scenes with abrupt transitions or gradual fades. This process isolates and removes inconsistent segments, resulting in video clips that maintain visual consistency, reducing artifacts and improving the overall quality. After that, we extract several initial features for future filtering, including motion score, aesthetic score, text area, frame height, frame width, clarity score, temporal consistency, and video orientation, et al. Details of Video Captioning and Categorization Video-text pairs play crucial role in training text-to-video generation models. However, many video datasets lack well-aligned, high-quality captions and often include noisy or irrelevant content. Therefore, weve incorporated an additional video captioning module in our pipeline to generate comprehensive textual descriptions. We mainly explore two directions: (1) sample few frames, apply an image captioner, and then use an LLM to summarize the resulting captions [2]; (2) apply video LLM to generate captions. We initially explored the first direction but found two major limitations. Firstly, the image captioner can only capture visual details in single frame, resulting in lack of descriptions of video motions. Secondly, the LLM may hallucinate when prompted to generate dense caption based on multiple frame captions. Recent works [9, 69, 71, 72] use GPT-4V or GPT-4o to curate fine-tuning dataset and train their video LLMs. To balance quality and cost in large-scale captioning, we select LLaVA-Hound-7B [69] as our video captioner. Then, we use an LLM to categorize the generated captions and obtain the distribution of videos. 12https://github.com/Breakthrough/PySceneDetect 11 Figure 17. Category distribution of our curated Panda30M dataset. Details of DSG-Video: Hallucination Evaluation To compare various captioning techniques, we develop an evaluation module that assesses both caption richness and accuracy. We quantify caption richness by measuring the diversity of unique objects referenced across captions. And we identify hallucinations to assess accuracy. Inspired by DSG [12] for fine-grained text-to-image evaluation, we introduce DSG-Video to validate the presence of objects referenced in captions within the video content  (Fig. 9)  . First, we use an LLM to automatically generate questions that probe key details of captions, such as object identity, actions, and context. For instance, given caption mentioning \"a cat sitting on couch\", the LLM will generate questions like \"Is there cat in the video?\" and \"Is the cat on couch?\". Second, we utilize multimodal LLM to answer these object verification questions by evaluating the presence of each referenced object in uniformly sampled frames from the video. For each question generated (e.g., \"Is there cat in this frame?\"), the multimodal LLM examines each sampled frame and provides response. If, for given question, all frame-based responses suggest that the entity in question is absent, we classify the object as hallucinated. This approach ensures thorough and frame-by-frame validation for each object in the video. Based on this, we have two metrics: one that measures the fraction of hallucinated object instances (referred to as DSG-Videoi), and the other that calculates the fraction of sentences containing hallucinated objects (referred to as DSG-Videos). We use these two metrics to assess caption quality. DSG-Videoi = {hallucinated objects} {all objects mentioned} , DSG-Videos = {sentences with hallucinated object} { all sentences} . J. Details for Flexible Applications J.1. Video Prediction We start from STIV-XXL model to train text-video-to-video model. Specifically, we replace the first four frames with ground truth frame latents to train additional 400K steps. The results are shown in the main text. J.2. Frame Interpolation We start from STIV-XL model to train text-video-to-video model. Specifically, we use stride of two to sample the frames with ground truth frame latents to train additional 400K steps. The results are shown in the main text. J.3. Multi-view Generation Our model employs temporal attention to model cross-frame correspondence. However, this approach may be less effective than full-attention mechanisms, as temporal attention focuses solely on the same spatial position across all frames. To investigate this limitation, we conducted experiments on multiview generation, which involves predicting novel views of specific object. For evaluation, we adopted the six-view scheme from Zero123++, consisting of elevation angles of 30, 20, 30, 20, 30, 20 and azimuth angles of 30, 90, 150, 210, 270, 330. This scheme was chosen because the large camera changes between adjacent views simulate challenging scenario where the camera rotates around an object with significant motion, providing robust test for the temporal attention mechanisms ability to capture large inter-frame motions. For implementation, we redefined the input frames in our text-image-to-video model as the given views of the object, with an empty string as the text conditioning. The negative input views were set to zero latent vectors during classifier-free guidance training. For the generation, we used guidance scale of 3.5. visual comparison between our approach and Zero123++ is shown in Fig. 11. K. More Examples We show more examples at the end of the Appendix using the text prompts and image as first frame condition borrowed from MovieGenBench [46] and Sora [42]. 13 Prompt: pirate ship sailing through storm with enormous waves crashing against the sides, its crew fighting against the wind as lightning illuminates the scene. Text-to-Video Prompt: samurai on horseback charging across field of cherry blossoms, slicing petals in mid-air as they fall, leaving trail of pink in their path. Prompt: Two pigs are eating hotpot. Prompt: Giant Pandas are eating hot noodles in Chinese restaurant. 14 Prompt: zoom-in on clock face, focusing on the intricate movement of the hands and the ticking mechanism inside. Text-to-Video Prompt: Robots move efficiently through futuristic laboratory, adjusting holographic displays and conducting experiments, while scientists observe and interact with the high-tech equipment. Prompt: robotic arm wielding glowing sword, battling shadowy figure in high-tech dojo, each strike creating sparks that light up the space. Prompt: city skyline reflected in the water, but the reflection shows an alternate world with flying cars, towering robots, and futuristic architecture. 15 Text-to-Video Prompt: dog dressed as chef, expertly flipping pancakes in kitchen. Prompt: motocross bike accelerating out of tight turn on dirt track. Prompt: snowboarder performing dramatic backflip over frozen lake, landing gracefully and leaving trail of sparkling ice dust in the air. Prompt: surfer accelerating on wave, carving through the water. 16 Text-to-Video Prompt: person dancing with their own shadow, which has come to life. Prompt: bobsled team accelerating down an icy track. Prompt: cyclist accelerating out of the saddle during steep climb. Prompt: speed skater accelerating during short track race. 17 Text-Image-to-Video Prompt: Reflections in the window of train traveling through the Tokyo suburbs. Reference Image Prompt: The Glenfinnan Viaduct is historic railway bridge in Scotland, UK, that crosses over the west highland line between the towns of Mallaig and Fort William. It is stunning sight as steam train leaves the bridge... Reference Image Prompt: The camera follows behind white vintage SUV with black roof rack as it speeds up steep dirt road surrounded by pine trees on steep mountain slope, dust kicks up from its tires, the sunlight shines on the SUV... Reference Image Prompt: Photorealistic closeup video of two pirate ships battling each other as they sail inside cup of coffee. Reference Image 18 Prompt: litter of golden retriever puppies playing in the snow. Their heads pop out of the snow, covered in. Text-Image-to-Video Reference Image Prompt: An adorable happy otter confidently stands on surfboard wearing yellow lifejacket, riding along turquoise tropical waters near lush tropical islands, 3D digital render art style. Reference Image Prompt: dog dressed as chef, expertly flipping pancakes in kitchen. Reference Image Prompt: skeleton wearing flower hat and sunglasses dances in the wild at sunset. Reference Image 19 Text-Image-to-Video Prompt: The video features central spacecraft with predominantly white and gray color scheme, accented with red and black details. It has sleek, angular design with multiple protruding elements that suggest advanced technology... Reference Image Prompt: The video begins with dark space background, dotted with stars, and central object that appears to be spacecraft with glowing blue light at its core. The spacecraft is detailed with various components... Reference Image Prompt: Robots move efficiently through futuristic laboratory, adjusting holographic displays and conducting experiments, while scientists observe and interact with the high-tech equipment. Reference Image Prompt: The video presents serene scene with group of camels walking in line across desert landscape. The camels are adorned with colorful saddles and are led by person wearing green garment. The background features clear sky... Reference Image 20 Text-Image-to-Video Prompt: crab made of different jewlery is walking on the beach. As it walks, it drops different jewelry pieces like diamonds, pearls, etc. Reference Image Prompt: The video captures single sea turtle with patterned shell and flippers, swimming in clear blue underwater environment. The turtle moves gracefully over bed of coral reefs, which exhibit variety of colors... Reference Image Prompt: mesmerizing video of jellyfish moving through water, with its tentacles flowing gracefully. Reference Image Prompt: video of diver creating bubbles underwater, with bubbles rising and interacting with each other. Reference Image Text-Image-to-Video Prompt: The individual in the video is dressed in blue protective suit with hood, mask with filter, and white gloves. They are holding spray bottle in one hand and spray nozzle in the other... Reference Image Prompt: The video captures bustling city street scene during the evening. The sky is overcast, and the street is wet, reflecting the lights from the vehicles and buildings. The buildings are tall with modern architecture... Reference Image Prompt: The video presents series of images capturing the Colosseum from an aerial perspective during the evening. The ancient amphitheater is illuminated by artificial lighting, which highlights its circular shape and the arches... Reference Image Prompt: The video features two dogs, one with predominantly white coat and the other with mix of black, brown, and white fur. Both dogs are adorned with accessories; the white dog wears red tie, while the other sports purple bow tie... Reference Image"
        }
    ],
    "affiliations": [
        "Apple",
        "University of California, Los Angeles"
    ]
}