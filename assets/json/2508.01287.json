{
    "paper_title": "Exploitation Is All You Need... for Exploration",
    "authors": [
        "Micah Rentschler",
        "Jesse Roberts"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Ensuring sufficient exploration is a central challenge when training meta-reinforcement learning (meta-RL) agents to solve novel environments. Conventional solutions to the exploration-exploitation dilemma inject explicit incentives such as randomization, uncertainty bonuses, or intrinsic rewards to encourage exploration. In this work, we hypothesize that an agent trained solely to maximize a greedy (exploitation-only) objective can nonetheless exhibit emergent exploratory behavior, provided three conditions are met: (1) Recurring Environmental Structure, where the environment features repeatable regularities that allow past experience to inform future choices; (2) Agent Memory, enabling the agent to retain and utilize historical interaction data; and (3) Long-Horizon Credit Assignment, where learning propagates returns over a time frame sufficient for the delayed benefits of exploration to inform current decisions. Through experiments in stochastic multi-armed bandits and temporally extended gridworlds, we observe that, when both structure and memory are present, a policy trained on a strictly greedy objective exhibits information-seeking exploratory behavior. We further demonstrate, through controlled ablations, that emergent exploration vanishes if either environmental structure or agent memory is absent (Conditions 1 & 2). Surprisingly, removing long-horizon credit assignment (Condition 3) does not always prevent emergent exploration-a result we attribute to the pseudo-Thompson Sampling effect. These findings suggest that, under the right prerequisites, exploration and exploitation need not be treated as orthogonal objectives but can emerge from a unified reward-maximization process."
        },
        {
            "title": "Start",
            "content": "Exploitation Is All You Need... for Exploration Micah Rentschler1, Jesse Roberts1 1Tennessee Technological University mrentschler@tntech.edu, jtroberts@tntech.edu 5 2 0 2 2 ] . [ 1 7 8 2 1 0 . 8 0 5 2 : r Abstract Ensuring sufficient exploration is central challenge when training meta-reinforcement learning (meta-RL) agents to solve novel environments. Conventional solutions to the explorationexploitation dilemma inject explicit incentives such as randomization, uncertainty bonuses, or intrinsic rewards to encourage exploration. In this work, we hypothesize that an agent trained solely to maximize greedy (exploitation-only) objective can nonetheless exhibit emergent exploratory behavior, provided three conditions are met: (1) Recurring Environmental Structure, where the environment features repeatable regularities that allow past experience to inform future choices; (2) Agent Memory, enabling the agent to retain and utilize historical interaction data; and (3) Long-Horizon Credit Assignment, where learning propagates returns over time frame sufficient for the delayed benefits of exploration to inform current decisions. Through experiments in stochastic multi-armed bandits and temporally extended gridworlds, we observe that, when both structure and memory are present, policy trained on strictly greedy objective exhibits information-seeking exploratory behavior. We further demonstrate, through controlled ablations, that emergent exploration vanishes if either environmental structure or agent memory is absent (Conditions 1 & 2). Surprisingly, removing long-horizon credit assignment (Condition 3) does not always prevent emergent explorationa result we attribute to the pseudo-Thompson Sampling effect. These findings suggest that, under the right prerequisites, exploration and exploitation need not be treated as orthogonal objectives but can emerge from unified reward-maximization process."
        },
        {
            "title": "Introduction",
            "content": "Consider children playing hide-and-go-seek repeatedly. The seeker, eager to win, pays attention to where friends have hidden in past rounds. She remembers which spots were successful, which were not, and adapts her searching accordingly. She explores less familiar places not out of intrinsic curiosity, but because the knowledge she gains may help her tag more hiders and score more points in the long run. Can machines do the same? Can they learn to explore in order to exploit? That is the subject of this paper. We define exploration as the selection of actions with the aim of acquiring information about the environments dynamics or reward structure, thereby reducing epistemic uncertainty and enabling more effective decision-making. Exploration is ubiquitous in humans and animals, yet unlike drives such as hunger or pain, there is no clear, biologically plausible mechanism directly incentivizing it. Neuroscience shows the brain balances exploration and exploitation through meta-learning, which relies on repeating tasks, memory, and rewards (Botvinick et al. 2018). In reinforcement learning (RL), designers face similar explorationexploitation dilemma: how should an agent balance exploring unknown actions (potentially yielding future gain) and exploiting known rewards (Sutton and Barto 2018; Thrun 1992)? Traditional RL methods address this by introducing explicit exploratory incentives such as ϵ-greedy sampling (Sutton and Barto 2018), Upper Confidence Bound (UCB) algorithms (Auer, Cesa-Bianchi, and Fischer 2002), or intrinsic motivation mechanisms like curiosity-driven exploration (Pathak et al. 2019; Burda et al. 2018) that treat exploration as orthogonal to exploitation. In contrast, we ask: Can exploration emerge organically from purely greedy objective? We hypothesize that it canunder three key conditions: 1. Recurring Environmental Structure. The task repeats so early-acquired information remains valuable later. 2. Agent Memory. The policy retains and utilizes past experiences across episodes. 3. Long-Horizon Credit Assignment. Learning must connect information gathering to long-term payoffs. We empirically test this hypothesis via controlled ablation studies in both multi-armed bandits and temporally extended gridworlds. With all three conditions met, our results show that the greedy agent consistently exhibits informationseeking behavior even without explicit incentives. Removing structure or memory eliminates this effect. Remarkably in some contexts, emergent exploration persists even when training optimizes only the immediate cumulative reward of the episode, suggesting distinct alternate mechanism. Our findings challenge the view that exploration and exploitation require separate objectives. In structured environments with memory, reward maximization alone enables effective exploration. This suggests shifting focus from exploration bonuses to memory-rich architectures that leverage repeated patterns, simplifying RL design and offering biologically plausible explanation for exploration."
        },
        {
            "title": "2 Related Work",
            "content": "The explorationexploitation trade-off is longstanding challenge in reinforcement learning (RL) and sequential decision making (Sutton and Barto 2018; Lattimore and Szepesvari 2020; Thrun 1992). Early approaches focused on randomization techniques, such as ϵ-greedy or softmax action selection, as well as principled methods like Upper Confidence Bound (UCB) (Auer, Cesa-Bianchi, and Fischer 2002) and Thompson Sampling (TS) (Thompson 1933), which provide regret guarantees. To extend tabular exploration to high-dimensional environments, density modeling and pseudo-count approaches were proposed (Bellemare et al. 2016). Hashing-based methods (Tang et al. 2017) and successor representations (Machado, Bellemare, and Bowling 2020) have shown strong performance in complex domains by encouraging exploration through intrinsic bonuses. Intrinsic motivation mechanisms, such as curiosity-driven exploration, reward agents for visiting novel or unpredictable states (Oudeyer and Kaplan 2007; Pathak et al. 2019). Random Network Distillation (RND) (Burda et al. 2019) is notable example, providing intrinsic bonuses in sparse-reward Atari games. Recent methods, such as SPIE (Yu et al. 2023), combine counts and trajectory structure to target bottlenecks in exploration. Meta-reinforcement learning (meta-RL) tackles rapid adaptation by learning across distribution of related tasks (Duan et al. 2017; Zintgraf et al. 2020). Memory-rich architectures, such as recurrent neural networks (Hausknecht and Stone 2015) and transformers (Melo 2022; Chen et al. 2021; Parisotto et al. 2020), allow policies to internalize exploratory strategies. In meta-RL, recurrent policy successfully learns to explore via reward maximization alone (Duan et al. 2017; Wang et al. 2017). VariBAD (Zintgraf et al. 2020) applies Bayesian principles to guide implicit exploration, though with explicit modeling of uncertainty. Neuroscientific evidence suggests that the brains metalearning mechanisms manage the explorationexploitation tradeoff by relying on specific prerequisites: recurring task structures, memory, and reward-driven learning processes (Botvinick et al. 2018). Notably, these prerequisites correspond closely to the conditions we have identified as essential for emergent exploration. In addition to presenting new empirical findings, this study clarifies the boundaries and limitations of emergent exploration in meta-reinforcement learning. We systematically delineate and validate the specific conditions necessary for exploration to emerge from pure exploitation objectives, emphasizing the critical roles of recurring environmental structure and agent memory. Our analysis further raises new questions about long-term credit assignment, suggesting that its necessity may be context-dependent. To our knowledge, this is the first work to clearly articulate and formalize the preconditions for emergent exploration, demonstrating that it arises in multi-step environments when these preconditions are met. This provides concrete foundation for ideas that have long been discussed in the field."
        },
        {
            "title": "3 Background\n3.1 Repeated Markov Decision Processes (MDPs)",
            "content": "To investigate when exploration emerges from pure exploitation, we consider repeated-task setting: fixed, partially observable Markov decision process (POMDP) instance = (S, A, O, P, Ω, r, γ) is sampled from distribution, and the agent interacts with this same environment for multiple episodes. At the end of each episode, with probability 1/n, new parameterization of the environment is sampled, marking the start of new task block. Thus, each block consists of random number of episodes, geometrically distributed with mean of n, during which the environments parameters remain fixed. Each episode begins from an initial state s0 ρ, while the transition dynamics , observation kernel Ω, and reward function are unchanged within block. As result, information acquired in early episodessuch as the location of goal or the optimal armremains valuable in later episodes of the same block. This regime enables agents to accumulate and exploit knowledge across episodes."
        },
        {
            "title": "3.2 Meta-Reinforcement Learning (Meta-RL)",
            "content": "Traditional RL trains agents to solve single environment; meta-reinforcement learning (meta-RL) instead aims to create agents that rapidly adapt to new in-distribution tasks. In meta-RL, the agent is trained end-to-endoften via recurrent neural networks or transformersto internalize optimization strategies within its policy, effectively learning to learn across episodes (Duan et al. 2017; Wang et al. 2017). The agents memory (e.g., RNN hidden state or transformer context window) encodes past interactions, enabling the use of prior experience for fast adaptation within block of repeated tasks. Recently, meta-RL has more often employed transformers, which maintain memory by directly attending over the input sequence rather than through an explicit hidden state (Rentschler and Roberts 2025; Melo 2022). The entire sequence of actions, observations, and rewards is tokenized and fed to the transformer as input. Causal transformers restrict attention to past tokens, supporting autoregressive sequence modeling. Pretraining and subsequent finetuning can enhance adaptability and context sensitivity."
        },
        {
            "title": "4.1 Environments",
            "content": "We evaluate our hypothesis using both multi-armed bandit problems and multi-step gridworlds: Multi-Armed Bandits: In the K-armed bandit setting, each arm provides rewards from stationary distribution (in our experiments Bernoulli distribution). The agent selects arms across multiple episodes, with reward parameters fixed within each task block. At the end of each episode, with probability 1/n, new block begins and new reward distributions are sampled; otherwise, the current block continues. Thus, the block length is geometric random variable with mean n. By varying n, we control the degree of recurring structure: small yields little structure, while larger allows information acquired early to generalize to later episodes. Gridworlds: For longer-horizon tasks, we use variants of the Frozen Lake gridworld. Agents must navigate from start state to goal, receiving reward only upon success. We modified the typical reward for success to 1/t where is the number of time steps taken to reach the goal, giving the agent more reward the faster it reaches the goal. The environment remains fixed for randomly determined number of episodes per block, with each episode terminating the block with probability 1/n (i.e., mean block length n), after which new grid is generated. This setup allows us to test whether agents are willing to forgo shortterm rewards to gather information that aids later performance."
        },
        {
            "title": "4.3 Training Procedure\nAgents are trained with the Deep Q-Network (DQN) algo-\nrithm (Mnih et al. 2015), using a step discount factor γstep\nto set the effective temporal credit assignment. At the end\nof each episode, we use the episode discount factor γepisode.\nThe value function is reset at block boundaries, so the ob-\njective is to maximize total expected discounted reward over\neach block. A delayed target network is necessary to stabi-\nlize learning. We use Polyak averaging to update the weights\nof the the target network with a factor α = 0.1.",
            "content": "We trained on single H200 GPU. Training is performed for 400 iterations in bandit environments and 1200 iterations in gridworlds unless otherwise noted. Training is conducted offline: data streams are generated by running environments with uniformly sampled actions until each stream contains at least 20X tokens. In each training iteration, fixed number of streams (32 for = 1024, 128 for = 256, 256 for = 128, 512 for = 64) are sampled to make batch, thus keeping the number of tokens per batch constant for different context windows. Action masking is applied to ensure only valid actions are included in the DQN loss. Figure 1: Setup for DQN meta-RL learning with transformer. Tokens make up elements, which comprise episodes, which in turn make up task blocks. This is fed to the transformer which is trained to output the value of each action."
        },
        {
            "title": "4.4 Testing Procedure",
            "content": "Testing is performed on either H200 or RTX3090 GPU. Before testing, we fill the context of the transformer with experience trajectories generated by taking random actions (i.e. seeding the context). Then, we let the transformer take actions for predefined number of episodes."
        },
        {
            "title": "4.5 Ablation Studies",
            "content": "To isolate the contribution of each precondition, we systematically vary: Recurring Structure: Controlled by the mean block length (large is equivalent to high degree of recurrence, while small indicates limited repetitiveness). Agent Memory: Controlled by transformer context window (small limits memory, large enables crossepisode memory). Temporal Horizon: Controlled by discount factor γepisode (lower values shorten the planning horizon; higher values enable long-term credit assignment). Unless otherwise specified, all other hyperparameters and architectural details are held constant. Performance is assessed by the extent to which agents take actions that forgo immediate reward in favor of gathering information that benefits future episodes in the same block. This exploratory behavior is best measured by the total average reward over the whole episode. Agents that explore are more likely to find better strategies and achieve higher total rewards while agents that do not explore will tend to exploit randomly encountered strategies, leading to low expected reward."
        },
        {
            "title": "4.6 Baselines\nOur goal is not to outperform standard baselines, but to\ndemonstrate that competitive performance can be achieved\nwithout explicit exploration mechanisms.",
            "content": "For bandit environments, we compare to Thompson sampling, ϵ-greedy, oracle, and random strategies. In the gridworld setting, where standard baselines are less established, we evaluate agent performance using both average reward and state visitation counts as indicators of exploration. For reference, we compare these metrics to those achieved by an oracle policy and random policy, providing context for interpreting the effectiveness of our approach."
        },
        {
            "title": "5.1 Experimental Setup\nFor both bandit and gridworld experiments, we follow the\nrepeated-task setup described in Section 4.",
            "content": "All results are averaged over multiple seeds: 10,000 runs for baselines and 1,000 for meta-RL in the bandit task, and 1,000 runs for baselines and 100 for meta-RL in gridworld. We also report 95% confidence intervals. To facilitate comparison and interpretation, we linearly normalize all reported rewards: value of 1 denotes performance of an oracle agent, while 0 indicates the average performance of random agent under identical conditions."
        },
        {
            "title": "5.2 Multi-Armed Bandit Results\nFigure 2 shows the performance of the meta-RL agent in a 3-\narm bandit environment. The agent’s returns exceed those of\nthe Thompson Sampling and ϵ-greedy baselines. This result\nis expected, as both Thompson Sampling and ϵ-greedy are\ndesigned to maximize asymptotic reward; in finite-horizon\nsettings, there is room for other approaches to achieve better\nperformance (Duan et al. 2017).",
            "content": "Figure 2: Reward per episode in the 3-arm bandit environment, comparing meta-RL (n = 30 = 1024 γepisode = 0.9) to baseline strategies. Shaded areas denote 95% confidence intervals. The meta-RL agent exceeds Thompson Sampling and ϵ-greedy baselines, demonstrating its ability to explore in the absence of explicit incentives. Effect of Recurring Structure Table 1 shows that higher mean block lengths (n)i.e., greater environment recurrenceenable agents to leverage early exploration for improved performance in later episodes. Performance drops sharply for smaller n, confirming that recurring structure is necessary for emergent exploration. 30 10 3 1 ϵ-Greedy TS Meta-RL 0.499 0.018 0.354 0.020 0.147 0.027 -0.083 0.041 0.614 0.017 0.339 0.019 0.092 0.025 -0.066 0.041 0.704 0.055 0.509 0.064 0.325 0.082 0.043 0.130 Table 1: Cumulative reward per task block as function of mean block length in the 3-armed bandit environment. Effect of Memory Capacity Table 2 demonstrates that reducing the agents memory capacity (context window X) leads to sharp decline in cumulative reward. Below critical threshold, exploration fails to emerge, confirming that sufficient memory is essential. 1024 256 128 64 32 ϵ-Greedy TS Meta-RL 0.499 0. 0.614 0.017 0.704 0.055 0.792 0.054 0.574 0.062 0.547 0.060 -0.052 0.099 Table 2: Cumulative reward per task block as function of transformer context window in the 3-armed bandit. Figure 3: Entropy of the action distribution derived from multiple trials with identical environment parameters, seeded with randomly generated episodes. The meta-RL agents (trained with γepisode = 0) actions are initially stochastic (exploratory), becoming more deterministic (exploitative) as it accumulates experience, demonstrating that the agent can output pseudo-stochastic samples conditioned on contexts. Effect of Temporal Credit Assignment When the temporal horizon is ablated (γepisode = 0), surprisingly, metaRL agents continue to display exploratory behavior, without appreciable reduction in performance. Figure 3 shows the entropy of the action distribution, measured by running multiple trials with identical environment parameters while seeding the context with randomly generated episodes. The agent starts with random actions and becomes more deterministic as it accumulates experience. This shows that the agent can output pseudo-stochastic samples conditioned on long, complex, and chaotic contexts."
        },
        {
            "title": "5.3 Gridworld Results\nFigure 4 shows the performance of the meta-RL agent in\nthe Frozen Lake gridworld. The agent achieves decent per-\nformance, closing the gap between a random policy and the\noracle (always-win policy) by 55.2%.",
            "content": "Effect of Recurring Structure As with bandits, increasing block length in gridworlds yields higher cumulative reward  (Table 3)  . The agent exploits repeated structure to improve over time. 30 10 3 1 Meta-RL 0.670 0.074 0.441 0.070 0.254 0.071 -0.050 0.004 Table 3: Cumulative reward per task block in Frozen Lake gridworld as function of mean block length n. Figure 4: Reward per episode in the Frozen Lake gridworld, comparing meta-RL (n = 30 = 1024 γepisode = 0.9) to random and oracle strategies. Shaded areas denote 95% confidence intervals. The meta-RL agent achieves significant gains, demonstrating that it gathers information (exploration) that is later used to gather rewards (exploitation). Effect of Memory Capacity Reducing the context window also decreases agent performance in gridworlds, though the critical threshold for collapse occurs at higher than in bandits due to the increased episode length  (Table 4)  . 1024 256 128 64 Meta-RL 0.670 0.074 0.120 0.056 0.047 0.026 -0.001 0. Table 4: Cumulative reward per task block as function of memory window in gridworld. Effect of Temporal Credit Assignment Our results show that while meta-RL agents can exhibit exploratory behavior in bandit tasks even with γepisode = 0, in gridworld environments, increasing the discount factor from zero to nonzero value leads to moderate improvement in overall performance (which is indicative of emergent exploration), with the average reward rising from 0.408 0.089 to 0.670 0.074. State Visitation Figure 5 shows the state visitation distribution across block progression aggregated over 10 trials in the same environment with different randomly seeded contexts (percentages represent the relative number of time steps spent in particular state). Early on, agents visit broad range of states. As training progresses, visitation becomes concentrated along paths leading to the nearest goal, and agents appear to avoid hazardous states such as holes. Notably, exploration seems to expand outward from the starting position, gradually covering more distant regions."
        },
        {
            "title": "Early",
            "content": "Middle Late Figure 5: State visitation heatmaps in the same gridworld (top: episodes 13; middle: 47; bottom: 830; over ten trials). Early episodes show wide exploration; later episodes focus on efficient exploitation of discovered paths to goals. This progression illustrates emergent exploration followed by exploitation. It is important to note that not all trials follow this typical pattern. In some instances, agents fail to find the goal entirely, while in others, they discover the goal early by chance and subsequently cease exploring further. The most significant failure cases occur when agents waste time executing sequences of actions that result in running into walls (evidenced by high state occupation). Nevertheless, the overall trends observed are consistent with incremental progress toward effective exploration."
        },
        {
            "title": "6.2 Long-Term Credit Assignment\nPerhaps most surprisingly, our experiments with multi-\narmed bandits reveal that long-term credit assignment is not\nessential for exploration to emerge, in seeming contradic-\ntion to our initial hypothesis. In bandit domains, even when\nthe temporal horizon is removed (γepisode = 0), the agent\ncontinues to exhibit behavior similar to state-of-the-art base-\nlines. This suggests that, under some circumstances, greedy\npolicies with memory can approximate efficient exploration\nstrategies without explicitly propagating delayed reward.",
            "content": "To understand this, recall that with γepisode = 0, the action-value function in DQN approximates the immediate reward received for chosen action. If the value function could generate sample from the distribution of rewards likely to be received, then the DQN algorithm would be doing Thompson Sampling (sample the distribution, take the most promising action, update the belief). However, standard neural networks generally approximate the mean reward, not the full distribution, seemingly making this equivalence impossible. However, recent studies suggest that in-context learning with transformers can produce outputs that effectively sample from pseudo-distribution, even exhibiting pseudo-random behavior conditioned on context (Hataya and Imaizumi 2024). We therefore propose that the mechanism by which exploration emerges in our setting when γepisode = 0 is not due to long-term credit assignment, but rather is an artifact of the transformers ability to generate context-dependent samples from an approximate reward distribution. When combined with the DQN update rule, this can closely approximate Thompson Sampling, even without explicit Bayesian modeling. We term this phenomenon pseudo-Thompson Sampling (pseudo-TS). There is other evidence that leads us to believe in the possibility of this mechanism. Bayesian dynamic programming (Strens 2000), model-based algorithm, demonstrated an ability to explore the environment without any auxiliary reward by sampling model of the environment from distribution and learning the optimal policy for that model with dynamic programming. This is, in effect, Thompson Sampling for model-based RL (Tziortziotis, Dimitrakakis, and Blekas 2013). Osband et al. (2019) showed in deep RL that one can induce efficient exploration without any added reward bonus by using randomized value functions, which is conceptually what we are claiming is happening, although the source of randomization is different. Moreover, this pseudo-Thompson Sampling (pseudo-TS) effect is not necessarily limited to stateless environments. If the action-value function can accurately represent the distributional value of actions far into the future, then Thompsonstyle sampling from such value function could, in principle, induce exploratory actions even in temporally extended tasks. However, our gridworld experiments show that non-zero discount factor is indeed beneficial for emergent exploration and improved cumulative reward. This suggests that the effectiveness of pseudo-TS is closely tied to how well the value function captures the distribution of future rewards. As the temporal reasoning horizon grows and estimating the value function becomes more challenging, pseudo-TS becomes less accurate and cannot fully substitute for learning signal that directly motivates information-seeking actions with long-term payoff. Thus, in multi-step environments, an appropriately chosen discount factor remains important for supporting effective exploration."
        },
        {
            "title": "6.4 Limitations\nNonetheless, there are important limitations. Our study is fo-\ncused on environments with explicit recurring structure and\nassumes sufficient agent memory and context length. While\nwe claim these conditions are necessary, they may not be\nsufficient; exploration may fail to arise for a variety of other\nreasons. Additionally, the mechanism underlying pseudo-\nThompson Sampling is empirical and not theoretically es-",
            "content": "tablished, and our results may not generalize to more complex or high-dimensional tasks. Further research is needed to assess the robustness and scalability of emergent exploration in broader domains. that initially hypothesized"
        },
        {
            "title": "7 Conclusion\nWe have shown that exploration can emerge naturally from\npure exploitation objectives, provided two core conditions\nare met: (1) the environment contains recurring, exploitable\nstructure, and (2) the agent has sufficient memory to re-\ntain and utilize past experience. Contrary to conventional\nwisdom, our experiments suggest that explicit exploration\nbonuses or intrinsic rewards are not always required; instead,\ninformation-seeking behavior can arise naturally without ex-\nternal incentives.\nWhile we",
            "content": "third conditionlong-term temporal credit assignmentwould be necessary for emergent exploration, our ablation studies reveal more nuanced picture. In some cases, particularly the when the value function can effectively represent distribution over possible returns, exploration can arise even without explicit long-horizon credit assignment. This phenomenon is most plausibly explained by pseudoThompson Sampling effect, enabled by transformer-based architectures that can condition on diverse contexts and approximate distributional value functions. In essence, if generative model can produce samples from the value functions distribution, exploration can emerge for free. However, when the models distributional estimates are insufficientsuch as in more complex, temporally extended tasksa nonzero discount factor and longer temporal horizon become essential for supporting effective exploration. These findings suggest potential paradigm shift: rather than treating exploration and exploitation as fundamentally orthogonal, we can view exploration as an emergent property of reward-maximizing behavior in structured, memory-rich contexts. This emphasizes the need for metareinforcement learning architectures that support persistent memory in order to leverage environmental regularities. We hope these results encourage continued investigation into how context, memory, long horizons, and distributional modeling contribute to emergent exploration, ultimately bringing us closer to agents that explore simply because it is the best way to maximize reward. References Auer, P.; Cesa-Bianchi, N.; and Fischer, P. 2002. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3): 235256. Bellemare, M. G.; Srinivasan, S.; Ostrovski, G.; Schaul, T.; Saxton, D.; and Munos, R. 2016. Unifying count-based exIn Advances in Neural ploration and intrinsic motivation. Information Processing Systems 29 (NeurIPS), 14711479. Botvinick, M.; Wang, J. X.; Kwisthout, J.; and et al. 2018. Prefrontal cortex as meta-reinforcement learning system. Nature Neuroscience, 21(6): 860868. Strens, M. 2000. Bayesian framework for reinforcement learning. In International Conference on Machine Learning (ICML), 943950. Sutton, R. 2019. The bitter lesson. Incomplete Ideas (blog), 13(1): 38. Sutton, R. S.; and Barto, A. G. 2018. Reinforcement Learning: An Introduction. MIT Press, 2nd edition. Tang, H.; Houthooft, R.; Foote, D.; Stooke, A.; Chen, X.; Duan, Y.; Schulman, J.; De Turck, F.; and Abbeel, P. 2017. #Exploration: Study of Count-Based Exploration for Deep Reinforcement Learning. In Advances in Neural Information Processing Systems 30 (NeurIPS), 27502759. Thompson, W. R. 1933. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4): 285294. Thrun, S. 1992. Efficient exploration in reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 433440. Tziortziotis, N.; Dimitrakakis, C.; and Blekas, K. 2013. LinIn International ear Bayesian Reinforcement Learning. Joint Conference on Artificial Intelligence (IJCAI), 1721 1728. Wang, J. X.; Kurth-Nelson, Z.; Tirumala, D.; Soyer, H.; Leibo, J. Z.; Munos, R.; Blundell, C.; Kumaran, D.; and Botvinick, M. 2017. Learning to reinforcement learn. In Proceedings of the 39th Annual Conference of the Cognitive Science Society (CogSci). Yu, C.; Burgess, N.; Sahani, M.; and Gershman, S. J. 2023. Successor-Predecessor Intrinsic Exploration. In Advances in Neural Information Processing Systems (NeurIPS). Zintgraf, L.; Shiarlis, K.; Igl, M.; Schulze, S.; Gal, Y.; Hofmann, K.; and Whiteson, S. 2020. VariBAD: Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning. In International Conference on Learning Representations (ICLR). Transformers as In ICML 2024 Workshop on InBurda, Y.; Edwards, H.; Pathak, D.; Storkey, A.; Darrell, T.; and Efros, A. A. 2018. Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355. Burda, Y.; Edwards, H.; Pathak, D.; Storkey, A.; Darrell, T.; and Efros, A. A. 2019. Exploration by random network disIn International Conference on Learning Repretillation. sentations (ICLR). Chen, L.; Lu, K.; R. Kumar, A.; Zhou, H.; Bavarian, M.; et al. 2021. Decision Transformer: Reinforcement Learning via Sequence Modeling. In Advances in Neural Information Processing Systems (NeurIPS). Duan, Y.; Schulman, J.; Chen, X.; Bartlett, P. L.; Sutskever, I.; and Abbeel, P. 2017. RL2: Fast reinforcement learning via slow reinforcement learning. In International Conference on Learning Representations (ICLR). Hataya, R.; and Imaizumi, M. 2024. Stochastic Optimizers. Context Learning. Hausknecht, M.; and Stone, P. 2015. Deep Recurrent QLearning for Partially Observable MDPs. In AAAI Conference on Artificial Intelligence. Lattimore, T.; and Szepesvari, C. 2020. Bandit Algorithms. Cambridge University Press. Machado, M. C.; Bellemare, M. G.; and Bowling, M. 2020. Count-Based Exploration with the Successor Representation. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04): 51255133. Melo, L. C. 2022. Transformers are meta-reinforcement learners. In International Conference on Machine Learning (ICML), 1534015359. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al. 2015. Human-level control through deep reinforcement learning. Nature, 518(7540): 529533. Osband, I.; Van Roy, B.; Russo, D. J.; and Wen, Z. 2019. Deep exploration via randomized value functions. Journal of Machine Learning Research, 20(124): 162. Oudeyer, P.-Y.; and Kaplan, F. 2007. Intrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computation, 11(2): 265286. Parisotto, E.; Song, F.; Rae, J. W.; Pascanu, R.; Guez, A.; and et al. 2020. Stabilizing Transformers for Reinforcement Learning. In International Conference on Machine Learning (ICML). Pathak, D.; Agrawal, P.; Efros, A. A.; Darrell, T.; and Malik, J. 2019. Curiosity-driven exploration by self-supervised prediction. In International Conference on Learning Representations (ICLR). Rentschler, M.; and Roberts, J. 2025. RL + Transformer = General-Purpose Problem Solver. In Kamalloo, E.; Gontier, N.; Lu, X. H.; Dziri, N.; Murty, S.; and Lacoste, A., eds., Proceedings of the 1st Workshop for Research on Agent Language Models (REALM 2025), 401410. Vienna, Austria: Association for Computational Linguistics. ISBN 9798-89176-264-0."
        }
    ],
    "affiliations": [
        "Tennessee Technological University"
    ]
}