{
    "paper_title": "ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing",
    "authors": [
        "Lingen Li",
        "Guangzhi Wang",
        "Zhaoyang Zhang",
        "Yaowei Li",
        "Xiaoyu Li",
        "Qi Dou",
        "Jinwei Gu",
        "Tianfan Xue",
        "Ying Shan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production."
        },
        {
            "title": "Start",
            "content": "ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing Lingen Li1,2 Guangzhi Wang2 Zhaoyang Zhang2 Yaowei Li3,2 Xiaoyu Li2 Qi Dou1 Jinwei Gu1 Tianfan Xue1 Ying Shan 1The Chinese University of Hong Kong 2ARC Lab, Tencent PCG 3Peking University 5 2 0 2 4 1 ] . [ 1 1 8 8 0 1 . 8 0 5 2 : r Figure 1. Video samples generated by ToonComposer using sparse keyframe sketches, featuring scenes from cartoon movies (Big Fish & Begonia, used with permission). ToonComposer supports precise keyframe control and flexible inference with varying numbers of input keyframe sketches and output video lengths (33 frames for the first and third samples, and 69 frames for the second sample). The frames are evenly sampled for illustration in this figure. Each input and output frame is annotated with its corresponding temporal index in the bottom right corner. These movies were excluded from the training data."
        },
        {
            "title": "Abstract",
            "content": "cartoon and anime production involves Traditional keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, generative model that unifies inbetweening and colorization into single post-keyframing stage. ToonComposer employs sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses cartoon adaptation method with the spatial Project Lead. Corresponding authors. low-rank adapter to tailor modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as single sketch and colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering superior and more flexible solution for AI-assisted Project page: https : / / lg - cartoon production. li.github.io/project/tooncomposer. 1. Introduction Cartoons and anime are celebrated for their vibrant aesthetics and intricate narratives, standing as cornerstone of global entertainment. Traditional cartoon production involves keyframing, inbetweening, and colorization stages, each of which requires artists to craft numerous frames to ensure fluid motion and stylistic consistency [26]. While the keyframing stage is creative process that embodies human artistry, the subsequent inbetweening and colorization stages are highly labor-intensive and time-consuming. Specifically, the inbetweening and colorization stages, which require less creative input, demand the production of hundreds of drawings for mere seconds of animation, resulting in significant time and resource costs. Recent advances in generative models have facilitated the inbetweening and colorization stages, such as ToonCrafter [14, 32], AniDoc [19], or LVCD [12]. However, these methods face critical limitations: (1) inbetweening approaches [14, 32] struggle to interpolate large motions from sparse sketch inputs, often requiring multiple keyframes for smooth motion; (2) colorization methods [12, 19] demand detailed per-frame sketches, imposing significant artist workload. (3) Additionally, their sequential processing leads to error accumulation, where inaccuracies in interpolated sketches affect the colorization stage, resulting in artifacts and reduced quality [26]. These shortcomings highlight significant gap in achieving streamlined and efficient production pipeline that produces high-quality results. In fact, the inbetweening and colorization stages are highly interdependent. Both of them require searching for correspondences among keyframe sketches or color reference frames. Therefore, we introduce the post-keyframing stage, novel paradigm that follows the keyframe creation stage and merges inbetweening and colorization into single automated process. This unification enables the model to jointly utilize the elementary and style information in keyframe sketches and reference frames in single stage, avoiding the risk of cross-stage error accumulation. As illustrated in Figure 2, the post-keyframing stage requires only few keyframe sketches and colored reference frame to generate complete high-quality cartoon video. This approach significantly reduces manual effort, allowing artists to focus on creative keyframe design, while AI manages repetitive tasks. To achieve this, we adopt modern Diffusion Transformer (DiT)-based foundation models [27], which demonstrate superior video generation performance. Although it offers new possibilities for cartoon video production, it also presents two significant challenges: 1) Controllability: DiT foundation models are typically weakly conditioned on text prompts or initial frames, lacking the precision needed to incorporate sparse keyframe sketches for motion guidance Figure 2. Comparison between previous cartoon production workflow and ours. ToonComposer enables the post-keyframing stage, seamlessly integrating inbetweening and colorization into single automated process, streamlining cartoon production compared to previous traditional and existing AI-assisted workflows. at specific temporal position. 2) Adaptation: Since they are trained on natural video datasets, adaptation to the cartoon domain is necessary to produce high-quality cartoon videos. However, the previous adaptation method for cartoon [32] is limited to UNet-based models, which alters the models spatial behavior while preserving temporal prior by only tuning the decoupled spatial layers. In our context, the full attention mechanism in the DiT model jointly learns spatial and temporal behaviors, where the previous cartoon adaptation method [32] is no longer valid. To address the above challenges, we propose ToonComposer, generative post-keyframing model built on the state-of-the-art DiT video foundation model Wan 2.1 [27]. First, to solve the challenge of precise controllability with sparse sketches, we devise the sparse sketch injection mechanism, which enables accurate control in cartoon generation using sparse keyframe sketches. Second, to handle the challenge of domain adaptation in DiT models, we tailor the cartoon adaptation for ToonComposer. It adapts the foundation DiT model to the cartoon domain with novel spatial low-rank adapter (SLRA) strategy, which adapts the appearance to the cartoon domain while preserving its powerful temporal prior intact. In addition, we improve the flexibility of ToonComposer by introducing the region-wise control, which enables flexible motion generation without drawing sketches in indicated regions. These contributions ensure that ToonComposer generates stylistically coherent animations with minimal input, as shown in Figure 1, effectively realizing the post-keyframing stage within one model. To support the training of the proposed model, we curated large-scale dataset PKData, which contains highquality anime and cartoon video clips. Each clip is accompanied by keyframe sketches in multiple styles, providing In addition to evaluating solid foundation for training. our model on synthetic benchmark, we curated PKBench, new benchmark that contains 30 original cartoon scenes with human-drawn keyframe sketches and reference color frames. Extensive experiments on both benchmarks demonstrate that ToonComposer outperforms existing methods in visual quality, motion coherence, and production efficiency. Our contributions are summarized as follows: We introduced the post-keyframing stage, new cartoon production paradigm that integrates inbetweening and colorization into single AI-driven process, significantly reducing manual labor. We proposed ToonComposer, the first DiT-based cartoon generation model for post-keyframing, incorporating sparse sketch injection and region-wise control to generate high-quality cartoon videos from sparse inputs. We design cartoon adaptation mechanism using SLRA, novel low-rank adaptation strategy that effectively tailors the spatial behavior of the DiT model to the cartoon domain while preserving its temporal prior. We curate dataset of cartoon video clips with diverse sketches for training and develop high-quality benchmark with real human-drawn sketches, PKBench, for cartoon post-keyframing evaluation. 2. Related Work AI-assisted Cartoon Production AI has increasingly been applied to automate labor-intensive tasks in cartoon and anime production, such as inbetweening and colorization. For inbetweening, early methods like AnimeInterp [16] and AutoFI [23] focus on linear and simple motion interpolation. More recently, diffusion-based methods [14, 32] become capable of handling cases with more complex motion by harnessing the generative priors of pretrained model. For colorization, early GAN-based [13] and recent diffusion-based methods [12, 19, 40, 41] have automated the colorization of line art based on one or series of reference frames. However, while existing AI-assisted methods have advanced cartoon production by automating inbetweening and colorization, they often require dense frame inputs, operate as separate, isolated stages, and face challenges with complex motions and stylistic consistency [26]. ToonComposer overcomes these hurdles by offering unified, sparse-input solution for post-keyframing that simplifies the production workflow. Video Diffusion Model Diffusion models have emerged as the cornerstone for generative tasks [6], particularly in image and video synthesis, by iteratively denoising samples from noise distribution to produce high-quality outputs [2]. For video generation, these models must effectively capture both spatial details and temporal dynamics, challenge that has led to distinct architectural approaches. Traditional UNet-based diffusion models [1, 2, 7, 34] extend 2D U-Nets to handle videos by incorporating 3D convolutions and separated spatial and temporal attention In these models, spatial attention layers process layers. intra-frame features, often across channels or spatial positions, while separate temporal attention layers model interIn contrast, Diffusion Transformers frame dependencies. (DiTs) [20] leverage transformer architectures, replacing UNets convolutional backbone with full attention mechanisms that model long-range dependencies in both spatial and temporal dimensions [15, 27, 35]. Although showing stronger performance compared to spatial-temporal decoupled design, such full attention mechanism eliminates the availability of spatial adaptation tailored for domains such as cartoon [32]. Our work builds upon the DiT-based foundation model to harness the high-quality video prior with new cartoon adaptation mechanism, which adapts the DiTbased foundation model to the cartoon domain in spatial behavior while keeping its temporal motion prior intact. Controllable Generation Controllable generation seeks to steer image and video synthesis with explicit conditions such as reference images [36, 37], depth maps [33], human poses [9, 39], and semantic labels. Techniques such as IP-Adapter [36] and ControlNet [37] inject these visual cues into diffusion models alongside text prompts, allowing fine-grained manipulation of both content and style. The value of controllability of generation is particularly evident in domain-specific pipelines. For cinematography, cameraaware generators expose handles for 2D scene layout and 3D camera trajectories [5, 17, 28, 30], allowing video creIn ators to frame shots and motion with high precision. cartoon production, sketch-guided models support interpolation, inbetweening, and colorization [12, 19, 32]. Our method focuses on the controllable cartoon generation using sparse keyframe sketches to accelerate the production process. 3. Methodology We introduce ToonComposer, novel generative postkeyframing model that produces high-quality cartoon videos with sparse control. To achieve this, we propose curated sparse sketch injection strategy, which effectively enables precise sketch control at arbitrary timestamps (Sec. 3.2). Furthermore, to fully leverage the temporal prior in video generation models, we design novel low-rank adaptation strategy that efficiently adapts the spatial prior to the cartoon domain while leaving the temporal prior intact (Sec. 3.3). To further alleviate artist workload and improve efficiency, our method also enables region-wise control, empowering artists to draw only part of the sketches while leaving the model to reason how the motion should be generated in blank areas (Sec. 3.4). 3.1. Post-Keyframing Stage In recent years, the cartoon industry has benefited significantly from the development of generative AI, facilitating the stage of inbetweening [32] and colorization [12, 19]. Although they are helpful in cartoon video production, existFigure 3. The model design of ToonComposer. sparse sketch injection mechanism enables precise control using keyframe sketches, and cartoon adaptation method incorporating spatial low-rank adapter tailors the DiT-based video foundation model to the cartoon domain, preserving its temporal priors. ing methods are often bottlenecked by high labor demand or low video quality. For example, colorization methods often require one colored reference frame and per-frame sketches, which is expensive to obtain. Although recent inbetwenning methods [32] can be utilized to generate per-frame sketches, it still faces challenges with large motions, leading to error accumulation in the colorization stage. In fact, the two stages are highly interdependent: both require searching and interpolating along the correspondence between elements in the keyframes/sketches, indicating that their internal mechanisms are similar. Inspired by this, we propose the post-keyframing stage, new stage that automates cartoon production and consolidates the inbetweening and colorization into unified generative process. Given one colored reference frame and one sketch frame, the postkeyframing stage aims to directly produce high-quality cartoon video that adheres to the guidance provided by these inputs. This process significantly alleviates the requirement of dense per-frame sketches, avoiding the risk of cross-stage error accumulation. Formally, given colored reference frame f1 and sketch frame sj, we aim to obtain model Gθ that directly generates high-quality cartoon video with frames: { ˆfk}K k=1 = Gθ(f1, sj, etext) (1) where represents the temporal location of sj, and etext represents the text prompt describing the scene. In this work, we adopt the recently proposed powerful video generation model Wan [27] as the basis. 3.2. Sparse Sketch Injection Advanced video generation models, such as Wan [27], demonstrate exceptional performance in producing highquality videos. While its image-to-video (I2V) variant supports video generation guided by an initial frame, precise control using sparse sketches at arbitrary temporal positions remains unexplored. To this end, we introduce novel sparse sketch injectionmechanism that seamlessly integrates sketches into the latent token sequence of an I2V DiT model for precise temporal control. In standard I2V DiT model ϵθ, the input image is concatenated with the noisy latent along the channel dimension: (cid:16) ˆϵ = ϵθ [{z(t) }K k=1, pad(f 1)]c, etext, (cid:17) , (2) where ˆϵ denotes model output, z(t) is the noisy latent tok kens of the k-th frame at timestep t, pad() represents the zero padding along the temporal dimension, [, ]c represents concatenation along the channel dimension, and 1 represents the latent of f1 encoded by the VAE. Our sparse sketch injection mechanism enhances the model ϵθ to support precise control using sparse sketches and keyframes through position encoding mapping and position-aware residual mechanism. Position Encoding Mapping To inject the sketch frame sj into the latent representation of the DiT model ϵθ for precise control over the temporal location in the generated cartoon, we first introduce an additional projection head that embeds the conditional sketch latents into sketch tokens that are compatible with the latent dimension of the model. Then, we apply the position embedding mapping process that borrows the RoPE [25] encodings from the corresponding video tokens at the temporal index before each attention operator. These sparse sketch tokens are concatenated with the video tokens along the sequence dimension to facilitate the attention process. This mechanism enables efficient integration of sketch conditions into the latent space with temporal awareness In addition, it facilitates during the generation process. the simultaneous use of multiple keyframes and sketches as control input. Given the complexity of motion in some cartoon scenes, precise control often necessitates multiple keyframes and sketches. Therefore, we extend the formulation to support both multiple colored reference frames and multiple sketch inputs. Consequently, the forward step of generation models [15, 27, 35], where spatial and temporal representations are intertwined in the latent space. As result, it is not feasible to directly perform spatial adaptation as is done in previous work. To address this, we introduce the Spatial Low-rank Adapter (SLRA), novel low-rank adaptation mechanism designed to effectively adapt video generation models to the cartoon domain. Similar to traditional LoRA [8], our SLRA also contains two trainable matrices Wdown and Wup. Different from traditional LoRA, our SLRA alters attention modules spatial behavior only, so as to preserve the temporal prior in the base model. Let and be the spatial sizes of DiTs latent tokens, and and be the temporal length of the video tokens and sketch tokens. Given token sequence RLD inside each self-attention module of ϵθ, where = (K + ) is the full token sequence length, SLRA operates first by downsampling the feature dimension of the input hidden token with linear layer: hlow = hWdown, (5) hlow Then, SLRA reshapes where Wdown RDD D, yielding hlow RLD . R[K+N ][HW ]D recovering their original spatialtemporal arrangements. After reshaping, we perform self-attention operation on spatial dimension only. This is achieved by performing the attention mechanism on the spatial dimension of each frame independently: hlow to , = hlow WQ, = hlow WK, = hlow WV = softmax (cid:18) QK (cid:19) V, (6) (7) where the subscript represents the index along the l-th temporal dimension. Thus, the attention computation is performed within each frame. WQ, WK, WV RDD are trainable matrices and RHW . The same positional embeddings as the main model will be applied to both video and sketch tokens during this attention operation. As result, the information propagation in the SLRA module is performed only in the spatial dimension, while leaving the temporal dimension intact. Following that, we reshape to ˆhlow RLD quence, then upsample it to the original dimension: as se- (8) hres = ˆhlowWup, where Wup RDD. Finally, the adapted self-attention output will be modified as: = SelfAttention(h) + hres. (9) The operation process of SLRA is illustrated in Figure 4. Figure 4. The structure of the Spatial Low-Rank Adapter (SLRA) used for cartoon adaptation in ToonComposer. The SLRA takes the hidden states before the spatial-temporal self-attention module as input and outputs residual that is added after the self-attention operation. the DiT model is expressed as: ˆϵ = ϵθ (cid:16)(cid:104) [{z(t) }K k=1, pad({f ic c=1)]c, {s }C in }N n=1 (cid:105) (cid:17) , etext, , }N n=1 represents sketch frames and {f ic (3) where {s }C c=1) in denotes colored reference frames. [, ]s means concatenation along the token sequence dimension. This formulation enables precise control over multiple inputs, while also supporting the minimal input requirement of the postkeyframing stage (one colored and one sketch frame, described in Section 3.1). Position-aware Residual To enhance the flexibility of sketch control, our ToonComposer also allows users to dynamically adjust the control strength of input sketches, through an adjustable weight during inference. This is done through new positional-aware residual module in the injection process. For sparse sketch tokens at controlled keyframe indices {in}N n=1, we transform these tokens via linear layer Wres and add them to the corresponding video tokens with the same indices with scaling weight α: n=1 {z(t) := {z(t) + α{s in }k{in}N }k{in}N }N n=1Wres, (4) where Wres RDD is trainable weight, is the feature dimension of tokens. During training, the scaling weight α is set to 1. During inference, users can adjust the weight α to loosen or strengthen the control of keyframe sketches. n=1 3.3. Cartoon Adaptation Previous work [32] has demonstrated the success of adapting the video generation model to the cartoon domain. By tuning only the spatial layers of the spatial-temporal U-Net, the temporal motion prior in the original model is preserved, while the appearance part is adapted to the cartoon. However, with the development of video generation models, 3Dfull attention has been extensively used in modern video SLRA ensures that cartoon-specific spatial features are learned without disrupting temporal coherence, efficiently adapting DiT-based video diffusion model to the cartoon domain. 3.4. Region-wise Control Sometimes cartoon creators may only want to draw the foreground sketch and let the generator create the background for them. If they simply leave the background blank, this may result in undesirable artifacts, as shown in the second row of Figure 9. To this end, we propose novel region-wise control mechanism that allows artists to specify blank regions in sketches for the model to generate plausible content based on context or text prompts. During training, random masks min {0, 1}HW are applied to the sketch frames sin, where min (i, j) = 0 indicates region where the sketch is not provided. An additional channel is concatenated to sin, which is encoded as: in = [E(sin ), min ]c , (10) in is used to replace the where in described in Equation (3) during training. The model learns to reconstruct full frames in masked regions, enabling flexible inference where artists can assign the value of min and leave masked areas blank for context-driven generation. Complementary to the support of temporally sparse keyframes and sketches, our region-wise control allows the input sketch to be spatially sparse, further alleviating the requirements and labors for cartoon creators. 3.5. Training Objective ToonComposer is trained as conditional diffusion model following Rectified Flow [4], which predicts the velocity vt at timestep sampled from logit-normal distribution. For simplicity, we write the input part in Equation (3) as xin: xin = (cid:104) [{z(t) }K k=1, pad({f ic c=1)]c, {s }C in }N n=1 (cid:105) , (11) and let z0 = {z(0) k=1 be clean cartoon video latent, the training objective minimizes the expected velocity prediction error: }K = Ez0,η,t (cid:104) vt ϵθ (xin, etext, t)2 2 (cid:105) , (12) where η is the random Gaussian noise, vt is the velocity derived from {z(t) k=1 η, and ϵθ is the ToonComposer model to be trained. }K 4. Experiments 4.1. Experimental Settings Dataset Based on our internal video sources, we constructed the PKData, high-quality cartoon dataset containFigure 5. Examples of different sketch types used during training and evaluation. All variants except human-drawn sketches are included in the training set. The diversity of training sketches improves ToonComposers robustness to varying sketch styles in real-world use cases. Human-drawn sketches are reserved for evaluation in the real benchmark, as discussed in Section 4.3. ing 37K diverse cartoon video clips. Each clip was accompanied by descriptive caption generated by CogVLM [29] and set of sketch frames. Recognizing the stylistic diversity in sketches due to different artist preferences or creation tools, we augmented our dataset with diverse types of sketches. Specifically, we synthesize four versions of sketches per frame using four open-source CNN-based sketch models, including two basic lineart models used in ControlNet [37], Anime2Sketch [31], and Anyline [24]. Furthermore, we tune FLUX-based image-to-image generative model with in-context LoRA [10] on small realsketch dataset from multiple artists. This model, named IC-Sketcher, is then used to produce another version of sketches. Figure 5 illustrates one example frame with diverse sketches. Benchmark We first evaluate our methods on synthetic benchmark obtained from cartoon movies (use with permission, for evaluation only), where sketches for each video frame are produced using sketch models. We adopt reference-based metrics on this benchmark since the ground truth is available. Furthermore, we developed PKBench, novel benchmark featuring human-drawn sketches to enable more comprehensive evaluation of cartoon postkeyframing in real-world scenarios. PKBench contains 30 samples, each including 1) colored reference frame, 2) textual prompt that describes the scene, and 3) two real sketches that depict the start and end frames of scene, drawn by professional artists. Metrics For evaluation metrics, we adopt 1) referencebased perceptual metrics for synthetic benchmark, including LPIPS [38], DISTS [3], and CLIP [21] image similarity, 2) reference-free video quality metrics from VBench [11] for both synthetic and real benchmarks, including subject consistency (S. C.), motion consistency (M. C.), backTable 1. Quantitative evaluation results on the synthetic benchmark, comparing ToonComposer with previous AI-assisted cartoon generation methods: AniDoc [19], LVCD [12], and ToonCrafter [32]. Method LPIPS DISTS CLIP Subject Con. Motion Smo. Background Con. Aesthetic Qua. AniDoc [19] LVCD [12] ToonCrafter [32] ToonComposer (Ours) 0.3734 0.3910 0.3830 0.1785 0.5461 0.5505 0.5571 0.0926 0.8665 0.8428 0.8463 0.9449 0.9067 0.8316 0.8075 0.9451 0.9798 0.9810 0.9550 0.9886 0.9408 0.9183 0.8920 0. 0.4962 0.4984 0.5035 0.5999 Table 2. Quantitative evaluation results on the real sketch benchmark PKBench, comparing ToonComposer with prior AI-assisted cartoon generation models: AniDoc [19], LVCD [12], and ToonCrafter [32]. Table 3. User preference rates for aesthetic quality and motion quality of cartoons generated by ToonCrafter [32], AniDoc [19], LVCD [12], and ToonComposer. Method S. C. M. S. B. C. A. Q. AniDoc [19] LVCD [12] ToonCrafter [32] ToonComposer (Ours) 0.9456 0.8653 0.8567 0.9509 0.9842 0.9724 0.9674 0.9910 0.9664 0.9394 0.9343 0.9681 0.6611 0.6479 0.6822 0.7345 Method Aesthetic Q. Motion Q. AniDoc [19] LVCD [12] ToonCrafter [32] ToonComposer (Ours) 4.45% 7.54% 17.02% 70.99% 5.34% 7.91% 18.19% 68.58% ground consistency (B. C.) and aesthetic quality (A. Q.). 3) user study on human perceptual quality for the real benchmark. Training Details We train our model for 10 epochs using our dataset, with an equivalent batch size of 16, the AdamW [18] optimizer, and learning rate of 105. The rank of SLRA Dlow is set to 144. We use the zero redundancy optimizer [22] stage 2 to reduce memory cost during training. 4.2. Evaluation on Synthetic Benchmark We first evaluate our ToonComposer on synthetic cartoon benchmark and compare it with previous methods, including AniDoc [19], LVCD [12], and ToonCrafter [32]. In this synthetic evaluation, sketches are obtained from cartoon video frames using the same sketch model [31]. To ensure evaluation fairness, we align the ground truths in both spatial and temporal dimension to fit the pre-defined settings of each model for metrics calculation. Baseline Methods Although our model requires only one inference to get the final cartoon video, previous methods demand two-stage process, as shown in Figure 2. For ToonCrafter [32], we first generate the dense sketch sequence by interpolating the first and last sketch frames, then we use its sketch guidance mode (which requires the first and last color frames as input) to generate the final cartoon video. For LVCD [12] and AniDoc [19], we first generate the dense sketch sequence interpolated by ToonCrafter, then colorize the sketches into final cartoon video using the two models, respectively. Results Table 1 shows the numeric results of the synthetic evaluation. Our method outperforms previous methods in both reference-based metrics and reference-free metrics. For example, our model reports significantly lower DISTS score, indicating that its perceptual quality is much better than that of its counterparts. Figure 6 visualized the qualitative comparison between these methods, with the ground truth video provided as references. In both samples, our method produces smooth and natural cartoon video frames, while other methods fail to handle such challenging cases with sparse sketches. For example, in the zoom-in patches of the first sample, AniDoc and ToonCrafter produce distorted faces. LVCD generates reasonable face but loses all details in subsequent frames. In contrast, our method produces clear face which preserves the identity of the first reference frame. These observations align with our methods numeric performance advantages in Table 1. More results are provided in the supplementary video. 4.3. Evaluation on Real Benchmark In addition to the evaluation on the synthetic test set, we further compared all methods on our proposed benchmark PKBench with real human sketches. Since ground truth is not available for each sample, we evaluated the generated videos using reference-free metrics from VBench [11]. The quantitative comparison is shown in Table 2, where our model outperforms previous methods in all metrics, achieving superior appearance and motion quality. Figure 7 visualizes the comparison among all methods, Figure 6. Comparison on the synthetic benchmark among AniDoc, LVCD, ToonCrafter, and our ToonComposer. Zoom-in patches of randomly selected region are shown in the rightmost column. Our method demonstrates superior visual quality, smoother motion, and better style consistency with the input image. Evaluation scenes are sourced from movies with permission (Mr. Miao and Big Fish & Begonia). Please refer to the supplementary video for additional results. Table 4. Ablation study on the Spatial Low-Rank Adapter (SLRA) for cartoon adaptation by modifying its internal attention module: temporal-only (Temp. Adapt.), spatial-temporal (S. T. Adapt.), and removal of the attention module (Linear Adapt.). We also compare against baseline model adapted using LoRA [8]. Adaptation Method LPIPS DISTS CLIP Temp. Adapt. S. T. Adapt. Linear Adapt. LoRA SLRA (Ours) 0.1956 0.1977 0.2030 0.1922 0. 0.1109 0.1068 0.1091 0.1082 0.0955 0.9581 0.9587 0.9589 0.9628 0.9634 with zoom-in patches from randomly selected region provided in the rightmost column. It is observed that previous methods deviate from the overall style of the first reference frame. Specifically, ToonCrafter generates intermediate frames with prominent bold lines, likely influenced by the bold brush strokes in the human-drawn sketches, revealing its limited robustness to diverse sketch styles. In contrast, our ToonComposer produces video frames with superior visual quality, motion coherence, and style consistency, consistent with the quantitative results. 4.4. Human Evaluation To further investigate users preferences on the generation results, we conducted human evaluations to compare the Figure 7. Comparison on the benchmark PKBench, using keyframe sketches drawn by the human artists. Zoom-in patches are shown in the rightmost column. Our method generates high-quality results from real sketch inputs, whereas other methods struggle to maintain visual consistency. Please refer to the supplementary video for additional examples. Figure 8. Ablation study of the Spatial Low-Rank Adapter (SLRA) in ToonComposer, comparing cartoon output frames produced using different adaptation methods. SLRA yields higher visual quality and better coherence with the input keyframe sketches compared to alternative approaches. generation results produced by our method and other baselines. We randomly select 30 samples from the benchmarks and generate cartoon videos for each method using the aforementioned pipeline. Our evaluation process involved 47 participants, each of whom was asked to select the video with the best aesthetic quality and motion quality. The results are shown in Table 3, where our method achieves the highest win rate on both metrics, significantly exceeding the second best competitor. 4.5. Discussion and Analysis Ablation on the SLRA To assess the importance of spatial adaptation in ToonComposer, we conducted an ablaFigure 9. Illustration of region-wise control in ToonComposer. Without region-wise control, blank areas in keyframe sketches are misinterpreted as textureless regions, producing flat blue train (second row, highlighted with dashed box). With region-wise control, users can specify areas for context-driven generation without explicit sketches, enabling the model to create plausible and detailed content, such as the dynamic train motion (third row, highlighted with dashed box). Figure 10. ToonComposers flexible controllability with varying keyframe sketches. Using only sketch #1 as the final keyframe and the prompt an old man turns back, ToonComposer generates sequence where the old man turns directly (first row). Adding sketch #2 to control the middle keyframe, while keeping the prompt unchanged, results in sequence where the old man first picks up fruit before turning back (second row). tion study on the SLRA, with results detailed in Figure 4. We modify SLRAs internal attention mechanism to explore alternative adaptation behaviors: a) temporal adaptation (Temp. Adapt.), which focuses on temporal dynamics; b) spatial-temporal adaptation (S.T. Adapt.), which jointly adjusts both; c) degraded linear adapter (Linear Adapt.), which removes the attention block entirely. d) baseline using LoRA [8], which modifies all linear layers (query, key, value, and output) in DiTs attention modules. This design implicitly alters both spatial and temporal behaviors. To ensure fairness, LoRAs rank was set to 24 to match the trainable parameter count of SLRA. All models were trained with identical settings. LPIPS, DISTS, and CLIP image similarity are used for evaluation. The results are presented in Table 4 and Figure 8, where SLRA outperforms all variants in both numeric results and visual quality. Specifically, a) Temp. Adapt. and b) S.T. Adapt. yield higher errors due to insufficient or conflicting spatial adjustments, while c) Linear Adapt. lacks the nuanced adaptation required for cartoon aesthetics. Despite the broader scope of d) LoRA, it underperforms SLRA due to its less targeted adaptation, which disrupts the temporal priors critical for smooth transition. These findings underscore SLRAs effectiveness in adapting DiTs spatial behavior for cartoon-specific features, while preserving the temporal prior intact. Use Case of Region-wise Control We visualize how region-wise control affects the generated video. Without region-wise control, leaving blank area in the keyframe sketch causes the model to interpret it as textureless region, resulting in flat areas in the generated frames, as illustrated in the second row of Figure 9. In contrast, with the region-wise control enabled, users can simply draw an area with brush tools to indicate areas that require generating proper motion according to the context. As shown in the last row of Figure 9, our model is able to infer from the input keyframe, the sketch, and the mask given, and automatically generate plausible movement of the train in the masked area. This mechanism significantly improves flexibility, further alleviating manual workload in real scenarios. Controllability with Increasing Keyframe Sketches The sparse sketch injection mechanism of ToonComposer enables flexible control by supporting variable number of input keyframe sketches, increasing its utility in the cartoon production pipeline. This adaptability allows artists to balance creative control and automation based on the complexity of the desired motion. As shown in Figure 10, we demonstrate the ability of ToonComposer to generate distinct cartoon sequences from different numbers of input sketches, all conditioned on the same text prompt. Additional examples are available in the supplementary video, which illustrates the versatility of our method in diverse scenarios. Generalization to 3D Animation Despite differences in production pipelines, ToonComposer extends its applicability to 3D-rendered animation by adapting the initial reference frame to 3D-rendered image. We fine-tuned the model on compact dataset of 3D animation clips, enabling it to generate high-quality 3D-style sequences in the postkeyframing manner. This adaptability highlights ToonComposers versatility and potential for broader animation applications. These 3D animation samples are provided in the supplementary video. 5. Conclusion In this paper, we present ToonComposer, novel model that streamlines cartoon production by automating tedious tasks of inbetweening and colorization through unified generative process named post-keyframing. Built on the DiT architecture, ToonComposer leverages sparse keyframe sketches and single colored reference frame to produce highquality, stylistically consistent cartoon video sequences. Experiments show that ToonComposer surpasses existing methods in visual fidelity, motion coherence, and production efficiency. Features such as sparse sketch injection and region-wise control empower artists with precision and flexibility, making ToonComposer versatile system for cartoon creation. Despite limitations such as computational costs, ToonComposer offers promising solution to streamline the cartoon production pipeline through generative models."
        },
        {
            "title": "Acknowledgments",
            "content": "express our to B&T Studio We sincere gratitude and Gudong Animation Studio for generous perin our work. mission to use their animation content"
        },
        {
            "title": "References",
            "content": "diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 3 [3] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture IEEE transactions on pattern analysis and masimilarity. chine intelligence, 44(5):25672581, 2020. 6 [4] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 6 [5] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 3 [6] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arXiv:2006.11239, 2020. [7] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 3 [8] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 5, 8, 10 [9] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 3 [10] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. 6 [11] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 6, 7 [12] Zhitong Huang, Mohan Zhang, and Jing Liao. Lvcd: reference-based lineart video colorization with diffusion models. ACM Transactions on Graphics (TOG), 43(6):111, 2024. 2, 3, [13] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Image-to-image translation with conditional adverEfros. sarial networks. arXiv preprint arXiv:1611.07004, 2017. 3 [1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video [14] Yudong Jiang, Baohan Xu, Siqian Yang, Mingyu Yin, Jing Liu, Chao Xu, Siqi Wang, Yidi Wu, Bingwen Zhu, Jixuan Xu, et al. Exploring the frontiers of animation video generation in the sora era: Method, dataset and benchmark. arXiv preprint arXiv:2412.10255, 2024. 2, 3 erative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 4, [15] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3, 5 [16] Xiaoyu Li, Bo Zhang, Jing Liao, and Pedro V. Sander. Deep sketch-guided cartoon video inbetweening. IEEE Transactions on Visualization and Computer Graphics, 28(8):2938 2952, 2021. 3 [17] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Yuexian Zou, and Ying Shan. Image conductor: Precision control for interactive video synthesis. arXiv preprint arXiv:2406.15339, 2024. 3 [18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 7 [19] Yihao Meng, Hao Ouyang, Hanlin Wang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Zhiheng Liu, Yujun Shen, and Huamin Qu. Anidoc: Animation creation made easier. arXiv preprint arXiv:2412.14173, 2024. 2, 3, [20] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2303.12345, 2023. 3 [21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 6 [22] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training In SC20: International Confertrillion parameter models. ence for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. 7 [23] Wang Shen, Cheng Ming, Wenbo Bao, Guangtao Zhai, Li Chenn, and Zhiyong Gao. Enhanced deep animation video interpolation. In 2022 IEEE International Conference on Image Processing (ICIP), pages 3135. IEEE, 2022. 3 [24] Xavier Soria, Yachuan Li, Mohammad Rouhani, and Angel Sappa. Tiny and efficient model for the edge detection generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13641373, 2023. 6 [25] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [26] Yunlong Tang, Junjia Guo, Pinxin Liu, Zhiyuan Wang, Hang Hua, Jia-Xing Zhong, Yunzhong Xiao, Chao Huang, Luchuan Song, Susan Liang, Yizhi Song, Liu He, Jing Bi, Mingqian Feng, Xinyang Li, Zeliang Zhang, and Chenliang arXiv Xu. Generative ai for cel-animation: survey. preprint arXiv:2503.00000, 2025. 2, 3 [27] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video gen- [28] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation. arXiv preprint arXiv:2502.08639, 2025. 3 [29] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems, 37:121475121499, 2024. 6 [30] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3 [31] Xiaoyu Xiang, Ding Liu, Xiao Yang, Yiheng Zhu, Xiaohui Shen, and Jan Allebach. Adversarial open domain adaptation for sketch-to-photo synthesis. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 14341444, 2022. 6, [32] Jinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, Xintao Wang, Ying Shan, and Tien-Tsin Wong. ToonarXiv preprint crafter: Generative cartoon interpolation. arXiv:2405.12345, 2024. 2, 3, 4, 5, 7 [33] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, et al. Make-your-video: Customized video generation using textual and structural guidIEEE Transactions on Visualization and Computer ance. Graphics, 2024. 3 [34] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating In Euopen-domain images with video diffusion priors. ropean Conference on Computer Vision, pages 399417. Springer, 2024. 3 [35] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3, 5 [36] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3 [37] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023. 3, [38] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 [39] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, pages 145162. Springer, 2024. 3 [40] Junhao Zhuang, Xuan Ju, Zhaoyang Zhang, Yong Liu, Shiyi Zhang, Chun Yuan, and Ying Shan. Colorflow: RetrievalarXiv preprint augmented image sequence colorization. arXiv:2412.11815, 2024. 3 [41] Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, and Ying Shan. Cobra: Efficient line art colorization with broader references. arXiv preprint arXiv:2504.12240, 2025."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "Peking University",
        "The Chinese University of Hong Kong"
    ]
}