{
    "paper_title": "When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs",
    "authors": [
        "Ammar Khairi",
        "Daniel D'souza",
        "Ye Shen",
        "Julia Kreutzer",
        "Sara Hooker"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting. Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages."
        },
        {
            "title": "Start",
            "content": "Ammar Khairi1, Daniel Dsouza1, Ye Shen2, Julia Kreutzer1, and Sara Hooker1 1Cohere Labs, 2Cohere Corresponding authors: {ammar, juliakreutzer, sarahooker}@cohere.com Abstract Recent advancements in large language models (LLMs) have shifted focus toward scaling inferencetime computeimproving performance without retraining the model. common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in multilingual, multi-task setting. Our findings show that both sampling strategybased on temperature variationand selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, substantial increase at minimal cost. Our results underscore the need for languageand task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages. 5 2 0 2 J 5 2 ] . [ 1 4 4 5 0 2 . 6 0 5 2 : r a"
        },
        {
            "title": "1 Introduction",
            "content": "Traditionally, if you wanted higher performance from machine learning model, you paid for it with more training or data or parameters. key departure from this is the recent emphasis on scaling up First author. Principal senior advisors. Released as preprint on June 26, 2025 1 compute at inference time rather than at training time [Wu et al., 2024a; Hooker, 2024; Snell et al., 2025]. The combination of growing generative capabilities of large language models (LLMs) paired with better sampling techniques has spurred progress in inference-time compute strategies. These strategies allow for improvements in performance by spending more compute without any alterations to the model itself. However, much remains unknown about how to best search for optimal solutions using inference compute alone, especially for open-ended generative tasks [Zhang et al., 2025b]. Even less established is how to tailor inference compute strategies to languages beyond English, which are traditionally under-served by state-of-the-art systems [Üstün et al., 2024; Dang et al., 2024b; Dash et al., 2025], and underrepresented in LLM research. In our work, our goal is to understand how to most robustly scale inference compute across languages for generative tasks. For given model, how can we best invest fixed budget of inference-time compute to improve performance across all languages? We are most interested in techniques that generalize across open-ended tasks and formally verifiable tasks, and across languages. Hence, our setting is extremely multi-task with many different performance constraints to balance. We focus on parallel scaling [Wang et al., 2023; Welleck et al., 2024; Zhang et al., 2025b] which increases inference-time compute by first generating multiple generations in parallel and then selecting one of the sampled outputs as final output.1 You can think of parallel scaling as an endeavor to make the best lemonade from an already grown lemon tree (the trained model). This requires two steps, 1) sampling: strategically choosing which lemons and how many lemons to harvest, 2) selection: carefully evaluating each to isolate lemon within that harvest that produces the best lemonade. These two stages, and and how well they are aligned with the evaluation metric of interest, determine the power of inference scaling [Stroebl et al., 2024; Brown et al., 2024; Huang et al., 2025]. In this work, we revisit both stages from first principles and exhaustively evaluate existing techniques to see which work in massively diverse task environment. In contrast to previous work, our focus is on generalizable solutions across diverse set of tasks (both open-ended and more studied tasks like math and machine translation), models and languages, starting from competitive baselines. Our results suggest that existing sampling and selecting strategies are not well fit to settings beyond English. We propose novel techniques that depart from go-to solutions for English to account for diverse language and task setting. We are interested in competitive production settings so throughout our work, we measure gains on state-of-art multilingual models which are already extremely well performing and well polished, such as Aya Expanse and Qwen3 at the 8B scale, and Command-A with 111B parameters. We rigorously compare against standard Best-of-N scoring with the strongest reward model according to prior benchmarking, and ambitiously compare performance of the 8B models against Gemini 2.0, proprietary model. Therefore, wins in this competitive setup are hard to obtain. Our methods, thanks to being carefully crafted for the multilingual and multi-task demands with small factors of inference compute growth, achieve impressive gains across the bench, which means that the additional compute is extremely well spent. Our primary contributions are the following: 1. Extensive study of existing methods. While many prior approaches exist for this problem [Ippolito et al., 2019; Shi et al., 2024; Snell et al., 2025], they have studied subsets of tasks and languages. We empirically investigate sampling and selection inference strategies, including key methods like Minimum Bayes Risk (MBR) and Best-of-N (BoN) [Huang et al., 2025; Eikema & 1This idea is known under many other names, e.g. Best-of-N [Huang et al., 2025], repeated-sampling-thenvoting [Chen et al., 2025], or rejection sampling [Touvron et al., 2023]. 2 Aziz, 2022], under our massively multi-task constraints, spanning two multilingual LLMs, seven languages and three generative tasks (open-ended generation, math reasoning, machine translation). This broader view lets us pinpoint where existing methods fail and leads us to design methods tailored to generalization across tasks and languages. 2. Novel risk-reducing sampling. We introduce novel hedged sampling method to better exploit the sample diversity obtained through sampling under increased softmax temperature. It specifically benefits languages that come with higher risk of sample quality deterioration at high temperatures. Opposed to prior works that recommend either stochastic or deterministic sampling depending on the task [Song et al., 2025]we show that including both in the sample set is key to multilingual generalization. For example, hedged sampling for MBR yields gains of +8.2 percentage points on average (across languages and models) increase in win-rates over greedy outputs on mArenaHard-V2; outperforming traditional temperature sampling by +2.2 percentage points. 3. Improved selection strategies. This leads us to proposing two new selection strategies which capitalize on long-context modeling and the versatility of cross-lingual generation abilities of recent generalist LLMs. We call these Checklisted One-Pass Selection (CHOPS) and Cross-lingual MBR (X-MBR) and show thesecombined with hedge sampling obtain +17.3 (Aya Expanse 8B) and +9.4 (Qwen3 8B) percentage points average increase in multilingual win rates when evaluated against the single sample baseline. When evaluated in more realistic adverse settings, our methods increase the win-rates on average by +6.8 points against the much larger and stronger Gemini 2.0 Flash. Finally, we demonstrate the robustness of these techniques at scale as we improve the performance of Command-A by an average of +9.0 points against single sample decoding. Notably, using the best open source specialized reward model (RM) with BoN scoring only gets modest +4.5 increase from the same set of samples. We distill our findings into recipe for squeezing the most out of multiple samples in multilingual and multifaceted generation paradigm, which we coin the Multilingual LLMonade Recipe. In our test bed, the recipe for instance improves the Aya Expanse 8B model by up to +17.3 percentage points in multilingual win rates on open-ended generations, +7.9 points accuracy on MGSM, and +0.72 points in XCOMET on WMT24++.2 This demonstrates how much can be done with as little as 4 more samples without any compute going into training. Our findings have implications for the broader test-time scaling landscape, as they demonstrate that careful design of sampling and selection techniques can bring important gains even at the low-end scale of inference-time scaling for high-end multilingual LLMs. Contrary to the trend of exploiting specialized reward models for single-task inference-time scaling, we show that even in challenging and diverse multi-task setup, robust improvements can be found with generalist LLM judges across the bench, leveraging their versatility and on-the-fly adaptability. In the following, we will take apart the question on how to optimize multi-sample inference, by first investigating the sampling strategy (section 2), then comparing multiple selection strategies (section 3). Throughout the paper, we mark our newly introduced methods and their empirical effects with , and contrast English results () with non-English () results. 2Which is notable improvement with respect to the XComet-XL metric, see https://kocmitom.github.io/MT-T hresholds/ [Kocmi et al., 2024b]."
        },
        {
            "title": "2 How to Sample?",
            "content": "The first ingredient for successful test-time scaling is the creation of sample pool of sufficient quality: At least one sample in the pool needs to be of higher quality than what can be expected from single sample. Our research question is here: How to create sample pool with strategy that is robust across languages and tasks? Section Findings Even with as little as = 5 samples, parallel scaling can improve multilingual generation quality significantly, when done right. Addressing multiple languages and tasks at once with the same inference strategy requires careful design of the sampling algorithm to anticipate higher variance for non-English languages. Adding deterministic outputs, here from greedy decoding, to the sample set for parallel scaling effectively helps hedging risks of subpar sample quality for non-English languages. 2.1 Methodology Temperature Sampling Our first task is to create valuable pool of generations via stochastic sampling. We explore different variants of temperature sampling [Ackley et al., 1985] as it offers an intuitive way of steering the diversity and quality of the generation pool. Temperature sampling divides the logits lθ for each token prediction by fixed constant τ > 0 before the softmax normalization: pτ θ (yt x, y<t) = softmax(lθ/τ ). (1) Under high temperatures (τ > 1) the resulting probability distribution becomes more uniform, while at its extreme (close to 0), it becomes more unimodal. As consequence, higher temperatures generate more diverse pool of samples. The quality, however, varies depending on the task, language, and model. At 0, sampling becomes greedy decoding, picking the token with the maximum likelihood at each decoding step, which we refer to as τ = 0 for simplicity. Multi-Temperature Sampling Prior works investigated sampling the entire pool from the same temperature [Du et al., 2025; Renze, 2024; Song et al., 2025]. When we have large variety of tasks and languages, representing very different subspaces of the data distribution that the model is trained on, it is impossible to set the temperature optimally for all. To increase robustness, we thus investigate composing the sample set from outputs generated under multiple temperatures τ [0, 1]. The temperatures to sample from can either be chosen according to insights from development set, or randomly chosen from range if there is no prior intuition. As we will show below, temperature sensitivity across languages and tasks varies, so if temperatures are not tuned individually per task and languagefacing combinatorial explosion, especially with massively multilingual and multitask goalsthis is better than choosing just one temperature for all settings. Hedged Sampling As more controlled variation of multi-temperature sampling, we propose hedged sampling strategy which additionally includes deterministic outputs from greedy search (τ = 0) in this mix. This helps hedge risk because even if there are lower-quality samples due to variance caused by higher temperatures, we can default back to strong single-sample candidates for aligned models [Song et al., 2025]. Hedged sampling is complementary to token-level techniques that 4 Figure 1: Overview of the multilingual multi-task experimental scope. New methods are marked with ."
        },
        {
            "title": "Name",
            "content": "Data Splits (# Prompts per Language) dev devtest test"
        },
        {
            "title": "Metric",
            "content": "Arena MGSM WMT WMT24/15 dev (997/1.5k) NTREX (1997) m-ArenaHard (250/250) GSM8K-instruct-parallel (250/250) m-ArenaHard-v2.0 (498) Win rate Accuracy XComet-XL MGSM (258) WMT24++ (960) Table 1: Overview of the benchmarks used in this work for open-ended generation (Arena), mathematical reasoning (MGSM), and machine translation (WMT). We compile dev and devtest splits to prevent overfitting our sampling and selecting methods to the test set. For WMT dev, French prompts were retrieved from WMT15 dev, the remaining ones from WMT24 dev. For WMT24++ with originally 998 instances, we skip those marked as bad source. truncate the output space of individual stochastic samples, e.g. by pruning low-probability tokens with fixed threshold (ϵ-sampling) [Hewitt et al., 2022; Freitag et al., 2023], or dynamic threshold based on the models confidence (min-p sampling) [Minh et al., 2025]. Hedging risks is particularly important for multilingual applications, where the risk tends to be higher in less dominant languages, as we will show in the following experiments. 2.2 Experimental Setup Multilingual Multi-tasking: Models and Benchmarks Open-ended generation tasks have received less attention in test-time scaling works. It is harder to fit single method or reward model to the diverse challenges that open-ended generations pose. Our goal here is to take wider view, which means considering both open-ended tasks and tasks with underlying correctness. The experimental setup is summarized in fig. 1. We test Aya-Expanse-8B [Dang et al., 2024b] (short: Aya) and Qwen3-8B [Yang et al., 2025a] (short: Qwen) models on 7 languages (English, French, German, Japanese, Modern Chinese, Russian, Spanish), selected for their inclusion in the benchmarks for generative tasks that we focus on. We evaluate on the following benchmarks, detailed in table 1: Open-ended generation quality: Arena. We source data from m-ArenaHard [Dang et al., 2024b]3, an automatically translated version of the English Arena-Hard-Auto v0.1 [Li et al., 2024] prompts that are diverse and challenging open-ended prompts from LMArena,4 and m-ArenaHard v2.0, created by translating the English portion of Arena-Hard-Auto v2.05 with an in-house translation model. We measure win rate % using GPT-4o (gpt-4o-2024-05-13) as judge, which 3https://huggingface.co/datasets/CohereLabs/m-ArenaHard 4https://lmarena.ai/ 5https://huggingface.co/datasets/lmarena-ai/arena-hard-auto 5 Figure 2: Multilingual win-rates gains vs greedy output on the dev set of m-ArenaHard as we increase the sample size from 1 to 20. Performance improvements are steepest at low sample sizes (3-5) with more modest changes beyond that for both selection methods. Results shown for Aya-8B across French, Japanese, and Russian. is the standard choice for the m-ArenaHard benchmark. Win rates are computed in comparison to (1) greedy decoding for intrinsic comparison and (2) Gemini (gemini-2.0-flash) outputs for extrinsic comparison to stronger commercial model. Mathematical reasoning: MGSM. Data stems from collection of translated English gradeschool math problems GSM8K [Cobbe et al., 2021], either translated automatically for GSM8Kinstruct-parallel6, or manually for MGSM [Shi et al., 2022]. Final answers are extracted from step-by-step solutions and evaluated for accuracy using exact match, following simple-evals.7 Machine translation: WMT. We leverage several collections of multi-way parallel translations from the yearly shared task on general MT hosted by the Conference of Machine Translation (WMT) [Federmann et al., 2022; Deutsch et al., 2025]. We are only interested in translations from English into other languages, as this requires more complex target language generations.: Translation quality is measured by comparing model outputs against reference translations using the state-of-the-art evaluation metric XComet-XL [Colombo et al., 2023]. This set of generative tasks vary notably in their difficulty and metric sensitivity, so they challenge the robustness of sampling and selection methods that might have been reported successful in prior work on individual tasks. To ensure rigorous evaluation, we use dev and devtest splits for development of our methods, and only test on test splits once to prevent overfitting. Full experimental details and the process for creating the exact splits are described in appendix A. Throughout the paper, we refer to these tasks as Arena, MGSM, and WMT respectively, explicitly noting when we are using test splits (Section 3.3.2). All other results in Sections 2 and 3 are on the dev splits unless stated otherwise. Budget Size for Parallel Scaling Compared to prior works that investigate sample sizes in the hundreds to thousands [Freitag et al., 2023; Song et al., 2025; Huang et al., 2025], we focus on the lower end of inference-compute scale. We set = 5, given that it is more realistic workload for large scale production systems (i.e. many inputs applied with 5 the amount of the normal compute). Complementing this view, scaling curves tend to have their steepest incline in the first steps, i.e. the highest return for additional invested compute, especially for imperfect selection methods [Brown et al., 2024; Chen et al., 2025]. In preliminary explorations on open-ended generations, we found 6https://huggingface.co/datasets/nthakur/GSM8KInstruct-Parallel-instruct-dpo-v0.1 7https://github.com/openai/simple-evals/tree/main 6 that perfect match between utility metric and evaluation metric leads to continuous gains with steep initial increase (see appendix E.4). However when the evaluation metric is win rates across languages, returns of scaling are not as guaranteed, in that an even larger inference budget, especially beyond = 10, does not necessarily translate to much higher performance as shown in fig. 2. Measuring Sample Pool Quality With an optimistic perspective on the selection methods to work with samples downstream, our main focus lies on the quality of the best of the samples as determined by our evaluation metric r, expressing an upper bound of the performance of any selection strategy. For open-ended generations, we query an in-house multilingual reward model to estimate the quality of individual samples, while for the other tasks we can compare the evaluation metric to the reference response. In order to have scores that can be compared across tasks and languages, we introduce hope and risk, which intuitively represent the hope and risk of scaling up compared to single sample. hope is defined as the relative change between the score of the best sample in the set y+ = arg maxyY r(y) and the evaluation score of the greedy output ˆy: r(y+) r(ˆy) r(ˆy) . (2) For risk, we analogously compare the relative change in evaluation score between the worst sample and the greedy output: r(y) r(ˆy) r(ˆy) . (3) We report hope and risk averaged across instances from each benchmark. 2.3 Results Figure 3: Quality under single temperature sampling For each temperature, we evaluate best, worst and mean quality for = 5 samples from Aya for English, Japanese, French and Russian on each of the dev sets of the tasks (rows: Arena, MGSM, WMT). While best-case scores improve over greedy outputs (τ = 0), the effect varies across languages and tasks, with notable drop in worst-case quality for Japanese and Russian Arena and WMT at high temperatures. Higher temperature sensitivity for non-English. In classic single temperature sampling, we dedicate our entire inference budget to sampling at one fixed temperature. Figure 3 compares how best, worst and average performance differ across all three tasks if we spend this budget at different 7 Setting English Task Arena MGSM Average Non-English Arena MGSM Average hope risk -51.5 38.8 11.9 -23.8 25.4 -37.7 -65.3 55.5 18.7 -23.7 37.2 -44. (a) Sampling methods comparison (b) Risk & Hope across tasks and languages. Figure 4: Performance analysis of temperature sampling methods. Left: Sampling from one vs multiple temperatures, with and without hedging: Win rates over greedy outputs on mArenaHard with Judge MBR and = 5 samples, averaged across models. We choose τ = 0.7 as single temperature, and random temperatures are uniformly sampled from (τ [0.0, 0.3, 0.7, 0.8, 0.9, 1.0]). Hedging replaces one sample from the pool with the greedy output. Right: hope and risk of sampling at τ = 0.7, relative to evaluation scores of greedy decoding for Aya Expanse 8B. Values are reported as percentages. temperatures. We observe consistent trends: As temperature increases, the gap between best-case and worst-case outcomes widens. While higher temperatures lead to improved best-case scenarios, they also increase the chances of generating lower-quality examples. Notably, the rate at which variance increases is influenced by both the language and the nature of the task. Comparing English against the other languages, its average sample quality is more stable even at higher temperatures (more eurythermal ), while it drops earlier for other languages. Likewise, best-sample quality continues to grow till close to τ = 1 for English (French for WMT), while it starts decaying much earlier and steeper for other languages (here Japanese & Russian). Trends hold beyond the selection discussed here, see appendix C. Greedy outputs are the best single-sample bet. When inspecting the average scores at different temperatures, i.e. their expected quality, it becomes clear that greedy outputs (τ = 0) have equal or greater than the average quality of outputs at all temperatures, since they are not negatively affected by variance. This is consistent across languages and tasks and corroborates the recommendation for greedy decoding by [Song et al., 2025]. Hence, we will compare to greedy decoding as single-sample baseline when measuring the benefits of scaling up. Higher risk and hope for non-English. Figure 4b quantifies this trade-off at τ = 0.7 for mArenaHard and MGSM. English () sampling yields an average hope of 25.4% and risk of 37.7%, whereas non-English () attains higher hope (37.2%) but at greater risk (44.5%). This illustrates the importance of balancing potential gains against possible losses, particularly in multilingual settings where it is thus more risky to sample at extreme temperatures. These results also highlight the significant headroom achievable with just five samples. Benefits of multi-temperature sampling With the knowledge of these trends across temperatures, one could pick single temperature for each task and language to optimize the respective metrics. However, this analysis is expensive since it has to be optimized for each condition separately. If we assume no prior intuition for the task and language, we could instead use our budget to sample across multiple temperatures, (τ [0.0, 0.3, 0.7, 0.8, 0.9, 1.0]). We compare this against sampling from single temperature (here τ = 0.7, chosen for its overall highest best-case performance across tasks, languages and models). How does this choice affect downstream performance? We utilize Judge MBR as standard selection technique (details will follow in section 3) and measure m-ArenaHard win rates for all sampling techniques against greedy outputs, averaging results across our 8B models, 8 Aya and Qwen. fig. 4a shows clear downstream benefits for random multi-temperature sampling over single-temperature sampling, with gains of +8.8 points for English () and +1.5 points for non-English (). We attribute Englishs larger improvement to its comparatively lower risk at high temperatures  (fig. 3)  combined with high hope. Hedged sampling reduces risk by including greedy outputs Given our focus on multilingual generations, we explore approaches to manage the higher risk across languages at high temperatures. To mitigate this risk, we propose hedged variants, where one sample in the pool is obtained from greedy decoding. Figure 4a visualizes the effect of hedging for m-ArenaHard win-rates over greedy outputs. When we combine 4 samples at τ = 0.7 plus the greedy output for the MBR method to choose from, we find that this safety net effectively increases win-rate gains by +2.4 points for English () and provides the largest improvement for non-English () with +2.2 points. This constitutes the largest improvement over single temperature sampling in non-English languages. Our analysis shows that MBR selects greedy for 35.3% of prompts on average across languages and tasks. Overall, single temperature sampling with hedging balances English and non-English performance best, so we choose it as base for our selection experiments. Combination with probability pruning Furthermore, hedged sampling can be combined with techniques that reduce risk at the token level, such as min-p [Minh et al., 2025]. In our setup, we find that for the majority of tasks and selection methods configurations, min-p provides additional gains over hedged sampling alone. The improvements are most consistent for machine translation, where similar probability pruning techniques have previously been shown to be essential for MBR [Freitag et al., 2023]. The results in Appendix E.2 confirm these gains across tasks and selection methods. Therefore, we incorporate min-p into our hedged sampling approach for test set evaluations and denote this combination with the subscript min-p. This discussion highlights how even simple adjustments to the composition of small sampling pool can have noticeable downstream effects. Before turning to optimizing selection methods, we summarize our recommendations for squeezing the most of temperature sampling:"
        },
        {
            "title": "Multilingual LLMonade Recipe Part I",
            "content": "Step 1: Use hedged single-temperature sampling at moderate temperature (0.70.9) to generate samples. Optional: Localize reasonably high temperature for different contexts."
        },
        {
            "title": "3 How to Select?",
            "content": "Once we have sampled pool of generations with promising quality, our goal is to correctly identify the best generation in our pool of candidates. Our research question here is: How to select from sample pool with strategy that is robust across languages and tasks? 9 Section Findings Strong reward models or judges are required to robustly select the best of the sampled candidates. Our methods based on multilingual generalist LLM as judge are competitive with BoN selection with specialized RM. Crosslingual evidence (our proposed X-MBR) can further boost improvements over single-sample decoding. One-pass selection (our proposed CHOPS) is simple and cost-effective solution especially for multilingual open-ended tasks. X-MBR and CHOPS can even improve the strong judge LLM itself (here Command A), confirming their robustness and competitiveness. 3.1 Methodology In the following section, we briefly review multiple selection techniques of varying complexity, and propose our own extensions ( ) that are particularly equipped for multilingual generative tasks. Maximum Likelihood Given pool of samples , the sample ˆy with the highest likelihood under the model distribution pθ(y x) should be good candidate for selection when the model is well calibrated (i.e. likelihood and quality correlate): This constitutes an intrinsic metric that relies only on the sampling model itself. ˆy = arg max yY pθ(y x). (4) Best-of-N (BoN) introduces an extrinsic utility metric (y) to score each sample independently. The selected sample ˆy is then the one with highest utility score: (y). ˆy = arg max yY This approach relies on the utility metric being well aligned with the task evaluation metric and well calibrated, i.e. rating outputs adequately on common scale even if scored independently. It is commonly used with specialized reward models (RM BoN) [Zhang et al., 2024a; Ichihara et al., 2025; Pombal et al., 2025; Son et al., 2025] or verifiers in math or code tasks [Snell et al., 2025; Cobbe et al., 2021; Lightman et al., 2024; Zhang et al., 2025a]. One could also leverage an LLM for absolute BoN scoring, but this approach was not competitive out-of-the-box in preliminary explorations, as it rated all outputs similarly high. (5) Minimum Bayes Risk (MBR) decoding searches for the candidate ˆy that minimizes the expected risk over the distribution of samples [Kumar & Byrne, 2002; 2004; Eikema & Aziz, 2020; 2022]. The risk R(y) of candidate is approximated with pairwise comparisons from sample pool : 1 L(y, y), R(y) (cid:88) (6) yY where L(y, y) is pairwise loss function measuring the discrepancy between candidate and and pseudo-reference y. MBR thus selects ˆy = arg min yYh L(y, y), (cid:88) yYe 10 (7) where Yh is the hypothesis set, and Ye is the evidence set used to estimate the risk. As Bertsch et al. [2023] highlighted, the evidence set Ye aims to cover representative portion of the space for accurate risk estimation, while the hypothesis set Yh focuses on the narrower, high-quality region to avoid considering low-quality candidates, but they do not need to be identical. For loss functions, there are many possible implementations: When aligned well with the task evaluation metric, it reduces the optimization-evaluation gap and brings larger empirical gains [Kovacs et al., 2024], which has made it popular method in machine translation and open-ended generation [Fernandes et al., 2022; Freitag et al., 2023; Wu et al., 2025]. In this way, we can optimize for pairwise comparisons under an LLM judge at test time, such as win-rate evaluations. When loss functions focus on similarity (e.g. token-based similarity, which we will test with 2-shingles), MBR becomes the equivalent to majority voting in classification tasks [Bertsch et al., 2023]. It selects the sample that is most consistent with the evidence set, which relates it to the notion of self-consistency [Wang et al., 2023; Shi et al., 2024; Chen et al., 2025; Wang et al., 2025]. Checklisted One-Pass Selection (CHOPS) Most of the prior selection methods present considerable computational cost: BoN requires O(N ) forward passes, and MBR even O(N 2) due to pairwise comparisons. This may be reasonable approach for some latency-insensitive tasks, but we explore proposing an alternate approach that reduces this efficiency penalty. Capitalizing on the development of longer context windows for LLMs and their efficient processing, we prompt judge model to first generate checklist tailored to the given task, then evaluate all candidate samples against this checklist to directly choose the best response in one pass (see prompt in appendix D). This is inspired by the success of rubrics to facilitate LLM judge decisions [Kim et al., 2024] and the ability of LLMs to generate prompt-specific checklists [Cook et al., 2024], which might help to adapt the judge on-the-fly to diverse selection scenarios across languages and tasks. CHOPS requires only O(1) forward pass of the LLM judge, fitting all samples into the LLM context in single call. While BoN rates samples independently, CHOPS rates them in one global context, anchored in the checklist criteria, which arguably alleviates concerns of LLM judge calibration across multiple independent ratings. Our ablation reported in appendix E.3 confirms the effectiveness of the self-generated checklists in contrast to more straightforward prompt, especially for non-verifiable tasks and languages other than English. Crosslingual MBR (X-MBR) We propose X-MBR as method to improve performance by utilizing the multilingual capabilities of LLM models. Building directly on the MBR paradigm, X-MBR uses cross-lingual evidence to more robustly select from target-language candidates. For an input x, X-MBR uses the same hypothesis set as standard MBR, i.e. the same five samples in the target language. The novelty lies in the cross-lingual evidence set Yex, that extends original in-language evidence set Ye from eq. (7) by smaller set of cross-lingual samples (M < ). These samples are generated by instructing the same LLM to respond in different evidence language (e.g. English, see prompt in Appendix D). This approach does not require prompt translation, it directly prompts the multilingual model to output the response in another language. We then pick the candidate from the hypothesis set that accumulates the highest cross-lingual support, with the same MBR selection criterion as for classic MBR (eq. (7)), but including additional cross-lingual comparisons by the LLM judge between the hypothesis set Yh and the set of new crosslingual samples Yex: ˆy = arg min yYh (cid:88) L(y, y). y(YeYex) (8) This exploits both the cross-lingual generation abilities of the model that we sample from, as well as the cross-lingual comparison abilities of the judge LLM. X-MBR requires generating + total 11 Figure 5: Comparison of baselines vs RM and LLM Judge on = 5 generations in terms of the win-rate comparing to greedy outputs on mArenaHard. Averaged across models, and non-English languages. samples and performing (N + ) pairwise comparisons, approximating O(N 2) complexity. We set = 3 cross-lingual evidence samples, which increases the compute budget but in turn it improves precision of the selection, as we will see in the next section. It is an interesting direction which explores the return on additional compute investment through cross-lingual validation. 3.2 Experimental Setup Baseline We compare against greedy decoding, which in each generation step selects the token with the highest model probability, resulting in deterministic and predictable output. This gives us simple yet effective comparison point [Song et al., 2025], as we have empirically confirmed in section 2. In this way we can quantify the benefits from scaling the generation budget from 1 to 5 samples. For open-ended generation, we additionally establish more challenging benchmark by comparing selected samples from our models against single greedy samples from the significantly larger Gemini 2.0 Flash model, ensuring more rigorous assessment. Choice of Utility Metrics Some of the above selection methods can be used with multiple utility metrics or backbone models, such as BoN or MBR. In order to disentangle the effect of the method from the underlying utility metric, we compare multiple instantiations of each. Concretely, we benchmark two versions of MBR, 2-Shingle MBRwhich relies on the simple Jaccard similarity of token-level 2-grams from pairs of generations, and Judge MBRwhich queries an LLM judge for pairwise comparisons. In preliminary experiments, we also explored using the LLM judge for BoN vs the RM, but the LLM judge did not perform competitively due to missing calibration for generating absolute scores. Choice of RM and Judge Model Based on prior findings that the precision of the utility score can have major impact on the success of inference-scaling [Huang et al., 2025; Stroebl et al., 2024], we aim to pick the best scoring open judge or RM model for our experiments. For techniques which use an LLM judge, we use Command [Cohere et al., 2025], an 111B model optimized for multilingual performance supporting 23 languages. Command scores competitively to GPT-4o on mRewardBench [Gureja et al., 2024]. For BoN RM, we choose the leader from the RewardBench leaderboard [Lambert et al., 2024] INF-ORM-Llama3.1-70B Minghao Yang [2024], which is based on the multilingual Llama3.1 base. INF-ORM-LLama3.1-70B is trained on mix of open-sourced preference pairs [Liu et al., 2024] with difference magnitudes determined by GPT-4o for scaling [Wang et al., 2024]. This constitutes very strong competitor for any LLM judge approach, as this model is specifically engineered and trained to perform generation scoring aligned with GPT-4o. Appendix 12 Model Lang. S-MBR En-MBR Zh-MBR R-MBR Aya Qwen 12.00 13.27 9.20 6. 15.93 9.87 15.20 12.64 20.00 8.40 13.20 11.53 4.40 7. Table 2: Expanding MBR evidence sets for X-MBR: Comparison of different sources of evidence for 3 additional samples: S: from the same language, R: random samples across languages, En: from English, Zh: from Chinese. The benefits of for win-rates on mArena are highest when sampling evidence from more dominant languages. details the selection process of RM and LLM judge. Each selection method with LLM judge requires specific prompt, which we list in appendix D. For consistency, the instruction prompts are always in English. 3.3 Results First (section 3.3.1), we compare selection methods that do not rely on LLM judges or RMs (Maximum Likelihood, Similarity MBR) with those that do rely on them (RM BoN, Judge MBR), and develop our newly proposed selection methods (CHOPS, X-MBR). These initial experiments are based on on the development set of m-ArenaHard-v0.1 benchmark and hedged sampling at τ = 0.7. Second (section 3.3.2), we put our final best RM/judge-based methods to the test on the held-out test sets across all three tasks of open-ended generation, machine translation and math reasoning. 3.3.1 Establishing the Best Selection Methods Maximum Likelihood and Similarity-based MBR are not competitive crosslingually. From Figure 5, we observe that Maximum Likelihood selection results in losses across the bench, suggesting that the models internal probabilities are not well calibrated for win rate evaluation. Averaged across our models, similarity-based MBR, which selects the sample most similar to others in the pool, yields +5.6 % increase in win-rate over greedy in English () but provides no improvement in non-English (). We suspect this is due the quality of the pool of samples which does not sustain picking the most consistent sample, as it is expected to have higher variance compared to English (see section 2). BoN and judge-based selection outperform greedy. RM BoN shows consistent improvements over greedy decoding for both groups (6.4 % in English, 7 % in non-English), establishing it as strong baseline. This corroborates recommendations by Wu et al. [2024b] to use reward models crosslingually for BoN when they have multilingual LLM backbone. Judge-based MBR achieves the highest deltas (7.2 % in English, 8.1 % in non-English), showing that general-purpose multilingual LLM judge can outperform specialized reward model. The flexibility of using LLMs for pairwise comparisons in the MBR setup aligns well with the pairwise setup in win rate evaluation, though it requires O(N 2) comparisons at test time. Better judgment with crosslingual evidence We extend the evidence set of MBR with = 3 more samples, either (1) samples of the same language (S-MBR)which corresponds target-language extension of the standard MBR, (2) hard-coded fixed language (English, Chinese; En/Zh-MBR), or (3) randomly chosen languages for all prompts (R-MBR). For randomly chosen languages we sample uniformly from the set of languages of our experimental setup. In table 2, we compare these variants of X-MBR. Note that they all use the same candidates for selection, they just differ in the composition of the evidence set. Using additional evidence from the same language 13 Task Model RM BoNmin-p CHOPSmin-p X-MBRmin-p Greedy Arena MGSM Aya Qwen Aya Qwen WMT Aya Qwen 19.60 16.27 2.00 5.87 7.76 9.59 3.04 3.65 1.04 1.43 14.40 17. 7.60 8.27 6.96 6.19 1.84 2.19 0.72 1.12 16.80 15.67 8.80 9. 7.76 7.92 0.64 3.85 0.20 0.93 77.84 69. 94.96 84.41 71.92 76.15 Table 3: Test set results: Quality gains over greedy decoding by selecting from five samples ( hedged τ = 0.7 and min-p with = 0.2) for 7 languages of study. X*-MBR uses Chinese as evidence languages for English, and English for the rest. Highest values for each row are bold. results in an average 10% win rate across Aya and Qwen over greedy, compared to 7.6% from MBR without extended evidence. Both En-MBR and Zh-MBR result in significant improvement over both greedy (up to 14% for Zh-MBR) and the S-MBR baseline, particularly in non-English languages (). This shows that it is best to sample evidence from dominant other languages. This works better than randomly drawing samples from mix of languages (R-MBR). Hence, we set this as the default for all following experiments with X-MBR: Sample in Chinese, if responding to an English prompt, else sample in English. The results presented in this experiment demonstrate that we can effectively leverage the models multilingual capabilities to enhance performance across all languages. Furthermore, this underscores the potential of multilingual LLM judges with strong crosslingual capabilities to optimize available test-time compute. key ingredient in this part of the recipe is the careful selection of cross-lingual approaches. 3.3.2 Testing in Multilingual Multitasking Bringing it all together: Test set performance across tasks and models. Table 3 present the final results for our LLM-judge based aggregation methods using hedged sampling with τ = 0.7 and min-p with = 0.2 across models and tasks. We find improvements over the greedy baseline (i.e., the best single-sample method) in all tasks, languages, and methods with magnitudes of improvement that are substantial; considering that we are working with as few as five samples to choose from. CHOPS vs X-MBR For open-ended tasks, both CHOPS and X-MBR outperform reward modelbased BoN selection. Across models, BoN achieves win deltas of 10.8% (English ) and 11.0% (non-English ) over greedy decoding. CHOPS improves performance to 11.0% () and 12.8% (), showing strength in multilingual settings. X-MBR performs best overall with 12.8% () and 12.5% (), respectively. In close-ended evaluation, selecting from 5 samples yields significant gains across all methods, even with strong initial greedy performance. BoN often performs best, particularly for WMT. CHOPS overall yields consistent gains with lower compute costs. X-MBR shines particularly for non-English Qwen, where there are larger language disparities to bridge. LLMs-as-Judge based methods are more robust in open-ended tasks. We conduct two additional evaluations to assess robustness beyond greedy baseline comparisons. In Table 4, we compare our 8B models against the much larger and more capable Gemini 2.0 Flash model. We 14 RM BoN CHOPS X-MBR Open-Ended Eval Aya vs Gemini Qwen vs Gemini O(N ) 7.60 6.87 1.60 6.00 O(1) 2.80 5.07 2.80 5.00 O(N (N + )) Greedy 3.60 7. 0.40 6.33 20.80 25.80 38.40 42.07 Table 4: Open ended test results with extrinsic comparison: We show gains in win rates in more challenging setup: against Gemini 2.0 flash as strong baseline. We report the delta in win rates from using 5 samples with specific selection method compared to the single sample. Figure 6: Self-improvement with parallel scaling: Command win rates against the greedy single sample baseline for each of the selection methods. Our methods, which also use Command for selection, outperform the RM BoN in the majority of languages. report the delta in win-rates obtained from the selection method with 5 samples compared to single sample. X-MBR achieves the highest multilingual () gains with an average of +6.8 points, while BoN and CHOPS follow with +6.4 and +5.0, respectively. Gains transfer to self-improvement of high-end LLM. One could argue that the improvements are only due to performance or size gradients between the smaller 8B models that we have tested, and the much larger RM/LLM judge models. However, improvements can still be found at larger scale and without this gradient: In Figure 6, we evaluate self-improvement scenario where we use Command to both generate samples and perform selection for CHOPS and X-MBR. Not only do we find consistent gains across languages, but also our new selection methods outperform RM BoN, with CHOPS and X-MBR obtaining remarkable average deltas of 9.0% and 8.3% compared to BoNs modest 4.5%. This consistent cross-lingual performance demonstrates the robustness of our methods even when the same model must evaluate its own outputs. This leads us to complete the second part of our recommended recipe for parallel scaling of multilingual LLMs in generative tasks:"
        },
        {
            "title": "Multilingual LLMonade Recipe Part II",
            "content": "Step 2: Use CHOPS to select the best sample. If you can afford O(N 2) calls to the LLM judge and have task with strong language disparities, use X-MBR. Optional: small exploration of multilingual LLM judges to find the best suited one."
        },
        {
            "title": "4 Related Work",
            "content": "Stochastic vs Deterministic Inference Early LLM research suggested that diversity through stochastic inference often comes at cost of quality [Holtzman et al., 2020], benefiting some tasks while hindering others [Holtzman et al., 2020; Peeperkorn et al., 2024; Renze, 2024]. In the context of multi-sample inference, Song et al. [2025] found that the optimal approach depends on the task: closed-ended and verifiable tasks favor deterministic decoding, while open-ended generation benefits from the variance introduced by stochastic sampling. Du et al. [2025] optimize the temperature across math and coding tasks for English with insights from entropy measures for high samples sizes (N = 256), but excluding τ = 0. In our work, we focus on exploiting variance in the smaller sample range (N = 5) across multiple generative tasks. We add the dimension of language that has previously been ignored: Going beyond English, higher temperatures pose higher risk, and when running inference for multiple languages at once, this requires more caution. We address this by combining stochastic and deterministic inference. Multilingual Test-time Scaling and Alignment While most test-time scaling and alignment research focuses on English, few recent works have explored multilinguality. Pombal et al. [2025] propose multilingual judge LLM for BoN, showing improvements in win rates across three languages. Similarly, the contemporaneous work by Gupta & Srikumar [2025] confirms the potential benefits of RM BoN for multilingual open-ended tasks. They find improvements in win-rates for dozen languages with RM BoN for 5 to 100 samples across model sizes. Our study expands on these previous RM BoN explorations with broader set of tasks and novel methods, specifically crafted for the challenges in multilingual generation both on the sample generation and the sample selection side. What emerges as consistent pattern in their and our work is that RMs appear to generalize well across languages for parallel scaling, even if trained only on rewards for English [Wu et al., 2024b]. Similarly, Yong et al. [2025] demonstrate cross-lingual scaling benefits in math and STEM reasoning with LLM with multilingual backbone tuned for English reasoning. They show benefits of non-target language reasoning/scaling, which is loosely related to the effectiveness of crosslingual evidence for X-MBR that we find in our experiments. With the shared motivation to reduce imbalance across languages, Yang et al. [2025b] and Zhu et al. [2024] use cross-lingual sample generation with translation pipeline, while Yoon et al. [2024] combine expert models for task and language expertise. Our X-MBR approach achieves significant gains without intermediate translation or experts, directly leveraging the LLMs strong cross-lingual generation capabilities."
        },
        {
            "title": "5 Conclusion",
            "content": "We have conducted extensive experiments on three generative tasks to compile recipe for multilingual parallel scaling that generalizes across both tasks and models. Based on our insights on the impact of temperature on sample pool quality, we designed hedged temperature sampling variant, and combine it with selection methods tailored towards multilingual judges. We propose two approaches which improve upon existing methods: Checklisted One-Pass Selection (CHOPS) and Cross-lingual MBR (X-MBR). These techniques show consistent cross-lingual gains in the benefits of test-time scaling. This has not only implications for inference, but also for applications where multilingual inference is an intermediate step in model improvement, e.g. for synthetic data generation [Thakur et al., 2024; Dang et al., 2024a; Odumakinde et al., 2024] or distillation [Zhang et al., 2024b], test-time alignment [Sun et al., 2024; Amini et al., 2025] or model fine-tuning [Touvron et al., 2023; Snell et al., 2025]. Moreover, the gains observed at small scale and the steep incline suggest even greater potential lies at the close horizon when scaling these methods to larger sample pools. Additionally, the effectiveness of using models for both generation and selection opens promising avenues for self-improvement frameworks in multilingual settings."
        },
        {
            "title": "Limitations",
            "content": "Reliance on judge alignment All methods that use extrinsic signals (reward models or LLM judges) for selecting from multiple samples are bounded by their alignment with the evaluation metric, as has previously been pointed out in [Stroebl et al., 2024; Huang et al., 2025]. Our methods do not directly address this issue. By selecting the latest and most generalist judge models for selection, we hope that the effects of task-specific reward hacking / mismatch are reduced. Language selection Our selected languages are all high-resource languages and well represented throughout the stages of LLM training. Our study does not cover the test case of generalizing to underrepresented languages that are unsupported by the model or not included in stages beyond base model training. We can expect that both quality of samples and LLM aggregation precision will be significantly lower, so approaches like X-MBR that leverage crosslingual knowledge might be more promising. Cost of selection method We found that larger generative model was needed to improve upon the base model performance (based on preliminary explorations with mPrometheus [Pombal et al., 2025]) with small . In practice, there is balance to be found between scaling up versus scaling up the judge model. Distilling the outputs of the larger generative judge into smaller model might be an interesting avenue for optimizing that trade-off."
        },
        {
            "title": "6 Acknowledgments",
            "content": "We thank Ahmet Üstün, John Dang, Samuel Cahyawijaya, Arash Ahmadian and other colleagues at Cohere and Cohere Labs for their support and thoughtful feedback. We also thank Sander Land for his contributions to our evaluations."
        },
        {
            "title": "References",
            "content": "Arash Ahmadian Aakanksha, Beyza Ermis, Seraphina Goldfarb-Tarrant, Julia Kreutzer, Marzieh Fadaee, Sara Hooker, et al. The multilingual alignment prism: Aligning global and local preferences to reduce harm. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1202712049, 2024. David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. learning algorithm for boltzmann machines. Cognitive Science, 9(1):147169, 1985. ISSN 0364-0213. doi: https://doi.org/10.1016/S0 364-0213(85)80012-4. URL https://www.sciencedirect.com/science/article/pii/S0364021 385800124. Arash Ahmadian, Seraphina Goldfarb-Tarrant, Beyza Ermis, Marzieh Fadaee, Sara Hooker, et al. Mix data or merge models? optimizing for diverse multi-task learning. arXiv preprint arXiv:2410.10801, 2024. Afra Amini, Tim Vieira, Elliott Ash, and Ryan Cotterell. Variational best-of-n alignment. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openrevi ew.net/forum?id=W9FZEQj3vv. Amanda Bertsch, Alex Xie, Graham Neubig, and Matthew Gormley. Its MBR all the way down: Modern generation techniques through the lens of minimum Bayes risk. In Yanai Elazar, Allyson Ettinger, Nora Kassner, Sebastian Ruder, and Noah A. Smith (eds.), Proceedings of the Big Picture Workshop, pp. 108122, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.bigpicture-1.9. URL https://aclanthology.org/2023.bigpicture-1.9/. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Jianhao Chen, Zishuo Xun, Bocheng Zhou, Han Qi, Qiaosheng Zhang, Yang Chen, Wei Hu, Yuzhong Qu, Wanli Ouyang, and Shuyue Hu. Do we truly need so many samples? multi-llm repeated sampling efficiently scale test-time compute. arXiv preprint arXiv:2504.00762, 2025. Nuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Dongmei Zhang, and Jia Li. Breaking language Insights and observations. arXiv preprint barriers in multilingual mathematical reasoning: arXiv:2310.20246, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Team Cohere, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Yazeed Alnumay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, Raphaël Avalos, et al. Command a: An enterprise-ready large language model. arXiv preprint arXiv:2504.00698, 2025. Pierre Colombo, Nuno Guerreiro, Ricardo Rei, Daan Van, Luisa Coheur, and André Martins. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 2023. Jonathan Cook, Tim Rocktäschel, Jakob Foerster, Dennis Aumiller, and Alex Wang. Ticking all the boxes: Generated checklists improve llm evaluation and generation. arXiv preprint arXiv:2410.03608, 2024. John Dang, Arash Ahmadian, Kelly Marchisio, Julia Kreutzer, Ahmet Üstün, and Sara Hooker. RLHF can speak many languages: Unlocking multilingual preference optimization for LLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1313413156, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.729. URL https://aclanthology.org/2024.emnlp-main.729/. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, et al. Aya expanse: Combining research breakthroughs for new multilingual frontier. arXiv preprint arXiv:2412.04261, 2024b. Saurabh Dash, Yiyang Nan, John Dang, Arash Ahmadian, Shivalika Singh, Madeline Smith, Bharat Venkitesh, Vlad Shmyhlo, Viraat Aryabumi, Walter Beller-Morales, Jeremy Pekmez, Jason Ozuzu, Pierre Richemond, Acyr Locatelli, Nick Frosst, Phil Blunsom, Aidan Gomez, Ivan Zhang, Marzieh Fadaee, Manoj Govindassamy, Sudip Roy, Matthias Gallé, Beyza Ermis, Ahmet Üstün, and Sara Hooker. Aya vision: Advancing the frontier of multilingual multimodality, 2025. URL https://arxiv.org/abs/2505.08751. Daniel Deutsch, Eleftheria Briakou, Isaac Caswell, Mara Finkelstein, Rebecca Galor, Juraj Juraska, Geza Kovacs, Alison Lui, Ricardo Rei, Jason Riesa, et al. Wmt24++: Expanding the language coverage of wmt24 to 55 languages & dialects. arXiv preprint arXiv:2502.12404, 2025. 18 Weihua Du, Yiming Yang, and Sean Welleck. Optimizing temperature for language models with multi-sample inference. arXiv preprint arXiv:2502.05234, 2025. Bryan Eikema and Wilker Aziz. Is MAP decoding all you need? the inadequacy of the mode in neural machine translation. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, pp. 45064520, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.398. URL https://aclanthology.org/2020.coling-main.398/. Bryan Eikema and Wilker Aziz. Sampling-based approximations to minimum Bayes risk decoding for neural machine translation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1097810993, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.754. URL https://aclanthology.org/2022.em nlp-main.754/. Christian Federmann, Tom Kocmi, and Ying Xin. NTREX-128 news test references for MT evaluation of 128 languages. In Kabir Ahuja, Antonios Anastasopoulos, Barun Patra, Graham Neubig, Monojit Choudhury, Sandipan Dandapat, Sunayana Sitaram, and Vishrav Chaudhary (eds.), Proceedings of the First Workshop on Scaling Up Multilingual Evaluation, pp. 2124, Online, November 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.sumeval-1.4. URL https://aclanthology.org/2022.sumeval-1.4/. Patrick Fernandes, António Farinhas, Ricardo Rei, José G. C. de Souza, Perez Ogayo, Graham Neubig, and Andre Martins. Quality-aware decoding for neural machine translation. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 13961412, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.100. URL https://aclanthology .org/2022.naacl-main.100/. Markus Freitag, Behrooz Ghorbani, and Patrick Fernandes. Epsilon sampling rocks: Investigating sampling strategies for minimum Bayes risk decoding for machine translation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 91989209, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.f indingsemnlp.617. URL https://aclanthology.org/2023.findings-emnlp.617/. Ashim Gupta and Vivek Srikumar. Test-time scaling with repeated sampling improves multilingual text generation, 2025. URL https://arxiv.org/abs/2505.21941. Srishti Gureja, Lester James Miranda, Shayekh Bin Islam, Rishabh Maheshwary, Drishti Sharma, Gusti Winata, Nathan Lambert, Sebastian Ruder, Sara Hooker, and Marzieh Fadaee. Mrewardbench: Evaluating reward models in multilingual settings. arXiv preprint arXiv:2410.15522, 2024. John Hewitt, Christopher Manning, and Percy Liang. Truncation sampling as language model desmoothing. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 34143427, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022 .findings-emnlp.249. URL https://aclanthology.org/2022.findings-emnlp.249/. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=rygGQyrFvH. 19 Sara Hooker. On the limitations of compute thresholds as governance strategy, 2024. URL https://arxiv.org/abs/2407.05694. Audrey Huang, Adam Block, Qinghua Liu, Nan Jiang, Dylan Foster, and Akshay Krishnamurthy. Is best-of-n the best of them? coverage, scaling, and optimality in inference-time alignment. arXiv preprint arXiv:2503.21878, 2025. Yuki Ichihara, Yuu Jinnai, Tetsuro Morimura, Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, and Eiji Uchibe. Evaluation of best-of-n sampling strategies for language model alignment. Transactions on Machine Learning Research, 2025. ISSN 2835-8856. URL https://openreview.net/forum?i d=H4S4ETc8c9. Daphne Ippolito, Reno Kriz, João Sedoc, Maria Kustikova, and Chris Callison-Burch. Comparison of diverse decoding methods from conditional language models. In Anna Korhonen, David Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 37523762, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1365. URL https://aclanthology.org/P19-1365/. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 43344353, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.248. URL https://aclanthology .org/2024.emnlp-main.248/. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, et al. Findings of the wmt24 general machine translation shared task: the llm era is here but mt is not solved yet. In Proceedings of the Ninth Conference on Machine Translation, pp. 146, 2024a. Tom Kocmi, Vilém Zouhar, Christian Federmann, and Matt Post. Navigating the metrics maze: Reconciling score magnitudes and accuracies. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 19992014, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.110. URL https://aclanthology.org /2024.acl-long.110/. Geza Kovacs, Daniel Deutsch, and Markus Freitag. Mitigating metric bias in minimum Bayes risk decoding. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz (eds.), Proceedings of the Ninth Conference on Machine Translation, pp. 10631094, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.wmt-1.109. URL https://aclanthology.org/2024.wmt-1.109/. Julia Kreutzer, Eleftheria Briakou, Sweta Agrawal, Marzieh Fadaee, and Kocmi Tom. Deja vu: Multilingual llm evaluation through the lens of machine translation evaluation. arXiv preprint arXiv:2504.11829, 2025. Shankar Kumar and William Byrne. Minimum bayes-risk word alignments of bilingual texts. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10, EMNLP 02, pp. 140147, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1118693.1118712. URL https://doi.org/10.3115/1118693.1118712. 20 Shankar Kumar and William Byrne. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pp. 169176, Boston, Massachusetts, USA, May 2 - May 7 2004. Association for Computational Linguistics. URL https://aclanthology.org/N04-1022/. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling. https://huggingface.co/spa ces/allenai/reward-bench, 2024. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/f orum?id=v8L0pN6EOi. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024. Xiaoyu Tan Minghao Yang, Chao Qu. Inf-orm-llama3.1-70b, 2024. URL [https://huggingface.co /infly/INF-ORM-Llama3.1-70B](https://huggingface.co/infly/INF-ORM-Llama3.1-70B). Nguyen Nhat Minh, Andrew Baker, Clement Neo, Allen Roush, Andreas Kirsch, and Ravid Shwartz-Ziv. Turning up the heat: Min-p sampling for creative and coherent LLM outputs. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=FBkpCyujtS. Ayomide Odumakinde, Daniel Dsouza, Pat Verga, Beyza Ermis, and Sara Hooker. Multilingual arbitrage: Optimizing data pools to accelerate multilingual progress. arXiv preprint arXiv:2408.14960, 2024. Max Peeperkorn, Tom Kouwenhoven, Dan Brown, and Anna Jordanous. Is temperature the creativity parameter of large language models? In International Conference on Computational Creativity, April 2024. URL https://kar.kent.ac.uk/105743/. José Pombal, Dongkeun Yoon, Patrick Fernandes, Ian Wu, Seungone Kim, Ricardo Rei, Graham Neubig, and André FT Martins. M-prometheus: suite of open multilingual llm judges. arXiv preprint arXiv:2504.04953, 2025. Matthew Renze. The effect of sampling temperature on problem solving in large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 73467356, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.432. URL https://aclanthology.org/2024.findings-emnlp.432/. 21 Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, and Wai Lam. thorough examination of decoding methods in the era of LLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 86018629, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.489. URL https://aclantholo gy.org/2024.emnlp-main.489/. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-ofthought reasoners. arXiv preprint arXiv:2210.03057, 2022. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?i d=4FWAwZtd2n. Guijin Son, Jiwoo Hong, Hyunwoo Ko, and James Thorne. Linguistic generalizability of test-time scaling in mathematical reasoning. arXiv preprint arXiv:2502.17407, 2025. Yifan Song, Guoyin Wang, Sujian Li, and Bill Yuchen Lin. The good, the bad, and the greedy: Evaluation of LLMs should not ignore non-determinism. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 41954206, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.211. URL https: //aclanthology.org/2025.naacl-long.211/. Miloš Stanojević, Amir Kamran, Philipp Koehn, and Ondřej Bojar. Results of the wmt15 metrics shared task. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 256273, 2015. Benedikt Stroebl, Sayash Kapoor, and Arvind Narayanan. Inference scaling {F } laws: The limits of llm resampling with imperfect verifiers. arXiv preprint arXiv:2411.17501, 2024. Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. Fast best-of-n decoding via speculative rejection. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=348hfcprUs. Nandan Thakur, Jianmo Ni, Gustavo Hernandez Abrego, John Wieting, Jimmy Lin, and Daniel Cer. Leveraging LLMs for synthesizing training data across many languages in multilingual dense retrieval. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 76997724, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.426. URL https://aclanthology.org/2024.naacl-long.426/. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv, abs/2302.13971, 2023. Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al. Aya model: An instruction finetuned open-access multilingual language model. arXiv preprint arXiv:2402.07827, 2024. 22 Junlin Wang, Shang Zhu, Jon Saad-Falcon, Ben Athiwaratkun, Qingyang Wu, Jue Wang, Shuaiwen Leon Song, Ce Zhang, Bhuwan Dhingra, and James Zou. Think deep, think fast: Investigating efficiency of verifier-free inference-time-scaling methods, 2025. URL https://arxiv.org/abs/25 04.14047. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673, 2024. Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms for large language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=eskQMcIbMS. Survey Certification. Ian Wu, Patrick Fernandes, Amanda Bertsch, Seungone Kim, Sina Khoshfetrat Pakazad, and Graham Neubig. Better instruction-following through minimum bayes risk. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=7xCS K9BLPy. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Scaling inference computation: Compute-optimal inference for problem-solving with language models. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024a. URL https://openreview.net/for um?id=j7DZWSc8qu. Zhaofeng Wu, Ananth Balashankar, Yoon Kim, Jacob Eisenstein, and Ahmad Beirami. Reuse your rewards: Reward model transfer for zero-shot cross-lingual alignment. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 13321353, Miami, Florida, USA, November 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.79. URL https: //aclanthology.org/2024.emnlp-main.79/. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025a. URL https://arxiv.org/abs/2505.09388. Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, and Jiajun Zhang. Language imbalance driven rewarding for multilingual self-improving. In The Thirteenth International Conference on Learning Representations, 2025b. Zheng-Xin Yong, Farid Adilazuarda, Jonibek Mansurov, Ruochen Zhang, Niklas Muennighoff, Carsten Eickhoff, Genta Indra Winata, Julia Kreutzer, Stephen Bach, and Alham Fikri Aji. Crosslingual reasoning through test-time scaling. arXiv preprint arXiv:2505.05408, 2025. Dongkeun Yoon, Joel Jang, Sungdong Kim, Seungone Kim, Sheikh Shafayat, and Minjoon Seo. LangBridge: Multilingual reasoning without multilingual supervision. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 75027522, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.405. URL https://aclanthology.org/2024.acl-long.405/. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024a. URL https://openreview.net/forum?id=Cx HRoTLmPX. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The Thirteenth International Conference on Learning Representations, 2025a. URL https://openreview.net/forum?id=Ccwp 4tFEtE. Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, and Chen Ma. What, how, where, and how well? survey on test-time scaling in large language models. arXiv preprint arXiv:2503.24235, 2025b. Yuanchi Zhang, Yile Wang, Zijun Liu, Shuo Wang, Xiaolong Wang, Peng Li, Maosong Sun, and Yang Liu. Enhancing multilingual capabilities of large language models through self-distillation from resource-rich languages. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1118911204, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.603. URL https://aclanthology.org/2024.acl-long.603/. Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, and Alexandra Birch. Question In Lun-Wei Ku, Andre Martins, and translation training for better multilingual reasoning. Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 84118423, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.498. URL https://aclanthology.org/2024.findings-acl.498/."
        },
        {
            "title": "A Detailed Experimental Setup",
            "content": "Models and Language Coverage For the multilingual generative model, we consider two 8B models from different families: Aya-Expanse-8B [Dang et al., 2024b] and Qwen3-8B [Yang et al., 2025a]. Aya-Expanse-8B is an open-weights multilingual LLM supporting 23 languages. It employs post-training recipe focused on open-ended generative tasks that includes supervised fine-tuning, multilingual preference tuning [Aakanksha et al., 2024; Dang et al., 2024a] and synthetic data optimization [Odumakinde et al., 2024], and model merging [Ahmadian et al., 2024] to achieve strong competitive performance on open-ended benchmarks for models of this size. Qwen3-8B is an open-source dense model from the Qwen3 model family, supporting up to 119 languages and dialects. It was post-trained through distillation from larger models in the family and is used here in its non-thinking mode without reasoning. We focus on subset of 7 languages which are covered by both models: English, French, German, Japanese, Russian, Simplified Chinese, Spanish prioritized due to their inclusion in generative benchmarks of interest. For WMT, we translate from English into all other languages. Development Sets To ensure the generalization of our methods to unseen data, we follow best practices and meticulously craft development set and development testing set (devtest) for our evaluations. For all development and test set we use 250 randomly sampled prompts, we describe them below: m-ArenaHard: We sample 250 prompts from the original Arena-Hard-Auto dataset [Li et al., 2024] by randomly picking one from each of the 250 clusters. In addition to this stratification across clusters, we use the translations of the exact prompts from mArena-Hard for each language when creating our development set. This way we ensure no cross-contamination across both languages and clusters. m-ArenaHard-v2.0: To create the m-ArenaHard-v2.0 test set, we obtain the 750 prompts from Arena-Hard-v2.08 and use papluca/xlm-roberta-base-language-detection9 to perform language identification. Of these, the 498 identified as \"English\" were then translated into 23 languages by using an in-house state-of-the-art translation model. This results in total test set of 11,454 multilingual prompts. WMT: We use the development sets from WMT24 [Kocmi et al., 2024a] for most language pairs with the except of en-fr, which we obtain from WMT15 [Stanojević et al., 2015]. MGSM: We obtain the GSM8K Parallel Translated Corpus [Chen et al., 2023] and group them by the original prompt. We then randomly select 250 prompt groups and select the same for all languages to avoid cross-lingual contamination. For each math problem, the model is instructed in the specific language to solve step-by-step and provide final answer. Final answers are extracted from the step-by-step solutions and evaluated for accuracy using exact match to the correct answer, following simple-evals10. Model Serving: We use vLLM [Kwon et al., 2023] to generate outputs from our 8B models (Aya and Qwen), loading them with FP8 quantization and maximum sequence length of 8,192 tokens. For the larger models (Command and Gemini 2.0 Flash), we use their dedicated hosted APIs. For greedy decoding, we set top-k to 1, while for multi-sample generation we obtain five completions at 8https://github.com/lmarena/arena-hard-auto/blob/main/data/arena-hard-v2.0/ 9https://huggingface.co/papluca/xlm-roberta-base-language-detection 10https://github.com/openai/simple-evals/tree/main 25 specified temperature and min-p values."
        },
        {
            "title": "B Choosing Judge and Reward Model",
            "content": "Model GPT-4o1 (gpt-4o-2024-08-06) GPT-4o2 (gpt-4o-2024-11-20) Aya Expanse 8B1 Llama 3.1 70B1 Gemma 2 9B1 M-Prometheus-14B2 Qwen2.5-14B-Instruct2 Command (111B) Avg 81.1 85.8 65.2 75.5 76.6 79.5 80.8 84.5 Table 5: Average accuracy on the M-RewardBench across 24 languages, comparing open models of various sizes (below) to GPT-4o variants. 1: Results are copied from [Gureja et al., 2024], 2: from [Pombal et al., 2025]. Table 5 compares Multilingual RewardBench [Gureja et al., 2024] scores for multiple generative multilingual LLMs. We add benchmark scores for Command by running the official code released with the benchmark11. Remaining scores are taken from prior reports [Gureja et al., 2024; Pombal et al., 2025]. Table 6 details the performance for Command for each language. According to this benchmark and the scores reported in [Gureja et al., 2024] and [Pombal et al., 2025], Command is the best open judge, scoring closely to GPT-4o (and even outperforming an older variant). There is further support in experiments by Kreutzer et al. [2025] where its agreement with pairwise human preferences from Chatbot Arena battles in multiple languages is close to GPT-4os, with particular strengths in Chinese, Vietnamese, French, Turkish and Dutch. On the English RewardBench benchmark [Lambert et al., 2024], classifier RMs are outperforming generative ones, so we pick the top-performing open model as our RM for BoN, INF-ORM-Llama3.1-70B Minghao Yang [2024], which is based on the multilingual Llama3.1-70B. The model underlying the RM, Llama3.1, supports English, German, French, Italian, Portuguese, Hindi, Spanish, and Thaiwhich we suspect yields the strong crosslingual generalization."
        },
        {
            "title": "C Temperature Sensitivity",
            "content": "Figures 7 to 9 show the effect of temperature on the scores of best and worst generations and their mean on the dev splits under temperature sampling of varying temperatures for Arena, MGSM, and WMT, respectively. All consider = 5 samples."
        },
        {
            "title": "D Selection Prompts",
            "content": "Listing 1 reports the judge prompt for CHOPS, Listing 2 the one for X-MBR."
        },
        {
            "title": "E Ablations",
            "content": "11https://github.com/Cohere-Labs-Community/m-rewardbench, commit 5e5a0d3 . 26 Language Accuracy arb_Arab ces_Latn deu_Latn ell_Grek fra_Latn heb_Hebr hin_Deva ind_Latn ita_Latn jpn_Jpan kor_Hang nld_Latn pes_Arab pol_Latn por_Latn ron_Latn rus_Cyrl spa_Latn tur_Latn ukr_Cyrl vie_Latn zho_Hans zho_Hant Avg 84.61 83.83 85.08 84.04 85.36 83.62 83.74 84.35 85.90 84.94 83.43 86.32 81.98 84.91 86.30 83.22 84.12 86.55 82.96 83.83 84.49 84.59 84. 84.45 Table 6: Language breakdown of M-RewardBench scores for Command A. E.1 Choosing Single Temperatures One could tune the single temperature, but in practice, resources invested in such tuning might have limited return. For our example, we measure the maximum hope at τ = 0.7 of 48% but it is close to the value at τ = 0.8 and τ = 0.9 of 47% and 45%, thus the effects of choosing one over the other might be negligible. E.2 Token-level hedging with min-p sampling We consider min-p sampling as an additional token-level hedging mechanism during generation. Figure 10 demonstrates that adding min-p consistently improves performance across selection methods compared to only Hedged Sampling. For multilingual win-rates evaluation, min-p provides substantial improvements for both RM BoN and CHOPS, while machine translation tasks show more modest but consistent benefits. E.3 One-Pass Selection Figure 11 compares our one-pass, checklisted selection method (CHOPS) against simpler one-pass selection (OPS) setup that chooses without any grounding checklist. We report the average win-rate delta over greedy decoding for English and non-English languages on the m-ArenaHard dev set in Figure 11a. OPS achieves high win rate in English (9.0%) but drops substantially in non-English 27 Figure 7: Arena: Evaluation score under different temperatures with = 5 samples. (5.3%), whereas CHOPS gives more balanced outcome of 6.8% and 7.1% win-rates over greedy in English and non-English languages respectively. We hypothesize that this shift comes form the fact that generating task-specific checklist before selection helps ground the judges decision process, reducing the effects of the variance in the sample and improving cross-lingual robustness. Notably, in Figure 11b the benefit of check-listed is less pronounced in verifiable tasks where comparison criteria are more explicitly defined. E.4 Sample Size Scaling We compare reward scores for mArenaHard when sampling (τ = 0.7, hedged) from Aya (on mArenaHard dev split) beyond the five times we have focused our experiments on. When reward and selection metric are perfectly aligned, such as in fig. 12, there is potential for improvement up till or even beyond = 40 for all languages. However, given that we are working with imperfect selection methods, that might not be perfectly aligned with the evaluation metric (e.g. MBR Judge vs accuracy), we do not expect these to transfer to realistic use cases. This was established theoretically by Huang et al. [2025] and Stroebl et al. [2024]. The results in Figure 13 illustrates this issue: Win rate improvements over greedy are not developing smoothly across languages, sometimes even dropping below zero, so that it is not always the case that sampling more will result in larger improvement in win rate. We can observe that RM BoN is most reliable in terms of its expected improvement with more samples, likely because it evaluates all samples in isolation and is thereby less affected by sample artifacts that the selection LLM could get misled by in pairwise (MBR) or direct comparisons (CHOPS)."
        },
        {
            "title": "F Evaluation Results",
            "content": "Table 7 shows the breakdown of baseline comparisons on the development set for hedged sampling (τ = 0.7, = 5). Table 8 contains the breakdown into individual languages and tasks for the test set evaluations of hedged sampling (τ = 0.7, = 5). 28 Figure 8: MGSM: Evaluation score under different temperatures with = 5 samples. Figure 9: WMT Evaluation score under different temperatures with = 5 samples. \"\"\" Please act as fair judge . Based on the provided Instruction and Generated Texts in { language } , analyse the Generated Texts step by step and reply with the index of the best response . (1) As first step , write tailored ** evaluation checklist for the Instruction ** that was given to an AI assistant . This evaluation checklist should be list of questions that ask whether or not specific criteria relevant to the Instruction were met by an AI assistants response . Each question in the checklist should measure one distinct aspect of quality . Criteria covered by your checklist could be explicitly stated in the Instruction , or be generally sensible criteria for the problem domain . Factuality is always of high importance , but where relevant , the checklist should also take language and culture - specific context into account . Questions should be concise and the checklist should not include unnecessary entries . Avoid vague wording and instead evaluate specific aspects of response . (2) As second step , ** compare the Generated Texts ** with respect to the checklist . Write brief explanation of the evaluation . (3) In the third step , ** output the index of the best Generated Text ** according to the checklist evaluation . ## Output Format Checklist : ( each question should appear on new line ) Q1 : xxx Q2 : xxx Explanation : xxx Answer : [[ INDEX ]] ( this should be an integer and nothing else ; the index should be enclosed in double brackets as indicated ) ## Evaluation Information ** Instruction ** { message } ** Generated Texts ** { generations } Please analyse the Generated Texts according to custom checklist and provide your selected text according to the guidelines . Remember to stick to the requested Output Format , providing the checklist questions and short explanation and return the index of your selection inside double brackets [[]]. \"\"\" Listing 1: Prompt Used for Checklisted One Pass Selection (CHOPS) \"\"\" Respond to the following instruction in { language_name }. Only return the answer in { language_name }. { prompt } Now Give your response language_name }. \"\"\" following the above instruction ONLY in { Listing 2: Prompt Used for Cross-Lingual MBR (X-MBR) (a) Multilingual win-rates. (b) Machine translation Figure 10: Effect of adding min-p sampling across different selection methods. Min-p provides consistent improvements across both methods and tasks. Results are on Arena (left) and WMT (right) dev splits, using Aya. (a) Win-rate delta on the Arena open-ended benchmark (b) Accuracy on the GSM8K math benchmark Figure 11: Comparison of CHOPS (with checklists) versus OPS (without checklists): (Left) The self-generated checklists improve multilingual performance on open-ended evaluation; (Right) Checklists have negligible effect on close-ended MGSM tasks. Results are averaged across models, using = 5 hedged samples and evaluated on dev splits. Figure 12: BoN Reward score as we increase the sample size from from 1 sample to 40 samples 31 Figure 13: Scaling pool sample size using τ = 0.7 hedged sampling for selected languages with different selection methods on mArenaHard dev set. 32 Model Task Language Likelihood Sim. MBR BoN Judge MBR Arena MGSM WMT Arena Q MGSM WMT Chinese English French German Japanese Russian Spanish Chinese English French German Japanese Russian Spanish Chinese French German Japanese Russian Spanish Chinese English French German Japanese Russian Spanish Chinese English French German Japanese Russian Spanish Chinese French German Japanese Russian Spanish 4.00 -7.60 -15.20 -12.80 0.80 -6.80 -6.00 1.76 13.92 2.16 0.80 -0.32 -0.08 -0. 3.28 0.61 0.42 -0.19 0.43 -0.08 -1.60 -7.20 1.60 -7.60 -6.00 -1.60 -4.00 -1.44 -0.88 -1.20 0.72 1.12 1.28 0.00 0.26 0.14 0.80 0.51 0.68 0.50 -6.40 10.40 0.40 -1.20 -1.20 2.80 3.60 2.96 16.72 2.56 2.00 4.88 0.72 1. 1.06 -0.07 0.39 0.10 -0.07 -0.42 -2.40 0.80 -1.20 7.20 -1.20 -2.40 -1.20 0.96 -0.48 0.40 1.12 1.12 0.48 -0.40 -0.08 -0.28 0.27 0.17 0.26 0.09 5.20 12.00 14.80 17.20 16.40 11.60 4.00 12.16 13.92 4.56 6.40 7.68 7.52 6. 0.92 1.04 0.32 -0.18 1.08 1.49 3.20 0.80 1.60 -4.00 7.20 -1.60 8.80 3.36 2.32 2.00 2.72 5.12 4.48 0.40 1.28 1.17 1.99 1.941 2.27 1.37 4.00 11.20 1.60 6.40 20.80 11.20 4.80 9.36 17.92 4.96 4.80 8.88 3.52 4. -0.07 -0.09 0.04 0.11 0.15 0.08 7.20 3.20 3.20 5.20 4.00 12.40 16.80 2.16 0.32 1.60 1.52 2.72 1.68 0.80 0.53 0.52 1.37 1.61 1.11 0.68 Table 7: Breakdown of dev set results: Baseline Methods Model Task Language CHOPS Judge MBR Reward BoN X-MBR Greedy n Q Arena WMT Chinese English French German Japanese Russian Spanish MGSM Chinese English French German Japanese Russian Spanish Chinese French German Japanese Russian Spanish Chinese English French German Japanese Russian Spanish MGSM Chinese English French German Japanese Russian Spanish Chinese French German Japanese Russian Spanish WMT Arena 18.40 13.60 13.20 11.60 9.20 2.00 24.00 8.56 7.76 9.04 5.12 6.56 6.08 8.96 0.53 0.39 0.17 1.12 0.60 0.28 0.80 7.20 8.00 8.80 8.80 8.00 8.00 3.04 -0.88 2.48 2.56 3.28 1.36 2.48 0.51 1.75 0.87 1.69 2.21 1. 17.20 6.40 7.60 5.20 4.00 -5.20 12.00 7.36 7.36 7.84 5.92 6.16 5.68 8.56 0.10 0.09 0.04 0.65 0.31 0.01 -4.00 4.80 1.20 -2.40 6.40 2.40 4.80 3.04 -0.88 2.08 2.16 3.68 2.56 2.48 -0.29 0.06 -0.43 -0.68 -1.68 -0.36 7.60 5.20 8.40 0.80 -0.40 8.80 5.60 13.96 13.16 13.84 10.32 10.36 11.68 12.96 2.59 2.55 0.86 3.03 2.86 1.68 8.80 12.80 8.40 -0.40 8.80 3.60 16.00 6.44 2.12 4.28 4.56 6.48 3.96 5.28 2.18 2.81 1.56 3.75 3.86 2.24 6.80 13.20 6.80 16.40 10.40 12.40 8.00 12.16 9.36 10.24 6.72 9.36 8.48 9.76 -1.05 -2.14 -1.05 -3.62 -2.73 -0.74 13.60 9.60 18.40 12.00 4.40 12.00 4.80 4.24 -0.08 2.88 4.96 4.88 2.96 4.48 -0.23 0.09 -0.23 -0.60 -1.26 0.26 68.64 77.84 62.96 73.68 68.64 73.12 70.24 76.09 79.91 91.88 78.99 82.51 87.90 83.76 95.68 79.92 87.04 80.72 89.04 86.72 78.42 77.71 89.86 74.36 77.64 85.96 Table 8: Breakdown of devtest set results: Judge based Methods"
        }
    ],
    "affiliations": [
        "Cohere",
        "Cohere Labs"
    ]
}