{
    "paper_title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles",
    "authors": [
        "Zifu Wang",
        "Junyi Zhu",
        "Bo Tang",
        "Zhiyu Li",
        "Feiyu Xiong",
        "Jiaqian Yu",
        "Matthew B. Blaschko"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: \\textit{Firstly,} we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. \\textit{Secondly,} training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. \\textit{Thirdly,} MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. \\textit{Fourthly,} we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. \\textit{Finally,} our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1."
        },
        {
            "title": "Start",
            "content": "Jigsaw-R1: Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles 5 2 0 2 2 ] . [ 2 0 9 5 3 2 . 5 0 5 2 : r Zifu Wang ESAT-PSI, KU Leuven Junyi Zhu ESAT-PSI, KU Leuven Bo Tang University of Science and Technology of China Institute for Advanced Algorithms Research, Shanghai Zhiyu Li Memory Tensor, Shanghai Feiyu Xiong Memory Tensor, Shanghai Jiaqian Yu Samsung R&D Institute China, Beijing Matthew B. Blaschko ESAT-PSI, KU Leuven"
        },
        {
            "title": "Abstract",
            "content": "The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in textonly domains, particularly for perception-heavy tasks. This paper provides comprehensive study of rule-based visual RL, using jigsaw puzzles as structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decisionmaking, making them ideal for this study. Our research reveals several key findings: Firstly, we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. Secondly, training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. Thirdly, MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. Fourthly, we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. Finally, our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1. Equal contribution."
        },
        {
            "title": "1 Introduction",
            "content": "Post-training has emerged as critical step for enhancing the performance of large language models (LLMs). significant contribution in this area is DeepSeek-R1 (Guo et al., 2025), which employs simple yet effective rule-based reinforcement learning (RL) strategy. This approach can mitigate reward hacking (Gao et al., 2023) without relying on traditional scaffolding techniques (Lightman et al., 2024; Wang et al., 2024a; Xie et al., 2024; Xin et al., 2025a), and has shown robust generalization capabilities in LLMs across various domains such as mathematics, coding, common-sense reasoning and logic puzzles (Chen et al., 2025b; Guo et al., 2025; Liu et al., 2025a; Xie et al., 2025b). Despite these advancements, the application of rule-based RL to multimodal contexts is still in its early stages. Unlike purely textual environments such as DeepSeek-R1, multimodal large language models (MLLMs) face the complex challenge of integrating and reasoning over both textual and visual information. This introduces unique difficulties and potential deviations from findings in purely linguistic domains (Gandhi et al., 2025; Guo et al., 2025; Hassid et al., 2025; Lee et al., 2025; Liu et al., 2025d; Marjanović et al., 2025; Xie et al., 2025b). For instance, key insight from DeepSeek-R1 is the models natural achievement of test-time scaling (Snell et al., 2025) through pure RL, evidenced by increased completion lengths and the emergence of complex reasoning patterns (Gandhi et al., 2025), phenomenon termed the aha moment. Nevertheless, perception-heavy tasks, such as spatial reasoning, often permit concise answers derived directly from visual understanding. This contrasts sharply with reasoning-intensive problems like mathematics and coding that benefit from extended, step-by-step reasoning. In such perceptual domains, an explicit, lengthy thinking processcharacteristic of some rule-based RL successes in textmight even prove detrimental (Jiang et al., 2025). This paper presents comprehensive study of rule-based RL within the multimodal domain. Rather than relying on verifiable answers from existing MLLM benchmarks, we revisit classic pretext task in computer vision: solving jigsaw puzzles (Carlucci et al., 2019; Chen et al., 2023; Doersch et al., 2015; Du et al., 2020; Kim et al., 2018; Noroozi & Favaro, 2016). This task (a visual illustration is presented Figure 1) offers compelling testbed for studying rule-based visual RL for several reasons: Firstly, jigsaw puzzles inherently provide ground truth. This allows for the direct generation of rule-based rewards across various visual domains, eliminating the need for expensive human annotation. Secondly, the complexity of these puzzles is readily adjustable by varying the number of pieces, facilitating structured experimental framework. Lastly, solving jigsaw puzzles involves an interplay of step-by-step reasoning and visual perception. The human approachiteratively placing pieces while considering local and global visual coherenceprovides rich analog for the complex decision-making processes we aim to explore in MLLMs. Using jigsaw puzzles as our experimental framework, this research undertakes an in-depth exploration of multifaceted aspects within rule-based visual RL. Our investigation yields findings that address the following key research questions: Research Question #1: How do contemporary MLLMs perform on the classic pretext task of jigsaw puzzles? Without task-specific training, the performance of contemporary MLLMs on the simplest jigsaw puzzles (i.e., 2x1) is comparable to random guessing. However, fine-tuning enables these models to effectively solve such puzzles with near-perfect accuracy. Importantly, these learned abilities generalize to more complex configurations (e.g., 3x1) not encountered during training. Research Question #2: Can MLLMs trained to solve jigsaw puzzles develop generalizable abilities applicable to other visual tasks? Training models on jigsaw puzzles enables generalization to downstream tasks. The effectiveness of this generalization is dependent on specific task configurations, including puzzle size, question type and training dataset. 2 Research Question #3: Given that extended reasoning may be detrimental for some perceptual tasks, is an explicit thinking process still beneficial when employing rule-based visual RL to solve jigsaw puzzles? MLLMs can learn and generalize with or without an explicit reasoning process. However, open-source MLLMs typically show stronger performance in direct answering. As result, even when trained to employ step-by-step reasoning, they tend to disregard the thinking process in deriving the final answer. Research Question #4: Considering that many visual tasks can be solved with concise outputs, does the aha moment still emerge in MLLMs trained on jigsaw puzzles? The aha moment, characterized by the sudden emergence of complex reasoning patterns, is not observed. Instead, these patterns are pre-existing within MLLMs and are readily elicited by tasks with inherent reasoning structures, like jigsaw puzzles. Furthermore, the frequency of these reasoning patterns demonstrably increases throughout training and in response to greater task difficulty. Research Question #5: How does supervised fine-tuning (SFT) compare with RL in terms of generalization? SFT generally demonstrates less effective generalization compared to RL. Besides, initiating training with SFT cold start phase can make later RL optimization less effective."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Jigsaw Puzzles Since their inception, jigsaw puzzles have been closely linked to learning. Around 1760, British cartographer John Spilsbury created the first dissected mapan early jigsaw puzzlespecifically to teach geography (Wikipedia, 2025). More recently, the task of solving jigsaw puzzles has gained considerable attention within the computer vision community as pretext task. The central idea is that neural networks, by training to reassemble images from shuffled patches, can develop rich feature representations transferable to various downstream applications. For instance, Doersch et al. (2015); Noroozi & Favaro (2016) propose pre-training Convolutional Neural Networks (CNNs) on jigsaw puzzles before fine-tuning them for downstream tasks such as image classification and object detection. Building on this, Kim et al. (2018) increase task complexity by integrating jigsaw puzzles with inpainting and colorization, demonstrating transferable representations for image classification and semantic segmentation. Moreover, Carlucci et al. (2019) employ jigsaw puzzles as self-supervised regularization term, achieving strong domain generalization. Du et al. (2020) further advance this area by incorporating jigsaw puzzles into progressive training pipeline for fine-grained classification. Most recently, Chen et al. (2023) investigate the efficacy of jigsaw puzzles in the context of vision transformers (ViTs), highlighting the importance for architecture-specific modifications. To the best of our knowledge, the adoption of jigsaw puzzles as pretext tasks in the era of LLMs remains an unexplored area. 2.2 Rule-based Reinforcement Learning in Textual Domains In order to mitigate reward hacking (Gao et al., 2023), DeepSeek-R1 (Guo et al., 2025) adopts simple yet effective rule-based RL approach. This method diverges from traditional scaffolding techniques such as process reward models (Lightman et al., 2024; Wang et al., 2024a) and Monte Carlo Tree Search (MCTS) (Xie et al., 2024; Xin et al., 2025a), and has proven effective in acquiring reasoning skills transferable across diverse domains, including mathematics, coding, common-sense reasoning and logic puzzles (Chen et al., 2025b; Guo et al., 2025; Liu et al., 2025a; Xie et al., 2025b). Beyond the generalization capabilities, DeepSeek-R1 also demonstrates natural achievement of test-time scaling (Snell et al., 2025), evidenced by increased completion lengths and the sudden emergence of complex reasoning patterns, phenomenon termed the aha moment. However, Hassid et al. (2025); Xie et al. (2025b) find that longer responses do not guarantee better reasoning and Liu et al. (2025d) challenge the notion of 3 sudden emergence, positing that these complex behaviors might be inherent in the base model rather than appearing abruptly. Furthermore, its reasoning processes reportedly exhibit human-like language processing characteristics (Marjanović et al., 2025). Investigating these further, Gandhi et al. (2025) draw connections to human psychology, developing framework that categorizes these reasoning patterns into four key cognitive behaviors. Their findings suggest that Qwen2.5 (Qwen et al., 2025) naturally exhibits these behaviors, whereas Llama3.2 (Grattafiori et al., 2024) initially does not. Adding another dimension to this discussion, Lee et al. (2025) observe that the data format has significantly greater impact on reasoning behavior than the data domain. Their analysis reveals that models trained on multiple-choice data produce concise answers, whereas those trained on free-form data tend to be more verbose."
        },
        {
            "title": "2.3 Rule-based Reinforcement Learning without Labeled Datasets",
            "content": "While effective, rule-based RL often requires extensive human efforts to curate verifiable data. To mitigate this, Xie et al. (2025b) leverage logic puzzle called Knights and Knaves (K&K), which allows for controllable difficulty and algorithmically generated ground truth. They demonstrate that models trained on this synthetic dataset can develop complex reasoning skills transferrable to mathematical tasks. Subsequently, Chen et al. (2025b); Liu et al. (2025a) extend this approach to broader range of textual puzzles, including Sudoku and game of 24. For greater level of autonomy, Zhao et al. (2025a) introduce framework where single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Another stream of research bypasses the need for external annotations by leveraging signals from the models themselves. For instance, Xin et al. (2025b) use the format and length of model outputs as surrogate signals. Shafayat et al. (2025); Wei et al. (2025); Zuo et al. (2025) employ majority voting from model outputs as the supervision signal. Agarwal et al. (2025); Gao et al. (2025); Prabhudesai et al. (2025); Zhang et al. (2025); Zhao et al. (2025b) utilize models own confidence as the intrinsic reward. Perhaps most surprisingly, Shao et al. (2025) discover that even spurious rewards, such as random rewards or incorrect labels, can yield gains nearly matching those from ground truth rewards. Their analysis suggests that RL may not teach models new reasoning capabilities but instead activates latent ones already present in the base model. 2.4 Rule-based Reinforcement Learning in Multimodal Domains Applying rule-based RL to multimodal domains is an emerging field. Unlike models that operate solely on text (e.g., DeepSeek-R1), MLLMs operate in more complex environment by processing both textual and visual information. This inherent complexity introduces unique difficulties and potential deviations from established findings in purely linguistic domains. Recently, several contemporary studies have emerged, reporting varied findings across diverse task settings. significant line of exploration, motivated by the success of DeepSeek-R1 in mathematical reasoning, involves adapting these rule-based RL techniques to multimodal mathematical tasks (Chen et al., 2025a; Deng et al., 2025; Liu et al., 2025b; Meng et al., 2025; Peng et al., 2025; Wang et al., 2025b; Yang et al., 2025). These efforts have consistently demonstrated strong out-of-domain generalization. Notably, studies by Chen et al. (2025a); Meng et al. (2025) have documented the emergence of the aha momentthe spontaneous exhibition of sophisticated reasoning patterns like self-correctionarising from end-to-end RL training. However, the landscape appears different for visual perception tasks, including visual classification, visual grounding and spatial reasoning. While research in this area (Bai et al., 2025b; Chen et al., 2025d; Lai et al., 2025; Li et al., 2025b; Liao et al., 2025; Liu et al., 2025e;c; Shen et al., 2025; Yu et al., 2025a; Zhou et al., 2025) also show robust out-of-distribution generalization, the aha moment has not been observed when employing instruction-tuned models. Indeed, the necessity of explicit reasoning steps for these perception-intensive tasks has been questioned, as direct answers may often suffice (Jiang et al., 2025). In fact, work by Lai et al. (2025); Li et al. (2025b); Yu et al. (2025a) suggest that training models for direct answering often leads to superior performance compared to models trained for explicit step-by-step reasoning. 4 Figure 1: An illustration of the task design."
        },
        {
            "title": "3 Task Design",
            "content": "This section outlines the formulation of jigsaw puzzles in format suitable for processing by MLLMs. Subsequently, we introduce rule-based reward system designed for RL training. All specific prompts are detailed in Appendix E, and Figure 1 provides conceptual overview. 3.1 Jigsaw Puzzle Images The creation of jigsaw puzzles begins with an input image. The image is first partitioned into an mn grid of patches and the tasks difficulty can be readily adjusted by varying the values of and n. Optionally, masked region can be added between patches to highlight the grid layout. If the images height is not perfectly divisible by m, or its width by n, the image is trimmed from the bottom or right edges to ensure its dimensions are exact multiples of the patch count. Subsequently, these patches are randomly shuffled to create the jigsaw puzzle images. To uniquely identify each patchs location within this grid, position indices are assigned sequentially in row-major order, from 1 (top-left) to mn (bottom-right). 3.2 Question Types Based on these shuffled images, we formulate distinct question types. These questions either directly assess the MLLMs ability to reconstruct the original image or require it to reason about the relative positions between the shuffled patches in the initial image. An additional question type, assessing both spatial reasoning and visual grounding, is presented in Appendix B. Full. In this task, MLLMs are required to identify the initial position index for each shuffled patch, thereby enabling the reconstruction of the original image. The answer is list of mn numbers arranged in an mn grid, where each number corresponds to shuffled patch and indicates its original position index. The complexity of this task is therefore mn!. 5 Pair. For this task, two patches are randomly selected, and the MLLMs objective is to identify their relative positions in the original image. If the image is divided into single row (m = 1) or single column (n = 1), only two relative positions are possible (e.g. left/right or top/bottom, respectively). Otherwise, eight distinct relative directions are possible (e.g., top-left, directly above, to the right, bottom-right). This task is structured as multiple-choice question, requiring the model to output single letter corresponding to the correct relative position. Consequently, the task complexity is either 2 or 8, depending on whether the image is divided into single row/column or not."
        },
        {
            "title": "3.3 Thinking or Non-thinking",
            "content": "For any given question, regardless of its type, we investigate two prompting approaches for MLLMs. One approach instructs MLLMs to include an explicit thinking process in their response, similar to the format used in DeepSeek-R1 (Guo et al., 2025). The inclusion of explicit reasoning has been shown to improve generalization across diverse downstream tasks (Hu et al., 2025; Xie et al., 2025b) and is considered valuable for enhancing safety and transparency (Chen et al., 2025f; Wang et al., 2025a). Conversely, as explicit step-by-step reasoning might be detrimental for tasks heavily reliant on visual perception (Jiang et al., 2025), we also explore an alternative: prompting the MLLM to provide the final answer directly, without detailing intermediate reasoning. In summary, we examine the following two distinct instructions: Thinking. MLLMs are instructed to first output their thinking process, which should be enclosed within <think> and </think> tags. Subsequently, they must provide the final answer, enclosed within <answer> and </answer> tags. Non-thinking. MLLMs are prompted to directly output the final answer to the posed question. 3.4 Rule-based Rewards The reward serves as the primary training signal in rule-based RL. Our reward system consists of two components: an accuracy reward and format reward. The total reward is the sum of these two components. Accuracy reward. This reward assesses the correctness of the response. For full questions, the reward is calculated as the proportion of correctly identified position indices to the total number of indices (mn), resulting in fractional value between 0 and 1. For pair questions, the reward is binary: 1 for correct choice and 0 otherwise. Format reward. The final answer must be extractable in the prescribed format: list of mn integers arranged in an mn grid for full questions or single letter for pair questions. With thinking instructions, the answer is extracted from within the <answer> and </answer> tags. For non-thinking instructions, it is extracted directly from the raw output. Furthermore, for thinking, the model must adhere to the instruction of enclosing its reasoning process within <think> and </think> tags and the final answer within <answer> and </answer> tags. Each tag must appear exactly once and in the correct sequence (the thinking process before the final answer). The format reward is 0.5 if the output adheres to all these requirements, and 0 otherwise."
        },
        {
            "title": "4 Experimental Setups",
            "content": "4.1 Datasets COCO (Lin et al., 2014). This dataset serves as the foundation for training and evaluating jigsaw puzzles. We exclusively use the images and randomly generate the ground truth permutations. For training, we employ the train2014 split, and for testing, we randomly select 1,000 images from the test2014 split. CV-Bench (Tong et al., 2024a). This benchmark repurposes standard vision datasets such as COCO with multimodal context, offering 2,638 test examples. It includes four distinct tasks: spatial relationship and object counting for 2D understanding, and depth order and relative distance for 3D understanding. 6 MMVP (Tong et al., 2024b). Similar to CV-Bench, MMVP adapts classic vision datasets like ImageNet (Deng et al., 2009) to create 300 multimodal questions. This benchmark assesses MLLMs on nine fundamental visual patterns, such as orientation, perspective, and structural characteristics. SAT (Ray et al., 2024). This synthetic dataset features indoor scenes, from which we exclusively use its static split. We categorize the original questions into the four task types defined in CV-Bench. For testing, we randomly sample 500 questions per task, yielding total of 2,000 test questions. The remaining 96,924 questions constitute the training set. Super-CLEVR (Li et al., 2023). This is another synthetic dataset containing various vehicle models like cars and motorcycles. Following (Chen et al., 2025d), we select 200 images from the test split and adapt the dataset as counting problems."
        },
        {
            "title": "4.2 Models",
            "content": "Proprietary Models, We evaluate GPT-4.1 (OpenAI, 2025a), GPT-4.1-mini (OpenAI, 2025a), and Claude 3.5 Haiku (Anthropic, 2024). Open-Source Models. We consider Qwen2-VL-2B-Base (Wang et al., 2024b) and several instruction-tuned models: Qwen2.5-VL-72B/7B/3B (Bai et al., 2025a), Qwen2-VL-2B (Wang et al., 2024b), and InternVL2.52B (Chen et al., 2024). 4.3 Implementation Details We use GRPO (Shao et al., 2024) as the reinforcement learning algorithm. The GRPO iteration µ = 1, the KL efficient β = 0.04 and the clipping value ϵ = 0.2. The number of training steps is 1,000 for thinking and 2,000 for non-thinking. In each training step, 64 unique prompts are processed, with each prompt being sampled 8 times to calculate the advantages. The sampling temperature is set to 1, and top-k sampling is used with = 50. The learning rate initiates at 1e-6 and linearly decays to 0."
        },
        {
            "title": "5 Experiments",
            "content": "This section presents results designed to address the proposed research questions. The main paper focuses on instruction-tuned models; for discussion regarding Qwen2-VL-2B-Base, please refer to Appendix C. Research Question #1: How Do MLLMs Perform on Jigsaw Puzzles? To answer the question, we first train models on 2x1 jigsaw puzzles using the training split of the COCO dataset. To introduce task diversity for non-square puzzles, the piece order is randomly shuffled in 50% of instances (e.g. 2x1 becomes 1x2 and vice verse). We then evaluate model performance on the same question type but with varying puzzle sizes, utilizing the test split of the COCO dataset. Evaluation results for pair questions are shown in Table 1, and comprehensive results for full questions are provided in Appendix A. The training dynamics for pair jigsaw puzzles are illustrated in Figure 2. Finding 1.1: MLLMs struggle with jigsaw puzzles before fine-tuning. As demonstrated in Tables 1 and 7, jigsaw puzzles are notably difficult for MLLMs without task-specific training. Prior to fine-tuning, even powerful proprietary models perform at levels comparable to random guessing, struggling even with the simplest jigsaw puzzles (i.e., 2x1). Finding 1.2: MLLMs exhibit efficient learning and generalization for jigsaw puzzles after finetuning. Despite the initial difficulty, MLLMs show strong capacity to learn and solve these puzzles after fine-tuning. For example, the reward progression for Qwen2.5-VL-3B, depicted in Figure 2, indicates rapid convergence to near-perfect accuracy. Notably, models trained exclusively on 2x1 jigsaw puzzles successfully generalize their learned abilities to larger puzzle sizes beyond the training distribution (e.g., 3x1). Takeaways #1. Without task-specific training, modern MLLMs perform no better than random guessing on the simplest jigsaw puzzles (i.e., 2x1). Nevertheless, after fine-tuning, they can solve these puzzles almost perfectly and can generalize the learned abilities to more complex configurations (e.g., 3x1) unseen during training. Table 1: Evaluation results on pair jigsaw puzzles with different sizes. For thinking and non-thinking of the same model, the better result is underlined. Method Random GPT-4.1 GPT-4.1-mini Claude 3.5 Haiku Qwen2.5-VL-72B Qwen2.5-VL-7B + Jigsaw-R1 Qwen2.5-VL-3B + Jigsaw-R1 Qwen2-VL-2B + Jigsaw-R1 InternVL2.5-2B + Jigsaw-R1 Method Random GPT-4.1 GPT-4.1-mini Claude 3.5 Haiku Qwen2.5-VL-72B Qwen2.5-VL-7B + Jigsaw-R1 Qwen2.5-VL-3B + Jigsaw-R1 Qwen2-VL-2B + Jigsaw-R1 InternVL2.5-2B + Jigsaw-R 2x1 50.00 54.10 61.90 61.00 43.40 Thinking 3x1 50.00 53.40 54.50 49.10 50.20 4x1 50.00 54.70 54.80 51.30 52.80 2x2 12.50 20.70 20.30 15.20 18.00 AVG 40.63 45.73 47.88 44.15 41. 49.40 97.8048.40 48.50 96.8048.30 32.80 70.3037.50 44.90 99.3054.40 48.70 61.7013.00 47.50 58.8011.30 33.90 56.5022.60 41.90 63.0021.10 50.60 54.804.20 48.80 52.203.40 32.10 48.1016.00 48.60 53.004.40 15.80 15.200.60 12.20 13.100.90 10.30 10.700.40 9.70 13.704.00 41.12 57.3816.26 39.25 55.2215.97 27.27 46.4019.13 36.28 57.2520.97 Non-thinking 2x1 50.00 53.80 62.50 31.30 52.50 3x1 50.00 49.70 52.70 42.40 51.60 4x1 50.00 50.90 53.90 43.00 55.60 2x2 12.50 16.50 16.20 13.20 14.20 AVG 40.63 42.73 46.32 32.47 43.48 50.40 98.9048.50 52.20 98.8046.60 50.90 98.6047.70 51.00 99.3048. 49.60 65.9016.30 48.30 66.0017.70 53.60 65.0011.40 48.50 63.2014.70 54.20 53.900.30 48.60 53.204.60 46.90 53.506.60 53.50 53.400.10 13.20 14.901.70 13.70 16.803.10 9.60 12.302.70 10.90 12.902.00 41.85 58.4016.55 40.70 58.7018.00 40.25 57.3517.10 40.98 57.2016.22 Research Question #2: How Do Jigsaw Puzzles Generalize to Downstream Tasks? To address this question, we evaluate the performance of models trained on jigsaw puzzles across several downstream tasks, including CV-Bench, MMVP, SAT, and Super-CLEVR. Further details regarding these datasets can be found in Section 4.1. Finding 2.1: Jigsaw puzzles generalize. Our primary investigation  (Table 2)  reveals that models trained on jigsaw puzzles generally achieve improved performance on downstream tasks, indicating robust generalization. Notably, despite being trained exclusively on the COCO dataset for jigsaw puzzles, these models successfully adapt to spatial reasoning tasks on synthetic image datasets such as SAT and SuperCLEVR. Nevertheless, the improvements seen in thinking models might be superficial. They learn to neglect the reasoning process (further discussed in Research Question #3), and their performance after fine-tuning remains inferior to that of non-thinking models that have not undergone fine-tuning. 8 Figure 2: The training dynamics of Jigsaw-R1. Left: Rewards of Qwen2.5-VL-3B (thinking). Middle: Rewards of Qwen2.5-VL-3B (non-thinking). Right: The completion length of various models. All curves are exponentially smoothed for visualization. Table 2: Evaluation results on downstream tasks. For thinking and non-thinking of the same model, the better result is underlined. Thinking Method CV-Bench MMVP GPT-4.1 GPT-4.1-mini Claude 3.5 Haiku Qwen2.5-VL-72B Qwen2.5-VL-7B + Jigsaw-R1 Qwen2.5-VL-3B + Jigsaw-R1 Qwen2-VL-2B + Jigsaw-R1 InternVL2.5-2B + Jigsaw-R1 83.69 84.42 73.38 82.98 64.89 75.9711.08 63.87 69.485.61 51.55 59.367.81 56.02 60.734.71 88.66 82.00 71.33 76. 72.66 77.004.34 61.66 65.003.34 63.33 61.332.00 54.66 63.679.01 SAT 73.70 72.00 59.30 71.00 65.85 69.153.30 57.05 61.954.90 45.75 53.157.40 47.60 56.258.65 Super-CLEVR AVG 52.00 60.50 48.00 72.00 59.00 66.007.00 48.00 47.001.00 55.00 66.0011.00 15.50 46.0030.50 74.52 74.73 63.00 75.57 65.60 72.036.43 57.64 60.863.22 53.91 59.966.05 43.44 56.6613.22 Non-thinking Method CV-Bench MMVP GPT-4.1 GPT-4.1-mini Claude 3.5 Haiku Qwen2.5-VL-72B Qwen2.5-VL-7B + Jigsaw-R1 Qwen2.5-VL-3B + Jigsaw-R1 Qwen2-VL-2B + Jigsaw-R1 InternVL2.5-2B + Jigsaw-R1 81.95 81.46 63.87 82.83 79.87 80.440.57 70.35 73.573.22 64.89 67.402.51 65.84 67.361.52 86.33 80.66 67.00 78. SAT 73.30 69.90 56.90 72.00 78.00 77.670.33 66.00 70.004.00 66.33 66.000.33 66.00 72.006.00 69.55 69.800.25 65.50 65.650.15 61.65 64.703.05 61.50 61.300.20 Super-CLEVR AVG 55.75 65.50 37.00 97.50 92.50 92.50 76.50 83.507.00 72.00 72.500.50 51.00 83.5032.50 74.33 74.38 56.19 82.58 79.98 80.100.12 69.59 73.183.59 66.21 67.651.44 61.09 71.039.94 Finding 2.2: Jigsaw puzzle configuration impacts generalization. To understand the factors influencing generalization, we analyze the performance of Qwen2.5-VL-3B under various jigsaw puzzle configurations: Puzzle size: The choice of jigsaw puzzle size significantly affects downstream performance  (Table 3)  . For the non-thinking setting, training on larger, more challenging jigsaw puzzle size leads to better generalization. 9 Furthermore, employing curriculum learning approach that mixes different puzzle sizes (e.g. 3x14x1) proves more effective than training exclusively with single size. Question type: Pair jigsaw puzzles result in superior generalization on downstream tasks compared to full jigsaw puzzles  (Table 4)  . We believe that this advantage stems from the analogy of pair jigsaw puzzles to downstream tasks (e.g. requiring models to answer multi-choice questions and explicitly asking them to reason about spatial relationships between visual elements). Training dataset: As demonstrated in Table 5, aligning the training dataset with the target domain yields improved performance. For example, training directly on the SAT dataset enhances performance on SAT tasks. Given that jigsaw puzzles are label-free, it is even feasible to train on the test set of SAT for further performance gains. Takeaways #2. Training on jigsaw puzzles can induce generalization to downstream tasks. The degree of generalization is affected by specific task configurations, including puzzle size, question type and training dataset. Table 3: Averaged downstream task performance of Qwen2.5-VL-3B when trained on different jigsaw puzzle sizes. 2x13x1 (3x14x1): Mixing 2x1 and 3x1 (3x1 and 4x1) jigsaw puzzles in curriculum setting. For thinking and non-thinking of the same model, the better result is underlined. Thinking 2x1 60.86 3x1 58.86 4x1 59.05 2x2 58.79 2x13x1 61. Non-thinking 2x1 73.18 3x1 74.95 4x1 73.03 2x2 72.68 3x14x1 75. Table 4: Evaluation results on downstream tasks when Qwen2.5-VL-3B (non-thinking) is trained on different question types. Question Type CV-Bench MMVP SAT Super-CLEVR AVG Full Pair 71.76 73.571.81 69.67 70.000.33 65.20 65.650.45 84.00 83.500.50 72.65 73.180.53 Table 5: Evaluation results on downstream tasks when Qwen2.5-VL-3B (non-thinking) is trained on different datasets. Training Dataset CV-Bench MMVP SAT Super-CLEVR AVG COCOtrain SATtrain SATtrain + SATtest 73.57 72.291.28 72.461. 70.00 68.002.00 68.331.67 65.65 67.001.35 67.401.75 83.50 82.001.50 81.002.50 73.18 72.320.86 72.290.89 Research Question #3: Thinking or Non-thinking? Finding 3.1: MLLMs can learn from jigsaw puzzles and generalize to downstream tasks with or without explicit reasoning. As demonstrated by the jigsaw puzzle results (Tables 1 and 7), models can effectively learn with rule-based visual RL, whether or not an explicit reasoning chain is generated. Importantly, these learned capabilities can be generalized to various downstream tasks  (Table 2)  . 10 Finding 3.2: Open-source MLLMs often benefit from direct answering, while proprietary models tend to perform better with explicit reasoning. Consistent with observations in (Li et al., 2025b; Jiang et al., 2025), our results on jigsaw puzzles (Tables 1 and 7) and downstream tasks  (Table 2)  confirm that open-source models tend to achieve stronger results when prompted to output the answer directly. Indeed, even after fine-tuning, models that adopt reasoning process show weaker generalization on downstream tasks than direct-answering models that have not undergone fine-tuning. Conversely, we find that proprietary models generally demonstrate improved performance when an explicit reasoning process precedes the final answer. It is important to note, however, that this does not necessarily mean proprietary models are inherently stronger. For example, Claude 3.5 Haiku performs comparably to Qwen2.5-VL-3B on downstream tasks. Finding 3.3: MLLMs can neglect the reasoning process after fine-tuning. As illustrated in Figure 2 (Right), the completion length of InternVL2.5-2B significantly decreases during training. This occurs because the model increasingly circumvents step-by-step reasoning and often includes only the final answer in its thinking process (see examples in Appendix G). Conversely, while Qwen models do present explicit reasoning steps, these steps may not consistently inform the derivation of the final answer (examples are provided in Appendix F). To quantify this, we utilized GPT-4.1 to assess the consistency between the reasoning process of Qwen2.5-VL-3B and its final answer. As Figure 3 (Left) demonstrates, although the models final answer becomes more accurate with training, its reasoning chain becomes progressively more inconsistent. Takeaways #3. MLLMs can learn and generalize, irrespective of whether an explicit reasoning process is included. Nevertheless, open-source MLLMs usually excel at direct answering. Consequently, even when trained to utilize step-by-step reasoning, they may ignore the thinking process when deiriving the final answer. Figure 3: Left: Accuracy of the final answer and consistency of the reasoning process during training. Middle: Evolution of keyword frequency throughout the training process. Right: Comparison of keyword frequency when trained on different jigsaw puzzle sizes. All results are demonstrated using Qwen2.5-VL-3B. Research Question #4: Does the Aha Moment Emerge? Finding 4.1: Complex reasoning patterns are pre-existing in MLLMs. The aha moment is often associated with the emergence of complex reasoning patterns Guo et al. (2025). While contemporary studies focusing on perception-heavy tasks like visual classification, visual grounding and spatial reasoning (Bai et al., 2025b; Chen et al., 2025d; Lai et al., 2025; Li et al., 2025b; Liao et al., 2025; Liu et al., 2025e;c; Shen et al., 2025; Yu et al., 2025a; Zhou et al., 2025) typically do not observe these patterns in instruction-tuned models, our investigation into jigsaw puzzles reveals distinct phenomenon. Although the completion length is not increasing, as illustrated in Figure 2 (Right), we find that all these models, including InternVL2.5-2B and also Qwen2-VL-2B-Base (see Appendix C), exhibit complex reasoning patterns, such as verification and backtracking, even before training starts. Indeed, throughout the training process, we successfully identify all four cognitive behaviors as defined in (Gandhi et al., 2025)."
        },
        {
            "title": "Examples of the Four Cognitive Behaviors When Solving Jigsaw Puzzles",
            "content": "Verification: \"Let me check the numbers ...\" Backtracking: \"After re-evaluating the patches, observe the following ...\" Subgoal Setting: \"Lets try to match the descriptions of the patches to ...\" Backward Chaining: \"I can work backwards to find the correct placement ...\" Finding 4.2: Complex reasoning patterns evolve during fine-tuning. To monitor the evolution of these behaviors, we track the frequency of keywords indicative of backtracking and backward chaining (detailed in Appendix D). As depicted in Figure 3 (Middle), the occurrence of these keywords demonstrates steady and significant increase throughout the training process. Finding 4.3: Complex reasoning patterns emerge more frequently with harder jigsaw puzzles. To further investigate these reasoning patterns, we plot the frequency of these keywords as the Qwen2.5-VL-3B model is trained on jigsaw puzzles of varying sizes, as shown in Figure 3 (Right). Our analysis reveals clear trend: the frequency of these keywords increases when the model is trained on more challenging (i.e., larger) jigsaw puzzles. Takeaways #4: Rather than emerging abruptly, complex reasoning patterns are intrinsic within MLLMs. Tasks that inherently require structured reasoning, such as jigsaw puzzles, readily activate these pre-existing patterns. Furthermore, they become demonstrably more prominent both throughout the training process and when MLLMs face more challenging jigsaw puzzles. Research Question #5: SFT or RL? This section evaluates the generalization capabilities of SFT in comparison to RL. For these experiments, SFT data for thinking is curated via rejection sampling, while non-thinking utilizes ground-truth data. Finding 5.1: SFT exhibits weaker generalization than RL. As demonstrated in Table 6, applying SFT to either the reasoning chain (thinking) or directly to ground-truth answers (non-thinking) can yield some generalization. However, it is generally less effective than RL. Finding 5.2: cold start phase with SFT preceding RL can be detrimental. Compared to single-stage RL process, two-stage pipeline that incorporates cold start phase with SFT prior to RL can help models learn specific output formats (Guo et al., 2025). However, this is not essential in our experimental setting, as indicated by the rapid increase in rewards (Figure 2). More importantly, we observe that this cold start phase can diminish the effectiveness of subsequent RL optimization  (Table 6)  . Takeaways #5. SFT typically shows weaker generalization compared to RL. Additionally, initiating the training process with SFT cold start phase may limit the efficacy of subsequent RL optimization."
        },
        {
            "title": "Limitations and Future Work",
            "content": "Visual reasoning models. Recent advancements from OpenAI, particularly the o3 and o4-mini models (OpenAI, 2025b), have shown significant promise in reasoning with images for enhanced perception. While our work does not incorporate these visual reasoning models (Li et al., 2025a; Liu et al., 2025f; Qi et al., 2025; Su et al., 2025; Wang et al., 2025c), we believe jigsaw puzzles are an ideal candidate for exploring rule-based visual RL in this context due to their inherent reliance on image-based reasoning. For an early exploration, we conduct small-scale experiments using the ChatGPT console, where these models are equipped with tool-use capabilities. Our preliminary experiments indicate that OpenAI o3 can effectively solve 2x2 jigsaw puzzles, substantially outperforming other models considered in this paper. However, it still faces challenges with more complex puzzles (e.g. 3x3), highlights areas for further investigation. 12 Table 6: Evaluation results of SFT and RL models on downstream tasks. For thinking and non-thinking of the same model, the better result is underlined. Thinking Method CV-Bench MMVP SAT Super-CLEVR AVG Qwen2.5-VL-3B + Jigsaw-R1 + SFT + SFT + Jigsaw-R1 63.87 69.485.61 66.412.54 68.965.09 61.66 65.003.34 64.002.34 63.662. 57.05 61.954.90 58.851.80 60.053.00 48.00 47.001.00 42.006.00 43.005.00 57.64 60.863.22 57.810.17 58.911.27 Non-thinking Method CV-Bench MMVP SAT Super-CLEVR AVG Qwen2.5-VL-3B + Jigsaw-R1 + SFT + SFT + Jigsaw-R1 70.35 73.573.22 71.270.92 71.040.69 66.00 70.004.00 67.731.73 66.670. 65.50 65.650.15 62.203.30 62.003.50 76.50 83.507.00 76.750.25 80.003.50 69.59 73.183.59 69.480.11 69.920.33 Multimodal generative models. Our study does not consider models capable of both understanding and generating multimodal content (Chen et al., 2025c;e; Hurst et al., 2024; Team, 2024; Wu et al., 2024; Xie et al., 2025a). promising future research direction involves integrating our approach with these advanced models. Enabling them to generate their own inputs could reduce dependence on external datasets like COCO and create an autonomous environment to learn from experience (Silver & Sutton, 2025). Test-time training. We have demonstrated that aligning the training dataset with the target domain can yield enhanced performance. Jigsaw puzzles inherently provide readily available annotations, making them suitable for direct training on the test set during test time. Therefore, exploring the use of jigsaw puzzles as technique for test-time training (Akyurek et al., 2024; Behrouz et al., 2024; Zhu et al., 2024; Zuo et al., 2025) presents an interesting avenue for future work. Other pretext tasks. While this work primarily focuses on jigsaw puzzles as the pretext task, numerous alternatives exist and worth exploration (Gidaris et al., 2018; Gui et al., 2024). In particular, future research could extend our approach to pretext tasks in other modalities, including text (Lan et al., 2020), video (Ahsan et al., 2019; Kim et al., 2019; Wang et al., 2020), audio (Carr et al., 2021), point clouds (Poursaeed et al., 2020), and tabular data (Lee et al., 2024). Other RL algorithms. We exclusively employ GRPO in our current experiments, leaving other promising algorithms such as PPO (Schulman et al., 2017), DPO (Rafailov et al., 2023) and Reinforce++ (Hu, 2025) unexplored. Furthermore, investigating recent advancements and variations of GRPO, including DAPO (Yu et al., 2025b), Dr. GRPO (Liu et al., 2025d), GPG (Chu et al., 2025), and NoisyRollout (Liu et al., 2025b), could offer valuable insights and potential performance gains."
        },
        {
            "title": "Acknowledgements",
            "content": "We acknowledge support from the Research Foundation - Flanders (FWO) through project numbers G0A1319N and S001421N, and funding from the Flemish Government under the Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen programme. We acknowledge LUMI-BE for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through LUMI-BE Regular Access call."
        },
        {
            "title": "References",
            "content": "Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning. arXiv, 2025. Unaiza Ahsan, Rishi Madhok, and Irfan Essa. Video jigsaw: Unsupervised learning of spatiotemporal context for video action recognition. WACV, 2019. Ekin Akyurek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, and Jacob Andreas. The surprising effectiveness of test-time training for few-shot learning. arXiv, 2024. Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku, 2024. https: //www.anthropic.com/news/3-5-models-and-computer-use. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv, 2025a. Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, and Yansong Tang. Univg-r1: Reasoning guided universal visual grounding with reinforcement learning. arXiv, 2025b. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv, 2024. Fabio Carlucci, Antonio DInnocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. CVPR, 2019. Andrew Carr, Quentin Berthet, Mathieu Blondel, Olivier Teboul, and Neil Zeghidour. Self-supervised learning of audio representations from permutations with differentiable ranking. SPL, 2021. Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv, 2025a. Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, Hao Zhou, and Mingxuan Wang. Enigmata: Scaling logical reasoning in large language models with synthetic verifiable puzzles. arXiv, 2025b. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv, 2025c. Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3, 2025d. https://github.com/Deep-Agent/R1-V. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv, 2025e. Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner Fabien Roger Vlad Mikulik, Sam Bowman, Jan Leike Jared Kaplan, et al. Reasoning models dont always say what they think. arXiv, 2025f. Yingyi Chen, Xi Shen, Yahui Liu, Qinghua Tao, and Johan AK Suykens. Jigsaw-vit: Learning jigsaw puzzles in vision transformer. PRL, 2023. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv, 2024. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. ACL, 2024. Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: simple and strong reinforcement learning baseline for model reasoning. arXiv, 2025. 14 Huilin Deng, Ding Zou, Rui Ma, Hongchen Luo, Yang Cao, and Yu Kang. Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning. arXiv, 2025. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. CVPR, 2009. Carl Doersch, Abhinav Gupta, and Alexei Efros. Unsupervised visual representation learning by context prediction. ICCV, 2015. Ruoyi Du, Dongliang Chang, Ayan Kumar Bhunia, Jiyang Xie, Zhanyu Ma, Yi-Zhe Song, and Jun Guo. Fine-grained visual classification via progressive multi-granularity training of jigsaw patches. ECCV, 2020. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv, 2025. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. ICML, 2023. Zitian Gao, Lynx Chen, Joey Zhou, and Bryan Dai. One-shot entropy minimization. arXiv, 2025. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. ICLR, 2018. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv, 2024. Jie Gui, Tuo Chen, Jing Zhang, Qiong Cao, Zhenan Sun, Hao Luo, and Dacheng Tao. survey on self-supervised learning: Algorithms, applications, and future trends. TPAMI, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv, 2025. Michael Hassid, Gabriel Synnaeve, Yossi Adi, and Roy Schwartz. Dont overthink it. preferring shorter thinking chains for improved llm reasoning. arXiv, 2025. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv, 2025. Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, and Junnan Li. Beyond aha!: Toward systematic meta-abilities alignment in large reasoning models. arXiv, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv, 2024. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. ICML, 2025. Dahun Kim, Donghyeon Cho, Donggeun Yoo, and In So Kweon. Learning image representations by completing damaged jigsaw puzzles. WACV, 2018. Dahun Kim, Donghyeon Cho, and In So Kweon. Self-supervised video representation learning with space-time cubic puzzles. AAAI, 2019. Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. arXiv, 2025. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: lite bert for self-supervised learning of language representations. ICLR, 2020. Kyungeun Lee, Ye Seul Sim, Hye-Seung Cho, Moonjung Eo, Suhee Yoon, Sanghyu Yoon, and Woohyung Lim. Binning as pretext task: Improving self-supervised learning in tabular domains. ICML, 2024. 15 Seongyun Lee, Seungone Kim, Minju Seo, Yongrae Jo, Dongyoung Go, Hyeonbin Hwang, Jinho Park, Xiang Yue, Sean Welleck, Graham Neubig, Moontae Lee, and Minjoon Seo. The cot encyclopedia: Analyzing, predicting, and controlling how reasoning model will think. arXiv, 2025. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv, 2025a. Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, and Kaipeng Zhang. Think or not think: study of explicit thinking in rule-based visual reinforcement fine-tuning. arXiv, 2025b. Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning. CVPR, 2023. Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, and Zhijie Deng. Improved visual-spatial reasoning via r1-zero-like training. arXiv, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. ICLR, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. ECCV, 2014. Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, and Junxian He. Synlogic: Synthesizing verifiable reasoning data at scale for learning logical reasoning and beyond. arXiv, 2025a. Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv, 2025b. Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia. Visionreasoner: Unified visual perception and reasoning via reinforcement learning. arXiv, 2025c. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv, 2025d. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv, 2025e. Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual agentic reinforcement fine-tuning. arXiv, 2025f. Sara Vera Marjanović, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han Lù, Nicholas Meade, Dongchan Shin, Amirhossein Kazemnejad, Gaurav Kamath, Marius Mosbach, Karolina Stańczak, and Siva Reddy. Deepseekr1 thoughtology: Lets think about llm reasoning. arXiv, 2025. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv, 2025. Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. ECCV, 2016. OpenAI. Introducing gpt-4.1 in the api, 2025a. https://openai.com/index/gpt-4-1/. OpenAI. Introducing openai o3 and o4-mini, 2025b. https://openai.com/index/ introducing-o3-and-o4-mini. 16 Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv, 2025. Omid Poursaeed, Tianxing Jiang, Han Qiao, Nayun Xu, and Vladimir Kim. Self-supervised learning of point clouds via orientation estimation. 3DV, 2020. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Maximizing confidence alone improves reasoning. arXiv, 2025. Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, and Jie Tang. Cogcom: visual language model with chain-of-manipulations reasoning. ICLR, 2025. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 2023. Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv, 2017. Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, and Andrea Zanette. Can large reasoning models self-train? arXiv, 2025. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, and Luke Zettlemoyer. Spurious rewards: Rethinking training signals in rlvr. arXiv, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv, 2024. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv, 2025. David Silver and Richard Sutton. Welcome to the era of experience, 2025. https://storage.googleapis. com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. ICLR, 2025. Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, and Yu Cheng. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv, 2024. 17 Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. NeurIPS, 2024a. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. CVPR, 2024b. Cheng Wang, Yue Liu, Baolong Li, Duzhen Zhang, Zhongzhi Li, and Junfeng Fang. Safety in large reasoning models: survey. arXiv, 2025a. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv, 2025b. Jiangliu Wang, Jianbo Jiao, and Yun-Hui Liu. Self-supervised video representation learning by pace prediction. ECCV, 2020. Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. ACL, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv, 2024b. Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv, 2025c. Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, and Lichao Sun. Unsupervised post-training for multi-modal llm reasoning via grpo. arXiv, 2025. Wikipedia. Jigsaw puzzle, 2025. https://en.wikipedia.org/wiki/Jigsaw_puzzle. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv, 2024. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. ICLR, 2025a. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv, 2025b. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. NeurIPS Workshop, 2024. Huajian Xin, Z.Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Haowei Zhang, Qihao Zhu, Dejian Yang, Zhibin Gou, Z.F. Wu, Fuli Luo, and Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. ICLR, 2025a. Rihui Xin, Han Liu, Zecheng Wang, Yupeng Zhang, Dianbo Sui, Xiaolin Hu, and Bingning Wang. Surrogate signals from format and length: Reinforcement learning for solving mathematical problems without ground truth answers. arXiv, 2025b. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv, 2025. En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv, 2025a. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv, 2025b. Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv, 2025. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv, 2025a. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv, 2025b. Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros \"aha moment\" in visual reasoning on 2b non-sft model. arXiv, 2025. Junyi Zhu, Shuochen Liu, Yu Yu, Bo Tang, Yibo Yan, Zhiyu Li, Feiyu Xiong, Tong Xu, and Matthew Blaschko. Fastmem: fast memorization of prompt improves context awareness of large language models. EMNLP Findings, 2024. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. Ttrl: Test-time reinforcement learning. arXiv, 2025."
        },
        {
            "title": "A Full Jigsaw Puzzles",
            "content": "Table 7: Evaluation results on full jigsaw puzzles with different sizes. For thinking and non-thinking of the same model, the better result is underlined. Claude 3.5 Haiku fails to output answers in the required grid format. Method Random GPT-4.1 GPT-4.1-mini Claude 3.5 Haiku Qwen2.5-VL-72B Qwen2.5-VL-7B + Jigsaw-R1 Qwen2.5-VL-3B + Jigsaw-R1 Qwen2-VL-2B + Jigsaw-R1 InternVL2.5-2B + Jigsaw-R1 Method Random GPT-4.1 GPT-4.1-mini Claude 3.5 Haiku Qwen2.5-VL-72B Qwen2.5-VL-7B + Jigsaw-R1 Qwen2.5-VL-3B + Jigsaw-R1 Qwen2-VL-2B + Jigsaw-R1 InternVL2.5-2B + Jigsaw-R1 2x1 50.00 79.00 60.80 69.40 53. Thinking 3x1 16.67 27.30 26.60 19.00 20.40 4x1 4.17 8.20 8.40 5.30 5.10 2x2 4.17 6.40 6.20 6.50 5.60 AVG 18.75 30.23 25.50 25.05 21.13 25.60 97.3071.70 45.90 97.2051.30 46.90 98.0051.10 16.00 99.3083. 15.80 31.2015.40 11.30 31.4020.10 5.30 30.6025.30 8.60 30.1021.50 3.10 8.205.10 2.60 7.304.70 0.60 8.407.80 2.90 4.501.60 4.20 7.803.60 3.40 6.503.10 4.00 4.800.80 2.70 0.002.70 12.18 36.1223.94 15.80 35.6019.80 14.20 35.4521.25 7.55 33.4825.93 Non-thinking 3x1 16.67 17.30 18.80 0.00 26. 4x1 4.17 7.00 3.80 0.00 9.20 2x1 50.00 68.50 49.50 0.00 91.70 30.10 99.2069.10 51.60 98.8047.20 11.40 99.0087.60 20.90 99.2078.30 17.40 30.4013.00 16.70 29.7013.00 2.00 32.1030.10 11.30 31.4020.10 1.50 8.607.10 4.40 9.104.70 0.20 7.307.10 3.40 7.103.70 2x2 4.17 7.30 5.60 0.00 8. 0.50 9.008.50 3.70 7.704.00 0.00 3.503.50 2.80 7.304.50 AVG 18.75 25.03 19.43 0.00 33.83 12.38 36.8024.42 19.10 36.3217.22 3.40 35.4732.07 9.60 36.2526."
        },
        {
            "title": "B Box Jigsaw Puzzles",
            "content": "This task evaluates MLLMs ability to reason about spatial relationships between image patches and to visually ground its understanding by identifying the correct bounding box. The process begins by dividing an input image into an mn grid of regions. Each region is assigned unique position index. From each of these mn regions, patch of equal size is randomly selected, resulting in mn equally-sized patches. Next, target position index, (where 1 mn), is randomly chosen. All mn patches are then shuffled. key constraint during shuffling is that the patch originally from position must be relocated to different position. The MLLMs task is to provide the bounding box coordinates for the patch that originally belonged in position i. The accuracy reward is measured by the Intersection over Union (IoU) between the predicted and ground truth bounding boxes. As shown in Table 8, training MLLMs on this task can promote generalization to ScreenSpot Cheng et al. (2024), downstream task requiring visual grounding within Graphical User Interface (GUI) environment. Table 8: Evaluation results on ScreenSpot. For thinking and non-thinking of the same model, the better result is underlined. Thinking Method ScreenSpot Qwen2.5-VL-3B + Jigsaw-R1 57.38 72.4815.10 Non-thinking Method ScreenSpot Qwen2.5-VL-3B + Jigsaw-R1 79.08 81.051.97 21 Qwen2-VL-2B-Base The training dynamics of Qwen2-VL-2B-Base on 2x1 pair jigsaw puzzles are illustrated in Figure 4. While rewards show rapid initial increase, mirroring trends seen in instruction-tuned models, the improvement in accuracy reward is largely superficial. It primarily reflects the model learning to adhere to the specified output format, which allows for the extraction of final answer, rather than indicating genuine enhancement in its capabilities to solve jigsaw puzzles. Subsequently, the accuracy reward stagnates, not surpassing 0.5, which is equivalent to the performance of random guessing. This leads us to hypothesize that jigsaw puzzles present significant challenge for the base model, potentially requiring substantially extended training period to achieve meaningful performance gains. Furthermore, the completion length also shows swift initial growth. This parallels the reward behavior: the model initially tends to provide direct answers but then learns to include the required explicit reasoning process within the specified format, leading to longer outputs. After this adjustment period, the completion length remains relatively stable. Similar to observations with instruction-tuned models, the keywords are already present in the model before training begins (at step 0). This aligns with findings in textual domain (Liu et al., 2025d). Besides, while these keywords appear more often as training progresses, their overall occurrence remains relatively infrequent. Figure 4: The training dynamics of Jigsaw-R1 using Qwen2-VL-2B-Base."
        },
        {
            "title": "D Keywords",
            "content": "A key characteristic in our setting is that MLLMs often describe image content when answering the question. Consequently, many keywords may appear in these descriptions rather than reflecting the cognitive behaviors (e.g., the word \"wait\" in \"two people waiting for trains\"). To address this, after carefully examining model outputs, we select keywords specifically indicative of backtracking and backward chaining. While this targeted selection minimizes false positives, it inherently increases false negatives, resulting in relatively low observed frequency. Specifically, they might appear only once in hundreds of samples."
        },
        {
            "title": "Selected Keywords",
            "content": "Backtracking: recheck, reverify, reevaluate, reexmamine Backward Chaining: work backwards"
        },
        {
            "title": "E Prompts",
            "content": "This section provides prompts for different variants of jigsaw puzzles, using 2x2 puzzle as an example. 2x2 Full Jigsaw Puzzle (thinking) The input image is divided into 2x2 patches that have been randomly permuted from their original positions. Your task is to solve this 2x2 jigsaw puzzle and reconstruct the original image. Consider 2x2 grid, where each number represents position index ranging from 1 (topleft) to 4 (bottom-right): 1 2 3 4 For each patch, determine its correct position index in the original rently at position should belong at position Y, place \"Y\" at position X. image. If patch curFirst, output the thinking process within <think> </think> tags. Then, provide the final answer within <answer> </answer> tags. The final answer should be the position indexes arranged in 2x2 grid. 2x2 Full Jigsaw Puzzle (non-thinking) The input image is divided into 2x2 patches that have been randomly permuted from their original positions. Your task is to solve this 2x2 jigsaw puzzle and reconstruct the original image. Consider 2x2 grid, where each number represents position index ranging from 1 (topleft) to 4 (bottom-right): 1 2 3 4 For each patch, determine its correct position index in the original rently at position should belong at position Y, place \"Y\" at position X. image. If patch curDirectly output the final answer. The final answer should be the position indexes arranged in 2x2 grid. 2x2 Pair Jigsaw Puzzle (thinking) The input image is divided into 2x2 patches that have been randomly permuted from their original positions. Your task is to solve this 2x2 jigsaw puzzle and reconstruct the original image. Consider 2x2 grid, where each number represents position index ranging from 1 (topleft) to 4 (bottom-right): 1 2 3 4 For patches currently at positions 3 and 2, determine their relative position in the original image. Select the correct answer from the following 8 choices: (A) 3 is on the upper right of 2 (B) 3 is on the lower left of 2 (C) 3 is on the upper left of 2 (D) 3 is directly to the right of 2 (E) 3 is directly below 2 (F) 3 is directly above 2 (G) 3 is directly to the left of 2 (H) 3 is on the lower right of 2 First, output the thinking process within <think> </think> tags. Then, provide the final answer within <answer> </answer> tags. The final answer should be single letter. 25 2x2 Pair Jigsaw Puzzle (non-thinking) The input image is divided into 2x2 patches that have been randomly permuted from their original positions. Your task is to solve this 2x2 jigsaw puzzle and reconstruct the original image. Consider 2x2 grid, where each number represents position index ranging from 1 (topleft) to 4 (bottom-right): 1 2 3 4 For patches currently at positions 3 and 2, determine their relative position in the original image. Select the correct answer from the following 8 choices: (A) 3 is on the upper right of 2 (B) 3 is on the lower left of 2 (C) 3 is on the upper left of 2 (D) 3 is directly to the right of 2 (E) 3 is directly below 2 (F) 3 is directly above 2 (G) 3 is directly to the left of 2 (H) 3 is on the lower right of 2 Directly output the final answer. The final answer should be single letter. 2x2 Box Jigsaw Puzzle (thinking) The input image is divided into 2x2 regions. Consider 2x2 grid, where each number represents region index ranging from 1 (top-left) to 4 (bottom-right): 1 2 3 4 Some patches of equal size have been randomly swapped from their original positions, resulting in an unnatural appearance. Your task is to find the original location of the patch that currently belongs in region 4. First, output the thinking process within <think> </think> tags. Then, provide the final answer within <answer> </answer> tags. The final answer should be bounding box coordinates, formatted as integers and separated by comma. 2x2 Box Jigsaw Puzzle (non-thinking) The input image is divided into 2x2 regions. Consider 2x2 grid, where each number represents region index ranging from 1 (top-left) to 4 (bottom-right): 1 2 3 4 Some patches of equal size have been randomly swapped from their original positions, resulting in an unnatural appearance. Your task is to find the original location of the patch that currently belongs in region 4. Directly output the final answer. formatted as integers and separated by comma. The final answer should be bounding box coordinates, Qwen2.5-VL-3B Examples In this section, we present two illustrative examples of Qwen2.5-VL-3B where the models final answer, while correct, is inconsistent with its preceding reasoning process. In the first example, the models reasoning chain indicates that the patch containing rice should be placed on the left, yet it chooses option A, which is indeed the correct outcome. Similarly, in the second instance, the models rationale suggests that patch 2 should be positioned directly above patch 1; nevertheless, it again selects option A, also the correct answer. Qwen2.5-VL-3B Example 1 Prompt: ... Select the correct answer from the following 2 choices: (A) 2 is directly to the left of 1 (B) 2 is directly to the right of 1 ... Output: <think> The image on the left shows bowl of rice with spoon and fork placed next to it. The image on the right shows plate of mixed vegetables in sauce. If we consider the typical layout of meal tray, the rice would typically be served first, followed by the main dish. Therefore, the image on the left should be positioned to the left of the image on the right. </think> <answer>A</answer> Ground truth: 28 Qwen2.5-VL-3B Example 2 Prompt: ... Select the correct answer from the following 2 choices: (A) 2 is directly below 1 (B) 2 is directly above 1 ... Output: <think> The image shows scene of game being played on sandy surface with spectators in the background. The patch labeled \"2\" appears to be the top part of the image, showing the upper body of player and the frisbee in motion. The patch labeled \"1\" seems to be the lower part of the image, showing the lower body of the same player and the ground. Given the layout of the image, part (patch 1). Therefore, the correct answer is that patch 2 is directly above patch 1. </think> it is clear that the top part (patch 2) is above the bottom <answer>A</answer> Ground truth: InternVL2.5-2B Examples This section includes two illustrative examples from InternVL2.5-2B. In both cases, the models thinking process merely contains the final answer, instead of providing detailed step-by-step reasoning. InternVL2.5-2B Example 1 Prompt: ... Select the correct answer from the following 2 choices: (A) 2 is directly to the left of 1 (B) 2 is directly to the right of 1 ... Output: <think> 2 is directly to the left of 1. </think> <answer>A</answer> Ground truth: 30 InternVL2.5-2B Example 2 Prompt: ... Select the correct answer from the following 2 choices: (A) 2 is directly below 1 (B) 2 is directly above 1 ... Output: <think> 2 is directly below 1. </think> <answer>A</answer> Ground truth:"
        }
    ],
    "affiliations": [
        "ESAT-PSI, KU Leuven",
        "Institute for Advanced Algorithms Research, Shanghai",
        "Memory Tensor, Shanghai",
        "Samsung R&D Institute China, Beijing",
        "University of Science and Technology of China"
    ]
}