{
    "paper_title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks",
    "authors": [
        "Yuxiang Zhang",
        "Jiangming Shu",
        "Ye Ma",
        "Xueyuan Lin",
        "Shangxi Wu",
        "Jitao Sang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models face challenges in long-horizon agentic tasks as their constrained memory is easily overwhelmed by distracting or irrelevant context. Existing working memory methods typically rely on external, heuristic mechanisms that are decoupled from the agent's core policy. In this work, we reframe working memory management as a learnable, intrinsic capability. We propose a novel framework, Memory-as-Action, where an agent actively manages its working memory by executing explicit editing operations as part of a unified policy. This formulation allows an agent, trained via reinforcement learning, to balance memory curation against long-term task objectives under given resource constraints. However, such memory editing actions break the standard assumption of a continuously growing prefix in LLM interactions, leading to what we call trajectory fractures. These non-prefix changes disrupt the causal continuity required by standard policy gradient methods, making those methods inapplicable. To address this, we propose a new algorithm, Dynamic Context Policy Optimization, which enables stable end-to-end reinforcement learning by segmenting trajectories at memory action points and applying trajectory-level advantages to the resulting action segments. Our results demonstrate that jointly optimizing for task reasoning and memory management in an end-to-end fashion not only reduces overall computational consumption but also improves task performance, driven by adaptive context curation strategies tailored to the model's intrinsic capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 5 3 6 2 1 . 0 1 5 2 : r MEMORY AS ACTION: AUTONOMOUS CONTEXT CURATION FOR LONG-HORIZON AGENTIC TASKS Yuxiang Zhang1,, Jiangming Shu1, Ye Ma2, Xueyuan Lin2, Shangxi Wu3 & Jitao Sang1 1School of Computer Science and Technology, Beijing Jiaotong University 2Hithink Research 3Huawei Noahs Ark Lab {yuxiangzhang, jiangmingshu, jtsang}@bjtu.edu.cn maye@myhexin.com linxy59@mail2.sysu.edu.cn wushangxi1@huawei.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models face challenges in long-horizon agentic tasks as their constrained memory is easily overwhelmed by distracting or irrelevant context. Existing working memory methods typically rely on external, heuristic mechanisms that are decoupled from the agents core policy. In this work, we reframe working memory management as learnable, intrinsic capability. We propose novel framework, Memory-as-Action, where an agent actively manages its working memory by executing explicit editing operations as part of unified policy. This formulation allows an agent, trained via reinforcement learning, to balance memory curation against long-term task objectives under given resource constraints. However, such memory editing actions break the standard assumption of continuously growing prefix in LLM interactions, leading to what we call trajectory fractures. These non-prefix changes disrupt the causal continuity required by standard policy gradient methods, making those methods inapplicable. To address this, we propose new algorithm, Dynamic Context Policy Optimization, which enables stable end-to-end reinforcement learning by segmenting trajectories at memory action points and applying trajectory-level advantages to the resulting action segments. Our results demonstrate that jointly optimizing for task reasoning and memory management in an end-to-end fashion not only reduces overall computational consumption but also improves task performance, driven by adaptive context curation strategies tailored to the models intrinsic capabilities."
        },
        {
            "title": "INTRODUCTION",
            "content": "For agentic tasks demanding long-horizon reasoning and complex tool use, such as deep research and software engineering agents (Wei et al., 2025; Jimenez et al., 2024), the effectiveness of Large Language Model (LLM) is fundamentally constrained by what it can attend to. We define the substrate for this attention as working memory: the structured sequence of historical observations available within session to drive decision-making. Left unmanaged, however, this working memory can quickly become saturated with irrelevant or outdated information. This accumulation of noise degrades the quality of reasoning and can derail the agent from its primary objective. Therefore, the critical bottleneck for long-horizon tasks shifts from merely expanding memory capacity to actively curating its contents. We term this new meta-task Context Curationa general mechanism whereby the agent learns to autonomously manage its own working memory by strategically selecting, integrating, and pruning information to maintain focused and goal-relevant reasoning trace. Recent advances in long-context methods, enabled by techniques such as positional-encoding scaling, long-context sample synthesis and sparse/linear attention mechanisms, have successfully exCorresponding author. 1 Figure 1: Comparison between MemAct and conventional memory management. The left side illustrates representative design in existing systems, where memory operationssuch as selection, compression, and summarizationare governed by handcrafted heuristics or external controllers. These behaviors remain decoupled from the agents core decision-making process. In contrast, the Memory-as-Action (MemAct) framework integrates such operations into the policy itself, enabling the agent to learn when and how to edit its own working memory as part of unified decision loop. This formulation supports goal-directed, policy-driven memory management. panded the capacity of an agents working memory (Peng et al., 2023; DeepSeek-AI, 2025). However, simply increasing the context window does not guarantee improved reasoning performance. The effectiveness of long-context model is fundamentally determined by Context Engineering, which refers to the deliberate curation and structuring of information to ensure the most relevant evidence is accessible at the right time (Mei et al., 2025). The dominant approach to context engineering today relies on workflow of external, rule-based operations (Packer et al., 2023; Jin et al., 2025; Li et al., 2025). Processes such as information retrieval, content summarization, and sliding-window overflow handling are typically managed by separate controllers or heuristics. This design decouples memory management from the agents core reasoning policy, making it impossible to learn coherent strategy that holistically balances task performance against resource costs in an end-to-end manner (Yu et al., 2025; Zhou et al., 2025). We propose shift in this paradigm towards Context Curation, which targets an intrinsic and learnable capability of the agent itself. Context Curation is challenging because it requires agents to balance task rewards against diverse resource costs (e.g., tokens, latency, tool calls). We propose Memory-as-Action (MemAct), framework that treats context curation as sequence of learnable memory-editing operations. Rather than passively accumulating an ever-growing prefix, the agent learns to decide when to retain, compress, or discard segments of history, and may insert summaries to maintain coherence. These transformations are applied through explicit function-call actions, enabling the agent to develop memory strategies that improve reasoning efficiency (see Fig. 1 for schematic overview). The dynamic nature of these decisions makes them difficult to supervise with static labels, motivating an end-to-end reinforcement learning approach. However, the very flexibility that makes MemAct powerful introduces fundamental challenge: by enabling the agent to edit its history, we break the linear, prefix-accumulating nature of standard LLM trajectories. This disruption of the trajectorys linear structure is fundamental challenge for policy optimization. Standard RL methods for LLMs (Ouyang et al., 2022; Shao et al., 2024) rely on the prefix accumulation property, which dictates that each context is an extension of the previous one, to consistently compute policy gradient loss. Once the history becomes editable, this core assumption no longer holds. Memory actions can overwrite or remove earlier content from the working memory, thereby breaking the causal continuity of the trajectory. Standard RL algorithms for LLMs operate on single, continuous context sequence and are therefore not directly applicable to the trajectories produced by MemAct. To address the problem of trajectory fracture, we further propose Dynamic Context Policy Optimization (DCPO), novel RL learning algorithm that enables stable policy gradient estimation by dynamically segmenting the execution trajectory whenever context curation action occurs. The implementation of DCPO is designed to be compatible with the GRPO (Shao et al., 2024) training pipeline, ensuring ease of integration. In summary, our core contributions are: 2 Framework: We propose the MemAct framework, which redefines context curation as intrinsic, learnable actions. This design enables an agent to use unified policy to balance context management and task execution, achieving end-to-end optimization toward longterm goals. Algorithm: We propose the DCPO algorithm to address the challenge that memory editing disrupts the linear accumulation of context, rendering standard policy gradient methods inapplicable. Evaluation: We demonstrate that MemAct achieves performance competitive with much larger models while operating at substantially lower token cost, learning memory strategies that are both generalizable to new tasks and adaptive to the base models capabilities."
        },
        {
            "title": "2.1 OVERVIEW",
            "content": "We introduce the Memory-as-Action framework, which enables an agent to actively edit its own Importantly, because memory working memory (see Algorithm 2 for the full execution loop). operations are implemented as standard function calls, this design supports recursive memory managementthe agent can manage not only task-relevant content, but also records of prior memory actions, enabling meta-level reflection and refinement. This flexible editability introduces challenge: memory actions can overwrite or remove past context, breaking the common prefix assumption and resulting in trajectory fractures. To address this, we propose policy optimization algorithm, DCPO, which enables stable learning by partitioning histories into causally consistent segments and computing trajectory-level advantages(See Algorithm 1). 2.2 FORMALIZING CONTEXT MANAGEMENT AS DECISION PROCESS We model the agents interaction as Markov Decision Process (MDP), allowing the policy to explicitly edit its working memory while pursuing the primary task. The MDP is defined as follows: State: The state st at timestep is the working memory Ht, structured sequence of historical observations such as user instructions, tool calls, and their outputs. Action Space: = Atask Amem, where Atask contains task-oriented actions that interact with the external environment, and Amem contains memory actions that directly modify the working memory. Transition: Task actions at Atask yield observations ot, appended to history: Ht+1 = Ht (at, ot). Memory actions at Amem transform history: Ht+1 = at(Ht), potentially overwriting prior content and breaking the append-only assumption. Objective: Learn policy πθ that maximizes the expected cumulative reward J(θ). sparse, terminal reward combines final task success with adherence to resource constraints. 2.3 DYNAMIC CONTEXT POLICY OPTIMIZATION 2.3.1 OUTCOME REWARD We employ sparse, terminal reward function R(τ ) assigned based on the final outcome of trajectory τ . The reward is defined as follows: R(τ ) = rtask rpen 0 if the task is successfully completed, if resource constraint is violated, otherwise. Here, rtask > 0 is fixed positive reward for success, and rpen < 0 is fixed penalty. penalty is applied if the agent violates predefined operational constraints, such as exceeding the maximum context length. All other terminal states, such as failing the task without any constraint violations, result in zero reward. This sparse signal is designed to encourage the policy to learn complex behaviors that lead to successful outcomes while respecting resource limits. 3 : Initial policy πθ, prompt dataset D, environment E, trajectories per prompt Ntraj, Algorithm 1: DCPO Training Loop Input segments to sample per prompt Nseg Output: Optimized policy πθ 1 while not converged do Sample batch of prompts Ubatch // Initialize an empty batch for training Amap {} // Initialize map from trajectory to its advantage foreach Ubatch do // Collect trajectories for the current prompt // 1. Rollout Tu for = 1 to Ntraj do Generate trajectory τ from prompt and get its return R(τ ) Append (τ, R(τ )) to Tu // 2. Advantage Estimation group advantages ComputeAdvantages(Tu) Amap.update(group advantages) // 3. Segmentation and Per-Prompt Sampling Σu pool foreach (τ, . . . ) in Tu do // Create local segment pool for prompt Extract memory-action indices {tmem for = 0 to do } from τ ; set tmem 0 = 0, tmem K+1 = Hprefix Htmem tmem i+1 Ygen (yt) t=tmem +1 input ids tokenize(Hprefix) Ygen mσi [0, . . . , 0, 1, . . . , 1] where 0 = tokenize(Hprefix) Append (input ids, mσi, τ.id) to Σu pool Bu Sample(Nseg, Σu pool) Bu // Aggregate segments into the training batch 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 // 4. Policy Update ComputePolicyLoss(B, Amap, πθ) Update policy πθ 25 return Optimized policy πθ 24 2.3.2 TRAJECTORY SEGMENTATION AND SAMPLING As the agent executes memory actions, the standard assumption of continuously growing, appendonly context is broken. This creates what we term trajectory fracturea point where the working memory Ht+1 is no longer simple extension of Ht. Such non-prefix histories pose significant challenge for policy optimization: using mismatched context for gradient computation can lead to incorrect updates and instability. To address this, we introduce trajectory segmentation. Let tmem be the timesteps where memory actions occur, augmented such that tmem 0 = 0 (i.e., the beginning of the trajectory) and tmem K+1 = (i.e., the final timestep). We define segment σi as the subsequence between two consecutive memory actions at tmem and generating new sequence over (tmem i+1 ]. This ensures that the gradient for each token is computed using the exact context under which it was generated. i+1 , sharing common prefix Htmem , . . . , tmem and tmem , tmem 1 i During the training process, for each prompt, we generate Ntraj trajectories and sample Nseg segments from them for training. The ratio Nseg/Ntraj is tunable hyperparameter, whose optimal value depends on the task complexity and the frequency of memory operations. We sample segments using trajectory-based round-robin strategy, drawing one unique segment per trajectory per epoch, and repeating the process until Nseg segments are collected."
        },
        {
            "title": "2.3.3 TRAJECTORY-LEVEL POLICY OPTIMIZATION",
            "content": "To compute the policy gradient, we use the trajectory-level group-normalized advantage estimation method from GRPO (Shao et al., 2024). Each full trajectory τ is assigned normalized advantage: A(τ ) = R(τ ) µu σu , where µu and σu are the mean and standard deviation of returns over all trajectories generated from the same prompt u. The loss is then defined as: L(θ) = EuD 1 G(u) (cid:88) (cid:88) (cid:88) τ G(u) σiΣ(τ ) tσi mσi A(τ ) log πθ(yt Ht) , where Σ(τ ) is the set of sampled segments from trajectory τ , and mσi newly generated tokens within each segment. is binary mask indicating"
        },
        {
            "title": "3 EXPERIMENTS & RESULTS",
            "content": "3.1 DATASETS To ensure comprehensive evaluation of the proposed methods, this study utilizes both synthetic data and public benchmarks across different experimental stages, with each dataset tailored to specific objectives in training, validation, and testing. We created the Multi-objective QA dataset from HotpotQA to evaluate an agents long-range reasoning and memory management. In each task, the agent must answer several independent questions to provide single, consolidated answer. The training data is constrained to simpler tasks with two to four objectives, while evaluation is performed on more complex test sets with up to eight objectives to measure generalization. We also conducted experiments on collection of Multihop QA datasets preprocessed by Asearcher (Gao et al., 2025). This benchmark suite includes 2WikiMultihopQA (Ho et al., 2020), Bamboogle (Press et al., 2022), HotpotQA (Yang et al., 2018), Musique Trivedi et al. (2022), and Frames (Krishna et al., 2024). These datasets provide structured testbed to assess memory management under varying reasoning depths and context complexities. For supervised fine-tuning, we observed that even advanced models such as OpenAI o3, DeepSeekV3.1, and Qwen3-235B were unable to reliably learn memory editing behavior through prompting alone. key failure point lay in their inability to correctly interpret updated working memory states. To address this, we prompted DeepSeek-V3.1 to emulate MemAct-style behavior, thereby generating high-quality trajectory dataset for policy initialization. staged prompting strategy was adopted: memory operations were softly suggested when context lengths ranged between 8K and 16K tokens, and strictly enforced beyond 16K. The resulting dataset includes 700 multi-hop and 100 multi-objective QA instances. From more than 800 successful trajectories, over 3,000 segments were extracted for fine-tuning. For the reinforcement learning phase, we trained the agent on dataset combining 8,000 multi-hop QA examples from Asearcher with 8,000 synthesized multi-objective tasks from HotpotQA. The training distribution was deliberately biased toward lower-complexity tasks (fewer than four objectives) to increase generalization pressure and encourage the emergence of robust memory management strategies. 3.2 BASELINES To assess the effectiveness of MemAct, we compare it against the following three categories of representative baselines: Long-context language models without explicit memory mechanisms. We include Qwen3-235B-A22B-instruct-2507 and Qwen3-30B-A3B-instruct-2507, which represent the state of the art in long-context reasoning. These models leverage large 5 context windows and internal knowledge to perform multi-hop inference without any memory editing capability. Conventional memory management strategies based on external mechanisms. Two representative approaches are implemented: Sliding Window: discards the earliest segments of the interaction history once the context exceeds 8K tokens. Summarization: performs proactive compression when the context length approaches 8K tokens, condensing the oldest half of the history into summary. RL-base agent. We include Search-R1, which is retrained on the same supervised finetuning (SFT) dataset used for MemAct-SFT to ensure comparability. The retrained agent achieves stronger performance than previously reported and serves as competitive reference point. 3.2."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Memory Management Implementation In our implementation, memory management is operationalized via dedicated prune context tool. To enable targeted pruning, every tool calls output is assigned unique, randomly generated ID that serves as handle to its record in the history. When the agent determines that the context needs to be condensed, it invokes the prune context tool with two arguments: model-generated summary synthesizing the key information to be retained, and list of ids to prune corresponding to the historical records to be deleted. The frameworks executor then processes this call by removing the specified records from the active context, while preserving the summary. This effectively replaces detailed, pruned records with concise and useful piece of memory. Cold-start via Segmented Supervised Finetuning For the cold-start initialization of the policy model, we employ Segmented Supervised Finetuning (SFT) phase to adapt it to the MemAct action space, which includes both task-specific tools and our prune context memory action. This phase adopts both the segmentation strategy and the loss masking mechanism from DCPO to guide the models learning process. We perform training for 6 epochs with batch size of 256. The learning rate is set to 5 105 and follows cosine decay schedule with warmup ratio of 0.1. DCPO Hyperparameters For the primary DCPO training phase, we use batch size of 128. As described in Alg. 1, we generate Ntraj = 8 trajectories for each prompt and sample Nseg = 16 segments for each policy update step. The policy is optimized with the AdamW optimizer using learning rate of 1 106. Trajectories are terminated if they exceed maximum of 35 tool-use turns (including both task and memory actions). All experiments were conducted on NVIDIA H100 GPUs. Reward The reward signal is determined by gpt-oss (OpenAI, 2025) based evaluator, which assesses whether the agents final answer is consistent with the ground truth. Following the reward function defined in Section 2.3.1, we set the parameters to rtask = +1.0 and rpen = 0.1. The penalty rpen is applied for execution failures, such as exceeding 20,000 token context limit or producing an unparsable final answer. 3.3 METRICS We evaluate our method on both task success and resource efficiency, assessed by an evaluator based on gpt-oss OpenAI (2025) that judges consistency with the ground truth. Success Metrics. We define two metrics: 1) Accuracy: The percentage of queries where all subobjectives are answered correctly. 2) Per-Objective Accuracy: The average proportion of correctly addressed sub-objectives. Efficiency Metrics. We track two primary costs, excluding internal memory actions: 1) Function Calls: The number of external tool interactions per query. 2) Input Token Cost: The cumulative input tokens fed to the policy model for query, which is reported as Total Tokens per Query (the total sum) and Tokens per Round (the total sum divided by the number of LLM calls). 6 Figure 2: Model performance on the Multi-Objective QA dataset. (a) Per-objective accuracy versus average input tokens per round. The top-left quadrant represents higher accuracy with greater context efficiency. (b) Total input tokens per query versus the average number of function calls, illustrating the end-to-end efficiency of different strategies. Figure 3: Comparative performance on the Multi-Objective QA dataset. The left panel shows task accuracy and the right panel shows the average number of task-related tool calls, broken down by the number of objectives per query. Models are sorted by their overall average accuracy. Memory actions are excluded from the tool call count. 3.4 MAIN RESULT ANALYSIS OF MULTI-OBJECTIVE QA RESULTS The evaluation on the Multi-Objective QA dataset highlights the effectiveness of the MemAct framework in improving both accuracy and robustness. The detailed results are presented in Figure 2, which provides macro-level overview of accuracy-efficiency trade-offs, and Figure 3, which offers granular performance breakdown as task complexity increases. Our MemAct-14B-RL model achieves leading average accuracy of 59.1%, outperforming all baselines, including the much larger Qwen3-235B model. As shown in Figure 3, this performance is consistent across tasks of varying difficulty. Notably, its accuracy exhibits graceful degradation as the number of objectives increases, demonstrating generalization to task complexities unseen during training, as the training set was limited to tasks with maximum of four objectives. This underscores the models robustness. Furthermore, this level of performance is achieved with an average 7 Figure 4: Tool usage by objective count. (a) Average external-tool calls. (b) Average memorymanagement calls. Rows: models; columns: objectives. Independent color scales per panel. context of only 3,447 input tokens per round (Figure 2a), in sharp contrast to the Search-R1-14B agent, which requires substantially larger context (8,625 tokens) for lower accuracy. The detailed breakdown of tool usage in Figure 3 provides insight into different problem-solving strategies. Models with extensive internal knowledge, like Qwen3-235B, consistently use fewer function calls than agent-based models. The heatmap also exposes the failure mode of weaker models like Qwen2.5-7B. Its minimal tool usage correlates with its poor performance, which notably falls below even baseline that answers without using any tools, indicating failure to effectively engage with the tasks procedural requirements. In contrast, agents built upon smaller language models, such as our MemAct variants, engage in more frequent tool interactions to solve the tasks. While the 7B and 14B models exhibit similar behavioral patterns after supervised fine-tuning (SFT), their strategies diverge significantly after RL optimization with the same reward signal, trend suggested by Figure 3. The detailed breakdown of tool usage in Figure 4 further illuminates this divergence, revealing how the learned policies develop different emphases on external tool use versus internal memory management. For the more capable 14B model, RL leads to an efficiency-oriented strategy. As shown in Figure 4a, the MemAct-14B-RL model consistently uses fewer external tools than its SFT counterpart across all levels of difficulty. In contrast, for the smaller 7B model, RL promotes strategy of extending the reasoning process. The model makes more external tool calls than its SFT version at all difficulty levels. Concurrently, its usage of memory-management tools increases to level comparable to the 14B models (Figure 4). This suggests compensatory policy: the model attempts to overcome its limited internal knowledge by gathering more external information for each sub-objective, which in turn necessitates more intensive memory management. Crucially, as shown in Figure 2(b), both of these learned strategies are highly token-efficient. The 14B model becomes more direct without significant token penalty, while the 7B model extends its reasoning at far lower total token cost than the Search-R1-7B baseline. These findings indicate that the MemAct framework does not enforce single, rigid strategy. Instead, it provides the necessary mechanism for reinforcement learning to discover adaptive policies tailored to models intrinsic capabilities. By supporting these varied strategies at low computational cost, our approach eases the trade-off between reasoning depth and efficiency, enabling smaller models to effectively tackle more complex, long-horizon tasks. 8 Table 1: Comparison of different context-management methods on top of the Qwen2.5-14B-Instruct backbone across five multi-hop QA benchmarks. Our MemAct variants are highly competitive with the Search-R1 (Cold-Start) baseline and substantially outperform standard sliding-window techniques. Method 2Wiki Bamboogle HotpotQA Musique Frames Avg. Base + Sliding Window + Sliding Window w/ Summary + Search-R1 w/ Cold-Start + MemAct (SFT) + MemAct (RL) 0.580 0.535 0.540 0.775 0.764 0.767 0.488 0.472 0.442 0. 0.616 0.618 0.655 0.560 0.692 0.723 0.705 0.710 0.233 0.271 0.268 0. 0.330 0.353 0.275 0.215 0.335 0.376 0.359 0.385 0.446 0.411 0.455 0. 0.555 0.567 ROBUSTNESS ACROSS MULTI-HOP QA BENCHMARKS To assess robustness across tasks of varying reasoning complexity, we evaluated our models on five multi-hop QA benchmarks, with results presented in Table 1. The performance of both MemAct and Search-R1 is consistently higher than that of the base instruction-tuned model and standard sliding window methods, suggesting that structured agentic frameworks are beneficial for these tasks. Our final model, MemAct-14B-RL, achieves an average score of 0.567, nearly on par with the Search-R1 baselines score of 0.572. This level of accuracy is achieved with greater token efficiency, as established in our analysis of Figure 2(b), indicating favorable trade-off between task performance and computational cost for our framework. Within our method, we also observe consistent improvement from the SFT-trained model (0.555 avg.) to the final RL-tuned version (0.567 avg.). This gain is most pronounced on the Musique and Frames datasets. possible explanation is that these benchmarks require more complex or longer reasoning chains. The reinforcement learning phase may be particularly effective at refining the agents policy for such long-horizon tasks, optimizing the sequence of tool and memory actions beyond what can be learned from static demonstrations in SFT. TRAINING EFFICIENCY The reduction in token consumption enabled by memory pruning directly translates to improved training efficiency. For our 7B model, employing MemAct within the DCPO framework reduced the duration of the rollout phase by approximately 40% and the policy update phase by 25%. This comparison is against no-memory baseline using identical hyperparameters, highlighting practical benefit of our approach.."
        },
        {
            "title": "4 CONCLUSION",
            "content": "This work introduces Memory-as-Action, framework that treats working memory management as an integral part of an agents decision-making process, rather than as an external module. By formalizing memory operations as explicit actions, single policy can learn to interleave task reasoning with context curation. This approach creates challengethe violation of the prefix assumption in standard policy optimizationwhich we address with Dynamic Context Policy Optimization (DCPO). DCPO enables stable policy gradient training on non-monotonic histories by segmenting trajectories and ensuring correct gradient attribution. Our experiments on multi-objective and multi-hop question answering demonstrate two primary benefits. First, the method improves task success by maintaining focused and relevant context. Second, it reduces computational costs by pruning unhelpful history and adapting tool-use strategies to the base models capabilities. The training pipeline is compatible with existing reinforcement learning practices and improves training efficiency, suggesting that learned memory policies can be viable alternative to brute-force context expansion. In conclusion, the results indicate that end-to-end optimization of reasoning and memory yields agents that are both more effective and more efficient, learning adaptive strategies suited to their underlying capabilities."
        },
        {
            "title": "5 FUTURE WORK AND LIMITATIONS",
            "content": "The results presented here represent our initial findings on the efficacy of the MemAct framework. We are actively conducting further analyses, focusing on the emergent trade-offs between reasoning depth and computational cost, and how learned context management strategies adapt to models intrinsic capabilities. Future work will likely explore more complex memory operations and test the frameworks robustness on broader range of long-horizon tasks. We will continue to update this paper with additional results and analyses as they become available."
        },
        {
            "title": "REFERENCES",
            "content": "DeepSeek-AI. Deepseek-v3.2-exp: Boosting long-context efficiency with deepseek sparse attention, 2025. Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl, 2025. URL https://arxiv.org/abs/2508.07976. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In ICLR, 2024. Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yongkang Wu, Zhonghua Li, Qi Ye, and Zhicheng Dou. Hierarchical document refinement for long-context retrieval-augmented generation. In ACL Long, 2025. Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: unified evaluation of retrievalaugmented generation. arXiv preprint arXiv:2409.12941, 2024. Z. Li et al. Memos: memory os for ai system, 2025. Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, et al. survey of context engineering for large language models. arXiv preprint arXiv:2507.13334, 2025. OpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/ 2508.10925. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Memgpt: Towards llms as operating systems, 2023. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 10 Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, et al. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025. Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents, 2025. URL https://arxiv.org/abs/2506.15841."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 IMPLEMENTATION DETAILS OF MEMACT PSEUDOCODE Algorithm 2: MemAct Agent Execution Loop Input: Initial prompt x, environment E, model policy πθ // M: memory tool module that applies at on Ht1 and outputs (Ht, ot) Output: Final task output ofinal, trajectory τ 1 Initialize working memory H0 [x] 2 Initialize empty trajectory buffer τ 3 for = 1 to Tmax do 4 Sample action at πθ( Ht1) if at Atask then 5 6 7 9 10 11 12 13 Execute at in environment: ot E(at) Update memory: Ht Ht1 (at, ot) else // Execute memory tool, returning both new memory and execution outcome (Ht, ot) M(at, Ht1) // ot contains status indicators (e.g., success flag) Append (at, ot) to memory: Ht Ht (at, ot) Append (at, Ht) to trajectory buffer τ if termination condition met then break 14 Return final output ofinal, full trajectory τ"
        }
    ],
    "affiliations": [
        "Hithink Research",
        "Huawei Noahs Ark Lab",
        "School of Computer Science and Technology, Beijing Jiaotong University"
    ]
}