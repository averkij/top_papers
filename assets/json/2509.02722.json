{
    "paper_title": "Planning with Reasoning using Vision Language World Model",
    "authors": [
        "Delong Chen",
        "Theo Moutakanni",
        "Willy Chung",
        "Yejin Bang",
        "Ziwei Ji",
        "Allen Bolourchi",
        "Pascale Fung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. We introduce the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts a trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and a dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by a critic model that we trained in a self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and our proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 2 2 7 2 0 . 9 0 5 2 : r Planning with Reasoning using Vision Language World Model Delong Chen1, Théo Moutakanni1, Willy Chung1,2, Yejin Bang1, Ziwei Ji1, Allen Bolourchi1,3, Pascale Fung 1Meta FAIR, 2ISIR Sorbonne Université, 3University of Southern California Joint first author Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. We introduce the Vision Language World Model (VLWM), foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by critic model that we trained in self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and our proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark. Date: September 4, 2025 Correspondence: Delong Chen delong@meta.com, Pascale Fung pascalefung@meta.com"
        },
        {
            "title": "1 Introduction",
            "content": "World models enable AI agents to optimize action plans internally instead of relying on exhaustive trial-anderror in real environments (LeCun, 2022; Ha and Schmidhuber, 2018), showing strong performance in planning across low-level, continuous control tasks such as robotic control (Oquab et al., 2023; Yang et al., 2024; Assran et al., 2025; Pan et al., 2025) and autonomous driving (Hu et al., 2023; Wang et al., 2024c). However, learning world models for high-level task planning where actions involve semantic and temporal abstraction (Sutton et al., 1999; Chen et al., 2025) remains an open challenge. Bridging this gap could unlock wide range of practical applications, such as AI agents in wearable devices assisting humans in complex tasks and embodied agents capable of autonomously pursuing long-horizon goals. To obtain high-level world model, existing approaches fall short. Prompting-based practices (Hao et al., 2023; Tang et al., 2024; Wang et al., 2024b; Gu et al., 2024) is straightforward but inadequate as LLMs are not directly grounded in sensory experience. VLMs are primarily trained for visual perception and instead of action-conditioned prediction of world-state transitions. Meanwhile, learning from simulation environments (Hafner et al., 2024; Wang et al., 2025b) cannot scale to divers real-world activities. Existing world models learned from natural videos often rely on generative architectures (e.g., diffusion models) to generate future observations (Yang et al., 2023b; Brooks et al., 2024; Agarwal et al., 2025b). Such formulation is not only ill-posed due to partial observability and uncertainty, but also inefficient, capturing task-irrelevant details and imposing high computational costs for long-horizon roll-outs. These limitations highlight the need for world models that predict in abstract representation spaces, rather than raw pixels. In this work, we propose to learn world model that leverages natural language as its abstract world state representation. We introduce the Vision Language World Model (VLWM), which perceives the environment through visual observations and predicts world evolution using language-based abstraction (Figure 1). Language 1 Figure 1 Example of VLWM action-state trajectory given video observation and goal. VLWM can either generate plan using one roll-out (system-1), or search over multiple actions by inferring the new world states and minimizing given cost function (system-2). inherently provides semantic abstraction and is significantly more computationally efficient to generate compared to raw sensory observations. In comparison with latent embeddings in Joint Embedding Predictive Architecture (JEPA)-based world models (LeCun, 2022; Zhou et al., 2024; Assran et al., 2025), language-based abstraction is intuitive, interpretable, and enables seamless integration with prior knowledge and extensive engineering ecosystems developed for LLMs/VLMs. Compared to current LLMs/VLMs paradigms that primarily focus on perception (Cho et al., 2025), behavior cloning (SFT) (Zeng et al., 2023), or reinforcement learning with verifiable rewards (Shao et al., 2024), we propose to perform direct world modeling as an objective based on massive, uncurated videos, i.e., reward-free offline data (Sobal et al., 2025). An overview of the framework is shown in Figure 2. To construct training prediction targets, VLWM employs an efficient abstraction pipeline that first compresses raw video into hierarchical Tree of Captions, then refines it into structured goal-plan descriptions using an LLM-based Self-Refine (Madaan et al., 2023). The model is trained to predict these abstractionscapturing goal description, goal interpretation, actions (A) and world state changes (S) conditioned on visual context from past observations. From this, both predictive world model (St, At St+1) and an action policy (St At+1) are learned. It enables straightforward plan generation via text completion, using the proposed action directly as policy. We term this approach system-1 planning. However, the autoregressive nature of token decoding limits foresight and reflection, as each action decision become irreversible once made. Additionally, when training on large-scale, real-world video datasets which typically contain imperfect demonstrations, the resulting policy will also clone those suboptimal behaviors present in the data. To unleash the full potential of VLWM, we introduce reflective system-2 planning with reasoning mode. In this mode, VLWM first generates multiple roll-out based on action candidates (either proposed by itself or externally provided) and predicts resulting world states. We then search for the candidate action sequence that minimize scalar cost, which is evaluated by critic module that assess the desirability of candidate plans. This critic is language model trained through self-supervised objective: it learns to assign lower costs to valid progress toward the goal and higher costs to counterfactual or irrelevant actions, effectively measuring how closely each candidate action aligns with the desired goal state. The process of optimizing action plan by searching for cost-minimizing candidate is form of reasoning (LeCun, 2022). It enables the agent to perform trial-and-error internally with its learned world model to obtain the optimal action plans. The VLWM is extensively trained on large corpus of both web instruction videos and egocentric recordings, including COIN (Tang et al., 2019), CrossTask (Zhukov et al., 2019), YouCook2 (Zhou et al., 2018), 2 Figure 2 Overview of VLWM. (a) VLWM is JEPA-style world model that predict abstract representation of future world states, instead of generating noisy and high-volume raw observations. (b) Given video contexts, VLWMs prediction target is structured textual representation of the unobserved future. It includes goal and interleaved action (A) world state changes (S), all extracted automatically. (c) VLWM can infer possible goals from the context, and interpret them with current initial state and the expected final state. It supports both fast reactive system-1 plan generation and reflective system-2 reasoning based on cost minimization. HowTo100M (Miech et al., 2019), Ego4D (Grauman et al., 2022), EgoExo4D (Grauman et al., 2024), EPICKITCHENS-100 (Damen et al., 2018). Collectively, there are 180k videos spanning over 800 days of duration. We generate Tree of Captions for each video, resulting in total of 21M nodes of unique detailed video captions (2.7 trillion words). With iterative LLM Self-Refine, we extracted 1.2 million trajectories of goalplan pairs, consisting of 5.7 million steps of actions and states. We also reformulate text-only chain-of-thought reasoning paths in NaturalReasoning (Yuan et al., 2025) to action-state trajectories, obtaining additional 1.1 million goal-plan pairs. Our evaluations cover both human ratings of plan preference, and quantitative results on the Visual Planning for Assistance (VPA) benchmarks (Patel et al., 2023; Islam et al., 2024), achieving relative gains of +20% in SR, +10% in mAcc, and +4% in mIoU. Based on human ratings with our proposed PlannerArena, the procedural plans generated by VLWM system-2 mode is more preferred than prompting based methods. On the RoboVQA benchmark (Sermanet et al., 2024), VLWM achieves 74.2 BLEU-1 score, outperforming strong VLM baselines. We further evaluate critic models for goal achievement detection, and our trained critic outperform baseline semantic similarity models on both in-domain and OOD scenarios. It also established state-of-the-art on WorldPrediction procedural planning task with 45% accuracy. Models and data will be open-sourced."
        },
        {
            "title": "2 Methodology",
            "content": "We aim to train world model that understands and predicts how actions affect physical world states, and to develop framework for reasoning and planning where the world model serves as the core component. Our approach builds on the agent architecture introduced by LeCun (2022), where reward-agnostic world model perform roll-out given candidate action plans, and the agent evaluates how closely each roll-out advances the current state toward the desired goal, and select the plan that minimizes this distance (i.e., the cost). In the sections below, 2.1 details how we extract structured language-based representation as future world state abstractions, which includes semantic compression techniques for efficiency considerations and quality optimization strategies. Then, 2.2 introduces how the critic is trained to evaluate cost in self-supervised 3 manner and explain the system-2 plan search based on cost-minimization."
        },
        {
            "title": "2.1 Vision-language World Modeling",
            "content": "Given video, we aim to extract structured language representation shown in Fig. 2 (b), which consists of goal (description and interpretation) and procedural plan (action-state sequence). For such video-to-text extraction task, one straightforward approach would be to provide VLM with the full video and prompt it to extract the language representations. However, an impossible triangle arises: within practical compute and memory budget, it is not feasible to simultaneously achieve 1) high spatial resolution for fine-grained perception, 2) long temporal horizon that spans many procedural steps, and 3) the use of large and smart VLM that can follow complex instructions. To address this challenge, we propose two-stage strategy. First, the input video is first compressed into dense Tree of Captions, which significantly reduces the data volume while preserving essential semantic information (2.1.1). Then, structured goal-plan representations are extracted from these captions with LLMs. Because the second stage operates purely on text, it enables efficient processing with large LLMs and allows for iterative quality refinement through Self-Refine (2.1.2)."
        },
        {
            "title": "2.1.1 Compress Video into Tree of Captions",
            "content": "Each Tree of Captions consists of set of video captions generated independently from different local windows of video, collectively forming hierarchical tree structure. It aims to holistically capture both finegrained local details and long-horizon global information (Chen et al., 2024a). key challenge lies in adaptively determining the tree structure, i.e., the arrangement of different levels of windows for caption generation. Ideally, each node or leaf should correspond to coherent monosemantic unit (Chen et al., 2024b), avoiding span across semantic boundaries. Existing temporal action localization and segmentation models (Ding et al., 2023) are limited in their openness, as they rely on human annotations with closed-vocabulary action taxonomies and are typically trained on narrow video domains. We propose to create the tree structure via hierarchical feature clustering. Specifically, let be an untrimmed video, and let its feature stream be represented as = ϕ(X) = [z1; . . . ; zT ] RT d, where each zt is d-dimensional feature vector produced by video encoder ϕ. We segment the feature stream Z, and accordingly the underlying video X, using hierarchical agglomerative clustering (Murtagh and Contreras, 2012). Starting from the finest granularitytreating each item zt as an individual clusterthe algorithm iteratively merges adjacent segments with the smallest increase in within-segment feature variance (i.e., measure of polysemanticity). This merging procedure is continued until there is only single root node, and the full trace gives hierarchical structure, where each node corresponds to segment of the video. The choice of ϕ determines the behavior of the segmentation. In this paper, we adopt the Perception Encoder (Bolya et al., 2025)a state-of-the-art model that excels at extracting scene and action information from videos. Once the hierarchical tree structure is constructed, we generate detailed captions for each video segment, excluding short segments shorter than five seconds. We use PerceptionLM (Bolya et al., 2025) for detailed video captioning. The resulting Tree of Captions achieves substantial compression: for instance, 1.1 TB video files in Ego4D (Grauman et al., 2022) can be compressed to under 900 MB of caption files."
        },
        {
            "title": "2.1.2 Extract Plans with LLM Self-Refine",
            "content": "Given the compressed Tree of Captions extracted from the video, our next objective is to derive structured textual representation that serves as the prediction target for VLWM. This representation includes the following four components: 1. Goal description is high-level summary of the overall achievements (e.g., cook tomato and eggs). In downstream applications, goal descriptions given by users are typically concise (e.g., single sentence), omitting fine-grained details that holistically defines the final state. Therefore, explicit goal interpretations are required. 2. Goal interpretation includes contextual explanations that outlines both the initial and expected final world states. The initial state describes the current status of tools, materials, and dependencies, etc., providing 4 essential grounding for plan generation. The final state interprets the goal description concretely to facilitate cost evaluation in system-2 planning. For example, To achieve the goal, the eggs need to be cooked and mixed with tomatoes, and the mixture should be seasoned appropriately. The eggs should be whisked thoroughly to achieve uniform texture... 3. Action description are the final outputs of the system that will be passed to downstream embodiments for execution or presented to users (e.g., Preheat the skillet on the stove). They must be clear, concise, and sufficiently informative to enable the receiver to understand and produce the intended world state transitions. 4. World states are internal to the system and serve as intermediate representations for reasoning and plan search. They should be information bottleneck: sufficiently capturing all task-relevant consequences of actions while containing minimal redundancy. For example: This action prepares the skillet for cooking the eggs by increasing its temperature. The state of the skillet changes from cold to hot, making it ready for cooking. The oil used for preheating prevents the eggs from sticking to the skillet, ensuring they cook evenly.... See Appendix D.1 for more examples. To ensure that the generated components meet these requirements, we adopt an iterative Self-Refine procedure (Madaan et al., 2023), leveraging LLMs as optimizers (Yang et al., 2023a). We begin by providing the LLM with detailed descriptions of the output requirements, examples of the expected format, and the formatted Tree of Captions as input to generate an initial draft. In each refinement iteration, the LLM first provide feedback to the current draft and produces revised version accordingly. This self-refinement process is repeated for predefined number of iterations, progressively optimizing output quality. To input Tree of Captions to LLMs, we format it using depth-first search (DFS) traversal order. This linearization aligns with the hierarchical structure of textual documents that LLMs are typically trained on and familiar with (e.g., Section 1 1.1 1.1.1 1.1.2 ...). In this paper, we use Llama-4 Maverick for its efficient inference and support for extended context length. Notably, the Self-Refine methodology is not tailored to specific LLM architecture. Below are some example feedback messages generated by Llama-4 Maverick during the Self-Refine process: Prepare the ingredients for Zucchini Curry. in the draft could be broken down into more specific actions like Wash, peel, and chop the zucchini and Chop the onions and tomatoes. The state change after sautéing the onions, ginger, garlic, and green chilies could include more details about how this step affects the overall flavor and texture of the curry. The action of Display the Zucchini Curry in bowl is more of presentational step rather than meaningful action that advances the task progress, so it should be removed from the steps."
        },
        {
            "title": "2.1.3 Training of Vision Language World Model",
            "content": "The training task of VLWM is defined in Eq.1. Here the config acts as system prompts. The context provides environmental information and can be either visual, textual, or both. The VLWM is trained to predict the future, represented by 1) goal description along with its interpretation (i.e., the initial and expected final states), and 2) trajectory consisting of sequence action (A) state (S) pairs. VLWM optimize the cross-entropy loss for next-token prediction on the right-hand side of Eq.1: [config, context] VLWM [goal, interpretation, A0, S0, . . . , AN , SN (cid:125) (cid:124) (cid:123)(cid:122) trajectory ]. (1) This input-output formulation reflects three levels of world modeling: 1) contextual goal inference, the prediction of the possible future achievements, 2) action anticipationproposing possible next actions, and 3) action-conditioned world state dynamics prediction. Since actions and resulting state changes are generated in an interleaved, autoregressive manner, it enables straightforward System-1 Reactive Planning through direct text completion. Given the config, context, and the goal description, VLWM interprets the goal and generates sequence of action-state pairs until an <eos> token is reached. From language modeling perspective, the world state descriptions act as internal chains of thought: they articulate the consequences of each action, 5 Figure 3 System-2 planning of VLWM. (a): the critic is trained in self-supervised manner, assigning lower cost to valid progress, while assigning higher cost for adding irrelevant distractors or shuffling the steps. (b): VLWM generates candidate action sequences and simulates their future state transitions. critic evaluates the resulting state trajectories given the goal, and the planner selects the lowest-cost plan. allowing VLWM to track task progress and suggest appropriate next steps toward the goal. This planning mode is computationally efficient and well-suited for short-horizon, simple, and in-domain tasks. Due to the (video, text) text formulation in Eq.1, pretrained VLM can be used to initialize VLWM. This provides VLWM with strong visual perception, while also enabling it to inherit language understanding and generation capabilities, and commonsense knowledge in LLMs."
        },
        {
            "title": "2.2 Planning with Reasoning",
            "content": "While the System-1 mode allows fast plan generation, it lacks the capabilities of having foresight, evaluating of alternatives, or revising suboptimal decisions. Once an action is emitted, it is fixed, preventing the model from reconsidering or correcting errors. This reactive behavior can lead to error accumulation, particularly in long-horizon or complex tasks. To address these limitations, we introduce System-2 Reflective Planning, where the world model is coupled with critic module that evaluates the desirability of multiple predicted futures given the goal. This would enable reasoning process that involves searching for the optimal plan via cost minimization (LeCun, 2022)."
        },
        {
            "title": "2.2.1 Learning the Critic from Self-supervision",
            "content": "In world model-based planning, the cost function typically quantifies the distance between the world state resulting from candidate plan and the desired goal state (Zhou et al., 2024; Assran et al., 2025). It gives an estimation of how well the current task progress aligns with the intended goal and expected final state. In JEPA world models, this can be directly measured by L1 or L2 distance between fixed-dimensional embedding representations of world states. However, with VLWM, we must measure the semantic distance between language-based world state representations instead calculating distance in token space. Formally, given VLWM predictions as described in Eq. 1, we aim to establish distance function critic that evaluate cost = critic({goal, interpretation}, {trajectory}). Ideally, the cost should be low when the predicted trajectory reflects meaningful progress toward the goal, and high when it deviates due to irrelevant or erroneous actions. To model this behavior, we train language model in self-supervised manner, enabling it to assess the semantic quality of predicted plans without requiring explicit annotations. As shown in Fig. 3(a), we explore two types of self-supervised training signals for the critic: 1. We construct training samples by starting from base partial trajectory and appending either (i) valid next step(s) resulting from coherent continuation of the task, or (ii) distractor step(s) sampled from an unrelated task. The critic independently predicts three cost scores: Cbase, Cgood, and Cbad and the model is trained to satisfy the ranking constraints Cgood < Cbase < Cbad, encouraging the critic to distinguish meaningful progress from irrelevant or misleading continuations. 2. We generate negative samples by randomly shuffling the steps in base trajectory, producing corrupted sequence with cost Cshuffled. The critic is then trained to enforce Cbase < Cshuffled, ensuring sensitivity to procedural order and temporal coherence. 6 The critic is trained to minimize the following ranking loss with fixed margin, supplemented with cost centering regularization term weighted by small constant λ (Naik et al., 2024). To construct training pairs Cpositive, Cnegative, we iterate over all three types of self-supervised signal described above: Cgood, Cbase, Cbase, Cbad, and Cbase, Cshuffled. Lcritic = max (0, margin + Cpositive Cnegative)2 + λ (cid:0)C 2 positive + 2 negative (cid:1) . (2) In addition to VLWM progress data, the critic formulation also supports supervision from external sources to enhance generalization. For example, preference tuning datasets-comprising triplets of query, preferred (chosen) response, and rejected responsecan be directly leveraged. Similarly, since the critic aims to model semantic distance, it can benefit from triplet-based datasets designed for learning sentence embeddings. These sources provide additional positive/negative pairs that can be used to further augment the training data of the critic."
        },
        {
            "title": "2.2.2 System-2 Planning by Cost Minimization",
            "content": "System-2 planning involves the coordination of three components: the VLWM, the critic, and an actor. As illustrated in Fig. 3(b), the actor proposes candidate action sequences, the VLWM simulates their effects, and the critic evaluates their costs. The final plan is selected by identifying the candidate sequence with the lowest predicted cost. The actor can be instantiated either as the VLWM itself or as an external module (e.g., LLMs), particularly in cases where additional constraints on the action space or output format must be respected. The actor may vary the number of proposed candidates to control the search width or generate partial plans to enable more efficient tree search. In additional to the cost evaluated by the critic, task-specific penalties or guard-rails can be incorporated into the cost function, allowing the planner to respect external constraints, safety rules, or domain-specific preferences."
        },
        {
            "title": "3.1.1 VLWM-8B",
            "content": "Sources of Videos. As summarized in Table. 1, the training videos for vision-language world modeling are sourced from two primary domains: 1) Web instruction videos: COIN (Tang et al., 2019), CrossTask (Zhukov et al., 2019), YouCook2 (Zhou et al., 2018), and subset of HowTo100M (Miech et al., 2019) videos. These videos cover diverse range of tasks, and provide clean expert demonstrations. 2) Egocentric recordings: EPIC-KITCHENS-100 (Damen et al., 2022) and EgoExo4D (Grauman et al., 2024). These videos feature continuous, uncut recordings in realistic wearable agent scenarios. For all datasets, we collect videos from their training split. While Ego4D (Grauman et al., 2022) is available as large-scale egocentric recordings dataset, we excluded it from training data to avoid potential overlap with benchmarks due to inconsistent train/val splitting. Generation of Vision-language World Modeling Data. We use Perception Encoder PE-G14 (Bolya et al., 2025) and PerceptionLM-3B (Cho et al., 2025) (320320 spatial resolution, 32 frames per input can be fit in 32GB V100) to generate the Tree of captions. We sample up to 5 target window per video according to the tree structure (the first 5 nodes in BFS traversal order), and use Llama-4 Maverick (mixture of 128 experts, 17B activated and 400B total parameters, FP8 precision) to extract plans from the window with the sub-tree of captions and two rounds of Self-Refine. Additional speech transcripts for web videos and the expert commentary in EgoExo4D are provided along with video captions to improve LLMs video understanding during plan extraction. In addition to video-based extraction, we repurposed the NaturalReasoning (Yuan et al., 2025) dataset to world modeling by replacing Tree of captions with the chain-of-thoughts. Action-state trajectories are extracted by LLM Self-Refine with similar prompts. 7 Table 1 Statics of VLWM data. Vision-language world modeling data are extracted by generating Tree of Captions from videos and performing iterative LLM self-Refine. We combine six video sources and one text-only dataset. Domain Additional Info Dataset # Videos (k) Duration (hours) # Trajectories (k) # Steps (k) Text-only N/A"
        },
        {
            "title": "ASR\nTranscripts",
            "content": "HowTo100M COIN CrossTask YouCook"
        },
        {
            "title": "Egocentric\nRecordings",
            "content": "Expert Commentary N/A EgoExo4D EPIC-KITHCNES-"
        },
        {
            "title": "Overall",
            "content": "- 167.8 7.6 2.1 1.2 0.6 0.5 179.8 - 18,512.3 302.9 163.1 102. 53.5 68.9 1,086.4 1,093.2 36.2 10.4 5.8 3.1 2.2 5,166.2 5,438.1 181.7 55.1 31. 18.8 14.0 19,202.9 2,179.6 10,604.3 Training Details. We use PerceptionLM-8B (Cho et al., 2025) to initialize our VLWM. The model is trained with batch size of 128 and maximum of 11.5k token context length. We perform uniform sampling of 32 frames in 4482 resolution for visual context inputs. With 12 nodes of 8H100 GPUs, the training takes approximately 5 days."
        },
        {
            "title": "3.1.2 VLWM-critic-1B",
            "content": "Data. We generate paired data according to 2.2.1 from vision-language world modeling data of HowTo100M and NaturalReasoning. We also include Tree of Captions data by by sampling subtrees and use root as goal and leafs as trajectories. We also incorporate off-the-shelf preference modeling data to train the critic, where the user queries are treated as goals and model responses are treated as actions. We derive Cpositive, Cnegative using <query + chosen and query + rejected>. We include UltraFeedback (Cui et al., 2023), Orca DPO pairs (Lian et al., 2023), Math-Step-DPO (Lai et al., 2024) as sources of preference data. Lastly, we incorporate training data for learning semantic similarity, where we convert triplets of <query, positive sentence, negative sentence> sentences to query as goal, positive sentence as positive action and negative sentence as negative action. This type of data includes MS-MARCO (Bajaj et al., 2016), SQUAD (Rajpurkar et al., 2016), HotPotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), and FEVER (Thorne et al., 2018). Training Details. The critic model is initialized from Llama-3.2-1B and trained for one epoch with batch size of 128 (2.7k steps), maximum context length of 1536 tokens using single node of 8H100 GPUs. For hyper-parameters in Eq. 2, we set λ=0.01 and margin=1."
        },
        {
            "title": "3.2.1 VPA Benchmarks",
            "content": "To verify that VLWMs large-scale pre-training yields practical gains in procedural planning, we adopt the Visual Planning for Assistance (VPA) benchmark (Patel et al., 2023). VPA measures how well model can predict the next high-level steps of an ongoing activity given the video history and an explicit textual goal. We follow the standard evaluation horizons = 3 and = 4. Experiments are conducted on two widely used instructional-video corpora for procedual planning. COIN (Tang et al., 2019) contains 11 827 videos spanning 180 tasks, whereas CrossTask (Zhukov et al., 2019) comprises 2750 videos across 18 tasks. We adhere to the official train/val/test splits so results are directly comparable to prior work. We benchmark VLWM against four state-of-the-art planners: DDN (Chang et al., 2020), LTA (Grauman et al., 2022), VLaMP (Patel et al., 2023), and VidAssist (Islam et al., 2024), plus two frequency-based heuristics: Most-Probable (global action frequencies) and Most-Probable w/ Goal (task-conditioned frequencies). VLWM is fine-tuned on the VPA training splits of COIN and CrossTask using the same hyper-parameters as in pre-training. Following prior work, we report Success Rate (SR), Mean Accuracy (mAcc), and Mean IoU (mIoU) over the predicted step sequence, respectively measuring plan-level accuracy, step-level accuracy, and action proposal accuracy. Table 2 confirms that VLWM sets new state-of-the-art on the VPA benchmark. Across both COIN and 8 Table 2 Visual Planning for Assistance performances comparison against our finetuned VLWM."
        },
        {
            "title": "Model",
            "content": "COIN T=3 COIN T=4 CrossTask T=3 CrossTask T="
        },
        {
            "title": "SR mAcc mIoU SR mAcc mIoU SR mAcc mIoU SR mAcc mIoU",
            "content": "Most Probable Most Probable w/ goal DDN (Chang et al., 2020) LTA (Grauman et al., 2022) VLaMP (Patel et al., 2023) VidAssist (Islam et al., 2024) 4.3 1.6 10.9 18.0 10.1 22.3 6.8 24.9 32.2 - - - 18.3 39.2 56.6 21.8 44.4 64.4 15.3 32.2 37.3 - 8.2 1.6 9.1 16.3 7.0 21.0 - - 9.0 35.2 54.2 13.8 38.3 66.3 9.9 6.1 1.7 15.5 2.4 8.9 35.2 6.8 25.8 35.2 2.4 24.0 10.3 35.3 44.0 12.0 36.7 48.9 1.3 5.5 1.5 7.9 3.6 24.1 1.2 21.7 4.4 31.7 7.4 31.9 VLWM-8B (ours) 27.9 50.1 69.3 19.4 46.7 74. 13.5 36.4 48.3 7.2 33.6 13.9 20.5 37.0 36.8 43.4 51.6 51.1 CrossTask, and at both horizons = 3 and = 4, our model consistently outperform existing baselines. Compared to VidAssit which adopts 70B LLM, our VLWM is much smaller (8B) while achieving superior results on 8/12 metrics. Averaged over the four settings, VLWM delivers absolute gains of +3.2% in SR, +3.9% in mAcc, and +2.9 points in mIoU."
        },
        {
            "title": "3.2.2 Human Evaluation with PlannerArena",
            "content": "Traditional benchmarks for embedded AI assistants generating human-oriented plans are inadequate as they rely on biased or low-quality ground truth data, failing to capture real-world performance and human assistance. To overcome this, we created PlannerArena, human evaluation framework inspired by ChatbotArena (Chiang et al., 2024). This Arena/Elo-based system involves human evaluators choosing the better plan from those generated by different anonymous models, pairwise outcomes are converted to Elo scores and model win rates. This approach aligns closely with the actual use case of AI assistants, ensuring the models we develop are not only theoretically sound but also practically valuable in the real world. Our experimental setup includes three dataset (COIN, CrossTask and EgoExo4D), in which we compare VLWM with search over 20 plans guided by 8B critic that is minimizing the cost of generated plan (VLWM System-2) and 8B critic that is maximizing cost, against leading multimodal LLMs and ground truth plans. The pairs are sampled uniformly across every possible battle configuration to have balanced number of battles across models. The models start with an initial rating of 1000 and we use an Elo K-factor of 32 for the score updates after each battle. Five different annotators participated in the PlannerArena evaluation evaluating total of 550 battle pairs, with three annotators running fixed pilot run of 90 samples to calculate inter-annotator agreement score. Additional details about PlannerArena can be found in Appendix. We show the final Elo scores of different models in Fig 4 as well as the win rate of each model per dataset. VLWM System-2 has the highest Elo by large margin at 1261, with Llama-4-Maverick being the second most preferred model at an Elo of 1099. Despite using critic which maximizes cost, the plans generated Figure 4 Illustration of PlannerArena annotation interface. Table 3 PlannerArena results. Overall Elo score of our finetuned VLWM with cost minimizing critic (VLWM System-2) and VLWM with cost maximizing critic, compared to other multimodal LLMs and ground truth plans, as well as the win rate percentage of the different model on the three datasets (COIN, CrossTask and EgoExo4D) used for PlannerArena. We highlight in bold the best result score and underline the second best one Model # Parameters Overall Win Rate (%) Elo Score COIN CrossTask EgoExo4D VLWM System-2 Llama-4-Maverick VLWM System-1* Qwen2.5VL Ground Truth Plan PerceptionLM 8B VLWM + 1B critic 400B 8B VLWM 72B - 8B 1261 1099 992 974 952 721 87.9 66.7 34.3 38.2 43.6 33.3 70.6 89.6 37.0 34.8 42.2 27.0 87.9 57.1 50.0 18.3 69.5 14.8 by VLWM Cost-maximizing (992 elo score) are still generally preferred over the ground truth and plans generated by Qwen2.5 and PerceptionLM, which struggle more to generate meaningful plans given video context. Importantly, we see that the quality of ground truth is bad overall and has strong variation across datasets. EgoExo4D have higher quality annotations, where the ground truth plans yield the second highest win rate with 69.5% behind VLWM System-2 with 87.9%. However, in COIN and CrossTask, the ground truth plans are barely better than the worst performing models, respectively 43.6% and 42.2%, highlighting an major issue with current procedural planning datasets."
        },
        {
            "title": "3.3 RoboVQA",
            "content": "To further assess VLWMs capabilities in grounded high-level reasoning and planning, we evaluate it on the RoboVQA benchmark (Sermanet et al. (2024)). RoboVQA challenges models to perform robotics-focused visual question answering in realistic, multi-embodiment settings, requiring understanding of complex visual scenes and executing coherent action sequences. This benchmark complements the procedural planning evaluations by testing VLWMs ability to guide robotic agents effectively. We follow the standard evaluation protocols of RoboVQA and compare VLWMs performance using BLEU scores. We compare our model against state-of-the-art robotic LLMs: 3D-VLA-4B (Zhen et al., 2024), RoboMamba-3B (Liu et al.), PhysVLM-3B (Zhou et al., 2025), RoboBrain-7B (Ji et al., 2025), ThinkVLA-3B and ThinkAct (Huang et al., 2025). Table 4 RoboVQA BLEU-1 comparison against VLWM. *: results from Ji et al. (2025). : results from Zhou et al. (2025). Model BLEU-1 PerceptionLM-8B (Cho et al., 2025) Qwen2-VL-7B* (Wang et al., 2024a) GPT-4V* LLaVA-OV-7B* (Li et al., 2024a) 3D-VLA-4B (Zhen et al., 2024) RoboMamba-3B (Liu et al.) PhysVLM-3B (Zhou et al., 2025) ThinkVLA-3B (Huang et al., 2025) ThinkAct (Huang et al., 2025) RoboBrain-7B* (Ji et al., 2025) VLWM-8B (ours) 14.2 33.2 32.2 38.1 48.3 54.9 65.3 62.4 69.1 72.1 74.2 Table 4 demonstrates that VLWM achieves highly competitive performance on the RoboVQA benchmark. Despite not being specialized on robotic data like some of the top-performing models such as RoboBrain, VLWM attains strong BLEU scores across all n-gram levels, ranking within the top two models. Notably, VLWM achieves the highest BLEU-4 score of 55.6, surpassing RoboBrains 55.1, and closely follows it on BLEU-1 to BLEU-3. These results highlight VLWMs robust generalization and its ability to effectively integrate visual and language information for grounded reasoning and planning in embodied settings."
        },
        {
            "title": "3.4 Critic Evaluations",
            "content": "In this section, we conduct intrinsic evaluations of the critic model independently of VLWM-8B roll-outs to assess whether it exhibits the intended behavior."
        },
        {
            "title": "3.4.1 Goal Achievement Detection",
            "content": "Task Definition. Given goal and trajectory composed of concatenation of Ngold steps of reference plan that achieves the goal, and Ndistractor irrelevant steps appended after, the task asks the critic model to independently evaluate costs for every partial progress from the beginning, i.e., C1 = critic(goal, trajectory[0 : 1]), C1 = critic(goal, trajectory[0 : 2]), . . . , until CNgold+Ndistractor = critic(goal, trajectory[0 : Nbase + Ndistractor]). Since the distance to the goal should be the lowest after Ngold steps of reference plan, we calculate the goal achievement detection accuracy according to whether Ngold = arg min[C1, ..., CNgold+Ndistractor ]. Datasets. We construct testing sample from two sources. 1) Vision-language World Modeling (VLWM): 4,410 action-state trajectories extracted with Tree of Captions and Self-Refine. The goal field combines both goal description and goal interpretation. Since VLWM-critic-1B is trained on HowTo100M trajectories, we exclude it and only sample data from other sources of instruction videos (COIN, CrossTask, YouCook2), and egocentric recordings (EgoExo4D, EPIC-KITCHENS-100). 2) Open Grounded Planning (OGP): Guo et al. (2024) released collection of planning dataset containing goal-plan pairs sourced from different domains. We only use their robot subsets sourced from VirtualHoom and SayCan and WikiHow subset, since plans in the tool usage subset often contain too few number of steps. Different from VLWM data, trajectories in OGP only contain actions, and are OOD for both VLWM-critic-1B and baseline models. There are only 9,983 trajectories in OGP data. 10 Table 5 Goal achievement detection benchmark results. VLWM-Instruct subset shares the same distribution of VLWM-critic-1Bs HowTo100M training data. VLWM-Ego contains EgoExo4D and EPIC-KITCHENS100 data, which is unseen by our critic. Open Grounded Planning (OGP) provides action-only trajectories, and is OOD to our critic. Model VLWM OGP Instruct Ego Overall Robot WikiHow Overall Chance Performance Qwen3-Embedding-0.6B Qwen3-Embedding-4B Qwen3-Embedding-8B Qwen3-Reranker-0.6B Qwen3-Reranker-4B Qwen3-Reranker-8B 8.9 65.0 61.1 67.8 59.0 55.8 68.0 8.6 55.2 54.6 62.3 55.2 46.1 65. 8.8 62.5 59.4 66.2 57.9 53.8 67.3 12.3 30.9 32.1 29.4 43.4 55.7 65. 12.2 22.6 25.0 36.4 34.6 33.7 48.3 12.1 22.7 24.7 35.4 35.1 34.5 49. Table 6 Ablation of goal and trajectory representation. We ablate goal interpretation or world state change descriptions from VLWM-critic1Bs input. Both of them leads to consistent performance reduction across all subsets, and the drop is more significant on the Ego subset, showing the effectiveness of interpretation and states in facilitating generalization. Dataset Default Instruct Ego COIN CrossTask YouCook EgoExo4D EK-100 97.1 98.8 99.2 95.2 90.1 w/o Interp. w/o states 96.4 (-0.7) 98.5 (-0.3) 99.1 (-0.1) 91.4 (-5.7) 92.9 (-5.9) 94.5 (-4.7) 94.0 (-1.2) 88.6 (-1.5) 82.2 (-13.0) 64.0 (-26.1) VLWM-critic-1B 98.4 92. 96.9 72.9 48.3 50.0 Overall 96. 96.3 (-0.6) 88.1 (-8.8) Main Results. We compare VLWM-critic-1B with Qwen3-Embedding models and Qwen3-Reranker models (Zhang et al., 2025) as baselines, which are state-of-the-art models for measuring semantic similarity. the cost is computed as = simgoal, trajectory. Results are shown in Table 5. Our VLWM-critic-1B outperform baselines on most subsets by large margin. VLWM-critic-1B gives 98.4% on VLWM-Instruct while lower 92.7% on VLWM-Ego. This is probably caused by domain gap: our critic is only trained on HowTo100M instruction videos without seeing any egocentric recording data. On OGP, our critic shows clear advantage over the best performing baseline Qwen3-Reranker-8B (72.9% vs 65.6%), but performs comparably with it on OGP-WikiHow (despite having 8 fewer parameters). Possible reasons of this smaller gap includes data noise or potential overlap between Qwen3-Rerankers training data. In Figure 5, we visualize the normalized cost curves predicted by different critic models. The visualization can be viewed as energy landscape, and the desired shape is to have the minimum cost at the 100% goal achievement point. On VLWM data, VLWM-ciritc-1B gives much cleaner landscape compared to baselines. However, when comes to OGP datasets, the distribution becomes nosier. Despite domain gap and dataset noise problem mentioned above, one potential reasoning of performance degradation is the OGP gives action-only trajectory without any explicit world state descriptions, which makes cost evaluation harder. Ablation Studies. Table 6 provides an ablation of critic input representation using VLWM-critic-1B and the VLWM data. We tried to remove the goal interpretations which contains descriptions of current and expected final goal state, and state descriptions from the trajectory representation and leave actions only. We see both ablation leads to performance reduction on goal achievement detection, and the reduction on unseen OOD Figure 5 Cost curves estimated by different critic models. Each plot visualizes 3k cost curves on goal achievement detection trajectories, where each trajectory is composed of reference gold plan (0%-100%) and distractor steps (100%-200%). Red dots () mark cost-minimizing steps (detected goal achievement points). VLWM-Critic accurately detects goal completion around 100% plan length, while baselines show suboptimal or noisy behavior. 11 data (the Ego subset) is more severe, showing the importance of interpretation and world state description for effective generalization."
        },
        {
            "title": "3.4.2 Procedural Planning on WorldPrediction-PP",
            "content": "The WorldPrediction benchmark (Chen et al., 2025) is designed to evaluate high-level world modeling and procedural planning capabilities. Its procedural planning subset, WorldPrediction-PP, comprises 570 human verified samples. Each test case provides initial and final visual states alongside four candidate action plans, represented by video sequences. The task is to identify the correctly ordered sequence among shuffled counterfactual distractors, emphasizing the capability of goal-conditioned planning as well as models understanding of semantic and temporal action order. To evaluate our critic modules on WorldPrediction-PP, we followed the evaluation protocol for Socratic LLMs in (Chen et al., 2025). Visual inputs were first converted into textual descriptions using captions generated by Qwen2.5-VL. Specifically, two images depicting initial and final states produced goal description outlining the changes of world states, and video clips of candidate actions were similarly captioned. These textual inputs were provided directly to our VLWM-critic models to compute costs for each candidate plan, selecting the option with the lowest predicted cost. In Figure 6 (b), we compare our VLWM-critic models against baseline Socratic LLMs. Our models achieve Pareto-optimal balance of model size and accuracy. Importantly, this evaluation constitutes zero-shot scenario for VLWM-critic models, as neither the change captioning-based goal descriptions nor the detailed video captions as action steps were part of the training corpus. Figure 6 WorldPrediction-PP results. Our VLWM-critic-1B established new SoTA of 45.4% accuracy."
        },
        {
            "title": "4.1 Action Planning",
            "content": "Planning is the task to generate sequence of actions that can transit the world from initial state to desired goal state. Our VLWM focuses on planning high-level actions, which is characterized by semantic and temporal abstraction (Sutton et al., 1999; Chen et al., 2025), as opposed to the low-level, high-frequency continuous actions in autonomous driving (Teng et al., 2023), robotics (Yu et al., 2020), and games (Mnih et al., 2015; Brockman et al., 2016), etc. Below, we compare existing methodologies for action planning. Imitation learning (also known as behavior cloning) is effective when extensive expert demonstrations are available (Torabi et al., 2019; Baker et al., 2022). However, it becomes considerably more challenging when demonstrations are scarce or imperfect (Wu et al., 2019; Sagheb and Losey, 2025). For procedural planning and VPA tasks based on instructional videos (Chang et al., 2020; Patel et al., 2023), most approaches rely fundamentally on behavior cloning. Since the action annotations (Tang et al., 2019; Zhukov et al., 2019) are confined to limited vocabularies, the ground truth plans are frequently incomplete, making them not only suboptimal reference for benchmarking (which motivates our PlannerArena in 3.2.2), but also inadequate for imitation learning. Reinforcement learning typically requires environments where agents can perform trial-and-error and receive explicit rewards. When environments support such interactions, reinforcement learning verifiable rewards (RLVR) is highly effective (Guo et al., 2025). Although RL is well-suited for domains where constructing simulation environments is viable, scaling RL to more diverse and complex domains is less feasible. Planning with reward-agnostic world model. This approach exhibits superior generalization by learning from extensive, reward-free offline data (Sobal et al., 2025; Zhou et al., 2024; Assran et al., 2025). World models enable planning by simulating action outcomes internally and optimizing plans based on cost minimization. Unlike methods that predict task-specific rewards (i.e., model-based RL (Hafner et al., 2024)), here world models only predict future world states (Ha and Schmidhuber, 2018), and action plans are optimized by 12 minimizing the distance between the predicted resulting state and the desired goal state (LeCun, 2022). It allows inference-time scaling by conducting internal trial-and-error within the learned world model. Our VLWMs system 2 planning with reasoning leverages this paradigm, and we proved that it outperforms reactive system-1 behavior cloning."
        },
        {
            "title": "4.2 World Modeling",
            "content": "World models aim to simulate environmental dynamics, enabling agents to optimize the plan without direct online interaction with the real environment. They have demonstrated success primarily in low-level control domains, such as autonomous driving (Wang et al., 2024c; Gao et al., 2024; Li et al., 2024b) and robotics (Zhou et al., 2024), where models predict fine-grained, continuous sensory data over short horizons. Below, we compare existing world modeling approaches. Generative world models typically utilize powerful diffusion-based architectures to reconstruct future observations directly (e.g., in pixel space). Examples include Sora (Brooks et al., 2024), Cosmos (Agarwal et al., 2025a), Genie (Bruce et al., 2024; Parker-Holder et al., 2024) and UniSim (Yang et al., 2024), and recent multimodal chain-of-thought reasoning i.e., thinking with images models (Su et al., 2025). While intuitive, generative models inherently suffer from computational inefficiency and task-irrelevant details entangled in pixel-based representations, severely limiting their scalability for long-horizon planning. While these models generate realistic visuals, they have shown limited success in planning tasks. JEPA world models encode observations into compact abstract representations, with predictor trained to forecast these latent states. JEPA models have proven beneficial in representation learning, demonstrated by I-JEPA (Assran et al., 2023), IWM (Garrido et al., 2024), and V-JEPA (Bardes et al., 2024), and have facilitated MPC-based planning, exemplified by DINO-WM (Zhou et al., 2024), V-JEPA2 (Assran et al., 2025), and NWM (Bar et al., 2025). However, joint training of encoders and predictors poses challenges, notably the need for anti-collapse techniques such as EMA. Moreover, existing JEPA-based world models predominantly focus on low-level motion planning, and extending them to high-level action planning remains an open research challenge. Language-based world models exploit natural language as high-level abstraction interface, offering interpretability and computational advantages over pixel-based reconstruction. Prior work has explored prompting LLMs as world models (Hao et al., 2023; Tang et al., 2024; Wang et al., 2024b; Gu et al., 2024) or training language-based world model in narrowed domains, such as web navigation (Chae et al., 2024), text games (Lin et al., 2023; Wu et al., 2025), and in embodied environment (Wang et al., 2025a). In contrast, our VLWM approach explicitly learns world model directly from large-scale raw video data."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced the Vision Language World Model (VLWM), foundation model that learns to represent and predict world dynamics directly in language space, enabling interpretable and efficient high-level planning. By compressing raw videos into hierarchical Trees of Captions and refining them into structured trajectories of goals, actions, and world state changes, VLWM bridges the gap between perception-driven VLMs and reasoning-oriented LLMs. Its dual-mode design supports both fast, reactive System-1 planning through direct policy decoding and reflective System-2 planning via cost minimization guided by selfsupervised critic, which allows the model to internally perform trial-and-error reasoning and select optimal plans. Trained on large and diverse corpus of instructional and egocentric videos, VLWM establishes new state-of-the-art results on the Visual Planning for Assistance benchmark, demonstrates superior plan quality in PlannerArena human preference evaluations, and achieves top-tier performance on RoboVQA, all while producing interpretable action-state rollouts. Furthermore, the critic model independently excels in goal achievement detection and procedural planning benchmarks, highlighting the value of explicit semantic cost modeling for world-model-based reasoning. Taken together, these contributions show that by learning directly from large-scale natural videos and predicting in abstract, non-generative representation spaces rather than raw pixels, Vision Language World Model (VLWM) can provide powerful interface for bridging perception, reasoning, and planning, pushing AI assistants beyond imitation toward reflective agents capable of robust, long-horizon decision making."
        },
        {
            "title": "References",
            "content": "Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025a. Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025b. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1561915629, 2023. Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:2463924654, 2022. Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1579115801, 2025. Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. Transactions on Machine Learning Research, 2024. Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, et al. Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181, 2025. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. https://openai.com/research/video-generation-models-as-world-simulators. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232, 2024. Chien-Yi Chang, De-An Huang, Danfei Xu, Ehsan Adeli, Li Fei-Fei, and Juan Carlos Niebles. Procedure planning in instructional videos. In European Conference on Computer Vision, pages 334350. Springer, 2020. Delong Chen, Samuel Cahyawijaya, Etsuko Ishii, Ho Shu Chan, Yejin Bang, and Pascale Fung. What makes for good image captions? arXiv preprint arXiv:2405.00485, 2024a. Delong Chen, Samuel Cahyawijaya, Jianfeng Liu, Baoyuan Wang, and Pascale Fung. Subobject-level image tokenization. arXiv preprint arXiv:2402.14327, 2024b. Delong Chen, Willy Chung, Yejin Bang, Ziwei Ji, and Pascale Fung. Worldprediction: benchmark for high-level world modeling and long-horizon procedural planning. arXiv preprint arXiv:2506.04363, 2025. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. 14 Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, et al. Perceptionlm: Open-access data and models for detailed visual understanding. arXiv preprint arXiv:2504.13180, 2025. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback: Boosting language models with scaled ai feedback. arXiv preprint arXiv:2310.01377, 2023. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epic-kitchens dataset. In European Conference on Computer Vision (ECCV), 2018. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. International Journal of Computer Vision, pages 123, 2022. Guodong Ding, Fadime Sener, and Angela Yao. Temporal action segmentation: An analysis of modern techniques. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(2):10111030, 2023. Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Quentin Garrido, Mahmoud Assran, Nicolas Ballas, Adrien Bardes, Laurent Najman, and Yann LeCun. Learning and leveraging world models in visual representation learning. arXiv preprint arXiv:2403.00504, 2024. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, et al. Is your llm secretly world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Shiguang Guo, Ziliang Deng, Hongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun. Open grounded planning: Challenges and benchmark construction. arXiv preprint arXiv:2406.02903, 2024. David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models, 2024. https://arxiv.org/abs/2301.04104. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, 2023. Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. Thinkact: Vision-languageaction reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815, 2025. Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Fu-Jen Chu, Kris Kitani, Gedas Bertasius, and Xitong Yang. Propose, assess, search: Harnessing llms for goal-oriented planning in instructional videos. In European Conference on Computer Vision, pages 436452. Springer, 2024. Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, and 15 Shanghang Zhang. Robobrain: unified brain model for robotic manipulation from abstract to concrete, 2025. https://arxiv.org/abs/2502.21257. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. 2022. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024a. https://arxiv.org/abs/2408.03326. Qifeng Li, Xiaosong Jia, Shaobo Wang, and Junchi Yan. Think2drive: Efficient reinforcement learning by thinking with latent world model for autonomous driving (in carla-v2). In European Conference on Computer Vision, pages 142158. Springer, 2024b. Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\". Openorca: An open dataset of gpt augmented flan reasoning traces. https://https://huggingface.co/datasets/Open-Orca/OpenOrca, 2023. Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, and Anca Dragan. Learning to model the world with language. arXiv preprint arXiv:2308.01399, 2023. Jiaming Liu, Mengzhen Liu, Zhenyu Wang, Pengju An, Xiaoqi Li, Kaichen Zhou, Senqiao Yang, Renrui Zhang, Yandong Guo, and Shanghang Zhang. Robomamba: Efficient vision-language-action model for robotic reasoning and manipulation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26302640, 2019. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529533, 2015. Fionn Murtagh and Pedro Contreras. Algorithms for hierarchical clustering: an overview. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 2(1):8697, 2012. Abhishek Naik, Yi Wan, Manan Tomar, and Richard Sutton. Reward centering. arXiv preprint arXiv:2405.09999, 2024. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Minting Pan, Yitao Zheng, Jiajian Li, Yunbo Wang, and Xiaokang Yang. Video-enhanced offline reinforcement learning: model-based approach. arXiv preprint arXiv:2505.06482, 2025. Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rocktäschel. Genie 2: large-scale foundation world model. 2024. https: //deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/. Dhruvesh Patel, Hamid Eghbalzadeh, Nitin Kamra, Michael Louis Iuzzolino, Unnat Jain, and Ruta Desai. Pretrained language models as visual planners for human assistance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1530215314, 2023. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. 16 Shahabedin Sagheb and Dylan Losey. Counterfactual behavior cloning: Offline imitation learning from imperfect human demonstrations. arXiv preprint arXiv:2505.10760, 2025. Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil Joshi, et al. Robovqa: Multimodal long-horizon reasoning for robotics. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 645652. IEEE, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Vlad Sobal, Wancong Zhang, Kynghyun Cho, Randall Balestriero, Tim GJ Rudner, and Yann LeCun. Learning from reward-free offline data: case for planning with latent dynamics models. arXiv preprint arXiv:2502.14819, 2025. Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. Richard Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181211, 1999. Hao Tang, Darren Key, and Kevin Ellis. Worldcoder, model-based llm agent: Building world models by writing code and interacting with the environment. Advances in Neural Information Processing Systems, 37:7014870212, 2024. Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12071216, 2019. Siyu Teng, Xuemin Hu, Peng Deng, Bai Li, Yuchen Li, Yunfeng Ai, Dongsheng Yang, Lingxi Li, Zhe Xuanyuan, Fenghua Zhu, et al. Motion planning for autonomous driving: The state of the art and future perspectives. IEEE Transactions on Intelligent Vehicles, 8(6):36923711, 2023. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018. Faraz Torabi, Garrett Warnell, and Peter Stone. Recent advances in imitation learning from observation. arXiv preprint arXiv:1905.13566, 2019. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Ruoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan, Marc-Alexandre Côté, Peter Clark, and Peter Jansen. Can language models serve as text-based world simulators? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 117, 2024b. Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai, Jinlan Fu, and Xipeng Qiu. World modeling makes better planner: Dual preference optimization for embodied task planning. arXiv preprint arXiv:2503.10480, 2025a. Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai, Jinlan Fu, and Xipeng Qiu. World modeling makes better planner: Dual preference optimization for embodied task planning. CoRR, abs/2503.10480, 2025b. doi: 10.48550/ARXIV.2503.10480. https://doi.org/10.48550/arXiv.2503.10480. Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview In Proceedings of the IEEE/CVF visual forecasting and planning with world model for autonomous driving. Conference on Computer Vision and Pattern Recognition, pages 1474914759, 2024c. Jialong Wu, Shaofeng Yin, Ningya Feng, and Mingsheng Long. Rlvr-world: Training world models with reinforcement learning. arXiv preprint arXiv:2505.13934, 2025. Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama. Imitation learning from imperfect demonstration. In International Conference on Machine Learning, pages 68186827. PMLR, 2019. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023a. 17 Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 1(2):6, 2023b. Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. In The Twelfth International Conference on Learning Representations, 2024. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 10941100. PMLR, 2020. Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong Tian, Jason Weston, et al. Naturalreasoning: Reasoning in the wild with 2.8 challenging questions. arXiv preprint arXiv:2502.13124, 2025. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model, 2024. https://arxiv.org/abs/2403.09631. Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pre-trained visual features enable zero-shot planning. arXiv preprint arXiv:2411.04983, 2024. Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Weijie Zhou, Manli Tao, Chaoyang Zhao, Haiyun Guo, Honghui Dong, Ming Tang, and Jinqiao Wang. Physvlm: Enabling visual language models to understand robotic physical reachability, 2025. https://arxiv.org/abs/2503. 08481. Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Cross-task weakly supervised learning from instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35373545, 2019."
        },
        {
            "title": "A PlannerArena Details",
            "content": "A.1 Instructions & data To evaluate model-generated plans, we conducted controlled human evaluation study using custom-built streamlit application. Annotators were presented with (i) short video context, (ii) textual goal (e.g., Make fish curry), and (iii) two alternative plans generated by different anonymous models. The task is to select the preferred plan to achieve the goal given the provided video context. The instruction shown to annotators is:"
        },
        {
            "title": "PlannerArena Instruction",
            "content": "You will see short video that sets the context, then see goal sentence. Two alternative plans (Plan and Plan B) generated by model are shown. Your job is to select the plan you would prefer to follow in order to achieve the stated goal within the given video context. The evaluation setup is based on three datasets commonly used for procedural video planning and understanding: COIN, CrossTask, and EgoExo4D. For all datasets, the video context given to the annotators is the entire original video truncated right before the start of the first annotated step in order to prevent models from leveraging future visual information in their plan. This is similar to the Visual Planning for human Assistance (VPA) setup, but in order to evaluate human plan preference. For EgoExo4D, the exo point of view is given as video context to prevent any partial observation problems. We generate candidate plans with the other VLMs with zero-shot prompting, all models are provided with the same video context and were prompted with the following template: You are provided with context segment of procedural video about {goal_formatted}. Generate the remaining actions (steps) to take from that context segment in order to reach the goal. The plan should be composed of high-level descriptions starting with verb, and it should be clear and concise, including all essential information. There is no need to be overly descriptive. Generate only the action steps. A.2 Pairs sampling & IAA Unlike ChatbotArena which relies on an Elo-based sampling method to balance the evaluation across large number of models, we adopt uniform uniform sampling strategy as we only have six models to compare. Specifically, we first sample an equal number of battle pairs from each dataset, then enforce balanced participation across models such that each model competed equally against others within each dataset. setup is defined as (dataset, model pair) combination, and each setup is represented equally in the sample pool, yielding 3500 unique battle setups for PlannerArena. Five annotators participated in the study. Prior to annotation, they completed short warm-up consisting of five solved examples to familiarize themselves with the task. Inter-annotator agreement is computed over shared subset of 100 samples with three annotators: the Fleiss was 0.63, indicating substantial agreement, with raw agreement percentage of 72.22%. A.3 Example 19 Figure 7 PlannerArena interface. The sample shown here is from COIN, Plan from the ground truth annotations and Plan from Llama 4."
        },
        {
            "title": "B Prompts",
            "content": "B.1 Meta Prompt for LLM Self-Refine {TREE OF CAPTIONS} {ADDITIONAL VIDEO INFO} # Draft Here is draft for structured data extraction: {PREVISOUS DRAFT} --- # Your task You carefully examine the draft above and identify problems. The requirements are listed below. Go through each point one by one and discuss aspects in the draft that doesnt meet the requirements. Be specific and constructive, avoid vague and generic comments or simply repeating the requirements. Quote the draft for detailed discussion. Provide concrete points of explicit actionable revisions that could help improve and enrich the draft. Make sure your revisions and the added information is grounded in the provided content. After extensive analysis and discussion, give the revision in the desired format. {REQUIREMENTS OF PALN EXTRACTION} # Output format yaml discussion: - Free form chain-of-thought reasoning: analyze the draft, identify problems, and suggest actionable revisions or enrichments. plan: - action: <action description> state: - <world state change description and discussion> start: xx.xx # float between <min_start> and <max_end> round to two decimal digits end: xx.xx # float between <min_start> and <max_end> round to two decimal digits - action: <action description> state: - ... goal: <goal description> interpretation: - <detailed goal interpretation> Start your response with \"yamln...\" and end with \"n\" B.2 Requirements of Plan Extraction for LLM Self-Refine **Action Plan** 1. Identify sequence of physical actions that meaningfully advance the task progress; Omit vague, redundant, or purely presentational steps. 2. Each action is one informative imperative sentence said from the actors perspective. Avoid describing actions from the tutors or demonstrators voice. 3. Infer the span of each action according the provided timestamps. They must fall within <min_start> and <max_end> and do not overlap with each other. 4. Be selective - time in the video may be non-linear. For example, the final result may appear at the beginning of the video. Such actions should be skipped. **World State** 1. Explain how the action is performed according to the provided captions. Use imperative voice and instructional or tutoral style. 21 2. Provide elaborated discussion of the motivation, rationale, and purpose behind the action. 3. Discuss all relevant objects (can be both physical object or abstract concept, or the actor itself) whose states are changed by the action. 4. Cover various aspects, such as status, position, condition, temperature, etc. Highlight the causal relationship between actions and states. 5. Be logically coherent and semantically connected with neighboring steps. They are autoregressive and shouldnt conflict with each other. 6. Provide in-depth analysis. Perspectives may include (but are not limited to): * Implications of state changes * What the change enables; whether it is (or is not) ready for future steps * Whether and how the change advances or contributes to the overall goal * Whether it satisfy the desired final state for the activity, if not what is still required 7. Organize the discussion into single coherent paragraph, it should be comprehensive and detailed, but also avoid redundency and ensure readibility. **Goal Identification** 1. Summarize the overall achievements by the actions during <min_start> to <max_end> (not the entire video). 2. Ensure comprehensive coverage. Feel free to use multiple sentences if appropriate. 3. Use imperative voice. But it should not be simple concatenation of individual action names. 4. It summarize WHAT is achived (e.g., aggregation and abstraction of state changes) but not HOW it is achieved (e.g., \"do by doing y\"). **Goal Interpretation** 1. Infer and describe the initial state of the environment before any action is taken. Only describe task-relevant aspects. Start with \"Now, ...\" 2. Interpret the goal in detail by discussing objects needs to be what state such that goal can be considered achieved. 3. Start with \"To achieve the goal, ...\". You can also include related technical specifications if applicable. 4. Description of the desired world state should be grounded in the provided context and aggregate all the state changes caused by the actions. 5. Discuss all objects, tools, materials, dependencies, etc. needed or invovled in the action steps and explain the functional rationale. 6. Use the tone as if you are now at the starting time of the video (<min_start>) and tasked to plan towards the given goal. You are preparing by thinking and analyzing the task. 7. Provide one paragraph and ensure its coherence and readibility. Importantly, you should avoid the leakage of any action plan information in this section. **Overall Requirements** 1. Maintain faithfulness to the provided video content; Do not hallucinate or infer based on commonsense knowledge. 2. The output must strictly follow the given YAML format. Timestamps should be in the same format as <min_start> and <max_end>. 3. Except for the start and end times of the action, dont mention exact timestamp anywhere in your output. 4. Dont use the video / the segment in any part of the output. Instead, refer to the actions, objects, and environements directly. 5. Use specific functional description when referring to objects. Ignore task-irrelevant information such as appearance which does not affect the task. 6. Ensure comprehensiveness and detail in your output, but also conherence and readibility. Avoid repetition and redundancy. 22 Tree-of-Captions Example Figure 8 Structure of Tree of captions (bottom) extracted from video (top). Each box is associated with corresponding video caption. Tree-of-Captions formate by depth-first search (DFS): # 0.00s -> 164.53s (duration: 164.5s) The video features view of man repairing bicycle tire and tube. The man is wearing black gloves, and there is bicycle lift holding blue bike in the background. In the background is another person wearing gray shirt. black tool chest and wooden tool bench can also be seen ... ## Segment 1 - 0.00s -> 126.20s (duration: 126.2s) This video features man showing second man how to repair tire. The second man stands center screen in workshop. The man holding the tire is center screen and behind him is large workbench. In the background are several other objects of various sizes and shapes ... ### Segment 1.1 - 0.00s -> 83.40s (duration: 83.4s) The video features first-person view of someone in well-lit workshop. The camera wearer is constantly seen touching objects in their environment while working with tire and tube, which is later put on bicycle. The lighting is bright and focused. In the first shot, the camera wearer holds tire and tube and feels the thread with their hands, before passing the tire to the left hand ... #### Segment 1.1.1 - 0.00s -> 53.13s (duration: 53.1s) This video shows busy workshop where two men are busy with their work. The man in this video removes an inner tube from bicycle wheel using tire lever, then peels off the tube using the thumb and fingers of his right hand. He then puts the removed tube back on the table ... **0.00s -> 23.27s (duration: 23.3s)**: The video features person working on bicycle tire inside bike shop. They wear black gloves and pull blue bicycle towards them. They take out tire lever to remove the tire from the wheel, and the focus is entirely on the cycle maintenance process ... **23.27s -> 53.13s (duration: 29.9s)**: The video is set inside workshop, where bicycle is clearly visible at the left side of the frame. The persons point of view is through stere Pipeline video camera mounted on fishing rod, which caps off the end of his arm, so the video looks like it is shot through fish eye lens ... #### Segment 1.1.2 - 53.13s -> 83.40s (duration: 30.3s) The video shows ..."
        },
        {
            "title": "D VLWM Planning Examples",
            "content": "D.1 Full Trajectory <!------------ VLWM Input ------------> <GOAL> Cooking Tomato and Eggs </GOAL> <!------------ VLWM Output ------------> <INTERPRETATION> Now, the kitchen is set up with necessary ingredients and tools. To achieve the goal, the eggs need to be cooked and mixed with tomatoes, and the mixture should be seasoned appropriately. The eggs should be whisked thoroughly to achieve uniform texture. The tomatoes should be heated until they are soft and well coated with the eggs. The dish should be seasoned with salt and pepper to taste. The final state of the dish should be well-cooked, flavorful mixture of eggs and tomatoes, ready for serving. The kitchen environment and the tools used, such as the stove, pan, and utensils, are crucial for achieving this goal. The ingredients, including eggs, tomatoes, oil, salt, and pepper, are essential for creating the desired dish. </INTERPRETATION> --- <ACTION> Preheat the skillet on the stove </ACTION> <STATE> To preheat the skillet, turn on the stove to medium-high heat and wait for the skillet to heat up. The skillet is garnished with little bit of oil to prevent the eggs from sticking. This action prepares the skillet for cooking the eggs by increasing its temperature. The state of the skillet changes from cold to hot, making it ready for cooking. The oil used for preheating prevents the eggs from sticking to the skillet, ensuring they cook evenly. </STATE> --- <ACTION> Crack eggs into bowl and whisk them together </ACTION> <STATE> Crack eggs into bowl by breaking the eggshells over the edge of the bowl and pouring the contents into the bowl. Use spatula to mix the eggs in circular motion, being careful not to over-beate them. This action changes the state of the eggs from whole to mixed liquid, making them ready for cooking. The eggs are now in state that can be seasoned and cooked. The whisking action ensures the eggs are well mixed, contributing to uniform texture in the final dish. </STATE> --- <ACTION> Season the eggs with salt </ACTION> <STATE> Add salt to the eggs and mix well using spatula. This action seasons the eggs, enhancing their flavor. The state of the eggs changes from unseasoned to seasoned, making them ready for cooking. The seasoning adds flavor to the eggs, which is crucial for the overall taste of the dish. </STATE> --- <ACTION> Cook the eggs in the pan </ACTION> <STATE> Pour the egg mixture into the preheated pan with oil. Use spatula to stir the eggs, breaking them into curds. Continue cooking until the eggs are almost set. This action changes the state of the eggs from liquid to partially cooked state. The eggs are now partially cooked and are being transformed into curds. The heat from the pan cooks the eggs, and the stirring action ensures even cooking. </STATE> --- <ACTION> Remove the cooked eggs from the pan </ACTION> <STATE> Use spatula to scrape the cooked eggs out of the pan and onto plate. This action removes the eggs from the pan, making way for cooking other ingredients. The state of the eggs changes from being in the pan to being on plate. The eggs are now removed from the pan, allowing for the addition of other ingredients to be cooked. </STATE> --- <ACTION> Cook the tomatoes in the pan </ACTION> <STATE> Add tomatoes to the pan and stir them using spatula. Continue cooking until the tomatoes are soft. This action changes the state of the tomatoes from raw to cooked, making them ready for mixing with the eggs. The tomatoes are now cooked and can be combined with the eggs. The heat from the pan cooks the tomatoes, and the stirring action ensures even cooking. </STATE> --- <ACTION> Add the cooked eggs back into the pan with the tomatoes </ACTION> <STATE> 24 Add the cooked eggs back into the pan with the tomatoes. Use spatula to stir the mixture, combining the eggs and tomatoes. This action mixes the eggs and tomatoes together, creating uniform dish. The state of the mixture changes from separate components to combined dish. The eggs and tomatoes are now mixed together, creating cohesive and ready-to-serve dish. </STATE> --- <ACTION> Season the dish with salt and pepper </ACTION> <STATE> Add salt and pepper to the dish and stir well using spatula. This action seasons the dish, enhancing its flavor. The state of the dish changes from unseasoned to seasoned, making it ready for serving. The seasoning adds flavor to the dish, ensuring it is palatable. </STATE> --- <ACTION> Transfer the dish to serving plate </ACTION> <STATE> Use spatula to transfer the cooked dish from the pan to serving plate. This action changes the state of the dish from being in the pan to being on plate, making it ready for serving. The dish is now presented in serving plate, ready to be consumed. </STATE> --- <ACTION> Present the final dish </ACTION> <STATE> The final dish is presented on white surface. The dish is now ready to be served. The state of the dish changes from being in the cooking vessel to being on serving plate. </STATE> --- <GOAL_ACHIEVED> D.2 Cost Min/maximizing Plans ### Goal: Make Beef Tacos Cost Minimizing Plan (cost = -4.86): 1. Saute chopped onions and garlic in oil 2. Add ground beef to the pan and break it up 3. Add seasoning to the beef mixture 4. Add tomato sauce to the beef mixture and stir 5. Prepare taco shells 6. Fill taco shells with the beef mixture 7. Add cheese to the tacos 8. Serve the tacos Cost Maximizing Plan (cost = 3.03): 1. Add959 existing ingredients to the slow cooker 2. Mix the ingredients in the slow cooker 3. Add taco seasoning to the slow cooker 4. Stir the contents of the slow cooker 5. Open the slow cooker lid 6. Serve the cooked beef mixture onto tortilla 7. Add cheese to the taco 8. Fold the tortilla ### Goal: Make Kerala Fish Curry Cost Minimizing Plan (cost = -4.36): 1. Dry roast the chilies in pan over medium heat 2. Add ginger and garlic to the pan and mix with the chilies 3. Add water and tamarind paste to the pan and mix 4. Add salt and sugar to the curry and stir 5. Add fish to the curry and cook until done 6. Add water to the curry and stir 7. Simmer the curry and stir occasionally 8. Garnish with cilantro 9. Serve the curry Cost Maximizing Plan (cost = -0.04): 1. Dry red chili 2. Add mustard seeds 3. Saute the onions 4. Add turmeric powder 5. AddTransition metal pieces 6. Add significance 7. Add curry leaves 8. Add chili powder 9. Add coriander powder 10. Add fish 11. Add salt 12. Add tamarind juice 13. Finish with coconut oil ### Goal: Use Tapping Gun Cost Minimizing Plan (cost = -7.58): 1. Insert the gun head into the Tapping gun 2. Remove the existing gun head from the Tapping gun 3. Handle the Tapping gun to prepare it for new gun head 4. Clean the Tapping gun 5. Prepare the Tapping gun for use by loading it with new gun head 6. Load the Tapping gun with new gun head 7. Demonstrate the final preparation of the Tapping gun 8. Emphasize the completion of the task 9. Conclude the demonstration 10. Fill the Tapping gun with disposable scoops Cost Maximizing Plan (cost = 3.46): 1. Ignited the lighter to produce flame 2. Insert the gun head into the tag brand 3. Load the Tapping gun with degrees 4. Insert the Tapping gun into the red compartment 5. Finish up by Aer 6. Aer.YEAR4 doesnt come off easily ### Goal: Replace SIM Card Cost Minimizing Plan (cost = -5.43): 1. Eject the SIM card tray using SIM card ejector tool 2. Remove the SIM card from the tray 3. Place the SIM card into the tray 25 4. Reinsert the SIM card tray into the phone 5. Ensure the SIM card is properly seated and the tray is fully inserted 6. Adjust and finalize the SIM card tray insertion 7. Verify the SIM card tray is fully inserted and functional Cost Maximizing Plan (cost = 4.24): 1. Use the SIM card ejector tool to eject the SIM card tray 2. Remove the SIM card tray from the phone 3. Place the SIM card into the SIM card tray 4. Insert the SIM card tray back into the phone 5. Place the phone on flat surface 6. Display the final result or related message ### Goal: Use Rice Cooker To Cook Rice Cost Minimizing Plan (cost = -4.31): 1. Take out some rice 2. Wash the rice 3. Put the washed rice into the rice cooker 4. Measure the water level 5. Cook the rice using the rice cooker 6. Open the rice cooker and fluff the cooked rice 7. Serve the cooked rice Cost Maximizing Plan (cost = 3.91): 1. Take out some rice 2. Rinse the rice 3. Place the strainer over the rice cooker 4. Add water to the rice cooker 5. Start the rice cooker 6. Analytical balance assessment 7. Open the rice cooker to check the cooked rice 8. Scoop the cooked rice from the rice cooker ### Goal: Make Chinese Spring Rolls Cost Minimizing Plan (cost = -4.44): 1. Pour oil into wok and heat it 2. Add chopped garlic and ginger to the heated oil in the wok 3. Cook chopped vegetables such as carrots and onions in the wok 4. Add noodles and various sauces to the wok and stir-fry 5. Prepare spring roll sheets and fill them with the cooked mixture 6. Fry the filled spring rolls in hot oil until they are golden brown 7. Serve the fried spring rolls on plate Cost Maximizing Plan (cost = -0.59): 1. Pour vegetable oil into wok 2. Add chopped garlic and ginger to the wok 3. Stir-fry chopped onions 4. Add chopped carrots and capsicum to the wok 5. Add noodles and purified water to the wok 6. Drain the cooked noodles 7. Mix the cooked noodles with chopped parsley, salt, and pepper 8. Prepare the cooking vessel for frying"
        }
    ],
    "affiliations": [
        "ISIR Sorbonne Université",
        "Meta FAIR",
        "University of Southern California"
    ]
}