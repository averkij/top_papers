{
    "paper_title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields",
    "authors": [
        "Fabian Perez",
        "Sara Rojas",
        "Carlos Hinojosa",
        "Hoover Rueda-Chacón",
        "Bernard Ghanem"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic material properties. This limitation restricts accurate material perception, which is crucial for robotics, augmented reality, simulation, and other applications. We introduce UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling joint hyperspectral novel view synthesis and unsupervised material segmentation. Our method models spectral reflectance via diffuse and specular components, where a learned dictionary of global endmembers represents pure material signatures, and per-point abundances capture their distribution. For material segmentation, we use spectral signature predictions along learned endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF enables scene editing by modifying learned endmember dictionaries for flexible material-based appearance manipulation. Extensive experiments validate our approach, demonstrating superior spectral reconstruction and material segmentation to existing methods. Project page: https://www.factral.co/UnMix-NeRF."
        },
        {
            "title": "Start",
            "content": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields Fabian Perez1,2,, Sara Rojas2, Carlos Hinojosa2,, Hoover Rueda-Chacon1, Bernard Ghanem2 1Universidad Industrial de Santander 2 KAUST 5 2 0 2 J 7 2 ] . e [ 1 4 8 8 1 2 . 6 0 5 2 : r Figure 1. UnMix-NeRF: hyperspectral novel view synthesis framework that leverages spectral unmixing for scene editing and unsupervised material segmentation. By exploiting endmembers and abundances, our approach enables material separation and scene manipulation."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic material properties. This limitation restricts accurate material perception, which is crucial for robotics, augmented reality, simulation, and other applications. We introduce UnMix-NeRF, framework that integrates spectral unmixing into NeRF, enabling joint hyperspectral novel view synthesis and unsupervised material segmentation. Our method models spectral reflectance via diffuse and specular components, where learned dictionary of global endmembers represents pure material signatures, and per-point abundances capture their distribution. For material segmentation, we use spectral signature predictions along learned endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF enables scene editing by modifying learned endmember dictionaries for flexible material-based appearance manipulation. Extensive experiments validate our approach, demonstrating superior spectral reconstruction and material segmentation to existing methods. Project page: https: //www.factral.co/UnMix-NeRF. *Project lead; Work done during an internship at KAUST. Neural Radiance Fields (NeRF) [27] have revolutionized 3D scene representation, enabling photorealistic novel view synthesis through implicit volumetric rendering. Recent advances have extended NeRF to other tasks such as segmentation[6], object detection[12], and spatial reasoning [13], leveraging pre-trained vision models such as SAM [16] and CLIP [32] for higher-level scene understanding. Among these, semantic segmentation has emerged as powerful tool, enabling the grouping of regions into objectlevel categories in multi-view settings [26]. more challenging task lies in understanding and segmenting materials, moving beyond recognizing what an object is, to understanding what it is composed of. Materials are fundamental to how we interpret and interact with the world, as they define surface properties, functionality, and usability, enabling richer scene understanding and making them essential for applications such as robot autonomy [1], augmented reality [44], and simulation [37], where accurate material perception is crucial for decision-making and interaction. Most existing 3D segmentation approaches focus on semantic segmentation and rely solely on RGB data, which captures color and geometric structure but lacks intrinsic material properties. This limitation can lead to issues like metamerism, where two objects appear identical under particular light source despite having different materials and spectral radiance distributions. Furthermore, many materials exhibit distinct spectral behaviors outside the visible spectral range, such as in the near-infrared, where we can find indicators for vegetation health [17], or in the ultraviolet, where cues about minerals can be extracted from fluorescence [18]; information that RGB data fails to capture. Spectral imaging (SI) emerges as powerful framework for scene representation, as it provides rich spectral information across multiple wavelengths. This, in turn, allows for more complete characterization of scene, effectively encoding unique spectral cues for different materials. Integrating spectral data into implicit neural representations remains challenge. Standard NeRF formulations model scene appearance using only RGB values, which inherently limits their capacity to differentiate materials with similar color but distinct spectral properties. While recent efforts have attempted to extend NeRF to spectral data [3, 22] by replacing the standard RGB output with per-channel spectral radiance predictions, these approaches treat spectral information as an additional dimension in the output space without explicitly leveraging the inherent structure of it, e.g. materials have representative spectral signatures. In real-world scenes, the spectral signature at given point often arises from mixture of multiple materials rather than single homogeneous substance. Spectral unmixing [14] is the process of decomposing these mixed signals into their constituent materials, represented as set of pure spectral signatures (endmembers), and estimating their corresponding abundances, which define the proportion of each endmember at given 3D point. Consequently, spectral unmixing serves as fundamental tool for distinguishing materials within scene [40]. However, to the best of our knowledge, there is currently no work in view synthesis that fully exploits the rich structure of spectral data, not only to enhance the quality of novel view generation but also to enable more precise and physically meaningful identification of materials within scene. Contributions. In this paper we propose UnMix-NeRF, the first framework that integrates spectral unmixing into NeRF, allowing joint spectral novel view synthesis and unsupervised material segmentation. Our method learns global endmembers through dictionary optimized during training and models spectral reflectance via diffuse and specular decomposition. The diffuse component is represented by per-point abundances and scaling factors, while dedicated branch predicts view-dependent specular effects. The final spectral radiance is obtained by combining both components. Moreover, the per-point spectral unmixing naturally enables unsupervised material segmentation by leveraging the rendered abundance vectors. To summarize, our key contributions are: We introduce UnMix-NeRF, framework for hyperspectral novel view synthesis that leverages spectral unmixing to achieve high-quality spatial and spectral renderings by encoding material properties from multi-view 3D hyperspectral dataset. Our framework inherently enables unsupervised segmentation by leveraging the rendered abundance vectors learned during training, which accurately delineate spatially varying material properties within scenes. Exploiting the learned endmember dictionary, our framework supports scene editing through direct and customizable manipulation of scene appearance with fine-grained control over individual materials. Our approach not only outperforms state-of-the-art methods in hyperspectral view synthesis, but also achieves pixel-accurate 3D material segmentation. Also, we extend synthetic hyperspectral dataset with paired 3D material annotations, enabling rigorous evaluation of spectral unmixing and segmentation in complex scenes. 2. Related Work Neural Radiance Fields. NeRF revolutionized novel view synthesis by representing scenes as continuous neural fields. Through volumetric rendering and multi-view optimization, it learns to predict density and color at any 3D point and viewing direction, enabling photorealistic view synthesis [27]. Several works have extended NeRF to new imaging modalities: SeaThru-NeRF [19] models underwater light scattering, Thermal-NeRF [41] handles infrared emissions, LiDAR-NeRF [35] enables LiDAR point cloud synthesis, and NeSpoF [15] incorporates polarization information. For hyperspectral imaging, HS-NeRF [3] and SpecNeRF [20] directly extend NeRF to predict per-wavelength radiance. Recent approaches like SpectralNeRF [22], introduces physically-based spectral rendering by extending NeRF with an additional autoencoder to predict perwavelength radiance, and HyperGS [36], concurrent work that adapts 3D Gaussian Splatting for spectral scene reconstruction. However, these methods treat spectral bands as independent channels, failing to take advantage of the rich material information encoded within the spectral signatures. These extensions require substantial computational overhead and also ignore the structured nature of spectral information, limiting their ability to disentangle materials. Spectral Unmixing. Spectral unmixing decomposes pixels into endmembers (materials signatures) and their abundances (percentage of material per pixel) [14]. Beyond its use in remote sensing, unmixing enables material understanding in indoor and outdoor scenes in robotics [5], industrial inspection [8], and material classification [43]. The process typically involves identifying the different concurrent endmember spectra and their fractional abundances, per pixel, while enforcing physical constraints like nonnegativity and sum-to-one (i.e. the sum of all abundances must be 100%). Classical approaches employ the linear mixing model (LMM), which assumes linear combination of endmembers [14]. However, LMM struggles with spectral variability caused by illumination, atmospheric conditions, and material mixing effects [2]. Extensions like the Augmented LMM (ALMM) [11] and the Extended LMM (ELMM)[38] address this by incorporating scaling factors and environmental variations through learned dictionaries and bundle-based representations, improving abundance estimation accuracy under real-world conditions. Recent deep learning approaches leverage transformers and convolutional neural networks (CNNs) to learn endmembers and abundances directly from data [9, 42]. While these methods have shown promise for 2D spectral unmixing, they are currently limited to single-view analysis and have not been extended to 3D reconstruction, where material decomposition could enable richer scene understanding. The challenge of incorporating physical unmixing constraints into neural architectures for 3D scenes remains unexplored. Material Segmentation. Recent advances in selfsupervised learning and foundation models have revolutionized unsupervised segmentation. Methods like DINO [29] and SAM [16] learn powerful visual features that allow zero-shot segmentation without task-specific training. These approaches leverage large-scale pretraining to discover semantic boundaries and object-centric representations, reducing the need for manual annotations. Materialistic [33] builds on this foundation, enabling interactive material selection based on user-provided pixel, showing robustness to shading and geometric variations. For NeRF, several works explore unsupervised object segmentation through multi-view consistency. RFP [25] and ONeRF [24] segment objects by propagating visual features through the radiance field using photometric consistency. More recent approaches like LERF [13] and OpenNeRF [6] leverage language-vision models to enable open-vocabulary 3D segmentation. However, these methods focus on semantic segmentation rather than material understanding, as they rely on RGB appearance features that can be ambiguous for material discrimination. Recent works like MaterialSeg3D [23] attempt material segmentation in 3D but still operate on RGB images using semantic priors learned from labeled datasets. Concurrent work SAMa [7] requires user-provided click and is limited to RGB data. Hyperspectral imaging, with its rich spectral information, naturally enables unsupervised material segmentation due to the inherent discriminative power of spectral signatures [30], thus eliminating the ambiguity associated with RGB features. 3. Preliminary In this section, we review the NeRF volumetric rendering formulation and provide an overview of spectral unmixing. 3.1. Neural Radiance Fields In NeRF, scene is represented as continuous volumetric function parameterized by Multi-Layer Perceptron (MLP). For any 3D point = (x, y, z) and viewing direction = (θ, ϕ), the network predicts density σ and view-dependent RGB color as (σ, c) = (x, d). To render an image from novel viewpoint, NeRF performs volumetric rendering by sampling points along each camera ray, defined as r(t) = + td, where R3 is the ray origin and R3 is the unit direction vector. The radiance is then accumulated using the classical volume rendering equation [27]. In practice, this integral is approximated as C(r) = Nt(cid:88) i=1 Ti(1 exp(σiδi))ci, (1) where Ti = exp( (cid:80)i1 j=1 σjδj), δi = ti+1 ti is the distance between adjacent samples, and Nt is the total number of samples. The network is optimized by minimizing the mean squared error between rendered and ground truth pixel colors across multiple training views: (cid:88) = C(r) (r)2 2, (2) rR where represents the set of camera rays from all training views and denotes the corresponding ground truth. 3.2. Spectral Unmixing image with spectral bands and hyperspectral spatial pixels can be represented as = [y1, , yn, , yN ] RBN , where yn RB1 is spectral pixel, for = 1, . . . , , with = HW as the total number of pixels. Let = [e1, , ek, , eK] RBK be the endmember matrix, containing the spectral signatures of pure materials. The Linear Mixing Model (LMM) [14] assumes that each spectral pixel is linear combination of these endmembers, yn = Ean + ϵn, (3) where an RK is the abundance vector associated with pixel n-th, quantifying the contribution of each endmenmber and ϵn is the residual (error approximation) term. The abundance coefficients satisfy the following constraints: an 0, 1T an = 1, (4) ensuring that abundances are non-negative, and that they sum to one, enforcing physically meaningful representation of spectral mixing. To account for spectral variability caused by the environment and illumination, the Extended Linear Mixing Model (ELMM) [38] introduces pixel-wise scaling factors Sn RKK, yn = ESnan + ϵn, where Sn is diagonal matrix with non-negative entries. (5) Figure 2. Overview of NeRF unmixing approach. Given viewing direction (θ, ϕ) and 3D coordinates (x, y, z), our network predicts density σ and set of abundance vectors a, along with scalar factors that scale the spectral endmembers E. The scaled endmembers, when multiplied by the predicted abundances a, yield the diffuse reflectance component cd. In parallel, specular reflectance cs and an additional tint factor are also predicted to capture local illumination effects. The final spectral radiance is obtained by combining these diffuse and specular components cd + cs under volumetric rendering with the predicted density. An ℓ2 loss on the reconstructed spectral signatures is sufficient to guide unmixing and specular-diffuse modeling. 4. Proposed Method Figure 2 illustrates our framework for hyperspectral novel view synthesis and material segmentation, which integrates spectral unmixing with volumetric rendering. In our approach, given viewing direction (θ, ϕ) and 3D point (x, y, z), the network predicts density σ together with set of abundance vectors and scalar factors that scale set of spectral endmembers. These scaled endmembers, when combined with the predicted abundances, form the diffuse reflectance component cd. In parallel, the model estimates specular reflectance cs and an additional tint factor to account for local illumination effects. By blending these diffuse and specular components through volumetric rendering, the framework generates the final spectral radiance, which can be mapped from the scene coordinate space to sRGB for image synthesis (see, e.g., [10, 39]). Moreover, the per-point spectral unmixing naturally enables unsupervised material segmentation by leveraging the rendered abundance vectors. 4.1. Spectral Unmixing Field We extend the standard NeRF formulation by incorporating the ELMM at each 3D point (see section 3.2). Let = (x, y, z) be coordinate in the scene. We first predict density value along with an intermediate latent feature via an MLP, denoted as (σ, z) = Fσ(x), which maps to scalar density σ 0 and latent feature vector RD, where is the dimensionality of the feature vector. This density is used for volumetric rendering (Section 4.3), while serves as input for subsequent predictions. Next, we introduce learnable dictionary of endmembers RBK, where each column ek RB represents the pure spectral signature of the k-th material. These global endmembers are jointly optimized with the network parameters and can be initialized either by standard 2D unmixing methods (e.g., using vertex component analysis (VCA) [28]) or randomly. To model local spectral mixing, we predict two components at each point: (i) set of scaling factors to account for illumination and environmental variability, and (ii) set of abundances representing the fractional presence of each material. Specifically, for (i), dedicated MLP is used to obtain the scaling factors as = Fs(x), with RK. To ensure non-negativity, we apply sigmoid function to s, hence obtaining = sigmoid(s), with each element sk [0, 1]. (ii) The intermediate feature obtained from Fσ() is then used by an abundance head Fa() to predict raw abundances and tint factor (see Section 4.2), expressed as (a, h) = Fa(x), where RK and R. softmax activation with temperature τ > 0 is applied to a, and computed as follows: ak = exp(ak/τ ) j=1 exp(aj/τ ) (cid:80)K , (6) thus ensuring non-negativity and the sum-to-one constraint (4). Additionally, is passed through according to Eq. sigmoid activation to constraint its values within [0, 1], thus modulating the contribution of the diffuse color. Each abundance ak corresponds to the fractional presence of the k-th material at point x, providing physically meaningful decomposition of the current spectral signature. With these, the diffuse reflectance is computed as cd = Sa, (7) where is diagonal matrix with non-negative scaling factors on its diagonal. 4.2. Specular Field Since spectral unmixing is inherently designed for diffuse reflectance, it fails to capture view-dependent effects such as specular highlights. To address this limitation, we introduce specular field. Motivated by Ref-NeRF [39], we leverage the tint factor predicted by the abundance head Fa() (see Section 4.1) to modulate the specular reflectance, allowing for better control of view-dependent effects. Then, taking both and as input, an MLP Fc() is used to obtain the specular reflectance vector as cs = Fc(x, d), where cs RB. The tint factor is then used to modulate this specular term as by employing the learned spectral endmembers as cluster centers. Given the predicted spectral signature for each ray C(r) RB, we compute the normalized inner product against the dictionary of endmembers as p(r) = softmax (cid:18) EC(r) EC(r) (cid:19) , (12) where p(r) RK contains the probabilities indicating cluster memberships for each of the materials. Finally, material segmentation is obtained by assigning each ray to the material cluster with the highest probability following m(r) = arg max pk(r). (13) cs = hcs. (8) 4.5. Loss Function The final per-point spectral reflectance is obtained by combining the diffuse and specular components under the dichromatic model [4]: = cd + cs. (9) 4.3. Volume Rendering and Camera Response Following the standard NeRF formulation (see Section 3.1), we sample points along camera rays and accumulate the spectral radiance. The accumulated spectral radiance along ray is computed using Eq. (1). Similarly, we can volumerender the material abundances to obtain the per-ray abundance vectors as: We jointly optimize all parameters, including the endmember matrix E, and the weights of the MLPs for density (Fσ), scaling (Fs), abundance (Fa), and specular reflectance (Fc), by minimizing combination of hyperspectral and RGB reconstruction errors. Let C(r) and Crgb(r) denote the rendered hyperspectral and RGB predictions for ray r, and let (r) and rgb(r) be the corresponding ground-truth measurements. The losses are defined as Lspec = Lrgb = (cid:88) rR (cid:88) rR C(r) (r)2 2 , (cid:13) (cid:13)Crgb(r) rgb(r)(cid:13) 2 (cid:13) 2 , (14) (15) A(r) = Nt(cid:88) i=1 (cid:0)1 exp(σiδi)(cid:1) ai, Ti (10) where is the set of all training rays. Then, our final objective is given by for the i-th sample location xi along the ray r. This allows us to render the abundances for each point in the scene To recover an RGB image from the rendered hyperspectral signature, we employ spectral response model. This conversion involves transforming the spectral power distribution in the scene coordinate space to deviceindependent color space (such as XYZ) and then to the target sRGB color space. Let R3B be the camera spectral response matrix that encapsulates both the color matching functions and the display primaries; then, the RGB color is given by Crgb(r) = C(r). (11) This approach ensures accurate color reproduction by modeling the entire imaging pipeline from scene radiance to displayed color. 4.4. Material Segmentation via Cluster Probe For unsupervised material segmentation, we perform cluster assignment for each rays predicted spectral signature = λspec Lspec + λrgb Lrgb, (16) with hyperparameters λspec and λrgb balancing the importance of hyperspectral versus RGB fidelity. Minimizing this loss does not only leads to accurate novel view synthesis, but also ensures physically consistent spectral unmixing. This results in both high-quality spectral reconstructions and reliable material segmentation through the per-ray abundance predictions. 4.6. Implementation Details We implemented our method within the Nerfstudio framework [34], building on top of the Nerfacto implementation. The hidden feature dimension was set to 16, and we integrated Nerfacc [21] into the volumetric rendering and sampling process to reduce training time. Additionally, we applied gradient scaling based on the squared ray distance to each pixel, as suggested in [31]. For optimization, we used the Adam optimizer with learning rate of 1 102 and an exponential learning rate scheduler. All models were trained for 20, 000 iterations and all experiments were performed on an NVIDIA A100 GPU, with maximum memory usage of approximately 20 GB. For all experiments, we set λspec = 5 and λrgb = 1. To ensure physically meaningful spectral signatures, the endmembers was constrained at each iteration by applying element-wise clamping within the range [0, 1]. For each trained scene, the number of endmembers is selected as heuristic approximation of the number of distinct materials present in the scene. 5. Experimental Results We extensively evaluate the performance of our proposed approach using standard metrics and compare it with stateof-the-art methods. 5.1. Datasets, Preprocessing, Metrics and Baselines Datasets. Three different datasets are employed for hyperspectral novel view synthesis and material segmentation: NeSpoF Dataset [15]. We use the NeSpoF spectrofocusing exclusively on the S0 polarimetric dataset, Stokes component representing spectral radiance. Specifically, we utilize the synthetic dataset comprising 21 spectral bands ranging from 450 nm to 650 nm. This dataset includes four distinct scenes: ajar, hotdog, cbox dragon, and cbox sphere. We adopt the provided camera poses and their calibrated RGB projections. Additionally, we extended the dataset by generating material maps from the synthetic scenes, obtaining ground-truth labels for material segmentation. Further details about the extension are provided in the supplementary material. Surface Optics Dataset [3]. The Surface Optics dataset contains real-world imagery captured across 128 spectral bands, spanning wavelengths from 370 nm to 1100 nm. This dataset consists of four object-centric scenes: Rosemary, Basil, Tools and Origami. We adopt the original camera poses. BaySpec Dataset [3]. The BaySpec dataset offers hyperspectral data across 140 spectral bands, covering wavelength range from 400 nm to 1110 nm. BaySpec offers three scenes: Pinecone, Caladium and Anacampseros and includes measured camera poses as described in the original dataset publication. Metrics. We evaluate UnMix-NeRF and competing methods on hyperspectral view synthesis using rigorous metrics that capture both accuracy and spectral fidelity, including PSNR, SSIM, Spectral Angle Mapping (SAM), and RMSE. For unsupervised material segmentation, we report standard metrics, including mean Intersection over Union (mIoU) and F1 score. Baselines. We compare our method against existing novel view methods available for hyperspectral imaging. Specifically, we compare with NeSpoF [15], originally designed Ablation Step PSNR SSIM SAM RMSE Base: Unmix Only + Physical Constraint + Scaling Factors + RGB Loss + VCA Initialization + Specular Field 24.73 22.36 29.36 30.92 31.09 33.20 0.710 0.722 0.914 0.923 0.923 0.935 0.063 0.075 0.026 0.025 0.024 0.022 0.058 0.076 0.034 0.028 0.027 0. Table 1. Ablation study on the NeSpoF dataset, evaluating the impact of each component in UnMix-NeRF. Metrics include PSNR (), SSIM (), Spectral Angle Mapper (SAM, ), and RMSE (). Each row sequentially adds component to the baseline model. The final configuration, incorporating all elements, achieves the best overall performance. for spectro-polarimetric imaging, restricting our comparison to the spectral component only (S0 Stokes). Also, we include HS-NeRF, which extends the NeRF framework from the conventional RGB 3-channel output to multichannel spectral representation. Lastly, we also compare against HyperGS, framework for hyperspectral novel view synthesis based on the 3D Gaussian Splatting (3DGS). Also, we include comparisons with traditional NeRF models adapted for hyperspectral data (NeRF, MipNeRF, TensoRF, Nerfacto and MipNeRF-360) for the Bayspec and Surface Optics datasets. 5.2. Ablation Studies To evaluate the effectiveness and impact of individual components in our UnMix-NeRF model, we performed extensive ablation studies. Table 1 summarizes the results obtained from each ablation step performed over the NeSpoF dataset. The initial baseline model (Unmix Only) predicts abundances using global endmembers without further constraints or enhancements. We then sequentially added: 1. Physical constraints, from Section 3.1, using activation functions to enforce realistic abundance estimates. 2. Scaling factors, in line with the ELMM, enable the model to better represent local spectral variability. 3. An RGB loss term consisting of spectral-to-RGB projection that helps with realism and spectral consistency. 4. The VCA initialization of the global endmembers instead of random initialization. 5. Finally, the specular field component is used to accurately model specular reflectances via dichromatic model. Results in Table 1 illustrate the incremental contributions of each component added to our UnMix-NeRF baseline. The initial model (Unmix Only) achieves moderate spectral reconstruction quality (PSNR = 24.73), establishing performance baseline without additional constraints. Introducing physical constraints reduces the PSNR slightly by margin of 2.37 due to the restrictive nature of these constraints on spectral variability. The addition of scaling facFigure 3. Visualization of the top-4 performing methods for frame 51 out of 359 for the Caladium plant scene from the Bayspec dataset. The top row shows the 70th spectral channel out of the 141-channel predicted image, and the bottom row provides raw pixel-wise mean relative absolute error heatmap of the scene. tors significantly boosts the performance (PSNR increases by +7.00, SAM decreases by 0.049, and RMSE decreases by 0.042), underscoring their importance in modeling local spectral variations. Incorporating the RGB loss further improves the spectral accuracy (PSNR improves by +1.56), demonstrating the benefit of coupling hyperspectral predictions with RGB supervision. Using the VCA initialization provides minor improvement of +0.17 in PSNR, indicating that an informed initialization of the endmembers aids the optimization stability. Finally, integrating the specular field yields the best overall results (PSNR increases by +2.11, SSIM reaches 0.935), confirming that explicitly modeling specular reflectance substantially enhances spectral rendering and overall image quality. 5.3. Qualitative Results To further assess the effectiveness of our UnMix-NeRF framework, we provide qualitative comparisons against state-of-the-art methods in hyperspectral novel view synthesis. Figure 3 illustrates the quality of reconstruction between different methods in the Caladium scene of the Bayspec dataset. The top row displays the 70th spectral channel from the reconstructed hyperspectral cube, while the bottom row depicts the corresponding mean relative absolute error (MRAE) heatmaps, highlighting pixel-wise reconstruction errors. Our method achieves the most accurate spectral predictions, significantly reducing reconstruction artifacts and preserving fine-grained spectral details. Additionally, Figure 1 demonstrates the capabilities of UnMix-NeRF for material segmentation and scene editing. Leveraging the learned endmembers and their corresponding abundance maps, our approach enables precise material separation and targeted modifications to scene appearance. Furthermore, Figure 4 presents visualization of the learned material abundances for the Anacampseros scene in the BaySpec dataset. Each image corresponds to learned Method Scene PSNR RMSE Time HS-NeRF* NeSpoF avg. avg. Ours ajar hotdog cbox-dragon cbox-sphere avg. 26.0 33.0 38.09 34.47 32.21 27.96 33. 0.04 0.02 0.01 0.01 0.02 0.03 0.02 5 hours 11.9 hours 43 min 45 min 45 min 44 min 44 min Table 2. Comparison on the NeSpoF dataset. HS-NeRF* and NeSpoF report averaged results (avg.) over 4 scenes; for Ours, perscene results and the overall averaged metrics are provided. abundance map, highlighting the presence of distinct materials in the scene. The spatial variations observed across the different maps indicate that our approach successfully captures and disentangles material-specific contributions. Figure 4. Visualization of the learned material abundance maps for the Anacampseros scene from the BaySpec dataset (top row) and the Ajar scene from the NeSpoF dataset (bottom row). Each map represents the spatial distribution of materials abundance, highlighting distinct materials in the scene. Pinecone"
        },
        {
            "title": "Bayspec Dataset",
            "content": "Caladium Anacampseros PSNR SSIM SAM RMSE PSNR SSIM SAM RMSE PSNR SSIM SAM RMSE 22.82 21.45 24.12 15.36 20.93 20.07 22.65 27.0 27.13 0.6113 0.5738 0.6454 0.4935 0.7355 0.581 0.6039 0.7509 0.8174 0.0446 0.0410 0.0593 0.0707 0.0279 0.0725 0.0668 0.0309 0.0287 0.0728 0.0856 0.0625 0.1709 0.0507 0.1521 0.0819 0.0447 0. 23.12 23.36 24.79 20.67 26.93 19.084 23.50 27.70 30.08 0.58348 0.5935 0.6424 0.6208 0.7371 0.705 0.7131 0.8354 0.8541 0.0491 0.0487 0.0516 0.0529 0.0332 0.0533 0.2889 0.0271 0.0237 0.0709 0.0685 0.0577 0.0945 0.0461 0.0902 0.0758 0.0414 0.0312 24.12 23.43 25.07 21.32 26.73 20.32 22.59 26.62 28.20 0.6220 0.6160 0.6569 0.6423 0.7601 0.7260 0.5786 0.7545 0. 0.0384 0.0408 0.0394 0.0417 0.0230 0.0345 0.0447 0.0183 0.0154 0.0623 0.0786 0.0558 0.0867 0.0461 0.0789 0.0853 0.0460 0.0392 Method NeRF MipNeRF TensoRF Nerfacto MipNeRF360 HS-NeRF 3DGS HyperGS Ours Table 3. Quantitative results on the BaySpec dataset for the Pinecone, Caladium, and Anacampseros scenes. Our method achieves the best performance across all metrics Method NeRF MipNeRF TensoRF Nerfacto MipNerf360 HS-NeRF 3DGS HyperGS Ours Rosemary Basil PSNR SSIM SAM RMSE PSNR SSIM SAM RMSE 8.42 13.64* 12.1 18.66 8.47 *18.60 25.56 26.77 28. 0.7461 0.5684* 0.73351 0.8836 0.7518 *0.887 0.9695 0.9845 0.9355 0.3560 0.0284 0.2083* 1000* 0.2662 0.0212 0.1205 0.0078 0.0876 0.3825 *0.0077 *0.1187 0.0534 0.0028 0.0445 0.0021 0.0332 0.0019 9.91 10.11 15.23 16.54 13.92 *16.81 21.19 25.30 29.21 0.5256 0.0769 0.5534 0.5334 0.0728 0.5878 0.3628 0.0435 0.5811 0.1655 0.0176 0.7915 0.8584 0.2035 0.0497 *0.771 *0.0172 *0.1587 0.0897 0.9385 0.0101 0.0569 0.9503 0.00514 0.0364 0.0043 0.9584 Table 4. Quantitative results on the Surface Optics dataset for the Rosemary and Basil scenes. Our method achieves the best overall performance across all metrics. Results for the Tools and Origami scenes are provided in the supplementary material. 5.4. Quantitative Results Quantitative evaluations of our UnMix-NeRF method on three different datasets demonstrate superior performance across variety of scenes and conditions. In all tables, the best performing result for each metric is highlighted in red, the second-best in orange, and the third-best in yellow. Table 2 presents the quantitative comparison on the NeSpoF dataset, where our method achieves state-of-the-art performance while maintaining significantly lower computational cost. Our approach reaches an average PSNR of 33.2 while reducing the computation time to only 44 minutes. Additionally, we match the RMSE of NeSpoF, which requires over 11.9 hours per scene. The per-scene results highlight the robustness of our method. We further evaluate UnMix-NeRF on the BaySpec dataset, as shown in Table 3. Our method consistently outperforms all baselines in all scenes, achieving the highest PSNR, SSIM, and the lowest SAM and RMSE values. Notably, our model improves over HyperGS by +3.15 PSNR on the Caladium scene. Similar trends are observed in the Pinecone and Anacampseros scenes, where our approach provides both higher reconstruction fidelity and better spectral consistency. These results show that explicitly modeling spectral unmixing in NeRF improves spectral prediction accuracy over per-pixel regression. Figure 5. Unsupervised material segmentation of the Ajar scene. Table 4 reports the results on the Surface Optics dataset. Our method achieves the best performance in both the Rosemary and Basil scenes, obtaining +2.14 and +3.91 PSNR improvement over HyperGS, respectively. Furthermore, our approach achieves the lowest SAM and RMSE values, confirming its ability to accurately reconstruct spectral details across diverse material compositions. The results for the remaining scenes are provided in the supplementary material. Additionally, we evaluate the effectiveness of our method for unsupervised material segmentation on the extended NeSpoF synthetic dataset. Our approach achieves an average F1 score of 0.41 and mIoU of 0.28 across all scenes in the dataset, demonstrating UnMix-NeRFs ability to extract meaningful material clusters without requiring supervision. Figure 5 shows the unsupervised material segmentation results of our method on the Ajar scene. 6. Conclusions We introduced UnMix-NeRF, novel approach integrating spectral unmixing into neural radiance fields to achieve simultaneous hyperspectral novel view synthesis and unsupervised material segmentation. By modeling spectral reflectance through learned global endmembers and perpoint abundances, our method effectively captures intrinsic material properties. Our extensive evaluations show that UnMix-NeRF outperforms existing methods in spectral reconstruction quality and unsupervised material segmentation accuracy. Additionally, our framework supports intuitive material-based scene editing through direct manipulation of endmember dictionaries. Acknowledgments. This work is supported by the KAUST Center of Excellence for Generative AI under award number 5940. The computational resources are provided by IBEX, which is managed by the Supercomputing Core Laboratory at KAUST."
        },
        {
            "title": "References",
            "content": "[1] Adarsh, Mohamed Kaleemuddin, Dinesh Bose, and KI Ramachandran. Performance comparison of infrared and ultrasonic sensors for obstacles of different materials in vehicle/robot navigation applications. In IOP Conference Series: Materials Science and Engineering, page 012141. IOP publishing, 2016. 1 [2] Ricardo Augusto Borsoi, Tales Imbiriba, Jose Carlos Moreira Bermudez, Cedric Richard, Jocelyn Chanussot, Lucas Drumetz, Jean-Yves Tourneret, Alina Zare, and Christian Jutten. Spectral variability in hyperspectral data unmixing: comprehensive review. IEEE geoscience and remote sensing magazine, 9(4):223270, 2021. 3 [3] Gerry Chen, Sunil Kumar Narayanan, Thomas Gautier Ottou, Benjamin Missaoui, Harsh Muriki, Cedric Pradalier, and Yongsheng Chen. Hyperspectral neural radiance fields. arXiv preprint arXiv:2403.14839, 2024. 2, 6 [4] Robert Cook and Kenneth E. Torrance. reflectance model for computer graphics. ACM Transactions on Graphics (ToG), 1(1):724, 1982. 5 [5] Sibren Dieters. Robot-aided hyperspectral imaging for mineral exploration in underground mining environments. 2024. 2 [6] Francis Engelmann, Fabian Manhardt, Michael Niemeyer, Keisuke Tateno, Marc Pollefeys, and Federico Tombari. Opennerf: open set 3d neural scene segmentation with pixelarXiv preprint wise features and rendered novel views. arXiv:2404.03650, 2024. 1, [7] Michael Fischer, Iliyan Georgiev, Thibault Groueix, Vladimir Kim, Tobias Ritschel, and Valentin Deschaintre. Sama: Material-aware 3d selection and segmentation. arXiv preprint arXiv:2411.19322, 2024. 3 [8] Nahum Gat, Suresh Subramanian, Jacob Barhen, and Nikzad Toomarian. Spectral imaging applications: remote sensing, environmental monitoring, medicine, military operations, factory automation, and manufacturing. In 25th AIPR Workshop: Emerging Applications of Computer Vision, pages 63 77. SPIE, 1997. 2 [9] Preetam Ghosh, Swalpa Kumar Roy, Bikram Koirala, Behnood Rasti, and Paul Scheunders. Hyperspectral unmixing using transformer network. IEEE Transactions on Geoscience and Remote Sensing, 60:116, 2022. 3 [10] Peter Hedman, Pratul Srinivasan, Ben Mildenhall, Jonathan Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58755884, 2021. 4 [11] Danfeng Hong, Naoto Yokoya, Jocelyn Chanussot, and Xiao Xiang Zhu. An augmented linear mixing model to address spectral variability for hyperspectral unmixing. IEEE Transactions on Image Processing, 28(4):19231938, 2018. 3 [12] Benran Hu, Junkai Huang, Yichen Liu, Yu-Wing Tai, and Chi-Keung Tang. Nerf-rpn: general framework for object In Proceedings of the IEEE/CVF condetection in nerfs. ference on computer vision and pattern recognition, pages 2352823538, 2023. 1 [13] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded In Proceedings of the IEEE/CVF Internaradiance fields. tional Conference on Computer Vision, pages 1972919739, 2023. 1, 3 [14] Nirmal Keshava and John Mustard. Spectral unmixing. IEEE signal processing magazine, 19(1):4457, 2002. 2, 3 [15] Youngchan Kim, Wonjoon Jin, Sunghyun Cho, and SeungIn SIGHwan Baek. Neural spectro-polarimetric fields. GRAPH Asia 2023 Conference Papers, pages 111, 2023. 2, [16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 1, 3 [17] Neha Kureel, Jyoti Sarup, Shafique Matin, Suresh Goswami, and Kapil Kureel. Modelling vegetation health and stress using hypersepctral remote sensing data. Modeling Earth Systems and Environment, pages 116, 2022. 2 [18] Joseph Lakowicz. Principles of fluorescence spectroscopy. Springer, 2006. 2 [19] Deborah Levy, Amit Peleg, Naama Pearl, Dan Rosenbaum, Derya Akkaynak, Simon Korman, and Tali Treibitz. Seathrunerf: Neural radiance fields in scattering media. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5665, 2023. 2 [20] Jiabao Li, Yuqi Li, Ciliang Sun, Chong Wang, and Jinhui Xiang. Spec-nerf: Multi-spectral neural radiance fields. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 24852489. IEEE, 2024. [21] Ruilong Li, Hang Gao, Matthew Tancik, and Angjoo Kanazawa. Nerfacc: Efficient sampling accelerates nerfs. arXiv preprint arXiv:2305.04966, 2023. 5 [22] Ru Li, Jia Liu, Guanghui Liu, Shengping Zhang, Bing Zeng, and Shuaicheng Liu. Spectralnerf: Physically based spectral rendering with neural radiance field. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 3154 3162, 2024. 2 [23] Zeyu Li, Ruitong Gan, Chuanchen Luo, Yuxi Wang, Jiaheng Liu, Ziwei Zhu, Qing Li, Xucheng Yin, Man Zhang, Zhaoxiang Zhang, et al. Materialseg3d: Segmenting dense materials from 2d priors for 3d assets. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 370 379, 2024. 3 [24] Shengnan Liang, Yichen Liu, Shangzhe Wu, Yu-Wing Tai, and Chi-Keung Tang. Onerf: Unsupervised 3d obarXiv preprint ject segmentation from multiple views. arXiv:2211.12038, 2022. 3 [37] Giuseppe Vecchio, Renato Sortino, Simone Palazzo, and Concetto Spampinato. Matfuse: controllable material genthe eration with diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44294438, 2024. 1 In Proceedings of [38] Miguel Angel Veganzones, Lucas Drumetz, Guillaume Tochon, Mauro Dalla Mura, Antonio Plaza, Bioucas-Dias, and Jocelyn Chanussot. new extended linear mixing model to address spectral variability. In 2014 6th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS), pages 14. IEEE, 2014. 3 [39] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan Barron, and Pratul Srinivasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 54815490. IEEE, 2022. 4, 5 [40] Erin Wetherley, Dar Roberts, and Joseph McFadden. Mapping spectrally similar urban materials at sub-pixel scales. Remote Sensing of Environment, 195:170183, 2017. 2 [41] Tianxiang Ye, Qi Wu, Junyuan Deng, Guoqing Liu, Liu Liu, Songpengcheng Xia, Liang Pang, Wenxian Yu, and Ling Pei. Thermal-nerf: Neural radiance fields from an infrared camera. arXiv preprint arXiv:2403.10340, 2024. 2 [42] Xiangrong Zhang, Yujia Sun, Jingyan Zhang, Peng Wu, and Licheng Jiao. Hyperspectral unmixing via deep convolutional neural networks. IEEE Geoscience and Remote Sensing Letters, 15(11):17551759, 2018. 3 [43] Min Zhao, Jie Chen, and Zhe He. laboratory-created dataset with ground truth for hyperspectral unmixing evalIEEE Journal of Selected Topics in Applied Earth uation. Observations and Remote Sensing, 12(7):21702183, 2019. 2 [44] Junwei Zheng, Jiaming Zhang, Kailun Yang, Kunyu Peng, and Rainer Stiefelhagen. Materobot: Material recognition in wearable robotics for people with visual impairments. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 23032309. IEEE, 2024. [25] Xinhang Liu, Jiaben Chen, Huai Yu, Yu-Wing Tai, and ChiKeung Tang. Unsupervised multi-view object segmentation using radiance field propagation. Advances in Neural Information Processing Systems, 35:1773017743, 2022. 3 [26] Yichen Liu, Benran Hu, Chi-Keung Tang, and Yu-Wing Tai. Sanerf-hq: Segment anything for nerf in high quality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 32163226, 2024. 1 [27] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 1, 2, 3 [28] Jose MP Nascimento and Jose MB Dias. Vertex component analysis: fast algorithm to unmix hyperspectral data. IEEE transactions on Geoscience and Remote Sensing, 43(4):898 910, 2005. 4 [29] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3 [30] Fabian Perez and Hoover Rueda-Chacon. Beyond appearances: Material segmentation with embedded spectral information from rgb-d imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 293301, 2024. 3 [31] Julien Philip and Valentin Deschaintre. Floaters No More: Radiance Field Gradient Scaling for Improved Near-Camera In Eurographics Symposium on Rendering. The Training. Eurographics Association, 2023. [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1 [33] Prafull Sharma, Julien Philip, Michael Gharbi, Bill Freeman, Fredo Durand, and Valentin Deschaintre. Materialistic: Selecting similar materials in images. ACM Transactions on Graphics, 42(4), 2023. 3 [34] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: modular framework for neural radiance field development. In ACM SIGGRAPH 2023 conference proceedings, pages 1 12, 2023. 5 [35] Tang Tao, Longfei Gao, Guangrun Wang, Yixing Lao, Peng Chen, Hengshuang Zhao, Dayang Hao, Xiaodan Liang, Mathieu Salzmann, and Kaicheng Yu. Lidar-nerf: Novel lidar view synthesis via neural radiance fields. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 390398, 2024. 2 [36] Christopher Thirgood, Oscar Mendez, Erin Chao Ling, Jon Storey, and Simon Hadfield. Hypergs: Hyperspectral 3d gaussian splatting. arXiv preprint arXiv:2412.12849, 2024."
        }
    ],
    "affiliations": [
        "KAUST",
        "Universidad Industrial de Santander"
    ]
}