{
    "paper_title": "Nav-R1: Reasoning and Navigation in Embodied Scenes",
    "authors": [
        "Qingxiang Liu",
        "Ting Huang",
        "Zeyu Zhang",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Embodied navigation requires agents to integrate perception, reasoning, and action for robust interaction in complex 3D environments. Existing approaches often suffer from incoherent and unstable reasoning traces that hinder generalization across diverse environments, and difficulty balancing long-horizon semantic reasoning with low-latency control for real-time navigation. To address these challenges, we propose Nav-R1, an embodied foundation model that unifies reasoning in embodied environments. We first construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought (CoT) for embodied tasks, which enables cold-start initialization with structured reasoning. Building on this foundation, we design a GRPO-based reinforcement learning framework with three complementary rewards: format, understanding, and navigation, to improve structural adherence, semantic grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow reasoning paradigm, decoupling deliberate semantic reasoning from low-latency reactive control for efficient yet coherent navigation. Extensive evaluations on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms strong baselines, with over 8% average improvement in reasoning and navigation performance. Real-world deployment on a mobile robot further validates its robustness under limited onboard resources. Code: https://github.com/AIGeeksGroup/Nav-R1. Website: https://aigeeksgroup.github.io/Nav-R1."
        },
        {
            "title": "Start",
            "content": "Nav-R1: Reasoning and Navigation in Embodied Scenes Qingxiang Liu1 Ting Huang1 Zeyu Zhang2 Hao Tang2 2Peking University 1Shanghai University of Engineering Science Equal contribution. Project lead. Corresponding author: bjdxtanghao@gmail.com. The division of labor between System 1 (fast) and System 2 (slow) is highly efficient: it minimizes effort and optimizes performance. Daniel Kahneman (Nobel Prize in Economics) 5 2 0 2 3 1 ] . [ 1 4 8 8 0 1 . 9 0 5 2 : r Fig. 1. Nav-R1 is an embodied foundation model that integrates dialogue, reasoning, planning, and navigation capabilities to enable intelligent interaction and task execution in 3D environments. Abstract Embodied navigation requires agents to integrate perception, reasoning, and action for robust interaction in complex 3D environments. Existing approaches often suffer from incoherent and unstable reasoning traces that hinder generalization across diverse environments, and difficulty balancing long-horizon semantic reasoning with low-latency control for real-time navigation. To address these challenges, we propose Nav-R1, an embodied foundation model that unifies reasoning in embodied environments. We first construct Nav-CoT-110K, large-scale dataset of step-by-step Chains-of-Thought (CoT) for embodied tasks, which enables cold-start initialization with structured reasoning. Building on this foundation, we design GRPO-based reinforcement learning framework with three complementary rewards: format, understanding, and navigation, to improve structural adherence, semantic grounding, and path fidelity. Furthermore, we introduce Fast-in-Slow reasoning paradigm, decoupling deliberate semantic reasoning from low-latency reactive control for efficient yet coherent navigation. Extensive evaluations on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms strong baselines, with over 8% average improvement in reasoning and navigation performance. Real-world deployment on mobile robot further validates its robustness under limited onboard resources. Code: https://github.com/AIGeeksGroup/Nav-R1. Website: https://aigeeksgroup.github.io/Nav-R1. I. INTRODUCTION Embodied scene understanding is central problem in embodied AI, robotics, and intelligent agents, requiring an agent to perceive, reason, and act within complex 3D environments [4], [29]. robust understanding of embodied navigation not only supports goal-directed tasks such as object search, instruction following, and trajectory planning, but also enables higher-level embodied interactions including dialogue, reasoning, and decision-making. Such capabilities are fundamental for service robots, augmented reality assistants, and intelligent embodied systems deployed in realworld environments [41], [42]. Recent advances in large vision language models (LVLMs) have extended the success of 2D perception into the 3D embodied domain, giving rise to unified frameworks that couple perception, language, and action. As illustrated in Fig. 1, these advances lay the foundation for tackling four embodied tasks such as embodied dialogue [10], [20], [15], embodied reasoning [15], [10], [12], embodied planning [13], [10], [21], and embodied navigation [4], [29], [54], [36], which are the central focus of this work. Despite such progress, significant challenges remain. First, existing approaches often suffer from incoherent and unstable reasoning traces that fail to align with navigation instructions, leading to brittle generalization and semantically inconsistent output. Second, embodied navigation requires balancing long-horizon semantic reasoning with low-latency reactive control for real-time execution, dual requirement that remains largely unaddressed in current methods. Motivated by these limitations, there is critical need for embodied foundation models that can jointly address semantic reasoning and embodied action, rather than treating them as separate problems. To this end, our work aims to develop unified framework that balances long-horizon reasoning with real-time responsive control, thereby enabling robust generalization and reliable execution in diverse 3D environments. that To overcome these limitations, we introduce Nav-R1, an embodied foundation model integrates reasoning, planning, dialogue, and navigation into unified framework. Specifically, we construct large-scale dataset Nav-CoT110K, synthesizing high-quality Chains-of-Thought (CoT) for embodied tasks by prompting strong VLM with egocentric observations, instructions, and action options, followed by rule-based filtering for consistency. This dataset is used for cold-start stage that equips Nav-R1 with structured reasoning and alignment of instructions. Building upon this initialization, we design reinforcement learning stage with three complementary rewards: (i) format reward ensuring structural adherence, (ii) an understanding reward capturing semantic correctness and visual grounding, and (iii) navigation reward optimizing path fidelity and endpoint accuracy. To further address the tension between semantic fidelity and real-time control, we propose Fast-in-Slow dual system reasoning scheme inspired by cognitive science [27]: slow module aggregates long-term semantics from visual histories, while fast module executes short-horizon actions with low latency, coordinated asynchronously for robust yet efficient navigation. We evaluate Nav-R1 extensively on the embodied benchmarks. Nav-R1 consistently outperforms prior state-of-theart methods, achieving improvements in navigation success TABLE STATISTICS OF THE PUBLIC 3D-VL DATASETS THAT WE DRAW ON WHEN SYNTHESISING THE NAV-COT-110K DATASET. NS DENOTES THE SCENE NUMBER. Modality IS THE MODALITY WITHIN INSTRUCTIONS, WHERE [V, L, P] DENOTE [vision, language, point]. NT DENOTES THE TASK NUMBER. DE,CE DENOTE THE DISCRETE AND CONTINUOUS ENVIRONMENTS. Dataset R2R [4] R2R-CE [29] RxR-CE [30] SOON [57] OVON [52] Nav-CoT-110K (Ours) Scenes MP3D HM3D NS 90 90 90 90 181 342 Instruction Capability ObjNav VLN Modality L NT Env 22K 4.5K - 30K 53K DE CE CE DE CE CE V, L, 110K rates and reasoning accuracy. Beyond simulation, we deploy Nav-R1 on WHEELTEC R550 mobile robot equipped with Jetson Orin Nano, LiDAR, and RGB-D camera, demonstrating robust real-world performance under limited onboard computation by designing cloud-based inference. These results validate that Nav-R1 achieves strong balance of reasoning capability, semantic grounding, and embodied real-time control. Our contributions are summarized as follows: We introduce Nav-R1, an embodied foundation model equipped with GRPO-based reinforcement learning framework to enhance reasoning and navigation in 3D environments. Specifically, we design three complemenformat reward, understanding tary reward functions, reward, and navigation reward, to improve structural adherence, semantic grounding, and path fidelity. In addition, we construct the large-scale Nav-CoT-110K dataset through CoT data engine, which provides highquality step-by-step reasoning trajectories to bootstrap the model via cold-start initialization. We propose novel Fast-in-Slow reasoning paradigm that decouples long-horizon semantic reasoning from short-horizon reactive control. And this dual-system design ensures semantic coherence for planning while maintaining low-latency responses in dynamic environments. We conduct comprehensive evaluations on both embodied AI benchmarks and real-world robot deployment. Nav-R1 achieves an average improvement of 8% compared to strong baselines across dialogue, reasoning, planning, and navigation tasks, and further demonstrates robust performance when deployed on physical robot with limited on-board resources. II. RELATED WORK a) Embodied understanding: Embodied understanding seeks to equip agents with the ability to perceive, reason, and act in 3D environments by tightly integrating multimodal sensory data with linguistic instructions. Early work, limited to 2D visual abstractions, lacked the spatial expressiveness required for complex 3D reasoning. Recent advances address this gap by using large language models (LLMs) as Fig. 2. Architecture of Nav-R1. Nav-R1 designs Fast-in-Slow reasoning paradigm that processes egocentric RGB-D views, scene point cloud, and language instructions. The slow system performs long-horizon semantic reasoning, while the fast system executes real-time navigation, enabling coherent reasoning and low-latency control in embodied environments. the connective tissue between perception, grounding, and action planning [24], [23], [25]. LEO [21] exemplifies this trend, introducing an embodied generalist agent trained in two stages - 3D vision-language alignment and subsequent vision-language-action instruction tuning, thus achieving unified competence in captioning, question answering and embodied reasoning without task-specific sub-modules. Complementing LEO, GaussianVLM [15] adopts scene-centric paradigm that embeds linguistic features directly into 3D Gaussian splats. Its dual sparsification mechanism distills dense scene representations into task-aware tokens, obviating external object detectors and enabling zero-shot generalization across embodied reasoning tasks. Together, these studies underscore that unified architectures and language-aligned 3D representations are crucial to robust scene comprehension and action-oriented cognition. b) Embodied navigation: Embodied navigation tasks require agents to translate multimodal instructions into smooth, continuous motion through unstructured 3D scenes, which mainly include tasks such as object goal navigation (ObjectNav) and vision language navigation (VLN). Traditional pipelines rely on discrete topological graphs, which limit path flexibility. Recent research instead embraces end-to-end paradigms powered by large vision language models. VLN-R1 [36] couples reinforcement fine-tuning with time-decayed reward, using GRPO-style training and the VLN-Ego dataset to shrink the instructionaction gap. Uni-NaVid [54] further unifies the navigation subtasks - object search, instruction follow, and more - within single video-conditioned vision-language-action backbone, achieving real-time inference trajectories. Incoming generalist agents, OctoNav [13] introduces hybrid training paradigm that integrates the chain-of-thought of Think-Before-Action with GRPO and RL online, enabling faithful compliance with free-form commands and continuous control. Most recently, MTU3D [58] bridges visual grounding and active exploration by treating unexplored regions as frontier queries within unified vision-language-exploration objective; pre-training on one million trajectories lifts success rates by 1423% on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA while supporting language, category, and image goals alike. Collectively, these advances showcase the growing capacity of LVLMs to fuse perception, reasoning, and action for adaptive embodied navigation. III. DATASETS We build our framework on public 3D vision language datasets and the newly constructed Nav-CoT-110K dataset. Table summarizes the statistics of all datasets considered in this work, including scene coverage, task types, instruction modalities, and environment settings. A. Public Datasets We draw on several widely used embodied AI benchmarks to ensure diversity and comparability. R2R [4] and its continuous extension R2R-CE [29] provide natural language navigation instructions in Matterport3D scenes. RxR-CE [30] extends this setup to multilingual setting with dense temporal grounding. For object-goal navigation, SOON [57] introduces category-conditioned search in indoor environments, while HM3D-OVON [52] further supports open-vocabulary object navigation under zero-shot setting. Together, these benchmarks cover both instruction-following and objectcentric navigation tasks across discrete and continuous environments. B. Synthetic Dataset Building upon these resources, we introduce Nav-CoT110K, large-scale dataset consisting of 110K step-bystep Chain-of-Thought trajectories. Unlike prior datasets that primarily provide instructions and target locations, NavCoT-110K explicitly includes structured reasoning aligned with multimodal observations, thereby bridging perception, language, and action. This dataset serves as the foundation for the cold-start stage of Nav-R1, enabling it to acquire structured reasoning capabilities before reinforcement learning. IV. THE PROPOSED METHOD A. Overview The proposed Nav-R1 framework is designed as an embodied foundation model that unifies multimodal perception, structured reasoning, and embodied control. As illustrated in Fig. 2, Nav-R1 adopts Fast-in-Slow reasoning paradigm, where the slow system performs long-horizon semantic reasoning while the fast system ensures low-latency navigation. The framework follows two-stage training pipeline: CoT data engine is first used to construct the NavCoT-110K dataset, which provides high-quality step-by-step reasoning trajectories. Based on this dataset, cold-start stage initializes the models reasoning ability, followed by reinforcement learning stage with multi-dimensional rewards to refine semantic grounding and navigation fidelity. Fig. 3. CoT Data Engine. We construct the Nav-CoT dataset by defining navigation instructions, integrating egocentric visual inputs, providing action options and specifying the output format. These components are fed into Gemini 2.5 Pro, which generates step-by-step reasoning and action decisions aligned with navigation goals. environment constraints, and instruction semantics, and to produce structured step-by-step CoT sequences. The outputs follow standardized format, with reasoning enclosed in <think>...</think> tags and the chosen action in <action>...</action> tags, ensuring transparent alignment between observations, reasoning, and decisions. Running this pipeline across diverse environments yields approximately 115K CoT examples, each consisting of scene ID, navigation instruction, visual inputs, structured reasoning, and corresponding action. These raw outputs are then refined through two-stage filtering pipeline: (i) rulebased checks to discard incomplete or logically inconsistent responses, and (ii) quality verification by cross-validating actions against feasible navigation paths. After refinement, the 110K examples form the Nav-CoT-110K dataset, which serves as the cold-start initialization corpus for Nav-R1, providing rich reasoning trajectories that tightly couple perception, instruction following, and navigation decision making. B. CoT Data Engine C. Cold Start Stage We introduce CoT data engine designed to construct high-quality Chains of Thought (CoT) for embodied navigation and reasoning tasks. This engine harnesses the reasoning abilities of vision-language models (VLMs) to generate coherent step-by-step rationales that inform navigation decisions in complex 3D environments. As shown in Fig. 3, the process begins with egocentric visual observations extracted from 3D scenes, providing rich contextual views aligned with the agents perspective. In parallel, we incorporate navigation instructions drawn from standard embodied AI benchmarks - VLN tasks rely on R2R [4], R2R-CE [29], and RxR-CE [30] datasets, while ObjectNav tasks use instructions from SOON [57] and OVON [52]. To guide reasoning, we design composite prompt containing four essential components: (1) navigation instruction, (2) egocentric visual input, (3) set of feasible actions at each step, and (4) explicit formatting requirements. This prompt directs Gemini 2.5 Pro [43] to reason over spatial relations, Although reinforcement learning has shown remarkable effectiveness in reasoning-intensive models such as DeepSeekR1 [12], directly applying RL to large 3D vision language models often leads to unstable optimization. In particular, the policy tends to generate semantically incoherent CoT sequences or produce actions that fail to align with navigation instructions, making it difficult for the model to converge from scratch. To address this issue, we adopt cold-start stage based on supervised fine-tuning. Specifically, the NavCoT-110K dataset generated by our CoT data engine is employed to initialize Nav-R1. This supervised training step equips the policy with essential capabilities to produce structured reasoning sequences in the format of <think>...</think><action>...</action> and ground them to corresponding navigation actions. By bootstrapping the model with coherent reasoning and action patterns, the cold-start stage stabilizes subsequent RL optimization and provides smooth transition to the Fig. 4. The pipeline of RL Policy. The policy model generates outputs from text-image input. Then understanding reward (answer correctness and semantic alignment), navigation reward (path fidelity and endpoint accuracy), and format reward (structure adherence) are computed, grouped, and combined with KL term to frozen reference model to update the policy. reinforcement learning stage, where multi-dimensional rewards further refine semantic understanding, path fidelity, and structural adherence. D. Reinforcement Learning Group Relative Policy Optimization (GRPO) [40] has recently demonstrated strong effectiveness in reasoningintensive tasks, such as DeepSeek R1 [12]. Its core principle is to refine the policy through group-based feedback: multiple candidate responses are sampled from the current policy, scored by task-specific reward functions, and updated according to their relative advantages. As depicted in Fig. 4, extending this framework to embodied 3D vision-language tasks, Nav-R1 introduces three complementary rewards that jointly supervise training: Format Reward, which enforces the structural validity of the outputs; Understanding Reward, which ensures semantic correctness and visual grounding in 3D scene reasoning; and Navigation Reward, which evaluates path fidelity and goal-reaching accuracy in navigation. Together, these rewards provide multidimensional feedback that balances linguistic structure, semantic understanding, and action execution. a) Policy samples: For an input state (x, q), where encodes multi-modal scene representations and denotes the instruction or question, Nav-R1 generates candidate responses {o1, o2, , oN} from policy πθ . Each candidate corresponds to either an answer prediction for scene understanding or an action prediction for navigation. These responses are then evaluated using the following reward functions. reward RFormat adheres b) Format reward: To guarantee structural consistency, format each output strictly template: <think>...</think><answer>...</answer> or <think>...</think><action>...</action>. Formally, reasoning-decision verifies whether the to RFormat = (cid:26)1, 0, if output adheres to format otherwise . (1) This constraint not only ensures machine-parseable outputs but also disentangles reasoning from final predictions. c) Understanding reward: The understanding reward Runderstanding assesses whether the model demonstrates genuine comprehension of the 3D scene. It consists of two components: Answer Reward and Semantic Reward. The answer reward measures exact correctness against ground truth: Rans = (cid:26)1, 0, if predicted answer equals ground truth otherwise , (2) while the semantic reward measures alignment between the generated answer ˆa and the paired RGB-D image I: Rsem = CLIPScore(I, ˆa). The overall understanding reward is defined as: Runderstanding = Rans + Rsem. (3) (4) This design prevents both factual errors and semantically irrelevant outputs. d) Navigation reward: For navigation tasks, the navigation Reward Rnavigation evaluates whether the agent follows the instruction and successfully reaches the target. It integrates two components: path reward, which measures the fidelity of the trajectory with respect to the reference path ˆT , and an endpoint reward, which enforces the precision of the final location ˆp. Given predicted trajectory and ground-truth trajectory ˆT , the path reward is defined as: Rpath = exp (cid:0) kDF (T, ˆT )(cid:1), (5) where DF () denotes trajectory distance metric and is decay coefficient. Furthermore, the endpoint reward penalizes deviations between the predicted endpoint and the ground-truth endpoint ˆp: Rend = exp (cid:0) ˆp p2(cid:1). The final navigation reward is the combination of both: Rnavigation = Rpath + Rend. (6) (7) This formulation ensures that both trajectory-level alignment and goal-reaching ability are optimized. e) Policy optimization: Inspired by Group Relative Policy Optimization (GRPO) [40], we sample multiple candidate responses {o1, o2, , oN} from the current policy πθ , and compute their corresponding rewards = {r1, r2, , rN} from the above functions. Each reward is normalized to compute the relative advantage: ˆAi = ri mean(r) std(r) , (8) where ˆAi denotes the advantage of the i-th response. The policy is then updated by maximizing the clipped GRPO objective with KL regularization: JGRPO(θ ) =Ec (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "(cid:18) πθ (oiq) πθold (oiq) ˆAi, (cid:18) min i=1 (cid:18) πθ (oiq) πθold(oiq) clip , 1 ε, 1 + ε (cid:19) (cid:19) ˆAi (9) β DKL(πθ πref) (cid:19)(cid:35) . E. Fast-in-Slow Reasoning Inspired by dual system theories of human cognition [27], we propose Fast-in-Slow paradigm for Nav-R1, which tightly couples deliberate semantic reasoning with rapid action execution. As shown in Fig. 2, this design addresses the tension between accurate long-horizon planning and lowlatency control in dynamic embodied environments. TABLE II OBJECT GOAL NAVIGATION RESULTS ON HM3D-OVON [52]. DENOTES ZERO-SHOT METHODS. Method Val-Seen Val-Seen-Synonyms Val-Unseen SR SPL SR SPL SR SPL BC [35] DAgger [38] RL [39] BCRL [47] DAgRL [6] VLFM [51] DAgRL+OD [52] Uni-NaVid [54] MTU3D [58] Nav-R1 (Ours) 11.1 11.1 18.1 39.2 41.3 35.2 38.5 41.3 55.0 58. 4.5 4.5 9.4 18.7 21.2 18.6 21.1 21.1 23.6 26.3 9.9 9.9 15.0 27.8 29.4 32.4 39.0 43.9 45.0 48.1 3.8 3.8 7.4 11.7 14.4 17.3 21.4 21.8 14.7 23. 5.4 5.4 10.2 18.6 18.3 35.2 37.1 39.5 40.8 42.2 1.9 1.9 4.7 7.5 7.9 19.6 19.8 19.8 12.1 20.1 a) Slow reasoning: The slow system (System 2) operates at lower frequency and processes multimodal observations, including egocentric RGB-D frames and language instructions. It aggregates the historical context into compact memory states and outputs latent features ht that encode scene semantics, temporal dependencies, and global navigation goals. These structured features provide high-level guidance to ensure coherent decision-making. In practice, the slow system aggregates visual history into compact memory states, enabling Nav-R1 to maintain semantic consistency at the scene level while avoiding excessive computation. b) Fast reasoning: The fast system (System 1) runs at higher frequency and ensures real-time responsiveness in dynamic environments. Instead of independently reasoning, it reuses the final transformer blocks of Nav-R1 to inherit pretrained knowledge from System 2 while keeping its computation lightweight. At each step, the fast system fuses high-frequency egocentric multimodal inputs, including RGB frames, depth maps, and point cloud tokens (ot+1, . . . , ot+H ), with the latent feature ht from the slow system to predict short-horizon sequence of actions: {at+1, . . . , at+H } = πfast(ot+1:t+H , ht ), (10) where πfast denotes the high-frequency policy model that integrates visual, depth, and 3D geometric cues for real-time control. c) Asynchronous coordination: To balance efficiency and accuracy, we design an asynchronous update mechanism with frequency ratio of 1 : between the slow and fast systems. An update from the slow system provides latent guidance for consecutive steps of fast execution. This decoupling strategy ensures that global semantics remain stable, while local control is executed with low latency. Empirically, we find that 3 achieves the best balance between semantic fidelity and responsiveness in embodied navigation tasks, yielding both robust long-horizon reasoning and efficient real-time control. V. EXPERIMENTS A. Benchmarks and Metrics a) Benchmarks: To thoroughly evaluate Nav-R1, we adopt diverse set of embodied AI benchmarks that span TABLE III COMPARISON WITH STATE-OF-THE-ART METHODS ON THE VAL-UNSEEN SPLIT OF R2R-CE [4] AND RXR-CE [30]. INDICATES METHODS USING THE WAYPOINT PREDICTOR FROM [16]. NAV-R1 OUTPERFORMS ALL METHODS THAT DO NOT RELY ON SIMULATOR PRE-TRAINED WAYPOINT PREDICTORS, EVEN WHEN THOSE METHODS LEVERAGE ADDITIONAL INPUTS SUCH AS DEPTH, PANORAMIC VIEWS, AND ODOMETRY."
        },
        {
            "title": "Method",
            "content": "CMA [16] Sim2Sim [28] GridMM [49] Ego2-Map [17] DreamWalker [45] Reborn [2] ETPNav [1] HNR [48] AG-CMTP [8] R2R-CMTP [8] InstructNav [32] LAW [37] CM2 [14] WS-MGMap [9] AO-Planner [7] Seq2Seq [29] CMA [29] NaVid [55] Uni-NaVid [54] NaVILA [11] VLN-R1 [36] OctoNav [13] StreamVLN [50] CorrectNav [53] Nav-R1 (Ours)"
        },
        {
            "title": "Observation",
            "content": "R2R-CE Val-Unseen RxR-CE Val-Unseen S.RGB Pano."
        },
        {
            "title": "Depth",
            "content": "Odom. NE OS 6.20 6.07 5.11 5.54 5.53 5.40 4.71 4.42 7.90 7.90 6.89 6.83 7.02 6.28 5.55 7.77 7.37 5.47 5.58 5.22 7.00 - 4.98 4.24 3. 52.0 52.0 61.0 56.0 59.0 57.0 65.0 67.0 39.0 38.0 - 44.0 41.0 47.0 59.0 37.0 40.0 49.0 53.5 62.5 41.2 42.9 64.2 67.5 74.1 SR 41.0 43.0 49.0 47.0 49.0 50.0 57.0 61.0 23.0 26.0 31.0 35.0 34.0 38.0 47.0 25.0 32.0 37.0 47.0 54.0 30.2 37.1 56.9 65. 72.5 SPL 36.0 36.0 41.0 41.0 44.0 46.0 49.0 51.0 19.0 22.0 24.0 31.0 27.0 34.0 33.0 22.0 30.0 35.0 42.7 49.0 21.8 33.6 51.9 62.3 68.8 NE 8.76 - - - - 5.98 5.64 5.50 - - - 10.90 - - 7.06 12.10 - - 6.24 6.77 9.10 - 6.22 4.09 3.98 SR 26.5 - - - - 48.6 54.7 56.3 - - - 8.0 - - 43.3 13.9 - - 48.7 49.3 22.7 - 52.9 69. 71.3 SPL nDTW 22.1 - - - - 42.0 44.8 46.7 - - - 8.0 - - 30.5 11.9 - - 40.9 44.0 17.6 - 46.0 63.3 66. 47.0 - - - - 63.3 61.9 63.5 - - - 38.0 - - 50.1 30.8 - - - 58.8 - - 61.9 75.2 79.4 navigation, dialogue, reasoning, and planning tasks. For embodied navigation, we benchmark on R2R-CE [29] and RxR-CE [30], where agents navigate to goal locations in unseen environments, as well as HM3D for standard object goal navigation. To further test open-vocabulary generalization, we adopt HM3D-OVON [52], which introduces novel categories under zero-shot setting. Embodied dialogue and planning are evaluated on 3D-LLM [18], which requires generating natural responses and coherent multi-step action plans. Embodied reasoning is assessed on SQA3D [33], which involves spatial question answering over complex 3D scenes. b) Metrics: For embodied navigation, we follow standard metrics [4] including the navigation error (NE), success rate (SR), oracle success rate (OS), success weighted by path length (SPL) [3], and normalized dynamic time warping (nDTW) [26]. For embodied dialogue, planning, and reasoning tasks, we report widely used language generation metrics such as CIDEr (C) [44], BLEU-4 (B-4) [34], METEOR (M) [5], and ROUGE-L (R) [31]. B. Implementation Details provide RGB-D egocentric input, candidate action sets, and explicit output formatting to Gemini 2.5 Pro [43], which generates reasoning traces in <think> tags and the corresponding actions in <action> tags or answers in <answer> tags. two-stage filtering process is applied: (i) rule-based checks discard incomplete or logically inconsistent responses, and (ii) trajectory verification ensures action feasibility against ground-truth paths. After filtering, 110K high-quality trajectories remain and are used for cold-start training. b) Cold-start initialization: We initialize Nav-R1 from the pre-trained 3D-R1 model [23], which already provides strong 3D reasoning and vision-language alignment. On top of this initialization, we perform supervised fine-tuning (SFT) on Nav-CoT-110K for 2 epochs with batch size of 8. The AdamW optimizer is used with weight decay 0.01, and cosine annealing learning rate schedule decays from 104 to 105. This stage equips Nav-R1 with the ability to generate coherent reasoning-action sequences of the form <think>...</think><action>...</action>, ensuring structural adherence and semantic grounding before reinforcement learning. a) Data synthesis: We first construct the Nav-CoT110K dataset using our CoT data engine. The instructions are sampled from R2R [4], R2R-CE [29], RxR-CE [30], SOON [57], and HM3D-OVON [52]. For each scene, we c) Reinforcement learning: After cold-start initialization, we fine-tune the model with Group Relative Policy Optimization (GRPO) [40]. For each input, multiple responses are sampled and scored with three complementary rewards: TABLE IV EMBODIED DIALOGUE AND PLANNING RESULTS ON 3D-LLM [18]. EMBODIED REASONING RESULTS ON SQA3D [33]."
        },
        {
            "title": "Method",
            "content": "LL3DA [10] Spatial 3D-LLM [46] LSceneLLM [56] LEO [22] 3D-R1 [23]"
        },
        {
            "title": "Embodied reasoning",
            "content": "C 190.01 - 104.98 - 280.34 B-4 23.95 - - - 39.45 R 23.50 - 21.26 - 66.89 40.61 - 36.00 - 55.34 128.80 195.92 214.63 - 230.50 B-4 12.95 14.65 - - 25. R 17.05 18.95 21.05 - 48.34 39.25 36.93 47.05 - 55.67 - - - 124.70 138. B-4 - - - 9.40 23.56 R - - - 25.50 35.45 - - - 48.40 60. Nav-R1 (Ours) 281.20 39.34 67.53 55.12 230. 25.98 47.11 56.23 139.98 23.20 36. 59.50 TABLE QUANTITATIVE RESULTS OF REAL-WORLD EXPERIMENTS ACROSS THREE DISTINCT INDOOR ENVIRONMENTS. WE REPORT NAVIGATION ERROR (NE) AND SUCCESS RATE (SR) FOR NAV-R1 AND FOUR BASELINES. NAV-R1 CONSISTENTLY OUTPERFORMS ALL COMPETING METHODS."
        },
        {
            "title": "Method",
            "content": "NaVILA [11] NaVid [55] Uni-NaVid [54] MTU3D [58] Nav-R1 (Ours) NE 2.06 1.88 1.76 1.64 1.23 SR 0.45 0.55 0.64 0.73 1.03 NE 2.21 2.22 2.13 1.98 0.98 SR 0.45 0.50 0.64 0.81 1.12 NE 1.97 1.94 1.87 1.62 1.24 SR 0.50 0.55 0.65 0.70 1.02 (i) format reward enforcing structural validity, (ii) an understanding reward that combines exact match correctness with CLIP-based semantic alignment, and (iii) navigation reward measuring trajectory fidelity and endpoint accuracy. RL training runs for 2 epochs with batch size 12, fixed learning rate 105, and KL penalty β = 0.02 against the frozen SFT policy. d) Parameter efficient tuning: To reduce training cost, we adopt parameter-efficient fine-tuning by injecting LoRA adapters [19] into the last 8 transformer blocks of the backbone. Each adapter is configured with rank = 6 and scaling factor α = 8, introducing 12M trainable parameters. In total, about 142M parameters are updated, reducing trainable parameters by 98% compared to full fine-tuning. All experiments are conducted on 4NVIDIA H20 GPUs. C. Main Results a) Embodied dialogue: Table IV shows that Nav-R1 maintains strong dialogue capability, achieving results close to 3D-R1 while outperforming previous baselines. This indicates that incorporating navigation-oriented reasoning does not weaken interaction quality. b) Embodied reasoning: For embodied reasoning, as shown in Table IV, Nav-R1 performs comparably to 3D-R1. This is consistent with our design choice, as we do not train additional understanding modules but instead preserve the same scene reasoning ability while prioritizing navigation improvements. Fig. 5. Real-world robot setup and deployment pipeline. (a) Hardware platform: the WHEELTEC R550 robot equipped with Jetson Orin Nano (on-board PC), M10P LiDAR for mapping, Astra Pro RGB-D camera for perception, and STM32 microcontroller for motor control. (b) Deployment process: egocentric visual inputs are transmitted to the embodied foundation model Nav-R1, which performs reasoning and navigation. The decisions are then sent back to the on-board PC and converted into low-level motor commands by the STM32 controller. c) Embodied planning: For embodied planning, the results in Table IV indicate that Nav-R1 performs on par with prior methods, generating coherent multi-step action sequences. This confirms that our Fast-in-Slow design and GRPO training preserve planning skills while primarily optimizing navigation. d) Embodied navigation: As shown in Table III and Table II, Nav-R1 consistently outperforms prior methods on both instruction-following and object-goal navigation benchmarks. It achieves higher success rates and trajectory efficiency while reducing navigation errors, demonstrating superior generalization across unseen environments. D. Real World Evaluation a) Robot type settings: As shown in Fig. 5, we use the WHEELTEC R550 as the mobile platform for realworld evaluation. The robot is equipped with Jetson Orin Nano as the on-board computing unit, an M10P LiDAR for environmental mapping, and an Astra Pro camera for RGB-D perception. Considering the limited edge computing resources of the platform, the embodied foundation model Nav-R1 is deployed on cloud server rather than running locally. The system operates in closed loop manner: the robot transmits egocentric RGB inputs to the cloud, where NavR1 performs reasoning and generates navigation decisions. These commands are then transmitted back to the on-board Fig. 6. Qualitative results from the real-world deployment of Nav-R1. We evaluate the agent in three indoor scenarios: meeting room, lounge, and corridor. Each scene illustrates the BEV trajectory and ego-centric video frames, showing the models ability to generalize to diverse layouts and object configurations in real-world environments. system, where an STM32 microcontroller converts them into PWM signals that directly control the robots motors. b) Scene setup and task types: To assess generalizability under various real-world conditions, we evaluate NavR1 in three distinct indoor scenes: meeting room, lounge, and corridor, each characterized by unique spatial layouts and object distributions. Across these settings, the robot is instructed to perform navigation-oriented tasks that vary in difficulty, ranging from short-horizon paths with clear line of sight to long-horizon trajectories involving clutter, obstacles, and occlusions. This setup enables comprehensive assessment of the robustness of Nav-R1 in handling heterogeneous layouts and task complexities. c) Quantitative real-world evaluation: To quantify performance, we compare Nav-R1 against previous navigation models including NaVILA [11], NaVid [55], UniNaVid [54], and MTU3D [58]. Each model is tested in the meeting room, lounge, and corridor with both simple and complex instructions. As summarized in Table V, NavR1 consistently achieves the best results, with significantly reduced navigation error (NE) and higher success rate (SR) across all three environments. d) Qualitative real-world evaluation: As shown in Fig. 6, Nav-R1 exhibits coherent trajectories across meeting room, lounge, and corridor scenes. It reliably reaches diverse targets such as chairs, sofas, and umbrellas, demonstrating robustness to clutter, narrow passages, and long-horizon paths in real-world settings. VI. TEST-TIME EFFICIENCY Test-time efficiency is critical for embodied models in the on-board practical robotics. As illustrated in Fig. 5, Jetson Orin Nano faces strict resource limits, making largescale inference prohibitively slow. To address this, Nav-R1 adopts cloud-assisted design, where egocentric inputs are streamed to remote server for reasoning, and only compact navigation commands are returned for execution. We benchmark NaVid [55] and Uni-NaVid [54] on both on-board and server inference. As shown in Table VI, both baselines incur substantial delays on the Orin Nano, while TABLE VI AVERAGE INFERENCE LATENCY COMPARISON. COMPARISON OF AVERAGE PER-FRAME INFERENCE LATENCY (MS) FOR NAVID, UNI-NAVID, AND NAV-R1 ON JETSON ORIN NANO AND REMOTE SERVER."
        },
        {
            "title": "Method",
            "content": "On-board (ms) Server (ms) NaVid [55] Uni-NaVid [54] Nav-R1 (Ours) 320 410 85 90 95 server-side execution reduces latency to below 100 ms. NavR1 achieves comparable efficiency with 95 ms latency on the server, which is only slightly slower than NaVid and UniNaVid due to its dual-system reasoning overhead, yet this marginal gap does not affect real-time embodied navigation. To ensure stable transmission, all experiments adopt high-speed WiFi 6E (802.11ax, 6 GHz band) network with 1.2 Gbps peak bandwidth and <10 ms access latency, covering the entire 200 m2 indoor test area. VII. CONCLUSION In this paper, we presented Nav-R1, an embodied foundation model designed to enhance both reasoning coherence and real-time navigation. To overcome the instability of reasoning traces and the difficulty of balancing long-horizon semantics with real-time responsiveness, we introduced the large-scale Nav-CoT-110K dataset for cold-start initialization, GRPO-based reinforcement learning framework with format, understanding, and navigation rewards, and Fastin-Slow dual-system paradigm that decouples semantic reasoning from reactive control. Comprehensive experiments on VLN, ObjectNav, embodied dialogue, planning, and reasoning benchmarks show that Nav-R1 achieves consistent improvements in navigation success, trajectory fidelity, and reasoning coherence, while maintaining dialogue and planning performance on par with 3D-R1. Moreover, real-world deployment on Jetson Orin Nano-powered mobile robot further validates its robustness under limited edge resources, with cloud-assisted inference enabling real-time closed-loop control."
        },
        {
            "title": "APPENDIX",
            "content": "VIII. ABLATION STUDY TABLE IX ABLATION ON KL PENALTY β . EVALUATION ON RXR-CE VAL-UNSEEN. a) Dual-system vs single-system: We compare the full dual-system Nav-R1 with single-system variants that only retain either the slow semantic reasoning system or the fast reactive control system. As shown in Table VII, both variants perform worse: the slow-only version struggles with realtime execution, while the fast-only version fails to maintain global semantic consistency. The dual-system achieves the best trade-off, confirming the effectiveness of asynchronous coordination. TABLE VII ABLATION ON DUAL-SYSTEM DESIGN. EVALUATION ON R2R-CE VAL-UNSEEN."
        },
        {
            "title": "Method",
            "content": "Slow-only Fast-only Dual-system (Ours) NE 5.12 5.47 3.86 OS 68.3 65.1 74.1 SR 61.2 58.7 72.5 SPL 54.3 50.1 68.8 b) Reward decomposition: To verify the effectiveness of the proposed reward design, we ablate the three rewards in RL training: format reward RFormat, understanding reward RUnderstanding, and navigation reward RNavigation. Table VIII shows that removing any reward leads to performance degradation. Without RFormat, the model often generates unstructured outputs. Removing RUnderstanding reduces semantic grounding, while dropping RNavigation severely hurts trajectory fidelity. The full combination achieves the best results, demonstrating that the three rewards are complementary. TABLE VIII REWARD DECOMPOSITION ON HM3D-OVON VAL-UNSEEN. DENOTES INCLUSION OF THE REWARD. RFormat RUnderstanding RNavigation SR 34.5 36.1 37.0 36.5 39.4 38.7 39.9 42.2 SPL 15.2 16.0 16.7 15.9 18.2 17.5 18.7 20.1 c) Hyper-parameters: We further study the sensitivity of Nav-R1 to hyper-parameters on the RxR-CE Val-Unseen split. Tables IX report the ablation result. For the KL penalty β , too small values cause divergence from the reference policy, while too large values limit exploration. The best trade-off is achieved at β = 0.02. IX. LIMITATION AND FUTURE WORK Although Nav-R1 achieves strong results, it still has several limitations. The Nav-CoT-110K dataset, though large, is mainly synthesized from existing benchmarks and does not β 0.005 0.01 0.02 (Ours) 0.03 0.05 NE 5.12 4. 3.98 4.23 4.36 SR 64.3 66.3 71.3 68.9 67. SPL nDTW 59.5 60.7 66.3 64.2 62.4 70.1 72. 79.4 75.1 74.2 fully capture real-world complexity. Our model also relies on RGB-D and language inputs, without integrating richer modalities such as audio or tactile signals. Moreover, current deployment depends on cloud inference, limiting real-time scalability on edge devices. Future work will explore expanding data coverage, incorporating multimodal perception, improving efficiency for on-board deployment, and extending to longer-horizon and more diverse embodied tasks. X. VISUALIZATION a) CoT example: To better illustrate how structured reasoning traces are generated, Fig. 7 visualizes representative Nav-CoT-110K example. It shows how natural language instructions, egocentric observations, and candidate actions are transformed into step-by-step reasoning content and navigation decisions, highlighting the role of CoT supervision in stabilizing model training. b) Results visualization: To further illustrate the realworld performance of Nav-R1, we provide qualitative visualizations across multiple embodied tasks and diverse indoor environments. As shown in Fig. 810, we first evaluate navigation-oriented behaviors in three distinct scenes. Each example depicts the natural language instruction, egocentric RGB observations, depth maps, and LiDAR-based top-down maps, complemented with third-person views and BEV trajectories for reference. Beyond navigation, we further demonstrate the versatility of Nav-R1 in dialogue, reasoning, and planning tasks. Fig. 1113 present real-world qualitative results on embodied dialogue, reasoning, and planning, respectively. These visualizations highlight that Nav-R1 not only executes goaldirected trajectories robustly, but also maintains coherent interaction, safe reasoning, and multi-step planning capabilities under complex real-world layouts. In addition to real-world deployment, we also provide benchmark visualizations on simulation datasets. Fig. 14 and Fig. 15 highlight the VLN-CE R2R benchmark, where Nav-R1 successfully grounds long-horizon instructions into coherent navigation trajectories. Similarly, Fig. 16 presents results on HM3D ObjectNav, showing that the agent can robustly explore large-scale 3D layouts and accurately localize target objects. These benchmark results further confirm that the proposed model generalizes across navigation paradigms. Fig. 7. Nav-CoT-110K CoT data example. Fig. 8. Real-world qualitative results of Nav-R1 on VLN and ObjectNav tasks in meeting room. Fig. 9. Real-world qualitative results of Nav-R1 on VLN and ObjectNav tasks in lounge. Fig. 10. Real-world qualitative results of Nav-R1 on VLN and ObjectNav tasks in corridor. Fig. 11. Real-world qualitative results of Nav-R1 on embodied dialogue task. Fig. 12. Real-world qualitative results of Nav-R1 on embodied reasoning task. Fig. 13. Real-world qualitative results of Nav-R1 on embodied planning task. Fig. 14. Visual results of VLN on VLN-CE R2R. Fig. 15. Visual results of VLN on VLN-CE R2R. Fig. 16. Visual results of ObjectNav on HM3D."
        },
        {
            "title": "REFERENCES",
            "content": "[1] D. An, H. Wang, W. Wang, Z. Wang, Y. Huang, K. He, and L. Wang, Etpnav: Evolving topological planning for vision-language navigation in continuous environments, PAMI, 2024. [2] D. An, Z. Wang, Y. Li, Y. Wang, Y. Hong, Y. Huang, L. Wang, and J. Shao, 1st place solutions for rxr-habitat vision-and-language navigation competition, in CVPRW, 2022. [3] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, et al., On evaluation of embodied navigation agents, arXiv preprint arXiv:1807.06757, 2018. [4] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. Van Den Hengel, Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 36743683. [5] S. Banerjee and A. Lavie, METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, in Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, J. Goldstein, A. Lavie, C.-Y. Lin, and C. Voss, Eds. Ann Arbor, Michigan: Association for Computational Linguistics, June 2005, pp. 6572. [Online]. Available: https://aclanthology.org/W05-0909/ [6] H. Chen, A. Suhr, D. Misra, N. Snavely, and Y. Artzi, Touchdown: Natural language navigation and spatial reasoning in visual street environments, in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 12 53012 539. [7] J. Chen, B. Lin, X. Liu, X. Liang, and K.-Y. K. Wong, Affordancesoriented planning using foundation models for continuous visionlanguage navigation, arXiv preprint arXiv:2407.05890, 2024. [8] K. Chen, J. K. Chen, J. Chuang, M. Vázquez, and S. Savarese, Topological planning with transformers for vision-and-language navigation, in CVPR, 2021. [9] P. Chen, D. Ji, K. Lin, R. Zeng, T. Li, M. Tan, and C. Gan, Weaklysupervised multi-granularity map learning for vision-and-language navigation, in NeurIPS, 2022. [10] S. Chen, X. Chen, C. Zhang, M. Li, G. Yu, H. Fei, H. Zhu, J. Fan, and T. Chen, Ll3da: Visual interactive instruction tuning for omni-3d understanding, reasoning, and planning, in CVPR, 2024, pp. 26 418 26 428. [11] A.-C. Cheng, Y. Ji, Z. Yang, Z. Gongye, X. Zou, J. Kautz, E. Bıyık, H. Yin, S. Liu, and X. Wang, Navila: Legged robot vision-languageaction model for navigation, arXiv preprint arXiv:2412.04453, 2024. [12] DeepSeek-AI, Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [13] C. Gao, L. Jin, X. Peng, J. Zhang, Y. Deng, A. Li, H. Wang, and S. Liu, Octonav: Towards generalist embodied navigation, arXiv preprint arXiv:2506.09839, 2025. [14] G. Georgakis, K. Schmeckpeper, K. Wanchoo, S. Dan, E. Miltsakaki, D. Roth, and K. Daniilidis, Cross-modal map learning for vision and language navigation, in CVPR, 2022. [15] A.-M. Halacheva, J.-N. Zaech, X. Wang, D. P. Paudel, and L. V. Gool, Gaussianvlm: Scene-centric 3d vision-language models using language-aligned gaussian splats for embodied reasoning and beyond, arXiv preprint arXiv:2507.00886, 2025. [16] Y. Hong, Z. Wang, Q. Wu, and S. Gould, Bridging the gap between learning in discrete and continuous environments for vision-andlanguage navigation, in CVPR, 2022. [17] Y. Hong, Y. Zhou, R. Zhang, F. Dernoncourt, T. Bui, S. Gould, and H. Tan, Learning navigational visual representations with semantic map supervision, in ICCV, 2023. [18] Y. Hong, H. Zhen, P. Chen, S. Zheng, Y. Du, Z. Chen, and C. Gan, 3dLLM: Injecting the 3d world into large language models, in Thirtyseventh Conference on Neural Information Processing Systems, 2023. [Online]. Available: https://openreview.net/forum?id=YQA28p7qNz [19] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, LoRA: Low-rank adaptation of large language models, in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/forum?id=nZeVKeeFYf9 [20] H. Huang, Y. Chen, Z. Wang, R. Huang, R. Xu, T. Wang, L. Liu, X. Cheng, Y. Zhao, J. Pang, and Z. Zhao, Chat-scene: Bridging 3d scene and large language models with object identifiers, in The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. [21] J. Huang, S. Yong, X. Ma, X. Linghu, P. Li, Y. Wang, Q. Li, S.-C. Zhu, B. Jia, and S. Huang, An embodied generalist agent in 3d world, in ICLR 2024 Workshop: How Far Are We From AGI, 2024. [Online]. Available: https://openreview.net/forum?id=ltX3S0juSa [22] , An embodied generalist agent in 3d world, in ICLR 2024 Workshop: How Far Are We From AGI, 2024. [Online]. Available: https://openreview.net/forum?id=ltX3S0juSa [23] T. Huang, Z. Zhang, and H. Tang, 3d-r1: Enhancing reasoning in 3d vlms for unified scene understanding, arXiv preprint arXiv:2507.23478, 2025. [24] T. Huang, Z. Zhang, Y. Wang, and H. Tang, 3d coca: Contrastive learners are 3d captioners, arXiv preprint arXiv:2504.09518, 2025. [25] T. Huang, Z. Zhang, R. Zhang, and Y. Zhao, Dc-scene: Datalearning for 3d scene understanding, arXiv preprint centric arXiv:2505.15232, 2025. [26] G. Ilharco, V. Jain, A. Ku, E. Ie, and J. Baldridge, General evaluation for instruction conditioned navigation using dynamic time warping, arXiv preprint arXiv:1907.05446, 2019. [27] D. Kahneman, Thinking, Fast and Slow. Allen Lane, 2011. [28] J. Krantz and S. Lee, Sim-2-sim transfer for vision-and-language navigation in continuous environments, in ECCV, 2022. [29] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, Beyond the nav-graph: Vision-and-language navigation in continuous environments, in Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVIII 16. Springer, 2020, pp. 104120. [30] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge, Roomacross-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding, in EMNLP, 2020. [31] C.-Y. Lin, ROUGE: package for automatic evaluation of summaries, in Text Summarization Branches Out. Barcelona, Spain: Association for Computational Linguistics, July 2004, pp. 7481. [Online]. Available: https://aclanthology.org/W04-1013/ [32] Y. Long, W. Cai, H. Wang, G. Zhan, and H. Dong, Instructnav: Zero-shot system for generic instruction navigation in unexplored environment, arXiv preprint arXiv:2406.04882, 2024. [33] X. Ma, S. Yong, Z. Zheng, Q. Li, Y. Liang, S.-C. Zhu, and S. Huang, Sqa3d: Situated question answering in 3d scenes, in International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=IDJx97BC38 automatic [34] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, Bleu: in method for Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ser. ACL 02. USA: Association for Computational Linguistics, 2002, p. 311318. [Online]. Available: https://doi.org/10.3115/1073083.1073135 evaluation of machine translation, [35] D. A. Pomerleau, Alvinn: an autonomous land vehicle in neural network, in Proceedings of the 2nd International Conference on Neural Information Processing Systems, ser. NIPS88. Cambridge, MA, USA: MIT Press, 1988, p. 305313. [36] Z. Qi, Z. Zhang, Y. Yu, J. Wang, and H. Zhao, Vln-r1: Visionlanguage navigation via reinforcement fine-tuning, arXiv preprint arXiv:2506.17221, 2025. [37] S. Raychaudhuri, S. Wani, S. Patel, U. Jain, and A. Chang, Languagealigned waypoint (law) supervision for vision-and-language navigation in continuous environments, in EMNLP, 2021. [38] S. Ross, G. Gordon, and D. Bagnell, reduction of imitation learning and structured prediction to no-regret online learning, in Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, ser. Proceedings of Machine Learning Research, G. Gordon, D. Dunson, and M. Dudík, Eds., vol. 15. Fort Lauderdale, FL, USA: PMLR, 1113 Apr 2011, pp. 627635. [39] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, preprint optimization algorithms, arXiv Proximal arXiv:1707.06347, 2017. policy [40] Z. Shao, P. Wang, ihao Zhu, R. Xu, J. Song, M. Zhang, Y. W. Y.K. Li, and D. Guo, Deepseekmath: Pushing the limits of mathematical reasoning in open language models, CoRR, vol. abs/2402.03300, 2024. [Online]. Available: https://arxiv.org/abs/2402.03300 [41] Z. Song, G. Ouyang, M. Fang, H. Na, Z. Shi, Z. Chen, F. Yujie, Z. Zhang, S. Jiang, M. Fang, et al., Hazards in daily life? enabling robots to proactively detect and resolve anomalies, in Proceedings of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2025, pp. 73997415. the 2025 Conference of the Nations of [42] Z. Song, G. Ouyang, M. Li, Y. Ji, C. Wang, Z. Xu, Z. Zhang, X. Zhang, Q. Jiang, Z. Chen, et al., Maniplvm-r1: Reinforcement learning for reasoning in embodied manipulation with large visionlanguage models, arXiv preprint arXiv:2505.16517, 2025. [43] L. Team, A. Modi, A. S. Veerubhotla, A. Rysbek, A. Huber, A. Anand, A. Bhoopchand, B. Wiltshire, D. Gillick, D. Kasenberg, E. Sgouritsa, G. Elidan, H. Liu, H. Winnemoeller, I. Jurenka, J. Cohan, J. She, J. Wilkowski, K. Alarakyia, K. R. McKee, K. Singh, L. Wang, M. Kunesch, M. Pîslar, N. Efron, P. Mahmoudieh, P.-A. Kamienny, S. Wiltberger, S. Mohamed, S. Agarwal, S. M. Phal, S. J. Lee, T. Strinopoulos, W.-J. Ko, Y. Gold-Zamir, Y. Haramaty, and Y. Assael, Evaluating gemini in an arena for learning, arXiv preprint arXiv:2505.24477, 2025. [44] R. Vedantam, C. L. Zitnick, and D. Parikh, Cider: Consensus-based image description evaluation, in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 45664575. [45] H. Wang, W. Liang, L. Van Gool, and W. Wang, Dreamwalker: Mental planning for continuous vision-language navigation, in ICCV, 2023. [46] X. Wang, Z. Li, Y. Xu, J. Qi, Z. Yang, R. Ma, X. Liu, and C. Zhang, Spatial 3d-llm: Exploring spatial awareness in 3d vision-language models, arXiv preprint arXiv:2507.16524, 2025. [47] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang, W. Y. Wang, and L. Zhang, Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation, in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 66226631. [48] Z. Wang, X. Li, J. Yang, Y. Liu, J. Hu, M. Jiang, and S. Jiang, Lookahead exploration with neural radiance representation for continuous vision-language navigation, in CVPR, 2024. [49] Z. Wang, X. Li, J. Yang, Y. Liu, and S. Jiang, Gridmm: Grid memory map for vision-and-language navigation, in ICCV, 2023. [50] M. Wei, C. Wan, X. Yu, T. Wang, Y. Yang, X. Mao, C. Zhu, W. Cai, H. Wang, Y. Chen, X. Liu, and J. Pang, Streamvln: Streaming vision-and-language navigation via slowfast context modeling, arXiv preprint arXiv:2507.05240, 2025. [51] N. Yokoyama, S. Ha, D. Batra, J. Wang, and B. Bucher, Vlfm: Visionlanguage frontier maps for zero-shot semantic navigation, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 4248. [52] N. Yokoyama, R. Ramrakhya, A. Das, D. Batra, and S. Ha, Hm3dovon: dataset and benchmark for open-vocabulary object goal navigation, in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2024, pp. 55435550. [53] Z. Yu, Y. Long, Z. Yang, C. Zeng, H. Fan, J. Zhang, and H. Dong, Correctnav: Self-correction flywheel empowers visionlanguage-action navigation model, arXiv preprint arXiv:2508.10416, 2025. [54] J. Zhang, K. Wang, S. Wang, M. Li, H. Liu, S. Wei, Z. Wang, Z. Zhang, and H. Wang, Uni-navid: video-based vision-languageaction model for unifying embodied navigation tasks, Robotics: Science and Systems, 2025. [55] J. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu, Z. Zhang, and W. He, Navid: Video-based vlm plans the next step for vision-and-language navigation, in RSS, 2024. [56] H. Zhi, P. Chen, J. Li, S. Ma, X. Sun, T. Xiang, Y. Lei, M. Tan, and C. Gan, Lscenellm: Enhancing large 3d scene understanding using adaptive visual preferences, arXiv preprint arXiv:2412.01292, 2024. [57] F. Zhu, X. Liang, Y. Zhu, Q. Yu, X. Chang, and X. Liang, Soon: Scenario oriented object navigation with graph-based exploration, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 12 68912 699. [58] Z. Zhu, X. Wang, Y. Li, Z. Zhang, X. Ma, Y. Chen, B. Jia, W. Liang, Q. Yu, Z. Deng, S. Huang, and Q. Li, Move to understand 3d scene: Bridging visual grounding and exploration for efficient and versatile embodied navigation, International Conference on Computer Vision (ICCV), 2025."
        }
    ],
    "affiliations": [
        "Peking University",
        "Shanghai University of Engineering Science"
    ]
}