{
    "paper_title": "Yume: An Interactive World Generation Model",
    "authors": [
        "Xiaofeng Mao",
        "Shaoheng Lin",
        "Zhen Li",
        "Chuanhao Li",
        "Wenshuo Peng",
        "Tong He",
        "Jiangmiao Pang",
        "Mingmin Chi",
        "Yu Qiao",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \\method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \\sekai to train \\method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 4 4 7 7 1 . 7 0 5 2 : r YU E: AN INTERACTIVE WORLD GENERATION MODEL Xiaofeng Mao1,2, Shaoheng Lin1, Zhen Li1, Chuanhao Li1, Wenshuo Peng1, Tong He1, Jiangmiao Pang1, Mingmin Chi2, Yu Qiao1, Kaipeng Zhang1,3 1Shanghai AI Laboratory 2Fudan University 3Shanghai Innovation Institute Github:https://github.com/stdstu12/YUME (cid:5) Huggingface:https://huggingface.co/stdstu123/Yume-I2V-540P Project Page: https://stdstu12.github.io/YUME-Project Ä± Data: https://github.com/Lixsp11/sekai-codebase"
        },
        {
            "title": "ABSTRACT",
            "content": "Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present preview version of Yume, which creates dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer (MVDT) with memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset Sekai to train Yume, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. We are looking for collaboration and self-motivated interns interested in interactive world generation. Contact: zhangkaipeng@pjlab.org.cn Figure 1: We introduce Yume, streaming interactive world generation model, which allows using continuous keyboard inputs to explore dynamic world created by an input image. This work was done during Xiaofengs internship at Shanghai AI Laboratory. Project Leader Corresponding Author"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Works 2.1 Video Diffusion Models . . . . . . . 2.2 Camera Control in Video Generation . 2.3 Navigatable World Generation . 2.4 Mitigating Generation Artifacts 2.5 Video Diffusion Acceleration . . 3 Preliminaries 3.1 Rectified Flow. . . 3.2 Wan Architecture . . . . . . . . . . . . . 4 Data Processing 4.1 Dataset . . . . . . . . . . . . 4.2 Camera Motion Quantization . . . . . 5 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Model Architecture . . . 5.2.1 Masked Video Diffusion Transformers . 5.2.2 5.2.3 . . Image-to-Video (I2V) and Video-to-Video (V2V) Generation . . . . Long Video Generatetion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Sampler Design . 5.3.1 5.3.2 . . . . . Training-Free Anti-Artifact Mechanism . . . Time Travel Sampling based on SDE (TTS-SDE) for Enhanced Video Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Camera Motion Control . . . . 5.5 Application . . . . 5.5.1 World Generalization . . 5.5.2 World Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6 Acceleration . . . . . . . Adversarial Distillation for Accelerated Diffusion Sampling . . . Cache-Accelerating . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.1 5.6.2 6 Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.1 6.2.2 6.3.1 6.3.2 6.1 Experimental Settings 6.1.1 6.1.2 6.1. . Training Details . Evaluation Dataset Evaluation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Qualitative Results . . . . . Image-to-Video Generation . . Validation of Long-video Generation Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Ablation study . . . . . . . . Verification of TTS-SDE Effectiveness . . Validating the effect of model distillation . . . . . . . . . . . . 6.4 Visualization Results . . . . . . . . . . . . . . . . 7 Conclusion Ablation Study on Image to Video Quantized Camera Motion of Text Action Distribution Statistics . . . . . . . . . . . . . . . . . . . . Core Algorithm Implementation of Gaussian Blur Kernel The Joint Optimization of Adversarial Distillation and Caching 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 4 4 5 5 6 6 7 7 7 7 8 8 9 9 10 12 12 14 14 15 15 15 15 16 16 17 17 17 17 17 18 18 19 19 19 19 20 26 26 26"
        },
        {
            "title": "INTRODUCTION",
            "content": "Past or future, just let them be Link Click Yume aims to create an interactive, realistic, and dynamic world through the input of text, images, or videos. And we can explore and control the world using peripheral devices or neural signals Zhou et al. (2025). practical application is to enter world through photo, like time travel, regardless of its location, scenes, and time, while you can interact with everything and change everything following your mind. We can compensate for our real-world regrets or realize our wishes through the virtual world created by Yume. In this paper, we present preview version of Yume, which is an interactive world generation model that allows the use of keyboard inputs to explore dynamic world created by an input image. We have released all data, code, and model weights on https: //github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Video diffusion models Singer et al. (2023); Bar-Tal et al. (2024); Blattmann et al. (2023a); Ma et al. (2024); Nagrath et al. (2024); Singer et al. (2023); The Step-Video-T2V Team (2025); Chen et al. (2025), which have shown remarkable capabilities in synthesizing high-fidelity and temporally coherent visual content Ho et al. (2022); Blattmann et al. (2023a), present promising avenue to realize such sophisticated interactive world generation task. Recently, the automated generation of vast, interactive, and persistent virtual worlds Zhang et al. (2025); Agarwal et al. (2025) has advanced rapidly, driven by progress in generative models and the growing demand for immersive experiences in domains such as world simulation, interactive entertainment Li et al. (2025a), and virtual embodiment Xiao et al. (2025); Zhang et al. (2025). The motion of the camera is an important control signal for interactive video generation of world exploration. However, existing video diffusion methods face significant challenges when applied to producing interactive (continuous camera motion controlled) and realistic videos, especially in urban scenarios. First, existing approaches Zhang et al. (2025); Xiao et al. (2025) focus mainly on synthetic or static scenarios, in terms of methods and data. The domain gap between them and the real world limits their generalizability. In addition, these methods are mostly based on absolute camera motions, which require precise annotation and extra-learner modules that increase the training and architecture design difficulty. In addition, urban environments present unique complexities characterized by diverse architectural styles, dynamic objects, and intricate details. Existing approaches demonstrate limited adaptability to such complexity and struggle to maintain consistent realism across varied scenes. Common visual artifacts, including flickering, unnatural textures, and geometric distortions, significantly degrade the perceptual quality and disrupt immersive experiences. To address these limitations, we introduce Yume for the generation of autoregressive interactive video through an input image with discrete keyboard control. Yume is designed to offer more intuitive and stable camera control via keyboard input while improving the visual quality and realism of complex scene generation. Implements systematic optimizations on four key dimensions: camera motion control, model architecture, sampler, and model acceleration. First, we proposed Quantized Camera Motion (QCM) control module. Yume quantizes camera trajectories into translational movements (forward, backward, left, right) and rotational motions (turn right, turn left, tilt up, tilt down), which can be flexibly combined and transferred by keyboard input. The QCM is produced by changes in relative camera poses during training and naturally embeds temporal context and spatial relationships into the control signal. QCM are parsed into textual conditions without introducing new learnable modules for pretrained I2V foundation models. Second, as noted in Zhang et al. (2025), text-based control frequently leads to unnatural outputs. To address this limitation, we investigated an alternative approach using Masked Video Diffusion Transformers (MVDT), inspired by prior work Gao et al. (2023). Moreover, we achieved interactive video generation with theoretically infinite duration via chunk-based autoregressive generation framework with modified FramePack memory module Zhang & Agrawala (2025). Third, to improve the visual quality of sophisticated real-world scenes, we introduce training-free Anti-Artifact Mechanism (AAM) module. Specifically, it refines the high-frequency components of the latent representation at each diffusion step. This targeted refinement improves fine-grained details, smooths out inconsistencies, and substantially reduces visual artifacts, leading to an overall improvement in visual quality without requiring any additional model training or specialized datasets. 3 However, it does not perform well on autogressive long video generation due to lack of V2V foundation models, and we will add it to our long video generation in the next version of Yume. Furthermore, we propose novel sampling method based on the Time-Travel Stochastic Differential Equation (TTS-SDE) framework. Inspired by DDNM Wang et al. (2022) and OSV Mao et al. (2025), our approach leverages information from later denoising stages to guide the earlier denoising process, while incorporating stochastic differential equations to enhance sampling randomness, thereby improving textual controllability. Finally, we investigate multiple acceleration techniques for diffusion-based video generation and jointly optimize step distillation and cache mechanisms. It significantly enhances sampling efficiency without compromising visual fidelity or temporal coherence. Yume offers significant step towards generating high-quality, dynamic, and interactive infinite video generation, particularly for complex real-world scenes exploration."
        },
        {
            "title": "2.1 VIDEO DIFFUSION MODELS",
            "content": "Diffusion models Sohl-Dickstein et al. (2015); Ho et al. (2020), initially transformative for image synthesis, have rapidly become the cornerstone of video generation. The application of these models in compressed latent space, pioneered by Latent Diffusion Models (LDMs), was key enabler for efficient video synthesis, leading to works like Video LDM (Align your Latents) Blattmann et al. (2023b) which extended this paradigm to achieve high-resolution video generation by integrating temporal awareness. Early breakthroughs in text-to-video generation, such as Imagen Video Ho et al. (2022) and Make-AVideo Singer et al. (2023), quickly demonstrated the potential for creating dynamic scenes from textual descriptions. The field has since witnessed significant advancements in model scale, architectural design, and training strategies. Large-scale models like Googles Lumiere Bar-Tal et al. (2024), featuring Space-Time U-Net, and OpenAIs Sora Brooks et al. (2024), which employs diffusion transformer architecture, have significantly pushed the boundaries of generating long, coherent, and high-fidelity video content. Concurrently, the open-source ecosystem has flourished, with contributions such as Stable Video Diffusion Blattmann et al. (2023a) offering robust and accessible baselines. Recent notable open-source efforts include HunyuanVideo Ma et al. (2024), which provides systematic framework for very large video models, MoChi-Diffusion-XL Nagrath et al. (2024), focusing on efficient high-resolution video synthesis, Step-Video-T2V The Step-Video-T2V Team (2025), large-parameter foundation model, and SkyReels-V2 Chen et al. (2025), which aims for generating extended film-like content. Based on these advanced foundation models, we built Yume for realistic dynamic world exploration."
        },
        {
            "title": "2.2 CAMERA CONTROL IN VIDEO GENERATION",
            "content": "Precise and flexible camera control is crucial for creating compelling and customizable video content, allowing for the emulation of cinematic effects and more interactive user experience. Early video generation models often lacked explicit mechanisms for detailed camera path guidance, with camera movements emerging implicitly from textual prompts or initial frames. Subsequent research has focused on more direct methods for specifying and integrating camera motion into the generation process. significant body of work has emerged to explicitly condition video diffusion models on camera parameters. For instance, MotionCtrl Wang et al. (2023b) introduced unified controller to manage both camera and object motion, where camera motion is determined by sequence of camera poses that are temporally fused into the video generation model. Following this, Direct-a-Video Yang et al. (2024) enabled decoupled control of camera pan/zoom and object motion, utilizing temporal cross-attention layers for interpreting quantitative camera movement parameters, trained via selfsupervised approach. CameraCtrl He et al. (2024) proposed plug-and-play module to integrate accurate camera pose control into existing video diffusion models, exploring effective trajectory parameterizations. Further advancing fine-grained control, CameraCtrl II Zhang et al. (2024) focused 4 on dynamic scene exploration by allowing iterative specification of camera trajectories for generating coherent, extended video sequences. More recently, training-free approaches have also been investigated; for example, CamTrol (Training-free Camera Control) Geng et al. (2024) leverages 3D point cloud modeling from single image and manipulates latent noise priors to guide camera perspective without requiring model fine-tuning or camera-annotated datasets. These methods typically rely on explicit sequences of absolute camera pose matrices or derived parameters to define the camera trajectory, aiming to improve the stability and precision of the generated viewpoint transformations. In contrast to existing approaches that rely on explicit camera trajectories (requiring fine-grained motion parameter adjustments), we innovatively propose Quantized Camera Motion (QCM) mechanism, which achieves intuitive keyboard-based control by discretizing the camera pose space."
        },
        {
            "title": "2.3 NAVIGATABLE WORLD GENERATION",
            "content": "The generation of expansive, interactive, and temporally coherent virtual worlds is long-standing ambition in artificial intelligence, with significant implications for gaming, simulation, and robotics. Early efforts in this domain often involved learning world models Ha & Schmidhuber (2018) that capture environment dynamics to enable agents to plan or learn behaviors within learned latent space, as demonstrated by subsequent lines of work like the Dreamer series Hafner et al. (2023). These models emphasized understanding and predicting future states based on actions, laying the groundwork for more explicit world generation. More recent approaches have focused directly on generating interactive environments and controllable long-duration video sequences. For instance, Genie Bruce et al. (2024) introduced foundation model capable of generating an endless variety of action-controllable 2D worlds from image prompts, trained on unlabeled internet videos. Efforts to generate navigable driving scenes, such as GAIA-1 Wayve Technologies (2023) by Wayve, have demonstrated the potential for creating realistic, controllable driving experiences from text and action inputs. Similarly, projects like SIMA Google DeepMind (2024) aim to develop generalist AI agents that can understand and interact within diverse 3D virtual settings based on natural language instructions. The challenge of maintaining long-term consistency in generated worlds, especially for extended exploration, is actively being addressed. For example, StreamingT2V Henschel et al. (2024) proposed methods for consistent and extendable long video generation. Very recent works are pushing the envelope on creating interactive foundation models for worlds and ensuring their coherence. Matrix-Game Zhang et al. (2025) presents an interactive world foundation model aimed at controllable game world generation, emphasizing fine-grained action control. Concurrently, WORLDMEM Xiao et al. (2025) introduces framework to enhance long-term consistency in world simulation by employing memory bank and attention mechanisms to accurately reconstruct previously observed scenes, even with significant viewpoint or temporal changes, and to model dynamic world evolution. These advancements are critical for generating the kind of scene-level, interactive, and navigable experiences that are the focus of current research. However, these methods primarily rely on game-based scenarios and actions (e.g., Matrix-Game Zhang et al. (2025) utilizes the Minecraft dataset), which remain relatively simplistic compared to real-world environments. Furthermore, collecting motion data from real-world video datasets is significantly more challenging than collecting it in game settings."
        },
        {
            "title": "2.4 MITIGATING GENERATION ARTIFACTS",
            "content": "Despite the remarkable progress of diffusion models in generating realistic images and videos, the synthesized results can still suffer from various artifacts, such as unnatural textures, flickering, or semantic inconsistencies, particularly in complex scenes or long video sequences. Efforts to mitigate these issues can be broadly categorized into training-based and training-free approaches. Training-based methods often involve architectural modifications or specialized fine-tuning strategies. For instance, some works focus on improving the autoencoder stage in latent diffusion models to better capture high-frequency details and reduce reconstruction errors that can propagate into the generation process, as explored in models like LTX-Video HaCohen et al. (2025), which tasks the VAE decoder with final denoising step. Others employ parameter-efficient fine-tuning (PEFT) techniques with objectives specifically designed to enhance visual quality and temporal consistency, as seen in frameworks like DAPE Xia et al. (2025) for video editing. Diffusion models themselves 5 have also been adapted as post-processing tools to enhance the quality of already compressed or generated videos that may contain artifacts Liu & Wang (2023). While training-free methods typically operate during the inference or sampling stage, which often involves manipulations of the latent space, guidance of the denoising process, or modifications to sampling strategies. For video generation, FreqPrior Yuan et al. (2025) introduces novel noise initialization strategy by refining noise in the frequency domain to improve detail and motion dynamics. Enhance-A-Video Luo et al. (2025) proposes training-free module to enhance the coherence and visual quality of videos from DiT-based models by adjusting temporal attention distributions during sampling. These methods aim to improve perceptual quality and reduce artifacts without altering the underlying model weights, offering flexible solutions for enhancing generated content. The exploration of latent space refinement, especially concerning frequency components and detail enhancement during the denoising process, remains an active area of research for improving the visual fidelity of diffusion-based generation."
        },
        {
            "title": "2.5 VIDEO DIFFUSION ACCELERATION",
            "content": "Numerous distillation methodologies Wang et al. (2023a); Song et al. (2023); Wang et al. (2024b;a; 2025; 2024c); Mao et al. (2024a); Kim et al. (2023) have been proposed to reduce computational costs in video generation. For instance, some approaches implement joint consistency distillation and adversarial training Wang et al. (2025), accelerating diffusion models through phased consistency models integrated with GANs. Concurrently, Mao et al. Mao et al. (2024a) enhanced the discriminator architecture for adversarial distillation. Given the feature similarity across timesteps in DiT architectures, cache mechanisms have been developed for diffusion frameworks, such as ToCa Zou et al. (2024)s dynamic feature storage guided by token sensitivity and error propagation analysis, which accelerates diffusion transformers through adaptive caching strategies and layer-specific retention techniques. Similarly, AdaCache Kahatapitiya et al. (2024) improves diffusion transformer inference without retraining via dynamically adjusted caching policies and motion-aware resource allocation during denoising steps, and TeaCache Liu et al. (2024) accelerate sampling via estimating fluctuating differences among model outputs across timesteps. We introduced co-optimization strategy that integrates step distillation with cache acceleration to further boost sampling efficiency of interactive video generation."
        },
        {
            "title": "3.1 RECTIFIED FLOW.",
            "content": "Rectified Flow Liu et al. (2022) is technique that facilitates ordinary differential equation (ODE)- based training by minimizing the transport cost between marginal distributions Ï0 and Ï1, i.e., E[c(x1 x0)]. Here, x1 = Law(Ï1), x0 = Law(Ï0), and : Rd denote cost function. Given the computational complexity of Optimal Transport (OT) Villani et al. (2009), Rectified Flow provides simple yet effective approach to generate new coupling from preexisting one. This new coupling can be optimized using Stochastic Gradient Descent (SGD), an optimization method extensively employed in deep learning: Î¸ = arg min Î¸ EtU [0,1]Ex0,x1Ï0,Ï1 [MSE (x1 x0, vÎ¸(xt, t))] , (1) where the term MSE denotes Mean Squared Error. The parameter is in the range of [0, 1], we select xt = tx1 + (1 t)x0 to ensure that vÎ¸(xt, t) matches the identical target velocity x1 x0. Upon completion of the training phase, sampling can be performed through definite integral, specifically x1 = x0 + (cid:82) 1 0 vÎ¸(xÏ , Ï )dÏ . In practical scenarios, the aforementioned continuous system is typically approximated in discrete time using the Euler method (or its variants): xtn1 = xtn + (tn1 tn)vÎ¸(xtn , tn), where t0 < t1 < . . . < tN 1 is set of predefined time steps. In the video domain, x0 represents the pixel space of the videos, while denotes the conditioning inputs (such as text and image conditions for controlled generation). Following recent advancements in Rectified Flow that employ VAEs Rombach et al. (2022) for latent space compression during both training and inference to reduce computational costs, we formulate the training objective as: Î¸ = arg min Î¸ EtU [0,1]Ez0,z1Ï0,Ï1 (cid:2)z1 z0 vÎ¸(zt, c, t)2 2 (cid:3) , (2) 6 where = VAE Encoder(x) and = VAE Decoder(z) perform latent space projection and reconstruction respectively. During inference, the sampling process follows: ztn1 = ztn + (tn1 tn)vÎ¸(ztn, c, tn)."
        },
        {
            "title": "3.2 WAN ARCHITECTURE",
            "content": "The foundational architecture employs an identical design to Wan Wan et al. (2025), utilizing its spatio-temporal VAE encoder and denoising DiT model. The VAE encoder compresses input video sequences into latent representations of dimensionality [1 + T/4, H/8, W/8] with expanded channel depth C=16. The denoising DiT backbone incorporates: 1) Patchify module utilizing 3D convolution (kernel=(1,2,2) to downsample spatial resolution while expanding channels into transformer tokens; 2) CLIP Radford et al. (2021) for image encoder and umT5 Chung et al. (2023) for text encoder; 3) Transformer blocks that concurrently process modality-specific features, where cross-attention mechanisms fuse the video tokens (queries) with image/text embeddings (keys/values). This configuration maintains spatio-temporal coherence while ensuring computational efficiency throughout the encoding-to-denosing pipeline."
        },
        {
            "title": "4.1 DATASET",
            "content": "We use the Sekai-Real-HQ, subset of Sekai Li et al. (2025b), as the training dataset. It consists of large-scale walking video clips with corresponding high-quality annotations of camera trajectory and semantic labels. In this section, we briefly introduce the dataset and more details could be found in the paper of Sekai. Video Collection We manually collect high-quality video URLs from popular YouTubers and extend them by searching additional videos using related keywords (e.g., walk, drone, HDR, and 4K). In total, we collect 10471 hours of walking videos (with stereo audio) and 628 hours of drone (FPV or UAV) videos. All videos were released over the past three years, with 30-minute to 12-hour duration. They are at least 1080P with 30 to 60 FPS. We download the 1080P version with the highest Mbps for further video processing and annotation. Due to network issues and some videos are broken, there are 8409 hours of walking videos and 214 hours of drone videos after downloading. Video Preprocessing For YouTube videos, we trim two minutes from the start and end of each original video to remove the opening and ending. Then we do the five preprocessing steps, including shot boundary detection, clip extraction and transcoding, luminance filtering, quality filtering, camera trajectory filtering, to obtain 6620 hours of video clips as Sekai-Real. Video Annotation We annotate video clips using multiple tools, large vision-language models, and meta inforation from YouTube. The annotations including location, multiple categories, caption, and camera trajectories. Video Sampling We sample the best-of-the-best video clips considering content diversity, location diversity, category diversity, and camera trajectory diversity. Finally, we obtain 400 hours of video clips as Sekai-Real-HQ."
        },
        {
            "title": "4.2 CAMERA MOTION QUANTIZATION",
            "content": "Though low-quality camera trajectories from Sekai-Real-HQ are already filtered, the trajectories estimated by MegaSaM Li et al. (2025c) are inevitably imprecise enough. In addition, raw camera trajectories with large variances are difficult to follow and require extra learnable modules. Consequently, we developed trajectory quantization method to transfer continuous camera poses to discrete actions. This approach inherently facilitates filtering excessively jittery trajectories, balances trajectory distributions, and mitigates model training difficulty. Specifically, existing approaches for camera control in video generation often rely on providing dense sequence of per-frame camera-to-world (c2w) transformation matrices Wang et al. (2023b); He et al. (2024). While offering explicit control, this representation can be overly granular, potentially 7 leading to less stable or unintuitive camera motion, and may not effectively capture the inherent temporal coherence of continuous camera movements. To address these limitations and foster more robust and intuitive navigation, Yume introduces quantized camera motion representation. Our core idea is to define discrete set of predefined, holistic camera motion, Aset. Each motion Aj Aset (e.g., move-forward, tilts up, tilts down. See Supplementary Materials for these motion.) corresponds to canonical relative transformation matrix, Tcanonical,j, representing typical navigational maneuver. Instead of directly using the transformation matrices, we process the input sequence of c2w transformation matrices. For each segment of camera movement (defined by pair of consecutive transformation matrices, potentially after downsampling the trajectory), we construct the reference coordinate system using the transformation matrix at the current segment, and calculate the relative transformation matrix Trel,actual of the next segment. from our predefined set Aset whose canonical transformation matrix We then select the motion Tcanonical,j is closest to Trel,actual, as outlined in Algorithm 1. This matching process effectively quantizes the continuous camera trajectory into sequence of semantically meaningful motion, inherently integrating temporal context from the relative pose changes. In Algorithm 1, the Distance(T1, T2) function measures the dissimilarity between two transformation matrices, which can be weighted combination of differences in their translational and rotational components. As shown in Figure 2, the selected discrete motion are assigned to textual descriptions through predefined dictionary (detailed in the Supplementary Materials). By injecting the action descriptions into the text condition, this approach achieves camera pose-controlled video generation without introducing additional learnable parameters to the pre-trained I2V model, leading to fast model converged and stable and precise camera control. By quantifying camera motion for videos from Sekai-Real-HQ and extracting segments with consistent camera movements, we selected clips longer than 33 frames, ultimately obtaining 139019 video clips. The distribution of camera motions is provided in the supplementary materials. Algorithm 1 Camera Motion Quantization Require: Sequence of camera-to-world matrices = {C0, C1, . . . , CN 1}, Predefined set of camera motion Aset = {A(1), . . . , A(K)}, Corresponding canonical relative SE(3) transformations {T (1) canonical, . . . , (K) canonical}. Ensure: Sequence of selected camera motion = {A 0, . . . , 1} (where depends on processing stride). Initialize for each relevant pair (Ccurr, Cnext) from do 1: 2: 3: 4: 5: 6: 7: curr Cnext Trel,actual 1 Append current arg min A(j)Aset current to end for return Distance(Trel,actual, (j) canonical)"
        },
        {
            "title": "5.1 OVERVIEW",
            "content": "In this section, we provide comprehensive overview of the Yume. The architecture builds upon Wan Wan et al. (2025)s network design but introduces masked video diffusion transformers and employs ReFlow-based training methodology. To enable long-video generation, we concatenate downsampled historical video clips (using Patchify) with generated segments during training and feed them into the DiT denoising modela strategy replicated during inference by similarly downsampling and stitching multiple video segments. Moreover, we detail various sampler designs, including AAM and TTS-SDE, which yield distinct sampling effects. Then we introduced our camera motion control 8 Figure 2: Four core components of Yume: camera motion quantization, model architecture, long video training, and generation. We also make advanced sampler, please see Section 5.3. and the motion speed control. Finally, we discuss practical applications of Yume, including world generalization and world editing."
        },
        {
            "title": "5.2.1 MASKED VIDEO DIFFUSION TRANSFORMERS",
            "content": "We employ the video diffusion model described in Section 3.2 and introduce Masked Video Diffusion Transformers (MVDT) for better visual quality. Existing video diffusion models often overlook the critical role of masked representation learning, leading to artifacts and structural inconsistencies in cross-frame generation. Drawing inspiration from proven masking strategies in Gao et al. (2023); Mao et al. (2024b), we introduce MVDT to significantly enhance video generation quality. As shown in Figure 2, the architecture employs an asymmetric network to process selectively masked input features through three core components: encoder, side-interpolator, and decoder. Masking Operation. The feature transformation begins with stochastic masking of the input feature ztn RN (where denotes the channel dimension and represents the token count), derived from noisy video token embeddings. Applying random masking ratio Ï yields reduced set of active features ztn,u Rd ËN ( ËN = (1 Ï)N ) accompanied by positional binary mask ASK RN . The masking ratio Ï serves as hyperparameter. We set Ï = 0.3 in this paper. This selective processing concentrates computational resources on visible tokens ztn,u while maintaining representational accuracy and substantially reducing memory/computational overhead. Encoder. This stage utilizes streamlined architecture that exclusively processes preserved tokens ztn,u, mapping them to compact latent representations. By bypassing masked regions, it achieves computational load reduction compared to conventional full-feature encoders. Side-Interpolator. Inspired by prior work Gao et al. (2023), this component adopts an innovative dual-path architecture. During training, it combines learnable latent tokens zl with encoded features through zI = SA(Concat[zl, Encoder(ztn,u)]), dynamically predicting masked content via selfattention mechanisms. The gated fusion operation Ëztn,u = (1 ASK) ztn,u + ASK zI seamlessly integrates the original and synthesized features while preserving temporal coherence between video sequences. Decoder. Finally, we process the interpolated features Ëztn,u using the remaining DiT-blocks. 9 During training, it optimizes MVDT through end-to-end gradient propagation, while inference directly processes complete feature sets without interpolation. During model training, the diffusion module receives both ztn and ztn,u. This strategy prevents the system from concentrating exclusively on masked-area regeneration while compromising essential diffusion learning. Figure 3: Long-form video generation method. 5.2.2 IMAGE-TO-VIDEO (I2V) AND VIDEO-TO-VIDEO (V2V) GENERATION For image-to-video, we adopt the methodology established in Wan. In image-to-video synthesis, conditioning image initializes video generation. Temporally zero-padded frames xc undergo compression via the Wan-VAE encoder to yield latent representations zc RCthw. binary mask Mc {0, 1}1T hw (with 1 denoting preserved regions and 0 indicating generated regions) is reshaped to dimensions (s denoting the Wan-VAE temporal stride). Channelwise concatenation of [zt, zc, ] feeds into Wans DiT backbone. An additional zero-initialized projection layer accommodates the increased channel dimensionality inherent to image-to-video tasks (2c + versus for text-to-video). For video-to-video generation, our approach diverges from the image-to-video methodology by extracting exclusively the final frame of the video condition as the image input for CLIP encoding. We concurrently adjust the masking tensor Mc to ensure precise alignment between the conditional video sequence and generated output. This video-to-video synthesis lacks inherent memory retention capabilities."
        },
        {
            "title": "5.2.3 LONG VIDEO GENERATETION",
            "content": "Based on meticulous experimentation, we observe that autoregressive methods for long video generation frequently exhibit inter-frame discontinuity and lack temporal coherence. To address these limitations, we introduce long-form video generation methodology, adopting compression design analogous to FramePack Zhang & Agrawala (2025) (architectural details illustrated in Figure 3). FramePack employs contextual compression of historically generated frames via downsampling of pretrained Patchify parameters. The baseline Patchify configuration, specified by hyperparameters (2, 4, 4), denotes sequential compression ratios of 2 temporally, 4 spatially in height, and 4 spatially in width. Higher compression ratios are applied as generated video length increases. For conditional input to the diffusion model, our established compression scheme operates as follows: Frame 1 : Frames 2 to 5 : Frames 6 to 22 : ... (1, 2, 2) (1, 4, 4) (1, 8, 8) ... 10 (3) ... and so forth, including scenarios with spatiotemporal compression. More high-resolution tokens generally lead to better performance. We finalized: Frames 1 to 2 : Frames 3 to 6 : Frames 7 to 23 : ... Initial frame: (1, 2, 2) (1, 4, 4) (1, 8, 8) ... (1, 2, 2) (4) This configuration optimally balances temporal and spatial compression while preserving highresolution representation for recent frames and retaining critical features from initial input images. We implement stochastic sampling strategy for the training where, with 0.3 probability, we select historical frames from 33-400 frame videos to predict subsequent 33-frame sequences, and with 0.7 probability, we utilize 400-800 frame videos as historical context for 33-frame predictions. This sampling distribution optimizes the frame count allocation during training. In our sampling methodology, letting ztn RCF HW denote the latent representation at denoising step tn, where indexes channels, frames, and spatial dimensions. Given conditioning inputs (text embeddings), historical frames Iinput, noise znoise, and diffusion model fÎ¸, our sampling process implements: ztn1 = zin + (ti1 ti) vÎ¸(Ëzin, c, tn) Ëzin = ((1 tn)Iin + tnznoise) zin (5) where is frame-wise concatenation. Our method supports both video-to-video (V2V) and image-to-video (I2V) generation. However, during V2V synthesis, we observe that when the conditioning video contains prolonged homogeneous motion, the model becomes overly reliant on the motion characteristics of the conditioning video (resulting in identical motion patterns between source and generated sequences), weakening the motion following capability. To address this conditional video dependency issue, we implement training strategy where 30% of iterations use static image conditions, specifically, by temporally tiling individual images 16 times to construct static video sequences, thereby improving the models ability to generate diverse motion patterns. Algorithm 2 Anti-Artifact by High Frequency Refinement Require: Initial latent estimate zorig (from full standard denoising pass), Diffusion model vÎ¸, condition c, number of inference steps , refinement steps Kref ine < , Timesteps {ti}N 1 Low-pass filter operator (e.g., Gaussian blur). i=0 from down to 1, 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: ztN 1 is initialized during the first denoising step. for = 1 down to 0 do ti current timestep from schedule in z ti if Kref ine then zorig,ti (1 ti) zorig + ti ztN 1 zlow from orig B(zorig,ti) in B(z high current z in) in zlow from orig + high current end if ti1 end for return t0 in + (ti1 ti) vÎ¸(z in, c, ti)"
        },
        {
            "title": "5.3 SAMPLER DESIGN",
            "content": "This section details our advanced sampler, which incorporates two key innovations to enhance the quality of the generated videos. We first introduce Training-Free Anti-Artifact Mechanism (AAM) to eliminate visual artifacts and then present Time Travel Sampling based on SDE (TTS-SDE) to improve video sharpness and textual controllability."
        },
        {
            "title": "5.3.1 TRAINING-FREE ANTI-ARTIFACT MECHANISM",
            "content": "While diffusion models excel at generating diverse content, complex scenes such as urban cityscapes can often exhibit visual artifacts, such as blurred details, unnatural textures, or flickering, which detract from realism. To address this without requiring additional training or model modification, Yume incorporates novel Training-Free Anti-Artifact Mechanism, AAM. The core idea is to enhance high-frequency details and overall visual quality by strategically refining the latent representation during second stage of denoising pass, leveraging information from an initial standard generation. This approach draws inspiration from NVIDIA DLSS, which primarily employs neural networks to super-resolve low-resolution rendered frames in video games.It is also inspired by DDNM Wang et al. (2022), training-free method achieving image deblurring, super-resolution, etc.. Our mechanism involves two-stage process. Let ztN RCF HW denote the latent representation at denoising step tn, where indexes channels, frames, and spatial dimensions. First, standard multi-step denoising process is performed, starting from random noise ztN 1 and conditioned on text and any other relevant input, to obtain an initial latent estimate, which we denote as zorig. In the second stage, we employ the pre-trained diffusion model and remove motion control from the text conditioning, since zorig already incorporates trajectory characteristics. This zorig typically captures the overall structure and semantics of the scene well, but may contain the aforementioned artifacts or lack fine details. Second, refinement denoising pass is initiated, also starting from the sample of ztN 1 . During the initial steps of this refinement pass (where is small number, e.g., 5), we intervene in the denoising process before each model prediction. For the current latent ti at timestep (ti >= tN K) in the refinement pass, we perform detailed recombination as outlined in Algorithm 2. We begin by diffusing (noising) the initial estimate zorig back to the current noise level corresponding to timestep ti, resulting in zorig,ti = (1 ti) zorig + ti ztN 1. From this noised estimate, we extract the low-frequency component using low-pass filter (e.g., blur operator A, B(z) = APinvAz, the supplementary materials contain additional information about matrix A. Note that is not full-rank and APinv represents the pseudo-inverse of A), yielding zlow from orig = B(zorig,ti). This component represents the stable, coarse structure of the initial generation. Simultaneously, we extract the high-frequency component of the current refinement latent ti and its low-frequency version, giving us ti by taking the difference between high current = z ti B(z ti ) (6) which can also be expressed as (I APinvA)z t, where is the identity matrix. The latent that will be fed into the diffusion model for the current denoising step is then recomposed by combining these complementary frequency components: ti zlow from orig + (7) This approach effectively merges the low-frequency information from the initial generation with the high-frequency information from the current refinement step. After this recomposition for the first steps, the diffusion model vÎ¸ predicts the less noisy latent ti1 as usual. For the remaining steps of the refinement pass (i.e., after the initial steps), the standard denoising procedure continues without this frequency-domain intervention. high current This strategy allows the refinement pass to preserve the robust low-frequency structure established by the initial generation while focusing its generative capacity on producing higher-fidelity highfrequency details. By guiding the initial stages of the refinement in this manner, we effectively reduce common visual artifacts, enhance sharpness, and improve the overall perceptual quality of the generated urban scenes without incurring any additional training costs. This method increases sampling time but allows step adjustment. The standard denoising step is 50 with CFG at 100 Number of Function Evaluations (NFEs). We use 30 steps without CFG for the first 12 Algorithm 3 Time Travel Sampling based on SDE (TTS-SDE) Input: Initial conditioning c, model VÎ¸, schedule {ti}N 1 i=0 , and t1 = 0 Travel parameters: interval = 5, depth = 5 Output: Generated latent 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: Initialize noise ztN 1 for = 1 down to 1 do zin + (ti1 ti) vÎ¸(zin, c, ti) ti current timestep from schedule zin zout ti ti1 Ëzt0 zin + (0 ti) vÎ¸(zin, c, ti) Î²i -0.5Î·2 (zinËzt0 ) Î·(cid:112)ti1 tiN (0, I) zti+1 ti1 if 0 (mod s) then + Î²i(ti1 ti) + t2 kmax max(t l, 0) for = 1 down to kmax do in + (0 tk) vÎ¸(z in + (tk1 tk) vÎ¸(z in, c, tk) tk current timestep from schedule in ztk+1 z tk1 Ëzt0 Î²k -0.5Î·2 (z inËzt0 ) t2 Î·(cid:112)tk1 tkN (0, I) ztk+1 vk vÎ¸(z + Î²k(tk1 tk) + tk1 in, c, tk) in, c, tk) end for Ëv vkmax end if zout ti1 end for return zout zin + (ti1 ti) Ëv 13 denoising pass and 30 steps with CFG for the second pass at 90 NFE, achieving 10% NFE reduction while improving sampling quality. While AAM demonstrates excellent performance in generating high-quality individual frames, making it particularly suitable for image-to-video (I2V) conversion and high-quality synthetic data production, it exhibits significant limitations in autoregressive long-video generation scenarios, often resulting in discontinuity between generated frames and historical frames. This limitation may stem from the fact that AAMs pretrained diffusion model is fundamentally based on an I2V architecture rather than video-to-video (V2V) framework. Fine-tuning AAMs pretrained model on V2V tasks could potentially alleviate this issue."
        },
        {
            "title": "GENERATION",
            "content": "Inspired by DDNM Wang et al. (2022), Repaint Lugmayr et al. (2022), and OSV Mao et al. (2025) approaches, we introduced novel high-quality sampling methodology Time Travel Sampling based on SDE (TTS-SDE), as shown in Algorithm 3. For selected timestep tn, we first sample forward steps to obtain xtmax(nl,0), then leverage this future states more accurate velocity vector estimation to reconstruct xtn1, effectively utilizing prospective information to refine past states. However, given the deterministic nature of ODE-based sampling with fixed noise inputs, this methods capability to enhance textual controllability remains limited, primarily improving only the sharpness of generated videos. We find that replacing ODE with SDE sampling significantly boosts textual controllability by introducing controlled stochasticity into the generation process."
        },
        {
            "title": "5.4 CAMERA MOTION CONTROL",
            "content": "Existing approaches for camera control in video generation often rely on providing dense sequence of per-frame absolute camera-to-world (c2w) pose matrices Wang et al. (2023b); He et al. (2024). While offering explicit control, this representation can be overly granular, potentially leading to less stable or unintuitive camera trajectories, and may not effectively capture the inherent temporal coherence of continuous camera movements. Instead of directly using continuous pose matrices, we introduced discrete camera motion representation, as introduced in Section 4.2. We quantize the camera motion and parse the motion into text as textual condition. In addition, beyond the motion direction, we propose to control the motion speed to achieve stable movement in the generated videos. Specifically, as outlined in Algorithm 4, for each camera trajectory, we compute three quantitative indicators of the motion speed: Translational Motion (V) is captured by the displacement vector between the positions of two consecutive frames, which quantifies the local translational changes along the camera trajectory. By explicitly modeling this motion, the model learns to sustain consistent forward movements or lateral shifts, thereby preserving spatial continuity and fostering natural flow in the generated videos. Directional Change (D) is measured by the angle formed between displacement vectors across every three consecutive frames, effectively measuring local turning or bending of the trajectory. Incorporating this measure encourages the model to produce smooth directional transitions and avoid abrupt changes, resulting in more realistic and stable motion paths. Rotational Dynamics (R) are characterized by the change in camera orientation angle between two consecutive frames, reflecting the cameras rotational behavior. By explicitly incorporating this rotational cue, the model can better synchronize viewpoint adjustments with the underlying motion dynamics, leading to natural camera panning, tilting, and turning movements. We also parse these speeds into the text, and they are explicitly provided to the model during training and fixed during inference to prevent the generation of videos with irregular or fluctuating speeds. We then combine it with the discrete camera motion description as the final camera motion condition. It effectively quantizes the continuous camera trajectory into sequence of semantically meaningful actions, inherently integrating temporal context from the relative pose changes. 14 The camera motion condition is then combined with prefix text description of the video (This video depicts city walk scene with first-person view) as the final textual condition. Algorithm 4 Camera Motion Speed Calculation (cid:17) v1v2 (cid:16) v1v2 // Translational vector change Initialize , , for = 0 to 3 do pi Ci[:, : 3, 3] pi+1 Ci+1[:, : 3, 3] pi+2 Ci+2[:, : 3, 3] v1 pi+1 pi v2 pi+2 pi+1 Compute Ï = arccos Append v1 to Append Ï to // Directional angle change Require: Sequence of camera-to-world matrices = {C0, C1, . . . , CN 1}. 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: Append to 16: 17: 18: 19: 20: 21: 22: zi Ci[:, 2] zi+1 Ci+1[:, 2] Compute Î¸ = arccos Append Î¸ to // Rotation angle change end for pN 2 CN 2[:, : 3, 3] pN 1 CN 1[:, : 3, 3] pN 1 pN end for return V, D, for = 0 to 2 do (cid:16) zizi+1 zizi+1 (cid:17)"
        },
        {
            "title": "5.5 APPLICATION",
            "content": "We show some applications in the demo video on the project page."
        },
        {
            "title": "5.5.1 WORLD GENERALIZATION",
            "content": "Though Yume is trained by real-world videos, it shows impressive generalizability to diverse unreal scenes, such as animation, video games, and AI-generated images. Thus, Yume allows not only real-world exploration, but also facilitates unreal-world exploration. Moreover, Yume supports V2V and thus also can adapt to live images taken by iPhone."
        },
        {
            "title": "5.5.2 WORLD EDITING",
            "content": "Since Yume shows strong generalizability and thus we can achieve world editing by simply combining Yume with image editing methods such as GPT-4o. We show some examples that change weather, time, and style during video generation using GPT-4o in the demo video."
        },
        {
            "title": "5.6 ACCELERATION",
            "content": "We present an acceleration framework for the Yume model. Departing from conventional approaches, we devise co-optimization strategy integrating step distillation with cache acceleration. This design stems from an insight: aggressively reducing sampling steps to just one step severely compromises the models adaptability to diverse complex scenarios, while video quality progressively deteriorates due to error accumulation as generation length increases. To address this, we reduce sampling steps to 14 while incorporating cache acceleration mechanism."
        },
        {
            "title": "5.6.1 ADVERSARIAL DISTILLATION FOR ACCELERATED DIFFUSION SAMPLING",
            "content": "We introduce an adversarial distillation framework to reduce the sampling steps while preserving visual quality. The core innovation leverages Generative Adversarial Networks (GANs) to distill the iterative denoising process into fewer steps, following the formulation: Ltotal = Ldiffusion + Î»advLadv (8) where Ldiffusion is the standard diffusion loss and Ladv is the adversarial loss term. The training alternates between updating the denoiser DiT Model and the discriminator D. The discriminator is trained to distinguish between real samples xreal from the training distribution and denoised samples Ëx0 generated by the diffusion model: Ëx0 = zt vÎ¸(zt, t, c) The discriminator loss combines feature-level and image-level discrimination: LD = E[ReLU(1 D(xreal))] + E[ReLU(1 + D(Ëx0))] where returns both image-level predictions and intermediate feature maps. (9) (10) The denoiser DiT Model Yume aims to fool the discriminator while maintaining denoising accuracy: We adopt the discriminator design from OSV Mao et al. (2025), as this approach significantly reduces memory consumption while achieving excellent discriminative performance. Ladv = E[D(Ëx0)] (11)"
        },
        {
            "title": "5.6.2 CACHE-ACCELERATING",
            "content": "We use the caching mechanism that reduces computational redundancy by reusing intermediate residual features across denoising steps. The system employs layer-specific caching policies to achieve computation reduction while preserving output quality. For layer at timestep tn , the residual feature xl (cid:26)(Blockl(xl1 tn ) xl1 tn is cached when: tn )bfloat16 if Lcache otherwise"
        },
        {
            "title": "Cl\ntn",
            "content": "= (12) where Lcache denotes predefined cacheable layers. These cached residuals are stored in bfloat16 precision. At subsequent timestep tn1, layer computes: xl tn = (cid:40) xl1 tn1 + Cl tn Blockl(xl1 tn1) otherwise when Lcache (13) tn , after which computations for subsequent steps tn1 through tnlc utilize Cl We consider an acceleration ratio of 1 : lc, whereby full computation is performed at time step tn to obtain cache Cl tn for skip-step processing. To evaluate the impact of individual blocks within the DiT denoising model, we introduce an MSE-based importance metric. Specifically, across N/(lc + 1) cached computation segments from tn to tnlc, upon computing the reference cache Cl tn at tn, we systematically ablate each of the 40 DiT blocks at step tnlc. The influence score for the i-th block is quantified as: MSE Scorei = Mean , [1, 2, 3, .., 40] (14) (cid:18)(cid:12) (cid:12)xRemove (cid:12) tnlc (cid:19) xtnlc (cid:12) (cid:12) (cid:12) 2 2 where Mean denotes the averaging operation. Each video yields N/(lc + 1) measurements per block, generating 40 temporal MSE profiles per segment. These profiles are averaged across time steps and aggregated over 32 videos to produce 40 composite MSE scores. Figure 4 illustrates that central DiT blocks exhibit minimal impact while initial and terminal blocks demonstrate maximal influence, leading to the selection of the 10 lowest-scoring blocks as predefined cacheable layers. Figure 4: Significance of Individual DiT Blocks."
        },
        {
            "title": "6.1.1 TRAINING DETAILS",
            "content": "We utilized the SkyReels-V2-14B-540P as the pre-trained model. The training process involved video resolutions of 544960, with frame rate of 16 FPS, batch size of 40, and the Adam optimizer with learning rate of 1e-5. Training was conducted across NVIDIA A800 GPUs over 7,000 iterations."
        },
        {
            "title": "6.1.2 EVALUATION DATASET",
            "content": "Existing video generation evaluation methods are not well-suited for complex scenes and interactive generation with keyboard inputs. To address this issue, we developed the Yume-Bench evaluation framework. Specifically, we exclude training videos from the Sekai-Real-HQ dataset and instead sample test videos with quantized camera motions. For rare actions, such as walking backward or tilting the camera up and down, we randomly sample images rather than video clips. In total, we collected 70 videos or images, covering wide range of complex action combinations, as detailed in Table 1."
        },
        {
            "title": "6.1.3 EVALUATION DETAILS",
            "content": "Yume-Bench evaluates two core capabilities of models: visual quality and instruction following (camera motion tracking), using six fine-grained metrics. In the instruction following evaluation, we assess whether the generated videos correctly follow the intended walking direction and camera movements. While camera poses estimation methods, such as MegaSaM Li et al. (2025c), can automate this evaluation, the camera motion prediction in the generated videos is not sufficiently accurate, and quantization errors may occur. Therefore, we conduct human evaluation to identify the accuracy of the generated motion. For the remaining metrics, we adopt those from VBench Huang et al. (2024), including subject consistency, background consistency, motion smoothness, aesthetic quality, and imaging quality. The test data resolution is 544960 with frame rate of 16 FPS, comprising total of 96 frames. We applied 50 inference steps for all models tested. 17 Table 1: Keyboard-Mouse Action Combinations Keyboard-Mouse Action"
        },
        {
            "title": "Count",
            "content": "No Keys + Mouse Down No Keys + Mouse Up Key + No Mouse Movement W+A Keys + No Mouse Movement W+A Keys + Mouse Left W+A Keys + Mouse Right W+D Keys + No Mouse Movement W+D Keys + Mouse Left W+D Keys + Mouse Right 2 2 2 29 6 17 5"
        },
        {
            "title": "6.2 QUALITATIVE RESULTS",
            "content": "6.2.1 IMAGE-TO-VIDEO GENERATION We compared several state-of-the-art (SOTA) image-to-video generation models, including Wan-2.1 and MatrixGame, as shown in Table 2. Our experimental results revealed that: (1) Wan-2.1 shows limited instruction-following capabilities while using textual instructions to control camera motion. (2) Although MatrixGame demonstrates some degree of controllability, it struggles to generalize to real-world scenarios and lacks sufficient scene replication control. In contrast, our Yume excels in controllability, with its instruction-following capability scoring 0.657, significantly outperforming other models. Additionally, Yume achieves optimal or near-optimal performance across other metrics, demonstrating our superior visual quality. Figure 5: Metric Dynamics in Long-video Generation. We use TTS-SDE. We performed total of 9 extrapolations. 4 infs represents using videos obtained from 4 extrapolations (totaling 8 seconds) for metric calculation, while 2 infs represents using videos obtained from 2 extrapolations (also totaling 4 seconds) for metric calculation."
        },
        {
            "title": "6.2.2 VALIDATION OF LONG-VIDEO GENERATION PERFORMANCE",
            "content": "To assess the long-video generation capability, we created an 18-second video sequence where Yume generates 2-second segments incrementally. During the first 8 seconds, the motion patterns remained consistent with the test set, followed by transition to continuous forward movement (W) in the subsequent 10 seconds. As shown in Figure 5, mild content decay was observed: subject consistency decreased by 0.5% (0.9340.930), and background consistency dropped by 0.6% (0.9470.941) 18 Table 2: Quality comparison of different models. Wan-2.1 utilize text-based control. MatrixGame employs its own native keyboard/mouse control scheme. All models use the same random seed. Model Instruction Following Subject Consistency Background Consistency Motion Smoothness Aesthetic Quality Imaging Quality Wan-2.1 Wan et al. (2025) MatrixGame Zhang et al. (2025) Yume (Ours) 0.057 0.271 0.657 0.859 0. 0.932 0.899 0.932 0.941 0.961 0. 0.986 0.494 0.435 0.518 0.695 0. 0.739 between the 0-8s and 12-18s segments, indicating that Yume maintains reasonable stability over time. It is worth noting that during the motion transition phase (8-12s), instruction-following performance dropped by 8.6% (0.9470.941). This decline can be attributed to the inertia from the motion in the input video, which hindered an immediate reversal of direction. However, the inertial effect diminished after 12 seconds, leading to significant recovery in instruction-following performance by 22.3% (0.6360.819). Table 3: Ablation study on different samplers. Model Instruction Following Yume-ODE Yume-SDE Yume-TTS-ODE Yume-TTS-SDE 0.657 0.629 0.671 0.743 Subject Consistency 0.932 0.927 0.923 0.921 Background Consistency 0.941 0.938 0.936 0.933 Motion Smoothness 0.986 0.985 0.985 0.985 Aesthetic Quality 0.518 0.516 0.521 0. Imaging Quality 0.739 0.737 0.737 0."
        },
        {
            "title": "6.3.1 VERIFICATION OF TTS-SDE EFFECTIVENESS",
            "content": "To assess the effectiveness of TTS-SDE, we replaced the ODE sampling with SDE and TTS-SDE for comparison. As shown in Table 4, while SDE sampling resulted in decline across all metrics, TTSSDE achieved significant improvement in instruction following, despite slight reduction in other indicators. This indicates that TTS-SDE strategically introduces noise perturbations, enhancing the refinement of motion trajectories in the generated videos. Furthermore, the integration of TTS-SDE has resulted in improved aesthetic scores, with our observations revealing clearer and more detailed generated videos. Table 4: Validation of distillation method effectiveness. Model Time (s) Baseline Distil 583.1 158. Instruction Following 0.657 0.557 Subject Consistency 0.932 0.927 Background Consistency 0.941 0.940 Motion Smoothness 0.986 0.984 Aesthetic Quality 0.518 0. Imaging Quality 0.739 0."
        },
        {
            "title": "6.3.2 VALIDATING THE EFFECT OF MODEL DISTILLATION",
            "content": "After distilling the model to reduce the number of steps from 50 to 14, we compared it with the original model. We found that, except for instruction following, the other metrics showed minimal differences from the original model. This may be because fewer steps weaken the models text-control capability."
        },
        {
            "title": "6.4 VISUALIZATION RESULTS",
            "content": "As illustrated in Figure 6, we generated multiple video sequences using the initial frame image and quantized camera trajectories, demonstrating that Yume accurately follows the predefined motion paths during video generation. Figure 7 demonstrates the effectiveness of AAM by generating clearer videos while avoiding illogical scenes such as aberrant snowman artifacts. 19 Figure 6: Yume demonstrates superior visual quality and precise adherence to keyboard control in real-world and unreal scenarios. Figure 7: AAM Improves Structural Details in Urban and Architectural Scenes."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this paper, we introduce preview version of Yume, which is an interactive world generation model that allows the use of keyboard inputs to explore dynamic world created by an input image. Moreover, it can do infinite video generation in an autoregressive manner. Yume consists of four 20 main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. Yume is long-term project that has established solid foundation, yet still faces numerous challenges to address, such as the visual quality, runtime efficiency, and control accuracy. Moreover, many functions need to be achieved, such as interaction with objects."
        },
        {
            "title": "REFERENCES",
            "content": "Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: space-time diffusion model for video generation, 2024. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023a. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2256322575, 2023b. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. Technical report, OpenAI, February 2024. URL https://openai. com/research/video-generation-models-as-world-simulators. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Stefan Bechtle, Sumit Bileschi, Sebastian Borgeaud, Stephanie Borja, Arun Byravan, Ken Caluwaerts, Marion Caron, Tiago Carvalho, Andrew Cassirer, Yiding Chen, Michele Covell, Silvia de Abreu, Andrew Fant, AMA Glaese, Tom Henighan, Lennon Hughes, Egle Kasinskaite, Cosmin Kema, Misha Kumar, Matt Kyle, Hubert Laur, Tom Lovitt, Elliot Rutherford, Maria Rutherford, Martin Salz, Laurent Sifre, John Simon, Oleksandr Smytnis, Tom Valdevit, Oriol Vinyals, Greg Wayne, Jonathan Zack, and Zhaosen Zhu. Genie: Generative interactive environments. In Sanjoy Dasgupta, Analia Gomez, Andrew Lan, and Yee Whye Teh (eds.), Proceedings of the 41st International Conference on Machine Learning (ICML), volume 235, pp. 46174640. PMLR, 0915 Jul 2024. Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengchen Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151, 2023. Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 2316423173, 2023. Shuai Geng, Zexu Liu, Zuanbang Shang, Jinxiang Chai, Chen Change Loy, and Ziwei Liu. Trainingfree camera control for video generation with text and image prompts, 2024. DeepMind. Agent. MultiGoogle https://deepmind.google/discover/blog/ world sima-generalist-ai-agent-for-3d-virtual-environments/, March 2024. Accessed: May 20, 2025."
        },
        {
            "title": "Instructable",
            "content": "Scalable, SIMA: David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution. In Samy Bengio, Hanna Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 24512463. Curran Associates, Inc., 2018. URL https://papers.nips.cc/paper/2018/hash/ 7512c32074c2314308c0951959dee873-Abstract.html. 22 Yoav HaCohen, Mendi Shen, Daniel Dror, Daniel Glickman, Rotem Mulayoff, Oran Shimony, Oran Langman, David Elyada, Eden Kristal, Evgeny Bistrov, Eyal Molad, Hila Chefer, Michal Geyer, Shai Bagon, Yedid Hoshen, and Tomer Michaeli. LTX-Video: Realtime video latent diffusion, 2025. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. CameraCtrl: Enabling camera control for text-to-video generation, 2024. Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. StreamingT2V: Consistent, dynamic, and extendable long video generation from text, 2024. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic modIn Advances in Neural Information Processing Systems, volume 33, pp. 6840 URL https://proceedings.neurips.cc/paper/2020/file/ els. 6851, 2020. 4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models, 2022. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Chenyang Zhang, Michael Ryoo, and Tian Xie. Adaptive caching for faster video generation with diffusion transformers. arXiv preprint arXiv:2411.02397, 2024. Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Ruihuang Li, Caijin Zhou, Shoujian Zheng, Jianxiang Lu, Jiabin Huang, Comi Chen, Junshu Tang, Guangzheng Xu, Jiale Tao, Hongmei Wang, et al. Hunyuan-game: Industrial-grade intelligent game creation model. arXiv preprint arXiv:2505.14135, 2025a. Zhen Li, Chuanhao Li, Xiaofeng Mao, Shaoheng Lin, Ming Li, Shitian Zhao, Zhaopan Xu, Xinyue Li, Yukang Feng, Jianwen Sun, et al. Sekai: video dataset towards world exploration. arXiv preprint arXiv:2506.15675, 2025b. Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1048610496, 2025c. Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. arXiv preprint arXiv:2411.19108, 2024. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Zheng Liu and Lisong Wang. diffusion model based quality enhancement method for HEVC compressed video, 2023. Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. In Proceedings of the Repaint: Inpainting using denoising diffusion probabilistic models. IEEE/CVF conference on computer vision and pattern recognition, pp. 1146111471, 2022. 23 Yang Luo, Xuanlei Zhao, Mengzhao Chen, Kaipeng Zhang, Wenqi Shao, Kai Wang, Zhangyang Wang, and Yang You. Enhance-A-Video: Better generated video for free, 2025. Zhaoitem Ma, Yufei Liu, Lin Geng, Jiaxin Liu, Yaohua Tang, Ming Lu, Xinyuan Chen, Jingwen He, Zilong Huang, Fan Wen, Ping Li, Deliang Fan, Sitong Su, Kai Li, Can Wang, ShiFeng Zhang, Min Dou, Xiaoyi Dong, JiaLUnLiu, Boqin He, Yong He, Yang Song, Haibo E, Gang Yue, Yaokun Liu, Yixuan Liu, Songcen Xu, Shaoshuai Shi, Tao An, Chao Yang, Lin Cui, Libo Zhang, Dit-Yan Yeung, Yong Dou, Yujun Shen, Yu Qiao, and Tat-Seng Chua. Hunyuanvideo: systematic framework for large video generative models, 2024. Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Wenbing Zhu, Jiangning Zhang, Hao Chen, Mingmin Chi, and Yabiao Wang. Osv: One step is enough for high-quality image to video generation. arXiv preprint arXiv:2409.11367, 2024a. Xiaofeng Mao, Zhengkai Jiang, Qilin Wang, Chencan Fu, Jiangning Zhang, Jiafu Wu, Yabiao Wang, Chengjie Wang, Wei Li, and Mingmin Chi. Mdt-a2g: Exploring masked diffusion transformers for co-speech gesture generation. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 32663274, 2024b. Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Jiangning Zhang, Hao Chen, Mingmin Chi, Yabiao Wang, and Wenhan Luo. Osv: One step is enough for high-quality image to video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1258512594, 2025. Prem Nagrath, Nareddy Reddy, Jian Ren, Robin keyValue, Saad Nadeem, Meng Li, Ser-Nam Lim, Chao Liu, Guttu TG, Rama Chellappa, and Ajinkya Kale. Mochi-diffusion-xl: An efficient model for high-resolution video generation, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-tovideo generation without text-video data. In International Conference on Learning Representations (ICLR), 2023. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. The Step-Video-T2V Team. Step-Video-T2V technical report: The practice, challenges, and future of video foundation model, 2025. Cedric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Fu-Yun Wang, Zhaoyang Huang, Weikang Bian, Xiaoyu Shi, Keqiang Sun, Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm: Computation-efficient personalized style video generation without personalized video data. In SIGGRAPH Asia 2024 Technical Communications, pp. 15. 2024a. Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm: Accelerating the animation of personalized diffusion models and adapters with decoupled consistency learning. arXiv preprint arXiv:2402.00769, 2024b. 24 Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024c. Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency models. Advances in Neural Information Processing Systems, 37:8395184009, 2025. Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, and Nong Sang. Videolcm: Video latent consistency model. arXiv preprint arXiv:2312.09109, 2023a. Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. arXiv preprint arXiv:2212.00490, 2022. Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. MotionCtrl: unified and flexible motion controller for video generation, 2023b. Wayve Technologies. GAIA-1: Generative AI Model for Autonomous Driving. https: //wayve.com/blog/introducing-gaia1-generative-ai-model-driving/, 2023. Accessed: May 20, 2025. Junhao Xia, Shiyuan Yang, Hongbo Zhao, Xinyuan Chen, Di Zhang, Yake Wei, Ceyuan Yang, Yujun Shen, and Di Liu. DAPE: Dual-stage parameter-efficient fine-tuning for consistent video editing with diffusion models, 2025. Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. WORLDMEM: Long-term consistent world simulation with memory, 2025. Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-Video: Customized video generation with user-directed camera movement and object motion. arXiv preprint arXiv:2402.03162, 2024. Yunlong Yuan, Yuanfan Guo, Chunwei Wang, Wei Zhang, Hang Xu, and Li Zhang. FreqPrior: In International Improving video diffusion models with frequency filtering gaussian noise. Conference on Learning Representations (ICLR), 2025. Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. Yifan Zhang, Chunli Peng, Boyang Wang, Puyi Wang, Qingcheng Zhu, Zedong Gao, Eric Li, Yang Liu, and Yahui Zhou. Matrix-game: Interactive world foundation model. arXiv preprint, 2025. Technical report, https://github.com/SkyworkAI/Matrix-Game/ raw/main/assets/report.pdf. Accessed: May 20, 2025. (Year listed as 2025 in some project BibTeX, but content available 2024 or earlier.). Yucan Zhang, Zixu Zhang, Ceyuan Yang, Yuming Liu, Zhaowei Chen, Shiliang Pu, Yaxiong Wang, Yujun Shen, Yu Qiao, and Yuliang Liu. CameraCtrl II: Dynamic scene exploration via cameracontrolled video diffusion models, 2024. Pengfei Zhou, Jie Xia, Xiaopeng Peng, Wangbo Zhao, Zilong Ye, Zekai Li, Suorong Yang, Jiadong Pan, Yuanxiang Chen, Ziqiao Wang, et al. Neural-driven image editing. arXiv preprint arXiv:2507.05397, 2025. Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, and Linfeng Zhang. Accelerating diffusion transformers with token-wise feature caching. arXiv preprint arXiv:2410.05317, 2024."
        },
        {
            "title": "A ABLATION STUDY ON IMAGE TO VIDEO",
            "content": "Table 5: Effect of Controlled Condition Injection Methods. Name instruction following AdaLN-Zero Cross-Attention Text injection 0.35 0.45 0.45 aesthetic quality 0.517 0.507 0. imaging quality motion smoothness 0.694 0.701 0.694 0.988 0. 0.991 background consistency 0.936 0.935 0.902 We validated the effectiveness of the MVDT and AAM modules within our image-to-video (I2V) pipeline through comparative analysis of 20 randomly selected video sequences. For this experiment, all models were trained exclusively for 1,000 iterations. Effect of Controlled Condition Injection Methods. We replace Yumes MVDT architecture with DiT and removed the text injection approach as Baseline-1. To evaluate different controlled condition injection methods, we incorporated adaLN-zero, cross-attention, and text injection into Baseline-1 while maintaining identical training parameters. As shown in Table 5, these methods demonstrated complementary advantages in V-bench metrics. We adopt the text injection approach due to its superior controllability, seamless integration with pretrained models (requiring no architectural modifications), and parameter-efficient design that introduces no additional learnable parameters. Table 6: Effect of MVDT. MVDT + - aesthetic quality 0.517 0. imaging quality motion smoothness 0.702 0.694 0.985 0.991 background consistency 0. 0.902 Effect of MVDT Architecture. We validated the efficacy of MVDT by comparing Yume performance with and without this architecture in identical training configurations, we deliberately excluded the AAM in all experimental configurations. Table 6 confirms that the MVDT structure consistently enhances Yumes Generate capabilities. The MVDT architecture enhances Yumes ability to capture structural relationships between frames, significantly reducing artifacts in generated videos. Table 7: Effect of Anti-Artifact Mechanism (AAM). AAM + - aesthetic quality 0.529 0.517 imaging quality motion smoothness 0.737 0.702 0. 0.985 dynamic degree 0.937 0."
        },
        {
            "title": "B QUANTIZED CAMERA MOTION OF TEXT",
            "content": "We generate natural language descriptions for human movement and camera motion like the example in vocab human and vocab camera of Figure 8, 9."
        },
        {
            "title": "C ACTION DISTRIBUTION STATISTICS",
            "content": "Figure 10 presents the distribution of 21,526 video clips across different action combinations."
        },
        {
            "title": "D CORE ALGORITHM IMPLEMENTATION OF GAUSSIAN BLUR KERNEL",
            "content": "We implement separable 2D linear operator using height/width blur kernels with SVD decomposition. The projection B(z) = APinvAz extracts low-frequency components from first-stage results, while the null-space projection B(z) = (I APinvA)z preserves high-frequency details in second-stage outputs. We provide PyTorch-style implementation code. 26 Figure 8: vocab example for translational motion. Figure 9: vocab example for rotational motion. A_operator: LinearOperator2D) -> torch.Tensor: original_shape = x.shape x_flat = x.view(-1, *original_shape[-2:]) 1 def project_null_space(x: torch.Tensor, 2 3 4 5 6 7 8 9 10 11 12 13 Ax = A_operator.A(x_flat) A_Pinv_Ax = A_operator.A_inv(Ax) I_A_Pinv_A_x = x_flat - A_Pinv_Ax return I_A_Pinv_A_x.view(original_shape) LINEAROPERATOR2D CLASS def __init__(self, 1 class LinearOperator2D: 2 3 4 5 6 7 8 kernel_H: torch.Tensor, kernel_W: torch.Tensor, H: int, W: int, device: str = None): \"\"\"Initialize separable 2D operator with SVD decomposition\"\"\" self.device = device or (cuda if torch.cuda.is_available() 10 11 12 13 14 15 else cpu) self.H, self.W = H, # Height-direction operator A_H (H H) A_H = torch.zeros(H, H, device=self.device) for in range(H): for in range(i - len(kernel_H)//2, + len(kernel_H)//2 + 1): 27 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 if 0 <= < H: A_H[i,j] = kernel_H[j - + len(kernel_H)//2] U_H, S_H, Vt_H = torch.linalg.svd(A_H, full_matrices=False) self.U_H, self.S_H, self.Vt_H = U_H, S_H, Vt_H self.S_pinv_H = torch.where(S_H > 1e-6, 1/S_H, torch.zeros_like (S_H)) # Width-direction operator A_W (W W) A_W = torch.zeros(W, W, device=self.device) for in range(W): for in range(i - len(kernel_W)//2, + len(kernel_W)//2 + 1): if 0 <= < W: A_W[i,j] = kernel_W[j - + len(kernel_W)//2] U_W, S_W, Vt_W = torch.linalg.svd(A_W, full_matrices=False) self.U_W, self.S_W, self.Vt_W = U_W, S_W, Vt_W self.S_pinv_W = torch.where(S_W > 1e-6, 1/S_W, torch.zeros_like (S_W))"
        },
        {
            "title": "OPERATOR APPLICATION METHODS",
            "content": "def A(self, x: torch.Tensor) -> torch.Tensor: \"\"\"Forward operation: = A_W @ A_H\"\"\" # Height processing x_h = x.movedim(-2, -1) # [..., W, H] x_h = torch.matmul(x_h, self.Vt_H.T) x_h = torch.matmul(x_h, torch.diag(self.S_H)) x_h = torch.matmul(x_h, self.U_H.T) x_h = x_h.movedim(-1, -2) # [..., H, W] # Width processing x_hw = torch.matmul(x_h, self.Vt_W.T) x_hw = torch.matmul(x_hw, torch.diag(self.S_W)) x_hw = torch.matmul(x_hw, self.U_W.T) return x_hw def A_inv(self, y: torch.Tensor) -> torch.Tensor: \"\"\"Pseudoinverse operation\"\"\" # Width pseudoinverse y_w = torch.matmul(y, self.U_W) y_w = torch.matmul(y_w, torch.diag(self.S_pinv_W)) y_w = torch.matmul(y_w, self.Vt_W) # Height pseudoinverse y_hw = y_w.movedim(-2, -1) y_hw = torch.matmul(y_hw, self.U_H) y_hw = torch.matmul(y_hw, torch.diag(self.S_pinv_H)) y_hw = torch.matmul(y_hw, self.Vt_H) return y_hw.movedim(-1, -2) # [..., W, H] # [..., H, W]"
        },
        {
            "title": "OPERATOR INITIALIZATION EXAMPLE",
            "content": "1 # Blur kernels 2 kernel_H = torch.tensor([0.1, 0.8, 0.1], device=cpu) 3 kernel_W = torch.tensor([0.2, 0.6, 0.2], device=cpu) 4 5 # Operator instantiation 6 A_op = LinearOperator2D( 7 kernel_H=kernel_H, # Height kernel # Width kernel 28 8 9 10 11 12 ) kernel_W=kernel_W, H=544, W=960, device=cuda if torch.cuda.is_available() else cpu # Height dimension # Width dimension Figure 10: Overview of action distribution."
        },
        {
            "title": "E THE JOINT OPTIMIZATION OF ADVERSARIAL DISTILLATION AND CACHING",
            "content": "We consider the joint optimization of adversarial distillation and caching. As illustrated in Figure 11, the DiT module performs intermediate feature caching for specific layers Lcache during the state transition from xtn+k to xtn1 (where tn+k < 1). The Stop Grad operation denotes gradient truncation, simulating scenarios where the DiT model encounters cached features during inference. 29 Figure 11: Acceleration Method Design. This learning approach reduces errors when utilizing cached features. We optimize denoising fidelity using the Diffusion Loss while additionally incorporating the adversarial loss."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute"
    ]
}