{
    "paper_title": "Contextualized Visual Personalization in Vision-Language Models",
    "authors": [
        "Yeongtak Oh",
        "Sangwon Yu",
        "Junsung Park",
        "Han Cheol Moon",
        "Jisoo Mok",
        "Sungroh Yoon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization."
        },
        {
            "title": "Start",
            "content": "Contextualized Visual Personalization in Vision-Language Models Yeongtak Oh * 1 Sangwon Yu * 1 Junsung Park 1 Han Cheol Moon 2 Jisoo Mok 3 Sungroh Yoon 1 4 6 2 0 2 3 ] . [ 1 4 5 4 3 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the users specific experiences, as they lack the ability to associate visual inputs with users accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, unified framework that treats personalized image captioning as core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as crucial stage for enabling robust and generalizable contextualized visual personalization. Project Page 1. Introduction Recent advances in vision-language models (VLMs) (Liu et al., 2023; Li et al., 2024; Chen et al., 2024; Bai et al., 2025) have demonstrated impressive performance across wide range of vision-language tasks, including image cap- *Equal contribution 1Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea 2Samsung Electronics, South Korea 3Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea 4Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, Korea. Correspondence to: Sungroh Yoon <sryoon@snu.ac.kr>. Preprint. February 4, 2026. 1 tioning, visual question answering, and open-ended visual dialogue. Despite these advances, however, current VLMs remain limited in their ability to personalize visual understanding based on user-specific context (Wang et al., 2025; Team, 2025). For example, while VLM may correctly recognize person in an image as man wearing black suit, it typically fails to identify that the person corresponds to the users brother mentioned in prior interactions. This issue indicates that existing VLMs do not yet possess genuine capability for visual personalization. To address this limitation, growing body of work has explored methods for enhancing visual personalization in VLMs (Alaluf et al., 2024; Nguyen et al., 2024; Kim et al., 2025a). While existing methods successfully enable personalization over simple attributes or identities, they remain limited in scope. In particular, they do not account for personalization grounded in rich, experience-level user context, such as past interactions or episodic memories, which remains largely underexplored in existing literature (Hao et al., 2025; Nguyen et al., 2025; Hong et al., 2025; Oh et al., 2025; Doveh et al., 2025). In this work, we define such visual experiences as contextual history that integrates previously observed images with associated personal textual information. We consider setting in which VLMs maintain users past interactions in their context and are expected to leverage this history when interpreting new visual input. We refer to this realistic setting as contextualized visual personalization. Figure 1 illustrates the example of the use-case for contextual visual personalization in VLMs. In practice, personalization in real-world settings is inherently diverse and open-ended, as it depends on implicit user intent and fine-grained contextual cues. Consequently, relying solely on task-specific post-training is both insufficient and inefficient for achieving robust personalization, since it does not scale to the long tail of personalized, contextdependent behaviors in real-world interactions. To address this challenge, we propose unified approach to contextualized visual personalization that targets shared underlying process common across downstream tasks. We argue that this process naturally aligns with the objective of personalized image captioning, which focuses on grounding visual inputs in user-specific contextual knowledge, without reContextualized Visual Personalization in Vision-Language Models Figure 1. Qualitative example of the use-case for contextual visual personalization in VLMs. Note that our CoViP effectively responds to the question while integrating the mentioned personal details from the given multimodal contexts. quiring additional task-specific processing. Accordingly, we leverage personalized image captioning as proxy task to effectively model and learn this shared process. Building on this insight about personalization, we introduce CoViP (Contextualized Visual Personalization via Image Captioning), unified framework that enables holistic personalization by explicitly modeling this underlying process. CoViP formulates contextualized visual personalization as personalized image captioning task, in which VLM recognizes relevant visual concepts in query image, retrieves the corresponding user-specific context, and directly incorporates the retrieved information in the generated caption. Within this framework, we first construct novel and challenging personalized image captioning benchmark that faithfully captures the complexities of contextualized visual personalization. We then adopt reinforcement learning (RL)-based post-training strategy to optimize VLMs for producing personalized captions through our captioning benchmark. At inference time, we further introduce caption-augmented generation (CAG), where the VLMs own generated caption is reused as an explicit conditioning signal to guide personalized response generation. Beyond modeling and training, we place particular emphasis on evaluation, as contextualized visual personalization should avoid spurious textual shortcuts (e.g., directly retrieving query-related hints from context without recognition) that allow VLMs to answer questions while bypassing visual understanding. To this end, we design suite of diagnostic downstream tasks that explicitly assess whether VLM correctly recognizes and retrieves personalized visual context. These diagnostics span personalization from reactive to proactive settings, enabling fine-grained analysis of personalization capabilities in realistic settings. Our experiments demonstrate that CoViP effectively improves personalized image captioning performance and leads to consistent gains in contextualized visual personalization. Additionally, we observe that proprietary VLMs exhibit unstable behavior on our diagnostic tasks, underscoring the need for an explicit post-training stage for captioning prior to downstream adaptation to achieve robust and generalizable personalization. Moreover, the VLM trained with CoViP yields superior results across all proposed diagnostic tasks, indicating holistic gains in personalization capability. Finally, we show that CAG further amplifies performance during inference by capitalizing on the fine-grained details of the generated caption. Our contributions are summarized as follows: We introduce novel paradigm of contextualized visual personalization, formalizing personalization as the ability to leverage user-specific visual experience in contextualized visual understanding. We present CoViP, unified framework that operationalizes this concept through personalized image captioning, RL-based post-training, and captionaugmented generation. We design diagnostic evaluation tasks that systematically assess contextualized visual personalization from reactive to proactive settings, enabling systematic analysis of limitations of existing baselines. Experimental results demonstrate that CoViP consistently improves personalization performance across our benchmark and various diagnostic tasks. 2. Previous Works on VLM Personalization Early approaches to personalized VLMs (Alaluf et al., 2024; Nguyen et al., 2024; 2025; An et al., 2025) primarily leverage the inherent zero-shot capabilities of off-the-shelf mod2 Contextualized Visual Personalization in Vision-Language Models Table 1. Comparison between existing personalization baselines and CoViP. Method MyVLM YoLLaVA TAME RAP RePIC CoViP (Ours) PostTraining (SFT) (RL) (RL) MultiConcept External VLM InteractiveDialogues (1-turn) (3-turns) LongContexts Generalize Use case Evaluation (Cap) (Tasks 3) Cap/ VQA Cap/ VQA VQA Cap/ VQA Cap Cap/ VQA Name recall Name recall VQA accuracy Name recall Name recall CapEval-QAs els to handle explicitly defined concepts retrieved from external databases. Recently, post-training-based methods have emerged to enable VLM personalization in an in-context manner; for instance, Hao et al. (2025) integrates retrieval with personalized generation, while Oh et al. (2025) further advances this direction by achieving robust performance in multi-concept image captioning. Another emerging line of research incorporates contextual information to facilitate personalization. Doveh et al. (2025) enhances VLMs to recognize the same objects in query image and its context, and Kim et al. (2025a) introduces an evaluation benchmark when the same single object appears in both the context and the query. Hong et al. (2025) proposed training-free method for long-context personalized conversations. In Table 1, we further clarify how our approach differs from existing baselines. We exclude direct comparisons with MyVLM (Alaluf et al., 2024) and YoLLaVA (Nguyen et al., 2024), as these methods do not support long in-context personalization settings. Moreover, since TAME (Hong et al., 2025) relies on additional memory controlled by an external VLM, we also exclude it from direct comparison and focus our evaluation on post-training-based baselines. Overall, prior efforts largely focus on explicit personalization, evaluating models based on their ability to retrieve surface-level attributes (e.g., names) from context. Such settings overlook deeper semantic reasoning and the nuanced nature of humanmodel interaction. In contrast, we study more realistic and challenging setting of implicit personalization in interleaved multimodal contexts, where personalization cues must be inferred from visual experience, as shown in Figure 1. To this end, we propose post-training pipeline that enhances contextual reasoning and personalized generation beyond surface-level retrieval. 3. CoViP: Contextualized Visual"
        },
        {
            "title": "Personalization via Image Captioning",
            "content": "3.1. Problem Definition We introduce the notion of contextualized visual personalization, which refers to VLMs ability to generate personalized responses by jointly reasoning over visual input and user-specific interaction history. Formally, given query image x, user prompt p, and context that contains the past visual-textual interactions, VLM with parameters θ is expected to generate response = fθ(c, x, p). (1) Here, represents user experiences accumulated through prior usermodel interactions. Accordingly, the generated response should reflect these personalized experiences, rather than producing generic description. key challenge in this setting arises from the open-ended nature of the user prompt p. Since can vary arbitrarily across tasks and interaction scenarios, the output space of becomes extremely large. As result, directly optimizing task-specific objectives for downstream outputs is insufficient for achieving robust personalization, as it is infeasible to exhaustively control all possible personalization behaviors through supervised training alone. To this end, we posit that diverse personalization tasks share common underlying process. Specifically, regardless of the downstream task, VLM must first interpret the query image in the context of the users past experiences before producing response. To explicitly model this process, we decompose the internal mechanism into two stages: = hθ(c, x), = gθ(z, p), (2) where hθ serves as contextual visual encoder that grounds and into personalized latent representation z, and gθ acts as task-specific generator that generates the final response conditioned on both and p, all within single VLM. Crucially, while gθ varies significantly across tasks as determined by p, the personalization-critical component hθ is shared. This observation motivates us to focus on learning hθ to holistically improve contextualized visual personalization. Furthermore, we observe that hθ is inherently aligned with the objective of personalized image captioning. As captioning is fundamental generation task that avoids extraneous reasoning (e.g., thinking) steps, the resulting caption directly reflects the models user-specific contextual understanding. Accordingly, we adopt captioning as reliable proxy for externalizing the latent personalization state z. 3 Contextualized Visual Personalization in Vision-Language Models Figure 2. Illustration of the proposed personalized image captioning benchmark construction. Based on this insight, we propose leveraging personalized image captioning as proxy task to learn contextualized visual personalization. By optimizing models to generate captions that faithfully reflect visual context, we aim to indirectly, yet effectively, improve personalization performance across various downstream tasks. The following sections describe how we construct personalized image captioning benchmark, how we post-train VLMs through this benchmark, and how, at inference time, the generated captions are reused to guide personalized response generation. 3.2. Personalized Image Captioning Benchmark We propose challenging benchmark that captures the complexity and realism of contextualized visual personalization, as depicted in Figure 2. Our benchmark evaluates VLMs ability to implicitly infer personalization cues from interleaved multimodal contexts, while requiring accurate perception of each visual concept. Overall, the benchmark constitutes 2.8K training samples and 1.3K test samples. 3.2.1. DATASET CONSTRUCTION Image generation and quality filtering. To construct the benchmark, we use an image-generative VLM (Comanici et al., 2025) to synthesize controlled set of images. We first curate foundational image database from researchpermissible, open-source repositories (e.g., Unsplash (Unsplash, 2026)), and then design diverse interaction scenarios among concepts involving humans, objects, and animals. Based on these scenarios, we generate query images with varying visual complexity by including one to four concepts per image. Representative multi-concept query images are shown in Figure S.3. To ensure data reliability, we apply additional quality filtering using text-generative VLM (Comanici et al., 2025). This model evaluates instruction adherence by verifying whether each query image is generated in accordance with the prompt, and assesses visual faithfulness by checking whether the positive concept images used to generate query image actually appear in the resulting image. Samples exhibiting prompt inconsistencies or visual artifacts are removed through this filtering process. The detailed descriptions of the quality filtering criteria and procedures are provided in Appendix and Table S.6. Dialogue generation. To ensure the verifiability of the constructed contextual memory, we generate multi-turn dialogues that simulate free-form interactions between user and the model, while strictly grounding all utterances in factual information related to specific image. Here, factual information includes concrete locations, timestamps, events, or scenarios, while deliberately excluding subjective preferences, personal habits, or hallucinated content. This design choice enables reliable, unambiguous evaluation of the generated captions, as the correctness of each description can be directly verified against the grounded facts in the dialogue context. The prompt templates used for dialogue generation are provided in Table S.8. Construction of imagetext interleaved contexts. To construct interleaved imagetext contexts, we incorporate both positive and negative samples within each context. Here, each sample consists of an image and its associated dialogue. positive image corresponds to ground-truth concept image used to synthesize query image, while negative image is visually similar to positive one but does not appear in the query image. To construct visually fine-grained yet discriminative images within the context, we utilize retrieval process to explicitly control visual granularity by selecting negative images that are visually similar to the positive ones. Specifically, using the CLIP-L/14 vision encoder (Radford et al., 2021), we retrieve the top-2 most similar images for each positive image based on cosine similarity. These retrieved images are then paired with generated dialogues to form negative 4 Contextualized Visual Personalization in Vision-Language Models samples, which are collectively used with positive samples to compose the interleaved context. An illustration of constructed multimodal context is provided in Figure S.1. among in-context visual concepts and (ii) retrieval by encouraging context-groundedness in the generated captions. Concretely, we decompose the reward as follows: 3.2.2. EVALUATION PROTOCOL VIA CAPTION-BASED"
        },
        {
            "title": "MCQA PROBING",
            "content": "To evaluate the degree of personalization in generated captions, we introduce caption-based multiple-choice question answering (MCQA) probing protocol termed CapEval-QAs that assesses whether caption correctly reflects contextually relevant information while excluding irrelevant content. Specifically, given dialogue context = [d1, . . . , dN ], we construct factual MCQA pairs (qik, aik) G(di) using an LLM-based generator G, where each dialogue yields three QA pairs grounded in its factual content, that is, = 1, 2, 3. During evaluation, judge model is provided only with the generated caption and is asked to answer each question qik. For (qnk, ank) pairs derived from the dialogue of positive concepts dn, is expected to answer correctly, and the corresponding accuracy is reported as Positive Accuracy, denoted by Acc+, which measures how precisely the caption captures relevant contextual information. Conversely, for (qmk, amk) pairs derived from the dialogue of negative concepts dm, is expected to respond with uncertainty, reflecting the absence of such information in the caption. Performance in this setting is reported as Negative Accuracy, denoted by Acc, which quantifies the models ability to avoid incorporating irrelevant contextual details. The validity of this protocol is further confirmed by human evaluation results in Section 6. The prompt templates for generating and evaluating these MCQA pairs are provided in Tables S.9 and S.11. Pseudocode and detailed version of the evaluation protocol description are presented in Appendix B. 3.3. Post-Training for Personalized Image Captioning Building upon the benchmark introduced in Section 3.2, we propose an RL-based post-training framework for personalized image captioning. 3.3.1. OBJECTIVE FORMULATION Following the objectives outlined in Section 3.1, our task is to generate faithful caption conditioned on query image x, context c, and captioning prompt ps. Let πθ(s x, c, ps) denote the captioning policy induced by VLM. We posttrain the model parameters θ by maximizing the expected verifiable reward (VR). Formally, we optimize max θ E(x,c)Dtr Esπθ(x,c,ps) (cid:104) (cid:105) r(s, x, c) (3) where r(s, x, c) is VR designed to explicitly reinforce (i) recognition through fine-grained visual discrimination 5 r(s, x, c) = rvis(x, c) (cid:124) (cid:125) (cid:123)(cid:122) recognition + rcaps(s, c) (cid:125) (cid:123)(cid:122) retrieval (cid:124) . (4) We post-train the policy πθ to maximize the expected VR r(s, x, c) of Eq. (4) during optimizing Eq. (3) using GSPO (Zheng et al., 2025) algorithm. Detailed descriptions for post-training are presented in Appendix C. 3.3.2. VERIFIABLE REWARD DESIGN Set-level recognition VR. To augment fine-grained visual perception in VLMs, we introduce novel F1-based VR tailored to concept recognition within context. In each context, several concept images are provided along with query image, and the model is required to identify which concepts appear in the query. The model outputs only the indices of the matched concepts, yielding predicted index set ˆH (e.g., 1, 3, 5). We then compute dense set-level F1 score between ˆH and the ground-truth set H: rvis(x, c) = F1( ˆH, H) = 2 ˆH ˆH + , (5) where TP = ˆH H, FP = ˆH H, and FN = ˆH. This F1-based VR provides meticulous feedback: the model receives partial reward when only subset of relevant concepts is identified. Predicting irrelevant indices increases FP and reduces precision, while missing relevant indices increases FN and reduces recall. An illustration of multiimage example within context is presented in Figure S.6. MCQA-based retrieval VR. We incentivize the model to generate caption that encapsulates sufficient dialoguederived evidence required to resolve the MCQA tasks in Section 3.2.2. Let (ψ(s, q)) be the judges selected choice for and question following the evaluation prompt template ψ(), where the QA pairs (q, a) are pre-generated from c. We construct set of positive questions {q+ k=1 for positive images with corresponding correct answers ak, as well as set of negative questions {q ℓ=1 for negative images whose correct choice is (i.e., The answer cannot be determined). We can formulate the captioning VR by leveraging (s, c) to (s, QA) indirectly as ℓ }M }K (cid:40) rcaps(s, c) = 1, σ+(s; QA+) σ(s; QA). R(s) > 0, otherwise. (6) Here, R() serves as degeneration filtering indicator, while σ quantifies the accuracy score evaluated by an exter- (cid:3) and nal LLM: σ+(s, QA+) = (cid:80) )) = a+ )) = D(cid:3). In these exI(cid:2)J (ψ(s, σ(s, QA) = α (cid:80) ℓ pressions, α is scalar coefficient and I[] is the indicator I(cid:2)J (ψ(s, q+ Contextualized Visual Personalization in Vision-Language Models function. This contrastive construction rewards responses that correctly answer positive questions while penalizing responses that induce non-D answers on negative questions. The detailed description of the degeneration filtering indicator is provided in Appendix C. 3.4. Caption-Augmented Generation We introduce novel inference-time strategy to enhance the performance of contextualized visual personalization on downstream tasks. We term this approach CaptionAugmented Generation (CAG), in which the model first synthesizes descriptive caption and subsequently leverages it as conditioning signal for downstream personalization. Specifically, instead of directly generating the output response y, the model first generates caption and then conditions on it to produce the final output as follows: πθ( x, c, ps), πθ( x, c, pd, s), (7) where ps and pd denote the prompts for the captioning and downstream tasks, respectively. 4. Diagnostic Evaluation of General"
        },
        {
            "title": "Personalization Capability",
            "content": "To evaluate contextualized visual personalization beyond proxy training objectives, it is necessary to assess whether model can correctly leverage user-specific multimodal context when responding to realistic downstream queries. However, existing benchmarks for visual personalization (Nguyen et al., 2024; Kim et al., 2025a; Oh et al., 2025) do not capture whether model reflects fine-grained, episodic user information embedded in contextual memories. To address this gap, we introduce three diagnostic tasks designed to evaluate contextualized visual personalization under realistic interaction scenarios. Each task probes the model under challenging conditions that explicitly preclude shortcut behaviors, such as answering solely based on dialogue context without grounding the visual input in user-specific experiences. Figure 3 shows visualization of examples in diagnostic tasks. Design details on the downstream tasks are provided in Appendix D. Last-Seen Detection (LSD). LSD evaluates whether model can recognize the individual in query image and identify the most recent encounter with that person from the users contextual history. Given that the context contains multiple interactions involving the same individual, the model must retrieve all relevant entries and perform temporal reasoning to determine the correct answer. This task, therefore, requires grounding visual input in user-specific history rather than relying on partial matches or surfacelevel textual cues. Last-Action Recall (LAR). LAR extends LSD by requiring the model not only to identify the most recent encounter Figure 3. Visualization of diagnostic personalization tasks. with the person in the query image, but also to retrieve the fine-grained action described in that interaction. Specifically, the model must first perform temporal reasoning to locate the dialogue corresponding to the latest encounter, and then extract what the user was doing at that time. As result, LAR evaluates whether the model can go beyond explicitly specified temporal reasoning and actively retrieve fine-grained episodic information in response to loosely formulated user instructions, such as implicitly asking what the user was doing at the time. Instruction-Triggered Recall (ITR). ITR evaluates proactive personalization, where the model is expected to surface relevant personalized information even when it is not explicitly requested (Zhang & Sundar, 2019). In this task, the context includes past instruction indicating that the user wishes to be notified with specific keyword (e.g., SKS) upon encountering particular individual again. When presented with an image of that individual, the model must proactively recall this instruction and incorporate the keyword into its response, even though the user does not explicitly request it in the current turn. This evaluates the models ability to trigger personalized behavior based on implicit contextual cues rather than explicit queries. 5. Experiments Baselines. We evaluate performance on our benchmark against proprietary VLMs, including non-thinking models such as ChatGPT-4o and Gemini-2.0-Flash, and thinking models such as ChatGPT-5 (Singh et al., 2025) and Gemini-3.0-Pro (Google, 2025). In addition, among posttraining-based personalization methods, we compare against RAP (Hao et al., 2025) and RePIC (Oh et al., 2025). 6 Contextualized Visual Personalization in Vision-Language Models Table 2. CapEval-QAs performances (described in Section 3.2.2) on our personalized image captioning benchmark. Here, denotes the performance gain relative to the base VLM, and the VLM post-trained with CoViP shows superior performance. 1-Concept 2-Concepts 3-Concepts 4-Concepts Acc+ Acc Acc+ Acc Acc+ Acc Acc+ Acc Models GPT-4o GPT-5 Gemini-2.0-Flash Gemini-3.0-Pro Qwen3-VL-8B Qwen3-VL-30B-A3B 34.2 48.3 41.9 58.1 39.0 40.2 21.6 28.2 28.6 45.1 Proprietary VLMs (Close-sourced) 98.2 97.3 96.7 96. 98.6 97.9 97.3 97.2 Open-Sourced VLMs 97.7 97.7 25.6 27.5 97.5 96.2 20.4 26.1 26.6 39.0 23.3 25.3 Qwen3-VL-8B + RAP Qwen3-VL-8B + RePIC Qwen3-VL-8B + CoViP (Increased) 20.5 44.0 77.4 + 38.4 Post-Training-based Personalized VLMs 9.9 29.2 65.2 + 41.9 10.4 31.7 68.4 + 42.8 99.1 97.0 94.1 - 99.0 97.1 94.8 - 99.3 98.7 98.3 98. 98.1 97.7 99.5 97.8 94.8 - 15.3 18.9 23.1 32.4 18.6 20.1 7.3 24.0 59.7 + 41.1 99.2 98.7 98.3 97. 98.1 98.1 99.2 97.2 92.8 - Post-training setup. For fair comparison, we retrain all baselines using the same Qwen3-VL backbone and report the reproduced results. Training is conducted using LoRA (Hu et al., 2022), and details are provided in Appendix C. 5.1. Benchmarking Personalized Image Captioning In Table 2, we report results for each concept in the test set of our benchmark. Our evaluation protocol, CapEval-QAs, assesses whether VLMs successfully perform personalized image captioning, with MCQA accuracy serving as the primary metric. Here, positive accuracy measures the correctness of MCQ answers for positive concepts using only the generated caption. Negative accuracy, in contrast, measures the models ability to correctly determine that questions associated with negative concepts cannot be answered based solely on the generated caption. Key Finding 1 Existing VLMs lack the ability to generate contextgrounded captions. Interestingly, we observe that both open-source and proprietary VLMs exhibit limited ability to generate context-grounded captions. Among proprietary models, Gemini-3.0-Pro achieves the strongest performance. Furthermore, existing post-training-based personalization baselines provide only marginal gains on our benchmark: RAP even underperforms the zero-shot baseline, due to limited generalizability, as also noted by Oh et al. (2025), while RePIC yields only slight improvement. These findings suggest that prior approaches fail to adequately incorporate useful contextual information when generating personalized captions in our benchmark. Key Finding 2 CoViP substantially improves the VLMs contextual grounding capability through RL-based post-training. We analyze the performance gains achieved by RL-based post-training relative to the base VLM. Notably, our approach yields an average performance improvement of approximately 40% across all concepts. Appendix provides additional discussion on the Acc results. 5.2. Evaluations on Downstream Personalization Tasks We evaluate the generalization capability of CoViP on downstream personalized tasks and compare it against competitive baselines. As shown in Table 3, although Gemini-3.0-Pro achieves strong performance on both the ITR and LSD tasks, it underperforms CoViP on the LAR task. In contrast, the GPT-5 attains the best results on the LAR task but exhibits substantially weaker performance on the LSD and ITR tasks. RePIC shows only marginal or no improvement over the base VLM across all tasks. By comparison, CoViP consistently delivers stable and notable improvements over the base VLM across all diagnostic tasks, with the gains becoming even more pronounced when CAG is applied. We draw the following core observations. Key Finding 3 Personalized image captioning provides reliable bridge for downstream personalization by enabling CoViP to effectively leverage CAG. 1) CoViP improves personalization of VLM across all downstream tasks. Compared to baselines, CoViP exhibits consistent performance gains across all tasks. Specifically, 7 Contextualized Visual Personalization in Vision-Language Models Table 3. Recall score performances on the downstream diagnostic personalization tasks. LSD LAR ITR Models GPT-4o GPT-5 Gemini-2.0-Flash Gemini-3.0-Pro Qwen3-VL-8B Qwen3-VL-30B-A3B Direct w/ CAG Direct Proprietary VLMs (Close-sourced) 28.7 28.5 52.7 76.2 33.6 34.4 46.0 89.3 4.80 50.8 11.6 9.40 Open-Sourced VLMs 29.8 25.6 48.8 42. 17.4 7.60 Post-Training-based Personalized VLMs w/ CAG Direct w/ CAG 7.40 59.3 42.3 44. 19.6 16.8 0.80 17.8 49.2 + 29.6 8.40 18.6 66.1 89.4 9.40 8.80 0.00 27.2 28.0 + 18.6 13.5 10.5 12.2 19. 6.80 0.40 0.20 27.8 42.8 + 36.0 Qwen3-VL-8B + RAP Qwen3-VL-8B + RePIC Qwen3-VL-8B + CoViP (Ours) (Increased) 27.0 32.7 37.2 + 7.4 28.8 52.1 58.2 + 9.4 1.40 16.2 34.8 + 17. CoViP substantially outperforms the base VLM, indicating that post-training tailored for personalized image captioning guides the model to generate context-grounded captions and strengthens its fundamental personalization capability. 2) CoViP effectively benefits from CAG. We observe that applying CAG to CoViP leads to consistent, task-agnostic performance gains compared to direct zero-shot inference. Interestingly, while CAG improves performance on the LSD and LAR tasks for proprietary VLMs, it causes significant performance drop on the ITR task, with recall decreasing to below 20% across all VLMs after applying CAG. This trend is also observed in open-source VLMs. We attribute this behavior to the challenging nature of the ITR task, which requires reliably capturing easily overlooked triggering keywords. Generic captions that lack fine-grained details fail to benefit from CAG and instead degrade performance. In contrast, CoViP incorporates granular contextual information from dialogues into its captions, enabling it to effectively leverage the benefits of CAG and achieve consistent performance gains. 3) Prior baselines fail to generalize in diagnostic evaluations. Post-training-based personalization baselines, such as RAP and RePIC, that show limited improvements on our benchmark also demonstrate limited generalization on diagnostic tasks compared to CoViP. This suggests that only CoViP equips VLMs with the capability to generalize to more complex personalization scenarios. 6. Discussions 6.1. Caption Quality Preservation with CoViP Although CoViP explicitly encourages contextual inclusion and does not directly compromise caption fidelity, we verify that post-training does not degrade the descriptive quality of the base VLM. We evaluate the model on detailed image captioning and hallucination benchmarks. We evaluate detailed captioning performance on subset of 3,000 images from the DOCCI test set using five referencebased metrics. As shown in Table 4, CoViP achieves performance comparable to the baseline across all metrics, with the exception of minor drop in BLEU. These results indicate that CoViP preserves its fine-grained descriptive capability. We further assess hallucination behavior using the MMHal and POPE benchmarks. As shown in Table 5, the hallucination rate of CoViP on MMHal remains on par with the baseline. Similarly, as shown in Table 6, across multiple POPE metrics, we observe no notable degradation; in particular, CHAIR scores remain largely consistent, with only slight increase in CHAIRs relative to the baseline. Taken together, these results demonstrate that post-training under CoViP does not harm general captioning quality. While CoViP substantially improves contextualized visual personalization, it maintains competitive performance in detailed image description and does not exacerbate hallucination. 6.2. Human Alignment of CapEval-QAs We conduct human evaluation to examine whether our evaluation protocol, CapEval-QAs, aligns more closely with human preferences. We consider three strong baselines, and as shown in Fig. 4, captions favored under CapEval-QAs consistently achieve higher win or tie rates across both evaluation criteria, suggesting that our QA-based metric captures aspects of caption quality that better reflect human judgment. Further details are described in the Appendix G. Specifically, (i) Context Groundedness: in terms of contex8 Contextualized Visual Personalization in Vision-Language Models Table 4. DOCCI (Onoe et al., 2024) scores on subset of the test set. Model Qwen3-VL-8B CoViP BLEU 24.58 21.13 ROUGE-L 0.178 0. METEOR 415.83 415.71 SPICE 0.140 0.147 BERTScore 0.727 0.719 Table 5. Hallucination evaluations on MMHal-benchmark (Sun et al., 2024). Model Overall () Halluc. Qwen3-VL-8B CoViP 0.47 0.41 rate 0.42 0.42 Scores Attr. Adv. Comp. Count. Rel. Env. Hol. Other 3.75 4.33 4.08 4.33 3.92 3.42 2.92 3. 3.33 3.50 3.83 3.58 3.58 3.83 2.08 1.50 Table 6. POPE (Li et al., 2023) and CHAIR (Rohrbach et al., 2018) results. Model Acc Qwen3-VL-8B 91.3 91.6 CoViP Pre 98.3 97.9 Rec 84.5 85.5 F1 90.8 91.2 Yes (%) CHAIRs () CHAIRi () 44.3 45.0 15.4 16. 7.7 8.0 Figure 4. Results of the human preference evaluation. Here, Win denotes the win rates of CoViP compared to the baseline. Figure 5. Visualization of comparative results for visual-triggered personalization diagnostics. Scores indicate relative performance. tual grounding, our approach demonstrates clear preference advantage over all three baselines. (ii) New Image Description: regarding the quality of describing the new query image, our method performs on par with Qwen3-VL-8B, with no significant performance gap observed relative to other baselines. Importantly, these results indicate that CoViP does not overemphasize contextual recall at the expense of accurately describing the new image, further supporting its validity as balanced framework for contextualized visual personalization. task). We attribute this behavior to fundamental gap: proprietary VLMs do not reliably perform personalized image captioning. This limits the effectiveness of CAG, resulting in inconsistent and unreliable performance improvements in downstream applications. Therefore, to mitigate such instability and achieve robust improvements across diagnostic tasks, it is essential to incorporate post-training stage focused on personalized image captioning prior to downstream inference. 6.3. Why Personalized Image Captioning Should Precede Downstream Personalization Tasks 6.4. CoViP Enhances Context Retrieval and Integration Rather Than Recognition Itself As illustrated in Table 3 and Figure 5, proprietary VLMs exhibit relatively strong performance on diagnostic tasks, despite achieving substantially lower scores on the personalized image captioning benchmark compared to VLMs trained under the CoViP framework. However, this advantage of proprietary VLMs does not generalize across all diagnostic tasks, and performance gains obtained through CAG are often unstable and highly task-dependent (i.e., ITR Figure 6 analyzes the relationship between recognition and retrieval. As shown in the figure, the average F1 score exhibits moderate increase across models, whereas MCQA accuracy improves by substantially larger margin at comparable F1 levels. This indicates that baseline models already achieve reasonable recognition capability, but their low performance under our benchmark probing stems from retrieval as the primary bottleneck. Consequently, we show 9 Contextualized Visual Personalization in Vision-Language Models that the performance gains achieved by CoViP are driven primarily by improved retrieval, specifically more effective integration of implicit personal cues in captions through contextualized reasoning, rather than by improvements in recognition itself. 7. Conclusions and Limitations Summary of findings. We introduce new paradigm of contextualized visual personalization, defining it as grounding visual understanding in user-specific past visual experiences provided in the models context and using this grounding to guide personalized outputs. Our framework, CoViP, enables holistic personalization through post-training on novel, challenging personalized image captioning benchmark, along with caption-augmented generation (CAG). Extended studies on our diagnostic tasks demonstrate that CoViP serves as foundational stage for contextualized visual personalization, enhancing both the robustness and generalizability of existing VLMs. Limitations. Our benchmark relies on synthetic dialogues and generated images, which may contain factual inconsistencies or visual artifacts. Although we applied rigorous model-based quality filtering for benchmark construction, additional human verification could further improve the factuality of the dialogues and the visual fidelity of the images. Future directions. key next step is to build personalization benchmarks that better reflect real-world user signals, such as shopping histories, delivery records, or long-term user-model interaction logs, to enable more practical evaluations on various use-case scenarios."
        },
        {
            "title": "Impact Statements",
            "content": "This work proposes new paradigm for contextualized visual personalization in VLMs, enabling responses grounded in user-specific visual experiences and historical context. By introducing challenging benchmark and principled post-training framework, CoViP achieves reliable and robust performance across downstream personalization tasks, offering potential benefits for real-world applications such as proactive personal AI assistants and context-aware AI systems. However, enhanced personalization necessitates important ethical considerations, as leveraging personal visual histories may risk exposing sensitive information if deployed without adequate safeguards. While our benchmark and post-training framework are intended for controlled research settings, future work should investigate privacypreserving mechanisms to ensure that advances in contextualized visual personalization are deployed responsibly and for the benefit of users. Figure 6. Scatter plot of recognition versus retrieval on the proposed benchmark. Recognition is measured by the F1 score of entity name inclusion between generated captions and groundtruth dialogues, while retrieval is measured by positive MCQA accuracy. Here, denotes the slope of the linear regression line."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT) [NO.RS-2021-II211343, Artificial Intelligence Graduate School Program (Seoul National University); No.2022-000959, RS-2022-II220959], by the National Research Foundation of Korea (NRF) grant [No.2022R1A3B1077720, 2022R1A5A7083908], BK21 FOUR Program of the Education and Research Program for Future ICT Pioneers, Seoul National University in 2026, and by the Samsung Electronics Co., Ltd [IO250520-12926-01]."
        },
        {
            "title": "References",
            "content": "Alaluf, Y., Richardson, E., Tulyakov, S., Aberman, K., and Cohen-Or, D. Myvlm: Personalizing vlms for userspecific queries. In European Conference on Computer Vision, pp. 7391. Springer, 2024. An, R., Yang, S., Zhang, R., Shen, Z., Lu, M., Dai, G., Liang, H., Guo, Z., Yan, S., Luo, Y., et al. Unictokens: Boosting personalized understanding and generation via unified concept tokens. arXiv preprint arXiv:2505.14671, 2025. Baek, J., Chandrasekaran, N., Cucerzan, S., Herring, A., and Jauhar, S. K. Knowledge-augmented large language models for personalized contextual query suggestion. In Proceedings of the ACM Web Conference 2024, pp. 3355 3366, 2024. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Contextualized Visual Personalization in Vision-Language Models Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Doveh, S., Shabtay, N., Schwartz, E., Kuehne, H., Giryes, R., Feris, R., Karlinsky, L., Glass, J., Arbelle, A., Ullman, S., et al. Teaching vlms to localize specific objects from in-context examples. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9572 9582, 2025. Google. Gemini 3 pro: the frontier of vision ai, December 2025. URL https://deepmind.google/models/ gemini/pro/. Hao, H., Han, J., Li, C., Li, Y.-F., and Yue, X. Rap: Retrievalaugmented personalization for multimodal large language In Proceedings of the Computer Vision and models. Pattern Recognition Conference, pp. 1453814548, 2025. Hong, R., Lang, J., Zhong, T., Wang, Y., and Zhou, F. Tameing long contexts in personalization: Towards trainingfree and state-aware mllm personalized assistant. arXiv preprint arXiv:2512.21616, 2025. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Hu, Y., Liu, S., Yue, Y., Zhang, G., Liu, B., Zhu, F., Lin, J., Guo, H., Dou, S., Xi, Z., Jin, S., Tan, J., Yin, Y., Liu, J., Zhang, Z., Sun, Z., Zhu, Y., Sun, H., Peng, B., Cheng, Z., Fan, X., Guo, J., Yu, X., Zhou, Z., Hu, Z., Huo, J., Wang, J., Niu, Y., Wang, Y., Yin, Z., Hu, X., Liao, Y., Li, Q., Wang, K., Zhou, W., Liu, Y., Cheng, D., Zhang, Q., Gui, T., Pan, S., Zhang, Y., Torr, P., Dou, Z., Wen, J.-R., Huang, X., Jiang, Y.-G., and Yan, S. Memory in the age of ai agents. arXiv preprint arXiv:2512.13564, 2025. Kim, J., Kim, W., Park, W., and Do, J. Mmpb: Its time for multi-modal personalization. In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025a. Kumari, N., Zhang, B., Zhang, R., Shechtman, E., and Zhu, J.-Y. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 19311941, 2023. Li, F., Zhang, R., Zhang, H., Zhang, Y., Li, B., Li, W., Ma, Z., and Li, C. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.-R. Evaluating object hallucination in large visionarXiv preprint arXiv:2305.10355, language models. 2023. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Liu, J., Qiu, Z., Li, Z., Dai, Q., Yu, W., Zhu, J., Hu, M., Yang, M., Chua, T.-S., and King, I. survey of personalized large language models: Progress and future directions. arXiv preprint arXiv:2502.11528, 2025a. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pp. 3730 3738, 2015. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Mok, J., Kim, I.-h., Park, S., and Yoon, S. Exploring the potential of llms as personalized assistants: Dataset, evaluation, and analysis. arXiv preprint arXiv:2506.01262, 2025. Nguyen, T., Liu, H., Li, Y., Cai, M., Ojha, U., and Lee, Y. J. Yollava: Your personalized language and vision assistant. Advances in Neural Information Processing Systems, 37: 4091340951, 2024. Kim, T. S., Lee, Y., Park, Y., Kim, J., Kim, Y.-H., and Kim, J. Cupid: Evaluating personalized and contextualized alignment of llms from interactions. arXiv preprint arXiv:2508.01674, 2025b. Nguyen, T., Singh, K. K., Shi, J., Bui, T., Lee, Y. J., and Li, Y. Yochameleon: Personalized vision and language generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1443814448, 2025. 11 Contextualized Visual Personalization in Vision-Language Models Team, Q. Qwen3-vl: Sharper vision, deeper thought, broader action, 2025. Technical report, Technical Report on the Qwen3-VL Model Series. 6, 2025. Unsplash. Unsplash: Beautiful free images & pictures. https://unsplash.com/, 2026. Wang, T., Tao, M., Fang, R., Wang, H., Wang, S., Jiang, Y. E., and Zhou, W. Ai persona: Towards life-long personalization of llms. arXiv preprint arXiv:2412.13103, 2024. Wang, W., Gao, Z., Gu, L., Pu, H., Cui, L., Wei, X., Liu, Z., Jing, L., Ye, S., Shao, J., et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. Xiao, Y., Li, Y., Wang, H., Tang, Y., and Wang, Z. Z. Toolmem: Enhancing multimodal agents with learnable tool capability memory. arXiv preprint arXiv:2510.06664, 2025. Yu, S., Kim, I.-h., Song, J., Lee, S., Park, J., and Yoon, S. Unleashing multi-hop reasoning potential in large language models through repetition of misordered context. In Findings of the Association for Computational Linguistics: NAACL 2025, pp. 64356455, 2025. Zhang, B. and Sundar, S. S. Proactive vs. reactive personalization: Can customization of privacy enhance user experience? International journal of human-computer studies, 128:8699, 2019. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Zhou, H., Chen, Y., Guo, S., Yan, X., Lee, K. H., Wang, Z., Lee, K. Y., Zhang, G., Shao, K., Yang, L., et al. Memento: Fine-tuning llm agents without fine-tuning llms. arXiv preprint arXiv:2508.16153, 2025. Oh, Y., Chung, D., Shin, J., Park, S., Barthelemy, J., Mok, J., and Yoon, S. Repic: Reinforced post-training for personalizing multi-modal language models. arXiv preprint arXiv:2506.18369, 2025. Onoe, Y., Rane, S., Berger, Z., Bitton, Y., Cho, J., Garg, R., Ku, A., Parekh, Z., Pont-Tuset, J., Tanzer, G., et al. Docci: Descriptions of connected and contrasting images. In European Conference on Computer Vision, pp. 291 309. Springer, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PmLR, 2021. Rohrbach, A., Hendricks, L. A., Burns, K., Darrell, T., and Saenko, K. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2250022510, 2023. Salemi, A., Kallumadi, S., and Zamani, H. Optimization methods for personalizing large language models through retrieval augmentation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 752762, 2024a. Salemi, A., Mysore, S., Bendersky, M., and Zamani, H. Lamp: When large language models meet personalizaIn Proceedings of the 62nd Annual Meeting of tion. the Association for Computational Linguistics (Volume 1: Long Papers), pp. 73707392, 2024b. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shen, H., Liu, P., Li, J., Fang, C., Ma, Y., Liao, J., Shen, Q., Zhang, Z., Zhao, K., Zhang, Q., et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Singh, A., Fry, A., Perelman, A., Tart, A., Ganesh, A., ElKishky, A., McLaughlin, A., Low, A., Ostrow, A., Ananthram, A., et al. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. Sun, Z., Shen, S., Cao, S., Liu, H., Li, C., Shen, Y., Gan, C., Gui, L., Wang, Y.-X., Yang, Y., et al. Aligning large multimodal models with factually augmented rlhf. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 1308813110, 2024. 12 Contextualized Visual Personalization in Vision-Language Models Figure S.1. Overview of our personalized image captioning framework. Given query image, VLM generates personalized caption by referencing only the positive concept images and their associated dialogues within the context. An external LLM then evaluates whether the generated caption alone provides sufficient information to correctly answer set of MCQs, using accuracy as the evaluation metric. During RL-based post-training, this accuracy is further leveraged as VR signal. A. Related Works Contextualized personalization in LLMs. Contextualized personalization in large language models (LLMs) refers to the ability of model to generate responses aligned with users experiences contained in the prompt context (Liu et al., 2025a; Bai et al., 2022). Building on this foundation, growing body of work has explored settings in which user profiles (Wang et al., 2024), past dialogues (Baek et al., 2024), and previously shown content (Salemi et al., 2024b; Mok et al., 2025; Kim et al., 2025b) are incorporated into the context, allowing LLMs to produce personalized responses. More recently, studies have investigated personalization frameworks that retrieve past content relevant to the current user query, construct an appropriate memory, and condition the LLMs response on this memory (Hu et al., 2025; Salemi et al., 2024a; Zhou et al., 2025; Xiao et al., 2025). However, LLMs still frequently fail to effectively ground their outputs in the provided contextual information, and addressing this limitation remains an open challenge in existing research (Liu et al., 2024; Yu et al., 2025). In this work, we transfer the paradigm of contextualized personalization, actively studied in LLMs, to the setting of visual personalization in VLMs and conduct systematic exploration of this problem. B. Detailed Description of Evaluation Protocol in Personalized Image Captioning Benchmark We adopt multiple-choice question-answering (MCQA) formulation to evaluate caption-level personalization, as it enables precise, interpretable measurement of contextual grounding. Unlike open-ended caption evaluation, which is often sensitive to surface-level lexical variation, MCQA enables us to directly assess whether generated caption provides sufficient evidence to support specific contextual inferences. In particular, the inclusion of distractor options and an explicit cannot be determined choice enables us to distinguish between three behaviors: (i) correctly utilizing relevant contextual information, (ii) hallucinating unsupported details, and (iii) appropriately abstaining when evidence is insufficient. This design is especially well-suited for evaluating personalization, where the key challenge lies not in visual recognition itself, but in selectively retrieving and applying user-specific contextual Contextualized Visual Personalization in Vision-Language Models Algorithm 1 Algorithm for caption-based MCQA Probing of CoVIP Require: Personalization model pθ, judge LLM ; dataset = {(xi, ci, QAi)}N 1: Here, πθ denotes the captioning policy; xi is the query image; ci is the interleaved context; QAi is concept-indexed i=1. QA bank; and gi denotes the positive concept set. kgi (cid:83) (cid:83) Sample personalized caption si πθ( xi, ci). QAi[concept k] QAi[concept k] do Ensure: Per-split metrics: Acc+ (Positive accuracy), Acc (Negative accuracy), and verifiable reward rcaps. 2: for 1 to do 3: 4: Q+ 5: 6: 7: 8: 9: 10: 11: Sample outputs of following the evaluation prompt ψ(si, q): o(q) (ψ(si, q)). for each Q+ /gi I[o(q) = a(q)], where a(q) are GT answer for q. I[o(q) = D]. {Positive QAs} {Negative QAs} 12: 13: 14: (cid:80) qQ+ 1 Q+ 1 end for Scoring with accuracy metrics: (cid:80) Acc+ Acc qQ (Assign VRs only if on-policy RL training) σ 1 Calculate the sample-level VR: rcaps(i) Acc+ I[o(q) = D] ασ qQ (cid:80) 15: 16: 17: end for 18: Aggregate Acc+ , where α is coefficient that controls the strength of the penalty. (cid:80) Acc+ , Acc 1 Nneg (cid:80) Acc , where = Npos + Nneg Npos knowledge. As result, MCQA probing provides reliable and scalable proxy for measuring how effectively model internalizes and utilizes personalized context in caption generation. For each pre-generated dialogue described in Section 3.2.1, we construct set of discriminative multiple-choice question answering (MCQA) items using an external large language model. These questions are designed to probe whether generated caption accurately reflects the contextual information embedded in the dialogue, focusing on attributes such as temporal cues, personalized descriptions, and user-specific experiences. Concretely, each dialogue yields three MCQA samples. Each sample consists of four answer options: (i) one correct answer grounded in the dialogue context, (ii) two distractor options that are plausible but indistinguishable without access to the relevant contextual information, and (iii) an explicit cannot be determined option, which captures cases where the provided caption does not contain sufficient evidence to answer the question. This construction allows us to assess not only whether the caption captures relevant contextual details, but also whether it avoids hallucinating or over-generalizing beyond the available information. To enable fine-grained evaluation of contextualized personalization, we explicitly distinguish between positive and negative concepts within each dialogue context. Formally, for each dialogue di, we construct set of QA pairs (qik, aik) such that each pair is labeled as either positive or negative depending on whether it corresponds to concept relevant to the query image. Positive QA pairs evaluate whether the caption successfully incorporates relevant contextual information, whereas negative QA pairs test whether the caption avoids incorrectly transferring unrelated contextual details. This distinction allows us to separately measure the models ability to retrieve relevant personalized information and to suppress irrelevant or misleading context. To quantitatively evaluate the degree of contextualized personalization in generated captions, we define two complementary metrics: Positive Accuracy and Negative Accuracy. These metrics assess whether caption correctly incorporates relevant contextual information while avoiding irrelevant or misleading content. Let Q+ and denote the sets of MCQA samples derived from positive and negative concepts, respectively. For each questionanswer pair (qik, aik), judge model is provided with the generated caption and produces predicted answer ˆaik = (qik, s). 14 Contextualized Visual Personalization in Vision-Language Models Positive Accuracy measures how well the generated caption captures contextually relevant information. It is defined as the proportion of positive-concept questions for which the judge model selects the correct answer: Acc+ = 1 Q+ (cid:88) [J (qik, s) = aik] , (qik,aik)Q+ (S.1) where I[] denotes the indicator function. higher Positive Accuracy indicates that the generated caption successfully encodes relevant personalized information grounded in the dialogue context. Negative Accuracy evaluates whether the model avoids incorporating irrelevant contextual information. For questions derived from negative concepts, the correct behavior is to select the cannot be determined option, indicating that the caption does not contain sufficient evidence to answer the question. Formally, Negative Accuracy is defined as: Acc = 1 (cid:88) (qik,aik)Q [J (qik, s) = cannot be determined] . (S.2) While Positive Accuracy reflects the models ability to retrieve and express relevant personalized information, Negative Accuracy captures its robustness against hallucination and over-generalization. Together, these metrics provide balanced and interpretable evaluation of caption-level personalization, measuring both selective recall and selective omission of contextual information. C. Implementation Details of the Post-training Pipeline Post-training details. To mitigate training instability caused by small batch sizes, we employ gradient accumulation with 2 steps for both GRPO and DrGRPO, and 1 step for GSPO. To prevent excessively verbose generations, the maximum completion length is capped at 511-tokens. Our implementation builds on an open-source codebase.1 For training, we apply LoRA with rank 64 and scaling factor (alpha) 128, and sample 4 rollouts per prompt to compute group-level advantages. We adopt Qwen3-VL-Instruct-8B as the backbone VLM, which supports multimodal processing of images and text and exhibits strong instruction-following capabilities. This choice is motivated by its open-source availability and its ability to handle multi-image and long-context inputs, which are essential for contextualized personalization. We use this model for on-policy RL post-training. Optimization strategy for CoViP. GSPO (Zheng et al., 2025) is an on-policy RL algorithm that (i) samples group of completions per input and (ii) performs sequence-level off-policy correction and clipping, aligning the optimization unit with the sequence-level reward. For each (x, c), we roll out group {si}G i=1 πθold( x, c), compute VR ri = r(si, x, c) from Eq. (1) of our main paper, and define the group-normalized advantage It is worth noting that GSPO uses sequence-likelihood importance ratio νi(θ) with length normalization: ˆAi = ri mean({rj}G std({rj}G j=1) j=1) . νi(θ) = (cid:18) πθ(si x, c) πθold(si x, c) (cid:19) 1 si = exp 1 si si (cid:88) t=1 log πθ(si,t x, c, si,<t) πθold(si,t x, c, si,<t) . and optimizes the clipped surrogate objective at the sequence level: J(θ) = (cid:34) 1 (cid:88) i=1 (cid:16) min νi(θ) ˆAi, clip(νi(θ), ϵ1, ϵ2) ˆAi (cid:35) (cid:17) , 1https://github.com/om-ai-lab/VLM-R 15 (S.3) (S.4) (S.5) Contextualized Visual Personalization in Vision-Language Models where ϵ1 = 1 ϵ, ϵ2 = 1 + ϵ. Unlike token-level importance weighting (Shao et al., 2024), GSPO assigns single clipped weight to each sequence. This approach aligns with our reward design, which evaluates the entire caption as single unit, thereby enhancing training stabilityparticularly for long-form generation. Furthermore, length-normalization of the ratio mitigates variance and ensures that importance weights remain within comparable numerical range across varying caption lengths. Degeneration filtering. As noted in Eq. (6) in our main paper, to prevent reward hacking via degenerate repetition, we apply hard penalty to repetitive or overly long generations when computing rcaps. Let ρsent, ρn-gram (with n=5), and ρchunk(L) (with and = {10, 20, 30} words) denote sentence-level, n-gram-level, and chunk-level duplication ratios, respectively. Formally, we define repetition-based degeneration indicator as (cid:104) δ(y) = ρsent τs ρn-gram τn max LL ρchunk(L) τc (cid:105) , (S.6) where τs=0.3, τn=0.3, and τc=0.2. To further discourage verbose generations, we also impose length constraint. Let denote the tokenizer encoding length and be fixed threshold. We then define the overall degeneration filtering predicate as R(y) = (cid:0)δ(y) = 1(cid:1) (cid:0)y > l(cid:1). (S.7) D. Experimental Configurations Used VLMs For generating dialogues, we employ the VLM Qwen/Qwen3-VL-30B-A3B-Instruct-FP8. To generate MCQA pairs from these dialogues and to calculate accuracy-based VR via an LLM-as-a-judge, we use the Qwen/Qwen3-30B-A3B-Instruct-2507-FP8 LLM. Across all experiments, the decoding temperature is fixed to 0.0 to ensure deterministic generation. Design of Diagnostic Personalization Tasks We design suite of diagnostic tasks to evaluate implicit visual personalization in VLMs. These diagnostics are guided by core principle: the model must recognize the individual depicted in query image using visual evidence and retrieve relevant personalized experiences from contextual memory, while shortcut behaviors that bypass visual grounding are explicitly disallowed. Collectively, these tasks are designed to quantitatively assess models ability to perform personalized reasoning grounded in visual identity. To construct the diagnostic tasks, we require multiple images of the same individual to enable identity-level reasoning across contexts. We therefore sample 50 individuals from the MMPB dataset (Kim et al., 2025a), collecting 4 images per person, resulting in total of 200 human images. For each individual, one image is used as the query image, while the remaining three images are used within contextual dialogues. For each context, we generate total of 10 visual-textual dialogues. Among them, 3 dialogues correspond to the same individual as the query image, while the remaining 7 correspond to different individuals randomly sampled from the other 49 identities. Each individual serves as the query subject in 10 different contexts, yielding total of 500 contexts. Each dialogue is automatically generated using VLM-based generator. For the three dialogues associated with the query individual, the generator is provided with randomly sampled name, location, and date to construct personalized experiences. The prompt used for dialogue generation is provided in Table S.12. Across all diagnostic tasks, the model is required to: (1) recognize the identity of the person in the query image, (2) retrieve all relevant dialogues associated with that individual from the context, and (3) perform temporal reasoning to identify the most recent interaction. This setup prevents shortcut solutions such as only-text matching or partial context retrieval, as correct prediction requires joint visual recognition and memory-based reasoning over multiple dialogue instances. 1. Last-Seen Detection (LSD). In the LSD task, each dialogue contains an explicit reference to when and where the user encountered the individual, e.g., still remember the day saw John at Lake Francesborough on 2025-09-09. Given new query image, the user asks: Where did last see the person in this image? To answer correctly, the model must identify the person, retrieve all associated dialogues, compare their timestamps, and extract the location from the most recent one. Performance is measured using word-level F1 between the model output and the ground-truth location. 16 Contextualized Visual Personalization in Vision-Language Models Figure S.2. Used real images to construct our database. Figure S.3. Visualization of query images generated with varying numbers of concept images. 2. Last-Action Recall (LAR). LAR extends LSD by requiring recall of finer-grained personal action rather than location. For each context, we append an additional user utterance to the last-seen dialogue of the query individual, describing specific action, e.g., Oh wait, need to go to the post office to return this package. The action is randomly sampled from predefined candidate set (Table S.14) and injected into all 500 contexts. Given query image, the user asks: What was doing the last time mentioned the person in this image? The model must identify the correct individual, locate the most recent dialogue, and retrieve the embedded action. Unlike LSD, which focuses on factual recall, LAR evaluates the models ability to retrieve fine-grained, episode-level user states. We evaluate performance using an LLM-based judge that determines whether the generated response semantically matches the ground-truth action (see Table S.15). 3. Instruction-Triggered Recall (ITR). ITR evaluates more proactive form of personalization. In this task, the last-seen dialogue includes an instruction of the form: If this person ever shows up again, remind me by saying the keyword SKS. At inference time, the user asks generic question such as: Where did last see the person in this image? without explicitly mentioning the keyword. To succeed, the model must not only recognize the individual and retrieve the relevant memory, but also proactively incorporate the specified trigger keyword (SKS) into its response. Unlike LSD and LAR, which evaluate reactive personalization, ITR assesses whether the model can perform proactive personalization conditioned on visual recognition and prior instructions. We evaluate ITR using the trigger success rate, defined as the proportion of responses that include the keyword SKS. Procedures for Benchmark Construction Prior works adopt various data acquisition strategies to construct evaluation sets. For example, RAP leverages frame-level sampling from YouTube videos followed by automated caption generation, while RePIC curates images from movie teasers and award ceremonies where multiple celebrities co-occur. Despite these 17 Contextualized Visual Personalization in Vision-Language Models Figure S.4. Quality filtering results obtained with Gemini-2.5 Flash. Yes denotes the proportion of retained query images after filtering. efforts, collecting diverse real-world evaluation datasets that simultaneously exhibit varied backgrounds and offer finegrained, controllable visual granularity for each concept remains major bottleneck. To address this limitation, we construct benchmark composed of realistically generated images. Specifically, we built database of 188 images, consisting of 123 human images, 34 object images, and 31 pet images. Each image corresponds to unique identity, ensuring no identity overlap across the dataset. The database is carefully curated to support fine-grained visual personalization. For the human category, including YoLLaVA (Nguyen et al., 2024) and CelebA (Liu et al., 2015), as well as through web crawling from royalty-free image platforms such as Unsplash and Pexels. images were collected from combination of open-source datasets, For the object and pet categories, we utilized subset images from existing personalization benchmarks, including MyVLM (Alaluf et al., 2024), Custom101 (Kumari et al., 2023), RAP-LLaVA (Hao et al., 2025), and DreamBooth (Ruiz et al., 2023). Benchmark quality filtering results. As shown in Figure S.4, images synthesized with one or two concepts generally align well with the prompts, whereas the failure rate increases noticeably starting from three concepts. Specifically, for four concepts, nearly 45% of the synthesized images fail to faithfully reflect the specified prompts. Accordingly, more stringent quality filtering is applied for samples involving three or four concepts. Note that visualizations of representative samples are presented in Figure S.3. E. Additional Analysis Analysis of training dynamics Table S.1 reports an ablation over on-policy training algorithms, including GRPO, DrGRPO, and GSPO. Notably, GSPO consistently achieves the best performance across all concepts; therefore, we adopt GSPO as our default on-policy RL post-training algorithm. We further interpret these evaluation results through the lens of training-time reward trajectories. In Figure S.5(a), naively applying GSPO leads to gradual decline in accumulated reward as training proceeds, especially in later stages. This suggests that without degeneration filtering, optimization can drift toward misleading behaviors that degrade the personalization performance. Importantly, introducing our degeneration filtering further stabilizes training: completion length gradually converges while reward improves. This indicates that the VLM learns to selectively include only contextually relevant information without introducing verbosity, producing concise yet context-grounded captions that better explain the query. In Figure S.5(b), GRPO converges more slowly due to KL-divergence regularization, whereas DrGRPO and GSPO, which do not use KL regularization, exhibit faster convergence. However, DrGRPO is notably unstable and tends to diverge early in training. In contrast, our GSPO-based post-training remains stable and achieves substantially higher rewards, indicating favorable trade-off between optimization efficiency and training stability. Finally, Figure S.5(c) highlights the role of prompt design in training stability. When we randomly alternate prompt roles (i.e., switching between user and system prompts) instead of using fixed captioning prompt, the completion length rapidly diverges. We posit that this behavior arises because the VLM loses instruction-following fidelity and thus tends to incorporate information from both relevant and irrelevant dialogues indiscriminately, leading to excessively verbose outputs. Analysis of Acc results in Table 2 As shown in Table 2, the VLM post-trained with CoViP achieves substantially higher Acc+ compared to the baseline, while exhibiting slight decrease in Acc. We argue that this decrease does 18 Contextualized Visual Personalization in Vision-Language Models Table S.1. Ablation results of different algorithmic variants for on-policy RL post-training. Models Accuracy (%) Qwen3-GRPO (Shao et al., 2024) Qwen3-Dr.GRPO (Liu et al., 2025b) Qwen3-GSPO (Zheng et al., 2025) 1-Concept 2-Concepts 3-Concepts 4-Concepts Acc+ Acc Acc+ Acc Acc+ Acc Acc+ Acc 95.2 67.1 95.9 68.3 72.2 95. 95.5 94.7 94.8 96.1 95.1 95.4 95.5 94.2 94.7 53.3 51.6 53.9 41.4 43.3 45.7 47.6 48.6 51. Figure S.5. Reward trajectory ablations on VR designs, on-policy RL algorithms, and prompt templates during post-training. not indicate degradation in the models actual performance. Rather, it arises as consequence of improved contextual understanding and retrieval behavior. Specifically, the baseline model often succeeds in loosely matching the query image to relevant dialogue based on high-level visual concepts, yet fails to retrieve detailed user-specific information from that dialogue. This limitation is reflected in its relatively low Acc+. As result, even when the model misrecognizes the underlying concept, it frequently responds with cannot determine during MCQA evaluation, which is counted as correct response under the Acc metric, effectively producing false positives. In contrast, after post-training via CoViP, the model becomes significantly more capable of retrieving fine-grained, experiencelevel details from the associated dialogue once concept is recognized. Consequently, when the model mistakenly identifies concept, it is now more likely to commit to specific answer based on the retrieved information, leading to lower Acc. Therefore, the observed decrease in Acc does not reflect weaker reasoning ability; rather, it indicates that the model more actively incorporates personalized contextual knowledge into its predictions. F. Additional Results Experiments on other VLM personalization benchmarks. We report the performance of RePIC by reproducing the results with the Qwen3-VL baselines, and the corresponding results are reported. Since extensive hyperparameter optimization was not performed for this baseline, its performance may be slightly lower than the results reported in the original paper. Notably, as shown in Tables S.2 and S.3, CoViP shows comparable or even surpasses performances on the benchmark without relying on the VRs introduced in RePIC, demonstrating the effectiveness and generality of our approach. Ablations on VR designs. Table S.4 reports benchmark performance under different VR configurations. We further analyze (i) the contribution of each VR component and (ii) the effect of the number of concepts used during post-training. 1. Necessity of joint supervision. Training with only rvis (without rcaps) degrades performance, in some cases falling below the Qwen3-VL-8B baseline, indicating that visual supervision alone is insufficient for personalized image captioning. Conversely, optimizing only rcaps also yields consistently weaker results, suggesting retrieval signals without fine-grained visual supervision are inadequate. 2. F1-based vision VR vs. binary consistency VR. Replacing RePICs object-consistency VR (OCT), which provides binary correctness feedback, with our set-based F1 VR rvis consistently improves performance on positive accuracy. This indicates that set-level supervision provides denser and more robust learning signal for multi-concept perception. 19 Contextualized Visual Personalization in Vision-Language Models Table S.2. Evaluation results of single-concept personalized grounding performance. Models Backbone Zero-shot RAP RePIC Zero-shot RAP RePIC CoViP Zero-shot RAP RePIC Zero-shot RAP RePIC CoViP Qwen-2.5 VL Qwen-2.5 VL Qwen-2.5 VL Qwen-3 VL Qwen-3 VL Qwen-3 VL Qwen-3 VL Qwen-2.5 VL Qwen-2.5 VL Qwen-2.5 VL Qwen-3 VL Qwen-3 VL Qwen-3 VL Qwen-3 VL Pre. 100 100 100 100 100 100 100 91.5 95.5 99.0 90.4 93.6 92.6 95.2 MyVLM Rec. F1 Pre. YoLLaVA Rec. F1 DreamBooth Rec. Pre. F1 56.8 98.8 96.2 54.7 99.7 74.4 76. Skip-Retrieval Setting 99.6 100 99.7 98.0 100 100 100 Retrieval Setting 72.4 99.4 98.1 70.7 99.9 85.1 86.9 50.6 87.9 83.2 49.7 90.6 66.5 70.0 65.2 91.6 90.4 64.1 92.1 77.4 80.7 77.4 79.2 84.4 74.7 76.6 78.0 77.8 83.2 99.8 96.1 73.9 76.6 89.5 85. 42.3 75.1 69.7 60.4 61.0 67.3 68.5 90.7 99.8 97.9 84.2 86.7 94.5 92.1 55.2 76.2 76.3 66.8 67.9 72.3 72.8 96.0 100 100 98.3 100 99.3 100 95.2 98.7 98.6 98.3 98.0 97.8 98.5 76.6 100 98.1 73.4 99.4 86.1 86. 75.3 94.3 90.5 72.7 94.3 83.5 84.8 85.2 100 99.0 84.1 99.7 90.1 92.9 84.1 96.4 94.4 82.9 96.1 90.1 91.2 Table S.3. Evaluation results of multi-concept personalized grounding performance. Models Backbone Zero-shot RAP-Qwen RePIC Zero-shot RAP-Qwen RePIC CoViP Qwen-2.5 VL Qwen-2.5 VL Qwen-2.5 VL Qwen-3 VL Qwen-3 VL Qwen-3 VL Qwen-3 VL Skip-Retrieval Rec. 75.0 82.9 98.8 82.9 78.7 95.1 87.2 F1 85.7 90.7 99.4 90.7 87.7 97.5 93.2 Pre. 100 100 100 100 97.7 100 100 Retrieval Rec. 64.0 73.2 93.9 80.5 70.7 86.0 82. Pre. 98.1 100 97.5 100 100 99.3 99.3 F1 77.5 84.5 95.7 89.2 82.9 92.2 90.4 3. Effect of increasing the number of positive concepts. Varying the number of positive concepts included during post-training shows that using all four concepts yields the best performance. Comparing VRs used for object detection. While prior works (Oh et al., 2025; Shen et al., 2025) use an Intersection over Union (IoU) score as VR to strengthen localization capabilities, our proposed VR is fundamentally different from localization: it incentivizes the VLM to select only the relevant concepts, thereby balancing precision and recall to ensure discriminative visual grounding. Mitigating degeneration. Finally, rather than naively applying GSPO, adding the proposed n-gram and chunking-based filtering further improves positive accuracy, especially in multi-concept settings. This indicates that discouraging repetitive or degenerate generations stabilizes training and steers optimization toward meaningful, context-grounded solutions. Ablations on different prompt roles. In Table S.5, we conduct ablation experiments on personalized image captioning to analyze performance under different prompt configurations, including user prompts, system prompts, and settings without detailed task instructions. Here, randomized refers to training setup that uses multiple prompt templates to reduce reliance on single captioning prompt, while fixed denotes using the same prompt template during both training and inference. We observe that even when VLM is post-trained using only fixed user prompt, it achieves slightly better performance than the randomized setting when evaluated with system prompt at inference time. In contrast, removing detailed task descriptions from either the user or system prompt leads to significant drop in performance. These results demonstrate the effectiveness of the proposed captioning prompt as user prompt. Importantly, the observed performance gains are not due to overfitting to specific prompt template. Instead, the user prompt effectively encourages context-grounded responses, enabling CoViP to robustly include relevant contextual information even under changes in prompt roles. The user prompt used for image captioning is provided in Table S.7. 20 Contextualized Visual Personalization in Vision-Language Models Table S.4. Ablation of VR components and the effect of the number of concepts used during training on test accuracy. Models Accuracy (%) Qwen-3-VL-8B (Baseline) 1-Concept 2-Concepts 3-Concepts 4-Concepts Acc+ 39. Acc 97.5 Acc+ 25.6 Acc 97.7 Acc+ 23.3 Acc 98.1 Acc+ 18. Acc 98.1 Qwen3-w/o rcaps Qwen3-w/o rvis Qwen3-w/ OCT (Oh et al., 2025) 39.1 61.0 77.0 96.9 95.0 91.9 26.4 44.9 63.1 97.1 95.1 92. 23.5 39.8 58.3 Ablations on Verifiable Rewards Qwen3-upto 1-concept Qwen3-upto 2-concepts Qwen3-upto 3-concepts Qwen3-Full Ablations on Training Concepts (Naive GSPO) 63.7 68.4 65.4 72.2 94.4 94.7 92.4 94. 45.4 53.2 54.4 53.9 95.4 94.9 93.6 94.8 41.4 50.6 51.1 51.3 Training with All of Proposed VRs & Degeneration Filtering 97.7 95.7 91.8 96.1 95.2 94.9 95. 16.5 34.5 59.1 36.0 46.3 45.7 45.7 97.2 96.7 83.4 96.6 95.5 95.4 95.3 Qwen3-VL-8B + CoViP 77. 94.8 68.4 94.1 65.2 94.8 59. 92.8 Table S.5. Evaluation of generalization under variations of prompt templates (roles) applied during post-training. Models Accuracy (%) 1-Concept 2-Concepts 3-Concepts 4-Concepts Acc+ Acc Acc+ Acc Acc+ Acc Acc+ Acc w/ User Prompt Randomize Fixed Zero-Shot Randomize Fixed Zero-Shot Randomize Fixed Zero-Shot 71.7 77.4 39.0 47.8 48.3 25.8 32.9 35.0 17.7 92.2 94.8 97.5 93.8 94.5 98. 95.9 96.0 97.8 G. Specifications on Evaluation Settings 91.7 94.1 97.7 58.8 68.4 25.6 w/ System Prompt 36.7 41.4 17.2 w/o System Prompt 29.6 26.9 11.4 93.8 93.8 98.1 95.3 96.3 98. 53.5 65.2 23.3 32.4 34.7 14.3 24.7 23.1 8.9 93.7 94.8 98.1 94.1 94.4 98.3 95.6 96.0 99. 50.0 59.7 18.6 25.4 26.0 13.2 19.2 12.2 6.2 94.0 92.8 98.1 94.0 93.9 98.4 95.5 95.3 99. Details on human evaluation settings. To assess the quality of personalized, contextualized caption generation, we conducted human evaluation study with 21 different participants. Each participant was asked to complete approximately 1015 evaluation tasks, resulting in total of 276 human judgments. For each task, participants compared two captionsone generated by our method and the other by baseline VLMalong two criteria: context groundedness and new image description quality. The order of the compared captions was randomized per trial to avoid presentation bias. We compare thee selected VLMs: GPT-5, Gemini-3.0-Pro-Preview, and Qwen3-VL-8B. For each evaluation instance, CoViP was randomly paired against one baseline for pairwise comparison. Human preference evaluation templates. We visualize the evaluation template used for human evaluation. As illustrated in Figures S.7 and S.8, we conduct preference-based human evaluation along two considerations: context groundedness and new image description quality. For evaluation, we present representative multi-concept samples with interleaved imagetext contexts, where two imagetext pairs are provided to assess how well the VLM integrates prior context while accurately describing new query image. Used prompt templates. We visualize the prompt templates used throughout our experiments. Specifically, we present (in order): (i) the prompt used for quality filtering with Gemini-2.5-Flash; (ii) the fixed prompt template employed for image 21 Contextualized Visual Personalization in Vision-Language Models Figure S.6. Image-only visualization of an example from our proposed personalized image captioning benchmark for the case of the three-concept setting. Note that the number of negative samples is three times the number of positive samples. Figure S.7. Template of the survey form for human evaluation. captioning during training and inference; (iii) example prompts for dialogue generation and MCQA construction; (iv) the prompt template designed to induce the proposed F1 scorebased set-level recognition VR; and (v) the caption evaluation template used to answer MCQs based solely on the generated captions. Crucially, for MCQA pair generation, we design these questions to be unanswerable from the query image alone, ensuring that successful performance necessitates adequate contextualization. 22 Contextualized Visual Personalization in Vision-Language Models Figure S.8. Snapshot of the user interface used for collecting human evaluation results. Table S.6. Prompt template visualization used for quality filtering in benchmark construction. Evaluation Prompt (Yes/No): You are an evaluation expert. The given images are presented in order: the first image is the query, followed by the concept images. Your task is to determine whether all of the concept images are present in the first query image. The query image is the one generated by the given prompt: {prompt}. [Not preferred if any of the following occurs] 1. Any concept image is occluded or not fully visible in the query image. 2. Any concept does not appear in the query image at all. 3. Any object or person in the query image is significantly different from the corresponding concept image. [Answering rule] Output \"yes\" only when every concept appears in the query image. Carefully examine the images and output the final result only as \"yes\" or \"no\". 23 Contextualized Visual Personalization in Vision-Language Models Table S.7. Showcase of user prompt used for personalized image captioning. Captioning Prompt: You are an AI model that can perceive multiple past dialogues and use them as memory to personalize your description of new image. [Context] You are given several past dialogues. Each dialogue contains an image and corresponding conversation between user and you. These conversations describe specific objects (people, animals, items, or places) along with contextual details such as names, locations, times, and experiences. This entire context represents your prior shared experiences with the user. [Task] Now, you are given new image that may include one or more of the same objects mentioned in the previous dialogues. Your goal is to describe this new image by integrating relevant information from the context. Follow these rules carefully: 1. Recall and reuse details from the previous dialogues (object names, appearances, places, times, and relationships). Treat the previous dialogues as long-term memory. If an object in the new image appears similar to one mentioned in the past, refer to it using the same name and contextual background. 2. Ground your description in the new images visual content. Accurately describe what you see: composition, setting, lighting, and object state. Then integrate remembered details naturally (e.g., This looks like Pino again, perhaps older than in the park photo from Busan Station.). 3. Keep your tone natural and human-like, as if describing something familiar to the same user. 4. Do not restate previous dialogues verbatim; instead, synthesize and extend them with new imagegrounded observations. 5. Write in paragraph form, not in dialogue format. 6. Use only relevant memories. If an object or scene from the previous dialogues does not appear in the new image, ignore it completely. Include contextual information only for objects that actually appear. Avoid unrelated names, locations, or events. 24 Contextualized Visual Personalization in Vision-Language Models Table S.8. Prompt visualization used to generate multi-turn dialogue. Dialogue Generation Prompt: You are an AI model that can both perceive images and converse naturally with human user. [Goal] Generate short 6-turn dialogue between fictional user and the model based on the given image. The conversation should revolve around the main object in the image (person, animal, item, or place). [Given] The name of the main object is: {name} [Guidelines] 1. The main objects name ({name}) must be used consistently throughout the dialogue. Do not invent or alter the name. 2. The user should describe personal experience related to {name}. The experience must include at least one objective contextual element, such as specific place, time, event, or situation (e.g., last summer at the riverside, during my first year in college, in my grandmothers backyard). 3. The model should respond naturally and empathetically acknowledging, asking gentle questions, or adding brief reflections. 4. Keep the tone human-like, calm, and realistic not overly emotional or robotic. 5. The conversation should have 6 turns total (User Model User Model User Model). 6. Avoid encyclopedic or factual world knowledge. Focus on the personal connection and shared observation of the object. [Output Format] Dialogue: User: ... Model: ... User: ... Model: ... User: ... Model: ... 25 Contextualized Visual Personalization in Vision-Language Models Table S.9. Visualization of prompt used to generate MCQA pairs from the dialogue. MCQA Generation Prompt (JSON-only, 3 QA pairs): You are an AI model that creates factual multiple-choice questions and answers. [Input] You are given conversation between user and an AI model about specific object (person, animal, item, or place). The conversation contains objective details such as the objects name, location, time, or the users related experiences. [Goal] Generate 3 multiple-choice QA pairs that could later be answered by someone who only has access to caption describing new image of the same object (the original conversation will NOT be shown at evaluation time). [Guidelines] 1. Each question must target an objective detail present in the conversation (e.g., name, place, time, habit/action). 2. Avoid emotions, opinions, or meta-dialogue. 3. Each question must have exactly 3 options: A, B, C. 4. Exactly one option is correct among A, B, C. 5. Make the wrong options (A/B/C except the correct one) plausible but clearly incorrect. 6. Do NOT require external/world knowledge; answers must come from the conversation content. 7. Output must be valid JSON only: no additional text and no trailing commas. [JSON Output Schema] { \"qa\": [ { \"id\": \"Q1\", \"question\": \"<string>\", \"options\": { \"A\": \"<string>\", \"B\": \"<string>\", \"C\": \"<string>\" }, \"correct_answer\": \"A\" \"B\" \"C\" }, { \"id\": \"Q2\", \"question\": \"<string>\", \"options\": { ... }, \"correct_answer\": \"A\" \"B\" \"C\" }, { \"id\": \"Q3\", \"question\": \"<string>\", \"options\": { ... }, \"correct_answer\": \"A\" \"B\" \"C\" } ] } Contextualized Visual Personalization in Vision-Language Models Table S.10. Used prompt to extract an answer set from the given concept images. Set-based vision VR prompt: You are given multiple images. Each image corresponds, in order, to specific visual concept. [Task] You are given final query image. Your goal is to identify which of the concept images are present in the query image. [Constraint] The query image may contain up to four concepts randomly selected from the given concept images. Some concept images are irrelevant and do not appear in the query image. Do not include irrelevant concept images in your answer. [Answering Rules] Carefully observe all concept images and the final query image. Determine which concept images appear in the query image. You may briefly explain your reasoning in natural language. Then, on separate line, output your final answer in the exact format: Answer: boxed{[i1, i2, . . . ]} where the list contains the indices of the selected concept images. Inside boxed{}, include only the indices written as list in square brackets, with no extra text. 27 Contextualized Visual Personalization in Vision-Language Models Table S.11. Visualization of prompt used to extract the answer for an MCQ via judge LLM. Evaluation prompt to answer an MCQ: You are given description about an object. This description may or may not contain enough information to answer multiple-choice question. You must answer the question using only the information in the description. [Constraints] Do NOT use any external knowledge. Do NOT assume facts that are not clearly supported by the description. [Answering Rules] 1. Read the description carefully. 2. For each question, choose the single best option: If one of A/B/C is explicitly or clearly supported by the description, choose that option. If none of A/B/C can be confirmed from the description, choose D. 3. You must ignore any information that is not in the description. 4. For each question: You may briefly explain your reasoning in natural language. Then, on separate line, output the final choice in the exact format: [Required output format] Answer: boxed{X} where is one of A, B, C, or D. Inside boxed{} there must be exactly one letter, with no extra text. [Given] [Description] {Generated caption} [Question] {Pre-defined MCQ} 28 Contextualized Visual Personalization in Vision-Language Models Table S.12. Prompt visualization used to generate multi-turn dialogue for diagnostic downstream tasks. Dialogue Generation Prompt in Diagnostic Tasks (LSD, LAR, ITR): You are an AI model that can both perceive images and converse naturally with human user. [Goal] Generate short 6-turn dialogue between fictional user and the model based on the given image. The conversation should revolve around the main person in the image and describe specific past encounter that can be stored as personal memory. [Given] The name of the person in the image: {name} The date when the user saw {name}: {seen date} The place where the user saw {name}: {seen place} [Guidelines] 1. The persons name ({name}) must be used consistently throughout the dialogue. Do not invent, alter, or omit the name. 2. The user must describe personal experience related to {name}. The experience must include at least one concrete event or situation (e.g., bumping into them, having short conversation, noticing what they were doing). It should include at least one sensory or situational detail that makes the memory feel realistic. 3. The user must explicitly mention both the date and the place: Date: {seen date} Place: {seen place} Preferably within single user turn (e.g., saw them on {seen date} at {seen place}...). 4. The model should respond naturally and empathetically, acknowledging the users experience or asking gentle follow-up questions. Do not introduce new factual information beyond what the user provides. 5. Keep the tone calm, realistic, and human-like. Avoid encyclopedic or factual descriptions. 6. The conversation must have exactly 6 turns in total: User Model User Model User Model. [Output Format] Dialogue: User: ... Model: ... User: ... Model: ... User: ... Model: ... Contextualized Visual Personalization in Vision-Language Models Table S.13. Prompt visualization used for diagnostic downstream tasks. Personalized Image Understanding Prompt: You are an AI model that can perceive multiple past dialogues and use them as memory to personalize your understanding of new image. [Context] You have been given several past dialogues. Each dialogue contains an image and corresponding conversation between user and you. These conversations describe specific objects (people, animals, items, or places) along with contextual details such as names, locations, times, and experiences. This entire context represents your prior shared experiences with the user. [Task] Now, you are given new image that may include one or more of the same objects mentioned in the previous dialogues. Your goal is to interpret this new image by integrating relevant information from the context. Follow these rules carefully: 1. Recall and reuse details from the previous dialogues (object names, appearances, places, times, and relationships). Treat the previous dialogues as your long-term memory. If an object in the new image appears similar to one mentioned in the past, refer to it with the same name and contextual background. 2. Ground your understanding in the new images visual content. First describe what you see: composition, setting, lighting, actions, and object state. Then integrate relevant remembered details naturally (e.g., This looks like Pino again, now indoors instead of the park near Busan Station.). 3. Keep your tone natural and human-like as if you are interpreting something familiar to the same user. 4. Do not restate previous dialogues verbatim. Instead, synthesize memory with the current image content. 5. Write in paragraph form, not in dialogue format. 6. Use only relevant memories. If an object or context from past dialogues does not appear in the new image, ignore it completely. Add contextual information only when it helps understanding of what is visible. Avoid mentioning unrelated names, locations, or experiences. 30 Contextualized Visual Personalization in Vision-Language Models Table S.14. Candidates of user message about current actions used to generate LAR samples. Oh wait, think left my wallet at the Guess store, so Im going back to check. Oh wait, realized left my ID badge on my desk, so need to run back to the office. Oh wait, need to drive by the taco place to pick up dinner for everyone. Oh wait, my fuel light just came on, so Im stopping at the gas station next. Oh wait, need to drop off my suit at the dry cleaners before they close. Oh wait, its almost 3 PM, so Im leaving now to pick up the kids from school. Oh wait, need to go next door to ask the neighbor if they can water my plants while Im away. Oh wait, my phone is about to die, so Im going to my room to plug it in. Oh wait, have Zoom meeting starting in 5 minutes, so Im heading to my study. Oh wait, need to go to the post office to return this package. 31 Contextualized Visual Personalization in Vision-Language Models Table S.15. Prompt visualization used for llm-as-a-judge in LAR evaluation. Evaluation Prompt: You are an impartial judge evaluating whether models response correctly matches ground-truth reference. You will be given: 1. question asked to the model 2. ground-truth reference answer (GT) 3. generated response from the model Your task is to decide whether the generated response is Correct or Wrong. Evaluation Criteria: The generated response is Correct if it semantically includes the core information conveyed by the ground-truth reference. The wording does NOT need to match exactly. Paraphrases, rephrasings, or additional details are allowed. The generated response may contain extra information beyond the ground-truth reference. This is acceptable. The generated response is Wrong if it: Fails to include the core meaning of the ground-truth reference, OR Contradicts the ground-truth reference, OR Provides an unrelated or vague answer that does not clearly convey the same experience or action. Focus ONLY on whether the essential meaning of the ground-truth reference is present in the generated response. Do NOT judge based on style, fluency, length, or factual completeness beyond the ground-truth reference. Output: Respond with exactly one word: Correct or Wrong. Question: What was doing the last time told you about my most recent experience with the one in the new image? Ground-truth Reference: {ground truth} Generated Response: {response}"
        }
    ],
    "affiliations": [
        "Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea",
        "Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea",
        "Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, Korea",
        "Samsung Electronics, South Korea"
    ]
}