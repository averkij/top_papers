{
    "paper_title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
    "authors": [
        "Jiawei Chen",
        "Xintian Shen",
        "Lihao Zheng",
        "Zhenwei Shao",
        "Hongyuan Zhang",
        "Pengfei Yu",
        "Xudong Rao",
        "Ning Mao",
        "Xiaobo Liu",
        "Lian Wen",
        "Chaoqun Du",
        "Feng Gu",
        "Wei He",
        "Qizhen Li",
        "Shanshan Li",
        "Zide Liu",
        "Jing Luo",
        "Lifu Mu",
        "Xuhao Pan",
        "Chang Ren",
        "Haoyi Sun",
        "Qian Wang",
        "Wei Wang",
        "Hongfu Yang",
        "Jiqing Zhan",
        "Chunpeng Zhou",
        "Zheng Zhou",
        "Hao Ma",
        "Tao Wei",
        "Pan Zhou",
        "Wei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 2 1 4 3 2 . 2 1 5 2 : r Li Auto Base Model MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning MindGPT-ov Team Li Auto Inc Traditional workflow-based agents exhibit limited intelligence when addressing realworld problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design more eﬀicient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL. The agent reasoning framework, MWE-Bench, three smaller-scale agent models (2B, 3B, and 4B) distilled from MindWatcher 32B, and related resources will be open-sourced. (cid:195) Last Update Date: December 29, 2025 (cid:263) Correspondence: chenwei10@lixiang.com (cid:97) Code: https://github.com/TIMMY-CHAN/MindWatcher Hugging Face: https://huggingface.co/datasets/Lost-Cloud/MWE-Bench"
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) [27, 1, 14, 42, 9, 37, 25] have achieved remarkable progress in recent years, demonstrating strong capabilities in language understanding, knowledge acquisition, and complex reasoning tasks. However, despite the powerful world knowledge and Preprint. Figure 1: MWE-Bench Performance of MindWatcher. multimodal capabilities of the latest models such as Gemini 2.5 Pro [9], most LLMs remain fundamentally constrained by the limits of their parametric knowledge: they struggle to cover longtail information and finegrained domainspecific knowledge [7], and they cannot directly access realtime information that emerges after training. These structural bottlenecks hinder their reliability in many realworld applications, especially those requiring external knowledge, multistep information integration, or crossmodal reasoning. Equipping LLMs with external tools has therefore become key strategy to overcome these limitations. By connecting models with retrieval engines [19, 29], computation tools, or code interpreters, the boundary of problemsolving capabilities can be substantially extended. Traditional toolaugmented approaches typically rely on manually designed workflows [17, 33] to orchestrate tool invocation, yet such methods exhibit limited adaptability when confronted with the diversity and uncertainty inherent in open-domain environments, which become even more fragile when handling crossmodal demands. Multiagent systems [41, 31, 22, 21] partially alleviate these issues: powerful planner agent is responsible for decisionmaking, while toolspecialized agents execute designated subtasks. This architecture has become highly popular in the industry and significantly improves system flexibility and scalability. However, it also introduces new complexity and overhead, including redundant model deployment and latency caused by chained interactions, which limits its further expansion. With the emergence of thoughtaugmented models [38, 14], the research community increasingly recognizes that intelligent systems need not rely on multicomponent designs: single unified language model can assume both planning and acting roles. This has led to the rise of ToolIntegrated Reasoning (TIR) methods [26], exemplified by the ReAct [43] paradigm. The core idea is to let the model explicitly generate intermediate thoughts, autonomously invoke tools, and iteratively make decisions based on environmental feedback. TIR agents can dynamically plan multistep operations in openworld tasks and achieve endtoend problem solving, making them promising path toward more generalpurpose agents. However, current TIR systems still fall short of being truly practical and general intelligent agents, with significant limitations across several key dimensions. From an application perspective, existing TIR agents [26, 22, 11] are predominantly focused on textbased tasks, particularly DeepSearchstyle reasoning centered on retrieval. Only small number of works [13, 44] attempt to introduce visual capabilities, and most rely solely on image search tools without enabling the agent to directly manipulate images or perform finegrained crossmodal reasoning to support problem solving. This severely limits their performance on multimodal tasks and prevents them from tackling the many visiondriven decisionmaking scenarios found in realworld environments. 2 From training methodology perspective, TIR agents face triple challenge across data, algorithms, and training frameworks. Highquality reasoning trajectories involving multiple tools and multistep interactions are extremely diﬀicult to construct manually. SFTbased training [24, 6] often causes models to imitatethe thoughtaction format rather than truly learnthe underlying strategymanifested in excessive, redundant tool calls on simple problems and substantial performance degradation on general benchmarks. Moreover, existing training frameworks lack finegrained supervision over the interleaved process of thinking, tool invocation, and subsequent reasoning, preventing models from forming stable and reliable tooluse behaviors and exacerbating issues such as tool misuse and unnecessary calls. From the perspective of tool ecosystems, many core retrieval capabilities, especially visual retrieval, rely on expensive external APIs. Their high cost under frequent invocation further constrains the practical deployment of TIR agents in local or enterprise settings. To address the challenges outlined above, we introduce MindWatcher, TIR agent capable of autonomous planning and execution, multimodal perception, and robust tool coordination. Leveraging an interleaved thinking paradigm and multimodal ChainofThought (CoT) mechanism, MindWatcher can flexibly alternate between internal thinking and external tool invocation at any stage of the reasoning process. By integrating finegrained visual operations into the reasoning chain, the agent achieves precise regionlevel visual perception and more accurate crossmodal information retrieval. To avoid the drawbacks of conventional SFT, such as rigid imitation of reasoning formats and redundant tool calls on simple tasks, MindWatcher abandons standard SFT and instead adopts continuous reinforcement learning (RL) strategy conducted in both real and offline environments. We develop two automated imagetext pair construction pipelines to reduce data generation costs. In parallel, we equip MindWatcher with comprehensive set of tools that cover core multimodal reasoning needs, including image region cropping and zooming, object grounding and visual search, external text retrieval, webpage content extraction, and local Python code interpreter. Moreover, we construct largescale local visual corpus spanning categories such as person, animals, plants, cars, landmarks, and logos. We also build new multimodal benchmark: MindWatcher-Evaluate Benchmark (MWE-Bench) for systematically evaluating agentic multimodal tooluse and reasoning capabilities. At the system level, we design an RL training pipeline that supports asynchronous tool invocation, significantly improving learning eﬀiciency. We also introduce new GRPO-based agentic RL algorithm, which introduce step-wise normalization which ensure the optimization objective on individual action segments rather than the global token stream. As shown in Figure 1, MindWatcher demonstrates strong generality and eﬀiciency across wide range of tasks on MWE-Bench. The 32B model achieves stateoftheart (SOTA) performance in toolaugmented reasoning while maintaining robust general capabilities, and we distilled and open-sourced 2B, 3B, and 4B variants based on the MindWatcher, which also exhibit highly competitive results."
        },
        {
            "title": "2 Method",
            "content": "2.1 Working Paradigm To support flexible multimodal reasoning and autonomous tool use, MindWatcher models the TIR process as Markov Decision Process (MDP). Given an initial user prompt s0, the agent interacts with the environment by generating an interleaved sequence of actions and toolgrounded observations: = {a0, obs0, a1, obs1, . . . , obsn1, an}. (1) Each action aj is executed against the environmenttypically through thinking process and tool callyielding an observation obsj, which is appended to the context and becomes part of the next state. The agent iteratively continues this process until generating the final action an, which contains the concluding response. Interleaved Thinking and Multimodal CoT MindWatcher implements this MDP through an autoregressive generation loop. At each step t, the Policy πθ(atst) (param3 Figure 2: The Working Paradigm of MindWatcher. To address complex multimodal question answering tasks, we train our model using continuous RL to develop Multimodal CoT capabilities. By integrating interleaved thinking, the model is able to interact with the environment and autonomously invoke tools in the toolbox. Furthermore, to facilitate more accurate and lowcost visual search, MindWatcher have constructed large-scale local retrieval corpus spanning eight major categories. eterized by the LLM) conditions on the full history st. Distinct from traditional approaches where actions are strictly physical tool calls, we define unified action space = Athought Atool. In implementation, thoughts and tool calls are serialized through dedicated <think><think> and <tool_call><tool_call> tags, enabling the model to interleave reasoning and action generation within single decoding sequence. MindWatcher further incorporates multimodal CoT [38, 16] mechanism, which allows the agent tothink with imagesby embedding imagedependent operations into its reasoning chain. As shown in Figure 2, given an image and complex query, the model enters an iterative planning and tool-call process. After each tool-call completes, the tool response of current stage is obtained. Subsequently, the next action is determined based on the tool response, ultimately yielding the query result. 2.2 Training Algorithm While SFT remains the prevailing paradigm for training TIR agents, our empirical observations reveal significant limitations. We found that fine-tuning already robust instructionfollowing or thinking models on trajectory data often incurs heavy alignment tax, severely degrading performance on general-purpose tasks. Furthermore, SFT tends to induce tool abuse, manifested as redundant invocations for trivial queries and excessive, ineffective looping in complex scenarios. Consequently, we adopt pure RL approach to endow MindWatcher with genuine decision-making and self-correction capabilities. 2.2.1 Step-wise Normalized GRPO We employ an enhanced version of Group Relative Policy Optimization (GRPO) [30] as our core learning algorithm. Standard GRPO typically normalizes advantages over single dialogue turn or global sequence. However, in an agentic environment, observation tokens generated by the environment must be excluded from loss calculation. Let Oq = {o1, o2, . . . , oG} be group of trajectories generated from user prompt q. For each trajectory oi, we compute sequence-level reward ri. The advantage function is computed based on the distribution of rewards within the group: , (2) ˆAi = ri µr σr 4 where µr and σr are the mean and standard deviation of the rewards for the samples, respectively. In standard multi-turn agent setting, the expected objective function is typically formulated as summing over all action tokens: [ ( ) ] J(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G i=1 1 j=0 Tj +aj aj j=0 t=Tj min πθ(tst) πθold (tst) Ai,t, clip πθ(tst) πθold (tst) , 1 ϵ, 1 + ϵ Ai,t . (3) However, in the context of Interleaved Thinking, single trajectory comprises multiple Think and Tool-call cycles (episodes) with drastically varying action lengths. Simply summing gradients allows episodes to dominate optimization. To ensure balanced supervision across every reasoning step, we propose Step-wise Normalization. We define the optimization objective on individual Action Segments aj rather than the global token stream. Assuming the i-th trajectory contains ni action steps, and the j-th action segment aj has length of aj, our optimized objective function J(θ) is formalized as: J(θ) = 1 i=1 1 ni ni j=1 1 aj taj [ min πθ(tst) πθold (tst) ˆAi, clip ( πθ(tst) πθold (tst) ) ] , 1 ϵ, 1 + ϵ ˆAi . (4) This formulation introduces dual-normalization mechanism: 1. Action-Step Normalization ( 1 ni the number of Think and Tool-call cycles. ): Weighs each trajectory equally regardless of 2. Token-Length Normalization ( 1 aj ): Averages loss within each Think and Toolcall episode. 2.2.2 Hybrid Reward To steer the model toward both syntactic correctness and factual accuracy, we design hybrid reward function consisting of three components: Outcome Accuracy Reward, Format Reward, and Hallucination Tool-call Penalty. 1. Outcome Accuracy Reward (Racc): This is sparse reward computed only at termination. Given the complexity of open-ended multimodal QA, regular expressions are insuﬀicient for verification. We employ Model-based Judge to evaluate the factual consistency between the model output and the ground truth. { Racc = 1.0 0.0 if Judge returns 1 (Correct), if Judge returns 0 (Incorrect). (5) 2. Format Reward (Rf mt): We implement strict regex-based parser to enforce schema adherence. This includes: Structural Integrity: Verifying that tags such as <think>, <tool_call>, and <answer> appear in valid pairs and sequences. Residue Penalty: We strictly forbid chitchat outside of valid tags (e.g., outputting will now execute... after <tool_call> block). Any non-whitespace character outside tags incurs penalty, as we observed that such residues often lead to output collapse during training. { Rf mt = 0.5 0.5 0.01 len(residue) if strictly follows schema, if format error or residue detected. (6) 3. Hallucination Tool-call Penalty (Rhalluc): During experiments, we observed tendency for models to generate consecutive <tool_call> blocks without waiting for the environment feedback <tool_response>, effectively hallucinating execution results. To suppress this, we penalize the discrepancy between the number of model calls (Ncall) and actual environmental responses (Nresp): Rhalluc = min(0, (Nresp Ncall) 0.2). (7) This mechanism enforces strict Turn-taking protocol, ensuring that only tool calls actually processed by the environment are considered valid behaviors. The final reward is calculated as: In this paper, we set λf mt = 0.1 and λhalluc = 0.05. Rtotal = Racc + λf mt Rf mt + λhalluc Rhalluc. (8)"
        },
        {
            "title": "2.3.1 Tool Functions",
            "content": "In this section, we present the comprehensive multimodal toolkit within MindWatcher, comprising the following five tools: Region Cropping/Zooming: This tool encompasses diverse image processing operations designed to externalize visual reasoning and highlight critical regions to guide attention. It includes an image grounding tool for localizing and cropping target areas based on input boxes, thereby facilitating the thinking with images reasoning paradigm. Object Grounding and Visual Search: This tool accepts image interest regions and search categories, subsequently retrieving the corresponding knowledge from large-scale local image retrieval database (described in Sec 2.3.2). By adaptively localizing query-relevant regions, our tool performs precise and targeted regional searches, effectively addressing complex visual search challenges. External Text Retrieval: This tool leverages the search engines for information retrieval. It accepts textual queries as input and returns the top-10 ranked results, each comprising title and an abstract. Webpage Content Extraction: Taking URL as input, this tool employs Jina [2] to retrieve the webpage content. The agent can read its full content, the content within the window it provides, or use an AI assistant to generate structured summary based on the specific goal it provides. Local Code Interpreter: This tool executes Python code within sandbox environment isolated from external resources (e.g., files and the internet). It returns the execution results and supports the invocation of various Python libraries for diverse data computation tasks. 2.3.2 Local Multi-modal Retrieval Library Conventional image search methods leverage massive Internet data. However, directly acquired Internet resources in fine-grained specialized domains contain erroneous knowledge. Additionally, the high cost of external visual search API calls can significantly increase training expenses during large-scale training. To alleviate the above issues, we construct large-scale, high-quality image retrieval database. Based on the general taxonomy of world knowledge, we built our local search database through the following procedure: (1) We established knowledge entries that span eight major categories: Person, Car, Plant, Animal, Logo, Landmark, Fruit & Vegetable, and Dish. (2) Collect images corresponding to these knowledge entries from both Internet sources and professional museum databases. (3) We employ domain experts to conduct large-scale comparative filtering and knowledge categorization. Through rigorous identification and curation by domain experts, we ensure that the precision of our visual search image database exceeds 99%. Ultimately, our constructed specialized image retrieval database, MindWatcher Multi-modal Retrieval Database (MWRD), encompasses eight major categories of knowledge images and associated information, covering total of 50k retrieval entities. Each retrieval unit contains 3-10 high-quality images, amounting to over 300k images. To accommodate temporally dynamic data, we perform regular maintenance and updates on this image retrieval database."
        },
        {
            "title": "3 Training Data and MWE-Bench",
            "content": "The RL training data of MindWatcher includes both online and offline environment training data. The online environment refers to real interactions with internet environments. In this paper, the online training data comprises three distinct sources: two types of data constructed based on automated pipelines, and data collected from open-source datasets."
        },
        {
            "title": "3.1 Training Data Constructed from Private Images",
            "content": "To enable MindWatcher agents to master multimodal tools proficiently, we constructed cross-modal question-answering (QA) dataset with progressively increasing diﬀiculty. Unlike purely textual tasks, this task requires agents to jointly invoke visual perception and external search tools to solve problems. To ensure data robustness and training eﬀiciency, we designed Multimodal Knowledge-Augmented Pipeline comprising three core stages: source knowledge anchoring and generation, rigorous QA quality validation, and diﬀiculty grading based on tool invocation."
        },
        {
            "title": "3.1.1 Phase 1: Source Knowledge Annotation and Initial Generation",
            "content": "We first utilize high-quality private multimodal database as seed data to construct foundational multimodal dataset. To achieve deep alignment between visual signals and textual knowledge, we designed generation mechanism comprising the following steps: Fine-Grained Visual-Knowledge Mapping: We developed an integrated data processing pipeline combining object localizationand fine-grained retrieval.This automated pipeline extracts bounding boxes and corresponding retrieval labels from source images, establishing precise image-text mappings. Knowledge Graph Augmentation: Based on extracted visual labels, we utilize web search to construct dynamic knowledge graphs, acquiring relevant background knowledge and factual information. This external knowledge is then leveraged to generate initial question-answer pairs, ensuring questions rely not only on images but also integrate external world knowledge. 3.1.2 Phase 2: Timeliness and Uniqueness Verification The accuracy of reward signals is critical in reinforcement learning training. We found that directly generated QA data often faces two major challenges, which may lead to misjudgments in reward models: 1. Temporal Stability: Search engine environments are dynamically changing. If time gap exists between data production and actual training, updates to search results may cause answer drift. 2. Answer Uniqueness and Non-openness: Open-ended questions often have nonunique solutions. Even if an agent executes the correct search path, its generated answer may contain only partial correct information or be overly broad, making it diﬀicult for the reward model to evaluate. In response to these limitations, we implemented two-stage human-in-the-loop verification pipeline. This rigorous review ensures the final high-quality multimodal dataset maintains temporal consistency, with each question possessing unique, unambiguous ground truth. 3.1.3 Phase 3: Diﬀiculty Grading Based on Tool Invocation Curriculum learning is an effective strategy for training agents, hinging on reasonable diﬀiculty stratification. However, traditional diﬀiculty assessments based on human subjective perception are often biased. In tool-integrated scenarios, search engines can instantly resolve memory-based problems deemed diﬀicultby humans, creating disconnect between perceived diﬀiculty and the actual challenges faced by agents. To achieve more precise diﬀiculty screening, we designed Tool-Invocation Screening Engine. This engine abandons subjective judgments, instead 7 defining sample diﬀiculty through quantitative analysis of the number of tool invocation roundsrequired to solve problems and the complexity of multi-tool combinations.This approach constructs training data that truly aligns with the agent learning curve."
        },
        {
            "title": "3.2 Training Data Constructed from Open-sourced News",
            "content": "Constructing reliable reward signal for RL in open-ended web interactions is notoriously diﬀicult due to the noisy nature of internet content. General web corpora often abound with subjective commentary, unverifiable rumors, and ambiguous clickbait titles, which can lead the reward model to provide incorrect optimization signals. Furthermore, factual information in niche domains is often buried in low-traﬀic sub-pages that are diﬀicult for generic search engines to index instantly, causing agents to fail even when their reasoning path is correct. To mitigate these challenges, we selected Sports News as the seed domain for our automated pipeline. Sports data possesses unique characteristics ideal for training TIR agents: 1. Objective Verifiability: Unlike social news, sports events have definitive outcomes (scores, winners, rankings) that constitute unique ground truth. 2. Resistance to Ambiguity: Statistical facts in sports are less susceptible to the semantic pollution of opinions or fake news. 3. Multimodal Richness: Match reports are intrinsically multimodal, requiring the alignment of textual statistics with visual evidence (player jerseys, scoreboards, action shots). We developed robust Temporal-Aware Multimodal QA Pipeline to harvest and process this data, consisting of three sequential stages: Ingestion, Semantic Auditing, and ConstraintAware Generation. 3.2.1 Domain-Specific Ingestion and Filtering We deployed focused crawler targeting authoritative sports portals to ensure information reliability. The raw stream captures article metadata, textual bodies, and associated image sets. preliminary heuristic filter is applied to discard low-quality samples, retaining only articles with non-empty bodies and at least one relevant image. This creates raw repository of event-centric multimedia documents. 3.2.2 Phase 1: LLM-Based Semantic Auditing Quality control is paramount for RL training. We introduce Data Auditor agent (powered by strong LLM) to perform feasibility check before generation. The auditor evaluates raw news based on strict Factuality Protocol: Retention Criteria: The content must describe completed event with clear timeline (e.g., match results, completed transactions). The text must provide key information (entities, actions) visually corresponding to the images. Rejection Criteria: Purely subjective content, such as rumors, predictions of future games, gossip, or vague summaries without verifiable details, is discarded. This phase filters out approximately 40% of the raw feed, ensuring that the downstream generation model operates only on solid factual ground. 3.2.3 Phase 2: Constraint-Aware QA Generation The surviving samples are processed by Question Generator agent. To prevent the model from learning shortcuts or hallucinating, we designed Constraint-Aware Prompting Strategy that enforces strict rules on the generated QA pairs: 1. Temporal Anchoring: critical challenge in time-sensitive QA is Data Rota question like Who won the game yesterday? becomes invalid over time. Our pipeline forces the generator to explicitly resolve relative time expressions into absolute timestamps 8 (e.g., converting this season to the 2025 season) based on the publish time of the article. This ensures the question remains valid and unique indefinitely. 2. Visual-Textual Dependency: Questions are engineered to require information integration from both modalities. For instance, instead of explicitly naming player, the question might refer to the player in the No. 8 jersey on the right, compelling the agent to first identify the visual entity and then search for its identity using external knowledge. 3. De-referencing Context: To simulate real-world user queries, we strictly prohibit meta-references such as According to the article. The agent receives only the standalone question and the image, forcing it to use search tools to retrieve the knowledge originally contained in the source article (which is hidden from the agent during training)."
        },
        {
            "title": "3.3 Open-sourced Training Data and Offline Training Data",
            "content": "Supplementing the autonomously constructed online training data detailed previously, we curated focused collection of open-source datasets. These are strategically utilized to bolster the proficiency of MindWatcher in text-only search tasks and code-augmented mathematical reasoning. Furthermore, distinct from the three aforementioned data categories designed for real-world environment training (manual, online-automatic, and open-source), MindWatcher incorporates specialized offline training method for TIR. To facilitate this, we developed an automated pipeline to construct substantial corpus of high-quality, multimodal QA pairs with stratified diﬀiculty levels. 3.4 MWE-Benchmark The MWE-Bench covers six primary categories: Car, Animal, Plant, Person, Landmark, and Sports. While these categories align with those in our automated data construction pipeline, we deliberately adopted distinct construction methodology for the benchmark to ensure its integrity and prevent performance inflation caused by data-domain overlap. Specifically, for data derived from private images, we utilized knowledge entries from our internal database that were strictly excluded from the training set. The construction process followed multi-stage pipeline: we first expanded our knowledge base by collecting auxiliary web-based information to enrich the context. For each category, we then applied category-specific constraints and employed closed-source models to perform uniqueness deconstructionextracting core factual statements that uniquely identify an entity. These statements formed the basis for constructing initial single-turn QA pairs, which were subsequently synthesized into more complex and challenging multi-step reasoning tasks. Finally, all generated samples underwent two-tier verification process involving both automated model-based filtering and manual expert review to ensure quality and temporal accuracy. For sports category data, based on the data construction method outlined in Section 3.2, we merged text and image corpora belonging to the same entity or event across news data, which are from entirely non-overlapping time points. We then employed powerful LLM to extract atomic facts from all corpora. Subsequently, we constructed QA pairs with complex queries based on these atomic facts. Finally, data cleaning and filtering were performed following process similar to that described in Section 3.2.3. Following the aforementioned methodology, we have successfully constructed MWE-Bench. The dataset encompasses six categories: 373 car-related instances, 351 animal-related instances, 397 plant-related instances, 63 person-related instances, 90 landmark-related instances, and 142 sports-related instances."
        },
        {
            "title": "4.1 Application Details",
            "content": "The training data utilized in this study are segmented across online and offline environments. In the online training environment, we collected VQA data consisting of 1,639 samples based on private images and 2,949 samples derived from public news sources. The open-source domain data, primarily extracted from established benchmarks such as WebSailor [20], ToolStar [11], and SimpleDeepSearcher [35], totaled 5,000 samples. Furthermore, we leveraged approximately 20,000 samples within the offline RL training environment. The RL process employed curriculum learning strategy guided by data diﬀiculty. Training was conducted on the Qwen2.5-VL-32B [4] model for one epoch. Our training framework features synchronized rollout mechanism coupled with an asynchronous tool invocation logic (Details shown in the Appendix A.2). Specifically, within each step of the interleaved CoT trajectory, the presence of the <im_end> token triggers an immediate check for <tool_call> token. If <tool_call> is detected, it is dispatched instantly. Reward computation also utilizes an asynchronous model invocation method. We used the fully trained MindWatcher-32B model to distill its multimodal reasoning and tool-use capabilities into smaller, cost-effective models. This process involved collecting an initial, diverse set of base datasets, including the VLAA SFT dataset [5] (126K samples), the text-only WebWalker silver dataset [40] (15K samples), and self-built multimodal RAG QA dataset (30K samples). The MindWatcher-32B teacher model was then employed to roll out and generate 13 corresponding TIR trajectories for each sample. After straightforward filtering process, the final distillation dataset comprised 124K samples, split into 100K multimodal and 24K pure text samples. By using Qwen3-VL-2B [3], Qwen2.5-VL3B, and Qwen3-VL-4B as base models and training them for one epoch on the distillation dataset, we successfully produced the smaller distilled MindWatcher-2B, MindWatcher-3B, and MindWatcher-4B models, respectively. To comprehensively validate the performance of MindWatcher, in addition to the MWEBench, we conducted comparative testing against the model performance on several other open-source benchmarks, including MMSearch (subset) [15], SimpleVQA (subset) [8], and WabWalkerQA [40]. All tests were conducted with sampling temperature of 0.7 and top-p setting of 0.95. The primary evaluation metric utilized was pass@1, with correctness assessed by employing the LLM-as-Judges methodology. 4.2 Main Results Tabel 1 presents the detailed performance of different backbones on the MWE-Bench under both direct inference and React/Agent inference modes. Disparity in Parametric Knowledge. Under the Direct Inference mode, we observe that the freshness of model knowledge cutoff does not linearly correlate with its benchmark performance. Despite being the most recent release, the Qwen3-VL series achieves an average score of only 22.60. In contrast, Gemini 2.5 Pronotwithstanding an older knowledge cutoffattains SOTA zero-shot score of 42.09. This discrepancy underscores critical reality: when internal parameters alone are insuﬀicient for handling long-tail or specialized world knowledge, the integration of external reasoning tools is necessary. Tool-Augmented Performance Leap. Transitioning to the ReAct/Agent paradigm catalyzes significant performance surge for models previously limited by their internal knowledge. For instance, the score of Qwen3-VL 32B nearly triples when equipped with tool-use capabilities. Similarly, GPT-5 mini exhibits remarkable explosion in performance within the Sports domain, soaring from 13.38 to 80.28 upon gaining tool access. MindWatcher Dominance. MindWatcher-32B achieves overall SOTA performance on MWE-Bench with global score of 75.35, outperforming prominent closed-source commercial models such as Gemini2.5 Flash and GPT-5 mini. Notably, MindWatcher achieves the highest accuracy across four specific domains: Vehicle, Animal, Plant, and Person. Furthermore, the distilled variants, including MindWatcher-2B, 3B, and 4B, demonstrate per10 Table 1: Results on the MindWatcher-Evaluation Benchmark. Method MindWatcher-Evaluation Benchmark Car Animal Plant Person Landmark Sport GPT-4o mini GPT-4o Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen3-VL 8B Thinking Qwen3-VL 30B-A3B Thinking Qwen3-VL 32B Thinking Openai o4-mini Gemini 2.5 Pro Gemini 2.5 Flash GPT-5 mini 14.48 24.13 7.24 18.5 10.46 17.69 14.21 21.98 40.48 35.12 22.25 Direct inference 26.21 38.46 31.62 33.62 22.79 25.64 28.21 35.61 49.57 45.14 48.15 24.43 29.22 30.48 33.25 23.93 25.69 26.45 31.49 48.11 43.58 42. 9.52 12.7 7.94 9.52 15.87 19.05 20.63 11.11 28.57 22.22 4.76 ReAct/Agent Gemini 2.5 Flash GPT-5 mini Doubao-Seed-1.6-vision Qwen2.5-VL-32B Qwen3-VL 32B Thinking WebWatcher-7B WebWatcher-32B MindWatcher-2B MindWatcher-3B MindWatcher-4B MindWatcher-32B 68.17 61.93 57.64 51.74 58.98 32.71 38.87 50.13 49.87 56.03 71.31 Best results are in bold and the suboptimal results are in underline. 71.76 68.66 57.26 54.13 75.5 45.58 53.85 80.34 80.06 84.62 86.04 57.14 44.44 65.08 50.79 69.84 30.16 52.38 65.08 63.49 68.25 77. 77.26 81.61 61.46 58.19 77.83 49.62 67.51 86.4 85.89 87.66 88.92 25.56 38.89 17.78 33.33 17.78 20 21.11 47.78 46.67 40 44.44 46.51 57.78 38.89 41.11 48.89 36.67 42.22 44.44 44.44 41.11 47.78 4.23 6.34 2.82 8.45 16.90 15.50 21.83 6.34 14.09 17.60 13.38 37.32 80.28 39.39 31.69 46.48 21.57 32.88 16.90 17.61 36.62 46.48 Avg. 19.63 27.75 20.06 25.92 18.65 21.89 22.60 27.61 42.09 37.96 33.97 66.65 69.91 57.91 51.41 66.95 39.66 50.93 64.76 64.48 69.63 75.35 formance comparable to the Qwen3-VL 32B baseline. This empirically demonstrates that robust tool-call capabilities can effectively mitigate the knowledge gaps typically present in small-parameter models. Table 2 presents the comparative performance of MindWatcher-32B against other models in identical environments across two filtered multimodal subsets (MMSearch and SimpleVQA) and one pure-text benchmark (WebWalkerQA). MindWatcher continues to deliver SOTA results on MMSearch among all openand closed-source models evaluated. On the SimpleVQA subset, MindWatcher performance surpasses the next-generation Qwen3Table 2: Results on the Open-sourced Benchmarks. Method MMSearch SimpleVQA WebWalkerQA Chinese English Avg. Chinese English Avg. Qwen2.5-VL-32B 51.13 Qwen3-VL 32B thinking 57.01 Gemini 2.5 Flash 57.92 GPT-5 mini 57.47 Doubao-Seed-1.6-vision 58.37 WebWatcher-7B 49.68 WebWatcher-32B 56.45 MindWatcher-32B 58.82 Best results are in bold and the suboptimal results are in underline. 21.61 20.78 30.03 32.96 39.06 21.33 21.61 22. 21.99 28.91 35.93 33.29 43.44 24.41 30.49 32.32 21.86 34.85 39.96 32.9 46.1 26.41 37.01 39.83 42.51 57.09 48.18 65.99 63.56 40.89 39.68 50.2 17.55 24.71 22.17 37.41 28.87 19.63 19.63 23.79 26.62 36.47 31.62 47.79 41.47 27.35 26.91 33.38 Table 3: Comparison Results of the Distilled Models and their Base Models. Method MindWatcher-Evaluation Benchmark Car Animal Plant Person Landmark Sport Qwen3-VL 2B Thinking MindWatcher-2B Qwen2.5-VL 3B Instruct MindWatcher-3B Qwen3-VL 4B Thinking MindWatcher-4B 39.95 50.13 17.43 49.87 56.3 56.03 70.37 80.34 35.04 80.06 81.2 84. 80.1 86.4 37.78 85.89 80.86 87.66 50.79 65.08 6.3 63.49 74.6 68.25 24.44 44.44 12.22 44.44 38.89 41.11 29.51 16.90 0.00 17.61 32.79 39.51 Avg. 51.41 64.76 24.93 64.48 66.53 69. VL-32B base model. Importantly, on the pure-text WebWalkerQA benchmark, MindWatcher remains highly competitive. Compared to its base model, Qwen2.5-VL-32B, results indicate that our continuous multimodal agentic RL training has successfully enhanced agent capabilities without compromising its foundational text-based reasoning. Figure 3 presents the win-tie-loss comparing MindWatcheranalysis 32B against representative four models: Qwen3-VL 32B Thinking, WebWatcher-32B, Gemini 2.5 Flash, and GPT-5 mini, across both public open-source benchmarks and our MWE-Bench. The results indicate that MindWatcher-32B consistently equivoutperforms in both alent 32B counterparts evaluation settings. Notably, on the MWE-Bench, MindWatcher-32B demonstrates superior performance even when compared to SOTA closed-source models, specifically Gemini 2.5 Flash and GPT-5 mini. its parameter (a) Open-sourced Benchmark. to their respective Table 3 further details the performance gains achieved by the three distilled small-scale models relative foundation models on the MWE-Bench. Among these, MindWatcher-3B (derived from Qwen2.5-VL-3B-Instruct) exhibits the most significant improvement, with its proficiency score surging from 24.93 to 64.48. This substantial leap underscores the effectiveness of our distilled training approach in empowering small-scale models with robust agentic capabilities. Figure 3: Benchmark Performance Comparison. (b) MWE-Benchmark. 4.3 Analysis 4.3.1 The Impact of the Tool Capacity During experiments, we find that the proficiency of the integrated tools is pivotal determinant of an agents final performance. This is particularly evident in external retrieval tasks, where the indexing and recall mechanisms of different search engines lead to highly heterogeneous outcomes for identical queries. Beyond direct downstream performance, we observed that the choice of search engine during RL training induces distinct tool-call behavioral adaptations and search patterns within the model. To quantify this impact, we conducted experiments using sports-related datasets, subdivided into two domains (Football and Basketball) and two languages (Chinese and English). We 12 Table 4: Results on different search engines."
        },
        {
            "title": "Football",
            "content": "Avg."
        },
        {
            "title": "Sogou Search\nBing search\nQuark Search",
            "content": "15.19 20.25 39.24 Best results are in bold and the suboptimal results are in underline. 12.5 23.21 55.36 2.53 13.92 20.25 3.57 8.93 28.57 8.51 16.66 34.81 evaluated the agent performance using three search enginesSogou, Bing, and Quark under retrieval-only setting. The results, summarized in Table 4, demonstrate substantial performance variance that frequently overshadows the variations attributed to algorithmic optimizations or foundation model scales. Specifically, in the most extreme case (football queries written by Chinese), the Quark search engine outperformed Sogou by staggering 42.86%. However, these findings do not point to universally superior search engine; rather, we found that the effectiveness of search engine is highly volatile and contingent upon the specific domain and language of the query. This volatility highlights that the capacity of an agent is deeply coupled with its environment, suggesting that benchmark evaluations must account for tool-induced variance to ensure fair assessment of models intrinsic reasoning abilities. 4.3.2 Genetic Inheritance in Agentic RL We conduct granular analysis of the relationship between tool-calling frequency and model accuracy. Specifically, we compare the behaviors and performance of MindWatcher against its own foundation model, Qwen2.5-VL-32B, and GPT-5 mini on the WME-Bench. The visualization of these results is presented in Figure 4. Disparity in Decision Trigger Boundaries: significant divergence is observed in the decision-making boundaries regarding tool invocation. As illustrated, GPT-5 mini opts to reason without any tool-calls (Round 0) in nearly one-sixth of the samples, yet achieves mere 51.2% accuracy. This suggests manifest blind self-confidence in GPT-5 mini; by relying on internal parameters for tasks requiring external knowledge, it forfeits substantial scores at the onset, leading to lower overall performance compared to MindWatcher. Interestingly, when the number of tool-calls exceeds one, GPT-5 mini exhibits remarkable robustness in long-chain reasoning, with its accuracy showing negligible decay from Round 2 to Round 6. (a) MindWatcher vs GPT-5 mini. (b) MindWatcher vs Qwen2.5-VL-32B. Figure 4: Comparison of Tool-use Behavior and Performance Distribution. 13 Case study 1: The Impact of Model World Knowledge on Agent Performance. Question: In the image, significant battle took place during the early 19th century in this northern South American city where prominent female patriot played crucial role by leading company of soldiers and providing essential support to key military leaders forces. This patriot was also known for her contributions to the independence movement of her country. In the main cathedral of this city, how many women are interred? Ground-truth: 2 Qwen3-VL 32B: <think> Okay, lets try to figure this out. The user is asking about northern South American city where significant battle happened in the early 19th century. The flag has red color with some emblem. Let me think. In South American independence, key figures include Simón Bolívar, and female figures like Manuela Sáenz. Wait, Manuela Sáenz was prominent female patriot in the independence movement of Bolivia or Peru? <think> MindWatcher 32B: <think> The question provides enough context to proceed. The woman is described as prominent female patriot who played crucial role in significant battle during the early 19th century in northern South American city. She led company of soldiers and provided essential support to key military leaders forces. She is also known for her contributions to the independence movement of her country. Given this information, can use the web_search tool to search for prominent female patriots in northern South American cities who were involved in significant battles during the early 19th century. Once identify the correct patriot, can determine the city she is associated with and then search for how many women are interred in the main cathedral of that city. <think> <tool_call> { name: web_search, arguments: { query: prominent female patriot in northern South America early 19th century, } } <tool_call> This phenomenon highlights that for high-capacity models, agentic performance can be bottlenecked by the decision trigger boundary rather than the executive action capability itself. Under autonomous settings, the model potential can be severely constrained by its initial failure to recognize the need for external tools. Performance Shadowing and Genetic Inheritance in Agentic RL: While MindWatcher, trained via RL, significantly outperforms its foundation model (Qwen2.5-VL-32B), we observe profound Genetic Inheritance in reasoning capacity. This is evidenced by the striking consistency in both accuracy trends and sample distribution across different tool-calling rounds. As the required number of tool-calls increases, MindWatcher maintains higher accuracy than Qwen2.5-VL-32B but fails to reverse the downward trend (identical decay slope) inherited from its foundation. Furthermore, MindWatchers self-awarenessmanifested in 14 its sample distribution across varying tool-call roundsshows no significant deviation from Qwen2.5-VL-32B even after extensive RL training. These observations suggest that while agentic RL can substantially refine toolinvocation and reasoning proficiency, it cannot fully breach the performance bottlenecks of the foundation model regarding long-range reasoning and multimodal processing. The foundation model imposes fundamental performance constraint on the RL-derived agent; agentic RL serves as strategy optimizer but remains fundamentally coupled with the base models capabilities. We term this phenomenon the Genetic Constraint of the foundation model in agentic RL scenarios. In Appendix A.3, we conducted further investigations into genetic inheritance in the agentic SFT scenario."
        },
        {
            "title": "4.3.3 The Impact of Model World Knowledge on Agent Performance",
            "content": "Beyond numerical analysis, we conduct case-based qualitative study to investigate how the world knowledge inherent in different foundation models affects downstream task performance. We observe that when the provided tools are insuﬀicient for low-knowledge model to resolve query, the model internal world knowledge becomes the decisive factor for downstream benchmark metrics. Case Study 1 presents visual comparison between MindWatcher (based on Qwen2.5-VL32B) and the next-generation Qwen3-VL 32B Thinking on specific case. In this example, neither model can correctly answer the question based solely on their internal capabilities. However, once provided with an external text-retrieval tool, significant performance gap emerges: Qwen3-VL possesses the internal world knowledge to recognize the name of the person (Manuela Sáenz) in the artwork. This prior knowledge allows it to formulate precise search queries using the text-search tool, leading to successful resolution. Conversely, MindWatcher (based on Qwen2.5-VL) lacks any prior information regarding this specific artwork. Without starting point for inquiry and lacking auxiliary tools to bridge this knowledge gap, the model is unable to perform any viable reasoning or retrieval. This case study demonstrates that in identical tool environments, performance metrics may not exclusively reflect TIR capabilities of model. Current benchmarks contain significant number of tasks that implicitly rely on the long-tail knowledge of model to catalyze the tool-use process. This coupling introduces substantial challenges in isolating and accurately evaluating the intrinsic TIR capacity of model, as the benchmark results become confounded by the uneven distribution of world knowledge across different foundation models. For given benchmark, when vast number of queries cannot be adequately addressed by the provided tools, the evaluation of the agent functional capabilities essentially regresses into test of the models internal world knowledge."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 TIR Agent The landscape of TIR agents has witnessed meteoric evolution over the past six months. By empowering models to autonomously select and invoke tools, the capability boundaries of LLMsparticularly those with smaller parameter countshave been significantly expanded. However, stark contrast persists: while contemporary LLMs exhibit reasoning capabilities comparable to human experts (often likened to PhD-level cognition), their action competencespecifically, the precision and robustness of tool invocationremains at nascent, almost elementary stage. OpenAI o3 [23], as the first TIR agent deployed to global user base, demonstrated astonishing proficiency. By actively manipulating images, performing complex calculations via code execution, and navigating file systems, o3 illuminated the vast potential of TIR agents to the research community. This paradigm shift catalyzed surge of open-source initiatives inspired by o3, ranging from specialized code and search agents to DeepResearch systems. rStar2-Agent [28] leverages code execution as verifier and solver to bolster mathematical reasoning. DeepEyes [44] introduces active visual tools, such as image zoom-in, probing the ability of multimodal agents to resolve fine-grained visual details through iterative 15 manipulation. The Qwen DeepResearch [22, 13, 36] team has also made pivotal contributions to the open-source ecosystem by systematically diagnosing and addressing the multidimensional challenges inherent in long-horizon research tasks. Despite these strides, the chasm between an agents thinking and acting remains substantial. Critical challenges such as dynamic tool context management, long-term historical memory maintenance, and the attainment of training-free tool invocation capabilities represent significant hurdles that the field must address in the near future."
        },
        {
            "title": "5.2 Training Method of TIR Agent",
            "content": "Unlike traditional LLM training, training TIR agents presents distinct challenges due to the necessity of interacting with external environmentsspecifically, executing tool calls and interpreting heterogeneous feedback during generation. Beyond mere planning, agents must learn to act adaptively within dynamic information contexts. Several works focus on the offline training stages of continual pre-training and SFT. [34] proposes an agent-specific continued pre-training method designed to endow base models with native action capabilities, thereby effectively supporting subsequent fine-tuning. WebDancer [39] and WebSailor [20] concentrate on methodologies for constructing high-quality TIR trajectories; while they incorporate elements of RL, they predominantly rely on SFT to shape agent behavior. The transition to online RL, which requires interaction with real-world environments, precipitates steep rise in training complexity. Recent research has tailored algorithms specifically for this regime. ARPO [12] introduces an entropy balancing mechanism to prevent the training collapse often observed during TIR agent RL. LLDS [10] investigates the lazy likelihood displacement problem in agent RL, introducing likelihood preservation regularization to avert systemic stagnation in training. In summary, given the high cost of constructing premium trajectory data and the inherent instability of online RL, the path toward an optimal training strategy for TIR agents remains long and arduous."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced MindWatcher, high-performance TIR agent. We developed robust visual question answering training data construction pipelines and proposed specialized training methodology distinct from prior works. To empower MindWatcher, we established comprehensive yet cost-effective multimodal toolbox and introduced MWE-Bench for rigorous performance evaluation. Our experiments demonstrate that MindWatcher, through its superior tool-invocation capabilities, can match or even exceed the performance of significantly larger or updated models. Beyond empirical results, this study reveals several critical experimental findings discovered during our development of the TIR agent. We hope our work provides unique insights and contributes to the future advancement of tool-augmented intelligence."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We extend our gratitude to Tongyi Qwen for their outstanding contributions to open-source LLMs; and we thank all colleagues at LiAuto Base Model for their support of the MindWatcher project."
        },
        {
            "title": "Author List",
            "content": "Core Contributors (Equal contribution) Jiawei Chen Xintian Shen Zhenwei Shao"
        },
        {
            "title": "Tao Wei",
            "content": "Supervisors Pan Zhou Wei Chen"
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Open-sourced Benchmark In this work, rather than utilizing the full set or naive random subset of open-source benchmarks, we implemented rigorous data filtration pipeline. This decision stems from the observation that many existing benchmarks suffer from significant limitations, such as information lag due to insuﬀicient temporal constraints. Furthermore, models released after benchmark publication may exhibit inflated performance due to inadvertent data leakage. To address these issues, we established the Qwen3-VL 32B Thinking as baseline for direct inference on the original benchmarks. All samples correctly answered by the model through direct inference were discarded. For the remaining samples, we conducted meticulous manual review to filter out ambiguous questions or those with expired time-sensitivity. This process yielded high-quality subset of open-source benchmarks, which we subsequently used to evaluate reasoning and tool-integrated capabilities under the ReAct/Agent paradigm. Among these, the MMSearch subset contains 221 samples, while the simplevqa subset comprises 823 samples, including 361 Chinese samples and 462 English samples. A.2 Infrastructure Figure 5: Step-wise Synchronous Sampling Framework of MindWatcher. To facilitate eﬀicient agentic reinforcement learning, we developed step-wise synchronous sampling framework based on the Verl [32] to coordinate interactions between the agent and external environments as shown in Figure 5. In each rollout iteration, the vLLM engine [18] performs parallel batch inference to generate actions, followed by synchronized barrier where the environment collects feedback. This design ensures trajectory consistency across massive batches while simplifying state management for the on-policy training process. Empirical observations during training revealed that the primary bottleneck is not the trajectory generation itself, as the latency gap between synchronous and asynchronous sampling remains marginal. Instead, the dominant time expenditure arises from tool-calling latency. To mitigate this, we integrated an asynchronous tool invocation layer within the synchronous loop. By leveraging asyncio mechanisms and semaphore-based concurrency control, heterogeneous tools are dispatched and executed in parallel while strictly adhering to API QPS constraints. Furthermore, we implemented Tokenization Offloading, which offloads the computationally intensive task of processing environment observations tokenization from the master node to distributed CPU workers. Additionally, the LLM-as-a-Judge reward model is invoked immediately upon the completion of each trajectory to minimize evaluation overhead. This hybrid architecturesynchronous in step control but asynchronous in tool executionmaximizes hardware utilization and significantly reduces the actual rollout time. A.3 Genetic Inheritance in Agentic SFT Building upon our analysis of genetic inheritance in the agentic RL paradigm (Section 4.3.2), we extend our investigation to the agentic SFT scenario using three distilled small-scale agent models. Figures 6, 7, and 8 illustrate the tool-use behavior and performance distribu18 Figure 6: MindWatcher-2B vs Qwen3-VL 2B Thinking. Figure 7: MindWatcher-3B vs Qwen2.5-VL-3B. Figure 8: MindWatcher-4B vs Qwen3-VL 4B Thinking. 19 tions of MindWatcher-2B, 3B, and 4B alongside their respective foundations: Qwen3-VL-2B Thinking, Qwen2.5-VL-3B, and Qwen3-VL-4B Thinking. Our observations indicate that, unlike the RL scenario, SFT-tuned models do not exhibit consistent or predictable trend in tool-calling frequency relative to their base models. The decision trigger boundary in the SFT paradigm appears significantly less robust. For instance, after agentic SFT, the Qwen2.5-VL-3B model showed dramatic shift, with Round 0 cases (no tool use) plummeting from 116 to just 1. Across the three distilled models, the distribution of tool-call rounds fluctuates inconsistently before and after SFT, lacking the stable behavioral alignment observed in the RL-tuned MindWatcher-32B. Despite the behavioral volatility, the accuracy trends across different tool-call rounds reveal phenomenon strikingly similar to that of agentic RL. As the complexity of the task increases (i.e., more tool-call rounds), both the SFT-tuned agents and their base models exhibit synchronized downward trend in accuracy. This reinforces the existence of genetic inheritance within the SFT paradigm: supervised fine-tuning is inherently limited by the base model capabilities in long-range reasoning and multimodal processing. Like RL, SFT serves as method for policy alignment but fails to break through the fundamental cognitive ceiling established by the foundation model. In the agentic RL key distinction arises in the elegance of the performance curves. scenario, the performance of the agent and the base model decay at nearly identical slopes, showing highly structured coupling. In contrast, the performance curves in the SFT are less congruent; while they share the same downward trajectory, the lack of perfectly parallel slope suggests that SFT introduces more noise or less systematic optimization into the models reasoning-tool-use integration compared to the more rigorous RL process. 20 A.4 Tool description for MindWatcher Tool: Region Croping/Zooming Description: Zoom into specific area of **the first input image** based on your provided bounding box. input: the image and bounding box. output:a new image. Arguments: bbox: [x1, y1, x2, y2], # The bounding box you provided, where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner. Tool: Object Grounding & Visual Search Description: Retrieve new similar images and their descriptions based on the provided bounding box area of **the first input image**. input:image and bounding box. output:only the most similar targets type name and the confidence score. Arguments: bbox_2d:[x1, y1, x2, y2], # The bounding box you provided, where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner. category:the category # The category of the image you want to search for, which can only be one of {plant, animal, car, person, landmark, vegetable, cuisine, logo}. Tool: External Text Retrieval Description: Retrieve external text information from the internet based on your provided text query. input:only text query. output:text. Arguments: query: the content. Tool: Webpage Content Retrieval Description: Visit specified web page URL under 3 modes: you can read its full content, the content within the window you provided, or use an AI assistant to generate structured summary based on the specific goal you provide. input:A JSON object containing three arguments: url, window, goal. output:A JSON object containing the visited URL and structured result. Arguments: url: https://example.com/article, # The webpage url you want to visit. window: [a, b], # Select the content you want to read within [a, b] (Optional) goal: . # What you want to get or find. (Optional) Tool: Local Code Interpreter Description: Execute Python code in sandboxed environment. Useful for calculations, data manipulation, and other programmatic tasks. This tool is isolated and cannot access files or the internet. input:A string of Python code. The tool call format can be standard JSON object or Python dictionary literal. output:A JSON object containing stdout (from print statements), stderr (error messages), and the final result value. Arguments: code: the code such as: = 5nb = 10nresult = * bnprint(result) # The Python code to execute. 21 A.5 Trajectory Display In this section, we visualize tool calling trajectory of MindWatcher to highlight the model multimodal chain-of-thought reasoning and interleaved thinking capabilities. Case Question: 在 2025 年 11 月 18 日图中这名球员与耐克续签代言后他成为现役 NBA/WNBA 与耐克仍有签名鞋合同的第几位球星截至 2025 年 11 月 18 日图中这名来自底特律球队的球 员的个人耐克签名鞋预计在哪个赛季首次亮相在 2025 年 11 月 18 日这次续约时图中球员所 属的联赛共有多少位现役球员拥有与耐克的签名鞋合同以 2025 年 11 月 18 日为准与图中这 名球员所属联赛不同的另一联赛共有多少位现役球员拥有与耐克的签名鞋合同在 2025 年 11 月 18 日的情况下图中这名身穿DETROIT球衣的球员与耐克续签的代言合同期限是几年 Ground-truth: 第 9 位; 2026-27 赛季; 6 位; 3 位; 6 年 Step 1: <think> 为了回答这个问题我需要识别出图片中的球员并获取以下信息 1. 截至 2025 年 11 月 18 日这位球员与耐克续签代言后成为现役 NBA/WNBA 与耐克仍 有签名鞋合同的第几位球星 2. 截至 2025 年 11 月 18 日这位球员的个人耐克签名鞋预计在哪个赛季首次亮相 3. 在 2025 年 11 月 18 日这次续约时这位球员所属的联赛共有多少位现役球员拥有与耐克 的签名鞋合同 4. 截至 2025 年 11 月 18 日与这位球员所属联赛不同的另一联赛共有多少位现役球员拥有与 耐克的签名鞋合同 5. 截至 2025 年 11 月 18 日这位球员与耐克续签的代言合同期限是几年 首先我需要识别出图片中的球员可以通过调用工具来帮助我完成这个任务 <think> <tool_call> { name: zoom_v_search, arguments: { bbox_2d: [1300, 0, 2761, 2209], category: person } } <tool_call> <tool_response> 名称凯德坎宁安, 检索置信度0.81 <tool_response> Step 2: <think> 通过使用工具我识别出图片中的球员是凯德坎宁安Cade Cunningham接下来我需 要查找与他相关的耐克代言信息包括他在 2025 年 11 月 18 日与耐克续签代言后的排名签 名鞋首次亮相的赛季续约时现役球员中有多少人拥有与耐克的签名鞋合同WNBA 中拥有 耐克签名鞋合同的现役球员数量以及他与耐克续约的合同期限为了获取这些信息我将使用 网络搜索工具 <think> <tool_call> { name: web_search, arguments: { query: 凯德坎宁安 2025 年 11 月 18 日耐克续 签 } } <tool_call> 22 Case1 <tool_response> { 'search_query': 凯德坎宁安 2025 年 11 月 18 日耐克续签, 'search_result': [ { 'id': 1, 'title': 下一位超巨坎宁安与耐克达成 6 年续约签名鞋将于下赛季亮相, 'content': 北京时间 11 月 18 日底特律活塞队的后卫凯德坎宁安将继续与耐克公司 保持合作关系... 根据知名记者 Shams 的报道坎宁安已经与耐克签署了一份新的为期六年的代言 合同..., 'url': https://m.sohu.com/a/956031720_121722346/?pvid=000115_3w_a, 'date': 2025-11-19T10:43:00+08:00 }, { 'id': 8, 'title': 耐克续约坎宁安他将成为后詹杜库时代美国本土球员耐克的新门面, 'content': 耐克把六年合同拍在坎宁安面前... 2027 年才能买到的第一双 CC 战靴其 实今天就已经在生产线排号..., 'url': https://m.toutiao.com/a7574051184151937574/ } ] } <tool_response> Step 3: <think> 通过网络搜索我获得了以下信息 1. 凯德坎宁安在 2025 年 11 月 18 日与耐克续签了一份为期六年的代言合同 2. 他的签名鞋预计将在 2026-27 赛季的下半程正式发布 3. 他是现役第六位拥有耐克签名鞋的 NBA 球员与勒布朗詹姆斯凯文杜兰特扬尼 斯安特托昆博等超级巨星并列顶尖阵营 根据这些信息我可以回答问题 1. 截至 2025 年 11 月 18 日凯德坎宁安与耐克续签代言后成为现役 NBA/WNBA 与耐 克仍有签名鞋合同的第 6 位球星 2. 截至 2025 年 11 月 18 日凯德坎宁安的个人耐克签名鞋预计在 2026-27 赛季首次亮相 3. 在 2025 年 11 月 18 日这次续约时凯德坎宁安所属的联赛NBA共有 6 位现役球员 拥有与耐克的签名鞋合同 4. 关于 WNBA 中拥有耐克签名鞋合同的现役球员数量我没有找到具体的信息为了回答这 个问题我将使用网络搜索工具 5. 截至 2025 年 11 月 18 日凯德坎宁安与耐克续签的代言合同期限是 6 年 接下来我将使用网络搜索工具来查找 WNBA 中拥有耐克签名鞋合同的现役球员数量 <think> <tool_call> { name: web_search, arguments: { query: WNBA 现役耐克签名鞋合同数量 } } <tool_call> <tool_response> { 'search_query': WNBA 现役耐克签名鞋合同数量, 'search_result': [ { 'id': 1, 'title': Nike 签约 6 位 NBA 及 3 位 WNBA 球员拥有签名鞋, 'content': 目前拥有 Nike 签名鞋合同的 6 位 NBA 球员和 3 位 WNBA 球员目 前拥有 Nike 签名鞋合同的 6 位 NBA 球员和 3 位 WNBA 球员..., 'url': http://cj.sina.cn/articles/view/2432009827/p90f5866302701jglm, 'date': 2025-11-18T12:00:00+08:00 }, 23 Case1 { 'id': 8, 'title': 现役拥有 Nike 签名鞋合同 9 位球星NBA6 人 WNBA3 人, 'content': 11 月 18 日讯据 Shams Charania 报道活塞球星康宁汉姆和耐克续签一份 为 6 年的代言合同并将拥有自己的签名鞋球员的个人签名鞋预计将 26-27 赛季首次亮相康 宁汉姆也成为了 NBA/WNBA 现役球员中位与耐克还有签名鞋合同的球星..., 'url': http://17449.xdgk.com.cn/dianshiju/38952adc0ec9b8fNw6lg.html 'date': 2025-12-20T01:37:34+08:00 } ] } <tool_response> Step 4: <think> 通过网络搜索我获得了以下信息 1. WNBA 中拥有耐克签名鞋合同的现役球员数量为 3 人 现在我已经有了所有问题的答案 1. 截至 2025 年 11 月 18 日凯德坎宁安与耐克续签代言后成为现役 NBA/WNBA 与耐 克仍有签名鞋合同的第 6 位球星 2. 截至 2025 年 11 月 18 日凯德坎宁安的个人耐克签名鞋预计在 2026-27 赛季首次亮相 3. 在 2025 年 11 月 18 日这次续约时凯德坎宁安所属的联赛NBA共有 6 位现役球员 拥有与耐克的签名鞋合同 4. 截至 2025 年 11 月 18 日与凯德坎宁安所属联赛不同的另一联赛WNBA共有 3 位 现役球员拥有与耐克的签名鞋合同 5. 截至 2025 年 11 月 18 日凯德坎宁安与耐克续签的代言合同期限是 6 年 我将给出最终答案 <think> <answer> 1. 截至 2025 年 11 月 18 日凯德坎宁安与耐克续签代言后成为现役 NBA/WNBA 与耐克仍 有签名鞋合同的第 6 位球星 2. 截至 2025 年 11 月 18 日凯德坎宁安的个人耐克签名鞋预计在 2026-27 赛季首次亮相 3. 在 2025 年 11 月 18 日这次续约时凯德坎宁安所属的联赛NBA共有 6 位现役球员拥有 与耐克的签名鞋合同 4. 截至 2025 年 11 月 18 日与凯德坎宁安所属联赛不同的另一联赛WNBA共有 3 位现役 球员拥有与耐克的签名鞋合同 5. 截至 2025 年 11 月 18 日凯德坎宁安与耐克续签的代言合同期限是 6 年 <answer> 24 Case2 Question: 这座山体雕刻上刻画了哪几位美国总统的面孔其中哪一位人物在独立战争中发挥了 关键作用他哪一年写了信件这封信授权美国首次正式开展间谍活动 Ground-truth: 刻画了乔治华盛顿托马斯杰斐逊西奥多罗斯福亚伯拉罕林肯乔 治华盛顿在美国独立战争中发挥了关键作用1777 年 Step 1: <think> 这张图片显示的是美国著名的地标拉什莫尔山Mount Rushmore根据图片中的特征 我可以识别出这座山体雕刻上刻画了几位美国总统的面孔为了确认具体是哪几位总统并找 到与独立战争相关的关键人物及其写信授权美国首次正式开展间谍活动的年份我需要调用工 具来获取更多信息 <think> <tool_call> { name: zoom_v_search, arguments: { bbox_2d: [186, 145, 637, 356], category: landmark } } <tool_call> <tool_response> 名称拉什莫尔山国家纪念公园, 检索置信度0.97 <tool_response> Step 2: <think> 根据工具返回的结果确认这座山体雕刻是拉什莫尔山国家纪念公园拉什莫尔山上雕刻了四 位美国总统的面孔乔治华盛顿托马斯杰斐逊西奥多罗斯福和亚伯拉罕林肯现 在我需要找出其中哪一位在独立战争中发挥了关键作用并确定他写信授权美国首次正式开 展间谍活动的年份为了获取这些详细信息我将使用 web_search 工具来查找相关信息 <think> <tool_call> { name: web_search, arguments: { query: 拉什莫尔山四位美国总统独立战争关键人 物首次正式开展间谍活动 } } <tool_call> <tool_response> { 'search_query': 拉什莫尔山四位美国总统独立战争关键人物首次正式开展间谍活动, 'search_result': [ { 'id': 1, 'title': 美国南达科他州拉什莫尔山上的四个人头像是哪四位啊, 'content': 美国南达科他州拉什莫尔山上的四个人头像是哪四位啊美国南达科他州 拉什莫尔山上的四座高 60 英尺的总统头像象征着美国建国 150 年来的历史这四位总统分 别是乔治华盛顿托马斯杰弗逊西奥多罗斯福和亚伯拉罕林肯华盛顿作为美..., 'url': https://zhidao.baidu.com/question/572951197473818884.html, 'date': 2024-10-23T00:00:00+08:00 }, { 'id': 5, 'title': 独立战争期间美国反间谍第一人, 25 Case2 'content': 独立战争期间美国反间谍第一人, 美国, 威廉, 杰伊, 约翰, 华盛顿, 反间 谍, 克劳斯比, 独立战争, 'url': https://m.163.com/dy/article/K30AB0GV0514R9P4.html 'date': 2025-06-26T16:33:59+08:00 } ] } <tool_response> Step 3: <think> 通过工具返回的搜索结果我了解到拉什莫尔山上雕刻的四位美国总统是乔治华盛顿托马 斯杰斐逊西奥多罗斯福和亚伯拉罕林肯为了找到在独立战争中发挥了关键作用的人 物以及他写信授权美国首次正式开展间谍活动的年份我需要进一步查看搜索结果中的相关内 容根据第三个搜索结果提到独立战争期间美国反间谍第一人并且涉及威廉杰伊和约翰 华盛顿等人物为了获取更多详细信息我将访问相关网页以获取具体细节 <think> <tool_call> { name: url_visit, arguments: { url:https://m.163.com/dy/article/K30AB0GV0 514R9P4.html, goal: 找到在独立战争中发挥了关键作用的人物以及授权年份 } } <tool_call> <tool_response> { \"visited_url\": https://m.163.com/dy/article/K30AB0GV0514R9P4.html, \"mode\": summarize, \"result\": { \"rational\": The user seeks to identify key figure in the American Revolutionary War and the year they authorized formal espionage activities. The content highlights John Jays pivotal role in establishing counterintelligence and mentions Washingtons 1776 correspondence about British spies. The formation of Jays committee in 1776 directly addresses the users goal., \"evidence\": [ 华盛顿能够用谍获情就知道反间工作同样必不可少1776 年 3 月 24 日华盛顿在与 部属交流的信中写道我最担心的一件事是他们的间谍因此我希望能对他们保持最严 密的监视于是浩浩荡荡的清除英国间谍活动在大陆军中展开, 1776 年 6 月年仅 30 岁的约翰杰伊受华盛顿的委派成立了纽约州侦查和挫败阴谋委员会负责情报搜集逮 捕英国间谍和调查可疑的英国同情者拥有逮捕审判监禁驱逐嫌疑犯的权力约翰杰 伊出任该委员会主席至 1777 年 2 月中旬 ], \"summary\": 在独立战争中约翰杰伊John Jay发挥了关键作用1776 年 3 月 24 日华盛顿在信中表达了对英国间谍的担忧随后于同年 6 月授权杰伊成立纽约州侦查和挫 败阴谋委员会这是美国首次正式开展反间谍活动的机构该委员会赋予杰伊逮捕审判和 驱逐英国间谍及同情者的权力标志着美国早期情报工作的制度化 } <tool_response> Step 4: <think> 根据工具返回的结果我已经找到了问题的答案拉什莫尔山上雕刻的四位美国总统是乔治 华盛顿托马斯杰斐逊西奥多罗斯福和亚伯拉罕林肯在独立战争中发挥关键作用的 人物是约翰杰伊John Jay他在 1776 年被授权组建纽约州侦查和挫败阴谋委员会这 是美国首次正式开展反间谍活动的机构因此我可以给出最终答案 <think> <answer> 拉什莫尔山上雕刻的四位美国总统是乔治华盛顿托马斯杰斐逊西奥多罗斯福和亚伯拉 罕林肯在独立战争中发挥关键作用的人物是约翰杰伊John Jay他在 1776 年被授权组 建纽约州侦查和挫败阴谋委员会这是美国首次正式开展反间谍活动的机构 <answer> 26 Case3 Question: What are the functions of these glasses? Ground-truth: CameraOpen-back headphonesVehicle controlAI assistant Step 1: <think> The image shows disassembled pair of glasses with various components labeled. To determine the functions of these glasses, need to identify each component and understand their roles. However, the labels on the components are not clearly visible in the current image. will zoom in on the image to get better view of the labels and components. <think> <tool_call> { name: im_zoom_in, arguments: { bbox_2d: [485, 385, 725, 580] } } <tool_call> <tool_response> <image> <tool_response> Step 2: <think> The zoomed-in image provides clearer view of the glasses and some of their components. However, the labels on the components are still not legible. To understand the functions of these glasses, need to identify the components and their roles. Since the labels are not readable, will perform web search using the brand name Livis to find more information about these glasses and their functionalities. <think> <tool_call> { name: web_search, arguments: { query: Livis glasses functions } } <tool_call> <tool_response> { 'search_query': Livis glasses functions, 'search_result': [ { 'id': 1, 'title': 视频评测理想 Livis首先是副好眼镜这真的很重要, 27 Case 'content': 而为了让 Livis 作为一副眼镜更好用理想也在镜片上花了不少功夫理 想 Livis 的镜片合作方为百年光学大厂蔡司..., 'url': https://new.qq.com/rain/a/LNK2025120406148900, 'date': 2025-12-04T12:05:00+08:00 }, { 'id': 5, 'title': 1699 元今年最值得买的智能眼镜 Livis居然来自一家车企, 'content': 依靠免唤醒和意图识别你可以在任何地方直接说打开后备箱查询胎 压或者打开座椅加热完全不需要掏出手机..., 'url': https://new.qq.com/rain/a/LNK2025120312421600 'date': 2025-12-03T22:21:00+08:00 } ] } <tool_response> Step 3: <think> am now ready to provide the final answer based on the information gathered. <think> <answer> The Livis glasses have multiple functions, including: 1. **Optical Function**: High-quality lenses made in collaboration with Carl Zeiss, offering wide range of vision correction options. 2. **Sound System**: Sophisticated dual magnetic circuit sandwich speakers with intelligent frequency adjustment for leak-proof functionality and enhanced audio experience. 3. **Camera Function**: 12-megapixel IMX681 sensor with 105-degree ultra-wide-angle lens, supporting flexible multi-format shooting and live photo capture. 4. **Vehicle Control**: Voice-controlled vehicle operations, including opening the trunk, checking tire pressure, and adjusting seat heating. 5. **Voice Assistant**: Integrated smart voice assistant for hands-free communication and access to various services. 6. **Wireless Charging**: Dedicated charging case that supports wireless charging. 7. **AI Features**: Advanced AI technology for recognizing user intentions, enhancing audio quality, and optimizing user interactions. These features make Livis glasses multifunctional device that combines eyewear, audio, camera, vehicle control, and AI capabilities. <answer> 28 A.6 Prompt Design In this section, we display the prompts utilized by policy model and evaluation judge model. Prompt: Policy Model 你是一个 ReAct 范式的 agent能够接受文本输入回答用户问题对于一些复杂的问题你可以 选择调用网络搜索工具帮助你解决问题你可以调用的工具包括 (** 具体描述请参考工具描述部分 **) 1. Region Croping/Zooming 2. Object Grounding Visual Search 3. External Text Retrieval 4. Webpage Content Retrieval 对于每一个问题你需要先思考然后调用工具如果需要你会得到工具调用返回的结果还 可以根据工具的返回结果进行进一步的思考最后给出答案你的思考过程工具调用请求以及回答 需要严格按照以下格式 <think> 你的思考过程 <think> <tool_call> name: <function-name>, arguments: <args-json-object> <tool_call> (如果需要调用工具, 你的工具调用请求参考 usage 中的示例) <think> 你的思考过程 <think> (如果需要进一步思考) <answer> 你的最终答案 <answer> 请记住你在每次调用工具之后也就是输出 </tool_call> 之后都需要结束本轮对话等待工 具调用的结果返回再进行后续动作在输出回答之后即在输出 <answer> 之后你需要立即结 束本轮对话不要再输出任何内容你的思考次数和工具调用次数没有限制但必须在最后给出你的 答案对于任何问题你不应该拒绝回答而应该通过不断思考或调用工具直到得到确信的结果 Prompt: Evaluation Judge Model 你是一个公正的评测员负责判断模型回答是否在事实和逻辑上符合标准答案 输入信息 - ** 问题 **question - ** 模型回答 **output - ** 标准答案 (GT)**ground_truth 请遵循以下 ** 通用等价性原则 ** 进行判断 1. ** 核心事实一致性 ** - 忽略措辞语序或详细程度的差异只要大概语义一致即为正确 2. ** 数学与单位等价性 ** - ** 自动换算 **如果模型回答的数值单位与 GT 或问题要求的单位不同** 必须先进行数学换 算 ** 再比较 - ** 精度容忍 **允许合理的精度误差换算后的前两位有效数字正确就行 - ** 格式识别 **识别45,610 (带逗号) 和45610 为同一个数 3. ** 跨语言与语义对齐 ** - 忽略中英文差异 - 识别同义指代例如NYC = New York City; 巴特勒码头 = Butlers Wharf 4. ** 非法问题处理 ** - 如果 GT 表示问题无解如invalid question, no answer只要模型 ** 否定了事件发生 ** 或 ** 拒绝回答 **均判定为 ** 正确 ** - 只有当模型顺着错误前提编造事实时才判错 5. ** 拒答处理 ** - 如果 GT 有明确答案而模型回答不知道或无法回答判定为错误 29 Prompt: Evaluation Judge Model Respond strictly in valid JSON object matching this schema: { 'extracted_final_answer': string, 'reasoning': string, 'result': 1 0, 'confidence': integer, 'strict': true }"
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Jina AI. Jina, 2025. https://jina.ai/. [3] Shuai Bai, Yuxuan Cai, Ruizhe Chen, et al. Qwen3-vl technical report, 2025. URL https://arxiv.org/abs/2511.21631. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025. [6] Jiawei Chen, Dingkang Yang, Yue Jiang, Mingcheng Li, Jinjie Wei, Xiaolu Hou, and Lihua Zhang. Eﬀiciency in focus: Layernorm as catalyst for fine-tuning medical visual language models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 31223130, 2024. [7] Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. arXiv preprint arXiv:2406.10185, 2024. [8] Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, et al. Simplevqa: Multimodal factuality evaluation for multimodal large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 46374646, 2025. [9] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [10] Wenlong Deng, Yushu Li, Boying Gong, Yi Ren, Christos Thrampoulidis, and Xiaoxiao Li. On grpo collapse in search-r1: The lazy likelihood-displacement death spiral. arXiv preprint arXiv:2512.04220, 2025. [11] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. arXiv preprint arXiv:2505.16410, 2025. [12] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. [13] Xinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong Wu, Yida Zhao, Kuan Li, et al. Webwatcher: Breaking new frontier of vision-language deep research agent. arXiv preprint arXiv:2508.05748, 2025. [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [15] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, et al. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv preprint arXiv:2409.12959, 2024. 31 [16] Yue Jiang, Jiawei Chen, Dingkang Yang, Mingcheng Li, Shunli Wang, Tong Wu, Ke Li, and Lihua Zhang. Comt: Chain-of-medical-thought reduces hallucination in medical report generation. In ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15, 2025. doi: 10.1109/ICASSP49660. 2025.10887699. [17] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022. [18] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Eﬀicient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [19] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [20] Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025. [21] Mingcheng Li, Xiaolu Hou, Ziyang Liu, Dingkang Yang, Ziyun Qian, Jiawei Chen, Jinjie Wei, Yue Jiang, Qingyao Xu, and Lihua Zhang. Mccd: Multi-agent collaboration-based compositional diffusion for complex text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1326313272, 2025. [22] Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, et al. Webweaver: Structuring web-scale evidence with dynamic outlines for open-ended deep research. arXiv preprint arXiv:2509.13312, 2025. [23] OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/, April 2025. Accessed: 2025-12-19. [24] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [25] MindGPT ov Team. Mindgpt-4ov: An enhanced mllm via multi-stage post-training paradigm. arXiv preprint arXiv:2512.02895, 2025. [26] Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, et al. Webresearcher: Unleashing unbounded reasoning capability in long-horizon agents. arXiv preprint arXiv:2509.13309, 2025. [27] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training, 2018. [28] Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, et al. rstar2-agent: Agentic reasoning technical report. arXiv preprint arXiv:2508.20722, 2025. [29] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with answer heuristics for knowledge-based visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1497414983, June 2023. 32 [30] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [31] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. [32] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and eﬀicient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [33] Yuchen Shi, Siqi Cai, Zihan Xu, Yuei Qin, Gang Li, Hang Shao, Jiawei Chen, Deqing Yang, Ke Li, and Xing Sun. Flowagent: Achieving compliance and flexibility for workflow agents. arXiv preprint arXiv:2502.14345, 2025. [34] Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, et al. Scaling agents via continual pre-training. arXiv preprint arXiv:2509.13310, 2025. [35] Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, et al. Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. arXiv preprint arXiv:2505.16834, 2025. [36] Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, et al. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025. [37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and eﬀicient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [39] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025. [40] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572, 2025. [41] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversations. In First Conference on Language Modeling, 2024. [42] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [43] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. [44] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025."
        }
    ],
    "affiliations": [
        "Li Auto Inc"
    ]
}