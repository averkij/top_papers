{
    "paper_title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue",
    "authors": [
        "Yichun Feng",
        "Jiawei Wang",
        "Lu Zhou",
        "Yixue Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully describe their symptoms in a single round, leading to nonspecific diagnostic recommendations when complaints are vague. Traditional multi-turn dialogue methods based on supervised learning are constrained by static data-driven paradigms, lacking generalizability and struggling to intelligently extract key clinical information. To address these limitations, we propose DoctorAgent-RL, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing models in both multi-turn reasoning capability and final diagnostic performance, demonstrating practical value in assisting clinical consultations. https://github.com/JarvisUSTC/DoctorAgent-RL"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 3 6 9 1 . 5 0 5 2 : r DoctorAgent-RL: Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue Yichun Feng1,2, Jiawei Wang3, Lu Zhou2, and Yixue Li2, 1School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences 2Guangzhou National Laboratory 3Department of EEIS, University of Science and Technology of China 4Shanghai Institute of Nutrition and Health, Chinese Academy of Sciences 1https://github.com/JarvisUSTC/DoctorAgent-RL"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on one-way information transmission mode where patients must fully describe their symptoms in single round, leading to nonspecific diagnostic recommendations when complaints are vague. Traditional multi-turn dialogue methods based on supervised learning are constrained by static data-driven paradigms, lacking generalizability and struggling to intelligently extract key clinical information. To address these limitations, we propose DoctorAgent-RL, reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as dynamic decisionmaking process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing models in both multi-turn reasoning capability and final diagnostic performance, demonstrating practical value in assisting clinical consultations."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) such as ChatGPT [1], LLaMA [2], and ChatGLM [3] have demonstrated remarkable capabilities in various natural language processing tasks, including open-domain question answering, dialogue generation, and code synthesis [4]. With their strong generalization abilities, these models are increasingly applied to healthcare domains, where they exhibit potential for providing preliminary medical advice and assisting clinical decision-making [5]. The core flaw of current medical LLMs lies in the inherent contradiction between their passive singleturn interaction paradigm and the active information-gathering requirements of clinical diagnosis. 1These authors contributed equally to this work. 2Corresponding author: li_yixue@gzlab.ac.cn Preprint. Under review. Although existing systems demonstrate high level of knowledge accuracy in structured medical question-answering tasks, their \"answer-only\" characteristic forces patients to fully describe their symptoms in single round of conversation [6]. However, in reality, complex medical conditions often require multiple targeted inquiries to gradually clarify the situation [7]. This misalignment in interaction patterns leads to two systemic limitations: On the one hand, users tend to provide vague or fragmented initial descriptions, and the diagnostic suggestions generated by the models based on incomplete information often lack specificity and may even pose potential risks [8]. On the other hand, existing methods that attempt to introduce multi-turn dialogue capabilities through supervised learning, limited by the pattern imitation of static dialogue datasets, fail to establish dynamic decision-making mechanism to weigh the value of information against dialogue efficiency [9]. To address these challenges, we propose DoctorAgent-RLa multi-agent collaborative reinforcement learning framework that reformulates clinical reasoning as Markov Decision Process (MDP). Within this framework: (1) high-fidelity patient agent based on LLMs, which generates pathologically consistent responses while mimicking the diversity of real-world communication; (2) clinician agent initialized by cloning medical actions from real consultation records and refined through reinforcement learning (RL) to master effective questioning strategies; and (3) consultation evaluator that provides multi-dimensional rewards based on diagnostic accuracy, patient information responsiveness, and the standardization of questions. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating realistic patient interactions. This dataset enables the training and evaluation of multi-agent systems in dynamic clinical reasoning scenarios. DoctorAgent-RL redefines the diagnostic process through reward-based strategic policy optimization: The doctor agent continuously optimizes its questioning strategy within RL framework, obtaining immediate feedback through multi-turn interactions with the patient agent, and dynamically adjusting the information-gathering path based on the comprehensive rewards provided by the consultation evaluator. This end-to-end RL fine-tuning mechanism enables large language models to autonomously evolve interaction strategies that align with clinical reasoning logic, rather than simply imitating surface patterns in existing dialogue data. Experiments demonstrate that DoctorAgent-RL outperforms existing models in both multi-turn reasoning capability and final diagnostic performance, as shown in Figure 1, demonstrating practical value in assisting clinical consultations. Through such multi-agent collaborative training, DoctorAgent-RL allows the model to essentially learn how to ask context-relevant questions and when to confidently terminate the consultation, thereby achieving proactive and strategic patient interaction. Our contributions are three-fold: We propose DoctorAgent-RL, multi-agent collaborative RL framework where the doctor agent autonomously develops clinically-aligned questioning strategies through interactions with the patient agent, guided by the consultation evaluators comprehensive reward mechanism. DoctorAgent-RL dynamically adjusts information-gathering pathways, overcoming the limitations of conventional static data-driven paradigms. We construct MTMedDialog, the first English multi-turn medical dialogue dataset capable of simulating patient interactions. DoctorAgent-RL achieves state-of-the-art performance in multi-turn medical consultations, demonstrating exceptional capabilities in generating high-quality questions and delivering more accurate clinical diagnoses."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Medical QA Systems Despite achieving high accuracy in standardized benchmark tests, current medical LLMs are still restricted by their passive single-turn QA pattern [10]. Systems like MedAlpaca [11] and MedDialog [12] concentrate on optimizing single-response generation via supervised fine-tuning, with BioMistral [13], medical adaptation of Mistral [14], demonstrating particularly strong few-shot performance on clinical benchmarks, which implicitly presumes that users can offer comprehensive symptom descriptions. Nevertheless, this assumption contradicts real-world scenarios where patients typically initial with vague complaints. Although models such as HuaTuo [15] and UltraMedical [16] enhance 2 Figure 1: Comparison of the average accuracy scores for diagnosis and recommendation, as well as the average interaction rounds, between DoctorAgent-RL and other models. answer quality by incorporating medical knowledge graphs, they still function reactively, compelling patients to serve as self-diagnosticians by interpreting general advice like consult doctor. This limitation arises from framing medical QA as language generation task rather than sequential decision process. Current approaches prioritize maximizing token-level prediction accuracy but fail to model the clinicians key challenge of strategically eliciting critical information through adaptive questioning [17]. Thus, even state-of-the-art systems struggle with ambiguous cases where diagnosis relies on iterative hypothesis refinement, capability beyond single-turn interaction design. 2.2 Multi-turn Dialogue Systems Although systems like Bianque [18] and DialoGPT [19] attempt to introduce multi-turn capabilities through supervised fine-tuning on synthetic or annotated dialogue datasets, they remain constrained by static training paradigms. The standardized multiple-choice evaluation framework [20] aims to mitigate LLMs hallucination risks through structured testing, yet its scripted key-value dialogue protocol fails to capture authentic doctor-patient interactions. Recent approaches like the APP system [21] propose entropy-minimized diagnostic optimization, achieving higher symptom recall than conventional methods through medical guideline grounding. MDDial [22] contributes specialized dataset of 1,200 expert-annotated diagnostic dialogues with turn-level reliability scores, though its template-based generation limits linguistic diversity and complexity, failing to fully reflect the richness of real clinical conversations. These methods still rely on predefined dialogue paths rather than learning optimal questioning strategies from clinical outcomes. While systems incorporating turn reliability scoring and iterative differential diagnosis surpass single-turn response capabilities, they fundamentally suffer from static training data limitations and lack mechanisms to optimize dialogue strategies based on interaction results [23]. Consequently, their ability to ask the right questions at the right time remains inadequate. 2.3 Multi-Agent Systems in Medicine Recent advances in medical multi-agent systems have demonstrated significant potential in clinical reasoning simulation and diagnostic accuracy improvement, yet fundamental limitations persist in achieving human-centric adaptability. AMIE [24] optimize diagnostic dialogues through selfplay simulation environments, outperforming primary care doctors in structured evaluations, but their reliance on static training data restricts adaptability to novel clinical scenarios. The MAC 3 framework [25] enhances rare disease diagnosis by simulating multidisciplinary team discussions, yet its template-driven dialogue patterns lack linguistic diversity and contextual flexibility. Agent Hospital [26] incorporate dynamic medical record libraries and LLM-generated disease progression models to evolve agent behaviors, though the generated experiences carry risks of factual inconsistencies. While the AI Hospital framework [27] improves diagnostic accuracy through multi-role collaboration protocols, it fails to adequately account for the diversity of treatment options and the effectiveness of alternative strategies in real clinical settings. Current systems neither achieve dynamic integration of real-time medical evidence nor adjust questioning depth based on patient comprehension levels, resulting in suboptimal information delivery efficiency. 2.4 Reinforcement Learning in Medicine The application of RL in the medical field is transitioning from static decision-making to dynamic interaction paradigms, demonstrating unique value in optimizing strategies for multi-turn consultation systems. MedVLM-R1 [28] and Med-R1 [29] employ the Group Relative Policy Optimization (GRPO) [30] framework, which integrates RL reward mechanisms with radiological imaging feature analysis to incentivize vision-language models in generating interpretable reasoning pathways. This approach significantly enhances diagnostic accuracy and reduces hallucinatory reasoning, yet exhibits limited adaptability to emerging modalities. HuatuoGPT-o1 [31] enhances clinical reasoning through verifiable question generation and medical validation feedback mechanisms, but its reliance on multiple-choice question data conversion limits adaptability to unstructured symptom descriptions. MedRIA [32] employs actor-critic frameworks to optimize inquiry efficiency in emergency scenarios, yet requires manual feature engineering for complex diagnoses. PPME [33] leverages clinical experience replay to prioritize high-value diagnostic pathways, achieving domain-specific accuracy gains at the cost of poor generalization. These studies collectively indicate that RL requires further refinement in dynamic interaction objective modeling and clinically oriented evaluation frameworks to address the complexity and uncertainty inherent in real-world medical practice."
        },
        {
            "title": "3 Method",
            "content": "3.1 Dataset and Evaluation Metrics 3.1.1 Dataset creation process We present MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. The dataset comprises 8,086 training samples and 2,082 test samples derived from three Chinese benchmark datasets: IMCS21 [34], CHIP-MDCFNPC [35], and MedDG [36]. The test set covers 8 major disease categories, with detailed distribution statistics provided Appendix B. As the source datasets were collected from real doctor-patient conversations, we implemented two-stage denoising strategy: (1) filtering shallow dialogues with less than three turns through exact turn-count matching, and (2) removing noisy segments containing consecutive meaningless responses using DeepSeek-V3 [37]. We strictly preserved the original data partitioning protocol during this cleaning process to ensure evaluation reliability. The retained dialogues were then translated into English using DeepSeek-V3. The test set serves dual evaluation purposes: 1) assessing doctor agents questioning and diagnostic capabilities using complete dialogue trajectories with gold-standard diagnosis labels; 2) evaluating patient agents response quality through randomly selected subset of 500 samples. 3.1.2 Evaluation metrics The doctor agent evaluation focuses on two core dimensions: (1) Diagnosis and Recommendation Accuracy: This dimension evaluates eight disease categories using 6-level quantitative matching scale (0=completely incorrect, 5=exact match). Semantic consistency between the agents diagnosis/recommendations and gold-standard labels is assessed via Qwen2.5-32B-Instruct [38], with specific prompts shown in Appendix C. The final score is converted to 100-point scale by multiplying the raw score by 20; (2) Interaction Turns: The average number of dialogue turns required for the doctor agent and the patient agent to reach final diagnosis. 4 The patient agent assessment employs three-dimensional DeepSeek-V3-based scoring system: (1) Information Control: Quantifies unsolicited information disclosure through baseline score of 1.0 (perfect compliance), with 0.2-point deductions per unauthorized disclosure instance; (2) Response Completeness: Evaluates critical information omission against doctor queries using an initial score of 1.0, penalized by 0.2 points per essential detail omission; (3) Factual Conflict: Detects contradictions with medical records through violation counter starting at 0.0, accumulating 0.2 points per identified inconsistency. Patient agents rating prompts shown in Appendix C. 3.2 Task Formulation We model the multi-round medical consultation process as multi-agent collaborative RL system, where the doctor agent serves as the main agent for strategy optimization, the patient agent acts as collaborative counterpart to form dynamic game relationship with it, and the consultation evaluator functions as neutral arbiter that coordinates doctor-patient interactions and guides the optimization process of the doctor agents strategy through well-designed reward mechanisms. 3.2.1 Doctor Agent As the decision-making doctor agent, its state space st encompasses the dialogue history Ht, providing comprehensive record of the consultation. The agents actions are drawn from the action space = {aquery, adiagnose}, which includes two distinct behaviors: generating medical inquiries and executing diagnostic decisions. Environmental dynamics are governed by the patient agent through state transitions: st+1 (st+1st, at) Here, represents the transition probability function, which quantifies the likelihood of moving to new state st+1 given the current state st and the action at taken by the doctor agent. The consultation continues until either predefined limit on conversation turns is reached or the doctor agent provides final diagnosis, at which point complete dialogue history HT forms the consultation trajectory. The reward signal for each trajectory is provided by an independent consultation evaluator (detailed in Section 3.2.3), considering diagnostic accuracy, information acquisition efficiency, and protocol compliance as metrics. To enhance the stability of policy optimization and eliminate the requirement for an additional value function approximation, we propose Group Relative Policy Optimization (GRPO) [30] as our policy gradient algorithm. Unlike Proximal Policy Optimization (PPO) [39], GRPO employs the average reward of multiple sampled outputs as baseline instead of relying on learned value function. Specifically, for each patients consultation x, GRPO samples set of trajectories y1, y2, . . . , yG through the interaction between the doctor agent πD and the patient agent πp. The doctor agent, as the policy model, is then optimized by maximizing the following objective function: JGRP O(θ) = xD,{yi}G i=1 πDold (x;πp ) (cid:34) 1 (cid:88) i=1 1 yi (cid:88) (cid:80)yi t=1 I(yi,t) t=1:I(yi,t)=1 (cid:32) min πD(yi,tx, yi,<t) πDold (yi,tx, yi,<t) ˆAi,t, (cid:32) clip πD(yi,tx, yi,<t) πDold (yi,tx, yi,<t) (cid:33) (cid:33) , 1 ϵ, 1 + ϵ ˆAi,t βDKL (cid:104) πDπDref (cid:105) (cid:35) , (1) where ϵ and β are hyperparameters, and ˆAi,t represents the advantage, computed based on the relative rewards of outputs within each group. Here, I(yi,t) = 1 indicates that the token yi,t is generated by πD. Since doctors cannot predict patients symptoms in advance, during training, the responses of the patient agent are masked. 3.2.2 Patient Agent The patient agent is implemented using Qwen2.5-7B-Instruct [38] and is incorporated into two-phase dialogue simulation framework via carefully crafted prompt engineering. 5 In the first phase, the system combines patient self-reports and multi-turn dialogue content to create case descriptions. Additionally, it augments potential symptom features using standard diagnostic results, thereby forming more comprehensive hidden medical profile. This design effectively mitigates the symptom coverage issues stemming from incomplete doctor inquiries in traditional datasets. In the second phase, the patient agent utilizes dynamic symptom release strategies in response to the real-time queries of the doctor agent. It maintains strict pathological consistency while mimicking the natural variability in patients symptom description granularity and the order of their complaints. The detailed prompt designs for both phases are presented in Appendix E. By retaining complete hidden case data, the patient agent ensures that its natural language responses adhere to clinical standards and are generated solely based on the dialogue history. 3.2.3 Consultation Evaluator In our RL framework, guiding the doctor agent to master essential clinical diagnostic skills is paramount. To achieve this, weve designed sophisticated Consultation Evaluator, acting as multi-faceted reward system that assesses the agents performance across critical dimensions of medical consultation. This evaluator comprises three core components, each contributing to comprehensive assessment of the agents diagnostic capabilities and consultation conduct. Diagnostic Accuracy Reward. The first pillar of our Consultation Evaluator focuses on the agents diagnostic precision and treatment recommendations. To ensure the reliability of this evaluation and prevent any potential reward hacking, we employ rule-based reward mechanism. This mechanism meticulously calculates the word-level F1 score between the doctor agents predicted diagnosis and the gold-standard diagnosis, as well as for the recommended treatments. The formulation for this reward is as follows: Raccuracy = 5 (F1diagnosis + F1recommendation) (2) The coefficient 5 serves to adjust the relative weight of this crucial reward signal within the overall evaluation. This design ensures balanced and robust assessment of both the diagnostic output and the quality of suggested interventions, providing stable foundation for learning. Information Acquisition Efficiency Reward. proficient clinician knows how to ask the right questions efficiently. To foster this skill in our doctor agent, the Consultation Evaluator incorporates dynamic reward mechanism that promotes valuable questioning while discouraging repetitive or unhelpful queries. This reward is directly tied to the patient agents feedback after each dialogue turn and accumulates throughout the interaction: Rt information = (cid:26)1, 2, if the patient agent answers normally if the patient agent refuses to answer Rinformation = (cid:88) Rt information (3) (4) Through this feedback loop, the model learns to optimize its questioning strategy, prioritizing the acquisition of diagnostically relevant information and refining its inquiry process. Protocol Compliance Reward. Adherence to established clinical interview protocols is hallmark of professional medical practice. To instill this discipline, our Consultation Evaluator includes compliance reward. This component penalizes deviations from predefined norms and ensures that the agent completes the diagnostic process within specified limits: Rt compliance = 2, 5, 0, if the question format violates predefined norms if no diagnosis is provided within the allowed turns otherwise (i.e., protocol is followed) Rcompliance = Rt compliance (cid:88) 6 (5) (6) This mechanism reinforces the learning of structured interview processes and ensures the timely completion of diagnosis, closely mirroring the practical constraints and expectations of real-world clinical environments. By combining these three critical components, the total consultation evaluation score, or reward, received by the doctor agent at each time step is calculated as: = Raccuracy + Rinformation + Rcompliance (7) This sophisticated, multi-dimensional Consultation Evaluator not only guides the model toward superior diagnostic accuracy but also actively encourages the development of efficient informationgathering strategies and professional, compliant clinical behavior, ultimately aiming to achieve diagnostic capabilities that closely resemble those of expert human clinicians. 3.3 Training Procedure Figure 2: The multi-agent collaborative reinforcement learning framework for DoctorAgent-RL. During the rollout stage, multi-turn interactions are conducted between the doctor agent and the patient agent. As illustrated in Figure 2, our training framework for the doctor agent is built upon Qwen2.5-7BInstruct, following the DeepSeek-R1 training paradigm [40]. The approach employs two-stage training pipeline, integrating SFT and RL to cultivate clinical reasoning capabilities. Detailed experimental settings are illustrated in Appendix H. Specifically, we randomly select 1,000 multi-turn consultation dialogues from the training corpus. Each doctors query in the sampled dialogues is augmented with structured thought processes using DeepSeek-V3. These include hypothesis generation, evidence evaluation, and differential diagnosis steps derived from context. The doctor agent is fine-tuned on this enriched dataset to activate core capabilities in question-asking, diagnostic reasoning, and recommendation generation. Following SFT, we refine the agents decision-making under interaction constraints using the policy optimization algorithm detailed in Section 3.2. To enhance robustness and mimic real-world clinical scenarios, we introduce Dynamic Turn Budget Training Strategy. Each training episode is assigned random dialogue turn budget (210 turns). The model is explicitly reminded of the remaining turns after each interaction step, encouraging efficient information gathering. This two-stage approach ensures the agent first internalizes clinical reasoning patterns via supervised learning, then refines its strategy through interactive reward optimization. Ablation studies comparing alternative training strategies (direct SFT, direct RL, fixed-turn training) are presented in the experimental section."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Patient Agent Behavior Analysis on MTMedDialog Table 1 presents the evaluation results of the effectiveness of patient agent in simulating real-world medical interactions. In the dimension of information control, Qwen2-5.7B demonstrates optimal compliance performance, precisely constraining output content and effectively avoiding the leakage of irrelevant information. In terms of response completeness, while DeepSeek-V3 performs best, 7 Model DeepSeek-V3 HuatuoGPT-o1-7B Qwen2.5-7B-Instruct LLaMA-3.1-8B GLM-4-9B"
        },
        {
            "title": "Information Control Response Completeness",
            "content": "86.4 88.1 88.8 80.3 72.6 86.1 81.7 84.4 77.2 70.7 Factual Conflict 0.0 0.0 0.0 0.0 0.0 Table 1: Performance comparison of different models in simulating patient agent on MTMedDialog. The best results are highlighted in bold. Qwen2-5.7B still maintains near-optimal levels of key information density required for doctor-patient dialogue. Notably, all tested models achieve zero error rates in the dimension of factual conflicts, fully validating the reliability of LLMs in medical knowledge. Considering both model performance and cost-effectiveness in training, we ultimately selected Qwen2-5.7B as the implementation solution for the patient agent. For more detailed description of the dialogue flow, please refer to Appendix 4.2 Comparative Performance Evaluation of Multi-Model Approaches on MTMedDialog Experimental results in Table 2 show DoctorAgent-RL achieving comprehensive average score of 53.9%, demonstrating significant improvements over frontier models, open-source base models, and domain-specific models. More detailed experimental results can be found in Appendix F. The system particularly maintains stable advantages in disease types requiring in-depth consultation, proving its superior ability to simulate doctors clinical consultation processes. Analysis of model dialogue processes reveals key limitations in existing methods interaction quality. While all models receive explicit instructions to \"ask only one question at time,\" frontier models show strict compliance but lack professional medical consultation knowledge, often missing critical symptom information in their questions. Some open-source models (e.g., GLM-4, Mistral) achieve decent comprehensive average scores but improperly combine multiple questions, reflecting inadequate instruction-following capability that affects complex condition diagnosis. The domain-specific model BioMistral exhibits the highest interaction frequency but frequently repeats similar questions due to ineffective planning, resulting in the lowest diagnostic accuracy. These findings demonstrate that question quality - not quantity - determines diagnostic effectiveness. DoctorAgent-RL employs phased training strategy: initial fine-tuning with medical dialogue data containing detailed reasoning processes establishes systematic consultation capabilities, followed by RL that optimizes questioning strategies through simulated doctor-patient dialogues. This approach enables strict compliance with consultation norms while allowing flexible, doctor-like question adjustment based on acquired symptom information. The resulting intelligent questioning strategy significantly improves comprehensive average scores while maintaining reasonable interaction frequency. 4.3 Adaptive Fine-Tuning Strategies for Task-Specific Optimization Experimental results demonstrate the significant advantages of our proposed two-stage optimization framework (SFT + RL) in medical dialogue tasks. As shown in Table 3, DoctorAgent-RL achieves superior average score of 53.9, outperforming all baseline models. The key advantage stems from our phased strategy: (1) SFT on doctor-patient dialogues establishes reasonable questioning baselines through behavioral cloning, followed by (2) RL optimization that enables dynamic adjustment of questioning strategies for high-value information acquisition. This combined approach improves the average diagnosis and recommendation score by 25.9% compared to the base model while enhancing proactive questioning efficiency by 36.7%. Ablation studies on three critical components further validate our design: w/o Dynamic_Turn: When trained with fixed budget of turns during RL, the model shows only 1.2% performance drop in matched scenarios but reveals strategy rigidity during inferenceit mechanically adheres to the training budget of turns regardless of specified variations, proving impractical for real-world deployment. 8 Model DSD RSD ID GSD ND CSD ED SD Avg. Score Avg. Turns Frontier Models GPT-4o DeepSeek-V3 LLaMA-3.1-70B GLM-4-9B LLaMA-3.1-8B Mistral-7B-Instruct Qwen2.5-7B-Instruct BioMistral UltraMedical-8B HuatuoGPT-o1-7B AI Hospital DoctorAgent-RL (Ours) 49.5 44.8 45.4 37.5 38.8 40.3 43. 28.8 42.8 43.4 46.3 54.5 50.7 46.6 44.5 38.3 38.7 41.5 42.2 28.9 43.8 44.5 47.5 52.9 47.9 45.1 42.1 46.8 45.0 46. 35.2 37.6 41.9 43.6 39.1 39.0 44.2 39.5 52.5 48.6 46.3 44.2 45.6 44.9 Open-Source Base Models 38.3 39.7 39.6 39.3 39.8 39.8 45.5 40.8 Domain-Specific Models 25.6 45.4 49.4 44.4 57.0 29.2 39.9 44.5 44.8 55.1 29.5 44.7 43.3 46.1 50.4 23.9 41.7 41.8 46.0 52. 46.3 43.7 39.5 40.0 36.3 36.3 35.0 37.0 42.2 45.6 45.9 51.4 41.5 43.0 39.0 30.5 41.5 35.5 40.5 25.5 38.5 37.0 43.5 48. 49.4 45.2 45.0 37.5 38.8 40.6 42.6 28.6 42.9 43.7 46.3 53.9 3.8 3.3 6.0 2.1 4.8 3.4 6.3 9.1 3.5 2.0 3.9 8. Table 2: Main results by disease category on MTMedDialog dataset, showing the average of diagnostic accuracy and recommendation accuracy scores. Abbreviations: DSD (Digestive System Diseases), RSD (Respiratory System Diseases), ID (Infectious Diseases), GSD (Genitourinary System Diseases), ND (Neurological Disorders), CSD (Circulatory System Diseases), ED (Endocrine Disorders), SD (Skin Diseases). Avg. Score represents the average of diagnostic and recommendation accuracy scores across all disease categories. Avg. Turns indicates the average number of interaction turns across all disease categories. Best performing metrics are highlighted in bold. The second best results are indicated with underlines. Method DoctorAgent-RL (Ours) w/o Dynamic Turn w/o SFT w/o RL Qwen2.5-7B-Instruct DSD RSD 52.9 54.5 52.4 53.5 48.3 48.4 46.5 48.3 42.2 43.2 ID 55.1 51.4 48.4 43.6 40.8 GSD ND CSD ED 51.4 52.4 44.1 51.4 48.2 48.8 48.5 44.5 35.0 43.6 57.0 51.8 50.0 50.2 45. 50.4 50.9 49.5 49.0 39.5 SD Avg. Score Avg. Turns 48.0 45.0 43.0 36.0 40.5 53.9 52.7 48.4 47.4 42.6 8.6 8.5 5.8 9.0 6.3 Table 3: Performance comparison of different fine-tuning methods for Qwen2.5-7B-Instruct on MTMedDialog. The best results are highlighted in bold. w/o SFT: Direct RL training without SFT initialization causes 5.5% average score degradation with the lowest average turns. While capable of planning effective questions for information gathering, the model demonstrates insufficient initiative in question generation due to the absence of behavioral cloning foundations. w/o RL: SFT-only training results in 6.5% lower average score despite having the highest turn count. The model memorizes question sequences from training data without truly understanding diagnostic logic, leading to mechanical questioning rather than strategic information acquisition. The comprehensive performance of DoctorAgent-RL confirms each components necessity: SFT establishes reliable behavioral baselines, RL injects dynamic decision-making capability, and adaptive turn mechanisms ensure strategic flexibilitytogether forming reproducible paradigm for taskoriented medical dialogue optimization. 4.4 Impact Analysis of Budget of Turns on Diagnosis and Recommendation Performance The experimental results shown in Figure 3 demonstrate that as the budget of turns increases, the average performance of diagnosis and recommendation exhibits distinct two-phase characteristic: in the initial phase (low-turn range), performance rises rapidly with additional turns, primarily due to the multi-turn dialogue mechanism enabling the LLM to progressively collect and refine patient information through iterative questioning; in the intermediate phase (medium-to-high turn range), the performance curves slope noticeably flattens as the valuable information patients can provide gradually becomes saturated, making it difficult for the LLM to extract more meaningful information through additional questioning. Nevertheless, on the whole, larger budget of turns still leads to better performance, as additional interaction opportunities can capture potential subtle information differences. It is worth noting that diagnostic performance consistently outperforms recommendation performance, as diagnosis tasks can progressively confirm symptom characteristics through multi-turn interactions, while recommendation tasks rely more on established medical knowledge bases, leaving relatively limited room for performance improvement. Diagnosis Score Recommendation Score Average Score 60 50 c 45 0 2 6 4 Budget of Turns 8 10 Figure 3: Comparative analysis of Diagnosis Score, Recommendation Score, and the resulting Average Score of DoctorAgent-RL on MTMedDialog at different levels of the Budget of Turns."
        },
        {
            "title": "5 Conclusion and future work",
            "content": "We propose DoctorAgent-RL, multi-agent collaborative reinforcement learning framework that establishes an innovative paradigm enabling LLMs to progressively refine diagnoses through proactive questioning. The framework effectively addresses the unrealistic requirement of conventional systems for patients to fully describe symptoms in single interaction by synergistically integrating high-fidelity patient agent simulating pathologically consistent symptom expressions with doctor agent optimizing adaptive questioning strategies through reinforcement learning, guided by an evaluator providing multidimensional clinical reward signals. Experimental results demonstrate that DoctorAgent-RL significantly outperforms traditional methods in diagnostic accuracy. These findings not only mark paradigm shift in medical AI from static Q&A to dynamic reasoning, but also provide clinicians with interpretable decision support through its proactive questioning strategies enabled by multi-turn interactions, while simultaneously pioneering new technical pathways for patient-led symptom screening. Future research will focus on advancing multimodal medical reasoning capabilities by integrating heterogeneous data sources such as medical imaging, pathological slides, and real-time physiological signals from wearable devices to enhance the models comprehensive understanding of complex conditions. Concurrently, systematic efforts are needed to address data bias and decision transparency, establishing fairness evaluation framework that accounts for demographic characteristics and disease spectrum variations to ensure model robustness and interpretability across diverse patient populations. Throughout technological iterations, an ethical governance framework for medical AI must be developed through interdisciplinary research, addressing critical issues such as diagnostic accountability, privacy and data security, and doctor-patient trust mechanisms, ultimately achieving dynamic balance between technological innovation and adherence to medical ethics and patient safety standards."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 10 [2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [3] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. [4] Qingyu Chen, Yan Hu, Xueqing Peng, Qianqian Xie, Qiao Jin, Aidan Gilson, Maxwell Singer, Xuguang Ai, Po-Ting Lai, Zhizheng Wang, et al. Benchmarking large language models for biomedical natural language processing applications and recommendations. Nature Communications, 16(1):3280, 2025. [5] Marvin Kopka, Niklas von Kalckreuth, and Markus Feufel. Accuracy of online symptom assessment applications, large language models, and laypeople for selftriage decisions. npj Digital Medicine, 8(1):178, 2025. [6] Chaoyi Wu, Pengcheng Qiu, Jinxin Liu, Hongfei Gu, Na Li, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards evaluating and building versatile large language models for medicine. npj Digital Medicine, 8(1):58, 2025. [7] Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, and Jennifer Neville. Llms get lost in multi-turn conversation. arXiv preprint arXiv:2505.06120, 2025. [8] Andrew Auerbach, Tiffany Lee, Colin Hubbard, Sumant Ranji, Katie Raffel, Gilmer Valdes, John Boscardin, Anuj Dalal, Alyssa Harris, Ellen Flynn, et al. Diagnostic errors in hospitalized adults who died or were transferred to intensive care. JAMA Internal Medicine, 184(2):164173, 2024. [9] Ruoyu Liu, Kui Xue, Xiaofan Zhang, and Shaoting Zhang. Interactive evaluation for medical llms via task-oriented dialogue system. In Proceedings of the 31st International Conference on Computational Linguistics, pages 48714896, 2025. [10] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. Nejm Ai, 1(3):AIoa2300138, 2024. [11] Tianyu Han, Lisa Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno Bressem. Medalpacaan open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023. [12] Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, et al. Meddialog: Large-scale medical dialogue datasets. In Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 92419250, 2020. [13] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. Biomistral: collection of open-source pretrained large language models for medical domains. arXiv preprint arXiv:2402.10373, 2024. [14] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. arXiv preprint, arXiv:2310.06825, 2023. [15] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo: Tuning llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975, 2023. [16] Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, et al. Ultramedical: Building specialized generalists in biomedicine. Advances in Neural Information Processing Systems, 37:2604526081, 2024. [17] Harsha Nori, Naoto Usuyama, Nicholas King, Scott Mayer McKinney, Xavier Fernandes, Sheng Zhang, and Eric Horvitz. From medprompt to o1: Exploration of run-time strategies for medical challenge problems and beyond. arXiv preprint arXiv:2411.03590, 2024. 11 [18] Yirong Chen, Zhenyu Wang, Xiaofen Xing, Zhipei Xu, Kai Fang, Junhong Wang, Sihang Li, Jieling Wu, Qi Liu, Xiangmin Xu, et al. Bianque: Balancing the questioning and suggestion ability of health llms with multi-turn health conversations polished by chatgpt. arXiv preprint arXiv:2310.15896, 2023. [19] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint arXiv:1911.00536, 2019. [20] Yusheng Liao, Yutong Meng, Hongcheng Liu, Yanfeng Wang, and Yu Wang. An automatic evaluation framework for multi-turn medical consultations capabilities of large language models. arXiv preprint arXiv:2309.02077, 2023. [21] Jiayuan Zhu and Junde Wu. Ask patients with patience: Enabling llms for human-centric medical dialogue with grounded reasoning. arXiv preprint arXiv:2502.07143, 2025. [22] Srija Macherla, Man Luo, Mihir Parmar, and Chitta Baral. Mddial: multi-turn differential diagnosis dialogue dataset with reliability evaluation. arXiv preprint arXiv:2308.08147, 2023. [23] Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan Ilgen, Emma Pierson, Pang Wei Koh, and Yulia Tsvetkov. Mediq: Question-asking llms and benchmark for reliable interactive clinical reasoning. Advances in Neural Information Processing Systems, 37:2885828888, 2024. [24] Tao Tu, Mike Schaekermann, Anil Palepu, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Yong Cheng, et al. Towards conversational diagnostic artificial intelligence. Nature, pages 19, 2025. [25] Xi Chen, Huahui Yi, Mingke You, WeiZhi Liu, Li Wang, Hairui Li, Xue Zhang, Yingman Guo, Lei Fan, Gang Chen, et al. Enhancing diagnostic capability with multi-agents conversational large language models. NPJ digital medicine, 8(1):159, 2025. [26] Junkai Li, Yunghwei Lai, Weitao Li, Jingyi Ren, Meng Zhang, Xinhui Kang, Siyu Wang, Peng Li, Ya-Qin Zhang, Weizhi Ma, et al. Agent hospital: simulacrum of hospital with evolvable medical agents. arXiv preprint arXiv:2405.02957, 2024. [27] Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, and Jingren Zhou. Ai hospital: Benchmarking large language models in multi-agent medical interaction simulator. arXiv preprint arXiv:2402.09742, 2024. [28] Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning. arXiv preprint arXiv:2502.19634, 2025. [29] Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. arXiv preprint arXiv:2503.13939, 2025. [30] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [31] Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms. arXiv preprint arXiv:2412.18925, 2024. [32] Xuan Zou, Weijie He, Yu Huang, Yi Ouyang, Zhen Zhang, Yu Wu, Yongsheng Wu, Lili Feng, Sheng Wu, Mengqi Yang, et al. Ai-driven diagnostic assistance in medical inquiry: Reinforcement learning algorithm development and validation. Journal of Medical Internet Research, 26:e54616, 2024. [33] Zhoujian Sun, Ziyi Liu, Cheng Luo, Jiebin Chu, and Zhengxing Huang. Improving interactive diagnostic ability of large language model agent through clinical experience learning. arXiv preprint arXiv:2503.16463, 2025. [34] Wei Chen, Zhiwei Li, Hongyi Fang, Qianyuan Yao, Cheng Zhong, Jianye Hao, Qi Zhang, Xuanjing Huang, Jiajie Peng, and Zhongyu Wei. Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets. Bioinformatics, 2022. 12 [35] Ningyu Zhang, Mosha Chen, Zhen Bi, Xiaozhuan Liang, Lei Li, Xin Shang, Kangping Yin, Chuanqi Tan, Jian Xu, Fei Huang, et al. Cblue: chinese biomedical language understanding evaluation benchmark. arXiv preprint arXiv:2106.08087, 2021. [36] Wenge Liu, Jianheng Tang, Yi Cheng, Wenjie Li, Yefeng Zheng, and Xiaodan Liang. Meddg: an entity-centric medical consultation dataset for entity-aware medical dialogue generation. In CCF International Conference on Natural Language Processing and Chinese Computing, pages 447459. Springer, 2022. [37] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [38] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. [39] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [40] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [41] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), 2024. [42] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. International Conference on Learning Representations, 2022. [43] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [44] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. [45] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [46] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [47] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025."
        },
        {
            "title": "A Comparison of Diagnostic Consultation Paradigms",
            "content": "This section introduces the comparison of different diagnostic consultation paradigms, as shown in Figure 4. Figure 4: Comparison of diagnostic consultation paradigms. (A) Single-Turn Consultation: Direct diagnosis based on initial query without follow-up interactions, prone to misdiagnosis due to insufficient symptom clarification. (B) SFT Multi-Turn Consultation: Implements basic multi-turn dialogue through supervised fine-tuning but lacks adaptive optimization. (C) SFT-RL Multi-Turn Consultation: Integrates RL with real-world feedback to dynamically optimize questioning strategies, enabling context-aware symptom investigation (e.g., pain nature, duration, triggers) and reducing diagnostic errors through iterative interaction."
        },
        {
            "title": "B Details of MTMedDialog",
            "content": "This section presents the statistical distribution of disease categories in the MTMedDialog test set. We employed the DeepSeek-V3 to automatically classify the diagnostic results of each data entry, strictly adhering to predefined eight-category disease classification system. Samples that did not conform to this classification framework were subsequently removed, resulting in final collection of 2,082 high-quality test samples. Detailed data are shown in Table 4. 14 Disease Category Digestive System Diseases Respiratory System Diseases Infectious Diseases Genitourinary System Diseases Neurological Disorders Circulatory System Diseases Endocrine Disorders Skin Diseases Number of Cases 1,290 402 118 100 77 48 27 20 Table 4: Disease Category Distribution in MTMedDialog Test Set"
        },
        {
            "title": "C Prompts for Evaluation Metrics",
            "content": "This section will provide detailed explanation of the evaluation prompts for both the doctor agent and patient agent. The evaluation prompt for the doctor agents Diagnosis and Recommendation Accuracy is presented in Figure 5, while the evaluation prompt for the patient agent is shown in Figure 6. Prompt for Evaluating the Doctor Agents Diagnosis and Recommendation Accuracy Task: As medical expert, evaluate the semantic similarity between the modelgenerated medical text and the ground truth reference. Score on 05 point scale based on meaning alignment (wording differences are acceptable if meaning matches). Criteria: 5: Identical meaning (different wording okay). 4: Minor wording/detail differences; overall meaning aligned. 3: Partial meaning overlap; important differences exist but core intent is partially shared. 2: Limited meaning overlap; key details or context differ significantly. 1: Minimal meaning overlap; mostly unrelated or only superficially related. 0: Unrelated or completely different meaning (no meaningful semantic connection). Now evaluate: Candidate: \"{candidate}\" Reference: \"{reference}\" OUTPUT FORMAT: <think> [Your Thinking Process] </think><answer> [Your Score] </answer> Figure 5: Prompt for Evaluating the Doctor Agents Diagnosis and Recommendation Accuracy. {candidate} represents the model-generated medical text (doctor agents output), while {reference} denotes the ground truth clinical reference standard."
        },
        {
            "title": "D Prompt for Doctor Agent",
            "content": "This section elaborates on the prompt design for the doctor agent during both training and inference phases. The complete prompt structure is illustrated in Figure 7."
        },
        {
            "title": "E Prompt for Patient Agent",
            "content": "This section first details the patient agents implicit disease knowledge generation mechanism, with the complete prompt structure shown in Figure 8. Subsequently, it examines the agents prompt design methodology for both training and inference phases, as fully illustrated in Figure 9."
        },
        {
            "title": "Prompt for Evaluating the Patient Agent",
            "content": "You are evaluating the quality of patients simulated responses during medical consultation. Original self-report: {self_report} Diagnosis: {diagnosis} Recommendation: {recommendation} Doctors questions and simulated patient answers: {simulated_dialogue} Instructions: 1. Information Control Rate (0-1): - Check whether the patients answers include extra information not asked by the doctor. - Deduct 20% for each extra information point. - Score = max(0, 1 - 0.2 number_of_extra_points) 2. Response Completeness Rate (0-1): - Check whether the patient answered all points asked by the doctor. - Deduct 20% for each missing information point. - Score = max(0, 1 - 0.2 number_of_missing_points) 3. Factual Conflict Rate (0-1): - Check if the patients response is completely opposite to the original selfreport. - Increase 20% for each completely opposite found. - Score = min(1, 1 - 0.2 number_of_opposite) Return ONLY in JSON format: { \"information_control_rate\": float, \"response_completeness_rate\": float, \"factual_conflict_rate\": float } Figure 6: Prompt for Evaluating the Patient Agent. Detailed Comparative Performance Evaluation of Multi-Model Approaches on MTMedDialog The experimental results of diagnostic accuracy are shown in Table 5. DoctorAgent-RL achieved an average diagnostic accuracy of 58.9% on the MTMedDialog dataset, showing significant advantage over the comparison models. This result indicates that multi-turn questioning trained with RL can gather more patient information and effectively improve disease identification accuracy. The experimental results of recommendation accuracy are shown in Table 6. DoctorAgent-RL outperformed the baseline models with an average recommendation accuracy of 48.9%. Notably, the AI Hospital model showed partial advantages in the recommendation tasks for skin diseases and endocrine diseases, possibly due to the smaller test sample sizes for these diseases, which amplified the output randomness of pre-trained models. In contrast, the multi-agent collaborative architecture of AI Hospital, through parallel interactions, more easily obtained effective solutions via probability sampling. Nevertheless, DoctorAgent-RL still maintained the best overall accuracy. All models exhibited higher diagnostic accuracy than recommendation accuracy. This stems from the core differences between the two tasks. Disease diagnosis, as symptom-driven reasoning process, has target space that, while not predefined as closed set, converges due to pathological constraints. In contrast, treatment recommendation generation requires coordinating multi-dimensional parameters such as drugs, dosages, and treatment durations, leading to an exponentially expanded space of"
        },
        {
            "title": "Prompt for Doctor Agent",
            "content": "You are an experienced doctor who needs to provide professional diagnosis and advice to patients through consultation. Please listen carefully to the patients description, ask targeted questions, and collect sufficient information before giving diagnosis and treatment recommendation. Quick Guide Objectives: 1. Obtain key information through effective questioning, each round of questions should be modified based on the previous rounds content, meaning you shouldnt ask similar questions. 2. Comprehensively analyze the patients condition to provide an accurate diagnosis and appropriate treatment recommendations. Rules: 1. You can only choose one of the options to respond, you cannot both answer questions and provide diagnosis simultaneously. 2. Absolutely do not repeat or ask questions similar or identical to those previously asked. Response: <think> [your thinking] </think> <answer>If you believe there is insufficient information, please only ask one question, in this format: Question: (your question). </answer> <answer>If you believe you have obtained enough information, please only provide diagnosis and recommendations, in this format: Diagnosis: (the patients most likely disease or symptoms) Recommendation: (corresponding treatment plan or advice) </answer> Rewards: Incorrect format: -2.0 Effective question (patient can provide an answer and the question is helpful for diagnosis): +1.0 Ineffective questions do not count towards score Repeated questions: -2.0 The number of conversation turn is limited. Reaching maximum interaction rounds without providing diagnosis: -5.0 Completely correct diagnosis and recommendations: +10.0 Figure 7: Prompt for Doctor Agent. possible solutions. Particularly in open scenarios, treatment decisions may have multiple equivalent solutions (e.g., drug substitution therapies), while the evaluation criteria only adopt the optimal path from clinical guidelines, objectively increasing the matching difficulty of the recommendation task. Overall, our method uses RL to teach LLM to dynamically plan questioning paths and infer reasonable answers, achieving state-of-the-art performance in both tasks. Detailed Description of the Dialogue Flow in DoctorAgent-RL This section provides detailed explanation of the dialogue flow in DoctorAgent-RL, as illustrated in Figure 10. The framework aims to simulate realistic diagnostic process by coordinating structured dialogue flow between two agents: the doctor and the patient. This appendix comprehensively describes the mechanisms for dialogue initiation, progression, and termination."
        },
        {
            "title": "Prompt for Patient Agent to Develop a Comprehensive Implicit Health Profile",
            "content": "As medical assistant, expand the patients symptom description based on: Original self-report: {self_report} Dialogue history: {dialogue_history} Diagnosis: {diagnosis} Recommendation: {recommendation} Processing Rules: 1. Summarize the patients information: Combine the Original self-report and all patient responses from dialogue into single coherent paragraph. Include only factual patient statements and exclude the doctors questions. If patient response only makes sense in the context of the doctors question, infer its meaning based on the context. 2. Based on diagnosis and recommendations, add medical evidence to clearly support symptoms. 3. Never contradict the patients original statements. 4. Keep the language natural and clinical. 5. Return ONLY the enhanced description. Figure 8: Prompt for Patient Agent to Develop Comprehensive Implicit Health Profile. Prompt for Patient Agent Training and Inference You are interacting with doctor. Medical Response Instructions: Answer each medical question concisely in one sentence, strictly describing symptoms while avoiding any mention of diagnoses or recommendations. If the question is unrelated to your chief complaint, state: \"Sorry, cannot answer this question.\" If the question is repetitive, reply: \"Sorry, youve already asked this question.\" Your chief complaint: {description} doctors question history: {history_questions} Current doctor question: {question} Output format: <think>[Your reasoning]</think><answer>[Your response]</answer> Figure 9: Prompt for Patient Agent Training and Inference. The diagnostic process unfolds through multiple rounds of interaction between the two agents. The patients self-report serves as the starting point for the first round of dialogue between the doctor and patient agents. During this phase, the doctor agent actively conducts comprehensive inquiry, collecting diagnostic information through targeted questioning. The patient agent, functioning as non-player character (NPC), provides feedback on their condition to the doctor agent in each dialogue round based on carefully designed prompts. The diagnostic phase concludes when either of the following conditions is met: (1) The doctor agent determines that the collected information is sufficient for diagnosis and directly outputs the diagnostic 18 Model DSD RSD ID GPT-4o DeepSeek-V3 LLaMA-3.1-70B GLM-4-9B LLaMA-3.1-8B Mistral-7B-Instruct Qwen2.5-7B-Instruct BioMistral UltraMedical-8B HuatuoGPT-o1-7B AI Hospital Ours 52.6 47.3 46.4 43.8 36.8 38.2 47.8 29.0 42.8 43.5 47.0 59. 54.9 49.5 46.6 45.1 38.5 40.2 47.2 29.8 45.3 46.3 48.9 58.3 GSD ND CSD ED Frontier Models 50.0 48.6 48.6 53.5 52.0 43.4 51.9 43.0 38. 50.5 49.7 48.8 46.3 36.8 37.0 44.7 41.4 37.2 38.0 50.6 47.8 36.9 45.7 41.3 58.3 52.5 49.1 Open-Source Base Models 46.7 40.0 39.6 52.7 Domain-Specific Models 25.8 49.2 50.8 46.7 65.5 23.4 38.2 41.8 46.2 55. 32.5 47.0 48.3 48.6 54.3 28.0 38.5 43.2 45.1 61.2 43.7 37.8 34.1 40.9 40.7 42.2 45.9 44.4 58.1 SD Avg. Diag Avg. Turns 46.0 45.0 41. 40.0 47.0 36.0 46.0 23.0 43.0 40.0 40.0 58.0 52.6 48.2 46.4 44.3 37.5 38.7 47.3 29.0 43.1 44.4 47.2 58.9 3.8 3.3 6. 2.1 4.8 3.4 6.3 9.1 3.5 2.0 3.9 8.6 Table 5: Mean diagnostic scores by disease category on MTMedDialog. The best results are highlighted in bold. The second best results are indicated with underlines. Model DSD RSD ID GPT-4o DeepSeek-V3 LLaMA-3.1-70B GLM-4-9B LLaMA-3.1-8B Mistral-7B-Instruct Qwen2.5-7B-Instruct BioMistral UltraMedical-8B HuatuoGPT-o1-7B AI Hospital Ours 46.9 42.3 44.5 30.5 40.8 42.5 38.6 28.5 42.7 43.3 45.6 49. 46.5 43.7 42.4 31.4 39.0 42.7 37.2 28.0 42.3 42.7 46.0 47.6 GSD ND CSD ED Frontier Models 43.6 41.4 43.6 46.7 40.0 40.6 42.3 38.2 40. 40.7 44.4 40.5 Open-Source Base Models 30.4 36.9 42.6 37.6 29.0 38.0 45.8 36.6 30.0 39.2 40.0 38.2 Domain-Specific Models 25.4 41.7 47.9 42.1 48.5 29.5 42.3 38.2 43.6 46. 24.4 45.2 38.4 45.8 49.0 36.3 34.8 38.5 29.0 33.3 42.2 45.2 47.4 44.8 46.8 38.8 42.4 33.1 41.9 42.7 36.9 30.3 41.4 45.8 44.6 49. SD Avg. Recom Avg. Turns 37.0 41.0 37.0 21.0 36.0 35.0 35.0 28.0 34.0 34.0 47.0 38.0 46.3 42.1 43.6 30.7 40.1 42.5 37. 28.2 42.6 42.9 45.5 48.9 3.8 3.3 6.0 2.1 4.8 3.4 6.3 9.1 3.5 2.0 3.9 8.6 Table 6: Mean recommendation scores by disease category on MTMedDialog. The best results are highlighted in bold. The second best results are indicated with underlines. results. (2) The predefined maximum number of interaction rounds is reached, compelling the doctor agent to provide final diagnosis. This mechanism ensures structured and finite diagnostic process. Settings of RL training in DoctorAgent-RL Hardware & Software. All training and evaluation were performed on system featuring eight NVIDIA A100 80GB PCIe GPUs, an Intel Xeon Platinum 8369B 32-Core Processor, and 1.0 TB of RAM. For supervised fine-tuning, we utilized the LLaMA-Factory framework [41] to fine-tune with LoRA [42]. Our DoctorAgent-RL, is built upon the VERL framework (v0.2) [43] for reinforcement learning with language models, with RAGEN [44] providing the multi-turn RL architecture. We employed vLLM (v0.8.5) [45] for efficient LLM inference and evaluation, PyTorch (v2.4.0) with CUDA 12.4 for deep learning, and Ray for distributed training and serving. Flash Attention 2 [46] was integrated to optimize attention computation. Hyperparameter. For completeness and reproducibility, all hyperparameters employed in DoctorAgent-RL are detailed in Table 7. We observed that the decoupled clip approach from DAPO [47] significantly enhances exploration during RL training, and thus, we adopted it for our final training process. 19 Figure 10: An example of dialogue flow among Doctor and Patient in DoctorAgent-RL. Supervised Fine-Tuning DoctorAgent-RL learning rate batch size epochs lora_rank 1e-4 64 3 8 actor learning rate state masking kl loss coef kl penalty entropy coeff clip high clip low batch size epochs rollout group size rollout temperature 1e-6 true 0.001 low_var_kl 0.001 0.28 0.2 128 1 8 0.7 Table 7: Hyperparameters for Supervised Fine-Tuning and Reinforcement Learning experiments."
        }
    ],
    "affiliations": [
        "Department of EEIS, University of Science and Technology of China",
        "Guangzhou National Laboratory",
        "School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences",
        "Shanghai Institute of Nutrition and Health, Chinese Academy of Sciences"
    ]
}