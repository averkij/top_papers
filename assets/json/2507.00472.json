{
    "paper_title": "ARIG: Autoregressive Interactive Head Generation for Real-time Conversations",
    "authors": [
        "Ying Guo",
        "Xi Liu",
        "Cheng Zhen",
        "Pengfei Yan",
        "Xiaoming Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Face-to-face communication, as a common human activity, motivates the research on interactive head generation. A virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be real-time and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as a non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectional-integrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model."
        },
        {
            "title": "Start",
            "content": "ARIG: Autoregressive Interactive Head Generation for Real-time Conversations Ying Guo*, Xi Liu*, Cheng Zhen, Pengfei Yan, Xiaoming Wei Vision AI Department, Meituan https://jinyugy21.github.io/ARIG/ 5 2 0 2 ] . [ 1 2 7 4 0 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Face-to-face communication, as common human activity, motivates the research on interactive head generation. virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be realtime and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectionalintegrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model. 1. Introduction Face-to-face communication is common human activity around us [32]. Research on head generation makes it possible for virtual agents to become communication participants. Early works focus on single-role generation (talking/listening). In talking head generation, the input signal *Equal Contribution Corresponding Author Figure 1. Development of interactive head motion generation. a) Only speaker generation based on last speaking behaviors. b) Different explicit generators for listener/speaker. c) Implicit clip-byclip generator based on unified model. d) Implicit frame-byframe generator based on unified non-quantized AR model. comes only from the speaker himself and does not receive signals from the other participant for interaction. The listening head generation receives signals from both participants to provide appropriate listener responses, but it only offers non-verbal motion feedback and cannot generate speaker responses. In real conversations, each participants response should integrate the behaviors of both parties and possess the ability to both speak and listen. Thus, single-role generation cannot fully simulate behaviors in conversations. Recently, some works [19, 27, 29, 30, 37, 39] have studied the interactive head generation (IHG), as shown in Fig. 1. Suppose the conversation between user and virtual agent, IHG generates interactive head responses for the agent by utilizing available information from both parties. Initially, early works (Fig. 1a) such as MRecGen [30] and [19] model the conversation as multi-turn mode and only generate the agents speaker response based on the users speaking behavior in the last turn. Then, methods (Fig. 1b) in [37], DIM [27] and AgentAvatar [29] can generate interactive responses of both speakers and listeners. However, they have to assign explicit roles (speaker/listener) for the agent and switch between two respective generators. Conversations involve more complex states beyond speaker/listener alternation (e.g. interruption, pause, giving verbal feedback), requiring immediate and smooth role switching, and sometimes even both parties play the same role, which is difficult to achieve with explicit role switching. To solve this, in Fig. 1c, INFP [39] uses unified generator to switch roles implicitly and dynamically. Although its inference speed is ideal, it is clip-wise paradigm that inputs two fixed-length clips (TT + n) of audio and output motions within + n. At time , since the signal about the future + cannot be acquired, the model must wait until the audio is acquired at + and then forms clip to generate the motion at , which will cause delay of at least one clip, making real-time generation difficult. Besides, for interaction realism, the clip-wise paradigm limits the visible range to short clips. Without long context, it may misinterpret the logic of statements and the persons intent, resulting in unreasonable motions. Moreover, its input in motion guider contains only audio without visual information, which may not be sufficient to fully represent the behavior of conversation participant, who may interact in many non-audio ways such as frowning and nodding. To solve these problems, we propose novel framework called ARIG based on AutoRegression to redefine the Interaction Generation paradigm which is real-time and has better interaction realism. For real-time generation, we design frame-wise generation framework (Fig. 1d) which generates the -th frame promptly in an autoregressive (AR) manner based on previous -1 frames, which can respond in time, without future clip being available. We implement it with continuous non-quantized AR, enjoying the advantages of sequential processing and more accurate motion representation. For better interaction realism, we emphasize the impact of contextual semantics by combining longand short-range features in AR sequences and integrate dual-track (user and agent) dual-modal (audio and visual motion) signals to understand interactive behaviors. Furthermore, we analyze complex conversion states (interrupted, feedback, wait, overlap, pause, etc.), noting that even with the same audio, the motions differ in different states. For example, for wow its amazing to..., in normal speaking state, one might widen eyes to express exclamation, while in state of giving feedback to the other party, nodding may be performed as approval. Thus, we learn the state features as guide for motion generation. Specifically, to achieve real-time frame-wise generation with higher fidelity, we model the framework as nonquantized autoregression (AR) for continuous-valued predictions. Unlike conventional AR motion modeling that predicts discrete and finite codebook indices, we use diffusion procedure to model the probability distribution of motion, enabling the prediction of continuous values to reduce deviation of discretization and better reproduce microexpressions. To better understand the contextual interaction, we design an interactive behavior understanding module (IBU). Considering the learning difficulties and time costs of excessive long-range information, we first effectively summarizes dual-modal dual-track interaction by bidirectional-integrated learning within short chunk, and perform contextual understanding over long range. To comprehensively learn complex conversation states, we design conversation states understanding module (CSU), which combines the voice activity signal and the contextual feature of IBU module to learn current conversation states. Finally, we conduct progressive motion prediction (PMP) conditioned on context, conversation states, and audio, combining past motions to generate final motions. In summary, this paper has the following contributions: We propose novel frame-wise generation framework based on continuous AR with diffusion probability modeling, which formulates the real-time paradigm of interactive head generation with higher motion fidelity. We design an IBU module to learn the contextual interaction, which effectively summarize dual-track dual-modal signals of short range via bidirectional-integrated learning, and capture contextual information over long range. We analyze the complex states in real-world conversations in more detail, and design CSU module to learn the state feature to guide the motion generation. Extensive experiments demonstrate that our method has significant improvements in real-time realistic interactive head generation as well as single-role generation, and we provide the full effects in the Supplementary Video. 2. Related Work 2.1. Single-role Head Generation Talking head generation (THG): Audio-driven THG is to generate the speaking video with verbal-motions given driving audio and reference face image. Some methods [11, 26, 35] realize head animations by fine-tuning the pre-trained diffusion models. Hallo[31] uses hierarchical audio-driven visual synthesis module to enhance precision and EchoMimic[2] combines both audios and faIn general, THG receives audios solely cial landmarks. from the speakers side, and focuses on audio-lip sync and expression-audio alignment for talking generation. Listening head generation (LHG): LHG is to generate listener responses synchronized with the speaker based on the speakers audio and motions, as well as the listeners own past motions. Some works, such as RLHG [36], PCH [9], L2L [17], ELP [24], and MFR-Net [14] generate head motions based on 3D face coefficients, and CustomListener [15] further control motions through text-prior guidance. LHG provides non-verbal responses synchronized with the speaker, but cannot produce verbal motions. Figure 2. Overall framework of ARIG. Given the previous frames audio and motion Au T1 along with the current audio Aa , IBU first effectively uses dual-track dual-modal signals to perform bidirectional-integrated learning over short ranges and contextual understanding over long ranges, obtaining contextual interaction summary (cis-token). Then, CSU combines voice activity and cis-token to predict state features. Finally, PMP progressively predicts motions and uses the DiffusionMLP to sample the final motion. T1, T1, 2.2. Interactive Head Generation (IHG) In IHG, some works [10, 18, 25, 32] have studied personspecific generation. For generalized arbitrary identities, MRecGen [30] and [19] process the conversation as multiturn pattern and generate the agents speaker response based on the users speaking behavior in the last turn, without the generation of synchronous listener behavior. Then, some works [27, 29, 37] achieve both the role of speaker and listener. Zhou et al. [37] utilizes two independent generators and switch generators through role switcher network. DIM [27] uses the pretraining strategy to encode unified representation of both roles and adapts to the single role by downstream masking and fine-tuning. AgentAvatar [29] designs prompts to produce detailed motion descriptions through LLMs, which are then processed into motion token sequences, and further generate the final animations via rendering. Although they can generate both roles, they need explicit role pre-assignment to match respective generators, which is not smooth enough when switching immediately and cannot adapt to situations where both parties have the same role. Furthermore, INFP [39] uses unified generator to learn the dynamic state, without explicit role assignments. However, its clip-wise paradigm makes it difficult to achieve real-time generation, and learning based on short clip and only audio signals limits its understanding of long-range contextual information and the overall behavior of both parties. For our ARIG, we propose real-time frame-wise paradigm, and introduce long range understanding based on dual-modal inputs to achieve better interaction. 2.3. Autoregressive Motion Generation Autoregressive (AR) models have the nature of predicting next tokens based on known ones [13]. Influenced by the discrete vocabulary of language models, common AR Motion Generation pre-trains discrete-valued tokenizer, which involves finite codebook obtained by vector quantization (VQ) [25, 38], and then learn the correlation between the codebook indices. However, discrete VQ is not as accurate as continuous values in representing detailed motions and is sensitive to gradient estimation [12, 22, 28]. Recently, some works [4, 13] study non-VQ AR in continuousvalued space. Inspired by this, we model the per-motion probability using diffusion procedure instead of discrete categorical distribution. which enjoys both the sequence processing ability of AR and the accuracy of continuous values. Note that although [18] integrates diffusion in AR, it merely concatenates VQ and diffusion and does not uniformly model motion as diffusion distribution, and its diffusion input remains in clip form instead of frame by frame. 3. Method 3.1. Overview In our ARIG framework, we integrate the dual-modal (i.e. audio and visual motion) signals from dual-track (i.e. user and agent) to generate the agents motions frame by frame. We use motion coefficients extracted by [8] to represent head motions, which include expression, pose, and scale information, and use optical-flow based generation [8] to obtain videos. Let and represent the agents and and Au users motions at time t, and Aa represent their audios. When generating the -th frame, the audio and motions of the user and agent in previous -1 frames, as well as the agents audio at current are available. They are used as input to generate the agents motion . This process can be formulated as: = ARIG (cid:0)Aa 0T1, 0T1, 0T , Au"
        },
        {
            "title": "M a",
            "content": "0T1 (1) (cid:1) In our AR process, each update involves data at 1 and , while the historical data has been selectively utilized, transformed into model features, and cached. Therefore, the actual process can be reformulated as: = ARIG (cid:0)Aa T1, T1, T , Au"
        },
        {
            "title": "M a",
            "content": "(2) T1 (cid:1) The overview of our framework is shown in Fig. 2. First, in IBU, we merge audio and motion and perform contextual understanding via the long-short combination. We first divide time into short-range chunks of window size c, and learn the mutual interaction behavior within the chunk by the bidirectional-integrated structure and obtain single summary. Then we understand contextual behaviors in long range. The context is composed of chunks compressed interaction summary, which provides more concise and clearer perspective, and effectively increases the information capacity of the long range. In CSU, we first obtain the current voice activity signals of both parties through VAD, and then learn the current state representations from both voice activity signals and semantics of IBU. Then in PMP, we first predict coarse motion based on the audio and then refine it with state, context, and audio for finegrained adjustment. We use temporal layers to ensure interframe continuity, and finally predict motions in continuous space by modeling diffusion distributions in DiffusionMLP. 3.2. Interactive Behavior Understanding Module In this IBU module, we focus mainly on how to effectively utilize dual-modal (audio and visual-motion) information from dual-track (user and agent) participants to understand interactive behaviors. In general, we adopt combination of long and short ranges. Understanding long-range context is crucial for better capturing contextual semantic information; otherwise, the agent might fail to grasp the tone of character, misunderstand the logical relationships between sentences, or misinterpret persons intent based solely on short audio snippets. However, excessive information over long ranges can lead to computational cost and memory usage. Besides, learning independent, scattered information frame by frame also poses challenges for effective understanding. Therefore, we first summarize the interaction information within short period of time, and then conduct long-range learning based on the short-range summary. This can effectively condense information to improve efficiency, and using short chunks as units can help the content understanding from more concise and clear perspective. For dual-modal input, we believe that only by combining audio and visual motions can we accurately represent persons behavior. For example, silence with frown or an angry tone without obvious expression are both negative behaviors, and focusing on single modality may lead to misunderstandings. So we first use an MLP-based merge block to combine the information of the two modalities T1, T1 and T1 and the historical Aa to form comprehensive audio-visual behavior for each frame. Specifically at the generation of the -th frame, we can obtain audio-visual behaviors T1 based on the updated Au T1, T1. Short-range understanding We first divide the time into chunks of window size c, i.e., each chunk corresponds to frames. For t-th frame, the index of the corresponding chunk is = t/c, and the frame range corresponding to each chunk is [ci, c(i + 1)), where is the floor operation. We use 2 chunk caches for 2 parties to store the frame-level audio-visual behaviors of the latest chunk and dynamically update them with sliding window, which are the behavior sets of the user and the agent for subsequent interaction understanding via bidirectional-integrated learning. In bidirectional learning, we first consider the user and the agent as independent individuals, understand their own information based on the audio updated at time , and exchange information between each other through the attention mechanism. This is inspired by the way MMDiT [6] handles text and image modalities. In each independent channel, we use the modulation mechanism composed of adaptive LayerNorm and Linear layer to learn their own behavior, and use the shared attention mechanism to supplement respective understanding with the other partys information. Then, in integrated learning, we concatenate the dual-track output and integrate the two parties behavior understanding to summarize the interaction in the current chunk. We also use modulation mechanism to inject updated audio, so as to enhance the information fusion for interaction understanding conditioned on audio. We use the parallel Attention-MLP [3] to improve efficiency. In this way, we effectively achieve the cooperation of the five types of signals, including dual-track dual-modal information in the cache and the updated audio, so that they can fully understand the information between each other and obtain the interaction summary. Then we use linear layer to compress it and put it into the long-range context cache. Long-range understanding We store the summary of each chunk in context cache with capacity w. Contextual understanding is achieved through decoder-only structure with the causal mask, obtaining the final contextual interaction summary (cis-token). Note that when the frames in the chunk cache do not completely belong to the current chunk i, that is, when the frames of the current chunk are insufficient, we will refresh the summary of the chunk in the context cache instead of putting in new ones until the frames corresponding to the chunk are sufficient over time. More structure details of the IBU are shown in Appendix A.1. 3.3. Conversation States Understanding Module In this section, we first analyze the various conversational states of the agent in detail and further design conversational state understanding module (CSU) to obtain state guidance for motion generation. As shown in Fig. 3, besides the regular speaking and listening, the agent encounters more complex states, such as receiving feedback (wow, its amazing, etc.) while speaking, pausing to think about what to say next, being interrupted, waiting during the others pause, or giving feedback while listening. Even with the same audio, the motions differ in different states. For example, given silent audio, if the state is pause to think, it may display thoughtful expression with its eyes slightly tilted to one side. If the state is wait during pause, it may keep previous motions to patiently wait for the next speech. Thus, state prediction can guide motions, creating an explicit correlation with motions of similar states to enable more discriminative feature learning, so as to enhance facial expressiveness. Specifically, we combine audio signals and contextual interaction summary (cis-token) to predict the states classified into seven categories. We first use pre-trained Voice Activity Detector (VAD)1 to predict whether the current frame of agents and users audio are silent/active based on segment of audio. These 0/1 signals can roughly divide the state into four categories, as shown in Fig. 3. Then we embed VAD signals into vectors which together with cis-token constitute the state condition for specific state prediction. As shown in the upper right part of Fig. 2, we encode the state index sT 1 into vector as the query, let the state condition be the key and the value, and obtain the state latent feature ST through the cross attention mechanism. The state category index sT is obtained by SoftMax operation on ST , and we use the cross-entropy loss to constrain the latent state feature ST . Then we update sT 1 to prepare for the next frame prediction. The latent state feature ST is then input into the following PMP module as state guidance. 3.4. Progressive Motion Prediction Module After obtaining the cis-token and state features, the PMP module in this section focuses on how to utilize them effectively to generate motions. First, we predict the coarsegrained motion outline based on the previous motion T1, conditioned on the updated audio in the historical cache. Subsequently, we preform the fine-grained prediction based on cis-token, state, and the latest audio features, and ensure the inter-frame continuity through the temporal layer. The output is represented as latent z. For the latent Rr produced by AR process, the final is obtained by the sampler with the distribution p(M z). Conceptually, for common discretely quantized AR, is the index of codebook with the size , and the sampler is -classifier which is performed by Softmax operation with categorical probability distribution, and the cross-entropy is used to calculate the loss. However, include keypoints coordinates of optical flow within 0 1, where even small shifts can affect facial mo1https://github.com/wiseman/py-webrtcvad Figure 3. Illustration of agents conversational state. In addition to regular speaking/listening, it also includes many complex states, which are classified according to the voice activity detection. tions, and micro-expressions are crucial for conveying information. Thus, compressing these coordinates into combinations may reduce facial expressiveness, so we continue to model them in continuous, non-quantized form. Following [4, 13], to achieve continuous non-quantized AR, we use the diffusion procedure to represent the probability distribution p(M z) of the sampler, and the metric loss can be formulated as denoising criterion: L(z, ) = Eε,t (cid:104) ε εθ (M t, z)2(cid:105) (3) where εθ is the noise estimator to serve as the motion sampler, which is MLP network parameterized by θ with AdaLN to inject the condition z. ε is Gaussian noise sampled from (0, I), is time step. Let αt denote noise schedule, σt denote the noise level, then is sequentially denoised by :"
        },
        {
            "title": "M a",
            "content": "T t1 = 1 αt (cid:18)"
        },
        {
            "title": "M a",
            "content": "T 1 αt 1 αt εθ (M t, z) (cid:19) + σtε (4) The detailed model structures are shown in Appendix A.1. 4. Experiments 4.1. Experimental Settings Datasets For training data, we first use MultiDialog[19] and ViCo[36] for training in regular talking and listening scenarios, respectively. Then we use RealTalk[7] for diverse interactive scenario training. In order to ensure data quality, we carry out several-step data cleaning, removing data with fast camera movements and insufficient effective facial regions, ultimately retaining more than 200 hours of data. Then, we extract motions by [8] and extract audio features from pretrained feature extractor Wav2Vec2[1]. Furthermore, to learn conversational states, we conduct additional state annotations on the training dataset. Figure 4. Visual results of our method on RealTalk[7] dataset. The two shown scenarios involves regular speaking/listening, and complex states such as interruptions, feedback, and pauses for thinking. Our method can generate highly realistic and natural agent videos in complex conversational scenarios. Implementation Details We divide frames at 25fps, with each frame corresponding to 40ms. The values are = 6, = 512, = 6. The inference fps of the generation process is 31, which meets the real-time requirements. We use AdamW optimizer [16] to train our model, and the base learning rate is set to 1e4. The diffusion step during inference is 15. The videos are generated by calculating the optical flow relative to the reference image and warping appearance features using the method in [8]. More details are shown in Appendix A.2. 4.2. Interactive Head Generation 4.2.1. Qualitative Results Visual Results We present the visual results generated by our method in two complex conversation scenarios in Figure 4, which, in addition to regular speaking/listening, also involve complex states such as interruptions, feedback, and pauses for thinking. Given the reference image, the users video frames, and the dual-track audio (users audio and agents audio), our method can generate realistic and reasonable agent motion for different conversation content. Specifically, in the first example, we can see that the generated agent exhibits natural transitions when interrupted by the user. This can be attributed to our designs of the conversational state and the strong contextual understanding between two parties. Additionally, the agent motions can fluctuate naturally in response to the users behavior, such as smiling, demonstrating the benefits of incorporating the users visual information. In the second example, when both parties in the conversation are silent (i.e., no speech input), our model employs contextual understanding and state learning to interpret the current pause as an intention to continue speaking, thereby generating motions that exhibit thinking expression, resulting in very natural and smooth conversational performance. Comparisons In Figure 5, we present qualitative comparisons with DIM [27] and INFP[39] conditioned on the same reference image and the same audios. Since INFP[39] is not open-source, we directly utilize two videos from the website of INFP[39] for visual comparison. Due to the lack of the ground-truth user videos, we first generate talking head videos based on the given user audio as the user video input to our model, and then generate agent motions. It can be seen that the agent videos generated by our method exhibit more realistic and diverse facial expressions in both speaking and listening. For speaking, when pronouncing some specific words (e.g. Yes, Mm-hmm, wow, about, Yeah, Or and whatever), the lip movements generated by ours are more accurate and natural. For listening, our generated agent can display natural curious expression when the user says mean, demonstrating the advantages of our method in generating highly natural and realistic facial motions compared to other methods. In contrast, DIM[27] suffers from ill-timed and inaccurate responses, and INFP[39] suffers from unnatural facial expressions (e.g. suboptimal lip movements and disordered eye gaze). More Methods DIM [27] GT Ours w/o long-range context w/o conversational state guidance w/o visual modality in IBU RPCC 0.186 0.000 0. 0.140 0.127 0.155 CSIM 0.843 0.964 0.901 0.885 0.892 0.853 SyncScore 4.192 7.322 7.218 7.128 6.973 7.197 PSNR 21.31 N/A 29. 27.09 28.14 27.38 SSIM 0.573 1.000 0.827 0.803 0.816 0.812 FID 26.29 0.000 21.64 24.68 22.96 22.34 SID 1.083 2.902 2. 2.381 2.263 2.115 Var 1.206 2.514 2.397 2.326 2.174 2.032 Table 1. Quantitative comparisons on RealTalk[7]. Bold represents the best. The indicates higher is better, the indicates lower is better. Methods RLHG [36] L2L [17] DIM [27] INFP [39] Ours FD RPCC SID Var exp 39.02 33.93 23.88 18.63 18.39 pose 0.07 0.06 0.06 0.07 0. exp 0.08 0.06 0.06 - 0.05 pose 0.02 0.08 0.03 - 0.01 exp 3.62 2.77 3.71 4.78 4.82 pose 3.17 2.66 2.35 3.92 3.94 exp 1.52 0.83 1.53 2.83 2.91 pose 0.02 0.02 0.02 0.18 0. Table 2. Quantitative results with state-of-the-art listening head generation methods on ViCo[36] dataset. denotes the results are inherited from DIM[27] and INFP[39]. Method SadTalker[33] Hallo[31] EchoMimic[2] Ours PSNR SSIM FID CSIM SyncScore 25.65 28.32 28.65 28.63 0.786 0.801 0.805 0.806 23.46 21.77 20.91 18. 0.821 0.860 0.862 0.876 6.792 7.116 7.382 7.424 Table 3. Quantitative results with state-of-the-art talking head generation methods on HDTF[34] dataset. across multiple aspects. For example, our method achieves the lowest RPCC, indicating superior motion synchronization ability, which can be attributed to the effective modeling and understanding of interactive behavior between the user and the agent. Additionally, the SyncScore [21] of our method outperforms DIM[27] by large margin, which benefits from the introduction of the conversational state as well as the well-designed PMP module. Furthermore, compared to DIM[27], our proposed method exhibits higher SID and Var, showing great advantages in terms of motion diversity. For video realism, our method has the best performance in PSNR, SSIM and FID, which demonstrates that the motions generated by our model are closest to the ground-truth motions. Although identity preservation is not our main focus, we still achieve some performance improvement in CSIM. 4.3. Single-role Head Generation In addition to dyadic interactive head generation, our method can be directly applied to single-role head generation, such as listening head generation and talking head generation, without additional fine-tuning. To further validate the superiority of our proposed method, in the following, we will compare our method with the state-of-the-art listening head generation and talking head generation methods. Figure 5. Qualitative comparisons with DIM[27] and INFP[39]. The two sample videos are from the DyConv dataset proposed by INFP[39], which is not open-sourced, thus lacking ground truth. comparisons with DIM are in Appendix C.1. The full generated videos can be found in our supplementary video. 4.2.2. Quantitative Results Metrics We evaluate our proposed methods from five aspects, including motion synchronization between conversation partners, identity preservation, lip-sync performance, video realism and motion diversity. For motion synchronization, we utilize Residual Pearson Correlation Coefficient (RPCC) to measure the correlation between the user motions and the agent motions. For identity preservation, we employ CSIM to calculate the the cosine similarity of identity features between the reference image and the generated video. For lip-sync performance, we utilize SyncScore [21] to measure the lip-sync accuracy in generated videos. For video realism, we utilize Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) to measure the pixel-level similarity between the generated video frames and ground-truth video frames, and adopt Frechet Inception Distance (FID) to measure the feature-level difference between them. For motion diversity, we employ SI[17] for Diversity (SID) and Variance (Var). Comparisons As our method is not 3DMM-based method, we first extract 70-dim 3DMM coefficients (64dim for expression and 6-dim for pose) from our generated videos by pretrained deep 3D face reconstruction method[5], and then use them for the evaluation of RPCC, SID and Var. Since INFP[39] is not open-source, we only retrained DIM[27] on our dataset and present the quantitative comparisons between our method and DIM[27] in Table 1. It can be seen that our method outperforms DIM[27] 4.4. Ablation Study In this section, we present ablation results of long-range context, conversational state and visual modality in IBU. Additional ablation for continuous AR modeling and bidirectional-integrated learning are in Appendix B.1. Long-range Context We utilize long-range context in IBU module for more comprehensive contextual information understanding. To validate its effectiveness, we set the capacity of context cache as 4 in IBU and only consider short-range context (with length of 1s, refer to [29]). The quantitative results are shown in Table 1. Without longrange context, the RPCC worsened greatly, which validates long-range context is crucial for semantic understanding and thus has significant effect on motion synchronization between two parties in conversation. The qualitative results are shown in Figure 6 (a). It can be seen that without long-range context, the model may only consider the shortrange context (e.g. Im not that smart...) and misunderstand the contextual information. This can lead to incorrect feedback, such as mistakenly shifting the attitude from affirmation (e.g. smile) to disapproval (e.g. downturned mouth). Conversational State Guidance To validate the effectiveness of conversation state, we remove the CSU module in our framework. As shown in Table 1, without conversation state, the SyncScore, FID, PSNR and SSIM are all worsened, which validates that the conversation state is capable of providing strong guidance for motion prediction and making the generated motion more realistic and closer to the ground truth. In Figure 6 (b), we can also see that the conversation state can greatly enhance facial expressiveness (e.g. wide-open eyes, more precise lip motion). Visual Modality We remove the visual modality in the IBU module and only consider audio features. The ablation results are shown in Table 1. It can be seen that the RPCC increases significantly, indicating suboptimal motion synchronization without visual modality. The similar conclusion can also be reflected in Figure 6 (c), with the incorporation of visual modality, the agents facial motions can fluctuate naturally with the users motions (e.g. smile). 5. Conclusion In this paper, we propose frame-wise framework called ARIG based on continuous non-VQ autoregression to realize real-time and more realistic interactive head generation. In order to learn long-range contextual behavior, we design the IBU module which summarizes short-chunk audiovisual behaviors through Bidirectional-Integrated learning and then aggregates them to achieve long-range understanding. To understand complex states of real-world conversations, CSU utilizes voice activity signals and long-range Figure 6. Ablation study. (a) Ablation of long-range context. (b) Ablation of conversational state. (c) Ablation of visual modality in the IBU module. 4.3.1. Listening Head Generation Following DIM[27], we utilize Frechet Distance (FD) to evaluate motion realism of the generated listener, RPCC for motion synchronization, and SID and Var for motion diversity. We present quantitative comparisions with the state-ofthe-art listening head generation methods (e.g. RLHG[36], L2L[17], DIM[27] and INFP[39]) on ViCo[36] dataset in Table 2. It can be obviously seen that our proposed method has the lowest FD values for both expression and pose, indicating that our method can generate highly lifelike facial and head movements, which validates the effectiveness of our structural design. Furthermore, the RPCC of our method outperforms other methods, which justifies the superiority of our model in dyadic conversation modeling. For SID and Var, our method also shows great advantage compared to other SOTA methods, which further demonstrates that the motions generated by our method is more natural and realistic. For visual comparisons, please refer to our Appendix C.3 and supplementary video. 4.3.2. Talking Head Generation We choose several widely used metrics in talking head generation for evaluation (e.g. PSNR, SSIM, FID, CSIM and SyncScore [21]), and we compare our method with FolSadTalker [33], Hallo [31] and EchoMimic [2]. lowing INFP [39], we randomly selected 50 videos from HDTF [34] dataset as the test data. As shown in Table 3, our method achieves the best performance in metrics like SyncScore [21], CSIM and FID, showing great superiority in lip synchronization, identity preservation and featurelevel video quality. Moreover, although EchoMimic [2] is grounded in Stable Diffusion (SD) v1.5 [23], our method still has comparable performance with it on metrics related to image quality (e.g. PSNR and SSIM). Apart from quantitative comparison, we also present detailed visual comparisons in Appendix C.2 and the supplementary video. context to generate the state feature. In PMP, we progressively predict motions and achieve more accurate continuous motion generation through diffusion probability modeling in the DiffusionMLP. Comprehensive experiments validate the superiority of our proposed method."
        },
        {
            "title": "References",
            "content": "[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:1244912460, 2020. 5 [2] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions. arXiv preprint arXiv:2407.08136, 2024. 2, 7, 8, 12 [3] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 74807512. PMLR, 2023. 4 [4] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. 3, 5 [5] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 00, 2019. 7 [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 4 [7] Scott Geng, Revant Teotia, Purva Tendulkar, Sachit Menon, and Carl Vondrick. Affective faces for goal-driven dyadic communication. arXiv preprint arXiv:2301.10939, 2023. 5, 6, 7, 12, [8] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 3, 5, 6 [9] Ailin Huang, Zhewei Huang, and Shuchang Zhou. Perceptual conversational head generation with regularized driver and enhanced renderer. In Proceedings of the 30th ACM International Conference on Multimedia (MM22), 2022. 2 [10] Yinghao Huang, Leo Ho, Dafei Qin, Mingyi Shi, and Taku Komura. Interact: Capture and modelling of realistic, expressive and interactive activities between two persons in daily scenarios. arXiv preprint arXiv:2405.11690, 2024. 3 [11] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming audio-driven portrait avatar with long-term motion dependency. arXiv preprint arXiv:2409.02634, 2024. 2 [12] Alexander Kolesnikov, Andre Susano Pinto, Lucas Beyer, Xiaohua Zhai, Jeremiah Harmsen, and Neil Houlsby. Uvim: unified modeling approach for vision with learned guiding codes. Advances in Neural Information Processing Systems, 35:2629526308, 2022. 3 [13] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2025. 3, [14] Jin Liu, Xi Wang, Xiaomeng Fu, Yesheng Chai, Cai Yu, Jiao Dai, and Jizhong Han. Mfr-net: Multi-faceted responsive listening head generation via denoising diffusion model. In Proceedings of the 31th ACM International Conference on Multimedia (MM23), 2023. 2 [15] Xi Liu, Ying Guo, Cheng Zhen, Tong Li, Yingying Ao, and Pengfei Yan. Customlistener: Text-guided responsive interIn Proaction for user-friendly listening head generation. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24152424, 2024. 2 [16] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [17] Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa, and Shiry Ginosar. Learning to listen: Modeling non-deterministic dyadic facial motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2039520405, 2022. 2, 7, 8, 13 [18] Evonne Ng, Javier Romero, Timur Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, and Alexander Richard. From audio to photoreal embodiment: Synthesizing humans in conversations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10011010, 2024. [19] Se Jin Park, Chae Won Kim, Hyeongseop Rha, Minsu Kim, Joanna Hong, Jeong Hun Yeo, and Yong Man Ro. Lets go real talk: Spoken dialogue model for face-to-face conversation. arXiv preprint arXiv:2406.07867, 2024. 1, 3, 5 [20] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 11 [21] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM international conference on multimedia, pages 484492, 2020. 7, 8 [22] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 3 [23] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 8 European conference on computer vision (ECCV), 2022. 2, 5, 7, 8, 13, 15 [37] Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, and Tiejun Interactive conversational head generation. arXiv Zhao. preprint arXiv:2307.02090, 2023. 1, [38] Zixiang Zhou, Yu Wan, and Baoyuan Wang. unified framework for multimodal, multi-part human motion synthesis. arXiv preprint arXiv:2311.16471, 2023. 3 [39] Yongming Zhu, Longhao Zhang, Zhengkun Rong, Tianshu Infp: Audio-driven Hu, Shuang Liang, and Zhipeng Ge. interactive head generation in dyadic conversations. arXiv preprint arXiv:2412.04037, 2024. 1, 2, 3, 6, 7, 8, 13 [24] Luchuan Song, Guojun Yin, Zhenchao Jin, Xiaoyi Dong, and Chenliang Xu. Emotional listener portrait: Neural listener head generation with emotion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2083920849, 2023. 2 [25] Mingze Sun, Chao Xu, Xinyu Jiang, Yang Liu, Baigui Sun, and Ruqi Huang. Beyond talkinggenerating holistic 3d human dyadic motion for communication. International Journal of Computer Vision, pages 117, 2024. 3 [26] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pages 244260. Springer, 2024. 2 [27] Minh Tran, Di Chang, Maksim Siniukov, and Mohammad Soleymani. Dim: Dyadic interaction modeling for social beIn European Conference on Computer havior generation. Vision, pages 484503. Springer, 2024. 1, 3, 6, 7, 8, 12, [28] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 3 [29] Duomin Wang, Bin Dai, Yu Deng, and Baoyuan Wang. Agentavatar: Disentangling planning, driving and renarXiv preprint dering for photorealistic avatar agents. arXiv:2311.17465, 2023. 1, 3, 8 [30] Jiaqi Xu, Cheng Luo, Weicheng Xie, Linlin Shen, Xiaofeng Liu, Lu Liu, Hatice Gunes, and Siyang Song. Mrecgen: Multimodal appropriate reaction generator. arXiv preprint arXiv:2307.02609, 2023. 1, 3 [31] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Yao Yao, and Siyu Zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation. arXiv preprint arXiv:2406.08801, 2024. 2, 7, 8, 12 [32] Yichao Yan, Zanwei Zhou, Zi Wang, Jingnan Gao, and Xiaokang Yang. Dialoguenerf: Towards realistic avatar faceto-face conversation video generation. Visual Intelligence, 2 (1):24, 2024. 1, 3 [33] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86528661, 2023. 7, 8, [34] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with In Proceedings of high-resolution audio-visual dataset. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36613670, 2021. 7, 8, 12, 14 [35] Longtao Zheng, Yifan Zhang, Hanzhong Guo, Jiachun Pan, Zhenxiong Tan, Jiahao Lu, Chuanxin Tang, Bo An, and Shuicheng Yan. Memo: Memory-guided diffusion arXiv preprint for expressive talking video generation. arXiv:2412.04448, 2024. 2 [36] Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao, and Tao Mei. Responsive listening head generation: In Proceedings of the benchmark dataset and baseline. A. Implementation Details A.1. Network Details Due to the page limitation, we show network details of modules in this Appendix. Bidirectional-learning In the bidirectional learning block, we treat the audio-visual behavior of the agent and the user as two independent individuals and use independent model parameters to first understand their own behavior and exchange information through shared attention. This independent understanding is conditioned on the updated audio, which is injected by the modulation block. We embed the audio in 512-dim and, as adaLN in DiT[20], we compute the scale, shift, and gate parameters in the modulation. The depth of the bidirectional learning block is 2. The detailed block structure is shown in Fig. 7. Integrated-learning In the integrated learning stage, we concatenate the outputs of both parties in the Bidirectionallearning to perform unified learning. We also inject the update audio and perform parallel learning of attention and MLP to improve efficiency. Finally, we extract the information of the agent part as the interaction summary. The detailed structure is shown in Fig. 8. Progressive Motion Prediction In this PMP module, we first generate an outline feature for the current motion based on audio information. We set the latest audios from three frames as conditions to provide more complete word pronunciation, and use the cross-attention to predict the coarse outline feature. Then, we utilize the contextual interaction summary (cis-token) and the state feature to perform finegrained prediction via the condition block which includes cross-attention and feedforward function. We also combine Aa to enhance the audio part. Then we refer to the 5-frame motions at the temporal layer to further enforce inter-frame continuity. We then use the output of the temporal layer as condition for denoising in the DiffusionMLP to sample motions. The DiffusionMLP is lightweight network composed of 3 MLP blocks, with conditional injection also performed via adaLN[20]. The specific structure of DiffusionMLP is shown in Fig. 9. A.2. Inference Details The dimensions of the input audio and motion are 768 and 262. We first encode them into 512 dimensions and put them into the historical input. In the initial stage, we repeat the motion vector of the agents reference image and the audio corresponding to the first frame to initialize each cache. The embedding dimension in Bidirectional-learning, Integrated-learning, Contextual-understanding and Stateprediction is 512. The dim of feedforward function is 2048. The condition dim in DiffusionMLP is projected into 262. Figure 7. The structure of the Bidirectional-learning. Figure 8. The structure of the Integrated-learning. Figure 9. The structure of the DiffusionMLP. B. More Experiments B.1. More Ablation Continuous AR Modeling To validate the effectiveness of continuous autoregressive (AR) modeling, we utilize disFigure 10. Ablation Study. Methods w/o continuous AR modeling w/o bidirectional-integrated learning Ours RPCC 0.129 0.173 0.125 CSIM 0.887 0.841 0.901 SyncScore 7.036 6.813 7.218 PSNR 27.62 24.17 29.67 SSIM 0.813 0.749 0. FID 23.78 25.36 21.64 SID 2.261 2.138 2.428 Var 2.154 2.016 2.397 Table 4. Ablation study for continuous AR modeling as well as the bidirectional-integrated learning. Bold represents the best. Methods DIM [27] Ours Overall User-agent Motion Lip Naturalness Coordination Diversity Synchronization 2.48 4. 2.04 4.18 2.12 4.52 2.57 4.36 Table 5. User study. The best results are highlighted in bold. crete indices in an N-sized codebook to represent the motions, and employ the Softmax function as the sampler in PMP. The ablation results are shown in Table 4, and the visual ablation is shown in Figure 10 (a). As shown in Table 4, metrics related to video realism (e.g. FID, PSNR and SSIM) and motion diversity (e.g. SID and Var) are greatly worsened when using discrete AR modeling. In Figure 10 (a), we can also see that compared to continuous AR modeling, the discrete AR modeling may reduce facial expressiveness significantly, which demonstrates that the discrete modeling is not sufficient for the representation of rich facial expressions, and thus fails to predict realistic and diverse motions. Bidirectional-integrated Learning We also ablate the network structure of our proposed IBU module and replace the bidirectional and integrated learning module with simple linear layers. As shown in Table 4, all metrics have deteriorated significantly without bidirectional-integrated learning, which validates that our designed bidirectional and integrated learning module is effective in interactive behavior understanding and modeling. In Figure 10 (b), we can see that our method can significantly improve the accuracy of facial motion prediction. B.2. User Study We asked 25 people to rate 20 different videos (on scale of 1-5, the higher the better) generated by DIM[27] and our method across four dimensions: overall naturalness, useragent coordination, motion diversity, and lip synchronization. To be specific, the 20 test videos are randomly chosen from RealTalk[7]. As shown in Table 5, our method outperforms DIM[27] in all aspects. C. Supplementary Visual Results C.1. Interactive Head Generation We present visual comparisons of our method with It can be DIM[27] on RealTalk[7] dataset in Figure 11. seen that the agent videos generated by our method are closer to GT and have significant improvement compared to DIM[27]. C.2. Talking Head Generation In Figure 12, we compare our results with SadTalker[33], Hallo[31] and EchoMimic[2] on HDTF[34] based on the same reference image and audio. It can be seen that compared to other methods, our method exhibits the closest lip movements and head motions to the ground truth, indicating that the motions produced by our method are the most Figure 11. Qualitative comparisons with DIM[27] on RealTalk[7] dataset. natural and photorealistic. C.3. Listening Head Generation We compare our method with the SOTA listening head generation methods (e.g., RLHG[36], L2L[17], DIM[27] and INFP[39]) on ViCo[36] based on the same speaker and reference image. As shown in Figure 13, the results generated by our method are the most similar to the ground-truth videos, demonstrating great superiority in the realism of facial motions. D. Limitations and Social Impact Although our method can achieve real-time interactive motion generation, its application scope is limited to the head and cannot cover the body generation, which deserves further research in the future. In practical applications, our generation method has many positive effects, such as the character animation in movies, creating virtual hosts for advertising, and developing interactive teaching tools to provide immersive experience. However, it may be abused in some scenarios (e.g. creating false content for bullying others, creating deceptive videos for spreading misinformation.). In order to prevent the technology from being abused, we can ensure that the technology serves positive purpose by forcibly adding watermarks to the generated content and managing the way the code is obtained. Figure 12. Qualitative comparisons with state-of-the-art talking head generation methods on HDTF[34] dataset. Figure 13. Qualitative comparisons with state-of-the-art listenning head generation methods on ViCo[36] dataset."
        }
    ],
    "affiliations": [
        "Vision AI Department, Meituan"
    ]
}