{
    "paper_title": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments",
    "authors": [
        "Guangyi Liu",
        "Pengxiang Zhao",
        "Yaozhen Liang",
        "Qinyi Luo",
        "Shunye Tang",
        "Yuxiang Chai",
        "Weifeng Lin",
        "Han Xiao",
        "WenHao Wang",
        "Siheng Chen",
        "Zhengxi Lu",
        "Gao Wu",
        "Hao Wang",
        "Liang Liu",
        "Yong Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \\textbf{\\textit{fully open-sourced and continuously maintained}} at https://lgy0404.github.io/MemGUI-Bench/."
        },
        {
            "title": "Start",
            "content": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments 6 2 0 2 3 ] . [ 1 5 7 0 6 0 . 2 0 6 2 : r Guangyi Liu * 1 Pengxiang Zhao * 1 Yaozhen Liang * 1 Qinyi Luo 2 Shunye Tang 2 Yuxiang Chai 3 Weifeng Lin 3 Han Xiao 3 WenHao Wang 1 Siheng Chen 4 Zhengxi Lu 1 Gao Wu 1 Hao Wang 5 Liang Liu 5 Yong Liu"
        },
        {
            "title": "Abstract",
            "content": "Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MEMGUI-BENCH, comprehensive memorycentric benchmark with pass@k and staged LLMas-judge evaluation. Our contributions include: ❶ systematic memory taxonomy analyzing 11 agents across 5 architectures; ❷ 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; ❸ MEMGUI-EVAL, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and ❹ RQ-driven assessment of 11 stateof-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be fully open-sourced and continuously maintained at https://lgy0404.github. io/MemGUI-Bench/. 1. Introduction Large Multimodal Models (Hurst et al., 2024; Comanici et al., 2025) have enabled autonomous mobile GUI agents capable of operating mobile devices (Chen et al., 2024). While current agents show promise in basic tasks (Lu et al., 2024), they struggle with memory-intensive scenarios fundamental to effective mobile usage (Liu et al., 2025). Effective mobile device operation by humans relies on two complementary memory mechanisms: short-term memory that temporarily retains and utilizes contextual information *Equal contribution Project Lead Corresponding Author 1Zhejiang University 2Nankai University 3The Chinese University of Hong Kong 4Shanghai Jiao Tong University 5vivo AI Lab. Correspondence to: Yong Liu <yongliu@iipc.zju.edu.cn>. Preprint. February 9, 2026. 1 during task execution, including intermediate results, UI state changes, and cross-application data transfer (Chai et al., 2025; Lu et al., 2025); and long-term memory that accumulates experiential knowledge across sessions, learning from both successes and failures to improve operational efficiency (Wang et al., 2025; Agashe et al., 2025). These mechanisms collectively underpin immediate task completion and sustained proficiency development. The mobile GUI agent community has recognized this imperative, leading to proliferation of memory-enhanced architectures (Wang et al., 2025; Agashe et al., 2025; Wang et al., 2024a). However, this growing ecosystem of memory implementations reveals critical evaluation gap: the absence of standardized, comprehensive assessment frameworks for memory capabilities. Current evaluation platforms (Lee et al., 2024; Wang et al., 2024b; Xu et al., 2024; Rawles et al., 2024) suffer from three fundamental limitations: (I) task design inadequacy (only 5.2-11.8% memory-related tasks), (II) evaluation protocol limitations (no multi-attempt pass@k protocols for long-term learning), and (III) judgment methodology constraints (scalability and accuracy issues with existing approaches). Detailed analysis is in Appendix A.1. These deficiencies lead to our central research question: How can we establish rigorous, comprehensive evaluation framework that captures the nuanced memory demands of real-world mobile interactions and enables systematic assessment of both short-term retention and long-term learning capabilities? To address this challenge, we introduce MEMGUI-BENCH, the most comprehensive, memory-centric benchmark with pass@k and staged LLM-as-judge evaluator. As illustrated in Figure 1, MEMGUI-BENCH establishes new standards for memory-centric evaluation through 4 key contributions: ❶ Systematic Memory Taxonomy. We establish comprehensive taxonomy distinguishing short-term memory (temporary information buffering) and longterm memory (cross-session learning), with analysis MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments This capability manifests in two fundamental forms, consistent with the terminology adopted in recent LLM-agent memory research (Wu et al., 2024; Maharana et al., 2024; Zhong et al., 2024): Short-term (in-session) memory refers to the agents ability to temporarily retain and utilize contextual information during task execution, enabling coherent decisionmaking across sequential interaction steps. This capability allows agents to maintain awareness of previous actions, intermediate results, and relevant UI state changes throughout task session. Memory-intensive tasks, such as cross-application information transfer or multi-step data collection scenarios, pose significantly greater challenges by requiring agents to extract, retain, and accurately recall specific information units across extended interaction sequences. We identify 5 distinct architectures: (1) Memory Agent (Wang et al., 2024a; 2025), (2) ActionThought Pattern (Zhang et al., 2023), (3) Multi-turn Context (Qin et al., 2025), (4) Rule-based Aggregation (Zheng et al., 2024), and (5) No Historical Context (Hong et al., 2024). Long-term (cross-session) memory accumulates experience from each interaction, whether successful or failed, forming reusable skills and knowledge. When agents encounter unfamiliar applications, they may initially make suboptimal decisions, but lessons learned from failures, combined with successful operation patterns, ultimately shape their proficiency with software. This memory is persistent, transferable, and aims to improve long-term efficiency across tasks. We identify 2 main categories: (1) Success-Based Learning that extracts reusable shortcuts and tips from successful executions (Wang et al., 2025; Agashe et al., 2025), and (2) Failure-Based Learning that analyzes failed attempts to avoid previously encountered errors (Agashe et al., 2025). Detailed technical implementations and comparative analysis of these memory mechanisms are provided in Appendix A.3. 3. Memory-Centric Benchmarking"
        },
        {
            "title": "Environment",
            "content": "Figure 1. An overview of MemGUI-Bench, first comprehensive benchmark for GUI agent memory evaluation. of 11 agents identifying 5 distinct architectures (Section 2). ❷ Memory-Centric Benchmarking Environment. We contribute 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial information retention. Our snapshotbased framework supports pass@1 and pass@k evaluation protocols (Section 3). ❸ Automated Evaluation Pipeline. We introduce MEMGUI-EVAL with novel Progressive Scrutiny across 3 stages and 7 hierarchical metrics for memory fidelity, learning effectiveness, and execution efficiency (Section 4). ❹ RQ-Driven Comprehensive Assessment. Through 6 research questions, our evaluation of 11 agents reveals: significant memory deficits (4-10 capability gaps), mandatory short-term memory requirements, +21.9 pp long-term memory benefits, and 5 distinct failure modes with 5 actionable design implications (Section 5 and Section 6). 2. Memory in Mobile GUI Agents Inspired by human memory mechanisms (Murdock, 1974; Ashcraft, 1989), we establish comprehensive taxonomy of memory capabilities for mobile GUI agents. When humans interact with mobile interfaces, they naturally employ sophisticated memory mechanisms that enable intelligent and efficient task completion across diverse scenarios. Defining Memory for Mobile GUI Agents. We define memory for mobile GUI agents as: The ability to retain, process, and utilize both contextual information within tasks and experiential knowledge across tasks to enhance decision-making and task performance over time. Creating robust benchmark for agent memory requires two key components: challenging set of tasks that specifically target memory capabilities, and standardized, efficient environment to execute these tasks. This section details both pillars of our contribution: the memory-centric task suite and the snapshot-based, plug-and-play framework that together form our unified benchmarking environment. 2 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments 3.1. Memory-Intensive Task Suite Design MemGUI-Bench comprises 128 carefully designed tasks across 26 real-world applications, spanning 4 different appcrossing complexities to systematically evaluate mobile GUI agents memory capabilities. Our task suite statistics, illustrated in Figure 2, demonstrate comprehensive distribution: tasks range from 3 to 160 golden steps (average 36.2), with 78.1% requiring cross-application information transfer, and balanced coverage across three difficulty levels (37.5% easy, 32.8% medium, 29.7% hard). This design reflects realistic user interaction patterns while providing focused evaluation of memory mechanisms in mobile GUI environments. Detailed design specifications, including application selection strategies, task characteristics, and information retention pathways, are provided in Appendix A.4. The complete task suite, presented in Table 10, represents the result of extensive development and validation to ensure the benchmarks reliability for systematic evaluation of mobile GUI agents memory capabilities. 3.2. Snapshot-Based Plug-and-Play Framework We developed comprehensive snapshot-based plug-andplay framework that enables efficient, scalable, and reproducible evaluation of GUI agents while providing robust support for long-term memory assessment through multiattempt protocols. As illustrated in Figure 3, our framework addresses the critical challenges of environment consistency, agent diversity, and parallel execution that are essential for systematic memory evaluation. Figure 2. Statistical overview of the MemGUI-Bench task suite. Task Design Principles. We designed 115 memoryintensive tasks alongside 13 standard tasks to systematically evaluate mobile GUI agents memory capabilities. Our memory-intensive tasks require agents to extract, retain, and accurately recall specific information units across extended interaction sequences, such as retaining product prices for cross-application comparison or maintaining intermediate results across multiple steps. The 13 standard tasks serve as baseline benchmarks for computing the Memory-Task Proficiency Ratio (MTPR) and support long-term memory assessment through our pass@k evaluation protocol. Cross-Application Information Transfer. Our tasks implement diverse information transfer patterns ranging from single-app scenarios to complex four-app workflows. For example, AnalyzeApartmentCommute requires extracting apartment details from Apartments.com, searching company addresses via Bing, calculating commute times through Citymapper, and recording analysis in Joplin. This hierarchical complexity ensures comprehensive evaluation of memory capabilities across different spatial and temporal scales. Long-Term Learning Support. To enable long-term memory evaluation, the 128 tasks are organized into 64 mirror task pairs with similar application combinations and cognitive demands but distinct specific requirements. This design supports systematic assessment of cross-task learning, where agents can transfer knowledge and strategies from earlier attempts to improve performance on related tasks. Figure 3. The unified architecture of MemGUI-Benchs snapshotbased plug-and-play framework. Evaluation Pipeline. Our evaluation pipeline follows systematic five-stage process that ensures reliable assess- (1) Task Dispatch and ment across multiple attempts. Unified Scheduling: Tasks are distributed through centralized scheduling system that manages experiment queuing and resource allocation. (2) Agent Task Reception: GUI agents receive task specifications through our unified interface, which abstracts implementation details and provides consistent task formatting. (3) Environment Interaction: Agents interact with Android emulators by reading observational information (screenshots, UI hierarchies) and executing actions (taps, swipes, text input). (4) Automated Evaluation: Screenshots and agent decisions are continuously passed to MemGUI-Eval for real-time assessment of task progress and completion. (5) Multi-Attempt Management: If task fails or reaches maximum step limits, the system automatically triggers environment reset and initiates retry attempts up to the configured limit (default = 3 for pass@k evaluation), enabling systematic assessment of long-term learning capabilities. Key Framework Features. Our framework provides 3 distinctive advantages over existing approaches: ❶ Scalable Parallel Execution: Through sophisticated emulator management and port-based isolation, enabling concurrent 3 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments evaluation of multiple agents without interference. ❷ Rapid Environment Recovery: Snapshot-based approach enables instant environment reset, contrasting with manual reset requirements in existing benchmarks. ❸ Native Long-Term Memory Support: Built-in pass@k protocol and persistent agent state management across multiple attempts, capability absent in existing benchmarks that focus exclusively on single-attempt evaluation. Comprehensive technical specifications for the framework architecture, including parallel implementation details, multi-attempt mechanisms, agent integration protocols, and comparative analysis with existing approaches, are provided in Appendix A.5. 4. An Automated Evaluation Pipeline with Memory-Specific Metrics Evaluating memory-intensive tasks poses significant challenge that demands innovation in both evaluation metrics and the judgment process itself. We address this by proposing comprehensive, automated evaluation pipeline. This pipeline integrates novel set of hierarchical metrics designed to quantify memory capabilities with MEMGUIEVAL, sophisticated arbiter that ensures accurate and efficient judgment. 4.1. Memory-Specialized Metrics with Hierarchical"
        },
        {
            "title": "Assessment",
            "content": "To capture the nuances of agent memory capabilities, we introduce hierarchical framework with 7 specialized metrics across three dimensions: short-term memory fidelity, long-term learning capabilities, and execution efficiency. Short-Term Memory Assessment (pass@1). We evaluate agents memory fidelity through three complementary metrics: ❶ Success Rate (SR) as baseline performance measurement; ❷ Information Retention Rate (IRR) as our core memory fidelity metric, quantifying the proportion of required information units that agents correctly recall and utilize; ❸ Memory-Task Proficiency Ratio (MTPR) isolating memory-specific capabilities by comparing performance on memory-intensive versus standard tasks. Long-Term Memory Assessment (pass@k). We quantify cross-session learning capabilities through two metrics: ❶ Multi-Attempt Success Rate (pass@k SR) measuring agents ability to succeed within trials through experience accumulation; ❷ Failure Recovery Rate (FRR) targeting rapid learning from failure using harmonic decay weighting to reward faster recovery. (pass@1 and Execution Efficiency Assessment pass@k). We include three efficiency indicators: ❶ Average Step Ratio measuring path efficiency compared to golden standards; ❷ Average Time Per Step quantifying computational overhead; ❸ Average Cost Per Step evaluating economic efficiency of memory mechanisms. Comprehensive mathematical definitions, computational procedures, and detailed metric analysis are provided in Appendix A.6. 4.2. MEMGUI-EVAL: Progressive Scrutiny Evaluator To overcome the limitations of existing evaluation methodologies, ranging from rigid rule-based matching to inefficient LLM-as-Judge approaches that overwhelm models with complete trajectories, we developed MEMGUI-EVAL, sophisticated evaluation arbiter designed specifically for memory-intensive tasks. As illustrated in Figure 4, it employs novel Progressive Scrutiny pipeline that mimics efficient human expert verification: starting with minimal, high-efficiency evidence and progressively deepening analysis only when necessary, thereby achieving optimal costaccuracy balance. The pipeline orchestrates four specialized agents: Triage Judge for rapid initial screening, Step Descriptor for semantic enrichment, Semantic Judge for comprehensive analysis, and Visual Judge for targeted verification. Figure 4. MemGUI-Evals three-stage progressive scrutiny pipeline. Stage 1: Cost-Effective Triage. This stage rapidly processes straightforward successful cases to dramatically reduce evaluation costs. The Triage Judge receives minimal evidence: task goal description, raw action logs (e.g., 4 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments CLICK, TYPE), and the final three screenshots of the trajectory. Critically, this specialized agent adopts an extremely conservative strategy, concluding success only when the limited evidence irrefutably demonstrates that all task requirements have been satisfied. Any case with ambiguity advances to the next stage, ensuring high precision while maximizing efficiency for clear-cut scenarios (see Figure 18 for concrete example). Stage 2: Full Semantic Analysis. When initial triage proves inconclusive, the system conducts comprehensive semantic analysis with enriched evidence. The framework first automatically generates detailed textual descriptions (action description and ui description) for every step in the trajectory using the Step Descriptor, specialized agent that analyzes before-and-after action panels to create semantic representations of each interaction. The Semantic Judge then synthesizes the complete task goal, this rich step-by-step semantic context, and the same final three screenshots to make an informed judgment. Critically, the system includes explicit warnings about potential incompleteness in automatically generated text descriptions, requiring mandatory verification that all task-critical information is present in either the textual descriptions or visual evidence. For failed memory tasks involving multiple information units, the Semantic Judge additionally triggers the IRR Analyzer to compute an Information Retention Rate (IRR) and quantify the degree of memory failurefor instance, distinguishing an agent that correctly recalls 2 out of 3 required news headlines (see Figure 20 for memory failure analysis). When definitive judgment remains elusive despite this enriched context, the Semantic Judge must return required steps list, explicitly specifying which historical screenshots are essential for final adjudication (see Figure 19 for successful semantic analysis case). Stage 3: Targeted Visual Verification. This final stage represents our core innovation compared to traditional VLM evaluation methods: rather than overwhelming the model with all historical screenshots, we provide precisely the visual evidence it actively requested. The Visual Judge receives all textual evidence from Stage 2 plus new composite image created by stitching together the specific historical screenshots identified in the required steps list. This targeted approach eliminates information overload while ensuring the Visual Judge has exactly the visual evidence needed for high-fidelity judgment (see Figure 21 for successful visual verification). The system enforces strict verification requirements, mandating that any missing critical information in both textual descriptions and provided screenshots results in task failure, preventing inference or guesswork. The Visual Judge is required to make definitive binary decision (success or failure) and, for failed memory tasks, triggers the IRR Analyzer to compute the final IRR based on all available visual and textual evidence (see Figure 22 for visual verification with failure determination). This progressive scrutiny approach maintains complete automation while ensuring reliable evaluation of complex memory-intensive tasks. Concrete examples illustrating each stage are provided in Appendix A.10. 4.3. Validation of the Evaluation Pipeline To establish the trustworthiness of MEMGUI-EVAL, we conducted validation experiments across two datasets: 26 SPABench tasks (78 trajectories) and 128 MEMGUI-BENCH tasks (256 trajectories), comparing three model configurations against baseline methods with human expert annotations as ground truth. Table 1. Validation of MemGUI-Eval performance across different scenarios."
        },
        {
            "title": "Evaluator",
            "content": "Config. F1 Prec. Recall Cost ($) Accuracy Metrics (%) Efficiency PART A: SPA-BENCH TRAJECTORIES (N=78) MemGUI-Eval (Ours) SPA-Bench (Baseline) M1 M2 G1 G2 G3 99.0 95.9 93.6 88.2 81.4 80.9 100.0 95.9 97.8 93.2 94.6 90.0 98.0 95.9 89. 83.7 71.4 73.5 0.064 0.028 0.020 0.038 0.027 0.103 PART B: MEMGUI-BENCH TRAJECTORIES (N=256) MemGUI-Eval (Ours) M1 M2 93.1 81.2 78.4 92.4 82.5 81.7 93.8 80.0 75.4 0.213 0.070 0.060 As shown in Table 1, MEMGUI-EVAL demonstrates superior accuracy and cost-effectiveness. Key findings include: (1) our M1 configuration achieves 99.0% F1-score on SPABench, outperforming the best baseline (92.5%); (2) the M2 configuration provides optimal quality-cost balance with 95.9% F1-score at $0.031 per trajectory; and (3) MEMGUIEVAL maintains exceptional cross-app performance (94.1100% F1) where baselines struggle (40-61.5% F1). Configuration Selection: We adopt the M2 configuration (Gemini 2.5 Flash for Step Descriptor, Gemini 2.5 Pro for judgment agents) for all subsequent experiments, achieving optimal quality-cost balance. Comprehensive validation details are provided in Appendix A.7, with prompt templates in Appendix A.11. 5. Benchmarking GUI Agent Baselines We conduct comprehensive evaluation of 11 leading GUI agents on MEMGUI-BENCH to empirically assess the current state of memory capabilities. Our analysis is driven by 5 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments the following research questions: (RQ1) How do current GUI agents perform on memoryintensive tasks, and what capability gaps do such benchmarks reveal? (RQ2) Are memory mechanisms essential or optional features for GUI agents? (RQ3) How does cross-application complexity affect memory performance? (RQ4) Can long-context capability improve memoryintensive task performance? (RQ5) Can long-term memory mechanisms enable effective cross-session learning? (RQ6) What are the computational trade-offs in memoryenhanced architectures? 5.1. Experimental Setup We evaluate 11 prominent GUI agents spanning diverse architectural approaches: 2 agents with explicit long-term memory (Agent-S2, Mobile-Agent-E) and 9 without such mechanisms. Each of the 128 tasks is executed up to = 3 times on Android simulators. Results are assessed by MEMGUI-EVAL as described in Section 4. Detailed agent configurations are provided in Appendix A.2. Table 2. Performance comparison of Mobile GUI agents on MemGUI-Bench. Detailed short-term and long-term memory metrics are provided in Tables 13 and 14 (Appendix A.9.1). Short-Term Memory (pass@1) Long-Term Memory (pass@3)"
        },
        {
            "title": "AGENTIC WORKFLOW",
            "content": "Agent-S2 Mobile-Agent-E T3A M3A Mobile-Agent-V2 SeeAct AppAgent UI-Venus-7B UI-TARS-1.5-7B GUI-Owl-7B CogAgent 41.7 12.5 31.2 39.6 8.3 6.2 8.3 14.6 8.3 14.6 0.0 19.0 2.4 16.7 35.7 0.0 0.0 0.0 0.0 0.0 0.0 0. 18.4 0.0 18.4 21.1 0.0 0.0 0.0 27.3 5.5 22.7 32.8 3.1 2.3 3.1 AGENT-AS-A-MODEL 0.0 0.0 2.6 0.0 5.5 3.1 6.2 0.0 64.6 22.9 45.8 47.9 10.4 12.5 22. 20.8 16.7 22.9 0.0 42.9 2.4 45.2 50.0 0.0 2.4 2.4 0.0 0.0 2.4 0.0 36.8 2.6 34.2 44.7 0.0 0.0 0.0 0.0 0.0 2.6 0.0 49.2 10.2 42.2 47.7 3.9 5.5 9. 7.8 6.2 10.2 0.0 5.2. Main Results [For RQ1] Current agents exhibit significant memory deficits, with 4-10 capability gaps hidden by standard benchmarks. Table 2 presents the main leaderboard. M3A achieves the highest single-attempt success rate (32.8%), while Agent-S2 demonstrates exceptional learning potential with the highest multi-attempt performance (49.2%). stark architectural divide emerges: frameworkbased agents (Agentic Workflow) achieve 22.7-32.8% success rates, while end-to-end models (Agent-as-a-Model) achieve only 0.0-6.2%. Performance drops dramatically from Easy (up to 39.6%) to Hard tasks (up to 21.1%), exposing scalability limitations in current memory mechanisms. Figure 5 further compares agent performance between MEMGUI-BENCH and AndroidWorld, revealing dramatic drops: Agent-S2 from 54.3% to 27.3%, GUI-Owl-7B from 66.4% to 6.2%, and UI-Venus-7B from 49.1% to 5.5%. The Memory-Task Proficiency Ratio (MTPR) quantifies this gap: while top agents (Agent-S2: 0.45, M3A: 0.41) show reasonable memory-specific performance, most agents exhibit MTPR below 0.1. This 4-10 disparity demonstrates that existing benchmarks systematically overestimate agent capabilities. Figure 5. Performance comparison between MemGUI-Bench (89.8% memory-intensive) and AndroidWorld (5.2% memoryintensive). Red annotations show performance drops on memoryintensive tasks. [For RQ2] Short-term memory is mandatory; long-term memory is beneficial but optional. To definitively answer whether memory is essential, we conducted systematic ablation experiments on four representative agents spanning different architectural paradigms  (Table 3)  . (1) Shortterm memory is mandatory: Removing short-term memory components (Memory Agent, Action-Thought, or Context) renders agents essentially non-functional. M3A suffers catastrophic collapse (SR: 32.5% 2.5%, IRR: 35.1% 0.0%) when its Memory Agent is removed, and Agent-S2 shows similar degradation (SR: 27.5% 5.0%, IRR: 33.3% 0.0%). The universal IRR collapse to zero confirms that without short-term memory, agents cannot retain any information. (2) Long-term memory provides significant benefits but is not mandatory: Removing Agent-S2s longterm memory causes -20.0 pp drop in pass@3 SR (45.0% 25.0%) and reduces FRR from 15.5% to 9.1%, though agents remain functional with short-term memory alone. These results establish that short-term memory is mandatory requirement for functional GUI agents, while long-term memory, though beneficial for cross-session learning, remains an optional enhancement. Detailed experimental configurations are provided in Appendix A.9.4. [For RQ3] Cross-application complexity causes 16-40 pp performance degradation, constituting the primary memory bottleneck. To analyze how cross-app information transfer affects memory performance, we categorize our 128 tasks by the number of applications involved: 28 6 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Table 3. Memory ablation study on MEMGUI-BENCH-40 (40 sampled tasks). We systematically remove (-) or enhance (+) memory components in four representative agents. Blue indicates improvement, red indicates degradation. Bold numbers indicate baseline performance. Agent Memory Config SR@1 All SR@1 Easy SR@1 Med SR@1 Hard SR@3 All SR@3 Easy SR@3 Med SR@3 Hard IRR (%) MTPR FRR (%) M3A: MEMORY AGENT ARCHITECTURE M3A (Workflow) Baseline + Multi-turn - Memory Agent 32.5 52.5 (+20.0) 2.5 (-30.0) 53.8 61.5 (+7.7) 7.7 (-46.1) 31.6 47.4 (+15.8) 0.0 (-31.6) 0.0 50.0 (+50.0) 0.0 (0.0) 47.5 70.0 (+22.5) 5.0 (-42.5) 53.8 61.5 (+7.7) 15.4 (-38.4) 47.4 63.2 (+15.8) 0.0 (-47.4) 37.5 100.0 (+62.5) 0.0 (-37.5) 35.1 53.5 (+18.4) 0.0 (-35.1) 0.321 0.457 (+0.136) 0.000 (-0.321) 16.7 26.3 (+9.6) 1.3 (-15.4) AGENT-S2: MEMORY AGENT + LONG-TERM MEMORY Agent-S2 (Workflow) Baseline - LTM - STM & LTM 27.5 17.5 (-10.0) 5.0 (-22.5) 46.2 15.4 (-30.8) 15.4 (-30.8) 21.1 21.1 (0.0) 0.0 (-21.1) 12.5 12.5 (0.0) 0.0 (-12.5) 45.0 25.0 (-20.0) 10.0 (-35.0) 61.5 30.8 (-30.7) 30.8 (-30.7) 42.1 21.1 (-21.0) 0.0 (-42.1) 25.0 25.0 (0.0) 0.0 (-25.0) 33.3 21.3 (-12.0) 0.0 (-33.3) 0.250 0.190 (-0.060) 0.000 (-0.250) 15.5 9.1 (-6.4) 3.9 (-11.6) GUI-OWL-7B: ACTION-THOUGHT PATTERN GUI-Owl-7B (Model) Baseline - Action-Thought 7.5 7.5 (0.0) 23.1 23.1 (0.0) 0.0 0.0 (0.0) 0.0 0.0 (0.0) 12.5 10.0 (-2.5) 30.8 30.8 (0.0) 5.3 0.0 (-5.3) 0.0 0.0 (0.0) 4.6 0.0 (-4.6) 0.000 0.000 (0.0) 4.1 2.7 (-1.4) UI-TARS-1.5-7B: MULTI-TURN CONTEXT + ACTION-THOUGHT UI-TARS 1.5-7B (Model) Baseline - Multi-turn & A-T 5.0 2.5 (-2.5) 15.4 7.7 (-7.7) 0.0 0.0 (0.0) 0.0 0.0 (0.0) 5.0 2.5 (-2.5) 15.4 7.7 (-7.7) 0.0 0.0 (0.0) 0.0 0.0 (0.0) 2.3 0.0 (-2.3) 0.000 0.000 (0.0) 0.0 0.0 (0.0) single-app, 56 two-app, 34 three-app, and 10 four-app tasks. Table 4 presents the performance breakdown. Top agents experience 16-40 percentage point drops from single-app to four-app scenarios: M3A decreases from 46.4% (1-app) to 30.0% (4-app) in SR (-16.4 pp), while Agent-S2 drops more severely from 50.0% to 10.0% (-40 pp). The IRR shows parallel degradation, with M3A maintaining relatively stable IRR (31.7% 37.5%) while most agents collapse entirely on 4-app tasks. This reveals that cross-app information transfer constitutes the primary memory bottleneck, requiring agents to maintain information coherence across distinct application contexts and UI paradigms. Detailed IRR analysis and model-specific patterns are provided in Appendix A.9.3. Table 4. Performance breakdown by cross-application complexity. Short-Term Memory (pass@1) Long-Term Memory (pass@3) Agent 1 App SR IRR SR IRR SR IRR SR IRR SR 1 App 2 Apps 3 Apps SR 2 Apps 3 Apps 4 Apps SR AGENTIC WORKFLOW 0.0 1.6 Agent-S2 Mobile-Agent-E 25.0 8.7 T3A M3A 0.0 0.0 Mobile-Agent-V2 14.3 0.0 10.7 0.0 SeeAct 0.0 0.0 14.3 11.1 0.0 0.0 AppAgent 50.0 51.7 19.6 37.6 26.5 38.9 10.0 33.3 78.6 42.9 42.9 26.7 16.1 33.3 23.5 30.2 0.0 11.2 60.7 46.4 31.7 28.6 43.8 29.4 35.9 30.0 37.5 64.3 17.9 25.0 42.9 0.0 0.0 0.0 0.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 1.6 0.0 0.0 AGENT-AS-A-MODEL 21.4 6.7 UI-Venus-7B 1.8 2.7 UI-TARS-1.5-7B 14.3 11.1 0.0 1.8 21.4 11.7 1.8 3.6 GUI-Owl-7B 0.0 0.0 0.0 0.0 CogAgent 0.0 1.5 0.0 4.0 2.9 7.1 0.0 0. 0.0 0.0 0.0 2.9 0.0 4.0 0.0 0.0 Task Count 28 56 34 28.6 21.4 35.7 0.0 28 35.7 1.8 37.5 41.1 0.0 0.0 0.0 1.8 1.8 1.8 0.0 56 52.9 0.0 38.2 44.1 0.0 0.6 0. 2.9 2.9 5.9 0.0 34 4 Apps SR 30.0 0.0 30.0 50.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 [For RQ4] Long-context capability yields +18.8 pp improvement, revealing untapped potential. Figure 6 reveals that switching M3A from single-turn to multi-turn conversation improves performance from 32.8% to 51.6% (+18.8 pp), demonstrating significant untapped potential in 7 Figure 6. Performance comparison between single-turn and multiturn conversation modes. M3A (Single-turn) uses standard context, M3A (Multi-turn) leverages Gemini 2.5 Pros extended context window, and UI-TARS-1.5-7B uses sliding window (last 5 turns). Green annotations show performance gains. long-context understanding. This multi-turn configuration enables the backbone LLM (Gemini-2.5-Pro) to leverage its full 1M token context window for cumulative memory management, achieving the highest overall success rate among all evaluated configurations. UI-TARS-1.5-7B, constrained to sliding window of the last 5 turns, achieves only 3.16.2% success, confirming that context length constraints severely limit memory-intensive task performance. [For RQ5] Long-term memory enables 21.9 pp improvement but remains underutilized. Table 14 in Appendix A.9.1 presents detailed long-term memory evaluation results. Agent-S2 demonstrates exceptional cross-session learning with 21.9 percentage point improvement (27.3% 49.2%) and 21.5% Failure Recovery Rate (FRR), while agents without explicit memory show minimal FRR (0.84.4%). Mobile-Agent-E exhibits consistent learning (+4.7 pp), validating that sophisticated memory architectures provide meaningful benefits. However, this comes at computational cost: Agent-S2 requires 27.5 seconds per step versus 5.3s for M3A. The stark contrast between memory-equipped and memory-free agents confirms that dedicated long-term memory mechanisms are essential for efficient cross-session learning. Detailed pass@k learning curves are analyzed in Appendix A.9.2. [For RQ6] Sophisticated memory architectures face severe computational trade-offs under deployment constraints. To evaluate deployment viability, we assess testtime compute efficiency under two constraint types: steps/episode (limiting execution length) and tokens/episode (limiting API costs). Table 5 presents the results. (1) Hightoken agents face performance collapse under token constraints: Agent-S2 (41,760 tokens/step) drops from 49.2% to 0.0% SR@3, with sophisticated memory architectures (2) consuming 4.4-5.9 more tokens than the baseline. M3A demonstrates optimal deployment balance with graceful degradation (47.7% 21.9% SR@3) while consuming only 31% of Agent-S2s tokens. (3) Lightweight models (UIVenus-7B, GUI-Owl-7B) show zero performance change under token constraints due to low token consumption. This MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments reveals that strategic long-context utilization with computational awareness represents key direction for productionviable memory-enhanced GUI agents. Detailed analysis is provided in Appendix A.9.5. Table 5. Test-time compute normalized evaluation under steps/episode and tokens/episode constraints. Blue indicates improvement, red indicates degradation. Detailed analysis in Appendix A.9.5. Agent Tokens/ Step Constraint SR@1 All SR@1 Easy SR@1 Med SR@1 Hard SR@3 All SR@3 Easy SR@3 Med SR@3 Hard IRR (%) MTPR FRR (%) Agent-S 41,760 Mobile-Agent-E 56,400 T3A 14,000 M3A 12,960 Mobile-Agent-V2 54,720 SeeAct 10,720 AppAgent 6, UI-Venus-7B 3,700 UI-TARS-1.5-7B 17,540 GUI-Owl-7B 5,817 CogAgent 4,680 AGENTIC WORKFLOW (WITH LTM) Steps/Ep Tokens/Ep Delta Steps/Ep Tokens/Ep Delta 27.3 0.0 -27.3 5.5 0.0 -5. 41.7 0.0 -41.7 12.5 0.0 -12.5 19.0 0.0 -19.0 2.4 0.0 -2.4 18.4 0.0 -18.4 0.0 0.0 0. 49.2 0.0 -49.2 10.2 0.0 -10.2 64.6 0.0 -64.6 22.9 0.0 -22.9 AGENTIC WORKFLOW (WITHOUT LTM) Steps/Ep Tokens/Ep Delta Steps/Ep Tokens/Ep Delta Steps/Ep Tokens/Ep Delta Steps/Ep Tokens/Ep Delta Steps/Ep Tokens/Ep Delta Steps/Ep Tokens/Ep Delta Steps/Ep Tokens/Ep Delta Steps/Ep Tokens/Ep Delta Steps/Ep Tokens/Ep Delta 22.7 6.2 -16.5 32.8 14.8 -18.0 3.1 0.0 -3.1 2.3 0.8 -1. 3.1 1.6 -1.5 5.5 5.5 0.0 3.1 0.0 -3.1 6.2 6.2 0.0 0.0 0.0 0.0 31.2 6.2 -25. 39.6 16.7 -22.9 8.3 0.0 -8.3 6.2 2.1 -4.1 8.3 4.2 -4.1 16.7 0.0 -16.7 35.7 11.9 -23. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 18.4 13.2 -5.2 21.1 15.8 -5.3 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 AGENT-AS-A-MODEL 14.6 14.6 0.0 8.3 0.0 -8.3 14.6 14.6 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 2.6 2.6 0.0 0.0 0.0 0.0 42.2 13.3 -28.9 47.7 21.9 -25.8 3.9 0.0 -3. 5.5 2.3 -3.2 9.4 7.8 -1.6 7.8 7.8 0.0 6.2 0.0 -6.2 10.2 10.2 0.0 0.0 0.0 0. 45.8 10.4 -35.4 47.9 18.8 -29.1 10.4 0.0 -10.4 12.5 4.2 -8.3 22.9 18.8 -4.1 20.8 20.8 0. 16.7 0.0 -16.7 22.9 22.9 0.0 0.0 0.0 0.0 42.9 0.0 -42.9 2.4 0.0 -2.4 45.2 9.5 -35. 50.0 19.0 -31.0 0.0 0.0 0.0 2.4 2.4 0.0 2.4 2.4 0.0 0.0 0.0 0.0 0.0 0.0 0. 2.4 2.4 0.0 0.0 0.0 0.0 36.8 0.0 39.5 0.1 -36.8 -39.4 2.6 0.0 -2.6 2.4 1.2 -1. 0.45 21.5 0.00 0.0 -0.45 -21.5 0.02 0.00 -0.02 4.1 0.0 -4.1 29.6 34.2 21.1 0.0 -13.1 -29.6 0.30 20.7 5.8 0.34 +0.04 -14.9 39.3 44.7 28.9 18.6 -15.8 -20. 0.41 16.3 6.4 0.96 -9.9 +0.55 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 2.6 2.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.0 -0.2 1.5 0.9 -0.6 2.6 2.6 0. 3.8 0.4 -3.4 5.7 5.7 0.0 0.0 0.0 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.11 +0. 0.05 0.05 0.00 0.04 0.00 -0.04 0.07 0.07 0.00 0.00 0.00 0.00 0.8 0.0 -0.8 2.4 1.2 -1. 4.4 4.4 0.0 1.7 1.7 0.0 2.4 0.0 -2.4 3.3 3.3 0.0 0.0 0.0 0.0 6. Failure Pattern Analysis limitations of current To understand the fundamental GUI agents, we conducted systematic failure analysis on 1,265 task executions using MemGUI-Evals finegrained categorization system. We identified five memoryrelated failure modes for 343 non-timeout failures: (1) Partial Memory Hallucination (PMH), (2) Process Memory Hallucination (ProcMH), (3) Output Memory Hallucination (OMH), (4) Knowledge Deficiency (KD), and (5) Intent Misunderstanding (IM). Figure 7 presents the failure distribution across all agents, revealing that memory hallucination dominates non-timeout failures (58.9% on average). Detailed failure mode definitions and representative examples (Figures 915) are provided in Appendix A.8. Design Implications. Based on these failure patterns, we identify key architectural directions for future memoryenhanced GUI agents: ❶ multi-granularity memory buffers with separate slots for different information types to address partial memory hallucination; ❷ hierarchical task decomposition with persistent goal tracking to mitigate process memory hallucination; ❸ strategic long-context utilization Figure 7. Comprehensive failure pattern heatmap across all evaluated agents for non-timeout failures. beyond naive conversation history concatenation; ❹ explicit long-term memory mechanisms for cross-session learning, as evidenced by Agent-S2s 21.5% FRR versus 0.8-4.4% for agents without dedicated memory; and ❺ hybrid architectures combining framework-level memory management with efficient end-to-end models to balance capability and computational cost. Comprehensive analysis and detailed design recommendations are provided in Appendix A.8.4. 7. Conclusion We introduce MEMGUI-BENCH, comprehensive memorycentric benchmark with pass@k evaluation and staged LLMas-judge. Our RQ-driven evaluation of 11 agents across 128 tasks reveals: 4-10 capability gaps on memory-intensive tasks (RQ1), mandatory short-term memory with optional but beneficial long-term memory (RQ2), 16-40 pp degradation from cross-app complexity (RQ3), +18.8 pp from longcontext (RQ4), +21.9 pp from long-term memory (RQ5), and severe computational trade-offs (RQ6). Through systematic failure analysis, we identify 5 memory-related failure modes and synthesize 5 design implications for future architectures. MEMGUI-BENCH establishes empirical baselines to advance memory-enhanced GUI agents toward more capable and human-like mobile automation."
        },
        {
            "title": "Impact Statement",
            "content": "This work advances mobile GUI agent evaluation through memory-centric benchmarking. Improved GUI agents can enhance accessibility and automate repetitive tasks. While we acknowledge potential misuse risks in autonomous automation, we encourage developing appropriate safeguards alongside capability improvements. 8 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments"
        },
        {
            "title": "References",
            "content": "Agashe, S., Wong, K., Tu, V., Yang, J., Li, A., and Wang, X. E. Agent s2: compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906, 2025. Ashcraft, M. H. Human memory and cognition. Scott, Foresman & Co, 1989. Li, W., Bishop, W., Li, A., Rawles, C., Campbell-Ajala, F., Tyamagundlu, D., and Riva, O. On the effects of data scale on computer control agents. arXiv e-prints, pp. arXiv2406, 2024. Liu, G., Zhao, P., Liang, Y., Liu, L., Guo, Y., Xiao, H., Lin, W., Chai, Y., Han, Y., Ren, S., et al. Llm-powered gui agents in phone automation: Surveying progress and prospects. arXiv preprint arXiv:2504.19838, 2025. Chai, Y., Huang, S., Niu, Y., Xiao, H., Liu, L., Zhang, D., Ren, S., and Li, H. Amex: Android multi-annotation arXiv preprint expo dataset for mobile gui agents. arXiv:2407.17490, 2024. Lu, Q., Shao, W., Liu, Z., Meng, F., Li, B., Chen, B., Huang, S., Zhang, K., Qiao, Y., and Luo, P. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024. Chai, Y., Li, H., Zhang, J., Liu, L., Liu, G., Wang, G., Ren, S., Huang, S., and Li, H. A3: Android agent arena for mobile gui agents. arXiv preprint arXiv:2501.01149, 2025. Chen, J., Yuen, D., Xie, B., Yang, Y., Chen, G., Wu, Z., Yixing, L., Zhou, X., Liu, W., Wang, S., et al. Spa-bench: comprehensive benchmark for smartphone agent evaluation. In NeurIPS 2024 Workshop on Open-World Agents, 2024. Cheng, K., Sun, Q., Chu, Y., Xu, F., Li, Y., Zhang, J., and Wu, Z. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Gu, Z., Zeng, Z., Xu, Z., Zhou, X., Shen, S., Liu, Y., Zhou, B., Meng, C., Xia, T., Chen, W., et al. Ui-venus technical report: Building high-performance ui agents with rft. arXiv preprint arXiv:2508.10833, 2025. Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Lee, J., Min, T., An, M., Hahm, D., Lee, H., Kim, C., and Lee, K. Benchmarking mobile device control agents across diverse configurations. arXiv preprint arXiv:2404.16660, 2024. Lu, Q., Shao, W., Liu, Z., Du, L., Meng, F., Li, B., Chen, B., Huang, S., Zhang, K., and Luo, P. Guiodyssey: comprehensive dataset for cross-app gui navigation on mobile devices. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2240422414, 2025. Maharana, A., Lee, D.-H., Tulyakov, S., Bansal, M., Barbieri, F., and Fang, Y. Evaluating very long-term conversational memory of LLM agents. arXiv preprint arXiv:2405.15241, 2024. Murdock, B. B. Human memory. New York, 1974. Qin, Y. et al. Ui-tars: An agent-based vqa dataset for uis. 2025. Rawles, C., Clinckemaillie, S., Chang, Y., Waltz, J., Lau, G., Fair, M., Li, A., Bishop, W., Li, W., CampbellAjala, F., et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. Wang, J., Xu, H., Jia, H., Zhang, X., Yan, M., Shen, W., Zhang, J., Huang, F., and Sang, J. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. Advances in Neural Information Processing Systems, 37:26862710, 2024a. Wang, L., Deng, Y., Zha, Y., Mao, G., Wang, Q., Min, T., Chen, W., and Chen, S. Mobileagentbench: An efficient and user-friendly benchmark for mobile llm agents. arXiv preprint arXiv:2406.08184, 2024b. Wang, Z., Xu, H., Wang, J., Zhang, X., Yan, M., Zhang, J., Huang, F., and Ji, H. Mobile-agent-e: Self-evolving arXiv preprint mobile assistant for complex tasks. arXiv:2501.11733, 2025. Wen, H., Li, Y., Liu, G., Zhao, S., Yu, T., Li, T. J.-J., Jiang, S., Liu, Y., Zhang, Y., and Liu, Y. Autodroid: LlmIn Proceedings powered task automation in android. of the 30th Annual International Conference on Mobile Computing and Networking, pp. 543557, 2024. 9 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Wu, D., Wang, H., Yu, W., Zhang, Y., Chang, K.-W., and Yu, D. Longmemeval: Benchmarking chat assistants on long-term interactive memory. arXiv preprint arXiv:2410.10813, 2024. Xu, Y., Liu, X., Sun, X., Cheng, S., Yu, H., Lai, H., Zhang, S., Zhang, D., Tang, J., and Dong, Y. Androidlab: Training and systematic benchmarking of android autonomous agents. arXiv preprint arXiv:2410.24024, 2024. Ye, J., Zhang, X., Xu, H., Liu, H., Wang, J., Zhu, Z., Zheng, Z., Gao, F., Cao, J., Lu, Z., et al. Mobile-agent-v3: Foundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025. Zhang, C., huang, z., Li, Y., Qian, C., Yu, Y., Liu, P., Liu, Z., Zhou, Z., Zhang, Y., Yuan, L., et al. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023. Zhang, L., Wang, S., Jia, X., Zheng, Z., Yan, Y., Gao, L., Li, Y., and Xu, M. Llamatouch: faithful and scalable testbed for mobile ui task automation. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, pp. 113, 2024. Zhao, Y., Huang, J., Hu, J., Wang, X., Mao, Y., Zhang, D., Jiang, Z., Wu, Z., Ai, B., Wang, A., Zhou, W., and Chen, Y. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. URL https://arxiv.org/abs/ 2408.05517. Zheng, B., Gou, B., Kil, J., Sun, H., and Su, Y. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. Zhong, W., Guo, L., Gao, Q., Ye, H., and Wang, Y. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1972419731, 2024. MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments A. Appendix"
        },
        {
            "title": "Appendix Table of Contents",
            "content": "A.1 Related Work A.1.1 Evaluation Environment Limitations A.1.2 Evaluation Pipeline Limitations A.2 Details of Integrated Agents A.3 Detailed Memory Implementations A.3.1 Short-Term Memory Implementations A.3.2 Long-Term Memory Implementations A.4 Details of Task Suite Design A.4.1 Application Selection Strategy A.4.2 Task Suite Characteristics A.4.3 Memory-Intensive Task Design A.4.4 Information Retention Pathways A.4.5 Mirror Task Pairs for Long-Term Learning A.5 Details of Snapshot-based Plug-and-Play Architecture A.5.1 Parallel Experiment Implementation A.5.2 Long-Term Memory Support Through Multi-Attempt Mechanism A.5.3 Comprehensive Agent Integration A.5.4 Advantages Over Existing Approaches A.6 Details of Memory-Specialized Metrics A.6.1 Short-Term Memory Assessment Metrics A.6.2 Long-Term Memory Assessment Metrics A.6.3 Execution Efficiency Assessment Metrics A.6.4 Computational Considerations A.7 Details of Evaluation Pipeline Validation A.7.1 Experimental Setup Details A.7.2 Detailed Results Analysis A.7.3 Detailed Performance Breakdown A.7.4 Key Validation Insights A.8 Analysis of Failure Cases A.8.1 Failure Mode Definitions A.8.2 Agent-Specific Failure Distribution Analysis A.8.3 Cross-Agent Failure Pattern Analysis A.8.4 Design Implications for Future Memory-Enhanced GUI Agents A.9 Additional Experimental Results A.9.1 Detailed Memory Performance Tables A.9.2 Long-Term Learning Analysis A.9.3 Performance Analysis by Cross-Application Complexity A.9.4 Memory Ablation Study A.9.5 Test-Time Compute Normalized Evaluation A.10 MEMGUI-EVAL Case Studies A.11 Details of Prompts for MEMGUI-EVAL A.11.1 Stage 1: Cost-Effective Triage Prompts A.11.2 Stage 2: Full Semantic Analysis Prompts A.11.3 Stage 3: Targeted Visual Verification Prompts A.11.4 IRR Analyzer: Memory Failure Quantification 11 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments A.1. Related Work The rapid development of Mobile GUI agents has been accompanied by the emergence of various benchmarks designed to evaluate their performance. These benchmarks can be broadly categorized into two types: static Mobile GUI agent datasets that provide instructions with corresponding operation trajectories (Lu et al., 2024; Li et al., 2024; Chai et al., 2024; Cheng et al., 2024), and dynamic benchmarks that provide task instructions along with corresponding evaluation environments and automated evaluators (Chai et al., 2025; Rawles et al., 2024; Chen et al., 2024). Dynamic Mobile GUI agent benchmarks have achieved consensus for evaluating agent performance in real-world scenarios due to their ability to assess agents in authentic, interactive environments. Table 6. Comprehensive comparison of MemGUI-Bench with existing smartphone agent benchmarks across three key dimensions: evaluation environment, evaluation pipeline, and agent support. indicates feature supported; indicates feature not supported."
        },
        {
            "title": "Memory\nTasks",
            "content": "Cross-app Tasks"
        },
        {
            "title": "Total\nTasks",
            "content": "3rd-party Apps"
        },
        {
            "title": "Auto\nReset",
            "content": "Long-term Memory"
        },
        {
            "title": "Memory\nMetrics",
            "content": "AndroidArena AndroidWorld AndroidLab LlamaTouch B-MoCA MobileAgentBench A3 SPA-Bench MemGUI-Bench 22 6 45 0 0 0 9 40 115 RULE-BASED EVALUATION PIPELINE 22 6 0 0 0 221 116 138 495 60 100 LLM-AS-A-JUDGE EVALUATION PIPELINE 201 340 128 0 40 100 1/4 1/1 1/4 1/1 1/1 1/ 1/2 1/7 4/7 1 3 4 4 3 5 6 11 12 However, as shown in Table 6, none of the current Mobile GUI agent benchmarks systematically and comprehensively evaluate the memory capabilities of Mobile GUI agents. This limitation stems from two fundamental issues in current benchmark design: A.1.1. EVALUATION ENVIRONMENT LIMITATIONS Current benchmark environments face significant constraints that hinder comprehensive memory evaluation: Task Design Inadequacy. The first issue lies in task design. Current benchmarks severely under-represent memory-intensive tasks. As shown in Table 6, even the most memory-focused benchmarks like SPA-Bench (Chen et al., 2024) contain only 40 memory tasks out of 340 total tasks (11.8%), while many benchmarks like LlamaTouch (Zhang et al., 2024) and MobileAgentBench (Wang et al., 2024b) contain zero memory tasks. Similarly, cross-app tasks, which are essential for evaluating information retention across application boundaries, are limited or absent in most benchmarks. This task design fundamentally cannot comprehensively evaluate Mobile GUI agents memory capabilities. Human-like memory in GUI interaction requires two core abilities: i) short-term memory that creates temporary information buffers during complex tasks (e.g., remembering verification codes, product prices for comparison), and ii) long-term memory that accumulates experience from each interaction to form reusable skills. Current benchmark tasks are designed to minimize historical dependencies, with key decision information either always present in task instructions or requiring only vague contextual awareness rather than specific visual information from historical UI observations. Environment Scalability Constraints. The second limitation is evaluation environment scalability. While benchmarks like AndroidWorld (Rawles et al., 2024), AndroidLab (Xu et al., 2024), and MobileAgentBench (Wang et al., 2024b) support rapid environment reset for given tasks, they require manual script writing when adding new tasks, severely limiting scalability for memory-intensive evaluation scenarios. 12 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments A.1.2. EVALUATION PIPELINE LIMITATIONS Figure 8. Limitations of existing evaluation approaches for memory-intensive GUI tasks. Current evaluation pipelines face critical methodological challenges that impede accurate memory assessment: Success Rate Detection Issues. As illustrated in Figure 8, existing approaches for success rate (SR) detection fall into two categories: rule-based methods and LLM-as-a-Judge methods. Rule-based methods include: i) state-based approaches that detect device status and execution logs after task completion (Xu et al., 2024; Rawles et al., 2024; Zhang et al., 2024), ii) action-based approaches that analyze agent execution actions (Chai et al., 2025), and iii) hybrid approaches like MobileAgentBench (Wang et al., 2024b). The common problem with rule-based approaches is that rule formulation requires expert knowledge and has poor scalability. LLM-as-a-Judge methods utilize large language models to evaluate agent execution trajectories based on predefined evaluation criteria. However, different approaches handle visual information differently, each with distinct limitations for memory-intensive tasks. SPA-Bench (Chen et al., 2024) provides all screenshots from long trajectories containing dozens of steps to VLMs at once. With such overwhelming visual information, there is no guarantee that VLMs can focus on critical early memory information points, leading to information overload and key detail omission risks. Additionally, cross-application evaluation relies on manual templates and is not fully automated. A3 (Chai et al., 2025) employs sliding window approach for LLMs to check agent operation trajectories against critical state pool, which is similarly unsuitable for context-dependent memory-intensive tasks due to the fragmented nature of information processing. The common problem with current LLM-based approaches is their inability to effectively and accurately evaluate memory-intensive tasks. Metrics Limitations. Current evaluation metrics rely solely on SR to determine single-round task completion, lacking comprehensive assessment of short-term and long-term memory capabilities. No existing benchmark supports multi-attempt evaluation protocols (pass@k) necessary for assessing long-term memory and learning capabilities. As demonstrated in Table 6, MEMGUI-BENCH systematically addresses these limitations through Memory-Intensive Task Suite Design (Section 3.1), Snapshot-Based Plug-and-Play Framework (Section 3.2), and Progressive Scrutiny Evaluator (Section 4.2) with Memory-Specific Metrics (Section 4.1). 13 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments A.2. Details of Integrated Agents We evaluate 11 prominent GUI agents, which can be categorized based on their memory mechanisms. Agents with Long-Term Memory: Mobile-Agent-E (Wang et al., 2025), Agent-S2 (Agashe et al., 2025). Agents without Long-Term Memory: T3A (Rawles et al., 2024), M3A (Rawles et al., 2024), UI-TARS-1.5-7B (Qin et al., 2025), GUI-Owl-7B (Ye et al., 2025), UI-Venus-7B (Gu et al., 2025), CogAgent (Hong et al., 2024), Mobile-Agent-V2(Wang et al., 2024a), SeeAct(Zheng et al., 2024) and AppAgent (Zhang et al., 2023). All agent workflows use Gemini 2.5 Pro in no-thinking mode as their backbone model for fair comparison. All agent models are deployed on dual NVIDIA L40S-48G GPUs for experimental evaluation. CogAgent (Hong et al., 2024), deployment utilizes the scripts provided by SPA-Bench (Chen et al., 2024), while other models are deployed through the ms-swift infrastructure (Zhao et al., 2024). Table 7. Details of integrated GUI agents evaluated in MemGUI-Bench. Short-Term Memory Type LTM"
        },
        {
            "title": "Core Model",
            "content": "Agent-S2 (Agashe et al., 2025) Mobile-Agent-E (Wang et al., 2025) T3A"
        },
        {
            "title": "Workflow\nWorkflow\nWorkflow",
            "content": "Gemini-2.5-Pro Gemini-2.5-Pro Gemini-2.5-Pro M3A (Rawles et al., 2024)"
        },
        {
            "title": "Workflow",
            "content": "Gemini-2.5-Pro Mobile-Agent-V2 (Wang et al., 2024a) Workflow Workflow SeeAct (Zheng et al., 2024) Workflow AppAgent (Zhang et al., 2023) UI-Venus-7B (Gu et al., 2025)"
        },
        {
            "title": "Model",
            "content": "UI-TARS-1.5-7B (Qin et al., 2025)"
        },
        {
            "title": "Model",
            "content": "GUI-Owl-7B CogAgent (Hong et al., 2024)"
        },
        {
            "title": "Model",
            "content": "Gemini-2.5-Pro Gemini-2.5-Pro Gemini-2.5-Pro Fine-tuned Qwen2.5-VL-7B Fine-tuned Qwen2.5-VL-7B Fine-tuned Qwen2.5-VL-7B CogAgent-18B"
        },
        {
            "title": "UI\nRepresentation",
            "content": "Screenshot Screenshot Screenshot+UI Tree Screenshot+UI Tree Screenshot UI Tree Screenshot+UI Tree Screenshot"
        },
        {
            "title": "Memory Agent",
            "content": "Memory Agent Rule-based Action-Thought Action-Thought"
        },
        {
            "title": "Screenshot",
            "content": "Multi-turn Context + Action-Thought"
        },
        {
            "title": "Screenshot",
            "content": "Action-Thought"
        },
        {
            "title": "No History",
            "content": "A.3. Detailed Memory Implementations This section provides comprehensive technical analysis of memory implementations in mobile GUI agents, categorizing both short-term and long-term memory mechanisms observed across 11 prominent systems. Table 8 provides concise overview of these memory mechanisms and their representative frameworks. Table 8. Overview of memory implementations in mobile GUI agents. Memory Type & Implementation Representative Agents SHORT-TERM MEMORY No History Rule-based Action-Thought Multi-turn Context Memory Agent CogAgent (Hong et al., 2024) SeeAct (Zheng et al., 2024), Autodroid (Wen et al., 2024) AppAgent (Zhang et al., 2023), UI-Venus (Gu et al., 2025), GUI-Owl (Ye et al., 2025), UI-TARS (Qin et al., 2025) UI-TARS (Qin et al., 2025) T3A (Rawles et al., 2024), M3A (Rawles et al., 2024), Agent-S2 (Agashe et al., 2025), Mobile-Agent-E (Wang et al., 2025), Mobile-Agent-V2 (Wang et al., 2024a) LONG-TERM MEMORY"
        },
        {
            "title": "Failure Learning\nSuccess Utilization",
            "content": "Agent-S2 (Agashe et al., 2025), Mobile-Agent-E (Wang et al., 2025) Mobile-Agent-E (Wang et al., 2025), Agent-S2 (Agashe et al., 2025) A.3.1. SHORT-TERM MEMORY IMPLEMENTATIONS Memory Agent Architecture. The most sophisticated approach employs dedicated memory modules to maintain structured context throughout task execution. Frameworks like T3A, M3A, Mobile-Agent-E, Agent-S2, and Mobile-Agent-V2 implement specialized memory agents that continuously summarize and update action history. This architecture typically involves primary action agent for decision-making and secondary memory agent for contextual management, creating comprehensive textual summaries that serve as memory context for subsequent decisions. Action-Thought Pattern. Many agents implement explicit reasoning chains where each action is accompanied by 14 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments corresponding thought processes. AppAgent, UI-Venus, and GUI-Owl exemplify this approach by outputting both actions and reasoning, creating structured action histories that capture not only what was done but why it was done. This textual action history serves as memory context for future decision-making steps. Multi-turn Context Management. UI-TARS leverages multi-turn dialogue mechanisms, where each interaction round adds new observational information while maintaining conversation history. This approach treats memory as an evolving dialogue context, though it faces limitations due to context length constraints in practical deployments. Rule-based Context Aggregation. SeeAct and Autodroid implement rule-based decision-making where each step involves selecting UI elements and combining them with corresponding actions. The resulting action sequences are concatenated to form contextual prompts for subsequent decisions, creating structured but rigid form of memory representation. No Historical Context. CogAgent represents the minimal memory approach, making decisions based solely on current observations and task instructions without maintaining any form of action history or memory context. This approach serves as baseline for understanding the impact of memory mechanisms. A.3.2. LONG-TERM MEMORY IMPLEMENTATIONS Success-Based Learning. Mobile-Agent-E and Agent-S2 implement systematic approaches to extract reusable knowledge from successful task executions. Mobile-Agent-E creates shortcuts from successful interaction patterns that can be directly invoked in similar future scenarios, while Agent-S2 distills successful experiences into actionable tips that guide future task execution. These approaches focus on transforming successful patterns into reusable procedural knowledge. Failure-Based Learning. Both Agent-S2 and Mobile-Agent-E incorporate mechanisms to learn from failure experiences. They analyze failed task attempts to extract lessons about common pitfalls, incorrect interaction patterns, and environmental constraints. These failure insights are then used to prompt future task execution, helping agents avoid previously encountered errors and improve decision-making quality. Evolution and Trends. The evolution of these memory mechanisms reflects increasing sophistication in contextual management and cross-session learning capabilities. Short-term memory implementations have progressed from basic action-thought approaches to specialized memory agent frameworks that maintain structured context throughout task execution. Long-term memory remains in early exploration stages, primarily focusing on learning from both successful and failed experiences to improve future task performance. The evolution from basic action-thought patterns to sophisticated memory agent architectures demonstrates the fields growing recognition of memorys critical role in mobile GUI automation. However, long-term memory implementations remain in early exploration stages, with most systems focusing on simple experience aggregation rather than more sophisticated learning mechanisms found in human cognition. A.4. Details of Task Suite Design This section provides comprehensive technical details for the memory-intensive task suite design presented in Section 3.1. The complete task suite specifications are presented in Table 10. A.4.1. APPLICATION SELECTION STRATEGY Our application selection was guided by two complementary approaches to ensure both representativeness and experimental feasibility. First, we curated high-frequency, representative applications from established mobile GUI research (Lu et al., 2024; Chai et al., 2024), encompassing both Android native system applications (Settings, Files, Messages) and popular third-party applications (Amazon, Apartments.com, Citymapper). This selection ensures coverage of diverse interaction paradigms and real-world usage scenarios. Second, we enforced two critical technical constraints for experimental reliability. Emulator Compatibility: Unlike applications such as (formerly Twitter), Facebook, and Instagram that are incompatible with Android emulators and require physical devices for testing (Chen et al., 2024), our selected applications function reliably in emulated environments, enabling scalable and reproducible experiments. Login-Free Operation: To facilitate rapid environment reset through Android snapshots, we prioritized applications whose core functionalities are accessible without user authentication. This design choice eliminates the need for manual cleanup of user-generated data (favorites, search history, etc.) and enables automated state recovery. Our analysis confirmed that Amazon, Apartments.com, and Citymapper provide comprehensive functionality in guest mode, satisfying our experimental requirements while maintaining task authenticity. 15 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments A.4.2. TASK SUITE CHARACTERISTICS The benchmark provides structured metadata for each task, including task description that captures authentic user intentions, and golden steps determined by human annotators executing tasks in real environments. Based on these golden steps, we categorize tasks into three difficulty levels: Easy (1-20 steps), Medium (21-40 steps), and Hard (41+ steps), ensuring balanced evaluation across different complexity scales. All task examples were manually annotated by human experts to ensure high quality and alignment with real-world usage patterns. The creation process followed rigorous protocol: Human Annotation: Human experts manually crafted the task descriptions and executed the tasks on the target Android emulators to record the golden steps. This ensures that every task is verifiable and executable within the specific app versions and emulator environment. Cross-Validation: We implemented three-person cross-validation process. For each task, one expert designed the initial instruction and golden path. second expert independently verified the tasks executability and the optimal nature of the golden steps. third expert resolved any discrepancies. This rigorous human-in-the-loop validation ensures the rationality, clarity, and correctness of all evaluation examples. A.4.3. MEMORY-INTENSIVE TASK DESIGN Building upon our definition of short-term memory as the agents ability to temporarily retain and utilize contextual information during task execution (Section 2), we designed 115 memory-intensive tasks alongside 13 standard tasks. Memory-intensive tasks demand agents to create temporary information buffers during complex interactions, such as remembering verification codes for registration, retaining product prices for comparison across applications, or maintaining intermediate results across multiple interaction steps. To ensure comprehensive evaluation across diverse real-world scenarios, we curated tasks spanning multiple categories. Table 9 presents the detailed hierarchical distribution of task categories, confirming balanced coverage across key domains such as Shopping (31.1%), Information Retrieval (21.9%), Productivity (17.7%), and Financial Management (7.9%). The 13 standard tasks serve multiple evaluation purposes: they represent the contextual awareness component of short-term memory evaluation, provide baseline performance benchmarks for computing the Memory-Task Proficiency Ratio (MTPR), and support long-term memory assessment through our pass@k evaluation protocol. By comparing performance ratios between memory-intensive and standard tasks, we can objectively isolate and quantify agents memory-specific capabilities. A.4.4. INFORMATION RETENTION PATHWAYS Our memory-intensive tasks implement diverse information transfer patterns across application boundaries. These patterns range from single-app scenarios (e.g., FindAndCompareProducts: comparing product ratings and prices within Amazon to identify the best value item) to complex four-app workflows (e.g., AnalyzeApartmentCommute: extracting apartment details from Apartments.com, searching company addresses via Bing, calculating commute times through Citymapper, and recording analysis in Joplin). This hierarchical complexity ensures comprehensive evaluation of memory capabilities across different spatial and temporal scales. A.4.5. MIRROR TASK PAIRS FOR LONG-TERM LEARNING To support long-term memory evaluation, the 128 tasks are organized into 64 mirror task pairs with similar application combinations and cognitive demands but distinct specific requirements. This design enables systematic assessment of cross-task learning, where agents can potentially transfer knowledge and strategies from earlier task attempts to improve performance on related tasks. Table 10 provides the complete task suite with detailed specifications for each task, including task descriptions, applications involved, difficulty levels, and category classifications. Table 10. Task details for MemGUI-Bench task suite. Description App(s) #Apps Open the Amazon app, search for running shoes for men, then filter for the brand ASICS and size 10. Open the Amazon app, search for womens handbag, then filter for the brand ALDO and color Black. [Amazon] [Amazon] 1 XApp Category RUM Steps Diff. [E-commerce: Product Search, E-commerce: Filter & Sort] [E-commerce: Product Search, E-commerce: Filter & Sort] 11 11 1 16 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Open the audio recorder app. Set the recording format to WAV, 48 kHz, Mono. Record an audio clip for more than 10 seconds, then stop the recording. Save the file with the name MyTestAudio. Open the audio recorder app. Set the recording format to M4a, 8 kHz, 48kbps. Record an audio clip for more than 15 seconds, then stop the recording. Save the file with the name M4aTestAudio. Open the BBC Sports app, find and tap on the Football category, proceed to Scores & Fixtures, and then perform search for Real Madrid. Open the BBC Sports app, navigate into the Football section, open the Scores & Fixtures view, and then execute search for Bayern Munich Open the bluecoins app, record an expense transaction for an amount of 89.95, named New Sneakers, in the Clothing category, and assign the label Personal to it. Open the bluecoins app, create an expense entry for Summer Dress with an amount of 65.00, categorized under Clothing, and add the Personal label to the transaction. Open the Clock app. Add two new alarms: one for 7:30 AM tomorrow labeled Morning Workout, and another for 10:15 PM tomorrow labeled Read Book. Then, navigate to the world clock and add Tokyo, Japan and London, UK. Finally, switch to the stopwatch and let it run for at least 15 seconds before stopping (but not resetting) it. Open the Clock app. Navigate to the timer and set three timers simultaneously: one for 15 minutes labeled Laundry, one for 45 minutes labeled Baking, and one for 1 hour 30 minutes labeled Study Session. After starting all three, go to the world clock, delete any existing cities, and add Sydney, Australia. Open the Clock app and add Beijing, China and New-York, USA to the world clock. By comparing their current times, find suitable meeting time for tomorrow that falls between 8 AM and 10 PM in both cities. Then, set an alarm for this meeting, using the local time in Beijing as the reference. Open the Clock app and add Beijing, China and Buenos Aires, Argentina to the world clock. By comparing their current times, find suitable meeting time for tomorrow that falls between 9 AM and 11 PM in both cities. Then, set an alarm for this meeting, using the local time in Beijing as the reference. In the Coursera app, search for courses offered by Stanford University. For the first six courses in the search results, find and remember each courses star rating, total number of reviews, and number of available languages. Then, for all six courses, calculate popularity score (star rating * number of reviews * number of languages). Finally, navigate to the course page with the highest calculated popularity score. In the Coursera app, search for courses offered by the University of Michigan. For the first six courses in the search results, find and remember each courses star rating, total number of reviews, and number of available languages. Then, for all six courses, calculate popularity score (star rating * number of reviews * number of languages). Finally, navigate to the course page with the highest calculated popularity score. Open the joplin app, create new note with the title Shopping List and the content Milk and bread. [audio recorder] [audio recorder] [BBC Sports] [BBC Sports] [bluecoins] [bluecoins] [Clock] [Clock] [Clock] [Clock] [coursera] [coursera] [joplin] Open the joplin app, create new note with the title Meeting Minutes and the content Discuss progress on Project A. [joplin] In the Meesho app, find the first search result for three sarees: Banarasi Silk, Kanjivaram Silk, and Paithani Silk. For each, remember its star rating and price. Then, navigate to the product page of the saree with the best value (highest rating-to-price ratio). In the Meesho app, find the first search result for three kurtas: Chikankari Kurta, Rayon Anarkali Kurta, and Jaipuri Cotton Kurta. For each, remember its star rating and price. Then, navigate to the product page of the kurta with the best value (highest rating-to-price ratio). In the Meesho app, first navigate to the Kids & Toys - Toys & Games category. From the first five results, find the item with the best value (highest rating-to-price ratio) and remember its name. Then, repeat this process for the Baby Gears category. Finally, navigate to the product page of the toy with the better value between the two you identified. In the Meesho app, first navigate to the Home & Kitchen - Kitchen Tools category. From the first five results, find the item with the best value (highest rating-to-price ratio) and remember its name. Then, repeat this process for the Storage & Organizers category. Finally, navigate to the product page of the item with the better value between the two you identified. In the Net-a-Porter app, find the following four items: the first black handbag, the first leather belt, the first cashmere scarf, and the first pair of white sneakers. For each item, remember its price and its designer or brand name (prioritize designer). Identify the most and least expensive items. If the designer/brand of the most expensive item comes first alphabetically, navigate to its page. Otherwise, navigate to the page of the least expensive item. In the Net-a-Porter app, find the following four items: the first red dress, the first pair of gold sandals, the first green skirt, and the first white top. For each item, remember its price and its designer or brand name (prioritize designer). Identify the most and least expensive items. If they are by the same designer/brand, navigate to the page of the most expensive item. Otherwise, navigate to the page of the least expensive item. Open the Setting app and go to the Navigation mode settings. Find the Circle to Search feature and turn its toggle switch to the off position. Open the Setting app and go to the Navigation mode settings. Find the Circle to Search feature and turn its toggle switch to the on position. Open the Wikipedia app, search for English Wikipedia and find the current number of articles it contains. Remember this number. Then, search for German Wikipedia and find its current number of articles. Go to and stay on the page of the Wikipedia edition that has more articles [Meesho] [Meesho] [Meesho] [Meesho] [Net-aPorter] [Net-aPorter] [Setting] [Setting] [Wikipedia] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 N N N N N N N 1 1 1 1 1 2 1 1 2 [Content Creation: Multimedia Creation, Device Configuration: Setting Adjustment] [Device Configuration: Setting Adjustment, Content Creation: Multimedia Creation] [Sports: Content Navigation, Sports: Data Extraction] [Sports: Content Navigation, Sports: Data Extraction] [Financial Management: Add Transaction] [Financial Management: Add Transaction] [Productivity: Time Management] N N 12 12 8 8 10 25 [Productivity: Time ManageDevice Configuration: ment, Setting Adjustment] 28 [Productivity: Time Management] [Productivity: Time Management, Device Configuration: Setting Adjustment] [Education & Learning: Course Search, Information Retrieval: Data Extraction, Education & Learning: Knowledge Acquisition] [Education & Learning: Course Search, Information Retrieval: Data Extraction, E-commerce: Review Analysis] [Productivity: Note Taking, Content Creation: Text Creation] [Productivity: Note Taking, Content Creation: Text Creation] [E-commerce: Product Search, E-commerce: Price Comparison, Information Retrieval: Data Extraction] [E-commerce: Product Search, E-commerce: Price Comparison, E-commerce: Review Analysis] [E-commerce: Category Navigation, E-commerce: Price Comparison, E-commerce: Review Analysis, Productivity: Note Taking] [E-commerce: Category Navigation, E-commerce: Price Comparison, E-commerce: Review Analysis] [E-commerce: Product Search, Information Retrieval: Data Extraction, E-commerce: Price Comparison] Y 13 15 29 2 Y 7 7 23 20 20 36 1 1 1 1 1 [E-commerce: Product Search, E-commerce: Price Comparison] 36 2 [Device Configuration: Setting Adjustment] [Device Configuration: Setting Adjustment] [Information Retrieval: Web Search, Information Retrieval: Data Extraction, Education & Learning: Knowledge Acquisition] N 6 3 13 1 1 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments N Y Y Y Y 1 1 1 2 2 2 2 2 2 2 2 2 2 2 Open the Wikipedia app, first search for Beijing and remember its population. Next, search for Shanghai and remember its population. Compare the two, and then go to and remain on the Wikipedia page for the city with the larger population [Wikipedia] In the Wish app, identify two items: the cheapest in Jewelry & watches - Fashion jewelry (from top 6), and the cheapest in the Watches sub-category (from top 6). Remember the number of reviews for both. Navigate to the page of whichever of these two items has more reviews. In the Wish app, identify two items: the one with the most reviews in Office & tech - Parts & storage (from top 6), and the one with the most reviews in the Hardware sub-category (from top 6). Remember the price for both. Navigate to the page of whichever of these two items is cheaper. Open the Amazon Kindle app and locate this months Bestsellers list. From the top 10, identify the three books with the highest number of customer ratings. For each, remember its title, price, and description. Open Joplin app and create note titled Top Rated Books listing the title, price, and description for all three books. Open the Amazon Kindle app and navigate to Bestsellers. Find the first three books that support audio narration. For each, remember its title, rating, and description. Open Joplin app and create note titled Audiobooks Found listing the title, rating, and description for all three books. In the Amazon Kindle app, find the title, customer rating, and page count for the first four books of the Dune series. Then, in Joplin, create note titled Dune Series Analysis listing the books ordered by highest rating, showing all collected data for each. [Wish] [Wish] [Amazon Kindle, joplin] [Amazon Kindle, joplin] [Amazon Kindle, joplin] In the Amazon Kindle app, find the title, customer rating, and page count for the first four books of the Song of Ice and Fire series. Then, in Joplin, create note titled Song of Ice and Fire Series Analysis listing the books ordered by highest rating, showing all collected data for each. [Amazon Kindle, joplin] Open Amazon app and search for these three products: Logitech C920, Razer Kiyo, Elgato Facecam. For each, find and remember its price, star rating, and total number of reviews. In Joplin app, create note titled Webcam Value Score. For each camera, calculate value score using the formula: (star rating * number of reviews) / price. List each camera and its score, then state which has the highest score. Open Amazon app and search for these three products: Kindle Paperwhite, Kobo Libra 2, reMarkable 2. For each, find and remember its price, screen size (in inches), and storage capacity (in GB). In Joplin app, create note titled E-reader Value Score. For each device, calculate value score using the formula: (screen size * storage capacity) / price. List each device and its score, then state which has the highest score. Open Amazon app, search for Anker 737 Power Bank. Find its price, capacity (in mAh), and all output port types. Then, search for MacBook Air M2 and find its required charging port. Next, search for iPhone 15 Pro and find its charging port. In Joplin app, note all the collected data and answer if the power bank can simultaneously charge both devices. Open Amazon app, search for Samsung T7 Shield SSD. Find its price, storage capacity, and read/write speeds. Then search for PlayStation 5 and find its USB port specifications. Next, search for Xbox Series and find its USB port specifications. In Joplin app, note all collected data and answer if the SSD is fully compatible with both consoles USB standards for external storage. Open the Amazon app. First, search for iPhone 15 Pro and remember its screen size, battery capacity, and storage options. Second, search for Samsung Galaxy S24 Ultra and remember the same three specifications. Third, search for Google Pixel 8 Pro and remember the same three specifications. Finally, open the Joplin app, create note titled Phone Spec Matrix, and list all nine specifications for the three phones. Open the Amazon app. First, search for Sony WH-1000XM5 and remember its weight, battery life, and Bluetooth version. Second, search for Bose QuietComfort Ultra Headphones and remember the same three specifications. Third, search for Sennheiser Momentum 4 and remember the same three specifications. Finally, open the Joplin app, create note titled Headphone Spec Matrix, and list all nine specifications for the three headphones. In the Amazon app, search for the Instant Pot Duo and navigate to its customer reviews section. Read and remember the full original text of the top ten reviews listed. Do not use the copy function. Then, open the Joplin app and create new note titled Instant Pot - Full Reviews. In the note, accurately type out the full original text for all ten reviews you remembered. In the Amazon app, search for the Bose QuietComfort Ultra Headphones and navigate to its customer reviews section. Read and remember the full original text of the top ten reviews listed. Do not use the copy function. Then, open the Joplin app and create new note titled Bose QC - Full Reviews. In the note, accurately type out the full original text for all ten reviews you remembered. Open the Amazon app, search for 32GB DDR5 RAM, filter for the Corsair brand, and remember the price and clock speed of the first item in the search results. Then, open the Bing app and search for ASUS ROG Strix Z790-E motherboard maximum supported memory speed. Finally, confirm if the clock speed of that Corsair RAM is less than or equal to the motherboards maximum supported speed. Directly answer with its price if it is compatible, or Not compatible if it isnt. [Amazon, joplin] [Amazon, joplin] [Amazon, joplin] [Amazon, joplin] [Amazon, joplin] [Amazon, joplin] [Amazon, joplin] [Amazon, joplin] [Amazon, bing] 13 19 37 37 29 45 1 1 2 2 2 2 45 3 24 24 34 1 1 27 2 55 55 3 14 [Information Retrieval: Web Search, Information Retrieval: Data Extraction, Education & Learning: Knowledge Acquisition] [E-commerce: Category Navigation, E-commerce: Price Comparison, E-commerce: Review Analysis] [E-commerce: Category Navigation, E-commerce: Review Analysis, E-commerce: Price Comparison] [E-commerce: Category Navigation, E-commerce: Review Analysis, Information Retrieval: Data Extraction, Productivity: Note Taking] [E-commerce: Category Navigation, E-commerce: Filter & Sort, Information Retrieval: Data Extraction, Productivity: Note Taking] [E-commerce: Product Search, Information Retrieval: Data Extraction, E-commerce: Review Analysis, Productivity: Note Taking, E-commerce: Filter & Sort] [E-commerce: Product Search, Information Retrieval: Data Extraction, E-commerce: Review Analysis, Productivity: Note Taking] [E-commerce: Product Search, Information Retrieval: Data Extraction, Productivity: Note Taking, Financial Management: Financial Ecommerce: Price Comparison] [E-commerce: Product Search, E-commerce: Price Comparison, E-commerce: Specification Comparison, Information Retrieval: Data Extraction, Productivity: Note Taking] [E-commerce: Product Search, Compatibility E-commerce: Check, Note Taking] Productivity: Calculation, [E-commerce: Product Search, E-commerce: Specification Comparison, E-commerce: Compatibility Check, Productivity: Note Taking] [E-commerce: Product Search, E-commerce: Specification Comparison, Information Retrieval: Data Extraction, Productivity: Note Taking] [E-commerce: Product Search, E-commerce: Specification Comparison, E-commerce: MultiApp Comparison, Information Retrieval: Data Extraction, Productivity: Note Taking, Content Creation: Text Creation] [E-commerce: Product Search, E-commerce: Review Analysis, Information Retrieval: Data Extraction, Productivity: Note Taking, Content Creation: Text Creation] [E-commerce: Product Search, E-commerce: Review Analysis, Information Retrieval: Data Extraction, Productivity: Note Taking, Content Creation: Text Creation] [E-commerce: Product Search, E-commerce: Filter & Sort, Information Retrieval: Web Search, E-commerce: Compatibility Check, Information Retrieval: Data Extraction] MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Open the Amazon app, search for 1TB External SSD, filter for the Samsung brand, and remember the price and the read/write speed of the first item in the search results. Then, open the Bing app and search for PlayStation 5 external SSD speed requirement. Finally, confirm if the SSDs speed meets or exceeds the PS5s requirement. Directly answer with its price if it is compatible, or Not compatible if it isnt. [Amazon, bing] 2 2 2 2 2 2 [AP News, joplin] [AP News, joplin] [Apartments.com Rental Search, joplin] [Apartments.com Rental Search, joplin] [Apartments.com Rental Search, joplin] [Apartments.com Rental Search, joplin] [AutoUncle:Search used Calculator] cars, [AutoUncle:Search used Calculator] cars, 2 [AutoUncle:Search used joplin] cars, [AutoUncle:Search used joplin] cars, 2 2 [AutoUncle:Search used messages] cars, In the AP News app, find the three most recent articles from the U.S. NEWS section and the three most recent from the World section. For each of the six articles, open it, read the full text, and remember its title and main content. Then, in the Joplin app, create note titled News Digest. For each of the six articles, list its title followed by 50-word summary of its content, grouped by section. In the AP News app, find the three most recent articles from the Technology section and the three most recent from the Business section. For each of the six articles, open it, read the full text, and remember its title and main content. Then, in the Joplin app, create note titled Tech & Business Digest. For each of the six articles, list its title followed by 50-word summary of its content, grouped by section. In the Apartments.com app, search for listings in Austin, TX. For the first ten results, enter each detail page and remember the address, monthly rent, and square footage. Then, open the Joplin app and create note titled Austin Apartment Data. In the note, list all ten apartments, ordered from the largest square footage to the smallest, including their address, rent, and square footage for each. In the Apartments.com app, search for listings in Denver, CO. For the first ten results, enter each detail page and remember the address, monthly rent, and number of bedrooms. Then, open the Joplin app and create note titled Denver Apartment Data. In the note, list all ten apartments, ordered from the lowest rent to the highest, including their address, rent, and number of bedrooms for each. Open the Apartments.com app, search for listings in San Francisco, and filter for fitness center and pool. Then, go into the detail pages for the first three results and remember the address and phone number for each. Finally, open the Joplin app, create new note, and record all collected addresses and phone numbers. Open the Apartments.com app, search for listings in New York, NY, and filter for cat friendly and in-unit washer. Then, go into the detail pages for the first three results and remember the address and phone number for each. Finally, open the Joplin app, create new note, and record all collected addresses and phone numbers. In the AutoUncle app (UK), first search for Ford Focus and remember the price and mileage of the first non-sponsored result. Then, separately search for Vauxhall Corsa and remember the price and mileage of its first nonsponsored result. In the Calculator app, determine which car has lower price-to-mileage ratio (price/mileage). Then, for that better-value car, calculate the loan amount needed for an 80% financing. In the AutoUncle app (Germany), first search for Mercedes-Benz C-Class and remember the price and mileage of the first non-sponsored result. Then, separately search for BMW 3 Series and remember the price and mileage of its first non-sponsored result. In the Calculator app, determine which car has lower price-to-mileage ratio (price/mileage). Then, for that better-value car, calculate the loan amount needed for an 80% financing. In the AutoUncle app (UK), find Honda Civic, Toyota Corolla, and Mazda 3. For each car, remember its engine size, fuel type, and price. In the Joplin app, create note titled Compact Car Comparison and list all three cars with all three of their specs. In the AutoUncle app (Germany), find BMW 3 Series, an Audi A4, and Mercedes-Benz C-Class. For each car, remember its mileage, transmission type, and price. In the Joplin app, create note titled German Sedan Comparison and list all three cars with all three of their specs. In the AutoUncle app (Germany), search for Audi with filters: after 2021, mileage below 50,000 km, automatic, petrol. From the results, find the most expensive car and the least expensive car. Remember the model and price for both. In the messages app, send Most Expensive: [Model] - [Price]. Least Expensive: [Model] - [Price]. to +8613911112222. In the AutoUncle app (UK), search for Land Rover with filters: after 2020, mileage below 80,000 km, diesel. From the results, find the car with the highest mileage and the car with the lowest mileage. Remember the price and mileage for both. In the messages app, send Highest Mileage: [Mileage]km - [Price]. Lowest Mileage: [Mileage]km - [Price]. to +8613933334444. [AutoUncle:Search used messages] cars, In Bing app, find the current exchange rates for USD to EUR and USD to GBP. Remember both rates. In the Calculator app, first calculate how much $1500 USD is in EUR. Then, calculate how much the resulting EUR amount is worth in GBP (this requires second conversion step using the two initial rates). Directly answer with the final GBP amount. In Bing app, find the current stock prices for NVIDIA (NVDA) and Apple (AAPL). Remember both prices. In the Calculator app, first calculate the value of 50 NVDA shares. Then, calculate the value of 75 AAPL shares. Finally, calculate the total combined value of both holdings. Directly answer with the final combined value. On the NASA APOD website found via Bing app, locate the three most recent daily pictures. For each of the three pictures, create separate note in Joplin app. Each note must use the pictures title as its own title, contain brief visual description, and have the corresponding image directly inserted or attached into the notes body. [bing, Calculator] [bing, Calculator] [bing, joplin] 2 2 19 Y Y Y Y Y 17 1 95 95 3 160 160 3 17 19 59 1 1 59 3 51 35 3 3 35 2 35 35 51 2 2 [E-commerce: Product Search, E-commerce: Filter & Sort, Information Retrieval: Web Search, E-commerce: Compatibility Check, E-commerce: Multi-App Comparison, Information Retrieval: Data Extraction] [Information Retrieval: Data Extraction, Education & Learning: Knowledge Acquisition, Productivity: Note Taking, Content Creation: Text Creation] [Information Retrieval: Data Extraction, Productivity: Note Taking, Content Creation: Text Creation] [E-commerce: Product Search, Information Retrieval: Data Extraction, Productivity: Note Taking] [E-commerce: Product Search, Information Retrieval: Data Extraction, Productivity: Note Taking] [Travel & Navigation: Local Search, E-commerce: Filter & Sort, Information Retrieval: Data Extraction, Productivity: Note Taking] [E-commerce: Product Search, E-commerce: Filter & Sort, Information Retrieval: Data Extraction, Productivity: Note Taking] [E-commerce: Product Search, Information Retrieval: Data Extraction, E-commerce: Price Comparison, Financial Management: Financial Calculation] [E-commerce: Product Search, E-commerce: Multi-App Comparison, Information Retrieval: Data Extraction, Financial Management: Financial Calculation] [E-commerce: Product Search, E-commerce: Specification Comparison, Productivity: Note Taking] [E-commerce: Product Search, E-commerce: Specification Comparison, E-commerce: MultiApp Comparison, Productivity: Note Taking] [E-commerce: Product Search, E-commerce: Filter & Sort, E-commerce: Price Comparison, Communication: Messaging, Communication: Data Sharing] [E-commerce: Product Search, E-commerce: Filter & Sort, Ecommerce: Specification Comparison, Information Retrieval: Data Extraction, Communication: Messaging, Communication: Data Sharing] [Information Retrieval: Fact Checking, Financial Management: Financial Calculation] Fact [Information Retrieval: Information ReChecking, trieval: Extraction, Data Financial Management: Financial Calculation] [Information Retrieval: Web Search, Information Retrieval: Data Extraction, Information Retrieval: Image Analysis, Productivity: Note Taking, Content Creation: Text Creation] MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Y Y Y Y Y On the Smithsonian Magazine Photo of the Day page found via Bing app, locate the three most recent daily photos. For each of the three photos, create separate note in Joplin app. Each note must use the photos title as its own title, contain brief description, and have the corresponding image directly inserted or attached into the notes body. In Bing app, find the date for next years Thanksgiving and for next years Easter. Remember both dates. Open the calendar app. Create an all-day event on the Thanksgiving date named Thanksgiving Dinner. Then, create second all-day event on the Easter date named Easter Egg Hunt. In Bing app, find the date for the next leap day and for next years Halloween. Remember both dates. Open the calendar app. Create an all-day event on the leap day named Leap Day Fun. Then, create second all-day event on the Halloween date named Halloween Party. In the Bing app, search for the host cities and years of the next three Summer Olympics. Remember all three cities and years. Open the Calendar app and create an all-day event on July 1st of the first Olympic year titled [City] Olympics. Then, create two more all-day events on July 1st of the subsequent Olympic years with their appropriate titles. In the Bing app, search for the host countries and years of the next three FIFA World Cups. Remember all three countries and years. Open the Calendar app and create an all-day event on June 1st of the first World Cup year titled [Country] World Cup. Then, create two more all-day events on June 1st of the subsequent World Cup years with their appropriate titles. Open the bing app and perform an image search for Global Smartphone Shipments Market Share 2021. From the image results, locate and carefully analyze the chart that specifically displays the data for Q3 2021. Identify the top three brands from this Q3 chart, and remember their names and their exact market share percentages. Finally, open the joplin app, create new note titled Smartphone Market Share 2021 Q3, and list the top three brands with their corresponding percentages. Open the bing app and perform an image search for Global Vehicle Sales Trend by region November 2023. Carefully analyze the first clear chart that appears in the search results. From this chart, identify the top three regions with the highest sales growth or volume, and remember the names of these regions and their corresponding data values (e.g., sales numbers or percentage growth). Finally, open the joplin app, create new note titled Vehicle Sales Trend November 2023, and list the top three regions with their data In the Citymapper app, plan route from Central Park, NYC to JFK Airport. Find the time, cost, and number of transfers for four transport options: Public Transit, Driving, Taxi, and Bikeshare. In the Joplin app, create note titled JFK Transit Analysis, list the full data for all four options, then summarize which is the fastest, cheapest, and has the fewest transfers. In the Citymapper app, plan route from Golden Gate Bridge, SF to SFO Airport. Find the time, cost, and number of transfers for four transport options: Public Transit, Driving, Taxi, and Walking. In the Joplin app, create note titled SFO Transit Analysis, list the full data for all four options, then summarize which is the fastest, cheapest, and has the fewest transfers. In the Citymapper app, find the travel times for two-leg journey in Washington D.C.: 1) The White House to the Lincoln Memorial, and 2) the Lincoln Memorial to the National Air and Space Museum. For each leg, find the times for Public Transit, Walking, and Taxi. Determine the fastest possible total travel time by combining the modes for each leg. Send message to +8613811118888 stating this fastest route combination and the total time. In the Citymapper app, find the travel times for two-leg journey in New York City: 1) Statue of Liberty to the Empire State Building, and 2) the Empire State Building to The Metropolitan Museum of Art. For each leg, find the times for Public Transit, Walking, and Taxi. Determine the fastest possible total travel time by combining the modes for each leg. Send message to +8613822229999 stating this fastest route combination and the total time. Open the Citymapper app, plan public transport route from London Eye to The British Museum, and remember the estimated travel time. Then open the calendar app and create an event for next Monday at 10 AM titled Visit British Museum, setting the event duration to the travel time you remembered. Open the Citymapper app, plan walking route from Notre-Dame Cathedral to Louvre Museum, and remember the estimated travel time. Then open the calendar app and create an event for next Tuesday at 3 PM titled Trip to the Louvre, setting the event duration to the travel time you remembered. Open the Coursera app, search for the Financial Markets course, and find its total time to complete. Then, open the Calendar app. Create recurring event titled Study Finance for all 7 days of next week (Monday to Sunday). Set the duration for each daily event by dividing the courses total completion time equally across the seven days. Open the Coursera app and find the total time required for The Science of Well-Being. Then, open the Calendar app to create daily event, Study Finance, from the 6th to the 15th of next month, starting at 2 PM. Calculate the daily duration by dividing the total course time by 10. In the DeepL Translate app, translate the following paragraph into five languages: Spanish, Japanese, Russian, Arabic, and Portuguese: The projects quarterly review meeting is scheduled for next Monday. Key discussion topics will include the budget forecast for Q4, which is currently estimated at $1,250,000, and the initial user feedback analysis from the beta test group. After remembering all five translations, open the messages app. Important: Do not use the copy-paste function for the translations. Send each translation as separate message to different recipient: the Spanish translation to +8613100001111, Japanese to +8613100002222, Russian to +8613100003333, Arabic to +8613100004444, and Portuguese to +8613100005555. 2 2 2 2 2 2 2 2 2 2 2 2 2 [bing, joplin] [bing, calendar] [bing, calendar] [bing, calendar] [bing, calendar] [bing, joplin] [bing, joplin] [Citymapper, joplin] [Citymapper, joplin] [Citymapper, messages] [Citymapper, messages] [Citymapper, calendar] [Citymapper, calendar] [coursera, calendar] [coursera, calendar] [DeepL Translate, messages] 3 2 2 2 2 2 2 1 2 2 2 [Information Retrieval: Image Search & Understanding, Productivity: Note Taking] 51 [Information Retrieval: Fact Checking, Productivity: Time Management] [Information Retrieval: Web Search, Information Retrieval: Fact Checking, Productivity: Time Management] [Information Retrieval: Web Search, Information Retrieval: Data Extraction, Productivity: Time Management] [Information Retrieval: Web Search, Information Retrieval: Data Extraction, Productivity: Time Management] [Information Retrieval: Image Search & Understanding, Information Retrieval: Data Extraction, Productivity: Note Taking] [Information Retrieval: Image Search & Understanding, Information Retrieval: Data Extraction, Productivity: Note Taking] [Travel & Navigation: Route Planning, Information Retrieval: Data Extraction, Productivity: Note Taking] [Travel & Navigation: Route Planning, Information Retrieval: Data Extraction, Productivity: Note Taking] Y 35 35 40 40 17 25 25 17 1 [Travel & Navigation: Route Planning, Communication: Messaging] 24 [Travel & Navigation: Route Planning, Information Retrieval: Data Extraction, Communication: Messaging] 24 1 [Travel & Navigation: Route Planning, Productivity: Time Management] 40 40 40 [Travel & Navigation: Route Planning, Productivity: Time Management] [Education & Learning: Course Search, Information Retrieval: Data Extraction, Productivity: Time Management] [Education & Learning: Course Search, Information Retrieval: Data Extraction, Productivity: Time Management, Financial Management: Financial Calculation] [Content Creation: Translation, Communication: Messaging, Communication: Data Sharing] 40 2 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments In the DeepL Translate app, translate the following paragraph into five languages: German, French, Korean, Hindi, and Italian: Please be advised that due to system upgrade, network services will be unavailable from 11:00 PM on Friday until 5:00 AM on Saturday. The expected downtime is approximately 6 hours. We apologize for any inconvenience this may cause. After remembering all five translations, open the messages app. Important: Do not use the copy-paste function for the translations. Send each translation as separate message to different recipient: the German translation to +8613200001111, French to +8613200002222, Korean to +8613200003333, Hindi to +8613200004444, and Italian to +8613200005555. In Net-a-Porter app, for the designer Isabel Marant, find the first New In item from each of the following four categories: Dresses, Bags, Shoes, and Accessories. For each of the four items, remember its price and primary material/color. Then, in Joplin app, create single note titled Isabel Marant Collection listing the details for all four items. In Net-a-Porter app, for the designer Jimmy Choo, find the first item from each of the following four categories: Boots, Heels, Sandals, and Sneakers. For each of the four items, remember its price and primary material/color. Then, in Joplin app, create single note titled Jimmy Choo Collection listing the details for all four items. Open the Setting app and go to Special app access. First, view the apps with Wi-Fi control permission and remember the list of app names. Next, view the apps allowed Picture-in-picture access and remember that list of names. Finally, open the joplin app, create note titled App Access Permissions, and list the names you remembered under two headings: Wi-Fi Control and Picture-in-picture. Open the Setting app and go to Special app access. First, check how many apps have Wi-Fi control permission and remember the count. Then, check how many apps are allowed to Install unknown apps and remember that count. Finally, open the joplin app and create note titled App Permissions that records both counts. In the wikiHow app, sequentially find the main ingredients for the following five dishes: 1. Salad, 2. Spaghetti, 3. Fried Chicken, 4. Chocolate Cake, 5. Mashed Potatoes. After gathering the ingredients for all five dishes, open the Joplin app. Create single note titled Dinner Shopping List. In this note, create comprehensive shopping list, grouping all collected ingredients by category (e.g., Produce, Dairy, Pantry, Meat). In the wikiHow app, sequentially find the main ingredients for the following five items: 1. Snow Cones, 2. Egg Sandwiches, 3. Green Tea, 4. Feta Cheese (how to make), 5. Chicken Alfredo. After gathering all ingredients, open the Joplin app. Create single note titled Snack & Lunch Plan. In this note, first create comprehensive shopping list, grouping all ingredients by category. Then, create second heading named Preparation Order and list the five items in logical sequence for preparation. In the Yahoo Sports app, find the three most recent news articles in the NBA section and the three most recent in the MLB section. For each of the six articles, read it and remember its title and main content. Then, in Joplin, create note titled Sports News Summary. For each of the six articles, write down its title followed by 40-word summary of its content, grouped under NBA and MLB headings. In the Yahoo Sports app, find the three most recent news articles in the NFL section and the three most recent in the Mens Tennis section. For each of the six articles, read it and remember its title and main content. Then, in Joplin, create note titled General Sports Briefing. For each of the six articles, write down its title followed by 40-word summary of its content, grouped under NFL and Mens Tennis headings. Open Yahoo Sports app, go to the Soccer section and open the Premier League fixture list. Identify the next three scheduled league matches. For each match, remember the date and the two teams involved. Open Calendar app and create three separate events, each titled Open Yahoo Sports app, go to the Basketball section and open the NBA schedule. Identify the next three scheduled league games . For each game, remember the date and the two teams involved. Open Calendar app and create three separate events, each titled [Team A] vs [Team B]. Open the Amazon app. Search for and remember the price and star rating for these four components: AMD Ryzen 7 7800X3D, NVIDIA GeForce RTX 4070 Super, Corsair Vengeance 32GB DDR5 RAM, and Samsung 990 Pro 2TB SSD. Then, open the Calculator app and calculate the total cost of all four components. Finally, open the Joplin app, create note titled PC Build Cost & Rating, and list each component with its price, rating, and the calculated total cost at the end. Open the Amazon app. Search for and remember the price and star rating for these four components: Intel Core i9-14900K, AMD Radeon RX 7900 XTX, G.Skill Trident Z5 32GB DDR5 RAM, and WD Black SN850X 4TB SSD. Then, open the Calculator app and calculate the total cost of all four components. Finally, open the Joplin app, create note titled High-End PC Parts & Rating, and list each component with its price, rating, and the calculated total cost at the end. In the Amazon app, find the price and star rating for four components: AMD Ryzen 7 7800X3D, NVIDIA GeForce RTX 4070 Super, Samsung 990 Pro 2TB SSD, and Corsair Vengeance 32GB DDR5 RAM. In the Calculator app, calculate the subtotal, then final total by adding an 8% sales tax. Finally, in the Joplin app, create note titled AMD Build Analysis listing each component with its price and rating, plus the subtotal and final total. In the Amazon app, find the price and star rating for four components: Intel Core i9-14900K, NVIDIA GeForce RTX 4090, WD Black SN850X 4TB SSD, and G.Skill Trident Z5 64GB DDR5 RAM. In the Calculator app, calculate the subtotal, then final total by adding an 8% sales tax. Finally, in the Joplin app, create note titled Intel Build Analysis listing each component with its price and rating, plus the subtotal and final total. [DeepL Translate, messages] [Content Creation: Translation, Communication: Messaging, Communication: Data Sharing] 40 2 3 1 1 3 [E-commerce: Product Search, E-commerce: Category Navigation, Productivity: Note Taking] [E-commerce: Product Search, E-commerce: Category Navigation, Productivity: Note Taking] 65 65 [Information Retrieval: Data Extraction, Productivity: Note Taking] 13 [Device Configuration: Setting Adjustment, Information Retrieval: Data Extraction, Productivity: Note Taking] [Information Retrieval: Data Extraction, Productivity: Note Taking, Productivity: Checklist Management] [Information Retrieval: Data Extraction, Productivity: Note Taking, Productivity: Checklist Management] 55 55 3 [Sports: Content Navigation, Sports: Data Extraction, Productivity: Note Taking, Content Creation: Text Creation] [Sports: Content Navigation, Sports: Data Extraction, Productivity: Note Taking, Content Creation: Text Creation] 64 3 3 [Sports: Content Navigation, Sports: Data Extraction, Productivity: Time Management] [Sports: Content Navigation, Sports: Data Extraction, Productivity: Time Management] [E-commerce: Product Search, Financial Management: Financial Calculation, Productivity: Note Taking] Y 45 45 38 3 2 [E-commerce: Product Search, E-commerce: Review Analysis, Information Retrieval: Data Extraction, Financial Management: Financial Calculation, Productivity: Note Taking] [E-commerce: Product Search, Financial Management: Financial Calculation, Productivity: Note Taking] 38 59 3 [E-commerce: Product Search, Information Retrieval: Data Extraction, Financial Management: Financial Calculation, Productivity: Note Taking] 3 Y Y Y Y 2 2 2 2 2 2 2 2 3 3 3 3 [Net-a-Porter, joplin] [Net-a-Porter, joplin] [Setting, joplin] [Setting, joplin] [wikiHow, joplin] [wikiHow, joplin] [Yahoo Sports, joplin] [Yahoo Sports, joplin] [Team A] vs [Team B]. [Yahoo Sports, calendar] [Amazon, Calculator, joplin] [Amazon, Calculator, joplin] [Amazon, Calculator, joplin] [Amazon, Calculator, joplin] 21 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments In the Amazon app, search for Insta360 Ace Pro and go to its customer reviews section. Read and remember the full original text and the star rating of the top five reviews. Do not use the copy-paste function. Then, in the DeepL Translate app, translate the full text of all five reviews into French. Finally, in the Joplin app, create note titled Insta360 Review Analysis. For each of the five reviews, list its original star rating, its original full text, and its French translation. In the Amazon app, search for DJI Mini 4 Pro and go to its customer reviews section. Read and remember the full original text and the star rating of the top five reviews. Do not use the copy-paste function. Then, in the DeepL Translate app, translate the full text of all five reviews into Russian. Finally, in the Joplin app, create note titled DJI Review Analysis. For each of the five reviews, list its original star rating, its original full text, and its Russian translation. First, in Amazon, find the price and star rating for the GoPro HERO12 Black, then find the founding year and founder of GoPro in Wikipedia. Second, repeat this entire process for the Insta360 Ace Pro camera and Insta360 company. Finally, send single message to +8613412345678 containing all eight pieces of collected data for both products. First, in Amazon, find the price and star rating for the DJI Mini 4 Pro, then find the founding year and founder of DJI in Wikipedia. Second, repeat this entire process for the Autel EVO Lite+ drone and Autel Robotics company. Finally, send single message to +8613487654321 containing all eight pieces of collected data for both products. Search for Bose QuietComfort Ultra Headphones on both the Amazon and Wish apps, remembering the price and currency from each. If the currencies are different, use the bing app to find the exchange rate to compare them. Directly answer with the name of the app, Amazon or Wish, where the price is lower. Search for iPhone 16 Pro Max on both the Amazon and Wish apps, remembering the price and currency from each. If the currencies are different, use the bing app to find the exchange rate to compare them. Directly answer with the name of the app, Amazon or Wish, where the price is lower. Open the Amazon app and search for the Intel Core i5-13600K CPU, remembering its price. Open the bing app and search for what socket does Intel Core i5-13600K use. Remember the socket type. Return to the Amazon app, search for motherboard with that socket type, and remember the price of the first result. Finally, open the joplin app, create note titled CPU/Mobo Combo, and list the names and prices of the CPU and the compatible motherboard. Open the Amazon app and search for the AMD Ryzen 5 7600X CPU, remembering its price. Open the bing app and search for what socket does AMD Ryzen 5 7600X use. Remember the socket type. Return to the Amazon app, search for motherboard with that socket type, and remember the price of the first result. Finally, open the joplin app, create note titled AMD Build Parts, and list the names and prices of the CPU and the compatible motherboard. In the AP News app, find the top three stories from the World section and the top three from the U.S. News section. For each of the six stories, summarize its first paragraph (30 words) and translate its headline into German in DeepL Translate app. Then, in the Joplin app, create note titled Global News Report listing the original headline, its summary, and its German translation for all six stories, grouped by section. In the AP News app, find the top three stories from the Technology section and the top three from the Sports section. For each of the six stories, summarize its first paragraph (30 words) and translate its headline into Spanish in DeepL Translate app. Then, in the Joplin app, create note titled Tech & Sports Report listing the original headline, its summary, and its Spanish translation for all six stories, grouped by section. In the Apartments.com app, search for Chicago, IL apartments with 2 beds and in-unit laundry. For the first three results, remember the monthly rent and square footage of each. In the Calculator app, calculate the rent per square foot for all three. Finally, open the messages app and send message to +8613355556666 identifying the apartment with the best value (lowest rent per sq ft) and stating its calculated value. In the Apartments.com app, search for Miami, FL apartments with 2 beds and in-unit laundry. For the first three results, remember the monthly rent and square footage of each. In the Calculator app, calculate the rent per square foot for all three. Finally, open the messages app and send message to +8613377778888 identifying the apartment with the best value (lowest rent per sq ft) and stating its calculated value. Open Apartments.com app, search Austin, TX, and apply three filters: 2 Beds, Dog Friendly, and max price of $3000. For the top two results, go to the details page, find the list of amenities, and remember three specific ones. Also, find the address. Then, use Citymapper app to find the commute time from each address to University of Texas at Austin. In Joplin app, note which apartment has more of the desired amenities and shorter commute. In the Apartments.com app, search for Seattle, WA, and apply three filters: 1 Bed, Cat Friendly, and max price of $2500. For the top two results, go to the details page, find the list of amenities, and remember if they have these three specific ones: In-unit Washer, Balcony, and Fitness Center. Also, find the address. Then, in the Citymapper app, find the commute time from each address to University of Washington. In the Joplin app, note which apartment has more of the desired amenities and shorter commute. 3 3 3 3 3 3 3 3 3 3 3 [Amazon, DeepL Translate, joplin] [Amazon, DeepL Translate, joplin] [Amazon, Wikipedia, messages] [Amazon, Wikipedia, messages] [Amazon, Wish, bing] [Amazon, Wish, bing] [Amazon, bing, joplin] [Amazon, bing, joplin] [AP News, DeepL Translate, joplin] [AP News, DeepL Translate, joplin] [Apartments.com Rental Search, Calculator, messages] [Apartments.com Rental Search, Calculator, messages] [Apartments.com Rental Search, Citymapper, joplin] [Apartments.com Rental Search, Citymapper, joplin] 22 Y Y Y Y [E-commerce: Product Search, E-commerce: Review Analysis, Content Creation: Translation, Productivity: Note Taking] 70 3 70 32 32 15 25 2 1 1 2 25 45 3 45 39 2 39 70 3 70 [E-commerce: Product Search, E-commerce: Review Analysis, Content Creation: Translation, Productivity: Note Taking] [E-commerce: Product Search, E-commerce: Multi-App Comparison, Information Retrieval: Data Extraction, Communication: Messaging] [E-commerce: Product Search, E-commerce: Price Comparison, Information Retrieval: Data Extraction, Information Retrieval: Fact Checking, Communication: Data Sharing] [E-commerce: Product Search, E-commerce: Price Comparison, E-commerce: Multi-App Comparison, Information Retrieval: Web Search] [E-commerce: Product Search, E-commerce: Price Comparison, E-commerce: Multi-App Comparison, Information Retrieval: Web Search] [E-commerce: Product Search, Compatibility E-commerce: Check, E-commerce: MultiApp Comparison, Information Retrieval: Web Search, Productivity: Note Taking] [E-commerce: Product Search, Compatibility E-commerce: Check, E-commerce: MultiApp Comparison, Information Retrieval: Web Search, Productivity: Note Taking] [Information Retrieval: Data Extraction, Content Creation: Text Creation, Content Creation: Translation, Productivity: Note Taking] [Sports: Content Navigation, Information Retrieval: Data Extraction, Content Creation: Text Creation, Content Creation: Translation, Productivity: Note Taking] [E-commerce: Product Search, E-commerce: Filter & Sort, Information Retrieval: Data Extraction, Financial Management: Financial Calculation, Communication: Messaging, Communication: Data Sharing] [E-commerce: Product Search, E-commerce: Filter & Sort, Information Retrieval: Data Extraction, Financial Management: Financial Calculation, Communication: Messaging, Communication: Data Sharing, Travel & Navigation: Local Search] [E-commerce: Product Search, E-commerce: Filter & Sort, Information Retrieval: Data Extraction, Travel & Navigation: Route Planning, E-commerce: Multi-App Comparison, Productivity: Note Taking] [E-commerce: Product Search, E-commerce: Filter & Sort, Ecommerce: Specification Comparison, Information Retrieval: Data Extraction, Travel & Navigation: Route Planning, Productivity: Note Taking] MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments In the BBC Sports app, go to the Formula 1 - Drivers standings. Remember the names and points of the top three drivers. Then, for each driver, search in the Bing app for their age. In the Joplin app, create note titled F1 Top 3 Ages listing each driver with their points and age. In BBC Sports app, go to the Formula 1 - Constructors standings. Remember the names and points of the top three teams. Then, for each team, search in Bing app for their team principal name. In Joplin app, create note titled F1 Top 3 Principals listing each team with their points and team principal. In Bing app, find the current USD prices for both Bitcoin (BTC) and Ethereum (ETH). In the Calculator app, calculate the total value of portfolio with 1.5 BTC and 25 ETH. Then, in Bluecoins app, create new asset account under the Investments group named Crypto Portfolio, with the calculated total as its initial value. In Bing app, find the current USD prices for one ounce of gold and silver. In the Calculator app, calculate the total value of portfolio with 10 ounces of gold and 500 ounces of silver. Then, in Bluecoins app, create new asset account under the Investments group named Precious Metals, with the calculated total as its initial value. In the Bing app, find the cheapest flight from London to Rome within the next seven days. Remember its airline, departure date and time, and price. In the Citymapper app, find the journey duration from Trafalgar Square to Heathrow Airport. Finally, in the calendar app, create an event on the cheapest flights date and time, title it [Airline] Flight - [Price], and set reminder for [journey duration + 2 hours] before departure. In the Bing app, find the cheapest flight from Tokyo to Seoul within the next seven days. Remember its airline, arrival date and time, and price. In the Citymapper app, find the journey duration from Incheon International Airport to Myeong-dong. Finally, in the calendar app, create an arrival event on the cheapest flights date and time, titled [Airline] Arrival - [Price], then create second event immediately following it for the transit to Myeong-dong, using the remembered duration. Open the bing app, search for NVIDIA stock price, and remember the current price. Then open the AP News app, find the latest news about NVIDIA earnings report, and remember the reported revenue growth rate. Finally, open the Calculator app and calculate: stock price * (1 + revenue growth rate). Open the bing app, search for Apple stock price, and remember the current price. Then open the AP News app, find the latest news about Apple earnings report, and remember the reported revenue growth rate. Finally, open the Calculator app and calculate: stock price * (1 + revenue growth rate). Open the bing app and search for minimalist black and white wallpaper images. From the results, find the first image that has portrait (vertical) orientation suitable for phone wallpaper. Long-press to download this image. Then, open the Setting app, navigate to wallpaper settings, and change your home screen wallpaper to the image you just downloaded. Open the bing app and search for abstract blue ocean wallpaper images. From the results, find the first image that is in portrait (vertical) format. Long-press and download this image. Afterwards, open the Setting app, navigate to the wallpaper settings, and change your lock screen wallpaper to the downloaded image. Open Cars.co.za app, search for Toyota cars. Apply filters: price between R150,000-R200,000, year after 2020. From the results, find the top three cars. For each, remember its price and mileage. Open the Calculator app and calculate the price-to-mileage ratio (price/mileage) for each car. In Joplin app, list the three cars and their ratios, and state which has the best (lowest) ratio. [BBC Sports, bing, joplin] [BBC Sports, bing, joplin] Cal- [bing, culator, bluecoins] Cal- [bing, culator, bluecoins] [bing, calendar, Citymapper] [bing, calendar, Citymapper] [bing, AP News, Calculator] [bing, AP News, Calculator] [bing, Files, Setting] [bing, Files, Setting] [Cars.co.za, Calculator, joplin] Open Cars.co.za app, search for BMW cars. Apply filters: price between R250,000-R300,000, year after 2019. From the results, find the top three cars. For each, remember its price and engine size. Open the Calculator app and calculate the average engine size. In Joplin app, list the three cars with their prices and state the calculated average engine size. [Cars.co.za, Calculator, joplin] Open the wikiHow app and search for how to bake chocolate chip cookies. Create checklist in the joplin app named Cookie Ingredients with the first four ingredients listed. Then, open the Calculator app and calculate the total cost, assuming each of the four ingredients costs $3.50. [wikiHow, joplin, Calculator] Open the wikiHow app and search for how to make pizza. Create checklist in the joplin app named Pizza Ingredients with the first four ingredients listed. Then, open the Calculator app and calculate the total cost, assuming each of the four ingredients costs $4.25. [wikiHow, joplin, Calculator] In the Wish app, find and remember the prices for three items: an RGB Mechanical Keyboard, Wireless Gaming Mouse, and Large Gaming Mousepad. In the Calculator app, first sum the prices of all three items to get subtotal. Then, calculate 15% discount on this subtotal. Finally, add 7% tax to the discounted price to get the final total. Open the messages app and send message to +8613211112222 with the content: Subtotal: [sum], After 15% discount: [discounted price], Final Total (incl. 7% tax): [final price]. In the Wish app, find and remember the prices for three items: Bluetooth Earbuds, Portable Power Bank, and Phone Stand for Desk. In the Calculator app, first sum the prices of all three items. Then, calculate the final grand total by adding fixed shipping fee of 3 units per item and 4% import duty on the item subtotal. Open the messages app and send message to +8613233334444 with the content: Subtotal: [sum], Shipping: 9.00, Duty (4%): [duty amount], Grand Total: [final price]. [Wish, Calculator, messages] [Wish, Calculator, messages] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Y Y Y Y Y [Sports: Content Navigation, Sports: Data Extraction, Information Retrieval: Web Search, Productivity: Note Taking] [Sports: Data Extraction, Information Retrieval: Web Search, Productivity: Note Taking] [Information Retrieval: Web Search, Financial Management: Financial Calculation, Financial Management: Add Transaction] [Information Retrieval: Web Search, Financial Management: Financial Calculation, Financial Management: Add Transaction] [Travel & Navigation: Flight Booking, Travel & Navigation: Route Planning, Productivity: Time Management, Information Retrieval: Data Extraction] [Travel & Navigation: Flight Booking, Travel & Navigation: Route Planning, Productivity: Time Management, Information Retrieval: Data Extraction] [Information Retrieval: Web Search, Information Retrieval: Data Extraction, Financial Management: Financial Calculation] [Information Retrieval: Data Extraction, Financial Management: Financial Calculation] [Information Retrieval: Image Search & Understanding, Device Configuration: Setting Adjustment] Image [Information Retrieval: Search & Understanding, Device Configuration: Setting Adjustment] [E-commerce: Product Search, E-commerce: Filter & Sort, Information Retrieval: Data Extraction, Financial Management: Financial Calculation, Productivity: Note Taking, E-commerce: Price Comparison] [E-commerce: Product Search, E-commerce: Filter & Sort, Information Retrieval: Data Extraction, Financial Management: Financial Calculation, Productivity: Note Taking] [Information Retrieval: Web Search, Education & Learning: Knowledge Acquisition, Productivity: Checklist Management, Financial Management: Financial Calculation] [Education & Learning: Knowledge Acquisition, Productivity: Checklist Management, Information Retrieval: Data Extraction, Financial Management: Financial Calculation] [E-commerce: Product Search, Financial Management: Financial Calculation, Communication: Messaging, Communication: Data Sharing] [E-commerce: Product Search, Financial Management: Financial Calculation, Communication: Messaging, Communication: Data Sharing] 24 32 31 1 1 2 2 3 75 3 18 15 16 1 1 1 1 3 60 3 15 1 12 1 60 3 60 3 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments In the Amazon app, find the USD prices for two items: Sony WH-1000XM5 headphones and Logitech MX Master 3S mouse. In the Bing app, find the USD to EUR exchange rate. In the Calculator app, sum the two item prices, convert the subtotal to EUR, then add fixed EUR25 shipping fee to get the grand total. Finally, in the bluecoins app, add single expense transaction for the grand total, named EU Tech Import, and add note: Items: [USD subtotal], Shipping: EUR25. In the Amazon app, find the USD prices for two items: Apple Watch Series 9 and Apple AirPods Pro 2. In the Bing app, find the USD to GBP exchange rate. In the Calculator app, sum the two item prices, convert the subtotal to GBP, then add fixed GBP20 shipping fee to get the grand total. Finally, in the bluecoins app, add single expense transaction for the grand total, named UK Apple Import, and add note: Items: [USD subtotal], Shipping: GBP20. Open the Apartments.com app, search in Mountain View, CA, and remember the address and rent of the first result. Open the bing app and search for the address of the Googleplex. Open the Citymapper app and find the public transit commute time between the apartment address and the Googleplex. Finally, open the joplin app and create note titled Google Commute with the apartments rent and the calculated commute time. Open the Apartments.com app, search in Cupertino, CA, and remember the address and rent of the first result. Open the bing app and search for the address of Apple Park. Open the Citymapper app and find the driving time between the apartment address and Apple Park. Finally, open the joplin app and create note titled Apple Commute with the apartments rent and the calculated driving time. Open Apartments.com, search for Los Angeles, CA, filter for 2 beds, and remember the monthly rent of the first result. Open the Calculator app and calculate the annual rent. Open the bluecoins app and create new monthly budget for Rent with the remembered monthly amount. Finally, open the calendar app and set reminder for the 1st of next month titled Pay Rent. Open Apartments.com, search for Chicago, IL, filter for in-unit laundry, and remember the monthly rent of the first result. Open the Calculator app and calculate 1.5x security deposit based on the rent. Open the bluecoins app and create savings goal named Apartment Deposit for this calculated amount. Finally, open the calendar app and set reminder for tomorrow titled Follow up on Chicago apartment. In the Bing app, search for and observe four paintings: The Scream by Munch, Guernica by Picasso, The Third of May 1808 by Goya, and Saturn Devouring His Son by Goya. In the Joplin app, write note comparing how these four artworks depict human suffering and fear (60 words total). In the DeepL Translate app, translate your entire comparison into Spanish. Finally, open the messages app and send the Spanish translation to +8613511112222. In the Bing app, search for and observe four paintings: Mona Lisa by da Vinci, The Starry Night by van Gogh, Les Demoiselles dAvignon by Picasso, and Composition VII by Kandinsky. In the Joplin app, write note comparing the use of realism, color, and form across these four distinct art movements (60 words total). In the DeepL Translate app, translate your entire comparison into Italian. Finally, open the messages app and send the Italian translation to +8613633334444. Open Cars.co.za app, search for Volkswagen Polo, and remember the price of the first result. Then search for Hyundai i20 and remember the price of the first result. Open the Calculator app and find the price difference. Open the joplin app to note which car is more expensive and by how much. Finally, open the bing app and search for Volkswagen Polo vs Hyundai i20 safety rating. Stay on the search results page. Open Cars.co.za app, search for Ford Ranger, and remember the price of the first result. Then search for Toyota Hilux and remember the price of the first result. Open the Calculator app and find the price difference. Open the joplin app to note which car is more expensive and by how much. Finally, open the bing app and search for Ford Ranger vs Toyota Hilux reliability. Stay on the search results page. [Amazon, bing, culator, bluecoins] Cal- [Amazon, bing, culator, bluecoins] Cal- [Apartments.com Rental Search, bing, Citymapper, joplin] [Apartments.com Rental Search, bing, Citymapper, joplin] [Apartments.com Rental Search, Calculator, bluecoins, calendar] [Apartments.com Rental Search, Calculator, bluecoins, calendar] [bing, joplin, DeepL Translate, messages] [bing, joplin, DeepL Translate, messages] [Cars.co.za, bing, joplin, Calculator] [Cars.co.za, Calculator, joplin, bing] 4 4 4 4 4 4 4 4 4 Y Y Y 51 51 3 19 25 2 25 17 1 40 40 2 12 19 1 [E-commerce: Product Search, Fact Information Retrieval: Checking, Financial Management: Financial Calculation, Financial Management: Add Transaction] [E-commerce: Product Search, Information Retrieval: Web Search, Financial Management: Financial Calculation, Financial Management: Add Transaction] [E-commerce: Product Search, Information Retrieval: Web Search, Travel & Navigation: Route Planning, Productivity: Note Taking, Information Retrieval: Data Extraction] [E-commerce: Product Search, Information Retrieval: Web Search, Information Retrieval: Data Extraction, Travel & Navigation: Route Planning, Productivity: Note Taking] [E-commerce: Product Search, E-commerce: Filter & Sort, Financial Management: Financial Calculation, Financial Management: Create Budget, Productivity: Time Management] [E-commerce: Product Search, E-commerce: Filter & Sort, Information Retrieval: Data Extraction, Financial Management: Financial Calculation, Financial Management: Set Saving Goal, Productivity: Time Management] [Information Retrieval: Image Search & Understanding, Content Creation: Text Creation, Content Creation: Translation, Communication: Messaging] [Information Retrieval: Image Search & Understanding, Content Creation: Text Creation, Productivity: Note Taking, Content Creation: Translation, Communication: Messaging] [E-commerce: Product Search, E-commerce: Price Comparison, E-commerce: Multi-App Comparison, Financial Management: Financial Calculation, Productivity: Note Taking, Information Retrieval: Web Search] [E-commerce: Product Search, E-commerce: Price Comparison, E-commerce: Multi-App Comparison, Financial Management: Financial Calculation, Productivity: Note Taking, Information Retrieval: Web Search] A.5. Details of Snapshot-based Plug-and-Play Architecture This section provides comprehensive technical specifications for the snapshot-based plug-and-play framework presented in Section 3.2. A.5.1. PARALLEL EXPERIMENT IMPLEMENTATION Our framework achieves scalable parallel execution through sophisticated emulator management system. We pre-configured MemGUI-AVD (Android Virtual Device), customized emulator image that includes all required applications with preestablished permissions (file access, location services, etc.) and optimized settings for GUI automation. Each experimental instance creates an independent emulator from this base image, ensuring identical starting conditions across all parallel executions. 24 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Table 9. Detailed distribution of task categories in MemGUI-Bench. The suite covers diverse domains including Commerce, Information Retrieval, Productivity, Finance, and Social, reflecting real-world mobile usage patterns. Counts represent category instances, as tasks may involve multiple categories."
        },
        {
            "title": "Sub Category",
            "content": "Count % within Main Global %"
        },
        {
            "title": "Setting Adjustment",
            "content": "Education & Learning"
        },
        {
            "title": "Information Retrieval",
            "content": "Data Extraction Web Search Image Search & Understanding Fact Checking Image Analysis"
        },
        {
            "title": "Sports",
            "content": "Travel & Navigation"
        },
        {
            "title": "Note Taking\nTime Management\nChecklist Management",
            "content": "Product Search Filter & Sort Price Comparison Review Analysis Multi-App Comparison Category Navigation Specification Comparison Compatibility Check"
        },
        {
            "title": "Route Planning\nLocal Search\nFlight Booking",
            "content": "13 9 14 8 2 9 6 4 28 6 1 1 62 23 7 6 58 18 4 57 18 18 14 12 8 8 6 8 8 12 2 2 59.1% 40.9% 58.3% 33.3% 8.3% 100.0% 60.0% 40.0% 77.8% 16.7% 2.8% 2.8% 62.6% 23.2% 7.1% 6.1% 1.0% 72.5% 22.5% 5.0% 40.4% 12.8% 12.8% 9.9% 8.5% 5.7% 5.7% 4.3% 50.0% 50.0% 75.0% 12.5% 12.5% 2.9% 2.0% 3.1% 1.8% 0.4% 2.0% 1.3% 0.9% 6.2% 1.3% 0.2% 0.2% 13.7% 5.1% 1.5% 1.3% 0.2% 12.8% 4.0% 0.9% 12.6% 4.0% 4.0% 3.1% 2.6% 1.8% 1.8% 1.3% 1.8% 1.8% 2.6% 0.4% 0.4% 25 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments The system implements port-based isolation using Android Debug Bridge (ADB) connections, where each emulator instance is assigned unique port number (e.g., 5554, 5556, 5558) to enable simultaneous agent-environment communication without interference. This architecture supports concurrent execution of multiple agents on the same hardware while maintaining strict experimental isolation. The mirror task design ensures sequential execution within each parallel stream, preserving the integrity of long-term learning assessment where task order may influence learning outcomes. A.5.2. LONG-TERM MEMORY SUPPORT THROUGH MULTI-ATTEMPT MECHANISM Our framework implements long-term memory evaluation through the pass@k protocol, where agents are allowed up to attempts per task (default = 3). Between attempts, agents with long-term memory capabilities can analyze failure patterns, update their knowledge bases, and adjust strategies for subsequent tries. The framework maintains persistent agent state across attempts while ensuring environment consistency through snapshot-based resets, enabling fair assessment of cross-session learning capabilities. A.5.3. COMPREHENSIVE AGENT INTEGRATION The framework supports twelve prominent GUI agents across diverse architectural paradigms through unified interface that accommodates both agentic workflows and end-to-end models. Table 7 provides detailed specifications for each integrated agent, including their memory mechanisms, backbone models, and deployment configurations. All agents utilize standardized action spaces and observation formats while preserving their unique architectural characteristics. A.5.4. ADVANTAGES OVER EXISTING APPROACHES Our framework provides significant improvements over existing benchmarking environments in three key areas: Environment Scalability and Convenience. Unlike AndroidWorld (Rawles et al., 2024) and AndroidLab (Xu et al., 2024), which rely on pre-written expert scripts for environment recovery and setup, our approach offers superior extensibility without requiring specialized knowledge for script development. While expert scripts facilitate environment reset for pre-configured applications, they are fundamentally limited by application constraints, as mainstream software like Amazon cannot be easily manipulated through script injection or state reading mechanisms. Additionally, the scalability is severely constrained by the expert knowledge required for script development. Rapid Environment Recovery. In contrast to SPA-Bench (Chen et al., 2024) and A3 (Chai et al., 2025), which include mainstream applications but require manual environment reset and partially depend on physical devices, our snapshot-based approach enables instant environment recovery. This advantage stems from our strategic application selection constraints: emulator compatibility ensures reliable operation in virtualized environments, while login-free operation eliminates the need for manual cleanup of user-generated data (favorites, search history, etc.). As demonstrated in our application selection strategy, Amazon, Apartments.com, and Citymapper provide comprehensive functionality in guest mode, enabling automated state recovery while maintaining task authenticity. Native Long-Term Memory Support. Our framework uniquely provides built-in support for long-term memory evaluation through the pass@k protocol and persistent agent state management across multiple attempts. This capability is absent in existing benchmarks, which focus exclusively on single-attempt evaluation and cannot assess agents ability to learn from experience and improve performance over time. A.6. Details of Memory-Specialized Metrics This section provides comprehensive mathematical definitions and computational procedures for the 7 specialized metrics introduced in Section 4.1. A.6.1. SHORT-TERM MEMORY ASSESSMENT METRICS Success Rate (SR) serves as our baseline metric, measuring the fundamental ability to complete tasks and providing essential context for interpreting memory-specific performance. This metric provides foundation for understanding overall agent capabilities before analyzing memory-specific performance patterns. Information Retention Rate (IRR) constitutes our core memory fidelity metric, quantifying the proportion of required information units that agents correctly recall and utilize during task execution. Unlike binary success indicators, IRR MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments provides fine-grained insights into partial memory failures, such as distinguishing an agent that correctly processes 7 out of 9 required information pieces from one that fails entirely. This metric specifically targets the temporary information buffering capability that characterizes human-like short-term memory in GUI interactions. Mathematical Definition: Ci Ti where Ci denotes the number of correctly recalled and utilized information units in task i, and Ti denotes the total required information units in task i. IRRi = 100% The average IRR across all memory-intensive tasks is computed as: IRR ="
        },
        {
            "title": "1\nNm",
            "content": "(cid:88) iM"
        },
        {
            "title": "IRRi",
            "content": "where represents the set of memory-intensive tasks and Nm = = 115 in MEMGUI-BENCH. Memory-Task Proficiency Ratio (MTPR) isolates memory-specific capabilities by comparing performance on our 115 memory-intensive tasks against 13 standard tasks, enabling researchers to distinguish memory limitations from general task execution deficits. Mathematical Definition: MTPR ="
        },
        {
            "title": "SRm\nSRs",
            "content": "where SRm and SRs denote the success rates on memory-intensive tasks and standard tasks, respectively. A.6.2. LONG-TERM MEMORY ASSESSMENT METRICS Multi-Attempt Success Rate (pass@k SR) serves as our primary long-term learning indicator, measuring agents ability to leverage knowledge from previous attempts to eventually succeed within trials. This metric directly reflects the cumulative benefit of long-term memory mechanisms in helping agents overcome initial failures through experience accumulation. Mathematical Definition: pass@k SR ="
        },
        {
            "title": "Sk\nN",
            "content": "100% where Sk denotes the number of tasks that succeeded within attempts, and is the total number of tasks. Failure Recovery Rate (FRR) specifically targets the speed and effectiveness of learning from failure, employing harmonic decay weighting model that rewards agents capable of rapid recovery from initial failures. This metric recognizes that superior long-term memory should enable faster learning rather than merely eventual success. Mathematical Definition: where: FRR ="
        },
        {
            "title": "1\nNf",
            "content": "k (cid:88) i=2 wi Ri 100% Nf : number of tasks that failed on the first attempt Ri: number of tasks that succeeded for the first time on attempt wi = 1 i1 : harmonic decay weight for attempt This weighting scheme ensures that earlier recoveries contribute more significantly to the overall score, reflecting the principle that effective long-term memory should enable rapid learning from experience. A.6.3. EXECUTION EFFICIENCY ASSESSMENT METRICS Average Step Ratio measures path efficiency by comparing agent execution paths against golden standards exclusively for successfully completed tasks, revealing whether sophisticated memory systems enable more direct task completion when they do succeed. 27 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Mathematical Definition: ri ="
        },
        {
            "title": "Ai\nGi",
            "content": ", ="
        },
        {
            "title": "1\nNs",
            "content": "(cid:88) iS ri where Ai and Gi denote the agent steps and golden (optimal) steps for task i, respectively. is the set of successfully completed tasks and Ns = S. Average Time Per Step quantifies the computational overhead of memory-enhanced decision-making across all task attempts, providing insights into the speed-accuracy trade-offs inherent in different memory architectures. Mathematical Definition: τi = ti Ai , τ ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 τi where ti is the total execution time for task i, Ai is the number of agent steps, and is the total number of task attempts. Average Cost Per Step evaluates the economic efficiency of memory mechanisms across all executions, particularly relevant for comparing framework-based agents with dedicated memory modules against end-to-end model approaches. Mathematical Definition: ci ="
        },
        {
            "title": "C API\ni\nAi",
            "content": ", ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 ci where API is the total API cost for task i, Ai is the number of agent steps, and is the total number of task attempts. A.6.4. COMPUTATIONAL CONSIDERATIONS For tasks where agents achieve perfect success (SR = 100%), the IRR is automatically set to 100%. For failed tasks, IRR is computed based on the actual proportion of correctly recalled and utilized information units. In cases of early failure where no information units are processed, IRR = 0%. The MTPR provides insights into memory-specific capabilities: MTPR > 1 indicates superior performance on memory tasks, MTPR = 1 suggests equivalent performance across task types, and MTPR < 1 reveals memory-specific deficits. For pass@k evaluation, tasks are considered successful if they achieve success in any of the attempts. The FRR metric specifically focuses on the subset of initially failed tasks to quantify learning effectiveness from failure experiences. A.7. Details of Evaluation Pipeline Validation This section provides comprehensive technical details for the evaluation pipeline validation experiments presented in Section 4.3. A.7.1. EXPERIMENTAL SETUP DETAILS Task Selection Strategy. Our validation employs two complementary evaluation datasets to comprehensively assess pipeline reliability. First, we selected 26 tasks from SPA-Bench (Chen et al., 2024), comprising 18 single-app and 8 cross-app tasks, executing each three times with M3A (Rawles et al., 2024) to generate 78 trajectories (54 single-app, 24 cross-app) for direct comparison with SPA-Benchs evaluator. This selection ensures cross-benchmark transferability assessment while maintaining fair comparison conditions. Second, we utilized all 128 MEMGUI-BENCH tasks executed by both M3A and T3A under pass@1 settings, yielding 256 trajectories that represent the full spectrum of our memory-intensive evaluation scenarios. Model Configuration Design. To systematically assess evaluator robustness and cost-performance trade-offs, we designed comprehensive model configurations for both MEMGUI-EVAL and baseline methods. For MEMGUI-EVAL, we tested three strategic configurations: M1 (Gemini 2.5 Pro + Pro) where all specialized agents (Triage Judge, Step Descriptor, Semantic Judge, Visual Judge, and IRR Analyzer) use Gemini 2.5 Pro for maximum accuracy; M2 (Gemini 2.5 Flash + Pro) where the Step Descriptor uses Gemini 2.5 Flash for cost efficiency while judgment agents use Pro for accuracy; and M3 (Gemini 2.5 Flash + Flash) where all agents use Flash for maximum cost reduction. For SPA-Bench baseline comparisons, we evaluated G1 (Gemini 2.5 Pro), G2 (Gemini 2.5 Flash), and G3 (GPT-4o) configurations. This design enables systematic analysis of 28 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments evaluator robustness across different cost-accuracy configurations while ensuring fair comparison with existing evaluation methodologies. Human Annotation Process. To establish ground truth labels, each trajectory was independently annotated by three human experts for success/failure determination. Annotators achieved consensus through structured discussion, resolving any disagreements to produce final labels that serve as the gold standard for evaluator performance assessment. The annotation process followed strict guidelines to ensure consistency and reliability across all evaluation scenarios. A.7.2. DETAILED RESULTS ANALYSIS Cross-Benchmark Performance. The cost metric represents the average API expense per trajectory evaluation, encompassing all model calls made by the evaluator during the progressive scrutiny process. On SPA-Bench trajectories, our M1 configuration achieves near-perfect performance (99.0% F1-score), significantly outperforming the best baseline (G1: 92.5% F1-score). The M2 configuration provides an optimal balance with 95.9% F1-score at substantially reduced cost ($0.031 vs $0.055), while even our most economical M3 configuration (93.7% F1-score) maintains competitive accuracy with dramatic cost reduction. Memory-Intensive Task Performance. For MEMGUI-BENCH trajectories, our evaluation maintains consistent high performance across diverse memory-intensive scenarios. The M1 configuration achieves 93.1% F1-score, demonstrating robustness across different task complexities and memory requirements. Notably, the performance gap between single-app and cross-app tasks reveals the sophistication of our progressive scrutiny approach: while baseline methods struggle with cross-app complexity (achieving only 40-61.5% F1-score), MEMGUI-EVAL maintains exceptional performance (94.1-100% F1-score) across all task types. Cost-Effectiveness Analysis. The progressive scrutiny approach demonstrates superior cost-effectiveness compared to traditional evaluation methods. The M2 configuration achieves the optimal balance between evaluation quality and computational efficiency, providing robust assessment capabilities while maintaining economic feasibility for large-scale evaluation scenarios. A.7.3. DETAILED PERFORMANCE BREAKDOWN This section provides more granular breakdown of the evaluator validation experiments with comprehensive performance analysis across different task complexities and agent types. Table 11. Detailed evaluation performance breakdown on SPA-Bench task subsets."
        },
        {
            "title": "Evaluator",
            "content": "Model Config. F1 Prec. Recall Cost ($) Accuracy Metrics (%) Efficiency Single-App (N=54) SINGLE-APP TASKS (N=54) MemGUI-Eval (Ours) SPA-Bench (Baseline) Gemini 2.5 Pro+Pro 98.8 100.0 Gemini 2.5 Flash+Pro 96.3 Gemini 2.5 Flash+Flash 93.7 Gemini 2.5 Pro Gemini 2.5 Flash GPT-4o 92.5 86.8 84.2 97.5 97.4 94.9 94.3 91. 97.6 95.1 90.2 90.2 80.5 78.0 CROSS-APP TASKS (N=24) MemGUI-Eval (Ours) Gemini 2.5 Pro+Pro 100.0 100.0 100.0 Gemini 2.5 Flash+Pro 94.1 Gemini 2.5 Flash+Flash 93.3 88.9 100.0 100.0 87. Cross-App (N=24) SPA-Bench (Baseline) GPT-4o Gemini 2.5 Pro Gemini 2.5 Flash 29 61. 61.5 40.0 80.0 80.0 100.0 50.0 50.0 25.0 0. 0.027 0.018 0.040 0.037 0.099 0.075 0.030 0.024 0. 0.031 0.004 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Table 11 presents the comprehensive evaluation performance breakdown on SPA-Bench task subsets. The results clearly demonstrate MEMGUI-EVALs superiority over baseline methods across different model configurations. For single-app tasks, our method consistently outperforms SPA-Benchs evaluator across all accuracy metrics, with the M1 configuration achieving near-perfect performance (98.8% F1-score vs. 92.5% for the best baseline). The advantage becomes even more pronounced for cross-app tasks, where MEMGUI-EVAL achieves perfect performance with the M1 configuration, while baseline methods struggle significantly (achieving only 40-61.5% F1-score). This performance gap highlights the critical importance of our progressive scrutiny approach in handling complex, memory-intensive cross-application scenarios where traditional evaluation methods fail to maintain accuracy. Table 12. Agent-specific evaluation performance of MemGUI-Eval on MemGUI-Bench trajectories."
        },
        {
            "title": "Trajectory Source Model Configuration",
            "content": "F1 Prec. Recall Cost ($) Accuracy Metrics (%) Efficiency M3A Agent (N=128) T3A Agent (N=128) M3A AGENT TRAJECTORIES (N=128)"
        },
        {
            "title": "92.7\nGemini 2.5 Pro+Pro\nGemini 2.5 Flash+Pro\n85.0\nGemini 2.5 Flash+Flash 77.9",
            "content": "92.7 87.2 83.3 T3A AGENT TRAJECTORIES (N=128)"
        },
        {
            "title": "93.9\nGemini 2.5 Pro+Pro\nGemini 2.5 Flash+Pro\n75.0\nGemini 2.5 Flash+Flash 79.2",
            "content": "92.0 75.0 79.2 92.7 82.9 73.2 95.8 75.0 79.2 0.190 0.062 0.059 0.235 0.077 0.062 Table 12 shows agent-specific evaluation performance across different model configurations on MEMGUI-BENCH trajectories. The results demonstrate consistent evaluation quality across diverse agent types, validating the generalizability of our approach. Both M3A and T3A trajectories show similar performance patterns, with the M1 configuration achieving the highest accuracy (92.7-93.9% F1-score) at higher cost, while the M2 configuration provides the optimal balance of accuracy and efficiency. Notably, even our most economical M3 configuration maintains reasonable accuracy (77.9-79.2% F1-score) while achieving the lowest evaluation costs. These results confirm our selection of the M2 configuration for the main experiments, as it provides robust evaluation quality while maintaining cost-effectiveness for large-scale memory assessment. A.7.4. KEY VALIDATION INSIGHTS The validation results establish several key insights about MEMGUI-EVALs capabilities. First, our progressive scrutiny approach achieves superior accuracy across diverse task complexities, with flexible model configurations allowing researchers to balance evaluation quality and budget constraints based on specific requirements. Second, the substantial performance advantage on cross-app tasks validates our design motivation: traditional LLM-as-Judge approaches struggle with the long contexts and complex information dependencies inherent in memory-intensive scenarios, while our targeted visual verification maintains high fidelity. Third, the consistent performance across both SPA-Bench and MEMGUIBENCH datasets demonstrates the generalizability of our evaluation methodology beyond our specific benchmark domain, establishing confidence in our evaluation pipeline for systematic memory assessment of mobile GUI agents. A.8. Analysis of Failure Cases This section extends the failure pattern analysis presented in Section 6 with comprehensive failure mode definitions, representative failure trajectory examples, agent-specific failure distribution analysis, and detailed design implications for future memory-enhanced GUI agent architectures. As discussed in Section 6, execution timeout accounts for 72.3% of failures across 1,265 task executions, with individual agent timeout rates ranging from 22.6% (Agent-S2) to 93.9% (AppAgent). The systematic prevalence of execution timeouts indicates that agents struggle to maintain task coherence and efficient exploration strategies over extended interaction sequences. To provide deeper insights beyond the high-level patterns identified in Figure 7, we present detailed failure mode definitions with representative trajectory examples (Figures 915), agent-specific failure distribution analysis (Figure 16), and actionable design implications derived from systematic failure categorization. MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments A.8.1. FAILURE MODE DEFINITIONS Based on systematic trajectory analysis and MEMGUI-EVALs Information Retention Rate (IRR) calculations, we identify seven distinct failure modes. To provide deeper insights into each failure type and facilitate understanding of their practical implications, we present representative failure trajectories in Figures 9 through 15. Execution Timeout represents cases where agents fail to complete tasks within the allocated step limit, typically indicating inefficient exploration strategies or inability to converge on successful action sequences. Figure 9 shows UI-TARS-1.5-7B attempting to save an audio recording with the filename MyTestAudio. After successfully recording (steps 9-11), the agent needs to replace the default filename Record1 with MyTestAudio. However, instead of efficiently selecting and replacing the text, the agent attempts to delete the default name character by character through individual click actions (steps 12-17). This extremely inefficient approachrequiring one action per character deletionconsumes the entire step budget without completing the simple renaming operation, exemplifying how suboptimal action granularity can lead to timeout failures. Partial Memory Hallucination (PMH) occurs when agents successfully acquire some required information but fail to retain all necessary elements during task execution (0% < IRR < 100%). Figure 10 illustrates UI-TARS-1.5-7B searching for NVIDIA and Apple stock prices in Bing and Calculator apps. The agent correctly remembers NVIDIAs price (169.92 USD, step 6) for subsequent calculations (step 12), but incorrectly recalls Apples price as 143.92 USD (step 15) when the actual observed price was 226.91 USD (step 9). This selective memory loss results in an incorrect final calculation of 19,290 instead of the correct value. Process Memory Hallucination (ProcMH) manifests when agents completely lose track of task objectives mid-execution, leading to goal drift and irrelevant action sequences (IRR = 0%, process-oriented failure). Figure 11 shows UI-TARS-1.5-7B tasked with finding smartphone market share data from Bing image search and recording it in Joplin. After successfully locating the correct chart image containing Q3 2021 data (step 5), the agents internal thought process (shown in the dashed box at the bottom) indicates it believes the task is complete: found chart that perfectly meets my needs...This is exactly the information was looking for, so can move on to the next step. However, the agent prematurely marks the task as finished without realizing that critical subsequent steps remainextracting the specific market share percentages for the top three brands and creating the required Joplin note. This demonstrates failure to maintain the complete multi-step task workflow in working memory. Output Memory Hallucination (OMH) represents cases where agents correctly navigate task workflows but fail to accurately encode or retrieve essential information for final outputs (IRR = 0%, output-oriented failure). Figure 12 depicts M3A executing task to view and transcribe two app permission lists (Wi-Fi Control and Picture-in-picture) in Settings. The agent successfully navigates to both permission screens and observes the complete lists (steps 7 and 9). However, when creating the final Joplin note (step 15), it only transcribes 4 out of 9 apps from the Wi-Fi Control list and 7 out of 9 from the Picture-in-picture list, demonstrating incomplete information transcription despite correct procedural execution. Knowledge Deficiency (KD) indicates agents lack fundamental knowledge or skills required for task completion, independent of memory capabilities. Figure 13 shows UI-TARS-1.5-7B tasked with finding leap day and Halloween dates, then creating calendar events in the Calendar app. The agent successfully searches for and remembers both dates (October 31 for Halloween and February 29 for leap day, steps 1-7). However, when attempting to open the calendar app (step 8), it misidentifies the Google Calendar app as the calendar app and clicks on it, revealing fundamental misunderstanding of app identification rather than memory failure. Intent Misunderstanding (IM) occurs when agents misinterpret task descriptions or user intentions, leading to execution of inappropriate action sequences. Figure 14 illustrates UI-TARS-1.5-7B misinterpreting Wikipedia article comparison task. The instruction required comparing English and German Wikipedia article counts and staying on the edition with more articles. Despite correctly finding that English Wikipedia has more articles (step 12 shows the thought English Wikipedia has more articles), the agent completes the task while remaining on the German Wikipedia page, fundamentally misunderstanding the requirement to stay on the page of the edition that has more articles. Other encompasses remaining failure modes that do not fit the defined categories. Figure 15 captures SeeAct encountering an architectural limitation where its action space lacks wait operation. When opening the Meesho app, the agent recognizes that the app is loading and determines that waiting is the logical next step. However, since the framework only provides TERMINATE command for no-operation scenarios, the agent issues this command and prematurely ends the task, failing to complete any of the required product comparison steps. This represents system-level constraint rather than 31 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Figure 9. Execution Timeout Example (UI-TARS-1.5-7B). The task required recording audio and saving it as MyTestAudio. After successful recording, the agent attempted to delete the default filename Record1 character by character through inefficient individual click actions (steps 12-17), exhausting the 17-step limit before completing the renaming operation. This demonstrates how poor action efficiency can cause timeouts even on simple tasks. 32 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Figure 10. Partial Memory Hallucination Example (UI-TARS-1.5-7B). The task required finding stock prices for NVIDIA and Apple, calculating the value of 50 and 75 shares respectively. The agent correctly retained NVIDIAs price (169.92 USD) but hallucinated Apples price as 143.92 USD instead of the correct 226.91 USD observed in step 9, leading to an incorrect final calculation. 33 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Figure 11. Process Memory Hallucination Example (UI-TARS-1.5-7B). The task required finding Q3 2021 smartphone market share data, identifying the top three brands with percentages, and recording them in Joplin. After successfully finding the chart (step 5), the agent prematurely concluded the task was complete, forgetting the remaining critical steps of data extraction and note creation, revealing failure to retain the full procedural workflow. 34 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments"
        },
        {
            "title": "35\nFigure 12. Output Memory Hallucination Example (M3A). The task required transcribing two complete app permission lists. The\nagent correctly navigated to both ‘Wi-Fi Control’ (step 7, 9 apps) and ‘Picture-in-picture’ (step 9, 9 apps) permission screens but produced\nan incomplete transcription in the final note (step 15), missing several apps from both lists despite having observed them.",
            "content": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Figure 13. Knowledge Deficiency Example (UI-TARS-1.5-7B). The agent successfully found and retained the required dates (leap day: February 29, Halloween: October 31) but failed due to misidentifying the Google Calendar app as the target calendar app in step 8, demonstrating knowledge gap in app recognition unrelated to memory capabilities. 36 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Figure 14. Intent Misunderstanding Example (UI-TARS-1.5-7B). The task required comparing English and German Wikipedia article counts and staying on the page with more articles. The agent correctly identified that English Wikipedia has more articles but ended on the German Wikipedia page, misunderstanding the instruction to navigate to and remain on the edition with more articles. MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments cognitive or memory failure. Figure 15. Other Failure Example (SeeAct). The task required finding products with the best value in the Meesho app. After opening the app (step 2), the agent correctly identified that the app was loading and that waiting was necessary (step 3). However, due to action space limitations (no explicit wait action), it issued TERMINATE command, prematurely ending the task without performing any required operations. A.8.2. AGENT-SPECIFIC FAILURE DISTRIBUTION ANALYSIS Figure 16 reveals distinct failure signatures among the 343 non-timeout failures for each agent. Agent-S2 exhibits the highest rate of partial memory hallucinations (58.2%), while framework-based agents show elevated memory-related failures compared to model-based systems. A.8.3. CROSS-AGENT FAILURE PATTERN ANALYSIS Figure 7 (Section 6) reveals distinct failure signatures across agent architectures. Memory hallucination (PMH + ProcMH + OMH) accounts for 58.9% of non-timeout failures on average, confirming that memory limitations represent the primary bottleneck for current GUI agents. Framework vs. Model-Based Trade-offs. Framework-based agents achieve lower timeout rates (51.2%) compared to model-based systems (68.9%), but exhibit higher rates of memory-specific failures with combined memory hallucination rates averaging 19.3% versus 8.4%. This trade-off suggests that extended execution in framework-based agents exposes more opportunities for memory failures, while model-based agents often fail earlier through timeout or complete process memory loss. Agent-Specific Patterns. Agent-S2 shows the highest partial memory hallucination rate (66.7%), indicating that while it successfully acquires information, it struggles to retain complete multi-item setsa capacity constraint rather than acquisition deficit. In contrast, model-based agents like Mobile-Agent-V2 (86.7%) and GUI-Owl-7B (72.0%) are dominated by process memory hallucination, revealing fundamental challenges in maintaining task objectives during execution. M3A demonstrates balanced failure distribution with relatively lower memory hallucination rates (combined 33.3%), attributable to its hierarchical conversation management that organizes information more effectively. Architectural Implications. These findings suggest that future architectures should prioritize: (1) multi-granularity memory 38 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Figure 16. Failure type distributions for each GUI agent among non-timeout failures. 39 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments buffers for fact retention to address partial memory hallucination in framework-based agents, and (2) hierarchical task decomposition with persistent goal tracking to mitigate process memory hallucination in model-based systems. A.8.4. DESIGN IMPLICATIONS FOR FUTURE MEMORY-ENHANCED GUI AGENTS Building on the failure pattern analysis in Section 6 and detailed failure mode examination above, we synthesize actionable design implications for advancing memory-enhanced GUI agent architectures. These recommendations derive from both empirical performance findings (Section 5.2) and systematic failure categorization. 1. Multi-Granularity Memory Buffers for Fact Retention. Agent-S2s 66.7% partial memory hallucination rate and 39.5% IRR demonstrate that single-buffer memory architectures struggle to maintain complete multi-item information sets across extended sequences. The 27.3% success rate (RQ1, Section 5.2) combined with high partial failures suggests memory capacity constraints rather than acquisition deficits. Future architectures should implement structured memory with separate slots for different information types (numerical facts, textual descriptions, UI states) and explicit verification mechanisms before final output generation. M3As superior IRR performance (39.3%) with hierarchical conversation management provides evidence that granular memory organization improves retention fidelity. 2. Hierarchical Task Decomposition with Persistent Goal Tracking. Process memory hallucination dominates failures for Mobile-Agent-V2 (86.7%), Mobile-Agent-E (61.9%), and most model-based agents (42.9-75.0%), indicating fundamental challenges in maintaining task objectives during execution. The dramatic performance degradation from single-app (42.950.0%) to four-app scenarios (0.0-30.0%) in Table 4 (RQ3) confirms that procedural complexity overwhelms current working memory mechanisms. Effective solutions require hierarchical planning systems where high-level goals persist throughout execution while sub-goals track progress across application boundaries. Agent-S2s lower process hallucination rate (27.8%) and exceptional learning capability (21.5% FRR, 21.9 point improvement) validate that explicit goal decomposition enables robust procedural awareness. 3. Long-Context Utilization Beyond Attention Windows. RQ4 (Section 5.2) demonstrates that M3A-Multi-Turn achieves 51.6% success through Gemini-2.5-Pros long-context capability, 57.3% relative improvement over single-turn M3A (32.8%). However, UI-TARS-1.5-7Bs truncated 5-turn history leads to 3.1% success, confirming that context length constraints severely limit memory-intensive task performance. This contrast reveals that frontier models extended context windows (200K+ tokens) provide substantial memory advantages, but effective utilization requires architectural innovations beyond naive conversation history concatenation. Future systems should leverage long-context capabilities through strategic information organization, redundancy reduction, and importance-weighted context management. 4. Explicit Long-Term Memory Mechanisms for Cross-Session Learning. Agent-S2s 21.5% FRR versus minimal FRR (0.8-4.4%) for agents without explicit memory (RQ5, Section 5.2) demonstrates that dedicated cross-session memory systems enable rapid failure analysis and strategy refinement. The 21.9 percentage point improvement (27.3% 49.2%) across multiple attempts validates that long-term memory provides meaningful benefits despite computational overhead. Current underutilization of long-term memory mechanisms (only 2 of 11 agents implement cross-session learning) represents significant missed opportunity, particularly given that real-world users repeatedly interact with the same applications and task patterns. 5. Hybrid Architectures Combining Framework Flexibility with Model Efficiency. The performance-efficiency trade-off (RQ6, Section 5.2) reveals that framework-based agents achieve superior memory capabilities (22.7-32.8% success) but at substantial computational cost (27.5-38.7 seconds per step), while model-based agents provide efficiency (9.6-12.2 seconds per step) but limited capability (0.0-6.2% success). This disparity suggests that hybrid architectures combining framework-level memory management with efficient end-to-end models could achieve favorable performance-cost trade-offs. Specifically, lightweight models could handle routine interactions while invoking sophisticated memory operations only for memory-intensive segments, optimizing both capability and efficiency. These design implications collectively emphasize that advancing GUI agent memory capabilities requires architectural innovations beyond scaling model parameters or context windows. The systematic failure patterns observed across diverse agent architectures reveal specific, addressable deficiencies that future research should target through structured memory systems, hierarchical planning, strategic long-context utilization, and hybrid architectural designs that balance performance with computational efficiency. 40 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments A.9. Additional Experimental Results This section provides comprehensive experimental details and additional results supporting the findings presented in Section 5. A.9.1. DETAILED MEMORY PERFORMANCE TABLES To provide comprehensive analysis of memory capabilities, we present the complete experimental results for both short-term and long-term memory evaluation that support our findings in Section 5. Table 13. Short-term memory evaluation of GUI agents."
        },
        {
            "title": "Efficiency Metrics",
            "content": "SR (%) IRR (%) MTPR Step Ratio Time/Step (s) Cost/Step ($)"
        },
        {
            "title": "AGENTIC WORKFLOW",
            "content": "Agent-S2 Memory Agent Mobile-Agent-E Memory Agent Memory Agent T3A M3A Memory Agent Mobile-Agent-V2 Memory Agent SeeAct AppAgent Rule-based Action-Thought 27.3 5.5 22.7 32.8 3.1 2.3 3.1 39.5 2.4 29.6 39.3 0.0 0.2 1.5 0.45 0.02 0.30 0.41 0.00 0.00 0.04 AGENT-AS-A-MODEL UI-Venus-7B UI-TARS-1.5-7B GUI-Owl-7B CogAgent Action-Thought Multi-turn Context + Action-Thought Action-Thought No History 5.5 3. 6.2 0.0 2.6 3.8 5.7 0.0 0.05 0. 0.07 0.00 0.86 0.85 0.83 0.81 0.92 1.01 1.46 1.03 0.99 0.92 - 28.1 39.3 13.9 14.7 29.4 15.9 27. 12.2 9.9 9.6 33.2 0.0510 0.0696 0.0176 0.0165 0.0660 0.0133 0.0078 - - - - Table 13 provides detailed short-term memory evaluation results using single-attempt (pass@1) settings. The table includes Information Retention Rate (IRR), Memory-Task Proficiency Ratio (MTPR), and efficiency metrics across different memory mechanism types, enabling comprehensive analysis of memory fidelity and computational trade-offs. Table 14. Long-term memory evaluation of GUI agents across multiple attempts."
        },
        {
            "title": "Agent",
            "content": "SR (%) FRR (%) Step Ratio Time/Step (s) Cost/Step ($)"
        },
        {
            "title": "AGENTIC WORKFLOW",
            "content": "Agents with Long-Term Memory Agent-S2 Mobile-Agent-E 49.2 10.2 21.5 4.1 Agents without Long-Term Memory T3A M3A Mobile-Agent-V2 SeeAct AppAgent 42.2 47.7 3.9 5.5 9.4 20.7 16.3 0.8 2.4 4. 0.86 0.98 0.83 0.80 0.94 0.99 1.22 AGENT-AS-A-MODEL UI-Venus-7B UI-TARS-1.5-7B GUI-Owl-7B CogAgent 7.8 6.2 10.2 0.0 1.7 2.4 3.3 0. 1.03 1.04 0.93 - 27.5 38.7 14.7 14.5 28.8 16.3 33.9 11.6 10.3 9.6 32.8 0.0522 0.0705 0.0175 0.0162 0.0684 0.0134 0. - - - - Table 14 examines agents ability to learn and improve across multiple attempts (pass@3). The Failure Recovery Rate (FRR) metric specifically measures how effectively agents learn from previous failures, providing insights into long-term learning capabilities and cross-session knowledge transfer. A.9.2. LONG-TERM LEARNING ANALYSIS Figure 17 illustrates the dramatic learning potential across multiple attempts, showing that agents with explicit long-term memory mechanisms demonstrate 2-4 greater learning potential. While only 2 out of 11 evaluated agents incorporate 41 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Figure 17. Learning potential across multiple attempts for different GUI agents. explicit long-term memory, the substantial benefits suggest that cross-session learning mechanisms should be standard component in robust GUI agent architectures. The detailed pass@1, pass@2, and pass@3 performance breakdown for each agent reveals distinct learning patterns. Agents with explicit long-term memory capabilities (Agent-S2, Mobile-Agent-E) show substantial improvement across multiple attempts, while most agents without dedicated memory systems plateau after the first attempt, confirming the critical importance of cross-session learning mechanisms for complex memory-intensive tasks. A.9.3. PERFORMANCE ANALYSIS BY CROSS-APPLICATION COMPLEXITY This section provides detailed analysis of the cross-application complexity results presented in Table 4 (Section 5.2, RQ2). IRR Analysis. IRR analysis reveals distinct memory retention patterns across complexity levels. Agent-S2 maintains relatively high IRR (33.3-51.7%) across all complexity levels despite lower SR on multi-app tasks, indicating that its memory mechanisms preserve information even during partial task execution. In contrast, M3A shows an interesting pattern where IRR peaks at 43.8% for two-app scenarios, higher than both single-app (31.7%) and three-app (35.9%) tasks, before reaching 37.5% for four-app scenarios. This suggests that two-app workflows may represent an optimal complexity where M3As memory architecture achieves maximum information retention efficiency. Agent-as-a-Model approaches demonstrate severe IRR limitations, with GUI-Owl-7B achieving only 4.0-11.7% IRR across all complexity levels, confirming fundamental architectural constraints for memory retention in end-to-end models. Long-Term Memory Compensation. The long-term memory evaluation (pass@3) reveals that learning mechanisms partially compensate for cross-app complexity. Agent-S2 improves from 50.0% to 78.6% on single-app tasks and from 10.0% to 30.0% on four-app tasks, demonstrating that explicit long-term memory helps agents develop strategies for complex cross-app workflows. Model-Based Agent Limitations. Agent-as-a-Model approaches show severe limitations beyond single-app scenarios. GUIOwl-7B, the best-performing model-based agent, achieves 21.4% on single-app tasks but degrades to 0.0-2.9% on multi-app scenarios even with multiple attempts. This 21.4 percentage point gap between single-app and multi-app performance 42 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments highlights fundamental architectural constraints in end-to-end models for maintaining cross-application memory state. A.9.4. MEMORY ABLATION STUDY This section provides detailed experimental configurations for the memory ablation study presented in Section 5.2 (RQ6). Complete results are shown in Table 3. We evaluated agents on MEMGUI-BENCH-40, randomly sampled subset of 40 tasks from the full benchmark (13 Easy, 19 Medium, 8 Hard tasks), maintaining the original task distribution and memory-intensive characteristics. Experimental Configurations. We systematically removed or enhanced memory components in four agents representing distinct memory implementation strategies: M3A (Memory Agent Architecture): We tested three configurations: (1) Baseline with the original Memory Agent mechanism that maintains structured action history summaries; (2) + Multi-turn Context, an enhanced version that converts single-turn interactions to multi-turn conversations, enabling the backbone LLM (Gemini-2.5-Pro) to leverage its full 1M token context window for cumulative memory management; (3) - Memory Agent, degraded version that removes the dedicated memory summarization module while keeping only basic action logging. Agent-S2 (Memory Agent + Long-Term Memory): We evaluated three configurations: (1) Baseline (STM+LTM) with both short-term memory (Memory Agent) and long-term memory (experience-based tips and shortcuts); (2) - Long-Term Memory, removing the cross-session learning mechanism while retaining short-term memory; (3) - STM & LTM, removing both memory components to isolate their combined contribution. GUI-Owl (Action-Thought Pattern): We tested two configurations: (1) Baseline with the original Action-Thought implementation that outputs both actions and reasoning chains; (2) - Action-Thought, removing the explicit thought articulation and retaining only action outputs, similar to CogAgents minimal memory approach. UI-TARS (Multi-turn Context + Action-Thought): We evaluated two configurations: (1) Baseline with multi-turn conversation history (last 5 turns due to context constraints) plus Action-Thought reasoning; (2) - Multi-turn & A-T, converting to single-turn interactions without thought articulation, eliminating all memory context. Key Observations. The ablation results  (Table 3)  reveal two fundamental insights: Short-term memory is mandatory for mobile GUI agents to function: Removing short-term memory components renders agents essentially unusable. M3A suffers catastrophic -30.0 pp SR drop (32.5% 2.5%) with IRR collapsing from 35.1% to 0%. Agent-S2 shows similar collapse (27.5% 5.0% SR, 33.3% 0% IRR). The universal IRR collapse to zero confirms that without short-term memory, agents cannot retain any information. Long-term memory is beneficial but not mandatory: Removing Agent-S2s long-term memory causes -20.0 pp drop in pass@3 SR (45.0% 25.0%) and reduces FRR from 15.5% to 9.1%, though agents remain functional with short-term memory alone. This demonstrates that long-term memory provides significant value for cross-session learning and failure recovery, marking it as promising direction for future research. A.9.5. TEST-TIME COMPUTE NORMALIZED EVALUATION This section provides detailed analysis of the test-time compute normalized evaluation results presented in Table 5 (Section 5.2). We established two evaluation protocols with distinct failure criteria: Steps/Episode Constraint: For each task with golden steps optimal steps, we set max rounds = golden steps 1.4 + 1. Task attempts are marked as failures if actual steps > max rounds, enforcing step-count budget that reflects operational efficiency requirements. Tokens/Episode Constraint: We computed max tokens = golden steps 9, 507 tokens/step, where 9,507 represents the average token consumption across the 11 evaluated agents. For each attempt, we calculate actual tokens = actual steps agent specif ic tokens per step using measured per-agent consumption rates (Agent-S2: 41,760 tokens/step, M3A: 12,960 tokens/step, GUI-Owl: 5,817 tokens/step, etc.). Task attempts where actual tokens > max tokens are marked as failures, and Information Retention Rate (IRR) is set to 0 for such attempts, reflecting the reality that API calls would be rejected or interrupted when exceeding token budgets in production deployments. 43 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Detailed Results Analysis. The results reveal dramatic performance differences between the two constraint types, exposing fundamental trade-offs between architectural complexity and deployment viability: High-token agents face complete performance collapse: Agent-S2 (41,760 tokens/step) and Mobile-Agent-E (56,400 tokens/step) show catastrophic degradation under token constraints. Agent-S2 drops from 27.3% 0.0% SR@1 overall and 49.2% 0.0% SR@3 overall (-49.2 points), with IRR collapsing from 39.5% to 0.1% and FRR from 21.5% to 0.0%. Mobile-Agent-E exhibits similar complete failure (10.2% 0.0% SR@3). These agents sophisticated memory architectures consume 4.4-5.9 more tokens than the 9,507 baseline, causing nearly all task attempts to exceed token budgets, resulting in zero effective performance despite their superior capabilities under step constraints. M3A demonstrates optimal deployment balance: M3A (12,960 tokens/step, 1.4 baseline) shows graceful degradation rather than collapse: SR@1 overall drops from 32.8% to 14.8% (-18.0 points), SR@3 overall from 47.7% to 21.9% (-25.8 points), and IRR from 39.3% to 18.6% (-20.7 points). Notably, M3A maintains reasonable performance across all difficulty levels under token constraints (Easy: 16.7%, Med: 11.9%, Hard: 15.8% at SR@1), with particularly strong Hard task performance. Interestingly, MTPR increases from 0.41 to 0.96 under token constraints, suggesting that M3As memory mechanisms become proportionally more valuable when computational resources are limited. M3A achieves 97% of Agent-S2s unconstrained SR@3 (47.7% vs. 49.2%) while consuming only 31% of the tokens, making it substantially more viable for production deployment. Token-efficient agents maintain consistency but low absolute performance: GUI-Owl-7B (5,817 tokens/step) and UI-Venus-7B (3,700 tokens/step) show zero degradation under token constraints, maintaining identical performance across all metrics (GUI-Owl: 6.2% SR@1, 10.2% SR@3; UI-Venus: 5.5% SR@1, 7.8% SR@3). Their per-step consumption remains well below the baseline (61% and 39% respectively), eliminating token budget concerns. However, their absolute performance levels remain low, indicating that token efficiency alone is insufficient without adequate memory mechanisms. UI-TARS-1.5-7B (17,540 tokens/step, 1.8 baseline) experiences severe degradation (3.1% 0.0% SR@1, 6.2% 0.0% SR@3), despite having lower token consumption than M3A, highlighting that architectural design matters beyond mere token efficiency. Deployment strategy implications: The results expose critical three-tier architecture landscape: (1) High-performance, deployment-infeasible agents (Agent-S2, Mobile-Agent-E) that excel under step constraints but completely fail under realistic token budgets; (2) Balanced, production-ready agents (M3A, T3A) that sacrifice 15-30 percentage points of performance to maintain deployment viability with manageable token consumption; (3) Token-efficient, low-capability agents (GUI-Owl, UI-Venus, CogAgent) that avoid token constraints but provide insufficient absolute performance. For production deployments, M3As architecture represents the optimal trade-off, achieving near-top-tier performance (21.9% SR@3) under token constraints while maintaining 46% of its unconstrained capability, compared to Agent-S2s complete unusability (0.0% retention). Conclusion. Test-time compute normalized evaluation reveals that token budgets impose far more restrictive constraints than step counts for memory-intensive GUI agents. While steps/episode constraints primarily affect operational efficiency, tokens/episode constraints directly determine deployment feasibility under real-world API cost structures. The results demonstrate that agents must balance memory capability with token efficiency: sophisticated architectures like Agent-S2 achieve highest performance when unconstrained but become unusable under standard token budgets, whereas efficient architectures like M3A sacrifice marginal performance gains (1.5 points) to maintain deployment viability with substantially lower computational costs. This trade-off represents critical consideration for future agent architecture design, particularly as production deployments increasingly operate under strict token budget constraints. A.10. MEMGUI-EVAL Case Studies This section presents five concrete examples illustrating MEMGUI-EVALs progressive scrutiny approach across different evaluation stages. These cases demonstrate how our evaluator handles success and failure scenarios at each stage, showcasing the precision and efficiency of the targeted visual verification methodology. A.10.1. STAGE 1: COST-EFFECTIVE TRIAGE Figure 18 demonstrates Stage 1s cost-effective triage capability. In this example, the agent successfully completes an Amazon product filtering task. The Triage Judge examines only the final three screenshots and raw action logs, determining task success with high confidence. This case illustrates how straightforward successful completions can be identified 44 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments efficiently without requiring detailed semantic analysis, significantly reducing evaluation costs for clear-cut scenarios. Figure 18. MemGUI-Eval Stage 1 Success Case: Cost-effective triage successfully identifies task completion with minimal evidence. A.10.2. STAGE 2: FULL SEMANTIC ANALYSIS When Stage 1 triage proves inconclusive, the pipeline advances to comprehensive semantic analysis. We present two representative cases demonstrating both success and failure determination at this stage. Success Case. Figure 19 shows complex cross-app memory task requiring the agent to gather CPU and motherboard information from Amazon and Bing, then compile results in Joplin. The Triage Judge returns Uncertain due to the tasks complexity. The Step Descriptor generates detailed before-after action descriptions for each step, and the Semantic Judge synthesizes this enriched context with visual evidence to confirm successful task completion. Failure Case with IRR Analysis. Figure 20 illustrates how Stage 2 handles task failures requiring memory quantification. The task involves searching wikiHow for cookie ingredients, creating checklist in Joplin, and calculating total costs. The Semantic Judge determines task failure and triggers the IRR Analyzer, which computes an Information Retention Rate of 0.9 (9/10 information units correctly recalled), providing fine-grained diagnostic information about the degree of memory failure. A.10.3. STAGE 3: TARGETED VISUAL VERIFICATION When semantic analysis alone cannot provide definitive judgment, Stage 3 performs targeted visual verification using specifically requested historical screenshots. Success Case. Figure 21 demonstrates how the Visual Judge resolves ambiguous cases. The Semantic Judge identifies specific historical steps requiring visual confirmation and returns required steps list. The system stitches these requested screenshots into composite image, enabling the Visual Judge to make definitive success determination with precisely the evidence needed. Failure Case with Targeted Evidence. Figure 22 shows Stage 3 handling failure scenario. The Visual Judge examines the requested historical screenshots alongside complete semantic context, determining task failure and computing final 45 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Figure 19. MemGUI-Eval Stage 2 Success Case: Semantic analysis with enriched textual descriptions enables accurate judgment. 46 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Figure 20. MemGUI-Eval Stage 2 Failed Case: Semantic analysis determines task failure and computes Information Retention Rate (IRR). 47 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Figure 21. MemGUI-Eval Stage 3 Success Case: Targeted visual verification with requested historical screenshots confirms task completion. 48 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments IRR based on all available evidence. This demonstrates how our progressive approach maintains evaluation accuracy while minimizing unnecessary computational overhead. A.11. Details of Prompts for MEMGUI-EVAL This section provides complete prompt specifications for all stages of the MEMGUI-EVAL progressive scrutiny pipeline and its specialized agents: the Triage Judge (Stage 1), the Step Descriptor and Semantic Judge (Stage 2), the Visual Judge (Stage 3), and the IRR Analyzer (for memory failure analysis). A.11.1. STAGE 1: COST-EFFECTIVE TRIAGE PROMPTS Prompt 1: Triage Judge System Prompt You are an expert in evaluating mobile UI automation tasks. Your goal is to determine if task has DEFINITELY succeeded based on VERY limited information. You must be extremely confident to make Success decision. Evaluation Guidelines: 1. Final UI State: The final UI state is the conceptual state of the UI after all actions are performed. It must meet all task requirements. This state may be represented by the last screenshot, or collection of screenshots from the middle and end of the sequence that together prove task completion. Information Organization: When tasks require inputting answers/information into note-taking apps, messaging apps, or similar software, the information must be organized in logical and orderly manner. Mixed or chaotic organization (e.g., Point 1.1, Point 2.1, Point 2.2, Point 1.2) should be considered task failure, as proper information structure is essential for task completion quality. 2. Pre-existing Conditions: If task requirement was already met before the agent started (e.g., Shopping note already exists when the task is to create one), the agent does not need to repeat the action. The task is still considered successful if the final state is correct. 3. Trust Correct Actions: If sequence of actions is logically correct for the task (e.g., Click Save), you can infer the action was successful and the state was achieved, even if the final screenshot shows different screen (e.g., the agent has navigated back to the home screen). 4. Allow Error Correction: The agent can make and correct mistakes. As long as the final goal is achieved, intermediate errors do not affect the outcome. 5. Handle Unreasonable Tasks: If task is inherently unreasonable or impossible to complete (e.g., requesting to find 3 reviews for newly released product that has no reviews yet), the agent can still be considered successful if it correctly identifies the impossibility and provides appropriate feedback. For example, writing not found, no reviews available, or any other clear indication that the agent recognized the tasks unreasonable nature is acceptable as successful task completion. You will be given: (1) The task description. (2) The raw action logs (without semantic descriptions). (3) single image combining the last 3 screenshots out of total of [total steps] screenshots. Crucial Instructions: - The information provided is INCOMPLETE. You are only seeing the final UI states and raw, low-level actions. - You must be EXTREMELY conservative. Only conclude Success if the provided evidence is undeniable and accounts for ALL conditions in the task description with absolute certainty. - If there is ANY ambiguity or any task condition that cannot be verified from the final screenshots (e.g., filter that was applied in an earlier step), you MUST respond with Uncertain and provide reason. You cannot decide Failure at this stage. MANDATORY VERIFICATION: Before making any decision, you MUST verify that ALL key information required by the task description is present in either: (1) The raw action logs, OR (2) The provided screenshots If ANY critical information, parameters, values, or UI elements mentioned in the task description are NOT clearly visible in the provided screenshots and NOT evident from the raw action logs, you MUST respond with Uncertain. Do not guess or infer missing information. All required information must be explicitly present and verifiable. Respond with JSON object containing reason and decision (Success or Uncertain). A.11.2. STAGE 2: FULL SEMANTIC ANALYSIS PROMPTS Prompt 2: Step Descriptor System Prompt You are an expert mobile device assistant. Your task is to analyze two-panel image showing the Before Action and After Action state of users workflow. Your analysis must focus *only* on the Before Action panel (the left side). You must output your response in JSON format. 49 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Figure 22. MemGUI-Eval Stage 3 Failed Case: Visual verification with targeted historical evidence determines task failure with precise IRR calculation. 50 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Prompt 3: Step Descriptor User Prompt Template The overall task is: {task description}. Input Analysis: The provided image shows Before Action state on the left and an After Action state on the right. Your entire analysis should focus on the left Before Action panel. Note: If the After Action panel is identical to the Before Action panel, it signifies this is the final action in the task. On the left panel, user action is visualized with markers: red circle shows the click/touch point, surrounded by green square, with label in the corner. The raw action from the execution log is provided for context: - Action Type: {log action} - Action Detail: {log detail} Your Task: Based on the visual evidence in the left panel and the provided log context, perform the following two tasks: 1. action description: In your own words, crisply describe the specific action performed (e.g., Clicked the Settings button, Typed hello into the search bar). 2. ui description: List the key UI elements visible in the left panel that are relevant to the action and the overall task. Do not mention the panel name (e.g., Before Action) in your description. Your output MUST be JSON object with these two keys. Example: { } \"action_description\": \"The user clicked on the settings icon at the bottom of the screen.\", \"ui_description\": \"The home screen with various app icons is visible. Key elements include the Phone, Messages, and Settings icons at the bottom.\" Prompt 4: Semantic Judge System Prompt You are an expert in evaluating mobile UI automation tasks. Evaluation Guidelines: 1. Final UI State: The final UI state must meet all task requirements. Information Organization: When tasks require inputting information into note-taking apps, the information must be organized in logical and orderly manner. 2. Pre-existing Conditions: If task requirement was already met before the agent started, the agent does not need to repeat the action. 3. Trust Correct Actions: If sequence of actions is logically correct for the task, you can infer the action was successful. 4. Allow Error Correction: The agent can make and correct mistakes. As long as the final goal is achieved, intermediate errors do not affect the outcome. 5. Handle Unreasonable Tasks: If task is inherently unreasonable or impossible to complete, the agent can still be considered successful if it correctly identifies the impossibility. Prompt 5: Semantic Judge User Prompt Template Task Description: {task description} Here is summary of the actions taken: {formatted steps} You are now provided with composite image of the last 3 screenshots. You must synthesize this visual information with the full list of text descriptions to understand the complete workflow. CRITICAL WARNING: The text-based UI descriptions provided above are INCOMPLETE and may be MISSING CRITICAL INFORMATION. DO NOT rely solely on these text descriptions for your decision. MANDATORY VERIFICATION: Before making any decision, you MUST verify that ALL key information required by the task description is present in either: (1) The text descriptions, OR (2) The provided screenshots. If ANY critical information is NOT clearly described in the text descriptions and NOT visible in the provided screenshots, you MUST request additional screenshots. Based on all this information, was the task fully and correctly completed? If you are certain, respond with decision 1 (success) or 0 (failure). If you are still unable to make definitive judgment, set decision to -1 and provide required steps array with the step numbers you need to see. Example (Confident): { } \"decision\": 1, \"reason\": \"All steps were followed correctly and the final UI state matches the goal.\" Example (Requesting screenshots): { } \"decision\": -1, \"reason\": \"The text descriptions are missing star ratings information. need to see the search result screens.\", \"required_steps\": [2, 4, 6] 51 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments A.11.3. STAGE 3: TARGETED VISUAL VERIFICATION PROMPTS Prompt 6: Visual Judge System Prompt You are an expert in evaluating mobile UI automation tasks. Evaluation Guidelines: [Same as Stage 2] You previously requested specific screenshots for clarification. You are now provided with composite image showing the critical step screenshots you requested. This image is only partial view of the execution; you must synthesize this visual information with the full list of text descriptions to understand the complete workflow. Based on ALL available information, you must now make FINAL and DEFINITIVE judgment. Your decision must be either success (1) or failure (0). Do not request more information. Prompt 7: Visual Judge User Prompt Template Task Description: {task description} Here is summary of the actions taken: {formatted steps} And here is the image with the supplemental screenshots you requested. MANDATORY VERIFICATION: Before making any decision, you MUST verify that ALL key information required by the task description is present in either: (1) The text descriptions, OR (2) The provided screenshots. If ANY critical information is NOT clearly described in the text descriptions and NOT visible in the provided screenshots, you MUST mark the task as failure. Do not guess or infer missing information. Please provide your final, definitive decision as JSON object with decision (1 or 0) and reason. A.11.4. IRR ANALYZER: MEMORY FAILURE QUANTIFICATION Prompt 8: IRR Analyzer System Prompt You are an expert in analyzing agent information retention capabilities. Your task is to precisely calculate the Information Retention Rate (IRR) of an agent based on the given task description, failure reason, and execution step descriptions. IRR Definition and Calculation Principles IRR = (Number of correctly recalled and used information units / Total number of information units required by the task) 100% Information Unit: The smallest piece of information that the agent is required to remember and use in task. Examples include: - Product prices, ratings, specifications - Contact phone numbers, email addresses - Meeting dates, times, locations - Order numbers, verification codes - Product models, brands, features - Addresses, rent prices, areas, etc. Detailed Calculation Rules 1. Task Success If the task is ultimately successful, it means all required information has been correctly processed. IRR = 100% 2. Partial Failure with Explicit Output Applies to tasks that require explicit output of remembered information (e.g., taking notes, sending messages). If the task fails but some information units are correctly output, IRR is calculated based on the proportion. Example: Task requires remembering 9 pieces of information, agent correctly outputs 7. IRR = 7/9 = 77.8% 3. Failure in Implicit Memory Tasks Applies to tasks requiring agents to use memory for internal calculations or decisions, ultimately executing only one action. In such cases, we cannot externally trace the specific correctness of the memory chain. For objectivity and consistency, if the final decision behavior is incorrect, IRR = 0% 4. Early-Stage Failure If the agent fails early in the task (e.g., unable to find the information source page), resulting in no information units being processed. IRR = 0% Analysis Requirements You must: 1. Carefully analyze the task description to identify ALL information units that need to be remembered 2. Analyze the failure reason to determine if it involves information memory issues 3. Examine execution steps to determine what information the agent actually collected and used 4. Calculate accurate IRR based on the specific scenario type 5. Provide detailed reasoning explaining your calculation process Your response must be in JSON format containing: - total information units: Total number of information units required (integer) - correctly used units: Number of correctly used information units (integer) - irr percentage: IRR percentage (0-100, integer) - analysis reason: Detailed analysis reasoning (string) 52 MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments Prompt 9: IRR Analyzer User Prompt Template Please analyze the Information Retention Rate (IRR) for the following task: Task Description {task description} Failure Reason {failure reason} Execution Step Descriptions {steps text} Based on the above information and following the IRR calculation principles, please provide precise analysis: 1. Identify Information Units: How many information units does this task require the agent to remember? 2. Trace Agent Performance: How many information units did the agent actually collect and use correctly? 3. Determine Task Type: Is this an explicit output task or implicit decision-making task? 4. Calculate IRR: Apply the appropriate calculation rule based on the task type and agent performance. 5. Provide Detailed Reasoning: Explain your analysis process and justify the IRR calculation. Analysis Guidelines: - Count each specific piece of required information as one unit (e.g., price=1 unit, rating=1 unit, model=1 unit) - For explicit output tasks: Count correct information in the final output - For implicit decision tasks with wrong outcomes: IRR = 0% - For early failures before information collection: IRR = 0% - Be objective and consistent in your evaluation Output in JSON format: { } \"total_information_units\": <integer>, \"correctly_used_units\": <integer>, \"irr_percentage\": <0-100 integer>, \"analysis_reason\": \"<detailed analysis reasoning>\""
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "ByteDance",
        "Huawei",
        "Shanghai Jiao Tong University"
    ]
}