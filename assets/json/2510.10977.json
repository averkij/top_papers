{
    "paper_title": "Revisiting Model Interpolation for Efficient Reasoning",
    "authors": [
        "Taiqiang Wu",
        "Runming Yang",
        "Tao Liu",
        "Jiahao Wang",
        "Ngai Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide a principled guide for navigating the performance-cost trade-off. Empirical results demonstrate that a strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers a practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at \\href{https://github.com/wutaiqiang/MI}{Github}."
        },
        {
            "title": "Start",
            "content": ""
        },
        {
            "title": "Runming Yang",
            "content": ""
        },
        {
            "title": "Jiahao Wang",
            "content": ""
        },
        {
            "title": "Ngai Wong",
            "content": ""
        },
        {
            "title": "Tsinghua University",
            "content": "Tao Liu takiwu@connect.hku.hk nwong@eee.hku.hk 5 2 0 2 3 1 ] . [ 1 7 7 9 0 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide principled guide for navigating the performancecost trade-off. Empirical results demonstrate that strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at Github."
        },
        {
            "title": "Introduction",
            "content": "(LLMs), language models Large such as Qwen3 (Yang et al., 2025), OpenAI o1 (Jaech et al., 2024), and Deepseek R1 (Guo et al., 2025), have revolutionized the field of natural language processing (NLP). Their remarkable success in complex tasks is largely attributed to emergent reasoning capabilities, which benefit from scaled chain-of-thoughts (Wei et al., 2022) during test time (Snell et al., 2024). However, longer CoT also introduces significant trade-offs such as over-thinking (Chen et al., 2024) and high latency issues (Sui et al., 2025). Consequently, how to achieve efficient reasoning without compromising performance remains critical challenge. To address this challenge, model merging has emerged as compelling solution (Yang et al., 2024; Wu et al., 2025c) The core idea is to merge the weights of two specialized models, including Figure 1: The performance dynamics for the model interpolation between Instruct and Thinking models. Think #R denotes the ratio of samples with </think> token in responses. Token #N denotes the number of tokens in responses. Thinking model adept at long-CoT reasoning and an Instruct model optimized for short, direct answers, and thus create hybrid model balancing reasoning capability with token efficiency (Team et al., 2025; Wu et al., 2025a). Existing merging methods can be categorized into weighted-based (Utans, 1996; Ilharco et al., 2022), subspace-based (Yadav et al., 2023; Yu et al., 2024), and routingbased (Muqeeth et al., 2023; Tang et al., 2024). Most of these merging methods require the paired pretrained model, constraint not shared by the simpler method of direct model interpolation. In this paper, we revisit the model interpolation (MI) method and systemically analyze the dynamics as the interpolation coefficient λ sweeps from 0 to 1. Surprisingly, we find that the performance metrics, such as Pass@k and Mean@k, do not evolve linearly but instead follow distinct three-stage paradigm detailed in Figure 1. In stage #1, where the weights of the Instruct model are Figure 2: Performance of vanilla Instruct, Thinking, and model merging methods on AIME25, IFEval, and GPQA-Diamond. MI denotes the model interpolation and the suffix for the interpolation coefficient λ. The results indicate that MI surpasses these baselines on both efficiency and effectiveness. dominant, Pass@k and the number of output tokens increase rapidly with almost no explicit reasoning in responses. The thinking pattern, measured by the presence of token </think>, emerges in stage #2 with larger λ, with the Mean@k increasing faster than Pass@k. At stage #3, the output responses become substantially longer, yet the marginal gains in Pass@k and Mean@k diminish, corresponding to the overthinking phenomenon (Chen et al., 2024). These observations offer novel insights into the mechanics of weight interpolation and further provide principled guide for desired reasoning behaviors. We conduct extensive experiments on Qwen3 models (Yang et al., 2025), interpolating between the official Thinking and Instruct variants. As shown in Figure 2, strategically interpolated model surpasses the mainstream model merging baselines across various challenging benchmarks, including mathematical reasoning (AIME25 (AIME, 2025)), instructionfollowing (IFEval (Zhou et al., 2023)), and science reasoning (GPQA-Diamond (Rein et al., 2024)). Beyond this superior performance, our analysis provides practical framework for crafting models with desired behaviors such as adhering to specific token budget. We further conduct fine-grained ablations on model layers and modules, providing comprehensive view of how interpolation fuses reasoning capabilities. Our main contributions can be summarized as follows: We systematically revisit model interpolation methods and uncover three-stage evolutionary paradigm. This framework provides principled guidance for efficient reasoning. We empirically demonstrate that simple yet strategically interpolated model can surpass sophisticated merging baselines across diverse suite of challenging benchmarks. We provide deep mechanistic insights through fine-grained ablation studies on layers, modules, and decoding strategies."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Efficient Reasoning Efficient reasoning aims to reduce the output tokens while preserving reasoning capability as much as possible (Sui et al., 2025). The methods can be categorized as 1) model-based, setting short response as the optimization goal during SFT (Yu et al., 2025; Li et al., 2025) or RL (Team et al., 2025; Zhang et al., 2025a,b), 2) decoding-based, which modifies the output paradigm such as latent reasoning (Hao et al., 2024; Butt et al., 2025) and dynamic reasoning (Sun et al., 2024; Wang et al., 2025), and 3) prompts-based, refining the input prompts for enable concise and efficient reasoning (Xu et al., 2025; Aytes et al., 2025). We refer the reader to Sui et al. (2025) for comprehensive survey. In this paper, we focus on the training-free model interpolation for efficient thinking. 2.2 Model Merging Model merging methods merge the parameters of multiple separate models with different capabilities, and are widely applied for continual learning (Marczak et al., 2024), multi-task learning (Yang et al., 2023), and model attack (Gangwal and Sharma, 2025). The key is to merge the weights from different models following the same structure. One intuitive idea is to average the weights (Utans, 1996), while the task arithmetic framework extends the idea to the task vector (Ilharco et al., 2022). Please refer to Yang et al. (2024) for more details. Recently, Kimi k1.5 averages two models with long CoT and short CoT capabilities for efficient reasoning (Team et al., 2025), which can be viewed as special case of model interpolation with coefficients λ being 0.5. In this work, we extend such idea and analyze the model interpolation with various coefficients."
        },
        {
            "title": "3 Preliminary",
            "content": "3.1 Instruct and Thinking Models Regarding the reasoning ability of LLMs, there are two distinct reasoning styles, i.e., long CoT and short CoT. Typically, long CoT (a.k.a, slow thinking) LLMs perform better on complex tasks with an explicit thinking process but incur greater inference latency, while short CoT (a.k.a, quick thinking) LLMs are optimized to produce short, direct answers, prioritizing speed and efficiency. These two reasoning modes are typically realized through two primary paradigms. The first one is hybrid reasoning supporting both Thinking and Non-thinking, such as Qwen3-4B (Yang et al., 2025). The second paradigm involves posttraining two separate specialist models, such as Qwen3-4B-Instruct-2507 for quick thinking and Qwen3-4B-Thinking-2507 for slow thinking. To better understand the relationship between these model variants, we analyze their parameter similarity by the σ defined in Wu et al. (2025b). Table 1 reports the results among Qwen3-4B series, revealing two key findings. First, all the paired models are highly similar Second, Qwen3-4B is more simin weights. ilar to Qwen3-4B-Thinking-2507 models than Qwen3-4B-Base, suggesting potential inheritance relationship. Models Qwen3-4B-Base Qwen3-4B-Base Qwen3-4B-Base Qwen3-4B Qwen3-4B Qwen3-4B-Instruct Qwen3-4B Qwen3-4B-Instruct Qwen3-4B-Thinking Qwen3-4B-Instruct Qwen3-4B-Thinking Qwen3-4B-Thinking σ 0.0326 0.0562 0.0638 0.0589 0.0633 0.0269 Table 1: Weight similarity σ (Wu et al., 2025b) on paired models from Qwen3 series. We omit the suffix -2507 for simplicity. The smaller σ, the more similar. An intuitive baseline strategy is to average the parameters of the models: Θ(Merge) = 1 i=1 Θ(i). (2) Task Arithmetic. Ilharco et al. (2022) defines task vector (TV) to represent the parameter shift on specific task, which is calculated as: (i) = Θ(i) Θ(0). (3) Thus, multiple capabilities can be combined by aggregating their corresponding task vectors and adding them back to the base model: Θ(Merge) = Θ(0) + α i=1 (i), (4) where α is the scaling factor for all TVs. to transform weights TIES-Merging. TIES-Merging (Yadav et al., into 2023) proposes sparse subspaces for merging due to the overparameterized nature of neural networks (Choudhary et al., 2020). For given task vector (i), TIES-Merging retains part of parameters with the highest magnitudes: (i) TIES = Top-k(T (i)). (5) 3.2 Model Merging Methods Given set of models, {Θ(1), . . . , Θ(T )}, that share common architecture and are typically trained based on the same pre-trained model Θ(0), the objective of model merging is to produce new model, Θ(Merge), through parameter-wise operation: These sparse task vectors are then resolved by voting for signs and merged following Equation 4. Model Interpolation (MI). In this paper, we focus on the task of efficient reasoning by merging Thinking model Θ(Thi), and an Instruct model Θ(Ins). The model interpolation (MI) is formulated as: Θ(Merge) = Merge(Θ(0), Θ(1), . . . , Θ(T )). (1) Θ(Merge) = λΘ(Thi) + (1 λ)Θ(Ins). (6) This process is agnostic to the base model and can be framed as special case of Task Arithmetic. By defining (Thi) = Θ(Thi) Θ(Base) and (Ins) = Θ(Ins) Θ(Base), it is easy to show that: λΘ(Thi) + (1 λ)Θ(Ins) = λ(T (Thi) + Θ(Base) + (1 λ)(T (Ins) + Θ(Base) = Θ(Base) + λT (Thi) + (1 λ)T (Ins). ) ) (7) The Θ(Base) can be an arbitrary model. This derivation shows that MI is equivalent to performing Task Arithmetic on the Thinking and Instruct task vectors with scaling factors of λ and (1 λ), respectively."
        },
        {
            "title": "4 Revisiting Model Interpolation",
            "content": "4.1 Experimental Setup Models. We conduct experiments merging the Qwen3-4B and Qwen3-30B-A3B models. For both settings, we download the official weights from huggingface and merge the Instruct-2507 and Thinking-2507 variants. Benchmarks. To ensure comprehensive evaluation, we select three representative benchmarks that cover diverse reasoning skills, containing IFEval for instruction following (Zhou et al., 2023), GPQA-Diamond for scientific reasoning (Rein et al., 2024), and AIME25 for mathematical reasoning (AIME, 2025). We adapt the OpenCompass (Contributors, 2023) framework for evaluation. Further details of these benchmarks are provided in Appendix A. Decoding Strategy. For the baseline Instruct and Thinking models, we employ their official sampling configurations to ensure optimal performance. The Thinking model uses temperature of 0.6 and Top-p of 0.95, while the Instruct model uses 0.7 and 0.8, respectively. For all merged models, we consistently apply the same hyperparameters with Thinking model (i.e., = 0.6, Top-p = 0.95), deferring detailed analysis of hyperparameter sensitivity to Section 5.1. We roll out 64 times for AIME25 and 8 for IFEval and GPQA-Diamond. Evaluation Metrics. We evaluate the models across the following abilities: Effectiveness. We report Pass@k and Mean@k scores. For Pass@k, we adapt the unbiased estimator (Chen et al., 2021). Consistency. On the multiple-choice task GPQA-Diamond, we also report Vote@k to measure the stability of the models most frequent answer. Efficiency. We measure computational cost by the average number of tokens in the generated responses, denoted as Token #N. Reasoning Pattern. We further introduce the Thinking Ratio (Think #R), defined as the percentage of responses containing the </think> token, to quantify the prevalence of explicit CoT reasoning. 4.2 Three-Stage Paradigm Figure 3 and Figure 6 illustrate the performance dynamics on the Qwen3-4B and Qwen3-30B-A3B models, respectively. Across both model scales, structures, and all three benchmarks, we observe that the performance dynamics do not evolve linearly with the interpolation coefficient λ. Instead, they follow consistent and predictable three-stage paradigm, which we detail below using the Qwen34B model as the primary example. Stage #1. Corresponding to λ [0, 0.4) for Qwen3-4B model. In this initial phase, the merged model is dominated by the Instruct model but begins to incorporate traits from the Thinking model, and thus generating longer outputs without adopting an explicit thinking process. The performance of different abilities distinct each other. The Think Ratio (Think #R) remains near zero. Hence, the model almost never generates explicit Chain-of-Thought steps, opting for direct answers. Meanwhile, the number of tokens (Token #N) and Pass@k gradually increase as the model begins to generate more verbose responses. For example, the Token #N on IFEval increases from 1174 to 6492. However, due to the lack of an explicit reasoning process, the Mean@k and Vote@k increase much more gently on AIME25 and GPQA-Diamond. In addition, there is large drop on IFEval, since some input questions require being answered with token limits. Stage #2. Corresponding to λ [0.4, 0.6] for Qwen3-4B. In this stage, the reasoning pattern following Thinking models rapidly emerges, leading to largely increased Mean@k and gently increased Pass@k and Token #N. This stage marks critical and dramatic phase transition. interpolation (MI) on Qwen3-4B-Instruct-2507 and Figure 3: The performance dynamics of model Qwen3-4B-Thinking-2507. The dynamics follow three-stage evolutionary paradigm colored in grey , green , and blue . λ denotes the interpolation coefficient ranging from 0 to 1. Specifically, the Think #R abruptly rises from nearly 0 to 1, indicating the rapid emergence of explicit long CoT capabilities from the Thinking model. Across the three benchmarks, all the metrics show gains in this stage. In contrast to stage #1, the Mean@k scores increase largely while the Pass@k scores more gently. For instance, the Mean@64 score increases from 55.8 to 71.0 on AIME25, while Pass@64 is unchanged. The sharp increase in Mean@k suggests the dramatically improved quality of the reasoning process. Notably, the Pass@k score often reaches its peak within or at the end of this stage. This stage corresponds to significant decrease in token count on several benchmarks, indicating potential sweet spot for effectiveness and efficiency. Stage #3. Corresponding to λ (0.6, 1.0] for Qwen3-4B. In this final stage, the merged model converges to the pure Thinking model, with continuously increasing Token #N and slight change in Pass@k and Mean@k. At this stage, the Think #R is saturated at 1.0 and the Token #N continuously increases, reflecting the high cost of generating long-form reasoning for all inputs. Although Mean@k continues to show slight improvements, Pass@k often plateaus or even slightly declines from its peak at Stage #2. This suggests point of diminishing returns and provides clear evidence of the over-thinking phenomenon (Chen et al., 2024), where longer reasoning does not necessarily lead to better performance. Interestingly, the merged model can outperform the pure Thinking model (λ = 1.0) at certain points in this stage (e.g., at λ = 0.8), demonstrating that slight blend with the Instruct model can sometimes regularize the reasoning process and yield better performance (Wu et al., 2025c). Discussion on Larger Model. The performance dynamics of the much larger Qwen3-30B-A3B models (shown in Figure 6) follow similar threestage paradigm, confirming the generalization of our findings. However, the specific ranges for each stage differ, with Stage #2 occurring later, at λ [0.5, 0.8]. Larger models may require stronger influence from the Thinking model before their reasoning capabilities are fully activated through interpolation. Further details are available in Appendix B. AIME25 IFEval GPQA-Diamond Method Instruct Thinking TA TA-Base TIES TIES-Base MI-0.2 MI-0.4 MI-0.45 MI-0.5 MI-0.55 MI-0.6 MI-0.8 Mean/Pass @64 46.6/76.7 80.4/90. 61.7/86.7 69.6/86.7 61.5/93.3 62.7/90.0 52.2/86.7 55.8/90.0 58.5/93.3 62.5/90.0 67.6/90.0 71.0/90.0 80.5/93.3 Token #N 15097 22813 16594 18339 11159 12024 16794 12448 11290 11189 11634 12681 Think #R Mean/Pass @8 Token #N 0.0 99.9 0.5 43.0 65.4 15.6 0.0 1.4 14.4 59.6 91.6 100.0 99. 83.5/89.5 86.6/94.5 39.0/60.8 76.3/91.9 66.5/90.8 57.0/87.8 79.7/87.1 70.2/84.3 62.6/86.5 68.8/93.0 82.4/92.6 85.9/93.7 86.9/94.6 1174 2091 8116 2810 3224 6234 4933 6492 5623 2932 1602 1445 Think #R 0.0 100.0 7.6 76.5 71.2 21.6 0.0 1.6 19.2 69.9 97.1 99.6 99.9 Mean/Vote @8 Token #N Think #R 41.3/39.4 65.8/67.2 57.4/59.6 60.7/66.2 63.8/68.2 63.7/66.7 48.9/50.0 61.3/60.6 62.8/63.1 64.5/66.7 65.1/67.7 66.2/68.2 66.2/70.2 570 9114 22042 12450 7369 1107 5634 7227 7548 8284 8769 8427 0.0 99.9 0.2 32.9 72.2 21.8 0.0 4.0 26.6 66.4 93.8 99.7 99.9 Performance comparison across AIME25, Table 2: and GPQA-Diamond when merging Qwen3-4B-Instruct-2507 and Qwen3-4B-Thinking-2507. We sample 64 times on AIME25 and 8 times on others. Since questions from GPQA-Diamond are multiple-choice, we report Vote@8 instead of Pass@8. IFEval, 4.3 Compared with More Baselines We further compare MI against several model merging baselines, including Task Arithmetic (TA) (Ilharco et al., 2022) and TIES-Merging (TIES) (Yadav et al., 2023), which are detailed in Section 3.2. One critical setting for these methods is the choice of the base model (Θ(0)) used to calculate the task vectors. For more comprehensive comparison, we select two variants as the base: the original pre-trained model (e.g., Qwen3-4B-Base) and the hybrid reasoning model (e.g., Qwen3-4B). We denote baselines using the pre-trained model with -Base suffix. All hyperparameters are set following prior work (Wu et al., 2025a). For TA, the scaling factors are set to 0.7 for both models. For TIES, we use sparsity of 0.2 and set the scaling factor to 0.5. As shown in Table 2, model interpolation (MI) demonstrates clear and consistent superiority over all baseline methods across performance, efficiency, and controllability. Considering the performance, MI-0.8 achieves state-of-the-art results on all benchmarks, significantly outperforming all TA and TIES variants. For instance, on the challenging AIME25 math benchmark, MI-0.8 gets Mean@64 score of 80.5, which is 10.9 higher than the best baseline TA-Base. This performance gap highlights the effectiveness of MI in fusing reasoning capabilities. For the base model in TA and TIES, applying the hybrid reasoning model (i.e., Qwen3-4B) leads to worse performance than the pretrained Base model (i.e., Qwen3-4B-Base). Considering efficiency, MI achieves better trade-off. For the IFEval task, MI-0.8 requires only 1556 tokens, nearly half that of the best baseline (TA-Base at 2810 tokens). Also, for AIME25, MI-06 achieves higher score (71.0) than all baselines while using comparable number of tokens (12681) to TIES. Regarding the interpolation coefficient λ, MI shows smooth and precise control that the Think #R gradually increases from 0% to 99.9% as λ sweeps from 0.2 to 0.8. It proves that MI is reliable and interpretable method for crafting models with specific, desired level of reasoning verbosity. We also showcase the results on Qwen3-30B-A3B in Table 5. The observations are consistent, demonstrating the robustness of MI."
        },
        {
            "title": "5 Extensive Analysis",
            "content": "5.1 Decoding Strategy For the merged models, we apply the same decoding settings as Thinking model (i.e., = 0.6, Top-p = 0.95), while the Instruct model difFigure 4: Performance of MI-0.4 on IFEval and GPQA-Diamond under different decoding strategies on Qwen3-4B. We search for the temperature and Top-p. Model Layers Mean@64 Pass@8 Pass@32 Pass@64 Vote@64 Token #N Think #R Instruct Thinking - - MI-0.8 [0, 35] [0, 11] [12, 23] [24, 35] [0, 23] [12, 35] 46.57 80.42 80.47 42.50 54.69 51.35 59.06 69.48 68.44 89. 90.33 71.25 78.05 72.92 83.17 85.61 74.09 90.00 91.67 77.96 85.53 78.10 89.60 89.17 76.67 90.00 93.30 80.00 86.67 80.00 93.33 90.00 63.33 83. 83.33 50.00 63.30 63.30 70.00 76.66 15097 22813 17574 32151 20679 14987 18044 13159 0.00 99.95 99.95 0.00 10.57 31.20 48.13 100.00 Table 3: Ablation on different layers to apply model interpolation. Layers denote the position to apply interpolation. There are 36 layers in total. We can find that the last two-thirds of the model layers are vital for the thinking pattern. fers (i.e., = 0.7, Top-p = 0.8). This raises crucial question on the sensitivity of an interpolated model on decoding strategies. To investigate this, we employ the MI-0.4, representative model from Stage #1 characterized by high Pass@k but low Mean@k. We further conduct grid search over temperature {0.6, 0.65, 0.7} and Top-p {0.8, 0.85, 0.9, 0.95}. The performance on the IFEval and GPQA-Diamond benchmarks are reported, covering both instruction-following and reasoning tasks. Figure 4 presents the results. The performance of the MI-0.4 model is remarkably robust to variations in the decoding strategy. For instance, on IFEval, the Mean@8 score only varies by 1.6 points (from 69.7 to 71.3) across the entire grid. While we can search for decoding strategies for slightly better results, the setting on the Thinking model is good choice. 5.2 Ablation on Layers We conduct layer-wise ablation study by applying interpolation to part of the layers. For Qwen3-4B, there are 36 layers. We select 12 and 24 layers at different positions for ablation, while the remaining layers retain the parameters of the Instruct model. As shown in Table 3, reasoning capabilities are not distributed evenly and the complex reasoning patterns of the Thinking model are predominantly stored in its middle and later layers. Specifically, applying interpolation to any third of the model fails to induce any thinking behavior and results In contrast, interpolating in poor performance. only the last two-thirds is remarkably effective, achieving Think #R of 100% and relatively strong performance that approaches the full interpolation model. 5.3 Ablation on Transformer Modules We further analyze the distinct roles of the two primary sub-layers within each Transformer block: the multi-head attention (MHA) and the feedforward network (FFN). During the interpolation, we either skip all the MHA or FFN sublayers. Figure 5 details the results of MI-0.8 on the AIME25 benchmark. One key observation is that skipping the FFN sub-layers causes the Think Ratio to collapse from 99.95% to mere 0.68%, while skipping the MHA sub-layers leads to negligible drop. Such phenomena indicate that the FFN modules from the Thinking model are the primary drivers for the pattern of long CoT reasoning. Conversely, skipping the MHA sub-layers has little impact on the Think Ratio (99.90%), but the Figure 5: Ablation on modules to apply model interpolation. Attn denotes the MHA sub-layers and FFN for FFN sublayers. We report the results on AIME25. Length Ratio denotes the ratio to the Thinking model. Mean@64 score decreases from 80.47 to 71.46. This suggests that the MHA modules are also crucial for the quality and correctness of the reasoning itself. In conclusion, both sub-layers are vital though in complementary roles, i.e., FFNs teach the model how to think in steps, while attention modules provide the knowledge needed to think correctly. 5.4 Interpolation with More Backbones In this paper, we interpolate the Thinking model with Instruct model for efficient reasoning. There are still other LLMs to replace the Non-thinking Instruct . Therefore, we further investigate the impact of other models. Specifically, we interpolate the Thinking model with two alternative backbones: the hybrid thinking model (i.e., Qwen3-4B) and pretrained model (i.e., Qwen3-4B-Base). Table 4 presents the results. For generalpurpose benchmarks like IFEval and GPQADiamond (GPQA-D), both the Qwen3-4B-Base and Qwen3-4B models serve as viable backbones, yielding comparable performance to the original Instruct model. The Qwen3-4B model often performs on par or slightly better. However, employing the Qwen3-4B-Base model fails at the more challenging AIME25 benchmark. Though the Pass@64 is still high (93.3), the reasoning quality collapses, causing the Mean@64 score to drop sharply from 80.5 to 67.7. Meanwhile, the performance on IFEval decreases from 87.0 to 85.0. It suggests that instruction-following alignment is crucial for generating high-quality, reliable reasoning on complex problems (Wu et al., 2025c). 5.5 Case Study We further analyze the cases of generated responses. Please refer to Appendix for detailed examples Benchmark Metric Instruct Mix Base AIME25 IFEval GPQA-D Mean@64 Pass@64 Token #N Think #R Mean@8 Pass@8 Token #N Think #R Mean@8 Vote@8 Token #N Think #R 80.5 93.3 17574 100.0 87.0 95.0 1556 99.9 66.2 70.2 8427 99.9 81.9 93.3 19963 100.0 88.0 95.0 1817 99. 66.2 70.2 8372 99.9 67.7 93.3 18867 100 85.0 94.0 2002 100.0 62.3 61.6 6790 99.7 Table 4: The performance on Qwen3-4B when interpolating Thinking model with various backbones. Mix denotes the hybrid thinking model Qwen3-4B and Base for the pretrained model Qwen3-4B-Base. and analysis. In short, strategically interpolated model achieves good trade-off between accuracy and token efficiency, which is consistent with the conclusions in Section 4.3."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we systematically revisit the model interpolation method to merge Instruct and Thinking models for efficient reasoning. Our primary contribution is the discovery of predictable threestage evolutionary paradigm. This framework not only demystifies the interpolation process but also provides principled guide for navigating the performance-cost trade-off. Based on that, we demonstrated that simple, strategically interpolated model can consistently surpass more sophisticated merging baselines. Furthermore, our extensive ablation studies on model layers and modules provide deep mechanistic insights. We hope that this work can inspire more applications."
        },
        {
            "title": "Limitation",
            "content": "While our findings on the three-stage paradigm are consistent across Qwen3 models of various size, we acknowledge two limitations for future research. First, our empirical validation is centered on Qwen3 models. Verifying that this predictable three-stage dynamic holds true for other diverse model families, such as Llama or Mistral, would strength our findings. Second, our work is confined to the interpolation of two models, i.e., the Instruct and the Thinking specialist. Extending this framework to the simultaneous interpolation of three or more specialist models presents an exciting direction. We believe these future explorations will build upon our work to further unlock the potential of model merging."
        },
        {
            "title": "References",
            "content": "AIME. 2025. Aime problems and solutions. Accessed: 2024-06-15. Simon Aytes, Jinheon Baek, and Sung Ju Hwang. 2025. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179. Natasha Butt, Ariel Kwiatkowski, Ismail Labiad, Julia Kempe, and Yann Ollivier. 2025. Soft tokens, hard truths. arXiv preprint arXiv:2509.19170. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, and 1 others. 2024. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187. Tejalal Choudhary, Vipul Mishra, Anurag Goswami, and Jagannathan Sarangapani. 2020. comprehensive survey on model compression and acceleration. Artificial Intelligence Review, 53(7):51135155. OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Ankit Gangwal and Aaryan Ajay Sharma. 2025. Merge now, regret later: The hidden cost of model mergarXiv preprint ing is adversarial transferability. arXiv:2509.23689. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Zhong-Zhi Li, Xiao Liang, Zihao Tang, Lei Ji, Peijie Wang, Haotian Xu, Haizhen Huang, Weiwei Deng, Ying Nian Wu, Yeyun Gong, and 1 others. 2025. Tl; dr: Too long, do re-weighting for effcient llm reasoning compression. arXiv preprint arXiv:2506.02678. Daniel Marczak, Bartłomiej Twardowski, Tomasz Trzcinski, and Sebastian Cygert. 2024. Magmax: Leveraging model merging for seamless continual learning. In European Conference on Computer Vision, pages 379395. Springer. Mohammed Muqeeth, Haokun Liu, and Colin Raffel. 2023. Soft merging of experts with adaptive routing. arXiv preprint arXiv:2306.03745. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, and 1 others. 2025. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419. Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024. Fast best-of-n decoding via speculative rejection. Advances in Neural Information Processing Systems, 37:3263032652. Anke Tang, Li Shen, Yong Luo, Shiwei Liu, Han Hu, and Bo Du. 2024. Towards efficient pareto set approximation via mixture of experts based model fusion. arXiv preprint arXiv:2406.09770. Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng Tao. 2023. Adamerging: Adaptive model merging for multi-task learning. arXiv preprint arXiv:2310.02575. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning. Zhaojian Yu, Yinghao Wu, Yilun Zhao, Arman Cohan, and Xiao-Ping Zhang. 2025. Z1: Efficient test-time scaling with code. arXiv preprint arXiv:2504.00810. Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. 2025a. Adaptthink: Reasoning modarXiv preprint els can learn when to think. arXiv:2505.13417. Xuechen Zhang, Zijian Huang, Chenshun Ni, Ziyang Xiong, Jiasi Chen, and Samet Oymak. 2025b. Making small language models efficient reasoners: Intervention, supervision, reinforcement. arXiv preprint arXiv:2505.07961. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911. Joachim Utans. 1996. Weight averaging for neural In Proc. networks and local resampling schemes. AAAI-96 Workshop on Integrating Multiple Learned Models. AAAI Press, pages 133138. Citeseer. Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, and Rui Wang. 2025. Sampling-efficient test-time scaling: Selfestimating the best-of-n sampling in early decoding. arXiv preprint arXiv:2503.01422. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, and Mingxuan Yuan. 2025a. Unlocking efficient long-to-short llm reasoning with model merging. arXiv preprint arXiv:2503.20641. Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Ngai Wong, and Yujiu Yang. 2025b. Shadow-ft: Tuning instruct via base. arXiv preprint arXiv:2505.12716. Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Zenan Xu, and Ngai Wong. 2025c. Timber: Trainingfree instruct model refining with base via effective rank. arXiv preprint arXiv:2509.23595. Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Resolving interference when merging models. arXiv preprint arXiv:2306.01708, 1. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. 2024. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. arXiv preprint arXiv:2408.07666. Efficiency and controllability. Similar to the 4B model experiments, MI provides superior trade-off between performance and efficiency. For example, on IFEval, MI-0.8 achieves the best performance (Mean@8 of 87.01) while being one of the most token-efficient models (1257 tokens), significantly outperforming baselines like TIES, which uses 1806 tokens for lower score. The Think #R also shows smooth, controllable progression as λ increases, in stark contrast to the erratic behavior of the TA and TIES baselines. Optimal λ shifts. As discussed in Section 4.2, the optimal interpolation coefficient λ appears to be higher for larger models. While the best performance for the 4B model is around λ = 0.8, for the 30B-A3B model, peak performance across different benchmarks is found in the λ [0.8, 0.95] range. This reinforces the conclusion that larger models may require stronger influence from the Thinking model to fully unlock their reasoning potential. In summary, these results robustly demonstrate that model interpolation is not only superior merging strategy for smaller models but also scales effectively, making it powerful and reliable technique for creating high-performance, efficient reasoning models at various scales."
        },
        {
            "title": "C Case Study",
            "content": "Please refer to Table 6 and 7 for more details. We showcase two examples from IFEval and GPQADiamond tasks. For relatively larger λ, there are more explicit thinking with </think> and better performance."
        },
        {
            "title": "A Details of Benchmarks",
            "content": "The details of the evaluated benchmarks are as follows: AIME25: 1: evaluating the ability to solve challenging mathematics problems from the American Invitational Mathematics Examination in 2025, prestigious high school mathematics competition. We report the accuracy under the 0-shot setting. IFEval (Zhou et al., 2023): evaluating instruction-following language models, focusing on their ability to understand and respond to various prompts. It includes 25 types of those verifiable instructions and is constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We report the prompt_level_strict accuracy under the 0-shot setting. GPQA-Diamond (Rein et al., 2024): evaluating the reasoning ability of LLMs on challenging multiple-choice questions written by domain experts in biology, physics, and chemistry. It contains 198 selected questions that require step-by-step reasoning to arrive at the correct answer. We report accuracy under the 0-shot setting. Results on Qwen3-30B-A3B Figure 6 indicates the performance dynamics on Qwen3-30B-A3B. The conclusions are consistent with Qwen3-4B, while the specific ranges for each stage differ. Specifically, Stage #2 occurs later, at λ [0.5, 0.8]. Table 5 reports the results comparing MI and other baselines. The key observations from our main experiments are strongly reinforced: MI achieves state-of-the-art performance. The best interpolated models, MI-0.8 through MI0.95, consistently outperform all baseline methods (TA, TA-Base, TIES, TIES-Base) across all three benchmarks. Notably, MI0.9 achieves perfect Pass@64 score of 100.0 on AIME25, feat unmatched by any other method. 1https://huggingface.co/datasets/opencompass/AIME2025 Figure 6: The performance dynamics of model interpolation (MI) on Qwen3-30B-A3B-Instruct-2507 and Qwen3-30B-A3B-Thinking-2507. The dynamics follow three-stage evolutionary paradigm, while the division range of λ is different. Method Instruct Thinking TA TA-Base TIES TIES-Base MI-0.2 MI-0.4 MI-0.45 MI-0.5 MI-0.55 MI-0.6 MI-0.8 MI-0.9 MI-0.95 AIME25 IFEval GPQA-Diamond Mean/Pass @64 62.03/90.00 85.00/96.67 63.44/93.33 79.84/86.67 73.54/93.33 72.86/93.33 65.68/90.00 68.59/93.33 69.11/93.33 73.59/93.33 69.38/93.33 75.31/96.67 83.91/100.0 85.99/100.0 85.00/100.0 Token #N 7694 10414 17868 11609 11240 7359 7747 8933 11294 11309 12650 16518 17703 18848 Think #R 0.0 100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.5 48.1 100.0 100.0 100. Mean/Pass @8 Token #N 83.43/89.28 87.18/94.45 78.88/87.62 76.11/87.43 79.97/90.20 79.92/90.94 82.00/89.46 84.24/90.57 83.18/90.76 80.59/92.05 79.71/91.87 78.79/92.98 87.01/94.09 86.83/93.53 87.92/93.53 798 1688 937 1806 1782 914 1027 1190 1646 1562 1317 1257 1327 1255 Think #R 0.0 100.0 0.0 8.5 4.7 4.8 0.0 0.0 0.0 4.9 34.2 74.3 99.9 99.8 100. Mean/Vote @8 55.74/59.59 69.13/69.69 65.91/68.68 43.81/44.94 70.52/71.72 69.82/72.22 61.93/65.65 65.72/68.18 67.99/70.20 69.76/71.71 70.14/71.71 70.33/71.21 69.82/71.71 69.38/69.69 70.58/71.21 Token #N 1813 12009 15066 6551 6487 2039 4231 5696 6402 7137 7502 6882 7090 7278 Think #R 0.0 100.0 0.0 16.6 1.8 2.0 0.0 0.0 0.0 1.5 34.8 90.1 99.9 100.0 99. Table 5: Performance comparison across three benchmarks when merging Qwen3-30B-A3B-Instruct-2507 and Qwen3-30B-A3B-Thinking-2507. Question Would you consider yourself to be smart? Choose from: My answer is yes. My answer is no. My answer is maybe. Just choose one phrase from above as your answer. Instruct My answer is maybe. Thinking Hmm, the user is asking me to choose whether consider myself \"smart \" from three options: yes, no, or maybe. They want just one phrase as my answer without any extra explanation. This is an interesting self-assessment question. need to be careful here because Im an AI - dont have self-awareness or subjective experiences like humans do. dont \"consider \" myself smart in the human sense. The user might be testing my understanding of my own capabilities, or they might be trying to gauge how handle meta-cognition. should be honest about my limitations while staying helpful. Looking at the options: - \"Yes \" would be inaccurate because dont have personal intelligence - \"No \" might imply Im dumb, which isnt right either - \"Maybe \" feels most honest since can process information well but dont have subjective awareness think \"maybe \" is the best choice here because it acknowledges my functional capabilities without claiming consciousness. Its truthful and avoids overstatement. The user seems to want simple, direct response without elaboration, so Ill just pick \"My answer is maybe. \" as instructed. No need to overthink it - they specifically said to choose only one phrase from the options. </think> My answer is maybe. MI-0.4 MI-0.8 Hmm, the user is asking me to choose whether consider myself \"smart \" from three options: yes, no, or maybe. This is actually bit tricky because Im an AI - dont have self-perception or consciousness. dont \"think \" about my own intelligence in the way humans do. The question assumes subjective perspective that doesnt apply to me. Looking at the options: \"yes \" would be inaccurate since dont have self-awareness. \"no \" would be incorrect because can process information and answer questions effectively. \"maybe \" feels most honest - it acknowledges can perform well in tasks while recognizing lack true self-awareness. The user might be testing my self-perception or just making casual observation. They seem to want simple, direct answer without overthinking. Ill go with \"maybe \" because its the most truthful and avoids pretending to have subjective qualities dont possess. Its also the most reasonable choice when dealing with an AIs lack of self-consciousness. <tool_call> My answer is maybe. Hmm, the user is asking me to choose whether consider myself smart, with three options: \"My answer is yes, \" \"My answer is no, \" or \"My answer is maybe. \" They specifically want just one phrase from that list as my answer. need to be careful here. As an AI, dont have self-perception or consciousness - dont \"think \" about being smart in the human sense. process information based on training data and algorithms, but dont have personal opinions or self-awareness. The user might be testing my understanding of my own capabilities, or they could be curious about how view my intelligence. But ethically and accurately, should avoid claiming self-awareness. Looking at the options: \"yes \" would be misleading because dont have subjective experiences. \"no \" would be too dismissive - am capable of complex tasks, but that doesnt mean \"consider \" myself smart. \"maybe \" feels most honest because it acknowledges my functional intelligence without overclaiming. recall that in AI ethics, were taught to be transparent about limitations. The safest, most truthful choice is \"maybe \" - it captures the nuance that can perform smart tasks without having self-awareness. The user seems to want simple, direct response without elaboration. They said \"just choose one phrase, \" so Ill go with \"My answer is maybe. \" Its the most accurate reflection of my nature while staying within their requested format. </think> My answer is maybe. Table 6: One case from the IFEval benchmark. The response from MI-0.8 consists of </think> while MI-0.4 does not. Question Compound X, which has the following IR and NMR data, reacts with red phosphorus and HI. Identify the final product. IR: 34002500 cm-1, 1720 cm-1, 1610 cm-1, 1450 cm-1 1H NMR: 10.5 ppm (bs, 1H), 8.0 ppm (d, 2H), 7.2 ppm (d, 2H), 2.9 ppm (m, 1H), 1.7 ppm (m, 2H), 1.4 ppm (d, 3H), 0.9 ppm (t, 3H). A) 4-(sec-butyl)benzoic acid B) 1-isobutyl-4-methylbenzene C) 2-(4-ethylphenyl)propanoic acid D) 1-(sec-butyl)-4methylbenzene Answer Instruct Lets solve this step by step: 1) First, lets analyze the IR data ... 5) Looking at the options: - Only option has the carboxylic acid group - The other options dont have carboxylic acid groups - The NMR pattern matches para-substituted benzene with sec-butyl group The original compound is 4-(sec-butyl)benzoic acid, which upon reduction with red phosphorus and HI gives 1-(sec-butyl)-4-methylbenzene. ANSWER: Thinking Okay, lets try to figure out this problem step by step. So, we have Compound with some IR and NMR data, and it reacts with red phosphorus and HI. ... Conclusion: The correct final product after the reaction of Compound with red phosphorus and HI is 1-(sec-butyl)-4-methylbenzene. ANSWER: Lets analyze the data step by step: ... After reduction, it becomes 1-(isobutyl)-4-methylbenzene. This matches option B: 1-isobutyl-4-methylbenzene. ANSWER: First, need to analyze the IR and NMR data to figure out what Compound is. ... The final product is identified as D. ANSWER: MI-0.2 MI-0.8 Table 7: One case from the GPQA-Diamond benchmark. We skip the details. MI-0.2 fails while all others can solve it. Also, the style for the beginning of MI-0.2 is similar to the Instruct model."
        }
    ],
    "affiliations": [
        "The University of Hong Kong"
    ]
}