{
    "paper_title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context",
    "authors": [
        "Haoran Hao",
        "Jiaming Han",
        "Yiyuan Zhang",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Large Language Models (LLMs) have led to significant breakthroughs in video understanding. However, existing models still struggle with long video processing due to the context length constraint of LLMs and the vast amount of information within the video. Although some recent methods are designed for long video understanding, they often lose crucial information during token compression and struggle with additional modality like audio. In this work, we propose a dynamic long video encoding method utilizing the temporal relationship between frames, named Temporal Dynamic Context (TDC). Firstly, we segment the video into semantically consistent scenes based on inter-frame similarities, then encode each frame into tokens using visual-audio encoders. Secondly, we propose a novel temporal context compressor to reduce the number of tokens within each segment. Specifically, we employ a query-based Transformer to aggregate video, audio, and instruction text tokens into a limited set of temporal context tokens. Finally, we feed the static frame tokens and the temporal context tokens into the LLM for video understanding. Furthermore, to handle extremely long videos, we propose a training-free chain-of-thought strategy that progressively extracts answers from multiple video segments. These intermediate answers serve as part of the reasoning process and contribute to the final answer. We conduct extensive experiments on general video understanding and audio-video understanding benchmarks, where our method demonstrates strong performance. The code and models are available at https://github.com/Hoar012/TDC-Video."
        },
        {
            "title": "Start",
            "content": "Haoran Hao1,2, Jiaming Han1, Yiyuan Zhang1, Xiangyu Yue1 1MMLab, The Chinese University of Hong Kong 2Nanjing University 5 2 0 2 4 1 ] . [ 1 3 4 4 0 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Large Language Models (LLMs) have led to significant breakthroughs in video understanding. However, existing models still struggle with long video processing due to the context length constraint of LLMs and the vast amount of information within the video. Although some recent methods are designed for long video understanding, they often lose crucial information during token compression and struggle with additional modality like audio. In this work, we propose dynamic long video encoding method utilizing the temporal relationship between frames, named Temporal Dynamic Context (TDC). Firstly, we segment the video into semantically consistent scenes based on inter-frame similarities, then encode each frame into tokens using visual-audio encoders. Secondly, we propose novel temporal context compressor to reduce the number of tokens within each segment. Specifically, we employ querybased Transformer to aggregate video, audio, and instruction text tokens into limited set of temporal context tokens. Finally, we feed the static frame tokens and the temporal context tokens into the LLM for video understanding. Furthermore, to handle extremely long videos, we propose training-free chain-of-thought strategy that progressively extracts answers from multiple video segments. These intermediate answers serve as part of the reasoning process and contribute to the final answer. We conduct extensive experiments on general video understanding and audio-video understanding benchmarks, where our method demonstrates strong performance. The code and models are available at https://github.com/Hoar012/TDC-Video. 1. Introduction Recently, advances in large language models (LLMs) [2, 41, 49] have significantly improved their ability in language processing and generation. Researchers have extended these models to other modalities, such as vision [30, 33, 40, 81], audio [10, 17] and point clouds [22, 23], Equal contribution Corresponding author Figure 1. Comparison of Visual and Audio Encoding in Video Modeling. (a) Existing methods encode each modality separately and then concatenate them, leading to inconsistencies and difficulties in handling long videos. (b) We propose Temporal Dynamic Context (TDC) compression, which incorporates both static visual features and dynamic video context to represent videos more effectively. This approach enables better multimodal integration and efficient compression for long videos. leading to the development of powerful multimodal LLMs (MLLMs). These MLLMs achieve strong performance in various tasks, such as image captioning [6, 24] and question answering [40, 54]. However, video understanding remains challenging problem due to the interplay of multiple modalities [10, 17] and the complexity of large-scale information [37, 52], especially in long videos. key challenge in long video processing is efficiently representing videos to minimize redundancy between frames while preserving crucial details. Some attempts have been made to address this challenge [35, 51, 59, 61], but they typically compress video tokens based 1 on visual similarity between frames, while overlooking high-level dynamic semantic information. This limits the models ability to capture comprehensive video information within fixed number of tokens, increasing the difficulty of understanding the content. Another challenge lies in integrating multiple modalities for comprehensive video understanding. For instance, when watching movie, humans naturally integrate speech, background music, visual scenes, and subtitles to grasp the full context. However, most existing video LLMs are limited to visual and textual modalities and struggle to tackle audio. Although some works [10, 17, 77] represented by VideoLLaMA [77] try to incorporate both audio and visual information, their approach of simply concatenating modality tokens often leads to suboptimal performance by treating different modalities separately (refer to Fig. 1 (a)). Developing unified representation method that effectively connects information between modalities is essential to improve multimodal video understanding. To address these challenges, we propose video representation model that integrates multiple modalities within unified video context and achieves effective token compression. As shown in Fig. 1 (b), (1) we segment the video into scenes based on the visual consistency between frames, which are then encoded separately. (2) Each video clip is represented using both static visual features of the key frame and dynamic video context of the video. We first extract per-second features using visual encoder and an audio encoder. The feature of the first frame is fully retained as static representation, while the features of subsequent frames are compressed by Q-Former [12], based on their temporal consistency and differences relative to the static frame. This approach enables effective token compression while integrating multiple modalities within the video context. (3) To enhance the effectiveness of the model on extremely long video, we introduce the Long Video Chain-ofThought (LVCoT) strategy, which guides the model to process long videos step by step before integrating the whole video to generate the final output. We train models of various sizes using multi-stage strategy, progressively optimizing them for vision-language alignment, video instruction tuning, and audio-video instruction tuning. We evaluate our models on range of video benchmarks, including MVBench [34], PerceptionTest [47], EgoSchema [43], MLVU [86], and VideoMME [16]. Furthermore, we assess their performance on audio-video question-answering benchmarks, such as Music-QA [31] and AVSD [3]. Resuls show that our models achieve strong performance on both video and multimodal understanding tasks. Our main contributions include: We propose framework for multimodal video modeling, which represents videos using both static visual features and dynamic multimodal context, effectively integrating visual and audio information within unified video context. We introduce the Long Video Chain-of-Thought (LVCoT), training-free strategy that enables MLLMs to process and reason over long videos step by step, enhancing the performance of existing models. We conduct extensive experiments with MLLMs of various sizes and evaluate them on multiple benchmarks, including general video question answering, long video understanding, and audio-visual video comprehension. Our models achieve strong performance, advancing the field of multimodal long video understanding. 2. Related Work Multimodal Large Language Models. Previously, multimodal models like CLIP [50] primarily focused on specific tasks and modalities, including vision and language. Recently, the integration of LLMs and the scaling of data and model size have produced advanced MLLMs. These MLLMs [19, 40, 44, 45, 57, 79, 81] possess powerful understanding, reasoning, and generation capabilities, enabling them to show brilliant performance on wide range of multimodal tasks. This proficiency in vision-language understanding can be seamlessly extended to video understanding. For instance, Video-LLaVA [38] and LLaMAVID [37] utilize LLMs as decoders to generate answers based on video content and input questions. Several works [10, 17, 19, 20, 22, 23, 23, 45, 57, 77, 78] attempt to incorporate additional modalities, such as audio, into LLMs to enhance their perception of the complex world. In addition, some works [15, 39, 62, 76] focus on developing unified MLLM for both understanding and multimodal generation tasks. Vision, language and audio are core components of human perception. To enable models to effectively understand dynamic environments, video comprehension has gained increasing attention. However, it remains field that requires further exploration. Long Video Understanding. The rapid development of MLLMs has enabled researchers to extend their visionlanguage understanding ability to process videos. VideoLLaVA [38] and VideoChat [33] use video instruction data to train LLMs to generate language responses for input video and questions. However, early video LLMs typically represent video using multiple images. While this simple representation shows some progress in short video processing, it struggles with longer videos due to significant information loss. Recently, some methods [29, 71, 82] are proposed to compress tokens used for image representation, among which LLaVA-Mini [82] utilizes modality prefusion to aggregate visual information into language tokens, which effectively reduces the number of tokens needed for an image. Similar approaches are employed to compress 2 Figure 2. Architecture of Our Multimodal Video Encoder. We first extract features for each second of the video, including both visual and corresponding audio tokens. The first frame is selected as the static frame, and Q-Former is used to perform Temporal Dynamic Context compression based on its relationship with subsequent frames, resulting in compressed tokens per frame. The final video representation consists of all static frame tokens and multimodal video context. frame tokens in long-video LLMs [35, 37, 51, 59, 69, 83]. Specifically, LLaMA-VID [37] uses context attention to extract video context relevant to the query. LongVU [51] reduces redundant frames based on inter-frame similarity and relevance with text query. VideoChat-Flash [35] compresses the visual representation of video clip by exploiting inter-frame redundancies and semantic correlations. However, most existing methods focus primarily on vision-based video modeling, which overlook other common modalities, such as audio and speech, that are essential for comprehensive video understanding. Multimodal Video Modeling. Advancements in MLLMs have made it possible to process videos with multiple modalities. VideoLLaMA2 [10] integrates BEATs [8] to encode audio for LLM understanding, while PandaGPT [53] combines ImageBind [18] and Vicuna [11] to process six modalities, enabling audio-video-language conversation. NExT-GPT [62] integrates an LLM with adaptors to perceive multimodal inputs, and uses different diffusion decoders to generate outputs in combinations of text, image, video, and audio. SAVEn-Vid [32] introduces an audiovisual video dataset containing over 58,000 audio-visual instructions and uses it to train an audio-visual MLLM, SAVEnVideo. LongVALE [17] develops an automatic pipeline for unified vision-audio-language-event video annotation and establishes novel benchmark. However, previous methods typically rely on simple frame sampling for video representation, and straightforward concatenation of different modalities for MLLM understanding. These simplifications limit the effectiveness of long video processing and multimodal integration. In this work, we propose unified video modeling approach to enhance multimodal long video understanding. 3. Methodology In this section, we first introduce the preliminary concepts of LLM-based video understanding in Section 3.1. Next, we present detailed explanation of our proposed video representation model, Temporal Dynamic Context, in Section 3.2. We then introduce our progressive training strategy to align video-audio information with LLMs in Section 3.3. Finally, we propose training-free chain-of-thought approach to process extremely long videos in Section 3.4. The architecture of our model is shown in Figure 2. 3.1. Preliminaries The common approach of LLM-based video representation starts by sampling set of frames from video and encoding each frame individually using pretrained visual encoder [50]. We denote video of seconds at rate of one frame per second as = {x1, x2, . . . , xT }. Previous methods typically sample fixed number of frames from X, regardless of the video length , e.g., only 8 frames for Video-LLaVA [38]. Each sampled frame is then encoded by the image encoder as Fxt = E(xt) and projected into space interpretable by the LLM. The resulting tokens from 3 all sampled frames are concatenated with text tokens Fs as input to the LLM. However, low sampling rate leads to sparse frame selection and significant information loss, while dense sampling results in an excessive number of tokens. This challenge stems from disregarding the temporal relations between frames, limiting the models ability to process long videos efficiently. 3.2. Temporal Dynamic Context Humans process visual input holistically, rather than treating each frame as an independent image. We typically recognize the overall scene first and then focus on dynamic changes. Inspired by this observation, we propose to model video using both the static features of key frames and the temporal dynamics of the whole scene. Static features allow the model to capture fine-grained visual details, while temporal dynamics encode the evolution of the video over time. In the following section, we introduce temporal dynamic context encoding method. Video Scene Segmentation. For each video, we maintain the original 1 frame per second (fps) rate to preserve content consistency and prevent temporal information loss. Next, we segment the video into semantically consistent clips based on inter-frame similarities. In contrast, existing methods typically segment videos into fixed-duration clips and encode them separately, neglecting the temporal relations between frames. We employ the self-supervised vision encoder DINOv2 [46], which is proved to be effective in capturing visual details [51], to extract high-dimensional embeddings. We compute the cosine similarities between consecutive frame pairs and identify the S1 points with the lowest frame consistency. Using these points, we segment the input video into scenes, which enhances temporal coherence in subsequent video encoding. Static Feature Encoding. For each segmented scene, we represent it using static features along with subsequent dynamic video context. For every second of video, we extract both visual and audio tokens using pretrained vision and audio encoders. Within sliding window of length , the first frame is selected as the static frame, where all visual and audio tokens are retained in their original form. The remaining frames are then compressed into temporal dynamic context. Temporal Dynamic Context. To encode the dynamic evolution of video, we exploit the relationships between consecutive frames and the static reference frame. Previous methods typically compress videos based on visual similarity, overlooking the semantic relationships. This often results in suboptimal compression performance and makes it harder for MLLMs to accurately understand the full video. In contrast, we adopt temporal difference-based strategy by computing the semantic differences between each frame and the static frame, aiming to better preserve meaningful temporal dynamics. Specifically, we implement it with Query Transformer [12] (Q-Former). We apply average pooling to the static features of the first frame, Fx1, to obtain query tokens RKD of dimension D: = AvgPool(Fx1). (1) Note that learnable query tokens are another option, but our experiments (Section 4.4) show that average pooled tokens are more effective. For each subsequent frame, its visual tokens Fxi and the corresponding audio tokens Fai are projected to the same dimension and fed to the Q-Former, which performs cross-attention between these tokens and the query tokens:"
        },
        {
            "title": "F i",
            "content": "Q = QFormer(Q, [Fxi Fai]), (2) where [] denotes token concatenation. To make the compression more effective and adaptive to user instructions, we also feed the instruction text Fs into the Q-Former:"
        },
        {
            "title": "F i",
            "content": "Q = QFormer(Q, [Fxi Fai ], Fs). (3) The Q-Formers query output serves as the compressed representation for each frame. These are then concatenated to form the temporal dynamic context FTDC of the video clip, aggregating both visual and audio information: FTDC = [Fx1 Fa1 2 3 Q ]. (4) This strategy allows the model to selectively allocate attention to specific modalities when answering given question. Additionally, to differentiate static tokens from dynamic context tokens, we introduce learnable token, <Sep>, as the separator. Our model tightly integrates visual, audio, and language modalities, providing promising solution for multimodal video modeling. 3.3. Multimodal Training Strategy Multi-stage training has been commonly used and demonstrated effective in previous work [10, 37, 69]. We train the model in three stages to progressively enhance its understanding of different modalities. In the first stage, we pretrain our model for vision-language alignment using the instruction tuning dataset, LLaVA-OneVision [30]. In the second stage, we train the model on vision-focused videolanguage dataset without audio. Specifically, we construct the training dataset using LLaVA-Video [85], VideoChat2IT [34] and MovieChat [52]. In the third stage, we train our model on an audio-visual video understanding dataset to enable the model to comprehend multiple modalities jointly. The data for this stage is collected from Music-AVQA [31], AVQA [70], AVSD [3], LongVALE [17] and AVInstruct [73]. We also sample subset of the data used in stage 2 to retain the capabilities learned in previous stages. 3.4. Long Video Chain of Thought tion details can be found in the Appendix C. While there have been some advancements in video modeling, processing extremely long videos as whole is still challenging. Just like we cannot summarize an entire movie before watching it, MLLMs also need to process long videos progressively. To address this, we propose method that allows MLLMs to watch and reason through long videos step by step. Previous approaches [5, 48, 51] often rely on key frame selection to answer questions, which disrupts the temporal continuity of the video and makes it more difficult for MLLMs to comprehend the content. Other methods [26, 60] employ hierarchical strategy, segmenting the video into smaller clips, generating captions for each clip, and then summarizing them to produce final video description. However, these approaches are typically designed for specific tasks, such as video captioning, and struggle to generalize across different applications. VideoCoT [58] introduces active annotation to generate CoT data for training reasoning abilities on videos, but it is limited to short videos. Developing versatile strategy that can adapt to diverse scenarios remains significant challenge. To this end, we propose Long Video Chain-of-Thought (LVCoT), training-free method that can be applied to various MLLMs for extremely long video understanding. We divide the video into multiple time-equivalent segments and query the model to summarize relevant information for answering the given question separately. During this process, the model identifies useful information within each segment and integrates it to generate the final answer. Once the model has processed the entire video, we concatenate all segment outputs along with their corresponding time intervals, representing the models thought process. We then query the model to generate the final response based on the global video content. This approach effectively combines segment-level information with the overall context, enabling deeper reasoning. 4. Experiment 4.1. Experimental Setup We mainly conduct experiments with two backbone LLMs: Qwen2-7B [49] and LLaMA3.2-3B [41]. We sample 1 frame per second for each video. Following previous work [51, 54], we use DINOv2 [46] and SigLIP [75] as visual encoders, and obtain 144 aggregated tokens per frame. For audio encoding, following the implementation in BEATs [8], we resample the raw audio waveform to 16,000 Hz, and extract audio tokens using the pretrained BEATs encoder, resulting in about 50 tokens per second. We set the maximum number of scene segments to 24, and the number of query tokens to 16 by default. We use the pretrained BERT [13] to initialize the Q-Former. More implementa4.2. General Video Understanding First, we evaluate models on vision-focused video understanding benchmarks, including MVBench [34], PerceptionTest [47], EgoSchema [43], MLVU [86] and VideoMME [16]. The results are present in Table 1. From the results, in short video understanding benchmarks such as MVBench and PerceptionTest, most MLLMs perform well and achieve high accuracy. Longer videos pose greater challenges, leading to performance drop in almost all models. Compared to existing audio-visual MLLMs, our model is the first to model dense frames and audios in unified framework and consistently achieves the best results in video understanding. Notably, models that rely on sparsely sampled frames experience significant decline in performance on long video benchmarks such as MLVU and VideoMME. In these cases, our model outperforms VideoLLaMA2 by 15.6% and 9.9%, respectively. Compared to vision-focused MLLMs, our model also shows competitive performance while being additionally capable of understanding audio within video inputs. Smaller Model. In addition, we train smaller model based on Llama3.2-3B [41]. For the sake of data and computational efficiency, we sample subset of the original data for training, with details provided in Appendix C. The results are presented in Table 2. At the parameter scale of 3B-4B, our model achieves the best performance in both short and long video understanding. Notably, with similar amount of training data, our TDC model outperforms LongVU on MLVU by 7.4%, further demonstrating its effectiveness. 4.3. Audio-Visual Omni Video Understanding We evaluate models on audio-visual joint video understanding benchmarks, including Music-AVQA [31], audio-visual scene-aware dialog (AVSD) [3]. Music-AVQA contains 9129 samples for evaluating models with visual and audio understanding of musical performance. And AVSD includes 18630 samples of open-ended questions about visual and audio scenes in daily dialogue scenarios. The results are provided in Table 3. Our model achieves the best result on AVSD and shows compatible performance with VideoLLaMA2 on Music-AVQA. 4.4. Ablation Study Effects of Segmentation. To evaluate the impact of consistency-based segmentation on video understanding, we vary the maximum number of segments, train the model, and assess its performance. The results are shown in (a) of Table 4. When the maximum is set to one, it means the entire video is processed as whole, the performance drops remarkably. This is because it incorrectly establishes relationships between non-contiguous video frames, mak5 Model Average duration (sec) Commercial Models GPT4-V [44] GPT4-o [45] Gemini-1.5-Pro [20] Vision-focused MLLMs InternVL2 [9] LLaVA-NeXT-Video [84] LLaVA-OneVision [30] LLaVA-OneVision [30] mPLUG-Owl3 [72] Qwen2-VL [55] VideoChat2-HD [34] InternVideo2-HD [57] VideoChat-TPO [66] InternVL2.5 [59] LLaMA-VID [37] LongVILA [65] LongVA [80] LongLLaVA [56] LLaVA-Video [51] LongVU [51] PVCInternVL2 [69] MAmmoTH-VL [21] Audio-visual MLLMs PandaGPT [53] NExT-GPT [62] VideoLLaMA2 [10] VideoLLaMA2 [10] VideoLLaMA2.1[10] TDC (Ours) Size #Frames #Tokens MVBench PerceptionTest per Frame 16 - - - 8B 7B 7B 72B 7B 7B 7B 7B 7B 7B 7B 7B 7B 9B 7B 7B 8B 8B 7B 7B 7B 72B 7B 7B 1fps 1fps 1fps 12 32 32 32 8 2fps 16 16 16 12 1fps 2048 128 128 - 1fps 96 5 10 24 16 16 16 1fps - - - 256 144 196 196 - - 72 72 64 256 2 196 144 144 169 144/64 64 729 196 196 72 72 72 16 43.7 64.6 60.5 66.4 53.1 56.7 59.4 54.5 67.0 62.3 67.2 66.8 72.0 41.9 - - 49.1 58.6 66.9 73.8 59.1 - - 54.6 62.0 57.3 68.3 - - - - 48.8 57.1 66.9 - 62.3 - 63.4 - 68.2 44.6 - - - 67.9 - 68.4 59.3 - - 51.4 57.5 54.9 67.5 EgoSchema MLVU VideoMME 651 1010 - 72.2 71.2 - - 60.1 - - 66.7 - 60.0 - 51.5 - 67.7 - - 57.3 67.6 59.6 58.5 - - 51.7 63.9 53.1 65.7 49.2 64.6 - - - 64.7 68.0 - - 47.9 - 54.7 68.9 33.2 - 56.3 - 70.8 65.4 72.4 64.7 - - 48.5 - - 64. 59.9 71.9 75.0 54.0 46.5 58.2 66.2 53.5 63.3 45.3 49.4 - 64.2 25.9 57.5 52.6 43.7 63.3 - 64.1 58.8 43.5 42.6 47.9 61.4 54.9 57.8 Table 1. Results on Video Question Answering Benchmarks, including short video and long video understanding. We compare our model with Vision-focused MLLMs and Audio-visual Omni MLLMs. We present the performance of our model with the proposed LVCoT. The best results among Audio-visual MLLMs are bold. Results on VideoMME are evaluated without subtitles. ing it difficult for the context tokens to capture the complete video information. This effect is particularly evident in short videos with rapid scene changes. On the other hand, increasing the number of segments to 48 does not result in additional improvements, indicating that our choice of 24 is sufficient to divide the video into appropriate scenes. Avg Pooling vs. Learned Queries. Learnable queries are commonly used in querying transformers to extract information from different modalities. We compare our model with variant trained using learnable query tokens, and the results are shown in (b) of Table 4. While learnable queries achieve comparable performance in context compression, they introduce additional computational overhead. In contrast, tokens obtained through average pooling effectively represent the static reference frame, and help extract dynamic changes in subsequent frames. This approach also has the advantage of adaptively adjusting the number of context tokens for dynamic compression. Number of Context Tokens. We conduct experiments with varying numbers of context tokens. The results, presented in (c) of Table 4, indicate that increasing context tokens does not necessarily improve performance. Although more context tokens can capture additional video information, they also increase the number of tokens per frame, thus restrict the number of frames processed and increasing the computational overhead for MLLM, This highlights trade6 Model Average duration (sec) Vision-focused MLLMs InternVL2 [9] VideoChat2 [34] Phi-3.5-vision-instruct [1] TinyLLaVA-Video [83] LongVU [51] Audio-visual MLLMs TDC (Ours)"
        },
        {
            "title": "LLM",
            "content": "Size #Frames #Tokens MVBench EgoSchema MLVU VideoMME 180 per Frame 1010 473 InternLM2 [79] 1.8B 4B Phi-3-mini [1] 4B Phi-3-mini [1] 3B Qwen2.5 [68] 3B Llama3.2 [41] 16 16 16 16 1fps 256 96 256 - 144/64 Llama3.2 [41] 3B 1fps 16 60.2 55.1 - 42.5 60.9 62.7 - 56.7 50.8 - 59.1 47.3 - - 48.1 51.5 - - - - 55. 61.0 58.9 59.5 Table 2. Results of Smaller Sized Models. We present the performance of our model with the proposed LVCoT. Results on VideoMME are evaluated with subtitles. The best results are bold. Model Size #Frames #Tokens AVSD Music-AVQA Dataset MVBench MLVU VideoMME Long Overall PandaGPT [53] 13B NExT-GPT [62] VideoLLaMA2 [10] VideoLLaMA2.1 [10] LongVALE [17] TDC (Ours) 7B 7B 7B 7B 7B 10 16 16 100 1fps 196 72 72 256 16 26.1 - 57.2 57.2 54.8 57.6 33.7 79. 79.2 80.9 49.4 78.7 Table 3. Results on Audio-Visual Omni Video Understanding, including AVSD [3] and Music-AVQA [31]. off between retaining more information within each frame and encoding greater number of frames. Text Instruction in Context Compression. We evaluate the contribution of text instructions in video context compression, the results are shown in (d) of Table 4. From the results, we can see that the text instructions help to improve models performance on various dataset. This is because text instructions offer valuable guidance to the compressor in identifying essential information to answer the question, thereby enhancing the efficiency of context compression. Effects of LVCoT. When processing the entire video as whole, understanding and summarizing useful information in video can be challenging. As shown in Table 4 (e), applying LVCoT to both 3B and 7B models improves performance on different video benchmarks. Notably, the improvements become more significant as video length increases, demonstrating LVCoTs effectiveness in long video understanding. 4.5. Qualitative Demonstrations In Figure 3, we present several examples demonstrating our models general video understanding capabilities. Specifically, Figure 3 (a) shows how our model uniformly comprehends both audio and visual information, which enhances its ability as personal assistant. Figure 3 (b) showcases its performance in movie understanding, where it generates (a) Maximum Number of Segments 1 (No Segment) 53.5 (-9.2) 24 48 62.7 62.2 (-0.5) 56.6(-1.7) 58.3 58. 58.7 (-0.9) 53.2 (+0.5) 59.6 58.5(-1.1) 52.7 51.0 (-1.7) (b) Query Type Learned Query AvgPooling 61.7(-1.0) 62.7 58.2(-0.1) 58. 59.5(-0.1) 59.6 52.1(-0.6) 52.7 (c) #Context Tokens per Frame 32 16 61.7(-1.0) 62.7 56.1(-2.2) 58. 58.4 (-1.2) 52.1 (-0.6) 59.6 52.7 (d) Text Instruction Without Text. Text Input. 62.3(-0.4) 62.7 58.1(-0.2) 58. 58.0(-1.6) 59.6 51.5(-1.2) 52.7 (e) Effect of LVCoT 3B 3B w/ LVCoT 7B 7B w/ LVCoT 62.7 62.7 68.3 68.3 59. 58.3 58.9(+0.6) 59.5(-0.1) 63.9 64.1(+0.2) 66.2(+0.3) 61.8 (+0.5) 52.7 52.7 61.3 65.9 Table 4. Results of Ablation Studies. We conduct ablation studies on: (a) the maximum number of scene segments in video encoding, (b) the type of query used for temporal context compression, (c) the number of context tokens for each frame, (d) the effect of text information in context compression, (e) the effect of LvCoT with 3B and 7B models. The row with gray background indicates our default setting. detailed descriptions of both the plot and visual elements. For extremely long videos, such as movies, our LVCoT processes them segment by segment, further improving the quality of the descriptions. Figure 3. Qualitative Demonstrations of Our 7B Model. (a) Our model can uniformly comprehend both audio and visual information, demonstrating strong performance in audio-visual dialogue tasks. (b) In movie description tasks, it can generate detailed descriptions of both the plot and visual elements. For extremely long videos, our LVCoT processes them segment by segment. The generated segment information, along with the timeline, serves as part of the reasoning process, enriching the final output with more details. 5. Conclusion In this paper, we introduce novel multimodal long-video modeling framework named Temporal Dynamic Context (TDC). This framework represents video using both static visual features and dynamic video context within each scene, which provide visual details and dynamic motions of the video, respectively. Our model integrates multiple modalities into unified video context, enhancing multimodal joint long-video understanding. For extremely long video, we introduce the Long Video Chain-of-Thought (LVCoT) strategy, which guides the model to process long videos step by step before integrating the full video to generate the final output. This approach improves model performance and allows models with limited context windows to effectively handle longer videos. Extensive experiment demonstrate that our model achieves strong performance across general video understanding tasks and audio-visual omni video understanding benchmarks. 8 A. Appendix Overview Section B: Additional evaluations of our models. Section C: More experimental details. Section D: Analysis on limitations of our work. B. Additional evaluations Model Size Frames L Overall Video-LLaVA [38] ShareGPT4Video [7] Chat-Univi-v1.5 [27] VideoLLaMA2 [10] VideoChat2 [34] LongVA [80] LLaVA-OneVision [30] LongVU [51] TDC (Ours) 7B 8B 7B 7B 7B 7B 7B 7B 7B 8 16 64 16 16 128 32 1fps 1fps 46.1 40.7 38.1 53.6 39.3 37.9 51.2 44.6 41.8 59.4 47.6 43.8 52.8 39.4 39.2 61.6 50.4 47.6 69.1 53.3 46.7 64.7 58.2 59.5 70.0 66.2 61.3 41.6 43.6 45.9 50.3 43.8 54.3 58.2 60.9 65.9 Table 5. Detailed Results on VideoMME. The best results are bold. Subtitles of videos are provided in this evaluation. S: Short. M: Medium. L: Long. In Table 5, we provide more detailed comparison on the VideoMME [16] dataset. In this evaluation, subtitles for each video are provided to the model. The results show that our model consistently achieves the best performance across both short and long video settings, which demonstrates its adaptability to wide range of video scenarios. C. Experimental details C.1. Training data Our training process contains three stage. In the first stage, we pretrain our model on vision-language alignment using the single image instruction tuning dataset, LLaVAOneVision [30]. In the second stage, we train our model on vision-focused video-language dataset without audio. Specifically, we construct the training dataset using LLaVAVideo [85], VideoChat2-IT [34] and MovieChat [52]. In the third stage, we train our model on an audio-visual video understanding dataset to enable the model to comprehend multiple modalities jointly. Our training data is collected from Music-AVQA [31], AVQA [70], AVSD [3], LongVALE [17] and AVInstruct [73]. We also sample subset from the data used in stage 2 to retain the capabilities learned in previous stages. The detailed data sources are listed in Table 7. C.2. Implementation details We mainly conduct experiments with two backbone LLMs: Qwen2-7B [49] and LLaMA3.2-3B [41]. We sample 1 frame per second for each video. Following previous work [51, 54], we use DINOv2 [46] and SigLIP [75] as visual encoders, and obtain 144 aggregated tokens per frame. For audio encoding, following the implementation in BEATs [8], we resample the raw audio waveform to 16,000 Hz, and extract audio tokens using the pretrained BEATs encoder, resulting in about 50 tokens per second. We set the maximum number of scene segments to 24, and the number of query tokens to 16 by default. We use the pretrained BERT [13] to initialize the Q-Former. The models are trained for one epoch in each stage. During training, the visual and audio encoders are kept frozen, while the temporal compressor and the MLLMs are trained. In the first two stages, we train the full model parameters. In the third stage, we apply Low-Rank Adaptation (LoRA) [25] to reduce GPU memory consumption. The detailed hyperparameter settings used during model training are presented in Table 5. Training Stage Stage 1 Stage 2 Stage 3 Max Sequence Length Number of Video Frames Number of Segmented Scenes Visual Tokens per Frame Audio Tokens per Frame Context Tokens per Frame Optimizer Learning Rate Learning Rate Schedule Warmup Ratio Training Mode 8192 1 fps 24 144 50 16 AdamW [42] 1e-5 Cosine Decay 0.03 Full 1e-5 2e-5 Full LoRA Table 5. Hyperparameters Used in Model Training. C.3. Evaluation setup Following the approach in [10], we adopt an LLM assisted evaluation for AVSD. We also provide an example as one shot. For LVCoT, we set the number of segments to 3 by default. D. Limitations The effectiveness of LVCoT depends on the reasoning ability of the MLLM, since the model has not been trained on this task, the improvement is relatively small. In the future, we will explore training the model to better utilize this strategy. Additionally, processing videos multiple times incurs additional computational costs. It would be promising to explore new methods for establishing more efficient memory mechanisms in MLLMs to enhance long video understanding."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, 9 Training stage # Samples Data Sources Stage1: Vision-Language Pre-training 3.2M LLaVA-OneVision [30] Stage2: Video Instruction Tuning Qwen2-7B: 2M Kinetics-710 [28], NExTQA [64], CLEVRER [74], TGIF [36], LLama3.2-3B: 540K WebVidQA [67], DiDeMo [4], ShareGPT4Video [7], LLaVA-Video [85], TextVR [63], YouCook2 [87], EgoQA [14], MovieChat [52] Stage3: Audio-Video Instruction Tuning Qwen2-7B: 300K AVQA [70], Music-AVQA [31], AVSD [3], LongVALE [17], LLama3.2-3B: 120K AVinstruct [73], subset from Stage 2 Table 7. Datasets used in multi-stage multimodal training. Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 7 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [3] Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop Cherian, Irfan Essa, Dhruv Batra, Tim Marks, Chiori Hori, Peter Anderson, et al. Audio visual sceneaware dialog. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7558 7567, 2019. 2, 4, 5, 7, 9, 10 [4] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE international conference on computer vision, pages 58035812, 2017. [5] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Mingchen Zhuge, Jian Ding, Deyao Zhu, Jurgen Schmidhuber, and Mohamed Elhoseiny. Goldfish: Vision-language understanding of arbitrarily long videos, 2024. 5 [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. 1 [7] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understandarXiv preprint ing and generation with better captions. arXiv:2406.04325, 2024. 9, 10 [8] Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, Wanxiang Che, Xiangzhan Yu, and Furu Wei. BEATs: Audio pre-training with acoustic tokenizers. In Proceedings of the 40th International Conference on Machine Learning, pages 51785193. PMLR, 2023. 3, 5, 9 [9] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. CoRR, abs/2404.16821, 2024. 6, 7 [10] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 1, 2, 3, 4, 6, 7, 9 [11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 3 [12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Instructblip: Towards generalFung, and Steven Hoi. purpose vision-language models with instruction tuning, 2023. 2, [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transIn Proceedings of formers for language understanding. the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 4171 4186, 2019. 5, 9 [14] Chenyou Fan. Egovqa-an egocentric video question answerIn Proceedings of the IEEE/CVF ing benchmark dataset. International Conference on Computer Vision Workshops, pages 00, 2019. 10 [15] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: unified pixel-level vision llm for understanding, generating, segmenting, editing. 2024. 2 [16] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 5, 9 [17] Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, and Feng Zheng. Longvale: Vision-audiolanguage-event benchmark towards time-aware omni-modal 10 perception of long videos. arXiv preprint arXiv:2411.19772, 2024. 1, 2, 3, 4, 7, 9, Chunyuan Li. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326, 2024. 1, 4, 6, 9, 10 [18] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023. 3 [19] Gemini Team Google. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2 [20] Gemini Team Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 2, 6 [21] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. 2024. 6 [22] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. Imagebind-llm: Multi-modality instruction tuning. arXiv preprint arXiv:2309.03905, 2023. 1, [23] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26584 26595, 2024. 1, 2 [24] Haoran Hao, Jiaming Han, Changsheng Li, Yu-Feng Li, and Xiangyu Yue. Remember, retrieve and generate: Understanding infinite visual concepts as your personalized assistant. arXiv preprint arXiv:2410.13360, 2024. 1 [25] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. 9 [26] Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, and Gedas Bertasius. Video recap: Recursive captioning of hour-long videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1819818208, 2024. 5 [27] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. arXiv preprint arXiv:2311.08046, 2023. 9 [28] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. [29] Rajat Koner, Gagan Jain, Prateek Jain, Volker Tresp, and Sujoy Paul. Lookupvit: Compressing visual information to limited number of tokens. In European Conference on Computer Vision, pages 322337. Springer, 2024. 2 [30] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and [31] Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, JiLearning to answer questions Rong Wen, and Di Hu. in dynamic audio-visual scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1910819118, 2022. 2, 4, 5, 7, 9, 10 [32] Jungang Li, Sicheng Tao, Yibo Yan, Xiaojie Gu, Haodong Xu, Xu Zheng, Yuanhuiyi Lyu, Linfeng Zhang, and Xuming Hu. Saven-vid: Synergistic audio-visual integration for enhanced understanding in long video context. arXiv preprint arXiv:2411.16213, 2024. 3 [33] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 1, 2 [34] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 2, 4, 5, 6, 7, 9 [35] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. 1, [36] Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, and Jiebo Luo. Tgif: new dataset and benchmark on animated gif description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 46414650, 2016. 10 [37] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2024. 1, 2, 3, 4, 6 [38] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 2, 3, 9 [39] Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Ghosh, Luke Zettlemoyer, and Armen Aghajanyan. Moma: Efficient early-fusion pre-training with mixture of modality-aware experts. arXiv preprint arXiv:2407.21770, 2024. 2 [40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 1, 2 [41] Meta Llama. Llama 3.2, 2024. 1, 5, 7, 9 [42] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [43] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 2, 5 [44] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. 2, 6 11 [45] OpenAI. Gpt-4o system card, 2024. 2, 6 [46] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 4, 5, 9 [47] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36:4274842761, 2023. 2, [48] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long video understanding with large language models. Advances in Neural Information Processing Systems, 37:119336 119360, 2025. 5 [49] Team Qwen. Qwen2 technical report, 2024. 1, 5, 9 [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pages 8748 8763. PMLR, 2021. 2, 3 [51] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 1, 3, 4, 5, 6, 7, 9 [52] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 1, 4, 9, 10 [53] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023. 3, 6, 7 [54] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. 1, 5, [55] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6 [56] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. CoRR, abs/2409.02889, 2024. 6 [57] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for mulIn European Conference on timodal video understanding. Computer Vision, pages 396416. Springer, 2024. 2, 6 [58] Yan Wang, Yawen Zeng, Jingsheng Zheng, Xiaofen Xing, Jin Xu, and Xiangmin Xu. VideoCoT: video chain-ofthought dataset with active annotation tool. In Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR), pages 92101, 2024. 5 [59] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Internvideo2. 5: Empowering video Jianfei Gao, et al. mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. 1, 3, 6 [60] Hongchen Wei, Zhihong Tan, Yaosi Hu, Changwen Chen, and Zhenzhong Chen. Longcaptioning: Unlocking the power of long caption generation in large multimodal models. arXiv preprint arXiv:2502.15393, 2025. [61] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453470. Springer, 2024. 1 [62] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and TatSeng Chua. NExT-GPT: Any-to-any multimodal LLM. In Proceedings of the International Conference on Machine Learning, pages 5336653397, 2024. 2, 3, 6, 7 [63] Weijia Wu, Yuzhong Zhao, Zhuang Li, Jiahong Li, Hong Zhou, Mike Zheng Shou, and Xiang Bai. large crossmodal video retrieval dataset with reading comprehension. Pattern Recognition, 157:110818, 2025. 10 [64] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explainIn Proceedings of the IEEE/CVF ing temporal actions. conference on computer vision and pattern recognition, pages 97779786, 2021. 10 [65] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. 6 [66] Ziang Yan, Zhilin Li, Yinan He, Chenting Wang, Kunchang Li, Xinhao Li, Xiangyu Zeng, Zilei Wang, Yali Wang, Yu Qiao, Limin Wang, and Yi Wang. Task preference optimization: Improving multimodal large language models with vision task alignment. arXiv preprint arXiv:2412.19326, 2024. [67] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF international conference on computer vision, pages 16861697, 2021. 10 [68] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei 12 [79] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 2, 7 [80] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. CoRR, abs/2406.16852, 2024. 6, 9 [81] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. 1, 2 [82] Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. Llava-mini: Efficient image and video large multimodal models with one vision token, 2025. [83] Xingjian Zhang, Xi Weng, Yihao Yue, Zhaoxin Fan, Wenjun Wu, and Lei Huang. Tinyllava-video: simple framework of small-scale large multimodal models for video understanding. arXiv preprint arXiv:2501.15513, 2025. 3, 7 [84] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 6 [85] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. 4, 9, 10 [86] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 2, 5 [87] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. 10 Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [69] Chenyu Yang, Xuan Dong, Xizhou Zhu, Weijie Su, Jiahao Wang, Hao Tian, Zhe Chen, Wenhai Wang, Lewei Lu, , and Jifeng Dai. Pvc: Progressive visual token compression for unified image and video processing in large vision-language models. arXiv preprint arXiv:2412.09613, 2024. 3, 4, 6 [70] Pinci Yang, Xin Wang, Xuguang Duan, Hong Chen, Runze Hou, Cong Jin, and Wenwu Zhu. Avqa: dataset for audiovisual question answering on videos. In Proceedings of the 30th ACM international conference on multimedia, pages 34803491, 2022. 4, 9, 10 [71] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. arXiv preprint arXiv:2412.04467, 2024. 2 [72] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. 6 [73] Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and Xiaochun Cao. Cat: Enhancing multimodal large language model to answer questions in dynamic audio-visual scenarIn European Conference on Computer Vision, pages ios. 146164. Springer, 2024. 4, 9, 10 [74] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019. 10 [75] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 5, 9 [76] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv preprint arXiv:2402.12226, 2024. [77] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 2 [78] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, Qipeng Guo, Haodong Duan, Xin Chen, Han Lv, Zheng Nie, Min Zhang, Bin Wang, Wenwei Zhang, Xinyue Zhang, Jiaye Ge, Wei Li, Jingwen Li, Zhongying Tu, Conghui He, Xingcheng Zhang, Kai Chen, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2.5-omnilive: comprehensive multimodal system for long-term streaming video arXiv preprint arXiv:2412.09596, and audio interactions. 2024."
        }
    ],
    "affiliations": [
        "MMLab, The Chinese University of Hong Kong",
        "Nanjing University"
    ]
}