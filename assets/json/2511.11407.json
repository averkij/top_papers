{
    "paper_title": "MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model",
    "authors": [
        "Manyu Li",
        "Ruian He",
        "Chenxi Ma",
        "Weimin Tan",
        "Bo Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 7 0 4 1 1 . 1 1 5 2 : r MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model Manyu Li* Fudan University 24210240029@m.fudan.edu.cn Ruian He* Fudan University rahe16@fudan.edu.cn Chenxi Ma Fudan University cxma17@fudan.edu.cn Weimin Tan Fudan University wmtan@fudan.edu.cn Bo Yan Fudan University byan@fudan.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of largescale, high-quality training data. We introduce MicroVQA++, three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA Stage one bootstraps supervision from expertarchive. validated figurecaption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based visionlanguage alignment, and agent signals to identify and filter inconsistent samples. Stage three use MultiModal Large Language Model (MLLM) agent to generate multichoice question (MCQ) followed by human screening. The resulting release comprises large training split and human-checked test split whose Blooms level hard sample distribution exceeds MicroVQA benchmark. Our work delivers (i) quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes. 1. Introduction With the explosive growth of data in the modern era, In particumany AI applications have rapidly advanced. *These authors contributed equally Corresponding Authors. Figure 1. Blooms levels across microscopy multimodal datasets. MicroVQA++ exhibits substantially higher proportion in harder level and absolute count of higher-difficulty questions than MicroVQA, reflecting stricter and more demanding evaluation setting. lar, MLLMs have shown impressive progress in domains such as industrial anomaly detection [24], medical imaging [33], autonomous driving [55], and remote sensing [54]. However, in the microscopy domain, most existing work on MLLMs has focused on exploring how to interface these models with microscope hardware [12, 32], rather than improving their scientific reasoning ability on microscopy data itself. key bottleneck is the lack of high-quality multimodal microscopy datasets, which makes it difficult to finetune MLLMs effectively. Recently, MicroVQA [11] was introduced as the benchmark dataset targeting microscopy question answering with MLLMs, signaling an initial step toward systematic evaluation of MLLMs on microscopy tasks. Although MicroVQA is challenge enough, it contains only 1,042 samples for evaluation; it does not address the data scarcity problem that limits further progress. As MLLMs continue to scale and improve, such small benchmark is suitable only for evaluating their reasoning performance and is unlikely to further enhance it. There is still no large-scale, high-quality multimodal microscopy dataset suitable for supervised finetuning of MLLMs, which severely restricts advances in microscopy-focused multimodal reasoning. To construct such dataset, we leverage strong MLLM agent to generate supervision. The recent release of the BIOMEDICA [30] dataset offers promising path forward. It is large automatically curated biomedical visionlanguage dataset constructed from PubMed Central Open Access articles, containing approximately 24M imImportantly, around 10.4% of these agecaption pairs. samples are microscopy-related, providing strong evidence that large-scale, domain-relevant microscopy data is available. Specifically, given an image and its caption, we first extract key factual answers from the caption, and then generate questions that are grounded in the corresponding image and aligned with realistic microscopy analysis workflows. The question types follow the scientific workflow design introduced in MicroVQA, which characterizes three core experimental reasoning capabilities in microscopy. These capabilities require both abductive reasoning (inferring the most plausible explanation among multiple hypotheses) and deductive reasoning (applying general biological or experimental principles to specific observation): (i) Expert Visual Understanding (EU): describing and interpreting visual phenomena in the image; (ii) Hypothesis Generation (HG): proposing mechanistic hypotheses that could explain the observed experimental data; (iii) Experiment Proposal (EP): suggesting follow-up experiments to validate or falsify the proposed hypotheses. After constructing the initial QA database, we further prompt the MLLM agent to convert QAs into MCQs, and ask it to explicitly provide chain-of-thought (CoT) [46] style rationales for MCQ construction. However, automatically generated supervision from an MLLM agent is not perfect. The agent can still introduce errors, shortcuts, or hallucinations. To systematically improve data quality, we introduce Heterogeneous Image-CaptionQA Graph (HiCQA-Graph) that explicitly models the relationships between images, their captions, and the generated QAs. Intuitively, our goal is to capture cross-modal consistency so that we can identify and filter out low-quality or hallucinated samples. To the best of our knowledge, MiTable 1. Compared to MicroVQA, Ours contains 19.2 and 5.8 more questions in the training and test splits, respectively, and 33.7 and 20.4 more images in the corresponding splits, demonstrating substantially larger scale and richer visual coverage. Dataset feature Total Qs Unique images CoTs Avg. MCQ len Avg. MCQ len Image Modalities Image Scales MicroVQA Ourstrain 1042 255 0 66 15 2000019.2 859433.7 20000 162 79 Light, Fluoro, Electron Tissue, Cell, Subcell, Atomic Ourstest 60005.8 519820.4 6000 123 55 croVQA++ is the first large-scale multimodal VQA training dataset specifically tailored to microscopy. After supervised fine-tuning (SFT) on MicroVQA++ training data, an MLLM with only 4B parameters matches the reasoning performance of the strongest proprietary commercial model GPT-5 on the MicroVQA benchmark, and establishes new state-of-the-art results among open-source MLLMs. Our main contributions are as follows: novel pipeline for QA and MCQ construction. Starting from high-quality imagecaption pairs extracted from the literature, we use powerful MLLM agent to generate grounded QA pairs, and then derive multiple-choice questions with explicit rationales. HiCQA-Graph for cross-modal consistency filtering. We propose Heterogeneous Image-Caption-QA Graph that jointly models images, captions, and QAs. By leveraging Natural Language Inference (NLI) based entailment [8] between caption and QA, CLIP-based visionlanguage alignment between image and QA, and agent-derived supervision signals, our graph learns to thus improving identify and filter unreliable samples, dataset quality. MicroVQA++ and strong microscopy reasoning at 4B scale. With finetuning on the MicroVQA++ training set, an MLLM with only 4B parameters already on par with GPT-5 on the MicroVQA benchmark and achieves new state-of-the-art performance among open-source MLLMs in microscopy reasoning. 2. Related work 2.1. MLLM benchmarks in microscopy Benchmarks for evaluating MLLM reasoning in microscopy remain limited. Figure 1 shows proportional distribution of Blooms level [18], used to measure question difficulty in the dataset. Higher Blooms levels are more cognitively challenging. µBench [29] assembles broad microscopy understanding suite with 22 tasks and 17,235 images, but its scientific depth is shallow. MicroVQA abstracts microscopy research workflows into three capaciFigure 2. MicroVQA++ is built in three stages. We first sample figurecaption pairs from the Microscopy category of BIOMEDICA. An MLLM agent then extracts answer spans from the captions to construct initial QA pairs, leveraging peer-reviewed articles to ensure expert-validated supervision. Next, we pass the data through HiCQA-Graph, which evaluates cross-modal consistency to judge generation quality. Finally, conditioned on the validated items (human check pipeline available in appendix), an MLLM agent produces CoT rationales and MCQ variants for each question. red indicates grounding informations and green indicates important entities. ties and emphasizes deeper reasoning, yet contains only 255 images and 1,042 QA pairs. Building on high-quality figurecaption pairs from scientific articles, HiCQA-Graph filtering, and MLLM-agent generation, we construct MicroVQA++ with 20K-question train set and 6K-question test set, far larger than MicroVQA and with substantially higher proportion of depth-oriented items. Tabel 1 shows dataset features among MicroVQA and MicroVQA++. 2.2. QA and MCQ construction Generating multiple-choice questions with strong distractors is critical for human education and machine learning evaluation [2, 7, 17]. Existing pipelines often rely on prompt engineering and expert-heavy designs: µBench converts collected imageQA pairs to MCQs using GPT-4o before expert verification, while MicroVQA adopts twostage expertagent strategy. These approaches, however, require complex manual heuristics and underuse the inherent ties among images, captions, and QAs. Our proposed HiCQA-Graph explicitly models Image, Caption and QA node types and learns their relationships under weak supervision, enabling principled filtering that yields new, higher-quality QA/MCQ data with CoT. 2.3. Graph-based data generation and filtering Graph methods are widely used to construct or denoise training sets from weak supervision or noisy labels [1, 9, 25]. common pipeline fuses multi-source weak signals into label graph, followed by graph-based denoising and consolidation [37]. Label propagation [56] provides classic framework for diffusing sparse, reliable seeds over similarity graph. In visual weak supervision and noisylabel learning, DualGraph [51] jointly reasons over sample graph and label graph to infer noise, while NGC [48] exploits inter-sample geometry to select clean instances, both embodying the principle that structural consistency implies sample confidence. In this work, we treat graph as an intermediate for data filtering and amplify multi-source consistency signals (e.g., imagetext alignment, textual entailment) via message passing with GraphSAGE [19] and GAT [43]. 3. Method 3.1. Motivation persistent bottleneck in microscopy is the lack of highquality multimodal datasets, which substantially limits the fields potential. The emergence of BIOMEDICA offers promising path forward. This is valuable for two rea- (i) Because the images and captions are extracted sons: from scientific publications, the captions are already written, checked, and curated by human experts, leading to comparatively high semantic quality [4, 26, 27, 38, 52]. (ii) The microscopy subset alone accounts for roughly 2.5M imagecaption pairs, which forms solid foundation for building large-scale multimodal microscopy dataset. 3.2. HiCQA-Graph 3.2.1. Notations formulation In this paper, we consider HiCQA-Graph = (V, E, I, ) with = nodes, = edges, + = sets of image and text. Each node vi is associated with an embedded image or embedded text . 3.2.2. Node definition Figure 3 illustrates the overall architecture of HiCQAGraph, comprising three node types: Image, Caption, and QA. Image node. We encode each image with CLIP ViTL/14 [15, 36]. Before encoding, all images are resized to 224 224. Let the CLIP visual embedding be vi Rf , caption text embedding be ti Rf (here, =768). We further append one-dimensional consistency score between the image and its caption: (cid:40) cimg-cap cimg-cap = cos(cid:0)vi, ti = cimg-cap (cid:1) [1, 1] [0, 1]. +1 2 (1) The final image node feature is the concatenation ximg = [ vi; cimg-cap ] Rf +1. Each image node has single outgoi ing edge to its unique caption node and outgoing edges to all associated QA nodes. Caption node. We encode the caption with the CLIP text Transformer. Due to CLIPs design, the maximum effective text length is 75 tokens (excluding BOS/EOS). To avoid truncation, overlong captions are first summarized to suitable length by an MLLM-based agent and then fed to CLIP, yielding xcap Rf . Each caption node connects to all QA nodes of the same sample with edge attributes given by NLI: for caption answer, we take the entailment probability: Figure 3. a) and b) shows CLIP and NLI only filtering method. c) indicates HiCQA-Graph structure. Cross-modal consistency token is added to Image and QA nodes. Two heads are used to predict soft weak supervised labels. QA node. For each QA pair, we concatenate question and answer texts, apply the same text preprocessing and CLIP encoding to obtain qi,k Rf . We also compute an image QA consistency score (cid:40)cimg-qa i,k cimg-qa i,k = cos(cid:0)vi, qi,k cimg-qa i,k +1 = 2 [0, 1] (cid:1) [1, 1] (3) i,k = [ qi,k; cimg-qa and form the QA node feature xqa ] Rf +1. Within each sample, QA nodes are fully connected with directed edges in both directions excluding self-loops; the edge attribute on (QAk QAℓ) is the nonnegative cosine similarity i,k i,kℓ = max(cid:8)0, cos(qi,k, qi,ℓ)(cid:9). aqa-qa (4) Weak labels for QA nodes. We define two supervision signals on QA nodes: (i) Keep score (weak supervised). We comthe bine CaptionAnswer NLI probability into scalar confidence and obtain soft Keep label. simple and effective fusion is the weighted sum imageQA CLIP similarity with entailment the pent i,k [0, 1] (2) i,k = α cimg-qa ykeep i,k + (1α) pent i,k, α [0, 1], (5) as the edge attribute on (Caption QAk) because the answer should be supported by the caption. 1We L2-normalize CLIP embeddings and use cosine similarity unless otherwise stated. (ii) Capacity (supervised). We use 3-way label ycap i,k {EU, HG, EP}, (6) indicating capacity generated by MLLM-based agent. 3.2.3. Graph neural network four define relation directed types = We {describe by, asked about, supports, similar}. Edges are (Image, described by, typed as (src, relation, dst): (Caption, (Image, asked about, QA), Caption), supports, QA), and (QA, similar, QA). Following Sec. 3.2.2, node features are CLIP embeddings with an additional 1-D consistency score for Image and QA, and plain CLIP text features for Caption. Input projections. Let ximg Rf +1, xcap Rf , xqa Rf +1 be node features. We project them into common hidden space Rd via separate linear layers. Heterogeneous message passing. We stack heteroconvolution layers (HeteroConv [16], L=2 default) with sum aggregation over relations. For each layer ℓ, we maintain per-relation operators and sum their outputs for every node type. (Image, deSAGEConv on cross-type edges. For scribed by, Caption) and (Image, asked about, QA), we use GraphSAGE [19] without edge attributes. Given destination node (Caption or QA), let Nℓ(u) be the set of incoming neighbors of type Image at layer ℓ. standard SAGE update is ˆh(ℓ+1) (cid:16) = σ W(ℓ) selfh(ℓ) + W(ℓ) neigh AGG{h(ℓ) (cid:17) (7) : Nℓ(u)} where AGG is usually mean aggregation and σ is nonlinearity ReLU. GATv2Conv with edge attributes. For (Caption, supports, QA) and (QA, similar, QA), we adopt GATv2 [10] with 4 attention heads and edge attributes (edge dim= 1). Let euv be the scalar edge attribute: NLI entailment probability for SUPPORTS and nonnegative cosine similarity for SIMILAR. GATv2 computes attention coefficients using content-based mechanism: α(ℓ) uv = softmaxvNℓ(u) (cid:16) (cid:0)a ϕ Wh(ℓ) Wh(ℓ) (cid:17)(cid:1), we euv and aggregates messages as ˆh(ℓ+1) (cid:16) (cid:88) = σ uv Wh(ℓ) α(ℓ) (cid:17) , vNℓ(u) (8) (9) where ϕ is pointwise nonlinearity (e.g., LeakyReLU), denotes concatenation, and multi-head outputs are averaged to match the hidden size d. Relation-wise fusion. For each node type, we sum the relation-specific outputs: h(ℓ+1) = (cid:88) rR(u) ˆh(ℓ+1) u,r , (10) where R(u) are relations incident to the type of in layer ℓ. We then apply residual connection, LayerNorm, and dropout: h(ℓ+1) = Dropout (cid:16) LN (cid:0)σ(h(ℓ+1) ) + h(ℓ) (cid:1)(cid:17) . (11) Task heads on QA nodes. Only QA nodes are supervised. Given the final representation h(L) qa , we use two MLP heads: (cid:40) zkeep = MLPkeep(h(L) zcap = MLPcap(h(L) qa ) R2, qa ) R3, (12) producing soft logits for KEEP and 3-way EU/HG/EP, respectively. In training, we optimize multi-task objective (cross-entropy for both heads) with optional class weights; gradients are clipped and dropout is applied after each layer as in the implementation. Intuitively, SAGEConv propagates cross-modal evidence from Image Caption/QA via mean-aggregated neighborhood messages (7), while GATv2Conv leverages edge-aware attention (9) to weight CaptionQA by NLI entailment and QAQA by textual similarity. Relation-wise summation and per-type normalization stabilize heterogeneous fusion across the HiCQA-Graph. 3.3. Data generation 3.3.1. QA generation pipeline Deriving answers directly from the captions in BIOMEDICA yields subset of data that is generally high quality. Based on each figure and its associated answer, we further employ an MLLM agent to generate QA samples spanning three capacity types: EU, HG, and EP. Figure 2 shows our generation pipeline. When constructing HiCQA-Graph, we are constrained by the maximum number of text tokens. For QA pairs whose length exceeds this limit, we apply summarization to compress the content while preserving most of the original information, more details in appendix. 3.3.2. MCQs generation pipeline Once high-quality QA database is obtained, we employ an MLLM agent to generate three high-quality distractor options and an accompanying explanation (used as CoT). For the MicroVQA++ test set, we subsequently conduct human review to identify any significant errors. MicroVQA++ exhibits no information leakage with respect to MicroVQA. Detailed dataset licensing information is provided in the appendix. Figure 4. Two-dimensional t-SNE of CLIP embeddings for images and questions. t-SNE uses perplexity of 30 and 1,000 iterations. 3.4. Data quality analysis 3.4.1. Blooms level Figure 1 presents the Blooms levels of our dataset. The evaluation method follows [18], which defines question difficulty. We follow MicroVQA and use an LLM to assess the difficulty of questions in our dataset. As shown, our dataset contains substantially higher proportion of high-difficulty samples and larger overall scale than MicroVQA. 3.4.2. CLIP feature distribution Figure 4 highlights dataset-specific geometry in the shared CLIP space using t-SNE [31]. (i) Intra-dataset compactness varies. Image feature space. MicroVQA++ exhibits compact and uniform arrangement, whereas MicroVQA forms two relatively tight clusters, indicating greater heterogeneity in its image distribution. MicroVQA++ also covers broader range of images. (ii) Text feature space. MicroVQA++ and MicroVQA present clear distributional boundaries, highlighting substantial differences between the questioning styles and perspectives of human experts and those of MLLM agents. Inter-dataset overlap is non-negligible. Overlap regions indicate transfer-friendly zones where content and language conventions align; these are the regimes where cross-dataset generalization after SFT tends to be strongest. Cross-modal co-location. Image points and Question points from the same dataset lie in nearby neighborhoods, indicating that CLIPs alignment is already strong prior; this justifies using CLIP-based signals inside our graph filter and helps interpret why combining textual NLI with visionlanguage alignment is superior to either alone. 3.5. MLLMs training details With the MicroVQA++ training split in place, we explored both the data format (free-form QA vs. close-form MCQ) and the learning algorithm (Supervised Fine-Tuning, SFT, vs. Group Relative Policy Optimization, GRPO). Unless otherwise noted, training used an InternVL-style instruct model; microscopy images were resized so the short side was 448 pixels. 3.5.1. Supervised fine-tuning We fine-tuned with LLaMA-Factory [53], using LoRA [20] with rank 16 applied to all attention projections (q, k, v, o) and MLP blocks (gate, up, down). For optimizer, we use AdamW with 1e-4 learning rate, weight decay 0.1, cosine decay with 5% warm-up; 3 epochs in bfloat16. We enabled gradient checkpointing and gradient clipping at 1.0. To stabilize throughput, we used sequence packing (context length 4,096) and gradient accumulation to reach an effective batch size of 256 sequences. QA-based SFT. For free-form QA samples, the input bundled system instruction, one image, and question; the target was the full textual answer. microscopy-oriented instruction encouraged concise, evidence-grounded responses. We optimized next-token cross-entropy over the entire answer span. In ablations, QA-only supervision improved factual grounding and descriptive precision but did not fully train option discrimination. MCQ-based SFT. For MCQ samples, the input included the system instruction, an image, question, and four options. The model was trained to produce brief chain-ofthought rationale and structured final choice tag. 3.5.2. Group relative policy optimization We trained GRPO [39] algorithm with the verl [40] framework. For each prompt we sampled = 6 candidate completions, computed per-sample rewards, and shaped advantages relative to the group mean using frozen SFT policy as reference. Rewards. We combined (i) format reward that penalizes extraneous markup, and (ii) an answer-accuracy reward. Invalidly formatted outputs receive small negative reward. Optimization details. We use four RTX 3090. Learning rate 5e-6, KL coefficient β = 0.05, entropy bonus 0.001, value-loss coefficient 0.5, and advantage normalization per batch. We trained for 1 epochs over the MCQ subset, reusing the SFT tokenizer and image encoder. GRPO sharpened option calibration and adherence to output format; used after SFT it provided reliable incremental gains. 4. Experiments 4.1. Dataset and evaluation metric MicroVQA and MicroVQA++. In subsequent experiments, we train on the MicroVQA++ training set and evaluate on the MicroVQA and MicroVQA++ test sets. To ensure fairness, we use an identical prompt template for inference on both test sets. HiCQA-Graph training details are available in appendix. Evaluation metric. We follow the evaluation protocol of MicroVQA: for each of the three capacities (EU, HG, EP), we compute MCQ accuracy via regex-based matching. 4.2. Comparison experiments MicroVQA benchmark. We evaluate both proprietary commercial systems and open-source MLLMs on the MicroVQA benchmark in Table 2. Consistent with observations in the original MicroVQA paper, we find: (i) Smallscale MLLMs can be surprisingly competitive. For example, the base model InternVL3.5-4B-Instruct achieves (ii) Modaverage accuracy on par with GPT-4o series. els that excel on natural-image multimodal data (e.g., o3 and GPT-5) still struggle with microscopy-centric reasoning that demands domain knowledge and multi-step de- (iii) SFT on our MicroVQA++ train split subduction. stantially boosts microscopy reasoning for small models: InternVL3.5-2B/4B-Instruct improve by 22.7% and 27.5% (iv) After SFT on Mirelatively on Avg, respectively. croVQA++, 4B-parameter model can match the strongest commercial models on MicroVQA despite being an order of magnitude smaller. Table 2. Comparison experiments on MicroVQA. *denotes results obtained from the MicroVQA leaderboard; denotes results after SFT on the MicroVQA++ training set. Bold mark means the best performance; single underline marks the second-best. Unless otherwise specified, the same conventions apply hereafter. The latest version of the closed-source models used are provided in the appendix. EU Models Baseline Random* [11] Human* [11] Close sourced (commercial) GPT-4o* [21] GPT-4o-mini* [21] o1* [23] Claude Sonnet 4.5* [3] o4-mini* [35] o3* [35] GPT-5* [34] Open sourced LLaVA-Med-Mistral-7B* [26] Qwen-2-VL-7b* [44] InternVL3.5-2B-Instruct [45] InternVL3.5-4B-Instruct MicroVQA++ train set fine-tuned LLaVA-Med-Mistral-7B InternVL3.5-2B-Instruct InternVL3.5-4B-Instruct 21.9 52.7 48.7 48.5 55.4 55.1 57.9 61.5 63. 37.3 54.1 46.4 45.9 54.6 55.4 61.0 HG EP Avg 21.8 47. 43.1 43.6 50.2 56.4 56.1 60.5 58.9 47.1 43.3 41.0 43.8 51.7 52.4 56.9 21.9 51.4 44.8 47.0 53.0 49.6 50.4 53.5 53.9 41.6 49.6 47.4 52. 56.1 57.0 61.3 22.0 50.3 45.6 46.2 52.8 54.4 55.6 59.3 59.4 43.0 48.8 44.4 46.6 53.7 54.5 59.4 Table 3. Comparison experiments on MicroVQA++ testing set. EU 25.0 31.5 26.1 Models Baseline Random Close sourced (commercial) GPT-4o-mini [21] qwen-vl-flash [5] Open sourced LLaVA-Med-Mistral-7B [26] Qwen2.5-VL-7B-Instruct [6] Qwen3-VL-4B-Instruct [49] MicroVQA++ train set fine-tuned LLaVA-Med-Mistral-7B InternVL3.5-4B-Instruct 26.6 32.9 32.1 39.3 36. HG EP Avg 25.0 25.0 25. 53.4 49.2 41.0 48.5 48.9 61.4 59.7 42.2 37.6 41.2 34.9 39.1 39.3 42. 37.3 32.2 33.5 36.3 36.4 45.3 41.3 MicroVQA++ test set. We further compare proprietary and open-source MLLMs on the more challenging MicroVQA++ test set in Table 3. As illustrated in Figure 1, it adopts higher Blooms level harder question distribution. Since the MicroVQA++ train and test distributions have intentionally low overlap (Figure 4), SFT on the train split yields only modest +0.6 absolute gain for InternVL3.54B-Instruct on the test set, underscoring the reduced dataset bias and stronger generalization requirements. Table 4. Comparison experiments on data filtering methods (2B/4B). Superscript in methods indicates the subset with the topx% of the dataset retained. Methods Full100% NLI25% NLI50% NLI75% CLIP25% CLIP50% CLIP75% NCLIP25% NCLIP50% NCLIP75% HiCQA25% HiCQA50% HiCQA75% EU 55.9/60.0 55.1/58.7 53.3/57.7 54.1/60.2 52.8/58.4 53.8/59.2 55.1/58.7 54.3/58.7 54.6/59.9 57.1/60.0 53.8/57.7 54.3/60.2 55.4/61.0 HG 51.9/55.0 49.5/54.3 48.8/55.0 50.7/55.2 50.0/53.8 48.6/53.6 49.3/56.0 51.4/53.6 50.0/54.0 50.0/54.8 51.0/53.3 48.8/53.8 52.4/56.9 EP 55.7/60.9 57.0/62.6 53.9/60.0 51.7/59.1 56.1/58.7 57.8/57.8 54.3/58.7 50.0/60.4 58.8/59.6 58.3/61.3 57.0/58.3 56.1/57.8 57.0/61.3 Avg 54.2/58.2 53.3/57.8 51.6/57.1 52.2/58.0 52.4/56.6 52.6/56.6 52.6/57.6 52.2/57.0 53.6/57.5 54.5/58.2 53.4/56.0 52.5/57.1 54.5/59.4 Filtering methods. For NLI, we fine-tune an XLM-R model [14, 28] on XNLI and MultiNLI [13, 47] to score caption Answer entailment. For CLIP, we use OpenCLIP [22] (ViT-L/14 image encoder + Transformer text encoder [42]) initialized from BIOMEDICA-pretrained weights. NCLIP is linear combination of the NLI and CLIP scores. As shown in Table 4, using only NLI or CLIP is inferior, while NCLIP improves robustness and generalization on InternVL3.5-2B/4B. Our HiCQA-Graph jointly models Image, Caption, and QA via heterogeneous message passing, achieving the best average performance at the top-75% keep ratio. 4.3. Ablation experiments We conduct three sets of ablations: (i) the effect of different weak supervised signals (w/o CLIP, w/o NLI and w/o Capa); (ii) the contribution of an additional cross-modal consistency token (w/o token); and (iii) training strategies (SFT with free-form QA, SFT with MCQ, GRPO). Supervised signals. Table 5 shows the effect of different supervision signals. Similar to the results in Table 4, removing any single weak supervision signal leads to degradation in Avg performance. Removing the capacity supervision signal leads to modest performance drop. Cross-modal consistency. As shown in Table 5. We append an extra similarity token to the embeddings of Image and QA nodes to encourage cross-modal consistency learning. Removing this token (HiCQA w/o t) degrades the average score, confirming its utility. Table 5. Ablation experiments on HiCQA-Graph (2B/4B). Methods Oursw/o CLIP Oursw/o NLI Oursw/o token Oursw/o Capa Oursbest EU 56.4/58.9 54.1/59.7 55.9/61.7 55.1/59.7 55.4/61. HG 50.2/56.9 50.5/56.2 51.9/53.3 51.9/55.0 52.4/56.9 EP 57.4/61.3 56.5/60.0 55.7/57.4 56.1/60.4 57.0/61.3 Avg 54.1/58.6 53.2/58.3 54.2/57.4 54.0/58.0 54.5/59.4 Table 6. Ablation experiments on training methods. Models SFT w/ QA SFT w/ MCQ GRPO w/ QA GRPO w/ MCQ 58.7 EU 49.2 61.0 - HG 39.5 56.9 - 54.5 EP 36.1 61.3 - 60.4 Avg 42.4 59.4 - 57.4 competitive performance as well, indicating that MCQformat supervision is an effective learning signal for microscopy reasoning. Training GRPO on QA data without format reward led to severe reward hacking [41, 50], more details will be discussed in the appendix. Table 7. Computational overheads (ms) of HiCQA per image. Component CLIP NLI Train Test forward 32.29 77.05 3.56 0.63 backward - - 10.21 - end-to-end - - 129.32 109.82 4.4. Error analysis on MicroVQA++ We briefly summarize typical failure modes observed on the MicroVQA++ test set (case studies are deferred to ap- (i) Visual grounding: mislocalizing the referpendix). enced structure or channel, especially for small or overlapping compartments, leading to plausible but wrong EU (ii) Distractor susceptibility: over-reliance on answers. lexical overlap yields selection of semantically close but incorrect MCQ options, indicating shortcut pattern match- (iii) Cross-modal priors: caption-like hallucinaing. tions and overuse of familiar biomedical tropes (e.g., infection/inflammation) that are unsupported by the image. 4.5. Computational overheads We report per-image latency for each stage on single RTX 3090 GPU  (Table 7)  . Graph construction is dominated by NLI inference and CLIP encoding, while graph training and inference are lightweight. The end-to-end training and inference times are 129.42 ms and 109.82 ms, respectively. 5. Conclusion Training methods. In Table 6, SFT with MCQ delivers large gain over SFT with QA, and GRPO with MCQ attains We present MicroVQA++, microscopy-focused VQA dataset generation pipeline that converts expert images and captions in PubMed articles into QA and MCQ via MLLM agent and enforces cross-modal consistency with HiCQAGraph before human checks, substantially improving supervision. The resulting MicroVQA++ test set is larger and harder in Bloom levels, providing stricter evaluation. After closed-form MCQs SFT on MicroVQA++ train set, small InternVL3.5-4B-Instruct rivals top proprietary systems on MicroVQA and sets new SOTA among opensource MLLMs; In general, carefully building high-quality dataset enables small MLLMs to advance in microscopy reasoning. Limitations and future work available in appendix."
        },
        {
            "title": "References",
            "content": "[1] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic affinity with image-level supervision for weakly supervised In Proceedings of the IEEE consemantic segmentation. ference on computer vision and pattern recognition, pages 49814990, 2018. 3 [2] Elaf Alhazmi, Quan Sheng, Wei Emma Zhang, Munazza Zaib, and Ahoud Alhazmi. Distractor generation for multiple-choice questions: survey of methods, datasets, arXiv preprint arXiv:2402.01512, 3(5), and evaluation. 2024. 3 [3] Anthropic. Claude sonnet 4.5 system card, 2025. 7 [4] Negin Baghbanzadeh, Sajad Ashkezari, Elham Dolatabadi, and Arash Afkanpour. Open-pmc-18m: high-fidelity large scale medical dataset for multimodal representation learning. arXiv preprint arXiv:2506.02738, 2025. 4 [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 7 [6] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 7 [7] Semere Kiros Bitew, Johannes Deleu, Chris Develder, and Thomas Demeester. Distractor generation for multiplechoice questions with predictive prompting and large lanIn Joint European Conference on Machine guage models. Learning and Knowledge Discovery in Databases, pages 48 63. Springer, 2023. [8] Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher Manning. large annotated corpus for learning natural language inference. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 632642, 2015. 2 [9] Yuri Boykov and M-P Jolly. Interactive graph cuts for optimal boundary & region segmentation of objects in nd In Proceedings eighth IEEE international conferimages. ence on computer vision. ICCV 2001, pages 105112. IEEE, 2001. 3 [10] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? arXiv preprint arXiv:2105.14491, 2021. 5 [11] James Burgess, Jeffrey Nirschl, Laura Bravo-Sanchez, Alejandro Lozano, Sanket Rajan Gupte, Jesus GalazMontoya, Yuhui Zhang, Yuchang Su, Disha Bhowmik, Zachary Coman, et al. Microvqa: multimodal reasoning benchmark for microscopy-based scientific research. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1955219564, 2025. 2, 7 [12] Kamal Choudhary. Microscopygpt: Generating atomicstructure captions from microscopy images of 2d materials with vision-language transformers. The Journal of Physical Chemistry Letters, 16:70287035, 2025. 1 [13] Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 conference on empirical methods in natural language processing, pages 24752485, 2018. [14] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 84408451, 2020. 8 [15] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 4 [16] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019. 5 [17] Mark Gierl, Okan Bulut, Qi Guo, and Xinxin Zhang. Developing, analyzing, and using distractors for multiplechoice tests in education: comprehensive review. Review of educational research, 87(6):10821116, 2017. 3 [18] Educational Goals. Handbook i: Cognitive domain. New York: David, 1956. 2, 6 [19] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017. 3, 5 [20] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 6 [21] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 7 [22] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. 8 [23] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [24] Xi Jiang, Jian Li, Hanqiu Deng, Yong Liu, Bin-Bin Gao, Yifeng Zhou, Jialin Li, Chengjie Wang, and Feng Zheng. Mmad: comprehensive benchmark for multimodal large arXiv language models in industrial anomaly detection. preprint arXiv:2410.09453, 2024. 1 [25] Philipp Krahenbuhl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. Advances in neural information processing systems, 24, 2011. 3 [26] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large languageand-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36:2854128564, 2023. 4, 7 [27] Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 525536. Springer, 2023. 4 [28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [29] Alejandro Lozano, Jeffrey Nirschl, James Burgess, Sanket Rajan Gupte, Yuhui Zhang, Alyssa Unell, and Serena Yeung. Micro-bench: microscopy benchmark for visionlanguage understanding. Advances in Neural Information Processing Systems, 37:3067030685, 2024. 2 [30] Alejandro Lozano, Min Woo Sun, James Burgess, Liangyu Chen, Jeffrey Nirschl, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Anita Rau, Austin Wolfgang Katzer, et al. Biomedica: An open biomedical image-caption archive, dataset, and vision-language models derived from scientific literature. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1972419735, 2025. 2 [31] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9 (Nov):25792605, 2008. 6 [32] Indrajeet Mandal, Jitendra Soni, Mohd Zaki, Morten Smedskjaer, Katrin Wondraczek, Lothar Wondraczek, Nitya Nand Gosvami, and NM Anoop Krishnan. Evaluating large language model agents for automation of atomic force microscopy. Nature Communications, 16(1):9104, 2025. 1 [33] Yoojin Nam, Dong Yeong Kim, Sunggu Kyung, Jinyoung Seo, Jeong Min Song, Jimin Kwon, Jihyun Kim, Wooyoung Jo, Hyungbin Park, Jimin Sung, et al. Multimodal large language models in medical imaging: Current state and future directions. Korean Journal of Radiology, 26(10):900, 2025. 1 [34] OpenAI. Gpt-5 system card, 2025. 7 [35] OpenAI. Openai o3 and o4-mini system card, 2025. 7 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [37] Alexander Ratner, Christopher De Sa, Sen Wu, Daniel Selsam, and Christopher Re. Data programming: Creating large training sets, quickly. Advances in neural information processing systems, 29, 2016. 3 [38] Johannes Ruckert, Louise Bloch, Raphael Brungel, Ahmad Idrissi-Yaghir, Henning Schafer, Cynthia Schmidt, Sven Koitka, Obioma Pelka, Asma Ben Abacha, Alba G. Seco de Herrera, et al. Rocov2: Radiology objects in context version 2, an updated multimodal image dataset. Scientific Data, 11 (1):688, 2024. 4 [39] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 7 [40] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. 7 [41] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. 8 [42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [43] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, et al. Graph attention networks. stat, 1050(20):1048550, 2017. 3 [44] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 7 [45] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 7 [46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 2 [47] Adina Williams, Nikita Nangia, and Samuel Bowman. broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long papers), pages 11121122, 2018. 8 [48] Zhi-Fan Wu, Tong Wei, Jianwen Jiang, Chaojie Mao, Mingqian Tang, and Yu-Feng Li. Ngc: unified framework In Proceedings for learning with open-world noisy data. of the IEEE/CVF International Conference on Computer Vision, pages 6271, 2021. 3 [49] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [50] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13807 13816, 2024. 8 [51] HaiYang Zhang, XiMing Xing, and Liang Liu. Dualgraph: graph-based method for reasoning about label noise. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 96549663, 2021. 3 [52] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, et al. Large-scale domain-specific pretraining for biomedical vision-language processing. arXiv preprint arXiv:2303.00915, 2(3):6, 2023. 4 [53] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. 6 [54] Guoqing Zhou, Lihuang Qian, and Paolo Gamba. Advances on multimodal remote sensing foundation models for earth observation downstream tasks: survey. Remote Sensing, 2025. 1 [55] Xingcheng Zhou, Mingyu Liu, Ekim Yurtsever, Bare Luka Zagar, Walter Zimmer, Hu Cao, and Alois Knoll. Vision language models in autonomous driving: survey and outlook. IEEE Transactions on Intelligent Vehicles, 2024. 1 [56] Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. ProQuest number: information to all users, 2002."
        }
    ],
    "affiliations": [
        "Fudan University"
    ]
}