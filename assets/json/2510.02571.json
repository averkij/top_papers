{
    "paper_title": "How Confident are Video Models? Empowering Video Models to Express their Uncertainty",
    "authors": [
        "Zhiting Mei",
        "Ola Shorinwa",
        "Anirudha Majumdar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents."
        },
        {
            "title": "Start",
            "content": "How Confident are Video Models? Empowering Video Models to Express their Uncertainty Zhiting Mei1, Ola Shorinwa1, Anirudha Majumdar1 1Princeton University Equal contribution. Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present framework for uncertainty quantification of generative video models, consisting of: (i) metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents. Keywords: Video Models, Uncertainty Quantification, Trustworthy Generative Models. Website: s-qubed.github.io Code: github.com/irom-princeton/s-qubed 5 2 0 2 2 ] . [ 1 1 7 5 2 0 . 0 1 5 2 : r Figure 1 Video models are unable to express their uncertainty, posing critical limitation especially in tasks where they lack requisite knowledge. Here, the video model generates an inaccurate video (showing Albert Einstein), when prompted to generate video of Jeff Einstein. To this end, we introduce metric for evaluating the calibration of video models, calibrated uncertainty quantification method (S-QUBED) which uses latent modeling to disentangle aleatoric and epistemic uncertainty, and UQ dataset for benchmarking calibration."
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in video generation models have led to huge strides in their capabilities [9, 27]. However, current text-to-video models tend to hallucinate, generating videos misaligned with the user intention, or disobeying physical laws. Despite this important limitation, existing video models are unable to express their own uncertainties, unlike LLMs, posing crucial safety concern. We illustrate hallucinations in video models in Figure 1. When prompted to generate video of Jeff Einstein walking on beach, the video model generates video of Albert Einstein, an entirely different person, without expressing any doubt in its output. We aim to address this critical challenge by empowering video models to express their uncertainty. Specifically, we propose framework for uncertainty quantification of video models, consisting of three fundamental components: First, we introduce metric for evaluating the calibration of video models that directly assesses the alignment of the uncertainty estimates with the accuracy of the video generation task. Our metric estimates the rank correlation between uncertainty and task accuracy to measure the calibration error. Second, we derive S-QUBED (Semantically-Quantifying Uncertainty with Bayesian Entropy Decomposition), black-box uncertainty quantification method for video generation models, preserving amenability to the ever-increasing set of closed-source video models. Our key insight is to quantify uncertainty with latent modeling, enabling the rigorous decomposition of predictive uncertainty into its aleatoric and epistemic components. By mapping the input text prompt to latent space, S-QUBED effectively distinguishes between uncertainty arising from ambiguous prompts and uncertainty arising from the models lack of knowledge. We demonstrate the calibration of S-QUBEDs estimates across broad variety of video generation tasks. Third, we curate UQ dataset of about 40K videos across diverse tasks to facilitate benchmarking UQ methods for video models. We generate the data using the open-source model Cosmos-Predict2 [27]. We hope that the dataset drives research on uncertainty quantification of video models."
        },
        {
            "title": "2 Related Work",
            "content": "Uncertainty Quantification in Deep Learning. Deep neural networks (DNNs) are generally difficult to interpret [20], motivating the development of UQ methods to examine the trustworthiness of their predictions [1]. UQ methods in deep learning can be broadly categorized into: training-free and training-based methods, which constitute majority of existing work. Training-free methods estimate uncertainty without modifying the models architecture, training algorithm, or dataset, e.g., via perturbation techniques [23], dropout injection [19, 24], and test-time data augmentation [2, 38]. In contrast, training-based methods impose specific architectural design choices to enable uncertainty quantification using Bayesian Neural Networks (BNN) and can be further classified into three categories: (i) variational inference, (ii) Monte-Carlo Dropout, and (iii) Deep Ensemble methods. Assuming that the parameters (weights) of learned models are random variables, BNN methods [] apply Bayes rule to estimate posterior distribution over these parameters given prior distribution. However, the exact application of Bayes rule is typically intractable, giving rise to approximation techniques, e.g., variational inference [39], which approximates the posterior distribution using parametric distribution; Monte-Carlo Dropout [12], which samples from the posterior distribution by zeroing-out some weights; and Deep Ensembles [18], which train multiple independent models to represent the posterior distribution. Despite their success, traditional UQ methods in deep learning are computationally expensive, limiting their applications in large generative models, e.g., large language models (LLMs) and vision-language models (VLMs). UQ methods for LLMs/VLMs generally leverage internal activations of these models, or utilize similarity-based metrics or natural-language inference techniques for more efficient UQ (see [32] for detailed discussion). Uncertainty Quantification in Generative Image/Video Models. Unlike DNNs and LLMs, UQ of generative image/video models has been relatively underexplored [11]. Prior work [7] extends Bayesian UQ techniques to denoising diffusion probabilistic models (DDPMs) in generative image modeling by learning distribution of weights for the diffusion model, enabling the estimation of epistemic uncertainty through the variance across the models predictions. Similarly, other approaches [5] train latent diffusion models (diffusion ensembles) 2 Figure 2 S-QUBED architecture. Given text prompt ℓ, our goal is to quantify the uncertainty of the video generation model. We first generate latent prompts consistent with ℓ in line with the prompt refinement used by video models, modeling the aleatoric uncertainty as the entropy of the distribution over latent prompts. Then, for each latent prompt, we generate videos, modeling the epistemic uncertainty as the conditional entropy of the distribution over generated videos. Finally, aggregating the two types of uncertainties yields the total predictive uncertainty. for UQ by estimating the mutual information over distribution of the models weights, analogous to deep ensembles. However, these training-based UQ methods are challenging to implement, given that diffusion models often have billions of parameters, creating significant computation overhead during training or inference. Drawing insights from black-box UQ methods for LLMs [4, 22, 25] which utilize similarity-based techniques for efficient UQ, PUNC [11] explores uncertainty quantification of generative image models in language space. By mapping generated images into language form using VLM, PUNC leverages widely-used text-based similarity metrics [21, 41] to estimate epistemic and aleatoric uncertainty of text-to-image models. Although PUNC addresses the computation limitations of prior UQ methods for generative image models, PUNC is not applicable to video modes. To our knowledge, this work is the first exploration of UQ for video world models."
        },
        {
            "title": "3 Problem Formulation",
            "content": "We examine uncertainty quantification of black-box text-conditioned video generation models, which map text prompt ℓ to video via an unknown stochastic model fθ : (cid:55) parametrized by weights θ. Specifically, the video generation process is described by the model: fθ(V ℓ), (1) where is sampled from the conditional distribution fθ. For an input prompt v, the video generation model has measure of doubt (uncertainty) associated with the sampled video output v. This uncertainty arises from variety of sources, e.g., vagueness in the conditioning input ℓ, randomness in the physical evolution of the real-world, limited training data, etc. In this work, we are interested in quantifying the total predictive uncertainty associated with v, which can be broadly classified into two categories: aleatoric uncertainty and epistemic uncertainty."
        },
        {
            "title": "4 Uncertainty Quantification of Generative Video Models",
            "content": "We present S-QUBED, an efficient method for uncertainty quantification of video generation models, summarized in Figure 2. Without loss of generality, we can decompose the video generation model in Equation (1) using latent variable Z, modeling the video generation as two-step process. In the first step, is sampled from the probability distribution p(Z ℓ) conditioned on the input prompt ℓ. In the second step, the video model samples the output video from the probability distribution p(V = z). Note that the application of latent variables is standard in generative modeling, e.g., in variational Bayesian learning [6, 17, 33], enabling efficient learning and analysis of complex data-generation distributions. Consequently, we can rewrite Equation (1) in the form: fθ(V ℓ) = (cid:90) zZ p(V z, ℓ)p(z ℓ)dz = (cid:90) zZ p(V z)p(z ℓ)dz , (2) 3 where we assumed conditional independence of and ℓ, given z. Note that the video generation model described by Equation (2) is not limiting. In fact, state-of-the-art text-to-video models refine users prompt using an LLM to generate much more detailed prompt that is passed into the video generation model. Hence, we can interpret Equation (2) as first sampling an instance of fully-specified prompt from the conditional distribution defined by the input prompt ℓ, e.g., given the input prompt cat doing something, may be the more specific prompt cat licking its paws before turning to the camera and meowing... Subsequently, the video model generates the output video conditioned on z. Proposition 1 (Uncertainty Decomposition). Define the total predictive uncertainty in the output video as the differential entropy h(V ℓ) of the distribution fθ(V ℓ). Then, this quantity can be decomposed as: h(V ℓ) = h(V Z) + h(Z ℓ), (3) where h(V Z) represents the epistemic uncertainty in v, and h(Z ℓ) the aleatoric uncertainty. This is standard decomposition. We provide the proof in Section A.2 for completeness. In the rest of this section, we introduce our approach to estimating these components."
        },
        {
            "title": "4.1 Aleatoric Uncertainty",
            "content": "Aleatoric uncertainty encompasses irreducible randomness from the vagueness (lack of sufficient specificity) of the conditioning inputs, e.g., generate video of cat doing something. In video generation, vagueness in the input prompt increases the randomness of the conditional probability distribution p(Z ℓ), which is represented by the second term h(Z ℓ) in Equation (3). Note that h(Z ℓ) is independent of since the source of uncertainty arises from the input prompt independent of the second stage of the video generation, e.g., the denoising process in video diffusion models. In particular, randomness in cannot be reduced by training the video model on additional data under the assumption that we can model p(Z ℓ) almost exactly. As measure of aleatoric uncertainty, we would expect h(Z ℓ) to be positively correlated with the vagueness of the input prompt. For example, consider two input prompts: ℓ1 = cat napping and ℓ2 = cat doing something. With ℓ1, the pdf of p(Z ℓ1) will be concentrated on the set: A(ℓ1) = {a black cat napping, cat napping on couch, cat snoring on couch, . . . }. (4) However, with ℓ2, the pdf of p(Z ℓ2) will be concentrated on the set: A(ℓ2) = {a black cat jumping, cat eating on couch, cat meowing next to door, . . . }. (5) Note that the elements of A(ℓ1) are more semantically-related (since ℓ1 is more specific) and are thus closer in the language (semantic) embedding space compared to elements in A(ℓ2). Hence, p(Z ℓ1) will have lower entropy relative to p(Z ℓ2). Modeling the conditional latent distribution. To compute h(Z ℓ), we need to define class of probability distributions that describe the latent-generation process. In this work, we model p(Z ℓ) in language embedding space using the Von-Mises Fisher (VMF) distribution [10, 15], drawing insights from prior work [3, 14, 31]. The Von-Mises Fisher (VMF) distribution describes n-dimensional probability distribution on the (n 1)- sphere over unit vectors embedded in Rn, with the probability density function (pdf): with parameters µ and κ denoting the mean direction and concentration parameters, where: fn(x, µ, κ) = Cn(κ) exp(κµTx), Cn(κ) = κn/21 (2π)n/2In/21(κ) , 4 (6) (7) with In/21 representing the modified Bessel function of the first kind. The concentration parameter functions analogously to the inverse variance, providing measure of the spread of the distribution. We need samples from p(Z ℓ) to fit the VMF distribution. Collecting such data is typically prohibitively expensive. To overcome this challenge, we leverage LLMs as cost-effective generative models of p(Z ℓ), noting that video models generally use LLMs to refine prompts prior to generating videos. Specifically, given an input prompt ℓ, we generate compatible-but-more-specific prompts from an LLM. generated prompt is compatible with the input prompt if the generated prompt is consistent with, i.e., entails, the input prompt. However, the converse need not be true: the input prompt might be underspecified. Subsequently, we compute language embeddings from an embedding model, e.g., SentenceFormer [30]. Although we could directly fit VMF to the language embeddings, we project the language embeddings to lower-dimensional subspace Rn using principal component analysis (PCA) to avoid numerical instability associated with high-dimensional spaces. We estimate the parameters µ and κ of the VMF distribution in closed-form using approximate methods [15, 35], circumventing iterative optimization methods. Estimating Aleatoric Uncertainty. Given p(Z ℓ), we can compute the aleatoric uncertainty h(Z ℓ) of in closed-form via: h(Z ℓ) = log(Cn(κ)) EZ[Z ℓ](κ), (8) κ µzℓ where VMF(µ, κ) and Cn represents the normalization constant given by Equation (7). The expected value of the VMF is given by EZ[Z ℓ](κ) = Wn(κ)µzℓ, where Wn = In/2(κ) In/21(κ) with the modified Bessel function of the first kind In/2. We summarize the method for computing aleatoric uncertainty in Algorithm 1. Algorithm 1: S-QUBED: Aleatoric Uncertainty Quantification of Generative Video Models AleatoricUncertainty (f, ℓ): Input: Video Model , Input Prompt ℓ; Output: Aleatoric Uncertainty h(Z ℓ); A(ℓ) Embed(LLM(ℓ)) ; µzℓ, κzℓ VFit(A(ℓ)) ; h(Z ℓ) Equation (8) ; return h(Z ℓ);"
        },
        {
            "title": "4.2 Epistemic Uncertainty",
            "content": "// Construct A(ℓ) from an LLM/VLM // Estimate p(Z ℓ) with VMF // Compute aleatoric uncertainty h(Z ℓ) Epistemic uncertainty represents the measure of doubt associated with lack of knowledge, which generally results from insufficient training data (e.g., Figure 1). As result, epistemic uncertainty is reducible by providing additional training data to the model. In Equation (3), h(V Z) represents the epistemic uncertainty of the generated video v, where the uncertainty arises from the limited knowledge of the video model about concepts expressed by the latent variable Z. For example, consider video model trained entirely on internet videos of cats and dogs performing different activities, e.g., running, eating, jumping, meowing/barking. Now, when asked to generate video of lion roaring in the wild, the video model might generate different videos across different runs, with some showing large cat meowing in park with significant tree canopy, others showing cat making barking-like sounds in forest, etc. Although the generated videos are all conditioned on semantically-consistent latent variables, the generated videos might be semantically-inconsistent, since the video model has not been trained on videos of lions. This uncertainty in the generated videos can be described as epistemic and is captured by the entropy term h(V Z). Estimating Epistemic Uncertainty. Note that we can express h(V Z) in the form: h(V Z) = Ezp(zℓ)[h(V = z)], (9) which can be interpreted as the expected entropy of the distribution of generated videos conditioned on sampled latent states from the conditional distribution p(z ℓ). Computing h(V Z) is challenging for two 5 reasons: (i) we do not have an explicit model of p(V = z) which is required to compute h(V = z), and (ii) even with an analytical expression for p(V = z), computing h(V Z) would require evaluating double integral, which is intractable in general. To address the first challenge, we approximate the conditional distribution p(V = z) using VMF distribution with the parameters µ and κ estimated from samples drawn from the video model. Likewise, we approximate the expectation in Equation (9) using Monte-Carlo sampling to address the second challenge, which we describe in greater detail. First, we sample set of latent variables Ezℓ conditioned on the input prompt ℓ from the distribution p(Z ℓ), with each Ezℓ representing specific instances of prompts entailing the input prompt. For each z, we estimate the distribution p(V = z) by generating set of videos Evz from the video model, conditioned on z. Subsequently, we embed these videos with video embedding model, e.g., S3D [26] and fit VMF to the samples in Evz. Afterwards, we compute the entropy h(V = z) with: h(V z) = log(Cn(κvz)) κvz µvz Evz[V = z](κvz), (10) using the estimated VMF parameters µvz and κvz. Finally, we compute an empirical estimate of the expectation of h(V = z) over sampled from p(Z ℓ). We outline these steps in Algorithm 2. Algorithm 2: S-QUBED: Epistemic Uncertainty Quantification of Generative Video Models EpistemicUncertainty (f, ℓ): Input: Video Model , Input Prompt ℓ; Output: Epistemic Uncertainty h(V z); Ezℓ Embed(LLM(ℓ)) ; foreach Ezℓ do Evz Embed(f (V z)) ; µvz, κvz VFit(Evz) ; h(V = z) Equation (10) ; end h(V Z) Equation (9) ; return h(V Z);"
        },
        {
            "title": "5 Experiments",
            "content": "// Construct Ezℓ from an LLM/VLM // Construct Evz from // Estimate p(V = z) from // Compute entropy h(V = z) // Compute epistemic uncertainty h(V Z) We examine the effectiveness of S-QUBED in uncertainty quantification of generative video models, specifically exploring the following questions: (i) How do we evaluate uncertainty calibration of video models? (ii) Are the total predictive uncertainty estimates computed by S-QUBED calibrated? (iii) Can S-QUBED effectively estimate both aleatoric and epistemic uncertainty?"
        },
        {
            "title": "5.1 Evaluation Setup",
            "content": "We describe the datasets, models, and metrics used in evaluating our proposed method. Datasets. We evaluate S-QUBED on two large-scale video generation datasets, VidGen-1M [36] and Panda70M [8]. Using GPT-5-nano [29], we classify the videos in each dataset into five broad categories: animals, food, games, people, and other, standard approach with video datasets. We subsample about 200 video generation tasks uniformly from each category for evaluation. To address issues with missing video data/metadata in some of the datasets, we sample additional videos from other categories, minimally changing the uniformity of the evaluation dataset. Implementation. We evaluate S-QUBED on the Cosmos-Predict2 video model [28] using the official implementation, which utilizes text-to-image-to-video pipeline for text conditioning that generates an image from text prompt, which is used as input to an image-to-video model. Although we explored alternative generative 6 video models, e.g., Veo3 [9], none were compatible with our experiments, either due to limitations on the number of permissible generation requests or prohibitive compute cost. For example, Veo3 only supports generating between 5-20 videos per month, which is insufficient for the scale of our evaluations. We implement our proposed method by sampling 10 latent states, z1:10 p(Zℓ), and subsequently 10 generated videos per latent state, vi 1:10 p(vi jZ = zi)."
        },
        {
            "title": "5.2 How do we evaluate uncertainty calibration of video models?",
            "content": "Uncertainty calibration of video generation models has been underexplored, evidenced by the lack of purposespecific calibration metrics. Widely-used calibration metrics, such as the expected calibration error (ECE) and maximum calibration metrics (MCE) apply only to evaluation settings with discrete ground-truth answers and errors, e.g., with multiple-choice questions, making them unsuitable in video generation tasks with real-valued task errors. Consequently, we propose appropriate metrics for evaluating the calibration of the uncertainty estimates of video models. Specifically, we examine the Kendall rank correlation (Kendalls τ ) [16] between the video models uncertainty estimates and an applicable accuracy metric, which captures the degree of monotonicity between uncertainty and accuracy. We do not utilize Pearsons rank correlation [13] due to its assumptions of linearity and normally-distributed data and likewise do not use the Spearmans rank correlation coefficient [34] due to its high sensitivity to outliers. Figure 3 Calibration Metrics for Video Models. Top: We examine the statistical significance of the Kendall rank correlation between uncertainty and widely-used perceptual metrics. We find that the CLIP cosine similarity score provides the most significant correlation. Bottom: With the CLIP accuracy metric, we observe that low humanannotated uncertainty corresponds to smaller variance in the generated videos and greater accuracy with respect to the ground-truth video. As uncertainty increases, video prediction accuracy decreases. To compute the the rank correlation coefficient, we use the SSIM, PSNR, LPIPS, and CLIP score metrics. To identify the best metric for assessing calibration, we select 10 generation tasks from the Panda-70M datasets and rank the tasks in order of increasing uncertainty based on the vagueness of the text prompt for the task. Note that the vagueness in the prompt directly corresponds to aleatoric uncertainty, making it an effective proxy measure. Given the human-annotated rankings, we compute the Kendall rank correlation between uncertainty and each accuracy metric along with p-value, which provides measure of the statistical significance of the correlation. While Panda-70M dataset consists of tasks with broad range of descriptive detail from vague to very specific, VidGen-1M consists of relatively well-detailed tasks. As result, we do not sample from VidGen-1M, given the less observable variation in the aleatoric uncertainty. We sample the tasks from Panda-70M dataset to retain the distribution of instruction detail. We summarize our results in Figure 3. In Panda-70M, the CLIP score metric is strongly negatively correlated with uncertainty at the 99% significance level. In contrast, the other perceptual metrics lack statistically significant correlation with uncertainty. This finding is not entirely surprising, since CLIP captures semantic 7 information that better reflects the accuracy of the generation task, unlike the other perceptual metrics which are more susceptible to differences in visual changes. Moreover, we visualize the text prompt, ground-truth video, and the first frame of the generated videos for few tasks in Figure 3, ranging from low to high uncertainty (rank). We observe that when uncertainty is low, the model tends to generate very similar videos, which are also close to the ground-truth, resulting in high accuracy with respect to the CLIP score. As we vary the uncertainty of the model, we observe greater variance in the generated videos accompanied by notably lower CLIP scores (compared to the other metrics), further demonstrating the utility of the CLIP score as an accuracy metric."
        },
        {
            "title": "5.3 Are our uncertainty estimates calibrated?",
            "content": "We examine the calibration of our uncertainty estimates in VidGen-1M and Panda-70M, using the CLIP score accuracy metric given its effectiveness in assessing calibration. We first compute the total predictive uncertainty associated with each video task using S-QUBED, and then evaluate the Kendall rank correlation. We define the accuracy of each task as the mean CLIP score across all generated videos for that task. Figure 4 (left) presents results for Panda-70M. We observe statistically significant negative correlation (99% confidence level) between the total uncertainty computed using S-QUBED and the CLIP score, demonstrating calibration of the uncertainty estimates. The results highlight that as the uncertainty of the video model decreases, its accuracy increases. Likewise, in VidGen-1M, the total predictive uncertainty is negatively correlated with the CLIP score at the 89.9% confidence level. From Figure 4, we see that when the total predictive uncertainty estimates is small (A), the video model generates more accurate videos; in contrast, in tasks with high estimated uncertainty (B), the video model is less accurate. Figure 4 Total Predictive Uncertainty for Video Models. We assess the calibration of the total predictive uncertainty computed by S-QUBED. Top: correlation between video prediction accuracy and total uncertainty for Panda-70M and VidGen-1M . We observe statistically significant correlation between accuracy and uncertainty for both datasets, signified by the small p-values. Bottom: visualization of two samples from Panda-70M."
        },
        {
            "title": "5.4 Can S-QUBED effectively estimate both aleatoric and epistemic uncertainty?",
            "content": "We examine the performance of S-QUBED in decomposing total uncertainty into aleatoric and epistemic uncertainty. To effectively assess calibration of aleatoric uncertainty, we consider subset of each dataset where the epistemic uncertainty is almost zero and compute the rank correlation between the aleatoric uncertainty of these samples and the CLIP score. Likewise, to evaluate calibration of epistemic uncertainty, we compute the rank correlation between the epistemic uncertainty and the CLIP score for samples with 8 relatively zero aleatoric uncertainty. In practice, we select samples with the lowest aleatoric or epistemic uncertainty, accordingly. In Figure 5, we visualize the Kendall rank correlation between the aleatoric and epistemic uncertainty and the CLIP score in both datasets. In Panda-70M, we find that aleatoric and epistemic uncertainty are negatively correlated with accuracy at the 94.5% and 98.3% confidence level. Similarly, in VidGen-1M, we observe statistically significant negative correlation between aleatoric and epistemic uncertainty and the accuracy at the 92.3% and 91.7%, respectively. These results highlight that S-QUBED can decompose total uncertainty effectively into its aleatoric and epistemic components Figure 5 Disentangling Aleatoric and Epistemic Uncertainty for Video Models. We demonstrate the calibration of the aleatoric uncertainty estimates of S-QUBED in tasks with no epistemic uncertainty, showing statistically significant negative correlation. We do the same for epistemic uncertainty. Further, we visualize text prompts, ground-truth-videos, and generated videos in tasks with low and high estimated aleatoric uncertainty. We observe that in the low-uncertainty case, the video model achieves high accuracy, unlike the high-uncertainty case, where the prediction accuracy is significantly lower. Similarly, we provide some visualizations in the case with low and high estimated epistemic uncertainty, showing the negative correlation between S-QUBEDs estimated epistemic uncertainty and video prediction accuracy. Notably, the model does not know the specific prompt Behind the Gloves logo, unlike predicting the person in the human-centric videos."
        },
        {
            "title": "6 Conclusion",
            "content": "We present framework for empowering video models to express their uncertainty, critical capability for safety. Concretely, we introduce metric for measuring the calibration of UQ methods for video models and present calibrated UQ method for video models. Our methods utilizes latent modeling to estimate both aleatoric and epistemic uncertainty, without making any limiting assumptions. Further, we provide an open-source video dataset for benchmarking UQ methods for video models. Our experiments demonstrate the calibration of our proposed method and its effectiveness in disentangling aleatoric and epistemic uncertainty."
        },
        {
            "title": "7 Limitations and Future Work",
            "content": "S-QUBED requires generating multiple videos from the video model to estimate epistemic uncertainty, which poses some computational overhead. Future work will explore more efficient strategies for sampling videos from the video model, e.g., in the latent space of the video model. Beyond the two benchmark datasets considered in this work, we will explore extensions to new datasets to augment the UQ dataset curated for benchmarking calibration. In addition, future work will examine the application of our method to new open-source models, as they become available."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors were partially supported by the NSF CAREER Award #2044149, the Office of Naval Research (N00014-23-1-2148), and Sloan Fellowship."
        },
        {
            "title": "References",
            "content": "[1] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, Rajendra Acharya, et al. review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information fusion, 76:243297, 2021. [2] Murat Seckin Ayhan and Philipp Berens. Test-time data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks. In Medical Imaging with Deep Learning, 2018. [3] Arindam Banerjee, Inderjit Dhillon, Joydeep Ghosh, Suvrit Sra, and Greg Ridgeway. Clustering on the unit hypersphere using von mises-fisher distributions. Journal of Machine Learning Research, 6(9), 2005. [4] Evan Becker and Stefano Soatto. Cycles of thought: Measuring llm confidence through stable explanations. arXiv preprint arXiv:2406.03441, 2024. [5] Lucas Berry, Axel Brando, and David Meger. Shedding light on large generative networks: Estimating epistemic uncertainty in diffusion models. In The 40th Conference on Uncertainty in Artificial Intelligence, 2024. [6] Apratim Bhattacharyya, Michael Hanselmann, Mario Fritz, Bernt Schiele, and Christoph-Nikolas Straehle. Conditional flow variational autoencoders for structured sequence prediction. arXiv preprint arXiv:1908.09008, 2019. [7] Matthew Chan, Maria Molina, and Chris Metzler. Estimating epistemic and aleatoric uncertainty with single model. Advances in Neural Information Processing Systems, 37:109845109870, 2024. [8] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. [9] DeepMind. Veo-3: text-to-video generation system with audio. Technical Report Tech Report, DeepMind / Google, 2025. Accessed: YYYY-MM-DD. [10] Ronald Aylmer Fisher. Dispersion on sphere. Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences, 217(1130):295305, 1953. [11] Gianni Franchi, Nacim Belkhir, Dat Nguyen Trong, Guoxuan Xia, and Andrea Pilzer. Towards understanding and quantifying uncertainty for text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 80628072, 2025. [12] Yarin Gal and Zoubin Ghahramani. Dropout as bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 10501059. PMLR, 2016. [13] Francis Galton. Note on regression and correlation. Proceedings of the Royal Society of London, 58:240242, 1895. [14] Siddharth Gopal and Yiming Yang. Von mises-fisher clustering models. In International Conference on Machine Learning, pages 154162. PMLR, 2014. [15] Peter Edmund Jupp and KV Mardia. unified view of the theory of directional statistics, 1975-1988. International Statistical Review/Revue Internationale de Statistique, pages 261294, 1989. [16] Maurice Kendall. new measure of rank correlation. Biometrika, 30(1-2):8193, 1938. [17] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [18] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017. [19] Emanuele Ledda, Giorgio Fumera, and Fabio Roli. Dropout injection at test time for post hoc uncertainty quantification in neural networks. Information Sciences, 645:119356, 2023. [20] Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, and Dejing Dou. Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond. Knowledge and Information Systems, 64(12):31973234, 2022. [21] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [22] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating with confidence: Uncertainty quantification for black-box large language models. arXiv preprint arXiv:2305.19187, 2023. [23] Yifei Liu, Rex Shen, and Xiaotong Shen. Novel uncertainty quantification through perturbation-assisted sample synthesis. IEEE transactions on pattern analysis and machine intelligence, 46(12):78137824, 2024. [24] Antonio Loquercio, Mattia Segu, and Davide Scaramuzza. general framework for uncertainty estimation in deep learning. IEEE Robotics and Automation Letters, 5(2):31533160, 2020. [25] Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896, 2023. [26] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 98799889, 2020. [27] NVIDIA, :, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klar, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos world foundation model platform for physical ai, 2025. https://arxiv.org/abs/2501.03575. [28] NVIDIA Cosmos. cosmos-predict2: General-purpose world foundation models for physical ai. https://github. com/nvidia-cosmos/cosmos-predict2, 2025. Accessed: YYYY-MM-DD. [29] OpenAI. GPT-5 nano, 2025. https://openai.com/gpt-5/. Large language model. Release date: August 7, 2025. [30] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, 2019. [31] Stephen Robertson. Understanding inverse document frequency: on theoretical arguments for idf. Journal of documentation, 60(5):503520, 2004. [32] Ola Shorinwa, Zhiting Mei, Justin Lidard, Allen Ren, and Anirudha Majumdar. survey on uncertainty quantification of large language models: Taxonomy, open research challenges, and future directions. ACM Computing Surveys, 2025. [33] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems, 28, 2015. 11 [34] Charles Spearman. The proof and measurement of association between two things. The American journal of psychology, 100(3/4):441471, 1987. [35] Suvrit Sra. short note on parameter approximation for von mises-fisher distributions: and fast implementation of (x). Computational Statistics, 27(1):177190, 2012. [36] Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, and Hao Li. Vidgen-1m: large-scale dataset for text-to-video generation. arXiv preprint arXiv:2408.02629, 2024. [37] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [38] Luhuan Wu and Sinead Williamson. Posterior uncertainty quantification in neural networks using data augmentation. In International Conference on Artificial Intelligence and Statistics, pages 33763384. PMLR, 2024. [39] Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt. Advances in variational inference. IEEE transactions on pattern analysis and machine intelligence, 41(8):20082026, 2018. [40] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [41] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Evaluation Setup We provide additional details on the evaluation setup. Metrics. We consider the following standard video accuracy metrics: structural similarity index measure (SSIM) [37], peak signal-to-noise ration (PSNR), learned perceptual image patch similarity (LPIPS) [40], and CLIP cosine similarity score. Note that the SSIM, PSNR, and LPIPS primarily assess visual fidelity while the CLIP score captures more semantic information. We take the negative of the LPIPS score to transform it from an error metric to an accuracy metric. To compute the perceptual metrics, we resize all videos spatially to the same dimensions and subsample the longer videos to ensure that all videos have the same duration. For CLIP, we map both the ground-truth video vgt and all the generated videos vi to the visual-semantic space using CLIP. We compute the mean of each metric over all generated videos per task, which represents the assigned value of the metric for that task. A.2 Proofs Proposition 1 (Uncertainty Decomposition). Define the total predictive uncertainty in the output video as the differential entropy h(V ℓ) of the distribution fθ(V ℓ). Then, this quantity can be decomposed as: h(V ℓ) = h(V Z) + h(Z ℓ), (3) where h(V Z) represents the epistemic uncertainty in v, and h(Z ℓ) the aleatoric uncertainty. Proof. The entropy of random variable quantifies its associated uncertainty. Given the probability distribution fθ(V ℓ), we find its entropy by: h(V ℓ) = (cid:90) fθ(V ℓ) log(fθ(V ℓ)) dv vV (cid:90) = (cid:90) vV zZ p(V z)p(z ℓ) log(p(V z)p(z ℓ)) dz dv , (11) (12) where we incorporate the latent state generation step introduced in Equation (2). We can then decompose the log terms into two components: (cid:90) (cid:90) h(V ℓ) = p(V z)p(z ℓ) (log(p(V z)) + log(p(z ℓ))) dz dv (13) zZ vV (cid:90) = p(z ℓ) zZ (cid:90) (cid:90) zZ vV (cid:90) vV p(V z) log(p(V z))dv dz p(V z)dv p(z ℓ) log(p(z ℓ))dz , (14) where Equation (14) applies the Fubini-Tonelli theorem. We note that each term of Equation (14) is an entropy itself: h(V ℓ) = (cid:90) zZ p(z ℓ)h(V = z)dz (cid:90) zZ p(z ℓ) log(p(z ℓ))dz = h(V Z) + h(Z ℓ). (15) (16) We recognize that the first term h(V Z) eliminates uncertainty in prompt ambiguity, and thus signifies the epistemic uncertainty in video generation. On the other hand, the second term h(Z ℓ) is independent of the video model, but rather only depends on the vagueness of the input prompt, signifying aleatoric uncertainty."
        }
    ],
    "affiliations": [
        "Princeton University"
    ]
}