{
    "paper_title": "UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory",
    "authors": [
        "Yongshi Ye",
        "Hui Jiang",
        "Feihu Jiang",
        "Tian Lan",
        "Yichao Du",
        "Biao Fu",
        "Xiaodong Shi",
        "Qianghuai Jia",
        "Longyue Wang",
        "Weihua Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released."
        },
        {
            "title": "Start",
            "content": "UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory Yongshi Ye 1 Hui Jiang Feihu Jiang 2 Tian Lan 2 Yichao Du 2 Biao Fu 3 Xiaodong Shi 1 Qianghuai Jia 2 Longyue Wang 2 Weihua Luo 2 Abstract Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)- based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), self-evolving agent framework that jointly optimizes Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains monotonic growth curve during continuous evolution. Codes and models will be publicly released. 6 2 0 2 1 1 ] . [ 1 2 5 6 0 1 . 2 0 6 2 : r 1. Introduction Self-evolution is fundamental capability for agents operating in dynamic, open-ended environments (Zhang et al., 2026). While Large Language Models (LLMs) serve as powerful backbones for agents, their parameters typically remain frozen after deployment, limiting their ability to 1Xiamen University 2Alibaba International Digital Commerce 3Tongyi Lab, Alibaba Group. Correspondence to: Longyue Wang <wanglongyue.wly@alibabainc.com>. Preprint. February 12, 2026. learn from continuous interactions. To overcome this limitation, long-term memory serves as trainable parameters of agents that can be updated from online experience (Cai et al., 2025b;a; Ouyang et al., 2025; Wei et al., 2025). Conceptually, self-evolving agent system mirrors the neural network optimization (Rumelhart et al., 1986; Cai et al., 2025b; Ouyang et al., 2025): (1) Forward Pass: the frozen agent executes task given retrieved memories from memory bank; and (2) Backward Optimization: memory optimizer extracts insights (memories) from the experience and consolidate them into the memory bank (Xu et al., 2025; Yan et al., 2025). Therefore, the bottleneck of the self-evolving agent lies in the capability of this memory optimizer. While numerous works have improved the memory optimizer, they predominantly focus on memory management, treating extraction as static process via prompting offthe-shelf LLMs (Wu et al., 2025; Yan et al., 2025; Fang et al., 2025), without optimizing for explicit generalization. Consequently, self-evolving agents suffer from two critical problems: (1) Accumulation of Instance-Specific Noise: As shown in Figure 1, static memory extraction blindly retains instance-specific details rather than generalizable principles (Qin et al., 2024), causing progressive memory pollution and poor generalization; (2) Management Misalignment: The extracted memories are often inconsistent with the corresponding management policy, rendering even an optimal management policy ineffective. Therefore, even an well-optimized management policy cannot compensate for low-quality extracted memories, undermining both task performance and cross-task generalization of the self-evolving agents. To bridge this gap, we propose Unified Memory Extraction and Management (UMEM), self-evolving agent framework that jointly optimizes the memory extraction and management capability of memory optimizer. Structurally, UMEM consists of three primary components: frozen Agent Executor (inference engine), Memory Bank (the external parameters of self-evolving agents), and learned memory optimizer (Mem-Optimizer). The Mem-Optimizer stands as the core of our proposed UMEM framework, designed to evolve the memory bank by extracting reusable UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory Figure 1. Comparison between the conventional memory pipeline and our proposed UMEM framework. Left: Vanilla methods suffer from the Rote Memorization trap, overfitting to instance-specific noise. Right: UMEM utilizes learnable Mem-Optimizer to jointly optimize extraction and management. This distills generalizable principles, ensuring robust performance and avoiding noise accumulation. memories from executors experience. Crucially, to address the instance-specific noise, we introduce the Semantic Neighborhood Modeling, which constructs clusters of semantically related queries to simulate cross-task variations, and design Marginal Utility Reward to guide the optimization process. By maximizing this reward via Group Relative Policy Optimization (GRPO), Mem-Optimizer performs end-to-end joint optimization. This guarantees that extracted memories are not only generalizable but also intrinsically aligned with the management policy. Besides, we implement Online Memory Evolution, where the memory bank is dynamically updated with optimal rollouts during training, forcing the agent to learn how to utilize continuously refining memory system. Ultimately, the trained Mem-Optimizer significantly enhancing the cross-task generalization capability of agents. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines like ReMem and Memp on single-turn reasoning tasks. Notably, ablation studies demonstrate that optimizing memory management in isolation leads to significant performance degradation, empirically validating the necessity of jointly optimizing memory extraction and management. Further analysis confirms that Semantic Neighborhood Modeling and the Marginal Utility Reward Function effectively empower the Mem-Optimizer to distill generalizable memories from individual experiences, rather than merely memorizing instance-specific shortcuts. Finally, results of testtime scaling evolution prove that UMEM enables agents to achieve robust and stable self-evolution, maintaining consistent performance gain and widening the performance gap compared to baselines as interactions proceed. These designs ensure our proposed UMEM could effectively transform interaction experience into helpful insights, paving the way for truly self-evolving agents. 2. Related Work From Parametric Memory to Non-Parametric Memory. Researches on memory-augmented language models have spanned from early architectural mechanisms (Weston et al., 2015; Borgeaud et al., 2022) to recent scalable lookup frameworks (Lan et al., 2023; Cheng et al., 2026). However, these approaches necessitate computationally fine-tuning costs. Recently, the community has converged on non-parametric paradigm: treating external memory bank as the agents evolvable parameters (Wei et al., 2025; Cai et al., 2025b;a). Self-Evolving Memory without Optimization. The effectiveness of non-parametric evolution hinges on how experiences are represented. Initial attempts, such as Synapse (Zheng et al., 2024), retrieved raw historical trajectories. However, this approach suffers from severe noise and context window inefficiencies. To distill clearer signals, subsequent works introduced structured abstraction. For example, emp (Fang et al., 2025) converts trajectories into executable programs. ReasoningBank (Ouyang 2 UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory et al., 2025) summarizes success and failure trajectories into reusable memory entries. SimpleMem (Liu et al., 2026) applies semantic compression. However, the memory extraction and management policy of these methods mainly rely on prompting LLMs or hand-crafted rules, preventing the further improvement of extraction and management capability. Self-Evolving Memory with Optimization. Recent research integrates optimization, like Reinforcement Learning (RL), into self-evolving agents, branching into two distinct streams: (1) Optimizing Working Memory or Shortterm Memory: Approaches such as DeepAgent (Li et al., 2025), MemAgent (Xu et al., 2025) and Mem-α (Wang et al., 2025) employ RL to manage working memory or short-term memory (Jiang et al., 2025). While effective for handling long-context inputs, they do not construct evolvable memory bank, which falls outside the scope of our comparison; (2) Optimizing Long-term Memory: This stream aims to enhance the memory management capabilities of agents, exemplified by MemRL (Zhang et al., 2026) and EvolveR (Wu et al., 2025). Existing works exhibit critical limitation: they predominantly optimize memory selection and management while treating memory extraction as static process (Yan et al., 2025). Furthermore, they lack explicit mechanisms to model generalization across future queries, often resulting in the accumulation of lowquality, instance-specific noise. In contrast, we propose the UMEM framework to jointly optimize memory extraction and management policy, ensuring that evolved memories are generalizable and aligned with future reuse. 3. Task Formulation of Self-Evolving Agents Self-evolving agent can be treated as parametric system where the executor (parameters Θ0) are frozen, and the external memory bank serves as the evolvable, nondifferentiable parameters, consisting of set of keyvalue pairs = {(ki, vi)}B i=1, where keys correspond to queries and values store the associated memory content. In our proposed UMEM, the self-evolving process of agents is conceptualized as analogous to network optimization process, comprising forward pass for inference and backward optimization for memory evolution. Feedforward Pass (Memory-Augmented Execution). At time t, given query q, the agent retrieves the Top-K relevant memory entries Btopk Bt. Then, the frozen executor performs inference conditioned on this context to generate complete trajectory τq and prediction ˆyt: τq, ˆyq E(q, Btopk ; Θ0) Here, since Θ0 is fixed, the systems performance is strictly bounded by the quality of the retrieved memory Btopk . Backward Pass (Memory Bank Update). The key to the self-evolving memory is to optimize memory bank B. Since Θ0 is fixed, the systems performance is strictly bounded by the quality of the memory bank Bt. Analogous to backward optimization process, Memory Optimizer model (Mem-Optimizer), parameterized by ϕ, extract memory entries (distills insights) from the trajectory τq, and samples pre-defined memory management operation optq {ADD, UPDATE, ...}: aq = (q, optq) πϕ( q, τq, ˆyq) where is the extracted memory and aq represents the action to the memory bank. The memory bank evolves after applying the action: Bt+1 Apply(Bt, aq). Note that while we formulate the input as the current trajectory τq, this representation is generic; it can easily extend to extracting insights from pairs of successful or failed trajectories (Ouyang et al., 2025). In conclusion, identifying the Mem-Optimizer (πϕ) as the core bottleneck (Zhang et al., 2026; Fang et al., 2025; Cai et al., 2025b), we propose the UMEM framework to jointly optimize its extraction and management policies. 4. Method This section describes our proposed UMEM framework. To ensure generalization, we first propose Semantic Neighborhood Modeling (Section 4.1), which constructs query clusters to prevent overfitting. Besides, we design the Marginal Utility Reward and apply GRPO algorithm to enforce crosstask generalization (Section 4.2). 4.1. Semantic Neighborhood Modeling critical risk in memory evolution is overfitting: an extracted insight may perfectly resolve the current query but fail to generalize to related queries due to instance-specific noise or shortcuts (Qin et al., 2024). To mitigate this, we introduce Semantic Neighborhood Modeling. Our core insight is to treat the local cluster of similar queries as proxy to approximate cross-task variations. Specifically, we first project all queries into shared semantic space using pre-trained encoder (e.g., BGE-M3 (Chen et al., 2024)). For given source query q, we construct its semantic neighborhood NN (q) by retrieving the Top-N nearest neighbors from the corpus based on cosine similarity. During training, we evaluate candidate memory updates not on the current q, but over the entire neighborhood NN (q). This mechanism forces the Mem-Optimizer to discard instance-specific details and extract generalizable insights. 3 UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory Figure 2. Overview of UMEM. Left: Semantic Neighborhood Modeling retrieves related queries to simulate cross-task variations. Right: The Mem-Optimizer distills trajectories from the frozen Executor into memory updates, which are optimized via GRPO. The process is guided by Marginal Utility Reward that measures performance gains across the entire neighborhood to ensure generalization. 4.2. Mem-Optimizer Training via GRPO The training process of Mem-Optimizer comprising following stages: (1) Memory-Augmented Execution; (2) MemOptimizer Policy Rollout; (3) Marginal Utility Reward; (4) Optimization via GRPO; and (5) Online Memory Evolution. The detailed procedural flow are provided in Appendix F. Memory-Augmented Execution. As described in Section 3, for each query in training dataset at training step t, we retrieve the Top-K relevant memory entries Btopk from the current memory bank. The frozen executor then generates trajectory τq and prediction ˆyq. Mem-Optimizer Policy Rollout. As shown in middle of the right panel of Figure 2, the Mem-Optimizer distills the τq into structured memory action (q, optq) (values). Adopting the GRPO algorithm (Shao et al., 2024), we sample group of memory update actions {a(g) }G g=1: {a(g) }G g=1 πϕ( q, τq, Btopk ) (1) Marginal Utility Reward. To evaluate the quality of the generated memory update actions {a(g) g=1, we strictly prohibit overfitting to the single source query q. Instead, we validate the memory update against the Semantic Neighborhood NN (q). For each neighbor query NN (q), we compute the per-neighbor utility u(q) by comparing two }G 4 execution states: reference execution without a(g) and memory-augmented execution where a(g) is used. The marginal utility is then computed by sum of two terms: (1) Success Gain (Gsucc): It quantifies the correction of execution failures: Gsucc(q) = c(τ mem ) c(τ ref ) (2) where subscripts c(τ mem ), c(τ ref ) {0, 1} denote the correctness of the augmented and reference execution trajectories, respectively. positive Gsucc indicates that the memory successfully fixed previously incorrect query, while negative value penalizes memory that introduces errors into originally correct reasoning; (2) Efficiency Regularization (Reff): Beyond correctness, high-quality memory should facilitate more efficient inference, pruning redundant and wrong reasoning steps (Ahmed et al., 2025; Didolkar et al., 2025). To encourage concise reasoning, we introduce an Efficiency Regularization term that rewards token reduction, but only when correctness is preserved: Reff(q) = (cmem cref) 1 (cid:32) (cid:33) ℓ(g) mem ℓref (3) Here, ℓ represents the length of the generated trajectory. The gating term (cmem cref) ensures that we do not reward brevity if it comes at the cost of accuracy (e.g., generating UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory short but wrong answer). The marginal utility reward is then defined as the sum of these two reward scores: u(q) = Gsucc(q) + Reff(q). (4) The final marginal utility reward for candidate memory is the average marginal utility over the neighborhood, rg = EqNN (q) (cid:2)u(q)(cid:3), which favors memory updates that both correct errors of semantically related queries. Optimization via GRPO. Finally, we train πϕ to maximize joint objective rfinal = rfmt + rg, where rfmt {0, 1} is binary format reward that validates if the output format of extracted memories and management operations strictly adheres to the XML schema defined in Appendix B. Online Memory Evolution. After GRPO optimization of one query q, we identify the memory update action a(g) with the highest marginal utility reward and immediately apply it to the memory bank: Bt+1 Apply(Bt, aq). This mechanism ensures that the memory bank is dynamically refined throughout the training process, forcing the agent to learn how to utilize and manage an evolving memory rather than static one. 5. Experiments 5.1. Setup Datasets. We derive our training data from the MMLU dataset (Hendrycks et al., 2021). Specifically, we randomly sample 2,000 queries from the training split. For each query q, semantic neighborhood cluster NN (q) is constructed by retrieving the Top-N (N = 3) most similar samples within the training set. Backbone. We employ Llama-3.2-1B-Instruct and Qwen34B-Instruct as the Mem-Optimizer policy πϕ. During training (Details are in Appendix A), Qwen3-8B serves as the frozen executor to generate trajectories. To evaluate crossmodel portability, we deploy the Mem-Optimizer to curate memory for diverse unseen executors, including GPT5.1, Qwen3-8B, and Gemini-2.5-Flash. This setup assesses whether UMEM distills architectural-agnostic insights that generalize to heterogeneous and stronger models. Baselines. We evaluate UMEM against several representative paradigms: (1) No Memory, which assesses the frozen backbone LLM without external memory; (2) No Train, non-learning ablation using identical prompt templates without policy training; (3) Self-RAG (Asai et al., 2024), which filters retrieved context via inference-time self-critique; (4) Memp (Fang et al., 2025), decoupled pipeline baseline that distills trajectories into fine-grained instructions and highlevel scripts through independent Build-Retrieve-Update stages; and (5) ReMem (Wei et al., 2025), baseline focusing on memory management that maintains trajectory-level memory via discrete operations interleaved with reasoning steps. Unlike these methods, UMEM uniquely targets the joint optimization and granularity alignment of memory extraction and management. Benchmark. We evaluate UMEM on five benchmarks designed to assess memory stability and reusability across single-turn reasoning and multi-turn embodied interaction. For single-turn tasks, we select AIME (merging AIME24 and AIME25) (Hugging Face H4, 2024; OpenCompass, 2025) and GPQA-Diamond (Rein et al., 2023) to test domain-specific mathematical and scientific reasoning, alongside HLE (Phan et al., 2025) for multidisciplinary complex reasoning. We also include HotpotQA (Yang et al., 2018) to evaluate strategy reuse in multi-hop question answering. For these single-turn benchmarks, performance is reported using Exact Match (EM) accuracy. For multi-turn embodied settings, we adopt ALFWorld (Shridhar et al., 2021), which requires long-horizon planning and statedependent decision-making; we report Cumulative Success Rate (CSR) and Progress Rate following prior benchmark/- metric practice (Wu et al., 2024; Wei et al., 2025). Evaluation Protocol. We adopt streaming protocol to assess the agents continuous self-evolution. Unlike static benchmarks, tasks are processed as sequential stream. This zero-reset setup ensures that experiences distilled from trajectory are immediately integrated into memory bank to facilitate the reasoning of all subsequent queries. 5.2. Main Results As illustrated in Table 1, UMEM consistently outperforms all baseline methods, including state-of-the-art memory management systems like ReMem and Memp, across the vast majority of benchmarks. Notably, our framework achieves significant performance leaps in complex reasoning tasks (e.g., AIME and GPQA Diamond) and embodied environments like ALFWorld, where UMEM-Qwen3-4B attains Success Rate of 82.84% when paired with GPT-5.1. key observation is that the effectiveness of UMEM is positively correlated with the strength of the frozen executor; more powerful executors such as GPT-5.1 and Gemini-2.5Flash tend to yield more pronounced gains compared to the Qwen3-8B-Thinking baseline. This phenomenon can be attributed to the higher-quality reasoning trajectories and interaction traces produced by stronger executors, which serve as high-fidelity source material for UMEM to distill more actionable and sophisticated insights. Furthermore, UMEM exhibits excellent scalability regarding its policy model size. While even compact 1B model (UMEM-Llama-3.2-1B) provides substantial improvement 5 UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory Table 1. Main Results. We evaluate UMEM using three distinct frozen executors: Qwen3-8B-Thinking, GPT-5.1, and Gemini-2.5-Flash. Performance gains () and drops () of UMEM compared to its direct backbone are explicitly marked. Models Parameters AIME GPQA HLE HotpotQA Frozen Executor: Qwen3-8B-Thinking No Memory Self-RAG ReMem Memp Llama-3.2-1B-Instruct UMEM-Llama-3.2-1B (Ours) Qwen3-4B-Instruct UMEM-Qwen3-4B (Ours) Frozen Executor: GPT-5.1 No Memory Self-RAG ReMem Memp Llama-3.2-1B-Instruct UMEM-Llama-3.2-1B (Ours) Qwen3-4B-Instruct UMEM-Qwen3-4B (Ours) Frozen Executor: Gemini-2.5-Flash No Memory Self-RAG ReMem Memp Llama-3.2-1B-Instruct UMEM-Llama-3.2-1B (Ours) Qwen3-4B-Instruct UMEM-Qwen3-4B (Ours) - 8B 8B 8B 1B 1B 4B 4B - API API API 1B 1B 4B 4B - API API API 1B 1B 4B 4B 51.67 30.00 61.67 46.67 51.67 60.008.3 55.00 58.333.3 40.00 50.00 30.00 45.00 43.33 45.001.7 46.67 51.675.0 53.33 56.67 56.67 53.33 51.67 58.336.7 56.67 60.003.3 52.53 43.94 53.54 49.49 52.02 54.042.0 53.54 52.021.5 57.57 57.58 62.63 62.12 61.11 62.631.5 62.63 65.152. 73.23 71.72 70.20 74.75 73.23 71.721.5 72.22 76.264.0 7.51 8.56 6.42 11.22 6.42 6.950.5 6.95 8.021.1 6.95 7.49 8.56 10.16 7.49 8.561.1 8.02 8.560.5 10.16 10.16 10.70 7.49 10.16 13.373.2 9.63 11.762.1 62.00 25.00 15.00 62.00 59.00 61.002 60.00 63.003 39.00 42.00 43.00 52.00 51.00 55.004 52.00 54.002. 30.00 42.00 36.00 41.00 42.00 42.00 42.00 45.003.0 ALFWorld SR PR 41.04 30.60 46.27 44.78 47.01 44.782.2 40.30 50.7510.5 61.94 70.90 73.13 77.61 61.94 64.182.2 70.90 82.8411. 55.22 59.70 56.72 60.45 53.73 58.965.2 52.24 61.199.0 68.91 54.23 66.17 69.78 74.13 72.142.0 65.92 73.137.2 66.67 83.71 79.60 81.34 73.63 75.371.7 78.86 84.205.3 72.89 74.50 75.62 76.74 71.27 77.246.0 72.64 78.616.0 Average 47.28 32.06 41.51 47.32 48.38 49.821.4 46.95 50.883. 45.36 51.95 49.49 54.71 49.75 51.792.0 53.18 57.744.6 49.14 52.46 50.99 52.29 50.34 53.603.3 50.90 55.474.6 over the base model and often surpasses larger models, further scaling the policy model to 4B consistently yields additional performance dividends across nearly all tasks. This suggests that while UMEM is highly efficient at small scale, increased model capacity allows it to capture more nuanced semantic relationships and implement more precise memory management strategies, thereby further pushing the performance upper bound of self-evolving agents. 5.3. Ablation Studies This section validates the effectiveness of our designs in UMEM by ablation studies: (1) the necessity and sensitivity of semantic neighborhood modeling; and (2) joint optimization on memory extraction and management. Semantic Neighborhood Modeling. We first examine the necessity of Semantic Neighborhood Modeling. The forth row in Table 2 reveals that removing it during training results in significant performance collapse, particularly on the reasoning-heavy AIME benchmark (GPT-5.1: dropping from 51.67 to 41.67; Qwen3-8B: dropping from 58.33 to 55.00). Furthermore, we also investigate the impact of the semantic neighborhood size {1, 3, 5}. As reported in last three rows in Table 2, = 3 yields the optimal balance between task-specific optimization and cross-task transfer. Performance degrades at both extremes: an overly narrow neighborhood (N = 1) fails to capture task shifts (GPT-5.1: AIME drops to 48.33; Qwen3-8B: dropping from 58.33 to 51.67), while an overly broad one (N = 5) introduces noise that dilutes the reward signal during optimization. Joint Optimization. We evaluate the contribution of memory extraction and management by masking the gradient of their respective tokens. As shown in the first two rows of Table 2, breaking the joint optimization leads to severe performance degradation across the majority of benchmarks. Specifically, disabling memory extraction optimization results in average performance decline of 4.7 points across all metrics, which is significantly higher than that observed when removing management optimization (0.73 points). These results reveal that optimizing the quality of extracted memory is the more important for effective self-evolution. 6 UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory Table 2. Ablation studies on joint optimization components and neighborhood size on GPT-5.1 and Qwen3-8B-Thinking. The full UMEM method for each model serves as the baseline. The performance drops (drop) or gains (gain) of each variant compared to the respective full method are explicitly marked to demonstrate the contribution of each component. Opt. denotes Optimization. SNM denotes the Semantic Neighborhood Modeling. Method AIME (Acc.) Joint Optimization Components GPT-5.1 Qwen3-8B-Thinking GPQA HLE HotpotQA ALFWorld AIME GPQA HLE HotpotQA ALFWorld (Acc.) (Acc.) (Acc.) SR PR (Acc.) (Acc.) (Acc.) (Acc.) SR PR UMEM (Full Method) w/o Extraction Opt. w/o Management Opt. w/o SNM 51.67 45.006.7 48.333.3 41.6710.0 65.15 59.605.6 64.650.5 64.141.0 8.56 5.882.7 9.090.5 6.951.6 54.00 51.003.0 55.001.0 52.002. 82.84 76.126.7 80.602.2 79.103.7 84.20 80.723.5 84.330.1 81.093.1 58.33 55.003.3 56.671.7 55.003.3 53.54 51.012.5 53.030.5 50.003.5 8.02 8.020.0 6.951.1 7.490.5 63.00 61.002.0 63.000.0 60.003. 50.75 45.535.2 44.706.1 52.992.2 73.13 67.795.3 69.034.1 72.300.8 Sensitivity to Neighborhood Size UMEM (N = 3, Ours) = 1 (Too Narrow) = 5 (Too Broad) 51.67 48.333.3 46.675.0 65.15 63.132.0 64.650. 8.56 7.491.1 7.491.1 54.00 50.004.0 52.002.0 82.84 78.364.5 81.341.5 84.20 81.342.9 84.080.1 58.33 51.676.7 58.330.0 53.54 51.522.0 52.531. 8.02 8.020.0 9.631.6 63.00 59.004.0 62.001.0 50.75 41.799.0 42.548.2 73.13 67.665.5 73.260.1 5.4. Stability of Self-Evolution We evaluate UMEM under continual learning setting across both single-turn reasoning benchmarks and the multiturn ALFWorld environment, reporting the cumulative accuracy in Figure 3. In this streaming protocol, the agent must continuously evolve its memory bank without resetting. This poses severe challenge: error accumulation. As interaction proceeds, flawed memory extraction policies tend to pollute the memory bank with noise or instance-specific shortcuts, degrading performance on subsequent tasks. As shown in Figure 3, under this challenging setting, UMEM consistently maintains superior performance curve compared to baselines, particularly in the later stages. It exhibits significantly slower and more controlled degradation than ReMem and MemP across all evaluations, with the performance gap widening as interaction proceeds. Crucially, ReMem (green curve), which optimizes memory management in isolation, suffers the most rapid degradation and results in the lowest final performance, proving the necessity of jointly optimization. This behavior indicates that UMEM accumulates fewer harmful memories over long horizons, and that its advantage stems not from whether memory is learned, but from how memory extraction and management are coordinated during continual evolution. The extracted memories of the baselines like ReMem and Memp may appear locally better, yet their long-horizon utility remains opaque to the memory manager. Consequently, such memories are often retained and repeatedly reused even when they introduce subtle reasoning errors, leading to progressive error amplification in cumulative evaluation. In contrast, the substantially reduced degradation observed for UMEM suggests that newly updated memories are more consistently aligned with future reuse. Taken together, these results support the conclusion that stable self-evolution requires memory updates to be tightly coupled with the context in which errors arise. By evolving memory primarily around experiences most relevant to the current trajectory, UMEM promotes structured knowledge consolidation rather than unconstrained accumulation. From an optimization perspective, this behavior corresponds to sparse, localized updates over external memory parameters, which naturally limit interference and mitigate long-horizon error accumulation. 5.5. Test-Time Self-Evolution To further validate the sustainability of self-evolution beyond the single-epoch setting in Section 5.4, we extend the experimental scope from 1 epoch to rigorous 10-epoch long-horizon continual interaction on the ALFWorld benchmark with GPT-5.1 as the executor. Figure 4 reports both epoch-wise and cumulative Success Rate and Progress Rate. As shown in the per-epoch Success Rate, UMEM consistently achieves the highest performance across all epochs. Although online retrieval and memory updates inevitably introduce performance fluctuations, UMEM recovers quickly after temporary drops, indicating well-balanced memory strategy between exploration and stability during continual evolution. The cumulative Success Rate further highlights UMEMs advantage. UMEM shows steady and sustained improvement trend, converging to substantially higher performance level than all baselines. Beyond final task success, UMEM also consistently outperforms baselines on Progress Rate, with particularly pronounced margin in cumulative metrics. This trend suggests that, even in partially unsuccessful episodes, UMEM tends to execute more correct intermediate steps, reflecting more stable multi-step decision-making. Overall, these results indicate that UMEM supports more stable and sustainable form of agent selfevolution under continual interaction. 5.6. Cross-Model Effectiveness and Efficiency Figure 5 reports Success Rate and Average Steps on ALFWorld across different executor LLMs. UMEM consistently 7 UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory Figure 3. Cumulative performance over sequential tasks on GPQA-Diamond and ALFWorld Benchmarks. Figure 4. Test-Time Self-Evolution on ALFWorld. achieves the highest Success Rate for all executors, indicating that the evolved experiences provide robust, executoragnostic performance gains. Notably, this improvement is accompanied by clear reduction in Average Steps, showing that higher success is not obtained through longer or more exploratory interaction trajectories, but through more efficient decision making during interaction. This efficiency gain is evident in case study 7. The joint improvement in success and efficiency provides insight into the nature of the experiences evolved by UMEM. In long-horizon interactive tasks, overly specific experiences often lead to shortcut behaviors that fail to generalize to similar tasks, ultimately causing execution failures; in contrast, overly coarse heuristics fail to sufficiently constrain execution and result in longer trajectories. Across all executor 8 Figure 5. Success Rate and Average Steps on ALFWorld benchmark across different executor models. models, UMEM consistently avoids these failure modes, achieving higher success with fewer execution steps. This pattern indicates that the observed gains reflect genuine improvement in execution efficiency that generalizes across executors, rather than an artifact of increased interaction length or model-specific behavior. 6. Conclusion In this paper, we introduced UMEM for self-evolving agents. Unlike prior approaches that treat memory extraction and management as static or decoupled processes, UMEM achieves joint optimization of extraction and management through Semantic Neighborhood Modeling and GRPO augmented with Marginal Utility Reward. This design effectively mitigates the accumulation of instance-specific noise and ensures that extracted memories are intrinsically UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory aligned with the agents management policy. Empirical results demonstrate that UMEM significantly outperforms highly competitive baselines in both cross-task generalization and execution efficiency. By enabling agents to continuously refine the memory bank during continuous interaction, UMEM offers robust paradigm for realizing lifelong learning in open-ended environments."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Ahmed, A., Khan, A. A., Ahmad, A., Di, S., Liu, Z., and Anwar, A. Retrieval-of-thought: Efficient reasoning via reusing thoughts, 2025. URL https://arxiv.org/ abs/2509.21743. Asai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H. SelfRAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=hSyW5go0v8. Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van den Driessche, G., Lespiau, J.-B., Damoc, B., Clark, A., de Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J. W., Elsen, E., and Sifre, L. Improving language models by retrieving from trillions of tokens, 2022. URL https://arxiv. org/abs/2112.04426. Cai, Y., Cai, S., Shi, Y., Xu, Z., Chen, L., Qin, Y., Tan, X., Li, G., Li, Z., Lin, H., Mao, Y., Li, K., and Sun, X. Training-free group relative policy optimization, 2025a. URL https://arxiv.org/abs/2510.08191. Cai, Z., Guo, X., Pei, Y., Feng, J., Su, J., Chen, J., Zhang, Y.-Q., Ma, W.-Y., Wang, M., and Zhou, H. Flex: Continuous agent evolution via forward learning from experience, 2025b. URL https://arxiv.org/abs/ 2511.06449. Chen, J., Xiao, S., Zhang, P., Luo, K., Lian, D., and Liu, Z. M3-embedding: Multi-linguality, multifunctionality, multi-granularity text embeddings through self-knowledge distillation. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 2318 2335, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-acl.137. URL https://aclanthology. org/2024.findings-acl.137/. Cheng, X., Zeng, W., Dai, D., Chen, Q., Wang, B., Xie, Z., Huang, K., Yu, X., Hao, Z., Li, Y., Zhang, H., Zhang, H., Zhao, D., and Liang, W. Conditional memory via scalable lookup: new axis of sparsity for large language models, 2026. URL https://arxiv.org/abs/2601. 07372. Didolkar, A., Ballas, N., Arora, S., and Goyal, A. Metacognitive reuse: Turning recurring llm reasoning into concise behaviors, 2025. URL https://arxiv.org/abs/ 2509.13237. Fang, R., Liang, Y., Wang, X., Wu, J., Qiao, S., Xie, P., Huang, F., Chen, H., and Zhang, N. Memp: Exploring agent procedural memory, 2025. URL https: //arxiv.org/abs/2508.06433. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding, 2021. URL https: //arxiv.org/abs/2009.03300. Hugging Face H4. AIME 2024 Benchmark. https://huggingface.co/datasets/ HuggingFaceH4/aime_2024, 2024. Jiang, X., Li, F., Zhao, H., Qiu, J., Wang, J., Shao, J., Xu, S., Zhang, S., Chen, W., Tang, X., Chen, Y., Wu, M., Ma, W., Wang, M., and Chen, T. Long term memory: The foundation of ai self-evolution, 2025. URL https: //arxiv.org/abs/2410.15665. Lan, T., Cai, D., Wang, Y., Huang, H., and Mao, X.-L. Copy is all you need, 2023. URL https://arxiv.org/ abs/2307.06962. Li, X., Jiao, W., Jin, J., Dong, G., Jin, J., Wang, Y., Wang, H., Zhu, Y., Wen, J.-R., Lu, Y., and Dou, Z. Deepagent: general reasoning agent with scalable toolsets, 2025. URL https://arxiv.org/abs/2510.21618. Liu, J., Su, Y., Xia, P., Han, S., Zheng, Z., Xie, C., Ding, M., and Yao, H. Simplemem: Efficient lifelong memory for llm agents, 2026. URL https://arxiv.org/abs/ 2601.02553. OpenCompass. AIME 2025 Benchmark. https://huggingface.co/datasets/ opencompass/AIME2025, 2025. Ouyang, S., Yan, J., Hsu, I.-H., Chen, Y., Jiang, K., Wang, Z., Han, R., Le, L. T., Daruki, S., Tang, X., Tirumalashetty, V., Lee, G., Rofouei, M., Lin, H., Han, J., Lee, C.-Y., and Pfister, T. Reasoningbank: Scaling 9 UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory Weston, J., Chopra, S., and Bordes, A. Memory networks, 2015. URL https://arxiv.org/abs/ 1410.3916. Wu, C.-K., Tam, Z. R., Lin, C.-Y., Chen, Y.-N., and yi Lee, H. Streambench: Towards benchmarking continuous improvement of language agents, 2024. URL https: //arxiv.org/abs/2406.08747. Wu, R., Wang, X., Mei, J., Cai, P., Fu, D., Yang, C., Wen, L., Yang, X., Shen, Y., Wang, Y., and Shi, B. Evolver: Self-evolving llm agents through an experience-driven lifecycle, 2025. URL https://arxiv.org/abs/ 2510.16079. Xu, W., Liang, Z., Mei, K., Gao, H., Tan, J., and Zhang, Y. A-mem: Agentic memory for llm agents, 2025. URL https://arxiv.org/abs/2502.12110. Yan, S., Yang, X., Huang, Z., Nie, E., Ding, Z., Li, Z., Ma, X., Bi, J., Kersting, K., Pan, J. Z., Schutze, H., Tresp, V., and Ma, Y. Memory-r1: Enhancing large language model agents to manage and utilize memories via reinforcement learning, 2025. URL https://arxiv.org/abs/ 2508.19828. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369 2380, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259/. Zhang, S., Wang, J., Zhou, R., Liao, J., Feng, Y., Zhang, W., Wen, Y., Li, Z., Xiong, F., Qi, Y., Tang, B., and Wen, M. Memrl: Self-evolving agents via runtime reinforcement learning on episodic memory, 2026. URL https:// arxiv.org/abs/2601.03192. Zheng, L., Wang, R., Wang, X., and An, B. Synapse: Trajectory-as-exemplar prompting with memory for computer control, 2024. URL https://arxiv.org/ abs/2306.07863. agent self-evolving with reasoning memory, 2025. URL https://arxiv.org/abs/2509.25140. Phan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., et al. Humanitys last exam, 2025. URL https://arxiv. org/abs/2501.14249. Qin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye, Y., Yuan, W., Liu, H., Li, Y., and Liu, P. O1 replication journey: strategic progress report part 1, 2024. URL https://arxiv.org/abs/2410.18982. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., Bowman, S. R., and Perez, E. Gpqa: graduate-level google-proof Q&A benchmark, 2023. URL https://arxiv.org/abs/ 2311.12022. Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning representations by back-propagating errors. Nature, 323:533536, 1986. URL https: //api.semanticscholar.org/CorpusID: 205001834. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https: //doi.org/10.48550/arXiv.2402.03300. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient RLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April 2025, pp. 12791297. ACM, 2025. doi: 10. 1145/3689031.3696075. URL https://doi.org/ 10.1145/3689031.3696075. Shridhar, M., Yuan, X., Cˆote, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. ALFWorld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations (ICLR), 2021. URL https://openreview.net/forum? id=0IOX0YcCdTn. Wang, Y., Takanobu, R., Liang, Z., Mao, Y., Hu, Y., McAuley, J., and Wu, X. Mem-α: Learning memory construction via reinforcement learning, 2025. URL https://arxiv.org/abs/2509.25911. Wei, T., Sachdeva, N., Coleman, B., He, Z., Bei, Y., Ning, X., Ai, M., Li, Y., He, J., Chi, E. H., Wang, C., Chen, S., Pereira, F., Kang, W.-C., and Cheng, D. Z. Evomemory: Benchmarking llm agent test-time learning with self-evolving memory, 2025. URL https://arxiv. org/abs/2511.20857. 10 UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory A. Implementation Details We optimize the Mem-Optimizer using GRPO (Shao et al., 2024). For each update, we sample batch of 128 training queries and generate G=8 rollouts per query. Training is conducted for 3 epochs. Semantic neighborhoods are constructed with Top-N =3 neighbors, while retrieval during memory evolution uses Top-K=3 memories. We apply KL regularization with coefficient β=0.001 and use clipping ratio of ϵ=0.2. The learning rate is set to 1 106. During training, generation is performed with temperature 1.0 to encourage exploration. At evaluation time, as well as for executor inference, we use greedy decoding with temperature 0.0. Our method is implemented using the verl framework (Sheng et al., 2025) and trained on 16 NVIDIA A100 GPUs for approximately 11 hours. B. Mem-Optimizer Action Template Each Mem-Optimizer action is represented as structured output following the template: <experience><value>...</value><operation>...</operation></experience> where <value> encodes the extracted memory content derived from the interaction trace, and <operation> specifies the corresponding memory evolution decision (e.g., addition, replacement). C. Theoretical Analysis C.1. Cosine Neighborhood as Proxy for Reuse-Semantic Proximity Lemma C.1 (Retrieval-score stability under cosine proximity). Let e() be ℓ2-normalized embeddings, i.e., e(x)2 = 1. For any two queries q1, q2 and any candidate key k, (cid:12)e(q1)e(k) e(q2)e(k)(cid:12) (cid:12) (cid:12) e(q1) e(q2)2 = (cid:113) 2 2 e(q1)e(q2). Proof. Since e(k)2 = 1, by CauchySchwarz, (cid:12)e(q1)e(k) e(q2)e(k)(cid:12) (cid:12) (cid:12) = (cid:12) (cid:12)(e(q1) e(q2))e(k)(cid:12) (cid:12) e(q1) e(q2)2. For unit vectors, v2 2 = 2 2uv, hence the equality. Interpretation. High cosine similarity guarantees that q1 and q2 assign nearly identical relevance scores to any memory key. This score stability ensures highly overlapping retrieval rankings (and thus similar Top-K sets). Consequently, the cosine neighborhood of source query effectively captures the cluster of future queries that will likely retrieve (and reuse) the same memory. D. Prompt Templates We present the detailed instruction templates used in our framework, encompassing both the Memory Optimizer and the Executor LLM. First, the system prompt for the Memory Optimizer, which is responsible for refining and organizing retrieved past experiences, is shown in Prompt Prompt 6. For the Executor LLM, we designed distinct system prompts to adhere to specific output formats across different domains during training and evaluation. Specifically, mathematical reasoning tasks follow the instructions in Prompt D.1. The unified template for multiple-choice questions (handling both index-based and letter-based outputs) is presented in Prompt D.2, while general question-answering tasks are guided by Prompt D.3. 11 UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory Prompt D.1: Executor Prompt Template: Mathematical Reasoning [System]: # Role You are an expert Math Task Execution Agent. Your goal is to solve mathematical problems by applying logic and methods from Past Effective Experiences. # Input Data 1. Past Experiences: Relevant formulas, theorems, or similar solved examples. 2. Question: The specific math problem you need to solve. # Instructions 1. Analyze the Question to identify the mathematical concepts involved. 2. Refer to the Past Experiences to find the correct formula, method, or logic pattern. 3. Perform the Problem Solving Process step-by-step. Show your work, calculations, and derivations clearly. [User]: # Current Task Solve the following problem. Question: {question} Past Experiences: {memory section} Please reason step by step, and put your final answer within boxed{}. Prompt D.2: Executor Unified Prompt Template: Multiple Choice Tasks [System]: # Role You are an expert Task Execution Agent. Your goal is to solve multiple-choice questions by applying Past Effective Experiences. # Input Data 1. Past Experiences: Historical context or rules to guide your decision. 2. Question: The specific problem you need to solve. 3. Options: list of candidate answers. # Instructions 1. Analyze the Question carefully. 2. Refer to the Past Experiences to find the logic or evidence required to solve the problem. 3. Evaluate the Options and select the best one. 4. CRITICAL: [For Index Tasks]: Identify the Index of the selected option based on 0-based system (i.e., 0, 1, ...). [For Letter Tasks]: Identify the Letter of the selected option (i.e., A, B, C, or D). [User]: # Current Task Question: {question} Options: {choice block} Past Experiences: {memory section} # Output Format Analyze the options and the question step-by-step. Output the final answer [For Index Tasks]: index wrapped in boxed{index}, e.g., boxed{0}. [For Letter Tasks]: single letter wrapped in boxed{Letter}, e.g., boxed{A}. 12 UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory Prompt D.3: Executor Prompt Template: Question Answering (QA) [System]: # Role You are an expert Question Answering Agent. Your goal is to answer questions based on the provided Context and applying Past Effective Experiences. # Input Data 1. Past Experiences: Historical context, strategies, or rules to guide your reasoning. 2. Context: Background information, documents, or text passages relevant to the question. 3. Question: The specific inquiry you need to answer. # Instructions 1. Read the Context carefully to extract relevant facts. 2. Refer to Past Experiences to find successful reasoning patterns or specific knowledge that supplements the context. 3. Synthesize the information to answer the Question accurately and concisely. [User]: # Current Task Question: {question} {context block} Past Experiences: {memory section} # Output Format You must strictly follow this format: First, provide your reasoning process, citing the context or experiences where applicable. Then, output the final answer wrapped in boxed{}. E. Case Study This case study 7 illustrates how retrieved experiences enable effective knowledge transfer and task completion. The task put clean cloth in countertop contains an implicit requirement: the cloth must be cleaned before placement, not merely moved. UMEM Enhanced Agent. By retrieving experiences from analogous tasks (cleaning plates, knives, and pans), the agent recognizes generalizable pattern: locate object pick up go to sinkbasin clean with sinkbasin place on target. Although the agent initially explores incorrect locations (handtowelholder) and picks up the wrong object (handtowel), it self-corrects upon discovering the cloth and successfully applies the cleaning procedure learned from memory. This demonstrates the agents ability to transfer procedural knowledge across different object types (plate/knife/pan cloth) and recover from exploration errors through experience-guided reasoning. Baseline. Lacking prior experiences, this agent interprets the task literally as simple pick-and-place operation. Despite locating the cloth quickly, it repeatedly executes take move actions without ever invoking the clean command. Notably, even after querying the help command and seeing clean (object) with (receptacle) in the available actions, the agent fails to connect this capability to the task requirement. This reveals critical limitation: without experiential knowledge linking the task semantics to the required action sequence, the agent cannot infer the missing step, resulting in an ineffective loop of 30 repeated attempts. Key Insights. (1) Semantic understanding: Experiences provide crucial context for interpreting implicit task requirements (clean as prerequisite, not just descriptor). (2) Efficiency: With memories extracted by UMEM, the exactor completes the task in 13 steps through meaningful exploration, whereas the baseline agent falls into futile loop of repetitive actions and exhausts 30 steps without solving the task. (3) Generalization: Experiences about cleaning plates/knives/pans successfully transfer to cleaning cloth, demonstrating cross-object procedural generalization. F. Procedure for Evolutionary Memory Management Algorithm 1 details the training process of UMEM, characterized by the co-evolution of the Mem-Optimizer πϕ and the memory bank B. Prior to training, we perform Semantic Neighborhood Modeling to identify NN (q) for each query 13 UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory Algorithm 1: UMEM Training: Semantic Neighborhood Modeling and GRPO Input: Query corpus D, frozen executor E, Mem-Optimizer πϕ, Neighborhood size , Group size Output: Trained parameters ϕ and evolved memory bank 1 Phase 1: Offline Semantic Neighborhood Modeling; foreach do NN (q) Retrieve nearest neighbors for from {q}; 2 3 Phase 2: GRPO-based Online Memory Evolution; for each training step do 4 Sample mini-batch D; foreach do 5 6 τq E(q, B); for 1 to do o(g) πϕ( q, τq, B); r(g) r(g) 1 (cid:80) NN (q) I[FormatOK(o(g))]; B(g) Apply o(g) to B; qNN (q) UtilityGain(q, B(g), B); r(g) r(g) + r(g) ; Update ϕ via GRPO using group advantages {r(g) mean(r)}G g=1; B(g) where = arg maxg r(g); 8 return ϕ, B; based on embedding similarity, preventing shortcut learning. The Mem-Optimizer is then optimized through the following iterative stages: (1) Memory-Augmented Execution: The frozen executor performs task using retrieved context from the current memory to generate an initial trajectory τq. (2) Policy Rollout: The Mem-Optimizer πϕ samples group of candidate operations {o(g)}G g=1 (e.g., ADD or UPDATE) based on q, τq, and the retrieved memory. (3) Marginal Utility Reward: For each rollout, we compute format reward rf for structural correctness and marginal utility reward rg, defined as the average performance gain (success rate and efficiency) across the semantic neighborhood NN (q). (4) Optimization via GRPO: The policy πϕ is updated using group-relative advantages derived from the combined rewards rf + rg, facilitating stable policy refinement without critic network. (5) Online Memory Evolution: The memory bank is updated by committing the best-performing operation o(g) from the group, ensuring the knowledge base evolves alongside the policy. 14 UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory Mem-Optimizer Prompt: Success Case (Training & Evaluation) # System Prompt # Role You are an expert Experience Summarizer for memory bank. Your job is to convert one episode into reusable, general experience. # Input Data 1. User Query: The problem context. 2. Past Experiences: Existing rules (indexed as [0], [1]...). 3. Model Execution: The reasoning process. 4. Execution Status: Success or Failure. # CRITICAL CONSTRAINTS 1. NO ANSWER LEAKAGE: Never mention specific option indices or answer strings. 2. NO SPECIFICS: Remove specific numbers/names. Replace with variables/concepts. 3. NO HALLUCINATION: Do not invent facts. # ACTION GUIDELINES ### SCENARIO: SUCCESSFUL EXECUTION Extract the underlying Truth or Method. - Content: Abstract the logic. If knowledge, extract the core fact. - Constraints: NO specific options (A/B) or specific numbers/entities. # MEMORY MANAGEMENT Compare the new insight with [Past Experiences]. Briefly determine whether to ADD new rule or UPDATE index (replace an existing one). # Output Format Strictly follow this index</operation> </experience> ## Analysis ... structure: ## Experience <experience> <value>...</value> <operation>ADD or UPDATE # User Prompt # Task Context [User Query] {question} {choice txt} [Model Execution] {trajectory} [Execution Status] CORRECT (Success) [Past Experiences] {memory content} # Instruction First, analyze the execution and compare with Past Experiences. Then, generate the XML block with the experience value and the operation (ADD or UPDATE index). Mem-Optimizer Prompt: Failure Case (Training & Evaluation) # System Prompt # Role You are an expert Experience Summarizer for memory bank. Your job is to convert one episode into reusable, general experience. # Input Data 1. User Query: The problem context. 2. Past Experiences: Existing rules (indexed as [0], [1]...). 3. Model Execution: The reasoning process. 4. Execution Status: Success or Failure. # CRITICAL CONSTRAINTS 1. NO ANSWER LEAKAGE: Never mention specific option indices or answer strings. 2. NO SPECIFICS: Remove specific numbers/names. Replace with variables/concepts. 3. NO HALLUCINATION: Do not invent facts. # ACTION GUIDELINES ### SCENARIO: FAILED EXECUTION Analyze the Root Cause of the error. - Content: Identify the type of confusion or trap. - Constraints: DO NOT simply say Dont choose X. DO NOT quote the wrong text as rule. # MEMORY MANAGEMENT Compare the new insight with [Past Experiences]. Briefly determine whether to ADD new rule or UPDATE index (replace an existing one). # Output Format Strictly follow this index</operation> </experience> ## Analysis ... structure: ## Experience <experience> <value>...</value> <operation>ADD or UPDATE # User Prompt # Task Context [User Query] {question} {choice txt} [Model Execution] {trajectory} [Execution Status] INCORRECT (Failure) [Past Experiences] {memory content} # Instruction First, analyze the execution and compare with Past Experiences. Then, generate the XML block with the experience value and the operation (ADD or UPDATE index). Figure 6. Comparison of Mem-Optimizer prompt templates for successful (top) and failed (bottom) executions. These templates are employed during both training and evaluation phases to either extract general methodologies or diagnose root causes. 15 UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory Task & Retrieved Experiences [Task Goal] Put clean cloth in countertop. [Retrieved Experiences] [0] key: clean some plate and put it in countertop. value: To clean plate and place it on countertop, first locate the plate (e.g., in cabinet), pick it up, go to the sinkbasin, clean it using the command clean [plate] with sinkbasin, and only after cleaning move it to countertop. Skipping the cleaning step or placing dirty plate on the countertop results in failure. [1] key: put clean knife in countertop. value: To put clean knife on countertop, first locate the knife (e.g., on countertop or in storage), pick it up, go to the sinkbasin, clean it using clean [knife] with sinkbasin, and only after cleaning move it to countertop. Skipping the cleaning step results in failure. [2] key: clean some pan and put it in countertop. value: To clean pan and place it on countertop, first locate the pan, verify its identity by examining it, pick it up, go to the sinkbasin, clean it using clean [pan] with sinkbasin, and only after cleaning move it to countertop. UMEM Enhanced Agent: w/ Memory (Success) [Environment] You are in the middle of room. You see cabinet 4, cabinet 3, cabinet 2, cabinet 1, countertop 1, garbagecan 1, handtowelholder 2, handtowelholder 1, sinkbasin 2, sinkbasin 1, toilet 1, toiletpaperhanger 1, and towelholder 1. [Execution Trajectory] Step 1: go to handtowelholder 1 You see handtowel 1. Step 2: take handtowel 1 from handtowelholder 1 You pick up the handtowel 1. Step 3: go to sinkbasin 2 You arrive at sinkbasin 1. Step 4: move handtowel 1 to sinkbasin 1 You move the handtowel 1 to the sinkbasin 1. Step 5: take handtowel 1 from sinkbasin 1 You pick up the handtowel 1. Step 6: go to countertop 1 You see candle 1, cloth 3, and spraybottle 1. Step 7: examine countertop 1 On the countertop 1, you see candle 1, cloth 3, and spraybottle 1. Step 8: move handtowel 1 to countertop 1 You move the handtowel 1 to the countertop 1. Step 9: take cloth 3 from countertop 1 You pick up the cloth 3. Step 10: go to sinkbasin 1 You arrive at sinkbasin 1. Step 11: clean cloth 3 with sinkbasin 1 You clean the cloth 3 using the sinkbasin 1. Step 12: go to countertop 1 You arrive at countertop 1. Step 13: move cloth 3 to countertop 1 You move the cloth 3 to the countertop 1. [Result] SUCCESS Total Steps: 13 The agent explores, finds the correct object, and applies the cleaning pattern from memory. Baseline Agent: w/o Memory (Failure) [Environment] (Same as above) [Execution Trajectory] Step 1: go to countertop 1 You see candle 1, cloth 3, and spraybottle 1. Step 2: take cloth 3 from countertop 1 You pick up the cloth 3. Step 3: move cloth 3 to countertop 1 You move the cloth 3 to the countertop 1. Step 4: examine countertop 1 You see candle 1, cloth 3, and spraybottle 1. Step 5: take cloth 3 from countertop 1 You pick up the cloth 3. Step 6: examine cloth 3 Theres nothing special about cloth 3. Step 7: move cloth 3 to countertop 1 You move the cloth 3 to the countertop 1. Step 8: examine countertop 1 You see candle 1, cloth 3, and spraybottle 1. Step 9: help Available commands: look, inventory, go to, open, close, take, move, examine, use, heat, clean, cool, slice... Step 10: take cloth 3 from countertop 1 You pick up the cloth 3. Step 11: move cloth 3 to countertop 1 You move the cloth 3 to the countertop 1. Step 12: examine countertop 1 You see candle 1, cloth 3, and spraybottle 1. Step 13: inventory You are not carrying anything. Step 14: take candle 1 from countertop 1 You pick up the candle 1. Step 15: move candle 1 to countertop 1 You move the candle 1 to the countertop 1. ... Step 30: move cloth 3 to countertop 1 You move the cloth 3 to the countertop 1. [Result] FAILURE Total Steps: The agent never uses the clean command despite seeing it in help. Figure 7. Case study comparing UMEM enhanced agent and baseline. 16 Exploring... Wrong object Found cloth! Correct object Key Step Key Step Missing clean! Repeated Ignored clean Repeated Wrong object Stuck in take move loop 10+"
        }
    ],
    "affiliations": [
        "Alibaba International Digital Commerce",
        "Tongyi Lab, Alibaba Group",
        "Xiamen University"
    ]
}