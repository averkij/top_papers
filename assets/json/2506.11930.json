{
    "paper_title": "Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback",
    "authors": [
        "Dongwei Jiang",
        "Alvin Zhang",
        "Andrew Wang",
        "Nicholas Andrews",
        "Daniel Khashabi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and change their incorrect answers to correct ones. In this paper, we systematically investigate LLMs' ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 (with and without extended thinking). Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term FEEDBACK FRICTION. To mitigate this limitation, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We also perform a rigorous exploration of potential causes of FEEDBACK FRICTION, ruling out factors such as model overconfidence and data familiarity. We hope that highlighting this issue in LLMs and ruling out several apparent causes will help future research in self-improvement."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 0 3 9 1 1 . 6 0 5 2 : r FEEDBACK FRICTION: LLMs Struggle to Fully Incorporate External Feedback Dongwei Jiang Alvin Zhang Andrew Wang Nicholas Andrews Daniel Khashabi djiang21@jhu.edu Equal contribution bzhang90@jh.edu Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and change their incorrect answers to correct ones. In this paper, we systematically investigate LLMs ability to incorporate feedback by designing controlled experimental environment. For each problem, solver model attempts solution, then feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 (with and without extended thinking). Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, limitation that we term FEEDBACK FRICTION. To mitigate this limitation, we experiment with samplingbased strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We also perform rigorous exploration of potential causes of FEEDBACK FRICTION, ruling out factors such as model overconfidence and data familiarity. We hope that highlighting this issue in LLMs and ruling out several apparent causes will help future research in self-improvement. Figure 1: Top: Accuracy of various solver models when iteratively exposed to feedback of feedback model (GPT-4.1 mini) with access to ground-truth answers. The horizontal dotted line represents the target accuracy models could theoretically attain if they successfully incorporated all feedback (details in 4.1). Despite receiving high-quality feedback, solver models consistently plateau below their target accuracy. Bottom: Breakdown of questions that remained unsolved by the strongest solver model tested (Claude 3.7 Thinking) after multiple correction attempts. Feedback resistance, rather than feedback quality issues, is responsible for the majority of persistent errors. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "The prospect of self-improving Large Language Models (LLMs) has sparked both excitement and debate. Questions persist about LLMs inherent ability to self-improve without external guidance [1, 2], yet several studies have shown that LLMs can boost their performance when provided with accurate external feedback during test time without any parameter updates or training [35]. However, while prior studies establish the existence of performance gains from external feedback, the upper bounds of such improvement remains largely unexplored. This improvement capability would have far-reaching implications for applications such as scientific discovery [68] and complex planning [9, 10], where repeated refinement cycles with feedback are critical for success. This raises critical question: can models fully incorporate correct feedback to self-improve and reach their maximum potential? Answering this question is not straightforward, as self-improvement performance depends on two connected factors: feedback quality and the ability to incorporate feedback. To decouple these two factors, we create controlled experimental environment that provides near-optimal conditions for feedback incorporationone where models receive high-quality, targeted guidance based on complete ground-truth information. Figure 2 demonstrates our setup. The solver model attempts to solve problems iteratively, receives feedback from strong feedback model on each incorrect answer, and retries again for up to 10 consecutive iterations. The feedback generator has access to the complete history of all previous attempts and responses, enabling targeted guidance that addresses persistent errors. While this controlled environment with high-quality feedback provides an ideal testing ground, the effectiveness of feedback incorporation may depend significantly on the quality of the feedback. To assess them, we implement three increasingly sophisticated feedback mechanisms to determine how much feedback quality impacts model performance and whether any level of feedback enables models to reach target accuracy. Binary Correctness Feedback (F1) provides simple indication of correctness (e.g., the answer is wrong). Self-Generated Reflective Feedback (F2) has the model itself analyze potential errors using correct answers and available solution steps (e.g., you correctly set up the equation but made an error when computing the derivative...). Finally, Strong-Model Reflective Feedback (F3) uses more capable external model (GPT-4.1 mini) to generate feedback. Details of our evaluation framework and feedback generation process can be found in 2. We conduct systematic study using all three forms of feedback across strong frontier models including Llama-3.3-70B-Instruct, Llama-4-Scout-17B-16E-Instruct, Llama-4-Maverick-17B-128EInstruct-FP8, Claude 3.7 Sonnet, and Claude 3.7 Sonnet with Extended Thinking. We evaluate these models across diverse tasks, including AIME 2024, MATH-500, TriviaQA, PopQA, MMLU, MMLU Pro, GPQA, and two synthetic digit multiplication tasks. While higher-quality feedback does improve self-improvement performance, fundamental limitation persists. As shown in Figure 1, even with our best feedback mechanismStrong-Model Reflective Feedback (F3)models consistently fall short of the target accuracy (i.e., the accuracy of an ideal model if it successfully incorporated all the given feedback). We name this weakness FEEDBACK FRICTION (3.2). Our analysis in 4 reveals several key insights into FEEDBACK FRICTION. First, we categorize errors that persist after multiple feedback iterations and find that feedback resistance (i.e., models failing to incorporate clear and accurate feedback) is the dominant failure mode across all tasks (4.1). Second, we attempt to mitigate it by avoiding repeated wrong answers through sampling strategies. While performance improves across tasks, models still fall below their target accuracy (4.2). Finally, we investigate why models resist feedback (4.3). Our investigations rule out several apparent causes, including model confidence, reasoning complexity, and data familiarity (specifically, whether frequently occurring entities in pre-training data affect receptiveness to feedback). We also examine whether the same questions consistently challenge models, but surprisingly, different models struggle with different subsets. This suggests that resistance to feedback is not solely determined by the target datasets. In summary, having shown that state-of-the-art LLMs consistently resist external feedback, we then explore and reject the most apparent explanations for this phenomenon. 1 1Code for this work is available at: https://github.com/JHU-CLSP/Feedback-Friction 2 Figure 2: Iterative self-improvement loop. The process involves: (1) solver model generating an answer, (2) feedback model generating feedback given incorrect responses and the ground-truth correct answer, and (3) the solver attempting again with this feedback. This cycle repeats for up to 10 iterations or until correct answer is produced."
        },
        {
            "title": "2 A controlled framework to surface FEEDBACK FRICTION",
            "content": "Our framework employs two key components: an iterative self-improvement loop that allows models multiple opportunities to correct their mistakes (2.1), and spectrum of feedback mechanisms with varying levels of detail and guidance (2.2). 2.1 Setup for iterative self-improvement loop Given task with evaluation dataset = {(xi, yi)}m i=1 and evaluation metric , we establish an iterative improvement protocol with two distinct models: solver model Msolver and feedback generator model Mfeedback. For each input xi, the solver model produces an initial answer a1(xi) using the standard task prompt. We evaluate the correctness of this answer using (a1(xi), yi), where yi is the ground truth. If the answer is incorrect, the feedback generator Mfeedback creates targeted guidance g1 based on the current answer and ground-truth information. For iteration 1, we construct the prompt pk+1(xi) = concat(xi, historyk), where historyk contains all previous answers and feedback pairs {(a1, g1), (a2, g2), ..., (ak, gk)}. This process repeats for up to = 10 iterations or until the correct answer is generated. In our empirical experiments, we observed that performance improvements tend to plateau within 10 iterations, suggesting practical upper limit for the iterative refinement process. (cid:80)m i=1 1[f (ak(xi), yi) = 1], where 1[] is the indicator function. The overall accuracy for the dataset at iteration is measured as the fraction of all problems solved correctly: Acck = 1 By construction, since we iterate over the incorrect responses only, the accuracy sequence {Acc1, Acc2, ..., AccK} is monotonically non-decreasing, as we retain correct answers across iterations and only modify incorrect ones. This protocol, illustrated in Figure 2, provides controlled environment for measuring how effectively models incorporate feedback while maintaining consistent evaluation criteria throughout the improvement process. 2.2 Designing different feedback mechanisms for iterative self-improvement We investigate three distinct feedback mechanisms for the self-improvement process, each offering progressively greater guidance and error specificity. All mechanisms are designed to identify errors without directly revealing the correct answer, ensuring fair evaluation of the models ability to incorporate feedback. 3 Binary Correctness Feedback (F1) The simplest form provides only correctness information: binary (xi, yi) = The answer is wrong! if (a(xi), yi) = 0 where a(xi) is the solver models answer and is the evaluation function. Self-Generated Reflective Feedback (F2) response using available information: In this approach, the solver model analyzes its own self (xi, yi) = Msolver(concat(xi, a(xi), yi, si, pprompt)) where pprompt is the instruction: Please give me feedback on which solution step is wrong and how to get to the correct answer without revealing the answer. Here, si represents the ground-truth solution process when available. For datasets without detailed solutions, only the answer yi is provided. Strong-Model Reflective Feedback (F3) We employ more capable external model with access to the same information: strong 3 (xi, yi) = Mstrong(concat(xi, a(xi), yi, si, pprompt)) where Mstrong represents more powerful model than Msolver in providing feedbacks."
        },
        {
            "title": "3 Experimental results",
            "content": "In this section, we present comprehensive experimental results evaluating FEEDBACK FRICTION. We first describe our experimental setup, including tasks, prompts, inference setups, and model configurations (3.1). We then demonstrate how models consistently plateau below target performance regardless of the feedback mechanisms employed (3.2). 3.1 Experimental setup Tasks and metrics. We employ nine diverse tasks for evaluation, deliberately choosing objective tasks with clear ground-truth answers to ensure reliable evaluation of feedback incorporation, as using another LLM to evaluate more subjective tasks like instruction following or translation could lead to issues like reward hacking and unreliable assessments. Our tasks include: AIME 2024 [11] and MATH-500 [12] for mathematical problem-solving, TriviaQA [13] and PopQA [14] for knowledge reasoning, MMLU [15] and MMLU Pro [16] for multi-domain evaluation, GPQA [17] for complex scientific reasoning, and two synthetic digit multiplication tasks. The first synthetic task involves 5-digit multiplication (e.g., 78934 62851), while the second task applies hexadecimal multiplication rules to decimal numbers (i.e., first mapping 0-9 to themselves and 10-15 to their hexadecimal digits (10 A, ..., 15 ), then performing the arithmetic as if operating in base-16creating counterfactual [18] arithmetic setting that challenges models learned numerical reasoning patterns. For both tasks, ground-truth solutions are generated using deterministic templates that break down the multiplication into smaller multiplication and addition operations. We include these synthetic tasks specifically to remove potential confounding variables from semantic context (more details about these tasks can be found in Appendix C). Other than the two synthetic tasks, only AIME 2024, GPQA, and MATH-500 include complete solutions, while the others provide only final answers. For MMLU and MMLU Pro, the multiplechoice format raises concerns about whether models might solve problems through simple elimination strategies rather than genuine reasoning (e.g., selecting in the first iteration, in the second, in the third, etc). However, our analysis reveals that models exhibit surprising choice persistence even when provided with corrective feedback. Rather than systematically eliminating options or switching between choices, models often remain anchored to their initial selections (typically one or two specific answer choices) across multiple feedback iterations, even when that choice is demonstrably incorrect. We use the same prompts, few-shot demonstrations, answer parsing mechanism, and metrics from lm-evaluation-harness and llama-evaluate where applicable. For MMLU, MMLU Pro, PopQA, and TriviaQA, we sample 10% of the total data points, as running the full dataset through our 10-iteration improvement process would be prohibitively time-consuming. Our preliminary experiments confirmed that this subset yields performance metrics nearly identical to those obtained from the complete dataset. 4 Preventing answer leakages in feedback messages Beyond the feedback mechanisms discussed in 2.2, we implement safeguards to prevent information leakage by ensuring feedback does not directly reveal the final correct answer. Specifically, we add filter mechanism to mask out the final answer in the feedback only when the complete correct answer is detected through regex pattern matching. This targeted masking preserves the intelligibility of feedback while preventing direct answer revelation. For example, in mathematical tasks, we replace standalone numerical answers with [masked] (e.g., The final answer is [masked] instead of The final answer is 42) while preserving intermediate steps and reasoning guidance 2. This approach ensures that feedback remains informative and comprehensible while maintaining evaluation integrity. Detailed prompts for problem solving, feedback generation, and concrete examples of the masking process can be found in Appendix A. Models and inference. As for solver models, we employs strong models including LLaMA-3.3 70B Instruct [19], Llama-4-Scout-17B-16E, Llama-4-Maverick-17B-128E-Instruct-FP8 [20], Claude 3.7 Sonnet and Claude 3.7 Sonnet with extended thinking [21]. Claude 3.7 with extended thinking is variant that employs an extended reasoning before generating final responses, allowing the model to engage in more deliberate problem-solving through explicit step-by-step thinking. All models are instruction-tuned versions of their respective base models, specifically optimized for handling natural language instructions and maintaining consistent output formatting. For the Llama models, during inference, we use temperature 0 to ensure deterministic outputs and conduct inference using vLLM [22] with each models corresponding chat template. All inference is performed on single H100 instance equipped with eight 80GB GPUs. For Claude models, we access them through Anthropics API. For Claude 3.7, we use temperature 0 to ensure deterministic outputs, while for Claude 3.7 with extended thinking, we use temperature 1 as suggested by Claude. For feedback models, we use different models depending on the feedback type. For Strong-Model Reflective Feedback (F3), we utilize GPT-4.1 mini [23] as the feedback generation model. From our internal testing, GPT-4.1 minis feedback performance is on-par with Claude 3.7. Due to the higher cost of Claude 3.7, we use GPT-4.1 mini for generating the strongest feedback. We also considered o4-mini [24] as the feedback generator model due to its reportedly superior reasoning capabilities, but our experiments showed it delivered comparable feedback quality while incurring substantially higher computational costs, leading us to proceed exclusively with GPT-4.1 mini. 3.2 Main findings FEEDBACK FRICTION persists across model scales and tasks. Figure 3 shows results using our strongest feedback mechanism (Strong-Model Reflective Feedback (F3)) across all datasets. Our results reveal striking pattern: Despite receiving high-quality feedback, all solver models consistently plateau below their target accuracy, which is the accuracy of an ideal model if it successfully incorporates all the given feedback (see 4.1 for calculation details). While Claude 3.7 Thinking achieves the highest initial accuracy on several tasks (particularly AIME, TriviaQA, GPQA, and MATH-500), and Claude 3.7 shows competitive performance across most benchmarks, both Claude variants exhibit the same fundamental plateauing behavior as the Llama models. The performance typically improves rapidly through the first 2-4 iterations before significantly slowing down. This FEEDBACK FRICTION is particularly pronounced on complex reasoning tasks like AIME and GPQA, where even the best-performing models remain 15-25% and 3-8% below their respective theoretical ceilings despite 10 correction opportunities. Interestingly, Claude 3.7 Thinking shows strong initial performance on AIME (starting around 50% vs. 20-35% for other models) but still plateaus well below the target, suggesting that even sophisticated reasoning processes struggle with iterative feedback incorporation. The synthetic tasks reveal particularly interesting patterns across model families. In the standard 5-Digit Multiplication task, both Claude models reach near-perfect accuracy after significant initial improvement, outperforming the Llama models. However, the Hexadecimal-5-Digit-Multiplication task reveals extreme difficulty with feedback incorporation across all modelsno model exceeds 20% accuracy even after 10 iterations, highlighting severe limitations in feedback integration for counterfactual arithmetic systems. 3 2We verified that models do not attempt to predict the [masked] token during inference 3While it is true that models imperfect understanding of hexadecimal arithmetic leads to imperfect feedback quality, this is reflected in the lower theoretical ceiling shown in the figure. Even accounting for this limitation, models still fall short of what they could achieve if they fully incorporated the available feedback. 5 Figure 3: The performance of frontier models we tested with Strong-Model Reflective Feedback (F3) across nine different tasks. Models are given multiple attempts with feedback that incorporates both the final answer and complete solution (when available). The dotted line represents the target accuracy that models could theoretically achieve if they fully incorporated all feedback (details in 4.1). Results demonstrate that despite strong feedback, models consistently plateau below their target accuracy across all tasks. Feedback quality significantly impacts performance gains. Figure 4 illustrates how different feedback mechanisms affect performance across models and tasks. Due to the cost of running extensive experiments with Claude models, we focus the feedback quality analysis on the Llama model families. All tasks show clear benefits from increasingly sophisticated feedback, with consistent progression at the final iteration (K=10). The impact of high-quality feedback is most pronounced on complex reasoning tasks. For AIME, MMLU Pro, and GPQA, Strong-Model Reflective Feedback (F3) outperforms binary feedback by significant margins across all models. Llama-4-Maverick shows the strongest overall performance, achieving 73.3% accuracy on AIME and 96.5% on GPQA with Strong-Model Reflective Feedback (F3)-improvements of +26.7% and +10.6% over binary feedback, respectively. Llama-4-Scout demonstrates the largest relative gains, particularly on AIME (+33.3%) and GPQA (+13.1%), compared to Llama-3.3 which shows more modest improvements (+26.7% on AIME, +6.6% on GPQA). This variation in improvement rates suggests that newer and more powerful models may be more receptive to high-quality feedback. Nevertheless, despite these substantial improvements, all models still plateau significantly below their theoretical performance ceiling."
        },
        {
            "title": "4 Analysis of FEEDBACK FRICTION",
            "content": "We conduct deeper analysis to better understand FEEDBACK FRICTION. We first categorize different cases where models fail to correct their mistakes despite multiple rounds of feedback (4.1), then 6 Figure 4: Performance comparison across benchmark datasets using different feedback mechanisms with Llama-3.3, Llama-4-Scout and Llama-4-Maverick. Model performance progressively improves as feedback quality increases from Binary Correctness Feedback (F1) to Strong-Model Reflective Feedback (F3). examine the extent to which we can alleviate this problem with sampling strategies (4.2), and finally, we present several hypotheses and the experiments to understand FEEDBACK FRICTION (4.3). 4.1 Feedback integration failures dominate persistent self-improvement errors Error category development. We manually examine cases where LLMs fail to improve despite receiving high-quality feedback and identifies three main categories of error: (1) Most critically for our analysis, feedback resistance failures represent cases where models fail to accurately incorporate feedback despite multiple iterations. (2) Feedback quality issues encompass cases where the provided feedback is incorrect, ambiguous, or fails to address the key problematic steps in the solution. This can still occur because the generated feedback might miss crucial conceptual errors or introduce new inaccuracies, even though ground-truth is provided to the feedback generator. (3) We maintain an Other category for cases that dont clearly fit into either of the above categories. From our initial examination, this includes cases where the problem itself contains ambiguities or the solution is conceptually correct but fails due to style or formalization issues (e.g., providing the correct information but not in the expected format required by the evaluation metric). Dataset Table 1: Distribution of error categories (%) of unsolved problems after 10 iterations of self-improvement classified by o4-mini. FR: Feedback Resistance, FQ: Feedback Quality, OTH: other issues. Automated error categorization and validation. To systematically categorize errors at scale, we used OpenAI o4-mini as an automated annotator to classify complete selfimprovement trajectories from Claude 3.7 and Claude 3.7 Thinking according to these predefined categories. To validate this automated approach, we conducted rigorous manual verification by randomly sampling 50 errors from each task and having two human annotators independently label them according to our defined categories. Our verification showed 96% agreement between human annotators and o4-minis classifications, significantly higher than the 78% agreement rate achieved with GPT-4.1 mini on the same samples. This confirms o4-minis reliability for this analysis task. Claude 3.7 Claude 3.7 Thinking Claude 3.7 Claude 3.7 Thinking Claude 3.7 Claude 3.7 Thinking 64.6 62.8 100.0 85.7 72.4 71.7 100.0 100.0 28.0 30.8 0.0 14.3 25.0 28.3 0.0 0. 7.4 6.4 0.0 0.0 2.6 0.0 0.0 0.0 AIME 2024 Claude 3.7 Claude 3.7 Thinking Solver Model MMLU Pro TriviaQA GPQA OTH FQ FR Feedback resistance dominates error patterns. Table 1 presents the distribution of error categories across different tasks, as classified by o4-mini. As shown, feedback resistance is consistently the dominant category across all tasks, accounting for 62.8-100% of errors. This finding suggests that the core challenge in achieving perfect performance lies not in the quality of feedback or problem complexity, but in fundamental limitations of how models process and incorporate corrective feedback. Detailed examples of each error category are provided in Appendix B. 4.2 Mitigating FEEDBACK FRICTION with sampling strategies Given the persistent plateau in performance we observed across models and tasks, natural question arises: can we mitigate FEEDBACK FRICTION through existing strategies? We explore sampling techniques as potential solution to help models overcome their apparent resistance to feedback. Progressive temperature increases show limited effectiveness. We first explore temperaturebased sampling strategies where the sampling temperature increases with the iterations: 0.0 for iteration 0, 0.15 for iteration 1, 0.3 for iteration 2, and so forth. While other schedules (e.g., exponential increase, fixed higher temperature) could be explored, we chose this linear progression as simple baseline that gradually introduces diversity while preserving early deterministic behavior. We hypothesize that progressive temperature increases would help models generate more diverse outputs, allowing them to escape from local optima in their output distributions and become more receptive to feedback. Due to the cost of running extensive experiments with Claude models, we focus the feedback quality analysis on Llama-4-Scout and Llama-4-Maverick. As shown in Figure 5, this approach alone produced minimal improvements compared to the baseline in Figure 3. Analysis of logs revealed that while increased temperature successfully diversified model outputs, the additional exploration often failed to converge on correct answers due to the vast search space of possible responses. Figure 5: Results of using progressively increasing temperature and rejection sampling with Llama-4Scout and Llama-4-Maverick. Rejection sampling can provide additional improvements over temperature-based sampling alone across both multiple-choice and non-multiple-choice tasks. Combining temperature increases with rejection sampling yields better results. To enhance performance further, we implement more targeted approach that combines increased temperature with rejection sampling. This method explicitly forces the model to explore new solution paths while avoiding previous attempts. Specifically, we instruct the model to generate 25 answers, and remove final answers that occurred in previous iterations (which by construction is incorrect, since we only continue iterating on problems that remain unsolved). If no answer remains after this filtering process, we randomly select one from those 25 answers. Otherwise, we randomly select one of the remaining novel answers as the final prediction. As shown in Figure 5, the combined strategy yields substantive performance gains across both multiple-choice datasets and non-multiple-choice datasets compared to the baseline that only increases the temperature. While the magnitude of these improvements varied, the consistent pattern suggests that forcing models to explore new solution paths by rejecting previously used answers is beneficial regardless of the task format. Despite these gains, we observed that all datasets still fall short of the target accuracy, indicating that sampling strategies alone cannot fully resolve model resistance to feedback. 4.3 Understanding FEEDBACK FRICTION Our sampling strategies (4.2), though promising, did not eliminate FEEDBACK FRICTION entirely. Developing more effective interventions requires deeper understanding of the fundamental causes. In this section, we investigate several hypotheses for why models resist incorporating feedback despite multiple correction opportunities. Throughout our analyses, we provide error bars. They 8 represent the standard error of binomial proportion, calculated as (cid:112)acc (1 acc)/n, where is the number of samples in each evaluated group. Model confidence and FEEDBACK FRICTION Models with high confidence in their answers may resist feedback and correction, so does excessive confidence cause FEEDBACK FRICTION? To test this, we measure confidence using the average probability of tokens in the models output in the first round, following similar methodology that has been proven effective in information retrieval domains [25]. We conduct this experiment on 5-digit multiplication tasks to eliminate confounding variables from semantic context. The results shown in Figure 6 indicate no significant correlation between confidence metrics and final accuracy on Llama-4-Scout and Llama-4Maverick. Higher confidence in initial answers neither predicts poor performance after self-improvement iterations nor correlates with the magnitude of improvement. Additional detailed analysis is available in Appendix D, including experiments with more models and across wider range of datasets that further support this finding. Figure 6: Accuracy of Llama4 Scout (top) and Llama4 Maverick (bottom) on 5-digit multiplication. Data familiarity and FEEDBACK FRICTION Prior work [14, 26] suggests that language models perform better with familiar entities and topics encountered frequently during training. Are these models more resistant to feedback about familiar entities? We investigated whether this familiarity bias contributes to FEEDBACK FRICTION using PopQAs popularity metrics as proxies for entity familiarity. This dataset includes monthly Wikipedia page views for subject entities (s_prop) and object entities (o_prop). We then analyzed how accuracy changes during iterative self-improvement correlate with the popularity of the subject (s_prop) and object (o_prop) entities. We use Llama-3.3 for this experiment since it was released closer to the publication of PopQA. These popularity metrics provide particularly relevant measure of potential training data frequency for this model. As shown in Figure 7, we found no consistent pattern between entity popularity and accuracy. We provide further supporting evidence in Appendix with additional statistical testing and alternative popularity metrics. Figure 7: Accuracy of Llama3.3 on PopQA with respect to s_pop and o_pop Further analyses We also explored whether problem complexity, as measured by the number of expected reasoning steps, correlates with FEEDBACK FRICTION since prior works [27] show that longer reasoning trajectories may impede problem-solving in LLMs. Additionally, we investigated whether certain questions consistently induced stubbornness across different models because some questions may inherently be more difficult to answer. For both hypotheses, our controlled experiments yielded negative results, with minimal correlation between reasoning complexity and stubbornness, and little overlap in stubborn questions across models. Complete analyses are provided in Appendix and Appendix G."
        },
        {
            "title": "5 Related Work",
            "content": "Self-Improvement with LLMs. Self-improvement in artificial intelligence has evolved significantly over time, with roots predating the current LLM era. Early work explored using Generative Adversarial Networks (GANs) to enable NLP systems to improve through self-generated feedback [28, 29]. The emergence of LLMs has dramatically expanded both the scope and capabilities of 9 self-improvement techniques. Modern applications span diverse domains including code generation [30], reasoning tasks [31, 32], instruction following [33, 34], and many others [3537]. Approaches to achieve self-improvement vary in their focus - some concentrate on training time improvements, where models learn to self-improve [38] or generate additional training data [39, 40], while others emphasize inference time improvements, often incorporating feedback mechanisms [3, 31] though sometimes utilizing other models without explicit feedback [41]. While several studies cast doubt on whether off-the-shelf LLMs possess the ability for intrinsic self-improvement (i.e., the ability to self-improve without using any external ground-truth feedback) [1, 2], there is consensus that LLMs can self-improve when such feedback is available [5, 42]. In this work, we probe the limits of self-improvement with external ground-truth feedback and investigate what prevents LLMs from fully integrating feedback. The elasticityplasticity dilemma LLMs often face an elasticityplasticity dilemmaa tradeoff between retaining prior knowledge and integrating new information across various adaptation In continual learning, LLMs exhibit catastrophic forgetting, as new knowledge can scenarios. overwrite or interfere with old knowledge, especially when updates conflict with the models existing beliefs [4345]. Similarly, in model editing, handful of targeted edits can successfully inject new facts, but beyond only few edits the models performance on unrelated queries and general benchmarks deteriorates, indicating that extensive edits irreversibly distort the models broader knowledge network [46, 47]. Finally, alignment fine-tuning techniques such as instruction tuning and reinforcement learning from human feedback (RLHF) encounter similar dilemma: while they instill desired behaviors, the tuned models often remain elastically tied to their pre-trained behavior distribution or can be coerced (via adversarial prompts) to revert to undesirable outputs, suggesting that alignment can be brittle or superficial [48, 49]. In this paper, we revisit this dilemma through the lens of feedback integration during self-improvement. We also try to unveil the factors that control this dilemma through analysis. Feedback for LLMs. Prior work has explored various approaches for providing feedback to LLMs during self-improvement processes. Feedback mechanisms can be broadly categorized into intrinsic feedback, where LLMs evaluate their own outputs through prompting [5053], and extrinsic feedback leveraging additional tools [5456], information sources [57, 58], or even ground-truth answer [3, 59]. While research demonstrates that generating correct feedback is the key for LLM self-improvement [5, 60], challenges remain in how effectively LLMs incorporate this feedback. Studies suggest that LLMs may struggle to accept new information that contradicts their prior knowledge [25], handle refuting instructions [61], or may be led astray by misleading feedback [62, 63]. Different from previous work, we focus specifically on high-quality and even perfect feedback, investigating why LLMs fail to fully incorporate such helpful feedback."
        },
        {
            "title": "6 Limitations",
            "content": "Better understanding of FEEDBACK FRICTION While our study successfully demonstrates the existence of FEEDBACK FRICTION and rules out several potential causes (model confidence, data familiarity, reasoning complexity), we lack definitive mechanistic explanation for why models resist incorporating feedback. better understanding of FEEDBACK FRICTION likely involves complex interactions between feedback understanding, instruction following, and belief updating. Future work would benefit from more sophisticated mechanistic interpretability techniques, such as causal intervention methods and circuit analysis [64, 65], to understand the specific computational pathways through which feedback resistance emerges and persists across model architectures. Limited mitigation strategies Despite extensive experimentation with sampling strategies, our attempts to fully mitigate feedback friction yielded only modest improvements. The most promising next step appears to be supervised fine-tuning or reinforcement learning approaches that could (1) enhance the solver models receptiveness to feedback incorporation, and (2) improve the feedback generators ability to provide more effective guidance [66]. However, the computational constraints of our experimental setup, particularly the large model sizes we tested (including 70B+ parameter models like Claude 3.7 and Llama-4-Maverick) and the associated computational costs, prevented us from conducting comprehensive fine-tuning experiments that might meaningfully address feedback resistance."
        },
        {
            "title": "7 Conclusion",
            "content": "Our study reveals fundamental limitation in LLMs ability to incorporate external feedback. Despite receiving high-quality feedback over multiple iterations, models consistently plateau below their theoretical performance ceiling across diverse reasoning tasks. We investigated few sampling strategies that mitigated this issue, but did not eliminate this stubborn plateau. Despite extensive analysis, the precise mechanisms underlying feedback resistance remain elusive, which we leave to future work. Understanding and addressing these limitations remain essential for developing more adaptable AI systems capable of genuine and sustained self-improvement."
        },
        {
            "title": "Acknowledgment",
            "content": "This work is supported by ONR grant (N00014-241-2089). The GPUs were provided by the DSAI and ARCH clusters. We sincerely thank the broader JHU community for discussions and feedback."
        },
        {
            "title": "References",
            "content": "[1] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet, 2023. URL https://arxiv.org/abs/2310.01798. [2] Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir, Benjamin Van Durme, and Daniel Khashabi. Self-[in]correct: Llms struggle with discriminating self-generated responses, 2024. URL https://arxiv.org/abs/2404.04298. [3] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. In NeuralPS, 2023. URL https://arxiv.org/abs/ 2303.11366. [4] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models, 2023. URL https://voyager.minedojo.org/assets/documents/voyager.pdf. [5] Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, and Victor Carbune. Llms cannot find reasoning errors, but can correct them! CoRR, 2023. URL https://arxiv.org/abs/2310. 01798. [6] Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? large-scale human study with 100+ nlp researchers, 2024. URL https://arxiv.org/abs/ 2409.04109. [7] Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike Darcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen tau Yih, Pang Wei Koh, and Hannaneh Hajishirzi. Openscholar: Synthesizing scientific literature with retrieval-augmented lms, 2024. URL https://arxiv.org/abs/2411.14199. [8] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery, 2024. URL https: //arxiv.org/abs/2408.06292. [9] Zhehua Zhou, Jiayang Song, Kunpeng Yao, Zhan Shu, and Lei Ma. Isr-llm: Iterative selfrefined large language model for long-horizon sequential task planning, 2023. URL https: //arxiv.org/abs/2308.13724. [10] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: benchmark for real-world planning with language agents, 2024. URL https://arxiv.org/abs/2402.01622. [11] HuggingFaceH4. Aime 2024 dataset. https://huggingface.co/datasets/ HuggingFaceH4/aime_2024, 2024. 11 [12] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. [13] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In ACL, 2017. URL https: //arxiv.org/abs/1705.03551. [14] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. In Annual Meeting of the Association for Computational Linguistics (ACL), 2023. URL https://arxiv.org/abs/2212.10511. [15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2020. [16] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024. URL https://arxiv.org/abs/2406. 01574. [17] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. [18] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks, 2024. URL https://arxiv. org/abs/2307.02477. [19] Meta AI. Llama 3. https://www.llama.com/models/llama-3/, 2024. [20] Meta AI. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai.meta.com/blog/llama-4-multimodal-intelligence/, 2025. [21] Anthropic. Claude 3.7 sonnet and claude code. https://www.anthropic.com/news/ claude-3-7-sonnet/, 2024. [22] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. URL https://arxiv.org/abs/2309. 06180. [23] OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2024. [24] OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/, 2024. [25] Kevin Wu, Eric Wu, and James Zou. Clasheval: Quantifying the tug-of-war between an llms internal prior and external evidence, 2024. URL https://arxiv.org/abs/2404.10198. [26] R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths. Embers of autoregression: Understanding large language models through the problem they are trained to solve. CoRR, 2023. URL https://arxiv.org/abs/2309.13638. [27] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. Faith and fate: Limits of transformers on compositionality, 2023. URL https://arxiv.org/abs/2305.18654. [28] Sandeep Subramanian, Sai Rajeswar, Francis Dutil, Chris Pal, and Aaron Courville. Adversarial generation of natural language. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 241251. Association for Computational Linguistics, 2017. URL https: //aclanthology.org/W17-2629/. 12 [29] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. arXiv preprint arXiv:1609.05473, 2017. URL https://arxiv. org/abs/1609.05473. [30] Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (stop): Recursively self-improving code generation, 2024. URL https://arxiv.org/abs/ 2310.02304. [31] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. CoRR, 2023. URL https://arxiv.org/abs/2303.17651. [32] Deepak Nathani, David Wang, Liangming Pan, and William Yang Wang. Maf: Multi-aspect feedback for improving reasoning in large language models, 2023. URL https://arxiv. org/abs/2310.12426. [33] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning Language Model with Self Generated Instructions. In Annual Meeting of the Association for Computational Linguistics (ACL), 2023. URL https://arxiv.org/abs/2212.10560. [34] Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, May 2023. URL https://kaistai.github.io/SelFee/. [35] Pinzhen Chen, Zhicheng Guo, Barry Haddow, and Kenneth Heafield. Iterative translation refinement with large language models, 2024. URL https://arxiv.org/abs/2306.03856. [36] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with \"gradient descent\" and beam search, 2023. URL https://arxiv. org/abs/2305.03495. [37] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: hallucination correction for multimodal large language models. Science China Information Sciences, 67(12), December 2024. ISSN 1869-1919. doi: 10.1007/s11432-024-4251-x. URL http://dx.doi.org/10.1007/ s11432-024-4251-x. [38] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning, 2024. URL https://arxiv.org/abs/2409.12917. [39] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play finetuning converts weak language models to strong language models. CoRR, 2024. URL https: //arxiv.org/abs/2401.01335. [40] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. CoRR, 2024. URL https://arxiv.org/ abs/2401.10020. [41] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In International Conference on Learning Representations (ICLR), 2023. URL https://arxiv.org/abs/2211.00053. [42] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies, 2023. URL https://arxiv.org/abs/2308.03188. [43] Simone Clemente, Zied Ben Houidi, Alexis Huet, Dario Rossi, Giulio Franzese, and Pietro Michiardi. In praise of stubbornness: The case for cognitive-dissonance-aware knowledge updates in llms. arXiv preprint arXiv:2502.04390, 2025. 13 [44] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747, 2023. [45] Yifei Ming, Senthil Purushwalkam, Shrey Pandit, Zixuan Ke, Xuan-Phi Nguyen, Caiming Xiong, and Shafiq Joty. Faitheval: Can your language model stay faithful to context, even if \"the moon is made of marshmallows\", 2025. URL https://arxiv.org/abs/2410.03727. [46] Qi Li, Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Xinglin Pan, and Xiaowen Chu. Should we really edit language models? on the evaluation of edited language models. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [47] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. Memory-based model editing at scale. In International Conference on Machine Learning (ICML), 2022. [48] Jiaming Ji, Kaile Wang, Tianyi Qiu, Boyuan Chen, Jiayi Zhou, Changye Li, Hantao Lou, Josef Dai, Yunhuai Liu, and Yaodong Yang. Language models resist alignment: Evidence from data compression. arXiv preprint arXiv:2406.06144, 2024. [49] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. [50] Aman Madaan, Niket Tandon, Prakhar Gupta, Gabriel Ilharco, Siddharth Singh, Tushar Khot, Hannaneh Hajishirzi, Wen-tau Yih, and Yih Tau. Self-refine: Iterative refinement with selffeedback. arXiv preprint arXiv:2303.17651, 2023. [51] Shehzaad Dhuliawala, Tushar Khot, Ashish Sabharwal, and Peter Clark. Chain of verification: How large language models perform reasoning with external tools. arXiv preprint arXiv:2305.00053, 2024. [52] Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, and Meng Jiang. Large language models can self-correct with key condition verification, 2024. URL https://arxiv. org/abs/2405.14092. [53] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation, 2023. URL https://arxiv.org/abs/2307.03987. [54] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing, 2024. URL https://arxiv.org/abs/2305.11738. [55] Shuyang Jiang, Yuhao Wang, and Yu Wang. Selfevolve: code evolution framework via large language models, 2023. URL https://arxiv.org/abs/2306.02907. [56] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug, 2023. URL https://arxiv.org/abs/2304.05128. [57] Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. Verify-and-edit: knowledge-enhanced chain-of-thought framework. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 58235840, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.320. URL https://aclanthology.org/2023.acl-long.320/. [58] Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. Improving language models via plug-and-play retrieval feedback, 2023. URL https://arxiv.org/abs/ 2305.14002. [59] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks, 2023. URL https://arxiv.org/abs/2303.17491. 14 [60] Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When can llms actually correct their own mistakes? critical survey of self-correction of llms, 2024. URL https: //arxiv.org/abs/2406.01297. [61] Jianhao Yan, Yun Luo, and Yue Zhang. Refutebench: Evaluating refuting instruction-following for large language models, 2024. URL https://arxiv.org/abs/2402.13463. [62] Rongwu Xu, Brian S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, and Han Qiu. The earth is flat because...: Investigating llms belief towards misinformation via persuasive conversation, 2024. URL https://arxiv.org/abs/2312. 09085. [63] Boshi Wang, Xiang Yue, and Huan Sun. Can chatgpt defend its belief in truth? evaluating llm reasoning via debate, 2023. URL https://arxiv.org/abs/2305.13160. [64] Yuqing Yang and Robin Jia. When do llms admit their mistakes? understanding the role of model belief in retraction, 2025. URL https://arxiv.org/abs/2505.16170. [65] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt, 2023. URL https://arxiv.org/abs/2202.05262. [66] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. Retroformer: Retrospective large language agents with policy gradient optimization, 2024. URL https://arxiv.org/abs/2308.02151. [67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. CoRR, 2023. URL https: //arxiv.org/abs/2306.05685."
        },
        {
            "title": "A Prompts and evaluation details for problem solving and feedback",
            "content": "generation A.1 Prompts used for the solver model We developed different prompting strategies for the solver model to incorporate feedback and generate new solutions across iterations. The system prompts in Figure 8 were used for the solver model across different tasks: At the initial generation round, we directly use the question as the prompt for the solver model. For multiple-choice questions, we format the question by concatenating the question and answer choices with their corresponding labels (i.e., to for MMLU and GPQA, to for MMLU Pro). In subsequent rounds, we provide the solver model with its complete previous history and the corresponding feedback from the feedback generator model. We clearly label each iteration so the solver model can track all its previous attempts. The general template for the iterative prompt structure is provided in Figure 9. A.2 Evaluation details for problem solving We employ few-shot prompting across all tasks to provide consistent context for the solver model. For TriviaQA and PopQA, we randomly sample 5 questions without replacement as few-shot examples. For MMLU and MMLU Pro, we similarly sample 5 questions from the corresponding question category to ensure domain-relevant examples. For PopQA, we employ an LLM-as-a-judge approach [67] to assess answer correctness. This is necessary because PopQA provides limited answer aliases (extensive alternative phrasings for exact string matching) compared to TriviaQA. Without this approach, models would be penalized for minor formatting differences rather than genuine comprehension errors, leading to an underestimation of their true problem-solving capabilities. For other tasks, we follow the same evaluation metrics provided by lm-eval-harness. 15 Figure 8: System prompts used for the solver model across all tasks. Figure 9: Prompt used for iterative self-improvement A.3 Prompts used for feedback generation We implement three distinct feedback generation strategies as described in 2.2. For Binary Correctness Feedback (F1), we provide minimal information: Your answer was incorrect. Please answer the question again. For Self-Generated Reflective Feedback (F2) and Strong-Model Reflective Feedback (F3), we employ identical prompt templates that differ only in the model used for generation. The feedback generator receives the complete interaction history, including all previous solver attempts and corresponding feedback. When available, we provide the feedback model with detailed solution explanations that justify the correct answer; for datasets lacking such explanations, we provide only the ground truth answer. This approach ensures the feedback model has sufficient context to generate targeted, informative guidance while maintaining consistency across feedback types, and its template is shown in Figure 10. A.4 Answer masking in feedback To ensure fair evaluation, we implement comprehensive answer masking to prevent feedback from directly revealing ground truth solutions while preserving feedback quality. Our approach allows feedback to contain detailed solution steps and guidance but strictly prohibits explicit disclosure of final answers. We use [masked] as the replacement token for filtered content. 16 Figure 10: Prompt used for generating the feedback. Multiple-choice questions. We mask all possible representations of the correct choice letter. For example, if the correct answer is A, we filter variants including (A), boxed{A}, **A**, etc. Open-ended questions. For TriviaQA, we filter all terms matching the words in aliases and normalized aliases answer fields. For PopQA, we mask entries from the possible answers answer field. For mathematical tasks (5-digit multiplication and MATH-500), we mask standalone numerical answers and those in boxed{} notation. Hexadecimal multiplication follows similar patterns. For multiplication tasks, we additionally mask intermediate partial products to prevent reduction to simple addition problems (detailed in Appendix C). A.5 Error Categorization Prompt The prompt template used for categorizing persistent model errors after 9 iterations is shown in Figure 11. Error categorization examples from iterative self-improvement This section presents representative examples of persistent errors that prevent models from achieving correct solutions despite multiple feedback iterations. We illustrate the main error categories identified in our analysis: feedback resistance (where models fail to incorporate valid corrections, see Figure 12) and feedback quality issues (where the provided guidance is incorrect or misleading, see Figure 13)."
        },
        {
            "title": "C Synthetic digit multiplication task details",
            "content": "C.1 5-digit multiplication We construct controlled arithmetic dataset consisting of 450 5-digit multiplication problems following the template: Calculate the following question: 19365 12534. Feedback generation We employ deterministic, human-designed template based on the distributive property to generate ground truth solutions. This template systematically decomposes each multiplication into partial products, providing clear step-by-step solution pathway. Our templatebased approach serves two key purposes: (1) demonstrating structured problem decomposition strategies for complex arithmetic, and (2) ensuring feedback correctness and interoperability. In Figure 14, we illustrate an example template solution, which serves as the reference for feedback Figure 11: Error Categorization Figure 12: Llama-4-Scout resisting feedback from GPT-4.1 mini in MMLU generation. The feedback model compares solver outputs against this structured breakdown to identify specific computational errors. Answer masking strategy To maintain task difficulty, we also mask intermediate partial answers before providing feedback to the solver model. The final feedback combines the masked template solution with model-generated guidance tailored to the specific errors observed. C.2 Hexadecimal 5-Digit Multiplication We also extend our synthetic arithmetic evaluation to hexadecimal multiplication, creating problems that challenge models ability to work with non-standard number systems. The question tem18 Figure 13: Wrong feedback provided by GPT-4.1 mini judged by o4-mini Figure 14: Templates for 5-digit multiplication solution. plate follows the format: Calculate the following question, where each number is represented in base 16: 69837 17635. All answers are expected in base-16 format. Template-based feedback. Similar to decimal multiplication, we generate feedback using deterministic step-by-step templates. The multiplication process involves sequentially multiplying the first operand by each digit of the second operand (interpreting digits in base-16), then summing the resulting partial products with appropriate positional shifts. Figure 15 demonstrates an example template solution. We validate solution correctness by verifying that partial product summation matches results from standard base-16 calculators. Masking strategy for hexadecimal multiplication. Given the increased computational complexity of hexadecimal arithmetic, we employ more permissive masking approach. We mask only the final summation step while preserving intermediate partial products. This design balances providing sufficient guidance with maintaining the core challenge of hexadecimal computation."
        },
        {
            "title": "D Analysis of model confidence and FEEDBACK FRICTION",
            "content": "Figure 16 presents confidence-accuracy relationships across four datasets: GPQA, MMLU, MMLU Pro, and TriviaQA. Each plot displays three metrics: initial accuracy at iteration 0, final accuracy after iterative feedback, and the improvement delta between them. 19 Figure 15: Hexadecimal 5-Digit Multiplication process solution. We define models initial confidence in its generated answer as the exponential of the average log-probability per token in the answer sequence: Initial Confidence = exp (cid:32)"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:33) log p(at a<t, q) where: is the number of tokens in the generated answer (with EOS excluded), at is the t-th token in the answer, a<t denotes the prefix of the answer up to (but not including) position t, is the input question, p(at a<t, q) is the models probability of generating token at given the previous tokens and the input. This corresponds to the geometric mean of the per-token probabilities assigned by the model, and serves as quantitative measure of the models confidence in its complete answer. We find that initial confidence strongly predicts initial accuracy across all datasets. Higher confidence bins consistently correspond to higher initial performance (Figures 16a16d), confirming that the models confidence is good predictor of its initial accuracy. However, confidence poorly predicts improvement potential. The relationship between initial confidence and accuracy gains varies substantially: GPQA shows peak improvements at moderate confidence levels, with diminishing returns at higher confidence MMLU and MMLU Pro exhibit relatively flat improvement patterns with the less confident questions getting more improvements. Nevertheless, this may caused by the low initial accuracy. TriviaQA demonstrates erratic improvement fluctuations regardless of initial confidence"
        },
        {
            "title": "E Analysis of data familiarity and RIGID THINKING",
            "content": "After analyzing data familiarity using answer frequency in the PopQA dataset, we found no clear correlation between model performance and frequency of answer words in the training data. However, surface-level frequency may not fully capture models true familiarity with content, as it fails to account for context quality, semantic relationships, and other factors affecting knowledge acquisition during pre-training. To better capture actual familiarity, we examine more direct behavioral signal called in-domain performance: the models accuracy in answering questions, measured using 100 generations per question with Llama-3.3. This behavioral familiarity metric reflects the cumulative effect of all factors contributing to the models internalized knowledge. 20 (a) GPQA confidence vs. accuracy (b) MMLU confidence vs. accuracy (c) MMLU Pro confidence vs. accuracy (d) TriviaQA confidence vs. accuracy Figure 16: Confidence vs. accuracy across different datasets using GPT-4.1 mini as feedback model and Llama-4-Scout as the solver. Figure 17 illustrates the in-domain performance of Llama-3.3 across four benchmarksGPQA, TriviaQA, 5-digit multiplication, and MMLU Pro. We bucket questions based on the models initial accuracy and report both initial and final accuracy after iterative feedback. While the model shows improvement across all buckets, questions with higher behavioral familiarity (higher initial accuracy) consistently achieve higher final accuracy as well. This suggests that behavioral familiarity is more informative predictor of both current performance and improvement potential than answer frequency alone. Nevertheless, we still cannot obverse any consistent patterns across all these datasets in the initial vs. final accuracy."
        },
        {
            "title": "F Analysis of reasoning complexity and FEEDBACK FRICTION",
            "content": "To investigate whether the models improvement over iterations correlates with question difficulty or the reasoning complexity, we compare the performance of Llama-4-Scout on two synthesized multiplication tasks: 5-digit and 6-digit problems. Unlike prior datasets, which lack clear separability in difficulty levels, these tasks were manually constructed with 450 questions each to ensure well-defined difference in complexity. The initial accuracy of Llama-4-Scout is 2.2% on 5-digit multiplication and 0.889% on 6-digit multiplication. Interestingly, while the 6-digit task is objectively more difficult, we observe greater improvement across iterations compared to the 5-digit task. One possible explanation is that more difficult tasks offer more room for feedback-driven correction because solver model has less initial knowledge about how to solve them. However, we also observe cases where simpler questions yield higher final accuracy, suggesting that the relationship between task complexity and feedback effectiveness is non-monotonic and influenced by additional factors. 21 (a) GPQA in-domain performance (b) TriviaQA in-domain performance (c) 5-digit multiplication in-domain (d) MMLU Pro in-domain performance Figure 17: In-domain accuracy of Llama-3.3 across four benchmark tasks. Figure 18: Comparison of the Improvement for 5-digit and 6-digit multiplication with GPT-4.1 mini as feedback model"
        },
        {
            "title": "G Analysis of model type and FEEDBACK FRICTION",
            "content": "To better understand the overlap in failure cases among Llama-3.3, Llama-4-Scout, and Llama-4Maverick, we compared their incorrect predictions across several benchmark datasets. Specifically, we report the number of shared mistakes between each pair of models, the number of questions all three models got wrong, the total number of unique mistakes (union), and the Overlap Ratio, defined as the proportion of all-three common errors to the total number of distinct errors. The Overlap Ratio offers normalized measure of agreement in model failures. Notably, AIME shows the highest overlap (35.7%), suggesting subset of examples that all three models consistently 22 struggle with. Conversely, datasets such as GPQA and 5-digit Multiplication exhibit minimal overlap (6.9% and 0.7%, respectively), indicating that the models tend to fail on different questions. These findings suggest that model failures are often idiosyncratic rather than being concentrated around universally difficult subset of examples. The relatively low overlap across datasets highlights the challenge of achieving robust self-correction: errors are not easily attributable to common set of pitfalls, but rather reflect distinct weaknesses in each models reasoning and generalization. Dataset L3.3Scout L3.3Maverick ScoutMaverick All-Three Union Overlap Ratio AIME TriviaQA MATH-500 MMLU MMLU Pro GPQA 5-digit Mult. 8 22 18 15 49 3 21 9 16 14 12 40 5 3 5 28 11 19 43 2 1 5 14 9 11 30 2 1 14 105 64 55 163 29 0.357 0.133 0.141 0.200 0.184 0.069 0.007 Table 2: Pairwise and three-way common failure cases among Llama-3.3, Llama-4-Scout, and Llama4-Maverick across datasets. Overlap Ratio is computed as the number of questions all three models failed on divided by the union of all distinct failures."
        }
    ],
    "affiliations": []
}