{
    "paper_title": "SAMTok: Representing Any Mask with Two Words",
    "authors": [
        "Yikang Zhou",
        "Tao Zhang",
        "Dengxian Gong",
        "Yuanzheng Wu",
        "Ye Tian",
        "Haochen Wang",
        "Haobo Yuan",
        "Jiacong Wang",
        "Lu Qi",
        "Hao Fei",
        "Anran Wang",
        "Zhuochen Wang",
        "Yujing Wang",
        "Cheng Chen",
        "Shunping Ji",
        "Xiangtai Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available."
        },
        {
            "title": "Start",
            "content": "SAMTok: Representing Any Mask with Two Words Yikang Zhou1,2 , Tao Zhang1,2 , Dengxian Gong1 , Yuanzheng Wu1 , Ye Tian2 , Haochen Wang2 , Haobo Yuan2 , Jiacong Wang2 , Lu Qi1 , Hao Fei3 , Anran Wang2 , Zhuochen Wang2 , Yujing Wang2 , Cheng Chen2 , Shunping Ji1 , Xiangtai Li2 Wuhan University1 ByteDance2 NUS3 : Equal contributions : Corresponding Author Project Page: https://zhouyiks.github.io/projects/SAMTok/"
        },
        {
            "title": "Abstract",
            "content": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available. Date: January 23, 2026 Correspondence: Xiangtai Li at xiangtai.li@bytedance.com Shunping Ji at jishunping@whu.edu.cn 6 2 0 2 2 2 ] . [ 1 3 9 0 6 1 . 1 0 6 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Multi-modal large language models [13, 7, 8, 21, 56, 57, 67, 69, 75, 100] have made significant progress in real-world applications, driven by rapid advancements in language models. Pixel-wise large language models [4, 30, 62, 68, 85, 89] have also drawn considerable research efforts, with most focusing on solving fine-grained visual language tasks, which are essential for building interactive intelligent systems. However, currently, building good pixel-wise MLLM with strong scalability faces four challenges: (1) There 1 Figure 1 SAMTok provides simple yet unified mask-token interface for MLLMs. (Left) SAMTok compresses region masks into two discrete tokens and faithfully reconstructs them across diverse visual domains. (Middle) Injecting these mask tokens into MLLMs enables wide range of region-level mask generation and understanding tasks. (Right) The text-based representation of region masks allows purely textual answer-matching reward for the GRPO of the mask generation task. are no unified designs to handle both mask-in and mask-out inputs. Existing models rely on complex regionlevel feature pooling designs [30, 62, 85, 87], while mask output depends on carefully designed segmentation decoders [25, 53, 73, 85, 93]. Although unified modeling can be achieved through alternative approaches such as bounding boxes or points [36, 37], this comes at the cost of reduced precision and introduced ambiguity. (2) Current state-of-the-art pixel-wise MLLMs [73, 85, 101] cannot directly and concisely apply reinforcement learning (RL) to mask generation tasks since they use continuous embeddings to connect the MLLM with the segmentation head. (3) Specially designed modules added for mask understanding and generation capabilities typically require co-training with the MLLM [53, 68, 73, 85, 93, 101]. In addition, the different training losses and forward pipelines introduce substantial complexity for scaling training with VQA and pure text data. (4) Meanwhile, several explorations [26, 66, 70, 71] attempt to circumvent these issues by treating masks as special image tokens or representing them as text in formats similar to RLE encoding or polygons. However, these typically incur enormous inference costs, with single mask being represented by dozens or even hundreds of tokens. Therefore, we pose one essential question: How can we non-intrusively endow base MLLMs (such as the QwenVL series) with pixel-wise capabilities, making the learning process as simple as VQA trainingrequiring only next-token prediction loss for supervised fine-tuning (SFT) and straightforward reinforcement learning (RL)?\" In this paper, we propose SAMTok, discrete mask tokenizer that tokenizes masks into textual special words (text tokens) and detokenizes these textual special words into masks, thereby transforming masks into new language for MLLMs to learn from, similar to regular text data. As shown in Fig. 1, our proposed SAMTok can convert diverse masks into textual special tokens and accurately reconstruct the corresponding masks. Through SAMTok, any MLLM can acquire powerful pixel-wise capabilities by learning like language data through supervised fine-tuning and reinforcement learning, without any additional architectural modifications or specialized loss design. SAMTok is initialized from the foundation segmentation model SAM2 [49] to accelerate convergence. We incorporate several additional components, including mask encoder and residual vector quantizer, to encode masks into compact, information-rich mask embeddings and to discretize the mask embeddings into two discrete tokens, respectively. To obtain strong mask generation capabilities, we train SAMTok on over 2 209M masks, drawn from diverse existing segmentation datasets. As result, SAMTok can tokenize any region masks and reconstruct them better as shown on the left of Fig. 1. To further validate SAMTok for pixel-understanding tasks, we collect approximately 5M diverse samples of mask-understanding and generation data. These data are converted into standard vision question answering format using SAMTok, which tokenizes masks as special text, thereby transforming them into conventional format containing only images and text. We use these data to perform supervised fine-tuning on the QwenVL series [3, 67] models. We are surprised to find that MLLMs can learn pixel-wise capabilities just like learning language, without any fancy design, as shown in the middle of Fig. 1. QwenVL-SAMTok achieves stronger performance or performance comparable to carefully designed expert MLLMs on mask understanding tasks, including region captioning and region VQA [30, 31, 87, 88], mask generation tasks, including grounded conversation generation [48], referring segmentation [19, 34, 42], and scene graph parsing [79], as well as interleaved mask understanding and generation tasks such as multi-round interactive segmentation [72]. Thanks to SAMToks efficient representation of masks as discrete textual special tokens, we can explore the upper bound of MLLM performance in mask generation using simple yet effective RL approach. Specifically, as illustrated in the right of Fig. 1, we propose textual answer-matching reward function for generated masks, which is better aligned with reward mechanisms in the language domain. After applying GRPO [51], QwenVL-SAMTok demonstrates significant improvements on the GRES [34] validation set, achieving gains of 8.9% in gIoU and 21.0% in N-acc, as well as on the GCG [48] validation set with improvements of 4.7% in AP50 and 6.6% in Recall. These results surpass the previous SOTA methods by 4.3% in gIoU, 8.3% in N-acc, 8.3% in AP50, and 8.4% in Recall. In summary, our contributions are as follows: We propose novel paradigm for MLLMs to model masks as new language, enabling them to learn mask understanding and generation capabilities just like natural language without requiring architecture modifications or additional loss design. We propose SAMTok, which can accurately achieve bidirectional conversion between masks and textual special tokens. Based on SAMTok, the QwenVL series of MLLMs acquire strong pixel-wise capabilities through next token prediction loss, achieving SOTA performance across dozens of diverse benchmarks. We design textual answer-matching reward function that enables MLLMs to perform reinforcement learning on mask generation tasks similar to natural language data, demonstrating significant performance improvements."
        },
        {
            "title": "2.1 SAMTok",
            "content": "An ideal region tokenizer should possess the following characteristics to better fit into LLM processing: (1) the ability to convert between 2D masks and latent region representations, enabling MLLMs to both input and output regions; (2) efficient and accurate region representations that minimize inference cost while maintaining precision; and (3) discrete region representations to facilitate reinforcement learning. Existing methods have demonstrated complementary strengths in different aspects: (1) Variational autoencoders (VAE) excel at the property of converting between images and latent representations. (2) Perception models like SAM excel at the property of accurately segmenting objects through single embedding. (3) Vector quantization methods excel at the property of effectively discretizing continuous latents into compact codes to facilitate RL training. SAMTok is mask VAE integrated with vector quantization, where the mask latent features are represented as single, highly condensed embedding mirroring the embedding characteristics of perception models. As shown in Fig. 2, SAMTok has an encoder fenc, vector quantizer with codebook C, and decoder fdec. Both SAMTok encoder fenc and decoder fdec are instantiated with SAM model, which includes an image backbone fimg, prompt encoder fprm, and mask decoder fmsk. The details of each component of SAMTok will be elaborated below. 3 Figure 2 Our SAMTok architecture (Left) and mask reconstruction examples (Right). SAMTok has encoder fenc, vector quantizer with codebook C, and decoder fdec. Both fenc and fdec are instantiated with SAM model, which includes an image backbone fimg, prompt encoder fprm, and mask decoder fmsk. Given an input image and region (e.g., the area outlined in purple), the SAMTok encoder fenc first encodes the 2D mask into mask embedding z, then performs two-stage quantization to obtain discrete mask embedding [e1, e2]. The SAMTok decoder fdec reconstructs the 2D mask ˆM from the original image and the regions discrete mask embeddings. SAMTok Encoder. Our encoder fenc is SAM model [24] with the final mask prediction head removed from the SAM mask decoder fmsk. Analogous to interactive segmentation, the SAM prompt encoder fprm encodes 2D mask into dense prompt embeddings that share the same spatial resolution as the image features encoded by SAMs image backbone fimg for the current image I. We add these dense prompt embeddings to the image features, then feed the result into the SAM mask decoder fmsk, where it interacts with pre-initialized mask embedding. At this point, we obtain the desired SAMTok encoder output: d-dimensional continuous mask embedding z: = fenc(I, M) = fmsk(fimg(I), fprm(M)) Rd (1) Quantizer. We employ residual quantization scheme [27] to discretize the continuous mask embedding z, since it can achieve high-fidelity quantization with compact codebook relative to other quantization schemes [27, 44, 58]. The continuous mask embedding is processed through two quantization steps, as shown in Eq. 2. First, we conduct nearest-neighbor lookup in the codebook for the continuous mask embedding and compute the residual embedding r1 between the continuous mask embedding and the retrieved latent embedding e1. Second, we perform nearest-neighbor lookup in the codebook for the residual embedding r1. By applying residual quantization, we achieve high-fidelity compression of the region mask, which can then be reconstructed by the SAMTok decoder. e1 = argmineC e2 2, r1 = e1, e2 = argmineC r1 e2 2, = [e1, e2] (2) SAMTok decoder. Our decoder fdec is full SAM model [24]. We treat the discrete mask embeddings [e1, e2] as special language prompt embeddings [25, 85] for the current image I, and require SAM to segment the corresponding region mask (Eq. 3). Inside the SAM prompt encoder fprm, we directly sum all discrete mask embeddings to form sparse prompt embedding. In the SAM mask decoder fmsk, the pre-initialized mask embedding interacts with the sparse prompt embedding via self-attention and with the image features via cross-attention, ultimately recovering the features of the continuous mask embedding z. Applying the mask prediction head then reconstructs the 2D region mask ˆM. ˆM = fdec(I, [e1, e2]) = fmsk(fimg(I), fprm([e1, e2])) (3) 4 Figure 3 Unified mask-token interface for MLLMs. For the mask understanding task (left to right), SAMTok first tokenizes region masks into quantization codes, then formats them into mask words, which are used in the MLLM prompt to refer to the corresponding image regions. For the mask generation task (right-to-left), the MLLM first produces mask words according to the instruction, then maps them to quantization codes, after which SAMTok reconstructs the 2D masks. Training of SAMTok. SAMTok is trained for the mask reconstruction task. We collect large-scale mask data from open-source datasets [24], totaling 209 million masks for training. The training objective follows [27], including reconstruction loss and commitment loss. The reconstruction loss is computed as the sum of the cross-entropy loss and the Dice loss between the reconstructed and original masks. = Lrecon + λLcommit, Lrecon = LCE(M, ˆM) + LDICE(M, ˆM), 2 + r1 sg(e2)2 2, Lcommit = sg(e1)2 (4) where sg() denotes the stop-gradient operator, and λ controls the quantization commitment."
        },
        {
            "title": "2.2 Unified Mask-Token Interface for MLLMs",
            "content": "Through SAMTok, any region mask can be converted into two discrete tokens. This enables MLLMs to understand and predict masks in text-based manner. Specifically, we treat mask tokens as new language and add mask special tokens to MLLMs vocabulary, equal in number to the SAMTok codebook size. Any region can then be represented as pair of mask special tokens for the MLLM to understand and generate. This paradigm enables MLLMs to learn region-mask understanding and generation capabilities through simple next-token prediction, just as they do with text. This training process no longer relies on any task-specific losses, such as the segmentation losses used in classical region-level MLLMs [25, 68, 85, 93], nor does it require any architectural modifications to the MLLM. Mask Understanding. SAMTok can tokenize masks of arbitrary image regions into compact, information-dense mask tokens. These mask tokens serve as efficient and accurate references to region masks for MLLM input. For example, in region captioning tasks, the input contains images, text instructions, and masks. The SAMTok encoder first encodes masks into quantization codes, which are then formatted into predefined special tokens. Then, we insert these special tokens into the text instruction, analogous to using text box but more efficient and easier for MLLMs to understand. Mask Generation. SAMTok can convert discrete mask tokens back into explicit segmentation masks. Consequently, MLLMs can achieve mask generation capabilities by predicting predefined special tokens, such as for referring segmentation tasks. When visualization of MLLM-generated masks is required during inference, the SAMTok decodes the mask special tokens in the response into segmentation masks. Specifically, the predicted special tokens are first converted into quantization codes, which are then used to retrieve the corresponding latent embeddings from the codebook. These latent embeddings are subsequently fed into the SAMTok decoder to decode the segmentation mask. Co-training of the MLLM. After SAMTok tokenizes all region masks into words, all mask-related tasks can be preprocessed into purely textual corpora, including mask-to-text, text-to-mask, interleaved textmask 5 Figure 4 Overview of dataset used to train the SAMTok (left) and MLLM (right). We use 209M masks to train SAMTok, and 5M conversations to fine tune MLLMs. generation, and interactive tasks (where masks serve as both inputs and outputs). In particular, we unify diverse set of taskssuch as grounded conversation generation (GCG), panoptic scene graph generation (PSG), region captioning, region-level VQA, generalized referring expression segmentation (GRES), and visual groundingunder single textual formulation. For example, to convert region captioning data into purely textual corpus, region masks are first transformed into mask words and inserted into the textual prompt, which is then paired with the corresponding region caption to form single dialog turn. Similarly, to preprocess GCG data, all region masks are converted into mask words and interleaved into the image caption immediately after the corresponding phrases, together with an instruction prompt, to construct unified conversational sequence. Consequently, under any multimodal training framework, all these tasks can be co-trained using the standard next-token prediction loss, without introducing any customized loss functions or architectural modifications."
        },
        {
            "title": "2.3 Reinforcement Learning for Mask Generation",
            "content": "The design of SAMTok is naturally compatible with RL, and we leverage RL to explore its upper bound. However, enhancing MLLMs mask generation capabilities through RL poses significant challenges [37, 82, 101]. In particular, many methods [25, 73, 85, 86] that pass segmentation embeddings can achieve strong performance via SFT, they struggle to benefit from further RL-based improvements due to their reliance on continuous features. Although several works [36, 37, 101] have employed RL by predicting bounding boxes or points as auxiliary signals, they depend on models like SAM [24] to convert these boxes or points into masks for reward computation, which substantially increases the overall system complexity. SAMTok successfully represents masks using special words, providing substantial benefits for RL: (1) The discrete textual representation of masks enables direct application of established RL algorithms. (2) Mask reward evaluation can be performed through simple character matching without requiring additional tools In this work, we adopt group relative policy optimization (GRPO) [51] as to extract explicit masks. the RL algorithm to optimize mask generation, as GRPO has been widely adopted by recent pixel-level MLLMs [36, 37, 66, 82, 101] in multimodal reinforcement learning settings. Specifically, during training, we first extract all special mask words from the models rollout response (as shown in the right part of Fig. 1). After removing duplicates, we check whether each mask word appears in the ground truth answer string. If it does, we count it as true positive (TP). We then use the ratio of TPs to the total number of mask words as the reward: Rmask = NTP/ max(Npred, Ngt), where NTP is the number of predicted true positive masks, Npred is the number of predicted masks without deduplication to penalize repetitive predictions, and Ngt is the number of ground truth masks. (5)"
        },
        {
            "title": "3 Experiment",
            "content": "For implementation details, please refer to Appendix Sec. B. For additional model experiments, please refer to Appendix Sec. C. For ablation studies, please refer to Appendix Sec. D. For further visualization results, 6 Table 1 Results on interleaved text-mask generation task (GCG). (ft) indicates models further finetuned on GCG after mixed training. The best results are bold and the second-best results are underlined (excluding our RL results). (rl) indicates models further trained with GRPO after mixed training. Method Size LISA [25] GLaMM [48] OMG-LLaVA [93] Sa2VA [85] Qwen25VL-SAMTok Qwen25VL-SAMTok (ft) Qwen3VL-SAMTok Qwen25VL-SAMTok Qwen25VL-SAMTok (ft) Qwen25VL-SAMTok (rl) 7B 7B 7B 8B 3B 3B 4B 7B 7B 3B Val Test METEOR CIDEr AP50 mIoU Recall METEOR CIDEr AP50 mIoU Recall 35.5 40.8 44.5 47.0 48.0 48.2 47.9 36.3 41.8 45.1 46.9 47.7 47.6 48. 25.2 30.8 29.9 33.2 36.8 37.0 37.8 38.2 24.8 29.2 28.6 32.2 36.1 36.3 37.4 37.0 62.0 66.3 65.6 67.7 71.7 71.7 72.1 72.6 33.9 47.2 41.2 49.5 51.8 54.7 49.5 54.8 13.0 16.2 14.9 16.4 16.9 17.3 16.1 17.2 12.9 15.8 14.5 16.2 16.7 17.0 16.1 17. 32.2 43.5 38.5 49.0 52.5 53.9 52.0 61.7 65.6 64.7 66.8 70.9 71.4 70.6 17.7 16.9 55.0 52.5 38.5 41.5 72.9 73. 49.8 53.5 17.4 16.5 54.5 53.7 51.0 71.7 71.2 73.2 37.5 40.4 48.7 53. Table 2 Results on multi-round interactive segmentation tasks: including MR-RefCOCO/+/g (object-level) and MR-PACO (part-level). The evaluation metric is cIoU. We report average cIoU across MR-RefCOCO/+/g. Method Size 7B LISA [25] 7B GLaMM [48] SegLLM [72] 7B Qwen25VL-SAMTok 3B 4B Qwen3VL-SAMTok Qwen25VL-SAMTok 7B MR-RefCOCO/+/g MR-PACO Round#2 Round#3 Round#4 Round#5 Round#6 Round#2 Round#3 Round#4 Round#5 48.7 52.7 72.3 77.8 20.5 41.9 51.7 54.1 59.2 78.7 79.9 21.3 40.9 49.2 18.7 39.4 46.4 55.4 56.3 76.6 80. 15.5 49.7 55.0 57.8 59.3 79.7 83.3 86.7 86.2 83.6 83.2 84.4 83.8 85.3 84. 58.7 58.4 51.1 48.8 50.8 48.2 54.0 52.9 52.3 54.2 74.6 77.1 78.5 80. Table 3 Results on text-to-mask task (GRES). (ft) indicates models further finetuned on GRES after mixed training. The best results are bold and the second-best results are underlined (excluding our RL results). (rl) indicates models further trained with GRPO after mixed training. Method Size LISA [25] SAM4MLLM [6] MLLMSeg [65] HiMTok [68] ARGenSeg [71] Qwen25VL-SAMTok Qwen25VL-SAMTok (ft) Qwen3VL-SAMTok Qwen25VL-SAMTok Qwen25VL-SAMTok (ft) Qwen25VL-SAMTok (rl) 7B 8B 8B 8B 8B 3B 3B 4B 7B 7B 3B Val Test Test Avg. 61.8 67.8 71.6 70.4 gIoU cIoU N-acc gIoU cIoU N-acc gIoU cIoU N-acc gIoU cIoU N-acc 52.2 61.6 63.3 71.9 70.4 75.1 72.1 74.7 58.2 70.5 72.9 77.1 65.3 74.7 68.2 74. 68.5 72.2 76.9 74.9 73.6 73.0 75.6 75.8 72.9 54.7 66.1 73.2 60.5 78.0 68.8 67.5 51.9 60.0 65.5 55.3 67.2 60.8 59.8 62.2 70.5 73.9 72.4 73.6 70.1 74.3 73.6 73.6 66.3 74.2 77.0 73.5 73.7 73.8 77.4 76.9 77.8 50.0 63.9 72.4 58.8 73.6 66. 72.4 72.1 68.9 71.1 71.7 70.8 58.8 65.3 69.7 71.7 63.6 67.8 72.3 60.6 63.4 68.5 78.0 79.4 79.5 81. 78.5 79.4 77.3 77.7 68.6 72.0 75.4 76.7 72.4 73.7 74.7 77. 72.0 70.4 65.4 66.5 68.3 68.2 67.7 69.8 72.4 65.9 68.3 69.1 68.5 69.6 71.3 72.2 68.2 71.2 71.0 71.4 72.1 73.7 77.4 75.9 77.7 please refer to Appendix Sec. E."
        },
        {
            "title": "3.1 Experiment Setup\nTraining Data. Our training process consists of three stages: (1) SAMTok training, (2) MLLM supervised\nfinetuning, and (3) reinforcement learning.",
            "content": "Tokenizer training data. To endow SAMTok with powerful mask compression and reconstruction capabilities, we collect diverse mask data [1315, 24, 28, 46, 49, 77, 80, 81, 98]. As shown in Fig. 4, these mask data encompass various scenarios, including indoor scenes, outdoor environments, and website user interfaces, spanning multiple granularities, including part-level, object-level, entity-level, and semantic-level annotations, 7 Table 4 Results on text-to-mask task (RES). The evaluation metric is cIoU. Table 5 Zero-shot results on text-to-mask task (GroundingSuite). The evaluation metric is gIoU. Size Method val 4B 82.4 Sa2VA [85] 3B 81.3 PaDT Pro [53] 3B 81.9 UniPixel [35] Qwen25VL-SAMTok 3B 82.4 4B 83.4 Qwen3VL-SAMTok 79.5 RefCOCO test test val 77.6 84.2 77.6 81.5 75.3 83.5 78.4 83.9 RefCOCO+ test test val 81.2 79.4 80.3 81.8 73.1 76.3 70.6 74. RefCOCOg test 79.7 80.4 78.1 78.5 77.2 78.5 79.0 79.1 85.0 80.2 83.4 76.6 80. 81.0 82.2 78.6 79.9 82.1 Size Stuff Part Multi Single All Method 7B 85.2 21.2 71.5 42.8 57.6 LISA [25] 7B 86.9 16.5 70.4 42.1 57.2 GLaMM [48] 1B 85.1 23.1 72.1 54.5 62.6 EVF-SAM [96] InstructSeg [74] 3B 56.2 24.2 66.8 51.3 52.5 Qwen25VL-SAMTok 3B 90.2 40.8 62.5 63.6 67.8 Table 6 Results on grounding task (REC). We de-tokenize the mask words generated by Qwen25VL-SAMTok into 2D masks and then derive bounding boxes for evaluation. Method Size val 89.1 RefCOCO test 91.7 test 84.0 val 82.4 RefCOCO+ test 88.0 RefCOCOg test 74.1 val 85.2 test 85.7 3B Qwen25VL [3] Qwen25VL-SAMTok 3B 92.7 (+3.6) 94.6 (+2.9) 89.7 (+5.7) 88.2 (+5.8) 92.2 (+4.2) 84.4 (+10.3) 89.9 (+4.7) 89.6 (+3.9) Qwen25VL [3] Qwen25VL-SAMTok 7B 93.0 (+3.0) 95.5 (+3.0) 90.5 (+5.1) 88.6 (+4.4) 93.2 (+4.1) 84.4 (+7.5) 90.8 (+3.6) 91.2 (+4.0) 85. 89.1 87.2 90.0 92.5 84.2 76. 87.2 7B totaling 209M masks. Supervised finetuning data. To enable the MLLM to develop deep understanding and generation capability for the discrete mask tokens produced by SAMTok, we collect large-scale dataset of interleaved mask-text data for supervised fine-tuning. As illustrated in Fig. 4, this supervised fine-tuning data comprises three components: (1) mask generation data, including grounding [54, 91, 102], referring segmentation [9, 23, 34, 42, 84], grounded conversation generation [48, 79, 83], instance segmentation [14, 64], and scene parsing generation [79] data; (2) region understanding data, including region captioning [30, 62] and region-based question answering [30, 62] data; (3) collaborative mask generation and understanding data, such as multi-turn interactive reasoning segmentation [72]. The supervised finetune dataset totals approximately 5M samples. All samples masks are pre-tokenized using SAMTok and reformatted into dialogue-style text sequences before training. RL data. To enhance the reasoning capability of MLLMs in mask generation tasks, we first generate 26k cold-start samples by prompting the Qwen3-VL-235B model to simulate chain-of-thought (CoT) reasoning. Additionally, 8k and 41k challenging samples are selected from the general referring segmentation dataset [34] and the grounded conversation generation dataset [48] for GRPO [51], respectively."
        },
        {
            "title": "3.2 Main Results\nInterleaved Text-mask Generation Tasks. We evaluate our model’s interleaved text–mask generation capability\non the grounded conversation generation (GCG) [48] benchmark. The GCG benchmark requires the model\nto describe an image while simultaneously generating region masks corresponding to the mentioned phrases.\nAs shown in Tab. 1, our method achieves state-of-the-art (SOTA) performance on this benchmark. On the\nvalidation set, the new SOTA improves over the previous best by +1.3% METEOR and +5.5% CIDEr in\ncaptioning metrics, and by +5.3% AP50, +5.2% mIoU, and +4.7% Recall in mask metrics. On the test set,\nsimilar consistent gains are observed, including +1.2% METEOR, +5.5% CIDEr, +5.3% AP50, +4.9% mIoU,\nand +4.2% Recall. These comprehensive improvements across both textual and visual dimensions demonstrate\nthat the proposed SAMTok enables more precise text–mask alignment, effectively bridging language and\npixel-level representations.",
            "content": "Multi-round interactive segmentation task. This task requires the model to reason about complex user intentions and segment objects in relation to previously identified entities across multiple interaction rounds. These interactions involve positional, interactional, and hierarchical relationships between objects, demanding the model to maintain long-term visuallinguistic consistency. We adopt MR-RefCOCO/+/g [72] and MRPACO [72] as evaluation benchmarks, which test the models ability to both embed masks in the input and generate masks in the output. Our proposed SAMTok provides exactly such unified mask-token interface for 8 Table 7 Results on mask-to-text task (DLC-Bench). This table reports region captioning performance on DLC-Bench, which evaluates the ability to generate accurate textual descriptions conditioned on given region masks. Method GPT-4o [21] o1 [22] Claude 3.7 Sonnet [1] Gemini 2.5 Pro [56] Qwen2.5VL [3] RegionGPT [17] OMG-LLaVA [93] VP-SPHINX [31] DAM [30] Qwen3VL-SAMTok Size 7B 7B 7B 13B 3B 4B Pos. 43.4 46.3 21.8 36.5 20.3 10.6 5.6 26.3 52.3 46. Neg. 79.6 78.8 50.4 75.2 62.2 46.4 32.6 71.6 82.2 85.2 Avg. 61.5 62.5 36.1 55.8 41.2 28.5 19.1 49.0 67.3 65.6 Table 8 Zero-shot region captioning results on MDVP-Bench across four different visual scene types: natural images, OCR-heavy documents, multi-panel layouts, and screenshots. The benchmark evaluates generalization to complex document-style and panel-based visual inputs without task-specific fine-tuning. Method Size Natural OCR Multi-Panel Screenshot Osprey [87] 7B PAM [32] 3B 3B DAM [30] Qwen3VL-SAMTok 4B 107.7 71.4 87.0 145.0 70.0 86.8 79.4 145. 81.3 84.5 76.4 109.6 99.4 94.3 127.7 127.3 MLLMs, enabling fine-grained spatial reasoning and compositional mask generation. As shown in Tab. 2, our model achieves new SOTA results on both benchmarks. On MR-RefCOCO/+/g benchmark, the new SOTA surpasses the previous best by an average of +7.7% across all interaction rounds, while on the MR-PACO benchmark, it yields an even larger average improvement of +10.7%. These substantial gains demonstrate that our approach not only reasons about inter-object relations but also effectively models hierarchical partwhole relationships within objects, maintaining consistent segmentation quality through multi-turn interactions. Text-to-mask task. We adopt the GRES [34], RefCOCO/+/g [42, 84], and GroundingSuite [19] as our evaluation benchmarks. As shown in Tab. 3, although our model is trained solely with the next-token prediction loss, it still outperforms existing methods that rely on task-specific loss on most metrics. Specifically, our approach surpasses the previous best by +1.5% in average gIoU across all three splits and achieves comparable performance in average cIoU, while improving average N-acc by substantial +4.3%. As shown in Tab. 4, our method is also competitive on the RefCOCO/+/g benchmarks; in particular, among models with fewer than 4B parameters, it achieves new SOTA. We further conduct zero-shot evaluation on GroundingSuite, as shown in Tab. 5. Our method demonstrates stronger zero-shot capability than other region-level MLLMs (67.8 vs. 62.6) that use task-specific losses. These results highlight the strong generalization ability of our model, demonstrating that explicit mask supervision is not strictly necessary for effective text-to-mask reasoning. Mask-to-text task. Region caption is representative mask-to-text task that requires producing textual descriptions conditioned on given region masks. We adopt three benchmarks for evaluation: DLC-Bench [30], MDVP-Bench [31], and VideoRefer-D [88]. Without introducing any architectural modifications to the base model, our approach achieves performance comparable to that of the expert model DAM [30] on the DLC-Bench (65.6 vs. 67.3), as shown in Tab. 7. In contrast, general MLLMs such as Qwen2.5VL-7B [3], despite being provided with richer prior information (e.g., category names), attain much lower score of 41.2. This proves our proposed SAMTok enables more precise and unambiguous region grounding for text generation. We also conduct zero-shot evaluations on the MDVP-Bench and VideoRefer-D. As shown in Tab. 8, our method surpasses the expert model DAM in three of the four metrics on MDVP-Bench, demonstrating strong generalization to document and panel-style visual scenes. Similarly, on the video region captioning benchmark VideoRefer-D (Tab. 9), our approach also achieves competitive performance compared to other region-level MLLMs. Table 9 Zero-shot region captioning results on VideoRefer-D benchmark, evaluated under the single-frame setting. Scores are reported for the overall average (Avg.) and four sub-categories: subject correspondence (SC), appearance description (AD), temporal description (TD), and hallucination detection (HD). Method GPT-4o [21] Elysium [59] Ferret [92] Osprey [87] Qwen3VL-SAMTok Size Avg. 2.95 1.57 2.18 2.34 2.88 7B 7B 7B 4B SC 3.34 2.35 3.08 3.19 4.48 AD 2.96 0.30 2.01 2.16 3. TD 3.01 0.02 1.54 1.54 1.28 HD 2.50 3.59 2.14 2.45 2.65 RL results on GRES and GCG. We evaluate the thinking RL setting on the GRES [34] benchmark and the non-thinking RL setting on the GCG [48] benchmark. As shown in Tab. 3, applying purely textual rewards leads to substantial performance improvements on GRES. Across all three splits, our model achieves an average gain of +6.8% gIoU, +4.9% cIoU, and +18.9% N-acc. Similarly, as shown in Tab. 1, on the GCG benchmark, the model exhibits consistent improvements across mask metrics, with +4.5% AP50, +2.0% mIoU, and +6.6% Recall on average. Since we did not use any rewards that evaluate caption quality, the two caption metrics decreased accordingly. To the best of our knowledge, this is the first successful attempt to optimize mask generation performance using text-only reward signals. These results further verify that the proposed SAMTok enables tighter textmask alignment and allows MLLMs to benefit from language-based reinforcement signals without direct pixel-level supervision. Visual Grounding. To assess whether the mask-token interface provided by SAMTok outperforms the native text-box interface of MLLMs on grounding tasks, we also conduct experiments on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks. The results are shown in Tab. 6. Specifically, we de-tokenize mask words into 2D masks and then derive bounding boxes for evaluation. Across both 3B and 7B model sizes, SAMTok yields substantial accuracy improvements while preserving the same natural-language interaction capabilities as the native text-box interface. For the 3D model This further supports our motivation: the new mask representation (SAMTok) outperforms the point format for visual grounding. Visualizations. Fig. 5 provides qualitative examples across multiple downstream tasks, including panoptic scene graph generation (PSG), generalized referring expression segmentation (GRES), region captioning, and grounded conversation generation (GCG). The visualizations highlight the effectiveness of SAMToks unified mask interface across diverse scenarios, enabling joint generation of structured relations and pixel-level masks, robust handling of complex referential language, accurate region-level descriptions, and precise localization of multiple phrases within long sentences."
        },
        {
            "title": "4 Related Work",
            "content": "Mask Understanding in MLLMs. In multimodal large language models (MLLMs) [1, 3, 18, 21, 22, 57, 69], accurately referring to and understanding spatial locations in images is crucial for fine-grained vision-language interaction. Recently, many works [2, 3, 5, 7, 8, 30, 48, 62, 67, 78, 87, 89, 99, 100] have focused on designing concise and effective region-level input representations to characterize users spatial inputs. These methods can be categorized into three approaches. (1) Visual prompts on images [5, 78, 99], which directly highlight target regions to indicate the region of interest (ROI). This approach is intuitive and precise, but may alter the original image content. (2) Textual coordinates [2, 3, 7, 8, 43, 61, 63, 67, 100], which specify regions via text-form 2D points/bounding boxes. While being most aligned with natural conversational interfaces, this approach often poses significant challenges for MLLMs in precisely identifying which image regions the coordinates refer to. (3) ROI features [30, 48, 62, 87, 89], which either encode the region mask into image features or extract ROI features from feature maps using the region mask. These methods typically require specialized module designs, integrated architectures, and complex pipelines, resulting in limited generalizability and scalability. Unlike the above approaches, our proposed SAMTok can compress input regions into representations of 2 special text tokens, thereby overcoming their deficiencies, including not affecting image content, enabling precise and efficient representation of regions, and being completely decoupled from the MLLM. 10 Figure 5 Qualitative examples of SAMTok on diverse downstream tasks, including panoptic scene graph generation (top-left), generalized referring expression segmentation (bottom-left), region captioning (top-right), and grounded conversation generation (bottom-right). Mask Generation in MLLMs. Spatial localization capability is crucial for MLLMs, as it constitutes fundamental component of their understanding of the physical world. Representing region-level outputs is key challenge, as it not only determines the upper bound of spatial localization performance but also affects whether relevant data can be leveraged to efficiently instill spatial understanding capabilities into MLLMs. Existing approaches can be categorized into three types. (1) Directly outputting 2D points [16], bounding boxes [3, 16, 100], or polygonal contours [70, 92, 95] in textual form, which is straightforward but struggles to achieve strong performance since there exists critical gap between textual cross-entropy and continuous localization. (2) Aggregating segmentation information via special tokens [25, 53, 60, 73, 85, 93, 94] and decoding the tokens into 2D masks with dedicated segmentation model [10, 24], which requires joint training of the MLLM with an additional segmentation decoder and relies on segmentation loss optimization. (3) treating region mask as an image [26, 66, 68, 71] and autoregressively generating mask images, which can yield high-precision segmentation but incur substantial computational costs. Compared to the above technical approaches, the core advantages of our SAMTok lie in its compact and efficient representation, high-fidelity mask reconstruction, and improved text-mask alignment, thereby substantially enhancing MLLMs capability to understand and generate spatial regions while preserving natural-language interaction. Reinforcement Learning in Pixel-wise MLLMs. Reinforcement Learning [45, 47, 50, 51] (RL) has become crucial post-training step for MLLMs, significantly enhancing the models capability ceiling. Some works [20, 29, 41, 61] have achieved tremendous success with RL on mathematics, coding, and multiple-choice VQA by measuring rewards through simple and effective answer matching. In the RL process for pixel-wise MLLMs [3638, 52, 101], reward is typically assessed via box IoU or mask IoU, which requires involving mask decoder to convert text boxes or hidden states into 2D masks. Thanks to SAMToks efficient textual region mask representation, purely textual answer-matching reward for region mask generation becomes feasible, computed by checking whether the predicted answer contains the target regions textual special tokens. Our approach removes the need for any de-tokenization process and does not rely on external tools or auxiliary models for correctness verification."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces SAMTok, SAM-based mask tokenizer that represents any region mask using only two discrete tokens, enabling MLLMs to both generate and understand image-region masks within unified textual interface. By converting region masks into special textual tokens, SAMTok reformulates mask understanding and generation as standard next-token prediction, removing the need for task-specific architectures or loss functions. Furthermore, we make the first successful attempt to optimize mask generation performance using purely text-based reward signals. Extensive experiments demonstrate that SAMTok significantly enhances pixel-wise understanding and generation across diverse benchmarks. We hope SAMTok points toward scalable, unified, and language-native paradigm for pixel-wise visual reasoning in future multimodal systems. Future Work Discussion. SAMTok is trained to reconstruct 2D image region masks and, therefore, lacks the ability to reconstruct region masks in video. In addition, SAMTok can only handle mask representation, where more visual entities, such as points, lines, and boxes, should also be included. Adding more visual entities enables more flexible interaction between VLMs and human inputs, which poses more chanlleges into tokener designs. Lastly, we will also explore applications of SAMTok in video tasks, general VQA tasks, and image generation and editing. Impact. Our work explores new mask tokenization design, which rethinks current pixel-wise multimodal large language model designs. Our work bridges regional-level understanding and reasoning through new perspective, making pixel-LLM training as easy as MLLM (LMM) training."
        },
        {
            "title": "A Overview",
            "content": "In this file, we present additional experimental results due to space constraints in the main paper. Here are the details: In B, we provide more implementation details for both the tokenizer and the multi-modal large language models. In C, we provide more experimental results. In D, we present ablation studies on tokenizer designs and their effectiveness. In E, we provide more visualizations."
        },
        {
            "title": "B Implementation Details",
            "content": "We initialize the encoder and decoder of SAMTok with the pretrained weights of SAM 2.1 [49] (Large), while the quantizers codebook is randomly initialized. During training, the parameters of the SAM image encoder and SAM prompt encoder are frozen, whereas the SAM decoder is trainable. Unless otherwise specified, the SAMTok codebook size is set to 256, and the number of quantization steps is 2, with the two codebooks non-shared. We train SAMTok using Xtuner [12] and the AdamW optimizer [40], with global batch size of 1024, an initial learning rate of 4e-5, and cosine decay schedule [39]. We employ Qwen-VL series [3] models as the base model for the main experiments. The only modification made to the base models is the addition of mask tokens in the vocabulary. By default, we introduce 512 mask tokens, formatted as <mt_0000> <mt_0511>. Among them, <mt_0000> <mt_0255> correspond to the first-level codebook, and <mt_0256> <mt_0511> correspond to the second-level codebook. In addition, we introduce two special tokens, <mt_start> and <mt_end>, to denote the start and end positions of mask token sequences. The embeddings of these newly added tokens are randomly initialized using the mean and variance statistics of the original token embeddings. During supervised fine-tuning (SFT) and reinforcement learning (RL), we freeze the MLLM image encoder and fine-tune the projection layer and LLM. SFT is conducted using the official Qwen-VL [3, 67] implementation, the AdamW optimizer [40], global batch size of 256, and learning rate of 2e-5 with cosine decay schedule [39]. All training and evaluation experiments are performed on NVIDIA A100 GPUs (80 GB). For RL, we employ the Easy-R1 [97] framework with the GRPO [51] algorithm, using learning rate of 1e-6."
        },
        {
            "title": "C Additional model experiments",
            "content": "We present more experiments for more vision-language tasks. Panoptic scene graph generation. We further evaluate our models interleaved textmask generation capability beyond the GCG benchmark on the more challenging PSG benchmark [79]. This task requires predicting all subjectrelationobject triplets in an image, where each subject and object includes its category label and 2D mask. As shown in Tab. 10, our Qwen2.5VL-SAMTok-7B achieves performance comparable to expert models (R@20 = 19.8 vs. 20.6, mR@20 = 15.4 vs. 14.8). This result indicates that the mask-token interface provided by SAMTok enables effective task generalization by converting 2D masks into specialized word tokens. Note that, our model does not need any task-specific designs, compared with previous expert models. SAMTok integration across MLLMs. SAMTok and MLLM are decoupled: once SAMTok is trained, it can be deployed with any MLLM. To substantiate this, we train and evaluate on two types of MLLMs: (1) models with tile-based visual encoders (e.g., PerceptionLM [11]), and (2) models with adaptive-resolution encoders (e.g., the Qwen-VL [3] series). We use the same training data across settings, and report results in Tab. 11. The mask-token interface provided by SAMTok works effectively across diverse MLLMs. 13 Table 10 Results on panoptic scene graph generation. Method IMP [76] GPSNet [33] MOTIF [90] VCTree [55] PSGFormer [79] Qwen25VL-SAMTok-7B R@20 mR@20 16.5 17.8 20.0 6.5 7.0 9.1 9.7 14.8 15.4 20.6 18.0 19.8 Table 11 Effectiveness of unified mask-token interface for different MLLMs. For the GCG benchmark, we report the average of each metric across the val and test splits; for the GRES benchmark, we report the average across the val, testA, and testB splits. Method Qwen25VL-SAMTok Qwen3VL-SAMTok PLM [11]-SAMTok Size 3B 4B 1B"
        },
        {
            "title": "D Ablation Study",
            "content": "GCG GRES DLC-Bench METEOR CIDEr AP50 mIoU Recall gIoU cIoU N-acc Pos. Neg. Avg. 60.0 65.6 63.9 70.1 73.6 74.7 58.2 65.3 66. 45.2 46.1 44.3 16.8 16.1 17.4 52.2 50.8 56.2 36.4 37.6 41.9 47.0 47.9 51.8 68.9 71.7 72. 71.3 71.4 74.8 74.8 85.2 83.4 Set Up. We evaluate SAMToks region-mask reconstruction capability on the EntitySeg [46] validation set, which provides 23,754 region masks with high-quality annotations. We use mask IoU as the reconstruction accuracy metric (r-Acc). To assess the mask generation capability of an MLLM integrated with SAMTok, we adopt Qwen2.5-VL-3B [3] as the base model and report the mean cIoU on the val splits of RefCOCO, RefCOCO+, and RefCOCOg in the RES setting as the generation accuracy metric (g-Acc). Quantization. We study three quantization strategies, including VQ [58], FSQ [44], and RQ [27], as summarized in Tab. 12. When adopting standard VQ, large codebook is essential, as region mask embeddings in images exhibit high diversity. Reducing the codebook size from 65,536 to 1,024 results in significant drop in both reconstruction and generative accuracy. With large codebook, FSQ improves codebook utilization, leading to better reconstruction and generation performance than standard VQ. RQ achieves comparable or even better performance while substantially reducing the codebook size. Specifically, using only two residual codebooks of size 1,024 each (i.e., 10242), RQ attains higher reconstruction and generation accuracy than FSQ with much larger codebook. Codebook size and quantization steps. We further analyze the effect of codebook size and the number of residual quantization steps on reconstruction and generation performance, as shown in Tab. 13. For the same codebook size, increasing the number of quantization steps (e.g., 10244 vs. 10242) yields higher-fidelity quantization and thus better reconstruction accuracy (0.75 vs. 0.70 in r-Acc). However, the exponentially expanded search space (10244 vs. 10242) makes it more difficult for the MLLM to learn to generate mask tokens effectively. In particular, in the dense mask setting, longer words incur much higher computational costs. Therefore, we set the number of residual steps to 2 by default. Under this configuration, increasing the codebook size yields only limited improvement in reconstruction accuracy but slightly enhances generation accuracy. We finally adopt the 2562 configuration as our default, since it offers compact codebook while maintaining substantial trade-off between reconstruction and generation performance. We use this setting for further experiments, including SFT and RL processes."
        },
        {
            "title": "E Visualization",
            "content": "In all visualizations in the main paper and the supplementary material, we represent mask words using quant-code pairs: for example, <mt_0011><mt_0347> is denoted as <11-347>. SFT vs. RL. We visualize the improvements brought by RL over SFT in Fig. 6. The gains manifest in three key 14 Table 12 Ablation study on the quantization strategy. 10242 means two residual codebooks with size of 1024 each. Quantization VQ [58] VQ [58] FSQ [44] RQ [27] Codebook Size 1024 65536 65536 10242 r-Acc 0.50 0.66 0.69 0. g-Acc 63.3 77.8 78.1 78.3 Table 13 Ablation study on the codebook size and quantization steps when using RQ [27]. Codebook Size 10242 10244 2564 2562 5122 r-Acc 0.70 0.75 0.72 0.70 0.71 g-Acc 78.3 77.3 77.3 77.6 77.8 aspects: (1) higher recall of targets in multi-object grounding scenarios; (2) more accurate localization for expressions involving relative positions and ordering; and (3) improved mask quality. Mask reconstruction. In Fig. 7, we demonstrate SAMToks ability to reconstruct small objects across variety of challenging scenarios. Since SAMTok and the MLLM are decoupled, the mask reconstruction capability is unaffected by any subsequent MLLM training. In contrast, other mask tokenizers [66, 68] require joint training with the MLLM, which ultimately leads to degraded mask reconstruction performance (with all masks reconstructed as ellipses). Thus, our method yields stronger mask reconstruction than these methods. Such mask reconstruction generalization further leads to strong generalization in MLLMs. PSG visualizations. Fig. 8 shows PSG predictions. Each example contains subjectrelationobject triplets, where both the subject and the object include their segmentation masks. Thanks to the unified mask-token interface, the MLLM can jointly generate structured relational descriptions and pixel-aligned masks, enabling dense, panoptic-level scene understanding. GRES visulizations. Fig. 9 illustrates GRES predictions. SAMTok allows the MLLM to resolve complex referring expressions, including attribute-dependent, context-dependent, and part-level descriptions. The generated masks accurately track the intended region even under heavy occlusion or clutter. Region caption visualizations. Fig. 10 presents region captioning results. Given region mask (tokenized into two special tokens), the MLLM produces detailed and contextually relevant descriptions. The compact mask tokens remove ambiguity inherent in bounding-box-based grounding and lead to more precise and consistent region-level captioning. GCG visualizations. Fig. 11 shows examples where the model simultaneously generates textual captions and region masks for phrases mentioned in the narrative. SAMTok provides lightweight and efficient mechanism for linking each phrase with precise pixel region, enabling both high-quality caption generation and aligned mask prediction within one unified response. Figure 6 SFT vs. RL. Examples are sampled from the GRES benchmark. RL finds more target objects, localizes relative positions better, and produces cleaner masks than SFT across diverse scenes. 16 Figure 7 Region mask reconstruction examples. For each region, the ground-truth mask is tokenized into two discrete codes, and SAMTok reconstructs the mask solely from the original image and the quantized mask tokens. SAMTok preserves fine structures for small, thin, or irregular objects even under challenging lighting or clutter. Since SAMTok is fully decoupled from the MLLM, its reconstruction quality remains stable regardless of downstream model trainingunlike joint-training mask tokenizers that tend to collapse to elliptical or blurred masks. 17 Figure 8 Panoptic scene graph generation(PSG) examples. The model predicts subjectrelationobject triplets where both subject and object categories are paired with their corresponding segmentation masks, represented through mask tokens. SAMToks interface allows the MLLM to generate consistent object masks and relational descriptions simultaneously, demonstrating strong alignment between textual predicates and pixel-grounded regions. Figure 9 GRES examples. Given natural-language referring expression, the MLLM outputs two mask tokens that decode into the final segmentation mask. SAMTok enables precise grounding for expressions involving fine attributes, part-level targets, or contextual reasoning. The examples show robustness to ambiguous descriptions, occlusion, and multi-object scenes. 19 Figure 10 Region Caption examples. Each visualization shows region mask input (tokenized as two mask tokens) and the models generated description. SAMTok provides unambiguous spatial grounding, enabling the MLLM to generate accurate and context-aware region descriptions about attributes, roles, and interactions. 20 Figure 11 GCG examples. The model simultaneously describes the scene and produces region masks for phrases mentioned in the caption. For each highlighted phrase, SAMTok decodes the predicted mask tokens into segmentation masks. SAMToks compact representation (two tokens per mask) enables efficient, aligned textmask generation with consistent grounding across multiple phrases within long captions."
        },
        {
            "title": "References",
            "content": "[1] Claude Anthropic. 3.7 sonnet and claude code, 2025. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda Shapiro, and Ranjay Krishna. Perception tokens enhance visual reasoning in multimodal language models. In CVPR, 2025. [5] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. Vip-llava: Making large multimodal models understand arbitrary visual prompts. In CVPR, 2024. [6] Yi-Chia Chen, Wei-Hua Li, Cheng Sun, Yu-Chiang Frank Wang, and Chu-Song Chen. Sam4mllm: Enhance multi-modal large language model for referring expression segmentation. In ECCV, 2024. [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. [9] Zhenfang Chen, Peng Wang, Lin Ma, Kwan-Yee Wong, and Qi Wu. Cops-ref: new dataset and task on compositional referring expression comprehension. In CVPR, 2020. [10] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In CVPR, 2022. [11] Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, et al. Perceptionlm: Open-access data and models for detailed visual understanding. arXiv preprint arXiv:2504.13180, 2025. [12] XTuner Contributors. Xtuner: toolkit for efficiently fine-tuning llm. https://github.com/InternLM/xtuner, 2023. [13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. [14] Xueqing Deng, Qihang Yu, Peng Wang, Xiaohui Shen, and Liang-Chieh Chen. Coconut: Modernizing coco segmentation. In CVPR, 2024. [15] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: large-scale benchmark for video segmentation with motion expressions. In ICCV, 2023. [16] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [17] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regiongpt: Towards region understanding vision language model. In CVPR, pages 1379613806, 2024. [18] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. [19] Rui Hu, Lianghui Zhu, Yuxuan Zhang, Tianheng Cheng, Lei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, and Xinggang Wang. Groundingsuite: Measuring complex multi-granular pixel grounding. arXiv preprint arXiv:2503.10596, 2025. 22 [20] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [21] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [22] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [23] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. [24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. [25] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, 2024. [26] Mengcheng Lan, Chaofeng Chen, Yue Zhou, Jiaxing Xu, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Text4seg: Reimagining image segmentation as text generation. ICLR, 2025. [27] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, 2022. [28] Yonglin Li, Jing Zhang, Xiao Teng, Haoyu Zhang, Xinwang Liu, and Long Lan. Refsam: Efficiently adapting segmenting anything model for referring video object segmentation. Neural Networks, 2025. [29] Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhenwen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan Boyd-Graber, Haitao Mi, et al. Self-rewarding vision-language model via reasoning decomposition. arXiv preprint arXiv:2508.19652, 2025. [30] Long Lian, Yifan Ding, Yunhao Ge, Sifei Liu, Hanzi Mao, Boyi Li, Marco Pavone, Ming-Yu Liu, Trevor Darrell, Adam Yala, et al. Describe anything: Detailed localized image and video captioning. arXiv preprint arXiv:2504.16072, 2025. [31] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. arXiv preprint arXiv:2403.20271, 2024. [32] Weifeng Lin, Xinyu Wei, Ruichuan An, Tianhe Ren, Tingwei Chen, Renrui Zhang, Ziyu Guo, Wentao Zhang, Lei Zhang, and Hongsheng Li. Perceive anything: Recognize, explain, caption, and segment anything in images and videos. arXiv preprint arXiv:2506.05302, 2025. [33] Xin Lin, Changxing Ding, Jinquan Zeng, and Dacheng Tao. Gps-net: Graph property sensing network for scene graph generation. In CVPR, 2020. [34] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In CVPR, 2023. [35] Ye Liu, Zongyang Ma, Junfu Pu, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. Unipixel: Unified object referring and segmentation for pixel-level visual reasoning. arXiv preprint arXiv:2509.18094, 2025. [36] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. [37] Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia. Visionreasoner: Unified visual perception and reasoning via reinforcement learning. arXiv preprint arXiv:2505.12081, 2025. [38] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. In ICCV, 2025. [39] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. [40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 23 [41] Yan Ma, Steffi Chern, Xuyang Shen, Yiran Zhong, and Pengfei Liu. Rethinking rl scaling for vision language models: transparent, from-scratch framework and comprehensive evaluation scheme. arXiv preprint arXiv:2504.02587, 2025. [42] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, 2016. [43] Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, et al. Open-o3 video: Grounded video reasoning with explicit spatio-temporal evidence. arXiv preprint arXiv:2510.20579, 2025. [44] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023. [45] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022. [46] Lu Qi, Jason Kuen, Weidong Guo, Tiancheng Shen, Jiuxiang Gu, Jiaya Jia, Zhe Lin, and Ming-Hsuan Yang. High-quality entity segmentation. arXiv preprint arXiv:2211.05776, 2022. [47] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. [48] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In CVPR, 2024. [49] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [50] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [51] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [52] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [53] Yongyi Su, Haojie Zhang, Shijie Li, Nanqing Liu, Jingyi Liao, Junyi Pan, Yuan Liu, Xiaofen Xing, Chong Sun, Chen Li, et al. Patch-as-decodable-token: Towards unified multi-modal vision tasks in mllms. arXiv preprint arXiv:2510.01954, 2025. [54] Mikihiro Tanaka, Takayuki Itamochi, Kenichi Narioka, Ikuro Sato, Yoshitaka Ushiku, and Tatsuya Harada. Generating easy-to-understand referring expressions for target identifications. In ICCV, 2019. [55] Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, and Wei Liu. Learning to compose dynamic tree structures for visual contexts. In CVPR, 2019. [56] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [57] Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, 24 Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2024. [58] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. [59] Han Wang, Yongjie Ye, Yanjie Wang, Yuxiang Nie, and Can Huang. Elysium: Exploring object-level perception in videos via mllm. arXiv preprint arXiv:2403.16558, 2024. [60] Hao Wang, Limeng Qiao, Zequn Jie, Zhijian Huang, Chengjian Feng, Qingfang Zheng, Lin Ma, Xiangyuan Lan, and Xiaodan Liang. X-sam: From segment anything to any segmentation. arXiv preprint arXiv:2508.04655, 2025. [61] Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, et al. Traceable evidence enhanced visual grounded reasoning: Evaluation and methodology. arXiv preprint arXiv:2507.07999, 2025. [62] Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, et al. Grasp any region: Towards precise, contextual pixel understanding for multimodal llms. arXiv preprint arXiv:2510.18876, 2025. [63] Jiacong Wang, Zijian Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, et al. Vgr: Visual grounded reasoning. arXiv preprint arXiv:2506.11991, 2025. [64] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin. V3det: Vast vocabulary visual detection dataset. In ICCV, 2023. [65] Jingchao Wang, Zhijian Wu, Dingjiang Huang, Yefeng Zheng, and Hong Wang. Unlocking the potential of mllms in referring expression segmentation via light-weight mask decoder. arXiv preprint arXiv:2508.04107, 2025. [66] Lingfeng Wang, Hualing Lin, Senda Chen, Tao Wang, Changxu Cheng, Yangyang Zhong, Dong Zheng, and Wuyue Zhao. Alto: Adaptive-length tokenizer for autoregressive mask generation. arXiv preprint arXiv:2505.16495, 2025. [67] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [68] Tao Wang, Changxu Cheng, Lingfeng Wang, Senda Chen, and Wuyue Zhao. Himtok: Learning hierarchical mask tokens for image segmentation with large multimodal model. In ICCV, 2025. [69] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [70] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. NeurIPS, 2023. [71] Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, and Jun Zhou. Argenseg: Image segmentation with autoregressive image generation model. In NeurIPS, 2025. [72] XuDong Wang, Shaolun Zhang, Shufan Li, Kehan Li, Konstantinos Kallidromitis, Yusuke Kato, Kazuki Kozuka, and Trevor Darrell. Segllm: Multi-round reasoning segmentation with large language models. In ICLR, 2025. [73] Cong Wei, Yujie Zhong, Haoxian Tan, Yong Liu, Zheng Zhao, Jie Hu, and Yujiu Yang. Hyperseg: Towards universal visual segmentation with large language model. arXiv preprint arXiv:2411.17606, 2024. [74] Cong Wei, Yujie Zhong, Haoxian Tan, Yingsen Zeng, Yong Liu, Hongfa Wang, and Yujiu Yang. Instructseg: Unifying instructed visual segmentation with multi-modal large language models. In ICCV, pages 2019320203, 2025. [75] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. 25 [76] Danfei Xu, Yuke Zhu, Christopher Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In CVPR, 2017. [77] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. In ECCV, 2024. [78] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [79] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, and Ziwei Liu. Panoptic scene graph generation. In ECCV, 2022. [80] Qi Yang, Weichen Bi, Haiyang Shen, Yaoqi Guo, and Yun Ma. Pixelweb: The first web gui dataset with pixel-wise labels. arXiv preprint arXiv:2504.16419, 2025. [81] Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao Peng, Shu Liu, and Jiaya Jia. Lisa++: An improved baseline for reasoning segmentation with large language model. arXiv preprint arXiv:2312.17240, 2023. [82] Zuyao You and Zuxuan Wu. Seg-r1: Segmentation can be surprisingly simple with reinforcement learning. arXiv preprint arXiv:2506.22624, 2025. [83] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. In TACL, 2014. [84] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, 2016. [85] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001, 2025. [86] Haobo Yuan, Yueyi Sun, Yanwei Li, Tao Zhang, Xueqing Deng, Henghui Ding, Lu Qi, Anran Wang, Xiangtai Li, and Ming-Hsuan Yang. Visual reasoning tracer: Object-level grounded reasoning benchmark. arXiv preprint arXiv:2512.05091, 2025. [87] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In CVPR, 2024. [88] Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao, Wenqiao Zhang, Yueting Zhuang, et al. Videorefer suite: Advancing spatial-temporal object understanding with video llm. In CVPR, 2025. [89] Yuqian Yuan, Wenqiao Zhang, Xin Li, Shihao Wang, Kehan Li, Wentong Li, Jun Xiao, Lei Zhang, and Beng Chin Ooi. Pixelrefer: unified framework for spatio-temporal object referring with arbitrary granularity. arXiv preprint arXiv:2510.23603, 2025. [90] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. Neural motifs: Scene graph parsing with global context. In CVPR, 2018. [91] Hanbo Zhang, Jie Xu, Yuchen Mo, and Tao Kong. Invig: Benchmarking interactive visual grounding with 500k human-robot interactions. arXiv preprint arXiv:2310.12147, 2023. [92] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for referring and grounding with large language models. arXiv preprint arXiv:2404.07973, 2024. [93] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and Shuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. NeurIPS, 2024. [94] Tao Zhang, Xiangtai Li, Zilong Huang, Yanwei Li, Weixian Lei, Xueqing Deng, Shihao Chen, Shunping Ji, and Jiashi Feng. Pixel-sail: Single transformer for pixel-grounded understanding. arXiv preprint arXiv:2504.10465, 2025. [95] Tao Zhang, Shiqing Wei, Shihao Chen, Wenling Yu, Muying Luo, and Shunping Ji. Vectorllm: Human-like extraction of structured building contours vis multimodal llms. arXiv preprint arXiv:2507.04664, 2025. 26 [96] Yuxuan Zhang, Tianheng Cheng, Lianghui Zhu, Rui Hu, Lei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, and Xinggang Wang. Evf-sam: Early vision-language fusion for text-prompted segment anything model. arXiv preprint arXiv:2406.20076, 2024. [97] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025. [98] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017. [99] Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, Qianyu Zhou, Yunhai Tong, Shunping Ji, Jiangning Zhang, Lu Qi, and Xiangtai Li. Are they the same? exploring visual correspondence shortcomings of multimodal llms. In ICCV, 2025. [100] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [101] Lianghui Zhu, Bin Ouyang, Yuxuan Zhang, Tianheng Cheng, Rui Hu, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Li Yu, Wenyu Liu, et al. Lens: Learning to segment anything with unified reinforced reasoning. arXiv preprint arXiv:2508.14153, 2025. [102] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In CVPR, 2016."
        }
    ],
    "affiliations": [
        "ByteDance",
        "NUS",
        "Wuhan University"
    ]
}