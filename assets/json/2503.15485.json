{
    "paper_title": "TULIP: Towards Unified Language-Image Pretraining",
    "authors": [
        "Zineng Tang",
        "Long Lian",
        "Seun Eisape",
        "XuDong Wang",
        "Roei Herzig",
        "Adam Yala",
        "Alane Suhr",
        "Trevor Darrell",
        "David M. Chan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for language-driven tasks. In this work, we introduce TULIP, an open-source, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization to learn fine-grained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to a $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot classification, and improving vision-language models, achieving over $3\\times$ higher scores than SigLIP on MMVP. Our code/checkpoints are available at https://tulip-berkeley.github.io"
        },
        {
            "title": "Start",
            "content": "TULIP: Towards Unified Language-Image Pretraining Zineng Tang, Long Lian, Seun Eisape, XuDong Wang, Roei Herzig, Adam Yala, Alane Suhr, Trevor Darrell, David M. Chan University of California, Berkeley 5 2 0 2 9 1 ] . [ 1 5 8 4 5 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and finegrained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for languagedriven tasks. In this work, we introduce TULIP, an opensource, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization to learn finegrained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing new SOTA zero-shot performance on ImageNet-1K, delivering up to 2 enhancement over SigLIP on RxRx1 in linear probing for fewshot classification, and improving vision-language models, achieving over 3 higher scores than SigLIP on MMVP. Our code/checkpoints are available at https://tulipberkeley.github.io. 1. Introduction Contrastive image-text (CIT) models, including CLIP [38], SigLIP [54], and ALIGN [28] have demonstrated stateof-the-art performance on high-level vision-language tasks, excelling in various applications such as retrieving images from text and vice versa, performing zero-shot classification, and serving as core components of vision-andlanguage models. Their success stems from their ability to leverage billion-scale datasets to create shared embedding space between image and language inputs, where similar concepts are close together and dissimilar ones are far apart. Nonetheless, existing CIT approaches come with several Figure 1. TULIP Overview. Existing contrastive image-text models struggle with high-fidelity visual understanding. TULIP is drop-in replacement for CLIP which leverages generative data augmentation, global-local patch-wise image contrastive learning, and reconstruction-based feature regularization to learn robust visual features and fine-grained language grounding. notable drawbacks. While representations learned by contrastive image-text models tend to encode the high-level semantics between images and text, encoding global alignment often comes at the cost of reduced performance in visual fine-grained tasks such as spatial reasoning. Existing CIT model representations are thus over-optimized for identifying what is present in an image over determining where it is located or noticing fine-grained details distinguishing similar objects. This limitation stems from training data and objectives that lack focus on precise spatial understanding and do not provide the detailed annotations necessary for fine-grained visual differentiation or grounding. As result, tasks requiring more subtle visual understanding, such as multi-view reasoning, counting, instance segmentation, depth estimation, and object localization, pose greater challenge compared to high-level tasks. In this paper, we introduce TULIP (Towards Unified Language-Image Pretraining), an open-source drop-in replacement for existing open-weights CIT models designed to enhance the learning of general-purpose visual features while preserving the language-grounding strengths of current CIT methods. Our method addresses two fundamen1 tal challenges of existing CIT methods: representation of detailed spatial information, and representation of nuanced visual details. To encode detailed spatial information, we incorporate patch-level global and local multi-crop augmentations and objectives, inspired by methods such as iBOT [55] and DINO [36]. To maintain the high-frequency local visual details that image-text contrastive objectives often overlook, we introduce reconstruction objective. While existing CIT approaches focus on high-level semantic representation and often miss these local details, we find that incorporating them enhances performance in various downstream tasks, such as visual question answering. Finally, we propose generative data augmentation strategy based on diffusion models, designed to produce challenging hard negatives that refine fine-grained semantic grounding. We demonstrate the efficacy of TULIP by evaluating it against existing CIT models (such as OpenAIs CLIP [38] and the recently introduced SigLIP 2 [48]) on diverse suite of vision-centric downstream tasks covering both traditional and specialized datasets. Specifically, we assess performance on general-purpose zero-shot classification datasets such as ImageNet-1K, iNAT-18, and Cifar-100, as well as fine-grained, task-specific classification datasets including RxRx1, fMoW, and Infographics. We demonstrate that TULIP outperforms SOTA models across all benchmarks (in some cases, even outperforming larger models). We further demonstrate SOTA text-based image retrieval performance in both COCO and Flickr, along with the inverse image to text problem. To examine the models robustness in vision-language tasks, we conduct evaluations using our model as visual encoder for LLaVA-style model on both the MMVP and MM-Bench datasets, demonstrating that when used as drop-in replacement for existing CIT models, TULIP can lead to more than 3x improvements on MMVP (vision-centric downstream tasks) over CIT models without degraded performance on language-centric tasks. Finally, we assess the reasoning and perceptual skills of our approach using the BLINK benchmark, demonstrating up to 12% relative improvement over SigLIP-trained baselines, and visio-linguistic compositional reasoning skills using the Winoground benchmark where we outperform existing CIT models by up to 30%, achieving above-random performance in Group-based reasoninga first for CIT models. We summarize our main contributions as follows: (i) We introduce TULIP, modified image-language pretraining framework that enhances the encoding of fine-grained visual representations while maintaining the languagegrounding capabilities of existing ILP methods. (ii) We incorporate patch-level global and local multi-crop augmen- (iii) tations and objectives to improve spatial awareness. We introduce reconstruction objective that preserves high- (iv) We propose generfrequency local visual details. ative data augmentation strategy based on diffusion models, designed to generate challenging hard negatives that refine fine-grained semantic grounding. (v) We evaluate TULIP on broad set of vision and vision-language benchmarks, establishing new state-of-the-art performance in zero-shot classification, fine-grained recognition, object detection, and multi-modal reasoning tasks. 2. Related Work Vision-Centric Self-Supervised Learning. Vision-centric self-supervised learning has witnessed remarkable progress, driven by the development of learning representations from unlabeled image data. Early approaches like DeepCluster [9] explored clustering-based techniques, while contrastive learning frameworks such as MoCo [13, 23], SimCLR [12], and SwAV [10] leveraged different augmentations of the same image and diverse augmentations to learn powerful representations. Further advancements were made with non-contrastive methods like BYOL [22] and Barlow Twins [53], which eschewed explicit negative samples. More recently, DenseCL [50] introduced dense contrastive learning, and VICReg [5] proposed varianceinvariance-covariance regularization framework. Meanwhile, DINO [9, 36] leveraged self-distillation with momentum encoder, and masked image modeling approaches like MAE [24], DiffMAE [51], and CrossMAE [19] demonstrated the effectiveness of reconstructing masked image patches. Finally, I-JEPA [1] and V-JEPA [6] advanced self-supervised visual representation learning by introducing masked prediction tasks, which involve predicting abstract representations of masked image regions. Unlike previous methods, our approach enhances contrastive image-text learning by explicitly incorporating visual local details. This is achieved through patch-level global and local multi-crop augmentations and objectives, complemented by reconstruction objective, leading to more comprehensive understanding of visual information. Generative Data Augmentation. Generative data augmentation has recently emerged as powerful technique to expand training datasets beyond traditional transformations. ALIA [18] develops diffusion-based augmentation pipeline that uses language-guided image editing to create realistic domain variations of training images, significantly enhancing dataset diversity and improving classification performance. Similarly, [47] leverages pre-trained text-toimage diffusion models to perform semantic image edits to create augmented examples, which boosts accuracy in fewshot classification tasks. [42] proposes model-agnostic approach using diffusion model to synthesize class-specific images for unseen categories to improve zero-shot classification performance. [25] showed that such synthetic data can improve model performance in low-data regimes and 2 assist large-scale model pre-training. [2] demonstrates that adding diffusion-generated samples to ImageNet yields significant gains in classification accuracy. StableRep [45] indicates that model trained exclusively on 20M Stable Diffusion-generated images can learn visual representations rivaling those obtained from training on 50M real images. In contrast to prior works that enrich datasets with generative images, our GeCo integrates generative augmentation directly into the contrastive learning framework. By leveraging large language models to create both positive and negative paraphrases alongside diffusion-based image edits, our dual-modality approach produces richer contrastive views that enhance both visual and textual representations. Unlike methods focused solely on classification or domain shifts, GeCo refines fine-grained semantic grounding by generating hard negatives that compel the model to discern subtle differences in image-text pairs. Contrastive Image-Text Learning. Contrastive imagetext (CIT) learning has emerged as powerful paradigm for learning joint representations of visual and textual information. Through training models on extensive datasets of image-text pairs, CIT enables the alignment of visual and textual representations within common embedding space. Pioneering works like CLIP [38] and ALIGN [28] demonstrated impressive zero-shot capabilities and achieved stateof-the-art performance on various vision-language tasks, including image-text retrieval and visual question answering. Subsequent efforts have focused on improving the efficiency and scalability of contrastive learning, such as SigLIP [54], which introduced novel sigmoid loss function, and SigLIP 2 [48] (introduced concurrent to our work), which extended the training objective of sigmoid loss with additional objectives for improved semantic understanding, localization, and dense features. Despite their successes, these models often struggle with fine-grained visual understanding and tasks requiring precise spatial reasoning. Finally, SLIP [35] also explored the usage of self-supervised learning with language supervision for visual representation learning. However, unlike our approach, SLIP focused solely on image-text and image-image contrastive learning with fixed augmentations. 3. TULIP We introduce TULIP, high-performing image-text contrastive model that unifies several diverse contrastive learning paradigms to improve representation learning. The underlying insight behind several of the contributions of TULIP is that images and their associated captions represent different views or perspectives of an underlying reality, an observation recently explored in Huh et al. [27]. For example, picture of cat with bench, and the caption cat is sitting on bench present different observations Figure 2. TULIP Image Encoder. Images undergo both traditional augmentations (such as cropping and color jittering) and generative augmentations via GeCo, which leverages large generative models to create semantically consistent or semantically altered views. These views are then used for image-image and image-text contrastive learning. Additionally, masked autoencoder (MAE)-based reconstruction loss is applied to encourage the model to encode both semantic and fine-grained details. of the same underlying true situation. Contrastive learning serves to unify these views in an unsupervised way - taking several views and projecting them to the same point in representation latent space. Thus, defining what constitutes valid view of the underlying content is fundamental in developing contrastive learning approach. In this section, we first discuss how TULIP uses images and text to provide different views in the contrastive learning process (See subsection 3.1). Next, we present how TULIP creates different views from the reality with generative augmentations (See subsection 3.2), and how TULIP regularizes the training with reconstruction loss to learn more robust representation (See subsection 3.3). 3.1. Diversifying Contrastive Views Previous image-text contrastive learning approaches primarily contrast an image with its corresponding text, while image-image contrastive learning methods contrast an image with an augmented version of itself. We propose unification of these approaches by treating every transformation of an image or text as valid view of the underlying semantic content, which is then incorporated into the contrastive learning framework. Thus, our contrastive learning loss comprises three key components: image-text contrastive learning, image-image contrastive learning, and text-text contrastive learning, as explained in Fig. 1. The contrastive loss in our method sources from SigLIP [48]. Denote and as two views from the same embedding xT and the transformed text embedding define our text-text contrastive loss as: , we LT-T = LSigLIP({xT }, {x }) Our overall contrastive learning loss is as follows: Lcont = LI-T + LI-I + LT-T (4) (5) Image Encoder. The image encoder for TULIP is shown in Figure 2. Following DINOv2, we use an EMA teacher model, combined with local/global view splits (where the teacher only sees global views, and the student sees both global and local views). Similar to DINOv2, we leverage the embeddings generated by the teacher model for imageimage contrastive learning and image-text contrastive learning. In our experiments, the image encoder takes the form of SigLIP image encoder, which is ViT model [16]. The reconstruction regularization shown in the pathway is discussed in subsection 3.3. Text Encoder. The text encoder for TULIP is shown in Figure 3. For text encoding, there is no clear global/local structure in the views, so we do not use an EMA teacher and instead leverage text encoder with directly tied weights. For text encoder, we use SigLIPs language encoder. The reconstruction regularization is further discussed in subsection 3.3. 3.2. GeCo: Generating Diverse Contrastive Views Existing models for contrastive learning focus on using fixed sets of views to force models to learn semantic invariance. While fixing the set of potential views is simple, choosing the right views is challenging task. The particular set of views chosen can also impact the level of features learned by the model. In DINO, models are trained to match local/small crops of the images with global crops of images, leading to strong global semantic features, but often leading models to ignore complex relationships between objects. Recent work has shown that many generative models inherently encode semantics at natural levels, for example, GPT-4V performs well when measuring semantic distance in natural language [11], and Stable Diffusion latent encode semantic correspondences between images [26]. This motivates view generation approach that relies on semantic information encoded by these large generative models, in addition to base set of simple pixel-level augmentations. Towards such generative augmentation, we introduce GeCo (GEnerative COntrastive view augmentation), method that leverages large generative models (both language and image), to generate semantically equivalent (and semantically distinct but visually similar) augmentations automatically during training. GeCo alters both image and text automatically in the axes of perception, space, and time, Figure 3. TULIP Text Encoder. Text undergoes generative augmentation through paraphrasing and controlled semantic alterations using large language models, generating both positive and negative contrastive pairs. These pairs are used for both text-text and image-text contrastive learning with SigLIP objective. Similar to image reconstruction, causal decoder (based on T5) is used for text reconstruction, ensuring that the model retains both highlevel semantics and fine-grained linguistic detail. underlying content with batch size B: LSigLIP({x}, {y}) = 1 B (cid:88) (cid:88) i=1 j=1 log 1 1 + ezij (txiyj +b) (cid:123)(cid:122) (cid:125) (cid:124) Lij (1) Image-Text Contrastive Learning. For each image in the batch B, we use the standard image-text contrastive learning objective from SigLIP: LI-T = LSigLIP({xI }, {xT }) (2) Image-Image Contrastive Learning. To construct transformed images, we leverage generative model instead of the traditional fixed set of augmentations commonly used in contrastive learning. Our generative transformations significantly outperform standard augmentation techniques such as those used in DINO, leading to more robust representations. We present details of the generative transformations in subsection 3.2. Given the original image embedding xI and the transformed image embedding , we define our image-image contrastive loss as: LI-I = LSigLIP({xI }, {x }) (3) Text-Text Contrastive Learning. To enhance textual representations, we apply generative augmentation using language model including syntactic paraphrasing and synonym replacement (See subsection 3.2). Given the original text 4 Figure 4. Overview of GeCo. Our generative augmentation framework leverages large generative models to create diverse contrastive views by generating both positive and negative augmentations for images and text. For text augmentation, we use Llama-3.1-8B-Instruct to generate paraphrases and semantically altered text variations. For image augmentation, we fine-tune an instruction-based image editing model (e.g., InstructPix2Pix) fine-tuned using soft-prompting to generate semantically consistent (positive) and semantically altered (negative) views. to create positive and negative pairs that are fed to the contrastive components making up TULIP. GeCo generates two types of view pairs: Positive views are views of the same content which contain identical semantics viewed in different (but similar) way. These views should be closer in semantic space. For example, rotating the camera around an object slightly does not significantly change the semantics of an image, but can change the local pixel values. Negative views are views of content that are semantically distinct, but contain many similar image characteristics, for example, adding car into the image of bike creates new image which is semantically distinct, but contains many of the same visual features. Unfortunately, such paired data is often unavailable, thus, GeCo makes use of generative modeling to generate these positive and negative views from existing pairs of images and text. The general process for GeCo is shown in Figure 4, and consists of two components: language augmentation and image augmentation. Language Augmentation. To augment language, several methods (primarily targeted at hallucination reduction) have pursued random word deletion or word synonym replacement [41]. Here, we leverage large language model (Llama-3.1-8B-Instruct) to perform similar-style augmentations. We ask the model to directly paraphrase the content of the text to produce positive (where the semantics are identical) and negative paraphrases (where the semantics have been subtly altered). By relying on language models to make this decision, we can take advantage of the underlying Figure 5. (Top) GeCo generates positive and negative augmentations of both images and text, (Bottom) TULIP uses these augmentations during training time with corresponding weights (+1 for positive pair, -1 for negative pair, 0 to ignore). Here, the generated positive image represents the same bird from different viewpoint, while the negative image is different bird (coloring, face structure) in the same physical location. semantic understanding in the LLM, and avoid pre-defining specific level of semantic similarity. The prompts, given in Appendix D, differ for positive and negative augmentation. When generating positive samples, concretely, the LLM should not change the semantics such as objects, counting, layout, etc, while it can paraphrase text by syntactics, synonyms, etc. When generating negative samples, we can follow similar logic to change the semantics of the text, such as changing 5 apples 4 apples or changing the compositional components of the image such as chair to the left of table table to the left of the chair. Image Augmentation. To augment the images, we finetune (using soft-prompting) an instruction-based image editing generative model to generate both positive and negative augmentations of an image. Formally, for an imageediting model G(I, E) where is an image and is vector embedding, we learn embeddings Ep (positive) and En (negative) corresponding to positive and negative views. To train these embeddings, we draw on several natural sources of image augmentation. In addition to traditional image augmentations (i.e., simple color jitter, flipping, global cropping, Gaussian blur, etc.) we also consider several further augmentations. For positive training, the primary addition is video data, where we consider closely related frames (< 0.2s apart) to be semantically identical, and multi-view data, where we consider multiple views of the same object to be semantically identical. For negative training, we use large-scale datasets for semantic image editing, as each image edit encodes the images semantic transformation. Together, TULIP supports taking an image and paired text, and generating augmented positive and negative views. We can then use these views for training, either online 5 during training time inference or by caching the augmentations and re-using them during the training process as shown in Figure 5. More formally, in the case of imageimage or texttext contrastive learning, GeCo takes an input (image or text) and produces both an augmented positive view and an augmented negative view. Following the notation from subsection 3.1 (with loss LSigLIP), let = {x1, . . . , xi, . . . , xn} be input images (or texts) and let = {y1, . . . , yi, . . . , yn} be positive and negative augmented views of that image (or text). Define as the set of indices corresponding to negative views. In Equation 1, we then set: zij = 1{j } (6) meaning that zij = 1 (two elements are negative pair) whenever the jth view is negative. In imagetext contrastive learning, let = {y1, . . . , yj, . . . , yn} be the generated augmented texts. GeCo generates only negative augmented views for both the image and the text, and we set: the text embedding as the initial text token. The loss from the regularization is formatted as: Lrecons = λiLimage-recons + λtLtext-recons (8) where λi and λt represent weighting tradeoff between the reconstruction loss and other objectives in our network. Since reconstruction can be expensive during training, to ensure minimal computational overhead we compute reconstruction in both modalities, but using the latent vectors for only one of the two modalities during each pass. For example, in image-image contrastive learning, we compute the reconstruction loss from one of the image embeddings, and later in the image-text contrastive learning, text reconstruction loss is also computed from the pre-existing image embedding (this is reasonable, as the contrastive objectives encourage the vectors coming from each positive pair to be the same at convergence). Overall, TULIP is pre-trained in one pass with weighted combination of losses: zij = 1 if (7) LTULIP = λcLcont + λrLrecons (9) Note that this omits the computation for pairs where both and belong to (since its unknown what their correspondence is), and focuses on situations where we know the image or text does not match the true value. 3.3. Regularization with Reconstruction While incorporating wide range of contrastive views through generative augmentation alone can help to improve the performance of the model on fine-grained semantics, this process also introduces hidden invariance in our model, where the different augmentations of an image encode to the same point. While such invariance is helpful for representation learning, it often leads to reduced performance on high-fidelity visual-centric tasks (such as coloridentification, orientation, or depth-estimation). To encourage the model to balance this high-frequency information with the representation of semantics, we additionally add pixel-level reconstruction objective to the latent vector of the model. The underlying assumption is that if the model can encode the information necessary for reconstructing the image itself from the latent space, it will also encode key visual detail (such as color/texture) while remaining invariant in the semantic space (due to the contrastive objective). The reconstruction objectives are shown in Figure 2 for the image pathway, and Figure 3 for the text pathway. For image reconstruction, we leverage masked autoencoder (MAE) style model augmented with the embedding as bottleneck for the information. Using MAE encourages the model to encode shape information and high-entropy detail instead of global patterns (as those global patterns can easily be inferred from unmasked patches). For the text model, we leverage causal decoder (based on T5), with 4. Experiments & Results In this section, we discuss the experimental design, training procedure, and experimental results for TULIP. 4.1. Experimental Design Data. To train GeCo, as described in subsection 3.2, we use video and multi-view datasets for our diffusion model. For next-frame prediction, we sample consecutive frames (within 0.2 seconds) from the WebVid-10M dataset [3]. For multi-view prediction, we use MVImgNet [52], and for negative view generation, we incorporate datasets from InstructPix2Pix [8]. To paraphrase text for augmentation, we leverage the Llama-3.1-8B-Instruct model [17]. For model pre-training, we train all variants of TULIP using the DataComp-1B dataset [21]. To augment the data, we randomly replace 20% of the original captions with recaptioned data from Li et al. [30]. During text reconstruction, we find that increasing the proportion of re-captioned data improves results, so we replace 50% of the base captions with re-captioned data. Optimization. We use Adam optimizer with learning rate 104, weight decay 104, and gradient clipping to norm 2. We set the batch size to 49,152. Our models are trained with up to 32 A100 GPUs over the course of several days. 4.2. Vision-Language Understanding Our first experiments focus on evaluating the quality of the image-text representations learned by TULIP, where we explore zero-shot classification, text-to-image and image-totext retrieval, and linear probing for fine-grained classification datasets. Table 1. Zero-shot classification results (% accuracy) on ImageNet-1K (val, v2, ReaL, 10-shot), ObjectNet, and text-image/image-text retrieval for TULIP vs. several existing SOTA vision and language models."
        },
        {
            "title": "Model Method",
            "content": "Res. Seq."
        },
        {
            "title": "Flickr",
            "content": "IN-val IN-v2 IN-ReaL ObjNet IN-10s TI IT TI IT OpenAI CLIP Open CLIP MetaCLIP EVA CLIP DFN SigLIP SigLIP 2 TULIP"
        },
        {
            "title": "SigLIP",
            "content": "SigLIP"
        },
        {
            "title": "TULIP",
            "content": "SigLIP"
        },
        {
            "title": "TULIP",
            "content": "224 224 224 224 224 224 224 224 224 384 224 384 384 256 384 384 196 196 196 196 196 196 196 196 256 729 256 729 729 256 576 B/16 So/14 g/16 68.3 70.2 72.4 74.7 76.2 76.2 78.2 79.5 82.2 83.2 83.2 84.1 85.0 84.5 85.0 85. 61.9 62.3 65.1 67.0 68.2 69.5 71.4 73.0 76.0 77.1 77.7 78.7 79.5 79.2 79.8 80.0 82.8 84.8 86.2 87.1 87.5 87.8 88.1 89.0 88.3 88.5 89. 55.3 56.0 60.0 62.3 63.2 70.7 73.6 74.2 80.5 82.9 84.6 86.0 87.2 87.1 88.0 88.6 69.9 72.1 73.8 78.2 79.4 79.5 80.4 80.9 82.1 82.5 82. 33.1 42.3 48.9 42.2 51.9 47.2 52.1 54.2 50.8 52.0 55.1 55.8 56.3 55.7 56.1 57.8 52.4 59.4 58.7 64.5 68.9 70.1 69.0 70.2 71.5 71.7 72.0 72.5 72.8 73. 62.1 69.8 77.1 71.2 77.3 77.9 80.7 81.8 76.6 80.5 84.3 85.7 85.3 85.3 86.0 87.2 81.9 86.3 85.7 89.6 93.0 93.9 90.7 93.5 94.6 94.9 95.1 95.3 95.4 95. Table 2. Results (% accuracy) of linear probe applied to representations learned by existing representation models. TULIP performs strongly across all datasets, even outperforming significantly larger vision foundation models such as AIMv2 3B."
        },
        {
            "title": "Model",
            "content": "MAE DINOv2 (L/16) OAI CLIP (B/16) FN-CLIP SigLIP (So/14) AIMv2 (H/14) AIMv2 (3B,448px) TULIP (B/16) TULIP (So/14, 384) TULIP (g/16, 384) IN1k 82.2 87.2 85.7 86.9 87.3 87.5 89.5 85.9 89.0 89.6 iNATCifar 100 RxRx1 fMoW Info 70.8 83.0 73.5 76.4 77.4 77.9 85.9 81.2 84.2 85.8 87.3 95.6 89.7 93.9 91.2 93.5 94. 93.9 96.4 96.9 7.3 9.0 5.7 6.1 4.6 5.8 9.5 7.4 9.3 9.8 60.1 65.5 62.0 63.4 64.4 62.2 66.1 63.0 65.8 66.3 50.2 59.4 66.9 68.1 72.3 70.4 74. 69.8 73.7 74.7 Table 3. Results (% accuracy) on the Winoground dataset across the text, image and group score metrics. TULIP is the only CIT model to outperform random chance on the group score metric."
        },
        {
            "title": "MTurk Human\nRandom Chance",
            "content": "VinVL CLIP (ViT-B/32) SigLIP (ViT-so/14, 384) SigLIP 2 (ViT-so/14) SigLIP 2 (ViT-g/14) TULIP (ViT-B/14) TULIP (ViT-So/14, 384) TULIP (ViT-G/16, 384)"
        },
        {
            "title": "Image Group",
            "content": "89.50 25.00 37.75 30.75 36.50 38.25 38.75 37.50 42.25 42.50 88.50 25.00 17.75 10.50 15.75 19.00 17.25 16.25 20.50 20. 85.50 16.67 14.50 8.00 12.25 16.00 14.00 11.25 17.75 18.50 ImageNet v2 [39], ImageNet ReaL [7] and ObjectNet [4]) following the general protocol from Zhai et al. [54], with results in Table 1. Generally, TULIP outperforms existing approaches within their parameter classes, and represents significant improvements over existing open-source models such as OpenCLIP. TULIP Text-To-Image Retrieval. In addition to zero-shot classification, we also benchmark on image-retrieval benchmarks (both text-to-image and image-to-text using the COCO [31] and Flickr-30K [37] datasets), where TULIP significantly outperforms existing benchmark models, particularly in text-to-image modeling at larger scales. Linear Probing. While TULIP performs well on largescale object understanding benchmarks, many of the improvements that we target in this work are focused on understanding fine-grained detail. Towards this end, we explore the performance of TULIP when training linear probes on domain-specific data. Towards understanding such performance, we evaluate on the IN-1K [15], iNAT-18 [49], CIFAR-100 [29], RxRx1 [43], fMoW [14], and Infographic [34] datasets (see Appendix for detailed dataset descriptions). The results, shown in Table 2 show that TULIP clearly outperforms existing vision and language representations for fine-grained/detail-oriented tasks (for example, achieving almost twice the performance of SigLIP on RxRx1, and higher performance than DINOv2 alone), while maintaining high-quality language representations (achieving 24% relative improvement over DINOv2 and outperforming SigLIP on the Infographic dataset). Zero-Shot Classification. We first benchmark TULIP on zero-shot classification (ImageNet [15] (1-shot/10-shot), Compositional Reasoning. To evaluate TULIPs ability to understand the composition of images, we further evaluate 7 Table 4. Results (% accuracy) on the BLINK benchmark. TULIP demonstrates strong results across all categories, particularly excelling in vision-driven tasks, outperforming GPT-4o in some cases. Model Overall Sim. Count Depth Jigsaw Art Human Random Choice GPT-4o GPT-4 Turbo GPT-4V LLaVA 1.6 34B QwenVL-Max Llama-3.2-11B + SigLIP (So/14) + DINOv2 (L/16) + TULIP (So/14) 95.67 38.09 60.04 54.61 51.14 46.80 40.28 48.70 49.51 50.83 96.70 72.59 80.74 78.52 48.89 51.11 65.29 67.13 68.29 93.75 25 49.17 57.50 60.83 66.67 56. 55.04 53.49 55.34 99.19 50 74.19 66.13 59.68 67.74 58.06 63.56 64.08 64.29 99.00 55.33 69.33 70.00 54.67 4.67 53.97 56.26 57.26 95.30 50 82.91 79.49 79.49 43.59 38. 66.09 67.88 68.39 Table 5. Llama-3.2 11B finetuned with several vision models on the MMVP and LLaVA benchmarks. While the LLaVA bench performance is limited by the LLM/training architecture, the MMVP benchmark shows reliance on visual representation quality."
        },
        {
            "title": "LLaVA",
            "content": "DINOv2 (ViT-L/16) OpenAI CLIP (ViT-B/16) SigLIP (Vit-So/14) +I/I & T/T Constrastive Learning + Reconstruction + GeCo (TULIP) SigLIP (Vit-B/14) +I/I & T/T Constrastive Learning + Reconstruction + GeCo (TULIP) 16.2 4.5 5.9 17.4 (+11.5) 18.2 (+1.2) 20.3 (+2.1) 5.2 14.4 (+9.2) 15.8 (+1.4) 17.1 (+1.4) 68.5 80.1 81.1 82.3 82.1 81.9 80.1 81.3 80.8 81.7 on the Winnoground dataset [44]. The results are shown in Table 3, and clearly demonstrate that TULIP is able to perform visual reasoning at high level compared to existing vision and language models. 4.3. Vision & Language Models One of the motivations for developing strong vision and language models is their applications as feature-encoders for large-scale multimodal models such as LLaVA [32, 33]. To evaluate our models performance in these applications, we fine-tune Llama-3.2 11B using set of visual encoders using the LLaVA mixture data. We then evaluate their performance on several benchmarks, including the BLINK benchmark [20] (which consists of 14 primarily perceptual tasks including correspondence, visual similarity, and depth estimation), the MMVP benchmark [46] (which tests models visual capability), and LLaVA Bench [32] (which tests models ability to perform conversation, detail description and complex reasoning). Results on the BLINK dataset are shown in Table 4. We can see here that TULIP performs strongly across all classes of problems, performing particularly well in vision-driven tasks compared to base methods, where TULIP outperforms Fun.- Corr. 80.77 25 40.77 24.62 26.15 20.77 28.46 25.16 23.12 25. Sem.- Corr. 96.07 25 53.96 30.94 28.78 23.74 23.02 24.93 27.59 29.61 Spatial Local. 98.25 50 69.23 69.23 72.73 74.83 69.93 74.56 75.01 76.23 98.00 50 59.84 52.46 54. 59.02 48.36 57.64 58.21 60.01 Vis.- Corr. 99.42 25 75.00 52.33 33.72 30.81 31. 47.90 46.23 48.97 Multiview 92.48 50 59.40 52.63 55.64 62.41 51.88 40.14 44.66 44. Reflect. Forensic IQ 95.14 33.33 37.31 32.84 38.81 31.34 36.57 34.78 33.01 35. 100.00 25 79.55 63.64 34.09 44.70 43.94 46.29 48.56 49.07 80.00 25 31.33 32.67 22. 26.00 21.33 26.03 28.08 28.38 GPT-4o in tasks such as spatial reasoning and localization. The results on MMVP and LLaVA are shown in Table 5. While DINOv2-fine-tuned models perform well on the MMVP benchmark, they struggle with language-centric tasks, while CLIP-style models perform better on languagecentric tasks, but struggle with visual perception. TULIP allows the best of both worlds in single model, outperforming DINOv2 and SigLIP in their respective best tasks. Ablations. Table 5 also shows the performance of TULIP with several components removed. We can see that the largest improvements on MMVP are drawn from the imageimage contrastive learning, along with our base data training pipeline. Reconstruction serves to further improve both the vision and LLaVA benchmark performance. GeCo primarily improves the performance on vision-centric tasks. Interestingly, the LLaVA bench performance seems saturated (in regards to both scale and improvement), suggesting that improving performance on this task requires improvements in the large language model or visual adapter. 5. Conclusion This work introduces TULIP, family of multimodal selfsupervised image-text contrastive foundation models that leverage learning fine-grained visual features while maintaining global semantic alignment. By unifying imageimage contrastive learning with multimodal generative data augmentation, TULIP achieves SOTA performance across range of benchmarks at scales up to 1B parameters. TULIP only represents the beginning for multi-view and generative-view models. As multimodal systems continue to advance, future work can explore broader modality integration and more efficient scaling techniques to push the boundaries of vision-language understanding."
        },
        {
            "title": "Acknowledgements",
            "content": "Authors, as part of their affiliation with UC Berkeley, were supported in part by the National Science Foundation, US Department of Defense, and/or the Berkeley Artificial Intelligence Research (BAIR) industrial alliance program."
        },
        {
            "title": "References",
            "content": "[1] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1561915629, 2023. 2 [2] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David Fleet. Synthetic data from diffusion models improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023. 3 [3] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for In IEEE International Conference on end-to-end retrieval. Computer Vision, 2021. 6, 12 [4] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019. 7, 12 [5] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for selfsupervised learning. arXiv preprint arXiv:2105.04906, 2021. 2 [6] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learnarXiv preprint ing visual representations from video. arXiv:2404.08471, 2024. 2 [7] Lucas Beyer, Olivier Henaff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020. 7, 12 [8] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. 6 [9] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In Proceedings of the European conference on computer vision (ECCV), pages 132149, 2018. 2 [10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912 9924, 2020. 2 [11] David Chan, Suzanne Petryk, Joseph Gonzalez, Trevor Darrell, and John Canny. Clair: Evaluating image captions with large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1363813646, 2023. [12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. 2 [13] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. 2 9 [14] Gordon Christie, Neil Fendley, James Wilson, and Ryan In Proceedings Mukherjee. Functional map of the world. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 61726180, 2018. 7, 13 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 7, 12 [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 4 [17] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [18] Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph Gonzalez, and Trevor Darrell. Diversify your vision datasets with automatic diffusion-based augmentation. Advances in neural information processing systems, 36:79024 79034, 2023. 2 [19] Letian Fu, Long Lian, Renhao Wang, Baifeng Shi, Xudong Wang, Adam Yala, Trevor Darrell, Alexei Efros, and Ken Goldberg. Rethinking patch dependence for masked autoencoders. arXiv preprint arXiv:2401.14391, 2024. 2 [20] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. 8 [21] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. DatIn search of the next generation of multimodal acomp: datasets. Advances in Neural Information Processing Systems, 36:2709227112, 2023. 6, 12 [22] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. 2 [23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. [24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 2 [25] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic data from generative models ready for image recognition? arXiv preprint arXiv:2210.07574, 2022. 2 [26] Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion. Advances in Neural Information Processing Systems, 36:82668279, 2023. 4 [27] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. arXiv preprint arXiv:2405.07987, 2024. 3 [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representaIn International tion learning with noisy text supervision. conference on machine learning, pages 49044916. PMLR, 2021. 1, [29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009. 7, 13 [30] Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, et al. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478, 2024. 6, 12 [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 7 [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 8 [33] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 8 [34] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. 7, [35] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: self-supervision meets language-image arXiv preprint pre-training. corr abs/2112.12750 (2021). arXiv:2112.12750, 2021. 3 [36] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2023. 2 [37] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correIn Prospondences for richer image-to-sentence models. ceedings of the IEEE international conference on computer vision, pages 26412649, 2015. 7 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 1, 2, 3 [39] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imIn International conference on machine learning, agenet? pages 53895400. PMLR, 2019. 7, 12 [40] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, 2018. [41] Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurelie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. Foil it! find one mismatch between image and language caption. arXiv preprint arXiv:1705.01359, 2017. 5 [42] Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, and Clinton Fookes. Diversity is definitely needed: Improving model-agnostic zero-shot classification via stable diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 769778, 2023. 2 [43] Maciej Sypetkowski, Morteza Rezanejad, Saber Saberian, Oren Kraus, John Urbanik, James Taylor, Ben Mabey, Mason Victors, Jason Yosinski, Alborz Rezazadeh Sereshkeh, et al. Rxrx1: dataset for evaluating experimental batch correction methods. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 42854294, 2023. 7, 13 [44] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visiolinguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52385248, 2022. 8, 13 [45] Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan. Stablerep: Synthetic images from text-toimage models make strong visual representation learners. Advances in Neural Information Processing Systems, 36: 4838248402, 2023. 3 [46] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 8 [47] Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective data augmentation with diffusion models. arXiv preprint arXiv:2302.07944, 2023. [48] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, 10 and dense features. arXiv preprint arXiv:2502.14786, 2025. 2, 3 [49] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 87698778, 2018. 7, 12 [50] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised In Proceedings of the IEEE/CVF convisual pre-training. ference on computer vision and pattern recognition, pages 30243033, 2021. 2 [51] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille, and Christoph Feichtenhofer. Diffusion models as masked autoencoders. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16284 16294, 2023. [52] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: large-scale dataset of multi-view images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 91509161, 2023. 6, 12 [53] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via In International conference on maredundancy reduction. chine learning, pages 1231012320. PMLR, 2021. 2 [54] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 1, 3, 7 [55] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image bert pre-training with online tokenizer. In International Conference on Learning Representations, 2021."
        },
        {
            "title": "Appendix",
            "content": "The appendix consists of the following further discussion: Appendix discusses the model release. Appendix discusses the datasets that we use for pretraining both TULIP and GeCo. Appendix discusses the datasets that we use to evalaute TULIP. Appendix discusses the implementation details for the generative data augmentation portion of our approach. Appendix discusses some model detail configurations. Appendix provides some visualizations of the selfattention weights of the TULIP model. directly associated textual descriptions, resulting in higherquality supervision for training vision-language models. MVImgNet: MVImgNet [52] is large-scale dataset of multi-view images, designed as bridge between 2D and 3D vision by capturing real-world objects from multiple viewpoints. The dataset consists of 6.5 million frames extracted from 219,188 videos, covering 238 object classes with extensive annotations including object masks, camera parameters, and point clouds. Unlike single-image datasets like ImageNet, MVImgNet is built from videos, capturing objects from different angles, which naturally introduces 3D-aware visual signals. A. Code Release C. Evaluation Datasets For more information on the code, and for all models, see https://tulip-berkeley.github.io. B. Training Data We pre-train all models on the DataComp-1B dataset [21]. DataComp-1B is large-scale dataset comprising approximately 1.4 billion image-text pairs, curated from the CommonPool collection of 12.8 billion samples. We also train with captions from Recap-DataComp-1B [30], large-scale dataset where approximately 1.3 billion images from DataComp-1B have been re-captioned using LLaMA3-powered LLaVA-1.5. The goal of this recaptioning process is to enhance the textual descriptions associated with web-crawled image-text pairs, addressing issues like misalignment, brevity, and lack of descriptive detail in original captions. The new dataset has longer and more diverse textual annotations, increasing from an average 10.22 words per caption to 49.43 words, capturing richer contextual details. GeCo is fine-tuned on standard augmentations, as well as the following data: WebVid-10M: WebVid-10M [3] is large-scale video-text dataset designed to support video-language model training and text-to-video retrieval tasks. The dataset is automatically collected from the web using pipeline similar to Conceptual Captions [40], ensuring diverse and naturally occurring video-caption pairs. key feature of WebVid-10M is that it focuses on real-world, diverse, and multimodal video content, making it more challenging and representative dataset compared to traditional manually annotated datasets. The dataset spans wide range of video types, including people performing actions, nature scenes, travel vlogs, and instructional content. Unlike other large-scale video datasets such as HowTo100M, which rely on automated speech recognition (ASR) transcriptions (often introducing noise and weak supervision), WebVid-10M provides ImageNet-1K: The ImageNet-1K dataset [15] is largescale benchmark dataset widely used for training and evaluating deep learning models in computer vision. It consists of approximately 1.28 million training images, 50,000 validation images, and 100,000 test images, categorized into 1,000 distinct object classes. These classes span diverse range of objects, including animals, vehicles, tools, and everyday items, making it comprehensive dataset for image classification tasks. ImageNet-V2 [39] is re-evaluated version of the original ImageNet dataset, designed to assess the generalization ability of models trained on ImageNet-1K. It consists of 10,000 images curated using the same class distribution and data collection process as the original validation set but sourced independently to reduce potential ImageNet-ReaL [7] is re-annotated verdataset biases. sion of the ImageNet validation set, created to provide more accurate and comprehensive labels. Unlike the original ImageNet-1K validation set, where each image is assigned single ground truth label, ImageNet-ReaL introduces multilabel annotations, acknowledging that many images contain multiple valid object categories. ObjectNet: ObjectNet [4] is real-world test dataset designed to evaluate the robustness and generalization of image classification models beyond standard benchmarks like ImageNet-1K. It consists of 50,000 images featuring objects from 313 categories, many of which overlap with ImageNet classes. Unlike ImageNet, ObjectNet introduces systematic variations in object orientation, background, and viewpoint, making it significantly more challenging for models. iNaturalist-2018: iNaturalist-2018 [49] is large-scale image classification dataset focused on fine-grained species recognition, designed to challenge models with real-world biodiversity data. It contains 437,513 training images and 24,426 validation images across 8,142 species, spanning diverse categories such as plants, insects, birds, mammals, iNaturalistand fungi. Unlike datasets like ImageNet, 12 D. Data Augmentation As discussed in subsection 3.2, we generate both positive view and negative views for contrastive learning. We show some example in Figure 5. To generate positive view of the image, we input positive embedding Ep. and high image classifier free guidance (cfg) scale 5. To generate negative view of the image, we input negative embedding En to the model with lower image cfg scale 3. To generate paraphrases for the image augmentation model, we use the prompt in Figure D.1, which can generate positive and negative example for an input caption. E. Model Configurations Table E.1 provides an overview of our model configurations, detailing key parameters such as image size, sequence length, hidden size, number of layers, and text context length. We follow SigLIP 2 to use So400M language encoder for ViT-G/16. F. Attention Visualization Figure F.1 shows visualization of the attention heads of the So/14 model. We can see that similar to DINOv2, the model performs local semantic segmentation as an emergent behavior the in the attention weights. 2018 exhibits long-tailed class distributions, meaning some species have thousands of images while others have only few, mimicking real-world imbalances in biodiversity data. CIFAR-100: CIFAR-100 [29] is small-scale image classification dataset designed for evaluating machine learning models, particularly in the context of deep learning. It consists of 60,000 color images of size 3232 pixels, with 50,000 training images and 10,000 test images. The dataset contains 100 classes, each with 600 images, and these classes are further grouped into 20 superclasses (e.g., aquatic mammals, vehicles, flowers). RxRx1: RxRx1 [43] is biological image dataset designed for evaluating domain generalization in deep learning models, specifically in the context of cellular microscopy images. It consists of 125,510 images of human cells treated with various chemical perturbations, captured using highthroughput fluorescence microscopy. key challenge in RxRx1 is that images come from multiple experimental batches across four cell types, introducing batch effectssystematic variations that can hinder model generalization. fMoW: fMoW (Functional Map of the World) [14] is large-scale remote sensing dataset designed to evaluate model performance on satellite image classification and change detection tasks. It contains over 1 million images from diverse geographic locations, covering 62 categories of functional land use and infrastructure, such as airports, military facilities, bridges, and solar farms. The dataset includes images captured under varied lighting conditions, seasonal changes, and resolutions, making it challenging benchmark for real-world geospatial analysis. Infographic: InfographicVQA [34] is dataset designed for Visual Question Answering (VQA) on infographics, which are complex document images combining text, graphics, and data visualizations. The dataset consists of 5,485 images and 30,035 questions, with annotations requiring reasoning over various elements such as tables, figures, maps, and textual content. Unlike traditional VQA datasets, InfographicVQA places emphasis on elementary reasoning skills, including counting, sorting, and basic arithmetic operations. Winnoground: Winoground [44] is dataset introduced to evaluate the ability of vision-and-language models to perform visio-linguistic compositional reasoning. Each of the 400 examples in the dataset consists of two images and two captions, where both captions contain the same set of words arranged differently, leading to distinct meanings. The task requires models to correctly match each image with its corresponding caption, testing their understanding of how word order affects meaning in visual context. 13 Given an input caption describing an image, generate two variants: Positive Example: paraphrased version that preserves the exact meaning using synonyms, grammatical reordering, or structural changes (e.g., active/passive voice). Negative Example: minimal, plausible alteration that subtly contradicts the original meaning. Prioritize compositional changes (e.g., swapped roles, spatial relations, object attributes, or verb actions) while keeping lexical overlap high. The negative should be visually distinct but textually similar to trick models. Guidelines: Positive Paraphrase: Use synonyms (cube square), reorder clauses (X beside Y next to X), or adjust syntax (holding leash gripping dogs lead). Ensure no key details (objects, relationships, attributes) are altered. Hard Negative: Swap Roles/Relations: Invert subject-object relationships (a man riding horse horse beside man). Modify Prepositions/Spatial Logic: Change directional/positional cues (left of under). Alter Attributes: Adjust colors, sizes, or quantities (three red apples two green apples). Reorder Phrases with Identical Words: Use the same words in different order to invert meaning (plants surrounding lightbulb lightbulb surrounding some plants). Example: Input: chef in white hat is slicing vegetables on stainless steel counter while cat watches from the windowsill. Positive: cook wearing white cap chops veggies on shiny metal countertop as feline observes from the window ledge. (Synonym substitution + rephrasing) Negative: cat in white hat is slicing vegetables on stainless steel counter while chef watches from the windowsill. (Role swap: chef cat + retained details create contradictory but plausible scene.) Figure D.1. The GeCo prompt."
        },
        {
            "title": "Hyperparameter",
            "content": "ViT-G/16 Embed Dim Init Logit Bias Image Size Patch Size Layers (Vision) Width (Vision) Head Width (Vision) MLP Ratio Pooling Projection Context Length Vocab Size Tokenizer Width (Text) Heads Layers (Text) No Causal Mask Projection Bias Pool Type Norm Eps Activation Approx. Attentional Pool Attn Pooler Queries Attn Pooler Heads Pos Embed Type Final LN After Pool Output Tokens Timm Pool Timm Proj Timm Proj Bias Timm Drop Timm Drop Path 1536 -10 384 16 43 1536768 64 3.7362 map none 70 109871 tuliptokenizer 1152 16 27 True True last 106 tanh False 256 8 learnable False False map none False 0.0 None ViTSO400M 1152 -10 384 14 27 1152768 64 3.7362 map none 70 109871 tuliptokenizer 1152 16 27 True True last 106 tanh False 256 8 learnable False False map none False 0.0 None ViT-H-14 ViT-B-16 1152 -10 224 14 32 1280 80 3.7362 tok linear 70 109871 tuliptokenizer 1024 16 24 True True last 106 tanh False 256 8 learnable False False avg linear False 0.0 None 768 -10 224 16 12 768 64 4.0 map none 70 109871 tuliptokenizer 768 12 12 True True last 106 - False 256 8 learnable False False map none False 0.0 None Table E.1. Comparison of Vision Transformer (ViT) Model Hyperparameters for different TULIP variants. Figure F.1. Visualization of the attention heads. Attention maps are averaged across transformer blocks, then up-sampled to the resolution of the original image."
        }
    ],
    "affiliations": [
        "University of California, Berkeley"
    ]
}