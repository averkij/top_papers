{
    "paper_title": "Logical Reasoning in Large Language Models: A Survey",
    "authors": [
        "Hanmeng Liu",
        "Zhizhang Fu",
        "Mengru Ding",
        "Ruoxi Ning",
        "Chaoli Zhang",
        "Xiaozhang Liu",
        "Yue Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems."
        },
        {
            "title": "Start",
            "content": "Logical Reasoning in Large Language Models: Survey Hanmeng Liu1 , Zhizhang Fu1 , Mengru Ding1 , Ruoxi Ning1 Chaoli Zhang2 , Xiaozhang Liu3 and Yue Zhang1 1 Westlake University 2 Zhejiang Normal University 3 Hainan University {liuhanmeng, zhangyue}@westlake.edu.cn, {fuzhizhang.fzz, dingmengru2021}@gmail.com, ruoxining@outlook.com, chaolizcl@zjnu.edu.cn, lxzh@hainanu.edu.cn 5 2 0 2 3 1 ] . [ 1 0 0 1 9 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms deductive, inductive, abductive, and analogical and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems. intelligence (AI) and natural"
        },
        {
            "title": "1 Introduction\nreasoning is a fundamental challenge to arti-\nLogical\nficial\nlanguage processing\n(NLP) [Newell and Simon, 1956; McCarthy and Hayes,\n1981; McCarthy, 1959]. While early formal logic-based rea-\nsoning approaches faced limitations in scalability and adapt-\n[Pereira, 1982; Cann, 1993], data-driven models\nability\nbecame the dominant method since the 1980s [McCarthy,\n1989]. Recently, pre-trained Large Language Models (LLMs)\nand their emergent logical reasoning abilities have attracted\nincreasing attention [Liu et al., 2023b; Xu et al., 2023].\nLogical reasoning integrates LLMs with inference struc-\nturing, enabling multistep deduction and abstraction, and\nimproving interpretability and reliability [Shi et al., 2021;\nStacey et al., 2022; Rajaraman et al., 2023]. It also strength-\nens generalization, helping models handle novel scenarios\nbeyond their training data [Haruta et al., 2020]. As LLMs\nbecome integral to domains like legal analysis and scien-\ntific discovery, ensuring the correctness and verifiability of",
            "content": "* Corresponding author. Equal contribution. their reasoning is increasingly vital. As result, post-training LLM for reasoning has garnered surge of interest in both industry and research[OpenAI, 2024; DeepSeek-AI, 2025; Muennighoff et al., 2025]. Despite growing research, existing surveys [Plaat et al., 2024; Sun et al., 2023; Yu et al., 2024] often conflate logical reasoning with general-purpose heuristic strategies like Chain-of-Thought (CoT) [Xia et al., 2024]. There has been lack of literature review dedicated to LLM and formal symbolic logic. This survey provides comprehensive review of logical reasoning in large language models (LLMs), with focus on formal and symbolic logic-based reasoning rather than general heuristic approaches. We begin by defining logical reasoning in AI, distinguishing it from generalpurpose reasoning, and categorizing key paradigms, including deductive, inductive, abductive, and analogical reasoning. Additionally, we analyze existing benchmarks and evaluation methodologies, identifying gaps in assessing symbolic inference, consistency, and robustness. We further explore state-of-the-art techniques for enhancing logical reasoning, such as instruction fine-tuning, logic-informed pre-training, reinforcement learning, inference-time decoding strategies, and hybrid neuro-symbolic methods. We examine recent advances in neuro-symbolic integration, along with applications of theorem provers, logic solvers, and formal verification frameworks in LLMs. Finally, we highlight open challenges in scalability, reasoning consistency, explainability, and efficiency, proposing future directions for multi-modal reasoning, hybrid architectures, and improved evaluation frameworks. The structure of the subsequent chapters is illustrated in Figure 1."
        },
        {
            "title": "2.1 History of Logic Reasoning Research\nLogical reasoning can be traced back to ancient Greece,\nwhere Aristotle’s syllogisms laid the foundation for classi-\ncal logic. During the Middle Ages, scholars refined these",
            "content": "Types & history (2) Natural Language Inference (3.1) ConTRoL [Liu et al., 2021a] , FOLIO [Han et al., 2024a] , LogicNLI [Tian et al., 2021] , RulteTaker [Clark et al., 2021] , LogiBench [Parmar et al., 2023] Task & Benchmarks (3) Reading Comprehension (3.2) LogiQA [Liu et al., 2023a] , ReClor [Yu et al., 2020] , AR-LSAT [Yu et al., 2020] , CLUTRR [Sinha et al., 2019] , GSM [Cobbe et al., 2021; Li et al., 2024a] LINGOLY [Bean et al., 2024] Benchmarks and test suites (3.3) GLoRE [liu et al., 2023d] , LogiGLUE [Luo et al., 2024] , LogiTorch [Helwe et al., 2022] Deductive Reasoning (4.1) [Saparov et al., 2023] , [Yuan et al., 2023] , [Ryb et al., 2022] Inductive Reasoning (4.2) [Yang et al., 2024b] , [Bowen et al., 2024] , [Sullivan, 2024] Evaluation & Analysis (4) Abductive Reasoning (4.3) True Detective [Del and Fishel, 2023] , [Nguyen et al., 2023] Analogical Reasoning (4.4) ANALOGICAL [Wijesiriwardene et al., 2023] , [Petersen and van der Plas, 2023] , [Qin et al., 2024] Overall Analysis & Metrics(4.5) [Liu et al., 2023b] , [Xu et al., 2023] , [Liu et al., 2024c] , [Gandarela et al., 2024] , [Thatikonda et al., 2025] Expert-Curated Datasets FOLIO [Han et al., 2024a] , P-FOLIO [Han et al., 2024b] , LeanDojo [Yang et al., 2023] , Symbol-LLM [Xu et al., 2024a] Data-Centric Approaches (5.1) Synthetic Datasets RulteTaker [Clark et al., 2021] , FLD2 [Morishita et al., 2024] LLM-distilled Datasets LogiCoT [Liu et al., 2023c] , LogicPro [Jiang et al., 2024] , PODA [Wang et al., 2024b] Instruction Fine-Tuning LogiCoT [Liu et al., 2023c] , LogiPT [Feng et al., 2024] , PGL [Wang et al., 2024a] , Symbol-LLM [Xu et al., 2024a] , TPCL [Wang et al., 2024b] , n e c L L Enhancement Methods (5) Discussion (6) Model-Centric Approaches (5.2) Reinforcement Learning [Jiao et al., 2024] , [Xi et al., 2024] , Marco-o1 [Zhao et al., 2024] , Deepseek-R1-Zero [DeepSeek-AI, 2025] , Deepseek-R1 [DeepSeek-AI, 2025] Inference-Time Decoding GoT [Lei et al., 2023] , Chain of Logic [Servantez et al., 2024] , Selection-Inference [Creswell et al., 2023] , [Malon et al., 2024] , Maieutic Prompting [Jung et al., 2022] , Logic-of-thought [Liu et al., 2024a] , DetermLR [Sun et al., 2024] , Neurologic [Lu et al., 2021] , Formal-LLM [Li et al., 2024b] External Knowledge Utilization (5.3) [Zayyad and Adi, 2024] , LeanDojo [Yang et al., 2023] , LQOT [Liu et al., 2024b] , [Ouyang et al., 2023] , KnowRA [Mai et al., 2025] Neuro-Symbolic Approaches (5.4) LINC [Olausson et al., 2023] , LOGICLLAMA [Yang et al., 2024a] , CLOVER [Ryu et al., 2024] , LOGIC-LM [Pan et al., 2023] , Logic Agent [Liu et al., 2024a] , LLM-TRes [Toroghi et al., 2024] , SymbCoT [Xu et al., 2024c] , Aristotle [Xu et al., 2024b] Figure 1: The structure of this survey theories, and in the 17th century, Leibnizs universal language and calculus ratiocinator bridged logic with mathematics, foreshadowing modern computational logic. The 19th century saw George Booles Boolean algebra, which transformed logic into mathematical framework, laying the foundation for digital computing. The 20th century ushered in modern logic, with Russell and Whiteheads Principia Mathematica formalizing complex logical systems. By the mid-century, AI pioneers like John McCarthy leveraged logic for knowledge representation and automated theorem proving, leading to logic programming and knowledge bases. The 1970s introduced non-monotonic logic, enabling AI to handle commonsense reasoning. The 1980s saw logical reasoning integrate with knowledge representation, advancing expert systems for real-world applications. The 1990s saw the rise of knowledge graphs, structuring vast knowledge for complex reasoning tasks. In the 21st century, neuro-symbolic approaches have merged deep learning with logical inference, resulting in tools like DeepLogic [Cingillioglu and Russo, 2019] and SATNet [Wang et al., 2019]. Logical reasoning remains cornerstone of AI research, evolving from philosophy to modern computing. As AI advances, logical reasoning continues to shape intelligent systems, ensuring structured, interpretable, and robust decision-making."
        },
        {
            "title": "2.2 Types of Logical Reasoning\nLogical reasoning can be broadly categorized into four main\ntypes, each serving distinct purposes and applications:",
            "content": "Deductive Reasoning. This type of reasoning derives specific conclusions from general principles or premises. It operates under the rule that if all premises are true and the reasoning is valid, the conclusion must also be true. For example, given the premises All apples are red and This fruit is an apple one can deduce that This fruit is red Deductive reasoning is fundamental in fields such as mathematics and formal logic, where certainty and rigor are paramount. Inductive Reasoning. Unlike deductive reasoning, inductive reasoning draws general conclusions based on specific observations or evidence. While the conclusions are often considered probable, they are not guaranteed to be true. For instance, observing that all swans seen so far are white might lead to the inductive conclusion that All swans are white Inductive reasoning is widely used in scientific discovery and data-driven decision-making, where patterns and trends are inferred from empirical data. Abductive Reasoning. This form of reasoning seeks the most plausible explanation or cause for set of observations, often in the presence of incomplete information. Abductive reasoning is particularly useful in diagnostic tasks and realworld problem-solving. For example, seeing wet spots on the street might lead one to infer that It has recently rained While abductive conclusions are not certain, they provide practical basis for hypothesis generation and decision-making under uncertainty. Analogical Reasoning. Analogical reasoning involves drawing comparisons between similar situations or domains to make inferences or solve problems. By identifying parallels between different scenarios, this type of reasoning enables creative problem-solving and knowledge transfer. For example, understanding that planets orbit the sun in elliptical paths might lead one to analogically reason that other celestial bodies, such as comets, exhibit similar orbital characterDataset LogiQA ReClor AR-LSAT CLUTRR GSM LINGOLY ConTRoL FOLIO LogicNLI ProofWriter LogicBench GLoRE LogiGLUE LogiTorch BIG-Bench Language Zh/En En En En En En En En En En En Zh/En En En En Question Type Multichoice Multichoice Multichoice Question-answering Math word problems Question-answering ternary classification binary classification ternary classification binary classification binary classification Miscellaneous Miscellaneous Miscellaneous Miscellaneous Source Exam-based Exam-based Exam-based Rule-based Exam-based Expert-designed Exam-based Expert-designed Exam-based Exam-based Rule-based Size 15,937 6,138 2,064 6,016 19K 1,133 8,325 1,351 30K - 1,270 17 tasks Miscellaneous 24 tasks Miscellaneous 16 tasks Miscellaneous Miscellaneous 7 tasks Table 1: Main Datasets and Benchmarks of Logical Reasoning Task. istics. Analogical reasoning is particularly valuable in fields like education, design, and innovation."
        },
        {
            "title": "3 Tasks and Benchmarks\nLogical reasoning datasets and benchmarks are essential for\nevaluating the reasoning capabilities of large language mod-\nels (LLMs). These datasets can be categorized into three\ntypes based on their data sources:",
            "content": "Rule-based Datasets [Tafjord et al., 2021; Sinha et al., 2019] are automatically generated using logical rules, enabling large-scale data collection. However, ensuring diversity is crucial to avoid repetitive patterns and comprehensively evaluate reasoning capabilities. Expert-Designed Datasets [Han et al., 2024a] are constructed by domain experts, ensuring high precision and accuracy. Although typically smaller than crowd-sourced corpora, their meticulous design makes them indispensable for in-depth logical reasoning evaluation. Exam-Based Datasets [Liu et al., 2021b; Yu et al., 2020; Wang et al., 2022] originate from standardized test questions (e.g., Chinese National Civil Service Exam, LSAT, GRE), offering high-quality, expert-crafted logic problems at scale. These datasets are widely used to evaluate reasoning in realworld scenarios. Table 1 summarizes important datasets for logical reasoning, which typically cover tasks such as Natural Language Inference (NLI) (3.1) and Machine Reading Comprehension (MRC) (3.2)."
        },
        {
            "title": "3.1 Natural Language Inference (NLI)\nNLI evaluates whether a hypothesis logically follows from\na premise, directly assessing a model’s reasoning ability. La-\nbels typically fall into binary (Entailment, Non-entailment) or\nternary (Entailment, Contradiction, Neutral) classifications.\nSome datasets use True and False labels instead.",
            "content": "ConTRoL [Liu et al., 2021a] is derived from recruitment exams (e.g., bank entry, U.S. police selection), containing 8,325 entries with Correct, Incorrect, and Cant Say labels, corresponding to Entailment, Contradiction, and Neutral. FOLIO [Han et al., 2024a] is an expert-constructed dataset for First-Order Logic (FOL) reasoning, consisting of 1,351 entries labeled as True or False, making it rigorous benchmark for formal logical inference. LogicNLI [Tian et al., 2021] contains 30K entries generated using logical rules, with Entailment, Contradiction, and Neutral labels. It isolates FOL-based inference from commonsense reasoning, enabling precise evaluation of reasoning accuracy and generalization. ProofWriter [Tafjord et al., 2021] extends RuleTaker [Clark et al., 2021] by introducing CWA (closedworld assumption) and OWA (open-world assumption) to handle negation and open-world reasoning. It includes BirdsElectricity (handcrafted domain theories) and ParaRules (crowdsourced paraphrased rules) for systematic evaluation of generalization across linguistic variations and real-world knowledge domains. LogicBench [Parmar et al., 2023] is GPT-3-generated dataset covering 25 types of reasoning, including propositional logic, FOL, and non-monotonic logic. It consists of 1,270 test entries labeled as Yes or No."
        },
        {
            "title": "3.2 Machine Reading Comprehension (MRC)\nMRC evaluates logical reasoning by requiring models to an-\nswer questions based on a given passage. Tasks are com-\nmonly formatted as multiple-choice, span extraction, or free\nresponse, with multiple-choice QA being particularly effec-\ntive due to its standardization.",
            "content": "LogiQA [Liu et al., 2023a] is sourced from the Chinese Civil Service Exam, containing 15,937 entries in Chinese and English. It targets complex logical reasoning and is widely used for evaluating LLMs. ReClor [Yu et al., 2020], derived from the GMAT, features 6,138 English entries with four-option multiple-choice questions. AR-LSAT [Wang et al., 2022] is based on the LSAT, containing 2,064 entries spanning ordering games, grouping games, and allocation games, each with five options. CLUTRR [Sinha et al., 2019] focuses on inductive reasoning, requiring models to infer kinship relationships in short narratives. It contains 6,016 entries, combining entity extraction and logical inference. GSM evaluates mathematical reasoning capabilities, comprising two datasets: GSM8K [Cobbe et al., 2021] (8.5K grade school math problems) and GSM-PLUS [Li et al., 2024a] (10,552), which is augmented with mathematical perturbations for robustness evaluation. LINGOLY [Bean et al., 2024] uses Linguistic Olympiad puzzles to evaluate in-context pattern identification and generalization in low-resource or extinct languages. It contains 1,133 problems across 6 formats and 5 difficulty levels, covering over 90 languages."
        },
        {
            "title": "3.3 Benchmark Suites\nBenchmark suites standardize evaluation and facilitate model\ncomparison in logical reasoning research.",
            "content": "GLoRE [liu et al., 2023d] is few-shot and zero-shot testing platform, including 17 test-only datasets to assess generalization in low-data scenarios. LogiGLUE [Luo et al., 2024] consists of 24 logical reasoning tasks, standardizing datasets into sequence-toIt provides sequence format for uniform input processing. both test and training sets, enabling extensive model training and targeted evaluations. LogiTorch [Helwe et al., 2022] is PyTorch-based library for natural language logical reasoning, offering 16 datasets, model architectures, and an accessible API for quick evaluation. BIG-bench [Srivastava et al., 2022] is collaborative benchmark with 7 tasks dedicated to logical reasoning, such as Logic Grid Puzzle and Logical Fallacy Detection. (a) multi-choice reading comprehension example from the LogiQA dataset. (b) An NLI example from the ConTRoL dataset. Figure 2: Example tests of Logical reasoning in NLP tasks."
        },
        {
            "title": "4.1 Deductive Reasoning\nDeductive reasoning, deriving specific conclusions from gen-\neral premises, is crucial for automated theorem proving. De-\nspite LLMs performing well on tasks like compositional\nproofs, standard benchmarks, and encoding entailment rela-\ntionships, they struggle with extended reasoning, hypothet-",
            "content": "ical sub-proofs without examples, generalization, and sensitivity to syntactic variations [Saparov et al., 2023; Yuan et al., 2023; Ryb et al., 2022]."
        },
        {
            "title": "4.5 Overall Analysis and Metrics\nLiu et al. [2023b] evaluate GPT-4 and ChatGPT on bench-\nmarks like LogiQA and ReClor, showing that while GPT-\n4 outperforms ChatGPT, both of them struggle with out-of-\ndistribution tasks. Xu et al. [2023] introduce the NeuLR\ndataset and propose a framework evaluating LLMs across six\ndimensions: correctness, rigor, self-awareness, proactivity,\nguidance, and absence of hallucinations.",
            "content": "Metrics for Evaluating Logical Reasoning. Traditional metrics like accuracy and F1 score are insufficient for assessing logical reasoning. Recent studies have introduced nuanced metrics such as consistency (invariance to logically equivalent inputs), generalization (performance on outof-distribution data), and explainability (clarity of reasoning steps). Thatikonda et al. [2025] find that combining BERTScore with traditional metrics improves alignment with human judgments. Liu et al. [2024c] propose framework for measuring logical consistency, showing that BERTScore aligns better with human rankings than LLM-based evaluators like GPT-4. Gandarela et al. [2024] emphasizes the need for metrics that account for the expressivity of logical theories, particularly in inductive reasoning."
        },
        {
            "title": "5.1 Data-Centric Approaches\nData-centric approaches enhance LLMs’ reasoning capabili-\nties by utilizing meticulously curated training datasets. For-\nmally, this can be expressed as:",
            "content": "D = arg max R(MD) (1) where: D: training datasets. MD: model trained on D. R: performance evaluator (e.g., LLM-as-a-judge, rulebased metrics). This formulation highlights the central role of dataset optimization in data-centric approaches. In practice, datacentric methods typically involve three types of datasets: expert-curated datasets, synthetic datasets, and LLM-distilled datasets. Expert-Curated Datasets. The FOLIO series [Han et al., 2024a; Han et al., 2024b] establish formal verification through FOL annotations, with P-FOLIO extending the complexity of reasoning chains for enhanced training. LeanDojo [Yang et al., 2023] provides 98k+ human-proven theorem pairs for mathematical reasoning. Additionally, SymbolLLM [Xu et al., 2024a] systematically organizes 34 symbolic reasoning tasks to capture inter-symbol relationships across 20 distinct symbolic families. Synthetic Datasets. Rule-based synthetic data remains fundamental for data generation. RuleTaker [Clark et al., 2021] formalizes this through three-phase pipeline: behavior formalization, example synthesis and linguistic equivalents generation. Similarly, Morishita et al. [2024] develops Formal Logic Deduction Diverse (FLD2), synthetic dataset based on symbolic theory and previous empirical insights. LLM-Distilled Datasets. Researchers employ advanced models such as GPT-4 for intermediate reasoning step distillation. LogiCoT [Liu et al., 2023c] augments existing datasets with GPT4-generated reasoning chains, while LogicPro [Jiang et al., 2024] combines algorithmic problems with code solutions to create variable-guided reasoning data. To advance, Wang et al. [2024b] propose PODA, which generates contrastive analyses of correct/incorrect options through premise-oriented augmentation, enabling reasoning path differentiation via contrastive learning. (θ, S) = arg max θ,S R(Mθ, S) (2) where: θ: learnable model parameters. Mθ: model with parameters θ. S: decoding strategy (e.g., chain-of-thought prompting, verification-based decoding). R: reasoning performance metric. This formulation highlights the joint optimization of model parameters θ and decoding strategy S. Practical implementations can be categorized as: Instruction Fine-Tuning: optimizing θ. Reinforcement Learning: optimizing θ. Inference-Time Decoding: optimizing S. Model-Centric approaches focus on directly improving the models reasoning capabilities by optimizing its internal mechanisms and decoding strategies, making them complementary to data-centric approaches. Instruction Fine-Tuning Instruction Fine-Tuning (IFT) adapts LLMs through supervised learning on task-specific instructions. For example, Liu et al. [2023c] design multi-grained instructions spanning diverse levels of abstraction and complexity. Similarly, Feng et al. [2024] IFT models to mimic logical solvers by replicating formal deduction reasoning processes. In addition, Xu et al. [2024a] implement two-stage symbolic fine-tuning through Injection (injecting symbolic knowledge) and Infusion (balancing symbol and NL reasoning). To overcome IFTs over-fitting limitations, Wang et [2024b] enforce contrastive learning between facal. tual/counterfactual paths with IFT. Further, Wang et al. [2024a] augment Llamas with Program-Guided Learning Framework and logic-specific architecture adjustments. Recently, Muennighoff et al. [2025] propose s1, achieving test-time scaling through IFT on 1,000 meticulously crafted long CoT samples. Combined with budget-forcing technique, it significantly enhances the reasoning capability of Qwen2.5-32B-Instruct model, allowing extrapolating beyond its performance without test-time intervention. Reinforcement Learning Reinforcement learning (RL) has become pivotal in optimizing large language models (LLMs), particularly since the breakthrough of Reinforcement Learning from Human Feedback (RLHF). Jiao et al. [2024] leverage RL for planningbased reasoning optimization, while Xi et al. [2024] develop R3, achieving process supervision benefits through outcomeonly supervision. The success of large-scale RL in OpenAI-o1 [OpenAI, 2024] has inspired numerous studies. RL algorithms train o1style models to enhance Chain-of-Thought (CoT) reasoning, addressing issues like formulaic outputs and limited longform reasoning. For instance, Zhao et al. [2024] integrate CoT instruction fine-tuning with Monte Carlo Tree Search (MCTS) decoding for multi-path reasoning exploration. In contrast, Zhang et al. [2024] employ MCTS to generate codereasoning data for instruction fine-tuning (IFT) and Direct Preference Optimization (DPO)."
        },
        {
            "title": "A significant breakthrough comes",
            "content": "from DeepSeekR1 [DeepSeek-AI, 2025], which pioneers novel RL strategy to enhance logical reasoning. DeepSeek-R1-Zero, trained purely through RL without IFT, demonstrates impressive reasoning capabilities but faces challenges in readability and language consistency. To address this, DeepSeek-R1 introduces minimal long-CoT IFT data as cold start before RL, achieving balance between usability and reasoning performance. By iteratively synthesizing high-quality reasoning data through RL, DeepSeek-R1 overcomes limitations imposed by human annotators, addressing issues such as mechanistic responses, repetitive patterns, and insufficient long-chain reasoning. This approach represents potential paradigm shift in logical reasoning optimization, pushing the boundaries of what LLMs can achieve in structured reasoning tasks. Inference-Time Decoding We categorize logical reasoning enhancement methods during inference-time into inference-time scaling and constrained decoding. Inference-time scaling employs computational augmentation without parameter updates. One common approach is decoding with structured outputs and modular workflows. GoT [Lei et al., 2023] creates structured reasoning nodes to improve complex multi-step logical reasoning. Similarly, Chain of Logic [Servantez et al., 2024] introduces Decomposition-Recomposition structure for legal reasoning. In other contexts, researchers design more complex modular workflows for better performance [Creswell et al., 2023; Malon et al., 2024]. Another inference-time scaling approach involves stimulating autonomous reasoning, guiding LLMs to iteratively refine their answers. Maieutic Prompting [Jung et al., 2022] eliminates contradictions through recursive reasoning. Similarly, Logic-of-Thoughts [Liu et al., 2024a] and DetermLR [Sun et al., 2024] progressively approach the answers in an iterative style. Constrained decoding methods, on the other hand, focus on improving the controllability and reliability of reasoning processes. Neurologic [Lu et al., 2021] enforces predicate logic constraints, while Formal-LLM [Li et al., 2024b] integrates automata for constraining plan generation."
        },
        {
            "title": "5.3 External Knowledge Utilization",
            "content": "LLMs often generate incorrect answers due to hallucinations when performing complex tasks such as logical reasoning, making it necessary to incorporate external knowledge to assist in producing accurate responses. Formally, the optimal integration of external knowledge can be formulated as joint optimization problem: (M , ) = arg max M,K R(M, K) (3) where: : the neural model, which includes both the models parameters and its decoding strategies (generally, the models parameters remain unchanged). K: knowledge integration strategy, including knowledge source curation, structured knowledge representation, retrieval-augmented mechanisms, etc. R: reasoning performance evaluator (e.g., factual accuracy, logical consistency). Zayyad and Adi [2024] and Yang et al. [2023] extract data from Lean, mathematical proof tool, to aid theorem proving. In contrast, Logic-Query-of-Thoughts (LQOT) [Liu et al., 2024b] decomposes complex logical problems into easier sub-questions before integrating knowledge graphs. In reading comprehension, Ouyang et al. [2023] construct supergraphs to address complex contextual reasoning, while KnowRA [Mai et al., 2025] autonomously determines whether to accept external knowledge to assist documentlevel relation extraction."
        },
        {
            "title": "5.4 Neuro-Symbolic Approaches\nNeural-symbolic hybrid methods represent a burgeoning re-\nsearch area that aims to combine the powerful representa-\ntional capabilities of deep learning with the precision and in-\nterpretability of symbolic reasoning.",
            "content": "Formally, neural-symbolic hybrid system aims to optimize both the neural model and the symbolic solver (where represents the symbolic reasoning process) to maximize logical reasoning performance. The overall objective can be expressed as: (M , ) = arg max M,P R(P (M (x))), where: : The neural model, which includes both the models parameters and its decoding strategies. It maps the input (e.g., natural language) into symbolic representation within formal language L: = (x), L. : The symbolic solver, which operates on the symbolic representation produced by to generate the final output y: = (z). R: The reasoning performance metric, which evaluates the ability to perform logical reasoning tasks. The optimization process involves two key directions: Improving : including refining the models parameters and decoding strategies to produce symbolic representations that are both accurate and compatible with . Enhancing : involving improving the symbolic solvers ability to process. By jointly optimizing and , neural-symbolic hybrid systems aim to leverage the strengths of both neural networks and symbolic reasoning to achieve superior logical reasoning capabilities. It is worth noting that in earlier neural-symbolic pipelines, is often implemented as fixed external logical reasoning engine, and thus is generally not optimized. However, in advanced practice, LLMs are increasingly being used to perform the role of , enabling diverse optimization. Fundamentally, these methods involve translating problems into symbolic representations with LLMs, and external symbolic solvers solving them. in LINC [Olausson et al., 2023], LLMs convert natural language (NL) into first-order logic (FOL) expressions, and utilize an external theorem prover for symbolic deductive inference. For example, Further efforts focus on improving NL-to-symbolic translation. One prevailing approach is directly optimizing translation through training [Yang et al., 2024a] or decoding strategies [Ryu et al., 2024], while the other depends on verification or correction mechanisms [Yang et al., 2024a; Pan et al., 2023]. Building upon these, recent advancements address the traditional pipeline limitations by fully integrating LLMs into reasoning processes. Logic Agent (LA) [Liu et al., 2024a] replaces external solvers with rule-guided LLM inference chains, while LLM-TRes [Toroghi et al., 2024] implements self-contained verifiable reasoning without external symbolic solvers. SymbCoT [Xu et al., 2024c] coordinates translation, planning, solving and verification entirely through LLMs. Xu et al. [2024b] propose Aristotle, which further systematizes the symbolic reasoning pipeline through three LLM-driven components: Logical Decomposer, Logical Search Router, and Logical Resolver."
        },
        {
            "title": "6 Discussion\nThe integration of logical reasoning into large language mod-\nels (LLMs) remains a critical challenge, marked by persistent\ngaps between heuristic performance and formal logical rigor.\nBelow, we analyze three unresolved tensions dominating the\nfield and outline future directions.\nRobustness vs. Generalization. LLMs exhibit inconsis-\ntent performance in structured reasoning tasks such as deduc-\ntive inference and abductive hypothesis generation. While\nmodels fine-tuned on datasets like FOLIO [Han et al., 2024a]\nexcel in controlled settings, they struggle with adversarial\nperturbations or semantically equivalent rephrasings. This\ninconsistency arises from their reliance on surface-level sta-\ntistical correlations rather than causal relationships, coupled\nwith limited out-of-distribution generalization. A key ques-\ntion persists: can LLMs achieve human-like robustness with-\nout sacrificing cross-domain adaptability? Current methods\nprioritize narrow task performance, leaving real-world appli-\ncability uncertain.\nInterpretability vs. Performance. A central tension lies in\nbalancing neural scalability with symbolic precision. Neuro-\nsymbolic approaches like Logic-LM [Pan et al., 2023] and\nSymbol-LLM [Xu et al., 2024a] embed formal logic solvers\ninto neural architectures, improving interpretability through\nstep-by-step proofs. However, these methods face scalability",
            "content": "bottlenecks with large knowledge bases or complex rule dependencies. Conversely, data-driven methods (e.g., instruction tuning on LogicBench [Parmar et al., 2024]) achieve broader task coverage but fail to generalize beyond syntactic patterns. How can we reconcile transparent reasoning with black-box model performance? Hybrid architectures offer promise but introduce computational overhead, limiting practical deployment. Evaluation Rigor. Existing benchmarks like LogiQA [Liu et al., 2021b] and ReClor [Yu et al., 2020] conflate reasoning ability with pattern recognition through multiple-choice formats. While efforts like NeuLR [Xu et al., 2023] curate neutral content to isolate reasoning from domain knowledge, they lack scope for holistic evaluation. Current metrics (e.g., accuracy, BLEU) fail to assess consistency (invariance to logically equivalent inputs) or soundness (adherence to formal proof structures). What defines gold standard for logical reasoning evaluation? Benchmarks must prioritize systematic testing of core principles (e.g., transitivity, contraposition) over task-specific performance. Future Directions. Addressing these challenges requires hybrid architectures that dynamically integrate neural and symbolic components, such as differentiable theorem provers, to balance scalability and precision. Equally important is the development of evaluation frameworks that stresstest models on perturbed logical statements (e.g., negated premises, swapped quantifiers) to isolate reasoning from memorization. Multimodal reasoning, which grounds inference in diverse modalities (text, images, code), presents untapped potential for enhancing robustness and interpretability. Finally, interdisciplinary collaborationleveraging insights from formal logic, cognitive science, and machine learningwill be essential to design systems that reason with and about uncertainty. Until LLMs reliably disentangle logic from lexicon, their deployment in high-stakes domains will remain precarious. Bridging this gap demands rigorous benchmarks, scalable hybrid methods, and redefinition of evaluation paradigms."
        },
        {
            "title": "7 Conclusion\nThis survey synthesizes the rapid advancements and persis-\ntent challenges in logical reasoning for large language models\n(LLMs). While LLMs demonstrate impressive heuristic rea-\nsoning, rigorous logical inference—spanning deductive, in-\nductive, abductive, and analogical paradigms—remains in-\nconsistent due to limitations in robustness, generalization,\nand interpretability. We analyzed strategies to enhance rea-\nsoning,\nincluding neuro-symbolic integration, data-centric\ntuning, reinforcement learning, test-time scaling, and other\nimproved decoding methods, and highlighted benchmarks\nlike FOLIO and LogiQA for systematic evaluation. Future\nprogress hinges on hybrid architectures that unify neural and\nsymbolic reasoning, robust evaluation frameworks, and scal-\nable methods for cross-domain and multimodal inference.\nAddressing these challenges will advance LLMs toward re-\nliable, interpretable reasoning critical for real-world applica-\ntions.",
            "content": "References [Bean et al., 2024] Andrew Bean, Simi Hellsten, Harry Mayne, Jabez Magomere, Ethan Chi, et al. Lingoly: benchmark of olympiad-level linguistic reasoning puzzles in low-resource and extinct languages. arXiv preprint arXiv:2406.06196, 2024. [Bowen et al., 2024] Chen Bowen, Rune Sætre, and Yusuke Miyao. comprehensive evaluation of inductive reasoning capabilities and problem solving in large language models. In Proc. of ACL Findings, pages 323339, 2024. [Cann, 1993] Ronnie Cann. Formal semantics: an introduction. Cambridge University Press, United States, 1993. [Cingillioglu and Russo, 2019] Nuri Cingillioglu and Alessandra Russo. Deeplogic: Towards end-to-end differentiable logical reasoning, 2019. [Clark et al., 2021] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. In Proc. of IJCAI, 2021. [Cobbe et al., 2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [Creswell et al., 2023] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. In Proc. of ICLR, 2023. [DeepSeek-AI, 2025] DeepSeek-AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. Technical report, 2025. [Del and Fishel, 2023] Maksym Del and Mark Fishel. True detective: deep abductive reasoning benchmark undoable for GPT-3 and challenging for GPT-4. In Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 314322, 2023. [Feng et al., 2024] Jiazhan Feng, Ruochen Xu, Junheng Hao, Hiteshi Sharma, Yelong Shen, et al. Language models can be deductive solvers. In Proc. of ACL Findings, pages 40264042, 2024. [Gandarela et al., 2024] Joao Pedro Gandarela, Danilo Carvalho, and Andre Inductive learning of logical theories with llms: complexityFreitas. graded analysis. arXiv preprint arXiv:2408.16779, 2024. [Han et al., 2024a] Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, et al. FOLIO: Natural language reasoning with firstorder logic. In Proc. of EMNLP, pages 2201722031, 2024. [Han et al., 2024b] Simeng Han, Aaron Yu, Rui Shen, Zhenting Qi, Martin Riddell, et al. P-FOLIO: Evaluating and improving logical reasoning with abundant human-written reasoning chains. In Proc. of EMNLP Findings, pages 1655316565, 2024. [Haruta et al., 2020] Izumi Haruta, Koji Mineshima, and Daisuke Bekki. Logical inferences with comparatives and generalized quantifiers. In Proc. of ACL, pages 263270, 2020. [Helwe et al., 2022] Chadi Helwe, Chloe Clavel, and Fabian Suchanek. Logitorch: pytorch-based library for logical reasoning on natural language. In Proc. of EMNLP, 2022. [Jiang et al., 2024] Jin Jiang, Yuchen Yan, Yang Liu, Yonggang Jin, Shuai Peng, et al. Logicpro: Improving complex logical reasoning via programguided learning. arXiv preprint arXiv:2409.12929, 2024. [Jiao et al., 2024] Fangkai Jiao, Chengwei Qin, Zhengyuan Liu, Nancy F. Chen, and Shafiq Joty. Learning planning-based reasoning by trajectories In Proc. of EMNLP, pages collection and process reward synthesizing. 334350, 2024. [Jung et al., 2022] Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, et al. Maieutic prompting: Logically consistent reasoning with recursive explanations. In Proc. of EMNLP, pages 12661279, 2022. [Lei et al., 2023] Bin Lei, Chunhua Liao, Caiwen Ding, et al. Boosting logical reasoning in large language models through new framework: The graph of thought. arXiv preprint arXiv:2308.08614, 2023. [Li et al., 2024a] Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. GSM-plus: comprehensive benchmark for evaluating the In Proc. of ACL, robustness of LLMs as mathematical problem solvers. pages 29612984, 2024. [Li et al., 2024b] Zelong Li, Wenyue Hua, Hao Wang, He Zhu, and Yongfeng Zhang. Formal-llm: Integrating formal language and natural language for controllable llm-based agents. arXiv preprint arXiv:2402.00798, 2024. [Liu et al., 2021a] Hanmeng Liu, Leyang Cui, Jian Liu, and Yue Zhang. Natural language inference in context - investigating contextual reasoning over long texts. Proc. of AAAI, pages 1338813396, 2021. [Liu et al., 2021b] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. 2021. [Liu et al., 2023a] Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, et al. Logiqa 2.0an improved dataset for logical reasoning in natural language understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pages 29472962, 2023. [Liu et al., 2023b] Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. Evaluating the logical reasoning ability of chatgpt and gpt-4, 2023. [Liu et al., 2023c] Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, and Yue Zhang. Logicot: Logical chain-of-thought instruction tuning. In Proc. of EMNLP Findings, pages 29082921, 2023. [liu et al., 2023d] Hanmeng liu, Zhiyang Teng, Ruoxi Ning, Jian Liu, Qiji Zhou, and Yue Zhang. Glore: Evaluating logical reasoning of large language models, 2023. [Liu et al., 2024a] Hanmeng Liu, Zhiyang Teng, Chaoli Zhang, and Yue Zhang. Logic agent: Enhancing validity with logic rule invocation, 2024. [Liu et al., 2024b] Lihui Liu, Zihao Wang, Ruizhong Qiu, Yikun Ban, Eunice Chan, et al. Logic query of thoughts: Guiding large language models to answer complex logic queries with knowledge graphs, 2024. [Liu et al., 2024c] Yinhong Liu, Zhijiang Guo, Tianya Liang, Ehsan Shareghi, Ivan Vulic, and Nigel Collier. Aligning with logic: Measuring, evaluating and improving logical consistency in large language models. arXiv preprint arXiv:2410.02205, 2024. [Lu et al., 2021] Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, NeuroLogic decoding: and Yejin Choi. (un)supervised neural text generation with predicate logic constraints. In Proc. of NAACL, pages 42884299, 2021. [Luo et al., 2024] Man Luo, Shrinidhi Kumbhar, Ming shen, Mihir Parmar, Neeraj Varshney, et al. Towards logiglue: brief survey and benchmark for analyzing logical reasoning capabilities of language models, 2024. [Mai et al., 2025] Chengcheng Mai, Yuxiang Wang, Ziyu Gong, Hanxiang Wang, and Yihua Huang. Knowra: Knowledge retrieval augmented method for document-level relation extraction with comprehensive reasoning abilities, 2025. [Malon et al., 2024] Christopher Malon, Martin Min, Xiaodan Zhu, et al. Exploring the role of reasoning structures for constructing proofs in multiIn Proc. of step natural language reasoning with large language models. EMNLP, pages 1529915312, 2024. [McCarthy and Hayes, 1981] J. McCarthy and P.J. Hayes. Some philosophical problems from the standpoint of artificial intelligence. In Readings in Artificial Intelligence, pages 431450. 1981. [McCarthy, 1959] John McCarthy. Programs with common sense. In Proceedings of the Teddington Conference on the Mechanization of Thought Processes, 1959. [McCarthy, 1989] John McCarthy. Artificial intelligence, logic and formalizing common sense. Philosophical Logic and Artificial Intelligence, pages 161190, 1989. [Morishita et al., 2024] Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, and Yasuhiro Sogawa. Enhancing reasoning capabilities of llms via prinIn Proc. of NeurIPS, pages 7357273604, cipled synthetic logic corpus. 2024. [Muennighoff et al., 2025] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, et al. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [Newell and Simon, 1956] A. Newell and H. Simon. The logic theory machinea complex information processing system. IRE Transactions on Information Theory, 1956. [Nguyen et al., 2023] Ha-Thanh Nguyen, Randy Goebel, Francesca Toni, Kostas Stathis, and Ken Satoh. How well do sota legal reasoning models support abductive reasoning?, 2023. [Olausson et al., 2023] Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, et al. LINC: neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. In Proc. of EMNLP, pages 51535176, 2023. [OpenAI, 2024] OpenAI. Learning to reason with LLMs. Technical report, 2024. [Ouyang et al., 2023] Siru Ouyang, Zhuosheng Zhang, and Hai Zhao. Factdriven logical reasoning for machine reading comprehension, 2023. [Pan et al., 2023] Liangming Pan, Alon Albalak, Xinyi Wang, and William Wang. Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. In Proc. of EMNLP Findings, pages 38063824, 2023. [Parmar et al., 2023] Mihir Parmar, Neeraj Varshney, Nisarg Patel, Santosh Mashetty, Man Luo, et al. Logicbench: benchmark for evaluation of logical reasoning, 2023. [Parmar et al., 2024] Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, et al. Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. In Proc. of ACL, pages 1367913707, 2024. [Pereira, 1982] Fernando Carlos Neves Pereira. Logic for natural language analysis. 1982. [Petersen and van der Plas, 2023] Molly Petersen and Lonneke van der Plas. Can language models learn analogical reasoning? investigating training objectives and comparisons to human performance. In Proc. of EMNLP, pages 1641416425, 2023. [Plaat et al., 2024] Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back. Reasoning with large language models, survey. arXiv preprint arXiv:2407.11511, 2024. [Qin et al., 2024] Chengwei Qin, Wenhan Xia, Tan Wang, Fangkai Jiao, Yuchen Hu, et al. Relevant or random: Can llms truly perform analogical reasoning?, 2024. [Rajaraman et al., 2023] Kanagasabai Rajaraman, Saravanan Rajamanickam, Investigating transformer-guided chaining for interpretable and Wei Shi. natural logic reasoning. In Proc. of ACL Findings, pages 92409253, 2023. [Ryb et al., 2022] Samuel Ryb, Mario Giulianelli, Arabella Sinclair, and Raquel Fernandez. AnaLog: Testing analytical and deductive logic learnability in language models. In Proceedings of the 11th Joint Conference on Lexical and Computational Semantics, pages 5568, 2022. [Ryu et al., 2024] Hyun Ryu, Gyeongman Kim, Hyemin Lee, and Eunho Yang. Divide and translate: Compositional first-order logic translaarXiv preprint tion and verification for complex logical reasoning. arXiv:2410.08047, 2024. [Saparov et al., 2023] Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Mehran Kazemi, et al. Testing the general deductive reasoning capacity of large language models using ood examples. In Proc. of NeurIPS, pages 30833105, 2023. [Servantez et al., 2024] Sergio Servantez, Joe Barrow, Kristian Hammond, and Rajiv Jain. Chain of logic: Rule-based reasoning with large language models. In Proc. of ACL Findings, pages 27212733, 2024. [Shi et al., 2021] Jihao Shi, Xiao Ding, Li Du, Ting Liu, and Bing Qin. Neural In Proc. of natural logic inference for interpretable question answering. EMNLP, pages 36733684, 2021. [Sinha et al., 2019] Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. Clutrr: diagnostic benchmark for inductive reasoning from text. Empirical Methods of Natural Language Processing (EMNLP), 2019. [Srivastava et al., 2022] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. [Stacey et al., 2022] Joe Stacey, Pasquale Minervini, Haim Dubossarsky, and Marek Rei. Logical reasoning with span-level predictions for interpretable and robust NLI models. In Proc. of EMNLP, pages 38093823, 2022. [Sullivan, 2024] Michael Sullivan. It is not true that transformers are inductive learners: Probing NLI models with external negation. In Proc. of EACL, pages 19241945, 2024. [Sun et al., 2023] Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, et al. survey of reasoning with foundation models. arXiv preprint arXiv:2312.11562, 2023. [Sun et al., 2024] Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, et al. Determlr: Augmenting llm-based logical reasoning from indeterminacy to determinacy. In Proc. of ACL, pages 98289862, 2024. [Tafjord et al., 2021] Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Proc. of ACL Findings, pages 36213634, 2021. [Thatikonda et al., 2025] Ramya Keerthy Thatikonda, Wray Buntine, and Ehsan Shareghi. Assessing the alignment of fol closeness metrics with human judgement. arXiv preprint arXiv:2501.08613, 2025. [Tian et al., 2021] Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, and Yaohui Jin. Diagnosing the first-order logical reasoning ability through LogicNLI. In Proc. of EMNLP, pages 37383747, 2021. [Toroghi et al., 2024] Armin Toroghi, Willis Guo, Ali Pesaranghader, and Scott Sanner. Verifiable, debuggable, and repairable commonsense logical reasoning via llm-based theory resolution. In Proc. of EMNLP, pages 66346652, 2024. [Wang et al., 2019] Po-Wei Wang, Priya L. Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and logical reasoning using differentiable satisfiability solver, 2019. [Wang et al., 2022] Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, et al. From lsat: The progress and challenges of complex reasoning. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2022. [Wang et al., 2024a] Chen Wang, Xudong Li, Haoran Liu, Xinyue Wu, and Wanting He. Efficient logical reasoning in large language models through program-guided learning. Authorea Preprints, 2024. [Wang et al., 2024b] Chenxu Wang, Ping Jian, and Zhen Yang. Thoughtpath contrastive learning via premise-oriented data augmentation for logical reading comprehension. arXiv preprint arXiv:2409.14495, 2024. [Wijesiriwardene et al., 2023] Thilini Wijesiriwardene, Ruwan Wickramarachchi, Bimal Gajera, Shreeyash Gowaikar, Chandan Gupta, et al. ANALOGICAL - novel benchmark for long text analogy evaluation in large language models. In Proc. of ACL Findings, pages 35343549, 2023. [Xi et al., 2024] Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, et al. Training large language models for reasoning through reverse curriculum reinforcement learning. In Proc. of ICML, 2024. [Xia et al., 2024] Yu Xia, Rui Wang, Xu Liu, Mingyan Li, Tong Yu, et al. Beyond chain-of-thought: survey of chain-of-x paradigms for llms. arXiv preprint arXiv:2404.15676, 2024. [Xu et al., 2023] Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. Are large language models really good logical reasoners? comprehensive evaluation and beyond, 2023. [Xu et al., 2024a] Fangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei Yuan, et al. Symbol-LLM: Towards foundational symbol-centric interface for large language models. In Proc. of ACL, pages 1309113116, 2024. [Xu et al., 2024b] Jundong Xu, Hao Fei, Meng Luo, Qian Liu, Liangming Pan, et al. Aristotle: Mastering logical reasoning with logic-complete decompose-search-resolve framework. arXiv preprint arXiv:2412.16953, 2024. [Xu et al., 2024c] Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, and Wynne Hsu. Faithful logical reasoning via symbolic chain-ofthought. In Proc. of ACL, pages 1332613365, 2024. [Yang et al., 2023] Kaiyu Yang, Aidan M. Swope, Alex Gu, Rahul Chalatheorem proving with retrievalmala, Peiyang Song, et al. Leandojo: augmented language models. In Proc. of ICONIP, 2023. [Yang et al., 2024a] Yuan Yang, Siheng Xiong, Ali Payani, Ehsan Shareghi, and Faramarz Fekri. Harnessing the power of large language models for In Proc. of ACL, pages natural language to first-order logic translation. 69426959, 2024. [Yang et al., 2024b] Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, et al. Language models as inductive reasoners. In Proc. of EACL, pages 209225, 2024. [Yu et al., 2020] Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: reading comprehension dataset requiring logical reasoning. In Proc. of ICLR, 2020. [Yu et al., 2024] Fei Yu, Hongbo Zhang, Prayag Tiwari, and Benyou Wang. Natural language reasoning, survey. ACM Computing Surveys, pages 1 39, 2024. [Yuan et al., 2023] Zhangdie Yuan, Songbo Hu, Ivan Vulic, Anna Korhonen, and Zaiqiao Meng. Can pretrained language models (yet) reason deductively? In Proc. of EACL, pages 14471462, 2023. [Zayyad and Adi, 2024] Majd Zayyad and Yossi Adi."
        },
        {
            "title": "Formal language",
            "content": "knowledge corpus for retrieval augmented generation, 2024. [Zhang et al., 2024] Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, et al. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024. [Zhao et al., 2024] Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, et al. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405, 2024."
        }
    ],
    "affiliations": [
        "Hainan University",
        "Westlake University",
        "Zhejiang Normal University"
    ]
}