{
    "paper_title": "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?",
    "authors": [
        "Yuanxin Liu",
        "Kun Ouyang",
        "Haoning Wu",
        "Yi Liu",
        "Lin Sui",
        "Xinhao Li",
        "Yan Zhong",
        "Y. Charles",
        "Xinyu Zhou",
        "Xu Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies have shown that long chain-of-thought (CoT) reasoning can significantly enhance the performance of large language models (LLMs) on complex tasks. However, this benefit is yet to be demonstrated in the domain of video understanding, since most existing benchmarks lack the reasoning depth required to demonstrate the advantages of extended CoT chains. While recent efforts have proposed benchmarks aimed at video reasoning, the tasks are often knowledge-driven and do not rely heavily on visual content. To bridge this gap, we introduce VideoReasonBench, a benchmark designed to evaluate vision-centric, complex video reasoning. To ensure visual richness and high reasoning complexity, each video in VideoReasonBench depicts a sequence of fine-grained operations on a latent state that is only visible in part of the video. The questions evaluate three escalating levels of video reasoning skills: recalling observed visual information, inferring the content of latent states, and predicting information beyond the video. Under such task setting, models have to precisely recall multiple operations in the video, and perform step-by-step reasoning to get correct final answers for these questions. Using VideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal LLMs (MLLMs), finding that most perform poorly on complex video reasoning, e.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our investigations on \"test-time scaling\" further reveal that extended thinking budget, while offering none or minimal benefits on existing video benchmarks, is essential for improving the performance on VideoReasonBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 9 5 3 3 2 . 5 0 5 2 : r VIDEOREASONBENCH: Can MLLMs Perform Vision-Centric Complex Video Reasoning? Yuanxin Liu1,2 Kun Ouyang1 Haoning Wu2 Yi Liu1 Lin Sui2 Xinhao Li3 Yan Zhong2,4 Y. Charles2 Xinyu Zhou2 Xu Sun1 1 National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University 2 Moonshot AI 3 Nanjing University 4 School of Mathematical Sciences, Peking University liuyuanxin@stu.pku.edu.cn wuhaoning@moonshot.cn"
        },
        {
            "title": "Abstract",
            "content": "Recent studies have shown that long chain-of-thought (CoT) reasoning can significantly enhance the performance of large language models (LLMs) on complex tasks. However, this benefit is yet to be demonstrated in the domain of video understanding, since most existing benchmarks lack the reasoning depth required to demonstrate the advantages of extended CoT chains. While recent efforts have proposed benchmarks aimed at video reasoning, the tasks are often knowledgedriven and do not rely heavily on visual content. To bridge this gap, we introduce VIDEOREASONBENCH, benchmark designed to evaluate vision-centric, complex video reasoning. To ensure visual richness and high reasoning complexity, each video in VIDEOREASONBENCH depicts sequence of fine-grained operations on latent state that is only visible in part of the video. The questions evaluate three escalating levels of video reasoning skills: recalling observed visual information, inferring the content of latent states, and predicting information beyond the video. Under such task setting, models have to precisely recall multiple operations in the video, and perform step-by-step reasoning to get correct final answers for these questions. Using VIDEOREASONBENCH, we comprehensively evaluate 18 state-of-the-art multimodal LLMs (MLLMs), finding that most perform poorly on complex video reasoninge.g., GPT-4o achieves only 6.9% accuracywhile the thinking-enhanced Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our investigations on test-time scaling further reveal that extended thinking budget, while offering none or minimal benefits on existing video benchmarks, is essential for improving the performance on VIDEOREASONBENCH. Data: Code: Project: huggingface.co/datasets/lyx97/reasoning_videos github.com/llyx97/video_reason_bench https://llyx97.github.io/video_reason_bench/"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in long chain-of-thought (CoT) reasoning [2, 8, 27] have remarkably enhanced the problem-solving capabilities of large language models (LLMs). By scaling up the test-time compute with extended CoT reasoning chains, substantial performance gains have been observed in complex tasks such as mathematics [18, 15, 17], coding [9, 10], and scientific reasoning [25]. However, the Project Lead Corresponding Author(s) Preprint. Under review. Figure 1: Examples from VIDEOREASONBENCH and three existing VideoQA benchmarks. Responses are generated by Gemini-2.5-Flash in both Thinking and No Thinking modes. The text highlighted in green/red indicate correct/incorrect responses. While questions from existing benchmarks can be answered correctly without Thinking using only few tokens, VIDEOREASONBENCH requires Thinking for accurate reasoning and consumes substantially more tokens (See Figure 5 for quantitative results). It also demands finer-grained visual perception during reasoning. benefits of long CoT reasoning have not been fully demonstrated in the domain of video understanding. This gap is largely due to limitations in existing benchmarks [19, 4, 16, 12, 29, 34, 14, 26], which often lack the reasoning depth necessary to showcase the advantages of extended CoT chains. As shown in Figure 1, the advanced multimodal LLM (MLLM) Gemini-2.5-Flash can correctly answer the questions from two popular benchmarks, Video-MME [4] and TempCompass [16] using only few response tokens without activating the thinking mode. To address this gap, several benchmarks have been proposed recently to better emphasize CoT reasoning in video understanding. Video-MMMU [7] and MMVU [33] integrate video understanding with domain-specific knowledge, thus introducing need for reasoning. However, the required reasoning process is primarily knowledge-driven, lacking strong reliance on the visual content. Two concurrent studies, VCR-Bench [24] and MINERVA [20], evaluate the correctness of video reasoning process in addition to the final answer. Nonetheless, the videos and questions in these benchmarks often resemble those in earlier benchmarks, and fall short in demanding deeper video reasoning. Motivated by these limitations, this work introduces the VIDEOREASONBENCH to evaluate the capabilities of MLLMs in performing vision-centric, complex video reasoning. We define three levels of video reasoning, each requiring progressively more sophisticated reasoning: The first level is to precisely recall the sequential visual observations from the video. The second level is to infer latent information that is not directly observable from the video. The third level is to predict new information beyond the video. For instance, as shown in Figure 1, video from VIDEOREASONBENCH presents \"sliding number puzzle\", in which numbered tiles are initially visible but become masked as sliding movements occur. To accurate answer the question, model must first recall the initial tile arrangement and all subsequent movements (Level 1), then infer the final arrangement of tiles (Level 2), and finally apply this inferred information to predict future tile positions (Level 3). 2 VIDEOREASONBENCH is constructed based on the aforementioned core ideas. Each video illustrates sequence of operations (e.g., sliding movements) performed to latent state (e.g., tile arrangement). The richness of visual information can be flexibly controlled by adjusting the size of the latent state and the number of operations. In addition to the \"sliding number puzzle\", our benchmark includes six types of video demonstrations spanning various scenes, featuring both synthetic and real-world videos. To evaluate reasoning across all three levels, we design six corresponding reasoning skills, with two for each level (see Figure 2). Based on VIDEOREASONBENCH, we comprehensively evaluate 18 state-of-the-art MLLMs. Our results reveal that most MLLMs struggle with vision-centric complex video reasoning, achieving accuracies below 10%. In contrast, the thinking-augmented Gemini-2.5-Pro significantly outperforms all other models, reaching an accuracy of 56%. Further analysis shows that while extended chainof-thought (CoT) reasoning offers minimal performance improvements on existing benchmarks, it is crucial to VIDEOREASONBENCH. Additionally, we observe that removing visual information from VIDEOREASONBENCH leads to substantially larger drop in performance compared to other benchmarks, highlighting its strong reliance on visual content. The main contributions of this work are summarized as follows: 1. We introduce the VIDEOREASONBENCH for evaluating vision-centric, complex video It poses necessity for models to correctly perceive multiple actions in reasoning. sequential order and perform step-by-step reasoning to finally answer the questions, therefore by-principle featuring higher demand for reasoning depth and stronger reliance on the visual content. 2. We reveal the concerning deficiency of most SOTA MLLMs in our benchmark: Several latest thinking models, such o4-mini and Seed1.5-VL, only gets around 10% accuracy; non-thinking SOTA MLLMs (e.g. GPT-4o and Qwen2.5VL-72B) scores lower than 10%; all efficient MLLMs (<10B) cannot reach even 2%. 3. Our experimental investigation confirms that the accuracy of Gemini-2.5-Flash drastically degrade while dropping 50% input video or disabling thinking-mode, while existing video benchmarks do not show similar properties. This result underscores the value of VIDEOREASONBENCH as paragon to evaluate vision-centric complex video reasoning abilities."
        },
        {
            "title": "2.1 Task Definition",
            "content": "Existing research lacks clear and established definition of what is vision-centric complex video reasoning. To address this gap, we propose systematic framework that formally defines the task, incorporating both video content design and different reasoning question skills."
        },
        {
            "title": "2.1.1 Videos",
            "content": "We conceptualize videos as sequence of state transitions, represented as {St, ot, St+1}T 1 t=1 , where an operation ot transforms state St into St+1. In our framework, the full sequence of operations is visually observable, while the states are only partially visibleeither at the beginning or at the end of the video. Thus, the visible components of video are either: {S1, o1, oT 1} or {o1, oT 1, ST }. This design enforces visual complexity via the dense sequence of operations and fosters reasoning complexity by requiring inference about latent states. As illustrated in Figure 2, we design six categories of video demonstrations based on this principle: Number: The latent state is an board with numbered tiles and one empty tile. Operations consist of sliding numbered tile into the empty space. Circle: The latent state is an grid containing black and white pieces. red circle moves across the grid, flipping the color of the pieces it passes over, as well as their neighbors. Cup: The latent state is an board with squares that may be empty or contain coin, all covered by cups. Operations involve swapping the positions of two cups, altering the contents beneath. 3 Figure 2: Illustration of vision-centric complex video reasoning. Upper: In each video, the latent state is revealed either at the begin or the end, and sequence of observable operations is applied to this state. There are six categories of videos, each featuring different type of demonstration. Lower: The questions assess video reasoning across three levels, with two skills for each level. File: The latent state consists of file paths. Operations include creating, deleting, copying, and moving files within/between these paths. Card: The latent state comprises piles of cards. Operations involve adding card to the top of pile or removing card from the bottom. Chip: The latent state consists of cups, each containing number of chips. Operations involve adding or removing chip from cup."
        },
        {
            "title": "2.1.2 Questions",
            "content": "As Figure 2 shows, our framework evaluates video reasoning skills across three progressive levels. Level 1 focuses on fine-grained visual perception, comprising two sub-tasks: Recall Order, which requires recalling the exact sequence of operations, and Recall Count, which involves counting the frequency of specific operations. Level 2 assesses reasoning about latent states based on the observed operations. This includes Infer State, where the task is to infer the content of latent state at certain moment, and Compare State, which requires comparing the latent state between two moments. Level 3 advances to counterfactual reasoning, requiring prediction based on inferred information. It includes Predict State, where the goal is to predict future state after sequence of operations, and Predict Operation, which involves identifying the operations needed to reach given target state. 4 Figure 3: Overview of our data construction framework. The video engine generates state transitions from given configuration, producing videos via Matplotlib, command-line screenshots, or real-world manual recordings. The question engine then generates questions and derives answers based on the state transitions, following the rules of each demonstration. Table 1: VIDEOREASONBENCH statistics. Statistics Value Question Word Count (avg/max) Full Prompt Word Count (avg/max) 81.5/262 198.8/420 Duration (Seconds, avg/max) Videos by Operation Count 54.3/154.8 5 9 10 14 Videos by State Size 1 & (3 3) 2 & (4 4) 120 120 120 Figure 4: VIDEOREASONBENCH video and question distributions."
        },
        {
            "title": "2.2 Data Construction",
            "content": "We construct our dataset based on the previously described task definitions. Table 1 and Figure 4 summarizes the dataset statistics. In total, VIDEOREASONBENCH consists of 1,440 questions and 240 videos, with an equal number of questions per skill and an equal number of videos per demonstration. The number of operations depicted in each video ranges from 5 to 14. For demonstrations involving File, Card, and Chip, the latent states consist of one or two sets (e.g., one or two piles of cards), with each category containing 120 videos. For the Number, Circle, and Cup demonstrations, the latent states are represented as boards ranging from 3 3 to 4 4, also with 120 videos per category. To collect the dataset at scale, we design semi-automatic data construction framework comprising two main components: video engine and question engine, as demonstrated in Figure 3."
        },
        {
            "title": "2.2.1 Video Engine",
            "content": "Given the state size and the number of operations , the video engine begins by randomly initializing state, such as an integer matrix ZN for the Number demonstration. It then generates sequence of operations and corresponding state transitions, represented as {St, ot, St+1}T 1 t=1 , according to predefined rules specific to each demonstration type. These transitions form \"script\" that guides the video construction process. For demonstrations such as Number, Circle, and Cup, videos are generated programmatically using the Python Matplotlib library3. In the File demonstration, we simulate file operations within command-line interface and capture screenshots. For real-world demonstrations like Card and Chip, we record actual videos manually. 3https://matplotlib.org/"
        },
        {
            "title": "2.2.2 Question Engine",
            "content": "For each combination of video demonstration and reasoning skill, we define specific question templates. For instance, in the Number demonstration with the Infer State skill, the corresponding template is \"What is the arrangement of numbers on the board at the {timestamp} of the video?\", where timestamp {start, end} based on whether the latent state is revealed at the start or end of video. To support accurate understanding of the task, each prompt also includes detailed description of the video demonstration, clarifying the state transition rules. Additionally, we append an answer prompt\"Provide summary of the final answer after Final Answer:\"after the question to help extract the final answer from the model response. The complete prompts and comprehensive list of templates is provided in Appendix A.1. Using these templates and the associated state transition data, we automatically generate answers via hand-crafted rules, enabling efficient and accurate dataset construction."
        },
        {
            "title": "2.3 Evaluation Scheme",
            "content": "Except for the Predict Operation category, all questions in VIDEOREASONBENCH are paired with ground-truth answers. For these questions, we evaluate model responses by inputting the question, ground-truth answer, and model-generated answer into text-only LLM, which assesses the correctness of the response. In the Predict Operation category, however, ground-truth answers are not provided, as multiple valid sequences of operations can achieve the given target state. Instead, we extract operations from the model response using the text-only LLM, simulate the corresponding state transitions using the same functions employed by the video generation engine, and then verify whether the resulting state matches the target state. Detailed evaluation and operation extraction prompts are provided in Appendix A.2."
        },
        {
            "title": "3.1 Experimental Setups",
            "content": "Evaluated Models. Based on VIDEOREASONBENCH, we conduct comprehensive evaluation of wide range of MLLMs: Proprietary models include the advanced GPT-4o (2024-11-20) [21] and Gemini-2.0-Flash [23], along with the latest thinking-augmented MLLMs: o4-mini [22], Seed 1.5-VL [6], and Gemini 2.5 (Flash and Pro-0506) [5]. Open-source models include mPLUG-Owl3 [31], MiniCPM-V 2.6 [30], MiniCPM-o 2.6 [30], Kimi-VL-A3B [28], LLaVA-OneVision (7B and 72B) [11], LLaVA-Video (7B and 72B) [32], InternVL3 (8B and 78B) [35], and Qwen2.5-VL (7B and 72B) [1]. These models represent the current SOTA in video understanding tasks. Implementation Details. For proprietary models, we use official APIs to obtain model responses. For open-source models, we perform local inference using publicly available checkpoints. For evaluation, we adopt Qwen2.5-72B as the judge model. To support future research, our benchmark has been integrated into the widely used VLMEvalKit framework [3], available in our code repository4. We also plan to contribute VIDEOREASONBENCH to the official VLMEvalKit in the future. Additional details regarding generation configurations and video processing are provided in Appendix B.1. To evaluate human performance on VIDEOREASONBENCH, we randomly sample 240 examples from the full set of 1,440, selecting 40 examples per reasoning skill. Three of the authors independently annotate the data. Each annotator is presented with the same video and question pairs shown to the models, and provides answers in free-text format."
        },
        {
            "title": "3.2 Main Results",
            "content": "Table 2 presents the evaluation results of various MLLMs and human baseline on the proposed VIDEOREASONBENCH, from which we can derive the following findings: Current MLLMs struggle with vision-centric complex video reasoning. All open-source \"Efficient Models\" (<10B active parameters) perform poorly, achieving less than 2% accuracy. open-source 4https://github.com/llyx97/video_reason_bench 6 Table 2: VIDEOREASONBENCH evaluation results across three levels of reasoning skills. The human baseline was assessed on subset of 240 examples (40 per skill), with an average response time of 223.2 seconds per example. Model Act. Params Think Level 1 Level 2 Level 3 Overall Human Proprietary Models GPT-4o o4-mini Seed1.5-VL Gemini-2.0-Flash Gemini-2.5-Flash Gemini-2.5-Flash Gemini-2.5-Pro-0506 Open-source Models Efficient Models mPLUG-Owl3 MiniCPM-V 2.6 MiniCPM-o 2.6 LLaVA-OneVision LLaVA-Video InternVL3 Qwen2.5-VL Kimi-VL-A3B Flagship Models LLaVA-OneVision LLaVA-Video InternVL3 Qwen2.5-VL Recall Order Recall Count Infer State Compare State Predict State Predict Operation - 223.2s 87.5 90.0 80.0 75.0 67.5 42. 73.8 - - 20B - - - - 7B 8B 8B 7B 7B 8B 7B 3B 72B 72B 78B 72B 14.2 14.2 24.2 18.3 22.5 44.6 69.2 0.0 2.1 1.2 0.0 0.0 0.4 3.8 1.7 0.0 0.0 11.2 12.5 15.8 20.4 27.1 22.5 34.2 41.7 70.4 0.0 0.4 0.4 0.0 0.0 0.8 0.8 3. 0.0 0.0 14.6 17.1 4.2 7.1 3.8 6.7 19.6 27.9 63.3 0.0 0.4 0.4 0.4 0.0 0.0 0.4 1.2 0.0 0.0 0.8 4.2 6.2 11.7 7.9 6.7 20.4 27.1 56.7 0.0 0.0 0.8 0.0 0.0 0.4 0.0 0. 0.0 0.0 2.1 4.2 0.8 6.2 3.8 5.0 8.8 13.8 42.1 0.0 1.2 1.2 0.4 0.0 1.7 2.1 1.7 0.8 0.4 3.8 2.9 0.0 4.6 2.1 3.3 7.1 9.6 34.6 0.0 0.4 0.4 0.8 0.0 0.0 0.8 0. 0.0 0.0 2.1 2.1 6.9 10.7 11.5 10.4 18.8 27.4 56.0 0.0 0.8 0.8 0.3 0.0 0.6 1.3 1.4 0.1 0.1 5.8 7.2 \"Flagship Models\" (72B+ active parameters), as well as GPT-4o, also struggle, with accuracies below 10%. Even the most recent thinking-enhanced models, such as o4-mini and Seed1.5-VL, show only modest improvements, scoring 10.7% and 11.5% respectively. Moreover, most models fail to surpass 30% accuracy even on \"Level 1\" tasks, suggesting that fine-grained perception of dense temporal information [16, 14, 26, 13] is still challenging for current MLLMs. In contrast, the Gemini 2.5 series demonstrates markedly stronger performance, with Gemini-2.5-Pro-0506 achieving 56% accuracy. Nonetheless, substantial gap remains when compared to human performance, which reaches 73.8%. These findings suggest that even the most advanced MLLMs still fall short of human-level capability in the complex video reasoning tasks posed by VIDEOREASONBENCH. VIDEOREASONBENCH poses substantial challengeseven for humans. Human annotators also face significant challenges, as answering single question requires recognizing multiple distinct operations (up to 14) within video and accurately inferring the corresponding latent state transitions. This process is cognitively demanding and time-intensive, taking annotators an average of 223.2 seconds per question. Furthermore, single misinterpretation could result in an incorrect final answer, which helps explain the relative low human accuracyespecially on the tasks requiring Level 3 skills. Thinking is critical for performance on VIDEOREASONBENCH. Compared with the conventional non-thinking MLLMs, the thinking-enhanced models exhibits substantial advantage. Even when using the same model, e.g., Gemini-2.5-Flash, enabling the thinking mode leads to substantial performance improvement from 18.8% to 27.4%. This highlights the importance of explicit reasoning mechanisms and extended CoT chains in tackling the complex video reasoning problems posed by VIDEOREASONBENCH. deeper analysis of the role of thinking in different video understanding benchmarks is provided in Section 3.3.1. 7 (a) Existing Benchmarks (b) VIDEOREASONBENCH Figure 5: Performance of Gemini-2.5-Flash with varying thinking budgets on five benchmarks. The \"Generated Tokens\" is the sum of \"Thinking Tokens\" and \"Response Tokens\". Table 3: Performance of Gemini-2.5-Flash with different visual inputs on five benchmarks. Benchmark Full Video Cut 50% Single Frame Text-only TempCompass (MCQ) [16] Video-MME (w/o subs) [4] MMVU [33] Video-MMMU [7] VIDEOREASONBENCH (ours) 79.6 71.5 70.3 68.7 27.4 73.5 ( 7.6%) 64.1 ( 10.3%) 69.1 ( 1.7%) 64.3 ( 6.4%) 12.2 ( 55.5%) 59.0 ( 25.9%) 51.3 ( 28.3%) 60.6 ( 13.8%) 56.3 ( 18.0%) 0.5 ( 98.2%) 40.2 ( 49.5%) 45.6 ( 36.2%) 44.8 ( 36.3%) 49.7 ( 27.7%) 1.0 ( 96.4%) Reasoning difficulty increases from Level 1 to Level 3. Performance consistently declines from Level 1 to Level 3 reasoning skill for both MLLMs and humans. This trend strongly aligns with the intended design of the benchmark, where higher level reasoning skills are built upon lower level skills. Such design ensures an increased difficult across the three levels."
        },
        {
            "title": "3.3.1 Effect of Thinking",
            "content": "The benefits of extended CoT reasoning remain underexplored in the domain of video understanding. To address this gap, we systematically investigate how varying the length of reasoning affects performance on VIDEOREASONBENCH and four representative video understanding benchmarks that focus on different abilities: TempCompass (multi-choice), Video-MME (w/o subwords), MMVU, Video-MMMU. We leverage the Gemini-2.5-Flash model, which enables explicit control over the number of reasoning tokens through \"Thinking Budget\" parameter5. As shown in Figure 5, with the increase in thinking budget, VIDEOREASONBENCH demonstrates notable accuracy improvementrising by roughly 9%. In contrast, existing benchmarks exhibit minimal gains, all under 2.5%, when the same increase in thinking budget is applied. This suggests that thinking contributes more to the performance of VIDEOREASONBENCH than existing benchmarks. Additionally, the number of response tokens varies notably across benchmarks when thinking budget is set to zero: For TempCompass and Video-MME, which primarily test basic temporal and general video understanding, responses are conciserequiring only tens of tokens. Conversely, MMVU and Video-MMU, which demand knowledge-intensive reasoning, show substantially higher response token counts, averaging 183 and 1,537 tokens respectively. Notably, VIDEOREASONBENCH produces even longer responses, averaging 1,860.1 tokens, when deprived of explicit \"thinking\" resources. This pattern highlights the challenging nature of VIDEOREASONBENCH. 5This parameter affects the number of thinking tokens but does not allow for precise token-level control. 8 Table 4: Results across different state sizes and operation counts. Model State Size Operation Count Table 5: Results across different state reveal timing. 1 & (3x3) 2 & (4x4) 5-9 10-14 Model Begin End Seed1.5-VL Gemini-2.0-Flash Gemini-2.5-Flash Gemini-2.5-Pro 11.9 11.8 30.4 59. 11.0 9.0 24.4 52.2 15.1 12.9 30.1 58.2 7.8 7.9 24.7 53.9 Seed1.5-VL Gemini-2.0-Flash Gemini-2.5-Flash Gemini-2.5-Pro 12.1 13.3 35.3 66.9 10.8 7.5 19.6 45."
        },
        {
            "title": "3.3.2 Effect of Vision Reliance",
            "content": "VIDEOREASONBENCH is designed to evaluate video reasoning that demands fine-grained visual perception. To assess its reliance on visual information and compare with existing benchmarks, we evaluate the performance of Gemini-2.5-Flash under four different visual input conditions: the full video, version that randomly cuts 50% of the video, single center frame, and text-only input (with no visual content). The results are shown in Table 3. For MMVU and Video-MMMU, which also involve reasoning, removing half of the video frames results in less than 7% relative performance drop. In contrast, performance on VIDEOREASONBENCH decreases by 55% under the same condition. When the visual input is further reduced to single frame, VIDEOREASONBENCH shows dramatic performance decline of 98.2%, whereas the largest drop observed in the existing benchmarks is only 28.3%. These results suggest that VIDEOREASONBENCH demands much higher degree of vision reliance than current video understanding benchmarks."
        },
        {
            "title": "3.3.3 Effect of Video Complexity",
            "content": "As introduced in Sec. 2.2, our videos vary in operation count and state size, both of which influence the richness of visual information. As we can see in Table 4, the performance of MLLMs generally decreases as operation count and state size increase. These findings indicate that video reasoning complexity can be effectively controlled by adjusting these two parameters, enabling flexible scaling of the benchmarks difficulty in future evaluations."
        },
        {
            "title": "3.4 Effect of State Reveal Timing",
            "content": "In VIDEOREASONBENCH, the latent state is revealed either at the beginning or at the end of each video. Table 5 compares model performance under these two different reveal timings. As shown, all four MLLMs exhibit lower accuracy when the latent state is revealed at the end. This performance drop occurs because, in this setting, the models must infer the initial state by reasoning backward through the sequence of state transitions. This reverse inference is inherently more challenging than following the transitions in their natural, forward order."
        },
        {
            "title": "4 Conclusions and Limitations",
            "content": "This paper presents VIDEOREASONBENCH to evaluate vision-centric, complex video reasoning abilities of MLLMs. Our results revealed that most SOTA MLLMs struggle with such reasoning, achieving very low accuracies, while the thinking-enhanced Gemini-2.5-Pro significantly outperformed others. Analysis show that extended chain-of-thought reasoning offers minimal benefits to existing video understanding benchmarks while is crucial for improving performance on VIDEOREASONBENCH. Additionally, we observe that removing visual information from VIDEOREASONBENCH leads to substantially larger drop in performance compared to other benchmarks, underscoring its strong reliance on visual content. Overall, VIDEOREASONBENCH provides challenging and timely testbed to advance research in complex video reasoning. However, this work does not propose concrete methods for improving MLLM performance on such tasks. We encourage future research to explore this important direction."
        },
        {
            "title": "References",
            "content": "[1] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. [3] H. Duan, J. Yang, Y. Qiao, X. Fang, L. Chen, Y. Liu, X. Dong, Y. Zang, P. Zhang, J. Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. [4] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang, P. Chen, Y. Li, S. Lin, S. Zhao, K. Li, T. Xu, X. Zheng, E. Chen, R. Ji, and X. Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. CoRR, abs/2405.21075, 2024. [5] Google and DeepMind. Gemini 2.5: Our most intelligent ai model, 2025. [6] D. Guo, F. Wu, F. Zhu, F. Leng, G. Shi, H. Chen, H. Fan, J. Wang, J. Jiang, J. Wang, J. Chen, J. Huang, K. Lei, L. Yuan, L. Luo, P. Liu, Q. Ye, R. Qian, S. Yan, S. Zhao, S. Peng, S. Li, S. Yuan, S. Wu, T. Cheng, W. Liu, W. Wang, X. Zeng, X. Liu, X. Qin, X. Ding, X. Xiao, X. Zhang, X. Zhang, X. Xiong, Y. Peng, Y. Chen, Y. Li, Y. Hu, Y. Lin, Y. Hu, Y. Zhang, Y. Wu, Y. Li, Y. Liu, Y. Ling, Y. Qin, Z. Wang, Z. He, A. Zhang, B. Yi, B. Liao, C. Huang, C. Zhang, C. Deng, C. Deng, C. Lin, C. Yuan, C. Li, C. Gou, C. Lou, C. Wei, C. Liu, C. Li, D. Zhu, D. Zhong, F. Li, F. Zhang, G. Wu, G. Li, G. Xiao, H. Lin, H. Yang, H. Wang, H. Ji, H. Hao, H. Shen, H. Li, J. Li, J. Wu, J. Zhu, J. Jiao, J. Feng, J. Chen, J. Duan, J. Liu, J. Zeng, J. Tang, J. Sun, J. Chen, J. Long, J. Feng, J. Zhan, J. Fang, J. Lu, K. Hua, K. Liu, K. Shen, K. Zhang, K. Shen, K. Wang, K. Pan, K. Zhang, K. Li, L. Li, L. Li, L. Shi, L. Han, L. Xiang, L. Chen, L. Chen, L. Li, L. Yan, L. Chi, L. Liu, M. Du, M. Wang, N. Pan, P. Chen, P. Chen, P. Wu, Q. Yuan, Q. Shuai, Q. Tao, R. Zheng, R. Zhang, R. Zhang, R. Wang, R. Yang, R. Zhao, S. Xu, S. Liang, S. Yan, S. Zhong, S. Cao, S. Wu, S. Liu, S. Chang, S. Cai, T. Ao, T. Yang, T. Zhang, W. Zhong, W. Jia, W. Weng, W. Yu, W. Huang, W. Zhu, W. Yang, W. Wang, X. Long, X. Yin, X. Li, X. Zhu, X. Jia, X. Zhang, X. Liu, X. Zhang, X. Yang, X. Luo, X. Chen, X. Zhong, X. Xiao, X. Li, Y. Wu, Y. Wen, Y. Du, Y. Zhang, Y. Ye, Y. Wu, Y. Liu, Y. Yue, Y. Zhou, Y. Yuan, Y. Xu, Y. Yang, Y. Zhang, Y. Fang, Y. Li, Y. Ren, Y. Xiong, Z. Hong, Z. Wang, Z. Sun, Z. Wang, Z. Cai, Z. Zha, Z. An, Z. Zhao, Z. Xu, Z. Chen, Z. Wu, Z. Zheng, Z. Wang, Z. Huang, Z. Zhu, and Z. Song. Seed1.5-vl technical report. CoRR, abs/2505.07062, 2024. [7] K. Hu, P. Wu, F. Pu, W. Xiao, Y. Zhang, X. Yue, B. Li, and Z. Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. CoRR, abs/2501.13826, 2025. [8] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, A. Iftimie, A. Karpenko, A. T. Passos, A. Neitz, A. Prokofiev, A. Wei, A. Tam, A. Bennett, A. Kumar, A. Saraiva, A. Vallone, A. Duberstein, A. Kondrich, A. Mishchenko, A. Applebaum, A. Jiang, A. Nair, B. Zoph, B. Ghorbani, B. Rossen, B. Sokolowsky, B. Barak, B. McGrew, B. Minaiev, B. Hao, B. Baker, B. Houghton, B. McKinzie, B. Eastman, C. Lugaresi, C. Bassin, C. Hudson, C. M. Li, C. de Bourcy, C. Voss, C. Shen, C. Zhang, C. Koch, C. Orsinger, C. Hesse, C. Fischer, C. Chan, D. Roberts, D. Kappler, D. Levy, D. Selsam, D. Dohan, D. Farhi, D. Mely, D. Robinson, D. Tsipras, D. Li, D. Oprica, E. Freeman, E. Zhang, E. Wong, E. Proehl, E. Cheung, E. Mitchell, E. Wallace, E. Ritter, E. Mays, F. Wang, F. P. Such, F. Raso, F. Leoni, 10 F. Tsimpourlas, F. Song, F. von Lohmann, F. Sulit, G. Salmon, G. Parascandolo, G. Chabot, G. Zhao, G. Brockman, G. Leclerc, H. Salman, H. Bao, H. Sheng, H. Andrin, H. Bagherinezhad, H. Ren, H. Lightman, H. W. Chung, I. Kivlichan, I. OConnell, I. Osband, I. C. Gilaberte, and I. Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024. [9] N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. [10] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. R. Narasimhan. Swe-bench: Can language models resolve real-world github issues? In ICLR. OpenReview.net, 2024. [11] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, Y. Li, Z. Liu, and C. Li. Llava-onevision: Easy visual task transfer. ArXiv preprint, abs/2408.03326, 2024. [12] K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, Y. Liu, Z. Wang, J. Xu, G. Chen, P. Lou, L. Wang, and Y. Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, pages 2219522206. IEEE, 2024. [13] L. Li, Y. Liu, L. Yao, P. Zhang, C. An, L. Wang, X. Sun, L. Kong, and Q. Liu. Temporal reasoning transfer from text to video. In ICLR 2025. OpenReview.net, 2025. URL https: //openreview.net/forum?id=sHAvMp5J4R. [14] S. Li, L. Li, Y. Liu, S. Ren, Y. Liu, R. Gao, X. Sun, and L. Hou. VITATECS: diagnostic dataset for temporal concept understanding of video-language models. In ECCV (70), volume 15128 of Lecture Notes in Computer Science, pages 331348. Springer, 2024. [15] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. In ICLR. OpenReview.net, 2024. [16] Y. Liu, S. Li, Y. Liu, Y. Wang, S. Ren, L. Li, S. Chen, X. Sun, and L. Hou. Tempcompass: Do video llms really understand videos? In ACL (Findings), pages 87318772. Association for Computational Linguistics, 2024. [17] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR. OpenReview.net, 2024. [18] MAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/ math-competitions/american-invitational-mathematics-examination-aime. [19] K. Mangalam, R. Akshulakov, and J. Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. In NeurIPS, 2023. [20] A. Nagrani, S. Menon, A. Iscen, S. Buch, R. Mehran, N. Jha, A. Hauth, Y. Zhu, C. Vondrick, M. Sirotenko, C. Schmid, and T. Weyand. Minerva: Evaluating complex video reasoning. arXiv preprint arXiv:2505.00681, 2025. [21] OpenAI. Gpt-4o system card, 2024. [22] OpenAI. Introducing openai o3 and o4-mini, 2025. [23] S. Pichai, D. Hassabis, and K. Kavukcuoglu. Introducing gemini 2.0: our new ai model for the agentic era, 2024. [24] Y. Qi, Y. Zhao, Y. Zeng, X. Bao, W. Huang, L. Chen, Z. Chen, J. Zhao, Z. Qi, and F. Zhao. Vcr-bench: comprehensive evaluation framework for video chain-of-thought reasoning. arXiv preprint arXiv:2504.07956, 2025. [25] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: graduate-level google-proof q&a benchmark. CoRR, abs/2311.12022, 2023. 11 [26] Z. Shangguan, C. Li, Y. Ding, Y. Zheng, Y. Zhao, T. Fitzgerald, and A. Cohan. TOMATO: assessing visual temporal reasoning capabilities in multimodal foundation models. In ICLR. OpenReview.net, 2025. [27] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, C. Tang, C. Wang, D. Zhang, E. Yuan, E. Lu, F. Tang, F. Sung, G. Wei, G. Lai, H. Guo, H. Zhu, H. Ding, H. Hu, H. Yang, H. Zhang, H. Yao, H. Zhao, H. Lu, H. Li, H. Yu, H. Gao, H. Zheng, H. Yuan, J. Chen, J. Guo, J. Su, J. Wang, J. Zhao, J. Zhang, J. Liu, J. Yan, J. Wu, L. Shi, L. Ye, L. Yu, M. Dong, N. Zhang, N. Ma, Q. Pan, Q. Gong, S. Liu, S. Ma, S. Wei, S. Cao, S. Huang, T. Jiang, W. Gao, W. Xiong, W. He, W. Huang, W. Wu, W. He, X. Wei, X. Jia, X. Wu, X. Xu, X. Zu, X. Zhou, X. Pan, Y. Charles, Y. Li, Y. Hu, Y. Liu, Y. Chen, Y. Wang, Y. Liu, Y. Qin, Y. Liu, Y. Yang, Y. Bao, Y. Du, Y. Wu, Y. Wang, Z. Zhou, Z. Wang, Z. Li, Z. Zhu, Z. Zhang, Z. Wang, Z. Yang, Z. Huang, Z. Huang, Z. Xu, and Z. Yang. Kimi k1.5: Scaling reinforcement learning with llms. CoRR, abs/2501.12599, 2025. [28] K. Team, A. Du, B. Yin, B. Xing, B. Qu, B. Wang, C. Chen, C. Zhang, C. Du, C. Wei, C. Wang, D. Zhang, D. Du, D. Wang, E. Yuan, E. Lu, F. Li, F. Sung, G. Wei, G. Lai, H. Zhu, H. Ding, H. Hu, H. Yang, H. Zhang, H. Wu, H. Yao, H. Lu, H. Wang, H. Gao, H. Zheng, J. Li, J. Su, J. Wang, J. Deng, J. Qiu, J. Xie, J. Wang, J. Liu, J. Yan, K. Ouyang, L. Chen, L. Sui, L. Yu, M. Dong, M. Dong, N. Xu, P. Cheng, Q. Gu, R. Zhou, S. Liu, S. Cao, T. Yu, T. Song, T. Bai, W. Song, W. He, W. Huang, W. Xu, X. Yuan, X. Yao, X. Wu, X. Zu, X. Zhou, X. Wang, Y. Charles, Y. Zhong, Y. Li, Y. Hu, Y. Chen, Y. Wang, Y. Liu, Y. Miao, Y. Qin, Y. Chen, Y. Bao, Y. Wang, Y. Kang, Y. Liu, Y. Du, Y. Wu, Y. Wang, Y. Yan, Z. Zhou, Z. Li, Z. Jiang, Z. Zhang, Z. Yang, Z. Huang, Z. Huang, Z. Zhao, and Z. Chen. Kimi-VL technical report, 2025. URL https://arxiv.org/abs/2504.07491. [29] H. Wu, D. Li, B. Chen, and J. Li. Longvideobench: benchmark for long-context interleaved video-language understanding. In NeurIPS, 2024. [30] Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao, Z. He, Q. Chen, H. Zhou, Z. Zou, H. Zhang, S. Hu, Z. Zheng, J. Zhou, J. Cai, X. Han, G. Zeng, D. Li, Z. Liu, and M. Sun. Minicpm-v: GPT-4V level MLLM on your phone. CoRR, abs/2408.01800, 2024. [31] J. Ye, H. Xu, H. Liu, A. Hu, M. Yan, Q. Qian, J. Zhang, F. Huang, and J. Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. CoRR, abs/2408.04840, 2024. [32] Y. Zhang, J. Wu, W. Li, B. Li, Z. Ma, Z. Liu, and C. Li. Video instruction tuning with synthetic data. CoRR, abs/2410.02713, 2024. [33] Y. Zhao, L. Xie, H. Zhang, G. Gan, Y. Long, Z. Hu, T. Hu, W. Chen, C. Li, J. Song, Z. Xu, C. Wang, W. Pan, Z. Shangguan, X. Tang, Z. Liang, Y. Liu, C. Zhao, and A. Cohan. MMVU: measuring expert-level multi-discipline video understanding. CoRR, abs/2501.12380, 2025. [34] J. Zhou, Y. Shu, B. Zhao, B. Wu, S. Xiao, X. Yang, Y. Xiong, B. Zhang, T. Huang, and Z. Liu. MLVU: comprehensive benchmark for multi-task long video understanding. CoRR, abs/2406.04264, 2024. [35] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, H. Tian, Y. Duan, W. Su, J. Shao, Z. Gao, E. Cui, X. Wang, Y. Cao, Y. Liu, X. Wei, H. Zhang, H. Wang, W. Xu, H. Li, J. Wang, N. Deng, S. Li, Y. He, T. Jiang, J. Luo, Y. Wang, C. He, B. Shi, X. Zhang, W. Shao, J. He, Y. Xiong, W. Qu, P. Sun, P. Jiao, H. Lv, L. Wu, K. Zhang, H. Deng, J. Ge, K. Chen, L. Wang, M. Dou, L. Lu, X. Zhu, T. Lu, D. Lin, Y. Qiao, J. Dai, and W. Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A More Details of VIDEOREASONBENCH",
            "content": "A.1 Question Templates Table 6, 7, 8, 9, 10, 11 present the question templates for the six types of video demonstrations, respectively. As we can see, the full prompt consists of three components: (1) The Task Instruction provides clear and precise description of the video demonstration along with the state transition rules, thereby eliminating ambiguity in task interpretation. (2) The Question states the query in complete sentence and specifies the expected answer format. (3) The Answer Prompt instructs the models to summarize the final answer after the identifier phrase Final Answer:, which facilitates easy extraction of the answer from the models response. A.2 Evaluation Prompts The evaluation prompt is illustrated in Table 12 and the prompts for extracting operations from the model response are shown in Table 13."
        },
        {
            "title": "B More Details of Experimental Setups",
            "content": "B.1 Inference Configurations Table 14 summarize the key inference configurations for the MLLMs evaluated in this paper, including the specific version of the models, the number of input frames, the temperature and maximum number of tokens for generation. For proprietary models, responses are obtained via official API calls, with videos processed at frame rate of 1 fps, with maximum frame number limit. For open-source models, inference is performed on internal clusters with GPUs of 80 GB memory. Videos are uniformly sampled at fixed number of frames32 frames for efficient models and 64 frames for flagship models. B.2 Human Annotation Figure 6 illustrates the annotation interface used to establish the human performance baseline. Annotators are shown the video along with the complete question promptexcluding the Answer Promptidentical to what the evaluated models receive. Figure 6: Screenshot of the human annotation interface. Video Full Prompt {task_instruction} {question} {answer_prompt} Task Instruction (Show at Begin): The video presents sliding puzzle on {N } board. The board is filled with numbered squares, with one empty space. The video begins by showing the starting arrangement of the numbers on the board. Then, the numbers are visually masked with blue overlay, and the puzzle undergoes series of movements-only the squares adjacent to the empty space can be shifted into it. Please carefully watch the video and answer the following question: Task Instruction (Show at End): The video presents sliding puzzle on {N } board. The board is filled with numbered squares, with one empty space. Initially, all numbered squares are visually masked with blue overlay. Then, the puzzle undergoes series of movements-only the squares adjacent to the empty space can be shifted into it. The video ends by showing the final arrangement of the numbers on the board. Please carefully watch the video and answer the following question: Answer Prompt: Provide summary of the final answer after Final Answer: Questions Recall Order: What are the {start_id} to {start_id} blue squares being moved? For each moved blue square, provide the following details: The order in which it was moved (e.g., 1st, 2nd, 3rd, etc). The coordinates before it was moved (e.g., (a,1), (c,2), etc). The direction it was moved (e.g., left, right, up, down) Recall Count: How many times was the {move} move performed in the video? For each occurrence, provide the coordinate of the square (e.g., (a,1), (c,2)...) before the move. Infer State: Assuming the empty square is represented by 0, what is the arrangement of numbers on the board at the {timestamp} of the video? Provide the coordinates of each square along with the corresponding number (e.g., (a,1): 3, (a,2): 0, (b,1): 1, (b,2): 2). Compare State: Assuming the empty square is represented by 0, compare the number arrangements on the board at the start and end of the video. What are the squares where the numbers differ between the two boards? Provide their coordinates along with the corresponding number at the {timestamp} of the video (e.g., (a,1): 3, (b,1): 1). Predict State: If the arrangement of numbers on the board is currently in the same state as it was at the {timestamp} of the video, and the following moves are executed: {moves}, what will be the arrangement of numbers on the board? Assume that the empty square is represented by 0. Provide the coordinates of each square along with the corresponding number (e.g., (a,1): 3, (a,2): 0, (b,1): 1, (b,2): 2). Predict Operations: If the arrangement of numbers on the board is currently in the same state as it was at the {timestamp} of the video, what sequence of moves (left, right, up, down) should be executed to achieve the desired number arrangement: number_arrangement? Assume that the empty square is represented by 0. Note that moves cannot push any square beyond the board boundary. Table 6: Prompt and question templates for the Number video demonstration. 14 Video Full Prompt {task_instruction} {question} {answer_prompt} Task Instruction (Show at Begin): The video presents Tricky Cup puzzle on {N } board. The board is filled with blue cups, each hiding either yellow coin or nothing underneath. At the start, all cups are briefly lifted to reveal whats beneath them. Then, the cups begin series of moveseach move swaps the positions of two cups, along with their hidden contents. Please carefully watch the video and answer the following question: Task Instruction (Show at End): The video presents Tricky Cup puzzle on {N } board. The board is filled with blue cups, each hiding either yellow coin or nothing underneath. Initially, the contents under the cups are completely hidden. Then, the cups begin series of moveseach move swaps the positions of two cups, along with their hidden contents. Toward the end, all cups are briefly lifted to reveal whats beneath them. Please carefully watch the video and answer the following question: Answer Prompt: Provide summary of the final answer after Final Answer: Questions Recall Order: Assume that each time two cups swap their positions, it counts as one move. What are the {start_id} to {start_id} moves shown in the video? For each move, provide the move number and the coordinates of the two cups that swapped positions. Format your response like this: 1st: (a1, b2), 2nd: (c2, b1), 3rd: (a3, c1). Recall Count: How many times were the cups in the row {row_idx} involved in the swaps? For each instance, provide the coordinate(s) of the cup(s) before the swap occurred. Format your response like this: 1st: a1, 2nd: a3, 3rd: (a1,a2) (Use single coordinate for individual cups, or tuple for multiple cups involved in the same swap.) Infer State: What are the positions of all the coins at the {timestamp} of the video? Provide the coordinates of each coin (e.g., a2, b1, c3). Compare State: Compare the distribution of contents beneath the cups at the start and end of the video. What are the positions where the contents beneath the cups differ between the two boards? Provide their coordinates along with the corresponding content at the {timestamp} of the video. Format your response like this: a1: empty, b3: coin. Predict State: If the distribution of coins on the board is currently in the same state as it was at the {timestamp} of the video, and the following cup swaps are executed in order: {moves}, what will be the new distribution of the coins? Provide the coordinates of the coins (e.g., a1, b2). Predict Operations: If the distribution of coins on the board is currently in the same state as it was at the {timestamp} of the video, what sequence of cup swaps should be executed to achieve the desired distribution of coins: board? Format your response as list of coordinate pairs, such as: (a1, b2), (c3, b1). Each pair represents single swap between two cups. Table 7: Prompt and question templates for the Cup video demonstration. 15 Video Full Prompt {task_instruction} {question} {answer_prompt} Task Instruction (Show at Begin): The video presents {N } grid. At the beginning of the video, all positions on the grid are filled with either black or white piece. Then, these pieces are visually hidden but still remain in their original positions. red circle then moves across the grid. Each time the red circle passes by position on the grid (excluding the starting position), the color of the piece at that position *and* the colors of its immediate orthogonal neighbors (up, down, left, and right) are flipped: black becomes white, and white becomes black. Note that diagonal neighbors are *not* affected. Neighbors are only considered if they exist within the grids boundaries. Please carefully watch the video and answer the following question: Task Instruction (Show at End): The video presents {N } grid. All positions on the grid are filled with either black or white piece. These pieces are visually hidden at the beginning of the video. red circle then moves across the grid. Each time the red circle passes by position on the grid (excluding the starting position), the color of the piece at that position *and* the colors of its immediate orthogonal neighbors (up, down, left, and right) are flipped: black becomes white, and white becomes black. Note that diagonal neighbors are *not* affected. Neighbors are only considered if they exist within the grids boundaries. The video ends by showing the final arrangement of black and white pieces on the grid. Please carefully watch the video and answer the following question: Answer Prompt: Provide summary of the final answer after Final Answer: Questions Recall Order: Assume that each time the red circle moves from one grid intersection to an adjacent one (horizontally or vertically), it counts as one move. What are the directions (left, right, up, down) of the {start_id} to {start_id} moves made by the red circle in the video? List them in order. Recall Count: Assume that each time the red circle moves from one grid intersection to an adjacent one (horizontally or vertically), it counts as one move. Given the movement direction {move}, how many times does the red circle perform this move? For each occurrence, provide the coordinate of the position before the move (e.g., (a,1), (c,2), etc). Infer State: What is the arrangement of the black and white pieces on the grid at the {timestamp} of the video? Provide each pieces coordinates and color using the format: (column, row): color (e.g., (a,1): black, (c,2): white). Compare State: Assume that each time the red circle moves from one grid intersection to an adjacent one (horizontally or vertically), it counts as one move. Compare the arrangement of black and white pieces on the grid at the start and end of the video. What are the coordinates where the piece color differ between the two grids? Provide these coordinates along with the corresponding piece color at the {timestamp} of the video, using the format: (column, row): color (e.g., (a,1): black, (c,2): white). Predict State: Assume that each time the red circle moves from one grid intersection to an adjacent one (horizontally or vertically), it counts as one move. If the arrangement of black and white pieces and the position of the red circle on the grid is currently in the same state as it was at the {timestamp} of the video, and the following moves are executed: {moves}, what will be the arrangement of black and white pieces on the grid? Provide each pieces coordinates and color using the format: (column, row): color (e.g., (a,1): black; (c,2): white). Predict Operations: Assume that each time the red circle moves from one grid intersection to an adjacent one (horizontally or vertically), it counts as one move. The red circle cannot move beyond the grid boundary. If the arrangement of black and white pieces and the position of the red circle on the grid is currently in the same state as it was at the {timestamp} of the video, what sequence of moves (left, right, up, down) should be executed by the red circle to achieve the desired arrangement of black and white pieces: {board}? List them in order. Table 8: Prompt and question templates for the Circle video demonstration. Video Full Prompt {task_instruction} {question} {answer_prompt} Task Instruction: The video demonstrates series of file manipulation commands executed in the Linux command line. To ensure accurate understanding, note these assumptions: * touch commands: All files created by touch do not exist in the target direc try prior to the commands execution. * rm -r commands: All files deleted by rm -r do exist in the target directory prior to the commands execution. * cp and mv commands: All source files used by cp and mv do exist in the source directory prior to the commands execution. * The destination path for cp and mv commands does not contain the target files prior to the command. Please carefully watch the video and answer the following question: Answer Prompt: Provide summary of the final answer after Final Answer: Questions Recall Order: What are the {start_id} to {start_id} {cmd_type}commands shown in the video? Provide the order of each command (e.g., 1st, 2nd, 3rd, etc) along with the command content. Recall Count: How many different {f ile_type}files were involved in the {cmd_type}commands throughout the video? Provide the file count along with the specific file names (e.g., 2 .txt files: a.txt, b.txt). Infer State: At the {timestamp} of the video, how many {f ile_type}files remain in {path_name}? Provide the file count along with the specific file names (e.g., 2 .txt files: a.txt, b.txt). Compare State - What files were in path0/ at the start of the video, but were not there at the end of the video? - What files were in path0/ at the end of the video, but were not there at the start of the video? - After the command {cmd} was executed, what files were in {path_name1} but were not in {path_name2}? Predict State: If the paths currently contain exactly the same files as they did at the {timestamp} of the video, and we run the command {cmd}, which {f ile_type}files would be in {path_name}? Predict Operations: If the paths currently contain exactly the same files as they did at the {timestamp} of the video, to ensure that {path_name} contains exactly the following files: {f iles}, what sequence of commands should be executed? Rules: 1. You may only use the commands touch and rm -rf. 2. You may use at most two commands. 3. Files specified in touch must not appear in rm -rf command, and vice versa (i.e., no overlap). Response Format: If multiple commands are used, separate them with &. For example, touch path0/{a.txt,b.txt} & rm -rf path0/{c.py,d.json}. Table 9: Prompt and question templates for the File video demonstration. 17 Video Full Prompt {task_instruction} {question} {answer_prompt} Task Instruction (Show at Begin): The video showcases sequence of operations involving one or more piles of cards. It begins by displaying the initial arrangement of cards in each pile from top to bottom. The cards are then turned face down, after which series of actions is carried out. Note that there are only two types of actions: adding one card to the top of the pile or removing one card from the bottom of the pile. Please carefully watch the video and answer the following question Task Instruction (Show at End): The video showcases sequence of operations involving one or more piles of cards. Throughout the video, before the final reveal of each pile, only two types of actions occur: adding one card to the top of the pile or removing one card from the bottom of the pile. Then, the video ends by displaying the final arrangement of cards in each pile from top to bottom. Please carefully watch the video and answer the following question: Answer Prompt: Provide summary of the final answer after Final Answer: Questions Recall Order: What are the {start_id} to {start_id} cards being {action_type} any pile throughout the video? For each card, provide the following details: 1. The order (e.g., 1st or 2nd) 2. The suit and value (e.g., 6 of Hearts) 3. The pile involved (e.g., pile0, pile1) Format your response like this: 1st: 6 of Hearts, pile0 2nd: Jack of Spades, pile1. Recall Count: How many cards were {action_type} {pile_name} throughout the video? For each card, provide its suit and value (e.g., 6 of Hearts) Format your response like this: 2 cards: 6 of Hearts, King of Clubs. Infer State: At the {timestamp} of the video, what cards are in {pile_name}? List them in order from top to bottom, including both the value and suit of each card. Format your response like this: 6 of Hearts, King of Clubs, 3 of Spades. Compare State: What cards were in {pile_name} at the {timestamp} of the video, but were not there at the {timestamp2} of the video? For each card, provide its suit and value. Format your response like this: 6 of Hearts, King of Clubs, 3 of Spades. Predict State: If the piles currently contain exactly the same cards as they did at the {timestamp} of the video, and now we perform these actions in order: {actions}. What cards would be in {pile_name}? List them in order from top to bottom, including both the value and suit of each card. Format your response like this: 6 of Hearts, King of Clubs, 3 of Spades. Predict Operations: If the piles currently contain exactly the same cards as they did at the {timestamp} of the video, to ensure that {pile_name} contains exactly the following cards from top to bottom: {cards}, what sequence of actions should be performed? Rules: 1. Each action must either add card to pile or remove card from pile. 2. You may only add cards to the top of pile or remove cards from the bottom of pile. Response Format: List the actions in sequence, specifying the action, card, and pile. Separate each action with comma. For example, add 6 of Hearts to pile0, remove King of Clubs from pile0 Table 10: Prompt and question templates for the Card video demonstration. 18 Video Full Prompt {task_instruction} {question} {answer_prompt} Task Instruction (Show at Begin): The video showcases sequence of operations involving one or more cup(s) and chips. It begins by showing the initial chips contained in each cup. Then, series of actions are carried out. Note that there are only two types of actions: adding one chip to cup or removing one chip from cup. Please carefully watch the video and answer the following question: Task Instruction (Show at End): The video showcases sequence of operations involving one or more cup(s) and chips. Throughout the video, before the final reveal of chips contained in each cup, only two types of actions occur: adding one chip to cup or removing one chip from cup. Then, the video ends by displaying the final chips contained in each cup. Please carefully watch the video and answer the following question: Answer Prompt: Provide summary of the final answer after Final Answer: Questions Recall Order: Disregarding the process of revealing all chips in the cup(s), what are the {start_id} to {start_id} chips being {action_type} any cup throughout the video? For each chip, provide the following details: 1. The order (e.g., 1st or 2nd) 2. The value (e.g., 20) 3. The cup involved (e.g., cup0, cup1) Format your response like this: 1st: 100, cup0 2nd: 20, cup1. Recall Count: Disregarding the process of revealing all chips in the cup(s), how many chips were {action_type} {cup_name} throughout the video? For each chip, provide its value (order does not matter). Format your response like this: 4 chips: 20, 5, 100, 100. Infer State: At the {timestamp} of the video, how many chips were in {cup_name}? For each chip, provide its value (order does not matter). Format your response like this: 4 chips: 20, 5, 100, 100. Compare State: At which point in the video is the total value of chips in {cup_name} higher, at {timestamp1} or {timestamp2}? Also, what is the difference in value between the two times? Format your response like this: \"time_with_higher_value\", \"difference_in_value\" (e.g., start, 115). Predict State: If the cups currently contain exactly the same chips as they did at the {timestamp} of the video, and now we perform these actions in order: {actions}. How many chips would be in {cup_name}? For each chip, provide its value (order does not matter). Format your response like this: 4 chips: 20, 5, 100, 100. Predict Operations: If the cups currently contain exactly the same chips as they did at the {timestamp} of the video, to ensure that {cup_name} contains exactly the following chips: {chips} (order does not matter), what sequence of actions should be performed? Rules: 1. Each action must either add chip to cup or remove chip from cup. 2. Available chips for addition are: 5, 10, 20, 50, 100. 3. You may only remove chip if it is already present in the cup. Response Format: List the actions in sequence, specifying the action, chip, and cup. Separate each action with comma. For example, add 20 to cup0, remove 50 cup0 Table 11: Prompt and question templates for the Chip video demonstration. 19 You will be given question, model response and ground-truth answer. Your task is to determine whether the model response is correct based on the ground-truth answer. The model response should contain all information in the ground-truth answer. Question: {question} Model Response: {response} Ground-Truth Answer: {ground_truth} Directly output \"Correct\" or \"Incorrect\": Table 12: Prompt used for LLM-based evaluation. Number & Circle: You will be given model-generated response describing sequence of movements. Your task is to extract the movements in the order they appear and return them as list (e.g., [left, up, down, right]). Model Response: {response} Extracted Movements: Cup: You will be given model-generated response describing sequence of cup swaps. Each swap is represented as pair of coordinatesfor example, (a1, b2)indicating the two positions being swapped. Your task: Extract all coordinate pairs from the response in the exact order they appear, and return them as list of tuples. Format your answer like this: [(a1, b2), (c1, b1), (a3, b2)] Model Response: {response} Extracted Swaps: File: You will be given model-generated response regarding file operation command in Linux system. Your task: Identify and extract only the actual command from the model response, removing any irrelevant or descriptive text. Model Response: {response} Extracted Command: Card: You will be given model-generated response describing sequence of operations performed to cards. Each operation either adds or removes card from pile0 or pile1. Your task: - Extract all valid operations and return them as list of strings. - Each operation must involve either adding or removing card to or from pile0 or pile1. - If no valid operations are found, return an empty list ([]). Format your answer like this: [add 6 of Hearts to pile0, remove King of Clubs from pile0] Model Response: {response} Extracted Operations: Chip: You will be given model-generated response describing sequence of operations involving chips and cups. Each operation either adds or removes chip from cup0 or cup1. Your task: - Extract all valid operations and return them as list of strings. - Each operation must involve either adding or removing chip to or from cup0 or cup1. - If no valid operations are found, return an empty list ([]). Format your answer like this: [add 20 to cup0, remove 50 cup0] Model Response: {response} Extracted Operations: Table 13: Prompts used for operation extraction. Table 14: Inference configurations for the evaluated MLLMs. \"1fps/N \" indicates that videos with duration seconds are processed at 1fps, while for videos longer than seconds, frames are uniformly sampled. The temperature is set to 0.0 by default and increased to 1.0 if the response exceeds the Max New Tokens limitthis adjustment is applied in token count experiments to prevent excessively long responses with repeated tokens. Model Version Proprietary Models GPT-4o o4-mini Seed1.5-VL Gemini-2.0-Flash Gemini-2.5-Flash Gemini-2.5-Pro Open-source Models Efficient Models mPLUG-Owl3 MiniCPM-V 2.6 MiniCPM-o 2.6 LLaVA-OneVision-7B LLaVA-Video-7B InternVL3-8B Qwen2.5-VL-7B Kimi-VL-A3B Flagship Models LLaVA-OneVision-72B LLaVA-Video-72B InternVL3-78B Qwen2.5-VL-72B gpt-4o-2024-11-20 o4-mini-2025-04-16 doubao-1-5-thinking-vision-pro-250428 gemini-2.0-flash gemini-2.5-flash-preview-04-17 gemini-2.5-pro-preview-05-06 mPLUG-Owl3-7B-240728 MiniCPM-V-2_6 MiniCPM-o-2_6 llava-onevision-qwen2-7b-ov LLaVA-Video-7B-Qwen2 InternVL3-8B Qwen2.5-VL-7B-Instruct Kimi-VL-A3B-Instruct llava-onevision-qwen2-72b-ov-sft LLaVA-Video-72B-Qwen2 InternVL3-78B Qwen2.5-VL-72B-Instruct Input Frames 1fps/50 1fps/50 1fps/128 1fps 1fps 1fps 32 32 32 32 32 32 32 32 64 64 64 64 Temperature Max New Tokens 1.0 1.0 0.0 0.0 0.0/1.0 0.0/1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.01 1. 0.0 0.0 0.0 0.01 8,192 8,192 16,384 4,096 65,536 65,536 1,024 1,024 1,024 2,048 2,048 4,096 4,096 16,384 2,048 2,048 4,096 4,"
        }
    ],
    "affiliations": [
        "Moonshot AI",
        "Nanjing University",
        "National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University",
        "School of Mathematical Sciences, Peking University"
    ]
}