{
    "paper_title": "Free Process Rewards without Process Labels",
    "authors": [
        "Lifan Yuan",
        "Wendi Li",
        "Huayu Chen",
        "Ganqu Cui",
        "Ning Ding",
        "Kaiyan Zhang",
        "Bowen Zhou",
        "Zhiyuan Liu",
        "Hao Peng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Different from its counterpart outcome reward models (ORMs), which evaluate the entire responses, a process reward model (PRM) scores a reasoning trajectory step by step, providing denser and more fine grained rewards. However, training a PRM requires labels annotated at every intermediate step, presenting significant challenges for both manual and automatic data collection. This paper aims to address this challenge. Both theoretically and empirically, we show that an \\textit{implicit PRM} can be obtained at no additional cost, by simply training an ORM on the cheaper response-level labels. The only assumption is to parameterize the outcome reward as the log-likelihood ratios of the policy and reference models, which can be optimized regardless of the specific choice of loss objectives. In experiments, we instantiate our implicit PRMs with various objectives and evaluate their performance on MATH. We show that our implicit PRM outperforms a strong MCTS-based baseline \\textit{\\'a la} Math-Shepherd using less than $1/38$ of the training data. Its performance can be further improved with majority voting. We further find that scaling up instructions and responses benefits our implicit PRM, and the latter brings a larger gain. Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction, the setup that suffers from extreme data scarcity and imbalance. Further, instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data. We hope that our work will encourage a rethinking of PRM training approaches and contribute to making training PRMs more accessible."
        },
        {
            "title": "Start",
            "content": "Lifan Yuan1 Wendi Li2,3 Huayu Chen2 Ganqu Cui2 Ning Ding2 Kaiyan Zhang2 Bowen Zhou2 Zhiyuan Liu2 Hao Peng1 1University of Illinois Urbana-Champaign 3Huazhong University of Science and Technology lifan4@illinois.edu wendili@hust.edu.cn 2Tsinghua University 4 2 0 2 2 ] . [ 1 1 8 9 1 0 . 2 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Different from its counterpart outcome reward models (ORMs), which evaluate the entire responses, process reward model (PRM) scores reasoning trajectory step by step, providing denser and more fine grained rewards. However, training PRM requires labels annotated at every intermediate step, presenting significant challenges for both manual and automatic data collection. This paper aims to address this challenge. Both theoretically and empirically, we show that an implicit PRM can be obtained at no additional cost, by simply training an ORM on the cheaper response-level labels. The only assumption is to parameterize the outcome reward as the log-likelihood ratios of the policy and reference models rθ(y) = β log πθ(y) πref(y) , which can be optimized regardless of the specific choice of loss objectives. In experiments, we instantiate our implicit PRMs with various objectives and evaluate their performance on MATH. We show that our implicit PRM outperforms strong MCTS-based baseline á la Math-Shepherd (Wang et al., 2023) using less than 1/38 of the training data. Its performance can be further improved with majority voting. We further find that scaling up instructions and responses benefits our implicit PRM, and the latter brings larger gain. Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction, the setup that suffers from extreme data scarcity and imbalance. Further, instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data. We hope that our work will encourage rethinking of PRM training approaches and contribute to making training PRMs more accessible1."
        },
        {
            "title": "INTRODUCTION",
            "content": "Training on high-quality supervised data has driven the advances in LLMs development (Meta, 2024; Ding et al., 2023; Luo et al., 2024b; Yue et al., 2024; Yuan et al., 2024; Zhang et al., 2024b). Building upon this progress, reward models push the boundaries even further, especially in tasks requiring complex reasoning (Lightman et al., 2023; Wang et al., 2023; Snell et al., 2024). Outcome Reward Models (ORMs), designed to evaluate full responses, have been primarily explored, which can be used in both reinforcement learning (RL) and inference. However, due to the sparsity of outcome rewards, ORMs often yield suboptimal performance when reranking responses at inference (Lightman et al., 2023) and struggle with stability and efficiency during RL training (Cao et al., 2024; Chan et al., 2024). This highlights the growing demand for denser and more fine-grained rewards. Process Reward Models (PRMs), evaluating intermediate steps to provide fine-grained guidance, naturally meet this need. Existing work has shown consistent results that PRMs outperform ORMs in best-of-N sampling (Wang et al., 2023; Snell et al., 2024) and RL (Setlur et al., 2024), and argues that scoring every intermediate step provides better transparency and interpretability (Leike, 2024). Equal Contribution. Work done during Wendis intership at Tsinghua University. Corresponding Authors: cgq22@mails.tsinghua.edu.cn, dn97@mail.tsinghua.edu.cn 1Models and data are available at: https://github.com/lifan-yuan/ImplicitPRM."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: The x-axis indicates the FLOPs required to collect the data and train the model, and axis the accuracies of best-of-64 performance. The accuracy is averaged over the best-of-64 accuracies of Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), Llama-3.1-8B-Instruct, and Llama-3.1-70B-Instruct (Meta, 2024) on MATH (Hendrycks et al., 2021). Different dots on the same line indicates models trained with the same approach but on different scales of data. The top-left zone is desirable in this figure, as it suggests model can achieve higher performance with less development overhead. Our implicit PRM is much cheaper to train while presenting the best performance under the same budget. Despite their promise, PRMs are much harder to train than ORMs, since collecting PRM training data requires annotating every intermediate step. To reduce human efforts, automatic annotation approaches have been proposed, where an intermediate step is labeled based on its estimated probability of leading to correct outcome. Typically, this is achieved through either sampling massive lookahead trajectories to estimate or directly training verifier to predict value, both incurring extensive overhead (Wang et al., 2023; Lu et al., 2024). For example, collecting step-level data through sampling look-ahead trajectories as Wang et al. (2023) requires 38.8 more FLOPs than training an ORM (4). We argue, from both theoretical and empirical perspectives, that building PRMs can be substantially cheaper than previously realized: strong PRM can be obtained at no additional cost from training an ORM on the cheaper response-level data, with simple reward parameterization. Specifically, by parameterizin the reward as the log-likelihood ratio of the policy and the reference models rθ(y) = β log πθ(y) πref(y) , common practice in DPO (Rafailov et al., 2023) and many of its variants (Azar et al., 2024; Ethayarajh et al., 2024; Chen et al., 2024; Rosset et al., 2024; Wu et al., 2024), PRM can be automatically learned during ORM training. The process reward is then the same log-likelihood ratio, but calculated over partial response. We dub our approach an implicit PRM since it only requries response-level data and ORM training. Moreover, our insights are agnostic to the specific choice of the training objective, and are applicable to both DPO and all the variants that adopt the same form of implicit reward; it further extends to other objectives like the Cross-Entropy (CE) loss. This fresh theoretical insight generalizes the conclusion from Rafailov et al. (2024) that DPO training enables the model to learn the function; practically, our approach is particularly well-suited for scenarios where pairwise data is hard to obtain and algorithms like CE loss remain equally applicable, as shown in 5.2. In experiments, we train our implicit PRMs on dataset consisting of 33K math instructions and eight solutions for each, and evaluate them through the best-of-N sampling on MATH (Hendrycks et al., 2021). We explore variants of our implicit PRMs instantiated with different training objectives, including DPO, KTO, NCA, and CE. All produce strong PRMs, outperforming competitive baselines including our reimplementations of Math-Shepherd (Wang et al., 2023) and AutoPSV (Lu et al., 2024) and six off-the-shelf open ORMs and PRMs, with substantially better trade-offs between accuracy and development overhead, as shown in Figure 1. Particularly, when integrated into weighted best-of-N, CE stands as the most effective. This makes CE loss appealing in scenarios where pairwise data is hard to collect, since it can handle unpaired and imbalanced data, and is demonstrated to be less data-consuming than DPO in order for an implicit PRM with decent performance. Further, we find out that our implicit PRM benefits from increased training data, with the scale of responses being more influential than that of instructions. However, instructions should be relevant to downstream tasks while the diversity of responses does not matter much according to our observations. Surprisingly, training on step-level data brings no further improvements to our implicit PRMs. Additionally, despite our implicit PRM remains language model, its ability to help best-of-N sampling does not translate into its performance on downstream tasks as policy model. Rather, our worst performing implicit PRM,"
        },
        {
            "title": "Preprint",
            "content": "instantiated by KTO, stands as the only experiencing with an improvement in policy performance. Finally, we observe that, at least for the models and tasks we consider, the reference model can be omitted from our implicit PRM, improving the inference efficiency without hurting the accuracy. Bypassing the need for step labels, our findings substantially lower the data collection and training overhead of building PRMs while delivering stronger performance than existing methods. We hope that our work will encourage rethinking of PRM training approaches and contribute to making training PRMs more accessible."
        },
        {
            "title": "2 ORMS VS. PRMS: DILEMMA OF PERFORMANCE AND EXPENSE",
            "content": "Background ORMs assign sparse rewards rθ(y) to the entire response, and no feedback is provided until the last token is generated. In contrast, PRM assesses the quality of every intermediate step and can provide reward after completing each (Lightman et al., 2023). Given an instruction and an n-step response with yt being the t-th step and y<t being the first 1 steps, PRM receives the concatenation of the instruction and the first 1 steps, and assigns reward to the t-th: rt θ(y<t, yt). The value qt θ(y<t, yt) indicates the expectation of outcome reward rθ conditioned on the observed response y<t and current step yt. Lightman et al. (2023) define the process reward as the correctness of each step, while Wang et al. (2023) directly consider values as process rewards. We follow Lu et al. (2024) and define process reward as advantages, namely the difference between values: θ := qt rt . The benefits of adopting advantages as process rewards have been discussed by concurrent work (Setlur et al., 2024). θ qt1 θ PRMs outperformans ORMs in both training and inference Both ORMs and PRMs can provide rewards to assess model outputs. The dense step-level rewards from PRMs lead to stable and effective RL training (Cao et al., 2024; Chan et al., 2024), and performs better on reranking responses, with better transparency and interpretability. Also, ORMs are trained on complete responses, but the value model initialized from it only receives incomplete responses during RL training. On the contrary, PRMs are intrinsically trained to provide dense rewards given partial responses, thus the resulting value models may mitigate out-of-distribution issues that ORMs encounter. Training PRMs is substantially more expensive than ORMs Despite its effectiveness, training PRMs is more difficult due to challenges in training data collection. To collect training data for PRMs, MCTS is commonly used for automatic step annotation (Wang et al., 2023; Luo et al., 2024a). However, it introduces substantial extra cost. For MCTS-based step label annotation, policy model will sample trajectories based on the concatenation of an instruction and partial response up to step t, each leading to final answer (Wang et al., 2023). E.g., assuming 10-step rollouts and 8 subsequent trajectories for each step as in Wang et al. (2023), total of 10 8 = 80 trajectories need to be generated to get step labels for each instruction, which is 80 times more than ORMs. Therefore, the scaling of PRMs is largely limited. Besides the overhead of training data collection, this MCTS approach can lead to suboptimal performance due to the noisy annotation process, as we will show below and in the experiments. MCTS estimation is not precise either We denote the set of correctness of subsequent trajectories as {c1, c2, . . . , cN }, each element being 0 or 1. Thereafter, two alternative label estimation strategies are available: (1) Hard Estimation, where step will be labeled as 1 if any rollout is correct and 0 otherwise: lt = max {c1, c2, . . . , cN }. (2) Soft Estimation, where step is labeled as the proportion of correct answers among all rollouts, namely lt = (cid:80)N t=1 ct/N . We refer the ORM used to judge the correctness of rollouts as θ, the PRM trained on data from hard estimation as θh, and the PRM trained on soft estimation data as θs. If θh and θs are perfectly fitted, namely training losses reduced to 0, we have qt θh (y<t, yt) = max yy<t rθ(y), qt θs (y<t, yt) = Eπref(yyt)rθ(y) (1) However, both estimation strategies may be noisy. Specifically, qt represents the maximum outcome θh reward rθ given y<t, rather than the expectation, thus overestimating the value; For qt , given the θs limited capability of the policy model in practice, it can be challenging to sample correct solutions for difficult instructions, suffering from false negative noises and thus underestimating Q."
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "IMPLICIT PRMS FOR FREE THROUGH REWARD PARAMETERIZATION",
            "content": "In this section, we show that an ORM can directly represent an expectation of the outcome reward by itself by simple reward parameterization. In other words, PRM can be inherently derived from the same ORM without any dedicated training, offering better performance than MCTS-based approaches with substantially lower overhead. Reward parameterization in existing work Current literature typically parameterize rewards by either (1) the linear transformation of hidden states, with the reward model being sequence classifier (Ouyang et al., 2022; Touvron et al., 2023; Zhu et al., 2023; Cui et al., 2024) or (2) generative logits, with reward models being an auto-regressive LM and trained to predict the label of partial or complete responses as good or bad tokens, and sometimes third neutral (Zhang et al., 2024c; Mahan et al., 2024; Lightman et al., 2023; Wang et al., 2023; Luo et al., 2024a). Unfortunately, under either of the two parameterizations, PRMs would require expensive step labels to train. To address this issue, we propose to train an ORM with implicit reward modeling, which will automatically enable PRM regardless of the loss functions. Next, we illustrate this in detail: Proposition 3.1. (Proof in Appendix A) Consider an ORM where the reward is parameterized by the log-likelihood ratio of two causal LMs, i.e. rθ(y) := β log πθ(y) θ(y<t, yt) := (cid:80)t πref(y) . Define qt θ is the exponential average of rθ at step t. i=1 β log πθ(yiy<i) πref(yiy<i) . qt θ(y<t, yt) = β log Eπref(yyt)e qt 1 β rθ(y) (2) Hence, qt θ represents an exact expectation of outcome reward rθ at step t, i.e., the value. Proposition 3.1 indicates that when modeling rθ(y) := β log πθ(y) πref(y) to train an ORM with the standard pipeline, where β is hyperparameter, θ can implicitly learn function. Hence, process reward rt θ can be obtained by: θ := qt rt θ qt1 θ = (cid:88) i=t1 β log πθ(yiy<i) πref(yiy<i) (3) Notably, this conclusion still holds when yt represents the t-th token rather than step t. This gives us an inspiring hint: we can indeed obtain PRMs, or more fine-grained token-level RMs, simply by collecting response-level data and training an ORM, without any burden of annotating step labels. The proposition is agnostic to specific choices of the training objective of ORMs. It can be instantiated with different objectives as vanilla ORM training, with the only difference being substituting the rθ (y) with β log πθ(y) πref(y) . Particularly, many existing preference learning algorithms have already met our assumption (Rafailov et al., 2023; Azar et al., 2024; Ethayarajh et al., 2024; Chen et al., 2024; Wu et al., 2024). Besides making PRM training more accessible, our implicit process reward can be more accurate than those derived from qt in Eq. 1 (Wang et al., 2023), as indicated by the following proposition: θs Proposition 3.2. The performance of qt qt θs , and can reach these bounds with specific values of β. That is, θ is guaranteed by the following conditions: qt θ is bounded by and qt θh and qt θh qt θs = Eπref(yy<t)rθ(y) qt θ(y<t, yt) max yy<t rθ(y) = qt θh (4) holds. The left-hand equality is attained as β and the right-hand one is attained as β 0. Proposition 3.2 demonstrates that qt θ ranges between the soft-estimated and hard-estimated values annotated by MCTS-based approaches. The above bounds suggest that our approach has better accuracy and robustness to noises than MCTS-based approaches. Specifically, as discussed in 2, qt underestimates due to false negative noises. Since qt overestimates the value while qt θ θh θs lies between qt and qt , it could potentially mitigate both issues and estimate the value more θh θs accurately. Concurrent work defines our qt θ as an entropy regularized process reward and has empirically shown its superiority over qt on best-of-N sampling (Zhang et al., 2024a). θs and qt θh"
        },
        {
            "title": "Preprint",
            "content": "Connection to Rafailov et al. (2024) An intuition similar to Proposition 3.1 has been brought up by Rafailov et al. (2024), which demonstrates that DPO enables models to learn the function implicitly, but our insights subsume their conclusion since this property is not limited to the DPO algorithm. For example, given response-level label l, we can further generalize to cross-entropy (CE) loss to handle practical scenarios with unpaired and imbalanced data: LCE = log σ β log (cid:18) (cid:19) πθ(y) πref(y) (cid:20) (cid:18) + (1 l) log 1 σ β log (cid:19)(cid:21) πθ(y) πref(y) (5) Reference Model One difference between our modeling of rewards and previous ones is the incorporation of reference model πref. We acknowledge that this comes at an inference cost: to calculate the reward, both the policy and reference model are served, which doubles the inference cost than vanilla PRM. However, it is prevalent in existing preference learning algorithms and works as the KL constraint to prevent the policy model πθ deviating too far from its starting checkpoint. Moreover, it is less problem in practice, as we will show in 5.5.1 that large proportion of the inference overhead in best-of-N sampling comes from the generation model, especially when the generation model is much larger than the reward model. Further, we also show in 5.5.2 that when the implicit PRM is built from strong model that has undergone preference learning, such as Llama-3.1-Instruct, excluding πref leads to little or no accuracy drop. This makes our approach appealing in practice since it can achieve better accuracy than existing PRMs with exactly the same inference overhead, but substantially lower development overhead."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 SETUP Evaluation Following standard practice (Lightman et al., 2023), we evaluate PRMs with best-of-N (BoN) on MATH-500 (Hendrycks et al., 2021). To study the generalizability of the PRMs, we test each PRM using three generation models with different levels of capabilities: Mistral-Instruct-v0.3 (Jiang et al., 2023), Llama-3.1-8B-Instruct, and Llama-3.1-70B-Instruct (Meta, 2024). For each completion, we apply PRMs to score each step and pick the lowest step reward as the score for overall responses. We also compare the development overhead of the models in terms of FLOPs, including those required in both the automatic data collection and PRM training. Training dataset Unless stated otherwise, we adopt the following training setup throughout all experiments: We use math instructions from UltraInteract (Yuan et al., 2024) and sample eight rollouts per instruction using Llama-3.1-8B-Instruct, and then assess rollout correctness with ground truths. We train PRMs based on Llama-3.1-8B-Instruct with β = 0.05, which is empirically determined. Implicit PRM instantiation As demonstrated in 3, our approach can be instantiated with any reward modeling objective with the reward parameterized as rθ := β log πθ(y) πref(y) . We explore various objectives that meet the requirements, including DPO (Rafailov et al., 2023), KTO (Ethayarajh et al., 2024), NCA (Chen et al., 2024), and the cross-entropy (CE) loss. Please refer to Eq. 5 for the implementation of CE loss. For DPO and NCA, we pair each correct rollout with an incorrect counterpart and train our RM on these response-level pairs, while for KTO and CE loss, we directly train on the unpaired and imbalanced rollouts, which is more general in practical scenarios. We also implement two data balanced setup for CE to analyze the impact of pairwise data, i.e. balancing the positive and negative responses simply for the entire dataset, or more strictly for the each each instruction. We denote the two setups as Dataset-wise Balanced and Instruction-wise Balanceed. Baselines Our baselines include our implementation of existing methods and off-the-shelf open models. We reimplement Math-Shepherd (Wang et al., 2023) and AutoPSV (Lu et al., 2024) for fair comparisons, representative algorithms in their categories. Math-Shepherd annotates step labels using MCTS estimations as illustrated in 2. AutoPSV annotates steps with two-stage strategy. It firsts trains an outcome supervision verifier (OSV) that predicts value for each step, then use the OSV to annotate step labels. PRM is obtained by continual training on the OSV with process labels. We also compare to six off-the-shelf ORMs and PRMs, namely EurusRM-7B (Yuan et al., 2024), SkyworkRMLlama3.1-8B (Liu et al., 2024), ArmoRM-Llama3-8B (Wang et al., 2024), Math-Shepherd-7B (the"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Different reward models best-of-N sampling performance on MATH test set with three different generation models. When completing instructions with temperature of 0.5, the three generation models accuracies are 9.6%, 44.6%, and 63.2% respectively. Type Reward Model Mistral-7B-Inst-v0.2 Pass@1: 9.6 Llama-3.1-8B-Inst Pass@1: 44.6 Llama-3.1-70B-Inst Pass@1: 63. Avg. @4 @16 @64 @4 @16 @64 @4 @16 @64 ORM PRM EurusRM-7B SkyworkRM-Llama3.1-8B ArmoRM-Llama3-8B Math-Shepherd-7B RLHFlow-8B-Mistral-Data RLHFlow-8B-DS-Data Baselines Math-Shepherd AutoPSV Implicit PRM DPO KTO NCA CE CE (Dataset-wise Balanced) CE (Inst.-wise Balanced) Open-Source Reward Models 17.2 16.0 16. 16.0 19.4 17.2 21.0 19.6 21.0 21.0 25.2 23.0 20.4 23.4 23.2 20.4 30.2 25.2 Our Implementations 17.6 16.6 18.6 15.6 18.6 18.8 18.0 17.6 24.4 20.6 24.4 18.4 23.8 24.0 23.6 22.6 26.8 22.2 28.8 18.6 28.0 28.0 27.0 26. 49.6 49.0 47.8 50.0 51.8 54.4 50.0 52.2 54.0 49.6 52.4 52.6 52.6 52.6 51.6 50.4 48.6 52.4 52.0 54. 51.4 51.4 55.4 51.8 53.4 54.4 54.2 55.2 51.8 48.2 49.4 52.8 50.6 55.8 52.8 52.2 57.0 50.8 55.2 53.0 52.6 54. 69.0 70.4 70.6 66.4 70.8 68.6 68.6 68.4 71.8 72.6 69.0 70.6 68.6 69.4 69.6 72.6 70.8 65.8 71.0 70. 69.4 65.4 71.2 67.0 73.0 67.0 66.8 71.2 72.2 72.0 71.0 65.6 71.2 73.0 68.8 62.4 72.2 67.2 71.6 67.2 67.0 72. 46.9 46.8 46.6 45.6 49.1 49.1 47.8 45.7 50.4 45.7 49.4 48.4 47.8 49.0 offical release of Wang et al. (2023)), RLHFlow-8B-Mistral-Data2, and RLHFlow-8B-DS-Data3. We note that these off-the-shelf baselines are trained on different instructions and responses, while our two reimplementations are trained on the same data as our implicit PRM. 4.2 RESULTS Various implicit reward modeling objectives outperform baselines According to BoN results shown in Table 1, all four variants of our implicit PRMs consistently improve the accuracies of the three different generation models. Among them, DPO achieves an averaged accuracy of 50.4, performing better in general, closely followed by NCA with an averaged accuracy of 49.4. CE presents strong performance, despite that it is trained on unpaired and imbalanced data. Specifically, with an averaged accuracy of 48.4, it beats our implemented Math-Shepherd and AutoPSV by 0.6 and 2.7 respectively, and outperforms other open-source reward models except RLHFlow-8B-Mistral-Data and RLHFlow-8B-DS-Data, both of which achieves 49.1. This indicates the potential in empowering real-world applications where pairwise data is hard to collect. Nevertheless, according to CE versus CE (Inst.-wise Balanced), it is still beneficial to have balanced positive and negative responses for each instruction in the training dataset, which aligns with conventional understandings on CE as classification loss. However, comparing CE (Dataset-wise Balanced) to CE, simply balancing the entire dataset by randomly filtering examples of the class with more data can be detrimental. Our Implicit PRMs reduce the overhead of data collection and training by 38.8 As shown in Figure 2, with three different training data scales. Math-Shepherd generally costs 38.8x more FLOPs than the implicit PRM (CE). Compared to implicit PRM (DPO), the number becomes 146.5x, 49.9x, and 21.3x under different number of responses per instruction respectively. We plot the scaling trends of the average performance of each method with corresponding number of tokens consumed in Figure 1, from which we can clearly see that our implicit PRMs achieve better performance with much less data collection and training overhead."
        },
        {
            "title": "5 ANALYSIS",
            "content": "5.1 INCORPORATING MAJORITY VOTING 2https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Mistral-Data 3https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-DeepSeek-Data"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overhead of developing different PRMs, in terms of FLOPs during data collection and training. The axis indicates the number of responses per instruction which determines the scale of training data, and the axis is the number of FLOPs. Our implicit PRM always consumes the least FLOPs compared to baselines, with CE being 38.6 to 38.8 more efficient than Math-Shepherd across different dataset scales. Our implicit PRMs can be integrated with majority voting to improve the performance even further. Previously, we apply our implicit PRMs to score each response and pick the response with highest individual score as the final answer. However, when incorporating with majority voting, the scores of responses that lead to the same answer will be aggregated and the answer with the highest aggregated score will be selected as the final answer. We present the results averaged over different numbers of candidate solutions per problems across all three generated models in Figure 3. Figure 3: Results with majority voting. We the averaged best-of-N accuracy present across three testsets. We observe that our implicit PRM can successfully adjust voting distributions, and achieves better results than using the implicit PRM or majority voting separately. Particularly, KTO and CE variants gain the most from the integration, both of which fail to surpass majority voting alone but outperforms it through weighted best-of-N. It is also noteworthy that CE loss become the most effective when augmented with majority voting, once again demonstrating its potential. 5.2 SCALING UP INSTRUCTIONS AND RESPONSES CAN IMPROVE IMPLICIT PRMS Setup We conduct scaling analysis with DPO and CE on both instructions and responses of the training dataset. For instruction scaling, we randomly down sample 25%, 50%, and 75% instructions to train our implicit PRM. For response scaling, since DPO can only train on paired responses, we train models with 2, 4, and 8 rollouts respectively; while for CE, we also implement training with only one rollout per instruction, the extreme case of unpaired setup. Results We present results in Figure 4 and Figure 5 respectively. Takeaways are summarized as follows: (1) Scaling instructions and responses consistently improve the performance of our implicit PRM. The trend is particularly clear on Mistral-7B-Inst-v0.2 and Llama-3.1-8B-Inst, but there are also few outliers on Llama-3.1-70B-Inst. (2) Compared to instructions, scaling up responses seems to be more influential on implicit PRMs, as reflected by the larger performance variations between the minimum and maximum data setups. Taking closer look at the response scaling, (3) DPO requires more data to obtain descent performance than CE. From Figure 5a, DPO is under-trained with two responses per instruction, which can be partly attributed to the insufficient amount of instructions: two responses may not constitute pair to train our DPO variant, and thus many instructions can not be used in training. In contrast, CE generally performs better with insufficient data and can always improve different generation model, even when it is trained with"
        },
        {
            "title": "Preprint",
            "content": "(a) Implicit PRM (DPO). (b) Implicit PRM (CE). Figure 4: Scaling instruction numbers. Our implicit PRMs performance on Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct scales well with the number of instructions, despite the trend is more complex on Llama-3.1-70B-Instruct. one response per instruction with pairs, the extreme case of the unpaired setup. This presents huge advantage in real-worl data scarcity scenarios. 5.3 ARE THERE ANY OTHER FACTORS CAN IMPROVE IMPLICIT PRM PERFORMANCE? We consider potential factors that may influence the performance of implicit PRMs, as listed below: Task-irrelevant Instructions We previously only consider math instructions. We now examine if increasing instructions diversity, even if the instructions are irrelevant to downstream tasks, can benefit implicit PRMs. To this end, we incorporate general instructions from UltraFeedback (Cui et al., 2024) and coding instructions from UltraInteract (Yuan et al., 2024) into our training dataset. We directly use responses from the original datasets, but for UltraFeedback we only randomly select one pair for each instruction, instead of using all the pairs. Response Diversity We first conduct deduplication on our preference dataset based on 8-gram overlap, aiming to verify if repeated responses hurt model performance. We then randomly replace four rollouts per instruction in the original training dataset with another four rollouts generated by Llama-3.1-8B-Base model. Training on Step Labels Our implicit PRMs do not require step labels for training. However, we are interested in exploring whether augmenting them with step labels can further improve their performance. Based on the definition of process labels, we adjust the implicit reward of step by increasing it for positive labels and decreasing it for negative ones. We use the labels obtained from our implemented Math-Shepherd, which has been demonstrated to be strong implementation with step labels of high-quality (4). We adapt KTO to step-level version for optimization. Therefore, considering n-step response with step labels {l1, l2, . . . , ln}, we conduct second stage training on our current implicit PRM to explicitly optimize the implicit reward: Lθ = 1 θ)). t=1 log (σ (lt rt (cid:80)n"
        },
        {
            "title": "Preprint",
            "content": "(a) Implicit PRM (DPO). (b) Implicit PRM (CE). Note that one repsonse per instruction is the extreme case of the unpaired setup. Figure 5: Scaling responses number for each instruction. Our implicit PRM generally benefits from scaling up the number of responese for each instruction. Particularly, DPO is under-trained with two responses per instruction. This can be partly attributed to the insufficient amount of instructions: two responses may not constitute pair to train our DPO variant, and thus many instructions can not be used in training. In contrast, CE generally performs better with insufficient data and can always improve different generation model, even when it is trained with one response per instruction with pairs. Table 2: Factors that may affect PRM performance. To our surprise, none of them consistently improve our implicit PRM. Setup Implicit PRM + UltraFeedback + UltraInteract (Code) + Dedup. + Base Resp. + Step Label Mistral-7B-Inst-v0.2 Llama-3.1-8B-Inst Llama-3.1-70B-Inst @4 @16 @64 @4 @16 @64 @4 @16 @ 18.6 19.4 19.2 18.2 17.8 18.8 24.4 24.4 24.6 22.8 23.2 25. 28.8 29.0 28.0 26.8 27.6 28.8 54.0 53.8 54.6 52.0 54.0 53. 55.4 55.0 54.0 53.2 55.0 54.8 57.0 55.8 56.8 51.6 54.8 54. 71.8 71.6 71.4 69.8 71.4 70.8 71.2 70.6 70.8 69.4 72.4 71. 72.2 72.2 70.0 70.4 73.2 73.0 Avg. 49.3 49.2 49. 47.6 48.7 49.2 Results We present results on implicit PRM (DPO) in Table 2. In general, none of these factors brings consistent gains. (1) Both adding UltraFeedback and UltraInteract (code) instructions hurt the performance, with the former suffers more severely. This implies that training instructions deviating from the downstream task could undermine the performance of implicit PRMs. (2) Regarding response diversity, we observe that the performance of deduplicating responses hurts the performance and is close to implicit PRMs trained on similar amount of data. This indicates that repeated responses function similarly as others and are still beneficial before model performance saturates. Replacing part of original rollouts with those generated by the base model also fails to improve performance. (3) Conducting step-level KTO with extra process labels does not bring gains, reinforcing our claim that we can already train strong PRM without process label. However, one should be cautious about concluding that stepwise labels are generally not helpful due to two factors in our experiments: Firstly, despite our efforts that lead to improved step annotation quality compared to previous work, the MCTS-based approach inevitably introduces noises in the data annotation process, as we discussed"
        },
        {
            "title": "Preprint",
            "content": "in 2; Secondly, our choice of algorithm may not be optimal. It is possible that more advanced PRM data annotation methods and training algorithms can finally integrate information from (noisy) stepwise labels into implicit PRM."
        },
        {
            "title": "5.4 PRM ABILITY DOES NOT TRANSLATE INTO POLICY PERFORMANCE",
            "content": "Implicit PRM is trained in an auto-regressive manner, sometimes directly using preference learning algorithms, which are primarily used to improve policy models. Therefore, it reserves the nature as causal LM and can still serve as policy model to solve downstream problems directly. In this section, we test on MATH500 (Hendrycks et al., 2021; Lightman et al., 2023) to analyze the correlation between their PRM ability and performance as policy model. Table 3: Implicit PRMs performance on MATH500 when used to solve the problems directly. Model Accuracy Llama-3.1-8B-Inst + DPO + KTO + NCA + CE 45.2 25.8 46.6 35.6 28. According to Table 3, only trainiing with KTO leads to an improvement on MATH500, compared to Llama-3.1-8B-Instruct. Interestingly, based on Table 1, KTO performs the worst as an implicit PRM. In contrast, DPO and CE, the two algorithms that perform the best in without majority voting and with majority voting setups, respectively, achieve the lowest accuracies. This indicates that PRM ability does not improve as the policy model improves, and there can even be an unexpected trade-off between the both abilities. 5.5 CAN WE REDUCE THE INFERENCE OVERHEAD OF THE REFERENCE MODEL? One concern on our approach is the need of an additional reference model at inference. However, we show that the the reference model does not double overall inference overhead in practice, especially when the generation model is much larger than the reward model (5.5.1). Next, in 5.5.2, we show that the reference model can be removed at inference in certain cases. 5.5.1 THE REFERENCE MODEL DOES NOT DOUBLE OVERALL INFERENCE OVERHEAD Setup We calculate the time costs of best-of-N sampling on MATH500 in practice. The entire process includes (1) generating multiple candidate solutions to the instruction using the generation model, and (2) scoring each candidate using PRM. We use vLLM (Kwon et al., 2023) to implement the former and Huggingface Accelerate (Gugger et al., 2022) for the latter. Table 4: GPU time costs during best-of-N sampling relative to the cost of generation model (%). The overall inference overhead of baselines on three test sets are 66.6%, 70.8%, and 90.9% of that of our implicit PRM, respectively. Namely, the reference model does not double the inference cost in practice, and the extra inference overhead becomes more marginal as the generation model gets larger. Results We present the GPU time costs on A100 80G relative to that of the generation model in Table 4. We find that the inference overhead from generation model takes large proportion of the total overhead, especially when is the generation model much larger than the reward model. Therefore, the reference model in our implicit PRM does not double the overall inference cost in practice: The overall inference overhead of baselines on three test sets are 66.6%, 70.8%, and 90.9% of that of ours, respectively. It is noteworthy that the extra overhead introduced by the reference model becomes more marginal as the generation model larger, and is almost negligible when Llama-3.1-70B-Instruct serves as the generation model. Baselines Implicit PRM Baselines Implicit PRM Mistral-7B-Inst-v0.2 Llama-3.1-70B-Inst Llama-3.1-8B-Inst Generation Model Source of Cost Reward Model 33.5 201.6 29.4 141.7 111.1 122. 171.1 241.7 200.9 301.6 9.1 22.2 Method 100.0 100. 100.0 Total - 5.5.2 THE REFERENCE MODEL CAN BE REMOVED AT INFERENCE IN CERTAIN CASES We note that our proposition still holds under uniformly distributed reference model, i.e. log πref = constant. In best-of-N sampling, only relative scores between steps or responses matter, where the constant log πref can be canceled out, equivalent to exclude the reference model in reward"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Ablating reference model in both training and inference. Neither consistently hurts our implicit PRM. More surprisingly, the reference model, Llama-3.1-8B-Instruct, already perfroms well on Best-of-N sampling. Setup Train Mistral-7B-Inst-v0.2 Llama-3.1-8B-Inst Llama-3.1-70B-Inst Inference @4 @16 @64 @4 @16 @64 @4 @16 @64 Llama-3.1-8B-Instruct w/o Ref + DPO w/ Ref + DPO w/o Ref w/ Ref w/o Ref w/ Ref w/o Ref 14.8 18.6 17.8 17.8 17.4 16.2 24.4 23.4 23.4 22. 18.4 28.8 27.8 28.4 25.6 49.0 54.0 54.2 54.0 54. 50.4 55.4 56.6 55.2 56.4 52.2 57.0 57.6 57.6 58. 69.6 71.8 71.6 70.6 70.4 71.0 71.2 73.6 72.0 73. 71.0 72.2 73.2 73.2 74.0 Avg. 45.8 50.4 50. 50.2 50.3 parametrization. Therefore, we derive more efficient implementation of our proposition by removing the reference model. We examine its effectiveness and explore if we can simply our method to reduce the inference overhead in practice. Setup To this end, we explore two model training configurations: parameterizing the outcome reward either with or without reference model. . We then apply both models to best-of-N sampling and evaluate whether including the reference model has any impact to the performance. We also compare to directly using Llama-3.1-8B-Instruct, the reference model in our implicit PRM in previous experiments, as the reward model. It serves as controlled baseline without any RM training on our data, but has undergone preference learning (Meta, 2024). Results Surprisingly, no performance degradation is observed when the reference model is ablated in both training and inference, suggesting more practically efficient variant of our approach. Besides, Llama-3.1-8B-Instruct achieves strong performance too. This potentially explains why the reference model can be removed: The reference model is already capable of appropriately assigning high rewards to good steps and low ones to bad steps. Recall the process reward is (cid:80)t i=t1 β log πθ(yiy<i)/πref(yiy<i). Intuitively, good step might receive high probabilities by both πθ and πref, and therefore lowering its reward; on the other hand, bad step might receive low probabilities by both, thereby increasing its reward. This creates confusion to the PRM. We argue that this behavior is actually beneficial during RL training: when the reference model πref already performs well on certain actions, smaller rewards and consequently smaller policy gradients prevent over-training the policy model πθ on these already-optimized actions. Nevertheless, it is undesired on such inference-time response selection tasks. This suggests that our implicit PRM is particularly appealing in practice, since most of the time practitioners will build their PRMs from strong reference model such as Llama-3.1-8B-Instruct. In such cases, πref can be dropped in inference without hurting the performance as the above results suggest, and our approach can achieve stronger performance than baselines with substantially cheaper training, without introducing any additional inference overhead."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Complex Reasoning of LLMs Complex reasoning has become key capability of Large Language Models (LLMs) yet remains challenging even to state-of-the-art ones (Jimenez et al., 2024; Tian et al., 2024). Various techniques have been explored to improve LLMs on reasoning throughout different stages of their lifecycles, such as pre-training (Azerbayev et al., 2024; Paster et al., 2024; Li et al., 2023), post-training (Luo et al., 2024b; Yue et al., 2024; Yuan et al., 2024; Meta, 2024; Ouyang et al., 2022), and inference (Wei et al., 2022; Fu et al., 2023; Hao et al., 2023; Lightman et al., 2023). Among them, the process reward model (PRM) (Lightman et al., 2023), which scores model outputs step by step, has attracted recent attention for its effectiveness in variety of settings. Implicit Reward Implicit reward has already been widely adopted in preference learning. Despite primary work mainly focus on applying these algorithms to align models on top of supervised finetuning (Rafailov et al., 2023; Azar et al., 2024; Ethayarajh et al., 2024; Chen et al., 2024; Rosset et al., 2024; Wu et al., 2024), recent work also tries to leverage the implicit rewards of resulting models as"
        },
        {
            "title": "Preprint",
            "content": "outcome rewards (Lambert et al., 2024; Zhong et al., 2024; Hosseini et al., 2024). Further, following Rafailov et al. (2024), which showed that DPO can automatically learn function, Qiu et al. (2024) devise self-guided decoding algorithm limited for DPO models leveraging such property. However, despite these applications of adopting DPO models as off-the-shelf reward models or functions, none of existing work specifically targets improving such ability or investigating how to derive decent PRMs upon those off-the-shelf models."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We start with theoretical proposition demonstrating that parameterizing the outcome reward as the log-likelihood ratios of the policy and reference models log πθ(y) πref(y) , PRM can be intrinsically learned at the same time without any extra training requirements. We discuss its universality to instantiate different training objectives. In experiments, we demonstrate that various implicit reward modeling objectives outperform baselines on MATH, with substantially better trade-offs between accuracy and development overhead, particularly the CE loss. The performance of implicit PRMs can be further improved with majority voting. Further, scaling up instructions and responses benefit our implicit PRM, with the latter having larger effect, but instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data."
        },
        {
            "title": "REFERENCES",
            "content": "Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. general theoretical paradigm to understand learning from human preferences. International Conference on Artificial Intelligence and Statistics, abs/2310.12036, 2024. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. ICLR, 2024. Meng Cao, Lei Shu, Lei Yu, Yun Zhu, Nevan Wichers, Yinxiao Liu, and Lei Meng. Enhancing reinforcement learning with dense rewards from language model critic. In EMNLP, 2024. Alex J. Chan, Hao Sun, Samuel Holt, and Mihaela van der Schaar. Dense reward for free in reinforcement learning from human feedback. ICML, 2024. Huayu Chen, Guande He, Lifan Yuan, Ganqu Cui, Hang Su, and Jun Zhu. Noise contrastive alignment of language models with explicit rewards. ArXiv, abs/2402.05369, 2024. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with scaled ai feedback. In ICML, 2024. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. ICML, 2024. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. ICLR, 2023. Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. EMNLP, 2023."
        },
        {
            "title": "Preprint",
            "content": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. ArXiv, 2021. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron C. Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners. COLM, 2024. Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. ArXiv, abs/2310.06825, 2023. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? ICLR, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Nathan Lambert, Valentina Pyatkin, Jacob Daniel Morrison, Lester James Validad Miranda, Bill Yuchen Lin, Khyathi Raghavi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hanna Hajishirzi. Rewardbench: Evaluating reward models for language modeling. ArXiv, abs/2403.13787, 2024. Jan Leike, 2024. URL https://x.com/janleike/status/1821940180032594393? s=46. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Randy Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nourhan Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, Alexander Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean M. Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! TMLR, 2023. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. ArXiv, 2023. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024. Jianqiao Lu, Zhiyang Dou, Hongru Wang, Zeyu Cao, Jianbo Dai, Yingjia Wan, Yinya Huang, and Zhijiang Guo. Autopsv: Automated process-supervised verifier. ArXiv, abs/2405.16802, 2024. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision. ArXiv, abs/2406.06592, 2024a. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. ICLR, 2024b. Dakota Mahan, Duy Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, JanPhilipp Franken, Chelsea Finn, and Alon Albalak. Generative reward models. 2024."
        },
        {
            "title": "Preprint",
            "content": "Meta. Llama 3 model card. Github, 2024. URL https://github.com/meta-llama/ llama3/blob/main/MODEL_CARD.md. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text, 2024. Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, and Mengdi Wang. Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling. 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 2023. Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From to q: Your language model is secretly q-function. ArXiv, 2024. Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general preferences. ArXiv, abs/2404.03715, 2024. Amrith Rajagopal Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. 2024. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. ArXiv, abs/2408.03314, 2024. Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Min Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, E. A. Huerta, and Hao Peng. Scicode: research coding benchmark curated by scientists. Arxiv, 2024. Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In EMNLP, 2024. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Y.Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. ArXiv, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. NeurIPS, 2022."
        },
        {
            "title": "Preprint",
            "content": "Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. ArXiv, abs/2405.00675, 2024. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. Advancing llm reasoning generalists with preference trees. ArXiv, 2024. Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. NeurIPS, 2024. Hanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang, Pavlo Molchanov, and Tong Zhang. Entropy-regularized process reward model, 2024a. Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, et al. Ultramedical: Building specialized generalists in biomedicine. arXiv preprint arXiv:2406.03949, 2024b. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. 2024c. Han Zhong, Guhao Feng, Wei Xiong, Li Zhao, Di He, Jiang Bian, and Liwei Wang. Dpo meets ppo: Reinforced token optimization for rlhf. ArXiv, abs/2404.18922, 2024. Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November 2023."
        },
        {
            "title": "A PROOF OF PROPOSITION",
            "content": "Proposition A.1. Consider an ORM where the reward is parameterized by the log-likelihood ratio of two causal LMs, i.e. rθ(y) := β log πθ(y) θ is the exponential average of rθ at step t. i=1 β log πθ(yiy<i) θ(y<t, yt) := (cid:80)t πref(y) . Define qt πref(yiy<i) . qt θ(y<t, yt) = β log Eπref(yyt)e qt 1 β rθ(y) (6) Proof. The Proposition can be proven using mathematical induction. Suppose response has tokens. (1) For < , if qt+1 β log Eπref(yyt)e θ 1 β rθ(y) would also hold. (y<t+1, yt+1) = β log Eπref(yyt+1)e 1 β rθ(y) holds, then qt θ(y<t, yt) = (2) At = , qT θ (y<T , yT ) = rθ(y) = β log Eπref(yyT )e 1 β rθ(y). proof of (1):"
        },
        {
            "title": "Preprint",
            "content": "β log Eπref(yyt)e 1 β rθ(y) = β log Eπref(yt+1yt)Eπref(yyt+1)e 1 β rθ(y) = β log Eπref(yt+1yt)e β qt+1 θ (y<t+1,yt+1) = β log Eπref(yt+1yt) t+1 (cid:89) i= πθ(yiy<i) πref(yiy<i) = β log = β log = β log = β log (cid:89) i=1 (cid:89) i=1 (cid:89) i=1 (cid:89) i= πθ(yiy<i) πref(yiy<i) πθ(yiy<i) πref(yiy<i) πθ(yiy<i) πref(yiy<i) πθ(yiy<i) πref(yiy<i) Eπref(yt+1yt) πθ(yt+1yt) πref(yt+1yt) πref(yt+1yt) πθ(yt+1yt) πref(yt+1yt) πθ(yt+1yt) (cid:88) yt+1 (cid:88) yt+1 proof of (2): The conclusion is straightforward. Since π is autoregressive, we have rθ(y) := β log πθ(y) πref(y) = β log (cid:89) i=1 πθ(yiy<i) πref(yiy<i) = (cid:88) i= β log πθ(yiy<i) πref(yiy<i) . Since yT = y, the expectation Eπref(yyT ) can be removed: β log Eπref(yyT )e β rθ(y) = β log 1 β rθ(y) = rθ(y)."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "Tsinghua University",
        "University of Illinois Urbana-Champaign"
    ]
}