{
    "paper_title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers",
    "authors": [
        "D. She",
        "Mushui Liu",
        "Jingxuan Pang",
        "Jin Wang",
        "Zhen Yang",
        "Wanggui He",
        "Guanghao Zhang",
        "Yi Wang",
        "Qihan Huang",
        "Haobin Tang",
        "Yunlong Yu",
        "Siming Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality."
        },
        {
            "title": "Start",
            "content": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers D. She * 1 Mushui Liu * 2 Jingxuan Pang 2 Jin Wang 1 Zhen Yang 3 Wanggui He Guanghao Zhang Yi Wang 2 Qihan Huang 2 H. Tang 1 Yunlong Yu 2 Siming Fu 2 Project Page 5 2 0 2 0 1 ] . [ 1 7 2 5 6 0 . 2 0 5 2 : r Figure 1: CustomVideoX synthesizes natural motions while preserving the fine-grained object details."
        },
        {
            "title": "Abstract",
            "content": "Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffu- *Equal contribution 1University of Science and Technology of China 2Zhejiang Univerisity 3Hong Kong University of Science and Technology (Guangzhou). Correspondence to: Siming Fu <fusiming@zju.edu.cn>, Yunlong Yu <yuyunlong@zju.edu.cn>. sion transformer for personalized video generation from reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware 1 CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality. 1. Introduction Text-to-video (T2V) generation (Wu et al., 2023; Guo et al., 2024; Yang et al., 2024) have recently garnered extensive attention in the fields of computer vision. The ability to generate visual content from textual descriptions holds significant promise for applications, e.g., digital art creation, and advertising. In this domain, customized video generation (Wei et al., 2024a; Chefer et al., 2024; Jiang et al., 2024; Wu et al., 2024a) aims to generate videos that not only adhere to textual instructions but also maintain consistency with provided reference images. Although strongly in need, this topic remains underexplored unlike customized image generation (Ruiz et al., 2023; Gal et al., 2023; Ye et al., 2023; Chen et al., 2023; Wang et al., 2024; Tan et al., 2024). Existing work on customized video generation (Wei et al., 2024a;b; Jiang et al., 2024; Wu et al., 2024b;a) still struggles to maintain the subject identity of reference images across varying frames with high fidelity, often overlooking significant detail information. Additionally, the temporal coherence between consecutive frames remains unsatisfactory (Chefer et al., 2024; Wei et al., 2024a), exhibiting discontinuities. These limitations stem from the dual flaws of current frameworks: (1) inefficient exploitation of pixellevel reference guidance during spatial feature encoding, which undermines identity preservation by neglecting local structural cues, and (2) the uniform propagation of reference features throughout the denoising process fails to accommodate the distinct requirements of early-stage structural establishment versus late-stage motion dynamics refinement. This inflexible paradigm results in both identity degradation and temporal coherence artifacts, highlighting the need for temporally-aware reference adaptation mechanisms. we develop 3D Reference Attention mechanism that enables direct interaction between reference image features and each frame of the video within the VDiT framework. This strategy streamlines the interaction process by eliminating the need for separate temporal and spatial attention stages, thereby enhancing both efficiency and effectiveness. The reference image features are extracted using the same VDiT model, obviating the need for additional encoders and ensuring seamless integration. Second, we introduce Time-Aware Attention Bias mechanism to modulate the influence of reference image features throughout the denoising process inherent in diffusion models. By dynamically modulating the weight of reference image featuresinitiating with minimal influence when the input predominantly consists of random noise, escalating during intermediate phases, and diminishing in the final stages. This dynamic weighting promotes better temporal coherence and visual quality in the generated videos, allowing the model to capture both the overall structure and fine-grained temporal details effectively. Finally, we introduce an Entity Region-Aware Enhancement (ERAE) module that adaptively focuses on the key entity regions. After computing the attention score of the key entitys textual token with the latent, we adjust the attention bias by setting threshold to refine the focus. To tackle the challenge of limited paired reference video data, we introduce method for synthesizing high-quality reference video pairs, creating large-scale dataset of two million (2M) pairs. This enhances training and improves model generalization. We also propose comprehensive benchmark for personalized video generation, covering wide range of objects and scenes. As shown in Figure 1, our approach demonstrates strong personalization capabilities. Extensive experiments confirm that our method outperforms existing approaches in video quality and temporal coherence, setting new state-of-the-art on both existing and proposed benchmarks. The contributions of this paper can be summarized as follows: We present CustomVideoX, zero-shot personalized video generation framework based on the VDiT architecture, enabling physically consistent articulations without compromising high-resolution morphological details. The integration of 3D Reference Attention and TimeAware Attention Bias within CustomVideoX facilitates more effective learning process between reference images and video generation, resulting in high-quality customized videos. To address the above challenges, we propose CustomVideoX, novel framework for zero-shot personalized video generation grounded in the Video Diffusion Transformer (VDiT) architecture, e.g., CogVideoX (Yang et al., 2024). First, We developed robust data generation pipeline and established comprehensive evaluation benchmark, demonstrating that CustomVideoX achieves the best video generation performance on both existing and 2 CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers newly introduced benchmarks. 2. Related Works Text-to-Video Diffusion Models. The ongoing advancements in diffusion models (Rombach et al., 2022) and textto-image (T2I) generation models (Betker et al., 2023; Esser et al., 2024; Podell et al., 2023; He et al., 2025; Liu et al., 2024; Ying et al., 2024; Zhao et al., 2024; Sun et al., 2024) have spurred new research initiatives in the realm of textto-video (T2V) generation models (Guo et al., 2024; Wu et al., 2023). Current studies in video generation predominantly fall into two categories: extending T2I models to T2V models and training T2V generation models directly from scratch. The former prominent works such as AnimateDiff (Guo et al., 2024) and Tune-A-Video (Wu et al., 2023) have emerged. Owing to the limited availability of high-quality video-text paired datasets, these models capitalize on the rich prior knowledge embedded in existing image generation models to synthesize video content. By adapting T2I frameworks, they generate coherent video sequences conditioned on textual input, effectively compensating for the scarcity of video data. In the second category, there is growing momentum in constructing T2V models directly. For instance, the Video Diffusion Model (VDM) (Ho et al., 2022) extends the capabilities of stable diffusion (Rombach et al., 2022) by incorporating temporal attention mechanisms and 3D-Conv modules to process video data effectively. With the introduction of the Diffusion Transformer (DiT) (Peebles & Xie, 2023), T2V generation models such as Sora and CogVideoX (Yang et al., 2024) have been developed based on the DiT architecture. Notably, CogVideoX employs 3D-VAE to encode video information, enhancing the models ability to capture complex spatiotemporal relationships. Overall, video generation models provide robust technical support for the creation of customized video content and represent significant research direction in the fields of computer vision and pattern recognition. Custom Video Generation. Customized video generation aims to produce video content that closely aligns with given reference image and textual description by integrating both inputs. Prior methods in customized image generation (Ye et al., 2023; Zhang et al., 2024; Wang et al., 2024) primarily fall into two categories: fine-tuning and zero-shot approaches. Fine-tuning methods, exemplified by DreamBooth (Ruiz et al., 2023), achieve high-fidelity generation of specific images through personalized adjustments to pre-trained models. Zero-shot methods, such as IP-Adapter (Ye et al., 2023) and SubjectDiffusion (Ma et al., 2024), incorporate novel concepts or styles into the generation process without additional model training. In the realm of customized video generation, these methodologies have been extended along similar lines. Fine-tuning approaches, including models like Still-Moving (Chefer et al., 2024) and DreamVideo (Wei et al., 2024a), accomplish customized video generation by specifically tuning video generation models. More recently, zero-shot methods like VideoBooth (Jiang et al., 2024) have showcased the potential for customized video generation without extra training by extending text-to-image (T2I) models (Wu et al., 2023; Yang et al., 2024) to text-to-video (T2V) generation. Different from these works, we explore zero-shot customized video generation framework based on DiT (Peebles & Xie, 2023) and introduce novel adaptive feature injection method. 3. Preliminary Diffusion Models for Text-to-Video Generation. Diffusion models generate samples from Gaussian random noise through multiple sequential denoising steps. These steps form Markov process that progressively transforms the initial noise xT into the target sample x0, defined as: xt = αtx0 + σtϵ, ϵ (0, I), (1) where αt and σt are scheduling parameters that jointly determine the denoising ratio at each time step according to different schedulers, such as DDPM (Ho et al., 2020) and Flow Matching (Kingma & Gao, 2024; Esser et al., 2024). The denoising model ϵθ is trained to predict the noise ϵ added to xt at each time step by minimizing the discrepancy between the true noise and the models prediction. The loss function is defined as: L(θ) = Et,x0,ϵ ϵθ(xt, t) ϵ2 2 (2) Latent Diffusion Models (LDMs) (Rombach et al., 2022) utilize Variational Autoencoders (VAEs) (van den Oord et al., 2017) to obtain low-dimensional latent representations, enabling more efficient diffusion processes. Similarly, in video generation, 3D-VAEs (Yu et al., 2023) compress both spatial and temporal dimensions of videos, simplifying the learning task for diffusion models. The denoising network ϵθ can be implemented using architectures such as U-Net or Diffusion Transformers. 4. CustomVideoX 4.1. 3D Reference Attention In CogVideoX, the video and text interact through 3D Full Attention. The video input RT HW is compressed into the latent space zv via 3D causal VAE (Yu et al., 2023). The text input is processed through the T5 text encoder (Chung et al., 2022) to obtain zt. Both zv and zt are encoded using their respective visual and text expert transformers, respectively. They then interact within the multi-modal attention mechanism (Esser et al., 2024) to 3 CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers Figure 2: The overall pipeline of CustomVideoX. CustomVideoX is capable of producing personalized videos that conform to specified instructions, utilizing provided image objects and textual descriptions. It enhances each video frame by incorporating reference image through 3D Reference Attention mechanism, allowing for dynamic interactions between the reference images and video frames, both temporally and spatially. Moreover, CustomVideoX employs Time-Aware Attention Bias strategy and an Entity Region-Aware Enhancement module to boost spatial and temporal coherence throughout the denoising process, enabling the model to maintain consistent reference feature capture across frames. produce the final output, as formulated as: zv = VisualExpert(zv), zt = TextExpert(zt) zv, zt = MM-Attention(zv, zt) (3) Prior methods (Ye et al., 2023; Hu et al., 2024) typically depend on auxiliary networks or adapters to inject reference image features, which often introduces misalignment with native video representations and necessitates costly retraining. To address this, as shown in Figure 2, we propose parameter-efficient adaptation of the video expert transformer by integrating LoRA (Hu et al., 2021) to adapt these blocks for handling the additional reference image condition features without compromising the pre-trained models integrity. Specifically, we encode the reference image using 3D causal VAE to obtain zr. The zr is then processed through vision expert transformer equipped with LoRA parameters for learnable training. Subsequently, we can naturally connect the zr, zv, and zt to perform 3D Reference Attention mechanism, formulated as: = Concat([zv, zt, zr]), z3D-ref = Softmax (cid:18) (zWQ)(zWK) dk (cid:19) (zWV ), (4) zt, zv, zr = Split(z3D-ref), where Concat denotes the concatenate operation, Split indicates the split operation. WQ, WK, WV are the learnable linear layer. Notably, in the 3D Reference Attention, the positional encoding of zr is shifted: pr = pv + δ, (5) 4 where pv is the token position in zv and the shift δ allows zr to interact more effectively with zv. 3D Reference Attention enables flexible token interactions, allowing direct relationships to emerge between any pair of tokens without imposing rigid spatial constraints. 4.2. Time-Aware Attention Bias To harmonize pre-trained denoising capabilities with robust reference feature integration, we propose the Time-Aware Bias (TAB) mechanism, phase-adaptive modulation strategy for cross-attention layers. TAB dynamically adjusts reference feature influence via parabolic temporal mask, formalized as: Aref = Softmax (cid:32) QK ref (cid:33) + tanh(M (t)) Vref , (6) where = zWQ, Kref = zref WK, and Vref = zref WV , the temporal modulation function (t) adopts an inverted parabolic profile: (t) = γ (cid:18) 2t (cid:19)2 1 + ϵ. (7) Here, [0, ] indexes the denoising timestep, γ = 10 (empirically determined) governs suppression intensity, and ϵ = 0.1 ensures numerical stability. As illustrated in Figure 3, the U-shaped design of (t) achieves three synergistic objectives: Boundary Suppression strategically attenuates reference feature influence at = 0 and = through parabolic minima, preserving structural fidelity by leveraging pre-trained denoising priors during critical initialization and finalization phases; Mid-phase Enhancement emerges via neutral bias conditions (M (t) 0) around = /2, CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers Algorithm 1 CustomVideoX Generation Pipeline 1: INPUT: Pre-trained video diffusion model Mθ, LoRAparameterized variant MθL, reference image xref RHW 3, entity tokens E, total timesteps 2: INIT: Sample z(T ) (0, I), initialize hyperparameters: {λ = 0.1, β1 = 0.1, γ = 10, ϵ = 0.1} 3: Extract reference features zref MθL(xref) 4: for = to 0 do 5: 6: 7: Compute time-aware attention bias Mt Generate entity region mask Re Update latent state via reference-attention: Eq. (7) Eq. (8) z(t1) Mθ (cid:0)z(t) , t, Mt Re, zref (cid:1) 8: end for 9: Output: Generated video = D(z(0) ) D: decoder Eq. (4) of entity-associated activations, and (ii) context preservation via controlled magnitude modulation. As demonstrated in Algorithm 1, the differentiable thresholding operation enables end-to-end learning of region-semantic correspondence. 5. Dataset 5.1. Training Video Dataset Despite the abundance of datasets for customized image generation, resources for customized video generation remain limited. In this study, we introduce high-quality, customized single-object dataset tailored for training purposes. We curated aesthetically appealing data from the open-source OpenVid dataset. Using Grounding-DINO (Liu et al., 2023) and SAM (Kirillov et al., 2023) for annotation, we extracted objects from selected frames to serve as references, with the subsequent frames used as ground truth. Overall, we have amassed collection of 51K data samples. The dataset encompasses diverse range of sources including humans, animals, vehicles, buildings, and natural scenes. As detailed in Appendix A, all data undergoes rigorous quality control through three-stage validation process incorporating video resolution, aesthetic score, motion score and temporal consistency checks. 5.2. VideoBench Benchmark To assess the performance of customized video generation, we have developed benchmark, VideoBench, depicted in Figure 4. This benchmark comprises video generation tasks for 50 different objects and over 100 prompts, ensuring no overlap with the training set. 5 Figure 3: The time-aware attention bias v.s. fixed attention bias in the sampling process. TAB dynamically regulates the influence of reference features using parabolic temporal mask, enhancing the consistency of reference images throughout the generation sequence. allowing full assimilation of reference features during the intermediate refinement stage; while Progressive Transition is mathematically guaranteed by the 2-continuous parabolic profile, ensuring differentiable temporal evolution that maintains gradient stability throughout the denoising trajectory. This temporally modulated architecture enables phase-specific feature emphasis without compromising the backbone models inherent generative capabilities. 4.3. Entity Region-Aware Enhancement Conventional approaches for principal content emphasis often employ explicit mask-guided paradigms to constrain video generation to key image regions, at the cost of sacrificing output diversity. To resolve this fundamental tradeoff, we propose the Entity Region-Aware Enhancement (ERAE) module, which achieves adaptive region emphasis through latent semantic alignment without hard spatial constraints. Given semantic entity token (e.g., dog), the module computes coarse entity regions via activation thresholding: Re = I(Ae > λ), Ar Ar + b1 Re, (8) where I() denotes the indicator function, Ae RHW represents activation intensity from cross-modal attention, and λ = 0.1 serves as the empirically determined intensity threshold. The target feature map Ar receives localized reinforcement through element-wise multiplication () with enhancement magnitude b1 = 0.1, calibrated to preserve background generation flexibility. This formulation induces two complementary effects: (i) semantic grounding through gradient-aware amplification CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers Figure 4: Overview of the Proposed VideoBench Framework. From left to right, panel left illustrates word counting, while panel right provides visual examples demonstrating the application of the framework. Table 1: Quantitative comparison of different methods across DreamBench and VideoBench benchmarks. Bold represents the best result and data underline represents the second-best result. Methods DreamBench VideoBench CLIP-T CLIP-I DINO-I T.Cons D.D CLIP-T CLIP-I DINO-I T.Cons D.D BLIP-Diffusion (Li et al., 2023) IP-Adapter (Ye et al., 2023) λ-Eclipse (Patel et al., 2024) SSR-Encoder (Zhang et al., 2024) MS-Diffusion (Wang et al., 2024) VideoBooth (Jiang et al., 2024) CustomVideoX (Ours) 29.97 28.87 34.01 29.32 33.74 28.99 34.28 84.39 82.00 82.92 83.24 85.47 76.42 85.47 84.14 82.18 84.10 83.92 87. 77.90 88.17 95.87 95.16 95.98 95.42 96.65 96.17 96.77 54.87 56.64 59.29 59.29 53.98 45.13 50.44 29.85 29.38 32.25 29.97 32. 28.92 33.38 88.83 86.92 89.45 87.01 90.55 82.60 90.26 87.99 87.87 89.94 87.74 91.29 84.06 91.49 96.88 96.23 97.54 96.63 97. 96.75 97.26 36.00 42.00 38.00 40.00 54.00 46.00 46.00 6. Experiments 6.1. Experimental Setup Implementation Details. We use CogVideoX-5B as our base model and apply LoRA (Hu et al., 2021) to fine-tuning, setting the rank to 128. This approach introduces only an additional 2.3% of trainable parameters, thereby reducing training complexity while maintaining model performance. All experiments are carried out on 16 NVIDIA H20 GPUs, with batch size of 1 per GPU and gradient accumulation set to 2. During training and inference, the resolution of the frame is fixed at 480 720 pixels, and 25 sequential frames are extracted per video at an interval of three frames. For optimization, we employ the AdamW optimizer with fixed learning rate of 1e-5 and weight decay of 1e-2. The model is trained for total of 10,000 steps. Regarding hyperparameters, we define and utilize Time-Aware Initialization Attention mechanism to enhance the models temporal awareness. During inference, we use DDIM (Song et al., 2020) with 50 inference steps to generate videos. Evaluation Metrics. Following (Jiang et al., 2024; Wu et al., 2024a), we evaluate the quality of the generated videos from two perspectives: overall consistency and subject fidelity. For overall consistency, we employ three metrics: CLIP-T, Temporal Consistency (T.Cons), and Dynamic Degree (D.D). CLIP-T measures the average cosine similarity between the CLIP (Radford et al., 2021) image embeddings of all generated frames and their corresponding text embeddings. T.Cons calculates the average cosine similarity between the CLIP image embeddings of consecutive frames, assessing temporal coherence. D.D (Huang et al., 2024) utilizes optical flow to quantify motion dynamics within the video. To evaluate subject fidelity, we use CLIP-I and DINO-I for both customized human and object video generation tasks. CLIP-I assesses the visual similarity between the generated frames and the target subjects by computing the average cosine similarity between the CLIP image embeddings of all generated frames and the reference images. DINO-I (Caron et al., 2021) is another metric of visual similarity, utilizing the ViT-S/16 model. Comparison Methods. We combine several customized image generation approaches, i.e., BLIP-Diffusion (Li et al., 2023), IP-Adapter (Ye et al., 2023), λ-Eclipse (Patel et al., 2024), SSR-Encoder (Zhang et al., 2024), MS-Diffusion (Wang et al., 2024), with the Cogvideox5b-I2V (Yang et al., 2024) model (image-to-video) to develop naive two-stage customized video generation pipeline for comparison with our proposed method. Additionally, we compared our approach with the fine-tuning-based model VideoBooth(Jiang et al., 2024). 6 CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers Figure 5: The qualitative results compared to the personalized model (+ I2V model). When compared to several different methods, CustomVideoX clearly demonstrates superior capabilities on concept fidelity and caption semantic consistency 6.2. Quantitative Comparison We evaluated the proposed method using two benchmarks: DreamBench and VideoBench. The quantitative results are illustrated in Table 1. Compared to the naive custom video generation approach, our model demonstrates comprehensive superiority across two different benchmarks. Specifically, in terms of text alignment, our approach outperforms the second-place method, ms-diffusion, by an average of 0.52%. Regarding subject fidelity, our method exceeds the second-place approach by 1.37% in CLIP-I and 1.72% in DINO-I, highlighting our models exceptional capability in maintaining consistency. In terms of video generation quality, our approach also achieves the best results in T.Cons. Although it performs slightly weaker in D.D compared to other methods, this is because the CogvideoX-5b I2V model tends to generate incoherent cases with large interframe variations. While the motion scale is higher, coherence is compromised, as corroborated by the T.Cons scores. Compared to the finetune-based customized video generation approach VideoBooth, our method significantly outperforms VideoBooth in all aspects, confirming the robust capabilities of our approach in concept fidelity and video quality. Qualitative Comparison. In Figure 5, we present comparative analysis between our proposed method and the naive customized video generation approach. In the first sample, 7 Figure 6: Compared with VideBooth. CustomVideoX adopts the optimal solution to effectively preserve the fidelity of image prompts and achieve better visual quality. CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers Table 2: Impacts of different modules. We refer to as the 3D-Reference-Attention module, as the TAB module, and as ERAE. Bold represents the best result and data underline represents the second-best result. CLIP-T CLIP-I DINO-I T.Cons DD 27.31 33.56 33.86 33.38 74.45 89.89 90.10 90.26 76.57 91.13 91.12 91.49 95.45 97.29 97.24 97. 38.00 42.00 44.00 46.00 which depicts vehicle in motion, our method significantly outperforms the naive methods in terms of subject fidelity. Furthermore, both BLIP-Diffusion and SSR-Encoder exhibit instances where an additional car passes by in the video, limitation inherent to naive customized video generation. This issue arises because I2V models struggle to bind subjects in the image with their corresponding concepts in the text, leading to the generation of extraneous subjects. Similarly, in the second sample, which features dog interacting with its owner, our method achieves the best results in both subject fidelity and text alignment. Figure 6 illustrates comprehensive visual comparison between our proposed method and Videobooth. In the first example, which involves generating robot toy, our method demonstrates superior performance in subject fidelity compared to Videobooth. Furthermore, our approach ensures consistent identity preservation of the robot during motion, effectively avoiding distortions and structural collapses. In the second example, depicting panda munching on bamboo shoots, Videobooth, although maintaining some level of subject fidelity, fails to accurately capture the munching action. In contrast, our method successfully executes this instruction, achieving both action accuracy and naturalness. 6.3. Ablation Studies To validate the effectiveness of the proposed components, we conduct comprehensive ablation studies. The quantitative results are presented in Table 2, while the qualitative results are shown in Figure 7. Naive Reference Image Injection. In this baseline approach, the reference target image is simply injected as an additional frame input to the model. As illustrated in the first column of Figure 7, this method struggles to maintain object consistency across the generated video, resulting in noticeable fluctuations in the targets appearance throughout the sequence. 3D Reference Attention Injection. The 3D Reference Attention injection method introduced in this paper demonstrates significant improvements across all evaluation metrics. Notably, the CLIP-I and DINO-I scores increased by 15.44% and 14.56%, respectively, particularly improving Figure 7: Ablation study visualizations comparing module contributions. Demonstration of the effectiveness of the 3D Reference Attention, TAB, and ERAE modules. target consistency. The proposed shift position encoding addresses the issue of high consistency between the initial frame and the reference target while ensuring sustained consistency in subsequent frames. By enabling each frame of the video to focus equally on the reference object, shift position encoding enhances the ability to generate consistent targets. Time-Aware Attention Bias. The proposed Time-Aware Attention Bias employs progressive reference information injection approach, providing the model with more stable learning process during the initial stages. This method further improves target consistency upon completion. Entity Region-Aware Enhancement. Entity Region-Aware Enhancement adaptively applies additional interactions to highly correlated tokens within the video based on the target text tokens. This approach not only improves target consistency in video generation but also enhances the interaction between the target and the environment, increasing the dynamic quality of the video. As indicated by the D.D metric in Table 2, the absence of this module results in reduction of 2. Notably, our training dataset does not include clothing categories, highlighting the strong generalization capability of CustomVideoX. 7. Conclusions In this paper, we present CustomVideoX, zero-shot model for customized video generation. CustomVideoX extracts reference image information through single network and injects the reference features into the noise using region8 CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers aware and time-aware feature adjustments. This enables convenient and continuous extraction of reference features. Additionally, the model leverages 3D reference attention, allowing for natural interaction between the reference object and each frame of the generated video. To train and evaluate CustomVideoX, we have collected high-quality customized video dataset. Experimental results demonstrate that CustomVideoX achieves state-of-the-art performance on DreamBench and VideoBench."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work that aims to advance the field of Machine Learning and Video Generation. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2:3, 2023. Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In ICCV, 2021. Chefer, H., Zada, S., Paiss, R., Ephrat, A., Tov, O., Rubinstein, M., Wolf, L., Dekel, T., Michaeli, T., and Mosseri, I. Still-moving: Customized video generation without customized video data. ACM Transactions on Graphics (TOG), 43(6):111, 2024. He, W., Fu, S., Liu, M., Wang, X., Xiao, W., Shu, F., Wang, Y., Zhang, L., Yu, Z., Li, H., et al. Mars: Mixture of auto-regressive models for fine-grained text-to-image synthesis. In AAAI, 2025. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In NeurIPS, 2020. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. In NeurIPS, 2022. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. In ICLR, 2021. Hu, L., Gao, X., Zhang, P., Sun, K., Zhang, B., and Bo, L. Animate anyone: Consistent and controllable image-tovideo synthesis for character animation. arXiv, 2024. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, pp. 2180721818, 2024. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Lu, K., et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Jiang, Y., Wu, T., Yang, S., Si, C., Lin, D., Qiao, Y., Loy, C. C., and Liu, Z. Videobooth: Diffusion-based video generation with image prompts. In CVPR, pp. 66896700, 2024. Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D., and Zhao, H. Anydoor: Zero-shot object-level image customization. CoRR, abs/2307.09481, 2023. Kingma, D. and Gao, R. Understanding diffusion objectives as the elbo with simple data augmentation. In NeurIPS, volume 36, 2024. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. In ICCV, pp. 40154026, 2023. Li, D., Li, J., and Hoi, S. C. H. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. In NeurIPS, 2023. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. An image is worth one word: Personalizing text-to-image generation using textual inversion. In ICLR, 2023. Liu, M., Ma, Y., Zhen, Y., Dan, J., Yu, Y., Zhao, Z., Hu, Z., Liu, B., and Fan, C. Llm4gen: Leveraging semantic representation of llms for text-to-image generation. In AAAI, 2024. Guo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D., and Dai, B. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR, 2024. Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 9 CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers Ma, J., Liang, J., Chen, C., and Lu, H. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In ACM SIGGRAPH, pp. 112, 2024. Patel, M., Jung, S., Baral, C., and Yang, Y. lambdaeclipse: Multi-concept personalized text-to-image diffusion models by leveraging clip latent space. arXiv preprint arXiv:2402.05195, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, pp. 41724182, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. SDXL: improving latent diffusion models for high-resolution image synthesis. CoRR, abs/2307.01952, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In ICML, pp. 87488763, 2021. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, pp. 1067410685, 2022. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pp. 2250022510, 2023. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Sun, W., Cui, B., Tang, J., and Dong, X.-M. Attentive eraser: Unleashing diffusion models object removal potential via self-attention redirection guidance. arXiv preprint arXiv:2412.12974, 2024. Tan, Z., Liu, S., Yang, X., Xue, Q., and Wang, X. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. van den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In NeurIPS, pp. 6306 6315, 2017. Wang, X., Fu, S., Huang, Q., He, W., and Jiang, H. Msdiffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv, 2024. Wei, Y., Zhang, S., Qing, Z., Yuan, H., Liu, Z., Liu, Y., Zhang, Y., Zhou, J., and Shan, H. Dreamvideo: Composing your dream videos with customized subject and motion. In CVPR, pp. 65376549, 2024a. 10 Wei, Y., Zhang, S., Yuan, H., Wang, X., Qiu, H., Zhao, R., Feng, Y., Liu, F., Huang, Z., Ye, J., et al. Dreamvideo2: Zero-shot subject-driven video customization with precise motion control. arXiv preprint arXiv:2410.13830, 2024b. Wu, J. Z., Ge, Y., Wang, X., Lei, W., Gu, Y., Hsu, W., Shan, Y., Qie, X., and Shou, M. Z. Tune-a-video: Oneshot tuning of image diffusion models for text-to-video generation. In CVPR, 2023. Wu, T., Zhang, Y., Cun, X., Qi, Z., Pu, J., Dou, H., Zheng, G., Shan, Y., and Li, X. Videomaker: Zero-shot customized video generation with the inherent force of video diffusion models. arXiv preprint arXiv:2412.19645, 2024a. Wu, T., Zhang, Y., Wang, X., Zhou, X., Zheng, G., Qi, Z., Shan, Y., and Li, X. Customcrafter: Customized video generation with preserving motion and concept composition abilities. arXiv preprint arXiv:2408.13239, 2024b. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Ye, H., Zhang, J., Liu, S., Han, X., and Yang, W. Ip-adapter: Text compatible image prompt adapter for text-to-image arXiv preprint arXiv:2308.06721, diffusion models. 2023. Ying, J., Liu, M., Wu, Z., Zhang, R., Yu, Z., Fu, S., Cao, S.- Y., Wu, C., Yu, Y., and Shen, H.-L. Restorerid: Towards tuning-free face restoration with id preservation. arXiv preprint arXiv:2411.14125, 2024. Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Birodkar, V., Gupta, A., Gu, X., et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. Zhang, Y., Song, Y., Liu, J., Wang, R., Yu, J., Tang, H., Li, H., Tang, X., Hu, Y., Pan, H., et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In CVPR, pp. 80698078, 2024. Zhao, Z., Tang, J., Wu, B., Lin, C., Wei, S., Liu, H., Tan, X., Zhang, Z., Huang, C., and Xie, Y. Harmonizing visual text comprehension and generation. arXiv preprint arXiv:2407.16364, 2024. CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers A. Data Details Figure 8 illustrates the pipeline for constructing our training data. In the first stage, we perform high-quality video data filtering. Firstly, we exclude videos with resolution below 1080P to ensure high-definition video quality and clear subjects. Next, we apply video aesthetic filter, removing videos with aesthetic scores below 5.3 to maintain aesthetic standards. Subsequently, we eliminate static or low-motion videos by evaluating motion scores and temporal consistency, discarding videos with poor temporal coherence. In the second stage, for the filtered high-quality video data, we use Qwen2.5 (Hui et al., 2024) to extract the main entity from the video captions and assess the complexity of the video backgrounds based on these captions. This allows us to further remove data with redundant or complex backgrounds, ensuring that the videos contain limited number of clear subjects with simple backgrounds. Finally, based on the extracted main entity, we utilize Grounding SAM to segment the first frame of the video, extract the corresponding objects, and create training pairs. B. Implementation Details of CustomVideoX With LoRA rank of 128, CustomVideoX introduces 132.12M trainable parameters, accounting for only 2.3% of the total parameters of the model. In contrast, VideoBooth fine-tunes certain model parameters, resulting in 97.63M trainable parameters, which represent 8.8% of the total. Additionally, because VideoBooth modifies the base model, it inevitably leads to reduction in video generation quality. C. More Experimental Results More results can be found in Figure 9 and Figure 10. 11 CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers Figure 8: The overview of video customization data collection pipeline. When dealing with complex scenarios that contain concepts with Qwen2.5 and Grounding SAM models. Our data pipeline could still extract precise video quality via video resolution, aesthetic score, motion score, and temporal consistency. Figure 9: Additional results of subject personalization for video generation on diverse scenarios (1/2). 12 CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers Figure 10: Additional results of subject personalization for video generation on diverse scenarios (2/2)."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology (Guangzhou)",
        "University of Science and Technology of China",
        "Zhejiang Univerisity"
    ]
}