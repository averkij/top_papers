{
    "paper_title": "Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling",
    "authors": [
        "Rishiraj Acharya"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Transformer architecture, underpinned by the self-attention mechanism, has become the de facto standard for sequence modeling tasks. However, its core computational primitive scales quadratically with sequence length (O(N^2)), creating a significant bottleneck for processing long contexts. In this paper, we propose the Gated Associative Memory (GAM) network, a novel, fully parallel architecture for sequence modeling that exhibits linear complexity (O(N)) with respect to sequence length. The GAM block replaces the self-attention layer with two parallel pathways: a causal convolution to efficiently capture local, position-dependent context, and a parallel associative memory retrieval mechanism to model global, content-based patterns. These pathways are dynamically fused using a gating mechanism, allowing the model to flexibly combine local and global information for each token. We implement GAM from scratch and conduct a rigorous comparative analysis against a standard Transformer model and a modern linear-time baseline (Mamba) on the WikiText-2 benchmark, as well as against the Transformer on the TinyStories dataset. Our experiments demonstrate that GAM is consistently faster, outperforming both baselines on training speed, and achieves a superior or competitive final validation perplexity across all datasets, establishing it as a promising and efficient alternative for sequence modeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 5 0 6 0 0 . 9 0 5 2 : r Gated Associative Memory: Parallel O(N) Architecture for Efficient Sequence Modeling Rishiraj Acharya 1Independent Researcher heyrishiraj@gmail.com Abstract The Transformer architecture, underpinned by the self-attention mechanism, has become the de facto standard for sequence modeling tasks. However, its core computational primitive scales quadratically with sequence length (O(N 2)), creating significant bottleneck for processing long contexts. In this paper, we propose the Gated Associative Memory (GAM) network, novel, fully parallel architecture for sequence modeling that exhibits linear complexity (O(N )) with respect to sequence length. The GAM block replaces the self-attention layer with two parallel pathways: causal convolution to efficiently capture local, position-dependent context, and parallel associative memory retrieval mechanism to model global, content-based patterns. These pathways are dynamically fused using gating mechanism, allowing the model to flexibly combine local and global information for each token. We implement GAM from scratch and conduct rigorous comparative analysis against standard Transformer model and modern linear-time baseline (Mamba) on the WikiText-2 benchmark, as well as against the Transformer on the TinyStories dataset. Our experiments demonstrate that GAM is consistently faster, outperforming both baselines on training speed, and achieves superior or competitive final validation perplexity across all datasets, establishing it as promising and efficient alternative for sequence modeling."
        },
        {
            "title": "Introduction",
            "content": "Since its introduction, the Transformer [11] has revolutionized the field of natural language processing. Its central innovation, the self-attention mechanism, allows for rich, pairwise interactions between all tokens in sequence, capturing complex dependencies regardless of their distance. This capability has led to state-of-the-art results across vast array of tasks. However, this expressive power comes at steep computational cost. The self-attention mechanism requires dot-product between Query and Key matrices of size (N, d), resulting in an attention map of size (N, N), where is the sequence length and is the model dimension. This leads to computational and memory complexity of O(N 2d), which is prohibitive for applications involving very long sequences, such as high-resolution document summarization, genomic data analysis, or processing lengthy video streams. This quadratic bottleneck has spurred wave of research into Efficient Transformers [10], which aim to approximate the self-attention matrix using methods like sparsity [1], low-rank factorization [12], or kernelization [2]. While successful, these methods often introduce architectural complexity or trade-offs in expressivity. Another line of work has revisited recurrent neural networks (RNNs) (e.g., LSTMs, GRUs), which are naturally O(N ) [6]. However, their inherently sequential nature makes them difficult to parallelize on modern hardware, often leading to slower training times despite their theoretical efficiency. More recent architectures like State Space Models (SSMs) have shown great promise in achieving linear-time performance. 1 Models like Mamba [4], in particular, have demonstrated strong performance by using selection mechanism and hardware-aware parallel scan algorithm. While these models are highly effective, they reintroduce form of recurrence into their design. In this work, we address these challenges by introducing the Gated Associative Memory (GAM) network. GAM is designed from the ground up to satisfy two critical criteria: (1) linear computational complexity and (2) maximum parallelizability on modern accelerators by avoiding recurrence entirely. It replaces the self-attention block with novel GAMBlock that combines two complementary context-modeling pathways: 1. Local Context Pathway: 1D causal convolution efficiently captures local syntactic and positional relationships. 2. Global Context Pathway: parallel associative memory retrieves global, contentbased patterns from learned memory bank for all tokens simultaneously. These pathways are fused with learnable gating mechanism, enabling the network to dynamically allocate resources to local or global context as needed. Our contributions are: We propose the Gated Associative Memory (GAM) architecture, novel O(N ) sequence model that is fully parallelizable and non-recurrent. We provide complete implementation and empirically demonstrate that GAM consistently trains faster than both comparable Transformer and Mamba on standard GPU. We show through experiments on the WikiText-2 and TinyStories datasets that GAM achieves better perplexity than well-trained Transformer baseline and the Mamba baseline, highlighting its effectiveness and generalizability."
        },
        {
            "title": "2 Related Work",
            "content": "The quest for efficient sequence modeling beyond the standard Transformer has been vibrant area of research. Our work is situated within this landscape and draws inspiration from several key ideas. Efficient Transformers large body of work, surveyed by Tay et al. [10], focuses on approximating the dense attention matrix. These methods can be broadly categorized: Sparsity-based Methods: Models like Longformer [1] use combination of local windowed attention and sparse global attention to reduce computation, allowing them to process thousands of tokens. Low-Rank Methods: Linformer [12] is based on the observation that the self-attention matrix is often low-rank and can be approximated by projecting the Key and Value matrices to lower-dimensional space, reducing complexity from O(N 2) to O(N ). Kernel-based Methods: Performers [2] use random feature maps to approximate the softmax kernel, enabling linear-time attention mechanism without direct computation of the matrix. While effective, these approaches primarily modify the self-attention mechanism itself. GAM, in contrast, replaces it entirely with different inductive bias. 2 Recurrent Models and State Space Models (SSMs) Before Transformers, RNNs, and particularly LSTMs [6], were the dominant paradigm for sequence modeling. Their O(N ) complexity is key advantage, but their sequential nature limits training parallelism. Recently, there has been resurgence of interest in models that combine recurrence with modern hardwareaware designs. Structured State Space Models (S4) [5] and Mamba [4] are prominent examples that achieve linear-time scaling and strong performance by drawing on principles from classical state-space theory. Mamba, in particular, introduces selective SSM that allows the model to dynamically focus on or ignore inputs based on context, implemented via highly optimized, hardware-aware scan operation. While Mamba achieves impressive performance through this recurrent scan, GAM pursues different path to efficiency by avoiding recurrence altogether, relying instead on fully parallel primitives (convolution and matrix multiplication) that are inherently well-suited to modern accelerators. Convolutional Sequence Models Using convolutions for sequence tasks is not new. Models like TCNs (Temporal Convolutional Networks) have shown that causal convolutions can be very effective at capturing historical context. GAM incorporates this idea in its local pathway, using it as specialized and efficient mechanism for positional and syntactic information. GAMs novelty lies in its explicit and parallel decomposition of context into local (convolutional) and global (associative memory) streams, which are then dynamically combined with learned gate. This hybrid approach avoids the approximations of many efficient Transformers and the sequential bottlenecks of RNNs, offering distinct and effective design point in the space of efficient sequence architectures."
        },
        {
            "title": "3 The Gated Associative Memory (GAM) Architecture",
            "content": "The GAM model is stack of identical blocks, similar to Transformer. It processes an input sequence of token embeddings and produces sequence of contextualized output vectors. The core innovation lies within the GAMBlock, which replaces the Multi-Head Self-Attention sublayer. The overall model follows standard structure with token and positional embeddings, stack of GAM blocks, and final layer normalization and language model head."
        },
        {
            "title": "3.1 The GAM Block",
            "content": "The GAM block is designed for maximal parallelism and linear-time computation. As shown in Figure 1, an input is first normalized. It then branches into parallel pathways to compute local and global context, which are fused via learned gating mechanism. This is followed by residual connection and standard feed-forward network (FFN) sub-layer."
        },
        {
            "title": "3.2 Local Context Pathway: Causal Convolution",
            "content": "To capture local structure, such as n-gram relationships and word order, we employ 1D causal convolution. convolution with kernel size allows token to gather information from its 1 predecessors. To ensure causality (i.e., information only flows from past to future), we apply asymmetric padding of 1 to the beginning of the sequence. For an input sequence of shape (B, N, d), where is batch size, is sequence length, and is the model dimension, the operation is: 1. Permute: The input is reshaped to (B, d, N) to match the Conv1d expectation. 2. Pad: Asymmetric padding of (k 1) is applied to the time dimension. 3. Convolve: The convolution is applied. 3 Figure 1: The GAM Block. The input first passes through Layer Normalization. It then splits into three branches. The first branch is residual connection. The second branch computes local and global context in parallel. The local context is generated by Causal 1D Convolution. The global context is generated by querying learnable Memory Bank. The outputs of these two pathways are combined via learned gate. The gated output is added to the residual connection. This is followed by another Layer Normalization and standard Feed-Forward Network, also with residual connection. 4. Trim & Permute: The output is trimmed to the original length and permuted back to (B, N, d). This operation is highly parallelizable and has complexity of O(N d), which is linear in sequence length . We use depthwise convolution (groups=d), where each feature channel is processed independently, to further improve efficiency and reduce parameters."
        },
        {
            "title": "3.3 Global Context Pathway: Parallel Associative Memory",
            "content": "This pathway is designed to model long-range, content-based dependencies, replacing the quadratic It consists of learnable Memory Bank , matrix of size self-attention mechanism. (num slots, model), initialized with Xavier uniform initialization. Each row of can be interpreted as learnable prototypical contextual pattern that is learned from the data. For an input sequence of shape (B, N, d), the retrieval process is performed for all tokens in parallel: 1. Similarity Scoring: The model computes the dot-product similarity between each of the token representations and every one of the num slots in the memory bank. This is single matrix multiplication: Scores = XM (1) The resulting Scores tensor has the shape (B, N, num slots). 2. Soft Retrieval: softmax function is applied over the num slots dimension for each token. This yields set of attention-like weights, indicating which prototypical patterns in are most relevant for each token. Weights = softmax(Scores) (2) The Weights tensor has the shape (B, N, num slots). 3. Context Aggregation: The final global context is weighted average of the memory bank vectors, computed via single matrix multiplication between the retrieval weights and the memory bank itself: GlobalContext = Weights (3) The resulting GlobalContext tensor has the shape (B, N, d). 4 This entire process involves only matrix multiplications with fixed-size matrices (M ), making its complexity O(N num slots d). Since num slots and are fixed hyperparameters, the complexity is linear with respect to the sequence length N."
        },
        {
            "title": "3.4 Gating and Fusion",
            "content": "The local and global context pathways offer complementary views of the sequence. The causal convolution is an expert on local syntax and strict ordering, while the associative memory is an expert on high-level, content-based patterns that are position-agnostic. To combine them effectively, we use dynamic gating mechanism. For each tokens input representation x, gate vector is computed using single linear layer, which is then split into two halves, glocal and gglobal. These gates modulate the flow of information from each pathway: = Linear(x) glocal, gglobal = chunk(g, 2, dim = 1) FusedContext = σ(glocal) LocalContext + σ(gglobal) GlobalContext (4) (5) (6) The sigmoid function σ squashes the gate values to the range (0, 1), acting as soft switch. This allows the model to learn, on per-token basis, whether to prioritize local syntactic cues (e.g., for function words) or global semantic information (e.g., for content words). This FusedContext is then added to the original input via the residual connection and passed to the FFN."
        },
        {
            "title": "4 Experimental Setup",
            "content": "We conducted experiments to compare our proposed GAM model against standard Transformer baseline and Mamba baseline on causal language modeling tasks."
        },
        {
            "title": "4.1 Datasets and Preprocessing",
            "content": "WikiText-2: We used the wikitext-2-raw-v1 version of the dataset [8], standard benchmark for language modeling consisting of high-quality Wikipedia articles. TinyStories: We also used the roneneldan/TinyStories dataset [3], large corpus of short stories generated by GPT-3.5/4 that use vocabulary typical of 3-4 year old. This dataset is designed to test models ability to learn fundamental language structures and reasoning in constrained setting. For each dataset, we trained Byte-Pair Encoding (BPE) tokenizer from scratch on its respective training corpus. The final vocabulary size was set to 10,000 tokens. All documents were concatenated and then split into fixed-length chunks of 256 tokens to form the input sequences."
        },
        {
            "title": "4.2 Models",
            "content": "To ensure fair comparison, all models were built with similar scale and hyperparameters. GAM (ours): 6-layer GAM model with an embedding dimension dmodel=512. The associative memory contained num slots=512, and the causal convolution used kernel size of = 3. dropout rate of 0.1 was applied. This resulted in total of 22.6 million trainable parameters. Transformer (baseline): 6-layer GPT-style decoder-only Transformer with dmodel=512 and nhead=8, resulting in head dimension of 64. dropout rate of 0.1 was used throughout the model. This resulted in total of 24.2 million trainable parameters. 5 Mamba (baseline): 6-layer Mamba model with dmodel=512. We used the official implementation with standard parameters: state space dimension dstate=16, convolution kernel dconv=4, and expansion factor expand=3. dropout rate of 0.1 was applied. This resulted in total of 20.5 million trainable parameters. Mamba was evaluated on the WikiText-2 benchmark to compare GAM against another prominent O(N) architecture."
        },
        {
            "title": "4.3 Training Details",
            "content": "All models were trained from scratch for 5 epochs on each dataset using single NVIDIA T4 GPU. We used the AdamW optimizer [7] with learning rate of 3 104, betas of (0.9, 0.95), and weight decay of 0.1. learning rate scheduler with linear warmup of 100 steps followed by cosine decay was used for all models to ensure stable training. The batch size was set to 32. Gradient clipping was applied at norm of 1.0."
        },
        {
            "title": "4.4 Evaluation Metrics",
            "content": "We evaluated the models on two primary axes: 1. Accuracy: Measured by Perplexity (PPL) on the validation set, calculated as exp(cross entropy loss). Lower perplexity indicates better language model. 2. Efficiency: Measured by the average wall-clock time per training epoch in seconds."
        },
        {
            "title": "5 Results and Analysis",
            "content": "The results of our comparative experiments are summarized in Table 1. Across two diverse datasets, the GAM architecture consistently demonstrates advantages in both training speed and modeling performance, outperforming both the quadratic Transformer and the linear Mamba baseline on WikiText-2. Table 1: Comparison of GAM, Transformer, and Mamba. GAM is the fastest and achieves the lowest (better) perplexity on WikiText-2. It also outperforms the Transformer on TinyStories. Dataset Model Params Avg. Time / Epoch (s) Val. Loss Val. PPL WikiText-2 Transformer Mamba GAM (Ours) TinyStories Transformer GAM (Ours) 24.2 20.5 22.6 24.2 22.6 131.9 127.1 117.2 671.6 601.4 6.8233 6.9251 6.7828 3.1591 3.1418 918.99 1017.54 882. 23.55 23."
        },
        {
            "title": "5.1 Efficiency Analysis",
            "content": "The GAM model demonstrated significant and consistent efficiency advantage. On WikiText-2, GAMs average epoch time of 117.2s was the fastest, outperforming Mamba (127.1s) by 7.8% and the Transformer (131.9s) by 11.1%. This highlights the efficiency of GAMs fully parallel, non-recurrent design, which avoids the scan operations inherent to SSMs, even when those operations are highly optimized. On TinyStories, much larger dataset, GAM maintained 10.5% speed advantage over the Transformer (601.4s vs. 671.6s per epoch). 6 These results empirically validate our architectural design goals. By replacing the O(N 2) selfattention with fully parallelizable O(N ) operations (causal convolution and associative memory retrieval), GAM better utilizes the parallel processing capabilities of modern GPUs. This leads to direct and consistent improvement in training throughput, even at moderate sequence length of 256."
        },
        {
            "title": "5.2 Scaling Benchmark",
            "content": "While the full training runs demonstrate GAMs efficiency at fixed sequence length of 256, the theoretical O(N ) advantage becomes most apparent as the sequence length increases. To empirically validate this, we conducted targeted scaling benchmark that isolates the computational and memory costs of single GAM block versus single Transformer block. In this benchmark, we measured the average forward-and-backward pass time and the peak allocated GPU memory for both blocks. We used fixed batch size of 16 and an embedding dimension of 512, while systematically increasing the sequence length from 256 to 8192. The results, shown in Table 2 and visualized in Figure 2, unequivocally demonstrate the practical implications of their differing complexities. Table 2: Scaling benchmark results for single block. Time is the average for forward and backward pass. Memory is the peak GPU memory allocated. The Transformer fails due to an Out-of-Memory (OOM) error at longer sequences, while GAM scales linearly. Sequence Length GAM Transformer GAM Transformer Time (ms) Peak Memory (MB) 256 512 1024 2048 4096 8192 8.90 8.97 23.86 13.09 74.19 25.86 51.94 279.37 105.03 Failed (OOM) 217.30 Failed (OOM) 216.03 179.42 552.98 325.48 1964.79 617.60 1201.85 7483.92 2370.35 Failed (OOM) 4707.35 Failed (OOM) Figure 2: GAM vs. Transformer Scaling Comparison. (Left) Average forward-backward pass time vs. sequence length. (Right) Peak GPU memory usage vs. sequence length. Both axes are on logarithmic scale. The Transformers quadratic growth is evident in the steep upward curve, while GAM exhibits clear linear scaling. The Transformer fails due to out-ofmemory errors beyond sequence length of 2048 in this setup. At short sequence length of 256, the models have nearly identical runtimes, as fixedcost operations (like the feed-forward network) dominate. However, the performance rapidly diverges. As the sequence length doubles, the Transformers runtime and memory usage roughly quadruple, clear sign of its O(N 2) complexity. In stark contrast, GAMs resource consumption approximately doubles, consistent with its O(N ) design."
        },
        {
            "title": "5.3 Performance Analysis",
            "content": "In terms of accuracy, the GAM model achieved superior performance on both datasets. On WikiText-2, GAM achieved final validation perplexity of 882.57, clearly outperforming both the Transformer baseline (918.99) and the Mamba baseline (1017.54). This shows that our simplified, parallel architecture does not sacrifice modeling capability, and can in fact be more effective than other strong baselines. On TinyStories, GAM also achieved better final perplexity of 23.15 compared to the Transformers 23.55. This demonstrates that GAMs architectural priors are also effective on text with simpler syntax and focus on narrative coherence. The validation learning curves, shown in Figures 3 and 4, illustrate that GAM not only reaches better final score but often maintains performance advantage throughout the training process. This strong performance across two textually different domains suggests that the explicit decomposition of context modeling is robust strategy. The causal convolution provides stable, hard-coded mechanism for local syntax, while the associative memory can focus entirely on learning and retrieving high-level semantic patterns. This clear division of labor proves to be highly effective. Figure 3: Training dynamics on the WikiText-2 dataset. (a) Validation perplexity, (b) validation loss, and (c) wall-clock time per epoch. The plots show GAM (in blue) achieving lower final perplexity and consistently faster epoch times compared to both the Transformer (in red) and Mamba (in green) baselines. Figure 4: Training dynamics on the TinyStories dataset. (a) Validation perplexity, (b) validation loss, and (c) wall-clock time per epoch. GAM (in green) demonstrates faster learning trajectory and maintains significant efficiency advantage throughout the 5 epochs compared to the Transformer (in violet)."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "To dissect the GAM architecture and validate the contribution of its core components, we conducted series of ablation studies on the WikiText-2 dataset. We evaluated four configurations of the GAM model: 1. GAM (Full): The complete proposed model with both local (convolution) and global (associative memory) pathways, fused by the learned gating mechanism. 2. GAM (Sum Fusion): model with both pathways, but with the gating mechanism removed. The outputs are combined via simple element-wise addition. 3. GAM (Global Only): model that uses only the parallel associative memory pathway, removing the causal convolution entirely. 4. GAM (Local Only): model that uses only the causal convolution pathway, removing the associative memory. This is analogous to Temporal Convolutional Network (TCN). The results, summarized in Table 3, clearly demonstrate the distinct roles and the combined power of each component. Table 3: Ablation study of the GAM architecture on WikiText-2. All componentsthe local pathway, the global pathway, and particularly the gating mechanismare shown to be essential for achieving the best performance. PPL is the best validation perplexity achieved during training. Model Configuration Gating? Local? Global? Params Val. PPL GAM (Full) GAM (Global Only) GAM (Sum Fusion) GAM (Local Only) 22.6 900.84 19.4 19.4 17.9 905.45 942.59 944. The full GAM model achieves the lowest perplexity, confirming the effectiveness of the complete architecture. The analysis of the ablated models provides several key insights: First, the gating mechanism is crucial for effective fusion. By simply removing the gate and using summation (GAM (Sum Fusion)), the perplexity degrades significantly from 900.84 to 942.59. This indicates that static, unweighted combination of local and global contexts is suboptimal. The learned gate provides necessary, dynamic control mechanism that allows the model to intelligently allocate resources between the two information streams on per-token basis. Second, the global associative memory is the primary driver of performance. The GAM (Global Only) model performs remarkably well, achieving perplexity of 905.45, very close to the full model. This suggests that for general language modeling task like WikiText-2, capturing long-range, content-based semantic patterns is the most critical factor. However, the fact that the full model still performs better demonstrates that the local context provided by the convolution, while not sufficient on its own, adds indispensable and complementary information about word order and syntax. Finally, local context alone is insufficient. The GAM (Local Only) model performs the worst, with perplexity of 944.70. This confirms that while convolutions can effectively model local structure, they fail to capture the long-distance dependencies required for robust language understanding. In conclusion, this ablation study validates our core architectural hypothesis. The GAM models strength lies not in any single component, but in the synergistic combination of 9 powerful global memory pathway and refined local syntax pathway, which are effectively integrated by dynamic gating mechanism."
        },
        {
            "title": "5.5 Discussion",
            "content": "The results strongly suggest that the quadratic complexity of self-attention is not strictly necessary to achieve high performance in language modeling. Our GAM architecture provides compelling alternative that is both computationally cheaper and more performant on the benchmarks tested. The consistency of these results across complex, factual dataset (WikiText-2) and simple, narrative dataset (TinyStories) highlights the generalizability of GAMs design. The fact that it outperforms not only the Transformer but also the efficient Mamba baseline on WikiText-2 underscores the effectiveness of its fully parallel, non-recurrent inductive biases."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "In this paper, we introduced the Gated Associative Memory (GAM) network, novel O(N ) architecture for sequence modeling. By combining causal convolution for local context with parallel associative memory for global patterns, GAM achieves significant and consistent gains in computational efficiency while simultaneously improving modeling accuracy over both standard Transformer baseline and strong linear-time competitor, Mamba. Our experiments on WikiText-2 and TinyStories confirm that GAM is faster and obtains better perplexity scores. This work opens several avenues for future research. The most immediate next step is to evaluate GAM on tasks with much longer sequences, such as those found in the Long Range Arena benchmark [9], where its linear complexity advantage should become even more pronounced. Furthermore, scaling up the model size and training on larger datasets will be crucial to determine its performance ceiling against state-of-the-art models. Finally, the learned memory bank in the GAM model is rich source of information; analyzing the contextual patterns captured by the memory slots could provide valuable insights into how language models represent knowledge."
        },
        {
            "title": "References",
            "content": "[1] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020. URL https://arxiv.org/abs/2004.05150. [2] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. URL https://arxiv.org/abs/2009.14794. [3] Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english?, 2023. URL https://arxiv.org/abs/2305.07759. [4] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https://arxiv.org/abs/2312.00752. [5] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces, 2022. URL https://arxiv.org/abs/2111.00396. [6] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):17351780, 1997. doi: 10.1162/neco.1997.9.8.1735. [7] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https://arxiv.org/abs/1711.05101. [8] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. URL https://arxiv.org/abs/1609.07843. [9] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: benchmark for efficient transformers, 2020. URL https://arxiv.org/abs/2011.04006. [10] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: survey, 2022. URL https://arxiv.org/abs/2009.06732. [11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https: //arxiv.org/abs/1706.03762. [12] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Selfattention with linear complexity, 2020. URL https://arxiv.org/abs/2006.04768."
        }
    ],
    "affiliations": [
        "Independent Researcher"
    ]
}