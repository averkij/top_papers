{
    "paper_title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text",
    "authors": [
        "Nikhil Kandpal",
        "Brian Lester",
        "Colin Raffel",
        "Sebastian Majstorovic",
        "Stella Biderman",
        "Baber Abbasi",
        "Luca Soldaini",
        "Enrico Shippole",
        "A. Feder Cooper",
        "Aviya Skowron",
        "John Kirchenbauer",
        "Shayne Longpre",
        "Lintang Sutawika",
        "Alon Albalak",
        "Zhenlin Xu",
        "Guilherme Penedo",
        "Loubna Ben Allal",
        "Elie Bakouch",
        "John David Pressman",
        "Honglu Fan",
        "Dashiell Stander",
        "Guangyu Song",
        "Aaron Gokaslan",
        "Tom Goldstein",
        "Brian R. Bartoldson",
        "Bhavya Kailkhura",
        "Tyler Murray"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 9 0 2 5 0 . 6 0 5 2 : r The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text Nikhil Kandpal1,2 Brian Lester1,2 Colin Raffel1,2,3 Sebastian Majstorovic4 Stella Biderman4 Baber Abbasi4 Luca Soldaini5 Enrico Shippole6 A. Feder Cooper7 Aviya Skowron4 John Kirchenbauer8 Shayne Longpre9 Lintang Sutawika4,10 Alon Albalak11 Zhenlin Xu12 Guilherme Penedo3 Loubna Ben Allal3 Elie John David Pressman4 Honglu Fan4,13 Dashiell Stander4 Guangyu Song4 Aaron Bakouch3 Gokaslan7 Tom Goldstein8 Brian R. Bartoldson14 Bhavya Kailkhura14 Tyler Murray5 1University of Toronto Artificial Intelligence 9MIT 10CMU 11Lila Sciences 2Vector Institute 6Teraflop AI 3Hugging Face 7Cornell University 12Independent 13poolside 4EleutherAI 5The Allen Institute for 8University of Maryland, College Park 14Lawrence Livermore National"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models."
        },
        {
            "title": "Introduction",
            "content": "A critical stage of large language model (LLM) development is pretraining [72, 136, 142], where an LLM is trained to predict the next token (i.e., word or subword unit) in corpus of unstructured text. Pretraining is widely regarded as the foundation for strong downstream performance, as it enables LLMs to learn the structure of natural language [32, 110, 154] and accumulate broad base of world knowledge [133, 152]. In an effort to push the capabilities of LLMs, pre-training datasets have grown steadily over time [143], with modern datasets containing trillions of tokens [132, 167, 193]. To meet this increasing demand for pre-training data, the de facto approach has been to leverage the public Internet as source of text [57, 95, 108, 132, 142]. While the web provides diverse and continuously growing supply of text, much of this content under most legal frameworksis protected by copyright. Yet, this text is routinely used to pretrain Equal contribution. For list of author contributions, see Appendix A. Work done while graduate student at Cornell University. Work done while at SynthLabs. Preprint. Figure 1: The Common Pile is an 8TB dataset of openly licensed text curated from 30 diverse sources. The sources comprising the Common Pile are shown above, categorized by textual domain. LLMs, often without compensation to the creators of this content. Recent estimates suggest that compensating the authors of pre-training data, even at conservatively low wage rates, would cost billions of US dollars [82]. While copyright exemptions for text and data mining exist in some jurisdictions [69, 79, 92, 130, 156], many rights holders have objected to the uncompensated use of their work, resulting in numerous lawsuits against LLM developers [24, 191] that could carry financial damages in the billions [40, 96, 159]. Beyond questions of intellectual property (IP) law, the use of web-scraped data also raises ethical concerns [9], as content creators rarely explicitly consent to the downstream use of their work for LLM training. In fact, recent evidence suggests that many content owners may not consent to its use as LLM training data, as shown by sharp mid-2023 increase in websites blocking AI crawlers [107], following growing awareness of web data being used to train models. Finally, while open models trained on publicly released pre-training datasets [18, 64, 103] support research into the study of learning dynamics [50, 76, 84], memorization [17, 22], data auditing [47, 128, 145], and more, the use of unlicensed training data heavily limits the ability of model trainers to share their datasets, and has previously resulted in DMCA takedowns of datasets such as the Pile [57]. The current landscape reflects growing divide between LLM developers and content creators. We submit that natural first step toward resolving this tension is to ask: Is it possible to train performant language models using only public domain and openly licensed text? We define openly licensed text as content that follows the Open Knowledge Foundations Open Definition 2.1 (further detailed in section 2 and Appendix C), which refers to content where the copyright holder has granted explicit permission for the content to be freely accessed, used, modified, and shared for any purpose. Our primary contribution in this paper is to demonstrate that this is indeed possible by collecting, curating, and releasing the Common Pile v0.1, an 8TB dataset thatto our knowledgeconstitutes the largest collection of openly licensed text to date. The Common Pile comprises 30 text sources (detailed in section 3), covering diverse domains including research publications, open-source code, government documents, historical books, educational resources, audio transcripts, and more. Crucially, we demonstrate that after appropriate filtering, deduplication, and reweighting, the Common Pile v0.1 can be used as the foundation for competitive LLMs. Specifically, we train Comma v0.1-1T and Comma v0.1-2T, pair of 7-billion-parameter models with comparable performance to budgetmatched models trained on unlicensed datasets such as Llama 1 and 2 7B. In the spirit of openness and transparency, we release the Common Pile v0.1, both Comma v0.1 models and their filtered and deduplicated pre-training dataset, and all data collection and processing code."
        },
        {
            "title": "2 What do we mean by “openly licensed”?",
            "content": "Copyright law grants content creators certain rights, such as exclusive rights (with certain exceptions) to reproduce, distribute, and create derivatives of their original works. Although copyright laws vary across jurisdictions, original, creative works (that are fixed in tangible medium, such as physically or digitally [see, e.g., 1]) typically fall within the scope of copyright. Works in the public domain [38] have had their copyrights expire (after legally dictated time period), were never eligible for copyright protection due to specific carve-outs (e.g., government documents in the U.S. [2]), or were otherwise dedicated to the public domain by their copyright owners (e.g., with CC0 license [35]). Copyright 2 owners can license their protected works, allowing others to adapt and reuse them under specified terms. For example, Creative Commons (CC) Licenses (except CC0) grant the right to reproduce and Share the Licensed Material, in whole or in part; and produce, reproduce, and Share Adapted Material [36]. For more in-depth and accessible discussion about licenses and generative AI, see Lee et al. [96, Parts II.III.J]. For the Common Pile, we collect and curate public domain and openly licensed text, where we consider openly licensed to mean any license that meets the Open Knowledge Foundations Open Definition 2.1. Some prominent examples of licenses that are considered to be open under this definition include CC BY [37], CC BY-SA [39], and software licenses certified by the Blue Oak Council (e.g., the MIT license) [20]. We note that CC NC (non-commercial) and ND (no derivatives) licenses are not considered open under this definition and we therefore do not include content distributed under these licenses. While the use of an open license does not necessarily imply that the rights holder has specifically contemplated use of their content to train LLMs, most open licenses include text like the above rights may be exercised in all media and formats whether now known or hereafter devised [37]. Overall, we consider our use of openly licensed data to be substantial first step towards ethical pre-training dataset curation."
        },
        {
            "title": "2.1 License due diligence",
            "content": "License laundering There is large quantity of data on the internet with incorrect, ambiguous, or missing licensing metadata [96, 106]. common pitfall is license laundering, where copyrighted work is redistributed (typically by non-rights holder) with an incorrect license. License laundering can undermine our ability to confidently source openly licensed content since it implies that we cannot always trust the license distributed with piece of content. To address this issue, we set strict standards for data sourcing, only including data from sources where we were confident that the licensing information was provided by the copyright holder, which ultimately led us to exclude certain sources such as OpenAlex [80, 126], YouTube Commons [74], and the Hacker News dataset on Kaggle. Use of collection licenses related issue is the licensing status of compilations of existing works. Many training corpora are released under open licenses, but these licenses do not necessarily align with the licensing status of the underlying documents [96, Part II.A]. As an example, the ODC-By license has been commonly used for large-scale web corpora such as Dolma [167], FineWeb [132], and TxT360 [173]. ODC-By, by definition, does not extend to individual documents within the corpus; therefore, the copyright of documents in these collections is still controlled by the document authors, and does not imply that the text itself is openly licensed. LLM-generated synthetic datasets Datasets containing text generated by LLMs trained on unlicensed data have been released under open licenses [e.g. 195]. It has not yet been established whether it is permissible to apply arbitrary licenses to the generations of an LLM that was trained on unlicensed data [96]. We therefore take conservative stance and avoid synthetic content that was generated by an LLM. Caveats Despite our best efforts at due diligence, data that falls outside of our curatorial principles and choices may have still ended up in our dataset. License laundering is notoriously hard problem to identify exhaustively in practice [96]. Copyright owners may also change the license they associate with their content. Since we collected and curated the Common Pile v0.1 in late 2024, the licensing information we include and rely on may not be completely aligned with more recent updates. Further, some documents that we collect that are in the public domain or are openly licensed may contain material with unclear status (e.g., quoted snippets of in-copyright books in public domain U.S. government publications). Finally, we note that while it is relatively straightforward to obey attribution requirements when redistributing data, attributing model predictions back to the training data points that influenced them remains an active area of research [129, 28]."
        },
        {
            "title": "2.2 Comparisons with related work",
            "content": "Our work is not the first that aims to construct dataset of openly licensed and/or public domain data for the purposes of training machine learning models. Past efforts include CommonCanvas [61], collection of approximately 70 million Creative Commons-licensed images designed for training image generation models, the PG19 dataset [140] of public domain novels sourced from Project 3 Gutenberg used for benchmarking language models, the C4Corpus tools for sourcing Creative Commons text from Common Crawl snapshots [68], and many datasets comprising CC BY-SAlicensed text from Wikipedia [66, 115]. More relevant to our work are the recent Open License Corpus (OLC) [119], Common Corpus [74, 91], and KL3M [78] datasets, which were constructed for use as LLM pre-training data. On the whole, OLC uses similar selection criteria to ours, including text that is in the public domain or is openly licensed. However, OLC also includes conversations scraped from Hacker News, which does not have an open license. Additionally, OLC is considerably smaller than the Common Pile v0.1, comprising data from 12 sources (vs. 30 for Common Pile) totaling 0.85 TB of text (vs. 7.6 TB for Common Pile). Common Corpus also uses similar set of allowable licenses/copyright statuses (e.g. CC BY, CC BY-SA, public domain, MIT-style, etc.) although the specific licenses/statuses are not clear because Common Corpus does not retain full per-document licensing information across all sources. Additionally, Common Corpus incorporates data from OpenAlex [126] which is known to provide inaccurate licensing information [e.g., 80]. Furthermore, while the Common Pile and Common Corpus are similar in size (7.6 TB vs. 7.4 TB), Common Corpus targets broader set of languages and therefore contains significantly less English text. Conversely, KL3M does not consider CC BY-SA to be acceptable and, as result, almost exclusively consists of government documents. Accordingly, the Common Pile is much larger than KL3M (3 TB), and is built from significantly more diverse data sources (Figure 1 & Section 3). In subsection 4.3, we compare the Common Pile v0.1 to these datasets in controlled setting, ultimately showing that it produces substantially more performant LLMs."
        },
        {
            "title": "3 Composition of the Common Pile",
            "content": "The Common Pile comprises content drawn from wide range of domains, including scholarly publications, government documents, online discussions, books, open educational resources, and more. In this section, we provide an overview of each of the domains contained in the Common Pile and briefly discuss their constituent data sources. In-depth discussion of each source is provided in Appendix B. Scientific and scholarly texts, which are often distributed under open licenses due to open access mandates, appear in many LLM pre-training datasets [e.g. 57, 167, 185] since they expose models to technical terminology, formal reasoning, and long-range document structure. To attain broad coverage of scholarly text, we filter peS2o [166] (a collection text extracted from open-access scientific PDFs based on S2ORC [104]) to only retain openly licensed research papers. For medical-domain text, we collect text from openly licensed articles in the U.S. National Institutes of Healths National Library of Medicines PubMed Central archive. Additionally, we collect data from ArXiv, which contains over 2.4 million articles in the quantitative sciences, most of which are uploaded as LaTeX source and may be distributed under various licenses chosen by given articles author. We include openly licensed articles sourced from ArXivs bulk-access S3 bucket and parsed using LATEXML and Trafilatura [10]. Furthermore, according to ArXivs licensing policy, all metadata (including abstracts) of articles posted to ArXiv are distributed under the CC0 license; we therefore include the abstracts for all ArXiv papers in the Common Pile, regardless of the papers full-text license. Online discussion forums comprise multi-turn question-answer pairs and discussions and therefore can be useful for training language models to follow conversational structure as well as for improving performance on question answering and dialogue-centric tasks. StackExchange is collection of websites that host user-provided questions and answers and allow their redistribution under CC BY-SA license. We leverage the user-provided StackExchange dumps from the Internet Archive and format questions/answers in the same order they appear on StackExchange, using PyMarkdown to convert each comment into plain text. Additionally, we collect text from issues, pull requests, and comments on GitHub, which, according to GitHubs terms of service, inherit the license of their associated repository. We extract this content from repositories with Blue Oak Council-approved licenses from the GitHub Archive. Finally, we include logs of all discussions on the Ubuntu-hosted Internet Relay Chat (IRC) since 2004, which are released into public domain. Government and legal texts are often published directly into the public domain or under open licenses. For example, in the US, text written by federal government employees is considered to be in the public domain. We therefore include all plain-text documents made available through the United 4 States Government Publishing Office (USGPO)s GovInfo.gov developer API. Additionally, we include all plain text regulatory documents published by U.S. federal agencies from Regulations.gov, an online platform that hosts newly proposed rules and regulations from federal agencies. The Common Pile also incorporates US Patents and Trademark Office (USPTO) patent documents sourced from the Google Patents Public Data dataset [77], containing millions of public domain patents and published patent applications dating back to 1782. Similarly, the Hansard (the official record of parliamentary proceedings) of the United Kingdom is distributed under the Open Parliament License, which stipulates similar terms to the CC BY license. We source UK Hansard data from ParlParse [131], covering Commons debates from 1918 forward and Lords proceedings from the 1999 reform. For legal text, we leverage the Caselaw Access Project (comprising 40 million pages of U.S. federal and state court decisions and judges opinions from the last 365 years) and Court Listener (including 900 thousand cases scraped from 479 courts). Only legal texts in the public domain were selected for the Common Pile. Curated task datasets are typically designed for fine-tuning on specific downstream tasks such as question answering, summarization, or text classification. To source datasets that are distributed under an open license and only contain content owned by the datasets rights holder (to avoid license laundering), we use metadata and redistributed datasets from the Data Provenance Initiative [106, 109]. Full details on the datasets we include are available in Appendix D. Books, particularly historic text, can fall into the public domain due to copyright expirationfor example, in the United States, books published prior to 1929 are currently in the public domain. We source public domain books from various sources, including the Biodiversity Heritage Library (BHL), an open-access digital library for biodiversity literature and archives; pre-1929 books digitized by the Internet Archive on behalf of HathiTrust member libraries; the collection of public domain books called Selected Digitized Books released by the Library of Congress; and select books from Project Gutenberg, an online collection of over 75,000 digitized books, most of which are in the public domain. Open Educational Resources (OERs) are educational materials (e.g. textbooks, lecture notes, lesson plans, etc.), typically published under Creative Commons licenses that support free and equitable access to education. We collect data from multiple OER repositories, including the Directory of Open Access Books (DOAB), an online index of over 94,000 peer-reviewed books curated from trusted open-access publishers; PressBooks, searchable catalog of over 8,000 open access books; OERCommons, an online platform where educators share open-access instructional materials; and LibreTexts, catalog of over 3,000 open-access textbooks. Wikis are topicor domain-specific encyclopedic websites that are collaboratively written, maintained, and moderated. Historical and cultural precedent has led many wikis to have an open license. We downloaded the official database dumps of wikitext (Mediawikis custom markup language) of the English-language wikis that are directly managed by the Wikimedia foundation and converted wikitext to plain text using wtf_wikipedia. For wikis not managed by Wikimedia, we make use of wikiteams unofficial database dumps and apply the same conversion process. Source code has proven to be useful part of LLM pre-training corpora, not only to support coding abilities but also to improve reasoning [7, 113, 120]. Due to the Free and Open Source Software (FOSS) movement, great deal of source code is distributed with an open license. We leverage prior work done by the Software Heritage Foundation and BigCode to compile the openly licensed subset of the Stack V2 [113], based on the license detection performed by the creators of Stack V2. Additionally we collected all Python Enhancement Proposals (PEPs)design documents that generally provide technical specification and rationale for new features of the Python programming languagethat were released into the public domain. YouTube allows users to upload content under CC BY license. We therefore sourced and transcribed speech-heavy CC BY videos from YouTube. To avoid license laundering and focus on high-quality speech-based textual content, we manually curated set of over 2,000 YouTube channels that release original openly licensed content containing speech. From these channels, we retrieved and transcribed (using Whisper [138]) over 1.1 million openly licensed videos comprising more than 470,000 hours of content. Web text is common source of LLM pre-training data. small fraction of content on the web is distributed under open licenses. To recover portion of this content, we process 52 Common 5 Crawl snapshots using regular expression (regex) adapted from the C4Corpus project [68] to retain pages that include CC BY, CC BY-SA, or CC0 marker. This regex naturally results in many false positives (e.g., it would retain page that included and provided attribution for CC BY image but otherwise contained unlicensed content), so we manually verified the top 1000 domains by content volume, retaining only those for which all content was assigned Creative Commons license. Text was extracted using pipeline similar to the one used for Dolma [167]. We provide more details on the composition of our web-sourced text, called CCCC, in Appendix G. Apart from CCCC, we additionally manually collected data from few select sites, including Foodista, communitymaintained site with recipes and food-related news as well as nutrition information; news sites that publish content under CC BY or CC BY-SA according to Open Newswire; and the Public Domain Review, an online journal dedicated to exploration of works of art and literature that have aged into the public domain."
        },
        {
            "title": "4 Assessing the Common Pile v0.1’s quality",
            "content": "The utility of an LLM pre-training dataset is mostly assessed in terms of whether or not it can be used to train performant LLMs. To validate our efforts in curating the Common Pile, we use it as the basis of an LLM pre-training dataset created through additional filtering (subsection 4.1) and rebalancing (subsection 4.2). Then, we perform controlled data ablation study (subsection 4.3) where we train otherwise-identical LLMs on different pre-training datasets, including prior datasets comprised of openly licensed text mentioned in subsection 2.2 as well as selection of representative pre-training datasets of unlicensed text. Finally, we train Comma v0.1-1T and Comma v0.1-2T, 7 billion parameter LLMs trained on 1 and 2 trillion tokens (respectively) of Common Pile-sourced content, and compare them to models with similar parameter count and training budget that were trained on unlicensed text (subsection 4.4)."
        },
        {
            "title": "4.1 Dataset preprocessing and filtering",
            "content": "Before training language model, it is considered important to clean data in hopes of retaining only high-quality text under some notion of quality [4, 105]. Consequently, before training on data from the Common Pile (which is distributed in relatively raw format), we independently preprocessed each of the Common Piles non-code datasets using pipelines implemented with the Dolma data processing toolkit [167]. Since the Common Pile v0.1 focuses primarily on English content, we apply language identification using FastText classifier [81] to filter out non-English text. When processing web text from CCCC, we employ the text quality classifier adapted from DataComp-LM [99] with an extremely low threshold to remove noisy text. We remove documents with pervasive OCR errors using the likelihood-based filtering approach from [166], which removes documents that are assigned an excessively low log-likelihood under unigram language model constructed from the Trillion Word Corpus [117]. To reduce the prevalence of toxic or inappropriate content, we apply pair of FastText toxicity classifiers implemented in Dolma [167] that were trained on the Jigsaw Toxic Comment Classification Challenge dataset [30]. We apply regex-based personally identifiable information (PII) redaction to remove email addresses, phone numbers, and IP addresses, and replace them with <EMAIL_ADDRESS>, <PHONE_NUMBER>, and <IP_ADDRESS> respectively. Finally, we perform source-specific regex filtering to remove repetitive or boilerplate text (e.g., page numbers, document preambles, license statements, etc.). For detailed breakdown of the pre-processing applied to each dataset, see Table 5 (appendix). After filtering, we perform global document-level fuzzy deduplication across all sources, as excessive data duplication is known to harm language modeling performance [94] and increase memorization [83]. We use the bloom filter-based deduplication functionality from Dolma [167] and deem two documents duplicates if they share more than 90% of their 20-grams. For code data from the Stack v2, we apply the Red Pajama V1 [185] code filtering heuristics. These include filters based on the mean and maximum line length in document, the proportion of alphanumeric characters, and the ratio of alphabetical characters to tokens. After this initial filter, we adopt the process used by SmolLM2 [5] where we keep only code in Python, C, C++, SQL, Java, PHP, Rust, Javascript, Typescript, Go, Ruby, Markdown, C#, Swift, or shell and filter this set using language-specific quality classifiers to retain only educational and well-documented code. We use 6 Figure 2: The Common Pile consistently outperforms other openly licensed corpora as pretraining dataset. Following the setup from Penedo et al. [132], we train and evaluate 1.7B parameter models on 28B tokens of data from each dataset. Stars denote benchmarks on which the model trained using the Common Pile outperforms all other models. lower threshold to filter out low-quality code than was used for SmolLM2, resulting in larger set of post-filtered text. Finally, we extract plaintext from HTML documents in the Stack V2 using Trafilatura [10] and apply our standard plaintext filtering pipeline including language, length, toxicity, and PII filtering."
        },
        {
            "title": "4.2 Data mixing",
            "content": "Recent work [3, 178, 189] has shown that upor down-weighting pre-training data sources in accordance with some notion of data quality can produce more performant models. Indeed, the sources in the Common Pile vary drastically in their characteristics, and we dont necessarily expect that our largest sources contain the highest quality text. For example, patent text sourced from the USPTO (our second-largest source) exhibits substantially different wording, terminology, and repetition than typical natural language. Consequently, we anticipate that appropriately mixing the sources in the Common Pile (rather than simply combining all sources, i.e., mixing in proportion to source size) is of particular importance. Additionally, while LLM pre-training datasets have been continuously scaled to avoid the diminishing returns that result from repeating data [94], recent work has highlighted that repeating high-quality data can be preferable to avoiding repetition by training on low-quality data [120, 52]. To determine mixing weights, we first trained per-source language models using the procedure outlined in subsection 4.3 below for 28 billion tokens on all sources that were sufficiently large to be repeated less than four times at this data budget. Based on the performance of these per-source models, we heuristically set mixing weights to upand down-weight highand low-performance sources respectively while targeting maximum of six repetitions over the course of 1 trillion token training run. Additionally, we assumed that our smaller sources were high quality and set their mixing rates such that they were also repeated six times over the course of 1 trillion tokens. The resulting mixture and per-source repetition rates are given in Table 7 (appendix). We also experimented with using MixMin [178] to automatically determine mixing weights but found that it did not improve over our heuristically determined mixture. Because we use this dataset mixture to train the Comma v0.1 models (subsection 4.4) and because it comprises heavily filtered and remixed version of the Common Pile v0.1, we refer to it as the Comma dataset to distinguish it from the Common Pile itself."
        },
        {
            "title": "4.3 Controlled dataset quality experiments",
            "content": "As preliminary measure of the Common Piles quality, we adopt the experimental setting of Penedo et al. [132] and identically train models on the Comma dataset and various preexisting datasets. By using controlled setting across datasets, we can assert that differences in model performance stem primarily from the quality of each dataset. Specifically, we train 1.7 billion parameter decoder-only Transformer models [182] that follow the Llama architecture [179] on 28 billion tokens of data from each dataset, tokenized using the GPT-2 tokenizer [137]. We follow the hyperparameters and setup 7 of Penedo et al. [132] exactly, except that we used weight decay of 0.2 instead of 0.1 due to slightly improved performance (possibly due to the large amount of repetition in the Comma dataset). Each model was then evaluated using the set of early signal tasks identified by Penedo et al. [132] which cover commonsense reasoning and knowledge capabilities; specifically, we evaluate zero-shot performance on ARC [33], MMLU [70], HellaSwag (HSwag) [192], OpenBookQA (OBQA) [118], CommonSenseQA (CSQA) [171], PIQA [19], and SocialIQA (SIQA) [160]. We omit Winogrande (which was used in Penedo et al. [132]) because it is included in the set of datasets we sourced from the Data Provenance Initiative; consequently all of the tasks we evaluate on are unseen by all models. We highlight that significant portion of the Comma dataset is code, but none of the tasks we evaluate on measure code capabilities. While it is possible that we could improve performance by omitting code data in this setting, we retained code for reliable reporting of the Comma datasets performance. As baselines, we compare to the prior datasets that aim to provide open licensed text discussed in subsection 2.2: OLC [119], Common Corpus [91], and KL3M [78]. We additionally compare to the Pile, as it one of the only LLM pre-training datasets that contains comparable number of diverse sources to the Common Pile (22 vs. 30). Finally, we report the performance of two web text-based unlicensed pre-training datasets: OSCAR [169], which incorporates relatively little filtering; and FineWeb [132], an recent dataset that reflects current best practice for LLM pre-training dataset curation. The resulting performance of each model is shown in Figure 2, with detailed results in Table 9 (appendix). Notably, the Comma dataset-based model outperforms the models trained OLC, Common Corpus, and KL3M across all benchmarks and outperforms the Pile-based model on all but two benchmarks. While the performance of the FineWeb-based model is the best on most benchmarks, the Comma dataset-based model performs best on the scientific and scholarly knowledge-based benchmarks MMLU and ARC, possibly due to the Common Piles large proportion of domainrelevant text. On the other hand, on the commonsense reasoning datasets HellaSwag, PIQA, and CommonSenseQA, the model trained on the Comma dataset has significantly worse performance than models trained on the Pile, OSCAR, and FineWeb, possibly indicating lack of relevant data in the Common Pile. We additionally note that recent work [188] highlights that performance on HellaSwag is most heavily influenced by coverage of certain domains and topics such as personal blogs, tutorials, hobbies, and sports, which are poorly represented in the Common Pile. Overall, these findings confirm that the Comma dataset performs best among datasets that aim to contain only openly licensed data and is also strong candidate in general, particularly when targeting scientific and scholarly applications. We note that the Comma dataset is the only dataset we evaluate that explicitly includes task-like data due to inclusion of data from the Data Provenance Initiative (DPI). To verify that this does not confer an unfair advantage, we trained an additional model on the Comma dataset with all sources retained except for the DPI-sourced data. Removing this source had minimal impact on model performance (full results in Table 9), with notable decrease only on HellaSwag, possibly suggesting that the DPI data contains domain-relevant data for this benchmark that other sources lack."
        },
        {
            "title": "4.4 Comma v0.1",
            "content": "Having established that Commas dataset produces models with competitive performance when compared to other datasets, we now validate our efforts at larger, more realistic scales. Specifically, we train Comma v0.1-1T and Comma v0.1-2T, pair of 7 billion-parameter LLMs trained on 1 and 2 trillion tokens of text respectively, and compare with other models trained using similar computational budgets. Tokenization While training tokenizer on unlicensed text is less likely to raise ethical or IP-related issues than training an LLM, we nevertheless trained custom tokenizer on the Comma dataset to ensure that our entire modeling pipeline was based on openly licensed data. In addition, the different characteristics of our dataset likely makes existing tokenizers (which are often trained on web text) suboptimal. We therefore trained BPE-based [55] tokenizer using the Hugging Face tokenizers library using vocabulary size of 64,000. We follow the same splitting regex as Llama 3.2 [62] and the Hugging Face ByteLevel preprocessor; no Unicode normalization was used. The tokenizer was trained on 600GB sample [150] of text from the Comma dataset. 8 Figure 3: Compared to models trained with similar resources (7 billion parameters, 1 trillion tokens), Comma v0.1-1T is the strongest model on several standard benchmarks. To contextualize these results, we include Qwen3 8B (trained on 36 trillion tokens) as current best-practices upper bound. Stars denote benchmarks on which Comma v0.1-1T outperforms all other computematched models (i.e., all models other than Qwen3). Full numerical results are provided in Table 10 (appendix). Training setup We trained Comma v0.1-1T and -2T using the lingua framework [183]. We base our model architecture and training hyperparameters on linguas Llama-7B configuration, which closely follows the conventions set by the Llama series of models [179, 62]. For Comma v0.1-1T, we trained with an effective batch size of 512 length-4096 sequences using the AdamW [111] optimizer and weight decay of 0.2. For the Comma v0.1 variant trained on 2 trillion tokens, we increased the batch size to 2048 length-4096 sequences. We performed two stage training, with first stage following cosine learning rate schedule and the second stage being cool-down [73], where we train only on subset of high-quality sources using the mixing weights provided in Table 8 (appendix) while decaying the learning rate linearly to 0. Comma v0.1-1T had first stage of 460,000 steps with 2,000 steps of warmup, an initial learning rate of 1e3, minimum learning rate of 1e9, cosine schedule period of 500,000 steps, and 18,000 steps of decay. For Comma v0.1-2T, the first stage instead had 230,000 steps with maximum and minimum learning rate of 2e3 and 2e9 respectively and period of 250,000 steps, with 9,000 steps of decay in the second stage. For both models, we average together ten evenly spaced checkpoints from the cool-down phase to produce final model as suggested by Grattafiori et al. [62]. Apart from our main Comma v0.1-1T and -2T training runs, we completed several additional runs to better understand how hyper-parameters impact the model, including using different batch size and following three-stage (rather than two-stage) curriculum. Overall, the results of these runs were consistent with the findings from our main training runs. Additional details can be found in Appendix O. Evaluation We evaluate the Comma v0.1 models on the suite of benchmarks used by Groeneveld et al. [64] in addition to two additional code benchmarks. Specifically, we evaluate models on ARC [33], MMLU [70], BoolQ [31], HellaSwag [192], OpenBookQA [118], CommonsenseQA [171], PIQA [19], and SIQA [160] to probe world knowledge and reasoning and HumanEval [25] and MBPP [8] to evaluate coding capabilities. Following Groeneveld et al. [64], we evaluate using OLMES [65], using zero-shot format for all tasks except MMLU, which uses 5-shot format. For the coding tasks, we report pass@10 accuracies. Baseline models For fairness, we primarily compare to prior models with the same parameter count and token budget. Since we are not aware of any such models trained on openly licensed data, we compare only to models trained on unlicensed data. For Comma v0.1-1T, we compare to Llama 1 7B [179], MPT-7B [175], RPJ-INCITE-7B [185], StableLM-7B [12], and OpenLLaMA-7B [58]. For the two trillion token variant, we compare to OLMo Twin (specifically OLMo-7B-Twin-2T) [64], Llama 2 7B [180], and DeepSeekLLM [16]. Over time, the token budgets of open pre-trained LLMs have continually grown [143], and current standard practice is to pretrain on significantly more than 1 or 2 trillion tokens. Consequently, recent models tend to outperform our baselines, which were released in 2023 and 2024. To provide state-of-the-art point of reference, we additionally include results for the recently released Qwen3 8B [176], which was trained for 36 trillion tokens. We emphasize that we cannot reliably compare to model with 36 or 18 larger training budget and we primarily include it as point of reference. 9 Figure 4: Comma v0.1-2T is also competitive with budget-matched models (7 billion parameters, 2 trillion tokens) trained on unlicensed data. We additionally include Qwen3 8B as higher budget upper bound. Stars denote benchmarks where Comma v0.1-2T outperforms budget-matched models. Full numerical results are provided in Table 11 (appendix). Results As shown in Figure 3, Comma v0.1-1T outperforms budget-matched baseline models on over half of the benchmarks tested. In line with our results from subsection 4.3, we observe that Comma v0.1-1T excels on knowledge-based benchmarks like ARC-C and MMLU, but lags behind on HellaSwag and PIQA. Comma v0.1-1T is also particularly strong at code-related tasks where it outperforms baseline models by wide margin. Comparisons to StableLM and OpenLLama can be found in Table 10 (appendix), but show similar trends. Our promising results from training on 1 trillion tokens motivated us to experiment with longer training durations. To test whether the filtered dataset supports training durations beyond 1T, we trained Comma v0.1-2T simply by repeating the same data mixture used for Comma v0.1-1T approximately twice. We note that this pre-training mixture involves repeating certain sources an excessive number of times (up to 16 passes for some sources). Prior work suggests that these extreme levels of data repetition may result in diminishing returns [121]. However, these experiments still give us preliminary picture of the performance achievable under larger budget. The performance of Comma v0.1-2T is compared with budget-matched models in Figure 4. Notably, we find that Comma v0.1-2T is competitive with OLMo, Llama 2, and DeepSeekLLM, with especially strong performance on MMLU, SIQA, ARC-E, and the coding tasks. We emphasize that the Comma v0.1-2T result here is likely not best-case 2T-token run using the Common Pile v0.1 due to excessive repetition, and better performance could likely be attained through 2T-specific mixture and curriculum. Nevertheless, this result highlights the promise of larger scale training runs based on the Common Pile. Qwen3 8Bs superior performance across most benchmarks confirms the benefit of larger training budgets and motivates future efforts on scaling up the Common Pile."
        },
        {
            "title": "5 Conclusion",
            "content": "We release Common Pile v0.1, an 8TB corpus thatto our knowledgeconstitutes the largest dataset built exclusively from openly licensed text. Alongside our dataset, we release Comma v0.1-1T and -2T, two performant 7-billion-parameter LLMs trained on text from the Common Pile, as well as the filtered and rebalanced data mixture we used for training. Our results demonstrate that not only is the Common Pile the strongest dataset for pretraining under an open-license constraint, but also that it produces models comparable to those trained on an equivalent amount of unlicensed data. This positive result holds promise for future of open-license pretraining, especially if the research community invests in collecting larger quantities of openly licensed text data in the future. Ultimately, we believe that the Common Pile v0.1 represents the first step on the path towards more ethical language model ecosystem, where performance need not come at the cost of creator rights and legal transparency."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Chris Maddison, Anvith Thudi, Pierre-Carl Langlais, Alec Radford, Adam Roberts, Sewon Min, and Weijia Shi for fruitful discussions and constructive feedback. An early draft of this work 10 was shared at the Dataset Convening hosted by the Mozilla Foundation and EleutherAI. We thank the participants for their discussion and feedback. This work was supported by funding from the Mozilla Foundation and Sutter Hill Ventures. We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC). Researchers funded through the NSERC-CSE Research Communities Grants do not represent the Communications Security Establishment Canada or the Government of Canada. Any research, opinions or positions they produce as part of this initiative do not represent the official views of the Government of Canada. Parts of this work were performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344 and was supported by the LLNL-LDRD Program under Project No. 24-ERD-010 and Project No. 24-SI-008 (LLNL-CONF2006420)."
        },
        {
            "title": "References",
            "content": "[1] 17 U.S. Code 102. Subject matter of copyright: In general, December 1990. URL https: //www.law.cornell.edu/uscode/text/17/102. [2] 17 U.S. Code 105. Subject matter of copyright: United States Government works, December 2024. URL https://www.law.cornell.edu/uscode/text/17/105. [3] Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for language model pre-training. arXiv preprint arXiv:2312.02406, 2023. [4] Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. survey on data selection for language models. Transactions on Machine Learning Research, 2024. [5] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big data-centric training of small language model, 2025. URL https://arxiv.org/abs/2502.02737. [6] Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolingual representations. In Annual Meeting of the Association for Computational Linguistics, pages 46234637, 2019. [7] Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. To code, or not to code? exploring impact of code in pre-training. arXiv preprint arXiv:2408.10914, 2024. [8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [9] Stefan Baack, Stella Biderman, Kasia Odrozek, Aviya Skowron, Ayah Bdeir, Jillian Bommarito, Jennifer Ding, Maximilian Gahntz, Paul Keller, Pierre-Carl Langlais, Greg Lindahl, Sebastian Majstorovic, Nik Marda, Guilherme Penedo, Maarten Van Segbroeck, Jennifer Wang, Leandro von Werra, Mitchell Baker, Julie Belião, Kasia Chmielinski, Marzieh Fadaee, Lisa Gutermuth, Hynek Kydlíˇcek, Greg Leppert, EM Lewis-Jong, Solana Larsen, Shayne Longpre, Angela Oduor Lungati, Cullen Miller, Victor Miller, Max Ryabinin, Kathleen Siminyu, Andrew Strait, Mark Surman, Anna Tumadóttir, Maurice Weber, Rebecca Weiss, Lee White, and Thomas Wolf. Towards best practices for open datasets for llm training, 2025. URL https://arxiv.org/abs/2501.08365. [10] Adrien Barbaresi. Trafilatura: Web Scraping Library and Command-Line Tool for Text Discovery and Extraction. In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 122131. Association for 11 Computational Linguistics, 2021. URL https://aclanthology.org/2021.acl-demo. 15. [11] Max Bartolo, A. Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat the ai: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics, 8:662678, 2020. [12] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6 technical report. arXiv preprint arXiv:2402.17834, 2024. [13] Jonathan Berant, A. Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Conference on Empirical Methods in Natural Language Processing, pages 15331544, 2013. [14] Janek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury, Gabriella Pasi, and Benjamin Piwowarski, editors, Advances in Information Retrieval. 40th European Conference on IR Research (ECIR 2018), Lecture Notes in Computer Science, Berlin Heidelberg New York, March 2018. Springer. [15] Janek Bevendorff, Martin Potthast, and Benno Stein. FastWARC: Optimizing Large-Scale Web Archive Analytics. In Andreas Wagner, Christian Guetl, Michael Granitzer, and Stefan Voigt, editors, 3rd International Symposium on Open Search Technology (OSSYM 2021). International Open Search Symposium, October 2021. [16] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. [17] Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. Emergent and predictable memorization in large language models. Advances in Neural Information Processing Systems, 36:2807228090, 2023. [18] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: suite for analyzing large language models across training and scaling, 2023. URL https://arxiv.org/abs/2304. 01373. [19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. URL https://arxiv.org/abs/ 1911.11641. [20] Blue Oak Council. License List (version 15), 2025. URL https://blueoakcouncil.org/ list. [21] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language inference with natural language explanations. In Neural Information Processing Systems, pages 95609572, 2018. [22] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations, 2022. [23] Ilias Chalkidis, Abhik Jana, D. Hartung, M. Bommarito, Ion Androutsopoulos, D. Katz, and Nikolaos Aletras. Lexglue: benchmark dataset for legal language understanding in english. In Annual Meeting of the Association for Computational Linguistics, pages 43104330, 2021. [24] Chat GPT Is Eating the World, 2024. URL https://chatgptiseatingtheworld.com. 12 [25] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. [26] Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. HybridQA: dataset of multi-hop question answering over tabular and textual data. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 10261036, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.91. URL https:// aclanthology.org/2020.findings-emnlp.91/. [27] Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, and William Yang Wang. Logic2text: High-fidelity natural language generation from logical forms. ArXiv, abs/2004.14579, 2020. [28] Sang Keun Choe, Hwijeen Ahn, Juhan Bae, Kewen Zhao, Minsoo Kang, Youngseog Chung, Adithya Pratapa, Willie Neiswanger, Emma Strubell, Teruko Mitamura, Jeff Schneider, Eduard Hovy, Roger Grosse, and Eric Xing. What is your data worth to gpt? llm-scale data valuation with influence functions, 2024. URL https://arxiv.org/abs/2405.13954. [29] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. Quac: Question answering in context. In Conference on Empirical Methods in Natural Language Processing, pages 21742184, 2018. [30] cjadams, Jeffrey Sorensen, Julia Elliott, Lucas Dixon, Mark McDonald, nithum, and Will Cukierski. Toxic comment classification challenge. https://kaggle.com/competitions/ jigsaw-toxic-comment-classification-challenge, 2017. Kaggle. [31] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. [32] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher Manning. What does BERT look at? an analysis of BERTs attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2019. [33] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [34] Karl Cobbe, V. Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. [35] Creative Commons. CC0 1.0 Universal (CC0 1.0) Public Domain Dedication, 2025. URL https://creativecommons.org/publicdomain/zero/1.0/. [36] Creative Commons. Creative Commons Attribution 4.0 International License 2(a)(1)(A), 2025. URL https://creativecommons.org/licenses/by/4.0/legalcode. [37] Creative Commons. Creative Commons Attribution 4.0 International License 3(a)(1)(A)(i), 2025. URL https://creativecommons.org/licenses/by/4.0/legalcode. [38] Creative Commons. Public Domain Mark 1.0, 2025. URL https://creativecommons. org/publicdomain/mark/1. 13 [39] Creative Commons. Creative Commons Attribution-ShareAlike 4.0 International License, 2025. URL https://creativecommons.org/licenses/by-sa/4.0/. [40] A. Feder Cooper and James Grimmelmann. The Files are in the Computer: Copyright, Memorization, and Generative AI. arXiv preprint arXiv:2404.12590, 2024. [41] A. Feder Cooper, Aaron Gokaslan, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley, Daniel E. Ho, and Percy Liang. Extracting memorized pieces of (copyrighted) books from open-weight language models. arXiv preprint arXiv:2505.12546, 2025. [42] Yiming Cui, Ting Liu, Li Xiao, Zhipeng Chen, Wentao Ma, Wanxiang Che, Shijin Wang, and Guoping Hu. span-extraction dataset for chinese machine reading comprehension. In EMNLP-IJCNLP, pages 58825888, 2019. [43] Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen tau Yih, and Peter Clark. Tracking state changes in procedural text: challenge dataset and models for process paragraph comprehension. In North American Chapter of the Association for Computational Linguistics, pages 15951604, 2018. [44] Pradeep Dasigi, Nelson F. Liu, Ana Marasovic, Noah A. Smith, and Matt Gardner. Quoref: reading comprehension dataset with questions requiring coreferential reasoning. In Conference on Empirical Methods in Natural Language Processing, volume abs/1908.05803, 2019. [45] Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. Few-nerd: few-shot named entity recognition dataset. ArXiv, abs/2105.07464, 2021. [46] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. In North American Chapter of the Association for Computational Linguistics, pages 23682378, 2019. [47] Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membership inference attacks work on large language models? arXiv preprint arXiv:2402.07841, 2024. [48] S. Dumitrescu, Petru Rebeja, Beáta Lorincz, Mihaela Gaman, M. Ilie, Andrei Pruteanu, Adriana Stan, Luciana Morogan, Traian Rebedea, and Sebastian Ruder. Liro: Benchmark and leaderboard for romanian language tasks. In NeurIPS Datasets and Benchmarks, 2021. [49] Yanai Elazar and Yoav Goldberg. Wheres my head? definition, data set, and models for numeric fused-head identification and resolution. Transactions of the Association for Computational Linguistics, 7:519535, 2019. [50] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schütze, and Yoav Goldberg. Measuring causal effects of data statistics on language modelsfactualpredictions. arXiv preprint arXiv:2207.14251, 2022. [51] Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. Moral stories: Situated reasoning about norms, intents, actions, and their consequences. ArXiv, abs/2012.15738, 2020. [52] Alex Fang, Hadi Pouransari, Matt Jordan, Alexander Toshev, Vaishaal Shankar, Ludwig Schmidt, and Tom Gunter. Datasets, documents, and repetitions: The practicalities of unequal data quality. arXiv preprint arXiv:2503.07879, 2025. [53] James Ferguson, Matt Gardner, Tushar Khot, and Pradeep Dasigi. Iirc: dataset of incomplete information reading comprehension questions. In Conference on Empirical Methods in Natural Language Processing, pages 11371147, 2020. [54] Nancy Fulda, Nathan Tibbetts, Zachary Brown, and D. Wingate. Harvesting common-sense navigational knowledge for robotics from uncurated text corpora. In Conference on Robot Learning, pages 525534, 2017. 14 [55] Philip Gage. new algorithm for data compression. The Users Journal archive, 12:2338, 1994. URL https://api.semanticscholar.org/CorpusID:59804030. [56] N. Gale, G. Heath, E. Cameron, S. Rashid, and S. Redwood. Using the framework method for the analysis of qualitative data in multi-disciplinary health research. BMC Medical Research Methodology, 13:117 117, 2013. [57] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020. URL https: //arxiv.org/abs/2101.00027. [58] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https://github.com/openlm-research/open_llama. [59] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, D. Roth, and Jonathan Berant. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361, 2021. [60] Max Glockner, Vered Shwartz, and Yoav Goldberg. Breaking nli systems with sentences that require simple lexical inferences. ArXiv, abs/1805.02266, 2018. [61] Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. CommonCanvas: Open Diffusion Models Trained on Creative-Commons Images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 82508260, June 2024. [62] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, 15 Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, 16 Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The Llama 3 Herd of Models, November 2024. URL http://arxiv.org/abs/2407.21783. arXiv:2407.21783 [cs]. [63] Grobid. Grobid. https://github.com/kermitt2/grobid, 20082025. [64] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating the science of language models, 2024. URL https://arxiv.org/abs/2402.00838. [65] Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and Hannaneh Hajishirzi. Olmes: standard for language model evaluations, 2025. URL https: //arxiv.org/abs/2406.08446. [66] Mandy Guo, Zihang Dai, Denny Vrandeˇcic, and Rami Al-Rfou. Wiki-40b: Multilingual language model dataset. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 24402452, 2020. [67] Aditya Gupta, Jiacheng Xu, Shyam Upadhyay, Diyi Yang, and Manaal Faruqui. Disflqa: benchmark dataset for understanding disfluencies in question answering. ArXiv, abs/2106.04016, 2021. [68] Ivan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpus with free license. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16), pages 914922, Portorož, Slovenia, May 2016. European Language Resources Association (ELRA). URL https://aclanthology. org/L16-1146/. [69] Seth Hays. AI Training and Copyright Infringement: Solutions from Asia, October 2024. URL https://www.techpolicy.press/ai-training-and-copyrightinfringement-solutions-from-asia/. [70] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [71] Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. Cuad: An expert-annotated nlp dataset for legal contract review. ArXiv, abs/2103.06268, 2021. [72] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 2018. [73] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. [74] HuggingFace: Common Corpus, 2025. URL https://huggingface.co/datasets/ PleIAs/common_corpus. [75] Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In AAAI Conference on Artificial Intelligence, pages 63846392, 2020. 17 [76] Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continually pre-train large language models. arXiv preprint arXiv:2403.08763, 2024. [77] IFI CLAIMS Patent Services and Google. Google patents public data. https://patents. google.com/, 2023. Licensed under Creative Commons Attribution 4.0 International License. [78] Michael Bommarito II, Jillian Bommarito, and Daniel Martin Katz. The kl3m data project: Copyright-clean training resources for large language models, 2025. URL https://arxiv. org/abs/2504.07854. [79] Infocomm Media Development Authority of Singapore (IMDA), Aicadium, and AI Verify Foundation. Model AI Governance Framework for Generative AI: Fostering Trusted Ecosystem, May 2024. URL https://aiverifyfoundation.sg/wp-content/uploads/2024/ 05/Model-AI-Governance-Framework-for-Generative-AI-May-2024-1-1.pdf. [80] Najko Jahn, Nick Haupka, and Anne Hobert. Analysing and reclassifying open access information in OpenAlex, 2023. URL https://subugoe.github.io/scholcomm_analytics/ posts/oalex_oa_status/?utm_source=chatgpt.com. [81] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 427431. Association for Computational Linguistics, April 2017. [82] Nikhil Kandpal and Colin Raffel. Position: The most expensive part of an llm should be its training data. arXiv preprint arXiv:2504.12427, 2025. [83] Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models, 2022. URL https://arxiv.org/abs/2202.06539. [84] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large In International Conference on language models struggle to learn long-tail knowledge. Machine Learning, pages 1569615707. PMLR, 2023. [85] Pride Kavumba, Naoya Inoue, Benjamin Heinzerling, Keshav Singh, Paul Reisert, and Kentaro Inui. When choosing plausible alternatives, clever hans can be clever. ArXiv, abs/1911.00225, 2019. [86] Tushar Khot, Ashish Sabharwal, and Peter Clark. Scitail: textual entailment dataset from science question answering. In AAAI Conference on Artificial Intelligence, pages 51895197, 2018. [87] Rodney Michael Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David W. Graham, F.Q. Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler C. Murray, Christopher Newell, Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, A. Tanaka, Alex Wade, Linda M. Wagner, Lucy Lu Wang, Christopher Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine van Zuylen, and Daniel S. Weld. The Semantic Scholar Open Data Platform. ArXiv, abs/2301.10140, 2023. URL https://api. semanticscholar.org/CorpusID:256194545. [88] Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi Rui Tam, K. Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, ES Shahul, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and A. Mattick. Openassistant conversations - democratizing large language model alignment. ArXiv, abs/2304.07327, 2023. 18 [89] T. Kwiatkowski, J. Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, D. Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc V. Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. [90] Faisal Ladhak, Esin Durmus, Claire Cardie, and K. McKeown. Wikilingua: new benchmark dataset for multilingual abstractive summarization. ArXiv, abs/2010.03093, 2020. [91] Pierre-Carl Langlais. Releasing Common Corpus: the largest public domain dataset for training LLMs, 2024. URL https://huggingface.co/blog/Pclanglais/common-corpus. [92] LDP Headquarters for the Promotion of Digital Society and Project Team on AI White Paper 2024: New Stratethe Evolution and Implementation of AIs. gies in Stage II, Toward the worlds most AI-friendly country, April 2024. URL https://aiverifyfoundation.sg/wp-content/uploads/2024/05/Model-AIGovernance-Framework-for-Generative-AI-May-2024-1-1.pdf. [93] R. Lebret, David Grangier, and Michael Auli. Neural text generation from structured data with application to the biography domain. In Conference on Empirical Methods in Natural Language Processing, pages 12031213, 2016. [94] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better, 2022. URL https://arxiv.org/abs/2107.06499. [95] Katherine Lee, A. Feder Cooper, James Grimmelmann, and Daphne Ippolito. AI and Law: The Next Generation. SSRN, 2023. http://dx.doi.org/10.2139/ssrn.4580739. [96] Katherine Lee, A. Feder Cooper, and James Grimmelmann. Talkin Bout AI Generation: Copyright and the Generative-AI Supply Chain. arXiv preprint arXiv:2309.08133, 2023. [97] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, D. Kontokostas, Pablo N. Mendes, Sebastian Hellmann, M. Morsey, Patrick van Kleef, S. Auer, and Christian Bizer. Dbpedia - large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web, 6:167195, 2015. [98] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, 2011. [99] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2025. URL https://arxiv.org/abs/2406.11794. [100] Xin Li and D. Roth. Learning question classifiers."
        },
        {
            "title": "In International Conference on",
            "content": "Computational Linguistics, pages 17, 2002. [101] Stephanie C. Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Annual Meeting of the Association for Computational Linguistics, pages 32143252, 2021. [102] Emmy Liu, Chenxuan Cui, Kenneth Zheng, and Graham Neubig. Testing the ability of language models to interpret figurative language. ArXiv, abs/2204.12632, 2022. 19 [103] Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric P. Xing. Llm360: Towards fully transparent open-source llms, 2023. URL https://arxiv.org/abs/2312.06550. [104] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: In Proceedings of the 58th Annual Meeting The semantic scholar open research corpus. of the Association for Computational Linguistics, pages 49694983, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https://www.aclweb.org/anthology/2020.acl-main.447. [105] Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel McDuff, Sayash Kapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, et al. The responsible foundation model development cheatsheet: review of tools & resources. Transactions on Machine Learning Research, 2024. [106] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, Xinyi (Alexis) Wu, Enrico Shippole, Kurt Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, and Sara Hooker. large-scale audit of dataset licensing and attribution in AI. Nature Machine Intelligence, 6(8):975987, August 2024. doi: 10/gt8f5p. [107] Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, William Brannon, Nayan Saxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, Kevin Klyman, Christopher Klamm, Hailey Schoelkopf, Nikhil Singh, Manuel Cherep, Ahmad Anis, An Dinh, Caroline Chitongo, Da Yin, Damien Sileo, Deividas Mataciunas, Diganta Misra, Emad Alghamdi, Enrico Shippole, Jianguo Zhang, Joanna Materzynska, Kun Qian, Kush Tiwary, Lester Miranda, Manan Dey, Minnie Liang, Mohammed Hamdy, Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Shrestha Mohanty, Vipul Gupta, Vivek Sharma, Vu Minh Chien, Xuhui Zhou, Yizhi Li, Caiming Xiong, Luis Villa, Stella Biderman, Hanlin Li, Daphne Ippolito, Sara Hooker, Jad Kabbara, and Sandy Pentland. Consent in crisis: The rapid decline of the AI data commons. Advances in Neural Information Processing Systems, 37, 2024. [108] Shayne Longpre, Robert Mahari, Naana Obeng-Marnu, William Brannon, Tobin South, Katy Gero, Sandy Pentland, and Jad Kabbara. Data authenticity, consent, & provenance for ai are all broken: what will it take to fix them? arXiv preprint arXiv:2404.12691, 2024. [109] Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska, William Brannon, Robert Mahari, Naana Obeng-Marnu, Manan Dey, Mohammed Hamdy, et al. Bridging the data provenance gap across text, speech and video. arXiv preprint arXiv:2412.17847, 2024. [110] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. pretrainers guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 32453276, 2024. [111] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. International Conference on Learning Representations, //openreview.net/forum?id=Bkg6RiCqY7. 2019. In URL https: [112] Annie Louis, D. Roth, and Filip Radlinski. id rather just go to bed: Understanding indirect answers. In Conference on Empirical Methods in Natural Language Processing, volume abs/2010.03450, 2020. [113] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, 20 Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation, 2024. [114] Robert Mahari and Shayne Longpre. Discit ergo est: Training data provenance and fair use. Robert Mahari and Shayne Longpre, Discit ergo est: Training Data Provenance And Fair Use, Dynamics of Generative AI (ed. Thibault Schrepel & Volker Stocker), Network Law Review, Winter, 2023. [115] Matt Mahoney. Large text compression benchmark, 2011. [116] Stephen Merity, Caiming Xiong, James Bradbury, and R. Socher. Pointer sentinel mixture models. ArXiv, abs/1609.07843, 2016. [117] Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Joseph P. Pickett, Dale Hoiberg, Matthew K. Gray, The Google Books Team, Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker, Martin A. Nowak, and Erez Lieberman Aiden. Quantitative analysis of culture using millions of digitized books. doi: 10.1126/science.1199644. URL https://www.science.org/doi/abs/10.1126/science.1199644. Science, 331(6014):176182, 2011. [118] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [119] Sewon Min, Suchin Gururangan, Eric Wallace, Weijia Shi, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer. SILO language models: Isolating legal risk in nonparametric datastore. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=ruk0nyQPec. [120] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36, 2023. [121] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 5035850376. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/ paper/2023/file/9d89448b63ce1e2e8dc7af72c984c196-Paper-Conference.pdf. [122] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. Crows-pairs: challenge dataset for measuring social biases in masked language models. In Conference on Empirical Methods in Natural Language Processing, pages 19531967, 2020. [123] Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. The e2e dataset: New challenges for end-to-end generation. ArXiv, abs/1706.09254, 2017. [124] Tomoko Ohta, Sampo Pyysalo, Junichi Tsujii, and S. Ananiadou. Open-domain anatomical entity mention detection. In Annual Meeting of the Association for Computational Linguistics, pages 2736, 2012. [125] Yasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and Greg Durrett. Creak: dataset for commonsense reasoning over entity knowledge. ArXiv, abs/2109.01653, 2021. [126] OpenAlex, 2025. URL https://openalex.org. 21 [127] Vassil Panayotov, Guoguo Chen, Daniel Povey, and S. Khudanpur. Librispeech: An asr corpus based on public domain audio books. 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210, 2015. [128] Ashwinee Panda, Xinyu Tang, Milad Nasr, Christopher Choquette-Choo, and Prateek Mittal. Privacy auditing of large language models. arXiv preprint arXiv:2503.06808, 2025. [129] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, Aleksander Madry. https://arxiv.org/abs/2303.14186. Trak: Attributing model behavior at scale, 2023. and URL [130] European Parliament and Council of the European Union. Directive (eu) 2019/790, URL https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX: 2019. 32019L0790#art_3. [131] ParlParse. Parser for uk parliament proceedings. https://parser.theyworkforyou.com/, 2025. Accessed: 2025-05-09. [132] Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The FineWeb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37, 2024. [133] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, 2019. [134] E. Ponti, Goran Glavavs, Olga Majewska, Qianchu Liu, Ivan Vulic, and A. Korhonen. Xcopa: In Conference on Empirical multilingual dataset for causal commonsense reasoning. Methods in Natural Language Processing, pages 23622376, 2020. [135] Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. Dynasent: dynamic benchmark for sentiment analysis. ArXiv, abs/2012.15349, 2020. [136] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018. [137] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. [138] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. URL https://arxiv.org/abs/2212.04356. [139] Filip Radlinski, K. Balog, B. Byrne, and K. Krishnamoorthi. Coached conversational In SIGDIAL preference elicitation: case study in understanding movie preferences. Conferences, pages 353360, 2019. [140] Jack Rae, Anna Potapenko, Siddhant Jayakumar, Chloe Hillier, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507. [141] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson dAutume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G. 22 Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. ArXiv, abs/2112.11446, 2021. URL https://api.semanticscholar.org/CorpusID:245353475. [142] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140), 2020. [143] Robi Rahman and David Owen. The size of datasets used to train language models doubles approximately every seven months, 2024. URL https://epoch.ai/datainsights/dataset-size-trend. Accessed: 2025-05-08. [144] Nazneen Rajani, Bryan McCann, Caiming Xiong, and R. Socher. Explain yourself! leveraging language models for commonsense reasoning. ArXiv, abs/1906.02361, 2019. [145] Inioluwa Deborah Raji, Peggy Xu, Colleen Honigsberg, and Daniel Ho. Outsider oversight: In Proceedings of the 2022 Designing third party audit ecosystem for ai governance. AAAI/ACM Conference on AI, Ethics, and Society, pages 557571, 2022. [146] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Conference on Empirical Methods in Natural Language Processing, pages 23832392, 2016. [147] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for squad. In Annual Meeting of the Association for Computational Linguistics, volume abs/1806.03822, 2018. [148] Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. Schema-guided dialogue state tracking task at dstc8. ArXiv, abs/2002.01359, 2020. [149] Abhilasha Ravichander, Matt Gardner, and Ana Marasovic. Condaqa: contrastive reading comprehension dataset for reasoning about negation. ArXiv, abs/2211.00295, 2022. [150] Varshini Reddy, Craig W. Schmidt, Yuval Pinter, and Chris Tanner. is enough? https://arxiv.org/abs/2502.20273. the diminishing returns of tokenization training data, 2025."
        },
        {
            "title": "How much\nURL",
            "content": "[151] Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah Mahani Aljunied, Luong Chi Mai, V. Thang, N. Thai, Vichet Chea, Rapid Sun, Sethserey Sam, Sopheap Seng, K. Soe, K. Nwet, M. Utiyama, and Chenchen Ding. Introduction of the asian language treebank. In Oriental COCOSDA International Conference on Speech Database and Assessments, pages 16, 2016. [152] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. [153] Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. Getting closer to ai complete question answering: set of prerequisite real tasks. In AAAI Conference on Artificial Intelligence, pages 87228731, 2020. [154] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. primer in bertology: What we know about how BERT works. Transactions of the association for computational linguistics, 8, 2021. [155] Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan Le Bras, Noah A. Smith, and Yejin Choi. Thinking like skeptic: Defeasible inference in natural language. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 46614675, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.418. URL https://aclanthology.org/2020.findings-emnlp.418/. [156] Matthew Sag and Peter K. Yu. The globalization of copyright exceptions for ai training. Emory Law Journal, 74, 2025. doi: http://dx.doi.org/10.2139/ssrn.4976393. URL https://ssrn.com/abstract=4976393. [157] Swarnadeep Saha, Yixin Nie, and Mohit Bansal. Conjnli: Natural language inference over conjunctive sentences. In Conference on Empirical Methods in Natural Language Processing, pages 82408252, 2020. [158] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande. Communications of the ACM, 64:99 106, 2019. [159] Pamela Samuelson. How to Think About Remedies in the Generative AI Copyright Cases. Lawfare, February 2024. URL https://www.lawfaremedia.org/article/how-tothink-about-remedies-in-the-generative-ai-copyright-cases. [160] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, interactions, reasoning about social Socialiqa: Commonsense https://arxiv.org/abs/1904.09728. and Yejin Choi. URL 2019. [161] A. Sboev, A. Naumov, and R. Rybka. Data-driven model for emotion detection in russian texts. In BICAAI, pages 637642, 2020. [162] Tal Schuster, Adam Fisch, and R. Barzilay. Get your vitamin c! robust fact verification with contrastive evidence. In North American Chapter of the Association for Computational Linguistics, pages 624643, 2021. [163] Emily Sheng and David C. Uthus. Investigating societal biases in poetry composition system. ArXiv, abs/2011.02686, 2020. [164] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. ArXiv, abs/2010.03768, 2020. [165] Shivalika Singh, Freddie Vargus, Daniel Dsouza, B\"orje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Minh Chien Vu, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, A. Ustun, Marzieh Fadaee, and Sara Hooker. Aya dataset: An open-access ArXiv, abs/2402.06619, 2024. URL collection for multilingual instruction tuning. https://api.semanticscholar.org/CorpusID:267617144. [166] Luca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report, Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o. [167] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024. [168] Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. Evaluating gender bias in machine translation. ArXiv, abs/1906.00591, 2019. [169] Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent Romary. Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures. In 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7). Leibniz-Institut für Deutsche Sprache, 2019. [170] Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. Quartz: An open-domain dataset of qualitative relationship questions. In Conference on Empirical Methods in Natural Language Processing, volume abs/1909.03553, 2019. [171] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: arXiv preprint question answering challenge targeting commonsense knowledge. arXiv:1811.00937, 2018. [172] Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi, Dheeraj Rajagopal, Peter Clark, Michal Guerquin, Kyle Richardson, and E. Hovy. dataset for tracking entities in open domain procedural text. ArXiv, abs/2011.08092, 2020. [173] Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, Bhaskar Rao, Linghao Jin, Huijuan Wang, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, Xuezhe Ma, Yue Peng, Zhengzhong Liu, and Eric P. Xing. Txt360: top-quality llm pre-training dataset requires the perfect blend, 2024. [174] Ishan Tarunesh, Somak Aditya, and M. Choudhury. Trusting roberta over bert: Insights from checklisting the natural language inference task. ArXiv, abs/2107.07229, 2021. [175] MosaicML NLP Team. Introducing mpt-7b: new standard for open-source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05. [176] Qwen Team. Qwen3, April 2025. URL https://qwenlm.github.io/blog/qwen3/. [177] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: large-scale dataset for fact extraction and verification. ArXiv, abs/1803.05355, 2018. [178] Anvith Thudi, Evianne Rovers, Yangjun Ruan, Tristan Thrush, and Chris J. MadURL dison. Mixmin: Finding data mixtures via convex minimization, 2025. https://arxiv.org/abs/2502.10510. [179] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971. [180] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [181] UK Parliament."
        },
        {
            "title": "Open parliament",
            "content": "license. information/copyright-parliament/open-parliament-licence/, Accessed: 2025-05-09. https://www.parliament.uk/siteUnknown. [182] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [183] Mathurin Videau, Badr Youbi Idrissi, Daniel Haziza, Luca Wehrstedt, Jade Copet, Olivier Teytaud, and David Lopez-Paz. Meta Lingua: minimal PyTorch LLM training library, 2024. URL https://github.com/facebookresearch/lingua. [184] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. Helpsteer: Multi-attribute helpfulness dataset for steerlm. ArXiv, abs/2311.09528, 2023. [185] Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang. Redpajama: an open dataset for training large language models, 2024. URL https://arxiv.org/abs/2411.12372. 25 [186] Kellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. Resolving gendered ambiguous pronouns with bert. ArXiv, abs/1906.01161, 2019. [187] Wei Wei, Quoc V. Le, Andrew M. Dai, and Jia Li. Airdialogue: An environment for goal-oriented dialogue research. In Conference on Empirical Methods in Natural Language Processing, pages 38443854, 2018. [188] Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini. Organize the web: Constructing domains enhances pre-training data curation. arXiv preprint arXiv:2502.10341, 2025. [189] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36, 2023. [190] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, R. Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing, pages 23692380, 2018. [191] Cat Zakrzewski, Nitasha Tiku, and Elizabeth Dwoskin. OpenAI prepares to fight for its life as legal troubles mount. The Washington Post, 2024. [192] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [193] Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. MAP-Neo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024. [194] Hongming Zhang, Xinran Zhao, and Yangqiu Song. Winowhy: deep diagnosis of essential commonsense knowledge for answering winograd schema challenge. In Annual Meeting of the Association for Computational Linguistics, pages 57365745, 2020. [195] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatGPT interaction logs in the wild. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=Bl8u7ZRlbM. [196] Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, and D. Roth. Temporal reasoning on implicit events from distant supervision. ArXiv, abs/2010.12753, 2020."
        },
        {
            "title": "Table of Contents",
            "content": "A Contributions . Detailed Description of Sources B.1 Scientific and Scholarly Text . B.2 Online Discussions and Forums . . B.3 Government and Legal Texts . . . B.4 Curated Task Data . . B.5 Books in the Public Domain . . B.6 Open Educational Resources . . . B.7 Wikis . . . . . . . B.8 Source Code . . B.9 Transcribed Audio Content . . B.10 Web Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional insights on licensing C.1 Why we cant always trust automatic license detection . . . . . . . . . . . . . . List of Data Provenance Initiative sources List of News sources List of WikiMedia wikis CCCC Source Statistics PeS2o Source Statistics Growth rates of openly licensed data Details on filtering pipelines Details on Commas pre-training data mixture Details on Commas cool-down data mixture Details on small-scale data ablations Additional Comma results Additional training runs O.1 Ablations at 1T Tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 28 28 28 29 30 30 31 31 32 32 33 33 34 34 44 44 44 47 47 49 51 52"
        },
        {
            "title": "A Contributions",
            "content": "Figure 5: Author contributions to this work. Large squares indicate major contribution and small squares indicate supporting contribution."
        },
        {
            "title": "B Detailed Description of Sources",
            "content": "Below, we give more in-depth overview of the sources that make up the Common Pile, including specific license decisions and tools used during collection. B.1 Scientific and Scholarly Text Scientific and scholarly texts are staple of modern LLM pretraining corpora, appearing in nearly all large-scale datasets [e.g. 57, 185, 167] since they expose models to technical terminology, formal reasoning, and long-range document structureskills that are essential for downstream tasks in science, education, and question answering. Thanks to open access mandates and academic cultural norms, many scholarly texts are either in the public domain or are distributed under open licenses. peS2o To ensure broad coverage across many scientific disciplines, we include version of peS2o [166] restricted to openly licensed articles. pes2o is derived from S2ORC [104], corpus of openly licensed abstract and full-text papers that have been converted to structured format using Grobid [63]. Starting from Grobids XML output, peS2o filters papers that are too short, have incorrect metadata, are in languages other than English, and contain OCR errors using combination of heuristicand model-based filtering steps. We refer the reader to the datasheet and code for more details on this processing pipeline. The subset of peS2o included in the Common Pile starts from v3 of the corpus, which contains documents from January 1, 1970 to October 6, 2024. We retain full-text papers with CC BY, CC BY-SA, or CC0 licenses, or that have been labeled as public domain; metadata is provided by the Semantic Scholar APIs [87]. After filtering, this set contains 6.3 million 28 papers, or 35.7 billion whitespace-separated segments. We provide more details on the composition of this subset in Appendix H. PubMed PubMed Central (PMC) is an open-access archive of biomedical and life sciences research papers maintained by the U.S. National Institutes of Healths National Library of Medicine. We collected papers from PMC whose metadata indicated that the publishing journal had designated CC BY, CC BY-SA, or CC0 license. PMC stores the text content of each article as single XML file, which we convert to markdown using pandoc. ArXiv Papers ArXiv is an online open-access repository of over 2.4 million scholarly papers covering fields such as computer science, mathematics, physics, quantitative biology, economics, and more. When uploading papers, authors can choose from variety of licenses. We included text from all papers uploaded under CC BY, CC BY-SA, and CC0 licenses in the Common Pile through three-step pipeline: first, the latex source files for openly licensed papers were downloaded from ArXivs bulk-access S3 bucket; next, the LATEXML conversion tool was used to convert these source files into single HTML document; finally, the HTML was converted to plaintext using the Trafilatura [10] HTML-processing library. ArXiv Abstracts Each paper uploaded to ArXiv includes structured metadata fields, including an abstract summarizing the papers findings and contributions. According to ArXivs licensing policy, the metadata for any paper submitted to ArXiv is distributed under the CC0 license, regardless of the license of the paper itself. Thus, we include as an additional source the abstracts for every paper submitted to ArXiv. We source the abstracts from ArXivs API via the Open Archives Initiative Protocol for Metadata Harvesting endpoint and reproduce them as-is. B.2 Online Discussions and Forums Online forums are rich source of multi-turn, user-generated dialogue covering wide range of topics. These platforms often feature questionanswer pairs, problem-solving discussions, and informal explanations of technical and non-technical concepts. The Common Pile incorporates online discussions from sources that distribute content under an open license. StackExchange While StackExchange formerly provided structured XML dumps of all of their content, since July of 2024, StackExchange has stopped publishing dumps to the Internet Archive. Instead, each site can provide logged-in user with custom URL to download the dump for that site. This means that dumps for defunct sites like windowsphone.stackexchange.com are inaccessible. Additionally, in dumps produced by the new export tool, many questions that are available in past dumps (and accessible on the site) are not present. We therefore extract all questions and answers from community uploaded dumps from December of 2024 from the Internet Archive and additionally extract missing questions and answers from the last official dumps in July of 2024 to account for the deficiencies listed above. We use question, its comments, its answers and the comments on each answer as single document. Following the display order on StackExchange, answers are ordered by the number of votes they received, with the exception that the accepted answer always appears first. PyMarkdown was used to convert each comment into plain text. GitHub Archive According to GitHubs terms of service, issues and pull request descriptions along with their commentsinherit the license of their associated repository. To collect this data, we used the GitHub Archives public BigQuery table of events to extracted all issue, pull request, and comment events since 2011 and aggregated them into threads. The table does not include edit events so the text from each comment is the original from when it was first posted. We filtered out comments from bots. This resulted in approximately 177 million threads across 19 million repositories. We then removed threads whose repositories did not have Blue Oak Council-approved license. License information for each repository comes from either 1) the public-data:github_repos BigQuery Table, 2) metadata from the StackV2, or 3) the GitHub API. License filtering left 10 million repositories. PyMarkdown was used to convert from GitHub-flavored markdown to plain text. When parsing failed, the raw markdown was kept. Ubuntu IRC Logs of all discussions on the Ubuntu-hosted Internet Relay Chat (IRC) since 2004 have been archived and released into the Public Domain. We downloaded all chats from all channels up until March of 2025. We consider all messages for given channel on given day as single document. We removed system messages as well as those from known bots. 29 B.3 Government and Legal Texts Governments produce vast amount of informational text, ranging from legislation and legal opinions to scientific reports, public communications, and regulatory notices. This content is explicitly intended to inform the public, and as such, in many jurisdictions it is published directly into the public domain or under open licenses. In the United States, for example, works authored by federal employees as part of their official duties are not subject to copyright. Government and legal texts offer language models exposure to formal argumentation, legal reasoning, and procedural language. US Government Publishing Office The United States Government Publishing Office (USGPO) is federal agency responsible for disseminating official documents authored by the U.S. government. The Common Pile v0.1 includes all plain-text documents made available through the USGPOs GovInfo.gov developer API. This collection comprises over 2.7 million documents, spanning issues of the Federal Register, congressional hearing transcripts, budget reports, economic indicators, and other federal publications. US Patents and Trademark Office In the US, patent documents are released into the public domain as government works. Patents follow highly standardized format with distinct required sections for background, detailed description, and claims. We include parents from the US Patents and Trademark Office (USPTO) as provided by the Google Patents Public Data dataset [77], which includes millions of granted patents and published patent applications dating back to 1782. We processed these documents to extract clean text while preserving this structured format. Mathematical expressions and equations were converted into LATEX. Caselaw Access Project and Court Listener The Common Pile contains 6.7 million cases from the Caselaw Access Project and Court Listener. The Caselaw Access Project consists of nearly 40 million pages of U.S. federal and state court decisions and judges opinions from the last 365 years. In addition, Court Listener adds over 900 thousand cases scraped from 479 courts. The Caselaw Access Project and Court Listener source legal data from wide variety of resources such as the Harvard Law Library, the Law Library of Congress, and the Supreme Court Database. From these sources, we only included documents that were in the public domain. Erroneous OCR errors were further corrected after digitization, and additional post-processing was done to fix formatting and parsing. UK Hansard Hansard represents the official record of parliamentary proceedings across the United Kingdoms legislative bodies. The Common Pile incorporates records from multiple sources, including debates and written answers from the UK Commons and Lords, devolved legislatures (Scottish Parliament, Senedd in both English and Welsh, Northern Ireland Assembly), London Mayors Questions, and ministerial statements. Data was sourced from ParlParse [131], covering Commons debates from 1918 forward and Lords proceedings from the 1999 reform. Each document was processed to preserve complete parliamentary sessions as cohesive units, maintaining the natural flow of debate. All content is published under the Open Parliament License [181]. Regulations.gov Regulations.gov is an online platform operated by the U.S. General Services Administration that collates newly proposed rules and regulations from federal agencies along with comments and feedback from the general public. The Common Pile includes all plain-text regulatory documents published by U.S. federal agencies on this platform, acquired via the bulk download interface provided by Regulations.gov. B.4 Curated Task Data Curated datasets that cover specific tasks such as question answering, summarization, or text classification are often released via open licenses to the research community. While not traditionally part of pretraining corpora, including small amount of task-oriented data during pretraining can help models acquire early familiarity with task formats and promptcompletion structures. Data Provenance Initiative The Data Provenance Initiative is digital library of supervised datasets that have been manually annotated with their source and license information [106, 109]. We leverage their tooling to filter HuggingFace datasets, based on range of criteria, including their licenses, which may be particularly relevant for supervised datasets [114]. Specifically, we filter the data according to these criteria: contains English language or code data, the text is not model-generated, the datasets audit yielded open license and the original sources of the data are only from recognized public domain sources. 30 B.5 Books in the Public Domain Books represent time-tested resource for language model pretraining, offering carefully edited, long-form prose that supports learning of narrative coherence and long-range dependency modeling. For these reasons, many large-scale pretraining corporaincluding the Pile [57], Dolma [167], and RedPajama [185]include content from books [41]. In the United States, as of 2024, books published prior to 1929 are in the public domain. Thus, the Common Pile includes public domain books drawn from curated collections, covering topics such as literature, science, and history. Biodiversity Heritage Library The Biodiversity Heritage Library (BHL) is an open-access digital library for biodiversity literature and archives. The Common Pile contains over 42 million public domain books and documents from the BHL collection. These works were collected using the bulk data download interface provided by the BHL and were filtered based on their associated license metadata. We use the optical character recognition (OCR)-generated text distributed by BHL. Pre-1929 Books Books published in the US before 1929 passed into the public domain on January 1, 2024. We used the bibliographic catalog Hathifiles produced by HathiTrust to identify digitized books which were published in the US before 1929. The collection contains over 130,000 books digitized and processed by the Internet Archive on behalf of HathiTrust member libraries. The OCR plain text files were downloaded directly from the Internet Archive website. Library of Congress The Library of Congress (LoC) curates collection of public domain books called Selected Digitized Books. We downloaded over 130,000 English-language books from this public domain collection as OCR plain text files using the LoC APIs. Project Gutenberg Project Gutenberg is an online collection of over 75,000 digitized books available as plain text. We use all books that are 1) English and 2) marked as in the Public Domain according to the provided metadata. Additionally, we include any books that are part of the pg19 [140] dataset, which only includes books that are over 100 years old. Minimal preprocessing is applied to remove the Project Gutenberg header and footers, and many scanned books include preamble information about who digitized them. B.6 Open Educational Resources Open Educational Resources (OERs) are educational materials, typically published under open licenses, to support free and equitable access to education. These resources include educational artifacts such as textbooks, lecture notes, lesson plans, syllabi, and problem sets. For language models, OERs offer exposure to instructional formatting and domain-specific information, making them valuable for improving performance on knowledge-based downstream tasks. The Common Pile includes range of such materials sourced from major OER repositories, including collections of open-access books and structured teaching resources. Directory of Open Access Books The Directory of Open Access Books (DOAB) is an online index of over 94,000 peer-reviewed books curated from trusted open-access publishers. To collect the openly licensed content from DOAB, we retrieve metadata using their official metadata feed. We then filter the collection to include only English-language books released under CC BY and CC BY-SA licenses. The filtered books are downloaded in PDF format and converted to plaintext using the Marker PDF-to-text converter. As an additional validation step, we manually create whitelist of open license statements and retain only texts explicitly containing one of these statements in their frontor back-matter. PressBooks PressBooks is searchable catalog of over 8,000 open access books. To collect openly licensed content from PressBooks we construct search query to retrieve URLs for all books written in English and listed as public domain or under CC BY or CC BY-SA licenses. For each matched book, we collect its contents directly from the publicly available web version provided by PressBooks. OERCommons OERCommons is an online platform where educators share open-access instructional materialssuch as textbooks, lesson plans, problem sets, course syllabi, and worksheetswith the goal of expanding access to affordable education. To collect the openly licensed content available on OERCommons, we construct search query to retrieve English-language content released into the public domain or under CC BY or CC BY-SA licenses. The resulting documents are converted to plain text directly from the HTML pages hosted on the OERCommons website. 31 LibreTexts LibreTexts is an online platform that provides catalog of over 3,000 open-access textbooks. To collect openly licensed content from LibreTexts we gather links to all textbooks in the catalog and check each textbook section for license statement indicating that it is in the public domain or under CC BY, CC BY-SA, or the GNU Free Documentation License. We extract plain text from these textbook sections directly from the HTML pages hosted on the LibreTexts website. B.7 Wikis Wikis are collaboratively maintained websites that organize information around specific topics or domains. Their crowd-sourced nature, coupled with community moderation and citation requirements, often results in text that is both informative and well-structured. Prominent examples such as Wikipedia have become staples in large-scale language model pretraining corpora due to their breadth of coverage and high quality. In addition, most major wikis are distributed under open licenses such as CC BY and CC BY-SA. The Common Pile includes content from range of openly licensed wikis to provide models with structured and well-researched informational text. Wikimedia We downloaded the official database dumps from March 2025 of the English-language wikis that are directly managed by the Wikimedia foundation (see Appendix for complete list). These database dumps include the wikitextMediawikis custom markup languagefor each page as well as talk pages, where editors discuss changes made to page. We only use the most recent version of each page. We converted wikitext to plain text using wtf_wikipedia after light adjustments in formatting to avoid errors in section ordering caused by bug. Before parsing, we converted wikitext math into LATEX math using our custom code. Finally, any remaining HTML tags were removed via regexes. Wikiteam There are many wikis on the internet that are not managed by the Wikimedia foundation, but do use their MediaWiki software to power their wiki. Many of these wikis have been archived by wikiteam, collection of volunteers that create unofficial database dumps of wikis and upload them to the Internet Archive. We download all dumps made by wikiteam when the metadata indicates the wiki was licensed under CC BY, CC BY-SA, or released into the public domain on the Internet Archive in September of 2024. This results in downloading approximately 330,000 wikis. When multiple dumps of the same wiki exists, we use the most recent dump. The wikitext was converted to plain text following the same steps as with Wikimedia wikis. After preprocessing, we removed documents from wikis that appeared to contain large amounts of license laundering, e.g. those that were collections of song lyrics or transcripts. B.8 Source Code Source code has become an increasingly important component of large-scale language model pretraining corpora, as it enables models to learn syntax, program structure, and problem solving strategies useful for both code generation and reasoning tasks. Thanks to the Free and Open Source Software (FOSS) movement, code also happens to be one of the most openly licensed forms of text, with many software repositories distributed under open licenses such as MIT, BSD, Apache 2.0, and the GNU Free Documentation License (GFDL). The Common Pile includes high-quality, openly licensed source code from large-scale public code datasets and documentation standards, enabling models trained on it to perform better on coding and technical writing tasks. The Stack V2 The Stack V2 [113] consists of mixture of openly licensed and unlicensed work. We use the tooling that the Software Heritage Foundation and BigCode created to build our dataset. In particular, we relied on the license detection performed by the creators of Stack V2. When multiple licenses are detected in single repository, we make sure that all of them meet our definition of openly licensed. Python Enhancement Proposals Python Enhancement Proposals, or PEPs, are design documents that generally provide technical specification and rationale for new features of the Python programming language. There are been 661 PEPs published. The majority of PEPs are published in the Public Domain, but 5 were published under the Open Publication License and omitted. PEPs are long, highly-polished, and technical in nature and often include code examples paired with their prose. PEPs are authored in ReStructured Text; we used pandoc, version 3.5, to convert them to plain text. 32 B.9 Transcribed Audio Content historically underutilized source of text data is speech transcribed from audio and video content. Spoken language in educational videos, speeches, and interviews provide an opportunity for models to learn conversational speech patterns. Creative Commons YouTube YouTube is large-scale video-sharing platform where users have the option of uploading content under CC BY license. To collect high-quality speech-based textual content and combat the rampant license laundering on YouTube, we manually curated set of over 2,000 YouTube channels that consistently release original openly licensed content containing speech. The resulting collection spans wide range of genres, including lectures, tutorials, reviews, video essays, speeches, and vlogs. From these channels, we retrieved over 1.1 million openly licensed videos comprising more than 470,000 hours of content. Finally, each video was transcribed to text using the Whisper speech recognition model [138]. B.10 Web Text The success of modern LLM pre-training relies on text scraped indiscriminately from the web, as web text covers an extremely diverse range of textual domains. In the Common Pile, we restrict this approach to only include web content with clear public domain status or open license statements. Creative Commons Common Crawl We sourced text from 52 Common Crawl snapshots, covering about half of Common Crawl snapshots available to date and covering all years of operations of Common Crawl up to 2024. We found higher level of duplication across this collection, suggesting that including more snapshots would lead to modest increase in total token yield. From these snapshots, we extract HTML content using FastWarc [15]. Then, using regular expression adapted from the C4Corpus project [68], we retain only those pages where CC BY, CC BY-SA, or CC0 license appears. To ensure license accuracy, we manually verified the top 1000 domains by content volume, retaining only the 537 domains with confirmed licenses where the Creative Commons designation is applied to all text content rather than only embedded media or subset of the text on the domain. We extract the main content of these documents and remove boilerplate using Resiliparse [14]. We perform URL-level exact deduplication and use Bloom filters to remove nearduplicates with 80% ngram overlap. We also employ rule-based filters matching Dolma [167]; namely, we use C4-derived heuristics [142] to filter pages containing Javascript, Lorem Ipsum, and curly braces {}. We also apply all Gopher rules [141] to remove low-quality pages. We provide more details on the composition of this subset in Appendix G. Foodista Foodista is community-maintained site with recipes, food-related news, and nutrition information. All content is licensed under CC BY. Plain text is extracted from the HTML using custom pipeline that includes extracting title and author information to include at the beginning of the text. Additionally, comments on the page are appended to the article after we filter automatically generated comments. News We scrape the news sites that publish content under CC BY or CC BY-SA according to opennewswire. full list of sites can be found in Appendix E. Plain text was extracted from the HTML using our custom pipeline, including extraction of the title and byline to include at the beginning of each article. Public Domain Review The Public Domain Review is an online journal dedicated to exploration of works of art and literature that have aged into the public domain. We collect all articles published in the Public Domain Review under CC BY-SA license."
        },
        {
            "title": "C Additional insights on licensing",
            "content": "There are many standards we could have chosen for what licenses to include in our dataset. The open source, knowledge, and culture movements have harmonized on the high level principles described in section 1: open means that permission is granted for content to be freely used, studied, modified, and shared for any purpose. This language is found in the Open Knowledge Definition we follow as well as the Open Source Institutes Open Definition, Creative Commonss statement on Open Culture, Wikimedias Acceptable licenses policy and more. Our work was also developed to be consistent with the Open movements work in the specific context of AI technologies such as the 33 Open Source Initiatives Open Source AI Definition and in consultations with leading members of the community [9]. C.1 Why we cant always trust automatic license detection There are many reasons why identifying the licensing status of internet text with automatic tooling can be challenging. In this section, we briefly discuss some major themes from our experience. There are many ways to say the same thing. While there exist standards for how to express license, people dont always follow those standards and failure to follow the standards doesnt mean that the license is invalid. For example, simple string matching on CC BY misses huge amount of CC BY licensed text because very common way to denote Creative Commons licenses is using an image badge. Current web-processing tools are substantially stronger at identifying text than images, and the failure rate on sites using image badges is quite high. Lack of understanding of licenses. Most people are not lawyers and do not understand the full legal scope and meaning of the licenses that they attempt to put on their text. Developers routinely tweak boilerplate to produce ambiguous language like (Licensed under MIT-ish terms) or write contradictory statements (All rights reserved / CC-BY). In general, it is common for people to write quasi-legal language along side more traditional license. Non-standard licenses require substantial amounts of work to interpret and are not always valid or meaningful. Licensing signals can be noisy. Even when developer intends to clearly communicate specific license, contradictions and errors can occur in practice. For example, Longpre et al. [107] found that there were substantial disagreements between the terms of service of website and the restrictions found in robots.txt file. We have not yet found reliable way to have an automatic system identify licensed text and therefore frequently resort to manual review by humans."
        },
        {
            "title": "D List of Data Provenance Initiative sources",
            "content": "The openly licensed supervised datasets included in the Common Pile are listed in Table 1. These datasets were identified and collected using metadata from the Data Provenance Initiative. For more information on these datasets, consult the Data Provenance Initiative Dataset Explorer. Table 1: Supervised datasets included in the Common Pile from the Data Provenance Initiative collection. Collection AgentInstruct HelpSteer Aya Dataset CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT Dataset Identifier AgentInstruct-alfworld[164] HelpSteer[184] aya-english[165] Licenses MIT License CC BY 4.0 Apache License 2.0 commitpackft-abap[165] MIT License commitpackft-agda[165] MIT License, BSD 3-Clause License commitpackft-apl[165] commitpackft-arc[165] commitpackft-aspectj[165] MIT License, ISC License MIT License Apache License 2.0, BSD 3-Clause License, MIT License commitpackft-ats[165] Apache License 2.0, MIT License commitpackft-blitzmax[165] commitpackft-bluespec[165] commitpackft-boo[165] MIT License MIT License MIT License Continued on next page 34 Collection Dataset Identifier Licenses CommitPackFT commitpackft-brainfuck[165] Apache License 2.0, BSD 2-Clause License, MIT License CommitPackFT commitpackft-gdscript[165] CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT commitpackft-bro[165] MIT License, BSD 3-Clause License commitpackft-cartocss[165] MIT License commitpackft-chapel[165] Apache License 2.0, BSD 3-Clause License, MIT License commitpackft-clean[165] Apache License 2.0, MIT License commitpackft-coldfusion[165] Apache License 2.0, MIT License commitpackft-creole[165] Apache License 2.0, MIT License commitpackft-crystal[165] Apache License 2.0, MIT License commitpackft-dns-zone[165] MIT License, BSD 3-Clause License commitpackft-dylan[165] commitpackft-eiffel[165] MIT License MIT License commitpackft-emberscript[165] Apache License 2.0, BSD 3-Clause License, MIT License commitpackft-fancy[165] MIT License, BSD 3-Clause License commitpackft-flux[165] Apache License 2.0, MIT License commitpackft-forth[165] MIT License commitpackft-g-code[165] Apache License 2.0, BSD 3-Clause License, MIT License Apache License 2.0, CC0 1.0, MIT License commitpackft-genshi[165] Apache License 2.0, MIT License commitpackft-graphql[165] Apache License 2.0, BSD 3-Clause License, CC0 1.0, MIT License commitpackft-harbour[165] MIT License commitpackft-hlsl[165] Apache License 2.0, MIT License commitpackft-http[165] Apache License 2.0, MIT License commitpackft-idris[165] MIT License, BSD 3-Clause License, BSD 2-Clause License commitpackft-igor-pro[165] MIT License, BSD 3-Clause License commitpackft-inform-7[165] MIT License, BSD 3-Clause License commitpackft-ioke[165] MIT License commitpackft-isabelle[165] MIT License, BSD 2-Clause License commitpackft-jflex[165] MIT License commitpackft-json5[165] MIT License, BSD 3-Clause License, BSD 2-Clause License Apache License 2.0, BSD 3-Clause License, CC0 1.0, MIT License CommitPackFT commitpackft-jsonld[165] CommitPackFT CommitPackFT CommitPackFT CommitPackFT commitpackft-krl[165] commitpackft-latte[165] MIT License MIT License commitpackft-lean[165] Apache License 2.0, MIT License commitpackft-lfe[165] Apache License 2.0, MIT License Continued on next page Collection CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT Dataset Identifier Licenses commitpackft-lilypond[165] MIT License commitpackft-liquid[165] Apache License 2.0, CC0 1.0, MIT License commitpackft-literate-agda[165] MIT License commitpackft-literatecoffeescript[165] MIT License commitpackft-literate-haskell[165] MIT License, BSD 3-Clause License commitpackft-llvm[165] Apache License 2.0, BSD 3-Clause License, BSD 2-Clause License, MIT License Apache License 2.0, BSD 3-Clause License, BSD 2-Clause License, MIT License, ISC License CommitPackFT commitpackft-logos[165] CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT commitpackft-lsl[165] MIT License, BSD 3-Clause License commitpackft-maple[165] MIT License, BSD 3-Clause License commitpackft-mathematica[165] MIT License, CC0 1.0 commitpackft-metal[165] Apache License 2.0, MIT License commitpackft-mirah[165] Apache License 2.0, MIT License commitpackft-monkey[165] Apache License 2.0, MIT License commitpackft-moonscript[165] MIT License commitpackft-mtml[165] MIT License commitpackft-mupad[165] commitpackft-nesc[165] commitpackft-netlinx[165] commitpackft-ninja[165] commitpackft-nit[165] commitpackft-nu[165] Apache License 2.0, BSD 3-Clause License, MIT License MIT License MIT License Apache License 2.0, BSD 3-Clause License, MIT License Apache License 2.0, MIT License Apache License 2.0, MIT License commitpackft-ooc[165] MIT License commitpackft-openscad[165] MIT License, CC0 1.0, BSD 2-Clause License commitpackft-oz[165] MIT License, BSD 2-Clause License commitpackft-pan[165] Apache License 2.0, MIT License commitpackft-piglatin[165] Apache License 2.0, MIT License commitpackft-pony[165] MIT License, BSD 2-Clause License commitpackft-propeller-spin[165] MIT License commitpackft-pure-data[165] MIT License commitpackft-purebasic[165] MIT License, BSD 3-Clause License commitpackft-purescript[165] Apache License 2.0, BSD 3-Clause License, MIT License commitpackft-ragel-in-rubyhost[165] MIT License CommitPackFT commitpackft-rebol[165] Apache License 2.0, MIT License Continued on next page 36 Collection CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT Dataset Identifier Licenses commitpackft-red[165] MIT License, BSD 2-Clause License commitpackft-rouge[165] Apache License 2.0, MIT License commitpackft-sage[165] commitpackft-sas[165] MIT License MIT License commitpackft-scaml[165] MIT License, BSD 2-Clause License commitpackft-scilab[165] MIT License, BSD 3-Clause License commitpackft-slash[165] Apache License 2.0, MIT License commitpackft-smt[165] MIT License, BSD 3-Clause License commitpackft-solidity[165] Apache License 2.0, MIT License commitpackft-sourcepawn[165] Apache License 2.0, MIT License commitpackft-squirrel[165] commitpackft-ston[165] MIT License MIT License commitpackft-systemverilog[165] Apache License 2.0, BSD 3-Clause License, MIT License Apache License 2.0, BSD 3-Clause License, BSD 2-Clause License, MIT License, ISC License, CC0 1.0 CommitPackFT commitpackft-unity3d-asset[165] CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT CommitPackFT Dolly 15k Dolly 15k Dolly 15k Dolly 15k Dolly 15k Dolly 15k Dolly 15k DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio commitpackft-uno[165] MIT License commitpackft-unrealscript[165] MIT License commitpackft-urweb[165] MIT License, BSD 3-Clause License commitpackft-vcl[165] Apache License 2.0, BSD 3-Clause License, MIT License commitpackft-xbase[165] Apache License 2.0, MIT License commitpackft-xpages[165] Apache License 2.0, MIT License commitpackft-xproc[165] Apache License 2.0, MIT License commitpackft-yacc[165] commitpackft-zephir[165] commitpackft-zig[165] dolly-brainstorming[165] dolly-classification[165] dolly-closedqa[165] dolly-creative_writing[165] dolly-infoextract[165] dolly-openqa[165] dolly-summarization[165] ds-ABCD[165] ds-ATIS[165] ds-ATIS-NER[165] ds-AirDialogue[165] ds-AntiScam[165] MIT License, ISC License, BSD 2-Clause License MIT License MIT License CC BY-SA 3.0 CC BY-SA 3.0 CC BY-SA 3.0 CC BY-SA 3.0 CC BY-SA 3.0 CC BY-SA 3. CC BY-SA 3.0 Apache License 2.0, MIT License Apache License 2.0, CC BY 4.0 Apache License 2.0, CC BY 4.0 Apache License 2.0 Apache License 2.0, CC0 1. ds-BANKING77[165] Apache License 2.0, CC BY 4.0 Continued on next page 37 Collection DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio Dataset Identifier Licenses ds-BANKING77-OOS[165] Apache License 2.0, CC BY 4.0 ds-BiTOD[165] Apache License 2.0 ds-CLINC-Single-Domain-OOSbanking[165] ds-CLINC-Single-Domain-OOScredit_cards[165] Apache License 2.0, CC BY 3.0 Apache License 2.0, CC BY 3.0 ds-CLINC150[165] Apache License 2.0, CC BY-SA 3.0 ds-CaSiNo[165] ds-CoQA[165] ds-CoSQL[165] ds-ConvAI2[165] Apache License 2.0, CC BY 4. Apache License 2.0, MIT License Apache License 2.0, CC BY-SA 4.0 Apache License 2.0 ds-CraigslistBargains[165] Apache License 2.0, MIT License ds-DART[165] ds-DSTC8-SGD[165] ds-DialogSum[165] Apache License 2.0, MIT License Apache License 2.0, CC BY-SA 4.0 Apache License 2.0, MIT License ds-Disambiguation[165] Apache License 2.0, MIT License ds-FeTaQA[165] ds-GECOR[165] ds-GrailQA[165] Apache License 2.0, CC BY-SA 4.0 Apache License 2.0, CC BY 4. Apache License 2.0 ds-HDSA-Dialog[165] Apache License 2.0, MIT License ds-HH-RLHF[165] ds-HWU64[165] ds-HybridQA[165] ds-KETOD[165] ds-MTOP[165] Apache License 2.0, MIT License Apache License 2.0, CC BY-SA 3.0 Apache License 2.0, MIT License Apache License 2.0, MIT License Apache License 2.0, CC BY-SA 4.0 ds-MULTIWOZ2_2[165] Apache License 2.0, MIT License ds-MulDoGO[165] Apache License 2.0, CDLA Permissive 1.0 ds-MultiWOZ_2.1[165] Apache License 2.0, MIT License ds-Prosocial[165] Apache License 2.0, MIT License ds-RESTAURANTS8K[165] Apache License 2.0, CC BY 4.0 Apache License 2.0, CC BY-SA 4. Apache License 2.0 Apache License 2.0 Apache License 2.0, CC BY-SA 4.0 Apache License 2.0, CC BY-SA 4.0 Apache License 2.0, MIT License Apache License 2.0, CC BY-SA 4. Apache License 2.0, CC BY-SA Apache License 2.0, CC BY-SA Apache License 2.0, CC BY 4.0 ds-SGD[165] ds-SNIPS[165] ds-SNIPS-NER[165] ds-SParC[165] ds-SQA[165] ds-STAR[165] ds-Spider[165] ds-TOP[165] ds-TOP-NER[165] ds-Taskmaster1[165] Continued on next page 38 Collection DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio DialogStudio Dataset Identifier Licenses ds-Taskmaster2[165] ds-Taskmaster3[165] ds-ToTTo[165] Apache License 2.0, CC BY 4.0 Apache License 2.0, CC BY 4.0 Apache License 2.0, CC BY-SA 3.0 ds-TweetSumm[165] Apache License 2.0, CC0 1.0 ds-WOZ2_0[165] ds-WebQSP[165] ds-WikiSQL[165] Apache License 2.0 Apache License 2.0, CC BY 4. Apache License 2.0, BSD 3-Clause License ds-WikiTQ[165] Apache License 2.0, CC BY-SA 4.0 ds-chitchat-dataset[165] Apache License 2.0, MIT License ds-wizard_of_internet[165] Apache License 2.0, CC BY 4.0 ds-wizard_of_wikipedia[165] Apache License 2.0, CC BY 4.0 Flan Collection (Chain-of-Thought) fc-cot-cot_gsm8k[34] Flan Collection (Chain-of-Thought) fc-cot-cot_strategyqa[59] MIT License CC BY-SA 3.0 Flan Collection (Chain-of-Thought) fc-cot-stream_creak[125] MIT License, CC BY-SA 4. Flan Collection (Chain-of-Thought) fc-cot-stream_esnli[21] MIT License, CC BY-SA 4.0 Flan Collection (Flan 2021) fc-flan-drop[46] Flan Collection (Flan 2021) fc-flan-e2e_nlg[123] CC BY 4.0 CC BY-SA 4.0 Flan Collection (Flan 2021) fc-flan-natural_questions[89] Apache License 2.0, CC BY-SA 3. Flan Collection (Flan 2021) fc-flan-quac[29] Flan Collection (Flan 2021) fc-flan-squad_v1[147] Flan Collection (Flan 2021) fc-flan-squad_v2[147] Flan Collection (Flan 2021) fc-flan-trec[100] Flan Collection (Flan 2021) fc-flan-true_case[100] CC BY-SA 4.0 CC BY-SA 4. CC BY-SA 4.0 CC0 1.0 CC0 1.0 Flan Collection (Flan 2021) fc-flan-wiki_lingua_english_en[90] CC BY 3. Flan Collection (Flan 2021) fc-flan-winogrande[158] Apache License 2.0, CC BY 4.0 Flan Collection (Flan 2021) fc-flan-wnli[98] Flan Collection (Flan 2021) fc-flan-word_segment[98] Flan Collection (Flan 2021) fc-flan-wsc[98] CC BY 4.0 CC0 1.0 CC BY 4. Flan Collection (P3) Flan Collection (P3) Flan Collection (P3) Flan Collection (P3) Flan Collection (P3) Flan Collection (P3) Flan Collection (P3) Flan Collection (P3) Flan Collection (P3) Flan Collection (P3) Flan Collection (Super-NaturalInstructions) fc-p3-adversarial_qa[11] CC BY-SA 3.0 BSD 3-Clause License CC BY-SA 3.0 Apache License 2.0, CC BY-SA 4.0 CC BY 4.0 CC BY 4. CC BY 4.0 CC BY 4.0 CC BY-SA 3.0 CC BY-SA 3.0 CC BY-SA 3.0 fc-p3-cos_e[144] fc-p3-dbpedia_14[97] fc-p3-hotpotqa[190] fc-p3-quarel[153] fc-p3-quartz[170] fc-p3-quoref[44] fc-p3-web_questions[13] fc-p3-wiki_bio[93] fc-p3-wiki_hop[93] fc-sni-adversarial_qa[11] Continued on next page 39 Collection Dataset Identifier Licenses Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) fc-sni-adverserial_qa[11] MIT License fc-sni-air_dialogue[187] Apache License 2. fc-sni-ancora_ca_ner[187] CC BY 4.0 fc-sni-anem[124] MIT License, CC BY-SA 3.0 fc-sni-argkp Apache License 2.0, CC BY-SA 3. fc-sni-asian_language_treebank[151] fc-sni-atomic[75] CC BY 4.0 CC BY 4.0 fc-sni-bard[54] Apache License 2. fc-sni-cedr[161] Apache License 2.0 fc-sni-circa[112] CC BY-SA 4.0 fc-sni-clue_cmrc2018[42] CC BY-SA 4. fc-sni-coached_conv_pref[139] CC BY 4.0 fc-sni-copa_hr BSD 2-Clause License fc-sni-crows_pairs[122] CC BY-SA 4. fc-sni-cuad[71] CC BY 4.0 fc-sni-defeasible_nli_atomic[155] MIT License fc-sni-disfl_qa[67] CC BY 4. fc-sni-e_snli[21] MIT License fc-sni-gap[186] Apache License 2.0 fc-sni-hotpotqa[190] Apache License 2.0, CC BY-SA 4. fc-sni-human_ratings_of_natural_language_generation_outputs[190] CC BY 4.0 fc-sni-hybridqa[26] CC BY 4.0, MIT License fc-sni-iirc[53] CC BY 4. fc-sni-jigsaw[53] CC0 1.0 Continued on next page 40 Collection Dataset Identifier Licenses Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) fc-sni-librispeech_asr[127] CC BY 4.0 fc-sni-logic2text[27] MIT License fc-sni-numeric_fused_head[49] MIT License fc-sni-offenseval_dravidian[49] CC BY 4.0 fc-sni-open_pi[172] CC BY 4.0 fc-sni-paper_reviews_data_set[172] CC BY 4.0 fc-sni-poem_sentiment[163] CC BY 4.0 fc-sni-propara[43] Apache License 2.0 fc-sni-quarel[153] fc-sni-quartz[170] fc-sni-quoref[44] CC BY 4.0 CC BY 4.0 CC BY 4.0 fc-sni-ro_sts_parallel[48] CC BY-SA 4.0 fc-sni-schema_guided_dstc8[148] CC BY-SA 4.0 fc-sni-scitail[86] Apache License 2.0 fc-sni-scitailv1.1[86] Apache License 2.0 fc-sni-semeval_2020_task4[86] CC BY-SA 4.0 fc-sni-sms_spam_collection_v.1[86] CC BY 4.0 fc-sni-splash[86] CC BY-SA 4.0 fc-sni-squad2.0[147] CC BY-SA 4.0 fc-sni-squad_1.1[146] CC BY-SA 4.0 fc-sni-strategyqa[59] MIT License fc-sni-universal_dependencies___english_dependency_treebank[59] CC BY-SA 4.0 fc-sni-web_questions[13] CC BY 4.0 fc-sni-wiki_hop[93] CC BY-SA 3.0 Continued on next page 41 Collection Dataset Identifier Licenses Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) Flan Collection (Super-NaturalInstructions) fc-sni-wikitext[116] CC BY-SA 3.0 fc-sni-winograd_wsc[98] CC BY 4. fc-sni-winomt[168] MIT License fc-sni-winowhy[194] MIT License fc-sni-wsc; enhanced_wsc[194] CC BY 4. fc-sni-wsc_fiexed[194] CC BY-SA 3.0 fc-sni-xcopa[134] CC BY 4.0 fc-sni-xquad[6] CC BY-SA 4. Open Assistant oasst-en[88] Apache License 2.0, CC BY 4.0 Open Assistant OctoPack oasst-en-octopack[88] Apache License 2.0, CC BY 4. Open Assistant v2 oasst2-en[88] OIG OIG OIG OIG OIG OIG OIG Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct oig-unified_canadian_parliament[88] oig-unified_cuad[88] oig-unified_grade_school_math_instructions[88] oig-unified_nq[88] oig-unified_sqlv1[88] oig-unified_sqlv2[88] oig-unified_squad_v2_more_neg[88] tsi-balanced_copa[85] tsi-breaking_nli[60] tsi-cladder[60] tsi-condaqa[149] tsi-conj_nli[157] tsi-defeasible_nli-atomic[155] tsi-defeasible_nli-snli[155] tsi-dynasentdynabench.dynasent.r1.all-r1[135] tsi-dynasentdynabench.dynasent.r2.all-r2[135] Apache License 2.0 Apache License 2.0 Apache License 2.0, CC BY 4.0 Apache License 2.0, MIT License Apache License 2.0, CC BY-SA 3.0 Apache License 2.0, CC BY-SA 4.0 Apache License 2.0, CC BY-SA 4.0 Apache License 2.0, CC BY-SA 4.0 BSD 2-Clause License CC BY-SA 4. MIT License Apache License 2.0 MIT License MIT License MIT License CC BY 4. CC BY 4.0 tsi-fever_evidence_related-mwong__fever_related[177] CC BY-SA 4.0 tsi-few_nerd-supervised[45] CC BY-SA 4.0 tsi-fig_qa[102] tsi-fracas[56] MIT License MIT License Continued on next page 42 Collection Dataset Identifier Licenses Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct Tasksource Instruct tsi-hyperpartisan_news[56] CC BY 4.0 tsi-lex_glue-case_hold[23] Apache License 2.0 tsi-lonli[174] tsi-moral_stories-full[51] tsi-neqa[51] tsi-prost MIT License MIT License CC BY 4.0 Apache License 2.0 tsi-quote_repetition CC BY 4.0 tsi-recast-recast_factuality CC BY-SA 4.0 tsi-recast-recast_megaveridicality CC BY-SA 4.0 tsi-recast-recast_ner tsi-recast-recast_puns tsi-recast-recast_sentiment tsi-recast-recast_verbcorner tsi-recast-recast_verbnet tsi-redefine_math tsi-tracie[196] tsi-truthful_qa-multiple_choice[101] CC BY-SA 4. CC BY-SA 4.0 CC BY-SA 4.0 CC BY-SA 4.0 CC BY-SA 4.0 CC BY 4.0 Apache License 2. Apache License 2.0 tsi-vitaminc-tals__vitaminc[162] MIT License tsi-winowhy[194] Tasksource Symbol-Tuning tsy-breaking_nli[60] Tasksource Symbol-Tuning tsy-cladder[60] Tasksource Symbol-Tuning Tasksource Symbol-Tuning tsy-condaqa[149] tsy-conj_nli[157] Tasksource Symbol-Tuning tsy-defeasible_nli-atomic[155] Tasksource Symbol-Tuning tsy-defeasible_nli-snli[155] Tasksource Symbol-Tuning Tasksource Symbol-Tuning Tasksource Symbol-Tuning tsy-dynasentdynabench.dynasent.r1.all-r1[135] tsy-dynasentdynabench.dynasent.r2.all-r2[135] tsy-fever_evidence_relatedmwong__fever_related[177] Tasksource Symbol-Tuning tsy-fracas[56] Tasksource Symbol-Tuning tsy-hyperpartisan_news[56] Tasksource Symbol-Tuning tsy-lonli[174] MIT License CC BY-SA 4. MIT License Apache License 2.0 MIT License MIT License MIT License CC BY 4. CC BY 4.0 CC BY-SA 4.0 MIT License CC BY 4.0 MIT License Tasksource Symbol-Tuning tsy-recast-recast_factuality[174] CC BY-SA 4.0 Tasksource Symbol-Tuning tsy-recast-recast_megaveridicality[174] Tasksource Symbol-Tuning tsy-recast-recast_ner[174] Tasksource Symbol-Tuning tsy-recast-recast_puns[174] CC BY-SA 4.0 CC BY-SA 4.0 CC BY-SA 4.0 Tasksource Symbol-Tuning tsy-recast-recast_sentiment[174] CC BY-SA 4.0 Tasksource Symbol-Tuning tsy-recast-recast_verbcorner[174] CC BY-SA 4.0 Continued on next page 43 Collection Dataset Identifier Licenses Tasksource Symbol-Tuning tsy-recast-recast_verbnet[174] CC BY-SA 4.0 Tasksource Symbol-Tuning tsy-tracie[196] Apache License 2.0 Tasksource Symbol-Tuning tsy-vitaminc-tals__vitaminc[162] MIT License Tasksource Symbol-Tuning tsy-winowhy[194] MIT License"
        },
        {
            "title": "E List of News sources",
            "content": "The Common Pile contains variety of openly licensed news sources released under CC BY and CC BY-SA licenses. The sources licensed under CC BY include: 360info, Africa is Country, Alt News, Balkan Diskurs, Factly, Freedom of the Press Foundation, Agenzia Fides, Global Voices, Meduza, Mekong Eye, Milwaukee Neighborhood News Service, Minority Africa, New Canadian Media, SciDev.Net, The Solutions Journalism Exchange, Tasnim News Agency, and ZimFact. The sources licensed under CC BY-SA include: Oxpeckers, Propastop, and The Public Record."
        },
        {
            "title": "F List of WikiMedia wikis",
            "content": "Official Wikimedia wikis are released under CC BY-SA license. The Common Pile includes the following Wikimedia wikis: Wikipedia, Wikinews, Wikibooks, Wikiquote, Wikisource, Wikiversity, Wikivoyage, and Wiktionary."
        },
        {
            "title": "G CCCC Source Statistics",
            "content": "We provide additional statistics on the CCCC subset of the Common Pile, including the number of unicode words and documents sourced from each Common Crawl snapshot, in Table 2. Table 2: Counts of words and documents extracted from 52 snapshots after filtering with our pipeline. Snapshot Unicode Words Documents CC-MAIN-2013CC-MAIN-2013-48 CC-MAIN-2014-10 CC-MAIN-2014-15 CC-MAIN-2014-23 CC-MAIN-2014-35 CC-MAIN-2014CC-MAIN-2014-42 CC-MAIN-2014-49 CC-MAIN-2014-52 CC-MAIN-2015-06 CC-MAIN-2015-11 CC-MAIN-20153,851,018,197 4,544,197,252 4,429,217,941 4,059,132,873 5,193,195,765 4,254,690, 4,289,814,449 3,986,284,741 3,316,075,452 4,307,765,289 3,675,982,679 3,932,442, 3,658,107,765 Continued on next page 44 5,529,294 6,997,831 6,682, 5,912,779 8,253,690 6,551,673 6,558,170 6,144,797 4,699, 6,338,983 5,181,955 5,438,533 4,954,273 Snapshot Unicode Words Documents CC-MAIN-2015-18 CC-MAIN-2015-22 CC-MAIN-2015-27 CC-MAIN-2016-07 CC-MAIN-2016CC-MAIN-2016-22 CC-MAIN-2017-04 CC-MAIN-2017-09 CC-MAIN-2017-13 CC-MAIN-2017-17 CC-MAIN-2017CC-MAIN-2017-26 CC-MAIN-2017-51 CC-MAIN-2018-13 CC-MAIN-2018-22 CC-MAIN-2018-26 CC-MAIN-2018CC-MAIN-2018-34 CC-MAIN-2018-47 CC-MAIN-2018-51 CC-MAIN-2019-04 CC-MAIN-2019-09 CC-MAIN-2019CC-MAIN-2019-30 CC-MAIN-2019-35 CC-MAIN-2019-39 CC-MAIN-2020-29 CC-MAIN-2020-34 CC-MAIN-2021CC-MAIN-2021-39 CC-MAIN-2021-43 CC-MAIN-2021-49 CC-MAIN-2022-05 CC-MAIN-2023-06 CC-MAIN-2023CC-MAIN-2023-23 CC-MAIN-2023-50 CC-MAIN-2024-10 CC-MAIN-2024-18 4,451,734,946 4,285,945, 3,639,904,128 1,588,496,703 3,228,754,200 3,217,827,676 3,852,699,213 4,186,915, 4,950,110,931 4,684,050,830 4,683,569,278 4,744,689,137 1,981,004,306 4,816,417, 3,921,533,251 4,506,583,931 4,936,722,403 3,865,953,978 3,933,439,841 4,745,124, 4,475,679,190 4,287,868,800 3,966,330,348 4,179,526,188 5,144,426,270 4,572,972, 5,200,565,501 4,458,827,947 1,768,757,386 4,599,961,675 5,337,349,331 3,980,018, 4,517,850,019 5,135,614,227 5,117,143,765 5,461,486,807 5,881,860,014 5,164,171, 4,745,457,054 6,319,757 5,949,267 4,975,152 3,798,207 4,446, 4,242,762 5,239,605 5,119,171 5,923,670 5,645,725 5,514, 5,514,047 2,529,289 5,520,099 4,401,956 4,916,546 5,282, 3,808,725 3,637,947 4,616,832 4,140,277 4,142,190 3,849, 4,430,572 5,048,106 4,527,430 4,984,248 4,297,009 1,824, 4,287,356 5,304,846 4,050,641 4,503,863 4,959,915 4,675, 4,869,627 4,901,306 4,335,071 3,949,186 Total 221,715,271, 259,728,610 45 PeS2o Source Statistics Additional statistics on the composition of the peS2o subset of the Common Pile can be found in Table 3 and Table 4. Table 3: Distribution of licenses in the peS2o subset."
        },
        {
            "title": "Train Split Validation Split",
            "content": "CC BY CC BY-SA CC0 Public domain 6,088,325 120,150 36,373 10,060 37,754 1,231 121 6 Table 4: Distribution of papers across 23 fields of study, as identified by the Semantic Scholar API [87]. paper may belong to one or more fields of study. Field of Study Train Split Validation Split Medicine Biology Environmental Science Engineering Computer Science Materials Science Physics Chemistry Psychology Education Business Economics Agricultural and Food Sciences Sociology Mathematics Political Science Geology Geography Linguistics History Law Philosophy Art 2,435,244 1,518,478 993,499 656,021 462,320 416, 413,461 406,429 364,441 220,014 193,536 185, 333,776 137,257 135,676 106,748 67,258 44, 41,737 36,848 30,888 27,518 26,658 23,734 8,879 7,601 5,005 3,003 3, 1,285 2,781 2,126 1,532 946 2,013 1,535 199 378 217 228 192 251"
        },
        {
            "title": "I Growth rates of openly licensed data",
            "content": "Over time, the volume of openly licensed data continues to grow as more creators release content under open licenses. In Figure 6, we quantify this growth between 2010 and 2024 by analyzing subsets of the Common Pile for which reliable creation date metadata is available. We plot the cumulative proportion of data created up to various cutoff dates and find that approximately half of the Common Pile (around 3.8TB) was created since 2020. This trend provides insight into the growing availability of openly licensed data and suggests promising trajectory for future LLMs trained entirely on openly licensed sources. Figure 6: The amount of openly licensed text grows steadily over time. We visualize the cumulative proportion of data created up to various cutoff dates for sources in the Common Pile with reliable creation date metadata. This includes all sources except for the Caselaw Access Project, Data Provenance Initiative, and the sources covering early 20th century Public Domain books."
        },
        {
            "title": "J Details on filtering pipelines",
            "content": "In subsection 4.1, we detail the steps used to produce the Comma v0.1 training dataset from the raw text in the Common Pile. These include applying filters based on language, text quality, length, likelihood, and toxicity; removing various forms of PII; and removal of source-specific boilerplate text using regular expressions. The Common Pile contains diverse range of sources and we therefore design separate filtering thresholds for each source. The exact source-specific thresholds used to post-process the Common Pile can be found in Table 5. Additionally, statistics on the preand post-filtered sizes of each source can be found in Table 6. Table 5: Pre-processing pipelines applied to each source in the Common Pile to construct the Comma dataset. Source Language Text Quality Doc Length Log-Likelihood Toxicity PII Regex Filter ArXiv Abstracts ArXiv Papers Biodiversity Heritage Library Caselaw Access Project CC Common Crawl > 0.5 > 0.5 > 100 > 100 > 0. > 0.0001 > 100 > -20 > 0.1 > 0. N N N Continued on next page Source Language Text Quality Doc Length Log-Likelihood Toxicity PII Regex Filter Data Provenance Initiative Database of Open Access Books Foodista GitHub Archive Library of Congress LibreTexts News OERCommons peS2o Pre-1929 Books PressBooks Project Gutenberg Public Domain Review PubMed PEPs Regulations.gov StackExchange Ubuntu IRC UK Hansard USGPO USPTO Wikimedia Wikiteam CC YouTube > 0. > 0.5 > 0.5 > 0.5 > 0.5 > 0. > 0.5 > 0.5 > 0. > 0.5 > 0.5 > 0.5 > 0.5 > 0.5 > 0.5 > > 100 > 100 > 700 > 100 > > 600 > 100 > > 100 > 100 > 100 > 100 > 700 > 100 > -20 > -20 > -20 > -20 > 0. > 0.1 > 0.1 > 0.1 > 0. > 0.1 > 0.1 > 0.1 > 0.1 > 0.1 N Y Y Y Y Y Y N N N N N N N N Table 6: Raw and filtered sizes of the Common Piles constituent datasets. Source Raw Filtered Raw Filtered Document Count Size (GB) ArXiv Abstracts ArXiv Papers Biodiversity Heritage Library 2,538,935 321,336 42,418, 2,504,679 304,048 15,111,313 Caselaw Access Project 6,919,240 6,735, 2.4 21 96 78 2.4 35 77 Continued on next page 48 Source Raw Filtered Raw Filtered Document Count Size (GB) CC Common Crawl Data Provenance Initiative Directory of Open Access Books Foodista GitHub Archive Library of Congress LibreTexts News OERCommons peS2o Pre-1929 Books PressBooks Project Gutenberg Public Domain Review PubMed PEPs Regulations.gov StackExchange Stack Ubuntu IRC UK Hansard USGPO USPTO Wikimedia Wikiteam CC YouTube Total 51,054,412 9,688,211 474,445 72, 30,318,774 135,500 62,269 172,308 9,339 6,294, 137,127 106,881 71,810 1,412 4,068,867 225,196 33,415,400 218,364,133 329,115 51,552 2,732, 20,294,152 63,969,938 219,139,368 1,129,692 692,854,953 6,852, 3,508,518 403,992 65,640 23,358,580 129,052 40, 126,673 5,249 6,117,280 124,898 54,455 55, 1,406 3,829,689 655 208,301 30,987,814 69,588, 234,982 47,909 2,148,548 17,030,231 16,311,574 26,931, 998,104 260 7 12.5 0.09 54. 47.8 5.3 0.4 0.1 188.2 73. 1.5 26.2 0.007 158.9 0.01 6. 103.7 4774.7 6.3 10 74.5 1003. 90.5 437.5 21.5 58 3 0.08 40.4 35.6 3.6 0.3 0. 182.6 46.3 0.6 20.1 0.007 147. 0.01 5.1 89.7 259.9 5.3 9. 36.1 661.1 57.4 13.7 18.6 233,817, 7557.9 1838.3 Details on Commas pre-training data mixture We estimated the quality of each source in the Common Pile by training 1.7B-parameter model for 28B tokens on each source individually and evaluating the resulting models on the set of early signal tasks from [132]. In doing so, we found that the amount of text in each source was poorly correlated with text quality, motivating the use of heuristic mixing weights to up-/down-weight different sources in our pre-training mix. In Table 7 we list the pre-training mixture weights for each of the sources in the Common Pile. 49 Table 7: Overview of the data mixing used to up/down-weight individual sources in the Common Pile to construct the Comma v0.1-1T pre-training dataset. Comma v0.1-2T simply repeats this full mixture twice."
        },
        {
            "title": "Source",
            "content": "Size (GB) Repeats Effective Size (GB) Tokens (Billions)"
        },
        {
            "title": "OERCommons",
            "content": "peS2o Pre-1929 Books"
        },
        {
            "title": "PEPs",
            "content": "Regulations.gov"
        },
        {
            "title": "StackExchange",
            "content": "Stack V2 2.4 19.5 35.5 77.5 58. 3.4 12 0.08 40.4 6 0.25 1 6 6 6 6 35.6 0.25 3.6 0.25 0. 182.6 46.3 0.6 20.1 0.007 147. 0.01 5.1 89.7 259.9 6 6 6 1 6 1 1 6 6 6 2 14. 117 8.9 77.5 348.6 20.4 3. 0.360% 29.3 2.2 19.4 87.1 2.932% 0.220% 1.941% 8.716% 5.1 0.510% 18 1.801% 0.48 242.4 8.9 21. 1.5 0.3 1,095.6 46.3 3.6 20. 0.04 147.1 0.06 30.6 538.2 519. 0.12 60.6 0.012% 6.064% 2.2 0.220% 5.4 0.38 0.08 273.9 11.6 0. 5 0.01 36.8 0.02 7.6 134. 130 0.540% 0.038% 0.008% 27.409% 1.161% 0.090% 0.500% 0.001% 3.683% 0.002% 0.761% 13.469% 13.009%"
        },
        {
            "title": "Continued on next page",
            "content": ""
        },
        {
            "title": "Source",
            "content": "Size (GB) Repeats Effective Size (GB) Tokens (Billions)"
        },
        {
            "title": "CC YouTube",
            "content": "5.3 9.6 36.1 661.1 57.4 13. 18."
        },
        {
            "title": "Total",
            "content": "1838.3 6 6 0.25 0.25 4 1 31.8 57.6 165.3 344.4 54.8 18.6 7.9 14. 2.3 41.3 86.1 13.7 4.7 3997. 999.3 0.791% 1.441% 0.230% 4.133% 8.616% 1.371% 0.470% 100% Details on Commas cool-down data mixture Following Hu et al. [73], we end training with cool-down where we train on 37.7B tokens of high-quality data while linearly decaying the learning rate to 0. We provide the source mixture weights for this cool-down phase in Table 8. Table 8: Overview of the data mixing used to up/down-weight individual sources in the Common Pile to construct the training distribution for Comma v0.1-1Ts cool-down phase. Comma v0.1-2T simply repeats this full mixture twice."
        },
        {
            "title": "Source",
            "content": "Size (GB) Repeats Effective Size (GB) Tokens (Billions)"
        },
        {
            "title": "OERCommons",
            "content": "peS2o"
        },
        {
            "title": "Public Domain\nReview",
            "content": "19.5 58.1 3.4 12 0.08 3. 0.25 0.05 182.6 0.6 0.007 0. 0.3 2 2 2 2 2 0.1 2 2 9.8 17. 6.8 24 0.16 7.2 0.5 0. 18.3 1.2 0.014 2.4 4.4 1. 6.50% 11.63% 4.55% 6 16.04% 0. 1.8 0.13 0.03 4.6 0.3 0. 0.11% 0.48% 0.33% 0.07% 12.18% 0.77% 0.01%"
        },
        {
            "title": "Continued on next page",
            "content": ""
        },
        {
            "title": "Source",
            "content": "Size (GB) Repeats Effective Size (GB) Tokens (Billions)"
        },
        {
            "title": "StackExchange",
            "content": "Stack V"
        },
        {
            "title": "Total",
            "content": "0.01 89.7 259.9 57.4 679.4 0.25 0.1 0.4 0.02 22. 26.0 23 149.9 0.005 5.6 6. 5.7 37.5 0.02% 14.96% 17.04% 15.32% 100% Details on small-scale data ablations In subsection 4.3 we report results from series of small-scale data ablations where we identically trained 1.7B parameter models on various openly licensed and unlicensed datasets and evaluate their performance on the early signal tasks from Penedo et al. [132] to compare their data quality against the Common Pile. In Figure 7 we show how the performance of these models evolve over the course of their training run, highlighting that differences in data quality become apparent very early in training. Additionally, we provide exact numerical results for each model in Table 9, showing that the Common Pile has higher data quality than any previously released openly licensed datasets and the Pile, and nearly matches the data quality of the OSCAR dataset. To validate that this is not purely due to the presence of high-quality supervised fine-tuning data from the Data Provenance Initiative (DPI) data source, we also perform an ablation on the Common Pile excluding the DPI data and find that the final performance of this model is largely unchanged. Figure 7: model trained on the Comma dataset consistently outperforms models trained on other corpora of openly licensed text and outperforms the Pile on all but two tasks. We train identical 1.7B parameter models on 28B tokens from each dataset following Penedo et al. [132]."
        },
        {
            "title": "N Additional Comma results",
            "content": "We provide exact numerical results for Comma v0.1-1T and -2T alongside baseline models results across variety of knowledge, reasoning, and coding tasks in Table 10 and Table 11 respectively. We find that particularly on knowledge-based benchmarks (such as MMLU) and coding benchmarks, 52 Table 9: Commas training dataset has higher quality than previous openly-licensed datasets and unlicensed datasets like the Pile. In the small-scale (1.7B parameter) data ablation setting, we find that Commas training dataset yields better models than previous openly licensed datasets and the Pile, and nearly matches the performance of models trained on OSCAR. Additionally, we find that removing the high-quality supervised data from the Data Provenance Initiative has marginal affect on the Comma datasets overall quality. Dataset ARC MMLU KL3M OLC Common Corpus Comma (no DPI) Comma The Pile OSCAR FineWeb 31.8 33.1 34.2 37.7 38.0 37.0 35.4 38.0 26.3 27.5 27.0 28.7 29.5 27.8 27.6 29.1 HS 29.9 33.8 33.6 37.6 39.9 35.8 40.8 48.2 OBQA CSQA PIQA SIQA Avg. 28.4 27.4 30.2 31.0 32.4 28.6 30.4 34.2 26.8 27.7 26.4 30.8 29. 31.5 32.1 33.6 58.2 59.4 61.0 63.8 65.8 66.8 69.7 73.4 38.0 38.5 37.7 39.8 39.4 38.2 39.7 40.3 36.2 37.3 37.6 40.0 40. 39.6 40.9 43.7 Comma v0.1-1T and -2T outperform baseline models trained on an equivalent amount (1T or 2T tokens, respectively) of unlicensed text. Table 10: Comparison between Comma v0.1-1T and baseline models trained with similar resources (7 billion parameters, 1 trillion tokens) across variety of knowledge, reasoning, and coding benchmarks. Model ARC-C ARC-E MMLU BoolQ HS OBQA CSQA PIQA SIQA HEval MBPP Avg. RPJ-INCITE LLaMA StableLM MPT OpenLLaMA Comma v0.1-1T 42.8 44.5 50.8 46.5 44.5 52.8 68.4 67.9 65.4 70.5 67.2 68.4 27.8 34.8 45.2 30.2 40.3 42.4 68.6 75.4 71.7 74.2 72.6 75.7 70.3 76.2 75.6 77.6 72.6 62.6 49.4 51.2 48.2 48.6 50.8 47. 57.7 61.8 57.2 63.3 62.8 59.4 76.0 77.2 77.0 77.3 78.0 70.8 46.9 50.3 48.2 49.1 49.7 50.8 11.1 19.9 23.1 27.3 27.6 36.5 15.9 27.9 32.0 33.2 33.9 35.5 48.6 53.4 54.0 54.3 54.5 54. Qwen3 57.2 74.5 77.0 86.1 77. 50.8 66.4 78.2 55.0 94.5 67. 71.3 Table 11: Performance of Comma v0.1-2T and variety of budget-matched baseline models. Model ARC-C ARC-E MMLU BoolQ HS OBQA CSQA PIQA SIQA HEval MBPP Avg. OLMo Twin Llama 2 Comma v0.1 2T DeepSeekLLM 45.2 48.5 45.8 49. 67.5 69.5 71.8 67.7 28.2 45.8 49.8 48.5 71.7 80.2 78.6 71.7 73.4 76.2 64.4 74.1 48.0 48.4 46.2 52.0 61.8 62.8 64.0 66. 77.9 76.7 72.5 77.8 48.5 50.8 52.3 51.6 18.2 26.1 44.2 43.1 27.5 28.5 41.5 43.8 51.6 55.8 57.4 58."
        },
        {
            "title": "O Additional training runs",
            "content": "To explore the sensitivity of our Comma v0.1 results to hyperparameter choices, we perform series of additional 7B parameter/1T token training runs on AMD MI300A GPUs with slight alterations to the training recipe. Due to both desire to reach the same 1T token target rapidly, and the lower single-GPU throughput on the system available for these ablations, for all additional runs the the training batch size is 8.3M (223) versus the 2.1M (221) tokens per step of Comma v0.1. Unless otherwise specified, we did not use the two phase training process described in subsection 4.4 (i.e. no separate high-quality cooldown phase is run and we do not perform checkpoint averaging at the end of training and before evaluation). O.1 Ablations at 1T Tokens We first performed set of training runs for 125,000 steps, resulting in 1.048T total tokens (referred to as 1T for brevity). 53 8M Batch We perform run with nearly the same training hyperparameters as Comma v0.1-1T, except with larger 8M token batch size. We also use single phase training setup; the base data mixture  (Table 7)  is run for the entire duration to 1T tokens. The learning rate schedule is 2,000 steps of warm-up from 0 to peak of 1e 3 with 123,000 steps of decay to minimum of 1.8e 9. Curriculum In this experimental run, different data mixture is used in each of three training stages of equal duration (we also use the modified hyperparameters from 8M Batch ablation above). The first stage of the curriculum comprises data from only the Common Piles largest sources (mostly USPTO, Table 13). The second stage uses the same data mixture as Comma v0.1s main pre-training phase (phase I), but run for only 1/3 of the duration. Finally, the third and last stage of the curriculum up-weights Common Piles highest quality, benchmark-relevant sources  (Table 14)  . We provide exact numerical results for Comma v0.1 and alternate Comma runs performed with different hyperparameters and data mixture curricula across variety of knowledge, reasoning, and coding benchmarks in Table 12. We find that the 8M Batch and Curriculum ablations are roughly comparable on average to the main Comma v0.1-1T run, with the notable exception that both ablations slightly outperform Comma v0.1-1T on the coding benchmarks. We conclude that the benchmark results reported for Comma v0.1-1T in subsection 4.4 seem relatively robust to minor changes in training hyperparameters, dataset mixture curriculum (assuming similar amounts of most data splits appear at some time during training), and the software environment and GPU hardware used to train the model. Table 12: Comparison between our main Comma v0.1-1T training run and alternate runs performed with different hyperparameters and data mixture curricula across variety of knowledge, reasoning, and coding benchmarks. For Main, we report the performance of Comma v0.1-1T without averaging the cooldown checkpoints so that it is fair comparison."
        },
        {
            "title": "Model",
            "content": "ARC-C ARC-E MMLU BoolQ HS OBQA CSQA PIQA SIQA HEval MBPP Avg. Curriculum 45.2 47.2 8M Batch 50.8 Main 69.1 69.6 68.4 41.4 42.9 40.2 74.7 69.9 72.9 60.8 62.9 62. 46.8 47.0 46.2 59.1 56.9 59.5 70.5 70.4 71.0 48.6 50.5 51.2 38.1 36.8 32.1 34.6 37.2 34. 53.5 53.8 53.6 Table 13: Overview of the data mixing used to up/down-weight individual sources for the Stage 1 of the Curriculum ablation run. In this table we omit the size columns for brevity."
        },
        {
            "title": "Repeats",
            "content": "Tokens (Billions)"
        },
        {
            "title": "USPTO",
            "content": "Pre-1929 Books Stack V2 (HTML)"
        },
        {
            "title": "Total",
            "content": "1.4125 5.65 11.3 1.41 1.41 1. 233.5 65.4 12.8 12.8 12.6 66.81% 18.71% 3.65% 3.65% 3.59% 12.52 3.58% 349.4 100% 54 Table 14: Overview of the data mixing used to up/down-weight individual sources for the Stage 3 of the Curriculum ablation run. In this table we omit the size columns for brevity."
        },
        {
            "title": "Repeats",
            "content": "Tokens (Billions)"
        },
        {
            "title": "Percentage",
            "content": "Stack V"
        },
        {
            "title": "Wikimedia",
            "content": "1"
        },
        {
            "title": "StackExchange",
            "content": "2.5 peS2o"
        },
        {
            "title": "Total",
            "content": "1 3 5 6 6 6 6 6 6 6 63.8 18 86.1 56.1 45.6 43. 24.4 5.1 0.87 0.54 0.37 0. 0.08 0.02 0.01 18.519% 5.230% 24.981% 16.259% 13.241% 12.638% 7.063% 1.485% 0.251% 0.157% 0.108% 0.036% 0.023% 0.005% 0.003% 344.7 100%"
        }
    ],
    "affiliations": [
        "CMU",
        "Cornell University",
        "EleutherAI",
        "Hugging Face",
        "Independent",
        "Lawrence Livermore National",
        "Lila Sciences",
        "MIT",
        "Teraflop AI",
        "The Allen Institute for",
        "University of Maryland, College Park",
        "University of Toronto Artificial Intelligence",
        "Vector Institute",
        "poolside"
    ]
}