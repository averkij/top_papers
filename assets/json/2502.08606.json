{
    "paper_title": "Distillation Scaling Laws",
    "authors": [
        "Dan Busbridge",
        "Amitis Shidani",
        "Floris Weers",
        "Jason Ramapuram",
        "Etai Littwin",
        "Russ Webb"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design."
        },
        {
            "title": "Start",
            "content": "Dan Busbridge 1 Amitis Shidani 2 Floris Weers 1 Jason Ramapuram 1 Etai Littwin 1 Russ Webb 1 5 2 0 2 2 1 ] . [ 1 6 0 6 8 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We provide distillation scaling law that estimates distilled model performance based on compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) teacher exists, or 2) teacher needs training. If many students are to be distilled, or teacher already exists, distillation outperforms supervised pretraining until compute level which grows predictably with student size. If one student is to be distilled and teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design. 1. Introduction The study of scaling laws (Hestness et al., 2017; Rosenfeld et al., 2020; Kaplan et al., 2020; Hoffmann et al., 2022) revealed that previously trained Language Models (LMs) could have been more capable if they had followed compute optimal training paradigm, which determines the model size and the number of training tokens that give the best performing model under given compute budget. Many subsequent works have followed compute optimal training (Dey et al., 2023; Muennighoff et al., 2023b). The size of compute optimal models grows with compute (Hoffmann et al., 2022), which makes them challenging to use due to the growth in inference costs. In practice, this means compute optimal models are slow, expensive to serve, consume more battery life, provide high barriers 1Apple 2University of Oxford, UK. Work done during an internship at Apple. For full breakdown of contributions see Appendix J. Correspondence to: Dan Busbridge <dbusbridge@apple.com>. Preprint. 1 Figure 1. Extrapolations of the Distillation Scaling Law. The distillation scaling law (Equation 8) is fitted on weak students (LS > 2.3) for range of teachers with losses LT . Solid lines represent predicted model behavior for unseen teachers for given student configuration (interpolation), and dashed lines represent predicted model behavior outside of seen teachers and for the strong student region (LS 2.3). As shown, the student can outperform the teacher (see Figures 2, 3 and 41 for details). to entry for academic study, and have significant carbon footprint. With inference volume up to billions of tokens per day (OpenAI & Pilipiszyn, 2021), the inference cost of an LM is typically significantly larger than its pretraining cost (Chien et al., 2023; Wu et al., 2024a) and is going to further increase in an era of test-time compute scaling (Snell et al., 2024; Brown et al., 2024; Wu et al., 2024b). Unsustainable inference costs have led to an alternative training paradigm, overtraining (Gadre et al., 2024), where the amount of training data used is much greater than in the compute optimal case, enabling small, capable models. Overtrained models better satisfy compute optimality when compute is measured over models lifetime, rather than just the pretraining cost (Sardana et al., 2024). As supervised scaling laws follow power laws in model size and training data, diminishing returns in performance ocDistillation Scaling Laws cur much sooner than in the compute-optimal case. To achieve reasonable capabilities, these models need to be trained on many trillions of tokens, (Snell et al., 2024; Brown et al., 2024; Wu et al., 2024b), which is expensive and time-consuming. We seek models that match the performance of small overtrained models but at lower training cost. popular candidate is distillation (Hinton et al., 2015), where capable teacher LM produces targets for smaller student LM. When distillation is used for LM pretraining, we will call this distillation pretraining. There are many explanations for why distillation works, from dark knowledge transfer, where information is contained in the ratio of probabilities of incorrect classes (Hinton et al., 2015), to being form of regularization (Mobahi et al., 2020), or reducing noise in the learning process (Menon et al., 2020), among many other explanations. Despite lack of consensus for why distillation works, distillation pretraining has produced more capable models than supervised pretraining in the Gemma and Gemini (Rivière et al., 2024), Minitron (Muralidharan et al., 2024; Sreenivas et al., 2024) and AFM (Gunter et al., 2024) families of LMs in terms of both pretraining loss and downstream evaluations. Yet, at the same time, Liu et al. (2024) reported that distillation produces less capable models than supervised pretraining does. With such significant compute resources being devoted to distillation pretraining of LMs, it is essential to understand how to correctly allocate these resources, to produce the most capable models possible, and to have an understanding if any gains are even possible compared to supervised pretraining when both methods have access to the same resources (Dehghani et al., 2021). To close this knowledge gap, we perform an extensive controlled study of distillation, with students and teachers ranging from 143M to 12.6B parameters, trained on data of few billion tokens, up to 512B tokens. These experiments result in our distillation scaling law, which estimates student performance as function of resources (the teacher, the student size, and the amount of data used for distillation), resolving questions about when distillation is and is not effective in terms of producing models of desired capability under resource constraints of interest. We find: 1. The cross entropy of student of size NS distilled on DS tokens from teacher of size NT trained on DT tokens can be predicted using our distillation scaling law (Equation 8). 2. The teacher size NT and number of teacher training tokens DT determines the student cross-entropy only through their determination of the teachers crossentropy LT = LT (NT , DT ) (Figure 3b). 3. The influence of the teacher cross-entropy upon the student loss follows power law which transitions between two behaviors depending on the relative learning capacities of student and the teacher, reflecting phenomenon in distillation called the capacity gap, where stronger teacher produces worse student. Our parameterization resolves outstanding questions about the capacity gap, showing that it is gap in learning capacity (both hypothesis space and ability to optimize) between the teacher and student, and not only about their relative sizes, which is special case. Our results show that distillation can not produce lower model cross-entropies than supervised learning when both learning processes are given enough data or compute. However, distillation is more efficient than supervised learning if both of the following are true: 1. The total compute or tokens used for the student is not larger than student size-dependent threshold given by our scaling law (Section 5.1). 2. teacher already exists, or the teacher to be trained has uses beyond single distillation (Section 5.3). We hope the laws and analyses we provide will guide the community to produce even more capable models with lower inference cost and lower lifetime compute costs. 2. Background Predicting model performance is essential when scaling as it lets us understand i) the value of increasing the available compute (C), and ii) how that compute should be distributed, typically between model parameters (N ) and data (D), in order to achieve model with desired properties. These properties may be predicting the data distribution sufficiently well, measured in cross-entropy (L), or achieving level of performance on downstream tasks of interest. Fortunately, cross-entropy is predictable, with substantial empirical and theoretical evidence that follows powerlaw in parameters and data (measured in tokens) L(N, D) = + Model Cross-Entropy Irreducible Error (cid:18) α + Dβ γ (cid:19) , (1) Model ability to mimic data (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) E, A, B, α, β, γ (cid:124) where cients1 estimated from training runs (cid:125) (cid:123)(cid:122) are task-specific positive coeffin (Ni, Di, Li) i=1. } { } { The choice of runs is critical; not all experiments enable identifying the coefficients of Equation 1. One could 1Hoffmann et al. (2022) use γ = 1 whereas Kaplan et al. (2020) use β = 1. We observe significantly better fit and extrapolation without coefficient tying, which may be due to our use of Maximal Update Parameterization (µP) (see Section 4.1). 2 Distillation Scaling Laws use compute optimal models whose size parameters and number of training tokens gives the lowest crossentropy subject to compute constraint , = arg min N,D L(N, D) s.t. FLOPs(N, D) = C. (2) This is tempting, as for total experiment budget, compute optimal models offer the largest loss variation. Unfortunately, compute optimal models have constant token to parameter ratio D/N = const. (Hoffmann et al., 2022), removing degree of freedom. To achieve reliable identification of scaling coefficients, Hoffmann et al. (2022) uses two training strategies: 1. (Fixed model, varied data) The number of training tokens is varied for fixed family of models. 2. (IsoFLOP profiles) Model size and training tokens are both varied subject to total compute constraint. Data from both strategies is then combined for the fit. See Appendix for an extended background. The goal of this paper is predict the cross-entropy LS of student produced by distillation. This will tell us the value of increasing the compute for distillation and, crucially, which distillation produces the student of given size with the lowest cross-entropy for given compute budget. Notation For sequence x, x(i:j) = (x(i), x(i+1), . . . , x(j)) returns slice of the sequence, and x(<i) = x(1:i1) = (x(1), . . . , x(i1)) is the context of x(i). We to denote the set of seuse the shorthand nN = 1, 2, . . . quences with arbitrary length } { = . Language modeling We focus on the LM setting where the training objective is to model the probability of sequences of tokens xi drawn from vocabulary = RV be next-token clasΘ . Let : 1, 2, . . . , { } sifier parameterized by θ Θ whose outputs define pregiven context x(<i) dictive categorical distribution over x(<i); θ) = σa(f (x(<i); θ)) = σa(z(i)), (3) ˆp(x(i) = where σa(z) = exp(za)/ exp(zb) is the softmax function. The next-token classifier outputs z(i) = (x(<i); θ) are the logits.2 Autoregressive LMs produce sequence x(<i); θ) and likelihoods through ˆp(x; θ) = are trained to maximize this likelihood on observed data i=1 ˆp(x(i) (cid:80) 2We do not write this as z(<i) to avoid confusion with the sequence z(<i) = (z(1), . . . , z(i1)). (cid:81) through the Next Token Prediction (NTP) loss LNTP(x(i), z(i)) = a=1 (cid:88) e(x(i))a log σa(z(i)), (4) where e(i) is the i-th basis vector. It is common to also use the following token-level Z-loss to improve training stability (Chowdhery et al., 2023; Wortsman et al., 2023) Z(z(i)) = 2 log Z(z(i)) 2 = log a=1 (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) exp(z(i) ) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2 . (5) Distillation In distillation, teacher with predicted nexttoken distribution ˆpT (x(i) x(<i); θT ) and corresponding logits z(i) replaces the one-hot basis vector in Equation 4 and is used as the target for student predicted next-token x(<i); θS) and corresponding logits distribution ˆqS(x(i) z(i) . The resulting knowledge distillation loss is used to optimize the student parameters LKD(z(i) ,z(i) )= τ 2 a=1 (cid:88) σa z(i) τ (cid:33) (cid:32) logσa z(i) τ (cid:33) (cid:32) , (6) and is equivalent to optimizing the Kullback-Leibler Divergence (KLD) between the teacher and student predictions. τ > 0 is the distillation temperature. Combining the losses together results in total token-level loss for the student: S(x(i), z(i) , z(i) LKD(z(i) ) = (1 , z(i) ) + λZ λ) LNTP(x(i), z(i) ) Z(z(i) ). + λ (7) 4. Distillation Scaling Laws Here we outline the steps taken to arrive at our distillation scaling law. First we describe the experimental setting (Section 4.1) and the experiments needed to determine the scaling coefficients (Section 4.2). Given the empirical observations, we discuss the form our distillation scaling law takes (Section 4.3), find the coefficients, and verify the law under extrapolation (Section 4.4). 4.1. Experimental setup All models are based on Gunter et al. (2024) and use decoupled weight decay Loshchilov & Hutter (2019) for regularization, as well as simplified version of µP (Yang & Hu, 2021; Yang & Littwin, 2023; Yang et al., 2022; Wortsman et al., 2023; Yang et al., 2023), following µP (simple) in (Wortsman et al., 2024). µP simplifies the scaling law experimental setup as it enables hyperparameter transfer of the learning rate across model sizes. We validate that µP functions as expected for distillation in Appendix G.3. 3 3. Preliminaries Distillation Scaling Laws Table 1. Expressions related to scaling laws used in this work. In each case, always refers to student and not supervised. Expression Meaning / NS / NT / DT DS The number of model/student/teacher non-embedding parameters. Whenever we mention parameters in text, we always mean non-embedding parameters unless explicitly stated otherwise. See Appendix H.2 for more details. The number of tokens the model/teacher is pretrained on. The number of tokens the student is distilled on. protocols to produce data for our distillation scaling law fit. D/N The tokens per parameter ratio, or -ratio. In Hoffmann et al. 20 which is (2022), takes compute optimal value the Chinchilla rule of thumb. LT L(N, D) The model cross-entropy, which is the model validation cross entropy under data, estimated by the supervised scaling law for model with parameters trained on tokens. (Equation 1). L(NT , DT ) The teacher cross-entropy, which is the teacher validation cross entropy under data, estimated by the supervised scaling law for teacher with NT parameters trained on DT tokens. LS LS(NS, DS, LT ) The student cross-entropy, which is the student validation cross entropy under data, estimated by our distillation scaling law for student with NS parameters distilled on DS tokens using teacher with pretraining loss LT (Equation 8). L(NS, DS) The student supervised cross-entropy, which is the student validation cross entropy under data if the student had been trained in supervised way, estimated by the supervised scaling law for student with NS parameters trained on DS tokens. (cid:101) LS Models have sizes which range from 143M to 12.6B parameters, and we allow the teacher to be smaller or larger than the student. Multi-headed attention (MHA) is used, with Pre-Normalization (Nguyen & Salazar, 2019) using RMSNorm (Zhang & Sennrich, 2019). We train all models with sequence length of 4096, with Rotary Position Embedding (RoPE) (Su et al., 2024). We use the Englishonly subset of the C4 dataset (Raffel et al., 2020) for all experiments. For all distillation trainings, the teacher is trained on different split from the student. Except for the largest models, all Chinchilla-optimal models do not repeat data. Full hyperparameters and details can be found in Appendix I. As our goal is to understand the role of the teacher in the distillation process we distill in the pure distillation case (λ = 1, Equation 7) to avoid confounding coming from the data, as was done in Stanton et al. (2021). We verify the choice λ = 1 produces results statistically similar to the optimal λ (see Appendix G.1). Similarly, all experiments use distillation temperature (τ = 1), as we found this produces the best performing students (see Appendix G.2). 4.2. Distillation Scaling Law Experiments Here we discuss the experiments that produce the data for fitting our distillation scaling law. The distillation scal3, which ing law will estimate student cross-entropy LS in general depends on the student parameters NS, number of distillation tokens DS, the teacher parameters NT and the number of teacher training tokens NT : LS LS(NS, DS, NT , DT ). As discussed in Section 2, only certain combinations of data support reliable identification of scaling law coefficients. We combine three experimental 3By cross-entropy, we always mean with respect to data, not the teacher. We summarize our scaling law notation in Table 1. 4 Figure 2. Fixed Teacher/Student IsoFLOP profiles. Two (of total of six) teachers with MT = DT /NT 20 are distilled into students with four IsoFLOP profiles, and small number with CS = 3 1021. Horizontal and vertical dashed lines indicate teacher cross entropy LT and size NT respectively. See Appendix E.4, Figure 38a for all six profiles. Fixed Teachers/Student IsoFLOPs To simplify the experimental protocol we make the following assumption: Training student (NS, DS) on the signal provided by teacher (NT , DT ) is qualitatively similar to training that student on fixed dataset. As power law behavior has been observed in wide variety of datasets and domains (Henighan et al., 2020), it is expected that there should be power law behavior in (NS, DS) given fixed teacher. To identify these coefficients correctly, similar protocol to the Chinchilla protocol described in Section 2 should be performed. However, we cannot only do this for only one teacher, as the way student size and tokens affects downstream performance may be different for different teachers, just as the scaling laws are different for different domains and dataset. For distillation we anticipate this is the case so that different teachers produce different students. To produce the widest range of teachers for compute budget, we train six Chinchilla-optimal (MT = DT /NT 20) teachers ranging from 198M to 7.75B parameters. 4 For each of those teachers, we distill into students with four IsoFLOP profiles, taking only the standard training cost into account. The resulting student cross-entropies are in Figure 2. We note that in some cases, the student is able exhibits weak-to-strongto outperform the teacher, i.e. generalization (Burns et al., 2024; Ildiz et al., 2024) and investigate this further in Appendix E.7. IsoFLOP Teachers/Fixed Students The fixed-M teacher IsoFLOP student protocol is insufficient to identify how NT and DT independently influence student crossentropy. To ensure our experiment can detect this influence, 4We generally refer to these as fixed-m models rather than Chinchilla-optimal models as we do not yet know whether 20 is good choice in this specific setting. Distillation Scaling Laws often in distillation (see Appendix B.3). The KLD between teacher and student is an increasing function of teacher capability in all cases (see Appendix E.3), which means as the teacher improves its own performance, the student finds the teacher more challenging to model, eventually preventing the student from taking advantage of teacher gains. We use calibration metrics to investigate aspects that the student finds challenging to model in Appendix E.8. In Appendices C.1 and C.2 we offer simple explanation in kernel regression and synthetic Multi-Layer Perceptron (MLP) setting and, to the best our knowledge, are the first controlled demonstrations of the capacity gap. 4.3. Distillation Scaling Law Functional Form We need to determine the functional form of the distillation scaling law. First, we observe that contributions from teacher size NT and pretraining tokens DT are summarized by the teacher cross-entropy LT . This can be seen from Figures 1 and 3b which contains the IsoFLOP Teacher/Fixed Students of Figure 3, yet only smooth dependence as function of LT is observed. Next, the distillation scaling law should reflect the following properties: 1. An infinitely capable student should be able to model LT . any teacher: limNS ,DS LS(NS, DS, LT ) 2. random teacher produces random students independent of how capable those students are: limLT LS(NS, DS, LT ) LT . 3. There is capacity gap: making teacher too capable eventually reduces the student performance. transition between two power law regions: i) where the student is stronger learner than the teacher, and ii) where the student is weaker learner than the teacher is described by broken power law (Caballero et al., 2023). Together, we propose that student cross-entropy follows broken power law in LT and power law in NS and DS: LS(NS,DS,LT ) = LT Student cross-entropy Teacher cross-entropy (cid:124) 1+ + 1 Lc0 (cid:32) (cid:18) (cid:123)(cid:122) LT LSd1 (cid:19) (cid:33) c1f 1/f1 (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) α (cid:32) + γ Dβ (cid:33) (8) Student ability to mimic teacher (cid:101) (cid:125) } c0, c1, d1, f1, α, β, γ are positive coefficients to where (cid:124) (cid:123)(cid:122) { be fitted following the procedure outlined in Appendix F.2 on the data produced in Section 4.2. The first two properties of our distillation scaling law can be readily checked. LS = L(NS, DS) is the cross-entropy For the third, recall, student would have achieved if it had been trained in supervised way  (Table 1)  , and is determinable from the supervised scaling law (Equation 1). The capacity gap behavior (cid:101) (a) One teacher IsoFLOP set. (b) All teacher IsoFLOPs. Figure 3. IsoFLOP Teacher/Fixed Students. (a) One (of four) student sizes trained with MS = DS/NS = 20 are distilled from teachers with four IsoFLOP profiles. See Appendix E.4, Figure 38b for all profiles. (b) All profiles against teacher cross-entropy. Horizontal (vertical) dashed lines show student supervised cross entropy (cid:101)LS (student size NS). we perform experiments where the student (NS, DS) is fixed, and vary NT and DT subject to compute constraint, i.e. teacher IsoFLOP. We perform distillations into four 20) students rangChinchilla-optimal (MS = DS/NS ing from 198M to 1.82B parameters from teachers trained according to four IsoFLOP profiles. The resulting student cross-entropies are in Figure 3. Fixed Teachers/Fixed Students Finally, although not necessary for fitting our distillation scaling law, it is instructive to see how student cross entropies vary over as large range as possible. To achieve this, we train fixed-M teacher fixed-M student combinations, with ten teachers with MT 20, and students of five sizes, with at least four choices of MS per student. The resulting student cross-entropies for two of the students are in Figure 4. Figure 4. Fixed Teacher/Fixed Student. Students of two sizes trained with different MS = DS/NS = 20 ratios are distilled from teachers with MT = DT /NT 20. Capacity gap In Figure 4, we observe the capacity gap, where improving teacher performance does not always improve student performance, and even reduces student performance eventually. The capacity gap has been observed 5 Distillation Scaling Laws follows from transition based on the ratio of the algorithmic learning capacities of the student and teacher, when LT / L(NT , DT )/L(NS, DS) = d1, which can be interpreted as measure of the relative learning abilities of the teacher and the student on reference task. LS (cid:101) 4.4. Distillation Scaling Law Parameteric Fit We use the teachers (NT , DT ) for fitting our supervised scaling law (Appendix E.2), and all the data for fitting our distillation scaling law (Equation 8). Our fitting procedure is described in detail in Appendix and resulting scaling coefficients are presented in Appendix F.3. Our supervised and distillation scaling laws fit the observations at the level of 1% relative prediction error, including when extrapolated from weaker to stronger models (see Figure 5b). Table 2. Scenarios considered in our scaling law applications. Compute Scenario δLgt δPre Description Best case amortized teacher) (fully Teacher inference Teacher pretraining Teacher pretraining + inference 0 0 1 0 The teacher produces no additional FLOPs and so we are free to choose the teacher that minimizes the student cross-entropy. 0 We dont account for the teacher cost because the teacher already exists, or we intend to use the teacher as e.g. server model. We still need to pay to use it for distilling student. The teacher needs training, but we store the logits for re-use, either during training, or after training for distilling into sufficiently many students. The teacher needs training and we pay for distilling into one student, the worst case scenario. 1 , δPre where δLgt [0, 1] indicate if we account for the cost of teacher logit inference for the student targets5, and teacher pretraining cost in the total compute budget (see Table 2). (N ) is the number of Floating Operations (FLOPs) model with parameters performs during 2N is often used, giving superforward pass. (N ) vised FLOPs 6N D. We cannot use the 2N approximation, as i) using non-embedding parameters induces systematic errors (Porian et al., 2024), and ii) we are interested in small models with large context sizes where the FLOP contribution from attention is significant. To resolve these issues. we derive simple expression (N ) 2N (1+c1N 1/3 +c2N 2/3) for fixed-aspect ratio models Appendix H.1, and recommend the scaling community to consider adopting this hyperparameter setting. 5.1. Fixed tokens or compute (best case) To build intuition for when distillation may (and may not) be beneficial, we ask how well can distillation do in the best case scenario, compared with supervised learning? We superimpose the data of Figures 2 and 3 onto contours of distilled cross-entropy LS compared to supervised model with the same resources LS (Figure 6). (cid:101) Supervised learning always outperforms distillation given enough student compute or tokens. For modest token budget, distillation is favorable, however, when large number of tokens are available, supervised learning outperforms distillation. This is expected; in the large data regime, supervised learning can find the best solution limited by model size (Equation 1), whereas distillation only finds this solution for the optimal teacher (see Appendix E.6), and is otherwise limited by the distillation process. This finding appears to contradict the patient teacher finding of Beyer et al. (2022). comment on this contradiction is provided in Appendix D.1. Student compute constrained version of Figure 6 and IsoFLOP Teacher/Fixed student contours are provided in Appendix D.2. 5Appendix G.4 evaluates distribution truncation via Top-p and Top-k to mitigate the overhead of computing these logits online. (a) Supervised. (b) Distillation. Figure 5. Scaling law fits. (a) The supervised scaling law (Equation 1) applied to the data in Figure 36a. (b) Our distillation scaling law (Equation 8) applied to the data in Figures 2 to 4. As further verification, we confirm that for fixed model size, distillation in the infinite data regime is consistent with supervised learning on infinite data (Appendix E.6). 5. Distillation scaling law applications Here, we apply our distillation scaling law (Equation 8) and investigate scenarios of interest. Typically, the resources in distillation pretraining include compute budget, or dataset containing number of tokens. For distillation process, the compute cost can be approximated by FLOPs 3F (NS)DS Student Training (cid:123)(cid:122) (cid:124) (cid:125) +F (NT )(δLgt DS Teacher Logits (cid:124) (cid:123)(cid:122) (cid:125) +δPre 3DT ) (9) Teacher Training (cid:125) (cid:123)(cid:122) (cid:124) 6 Distillation Scaling Laws Figure 6. Fixed-M Teacher/IsoFLOP students (data). For student size NS and token budget DS, the cross-entropy difference between best case distillation and supervised learning. Blue indicates distillation outperforms supervised learning, red otherwise. The white horizontal dashed line indicates teacher size. 5.2. Fixed tokens or compute (teacher inference) , (i) ) } Next, we focus on the common scenario of planning to distill, and trying to decide between an existing set of teachers (L(i) i=1. larger teacher may provide better { learning signal (lower cross-entropy) but will also be more expensive to use because of the teacher logits cost (Equation 9, δLgt = 1), inducing trade-off. Given target student size NS and budget DS or CTotal, the only degree of freedom is the choice of teacher. For fixed data budget, as the student size increases, teacher cross-entropy should be decreased as power law. Here, the compute cost from NT is not relevant as we are considering token budget. Student cross-entropy at different distillation token budgets is shown in Figure 7. An equivalent plot for different student sizes whilst varying tokens is shown in Appendix D.3. We see that the optimal teacher loss (red line) decreases as power law with student size NS until LS matches , when there is an inflection point in , causing the decrease of teacher loss to sharpen with NS. This generalizes the observation of Zhang et al. (2023a), that Optimal teacher scale almost consistently follows linear scaling with the student scale across different model architectures and data scales. which is special case of our finding when the teachers are compute optimal (Figure 36a). Note that our findings consistently show that teacher cross-entropy LT determines student cross-entropy LS, not NT itself (which leads to Figure 7. Students given teacher and token budget. For four distillation token budgets the student cross-entropy for range of students and teachers. The red line indicates the optimal teacher cross-entropy producing the lowest student cross-entropy. given LT ). We investigate fixed compute budget setting for teacher inference only in Appendix D.3. 5.3. Compute Optimal Distillation We extend the analysis of Hoffmann et al. (2022) to distillation, giving compute optimal distillation, determining how to produce the student of desired size NS with the lowest cross-entropy given compute budget S, , = arg min DS ,NT ,DT LS(NS, DS, NT , DT ) s.t. FLOPs = C, (10) To present the best and worst case for incorporating teacher inference into the compute constraints, we consider all scenarios presented in Table 2. We also compare against the optimal supervised performance. To find the minima in Equation 10 we perform constrained numerical minimization using Sequential Least SQuares Programming (SLSQP) (Kraft, 1988) in SciPy (Virtanen et al., 2019). Supervised learning always matches optimal distillation at sufficient compute budget, with the intersection favoring supervised learning increasing as student size grows. In Figure 8 we see that supervised learning always matches the best case distillation setting at some total compute budget, as anticipated from the asymptotic analysis in Figure 40. The compute transition point when supervised learning becomes preferable to distillation increases as function of student size. See also Figure 6. We also observe that smaller models are more likely to benefit from supervised pretraining, whereas larger models are more likely to benefit from distillation. Distillation Scaling Laws When teacher training is included in the compute, the best student cross-entropy is always higher than in the supervised setting. This means that if your only aim is to produce the best possible model of target size and you do not have access to teacher, then you should choose supervised learning, instead of training teacher and then distilling. Conversely, if the intention is to distill into family of models, or use the teacher as server model, distillation may be more computationally beneficial than supervised learning. On reflection, this finding should be expected, otherwise it would imply that for total amount of compute, distillation can outperform direct maximum likelihood optimization. detailed discussion of the compute Figure 9. Teacher pretraining + inference. For four student sizes, the optimal student and teacher configurations when teacher logit inference and teacher pretraining cost is accounted for. Table 3. Optimal compute allocation trends. Student size Compute (FLOPs) Allocation Small ( 3B) Small ( 3B) Large ( 10B) Large ( 10B) Small ( 1021) Mostly teacher pretraining. Large ( 1025) Evenly divided between student training and teacher inference, much less on teacher pretraining. Small ( 1021) Mostly standard student training. Large ( 1025) Equally divided between student training and teacher inference and teacher pretraining. 6. Conclusion We provide distillation scaling law that estimates distilled model performance based on compute budget and its allocation between the student and teacher. We then used our law to study practical distillation scenarios of interest, and showed that distillation is only more efficient than supervised learning if: i) the total compute or tokens used for distillation is not larger than student size-dependent threshold, and ii) teacher already exists, or the teacher to be trained has uses beyond single distillation. Moreover, we use this law to determine optimal distillation scenarios that are able to outperform supervised learning, enabling practitioners to select the best teacher for their use case. This work represents the largest controlled empirical study of distillation we are aware of, with systematic ablations of common distillation techniques. Just as supervised scaling has mitigated risks in supervised pretraining, our findings offer roadmap for producing smaller, more powerful models with lower inference costs, reducing carbon footprints, and enhancing the feasibility of test-time scaling. Figure 8. Compute optimal distillation student performance. For four student sizes , the best cross-entropy each student can achieve the five scenarios considered as total compute is varied. optimal configurations that produce (N scenarios is discussed in Appendix D.4. ) for all S, , To build intuition for how quantities play off against eachother, we take the most complex scenario, teacher pretraining + inference. view of the optimal distillation setup as compute varies is presented in Figure 9. Student and teacher tokens scale as power law, with student tokens at faster rate. Optimal teacher size increases initially until it is slightly larger than the student, after which it plateaus. This plateau occurs because inference with large teachers is expensive, and with the increase in number of student tokens, overtraining the teacher becomes more efficient. The values in Figure 9 can be recombined to produce the compute terms in Equation 9 as shown in Appendix D.4, Figure 29. We summarize the trend in Table 3."
        },
        {
            "title": "Acknowledgments",
            "content": "There are however, potential negative consequences: Distillation Scaling Laws 1. Using distillation as part of training pipeline introduces new sources of bias. Teacher models may contain bias from their pretraining data. Even if student is distilled on data that is unbiased, the bias of the teacher will be inherited by the student. 2. Small powerful language models are more efficient during inference, reducing the amount of resources needed for bad actors to achieve their goals, such as generating targeted misinformation at scale. We thank Pierre Ablin, Samira Abnar, Samy Bengio, Miguel Sarabia del Castillo, Federico Danieli, Eeshan Gunesh Dhekane, Angeliki Giannou, Adam Golinski, Tom Gunter, Navdeep Jaitly, Tatiana Likhomanenko, Preetum Nakkiran, Skyler Seto, Josh Susskind, Kunal Talwar, Barry Theobald, Vimal Thilak, Oncel Tuzel, Chong Wang, Jianyu Wang, Luca Zappella, and Shaungfei Zhai for their helpful feedback and critical discussions throughout the process of writing this paper; Okan Akalin, Hassan Babaie, Peter Bukowinski, Denise Hui, Mubarak Seyed Ibrahim, David Koski, Li Li, Cindy Liu, Cesar Lopez Nataren, Ruoming Pang, Rajat Phull, Evan Samanas, Guillaume Seguin, Dan Swann, Shang-Chen Yu, Joe Zhou, Kelvin Zou, and the wider Apple infrastructure and Foundation Model teams for assistance with developing and running scalable, fault tolerant code. Names are in alphabetical order by last name within group."
        },
        {
            "title": "Impact Statement",
            "content": "This work shows how to apply the framework of scaling laws to the distillation setting, investigating distillation as viable alternative to the overtraining paradigm for producing capable language models. The work explains when distillation should and should not be performed, from compute efficiency perspective, compared to supervised learning. There are number of benefits to this: 1. As compute-optimal recipes for distillation are now known, there is greater opportunity for producing powerful models with lower inference costs. Lowering inference costs lower the largest component of language model training carbon footprint. 2. When combined with other known scaling laws, there is larger space of models for which we know compute-optimal configurations. To produce models with given capability, the compute, hardware and climate costs have now been reduced compared to before, as the optimal recipe is known. 3. Our distillation scaling law lowers compute usage through removing unnecessary experimentation over various hyperparameters and distillation settings. We now understand that the primary driver of student cross-entropy is teacher cross-entropy, and so teacher size and tokens can be discarded as axes to search over. 4. Small powerful models democratize the study of models with significant capabilities, enabling the involvement of greater number of perspectives to study model capabilities and safety aspects. Distillation Scaling Laws"
        },
        {
            "title": "References",
            "content": "Abdin, M. I., Aneja, J., Behl, H. S., Bubeck, S., Eldan, R., Gunasekar, S., Harrison, M., Hewett, R. J., Javaheripi, M., Kauffmann, P., Lee, J. R., Lee, Y. T., Li, Y., Liu, W., Mendes, C. C. T., Nguyen, A., Price, E., de Rosa, G., Saarikivi, O., Salim, A., Shah, S., Wang, X., Ward, R., Wu, Y., Yu, D., Zhang, C., and Zhang, Y. Phi-4 technical report. CoRR, abs/2412.08905, 2024a. doi: 10.48550/ ARXIV.2412.08905. URL https://doi.org/10. 48550/arXiv.2412.08905. Abdin, M. I., Jacobs, S. A., Awan, A. A., Aneja, J., Awadallah, A., Awadalla, H., Bach, N., Bahree, A., Bakhtiari, A., Behl, H. S., Benhaim, A., Bilenko, M., Bjorck, J., Bubeck, S., Cai, M., Mendes, C. C. T., Chen, W., Chaudhary, V., Chopra, P., Giorno, A. D., de Rosa, G., Dixon, M., Eldan, R., Iter, D., Garg, A., Goswami, A., Gunasekar, S., Haider, E., Hao, J., Hewett, R. J., Huynh, J., Javaheripi, M., Jin, X., Kauffmann, P., Karampatziakis, N., Kim, D., Khademi, M., Kurilenko, L., Lee, J. R., Lee, Y. T., Li, Y., Liang, C., Liu, W., Lin, E., Lin, Z., Madan, P., Mitra, A., Modi, H., Nguyen, A., Norick, B., Patra, B., Perez-Becker, D., Portet, T., Pryzant, R., Qin, H., Radmilac, M., Rosset, C., Roy, S., Ruwase, O., Saarikivi, O., Saied, A., Salim, A., Santacroce, M., Shah, S., Shang, N., Sharma, H., Song, X., Tanaka, M., Wang, X., Ward, R., Wang, G., Witte, P., Wyatt, M., Xu, C., Xu, J., Yadav, S., Yang, F., Yang, Z., Yu, D., Zhang, C., Zhang, C., Zhang, J., Zhang, L. L., Zhang, Y., Zhang, Y., Zhang, Y., and Zhou, X. Phi-3 technical report: highly capable language model locally on your phone. CoRR, abs/2404.14219, 2024b. doi: 10.48550/ARXIV.2404.14219. URL https://doi. org/10.48550/arXiv.2404.14219. Abnar, S., Shah, H., Busbridge, D., Ali, A. M. E., Susskind, J., and Thilak, V. Parameters vs flops: Scaling laws for optimal sparsity for mixture-of-experts language models, 2025. URL https://arxiv.org/abs/2501. 12370. Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebrón, F., and Sanghai, S. GQA: training generalized multi-query transformer models from multi-head checkpoints. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 48954901. Association for Computational Linguistics, 2023. doi: 10.18653/ V1/2023.EMNLP-MAIN.298. URL https://doi. org/10.18653/v1/2023.emnlp-main.298. Apple. The axlearn library for deep learning., 2023. URL Achttps://github.com/apple/axlearn. cessed: 2025-02-11. Bahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U. Explaining neural scaling laws. CoRR, abs/2102.06701, URL https://arxiv.org/abs/2102. 2021. 06701. Barnett, M. An empirical study of scaling laws for transfer. CoRR, abs/2408.16947, 2024. doi: 10.48550/ARXIV. 2408.16947. URL https://doi.org/10.48550/ arXiv.2408.16947. Berant, J., Chou, A., Frostig, R., and Liang, P. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 1821 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, meeting of SIGDAT, Special Interest Group of the ACL, pp. 15331544. ACL, 2013. URL https://aclanthology.org/D13-1160/. Besiroglu, T., Erdil, E., Barnett, M., and You, J. CoRR, doi: 10.48550/ARXIV.2404. URL https://doi.org/10.48550/ Chinchilla scaling: replication attempt. abs/2404.10102, 2024. 10102. arXiv.2404.10102. Beyer, L., Zhai, X., Royer, A., Markeeva, L., Anil, R., and Kolesnikov, A. Knowledge distillation: good teacher In IEEE/CVF Conference is patient and consistent. on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 1091510924. IEEE, 2022. doi: 10.1109/CVPR52688. 2022.01065. URL https://doi.org/10.1109/ CVPR52688.2022.01065. Bhakthavatsalam, S., Khashabi, D., Khot, T., Mishra, B. D., Richardson, K., Sabharwal, A., Schoenick, C., Tafjord, O., and Clark, P. Think you have solved direct-answer question answering? try arc-da, CoRR, the direct-answer AI2 reasoning challenge. abs/2102.03315, 2021. URL https://arxiv.org/ abs/2102.03315. Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K., Du, Q., Fu, Z., Gao, H., Gao, K., Gao, W., Ge, R., Guan, K., Guo, D., Guo, J., Hao, G., Hao, Z., He, Y., Hu, W., Huang, P., Li, E., Li, G., Li, J., Li, Y., Li, Y. K., Liang, W., Lin, F., Liu, A. X., Liu, B., Liu, W., Liu, X., Liu, X., Liu, Y., Lu, H., Lu, S., Luo, F., Ma, S., Nie, X., Pei, T., Piao, Y., Qiu, J., Qu, H., Ren, T., Ren, Z., Ruan, C., Sha, Z., Shao, Z., Song, J., Su, X., Sun, J., Sun, Y., Tang, M., Wang, B., Wang, P., Wang, S., Wang, Y., Wang, Y., Wu, T., Wu, Y., Xie, X., Xie, Z., Xie, Z., Xiong, Y., Xu, H., Xu, R. X., Xu, Y., Yang, D., You, Y., Yu, S., Yu, X., Zhang, B., Zhang, H., Zhang, L., Zhang, L., Zhang, M., Zhang, M., Zhang, W., Zhang, Y., Zhao, C., Zhao, Y., Zhou, S., Zhou, S., Zhu, Q., and 10 Distillation Scaling Laws Zou, Y. Deepseek LLM: scaling open-source language models with longtermism. CoRR, abs/2401.02954, 2024. doi: 10.48550/ARXIV.2401.02954. URL https:// doi.org/10.48550/arXiv.2401.02954. Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. PIQA: reasoning about physical commonsense in natIn The Thirty-Fourth AAAI Conference ural language. on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 7432 7439. AAAI Press, 2020. doi: 10.1609/AAAI.V34I05. 6239. URL https://doi.org/10.1609/aaai. v34i05.6239. Blondel, M. and Roulet, V. The elements of differentiable programming. CoRR, abs/2403.14606, 2024. doi: 10.48550/ARXIV.2403.14606. URL https://doi. org/10.48550/arXiv.2403.14606. Brown, B. C. A., Juravsky, J., Ehrlich, R. S., Clark, R., Le, Q. V., Ré, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling. CoRR, abs/2407.21787, 2024. doi: 10.48550/ARXIV.2407.21787. URL https://doi. org/10.48550/arXiv.2407.21787. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, URL https://proceedings. virtual, 2020. neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract. html. Bucila, C., Caruana, R., and Niculescu-Mizil, A. Model compression. In Eliassi-Rad, T., Ungar, L. H., Craven, M., and Gunopulos, D. (eds.), Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Philadelphia, PA, USA, August 20-23, 2006, pp. 535541. ACM, 2006. doi: 10. 1145/1150402.1150464. URL https://doi.org/ 10.1145/1150402.1150464. Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J., Sutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=ghNRg2mEgN. Caballero, E., Gupta, K., Rish, I., and Krueger, D. BroIn The Eleventh Internaken neural scaling laws. tional Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum? id=sckjveqlCZ. CERN."
        },
        {
            "title": "Key",
            "content": "centre: data Cern mation, March 2018. information-technology.web.cern. ch/sites/information-technology. web.cern.ch/files/CERNDataCentre_ KeyInformation_02March2018V1.pdf. cessed: 2025-01-29."
        },
        {
            "title": "URL",
            "content": "inforhttp:// AcChien, A. A., Lin, L., Nguyen, H., Rao, V., Sharma, T., and Wijayawardana, R. Reducing the carbon impact of generative AI inference (today and in 2035). In Porter, G., Anderson, T., Chien, A. A., Eilam, T., Josephson, C., and Park, J. (eds.), Proceedings of the 2nd Workshop on Sustainable Computer Systems, HotCarbon 2023, Boston, MA, USA, 9 July 2023, pp. 11:111:7. ACM, 2023. doi: 10.1145/3604930.3605705. URL https: //doi.org/10.1145/3604930.3605705. Cho, J. H. and Hariharan, B. On the efficacy of In 2019 IEEE/CVF Internaknowledge distillation. tional Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pp. 47934801. IEEE, 2019. doi: 10.1109/ICCV. 2019.00489. URL https://doi.org/10.1109/ ICCV.2019.00489. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Distillation Scaling Laws Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: J. Mach. Scaling language modeling with pathways. Learn. Res., 24:240:1240:113, 2023. URL https: //jmlr.org/papers/v24/22-1144.html. Clark, A., de Las Casas, D., Guy, A., Mensch, A., Paganini, M., Hoffmann, J., Damoc, B., Hechtman, B. A., Cai, T., Borgeaud, S., van den Driessche, G., Rutherford, E., Hennigan, T., Johnson, M. J., Cassirer, A., Jones, C., Buchatskaya, E., Budden, D., Sifre, L., Osindero, S., Vinyals, O., Ranzato, M., Rae, J. W., Elsen, E., Kavukcuoglu, K., and Simonyan, K. Unified scaling laws for routed language models. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvári, C., Niu, G., and Sabato, S. (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 40574086. PMLR, 2022. URL https://proceedings.mlr. press/v162/clark22a.html. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. CoRR, abs/2110.14168, URL https://arxiv.org/abs/2110. 2021. 14168. Dao, T. Flashattention-2: Faster attention with better In The Twelfth Inparallelism and work partitioning. ternational Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/ forum?id=mZn2Xyh9Ec. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Ré, Flashattention: Fast and memory-efficient exact C. attention with io-awareness. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers. nips.cc/paper_files/paper/2022/hash/ 67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference. html. DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Zhang, H., Ding, H., Xin, H., Gao, H., Li, H., Qu, H., Cai, J. L., Liang, J., Guo, J., Ni, J., Li, J., Wang, J., Chen, J., Chen, J., Yuan, J., Qiu, J., Li, J., Song, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Wang, L., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N., Huang, P., Wang, P., Zhang, P., Wang, Q., Zhu, Q., Chen, Q., Du, Q., Chen, R. J., Jin, R. L., Ge, R., Zhang, R., Pan, R., Wang, R., Xu, R., Zhang, R., Chen, R., Li, S. S., Lu, S., Zhou, S., Chen, S., Wu, S., Ye, S., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S., Zhou, S., Pan, S., Wang, T., Yun, T., Pei, T., Sun, T., Xiao, W. L., and Zeng, W. Deepseekv3 technical report. CoRR, abs/2412.19437, 2024. doi: 10.48550/ARXIV.2412.19437. URL https://doi. org/10.48550/arXiv.2412.19437. Dehghani, M., Arnab, A., Beyer, L., Vaswani, A., and Tay, Y. The efficiency misnomer. CoRR, abs/2110.12894, URL https://arxiv.org/abs/2110. 2021. 12894. Dey, N., Gosal, G., Chen, Z., Khachane, H., Marshall, W., Pathria, R., Tom, M., and Hestness, J. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. CoRR, doi: 10.48550/ARXIV.2304. abs/2304.03208, 2023. URL https://doi.org/10.48550/ 03208. arXiv.2304.03208. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Rozière, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi. org/10.48550/arXiv.2407.21783. Epoch AI. Key trends and figures in machine learning, 2023. URL https://epoch.ai/trends. Accessed: 2025-02-11. 12 Distillation Scaling Laws Gadre, S. Y., Smyrnis, G., Shankar, V., Gururangan, S., Wortsman, M., Shao, R., Mercat, J., Fang, A., Li, J., Keh, S., Xin, R., Nezhurina, M., Vasiljevic, I., Jitsev, J., Dimakis, A. G., Ilharco, G., Song, S., Kollar, T., Carmon, Y., Dave, A., Heckel, R., Muennighoff, N., and Schmidt, L. Language models scale reliably with over-training and on downstream tasks. CoRR, abs/2403.08540, 2024. doi: 10.48550/ARXIV. 2403.08540. URL https://doi.org/10.48550/ arXiv.2403.08540. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/ records/12608602. Gunter, T., Wang, Z., Wang, C., Pang, R., Narayanan, A., Zhang, A., Zhang, B., Chen, C., Chiu, C., Qiu, D., Gopinath, D., Yap, D. A., Yin, D., Nan, F., Weers, F., Yin, G., Huang, H., Wang, J., Lu, J., Peebles, J., Ye, K., Lee, M., Du, N., Chen, Q., Keunebroek, Q., Wiseman, S., Evans, S., Lei, T., Rathod, V., Kong, X., Du, X., Li, Y., Wang, Y., Gao, Y., Ahmed, Z., Xu, Z., Lu, Z., Rashid, A., Jose, A. M., Doane, A., Bencomo, A., Vanderby, A., Hansen, A., Jain, A., Anupama, A. M., Kamal, A., Wu, B., Brum, C., Maalouf, C., Erdenebileg, C., Dulhanty, C., Moritz, D., Kang, D., Jimenez, E., Ladd, E., Shi, F., Bai, F., Chu, F., Hohman, F., Kotek, H., Coleman, H. G., Li, J., Bigham, J. P., Cao, J., Lai, J., Cheung, J., Shan, J., Zhou, J., Li, J., Qin, J., Singh, K., Vega, K., Zou, K., Heckman, L., Gardiner, L., Bowler, M., Cordell, M., Cao, M., Hay, N., Shahdadpuri, N., Godwin, O., Dighe, P., Rachapudi, P., Tantawi, R., Frigg, R., Davarnia, S., Shah, S., Guha, S., Sirovica, S., Ma, S., Ma, S., Wang, S., Kim, S., Jayaram, S., Shankar, V., Paidi, V., Kumar, V., Wang, X., Zheng, X., and Cheng, W. Apple intelligence foundation language models. CoRR, abs/2407.21075, 2024. doi: 10.48550/ARXIV.2407.21075. URL https://doi. org/10.48550/arXiv.2407.21075. Harutyunyan, H., Rawat, A. S., Menon, A. K., Kim, S., and Kumar, S. Supervision complexity and its role In The Eleventh Internain knowledge distillation. tional Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum? id=8jU7wy7N7mA. Havrilla, A. and Liao, W. Understanding scaling laws with statistical and approximation theory for transformer neural networks on intrinsically low-dimensional data. CoRR, abs/2411.06646, 2024. doi: 10.48550/ARXIV. 2411.06646. URL https://doi.org/10.48550/ arXiv.2411.06646. Hendrycks, D., Burns, C., Basart, S., Critch, A., Li, J., Song, D., and Steinhardt, J. Aligning AI with shared human values. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https: //openreview.net/forum?id=dNy_RKzJacY. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview. net/forum?id=d7KBjmI3GmQ. Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T. B., Dhariwal, P., Gray, S., Hallacy, C., Mann, B., Radford, A., Ramesh, A., Ryder, N., Ziegler, D. M., Schulman, J., Amodei, D., and McCandlish, S. Scaling laws for autoregressive generative modeling. CoRR, abs/2010.14701, 2020. URL https://arxiv.org/abs/2010.14701. Hernandez, D., Kaplan, J., Henighan, T., and McCandlish, S. Scaling laws for transfer. CoRR, abs/2102.01293, URL https://arxiv.org/abs/2102. 2021. 01293. Hestness, J., Narang, S., Ardalani, N., Diamos, G. F., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., and Zhou, Y. Deep learning scaling is predictable, empirically. CoRR, abs/1712.00409, 2017. URL http: //arxiv.org/abs/1712.00409. DistillHinton, G. E., Vinyals, O., and Dean, J. CoRR, ing the knowledge in neural network. abs/1503.02531, 2015. URL http://arxiv.org/ abs/1503.02531. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models. CoRR, abs/2203.15556, 2022. doi: 10.48550/ARXIV.2203.15556. URL https:// doi.org/10.48550/arXiv.2203.15556. Hu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z., Fang, Y., Huang, Y., Zhao, W., Zhang, X., Thai, Z. L., Zhang, K., Wang, C., Yao, Y., Zhao, C., Zhou, J., Cai, J., Zhai, Z., Ding, N., Jia, C., Zeng, Distillation Scaling Laws G., Li, D., Liu, Z., and Sun, M. Minicpm: Unveiling the potential of small language models with scalable training strategies. CoRR, abs/2404.06395, 2024. doi: 10.48550/ARXIV.2404.06395. URL https://doi. org/10.48550/arXiv.2404.06395. Ildiz, M. E., Gozeten, H. A., Taga, E. O., Mondelli, M., and Oymak, S. High-dimensional analysis of knowledge distillation: Weak-to-strong generalization and scaling laws. CoRR, abs/2410.18837, 2024. doi: 10.48550/ ARXIV.2410.18837. URL https://doi.org/10. 48550/arXiv.2410.18837. Jain, A., Montanari, A., and Sasoglu, E. Scaling laws for learning with real and surrogate data. CoRR, abs/2402.04376, 2024. doi: 10.48550/ARXIV. 2402.04376. URL https://doi.org/10.48550/ arXiv.2402.04376. Jelassi, S., Mohri, C., Brandfonbrener, D., Gu, A., Vyas, N., Anand, N., Alvarez-Melis, D., Li, Y., Kakade, S. M., and Malach, E. Mixture of parrots: Experts improve memorization more than reasoning. CoRR, abs/2410.19034, 2024. doi: 10.48550/ARXIV. 2410.19034. URL https://doi.org/10.48550/ arXiv.2410.19034. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de Las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b. CoRR, abs/2310.06825, 2023. doi: 10.48550/ARXIV. 2310.06825. URL https://doi.org/10.48550/ arXiv.2310.06825. Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Barzilay, R. and Kan, M. (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pp. 16011611. Association for Computational Linguistics, 2017. doi: 10.18653/V1/P17-1147. URL https://doi.org/10.18653/v1/P17-1147. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https:// arxiv.org/abs/2001.08361. Kim, Y. and Rush, A. M. Sequence-level knowledge distillation. In Su, J., Carreras, X., and Duh, K. (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp. 13171327. The Association for Computational Linguistics, 2016. doi: 10.18653/V1/D16-1139. URL https://doi.org/ 10.18653/v1/d16-1139. Kingma, D. P. and Ba, J. Adam: method for stochasIn Bengio, Y. and LeCun, Y. (eds.), tic optimization. 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980. Kraft, D. Software Package for Sequential Quadratic Programming. Deutsche Forschungsund Versuchsanstalt für Luftund Raumfahrt Köln: Forschungsbericht. Wiss. Berichtswesen d. DFVLR, 1988. URL https://books.google.co.uk/books?id= 4rKaGwAACAAJ. Li, Y., Bubeck, S., Eldan, R., Giorno, A. D., Gunasekar, S., and Lee, Y. T. Textbooks are all you need II: phi1.5 technical report. CoRR, abs/2309.05463, 2023. doi: 10.48550/ARXIV.2309.05463. URL https://doi. org/10.48550/arXiv.2309.05463. Liu, Z., Zhao, C., Iandola, F. N., Lai, C., Tian, Y., Fedorov, I., Xiong, Y., Chang, E., Shi, Y., Krishnamoorthi, R., Lai, L., and Chandra, V. Mobilellm: Optimizing sub-billion parameter language models for onIn Forty-first International Conferdevice use cases. ence on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=EIGbXbxcUQ. Lopez-Paz, D., Bottou, L., Schölkopf, B., and Vapnik, V. Unifying distillation and privileged information. In Bengio, Y. and LeCun, Y. (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv. org/abs/1511.03643. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https: //openreview.net/forum?id=Bkg6RiCqY7. Ludziejewski, J., Krajewski, J., Adamczewski, K., Pióro, M., Krutul, M., Antoniak, S., Ciebiera, K., Król, K., Odrzygózdz, T., Sankowski, P., Cygan, M., and Jaszczur, S. Scaling laws for fine-grained mixture of experts. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 2127, 2024. OpenReview.net, 2024. URL https:// openreview.net/forum?id=yoqdlynCRs. 14 Distillation Scaling Laws Lukasik, M., Bhojanapalli, S., Menon, A. K., and Kumar, S. Teachers pet: understanding and mitigating biases in distillation. Trans. Mach. Learn. Res., 2022, 2022. URL https://openreview.net/forum? id=ph3AYXpwEb. Menon, A. K., Rawat, A. S., Reddi, S. J., Kim, S., and Kumar, S. Why distillation helps: statistical perspective. CoRR, abs/2005.10419, 2020. URL https: //arxiv.org/abs/2005.10419. Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., Tafti, P., Hussenot, L., Chowdhery, A., Roberts, A., Barua, A., Botev, A., Castro-Ros, A., Slone, A., Héliou, A., Tacchetti, A., Bulanova, A., Paterson, A., Tsai, B., Shahriari, B., Lan, C. L., Choquette-Choo, C. A., Crepy, C., Cer, D., Ippolito, D., Reid, D., Buchatskaya, E., Ni, E., Noland, E., Yan, G., Tucker, G., Muraru, G., Rozhdestvenskiy, G., Michalewski, H., Tenney, I., Grishchenko, I., Austin, J., Keeling, J., Labanowski, J., Lespiau, J., Stanway, J., Brennan, J., Chen, J., Ferret, J., Chiu, J., and et al. Gemma: Open models based on gemini research and technology. CoRR, abs/2403.08295, 2024. doi: 10.48550/ARXIV. 2403.08295. URL https://doi.org/10.48550/ arXiv.2403.08295. Mirzadeh, S., Farajtabar, M., Li, A., Levine, N., MatImproved knowlsukawa, A., and Ghasemzadeh, H. In The Thirtyedge distillation via teacher assistant. Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 51915198. AAAI Press, 2020. doi: 10.1609/AAAI.V34I04.5963. URL https://doi. org/10.1609/aaai.v34i04.5963. 2305.16264. URL https://doi.org/10.48550/ arXiv.2305.16264. Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Tazi, N., Piktus, A., Pyysalo, S., Wolf, T., and Raffel, In C. A. Scaling data-constrained language models. Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023b. URL http://papers. nips.cc/paper_files/paper/2023/hash/ 9d89448b63ce1e2e8dc7af72c984c196-Abstract-Conference. html. Muralidharan, S., Sreenivas, S. T., Joshi, R., Chochowski, M., Patwary, M., Shoeybi, M., Catanzaro, lanB., Kautz, J., and Molchanov, P. guage models via pruning and knowledge distillation. CoRR, abs/2407.14679, 2024. doi: 10.48550/ARXIV. 2407.14679. URL https://doi.org/10.48550/ arXiv.2407.14679. Compact Nagarajan, V., Menon, A. K., Bhojanapalli, S., Mobahi, On student-teacher deviations H., and Kumar, S. in distillation: does it pay to disobey? In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December URL http://papers. 10 - 16, 2023, 2023. nips.cc/paper_files/paper/2023/hash/ 12d286282e1be5431ea05262a21f415c-Abstract-Conference. html. Mobahi, H., Farajtabar, M., and Bartlett, P. L. Selfdistillation amplifies regularization in hilbert space. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virURL https://proceedings. tual, neurips.cc/paper/2020/hash/ 2288f691b58edecadcc9a8691762b4fd-Abstract. html. 2020. Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Piktus, A., Tazi, N., Pyysalo, S., Wolf, T., and Raffel, C. Scaling data-constrained language models. CoRR, abs/2305.16264, 2023a. doi: 10.48550/ARXIV. 15 Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., Phanishayee, A., and Zaharia, M. Efficient large-scale language model training on GPU clusters using megatron-lm. In de Supinski, B. R., Hall, M. W., and Gamblin, T. (eds.), International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2021, St. Louis, Missouri, USA, November 14-19, 2021, pp. 58. ACM, 2021. doi: 10.1145/3458817.3476209. URL https: //doi.org/10.1145/3458817.3476209. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention. In Niehues, J., Cattoni, R., Stüker, S., Negri, M., Turchi, M., Ha, T., Salesky, E., Sanabria, R., Barrault, L., Specia, L., and Federico, M. (eds.), Proceedings of the 16th International Conference on Spoken Language Translation, IWSLT 2019, Hong Kong, NovemDistillation Scaling Laws ber 2-3, 2019. Association for Computational Linguistics, 2019. URL https://aclanthology.org/ 2019.iwslt-1.17. OpenAI and Pilipiszyn, A. Gpt-3 powers the next generation of apps, 2021. URL http://website-url. com. Accessed on Jan 19, 2025. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernández, R. The LAMBADA dataset: Word preIn Prodiction requiring broad discourse context. ceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/V1/P16-1144. URL https://doi.org/ 10.18653/v1/p16-1144. Paquette, E., Paquette, C., Xiao, L., and Pennington, J. 4+3 phases of compute-optimal neural scaling laws. CoRR, abs/2405.15074, 2024. doi: 10.48550/ARXIV. 2405.15074. URL https://doi.org/10.48550/ arXiv.2405.15074. Pareek, D., Du, S. S., and Oh, S. Understanding the gains from repeated self-distillation. CoRR, abs/2407.04600, 2024. doi: 10.48550/ARXIV.2407. URL https://doi.org/10.48550/ 04600. arXiv.2407.04600. Pearce, T. and Song, J. Reconciling kaplan and chinchilla scaling laws. CoRR, abs/2406.12907, 2024. doi: 10.48550/ARXIV.2406.12907. URL https://doi. org/10.48550/arXiv.2406.12907. Peng, H., Lv, X., Bai, Y., Yao, Z., Zhang, J., Hou, L., and Li, J. Pre-training distillation for large language models: design space exploration. CoRR, doi: 10.48550/ARXIV.2410. abs/2410.16215, 2024. URL https://doi.org/10.48550/ 16215. arXiv.2410.16215. Porian, T., Wortsman, M., Jitsev, J., Schmidt, L., and Carmon, Y. Resolving discrepancies in compute-optimal scaling of language models. CoRR, abs/2406.19146, 2024. URL https://doi.org/10.48550/arXiv.2406. 19146. 10.48550/ARXIV.2406.19146. doi: Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-totext transformer. J. Mach. Learn. Res., 21:140:1140:67, 2020. URL https://jmlr.org/papers/v21/ 20-074.html. 16 Rawat, A. S., Sadhanala, V., Rostamizadeh, A., Chakrabarti, A., Jitkrittum, W., Feinberg, V., Kim, S., Harutyunyan, H., Saunshi, N., Nado, Z., Shivanna, R., Reddi, S. J., Menon, A. K., Anil, R., and Kumar, S. little help goes long way: Efficient LLM training by leveraging small lms. CoRR, abs/2410.18779, 2024. doi: 10.48550/ARXIV.2410.18779. URL https://doi. org/10.48550/arXiv.2410.18779. Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T. P., Alayrac, J., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., Antonoglou, I., Anil, R., Borgeaud, S., Dai, A. M., Millican, K., Dyer, E., Glaese, M., Sottiaux, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Molloy, J., Chen, J., Isard, M., Barham, P., Hennigan, T., McIlroy, R., Johnson, M., Schalkwyk, J., Collins, E., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Meyer, C., Thornton, G., Yang, Z., Michalewski, H., Abbas, Z., Schucher, N., Anand, A., Ives, R., Keeling, J., Lenc, K., Haykal, S., Shakeri, S., Shyam, P., Chowdhery, A., Ring, R., Spencer, S., Sezener, E., and et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530, 2024. doi: 10.48550/ARXIV.2403.05530. URL https://doi. org/10.48550/arXiv.2403.05530. Rivière, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Ramé, A., Ferret, J., Liu, P., Tafti, P., Friesen, A., Casbon, M., Ramos, S., Kumar, R., Lan, C. L., Jerome, S., Tsitsulin, A., Vieillard, N., Stanczyk, P., Girgin, S., Momchev, N., Hoffman, M., Thakoor, S., Grill, J., Neyshabur, B., Bachem, O., Walton, A., Severyn, A., Parrish, A., Ahmad, A., Hutchison, A., Abdagic, A., Carl, A., Shen, A., Brock, A., Coenen, A., Laforge, A., Paterson, A., Bastian, B., Piot, B., Wu, B., Royal, B., Chen, C., Kumar, C., Perry, C., Welty, C., ChoquetteChoo, C. A., Sinopalnikov, D., Weinberger, D., Vijaykumar, D., Rogozinska, D., Herbison, D., Bandy, E., Wang, E., Noland, E., Moreira, E., Senter, E., Eltyshev, E., Visin, F., Rasskin, G., Wei, G., Cameron, G., Martins, G., Hashemi, H., Klimczak-Plucinska, H., Batra, H., Dhand, H., Nardini, I., Mein, J., Zhou, J., Svensson, J., Stanway, J., Chan, J., Zhou, J. P., Carrasqueira, J., Iljazi, J., Becker, J., Fernandez, J., van Amersfoort, J., Gordon, J., Lipschultz, J., Newlan, J., Ji, J., Mohamed, K., Badola, K., Black, K., Millican, K., McDonell, K., Nguyen, K., Sodhia, K., Greene, K., Sjösund, L. L., Usui, L., Sifre, L., Heuermann, L., Lago, L., and McNealus, L. Gemma 2: Improving open language models at practical size. CoRR, abs/2408.00118, 2024. doi: 10.48550/ARXIV.2408.00118. URL https://doi. org/10.48550/arXiv.2408.00118. Rosenfeld, J. S., Rosenfeld, A., Belinkov, Y., and Shavit, Distillation Scaling Laws N. constructive prediction of the generalization In 8th International Confererror across scales. ence on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum? id=ryenvpEKDr. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106, 2021. doi: 10.1145/3474381. URL https://doi.org/ 10.1145/3474381. Sardana, N., Portes, J. P., Doubov, S., and Frankle, J. Beyond chinchilla-optimal: Accounting for inference in In Forty-first Internalanguage model scaling laws. tional Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum? id=0bmXrtTDUu. Shazeer, N. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/ abs/2002.05202. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G. E., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. CoRR, abs/1701.06538, 2017. URL http:// arxiv.org/abs/1701.06538. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. CoRR, abs/2408.03314, 2024. doi: 10.48550/ARXIV.2408.03314. URL https:// doi.org/10.48550/arXiv.2408.03314. Sreenivas, S. T., Muralidharan, S., Joshi, R., Chochowski, M., Patwary, M., Shoeybi, M., Catanzaro, B., Kautz, J., and Molchanov, P. LLM pruning and distillation in practice: The minitron approach. CoRR, abs/2408.11796, 2024. doi: 10.48550/ARXIV. 2408.11796. URL https://doi.org/10.48550/ arXiv.2408.11796. Stanton, S., Izmailov, P., Kirichenko, P., Alemi, A. A., and Wilson, A. G. Does knowledge distillation really work? In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 6906 URL https://proceedings. 6919, 2021. neurips.cc/paper/2021/hash/ 376c6b9ff3bedbbea56751a84fffc10c-Abstract. html. Stein, C. Inadmissibility of the usual estimator for the mean of multivariate normal distribution. Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, 1:197206, 1956. Su, J., Ahmed, M. H. M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. doi: 10.1016/J.NEUCOM.2023.127063. URL https:// doi.org/10.1016/j.neucom.2023.127063. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023a. doi: 10.48550/ARXIV.2302.13971. URL https://doi. org/10.48550/arXiv.2302.13971. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Canton-Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/ARXIV.2307.09288. URL https://doi. org/10.48550/arXiv.2307.09288. Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., van der Walt, S., Brett, M., Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R., Larson, E., Carey, C., Polat, I., Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A., Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa, F., van Mulbregt, P., and SciPy. Scipy 1.0-fundamental algorithms for scientific computing in python. CoRR, abs/1907.10121, 2019. URL http: //arxiv.org/abs/1907.10121. Welbl, J., Liu, N. F., and Gardner, M. Crowdsourcing multiple choice science questions. In Derczynski, L., Xu, W., Ritter, A., and Baldwin, T. (eds.), Proceedings of the 3rd Workshop on Noisy User-generated Text, Distillation Scaling Laws NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017, pp. 94106. Association for Computational Linguistics, 2017. doi: 10.18653/V1/W17-4413. URL https://doi.org/10.18653/v1/w17-4413. Wortsman, M., Liu, P. J., Xiao, L., Everett, K., Alemi, A., Adlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-Dickstein, J., Xu, K., Lee, J., Gilmer, J., and Kornblith, S. Small-scale proxies for large-scale transformer training instabilities. CoRR, abs/2309.14322, 2023. doi: 10.48550/ARXIV. 2309.14322. URL https://doi.org/10.48550/ arXiv.2309.14322. Wortsman, M., Liu, P. J., Xiao, L., Everett, K. E., Alemi, A. A., Adlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-Dickstein, J., Xu, K., Lee, J., Gilmer, J., and Kornblith, S. Small-scale proxies for large-scale transformer training instabilities. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 711, 2024. OpenReview.net, 2024. URL https:// openreview.net/forum?id=d8w0pmvXbZ. Wu, C., Acun, B., Raghavendra, R., and Hazelwood, K. M. Beyond efficiency: Scaling AI sustainably. IEEE Micro, 44(5):3746, 2024a. doi: 10.1109/MM.2024. 3409275. URL https://doi.org/10.1109/MM. 2024.3409275. Wu, Y., Sun, Z., Li, S., Welleck, S., and Yang, inAn empirical analysis of compute-optimal Y. ference for problem-solving with language models. CoRR, abs/2408.00724, 2024b. doi: 10.48550/ARXIV. 2408.00724. URL https://doi.org/10.48550/ arXiv.2408.00724. Yang, G. and Hu, E. J. Tensor programs IV: feature learning in infinite-width neural networks. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 1172711737. PMLR, 2021. URL http://proceedings.mlr.press/ v139/yang21c.html. Yang, G. and Littwin, E. Tensor programs ivb: Adaptive optimization in the infinite-width limit. CoRR, doi: 10.48550/ARXIV.2308. abs/2308.01814, 2023. URL https://doi.org/10.48550/ 01814. arXiv.2308.01814. Yang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N., Pachocki, J., Chen, W., and Gao, J. Tensor programs V: tuning large neural networks via zero-shot hyperparameter transfer. CoRR, abs/2203.03466, 2022. doi: 10.48550/ARXIV.2203.03466. URL https:// doi.org/10.48550/arXiv.2203.03466. Yang, G., Simon, J. B., and Bernstein, J. spectral condition for feature learning. CoRR, abs/2310.17813, 2023. doi: 10.48550/ARXIV.2310.17813. URL https:// doi.org/10.48550/arXiv.2310.17813. Yang, G., Yu, D., Zhu, C., and Hayou, S. Tensor programs VI: feature learning in infinite depth neural netIn The Twelfth International Conference on works. Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=17pVDnpwwl. Yuan, M., Lang, B., and Quan, F. Student-friendly Knowl. Based Syst., 296: knowledge distillation. 111915, 2024. doi: 10.1016/J.KNOSYS.2024.111915. URL https://doi.org/10.1016/j.knosys. 2024.111915. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? In Korhonen, A., Traum, D. R., and Màrquez, L. (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pp. 47914800. Association for Computational Linguistics, 2019. doi: 10.18653/V1/P19-1472. URL https://doi.org/10.18653/v1/p19-1472. Zhang, B. and Sennrich, R. Root mean square layer normalization. In Wallach, H. M., Larochelle, H., Beygelzimer, A., dAlché-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 12360 URL https://proceedings. 12371, 2019. neurips.cc/paper/2019/hash/ 1e8a19426224ca89e83cef47f1e7f53b-Abstract. html. Zhang, C., Raghu, M., Kleinberg, J. M., and Bengio, S. Pointer value retrieval: new benchmark for understanding the limits of neural network generalization. CoRR, abs/2107.12580, 2021. URL https: //arxiv.org/abs/2107.12580. Zhang, C., Song, D., Ye, Z., and Gao, Y. Towards the law of capacity gap in distilling language models. CoRR, abs/2311.07052, 2023a. doi: 10.48550/ARXIV. 2311.07052. URL https://doi.org/10.48550/ arXiv.2311.07052. Zhang, C., Yang, Y., Liu, J., Wang, J., Xian, Y., Wang, B., and Song, D. Lifting the curse of capacity gap in 18 Distillation Scaling Laws distilling language models. In Rogers, A., Boyd-Graber, J. L., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 45354553. Association for Computational Linguistics, 2023b. doi: 10.18653/ URL https://doi. V1/2023.ACL-LONG.249. org/10.18653/v1/2023.acl-long.249. Distillation Scaling Laws"
        },
        {
            "title": "Appendices",
            "content": "A Limitations Extended background B.1 Knowledge Distillation . B.2 Neural Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 The Knowledge Distillation Capacity Gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Teacher Student Capacity Gaps C.1 Kernel Regression . C.1.1 Setup . . . . . . . . . . . . . . . C.1.2 Distilling the Teacher . . . . . . . C.1.3 U-shape in the student error C.2 MLPs on the Mapping Problem . C.2.1 Problem Definition . . . C.2.2 Experimental Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Distillation scaling law applications (additional results) D.1 contradiction with patient teachers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Fixed tokens or compute (best case) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Fixed size or compute (teacher inference) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Compute optimal distillation . D.4.1 Setup . . . . . D.4.2 Cross-entropy . . . . . . . . . D.4.3 Distillation (best case) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.4 Distillation (teacher inference) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.5 Distillation (teacher pretraining) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.6 Distillation (teacher pretraining + inference) . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.7 Optimal teacher training and student distillation tokens . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.8 Optimal teacher size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Compute and data efficiency gains for distillation compared to supervised learning . . . . . . . . . . . . Additional Results E.1 Downstream evaluations . . E.2 Teachers used in distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Fixed-M teacher/fixed-M students and the capacity gap . . . . . . . . . . . . . . . . . . . . . . . . . . 20 22 22 22 23 24 24 24 25 27 27 28 29 29 31 33 33 33 35 38 39 41 42 43 45 46 46 Distillation Scaling Laws E.4 Full distillation scaling law IsoFLOP profiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 Distillation scaling law IsoFLOP optima . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.6 Distillation with infinite data . . E.7 Weak-to-strong generalization . E.8 Model calibration . E.8.1 Teachers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.8.2 198M students trained on 20N tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.8.3 198M Students trained on 128B tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scaling coefficients F.1 Supervised scaling law coefficient estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Distillation scaling law coefficient estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Scaling law coefficients parameteric fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Distilling language models in practice G.1 Mixing coefficient (λ) sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Temperature (τ ) sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3 Learning rate (η) sensitivity analysis, verification of µP for distillation . . . . . . . . . . . . . . . . . . . G.4 Distribution truncation methods: Top-k and Top-p sensitivity . . . . . . . . . . . . . . . . . . . . . . . . G.5 Forward and reverse KL divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Parameters and Floating Operation Estimation H.1 Alternative approximation for FLOPs per token as function of . . . . . . . . . . . . . . . . . . . . . H.2 Model parameters . H.3 FLOPs per token . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Model architecture Contributions 47 48 48 49 50 51 54 56 56 57 57 58 59 60 61 62 62 64 65 67 21 A. Limitations This work has several limitations that we are aware of: Distillation Scaling Laws Our work is performed in the language modeling setting only. Although there is good evidence that the functional form of scaling laws applies across domains (Henighan et al., 2020), we cannot be absolutely certain that distillation behaves in the way we describe in this work in all domains. We perform our analysis on the English subset of C4 dataset (see Appendix I). This means that for our larger token runs, data has been repeated. Although it was shown in Muennighoff et al. (2023b) that on the C4 dataset, repeating data up to 4 times has negligible impact to loss compared to having unique data, this was shown in the supervised setting, and we cannot be absolutely certain that the same applies in the distillation setting. second downside of using the C4 dataset is that we are limited in our ability to analyze downstream evaluations of students resulting from distillation. Our performance over standard English language downstream tasks closely follows cross-entropy, however, C4 is not as well suited for pretraining in order to probe aspects like reasoning performance (see Appendix E.1). We focused on distillation as originally defined in Hinton et al. (2015), where the teacher produces full probability distribution for the student to target. More colloquially, distillation has become used to describe the more general process of using teacher in order to produce student. One popular approach for training language models is Sequence-Level Knowledge Distillation (Kim & Rush, 2016) where the teacher is sampled, e.g. with beam search, in order to produce sequences for training the student on in supervised way. This technique, also called synthetic data or hard distillation has been employed to great effect in the LLaMA families (Touvron et al., 2023a) and most recently, the smaller models distilled from DeepSeek-R1 (DeepSeek-AI et al., 2024). While we anticipate that our broader findings also apply in the Sequence-Level Knowledge Distillation, we cannot be absolutely sure. We suggest that verifying the scaling properties of Sequence-Level Knowledge Distillation in controlled, resource constrained manner as we have done here is important for future study. B. Extended background This section reviews related work on knowledge distillation, capacity gap phenomena, neural scaling laws, and foundation models, highlighting their relevance to our study. B.1. Knowledge Distillation Bucila et al. (2006) provided strong evidence that the knowledge gained by large ensemble of models can be effectively transferred to single smaller model. Later, Hinton et al. (2015) introduced knowledge distillation, where smaller student network learns from larger teacher network by mimicking its softened output probabilities, improving efficiency and generalization. Building on this, Stanton et al. (2021) studied both fidelity and student generalization, showing that while knowledge distillation often improves generalization, it frequently fails to achieve high fidelity, as student models do not fully match the teachers predictive distribution. We study fidelity in terms of calibration in Appendix E.8, and show that when the learning signal is consistent with the calibration measure, then the student in our setup is well-calibrated both with respect to the teacher and the actual data. Addressing this, Beyer et al. (2022) demonstrated that knowledge distillation is most effective when the teacher is patient and consistent, providing stable targets over prolonged training to improve student generalization and fidelity. Our Language Model (LM) setup automatically satisfies consistency: both the teacher and student see the same data during the students training. However, our conclusions differ from those of Beyer et al. (2022) in that although distilling student for longer does improve its performance, unless the teacher is chosen perfectly, distillation becomes less effective than supervised learning in the patient setting, see Appendix D.2 for discussion. Beyond empirical insights, Menon et al. (2020) established bias-variance tradeoff for the student, quantifying how access to teacher logits can significantly enhance learning. Meanwhile, Pareek et al. (2024) investigated self-distillation, where the student and teacher share the same architecture and size, to assess the potential gains from repeatedly applying knowledge distillation. While most studies assume the teacher is larger model, recent work explores weak-to-strong generalization, where weaker model distills knowledge into stronger one. This concept, introduced by Burns et al. (2024) and studied in LMs, was further analyzed by Ildiz et al. (2024), who extended the theoretical analysis to high-dimensional data and over-parameterized regression. Their findings show that distillation can provably outperform training with strong labels 22 Distillation Scaling Laws under the same data budget but does not improve the data scaling law. Our distillation scaling law (Equation 8) confirms this finding, which for fixed teacher cross-entropy does not improve the scaling law compared to the supervised one in Equation 1. Moreover, in many previous works, distillation happens with repeated data, that is, the student sees the same data as the teacher does during its training. In our setup, we do not repeat the data between teacher training and distillation, which allows us to examine only the effect of distillation rather than the possible diminishing returns of repeated data; see Muennighoff et al. (2023a) for more details on the effect of repeating data. B.2. Neural Scaling Laws Predictable scaling trends in neural networks were first empirically observed by Hestness et al. (2017) and later by Kaplan et al. (2020) who established empirical scaling laws for language model performance based on cross-entropy, which led to Hoffmann et al. (2022) and the pursuit of compute-optimal training. Beyond the empirical studies, there have been many theoretical works which provide explanations for why scaling laws should exist (Bahri et al., 2021; Paquette et al., 2024; Havrilla & Liao, 2024). More recent works explore scaling laws across different distributions, closely related to knowledge distillation. Hernandez et al. (2021) derived scaling law for transfer learning, analyzing effective data transfer in low-data regimes and diminishing returns in high-data regimes. Similarly, Barnett (2024) empirically studied pretraining on one distribution for optimizing downstream performance on another, showing that when the transfer gap is low, pretraining is cost-effective strategy. Finally, Jain et al. (2024) theoretically analyze how additional data from surrogate model affects generalization, demonstrating that surrogate data can reduce test erroreven when unrelateddue to Steins paradox (Stein, 1956), with test error following scaling law. This setup is related to tuning the coefficient λ in our case, where we also observe U-shape behavior depending on the teacher and student sizes (see Figure 51a). However, we are interested in studying the effect of distillation only (λ = 1.0), which differs from their setup. While these works are closely related to knowledge distillationsince one can compare the distribution of the teacher logits to that of the studentthey do not establish distillation scaling law. Moreover, their setup differs from practical knowledge distillation, as it does not involve training new student model using teacher but instead studies the effect of transferring training knowledge to downstream task. Our work is the first to determine and verify distillation scaling law and examine the regions where one should distill as well as the regions where supervised pretraining outperforms distillation; see Figures 6, 7 and 14 in Appendix D.2 and Section 5.2. Finally, for improving inference cost at given model capability, the scaling behavior of Mixture of Experts (MoE) (Shazeer et al., 2017; Jelassi et al., 2024) have been investigated in the context of scaling laws (Clark et al., 2022; Ludziejewski et al., 2024; Abnar et al., 2025) as one alternative to knowledge distillation. B.3. The Knowledge Distillation Capacity Gap Despite extensive research on knowledge distillation, persistent challenge is the curse of capacity gap, where larger teacher does not necessarily produce superior student compared to smaller teacher. This occurs because large gap in model capacity makes it harder for the student to effectively learn from the teachers outputs. As result, there exists an optimal teacher size along the scaling trajectory that maximizes student performance. Our distillation scaling law in Equation 8 confirms this, revealing u-shaped trend in the scaling law and validating the existence of an optimal teacher. However, our results further indicate that the capacity gap is influenced not only by the size of the teacher but also by its training tokens and, more generally, its loss. theoretical analysis in the kernel regression setup (Appendix C) supports these findings. Lukasik et al. (2022) showed that distillation gains are not uniform and can even degrade performance when small teacher errors are amplified by the student. Similarly, Nagarajan et al. (2023) found that deviations in predictive probabilities cause students to exaggerate the teachers confidence levels. Several works (Peng et al., 2024; Zhang et al., 2023a; Rawat et al., 2024) observed the capacity gap in pre-training distillation for Large Language Model (LLM)s, affecting both large-to-small and small-to-large distillation. Notably, Zhang et al. (2023a) proposed an empirical law of the capacity gap, showing that the optimal teacher scale follows an approximately linear relationship with the students scale. However, our findings suggest that scaling alone is insufficientone must account for the complexity of the effective hypothesis space (Equation 8) and we show that Zhang et al. (2023a) is special case of our work when the teachers are compute-optimal from supervised perspective (see Section 5.3). To address this issue, various strategies have been explored. Yuan et al. (2024) studied temperature scaling, which simplifies the teachers output into more learnable representations, aiding student generalization. We analyzed the effect of temperature and learning rate in distillation (Figures 52 and 53) and found that, contrary to existing literature, the optimal temperature is one. We hypothesize that this discrepancy arises because previous studies used repeated tokens, whereas our setup does not involve repeated data. Additionally, Cho & Hariharan (2019) found that early stopping of the teachers training mitigates the capacity gap, while Mirzadeh et al. (2020) proposed progressive distillation, where knowledge is transferred through intermediate models to improve student learning. From 23 Distillation Scaling Laws theoretical perspective, Harutyunyan et al. (2023) analyzed the capacity gap in distillation using supervision complexity in kernel classifiers. Their findings highlight trade-off between teacher accuracy, student margin with respect to teacher predictions, and teacher complexity, explaining why some teachers are easier for the student to learn. Earlier, Lopez-Paz et al. (2016) studied generalization error in distillation, proving that learning from teacher can be beneficial under certain conditions, particularly when the teachers capacity is small. Using similar techniques in LMs, Zhang et al. (2023b) demonstrated that among students of different capacities distilled from the same teacher, smaller students suffer from higher generalization error and lower performance, while larger teachers provide lower generalization error, reinforcing the trade-off in teacher-student capacity. Our distillation scaling law (Equation 8) also confirms this trend, and we observe the effect of capacity gap in our scaling law terms, see Section 4.3 for more details. Foundation model pretraining Foundation models were initially undertrained (Brown et al., 2020), then followed the compute-optimal scaling law carefully (Hoffmann et al., 2022; Pearce & Song, 2024; Besiroglu et al., 2024), and soon after started overtraining heavily (Sardana et al., 2024; Bi et al., 2024; Hu et al., 2024; Mesnard et al., 2024; Jiang et al., 2023). The LLaMA family (Touvron et al., 2023a;b; Dubey et al., 2024) and Phi line (Li et al., 2023; Abdin et al., 2024b;a) is following the same trend, where smaller models are overtrained according to the original Chinchilla scaling laws. In all these cases, the models are designed to be best possible foundation model that is still cheap and fast to run on lower end hardware. Besides overtraining, more recently, smaller foundation models tend to be distilled from larger models (Gunter et al., 2024; Rivière et al., 2024; Reid et al., 2024) to further increase performance. In these cases, the large model either specifically trained with the sole purpose of being distillation teacher, or an existing model is re-used. In both these cases, there are no reports of how the exact teacher size is decided when taking total compute into account. Determining the optimal allocation of compute budget in the distillation setting is one of the primary contributions of our work (see Section 5.3). C. Teacher Student Capacity Gaps In this section, we examine the capacity gap in two settings: kernel regression and synthetic example using Multi-Layer Perceptron (MLP) for mapping problem. The kernel regression setup provides theoretical and analytically tractable perspective on the capacity gap. The MLP-based synthetic example allows us to study the capacity gap in more practical, learnable function approximation scenario. By analyzing these two setups, we aim to better understand the fundamental limitations of distillation when there is significant mismatch between teacher and student capacities. C.1. Kernel Regression One of our main contributions is that the student loss follows broken power law, where the transition between the two power law regions occur when the student becomes stronger learner than the teacher (Equation 8). This implies that making the teacher too capable (relative to the student) reduces student performance. In this section we show how capacity gap provably degrades student performance in the setting of kernel regression. While simple, we believe the underlying principle causing the student performance degradation in this case carry over to much more general settings involving neural networks. C.1.1. SETUP Let denote the target function, identified by set of coefficients α = ϕi i=1 such that denote Hilbert space spanned by orthonormal bases functions } { R, i=1 αi { ϕi, ϕj = < α } = δij. Let such that: denote the teacher and student Hilbert spaces respectively: i=1 (cid:88) Let , (x) = αiϕi(x). which are the hypothesis spaces of the teacher and student. Note that while the Hilbert space is spanned by an infinite orthonormal basis, the teacher and student spaces are finite and spanned by and basis functions respectively, where represents the teacher and student capacity gap. m ϕ1, ϕ2, ..., ϕm = Span { ϕ1, ϕ2, ..., ϕn = Span { , } , } (11) (12) (13) The process of training the teacher and student models involves solving the following constrained optimization problems: Distillation Scaling Laws where g, are the optimal teacher and student respectively, and student are exposed to an infinite amount of training data, hence our analysis is carried over entirely in function space. Lemma C.1. The optimal teacher is given by: = min gHm = min hHn < . Note that we assume the teacher and H s.t s.t (14) (15) D, T, g(x) = C(m, ) αiϕi(x), C(m, ) = The teacher error i=1 (cid:88) teacher(m, ) is given by: 1 (cid:80)m i=1 α2 (cid:40) i=1 α2 otherwise. (cid:112)(cid:80) e teacher(m, ) = f = 1)2 α2 + α2 . i=1 (cid:88) i=m+1 (cid:88) (C(m, ) (cid:118) (cid:117) (cid:117) (cid:116) (16) (17) Proof. By construction we may assume the teacher model takes the form = can write the error of using: (cid:80) eteacher(m, T, β) = (βi αi)ϕi + αiϕi i=1 βiϕi. where i=1 β2 . We (cid:112)(cid:80) α2 . αi)2 + (18) (βi i=m+1 (cid:88) Note that the minimizing coefficients β of Equation 18 must take the form β = Cα for some coefficient C. Considering the norm constraint on g, the constant takes the form in Equation 16. Plugging the resulting into the expression for eteacher(m, T, β) completes the proof. i=m+1 (cid:88) (cid:13) (cid:0) (cid:13) (cid:13) i=1 (cid:88) i=1 (cid:88) (cid:13) (cid:13) (cid:13) = (cid:118) (cid:117) (cid:117) (cid:116) Notably and intuitively, the teacher error decreases monotonically as m, which represents the teacher model capacity, increases. C.1.2. DISTILLING THE TEACHER We now pick our student function by mimicking the teacher subject to norm constraint: s.t. Lemma C.2. Let = min(m, n) be the smaller of the teacher and student capacities. The optimal student is given by: (19) D. h(x) = min hHn h = Q(m, k, T, D)C(m, ) αiϕi Q(m, k, T, D) = 1 i=1 (cid:88) C(m, ) i=1 α2 < C(m,T )(cid:80)k i=1 α2 otherwise. (cid:113)(cid:80) The student error with respect to the target function is then: estudent(m, n, T, D) = = (C(m, )Q(m, k, T, D) (cid:118) (cid:117) (cid:117) (cid:116) 1)2 α2 + α2 i=1 (cid:88) i=k+1 (cid:88) (20) (21) (22) Proof. The proof follows the exact same logic as in Lemma C.1. = functions (cid:80) additional Q(m, k, T, D) multiplier in Equation 21. i.e, we can assume the optimal student is given by i=1 γiϕi. From the distillation loss, the optimal coefficients must match the teacher coefficients for the basis D. This rescaling then gives rise to the i=1, perhaps rescaled due to the norm constraint } i=1 γ2 ϕi { (cid:112)(cid:80) Distillation Scaling Laws C.1.3. U-SHAPE IN THE STUDENT ERROR"
        },
        {
            "title": "We will prove that the map",
            "content": "m estudent(m, n, T, D) (cid:55) is comprised of two distinct segments: i) where the student error monotonically decreases for < n, and ii) where it n, establishing U-shape in the student error echoing the trend seen in Figures 3 and 4. monotonically increases for Case 1: < n. (Student error is non-increasing in m) Claim. For 1 < n, we have In words, when < n, the error does not increase (and typically decreases) as the teacher capacity increases. estudent(m + 1, n, T, D) estudent(m, n, T, D). Proof."
        },
        {
            "title": "Let",
            "content": "m,T t denote the space of functions in { m+1,T Since hence it monotonically decreases. Now, let any < n, we can equivalently write the optimal student m+1,T , it follows that m,T m,T t = that are norm constrained by D. i.e: : . } (23) , which implies that the teacher error cannot increase as increases, , then for denote the optimal student given the teacher m. Since mn = min hHn = min hHm as the solution to the following optimization problem: m s.t s.t D, H (24) (25) which corresponds exactly to the objective of finding the optimal teacher with with norm constraint set to D. Therefore, from the fact that the teacher error monotonically decreases we can conclude that the student error monotonically decreases as well in the regime < n. Case 2: Claim. For n. (Student error eventually increases in m) n: estudent(m + 1, n, T, D) estudent(m, n, T, D). Hence once exceeds the student error cannot decrease any further, the error eventually starts to rise. Proof. Let β = i=1 β2 β1, ..., βm { denote the coefficients of the optimal teacher (i.e the norm of the coefficients corresponding to the basis n, as long as is smaller than D), we have from Equation 21 that Q(m, k, T, D) = 1, which means that the optimal student doesnt change, hence its error remains constant. (cid:112)(cid:80) If however m. Note that in the regime < D, then we have from Equation 21: ϕ1, ..., ϕn { i=1 β } } (cid:112)(cid:80) 1 > Q(m, k, T, D) Q(m + 1, k, T, D), (26) where the second inequality becomes strict if α2 m+1 > 0. strict inequality (i.e Q(m, k, T, D) > Q(m + 1, k, T, D)) implies the optimal student is further scaled down due to the teacher having to \"spread its capacity\" to additional basis functions that are not learnable by the student, thereby strictly increasing its error. Hence for n, we get estudent(m + 1, n, T, D) estudent(m, n, T, D), demonstrating that the error increases monotonically with once n. Conclusion (U-shaped trend). Combining these two cases: For < : estudent(m, n, T, D) monotonically decreasing in m, (cid:40) For : estudent(m, n, T, D) monotonically increasing in m. 26 Distillation Scaling Laws Therefore, as function of m, the student error estudent(m, n, T, D) first decreases and then increases (for n), giving u-shape in student error due to capacity gap between the teacher and the student. n) (for We present an empirical verification of these conclusions in Figure 10. Figure 10. Distillation in kernel regression. We randomly sample the α = {α1, ..., α1000} coefficients of the target function uniformly in the range [1, 1]. We fix = 5, = 4.5 and compute the optimal student and teacher errors according to Lemmas C.1 and C.2 for various values of (dashed curves), and for [1...1000]. As can be seen, the student error exhibits shaped error curve as predicted by the theory, where the error starts to increase when n. The black solid line indicates the teacher error, which always decreases with increasing m. The above theoretical analysis points to an intuitive interpretation of the potentially adverse effect of large teacher-student capacity gap; the degradation in student performance is due to the teacher learning basis functions that are unreachable by the student, at the expense of basis functions that are reachable by the student. In the following we provide empirical evidence in support of this picture in controlled yet more realistic setting. C.2. MLPs on the Mapping Problem C.2.1. PROBLEM DEFINITION Here we show synthetic setting which exhibits the U-shape phenomenon. Matching the kernel regression analysis (Appendix C.1), we find that the synthetic problem must include class of problems that are easy for the student to learn, and ones that are harder, in order for the u-shape to appear. The problem setting is the Mapping Problem, and is similar in spirit to Pointer Value Retrieval (Zhang et al., 2021), Here, the input is composed of small integers in {0,1,2}. The label for each sample is given by the code below, which shows the two cases: i) one where the label is simply given by one-hot position, and ii) one where the label is given by the location of matching element in the context portion of the input. def find(vector, value): \"\"\"Find locations of value in vector.\"\"\" return np.where(vector == value)[0] def remove(vector, value): \"\"\"Find value from vector.\"\"\" return np.delete(vector, find(vector, value)) def label(vector: np.ndarray, num_classes: int) -> np.ndarray: \"\"\"Return the label in [0, num_classes) for vector.\"\"\" assert len(vector) == 2 * num_classes one_hot = vector[num_classes:] context = vector[:num_classes] = find(one_hot, 1) if context[i] == 0: return else: # remapping = context[i] return remove(find(context, c), i) Examples: ----------------------------- 2020210001000000, label = 1 context [2 0 2 0 2 1 0 0] one-hot [0 1 0 0 0 0 0 0] ----------------------------- 1210120000000100, label = 2 context [1 1 2 0 1 2 0 0] one-hot [0 0 0 0 0 1 0 0] ----------------------------- 0122221201000000, label = 6 context [0 1 2 2 2 2 1 2] one-hot [0 1 0 0 0 0 0 0] ----------------------------- C.2.2. EXPERIMENTAL FINDINGS Distillation Scaling Laws We train MLPs with two hidden layers of equal width, all non-linearities are Rectified Linear Units (ReLUs). Teachers and students of different sizes are produced by varying the hidden layer width only. 104, single cycle cosine All model are trained with Adam (Kingma & Ba, 2015) using peak learning rate of 3 learning rate schedule with linear warmup of 5% of the total training steps. batch size of 512 is used for all models. Training samples are never repeated. Unless explicitly stated, model are trained on 500 512, or 20N samples, where is the number of model parameters, whichever is larger. In Figure 11, we look at varying the size of the teacher. For the width 256 model, student performance improves as the teacher size increases to point, and then student performance worsens. This is observable in both the student cross-entropy (Figure 11a) and accuracy (Figure 11b). Aligning with theory and large-scale experiments, the student cannot learn if it is too small, and can learns to match the teacher model when ther student is large enough. In the intermediate regime, where distillation is often used, we see an optimal teacher size and capacity gap phenomenon. (a) Cross-entropy (b) Accuracy Figure 11. Student performance when varying teacher width. (a) Student cross-entropy as teacher width dffn is varied. (b) Student accuracy as teacher width dffn is varied. Bands show the (25%,75%) values across four trials. In Figure 12, similar effect can be seen, when large teacher (dffn = 512) is trained with on different amounts of data. This observation aligns with the idea that it is the teachers completeness in modeling the problem that eventually harms the performance of student with lesser capacity, and not only the teacher size. (a) Cross-entropy (b) Accuracy Figure 12. Student performance when varying teacher training data. (a) Student cross-entropy as teacher training data is varied. (b) Student accuracy as teacher training data is is varied. Bands show the (25%,75%) values across four trials. 28 D. Distillation scaling law applications (additional results) Distillation Scaling Laws In this section, we present results referenced in Section 5. We explore the best-case scenario for distillation under fixed student tokens or compute, as well as under fixed teacher size or compute, while accounting for teacher inference. These results provide further insights into the optimal distillation strategies in different resource-constrained settings. D.1. contradiction with patient teachers Beyer et al. (2022) showed in computer vision that good teacher is: 1. Patient. Distillation works best when training for large number of epochs, and 2. Consistent. The teacher and the student see the same views of the data under an augmentation policy. Our setting automatically satisfies consistency as there is no augmentation policy. There is remaining question about patience, which in our scenario corresponds to the large DS limit. We observe that for given student size: 1. If the teacher is optimally chosen for the student, distilling on large number of tokens produces the same result as training the model in supervised way on the same number of tokens (Appendix E.6). 2. Otherwise supervised learning outperforms distillation (Section 5.3). The behavior we observe is expected if solutions model can access are limited only by function space. Uncertain of the driver behind our conclusion differences, we note the differences between our experimental setups in Table 4. Table 4. Experimental setting differences between Beyer et al. (2022) and ours. Component Beyer et al. (2022) Ours Data repetitions Many repetitions Data diversity Domain Objective Architecture Minimal repetitions Large number of unique tokens Low number of unique tokens Language Vision Fewer categories, more unimodal Many categories, highly multimodal Different computer vision architectures Maximal Update Parameterization (µP) optimized homogeneous transformers D.2. Fixed tokens or compute (best case) Distillation can outperform supervised learning given enough teacher training tokens or compute. As shown in Figures 13a and 13b, when the teacher size, student size, and number of student tokens are held constant, increasing the number of teacher training tokens makes distillation more favorable than supervised learning. This advantage arises because the teacher, with access to more training tokens, can better learn the approximation of the language distribution. As result, the teachers learned distribution become more informative for the student to follow, thus improving the students performance. Note that for fixed student size and compute, the teacher must be sufficiently large and well-trained; otherwise, supervised learning will outperform distillation. Without adequate teacher size or training, the student may not benefit from the distillation process, leading to inferior performance compared to direct supervised learning. We also see that the scatter data matches up well with the contour colors, despite these contour beings difference of two scaling laws, providing verification of our setup. Supervised learning always outperforms distillation given enough student compute or tokens. The trend observed in Figure 14 mirrors that of Section 5.1. It demonstrates that, for fixed teacher size and compute, supervised learning can outperform distillation when the students compute is sufficiently large. With enough resources allocated to the student, it can learn more effectively from the data directly, making distillation less advantageous in comparison. This advantage only happens at compute budget that grows with student size. 29 Distillation Scaling Laws (a) Fixed data (b) Fixed compute Figure 13. IsoFLOP Teacher Contours with Fixed students. (a) For given teacher size NT , for given teacher token DT , what is the difference between the loss achieved by distillation and supervised learning. Blue indicates distillation outperforms supervised learning, and red indicates when supervised learning outperforms distillation. The white horizontal dashed line indicates the student size. (b) For given teacher size NS, for given teacher compute budget, what is the difference between the loss achieved by distillation and supervised learning. Blue indicates distillation outperforms supervised learning, and red indicates when supervised learning outperforms distillation. The white horizontal dashed line indicates the student size. Figure 14. Fixed Teacher Contours with IsoFLOP students (compute). For given student size NS, for given student compute budget, what is the difference between the loss achieved by distillation and supervised learning. Blue indicates distillation outperforms supervised learning, and red indicates when supervised learning outperforms distillation. The white horizontal dashed line indicates the teacher size. 30 D.3. Fixed size or compute (teacher inference) Distillation Scaling Laws Fixed student size For fixed student size, as the number of student tokens increases, the optimal teacher cross-entropy decreases slightly; see Figure 15. This observation highlights an asymmetry between the growth of student size and student tokens (or their rates in the scaling law), as the behavior here differs from that observed in Section 5.1. Notably, when the student size is sufficiently large, such as NS = 30B, increasing the student tokens initially leads to decrease in the teachers loss, followed by saturation point and slow decrease in the optimal teachers loss. For four distillation student sizes NS Figure 15. Student performance given teacher varying distillation tokens. {1B, 3B, 10B, 30B} the validation loss achieved by students distilled on DS [250B, 16T ] tokens under teacher with loss LT [E, 2.5]. The red line indicates the value of the teacher loss resulting in the best performing student, and the vertical dashed line indicates the number of tokens at which supervised pretraining outperforms distillation. Fixed compute budget Given an inference budget NS, set of teachers CTotal, the number of distillation tokens is determined from Equation 9 (L(i) { , (i) ) i=1 and total compute budget } DS = CTotal/(3F (NS) + δTLogitsF (NT )), (27) where (N ) is the forward Floating Operations (FLOPs) per token of model of size (see Appendix H). If δTLogits = 0 then there is no price to pay for larger teacher, and the conclusions are identical to those of the fixed token analysis of Section 5.2. In the worst case scenario, δTLogits = 1, then using larger teacher will mean fewer distillation tokens are available for the student. Due to the capacity gap phenomenon, at small compute budgets, this means it is actually better to use large weak teacher rather than large strong teacher. Once compute is sufficient to allow enough distillation tokens, stronger teacher can be used for all student sizes (see Figure 16). 31 Distillation Scaling Laws Figure 16. Fixed compute distillation strategy. The student performance obtained for four total compute budgets CTotal {1021, 1022, 1023, 1024} FLOPs and four student sizes NS {1B, 3B, 10B, 30B} under teacher of size NT [1B, 1T ] and teacher loss LT [E, 2.5]. The red line indicates the value of teacher loss (NT ) that results in the best student performance for each teacher size NT . 32 Distillation Scaling Laws Table 5. Scenarios considered in our scaling law applications. Same as Table 2. Compute Scenario δLgt δPre Description Best case (fully amortized teacher) Teacher inference Teacher pretraining Teacher pretraining + inference 0 1 0 D.4. Compute optimal distillation D.4.1. SETUP 0 The teacher produces no additional FLOPs and so we are free to choose the teacher that minimizes the student cross-entropy. 0 We dont account for the teacher cost because the teacher already exists, or we intend to use the teacher as e.g. server model. We still need to pay to use it for distilling student. The teacher needs training, but we store the logits for re-use, either during training, or after training for distilling into sufficiently many students. The teacher needs training and we pay for distilling into one student, the worst case scenario. 1 The solutions resulting in the losses give guidance on how to scale depending on the use case, and are the result of constrained optimization , FLOPs(NS, DS, NT , DT ) = C, LS(NS, DS, NT , DT ) S, (28) s.t. = arg min DS ,NT ,DT where LS(NS, DS, NT , DT ) is the distillation scaling law (Equation 8), and FLOPs(NS, DS, NT , DT ) 3F (NS)DS +F (NT )(δLgt + δPre 3DT ) (29) Teacher Student Training Training is the total number of floating operations performed in the entire distillation setup. (N ) is the forward FLOPs per (cid:125) (cid:123)(cid:122) (cid:124) (cid:123)(cid:122) token of model of size (see Appendix H), and δLgt , δPre [0, 1] indicate if we account for the cost of teacher logit inference for the student targets and teacher pretraining cost in the total compute budget. For convenience, we restate our compute scenarios of interest in Table 5). Constrained numerical minimization using Sequential Least SQuares Programming (SLSQP) (Kraft, 1988) in SciPy (Virtanen et al., 2019). We allow numerical solutions for model sizes [1M, 100P ]. While this token upper-limit is larger than available resources (Epoch AI, 2023), and tokens NT , DS, DT it simplifies discussions when comparing to supervised learning at large compute budgets, which otherwise, for smaller students, would only by using fraction of the available compute. (cid:125) (cid:124) DS Teacher Logits (cid:124) (cid:123)(cid:122) (cid:125) We begin by looking at the student cross-entropy achievable in each compute scenarios alongside the corresponding teacher cross-entropies in Appendix D.4.2. We then investigate the compute-optimal distillation configurations for each scenario that produce those cross-entropies. We look at best case distillation in Appendix D.4.3, teacher inference in Appendix D.4.4, teacher pretraining in Appendix D.4.5, and teacher pretraining + inference in Appendix D.4.6. Finally, to aid comparisons across methods, we present the token and parameter configurations for all methods in Appendix D.4.7 and Appendix D.4.8 respectively. For completeness, in the following sections, some of the findings of Section 5.3 are restated. D.4.2. CROSS-ENTROPY In Figure 17 we show the student cross-entropies achieved in the compute optimal case for each scenario in Table 5, and the teacher cross-entropies that enable those student cross-entropies in Figure 18. Distillation and supervised learning produce the same student at large compute. The first thing to note in Figure 17 is that at low compute, in the best case and teacher inference scenarios, distillation outperforms supervised learning, consistent with our expectations from distillation and the existing literature (see Appendix B.1). However, once enough the compute is large enough6, distillation and supervised learning produce models with the same cross-entropy, i.e. in general, 6The level of compute at which this happens is larger for larger models, see Figure 17 for specific values. Distillation Scaling Laws distillation does not allow us to produce better models that supervised learning does, however, distillation does produce better models than supervised learning with modest resources. This behavior is consistent with the asymptotic analysis in Appendix E.6, and can be understood through noting that although distillation modifies the learning process the student undergoes, distillation does not alter the hypothesis space of the student, which is tied to the student size NS, is the same hypothesis space in the supervised and distillation settings, and can be explored in the limit of infinite compute or data. Figure 17. Compute optimal distillation student cross-entropies. For eight student sizes, the optimal student validation cross-entropy in each of the distillation scenarios considered as the total compute is varied. The compute at which distillation and supervised learning produce similar models grows with student size. Continuing the previous observation, we see in Figure 17 that supervised cross-entropy approaches the best case and teacher inference student cross-entropies at value of compute which increases with compute, meaning that larger students benefit from distillation for larger compute budgets than supervised learning. This implies that if your target student size is small and your compute budget is large, then supervised learning is more likely to be beneficial than if your target student size is larger. The phenomenon happens because larger supervised models saturate in performance at larger values of (Equation 1), and distillation accelerates progress towards this saturation with the correct choice of teacher (Equation 8), with more capable teachers producing more gains per token. Including teacher training in compute produces student cross-entropies higher than in the supervised setting. In Figure 17 supervised cross-entropy is always below the teacher pretraining and teacher pretraining + inference scenarios, except at very large compute budgets, when supervised learning and these distillation scenarios produce similar student cross-entropies. This means that if your only aim is to produce the model of target size with the lowest cross-entropy and you do not have access to teacher, then you should choose supervised learning, instead of training teacher and then distilling. Conversely, if the intention is to distill into family of models, or use the teacher as server model, distillation may be more computationally beneficial than supervised learning. This finding aligns with expectations, the alternative implies distillation can outperform direct maximum likelihood optimization given fixed compute. The optimal teacher cross-entropy decreases with increasing total compute. As shown in Figure 18, the optimal teacher cross entropy loss has decreasing trend with respect to the total compute. However, in the best case scenarios, at low compute for larger student, where the number of student tokens is lower than the Chinchilla rule of thumb, an inflection point happens in optimal teacher compute. We now turn to investigating the optimal distillation configurations that achieve these student cross-entropies. 34 Distillation Scaling Laws Figure 18. Compute optimal distillation teacher cross-entropies. For eight student sizes, the optimal teacher validation loss resulting in lowest student validation loss in each of the distillation scenarios considered  (Table 5)  the total compute is varied. D.4.3. DISTILLATION (BEST CASE) In the distillation (best case) scenario, δLgt the standard supervised learning case = δPre = 0, which means that we only account for compute associated with FLOPs(NS, DS, NT , DT ) 3F (NS)DS . (30) We call this best case as the scenario reflects freedom to choose the best distillation setting for given student size NS, with all of the compute being put into training the student for as long as possible (maximal DS). In this sense we can consider this the upper bound in performance for distillation in our experimental setting. Student Training (cid:123)(cid:122) (cid:124) (cid:125) Figure 19. Compute optimal configuration contours for distillation (best case). The compute optimal quantities (D giving rise to the student cross entropies for best case in Figure 17 for range of student sizes. (N optimal combination giving rise to , ) ) are the supervised compute in Figure 18. S, , This scenario represents the setting where teacher already exists, or we will use the teacher for another purpose, for example server model. In these scenarios, we do not need to worry about the teacher pretraining cost. Additionally, this teacher may be used to produce the logits for many different students, or we may have saved the logits from the teacher during its training. In these cases, the cost for producing the student logits can also be ignored. The optimal quantities (D In the best case scenario, compute constraint, yielding one-dimensional family (NT (L (Equation 28). To provide some guidance for producing (NT (L ) giving rise to the cross entropies in Figure 17 are shown in Figures 19 and 20. are not determined because they do not enter into the , DT ), DT ) of valid solutions to the minimization problem , in Figure 18 we present the supervised compute optimal S, is determined, however , DT ), DT ), i.e. the combination that minimizes FLOPs (NT )DT subject to L(NT , DT ) = LT . and , 35 Distillation Scaling Laws Figure 20. Compute optimal configurations for distillation (best case). For eight student sizes, the compute optimal quantities (D S, ) are the supervised compute optimal combination giving rise to ) giving rise to the student cross entropies for best case in Figure 17. (N in Figure 18. This is one-dimensional slice of Figure 19. , , In this scenario, all the compute goes into student tokens, and so in Figure 20 we see optimal student tokens increases with compute at the same rate as we could for the supervised model, which is higher for smaller students. The optimal , teacher parameters in Figure 20 represent the supervised compute optimal solution for producing the , but are not the only solution in this compute scenario, since in Figure 18. Again, the exact values of are not uniquely determined by the compute constraint. move together to produce the T and tokens , D.4.4. DISTILLATION (TEACHER INFERENCE) In the distillation (teacher inference) scenario, δLgt with the standard supervised learning case as well as the cost for producing the logits for the student = 1 , δPre = 0, which means that we account for compute associated FLOPs(NS, DS, NT , DT ) 3F (NS)DS + (NT )DS . (31) This scenario represents the setting where teacher already exists, but logits for the distillation still need producing. The optimal quantities (D ) giving rise to the cross entropies in Figure 17 are shown in Figures 21 and 22. S, , Student Training (cid:123)(cid:122) (cid:124) Teacher Logits (cid:123)(cid:122) (cid:125) (cid:125) (cid:124) Figure 21. Compute optimal configuration contours for distillation (teacher inference). The compute optimal quantities (D ) giving rise to the student cross entropies for teacher inference in Figure 17. S, , The teacher should be overtrained. instead indirectly subject to In the teacher inference scenario, at given , the solution is to maximize does not contribute directly to compute but as is seen in Figure 22; . To minimize 36 Distillation Scaling Laws Figure 22. Compute optimal configurations for distillation (teacher inference). For eight student sizes, the compute optimal quantities (D ) producing the student cross entropies for teacher inference in Figure 17. This is one-dimensional slice of Figure 21. , S, takes the largest value allowed in our numerical optimization, 1017 tokens. Although not surprising, this demonstrates the benefit of producing overtrained teachers, instead of taking the tempting strategy of using compute optimal teachers followed by long distillation process into smaller student model. As compute is increased, relatively less should be spent on student training, and more on teacher logit inference. The compute allocations resulting from the optimal combination are shown in Figure 23. We see that in all cases, the student training term (blue) decreases as compute increases, whereas the teacher logits (orange) increases. This happens because as compute increases: i) optimal student tokens increases at rate approximately independent of compute, ii) the teacher size increases with compute to provide stronger signal, while iii) the student size is fixed (see Figure 22). Figure 23. Compute optimal allocations for distillation (teacher inference). For eight student sizes, the compute optimal allocations corresponding to the terms in Equation 29 for the compute optimal values in Figure 22. 37 Distillation Scaling Laws D.4.5. DISTILLATION (TEACHER PRETRAINING) In the distillation (teacher pretraining) scenario, δLgt with training the teacher, in addition to the standard training cost of the student, but not the cost of producing the logits = 1, which means that we account for compute associated = 0 , δPre FLOPs(NS, DS, NT , DT ) 3F (NS)DS + 3F (NT )DT . (32) This scenario represents when we want to figure out which teacher to produce to distill into sufficiently many different students, storing the teacher logits for reuse, effectively ammortizing the cost of producing the logits. Here, contrary to the previous two scenarios (Appendices D.4.3 and D.4.5), the teacher size NT and teacher tokens DT contribute directly to the compute accounting (Equation 32). The optimal quantities (D ) giving rise to the cross entropies in Figure 17 are shown in Figures 24 and 25. S, , Student Training (cid:123)(cid:122) (cid:124) (cid:125) (cid:124) Teacher Training (cid:123)(cid:122) (cid:125) Figure 24. Compute optimal configuration contours for distillation (teacher pretraining). The compute optimal quantities (D S, ) giving rise to the student cross entropies for teacher pretraining in Figure 17. , Figure 25. Compute optimal configurations for distillation (teacher pretraining). For eight student sizes, the compute optimal quantities (D ) giving rise to the student cross entropies for teacher pretraining in Figure 17. This is one-dimensional size of Figure 24. , S, The compute optimal teacher for distillation is supervised compute optimal teacher. In Figure 25 we see that the MT DT /NT ratio of the teacher is constant for all values of compute, and can be compared to the ratio in Figure 19. This can be understood as there is no inference cost to pay for making the teacher large; we are only minimizing the training compute budgets of two models, and the most efficient way to produce teacher with given cross-entropy LT is teacher 38 Distillation Scaling Laws that is compute-optimal in supervised sense. Note that this conclusion is the opposite to the finding in Appendix D.4.4. There, the inference is expensive, and so the teacher should be overtrained. Here, teacher training is expensive, so teacher training should be compute optimal. As compute is increased, relatively less should be spent on teacher training, and more on student training. In Figure 26 we see the compute allocations for the configurations shown in Figure 25, and see that student training relative compute (blue) increases with increasing compute budget, while the teacher training (green) decreases with increasing compute budget. This happens because, as in all compute scenarios, with increasing compute, the optimal student tokens increases (Figure 25). Teacher size and tokens are also increasing with increasing compute, providing stronger signal for the student with more tokens to learn. However, this increase in teacher size and tokens plateaus, while the student tokens continues to increase. This is because here the teacher is compute optimal, and so the amount of compute needed to improve the learning signal for the student is much less than the amount of compute needed to train the student for to make use of that signal, due to the stronger diminishing returns with respect to DS at fixed NS (Equation 8). Figure 26. Compute optimal allocations for distillation (teacher pretraining). For eight student sizes, the compute optimal allocations corresponding to the terms in Equation 29 for the compute optimal values in Figure 25. D.4.6. DISTILLATION (TEACHER PRETRAINING + INFERENCE) In the distillation (teacher pretraining + inference) scenario, δLgt associated with distilling single student = δPre = 1, which means that we account for all costs FLOPs(NS, DS, NT , DT ) 3F (NS)DS + (NT )DS + 3F (NT )DT . (33) Student Training (cid:123)(cid:122) This scenario can be thought of as the compute optimal worst case scenario for distillation, i.e. one teacher is trained only for the purposes of one student. As in Appendix D.4.4, teacher size NT and teacher tokens DT contribute directly to the compute accounting (Equation 33). The optimal quantities (D ) giving rise to the cross entropies in Figure 17 are shown in Figures 27 and 28. Teacher Training (cid:123)(cid:122) Teacher Logits (cid:123)(cid:122) S, , (cid:124) (cid:124) (cid:125) (cid:125) (cid:125) (cid:124) Compute optimal teachers should be used for lower compute budgets and overtrained teachers should be used for larger compute budgets. In Figure 28 we see teacher configuration that interpolates between the teacher pretraining (Appendix D.4.5) and teacher inference (Appendix D.4.4) compute scenarios. At low compute, the optimal number of student tokens is not too large, this means there is little penalty to increasing the teacher size, resulting in an approximately supervised compute-optimal teacher given teacher compute budget. Once the optimal number of student tokens 39 Distillation Scaling Laws becomes higher than the optimal number of teacher tokens, there is significant penalty to increasing the teacher size. At this point, the teacher solution starts to become the overtrained solution seen in teacher inference, the optimal teacher tokens continue to increase polynomially, but this is not followed with an increase in the teacher size. For sufficiently high compute, corresponding to large number of student distillation tokens, the compute penalty for teacher size is so large that optimal teacher size decreases with compute. Figure 27. Compute optimal configuration contours for distillation (teacher pretraining + inference). The compute optimal quantities (D ) giving rise to the student cross entropies for teacher pretraining + inference in Figure 17. , S, Figure 28. Compute optimal configurations for distillation (teacher pretraining + inference). For eight student sizes, the compute optimal quantities (D ) giving rise to the student cross entropies for teacher pretraining + inference in Figure 17. This is one-dimensional size of Figure 27. , S, For small students, as compute grows, more should be spent on training the student and producing logits for the student. In Figure 29 we see the compute allocations for the configurations shown in Figure 28. Compute optimal smaller models tend to have smaller teachers, and optimal teacher tokens always grow at slower rate than student tokens, and so teacher the training cost is relatively small. As compute grows, the student is distilled on more tokens, and the teacher always becomes slightly larger than the student, which gives rise to most compute being allocated to standard student training compute component and producing the logits for this training. For large students, as compute grows, more should be spent on training the teacher, until transition happens where more should be spent on training the student and producing logits for the student. The explanation for the phenomenon is as above, except that the larger students need more capable teacher to learn from as compute grows, and so initially compute needs to bused to produce the teachers required. After certain amount of compute, the large number of 40 optimal student distillation tokens moves the optimal solution towards an overtrained teacher scenario, and more compute being allocated to student training and logit production. Distillation Scaling Laws Figure 29. Compute optimal allocations for distillation (teacher pretraining). For eight student sizes, the compute optimal allocations corresponding to the terms in Equation 29 for the compute optimal values in Figure 28. D.4.7. OPTIMAL TEACHER TRAINING AND STUDENT DISTILLATION TOKENS To aid in comparing the different compute strategies presented in Appendices D.4.3 to D.4.6, we now present each compute optimal value for all strategies, including supervised. Here, we show compute-optimal distillation student tokens in Figure 31 and compute-optimal teacher pretraining tokens in Figure 31. Figure 30. Compute optimal distillation student tokens. For eight student sizes, the compute optimal student tokens to the student cross-entropies for all compute scenarios, including supervised. giving rise In all scenarios, student tokens should be increased with compute similar to in the supervised case. We see in Figure 30 that, as in Chinchilla (Hoffmann et al., 2022), supervised tokens are increased polynomially with compute. Dis41 tillation (best case) follows the exact same allocation, as does distillation (pretraining) with asymptotically large compute. All other methods follow the same increase rate, but with scenario-dependent offsets. Distillation Scaling Laws Figure 31. Compute optimal distillation teacher tokens. For eight student sizes, the compute optimal teacher tokens to the student cross-entropies for all compute scenarios. giving rise Optimal teacher tokens interpolate between scenarios based on compute allocation. In Figure 31 we can see more clearly the interpolation behavior discussed in Appendix D.4.6. At low compute, teacher pretraining and teacher pretraining + inference share optimal solutions because the number of student tokens is small. At high compute, teacher pretraining + inference approaches teacher inference, while teacher pretraining approaches best case, as is large, and costs associated with teacher pretraining become less important. D.4.8. OPTIMAL TEACHER SIZE Figure 32. Compute optimal distillation teacher size. For eight student sizes, the compute optimal teacher size student cross-entropies for all compute scenarios. giving rise to the Optimal teacher size interpolate between scenarios based on compute allocation. As in the optimal teacher tokens in Figure 31, the same mechanism causes interpolation behavior in optimal teacher size (see Figure 32). 42 Distillation Scaling Laws D.5. Compute and data efficiency gains for distillation compared to supervised learning In this final section, we use the compute-optimal strategies developed through Appendices D.4.3 to D.4.6 and understand, for each distillation compute scenario  (Table 5)  if it is more compute and/or data efficient to use distillation compared to supervised learning in order to produce desired model (i.e. of given size NS with desired performance, measured in cross-entropy LS). In Figure 33 we show the amount of compute needed to distill student of given size to given cross-entropy as multiple of the compute that supervised learning needs to produce the same result. We do this for for each of the distillation compute scenarios, whose optimal configurations are given in Appendices D.4.3 to D.4.6. In Figure 34 we show the same, except we show the number of tokens needed to distill student of given size to given cross-entropy as multiple of the number of tokens that supervised learning needs to produce the same result. Our distillation token accounting depends on compute scenario: DDist. = DS + δPre DT , (34) i.e. we only count teacher tokens if the teacher pretraining cost is also included in the compute cost (see Equation 29). Figure 33. Compute optimal distillation compute ratios. For eight student sizes, the amount of supervised compute needed to produce student of the indicated size and cross-entropy. The horizontal dashed line indicates the break-even point, when doing supervised leaning is as computationally efficient as the corresponding distillation compute scenario. Values greater (less) than one indicate distillation is more (less) expensive than supervised learning for producing model of the indicated size and cross-entropy. The vertical dashed line indicates the lowest cross-entropy achievable by that student. When teacher training is discounted, distillation is often more efficient. In Figure 33, the base case (blue) and teacher inference (orange) compute scenarios are below the grey dashed line for cross-entropies slightly above the lowest possible cross-entropy (vertical grey dashed line), meaning less compute is needed for distillation than supervised learning. This compute efficiency translates into data efficiency (see Figure 34). To produce the strongest student possible, supervised learning is more efficient. In Figures 33 and 34, the base case (blue) and teacher inference (orange) compute scenarios attain values larger than one as the target cross-entropy LS approaches the limiting value L(N = NS, = ) for each student size NS, (vertical dashed line). This suggests i) the existence of more efficient training strategy where distillation is used as an initial training stage, with transition to 43 Distillation Scaling Laws supervised learning based on token or cross-entropy threshold, and ii) potentially increased importance of data mixtures (λ 1, see Appendix G.1) when distilling with significant token and/or compute budgets. We leave this for future work. In situations where teacher training is required, supervised learning is more efficient. As observed in Appendix D.4.2, for all student sizes, if teacher pretraining is included in the computational cost of producing student, supervised learning is always more efficient than distilling. This can be seen from Figure 33 as the teacher pretraining (green) and teacher pretraining + inference (red) compute scenarios are above the grey dashed line, which means more compute is needed for distillation than supervised learning in those compute scenarios. This compute efficiency translates into data efficiency (see Figure 34). Figure 34. Compute optimal distillation data ratios. For eight student sizes, the number of tokens compute needed to produce student of the indicated size and cross-entropy. The horizontal dashed line indicates the break-even point, when doing supervised leaning is as data efficient as the corresponding distillation compute scenario. Values greater (less) than one indicate distillation is more (less) expensive than supervised learning for producing model of the indicated size and cross-entropy. The vertical dashed line indicates the lowest cross-entropy achievable by that student. Distillation is more efficient for larger students. In Figure 33 we see in the pretrain + inference scenario, producing NS =500M student with cross-entropy of 2.4 has roughly 3/4 the compute cost of producing the same model with supervised learning, whereas producing NS =10B student with cross-entropy of 2.2 has roughly 1/2 the compute cost of producing the same model with supervised learning. In terms of data (Figure 34), the 500M and 10B configurations use roughly 2/3 and 1/2 the number of tokens of their supervised counterparts respectively. The efficiency gains from distillation are potentially greater for larger students when considering compute or data. 44 E. Additional Results Distillation Scaling Laws In this section, we provide an extensive list of studies, including downstream evaluations of distillation. We cover the models used as teachers, examine the Kullback-Leibler Divergence (KLD) between teacher and student in fixed token-tosize ratios, and present supplementary materials to Section 4.1. Additionally, we investigate the limiting behavior of our scaling law, weak-to-strong generalization, and conduct model calibration study to assess fidelity. These analyses offer comprehensive view of the factors influencing distillation performance and the behavior of our proposed scaling laws. E.1. Downstream evaluations In all settings, we optimize for and predict model cross-entropy on the validaiton set. To confirm that the validation crossentropy LS is good proxy for the downstream evaluation that we ultimately care about, we show how each downstream result is affected by the teacher and student loss. Figure 35 shows set of English downstream evaluation tasks. ARC Easy (Bhakthavatsalam et al., 2021), ARC Challenge (Bhakthavatsalam et al., 2021), HellaSwag (Zellers et al., 2019), Piqa (Bisk et al., 2020), Sciq (Welbl et al., 2017), WinoGrande (Sakaguchi et al., 2021) and Lambada OpenAI (Paperno et al., 2016) are zero-shot tasks. TriviaQA (Joshi et al., 2017) and WebQS (Berant et al., 2013) are one-shot tasks. TriviaQA evaluation is on the larger and more challenging Web split. CoreEn is the average of both the zero-shot and one-shot tasks. Finally, we have included GSM8K (Cobbe et al., 2021) and MMLU (Hendrycks et al., 2021b;a). GSM8K is used in an 8-shot chain of thought setting, following LLaMA (Touvron et al., 2023a;b; Dubey et al., 2024). MMLU is used in fiveshot setting. These perform near-random for most of the models, and only show slightly upwards trend when decreasing student/teacher loss. This is due to the use of the C4 dataset in training, and we note that we do not aim for competitive downstream evaluation results. All models are evaluated using an internal version of the open-source lm-evaluation-harness (Gao et al., 2024). Figure 35. All student downstream evaluations. For discussion of the individual metrics and datasets, see Appendix E.1. 45 E.2. Teachers used in distillation Distillation Scaling Laws In Figure 36 we show the cross-entropies of the models used as teachers in Section 4.2, and for fitting the supervised scaling law: i) eleven of fixed-M ratio models following the Chinchilla rule of thumb D/N = 20 (Hoffmann et al., 2022), ii) six models on = 512B tokens (Figure 36a), and iii) four IsoFLOP profiles (Figure 36b). Together this produces 74 runs corresponding to tuples of (N, D, L). (a) Fixed-M and 512B Teachers. (b) Supervised IsoFLOPs. (c) Supervised IsoFLOP minima. Figure 36. Supervised IsoFLOPs. (a) The cross-entropy of supervised models trained with either Chinchilla optimal = D/N 20 or on 512B tokens. (b) The cross-entropy supervised models trained with four ISOFLOP profiles {3 1019, 1020, 3 1020, 1021}. (c) The optimal supervised parameters (C) = arg minN L(C) for each IsoFLOP profile, and the loss L(C) achieved by that model. Coefficient estimation (Appendix F.1) yields the scaling coefficients shown in Table 6, and scaling law which has 1% relative prediction error, including when extrapolated from weaker to stronger models (see Figure 5a). E.3. Fixed-M teacher/fixed-M students and the capacity gap Figure 37. Fixed Teacher/Fixed Student. Students of three sizes trained with different MS = DS/NS = 20 ratios are distilled from teachers with MT = DT /NT 20. This is more complete version of Figure 3. In Figure 37, the capacity gap in knowledge distillation can be seen. Improving teachers performance does not always improve students, and even reduces the performance after certain point. The KLD between teacher and student is an increasing function of teacher size in all cases, which means as the teacher improves its own performance, the student finds 46 Distillation Scaling Laws the teacher more challenging to model, which eventually prevents the student from taking advantage of teacher gains. See Appendix E.8.2 for an investigation using calibration to understand where this mismatch occurs. E.4. Full distillation scaling law IsoFLOP profiles In Figure 38a we provide the full six fixed Teacher/IsoFLOP Student profiles, only two of which were shown in Figure 2. These experiments enable the reliable determination of α, β, γ, and B. In Figure 38b we provide the full four IsoFLOP teacher/ fixed student, only two of which were shown in Figure 3. These experiments enable the reliable determination of c0, c1, f1 and d1. Strong-to-weak generalization occurs. For the weaker teachers (NT 2.72B), The horizontal dashed line in each pane shows the cross-entropy achieved by the teacher (Appendix E.2). we see that for students larger than the teacher (NS > NT ) and for sufficiently large compute budgets, the student is able to outperform the teacher (see Appendix E.7 for detailed one-dimensional slice). stronger teacher signal is needed in order for stronger students to outperfom the supervised baseline. The horizontal dashed line in each pane shows the cross-entropy achieved by the student if trained using supervised learning (Appendix E.2). We see that weaker students benefit more from distillation, as e.g. the 198M student has all observed data below this dashed line, meaning all distillations outperform the supervised baseline. However, for the 1.82B student, only 1021 FLOP teachers produce distilled students that outperform the supervised baseline. (a) Fixed Teacher/Student IsoFLOP profiles. (b) IsoFLOP Teacher/Fixed Student profiles. Figure 38. Supervised IsoFLOPs. (a) Teachers of six sizes with MT = DT /NT 20 are distilled into Students with four IsoFLOP profiles, and small number with CS = 3 1021. The horizontal grey and vertical black dashed lines indicate teacher cross entropy LT and size NT respectively. (b) Students of four sizes trained with = DS/NS = 20 are distilled from teachers with four IsoFLOP profiles. Horizontal (vertical) dashed lines indicate student supervised cross entropy (cid:101)LS (student size NS). 47 E.5. Distillation scaling law IsoFLOP optima The optimal loss values of each IsoFLOP in Figure 38a are shown in Figure 39. Distillation Scaling Laws (a) Fixed -Ratio Teacher/Student ISOFlop optima. (b) Fixed -Ratio Student/Teacher ISOFlop optima. Figure 39. ISOFlop optima. a) The optimal student parameters L(NS) that give the lowest student validation loss for each teacher-student combination shown in Figure 38a. The dashed lines correspond to the validation loss of the optimal supervised models trained with the four corresponding compute budget. b) The optimal teacher parameters L(TS) that give the lowest student validation loss for each teacher-student combination shown in Figure 3. The black dashed line correspond to the validation loss of = D/N = 20 supervised model of the indicated student size. In both figures, the shaded region corresponds to where weak to strong generalization may occur, as NS > NT (see Appendix E.7). = arg minNT = arg minNS E.6. Distillation with infinite data From the supervised scaling law (Equation 1) model with parameters has cross-entropy lower bound L(N ) L(N, = ) = + (AN α)γ (35) which represents the best solution to the training objective subject to constraints from that models hypothesis space (Hoffmann et al., 2022) and is achieved when the number of training tokens is large (D ). As the hypothesis space of model is independent of the procedure used to find the solutions, we anticipate that the student with NS parameters has cross-entropy lower bound that is the same as the supervised one Equation 35. However, it not immediately clear if this is true in practice, since LS(NS) LS(NS, DS = , LT = ) = T + )γ )c0 (cid:32) (AN α (L 1 + d1 L(NS) (cid:18) 1/f1 c1f1 , (cid:19) (cid:33) (36) (37) = arg minL(NS, DS = where , LT ) is the teacher cross-entropy that minimizes Equation 8. Upon checking numerically, we do find that Equation 35 is consistent with Equation 37 for range of models N, NS [100M, 100B] (Figure 40). We stress that unlike our three motivations for the equation properties (Section 4.3), this infinite data limit was imposed added by hand, and is only true for certain values scaling coefficients. This lower bound consistency is evidence 48 Distillation Scaling Laws that that our distillation scaling law has desired behavior far outside of observed models, at least along the data and teacher axes. We also note that only the optimal teacher for each student size produces student cross-entropy lower bound that is consistent with the supervised one. Any other choice produces higher student cross-entropies, either because the teacher is too weak, or due to the capacity gap. Figure 40. Scaling behavior in the infinite data regime. For the optimal choice of teacher, the loss achieved by all student sizes under distillation is consistent with the loss achievable by supervised learning. This is not true for any choice of teacher, only the optimal one, which can be determined through numerical optimization of the provided distillation scaling laws (see Section 5). E.7. Weak-to-strong generalization In Figure 41 we see that weak-to-strong generalization (Burns et al., 2024; Ildiz et al., 2024) occurs only in the finite distillation data regime, and when the number of tokens is sufficiently large, the student cross-entropy increases again, eventually matching the teacher cross-entropy. This can be understood in the following way: i) when the student is larger than the teacher, the student contains in its hypothesis space the function represented by the teacher, ii) when the student is shown the teacher outputs on enough of the data manifold, it eventually matches what the teacher does on the whole data manifold. We note this doesnt explain how and why the student outperforms its teacher, and only constrains its asymptotic (low and high distillation data) behaviors. Figure 41. Fixed M-Ratio Teacher varying student data. We look at strong to weak generalization (left) and weak to strong (right) distillation, varying distillation tokens DS [8B, 512B]. 49 E.8. Model calibration Distillation Scaling Laws Calibration in LMs refers to the alignment between the models confidence in its predictions and the actual correctness of those predictions. Well-calibrated models provide confidence scores that accurately reflect their probability of correctness, enabling more decision-making. Expected Calibration Error (ECE) is common metric to quantify miscalibration, and measures the difference between predicted confidence and actual accuracy across multiple confidence intervals ECE = m=1 (cid:88) NSamples Accuracy( m) Confidence( , m) (38) where is the number of bins, notes the number of samples in bin Confidence( Lower ECE indicates better model calibration. dem m) and m) are the empirical accuracy and average confidence of the model being evaluated in bin respectively. is the set of samples whose confidence scores fall into the m-th bin, is the total number of samples, Accuracy( m, NSamples = m=1 B (cid:80) m To measure ECE, we use = 21 bins uniformly partitioned across the output probability space. Accuracy and confidence are computed in the standard manner: the predicted label is determined via the argmax over the output probabilities for each prediction, and the confidence is defined as the maximum probability assigned to the predicted label. Accuracy is then measured as the proportion of instances where the predicted label matches the ground truth. Notably, this approach focuses solely on the maximum probability prediction, disregarding the calibration of lower-probability predictions. To assess calibration across the entire output distribution rather than just the top prediction, alternative metrics could be considered. E.8.1. TEACHERS In Figure 42, we see that the ECE value across different sizes of teachers. For all models, the ECE ranges between 0.4% and 0.6%, suggesting that the models confidence estimates closely align with their actual accuracies. We also observe that for each plot, the blue points, i.e. , the teachers actual accuracy for predictions falling into specific confidence intervals, closely follow the diagonal, which shows that the models are well-calibrated. There is small deviation at low and high confidence values denoted by the orange points. Figure 42. Teacher calibration. The calibration of teachers of seven different sizes. The x-axis shows the teacher probability assigned to the most confident class, and the y-axis is the empirical accuracy of predictions within each confidence bin. Blue points represent the teacher accuracy for predictions falling into specific confidence intervals. Orange points represent the proportion of samples in each confidence bin (helpful for understanding sample distribution across confidence levels). The dashed line represents perfect calibration, where confidence matches empirical accuracy. The ECE (Equation 38) for each teacher is shown as the title of each plot. 50 E.8.2. 198M STUDENTS TRAINED ON 20N TOKENS Distillation Scaling Laws In this section we consider students trained on the teacher distribution, as in our main study. We also study students trained on the teacher top-1 distribution, as described in Appendix G.4, as the qualitative difference in behavior can be informative for student design. Evaluating the calibration of student can be done in number of ways: 1. We can compare student outputs relative ground-truth data, as in Appendix E.8.1 for the teachers. 2. We can compare student outputs with the outputs of its teacher. Calibration against ground-truth. First, lets consider comparison against ground truth data. In Figure 43 we show student calibration with respect to the dataset labels for both teacher distribution distillation and teacher top-1 distillation. 1. Distilled on the full teacher distribution. In Figure 43a, we observe that the student is well-calibrated against ground truth data. Similar to the teachers calibration plot in Figure 42, we see small discrepancy at very low and very high confidence values, and the ECE value is low. 2. Distilled on teacher top-1. In Figure 43b, we see that student trained only on its teachers top-1 prediction, is not calibrated against ground truth data. The blue points below the dashed line indicate an overconfident student, i.e. , its predicted confidence is higher than the actual accuracy in that confidence range. This is because training the student on top-1 assigns the student to the most plausible outcome rather than all the plausible outcomes with correct frequencies. Confidence proportions are low for all bins that are not the most confident bin, and ECE is high, although decreases with increasing teacher size NT . Figure 43 shows that training the student on the teachers distribution results in calibrated student, whereas training on the teacher top-1 does not. Indeed, optimizing against the teachers top-1 is not proper scoring metric, and that teacher top-1 is not an unbiased estimator for the data, while the teacher distribution is. (a) Distillation target: teacher distribution. (b) Distillation target: teacher top-1. Figure 43. Student calibration (data). Calibration of the student with respect to the actual data labels, trained with different teacher sizes (NT ), on (a) the teacher distribution and (b) the teachers top-1. For axis definitions and the figure legend, refer to Figure 42. Blue points below the dashed line indicate student overconfidence. Calibration against teacher top-1. Next we investigate the first student calibration against the teacher. In Figure 44 we show student calibration with respect to the teachers top-1 label. That is, the next-token label used for accuracy computation, and extract the students confidence is the most probable next-token according to the teacher, instead of the label from data. Here no next token labels are used at all. These teacher top-1 labels are also used for the ECE calculation, which is still computed using Equation 38. 51 Distillation Scaling Laws 1. Distilled on the full teacher distribution. We see in Figure 44a that when distilled from the full teacher distribution, the student is not calibrated against the teacher top-1. The blue points are above the dashed line, which means that the empirical accuracy is higher than the models predicted confidence, i.e. with respect to the teacher top-1, the student is underconfident. This can be understood by noting that the top-1 objective is an easier objective than modeling the full vocabulary at each step. 2. Distilled on teacher top-1. In Figure 44b we observe that student is distilled from its teachers top-1 is calibrated with respect to teachers top-1. (a) Distillation target: teacher distribution. (b) Distillation target: teacher top-1. Figure 44. Student calibration (teacher top-1). Calibration of the student with respect to the teachers top 1, trained with different teacher sizes (NT ), on (a) the teacher distribution and (b) the teachers top-1. For axis definitions and the figure legend, refer to Figure 42. Blue points above the dashed line indicate the student is underconfident. Figure 44 shows that training the student on teacher top-1 results in calibration against teacher top-1, whereas model trained on data, or distilled on the full teacher distribution is not calibrated against teacher top-1. As above, this can be understood as now teachers top-1 is now proper scoring metric, and teacher top-1 is an unbiased estimator for itself. Calibration against teacher distribution. Here we develop modified calibration measure that will help us understand if the student matches the teacher in distributional sense. As we have two distributions to compare, we can ask, for given teacher confidence, what is the expected student confidence. This leads to ECEDist, distributional form of ECE: ECEDist(A, B) = m=1 (cid:88) NSamples Confidence( m; A) Confidence( m; B) , (39) , and NSamples are defined as before, and and is similar in spirit to divergence measures like KLD. ConfidenceS( B) is the average confidence of model or in bin respectively. The bins are always witin the bins of confidence of model B. In the current evaluation, we take as the teacher and as the student, and we are measuring the average confidence of the teacher is measured within students confidence bin. m; m, B 1. Distilled on the full teacher distribution. In Figure 45a, we see that when the student is confident, it matches the teacher confidence. However, as the teacher model grows in size, when the student is less confident, it it systematically underestimates its confidence. This suggests that the student has not effectively learned low-probability outcomes, or that these outcomes are particularly challenging for the student to replicate. The underconfidence in these regions may be result of the distillation process not providing sufficient learning signal for these difficult cases, or the inherent difficulty of capturing the uncertainty associated with low-confidence predictions. This observation of confidence mismatch helps indicate which parts of the distribution the student finds challenging to model, giving rise to the increasing KLD and capacity gap observed in Figure 4 and Appendix E.3. 2. Distilled on teacher top-1. In Figure 45b, for small teachers, we observe student overconfidence. As the teacher increases in size, the students overconfidence in low-confidence bins transitions to underconfidence. At the same time, 52 Distillation Scaling Laws the students overconfidence in high-confidence bins improves, leading to an overall reduction in distributional ECE. This pattern of overconfidence in the student is similar to what we saw in Figure 43b, but the change in behavior at low-confidence bins as the teachers size varies is different. This shift in the students calibration behavior, especially in low-confidence bins, aligns with findings from Figure 45a and may highlight the difficulty the small student faces in learning rare events. (a) Train target: teacher distribution. (b) Train target: teacher top 1. Figure 45. Student calibration (teacher distribution). Calibration of the student with respect to the teachers distribution, trained with different teacher sizes (NT ), on (a) the teacher distribution and (b) the teachers top-1. For ECE calculation on the full distribution, see Equation 39. For axis definitions and the figure legend, refer to Figure 42. Blue points below the dashed line indicate student overconfidence, while points above the dashed line indicate underconfidence. We can also inspect the student confidences within bin of teacher confidences, and compute the distributional ECE (Equation 39), swapping the roles of teacher and student (see Figure 46). 1. Distilled on the full teacher distribution. In Figure 45a we complete the picture from Figure 45a and see that the part of the distribution the student struggles to model is actually the place where teacher is most confident. 2. Distilled on teacher top-1. In Figure 45b we see that the student is systematically overconfident for all values of teaacher confidence, except for the largest teachers, where the student is underconfident when those teachers are most confident. (a) Train target: teacher distribution. (b) Train target: teacher top 1. Figure 46. Student calibration (under teacher confidence bins). Calibration of the student with respect to the teachers confidence bins, trained with different teacher sizes (NT ), on (a) the teacher distribution and (b) the teachers top-1. For ECE calculation on the full distribution, see Equation 39. For axis definitions and the figure legend, refer to Figure 42. Blue points below the dashed line indicate the teacher is less confident than the student. 53 E.8.3. 198M STUDENTS TRAINED ON 128B TOKENS Distillation Scaling Laws In this section, we study the effect of increasing the number distillation tokens in Appendix E.8.2 from DS 512B. Here, we reserve discussion for the observed differences compared to Appendix E.8.2. DS 20NS to (a) Train target: teacher distribution. (b) Train target: teacher Top 1. Figure 47. Student calibration (data). Calibration of the student with respect to the actual data labels with increased training tokens. Compare to Figure 43 for the effect of tokens and refer to Figure 42 for legend and axis explanations. Calibration against ground-truth. As the number of distillation tokens increases, we observe consistent decrease in the ECE when the student is trained on the teachers distribution, as shown by the comparison between Figure 47a and Figure 43a across different teacher sizes. However, when the student is trained on the teachers top-1 predictions, increasing the number of tokens negatively impacts ECE, as evidenced by the comparison between Figure 47b and Figure 43b. This suggests that the teachers top-1 predictions are not reliable, unbiased estimator of the actual data, and increasing the number of training tokens only exacerbates this issue. See Appendix G.4 for further discussion. Calibration against teacher top-1. Increasing the number of distillation tokens leads to worse calibration between the student and the teachers top-1 predictions when the student is trained on the full distribution. This change primarily occurs in the low-confidence bins, and results in higher ECE (compare Figure 48a and Figure 44a). However, when comparing the ECEs for the student trained on the teachers top-1 predictions (Figures 44b and 48b), there is an improvement across all teacher sizes. When the student is trained and evaluated using the same metric, increasing the training tokens helps improve calibration, demonstrating consistency between the learning objective and the evaluation metric. (a) Train target: teacher distribution. (b) Train target: teacher top 1. Figure 48. Student calibration (teacher top 1). Calibration of the student with respect to the teachers top 1 when the training tokens have increased. Compare to Figure 44 for the effect of tokens and refer to Figure 42 for legend and axis explanations. 54 Distillation Scaling Laws Calibration against teacher distribution. comparison between Figure 49a and Figure 45a shows that when the student is trained on the teachers full distribution and evaluated against the full distribution using Equation 39, increasing the number of training tokens consistently improves calibration across all teacher sizes. However, when the student is trained on the teachers top-1 predictions, quick comparison between Figure 49b and Figure 45b reveals worse calibration uniformly across all confidence bins. (a) Train target: teacher distribution. (b) Train target: teacher Top-1. Figure 49. Student calibration (teacher distribution). Calibration of the student with respect to the teachers distribution as the number of training tokens increases. Compare to Figure 45 for the effect of tokens and refer to Figure 42 for legend and axis explanations. Similarly, when comparing within teacher confidence bins (Figure 50) increasing the number of distillation tokens from 20N to 128B primarily amplifies the observed phenomena at lower distillation token budgets, and improving calibration in cases where there is proper scoring metric present (Figure 50a). (a) Train target: teacher distribution. (b) Train target: teacher top 1. Figure 50. Student calibration (teacher distribution). Calibration of the student with respect to the teacher confidence bins distribution as the number of training tokens increases. Compare to Figure 46 for the effect of tokens. In general, increasing the number of training tokens has positive effect when the training metric is an unbiased estimator of the actual data or the measured calibration quantities (see Figures 47a, 48b and 49a) and reduces the ECE, while it has negative impact when there is mismatch between the learned and measured quantities (see Figures 47b, 48a and 49b). 55 F. Scaling coefficients Distillation Scaling Laws In this section, we analyze the process of deriving the coefficients for our scaling law. We follow the procedure outlined in (Hoffmann et al., 2022; Besiroglu et al., 2024), while incorporating our modified scaling laws F.1. Supervised scaling law coefficient estimation First, lets tackle the supervised scaling law Equation 1 restated for convenience L(N, D) = + α + Dβ (cid:18) γ . (cid:19) To aid numerical stability, we write this expression in log space. First note that for a, > 0 log(a + b) = log (exp log + exp log b) = LSE(log a, log b), where LSE is the log-sum-exp operator. We can now proceed to write the supervised scaling law in log form log L(N, D; A, B, E, α, β) = log γ (cid:20) (cid:18) + α + Dβ (cid:19) (cid:21) α + = LSE [log E, γ LSE (log log E, γ log = LSE (cid:18) (cid:20) Dβ (cid:19)(cid:21) αN, log αD)] . We make no assumptions about the relationships between the values (i.e. no parameter tying) and optimize (A, B, E, α, β, γ) = arg min Huberδ log L(N (i), D(i); A, B, E, α, β) {A,B,E,α,β,γ} (cid:88) (cid:16) L(i) (cid:17) (40) (41) (42) (43) (44) (45) with Huber δ = 104, where (i), D(i) and L(i) are the model size, number of training tokens and loss achieved by the i-th run. We fit on 73 samples over grid of L-BFGS-B initializations given by: log , , β log 0.5., 0., 0.5, 1., 1.5. 0., 5., 10., 15., 20. } , γ 0., 0.5, 1., 1.5 } { { , α } 0., 0.5, 1., 1.5 } { , log } 1., 2.2 case corresponds to 48 samples. { 0., 5., 10., 15., 20. { . The 0., 0.5, 1., 1.5 } { F.2. Distillation scaling law coefficient estimation Next, lets address the distillation scaling law Equation 8 restated for convenience LS(NS, DS, LT ) = LT + 1 Lc0 (cid:32) 1 + 1/f1 LT LSd1 (cid:19) (cid:18) (cid:33) c1f1 α (cid:32) + γ Dβ (cid:33) . (46) As in Appendix F.1, to aid numerical stability during optimization, we write this in log space (cid:101) log LS(NS, DS, LT ; θ) = log LT + 1 Lc0 (cid:32) 1 + LT LSd1 (cid:19) (cid:18) 1/f1 c1f1 (cid:33) α (cid:32) + γ Dβ (cid:33) = LSE log LT , (cid:34) (cid:101) c0 log LT c1f1 log 1 + (cid:32) (cid:18) + γ log 1/f (cid:33) α (cid:32) + Dβ (cid:33)(cid:35) = LSE (cid:34) log LT , (cid:32) c0 log(LT ) c1f1 LSE log LT log LS log d1 (cid:18) (cid:16) α log NS, log (cid:101) β log DS) , (cid:33)(cid:35) (cid:17)(cid:19) LT LS (cid:19) d1 1 (cid:101) f1 0, + γ LSE (log 56 (47) (48) (49) where θ = mize A, B, α, β, c0, c1, f1, d1 { . We make no assumptions about the relationships between the values and opti- } Distillation Scaling Laws θ = arg min θ (cid:88) Huberδ (cid:16) log LS(N (i) , D(i) , L(i) ; θ) L(i) (cid:17) (50) with Huber δ = 104, where (i) are the student model size, number of training distillation tokens, the teacher pretraining loss and the student validation loss on the data achieved by the i-th run. We fit on 697 samples over grid of L-BFGS-B initializations given by: log 0., 5., 10., 15., 20. , } α 0., 0.5, 1., 1.5 , } { log d1 , log { , f1 0., 0.5, 1., 1.5 } { , c1 0., 0.5, 1., 1.5 } 2.3 case corresponds to 551 samples. 0., 5., 10., 15., 20. } and L(i) , D(i) , L(i) 0., 0.5, 1. , c0 } { { { 0., 0.5, 1. , γ , β { } . The LS 0.5, 0., 0.5, 1. } { 0., 0.5, 1. } 1., { F.3. Scaling law coefficients parameteric fit The fitting procedure outlined in Appendices F.1 and F.2 applied to data described in Section 4.2 yields the scaling coefficients and associated confidence intervals shown in Table 6. Note in the supervised case, our values of and are consistent with those of Hoffmann et al. (2022). Table 6. Scaling law parameter estimates accompanied by 90% confidence intervals obtained by bootstrapping (4096 resamples) following the procedure of Besiroglu et al. (2024). = β/(α + β) and = β/(α + β) are the supervised compute optimal scaling estimates for and respectively (Hoffmann et al., 2022). Supervised Distillation 3355 (3346, 3360) 18186 (18157, 18236) 1.220 (1.190, 1.247) 0.408 (0.405, 0.411) 0.431 (0.428, 0.433) 0.452 (0.442, 0.461) 0.513 (0.513, 0.513) 0.486 (0.486, 0.486) 2243 (2227, 2255) 24181 (24084, 24266) 0.321 (0.319, 0.324) 0.637 (0.634, 0.640) 0.764 (0.732, 0.788) 2.549 (2.425, 2.615) 522.6 (522.6, 522.6) 0.090 (0.088, 0.093) 1.315 (1.302, 1.327) 0.664 (0.662, 0.665) 0.335 (0.334, 0.337) A() B() α() β() γ() c0 c1 f1 d1 a() b() Runs 73 697 We also note that our irreducible error term is lower than the one in Hoffmann et al. (2022). We suspect this is due to our use of µP (Yang & Hu, 2021; Yang & Littwin, 2023; Yang et al., 2022; Wortsman et al., 2023; Yang et al., 2023). G. Distilling language models in practice In the following analyses, we explore the sensitivity of student performance under modification of distillation hyperparameters. We demonstrate that the pure distillation setting (λ = 1, Appendix G.1), unit temperature (τ = 1, Appendix G.2), and learning rate η = 0.01 (Appendix G.3) under µP (Yang & Hu, 2021; Yang & Littwin, 2023; Yang et al., 2022; Wortsman et al., 2023; Yang et al., 2023) provides robust performance across model scales, while distribution truncation methods (Top-k, Top-p) degrade performance unless combined with ground-truth next-token prediction (Appendix G.4). Finally, we verify that forward KL divergence distillation, DKL(ˆpT ˆqS), consistently outperforms reverse KL (Appendix G.5). 57 For ease of reference, we restate the components of the token-level loss for the student: Distillation Scaling Laws LNTP(x(i), z(i)) = a=1 (cid:88) e(x(i))a log σa(z(i)), (Next-token prediction) (51) Z(z(i)) = 2 log Z(z(i)) 2 = a=1 (cid:88) log (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) z(i) (cid:12) (cid:12) (cid:12) (cid:12) τ (cid:33) LNTP(x(i), z(i) σa (cid:32) a=1 (cid:88) λ) log σa exp(z(i) ) (cid:32) , z(i) τ (cid:33) LKD(z(i) , 2 (cid:12) 2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ) + λ , z(i) ) + λZ (Z-loss) (52) (Distillation loss) (53) (Student loss) (54) Z(z(i) ). LKD(z(i) S(x(i), z(i) , z(i) ) = τ 2 , z(i) ) = (1 See Section 2 for discussion of each of the terms. G.1. Mixing coefficient (λ) sensitivity analysis ), and direct The distillation process combines two loss components: knowledge transfer from the teacher, λ learning from data, (1 ), weighted by the mixing coefficient λ (Equation 7). Our distillation scaling law analysis is performed in the pure distillation setting (λ = 1). Here we show this simple choice provides robust performance across wide range of configurations. LNTP(x(i), z(i) λ) LKD(z(i) , z(i) (a) Mixing Coefficient λ Sensitivity. (b) Optimal Mixing Coefficients λ Figure 51. Mixing Coefficients λ. (a) Students of six sizes NS {198M, 266M, . . . , 2.72B} trained with = DS/NS = 20 ratio are distilled from teachers of size sizes NT {546M, 975M, . . . , 7.75B} trained with = DT /NT = 20 ratio with different values of loss mixing coefficient λ [0, 1]. λ = 0 and λ = 1 correspond to supervised training and pure distillation cases respectively. (b) The mixing coefficients λ = arg minλ L(λ) that give the lowest student validation loss for each teacher-student combination shown in Figure 51a. Distillation Scaling Laws We examine various λ values across different teacher-student configurations in Figure 51a and find that while the optimal mixing coefficients λ vary based on the specific teacher-student combinations (Figure 51b), the student cross-entropy LS remains mostly flat for choices of λ > 0.5, with lower values of λ only preferred in the cases where the teacher is particularly weak and where the supervised signal is more informative. From Figure 51a it is also possible to get sense of when distillation λ > 0 generally outperforms supervised learning λ = 0 under the same token budget. To guide practitioners, Figure 51b shows empirically derived optimal mixing coefficients, λ, though the simplicity and robustness of pure distillation makes it reliable default choice for practical use and study. G.2. Temperature (τ ) sensitivity analysis In distillation, the temperature τ controls the entropy of teacher predictions by scaling logits z(i) /τ in the LKD (Equations 7 and 53). This scaling modulates the transfer of dark knowledge (Hinton knowledge distillation loss et al., 2015) the log-probability ratios between incorrect categories encode the teachers understanding of relationships [0.5, 10] (Figure 52) reveals that higher temperatures (τ > 3) reduces between those categories. Our analysis across τ performance by attenuating these ratios in σa(z(i) /τ ), particularly harming smaller students that rely heavily on this signal. Lower temperatures (τ < 1) similarly reduce effectiveness by concentrating probability mass on argmax tokens, diminishing the transfer of relationships between lower-ranked predictions. /τ and z(i) We find optimal performance at τ = 1 across all model scales, suggesting this temperature best preserves log-probability structure. Unlike the original distillation setting, which relied on dark knowledge to represents hierarchical relationships between incorrect classification predictions in the presence of true label, language modeling is inherently ambiguous and complex, with many valid continuations. It is precisely the understanding of the ambiguity of language we want to transfer to the student, which is supported by our finding that maintaining the teachers original probability ratios (τ = 1) produces the lowest student cross-entropies. Figure 52. Temperature τ Sensitivity Analysis. Students of four sizes NS {198M, 546M, 975M, 1.82B} trained with = DS/NS = 20 ratio are distilled from teachers of sizes NT {546M, 1.82B, 4.82B, 7.75B} trained with = DT /NT = 20 ratio with different distillation temperatures τ [0.5, 10]. 59 Distillation Scaling Laws G.3. Learning rate (η) sensitivity analysis, verification of µP for distillation The peak learning rate η determines the scale of student parameter updates in distillation. In our experiments we use simplified version of µP (Yang & Hu, 2021; Yang & Littwin, 2023; Yang et al., 2022; Wortsman et al., 2023; Yang et al., 2023), described as µP (simple) in (Wortsman et al., 2024). In the supervised case, in addition to improving the performance lower bound compared to the standard parameterization, µP simplifies experimental settings as it enables hyperparameter transfer; the optimal peak learning rate η and initialization scales found for reference model size can be reused when changing model size7. Here we validate that the optimal peak learning rate η = 0.01 determined in the supervised case transfers to the distillation setting. Sweeping values η [0.001, 0.1] (Figure 53) reveals that µP achieves optimal performance at η = 0.01 uniformly across all configurations, from 198M to 1.82B parameter students and 546M to 7.75B parameter teachers, consistent with the optimal peak learning rate in the supervised setting. Performance varies smoothly and modestly around this optimum, with cross-entropy changing by less than 0.1 nats over one order of magnitude in learning rate. This consistency validates µPs guarantee of scale-invariant training dynamics for distillation, confirming that our experimental setting for determining our distillation scaling law operates at the optimal learning rate or sufficiently close to it in all of our settings. The observed moderate learning sensitivity in distillation partially alleviates the requirement for careful learning rate tuning, showing that in practice the reference learning rate found in the supervised setting can be safely reused in the distillation setting. Figure 53. Learning Rate η Sensitivity Analysis. Students of four sizes NS {198M, 546M, 975M, 1.82B} trained with = DS/NS = 20 ratio are distilled from teachers of sizes NT {546M, 1.82B, 4.82B, 7.75B} trained with = DT /NT = 20 ratio with different learning rates η [0.001, 0.1]. G.4. Distribution truncation methods: Top-k and Top-p sensitivity We investigate how the truncation of the teacher distributions affects student performance. For these methods, when the teacher produces distribution ˆpT (x(i) = over the vocabulary for the student to match, only some entries in the distribution are used. This is done primarily to reduce repeated inference and storage costs in the case teacher outputs are being stored for re-use in the multiple distillations scenario discussed in Section 5.3. In our case, the x(<i)), 1, . . . , { } 7µP only guarantees learning rate optimality when varying widths. Empirically, the learning rate is also stable when changing the model depth within reasonable range (Yang et al., 2022). To guarantee transfer across model depths one can additionally employ depth-µP (Yang et al., 2024), although we do not use depth-µP here. 60 Distillation Scaling Laws vocabulary size = 32168, so assuming storage in float32, means each token requires 32168 129KB, and storing all of C4 (approximately 2T tokens) would take approximately 260 Petabytes, significant amount of data, roughly the total amount collected during the first ten years of the Large Hadron Collider (LHC) (CERN, 2018). 4 bytes Given truncation method distillation? Concretely, the truncation p(M)(x , can truncated teacher output ˆp(M) c) of distribution p(x can be stored whilst still achieving the gains of c) with truncation method p(x=bc) , M(p( c)), is p(x=ac) (55) p(M)(x=a c)= (cid:80) bSM 0, otherwise, M(p( where then undergoes renormalization over the retained categories. c)) represents the set of retained categories (i.e. non-zero probabilities) in the truncated distribution, which We explore two complementary approaches: Top-k and Top-p (nucleus) sampling. As in all of our settings, we evaluate the student cross-entropy against the data distribution with all categories, as this is the model property we are most interested in (a model can trivially match the target distribution if all categories except one are removed). Figure 54. Distribution truncation analysis. Top-k (left) and Top-p (right) truncation of teacher logits z(i) for student-teacher pairs with NS in {198M, 546M, 1.82B} and corresponding NT in {7.75B, 1.82B, 546M}. Standard truncation degrades performance: at = 128, validation loss increases by 0.11 nats compared to full distillation (k = 32768), while Top-p with = 0.9 degrades by 0.13 nats versus = 1.0. Using λ = 0.7 with = 128 maintains performance within 0.01 nats while enabling efficient post-hoc training. For Top-k, we zero-out all but the largest probabilities, and Top-p, we zero-out all but the smallest set of probabilities that sum to at least p. The set defintions for Top-k and Top-p are k(ˆp) = Top(ˆp, k), p(ˆp) = : { ˆp . } (cid:88)bsort( ˆp,a) (56) 1), both methods approach the full teacher distribution, and the As the truncation parameters increase (k students cross-entropy converges to the baseline using the entire ˆpT . Conversely, aggressive truncation (small or p) induces quantization that preserves only high-probability tokens while discarding information in the tail of the distribution. or Our empirical analysis (Figure 54) reveals that both truncation methods directly correlate with reduced evaluation likelihoods. However, this performance degradation can be effectively mitigated through combination of truncated distributions and ground truth next-token prediction using mixing coefficient λ (0, 1) (Equation 7). Specifically, with = 128 and λ = 0.7, we achieve validation losses statistically indistinguishable from those obtained using the complete teacher distribution. For large-scale distillation scenarios where maintaining multiple models in memory is prohibitive, particularly with large teacher models, storing only the Top-k teacher predictions (with λ > 0) enables efficient post-hoc distillation. G.5. Forward and reverse KL divergence We investigate both forward (mode spreading) and reverse (mode seeking) Kullback-Leibler divergences for distillation from NT = 1.82B to NS = 546M. The forward KLD DKL(ˆpT H(ˆpT ), where H(ˆpT ) is dropped during optimization as it depends on only fixed teacher parameters. In contrast, the reverse KLD DKL(ˆqS Lforward = H(ˆpT , ˆqS) H(ˆqS). ˆpT ) requires explicitly computing the students entropy, ˆqS) (Equation 7), minimizes Lreverse = H(ˆqS, ˆpT ) 61 Distillation Scaling Laws The forward KL achieves lower data cross-entropy compared to the reverse KL  (Table 7)  , with an average improvement of 0.28 nats. This suggests that explicitly regularizing with respect to the students entropy during training may not provide additional benefits for distillation quality. Given both the improved performance and reduced computational overhead of forward KL (which avoids computing student entropy), we recommend using standard forward KL for distillation. Table 7. Forward vs Reverse KL Divergence for NT = 1.82B to NS = 546M distillation. Reverse KL is slightly more expensive with respect to vocabulary size due to the entropy calculation. Method Cross-Entropy Computational Cost Forward KL Reverse KL 2.42 2.70 O(V ) O(2V ) H. Parameters and Floating Operation Estimation Here we outline the number of parameters (Appendix H.2) and the number of FLOPs per token (Appendix H.3) for our experimental settings. The symbol notation is provided in Table 8. For our scaling laws, we find, as in Kaplan et al. (2020) using that the number of non-embedding-parameters provides the cleanest fit and extrapolation behavior. Our expressions for approximate compute (FLOPs per token) differ from prior work in that we are interested in small models that are capable. This means we are unable to ignore the context-dependent term that arises from the quadratic computational complexity of the attention mechanism. As our architectures are fixed aspect ratio, there is modified approximation we can use. This expression is discussed in Appendix H.1 For ease of reference, we provide comparison of the expressions we use to commonly used existing expressions (Kaplan et al., 2020; Hoffmann et al., 2022; Narayanan et al., 2021), and provide comments for significant differences. Table 8. The notation we use for parameter and FLOPs estimation. Component Sequence length/context size Vocabulary size Number of blocks/layers Number of query heads Number of key/value heads Model/embedding dimension Head dimension Feed-forward dimension Number of feed-forward linears Group size in Group Query Attention (GQA) nheads/nkv-heads Model aspect ratio dmodel/nlayers Feed-forward ratio dffn/dmodel Notation nctx nvocab nlayers nheads nkv-heads dmodel dhead dffn nffn gsize ρmodel ρffn H.1. Alternative approximation for FLOPs per token as function of From Table 10 and Equation 71 and Table 12 we can read our approximate values for non-embedding parameters and total compute (dropping contributions from normalization layers) as8 = nlayersd model 2 + (cid:18) CForward = 2nlayersd2 model 2 + 2 gsize 2 gsize + nffnρffn (cid:19) + nffnρffn + 2nlayersnctxdmodel = 2N + 2nlayersnctxdmodel + 2nvocabdmodel. (cid:18) (cid:19) (57) (58) (59) 8It was shown in Porian et al. (2024) that ignoring the embedding parameters and FLOPs can lead to systematic estimation bias for small models, and is one of the primary drivers between different exponents reported in Kaplan et al. (2020) and Hoffmann et al. (2022). We find that the the non-embedding parameters gives tighter scaling behavior. However, in the fixed-aspect-ratio setting, we are able to use both the non-embedding parameters in the scaling law and the approximate total compute simultaneously, removing estimation bias. Indeed, in the supervised setting, our coefficients and are consistent with those from Hoffmann et al. (2022) (see Table 6). Distillation Scaling Laws Typically the term 2nlayersnctxdmodel would be dropped, and the embedding parameters included into the total parameters (Hoffmann et al., 2022) or discarded (Kaplan et al., 2020) yielding the expression CForward and the familiar expression = 6N (Kaplan et al., 2020; Hoffmann et al., 2022). For our investigation we are interested in small, capable models, which may have large context, and so both of these terms cannot be ignored in general at the peril of making systematic error in the region of configuration space we are most interested in. Fortunately, we will see that our choice of fixed aspect ratio ρmodel = dmodel/nlayers architectures allows us simple to use, more precise estimate. The trick will be to use this fixed aspect ratio to come up with an approximation for nlayers and dmodel as function of and ρmodel. With these approximated, the term 2nlayersnctxdmodel can be represented as function of . First define9 (61) (62) (63) (64) (65) (66) (67) (68) (69) so that 2 + ω 2 gsize + nffnρffn = nlayersd modelω. Then we can substitute in ρmodel dmodel/nlayers so that = nlayersd2 modelω = n3 layersρ modelω, and solve for nlayers and dmodel nlayers = 1/3 , ρ2 modelω (cid:18) (cid:19) dmodel = ρmodel ω (cid:18) (cid:19) 1/3 , The CForward term can then be represented as function of . The context-dependent term becomes 2nctxnlayersdmodel = 2nctxn2 layersρmodel = 2 2/ ρ2 modelω (cid:18) (cid:19) ρmodelnctx 2nctxσ1N 2/3 where The vocabulary projection term becomes σ1 = 1 ρ2 modelω (cid:18) 2/3 ρmodel = 1 ρmodelω2 (cid:18) (cid:19) 1/3 . 2nvocabdmodel = 2nvocab ρmodel ω (cid:18) = 2nvocab ρmodel ω (cid:16) (cid:17) 1/3 1/3 2nvocabσ2N 1/3, (cid:19) 1/ (cid:19) σ2 = 1/3 . ρmodel ω (cid:16) (cid:17) where In total CForward = 2N + 2nctxσ1N 2/3 + 2nvocabσ2N 1/3 = 2N 1 + σ1 nctx 1/3 + σ nvocab 2/3 , (cid:17) (cid:16) where σ1 and σ2 are independent of model and context size. In the large limit, or the small nctx small nvocab limit this becomes the familiar CForward = 2N . The backward FLOPS per token is taken as twice the forward FLOPs (Blondel & Roulet, 2024) CBackward = 2 CForward. (70) Given the simplicity of the compute expression as function of , the better tightness of fit in the scaling law, the improved intuition that the model size more directly corresponds to work being done by the model, and the predictability of hyperparameters at larger scales, we recommend the scaling law community consider adopting fixed aspect ratio models. 9In our setting (Appendix I) ω takes values ω = 2 + 2 gsize + nffnρffn = 2 + 2 1 + 3 8 3 = 12. (60) H.2. Model parameters Distillation Scaling Laws In Table 9 we present our parameter counting compared to commonly used existing expressions (Kaplan et al., 2020; Hoffmann et al., 2022; Narayanan et al., 2021). We present convenient substitution in Table 10 which can be easier to work with analytically. Our total expressions match the architecture we are using, which includes only gains for the normalization layers, whereas while (Narayanan et al., 2021) has both weights and biases. We account for potential use of (Ainslie et al., 2023) as well as use of gated linear attention mechanisms which are becoming prevalent in modern architectures (Shazeer, 2020) including the one used in this work (Appendix I). Table 9. Parameter counts for embedding projector, single transformer layer, final normalization and output layer. Ours indicates the expressions we use in the paper for the total number of parameters (note that the quantity that appears in our scaling laws is the number of non-embedding parameters, but still includes parameters associated with normalization layers). Approx. indicates taking the within-section total and dropping all terms that are not at least quadratic in one of dmodel, nvocab, and will be used for estimating the FLOPs per token from given model size (Appendix H.1), and does not differ significantly from the number of non-embedding parameters. Parameters (Narayanan et al., 2021) (Hoffmann et al., 2022) (Kaplan et al., 2020) Ours (Total) Embedding (nvocab + nctx)dmodel (nvocab + nctx)dmodel (nvocab + nctx)dmodel nvocabdmodel Attention (one transformer layer) PreNorm QKNorm QKV Project Total Approx. 3nheadsdmodeldhead nheadsdheaddmodel 4nheadsdheaddmodel 4nheadsdheaddmodel 3nheadsdmodeldhead nheadsdheaddmodel 4nheadsdheaddmodel 4nheadsdheaddmodel 2dmodel 3nheads(dmodel + 1)dhead (nheadsdhead + 1)dmodel 4nheadsdheaddmodel + 3(nheadsdhead + dmodel) 4nheadsdheaddmodel + 3(nheadsdhead + dmodel) dmodel 2dhead (nheads + 2nkv-heads)dmodeldhead nheadsdheaddmodel 2(nheads + nkv-heads)dheaddmodel + 2dhead + dmodel 2(nheads + nkv-heads)dheaddmodel Feed-forward (one transformer layer) PreNorm MLP Total Approx. OutputNorm Final logits 2dmodeldffn 2dmodeldffn 2dmodeldffn 2dmodeldffn 2dmodeldffn 2dmodeldffn 2dmodel 2dmodeldffn + dffn + dmodel 2dmodeldffn + dffn + 3dmodel 2dmodeldffn + dffn + 3dmodel dmodel nffndmodeldffn nffndmodeldffn + dmodel nffndmodeldffn dmodel Table 10. Parameter counts displayed in Table 9 using simplified notation nheadsdhead = dmodel, dffn = ρffndmodel, and nheads = gsizenkv-heads. Parameters (Kaplan et al., 2020) (Hoffmann et al., 2022) (Narayanan et al., 2021) Embedding (nvocab + nctx)dmodel (nvocab + nctx)dmodel (nvocab + nctx)dmodel Ours (Total) nvocabdmodel Attention (one transformer layer) PreNorm QKNorm QKV Project Total Approx. model 3d2 d2 model 4d2 4d2 model model Feed-forward (one transformer layer) model 3d2 d2 model 4d2 4d2 model model PreNorm MLP Total Approx. OutputNorm Final logits model model model 2ρffnd2 2ρffnd2 2ρffnd2 model model model 2ρffnd2 2ρffnd2 2ρffnd2 2dmodel 3(d2 model + dmodel) d2 model + dmodel 4d2 model + 6dmodel 4d2 model + 6dmodel 2dmodel 2ρffnd2 2ρffnd2 2ρffnd model + (1 + ρffn)dmodel model + (3 + ρffn)dmodel model + (3 + ρffn)dmodel dmodel 2dhead 2(1 + 1/gsize)d2 model (1 + 2/gsize)d2 d2 model model + 2dhead + dmodel model 2(1 + 1/gsize)d2 nffnρffnd2 model model + dmodel model dmodel nffnρffnd2 nffnρffnd2 dmodel This results in an approximation for the number of non-embedding parameters, dropping subleading terms which can be used to estimate forward FLOPs per token from the model size (Appendix H.1). nlayersd2 model 2 + (cid:18) 2 gsize + nffnρffn (cid:19) (71) H.3. FLOPs per token Distillation Scaling Laws In Table 11 we present our counting of the total number of FLOPs per token performed per token during forward pass compared to commonly used existing expressions (Kaplan et al., 2020; Hoffmann et al., 2022; Narayanan et al., 2021). We present convenient substitution in Table 12 which can be easier to work with analytically. Beyond the potential accounting for gated linear layers and grouped query attention, the most important discrepancy across methods is how the attention mechanism is handled. As was also noted in Porian et al. (2024), the expression used in Kaplan et al. (2020) is consistent with efficiently computing causal attention mechanism (Dao et al., 2022; Dao, 2024) whereas Hoffmann et al. (2022); Narayanan et al. (2021) are consistent with counting attention FLOPs for bidirectional (non-causal) attention mechanism, where the masked component of the attention matrix (zero by construction) is still being computed. We adopt the efficient expression of assuming causal computation as this more closely reflects best practice. Table 11. Forward FLOPs per for token for embedding projector, single transformer layer, final normalization and output layer. Ours indicates the expressions we use in the paper for the total (note that the quantity CForward that appears in compute constraints is the number of non-embedding floating operations. Approx. indicates taking the within-section total and dropping all terms that are not at least quadratic in one of dmodel, nvocab, and will be used for estimating the FLOPs per token from given model size (Appendix H.1). FLOPs (Kaplan et al., 2020) (Hoffmann et al., 2022) (Narayanan et al., 2021) Embedding 4dmodel Attention (one transformer layer) 2nvocabdmodel Ours (Total) 2dmodel PreNorm QKNorm QKV Logits Softmax Values Project Total Approx. 3nheads2dmodeldhead 2nheadsnctxdhead nheads2dheaddmodel 2nheadsdhead(4dmodel + nctx) 2nheadsdhead(4dmodel + nctx) 3nheads2dmodeldhead 2nheadsnctxdhead 3nheadsnctx 2nheadsnctxdhead nheads2dheaddmodel 4nheadsdhead(2dmodel + nctx) + 3nheadsnctx 4nheadsdhead(2dmodel + nctx) + 3nheadsnctx 3nheads2dmodeldhead 2nheadsnctxdhead 2nheadsnctxdhead nheads2dheaddmodel 4nheadsdhead(2dmodel + nctx) 4nheadsdhead(2dmodel + nctx) (nheads + 2nkv-heads)2dmodeldhead nheadsnctxdhead 2.5nheadsnctx nheadsnctxdhead nheads2dheaddmodel 4nheadsdhead(dmodel + nctx/2) + 4nkv-headsdmodeldhead + 2.5nheadsnctx 4nheadsdhead(dmodel + nctx/2) + 4nkv-headsdmodeldhead Feed-forward (one transformer layer) PreNorm MLP OutputNorm Final logits 4dmodeldffn 2nvocabdmodel 4dmodeldffn 2nvocabdmodel 4dmodeldffn 2nvocabdmodel 2nffndmodeldffn 2nvocabdmodel Table 12. Forward FLOPs counts per token from Table 11 simplified using nheadsdhead = dmodel, dffn = ρdmodel, and nheads = gsizenkv-heads. FLOPs (Kaplan et al., 2020) (Hoffmann et al., 2022) (Narayanan et al., 2021) Embedding 4dmodel 2nvocabdmodel Ours (Total) 2dmodel Attention (one transformer layer) PreNorm QKNorm QKV Logits Softmax Values Project Total Approx. 6d2 model 2dmodelnctx 2d2 6d model 2dmodelnctx 3nheadsnctx 2dmodelnctx 2d2 8d2 8d2 model model + 4nctxdmodel + 3nheadsnctx model + 4nctxdmodel + 3nheadsnctx 6d2 model 2dmodelnctx 2dmodelnctx 2d2 8d2 8d2 model model + 4nctxdmodel model + 4nctxdmodel 2(1 + 2/gsize)d2 model dmodelnctx 2.5nheadsnctx dmodelnctx 2d2 model (4 + 4/gsize)d2 model + 2nctxdmodel + 2.5nheadsnctx model + 2nctxdmodel (4 + 4/gsize)d2 model model + 2nctxdmodel model + 2nctxdmodel Feed-forward (one transformer layer) 8d2 8d2 PreNorm MLP OutputNorm Final logits model 4ρffnd2 2nvocabdmodel model 4ρffnd2 2nvocabdmodel model 4ρffnd2 2nvocabdmodel 2nffnρffnd2 model 2nvocabdmodel This results in an approximation for the number of non-embedding floating operations per token, dropping subleading terms CForward 2nlayersd2 model 2 + (cid:18) 2 gsize + nffnρffn + 2nlayersnctxdmodel + 2nvocabdmodel (72) (cid:19) which can be used to estimate forward FLOPs per token from the model size (Appendix H.1). 65 I. Model architecture Distillation Scaling Laws All models are based on Gunter et al. (2024) and are trained using AXLearn (Apple, 2023). All models use decoupled weight decay Loshchilov & Hutter (2019) of 104 for regularization, as well as simplified version of µP (Yang & Hu, 2021; Yang & Littwin, 2023; Yang et al., 2022; Wortsman et al., 2023; Yang et al., 2023), following what is described as µP 2 across all model sizes. Multi- (simple) in (Wortsman et al., 2024). Because of µP (simple), we fix the learning rate to 1e headed attention (MHA) is used (gsize = 1), with Pre-Normalization (Nguyen & Salazar, 2019) using RMSNorm (Zhang & Sennrich, 2019). We train all models with sequence length of nctx = 4096, with RoPE (Su et al., 2024) positional embeddings (base frequency set to 500k). All model architectures in this work are presented in Table 13, have fixed aspect ratio dmodel = 128 and fixed ffn ratio ρffn = 8/3 coupled with gated linear activation (nffn = 3). Table 13. The models used in this work. The different parameter values and FLOPs per token are shown in billions. is the number of non-embedding parameters and isthe value we use in our scaling laws. Ntotal counts all parameters in the model.Cfwd is the total number of forward FLOPs per token given by the fulltotal in Tables 11 and 12.Cfwd-approx(2N ) is the estimated value of forward FLOPs per tokenbased on the 2N approximation, and is accompanied by its relative error.Cfwd-approx(2N +σ) is the estimated value of forward FLOPs per tokenbased on the approximation given in Equation 69, and is accompanied by its relative error.The Cfwd-approx(2N +σ) is the one we use in this work. Name (B) Ntotal (B) nlayers dmodel dff Cfwd (B) Cfwd-approx(2N ) (B) Cfwd-approx(2N +σ) (B) 103M 0.1028 143M 0.1434 198M 0.1983 266M 0.2657 340M 0.3398 435M 0.4348 546M 0.546 664M 0.6636 810M 0.8096 975M 0.9755 1.147 1.15B 1.355 1.35B 1.586 1.59B 1.821 1.82B 2.102 2.1B 2.41 2.41B 2.718 2.72B 3.082 3.08B 3.478 3.48B 3.87 3.87B 4.329 4.33B 4.823 4.82B 5.309 5.31B 5.873 5.87B 6.476 6.48B 7.066 7.07B 7.747 7.75B 8.47 8.47B 9.173 9.17B 10.05 10B 10.84 10.8B 11.66 11.7B 12.61 12.6B 0.1363 0.1811 0.2402 0.3118 0.3901 0.4893 0.6047 0.7265 0.8767 1.047 1.222 1.434 1.67 1.909 2.194 2.506 2.819 3.187 3.587 3.983 4.446 4.944 5.434 6.003 6.611 7.204 7.889 8.617 9.324 10.2 11 11.83 12. 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 1024 1152 1280 1408 1536 1664 1792 1920 2048 2176 2304 2432 2560 2688 2816 2944 3072 3200 3328 3456 3584 3712 3840 3968 4096 4224 4352 4480 4608 4736 4864 4992 5120 2816 3072 3456 3840 4096 4480 4864 5120 5504 5888 6144 6528 6912 7168 7552 7936 8192 8576 8960 9216 9600 9984 10240 10624 11008 11264 11648 12032 12288 12672 13056 13312 13696 0.3411 0.4487 0.587 0.7524 0.9333 1.158 1.417 1.692 2.025 2.4 2.787 3.25 3.763 4.284 4.899 5.571 6.246 7.034 7.887 8.736 9.72 10.78 11.82 13.02 14.3 15.56 17 18.52 20.01 21.85 23.51 25.26 27.24 0.2056 (-39.74%) 0.2867 (-36.10%) 0.3965 (-32.44%) 0.5314 (-29.38%) 0.6796 (-27.19%) 0.8695 (-24.91%) 1.092 (-22.96%) 1.327 (-21.54%) 1.619 (-20.03%) 1.951 (-18.69%) 2.293 (-17.72%) 2.709 (-16.65%) 3.172 (-15.70%) 3.642 (-14.99%) 4.203 (-14.21%) 4.819 (-13.49%) 5.436 (-12.96%) 6.165 (-12.36%) 6.956 (-11.81%) 7.74 (-11.40%) 8.658 (-10.93%) 9.646 (-10.49%) 10.62 (-10.16%) 11.75 (-9.78%) 12.95 (-9.43%) 14.13 (-9.16%) 15.49 (-8.85%) 16.94 (-8.55%) 18.35 (-8.33%) 20.1 (-8.02%) 21.67 (-7.83%) 23.33 (-7.64%) 25.22 (-7.42%) 0.3398 (-0.39%) 0.4471 (-0.34%) 0.5853 (-0.29%) 0.7505 (-0.25%) 0.9312 (-0.22%) 1.156 (-0.19%) 1.415 (-0.17%) 1.689 (-0.15%) 2.022 (-0.14%) 2.397 (-0.12%) 2.784 (-0.11%) 3.247 (-0.10%) 3.759 (-0.09%) 4.28 (-0.09%) 4.895 (-0.08%) 5.567 (-0.07%) 6.241 (-0.07%) 7.03 (-0.06%) 7.883 (-0.06%) 8.731 (-0.05%) 9.715 (-0.05%) 10.77 (-0.05%) 11.81 (-0.05%) 13.01 (-0.04%) 14.29 (-0.04%) 15.55 (-0.04%) 16.99 (-0.04%) 18.52 (-0.03%) 20.01 (-0.03%) 21.84 (-0.03%) 23.5 (-0.03%) 25.25 (-0.03%) 27.23 (-0.03%) We rescale the gradients, such that the maximum of the global norm is 1.0. cosine learning rate schedule is used with warmup (2000 steps), with final learning rate of one thousandths of the peak learning rate. Z-loss (Chowdhery et al., 66 Distillation Scaling Laws 2023) of 104 is used for stability, slightly decreasing norm growth at the end of the training. For all experiments, the English-only subset of the C4 dataset (Raffel et al., 2020) is used. The C4 dataset was chosen because of its wide usage in the research community. While C4 is big enough for larger-scale experiments, it is small enough to allow for reproduction of experiments. For all distillation trainings, the teacher is trained on different split as the student. The C4 dataset has roughly 180B tokens in total, which results in 90B unique tokens for the teacher training and 90B unique tokens for the student training. Except for the largest models, all Chinchilla-optimal models do not repeat data. Models that overtrain on more than 90B tokens will have data repetition too. Muennighoff et al. (2023b) has shown (on the C4 dataset) that repeating data up to 4 times has negligible impact to loss compared to having unique data. J. Contributions All authors contributed to writing this paper, designing the experiments, discussing results at each stage of the project. Writing and framing Majority of writing done by Dan Busbridge, Jason Ramapruam, and Amitis Shidani. Research direction led by Dan Busbridge, with research framing, question identification, and prioritization done by all authors. Scaling law experiments Fixed aspect ratio models (Appendix I) FLOP counting methods (Appendix H.1), and model implementation done by Dan Busbridge, Amitis Shidani, and Floris Weers. Dataset preparation done by Floris Weers. IsoFLOP experimental design (Section 4.1) done by Dan Busbridge. Teacher training and distillations done by Dan Busbridge, Amitis Shidani, and Floris Weers. Longer training duration (512B token) teachers and students trained by Floris Weers. Scaling law analysis Original scaling law fitting code based on Besiroglu et al. (2024) developed by Amitis Shidani. Generalized, JAX Just In Time (JIT) compilation compatible scaling law fitting code, and numerical minimization approaches for compute optimal analysis (Section 5 and Appendix D) done by Dan Busbridge. Functional form (Equation 8) developed by Dan Busbridge, in collaboration with Jason Ramapuram, Amitis Shidani, Russ Webb, and Floris Weers. Scaling law downstream metrics (CDF) and top-k metrics done by Amitis Shidani. Downstream model evaluations (Appendix E.1) done by Floris Weers. Implementations of calibration Appendix E.8, Cumulative Distribution Function Teacher student capacity gaps Kernel regression demonstration of the capacity gap phenomenon (Appendix C.1) done by Etai Littwin. MLP synthetic demonstration of the capacity gap phenomenon (Appendix C.2) done by Russ Webb. Distilling language models in practice Mixing coefficient sensitivity analysis (Appendix G.1) done by Dan Busbridge and Jason Ramapuram. Temperature (Appendix G.2) and learning rate (Figure 53) sensitivity analyses done by Dan Busbridge. Top-k and top-p distribution truncation (Appendix G.4) implementation and analyses done by Jason Ramapuram. Mixing coefficient combined with truncation analysis (Appendix G.4) done by Jason Ramapuram. Reverse KL divergence Appendix G.5 implementation and analysis done by Jason Ramapuram."
        }
    ],
    "affiliations": [
        "Apple",
        "University of Oxford, UK"
    ]
}