{
    "paper_title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model Evaluation",
    "authors": [
        "Serry Sibaee",
        "Omer Nacar",
        "Adel Ammar",
        "Yasser Al-Habashi",
        "Abdulrahman Al-Batati",
        "Wadii Boulila"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper addresses critical gaps in Arabic language model evaluation by establishing comprehensive theoretical guidelines and introducing a novel evaluation framework. We first analyze existing Arabic evaluation datasets, identifying significant issues in linguistic accuracy, cultural alignment, and methodological rigor. To address these limitations in LLMs, we present the Arabic Depth Mini Dataset (ADMD), a carefully curated collection of 490 challenging questions spanning ten major domains (42 sub-domains, see Figure 1. Using ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant variations in model performance across different domains, with particular challenges in areas requiring deep cultural understanding and specialized knowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%, showing relative strength in mathematical theory in Arabic, Arabic language, and islamic domains. This work provides both theoretical foundations and practical insights for improving Arabic language model evaluation, emphasizing the importance of cultural competence alongside technical capabilities."
        },
        {
            "title": "Start",
            "content": "From Guidelines to Practice: New Paradigm for Arabic Language Model Evaluation Serry Sibaee1* Omer Nacar1 Adel Ammar1 Yasser Al-Habashi1 Abdulrahman Al-Batati1 Wadii Boulila1 1Prince Sultan University, Riyadh, Saudi Arabia {sibaee, onajar, aammar, yalhabashi, aalbatati, wboulila}@psu.edu.sa *Corresponding author: ssibaee@psu.edu.sa 5 2 0 2 2 ] . [ 1 0 2 9 1 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper addresses critical gaps in Arabic language model evaluation by establishing comprehensive theoretical guidelines and introducing novel evaluation framework. We first analyze existing Arabic evaluation datasets, identifying significant issues in linguistic accuracy, cultural alignment, and methodological rigor. To address these limitations in LLMs, we present the Arabic Depth Mini Dataset (ADMD), carefully curated collection of 490 challenging questions spanning ten major domains (42 sub-domains, see Figure 1). Using ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant variations in model performance across different domains, with particular challenges in areas requiring deep cultural understanding and specialized knowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30%, showing relative strength in mathematical theory in Arabic, Arabic language, and islamic domains. This work provides both theoretical foundations and practical insights for improving Arabic language model evaluation, emphasizing the importance of cultural competence alongside technical capabilities."
        },
        {
            "title": "Introduction",
            "content": "The evaluation of Arabic large language models (LLMs) presents unique challenges that extend beyond conventional metrics of linguistic accuracy. As these models become increasingly prevalent in various applications, the need for comprehensive and culturally aware evaluation frameworks has become critical. Recent developments in Arabic LLM evaluation have produced several datasets, including GPTArEval (Khondaker et al., 2023), Ghafa (Almazrouei et al., 2023), and ArabicMMLU from openAI (OpenAI, 2024), each attempting to address different aspects of model assessment. HowFigure 1: Representation of categories and subcategories of the proposed dataset. ever, these efforts often fail to provide comprehensive evaluation that includes both technical proficiency and cultural understanding. Current evaluation approaches frequently rely on translated content (Romanou et al., 2024) or simplified metrics that fail to capture the nuances of Arabic language and culture (OpenAI, 2024). This limitation is particularly evident in specialized domains such as Islamic studies, classical literature, and technical fields where cultural context and domain expertise are crucial. Furthermore, existing datasets often exhibit inconsistencies in linguistic standards and cultural representation, potentially resulting in misleading assessments of model capabilities. Our work addresses these challenges through three main contributions. First, we establish theoretical guidelines for Arabic evaluation datasets that encompass linguistic standards, cultural alignment, and methodological requirements. Second, we conduct detailed analysis of existing evaluation datasets, identifying common pitfalls and areas for improvement. Third, we introduce the Arabic Depth Mini Dataset (ADMD), specialized evaluation tool designed to assess both technical and cultural competencies across diverse domains. The ADMD represents significant advancement in the evaluation of Arabic LLM, featuring carefully curated questions that demand deep understanding rather than surface-level pattern matching. By evaluating leading language models using this dataset, we provide insights into current model capabilities and limitations, particularly in handling complex Arabic queries that require cultural awareness and specialized knowledge. This paper is organized as follows: Section 2 reviews related work in Arabic LLM evaluation, Section 3 presents our theoretical guidelines, Section 4 analyzes existing evaluation datasets, Section 5 introduces the ADMD and presents evaluation results, and Section 6 discusses limitations and future work directions."
        },
        {
            "title": "2 Related Works",
            "content": "The evaluation landscape for Arabic large language models (LLMs) has witnessed significant advancements through several benchmark initiatives, each with distinctive methodological approaches and inherent limitations (Eriksson et al., 2025). This section critically examines these evaluation frameworks while highlighting their methodological underpinnings and empirical contributions. GPTArEval (Khondaker et al., 2023) represents pioneering effort in Arabic LLM assessment, with its integration of ORCA (Elmadany et al., 2023) datasets and emphasis on natural language understanding and generation capabilities. The framework provides valuable insights into model performance but exhibits constraints in addressing the full spectrum of Arabic linguistic nuances. In parallel, Ghafa (Almazrouei et al., 2023) employs translation-based methodology supplemented by native speaker revisions, while ArabicMMLU (Koto et al., 2024) attempts to span diverse knowledge domains. Despite engaging ten native Arabic speakers in their validation processes, both datasets demonstrate substantial limitations in linguistic precision and comprehensive domain representation, with assessment complexity being constrained by the educational resources utilized in their development. The cultural dimension of Arabic LLM evaluation has been addressed through specialized datasets such as AraDICE (Mousi et al., 2025), which focuses specifically on dialectal variations and cultural contextual understanding, and ArSTEM (Mustapha et al., 2024), which emphasizes scientific knowledge assessment within Arabic linguistic frameworks. These initiatives reflect an emerging recognition of the importance of culturally nuanced evaluation metrics that extend beyond mere linguistic accuracy. Institutional contributions to Arabic LLM evaluation methodologies have come from teams developing models such as Jais (Sengupta et al., 2023) and Allam (Bari et al., 2024), each implementing distinctive approaches to dataset curation and evaluation protocols. However, comprehensive assessment of closed-source models such as Allam and Fanar (Team et al., 2025) remains challenging due to accessibility constraints. The Aya Expanse model (Dang et al., 2024) distinguishes itself through methodological transparency regarding its utilization of translated and GPT-generated materials, establishing an important precedent for disclosure in evaluation dataset construction. Critical methodological analysis by (Nacar et al., 2025) has illuminated significant deficiencies in existing benchmarks, particularly in ArabicMMLU (OpenAI, 2024), encompassing linguistic inconsistencies, semantic imprecisions, and fundamental methodological flaws. In response to these identified shortcomings, AraTrust (Alghamdi et al., 2024) was developed as methodologically rigorous framework specifically designed to enhance reliability assessment for Arabic LLMs (as illustrated in Table 1). While (Abdallah et al., 2024) has contributed substantially to the field with an extensive Arabic question-answering dataset comprising over 80,000 entries authored by native speakers, its reliance on Wikipedia articles as source material raises concerns regarding authoritative credibility. In contrast, Balsam (KSAA, 2024) represents the highest quality dataset produced through collaboration between prominent academic and governmental institutions across the Middle East, though its utility is limited by the relatively small number of samples within each category and it is not publicly available. Within the context of ongoing efforts to establish methodologically sound and linguistically accurate Arabic evaluation frameworks, our research makes two significant contributions: (1) compreDataset GPTArEval (Khondaker et al., 2023) Ghafa (Almazrouei et al., 2023) ArabicMMLU (Koto et al., 2024) AraDICE (Mousi et al., 2025) ArSTEM (Mustapha et al., 2024) Aya Expanse (Dang et al., 2024) AraTrust (Alghamdi et al., 2024) ILMAAM (Nacar et al., 2025) ArabicaQA (Abdallah et al., 2024) Balsam (KSAA, 2024) ADMD (Ours)"
        },
        {
            "title": "Reviewed Handwritten Generated Translated",
            "content": "Table 1: Comparison of Arabic LLM Evaluation Datasets based on annotation type and content origin. hensive theoretical analysis and empirical assessment of three critical evaluation datasetsGhafa, ArabicMMLU, and INCLUDEexamining their methodological approaches, linguistic accuracy, and domain coverage; and (2) the introduction of the Arabic Depth Mini Dataset (ADMD), conceived as foundational resource to address the current limitations in evaluating specialized domain knowledge within Arabic language contexts. The ADMD serves as an initial step toward developing more extensive and rigorous Arabic questionanswering dataset that can more effectively assess the depth of domain expertise in Arabic LLMs across disciplines."
        },
        {
            "title": "3 Theoretical Guidelines",
            "content": "This section outlines the theoretical standards and instructions necessary for building an Arabic evaluation dataset, ensuring linguistic, cultural, and methodological soundness. The guidelines are categorized into four areas: cultural, linguistics, methodology, and evaluation requirements (Figure 2), and were inspired by the work of (Nacar et al., 2025). 3.1 Linguistic Standards This section outlines the essential guidelines for ensuring high-quality and accurate translations, emphasizing linguistic precision, consistency, and contextual appropriateness in Arabic. Translation Quality: Ensure that all terms are translated accurately; untranslated terms must be transliterated if necessary (and the nonArabic word could be mentioned between brackets). Avoid literal translations by focusing on contextual adaptation, ensuring natural and consistent rendering. Review machine translations thoroughly and ensure alignment across multiple uses of the same term (e.g., consistency in letter choices for the answers like listing the Answers either in A,B,C or in Arabic (cid:104)(cid:46) (cid:44)(cid:72)(cid:46) (cid:13) (cid:64). (cid:44) Linguistic Accuracy: Adhere strictly to Arabic grammar, morphology, syntax, and orthographic rules. Avoid weak linguistic structures even if grammatically correct. Ensure stylistic adequacy and use expressions that match the intended purpose and context. Special Cases: Write poetry accurately, maintaining its structure and prosody. Write mathematical notations either in Arabic form or provide clear rules for using Latin symbols. Ensure consistent orthographic representation of dialects by adhering to standard framework, for example, (Habash et al., 2018) which gives standards to write Arabic dialects in consistent way. Figure 2: Mindmap Representation of the theoretical standards 3.2 Cultural Alignment Dataset Structure: This subsection emphasizes aligning content with Arabic cultural contexts, adapting philosophical concepts, and using culturally appropriate terminology. Cultural Relevance: Ensure questions, examples, and references align with the cultural, historical, and social contexts of the Arabicspeaking world. Avoid introducing examples or entities that are disconnected from Arab culture, such as irrelevant or Western-specific references. Philosophical and Ethical Basis: Refrain from presenting Western philosophical or ethical concepts as universal truths without explanation or adaptation. Avoid using expressions or examples that conflict with the Arab cultural context or are confusing. Terminological Adaptation: Replace Westernized terms with culturally and linguistically appropriate Arabic terms (in standard Arabic or in dialects). Provide Arabic equivalents or transliterations where necessary, maintaining cultural integrity. 3.3 Methodological and Structural Standards This subsection defines standards for organizing datasets, validating sources, and ensuring data depth and inclusivity. Organize questions logically, ensuring they are placed in their relevant categories. Avoid redundancy or confusion by grouping related queries appropriately. Ensure the information is current and includes accurate dates. Source Validation: Attribute knowledge and data to original Arabic primary sources, including books, studies, and statistical studies that are connected to Arabic societies. Avoid over-reliance on non-Arabic secondary references when constructing Arabic datasets. Writing Quranic texts with complete accuracy using the Uthmanic script. Data Depth: Ensure the dataset reflects depth and richness, avoiding straightforward, shallow, or overly simplistic questions and answers. Incorporate diverse perspectives within the Arabic-speaking world for inclusivity. 3.4 Evaluator Requirements Evaluators must demonstrate proficiency in Arabic, encompassing both linguistic nuances and cultural contexts, complemented by solid subject matter expertise. Following comprehensive analysis, we developed Python library that leverages the Claude Sonnet model to enhance the efficiency of the evaluation. This library, accessible via GitHub1, automates dataset evaluation by applying theoretical guidelines. The model analyzes provided question/answer pairs and evaluates them according to specific criteria derived from established theoretical standards (for comprehensive details regarding the prompt implementation, please refer to Appendix A.3)."
        },
        {
            "title": "Datasets",
            "content": "In this section, we review and assess three widely known Arabic evaluation datasets. representative sample from each dataset was manually evaluated by one of the authors. The evaluation followed the theoretical framework proposed in the previous section and focused on four key criteria: Language Rules, Scientific Writing, Cultural Values, and Information Correctness. Each criterion was scored on scale from 1 to 10. Language Rules: This criterion refers to the proper use of Arabic grammar, syntax, and morphology. It includes the correctness of linguistic structures, agreement in gender and number, appropriate verb forms, and adherence to Standard Arabic norms. Scientific Writing: This evaluates the clarity, precision, and formality of the writing, particularly in scientific and technical contexts. It assesses whether the text follows the conventions of scientific discourse, including proper terminology usage, logical organization, and the avoidance of informal or ambiguous expressions. Cultural Values: This assesses the datasets sensitivity to cultural norms and values in Arabic-speaking communities. It considers inclusivity, the use of culturally appropriate examples, and the avoidance of content that may be considered offensive or misaligned with regional social norms. Information Correctness: This criterion examines the factual accuracy and consistency of the information provided in the dataset. It checks whether the content aligns with reliable knowledge sources and avoids misinformation or logical inconsistencies. 1https://github.com/serrysibaee/EAED The datasets selected for evaluation are as follows: 1. Ghafa dataset (Almazrouei et al., 2023), multiple-choice zero and few-shot evaluation benchmark across wide range of tasks. 2. ArabicMMLU (OpenAI version) (OpenAI, 2024), an Arabic adaptation of the MMLU benchmark, It covers broad range of topics from 57 different categories, covering elementary-level knowledge up to advanced professional subjects like law, physics, history, and computer science.. 3. Cohere \"Include\" dataset (Romanou et al., 2024), an open-source multilingual dataset consists of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in variety of regional contexts covering 44 languages including Arabic. Each dataset was assessed independently, and the results highlight both the progress made and the areas that still require improvement in Arabic evaluation benchmarks. 4.1 Al Ghafa Dataset From this dataset (Almazrouei et al., 2023), we sampled 100 examples, which were reviewed by native Arabic speaker according to the evaluation criteria outlined previously. The dataset received the evaluation scores shown in Table 2. Criterion Language Rules Scientific Writing Cultural Values Information Correctness Score /10 4.5 4.6 3.9 6.1 Table 2: Evaluation Scores for Al Ghafa Dataset By considering that any sample that has an evaluation score lower than 5 is wrong sample, we extracted: 50 wrong samples from language rules (Linguistic Accuracy), 42 from Scientific Writing, 60 from Cultural Values, and 26 from Information Correctness. Below are examples of evaluated samples, along with their identified issues: (cid:16)(cid:233)(cid:9)(cid:74)(cid:131) (cid:189) (cid:17)(cid:130)(cid:203)(cid:64) (cid:208)(cid:241)(cid:75)(cid:10) (cid:208)(cid:65)(cid:74)(cid:10)(cid:147) Translation: Fasting on 1. the day of doubt is Sunnah. Issue: The answer is inconsistentits ruling depending on the disagreement. 2. (cid:40)(cid:40)(cid:49)(cid:56)(cid:41) (cid:11)(cid:16)(cid:233)(cid:11)(cid:74)(cid:10)(cid:9)(cid:75)(cid:11)(cid:65)(cid:11)(cid:75)(cid:46) (cid:21)(cid:89)(cid:203)(cid:64) (cid:12)(cid:168) (cid:21)(cid:89) (cid:11)(cid:9)(cid:74) (cid:11)(cid:131)(cid:41) Translation: Allah said, \"So let him call his associates (17), We will call the guards of Hell (18).\" Issue: Incorrect transcription of the Quranic text, including errors in diacrit- (cid:16)(cid:233)(cid:11)(cid:74)(cid:10)(cid:9)(cid:75)(cid:11)(cid:65)(cid:75)(cid:46) ics. The correct form is (cid:21) (cid:203)(cid:64) Translation: Thirteen (cid:21)(cid:208)(cid:65)(cid:11)(cid:171) (cid:49)(cid:51) (cid:5) (cid:11)(cid:15)(cid:9)(cid:81)(cid:203)(cid:64). (cid:21)(cid:9)(cid:81)(cid:21)(cid:9)(cid:30)(cid:74)(cid:10)(cid:203)(cid:11) 3. (cid:21)(cid:81)(cid:11)(cid:16)(cid:30)(cid:21)(cid:74)(cid:10)(cid:75)(cid:46)(cid:11) years old Peter Linz. (cid:21) Issue: Grammatical errorthe correct (cid:203)(cid:64). form is (cid:65)(cid:19)(cid:211)(cid:65)(cid:11)(cid:171) (cid:49)(cid:51) (cid:5) (cid:15)(cid:11)(cid:81)(cid:203)(cid:64) 4. (cid:63)(cid:200)(cid:11)(cid:65) (cid:11)(cid:103)(cid:46) (cid:12)(cid:89)(cid:15)(cid:11)(cid:74)(cid:10) (cid:11)(cid:131) (cid:11)(cid:241)(cid:12)(cid:235) (cid:9)(cid:224) (cid:213)(cid:230)(cid:148)(cid:170)(cid:11)(cid:203)(cid:64) (cid:233)(cid:11)(cid:74)(cid:10) (cid:11) (cid:11)(cid:16)(cid:74)(cid:21)(cid:170)(cid:11)(cid:75)(cid:10) (cid:65)(cid:11)(cid:210) (cid:187) Translation: As they believe in his infallibility, is he the master of men? (cid:11)(cid:9)(cid:224)(cid:240) (cid:12)(cid:89)(cid:16)(cid:174)(cid:11) (cid:9)(cid:175)(cid:11) Issue: Spelling and typographical error. (cid:12)(cid:16)(cid:233)(cid:11)(cid:210) (cid:21)(cid:146)(cid:170)(cid:11) (cid:21) (cid:203)(cid:64). The correct form is 4.2 ArabicMMLU The Arabic MMLU Benchmark (OpenAI, 2024), derived from the original English version (Hendrycks et al., 2020), exists in two translations: one by GPT-3.5 Turbo and another by native Arabic translators. Despite its widespread adoption for Arabic LLM evaluation, the benchmark exhibits significant limitations in cultural adaptation and translation quality. Empirical analysis revealed three primary deficiencies: (1): Linguistic Fidelity following Arabic Grammar and translation quality, (2): Cultural Alignment: variant western focus with no Arabic alignment and (3): Structural Integrity: Suboptimal organization and insufficient Arabic source attribution. The evaluation scores are shown in Table 3. Criterion Language Rules Scientific Writing Culture Values Information Correctness Score /10 6.5 5.5 3.4 6.5 did not translate the word Issue: (cid:16)(cid:233)(cid:74)(cid:10)(cid:107)(cid:46) (cid:241)(cid:203)(cid:241)(cid:74)(cid:10)(cid:130) (cid:9)(cid:174)(cid:203)(cid:64) which has Arabic term (cid:13) (cid:9)(cid:173)(cid:13)(cid:75)(cid:65) (cid:9)(cid:163)(cid:240) (cid:44) (cid:16)(cid:233)(cid:75)(cid:10)(cid:80)(cid:241)(cid:210)(cid:130)(cid:109)(cid:46)(cid:204)(cid:39)(cid:64). (cid:90)(cid:65) (cid:9)(cid:146)(cid:171) (cid:66)(cid:64) (cid:16)(cid:233)(cid:74)(cid:10)(cid:238)(cid:68)(cid:10)(cid:107)(cid:46) (cid:241)(cid:16)(cid:74)(cid:203)(cid:64) (cid:16)(cid:233)(cid:9)(cid:74)(cid:106)(cid:46) (cid:202)(cid:203) (cid:13)(cid:248)(cid:88)(cid:65)(cid:74)(cid:46)(cid:214)(cid:207)(cid:64) Translation: (cid:13)(cid:241)(cid:9)(cid:175)(cid:65)(cid:190)(cid:16)(cid:75) 2. Guidelines of the Equality Committee Issue: The reliance on Western laws and regulations without providing Arabic contextual alternatives. The solution is to train and use culturally aware models that understand the context and use the convenient words according to the Arabic culture. 3. No mention of studies or statistics of the Arabic society. 4.3 INCLUDE dataset INCLUDE (Romanou et al., 2024) is multilingual benchmark evaluating knowledge and reasoning across 44 languages. The Arabic subset (551 MCQs) exhibited significant quality issues: (1) Poor Quality 70% contained severe spelling errors, and 80% required major revisions in structure and content. (2) Incorrect Answers Notably in Islamic studies, where precision is critical. (3) Misinformation Some questions conveyed ambiguous or incorrect meanings, particularly in religious contexts. Table 4 presents the dataset evaluation (excluding culture-related data2). Criterion Language Rules Scientific Writing Cultural Values Information Correctness Score /10 4.5 3.5 - 7.0 Table 4: Evaluation Scores for INCLUDE Dataset. Below are examples of evaluated samples along with their identified issues: 1. Spelling Errors: Original: (cid:89)(cid:15)(cid:170)(cid:16)(cid:75) (cid:250)(cid:206)(cid:171) (cid:16)(cid:232) (cid:13)(cid:241) (cid:17)(cid:130)(cid:9)(cid:28)(cid:214)(cid:207)(cid:64) Translation: was Table 3: Evaluation Scores for ArabicMMLU Dataset. constructed on. Below are three representative examples of identified issues: (cid:16)(cid:233)(cid:74)(cid:10)(cid:107)(cid:46) (cid:241)(cid:203)(cid:241)(cid:74)(cid:10)(cid:130) (cid:9)(cid:174)(cid:203)(cid:64) (cid:16)(cid:72)(cid:65) (cid:9)(cid:174)(cid:171)(cid:65) (cid:9)(cid:146)(cid:214)(cid:207)(cid:64) Translation: Physio1. logical complications Issue: spelling mistake the correct is (cid:89)(cid:170)(cid:16)(cid:75) (cid:250)(cid:206)(cid:171) (cid:16)(cid:232) (cid:13) (cid:65) (cid:17)(cid:130)(cid:9)(cid:28)(cid:214)(cid:207)(cid:64). 2. Misleading Questions: 2No culture data was in the dataset (cid:16)(cid:233)(cid:9)(cid:74)(cid:131) (cid:9)(cid:224)(cid:65) (cid:9)(cid:146)(cid:211)(cid:80) (cid:208)(cid:241)(cid:147) Translation: Original: Fasting Ramadan is not mandatory. Issue: Ramadan Fasting in Islam is mandatory."
        },
        {
            "title": "5 MiniDataset",
            "content": "We developed (by our inner researchers in the lab: 3 Syrians, 1 Yemeni) compact yet highly challenging Arabic dataset3 consisting of 490 carefully curated questions sourced from diverse books and references (more detail in Appendix A.2). The dataset spans ten major domains, covering general science, Islamic studies, Arabic language, and cultural topics (detailed in Appendix A). Unlike conventional benchmarks that rely on automated statistical analysis, our evaluation methodology is based on thorough manual review4. See Table 6 for detailed statistics about the categories of the dataset, and Figure 3 for the used prompt. We did not use LLM Judge in this paper because recent research shows (Wu et al., 2025) that for non-English tasks, it is better to use manual evaluation. Figure 3: The LLM prompt translates to: You are an expert in [Scientific field] and you need to answer the question scientifically and correctly. \"Question\". To assess the ability of language models to handle complex Arabic inquiries with precision and depth, we conducted extensive testing on leading models, including GPT-4, Sonnet Claude 3.55, Gemini Flash 1.5, CommandR 100B6, and QwenMax 2.57. The primary results are presented in Figure 5, with key insights discussed in the following section. 3Data will be publicly available for sample check appendix A.2 4After several experiments, we found that the most effective way to automate the evaluation is by using judge LLM 5claude-3-5-sonnet-20241022 6https://huggingface.co/CohereForAI/c4ai-command-rplus 7https://qwenlm.github.io/blog/qwen2.5-max/ 5.1 Main insights The human evaluation results reveal significant performance differences among language models in handling complex Arabic questions8. Claude 3.5 Sonnet achieved the highest accuracy, correctly answering 147 questions (30%), with notable strength in Mathematics & Computational Sciences (50%), Philosophy & Logic (50%), and General & Miscellaneous Sciences (51.67%), as shown in Table 11. In Natural Sciences, it exhibited balanced mix of True (45%) and PartiallyTrue (45%) responses. GPT-4 had the weakest performance, with only 44 correct answers and the highest incorrect count (355)  (Table 7)  , indicating difficulty in nuanced Arabic queries. Gemini Flash 1.5 and CommandR100B showed moderate performance but high false rates (Table 10, Table 9). Qwen-Max had one of the lowest True counts (52) while being competitive in Partially-True responses  (Table 8)  , reflecting weaknesses in factual reasoning. Islamic & Religious Studies and Linguistics & Literature had the highest false rates, with Claude 3.5 Sonnet performing relatively better (41.82% False vs. over 80% for other models). These results highlight the models struggles with nuanced interpretation. Future improvements should focus on reducing False responses while refining Partially-True classifications to enhance factual accuracy. Model Sonnet 3.5 Gemini R+ Qwen GPT-4 (%) 33.5 22.1 15.0 13.1 11.8 (%) 43.5 56.2 54.0 57.4 67.3 PT (%) 18.2 12.0 15.6 17.8 17.3 PF (%) 4.8 9.7 15.4 11.7 3. Table 5: Model Performance Metrics (average for each model on the categories). T: True, F: False, PT: PartiallyTrue, PF: Partially-False. True means the model answered 100% correctly and False means not-correct. Partially-True corresponds to an answer 60-80% correct, whereas Partially-False corresponds to an answer only 20-30% correct."
        },
        {
            "title": "6 Limitations",
            "content": "The study faces several limitations, including the scalability challenge of manual evaluation and limited query diversity per topic. Key subjects such 8True means the model answered correctly and False is notcorrect. Partially-True it answered 60-80% correct, PartiallyFalse the answer is 20-30% correct. Figure 4: Visual summary of Q/A word counts et al., 2024), Fanar (Team et al., 2025), Aya (Dang et al., 2024), and DeepSeek (Liu et al., 2024), will be assessed for broader comparison. Moreover, optimized prompting strategies will be explored to improve response accuracy and quality."
        },
        {
            "title": "8 Conclusion",
            "content": "This paper proposed comprehensive framework for evaluating Arabic language models, addressing linguistic, cultural, and methodological aspects. Our analysis identified limitations in existing evaluation datasets, including linguistic inaccuracies and cultural misalignment. To bridge these gaps, we introduced the Arabic Depth Mini Dataset (ADMD) with 490 questions across ten domains. Model evaluations using ADMD revealed varied performance, with Claude 3.5 Sonnet excelling in Mathematics & Logic but all models struggling with culturally nuanced topics. These findings highlight the need for more refined evaluation methodologies to enhance Arabic NLP, ensuring both technical precision and cultural competence."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors thank Prince Sultan University for their support. Figure 5: Models results. True means the model answered 100% correctly and False means not-correct. Partially-True corresponds to an answer 60-80% correct, whereas Partially-False corresponds to an answer only 20-30% correct. as Physics, Chemistry, and advanced mathematics were excluded, alongside minimal expertise in specialized fields like Medicine. Subjective topics (e.g., psychology, sociology) complicate assessment, and dataset evaluation remains timeintensive. Additionally, the exclusion of several Arabic models restricts the breadth of comparative analysis."
        },
        {
            "title": "7 Future Work",
            "content": "Future work will focus on expanding the dataset to cover more topics and question types, including MCQs and logic-based questions, to enhance evaluation comprehensiveness. Additional models, such as Jais (Sengupta et al., 2023), Allam (Bari"
        },
        {
            "title": "References",
            "content": "Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, and Adam Jatowt. 2024. Arabicaqa: comprehensive dataset for arabic question answering. Preprint, arXiv:2403.17848. Emad A. Alghamdi, Reem I. Masoud, Deema Alnuhait, Afnan Y. Alomairi, Ahmed Ashraf, and Mohamed Zaytoon. 2024. Aratrust: An evaluation of trustworthiness for llms in arabic. Preprint, arXiv:2403.09017. Ebtesam Almazrouei, Ruxandra Cojocaru, Michele Baldo, Quentin Malartic, Hamza Alobeidli, Daniele Mazzotta, Guilherme Penedo, Giulia Campesan, Mugariya Farooq, Maitha Alhammadi, Julien Launay, and Badreddine Noune. 2023. AlGhafa evaluation benchmark for Arabic language models. In Proceedings of ArabicNLP 2023, pages 244275, Singapore (Hybrid). Association for Computational Linguistics. Saiful Bari, Yazeed Alnumay, Norah A. Alzahrani, Nouf M. Alotaibi, Hisham A. Alyahya, Sultan AlRashed, Faisal A. Mirza, Shaykhah Z. Alsubaie, Hassan A. Alahmed, Ghadah Alabduljabbar, Raghad Alkhathran, Yousef Almushayqih, Raneem Alnajim, Salman Alsubaihi, Maryam Al Mansour, Majed Alrubaian, Ali Alammari, Zaki Alawami, Abdulmohsen Al-Thubaity, Ahmed Abdelali, Jeril Kuriakose, Abdalghani Abujabal, Nora Al-Twairesh, Areeb Alowisheq, and Haidar Khan. 2024. Allam: Large language models for arabic and english. Preprint, arXiv:2407.15390. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Üstün, and Sara Hooker. 2024. Aya expanse: Combining research breakthroughs for new multilingual frontier. Preprint, arXiv:2412.04261. AbdelRahim Elmadany, ElMoatez Billah Nagoudi, and Muhammad Abdul-Mageed. 2023. ORCA: challenging benchmark for Arabic language understandIn Findings of the Association for Computaing. tional Linguistics: ACL 2023, pages 95599586, Toronto, Canada. Association for Computational Linguistics. Maria Eriksson, Erasmo Purificato, Arman Noroozian, Joao Vinagre, Guillaume Chaslot, Emilia Gomez, and David Fernandez-Llorca. 2025. Can we trust ai benchmarks? an interdisciplinary review of current issues in ai evaluation. Preprint, arXiv:2502.06559. Nizar Habash, Fadhl Eryani, Salam Khalifa, Owen Rambow, Dana Abdulrahim, Alexander Erdmann, Reem Faraj, Wajdi Zaghouani, Houda Bouamor, Nasser Zalmout, Sara Hassan, Faisal Al-Shargi, Sakhar Alkhereyf, Basma Abdulkareem, Ramy Eskander, Mohammad Salameh, and Hind Saddiki. 2018. Unified guidelines and resources for Arabic dialect orIn Proceedings of the Eleventh Interthography. national Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Md Tawkat Islam Khondaker, Abdul Waheed, El Moatez Billah Nagoudi, and Muhammad AbdulMageed. 2023. GPTAraEval: comprehensive evaluation of ChatGPT on Arabic NLP. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 220247, Singapore. Association for Computational Linguistics. Fajri Koto, Haonan Li, Sara Shatnawi, Jad Doughman, Abdelrahman Sadallah, Aisha Alraeesi, Khalid Almubarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata, Nizar Habash, Preslav Nakov, and Timothy Baldwin. 2024. ArabicMMLU: Assessing massive multitask language understanding in Arabic. In Findings of the Association for Computational Linguistics: ACL 2024, pages 56225640, Bangkok, Thailand. Association for Computational Linguistics. KSAA. 2024. Balsam benchmark for evaluating https:// arabic large language models (llms). benchmarks.ksaa.gov.sa/b/balsam. Balsam is collaborative initiative between prominent academic and governmental institutions in the Middle East. It aims to lead the development and provisioning of specialized evaluation datasets essential for assessing the performance of Arabic Large Language Models (LLMs) across wide range of Natural Language Processing (NLP) tasks. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. Basel Mousi, Nadir Durrani, Fatema Ahmad, Md. Arid Hasan, Maram Hasanain, Tameem Kabbani, Fahim Dalvi, Shammur Absar Chowdhury, and Firoj Alam. 2025. AraDiCE: Benchmarks for dialectal and cultural capabilities in LLMs. In Proceedings of the 31st International Conference on Computational Linguistics, pages 41864218, Abu Dhabi, UAE. Association for Computational Linguistics. Ahmad Mustapha, Hadi Al-Khansa, Hadi Al-Mubasher, Aya Mourad, Ranam Hamoud, Hasan El-Husseini, Marwah Al-Sakkaf, and Mariette Awad. 2024. Arastem: native arabic multiple choice question benchmark for evaluating llms knowledge in stem subjects. Preprint, arXiv:2501.00559. Omer Nacar, Serry Taiseer Sibaee, Samar Ahmed, Safa Ben Atitallah, Adel Ammar, Yasser Alhabashi, Abdulrahman S. Al-Batati, Arwa Alsehibani, Nour Qandos, Omar Elshehy, Mohamed Abdelkader, and Anis Koubaa. 2025. Towards inclusive Arabic LLMs: culturally aligned benchmark in Arabic large language model evaluation. In Proceedings of the First Workshop on Language Models for Low-Resource Languages, pages 387401, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. OpenAI. 2024. Multilingual massive multitask language understanding (MMMLU). Accessed: 202501-14. Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed Haggag, Alfonso Amayuelas, et al. 2024. Include: Evaluating multilingual language understanding with regional knowledge. arXiv preprint arXiv:2411.19799. Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, William Marshall, Gurpreet Gosal, Cynthia Liu, Zhiming Chen, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Xudong Han, Sondos Mahmoud Bsharat, Alham Fikri Aji, Zhiqiang Shen, Zhengzhong Liu, Natalia Vassilieva, Joel Hestness, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Hector Xuguang Ren, Preslav Nakov, Timothy Baldwin, and Eric Xing. 2023. Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models. Preprint, arXiv:2308.16149. Fanar Team, Ummar Abbas, Mohammad Shahmeer Ahmad, Firoj Alam, Enes Altinisik, Ehsannedin Asgari, Yazan Boshmaf, Sabri Boughorbel, Sanjay Chawla, Shammur Chowdhury, Fahim Dalvi, Kareem Darwish, Nadir Durrani, Mohamed Elfeky, Ahmed Elmagarmid, Mohamed Eltabakh, Masoomali Fatehkia, Anastasios Fragkopoulos, Maram Hasanain, Majd Hawasly, Musab Husaini, Soon-Gyo Jung, Ji Kim Lucas, Walid Magdy, Safa Messaoud, Abubakr Mohamed, Tasnim Mohiuddin, Basel Mousi, Hamdy Mubarak, Ahmad Musleh, Zan Naeem, Mourad Ouzzani, Dorde Popovic, Amin Sadeghi, Husrev Taha Sencar, Mohammed Shinoy, Omar Sinan, Yifan Zhang, Ahmed Ali, Yassine El Kheir, Xiaosong Ma, and Chaoyi Ruan. 2025. Fanar: An arabiccentric multimodal generative ai platform. Preprint, arXiv:2501.13944. Minghao Wu, Weixuan Wang, Sinuo Liu, Huifeng Yin, Xintong Wang, Yu Zhao, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2025. The bitter lesson learned from 2,000+ multilingual benchmarks. Preprint, arXiv:2504.15521."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Topics and References This dataset covers 42 topics across various domains (each topic has 10 questions except general language and diversified science, which each have 50). The topics and their corresponding references are as follows (all of the used resources are publicly available books and websites): Applied Sciences & Engineering: Mechanical Engineering * Asasiyat al-Ladoona 9 * Al-Tibaa al-Thulathiyya al-Abad 10 Computer Science * Uloom al-Computer: Muqaddima Mukhtasara Jiddan 11 Medicine * Mujam al-Mustalahat al-Tibbiyya (al-Qahira) * Mawsuat al-Tibb al-Nabawi li alAsfahani Nutrition (include Health & Fitness) * Zad al-Maad 12 * Al-Sihha wa al-Taghziya Dr. Iman Basheer Abukibda * Kitab al-Tabikh 13 Earth Science * Al-Mujam al-Geologi al-Musawwar Natural Sciences: Biology * Ilm al-Ahyaa al-Takhalluqiyy: Muqaddima Qasira Jiddan 15 * Suluk al-Hayawan Cosmology * Ilm al-Kawniyyat: Muqaddima Qasira Jiddan 17 Social Sciences & Humanities: Psychology * Mabadi al-Tahlil al-Nafsi 9https://research-solution.com/ 10https://www.hindawi.org/books/16479648/ 11https://www.hindawi.org/books/13726364/ 12https://shamela.ws/book/21713 13https://www.hindawi.org/books/93091736/ 14https://sgs.gov.sa/ 15https://www.hindawi.org/books/42830286/ 16https://www.hindawi.org/books/86942829/ 17https://www.hindawi.org/books/49725968/ 18https://www.hindawi.org/books/72426903/ * Diraasat fi al-Takamul al-Nafsi 19 Sociology * Mawsuat Ilm al-Ijtima Gordan Marshal * Ilm al-Ijtima inda al-Arab Anthropology * Mawsuat Ilm al-Insan Charlotte Seymour Media & Communication * Maqalat Ilmiyya fi Ulum al-Ilam wa al-Ittisal * Nazariyyat al-Ilam wa al-Ittisal 22 Economics * Muhadarat fi Mabadi al-Iqtisad 23 * Duroos Mubassata fi al-Iqtisad 24 Islamic & Religious Studies: Quranic Exegesis (Tafsir): Tafsir Ibn Jarir al-Tabari Hadith: Tarh al-Tathreeb Sharh alTaqreeb al-Iraqi 26 Mustalah: Fath al-Mugheeth Sharh Alfiyyat al-Hadith al-Subki 27 Fiqh: al-Mughni li Ibn Qudamah 28 Usul al-Fiqh: Sharh Mukhtasar al-Tahrir Ibn al-Najjar Faraid: Al-Asilah al-Miat al-Mukhtara fi al-Faraid 30 Aqeedah: Sharh al-Tahawiyyah Ibn Abi al-Izz 31 Tajweed: al-Tamheed fi al-Tajweed alJazari Qiraat: al-Nashr fi al-Qiraat al-Ashr Ibn al-Jazari 33 Seerah: Sirat Ibn Hisham 34 19https://www.hindawi.org/books/93638206/ 20https://asjp.cerist.dz/en/article/70499 21https://www.alukah.net/culture/0/84274/ 22https://pedia.svuonline.org/ 23https://www.hindawi.org/books/91480861/ 24https://www.hindawi.org/books/60808479/ 25https://shamela.ws/book/43 26https://shamela.ws/book/11036 27https://shamela.ws/book/5963 28https://shamela.ws/book/8463 29https://shamela.ws/book/12019 30https://almwareeth.com/ 31https://shamela.ws/book/8352 32https://shamela.ws/book/8194 33https://shamela.ws/book/22642 34https://shamela.ws/book/23833 Tarajim al-Rijal: Nuzhat al-Fudala, alMukhtar al-Masoon Muhammad alSharif Linguistics & Literature: Nahw: Sharh Ibn Aqeel ala Alfiyyat Ibn Malik 35 Sarf: Sharh Tasreef al-Izzi al-Taftazani Balagha: Sharh Talkhees al-Miftah alTaftazani Arood & Qawafi: Meezan al-Dhahab fi Sinaat Shir al-Arab Poetry: Shir al-Shuara al-Sitta alShantamri 37, Muntaha al-Talab fi Shir al-Arab Ibn Maymoon Arabic Language: Taj al-Aroos Sharh alQamoos 38, Lisan al-Arab Ibn Manzur 39 General Linguistics: Diwan al-Lugha alArabiyya 40 Arabic Linguistics: Kitab Ittila ala alNazariyyat al-Lisaniyya wa al-Dalaliyya alArabic Logic: Sharh Matn Shamsiyyah al-Katibi Philosophy & Logic: Philosophy: Mawsuat al-Falsafah alBadawi Culture & Arts: Music: Kitab al-Musiqa al-Farabi, Kitab al-Musiqa al-Sharqiyya 41 Folklore & Cultural Studies: Interviews with native speakers from Yemen, Syria, Saudi Arabia, and Algeria. Mathematics & Computational Sciences: Mathematics: Mujam al-Riyadiyyat Majma Dimashq Machine Learning: Mujam Mustalahat Ilm al-Bayanat wa al-Taallum al-Amiq Alaa Tuaymah 43 35https://shamela.ws/book/9904 36https://www.hindawi.org/books/50259706/ 37https://shamela.ws/book/5449 38https://shamela.ws/book/7030 39https://shamela.ws/book/1687 40http://diwanalarabia.com/ 41https://www.hindawi.org/books/46319638/3/ 42https://arabacademy-sy.org/ar/page17418/ 43https://dlarabic.com/ General & Miscellaneous Sciences: LLM Prompt General Sciences: Hindawi Science Books Cooking: Kitab al-Tabikh 45 Historical & Genealogical Studies: Scientific field <You are an expert Arabic linguist and evaluator> <You must evaluate the given Arabic text based on grammar, syntax, morphology, and stylistic clarity> <Ensure scoring includes special cases such as poetry, dialect, and mathematical notation> <Provide final numeric score only, from 1 to 10>. Question : <Evaluate the following Arabic text and return only numeric score.> Genealogy (Ansab): Kitab al-Ansab al-Samani 46 LLM Prompt Language Extensions: General Arabic Language (50 questions): Diwan al-Lugha al-Arabiyya 47 Diversified Sciences (50 questions): Hindawi Science Collection 48 Dialects (Lahajat): Mo3jam 49, Amiyah 50 This structured categorization ensures wellorganized representation of the datasets diverse topics, making it suitable for evaluating Arabic LLMs across multiple domains. A.2 Examples from the ADMD In this section, we present examples from each topic in the ADMD dataset. Due to the length of these examples and technical issues related to handling long Arabic texts in the ACL format, we have opted to provide the examples in more accessible format via Google Sheet. This allows for easier reading and also includes their English translations. You can access the examples and their translations through the following link: https://docs.google.com/spreadsheets/ d/1Nl9ZDzNK29yJPpFepx453Lhbwf6HAPSnc_ K5sGIfZ7U/edit?usp=sharing A.3 Prompt for Evaluating in EAED This prompt evaluates the linguistic quality of Arabic textsincluding grammar, style, and exceptions like poetry or dialects. It aims to ensure objective scoring based on strict language norms. Assesses the quality of Arabic translations, focusing on grammar, meaning accuracy, and cultural alignment. It follows professional standards for rating fidelity and fluency. 44https://www.hindawi.org/ 45https://www.hindawi.org/books/93091736/ 46https://shamela.ws/book/1656 47http://diwanalarabia.com/ 48https://www.hindawi.org/ 49https://ar.mo3jam.com/dialect/ 50https://3amyah.com/ Scientific field <You are highly skilled Arabic translator> <Your task is to assess translations quality, consistency, and grammatical integrity> <Evaluate special elements like poetic structure and notation formatting> <Give score from 1 to 10, printed alone>. Question : <Assess the quality of this Arabic translation and return only number>. The following prompt assesses how well the text aligns with Arabic cultural and ethical standards, including terminology usage and relevance to the societal context. LLM Prompt Scientific field <You are an expert in Arabic language and culture> <You must assess the cultural relevance and sensitivity of the given text> <Evaluate alignment with Arab societal, ethical, and terminological standards> <Return only number from 1 to 10>. Question : <Evaluate this texts cultural alignment and print numeric score>. The following prompt focuses on structure, credibility, and richness of Arabic data or texts. It ensures sources are validated and the methodology is coherent and rooted in authentic Arabic references. LLM Prompt Scientific field <You are an expert in Arabic data structuring and methodology> <Evaluate the dataset or text for organization, source validation, and informational richness> <Verify alignment with Arabic primary sources and proper use of terminology> <Respond with only numeric score between 1 and 10>. Question : <Assess the methodological quality of the following text or dataset and return only number>. A.4 Detailed Tables A.4.1 Details analysis about ADMD dataset QnAs This table is giving comprehensive analysis about each category of the dataset with it number of rows, number of words in each Question and Answer, avrage words in Question and answer. Subject Machine Learning Tafsir (Exegesis) Grammar (Nahw) Psychology Nutrition Dialects Mechanical Engineering Biology Cosmology Biography Morphology (Sarf) Tajweed Rhetoric (Balagha) Prosody Jurisprudence (Fiqh) Inheritance (Faraid) Usul al-Fiqh Creed (Aqidah) Hadith Terminology Hadith Prophetic Biography Mathematics Linguistics Logic (Arabic) Genealogy Poetry Quranic Readings Music Folklore & Culture Philosophy Geoscience Language (General) Economics Sociology Medicine Computer Science Anthropology Media & Communication General Linguistics General Sciences Cooking Rows Words Words Avg Avg Total 485 331 224 105 285 139 426 330 315 186 170 99 828 665 972 780 752 596 226 184 244 126 256 169 374 251 219 88 289 157 475 96 303 140 309 165 366 125 426 186 381 124 243 51 313 89 366 85 197 104 221 66 217 59 193 68 186 104 218 77 328 158 210 103 236 57 181 43 222 101 228 83 272 86 254 84 662 310 1091 402 334 15.40 11.90 14.60 9.60 12.90 7.10 16.30 19.20 15.60 4.20 11.80 8.70 12.30 13.10 13.20 37.90 16.30 14.40 24.10 24.00 25.70 19.20 22.40 28.10 9.30 15.50 15.80 12.50 8.20 14.10 17.00 10.70 17.90 13.80 12.10 14.50 18.60 17.00 7.04 13.78 16.10 33.10 10.50 13.90 33.00 18.60 9.90 66.50 78.00 59.60 18.40 12.60 16.90 25.10 8.80 15.70 9.60 14.00 16.50 12.50 18.60 12.40 5.10 8.90 8.50 10.40 6.60 5.90 6.80 10.40 7.70 15.80 10.30 5.70 4.30 10.10 8.30 8.60 8.40 6.20 8.04 17.30 154 119 146 96 129 71 163 192 156 42 118 87 123 131 132 379 163 144 241 240 257 192 224 281 93 155 158 125 82 141 170 107 179 138 121 145 186 170 352 689 161 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 50 50 10 Table 6: Word statistics across subjects (first 10 rows per sheet, or 50 for long sheets). A.5 Detailed tables for LLM Evaluation These tables show detailed numbers for each model evaluation on the ADMD dataset. Field of Study Applied Sciences & Engineering Natural Sciences Social Sciences & Humanities Islamic & Religious Studies Linguistics & Literature Philosophy & Logic Culture & Arts Mathematics & Computational Sciences General & Miscellaneous Sciences Historical & Genealogical Studies True (%) False (%) Partially-True (%) Partially-False (%) 22.00 20.00 12.00 0.91 1.82 10.00 10.00 25.00 16.67 0.00 42.00 35.00 56.00 80.91 94.55 80.00 75.00 45.00 65.00 100.00 28.00 45.00 26.00 10.00 2.73 10.00 10.00 25.00 16.67 0. 8.00 0.00 6.00 8.18 0.91 0.00 5.00 5.00 1.67 0.00 Table 7: Statistics for GPT-4 answers for the different categories. Field of Study Applied Sciences & Engineering Natural Sciences Social Sciences & Humanities Islamic & Religious Studies Linguistics & Literature Philosophy & Logic Culture & Arts Mathematics & Computational Sciences General & Miscellaneous Sciences Historical & Genealogical Studies True (%) False (%) Partially-True (%) Partially-False (%) 20.00 20.00 18.00 4.55 1.82 15.00 5.00 20.00 26.67 0.00 42.00 15.00 42.00 80.00 90.00 70.00 85.00 30.00 50.00 70. 18.00 40.00 24.00 5.45 3.64 5.00 10.00 35.00 16.67 20.00 20.00 25.00 16.00 10.00 4.55 10.00 0.00 15.00 6.67 10.00 Table 8: Statistics for Qwen-Max. Field of Study Applied Sciences & Engineering Natural Sciences Social Sciences & Humanities Islamic & Religious Studies Linguistics & Literature Philosophy & Logic Culture & Arts Mathematics & Computational Sciences General & Miscellaneous Sciences Historical & Genealogical Studies True (%) False (%) Partially-True (%) Partially-False (%) 30.00 30.00 18.00 3.64 4.55 10.00 15.00 25.00 13.33 0. 52.00 15.00 46.00 69.09 82.73 45.00 70.00 30.00 60.00 70.00 6.00 50.00 20.00 10.00 3.64 15.00 5.00 25.00 11.67 10.00 12.00 5.00 16.00 17.27 9.09 30.00 10.00 20.00 15.00 20.00 Table 9: Statistics for commandR_100B. Field of Study Applied Sciences & Engineering Natural Sciences Social Sciences & Humanities Islamic & Religious Studies Linguistics & Literature Philosophy & Logic Culture & Arts Mathematics & Computational Sciences General & Miscellaneous Sciences Historical & Genealogical Studies True (%) False (%) Partially-True (%) Partially-False (%) 24.00 40.00 38.00 0.00 2.75 15.00 10.00 45.00 36.67 10.00 46.00 15.00 32.00 88.18 84.40 60.00 70.00 30.00 56.67 80.00 24.00 20.00 14.00 5.45 4.59 5.00 10.00 25.00 1.67 10.00 6.00 25.00 16.00 6.36 8.26 20.00 10.00 0.00 5.00 0.00 Table 10: Statistics for Gemini-1.5-flash. Field of Study Applied Sciences & Engineering Natural Sciences Social Sciences & Humanities Islamic & Religious Studies Linguistics & Literature Philosophy & Logic Culture & Arts Mathematics & Computational Sciences General & Miscellaneous Sciences Historical & Genealogical Studies True (%) False (%) Partially-True (%) Partially-False (%) 42.00 45.00 38.00 30.00 12.84 50.00 15.00 50.00 51.67 0.00 28.00 5.00 38.00 41.82 66.97 50.00 65.00 20.00 40.00 80.00 24.00 45.00 20.00 16.36 13.76 0.00 15.00 20.00 8.33 20.00 6.00 5.00 4.00 11.82 6.42 0.00 5.00 10.00 0.00 0.00 Table 11: Statistics for Claude-3-5-sonnet."
        }
    ],
    "affiliations": [
        "Prince Sultan University, Riyadh, Saudi Arabia"
    ]
}