{
    "paper_title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs",
    "authors": [
        "Jian Chen",
        "Zhuoran Wang",
        "Jiayu Qin",
        "Ming Li",
        "Meng Wang",
        "Changyou Chen",
        "Yin Chen",
        "Qizhen Weng",
        "Yirui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 7 ] . [ 2 9 2 9 5 0 . 2 0 6 2 : r KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs Jian Chen1,3*, Zhuoran Wang2,4, Jiayu Qin1, Ming Li5, Meng Wang6 Changyou Chen1, Yin Chen2, Qizhen Weng2, Yirui Liu2 1University at Buffalo 2 Institute of Artificial Intelligence (TeleAI), China Telecom 3Dolby Laboratories 4Delft University of Technology 5University of Maryland 6ByteDance Abstract Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE (KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient datasetlevel, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) adopt the Transformer architecture [1] and generate text autoregressively under causal mask, which ensures that past key and value vectors can be cached (KV-cache) [2]. These caches are stored in high-bandwidth memory (HBM) on GPUs and repeatedly fetched into compute-unit registers during decoding, reducing computation but introducing memory-bandwidth bottleneck as context length grows. This challenge has motivated both hardware innovation [3] and algorithmic approaches to KV-cache compression [4], with our work focusing on the latter. natural way to reduce KV-cache cost is to compress key and value representations into lower-dimensional spaces. Many methods use low-rank approximation of projection matrices [5, 6], but they ignore the data-dependent nature of key/value activations, whose intrinsic rank can be smaller in domain-specific tasks that exercise only part of the models capacity [7]. Moreover, most approaches apply the same compression ratio across layers [8], overlooking distinct compressibility profiles. Methods for analyzing and comparing key/value rank across layers remain underexplored. To address these limitations, we propose KV-CoRE (KV-cache Compressibility by Rank Evaluation), an incremental singular value decomposition (SVD) method that directly operates on key and value features computed over large datasets. Unlike approaches that approximate projection weights, KV-CoRE is data-dependent and captures the intrinsic rank of the KV-cache induced by real inputs. It supports independent per-layer decomposition without cross-layer coupling, is gradient-free, and enables batch-wise computation with low memory overhead. KV-CoRE guarantees globally optimal low-rank projection under the Frobenius norm, and unlike existing methods [9, 8] that only compute the optimal compression matrix, it explicitly decomposes long sequences of key and value features to recover the *Equal contribution. Work done when Jian was at University at Buffalo and Zhuoran was at an intership in TeleAI. Corresponding author: liuyr17@chinatelecom.cn 1 singular value distributions of each layer on given dataset. This allows systematic evaluation of compressibility and provides an effective diagnostic tool for understanding representational capacity usage in LLMs. We conduct extensive experiments across open-source LLMs of different sizes and architectures, spanning datasets in instruction following, code generation, medical QA, and multilingual tasks. We introduce normalized effective rank (NER) as lightweight metric for per-layer compressibility and systematically compare key and value ranks across models and domains. Beyond static evaluation, end-to-end SVD-based truncation shows that NER correlates strongly with perplexity, validating it as reliable proxy for compression sensitivity. These results reveal consistent layer-wise and data-dependent patterns in KV-cache compressibility, laying the groundwork for dynamic and adaptive strategies. Our contributions are as follows: We propose KV-CoRE, an SVD-based, data-dependent framework for analyzing KV-cache compressibility, which enables efficient dataset-level, layer-wise evaluation with provably optimal low-rank approximations. We introduce NER as metric of compressibility and demonstrate its strong correlation with perplexity and GPT-score, validating it as reliable tool for evaluating and comparing models. We conduct extensive experiments across models, domains, and languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage."
        },
        {
            "title": "2 KV-CoRE Method",
            "content": "To evaluate and compress KV caches, we develop an efficient method that incrementally computes the SVD of keys and values over large datasets, enabling layer-wise and data-dependent evaluation of their compressibility in LLMs. At the same time, our method computes the optimal compression matrices, addressing challenges identified in prior work [9, 8, 10]. To illustrate our method and its advantages, we first introduce notations, building upon the preliminaries provided in Appendix A.1. As our method applies uniformly across layers and to both key and value spaces, we omit layer-specific notation in the rest of the section and focus on as an example in the following discussion for simplicity. Consider dataset containing tokens, for particular LLM, let = [x1; ...; xl] Rlde be the sequence of activations for an attention layer, computed based on the dataset. Let be the key projection weights of layer. The corresponding key = [k1; ...; kl] features are computed as = [k1; ...; kl] = XW K. The data-dependent optimal low-rank approximation problem, introduced in [9, 8, 10], is formulated as follows. For each key projection matrix K, we seek low-rank matrix (cid:102)W with rank that minimizes the compression error, arg min (cid:102)W XW (cid:102)W s.t. rank((cid:102)W K) = (1) The solution (cid:102)W can be expressed as pair of downand up-projection matrices, which compress key vectors into dimension and then reconstruct them for efficient autoregressive inference."
        },
        {
            "title": "2.1 SVD-based KV-cache Analysis",
            "content": "Our key idea is to perform SVD on key and value spaces for each layer, allowing an analytical study of the layer-wise compressibility of KV caches given LLM and particular dataset. For example, the SVD on Rlmhdh can be written as = UΣV , where Rll and Rmhdhmhdh are left and right singular values, respectively, and Σ Rlmhdh denotes singular values. By the Eckart-Young-Mirsky theorem [11], the truncation of the largest singular values along with corresponding singular vectors Kk = UkΣkV forms the best k-rank approximation to in the Frobenius norm sense, with the minimal approximation error calculated as follows, Kk = (cid:40) σk+1, ((cid:80)r j=k+1 σ2 ) 1 2 , for the 2-norm for the Frobenious norm (2) 2 Figure 1: Overview of KV-CoRE. At each Transformer layer, keys and values from all attention heads are concatenated to form the KV-cache. To measure compressibility, we apply incremental SVD to the cached key/value activations with low memory overhead, recovering their singular value spectra. The resulting singular vectors and values are used to compute the optimal data-dependent compression matrix, enabling KV-cache compressibility analysis via NER. where denotes the rank of K, σj denotes the j-th largest singular value in Σ. In other words, for any [1, r], the minimal krank approximation loss to is deterministic function of singular values. Thus, we use singular valuebased metrics to evaluate the compressibility of key and value spaces in each attention layer, as detailed in the experimental section 2.4."
        },
        {
            "title": "2.2 Optimal Data-dependent Compression Matrix",
            "content": "Given singular values Σ and right singular vectors of K, we can directly recover the dataset-dependent optimal k-rank approximation of as: (cid:102)W = KVkV . To see why (cid:102)W is optimal to minimize the error: XW (cid:102)W K, consider the following: (cid:102)W = XW KVkV T1= KVkV T2= UΣ(V Vk)V (cid:21)(cid:19) (cid:18) T3= Σ (cid:20)Ik 0 = (cid:21)(cid:19) (cid:18) (cid:20)Σk 0 = UkΣkV k (3) Where Ik is by identity matrix. T1 holds because by definition = XW K; T2 holds because UΣV is the Vk yields by identify matrix. forms the best k-rank approximation of = XW K, SVD of K; T3 holds because singular vectors form an orthogonal basis, thus Recall that by the Eckart-Young-Mirsky theorem [11], UkΣkV we can conclude that KVkV is optimal to the optimization problem. During LLM inference, direct implementation is to replace with pair of down-projection KVk and upk matrices. This allows caching the low-dimensional key vector xtW KVk instead of the full-dimensional projection xtW given t-th token, reducing both the memory and bandwidth consumption. 2.3 Incremental SVD Algorithm for Dataset-level KV-cache key challenge is that the size of Rlmhdh scales up with the number of tokens in Dataset, rendering direct SVD on impractical due to excessive memory and computational demands. We propose novel algorithm that mitigates memory and computation bottlenecks through batch computation without sacrificing accuracy. This method only requires holding and updating mhdh-by-mhdh covariance matrix, avoiding the direct SVD of and still generating the mathematically equivalent singular values Σ and right singular vectors V. Algorithm 1 shows the pseudo-code of our method. 3 Algorithm 1 SVD Computation of Dataset-level KV-Cache Input: Dataset containing tokens in total; LLM model weights Output: singular values Σ and right singular vectors of mhdh mhdh zero matrix for = 1, . . . , do kt xtW + kT kt end for V, Σ2, eigen-decomposition(C) return Σ, Initialize covariance matrix to zero Update covariance matrix in each step perform eigen-decomposition on the final covariance matrix For each token in Dataset containing tokens, kT iterations, we will have the complete covariance matrix of as = = (cid:80)l t=1 kT eigen-decomposition on to obtain the singular values Σ and right singular vectors of K. kt is computed and the covariance matrix is updated. After kt. Finally we perform To see why the final step of Algorithm 1 generates mathematically equivalent Σ and V, consider the following proof, T1= (UΣV )T UΣV = VΣT UΣV T2= VΣ2V (4) where T1 holds by applying SVD on K; T2 holds because singular vectors form an orthogonal basis, thus U yields identity matrix, and consequently ΣT UΣ = Σ2."
        },
        {
            "title": "2.4 Normalize Effective Rank as Compressibility Metric",
            "content": "Intuitively, the approximation loss derived in Eq.(2) provides direct measure of the compressibility of key and value spaces. For instance, one could fix an approximation rank across all key spaces, evaluate the corresponding losses, and interpret smaller losses as indicative of higher compressibility. While being simple and straightforward, such strategy has two main drawbacks: First, the metric depends on fixed approximation rank k, and there is no clear guidance on how to select appropriately. Second, it fails to account for the full spectrum of singular values. We thus introduce the Normalized Effective Rank (NER) as metric to measure the compressibility of key and value spaces. For matrix with singular values {σi} and rank r, the effective rankfirst introduced in [12]is defined as erank(K) = exp( (cid:80)r j=1 σj, and the logarithm is to the base (natural logarithm). Building on this, NER is defined as NER(K) = erank(K)/r. In other words, NER normalizes the effective rank by the matrixs actual rank. As proven in [12], the effective rank erank(K) satisfies 1 erank(K) r, consequently, NER yields score in [1/r, 1]. i=1 pi log pi), where pi = σi/ (cid:80)r"
        },
        {
            "title": "3 Related Work",
            "content": "Rank Analysis in Language Models: Early work has investigated the relationship between the rank of transformer weights or representations and model performance, seeking either to leverage low-rank structure for efficiency [13, 14, 15, 16], to prevent rank collapse that limits expressivity [17, 18, 19], or to maximize rank utilization for enhanced modeling capacity [20, 21]. With the growing use of large language models (LLMs), research has turned to their inherent low-rank properties.LoRA [22, 23] leverages this structure during fine-tuning, showing that many weight updates lie in low-dimensional subspaces. Loki [24] examined the key representations in attention layers and found that they often reside in lower-dimensional subspaces across models and datasets, which can be used for efficient sparse attention. These directions have also motivated growing efforts on KV-cache compression [4] to address the deployment bottleneck in reading and storing the KV cache [25]. Our work introduces fine-grained method for evaluating the compressibility of KV caches in LLMs through effective rank analysis, uncovering layer-wise and data-dependent patterns that can inform the design of dynamic and adaptive compression strategies. 4 Low-Rank KV-cache Compression: DeepSeek [26] introduced Multi-head Latent Attention (MLA), which applies low-rank joint KV cache compression to enable scalable inference, unlike Multi-Query Attention (MQA) [27] and Grouped-Query Attention (GQA) [28] which reduce KV caches by merging key and value heads in multi-head attention (MHA) [1]. Beyond attention mechanisms, low-rank compression has also been explored in broader contexts. For instance, AI Flow [29] proposes Hierarchical Principal Component Decomposition framework to achieve model-wide low-rank compression. More recently, such techniques have been extended to tool learningwhere compressed representations facilitate efficient tool retrieval and invocation [30]and historical learning, where past experiences are distilled into compact latent memories for long-context reasoning [31]. Further aligning with this trend, methods like MHA2MLA [5] and PALU [6] applies SVD to compress key and value projection weights, converting models based on MHA into the MLA structure for reduced KV cache size. However, this approach targets only the projection weights, while prior work [7] has shown that transformer weights typically have higher rank than the output features (keys/values), suggesting that data-dependent KV-cache compression is more effective. In this direction, DRONE [9] proposed closed-form solution for data-aware low-rank compression of projected keys/values, and SVD-LLM [8] introduced an incremental optimization based on Cholesky decomposition [32] that achieves the same optimal compression loss with lower memory overhead. In comparison, our method achieves the same optimality with much simpler formulation; moreover, it explicitly computes the SVD of key/value representations, whereas SVD-LLM only recovers the optimal compression matrix."
        },
        {
            "title": "4 Experiment",
            "content": "We evaluate our method on 5 open-source LLM series of varying sizes on 5 datasets. Models To evaluate the generality of our method across different architectures and model sizes, we apply our analysis to set of open-source LLMs including Qwen3 (4B, 8B) [33], Mistral-7B [34], Gemma-1.1 (2B, 7B) [35], and Phi-3mini-128k-instruct [36], where Gemma-1.11 is recent update of the original instruction-tuned Gemma, incorporating new RLHF method that improves overall performance. Datasets To study the data-dependence of KV-cache compression, we evaluate our method on datasets, spanning diverse English instruction-following tasks across multiple domains and multilingual QA. For English evaluation, the datasets cover general instruction following, code generation, medical QA, and function calling, including Alpaca [37], MedAlpaca [38], CodeAlpaca [39], WizardCoder [40], and FunctionCall2. For multilingual evaluation, we use the queries from the multilingual split of VisR-Bench [41], question-driven, retrieval benchmark spanning 15 languages (Spanish, Italian, German, French, Dutch, Arabic, Croatian, Japanese, Swedish, Vietnamese, Portuguese, Finnish, Czech, Slovenian, and Danish)allowing us to assess performance across linguistically diverse setting. Hardware and Software Setup All experiments are conducted on machines equipped with 8 NVIDIA A800 GPUs (80GB each), though all evaluations are executed on single GPU without distributed computation. We use PyTorch 2.7.1 and Hugging Face Transformers 4.53.2 for model loading, compression, and inference. All evaluations are conducted in inference mode without gradient computation."
        },
        {
            "title": "4.1 Evaluation Metric",
            "content": "We introduce several metrics to evaluate both the cross-dataset compressibility of various LLMs and the end-to-end performance of their compressed versions. Normalized Effective Rank We quantify the data-dependent, per-layer compressibility of the KV-cache using the NER [12], as introduced in Section 2.4. NER captures how evenly the singular values are distributed and yields score in [0, 1], with lower values indicating spectra dominated by few large singular values and thus higher compressibility. 1Gemma-1.1: https://huggingface.co/google/gemma-1.1-7b-it 2glaiveai/glaive-function-calling-v2: https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2 5 Perplexity We evaluate the performance of compressed models using perplexity (PPL) [42], the standard metric for language modeling. In practice, PPL is computed as the exponential of the empirical cross-entropy between the data distribution and the model distribution, which reduces to the Shannon entropy when the model perfectly matches the true distribution. Lower PPL indicates that the model assigns higher probability to the observed data, while higher PPL reflects greater uncertainty or degraded predictive performance. Given retain ratio and v, the perplexity will be denoted as PPL(k, v). Normalized Delta-Perplexity To directly measure the impact of KV-cache compression on model performance, we propose quantitative metric, Normalized Delta-Perplexity (ND-PPL) for keys and values, denoted ND-PPLK and ND-PPLV . Raw perplexity or absolute changes are not directly comparable across datasets, since their scale depends on the baseline. ND-PPL addresses this by normalizing pairwise perplexity differences across retained rank ratios by the corresponding baseline perplexity, and averaging over all candidate settings. This provides dataset-agnostic measure of robustness under compression and establishes direct link to the NER. The formal definition is as follows: Let = k1, . . . , km denote the set of retained rank ratios for the key matrices, where ki (0, 1]; the definition for values = v1, . . . , vn is analogous. For fixed value ratio V, we consider all pairs (ki, kj) PK, where PK = (k, k) > k, and define the key-side metric ND-PPLK: ND-PPLK = 1 PK 1 (cid:88) vV (cid:88) (ki,kj )PK (cid:18) PPL(kj, v) PPL(ki, v) PPL(ki, v) (cid:19) (5) The definition of ND-PPLV is symmetric, obtained by fixing and averaging over pairs of vi, vj V. GPT Score We assess whether compression preserves response quality using GPT-4o. For each test case, GPT-4o is provided with the input instruction together with the two responses from the original and compressed models. Under fixed prompt, GPT-5 outputs binary score: 1 if the two responses are judged roughly equal in quality, and 0 otherwise. Rough equivalence requires both answers to be reasonable and aligned with the instruction, tolerating stylistic differences but penalizing empty, irrelevant, or nonsensical outputs from the compressed model. The full system prompt used for evaluation is provided in Appendix C."
        },
        {
            "title": "4.2 Benchmarking Experiment on KV-Cache Compressibility",
            "content": "4.2.1 Average NER across Models and Datasets Table 1 reports the mean NER of keys (NER-K) and values (NER-V), averaged over all attention layers across seven models on five English datasets and fifteen languages from VisR-Bench. Several trends emerge: Keys are consistently more compressible than values. Across all models and datasets, NER-K is substantially lower than NER-V, indicating that the key cache admits more pronounced low-rank structure. This asymmetry highlights that compression techniques targeting keys may achieve higher savings with less impact on performance, whereas values tend to retain higher-rank structure and thus are less compressible. Cross-lingual variation outweighs cross-domain variation. For English datasets spanning different domains (e.g., Alpaca vs. FunctionCall), NER values remain relatively stable, suggesting that domain shifts do not strongly affect rank structure. In contrast, multilingual results reveal far greater variation, with languages such as Czech and German showing higher NER compared to Arabic and Finnish. This suggests that linguistic diversity, tokenization differences, and training data availability play larger role than domain differences in determining compressibility. KV Capacity governs compressibility. We observe that the original KV dimension strongly affects model compressibility. Earlier models such as LLaMA-2-7B exhibit substantially lower NER compared to other models, likely due to larger key/value dimensions and weaker utilization of rank capacity during training. Within the Gemma family, Gemma-7B shows much lower NER than Gemma-2B, which can be explained by its 16 larger KV dimension (16 heads 256 per head) compared to the 2B model (1 head 256). The higher compressibility of Gemma-7B suggests that this expanded KV capacity is under-utilized, whereas the smaller 2B models may make more efficient use of their Datasets Qwen3-4B Qwen3-8B Gemma-2B Gemma-7B Mistral-7B Phi-3 LLaMA-2-7B V Multi-domain Datasets Alpaca MedAlpaca CodeAlpaca WizardCoder FunctionCall Average 0.428 0.429 0.421 0.425 0.432 0.424 0.724 0.723 0.708 0.726 0.731 0.717 0.452 0.452 0.443 0.447 0.451 0.446 Multilingual Question in VisR-Bench Datasets Czech German Italian Dutch Croatian French Vietnamese Swedish Spanish Slovenian Portuguese Japanese Finnish Arabic Average 0.383 0.383 0.377 0.376 0.375 0.373 0.373 0.371 0.367 0.366 0.362 0.361 0.360 0.337 0.387 0.627 0.640 0.632 0.628 0.620 0.633 0.625 0.620 0.629 0.603 0.614 0.619 0.597 0.582 0.653 0.401 0.400 0.392 0.395 0.393 0.390 0.392 0.387 0.384 0.383 0.378 0.380 0.378 0.354 0.407 0.753 0.752 0.737 0.753 0.756 0.745 0.652 0.666 0.655 0.657 0.645 0.657 0.653 0.645 0.654 0.628 0.639 0.648 0.625 0.609 0. 0.612 0.594 0.589 0.597 0.608 0.597 0.536 0.536 0.529 0.534 0.525 0.532 0.542 0.526 0.523 0.518 0.521 0.506 0.502 0.503 0.548 0.900 0.889 0.869 0.889 0.900 0.884 0.803 0.802 0.793 0.802 0.791 0.797 0.814 0.794 0.790 0.785 0.785 0.775 0.769 0.769 0.822 0.359 0.344 0.321 0.329 0.342 0.337 0.292 0.305 0.299 0.299 0.292 0.301 0.291 0.296 0.299 0.281 0.284 0.276 0.280 0.270 0. 0.469 0.455 0.429 0.445 0.458 0.448 0.383 0.400 0.393 0.394 0.382 0.397 0.384 0.389 0.396 0.369 0.375 0.369 0.369 0.361 0.405 0.449 0.441 0.420 0.420 0.432 0.430 0.385 0.392 0.387 0.385 0.383 0.387 0.367 0.381 0.381 0.372 0.379 0.364 0.353 0.338 0.395 0.773 0.771 0.733 0.750 0.762 0.753 0.660 0.676 0.669 0.664 0.659 0.671 0.630 0.656 0.665 0.642 0.656 0.636 0.616 0.594 0. 0.409 0.403 0.380 0.385 0.397 0.393 0.313 0.345 0.339 0.331 0.320 0.340 0.305 0.322 0.334 0.306 0.326 0.321 0.305 0.288 0.345 0.616 0.604 0.571 0.587 0.604 0.593 0.470 0.523 0.515 0.499 0.479 0.519 0.442 0.486 0.513 0.458 0.498 0.467 0.455 0.413 0.520 0.023 0.176 0.028 0.265 0.135 0.088 0.099 0.092 0.084 0.116 0.081 0.087 0.071 0.078 0.064 0.075 0.083 0.079 0.073 0.052 0. 0.464 0.310 0.337 0.304 0.449 0.141 0.148 0.138 0.135 0.108 0.132 0.137 0.119 0.128 0.095 0.121 0.133 0.125 0.118 0.173 0.134 Table 1: Average NER of keys and values across all layers of 7 models on multilingual split of VisR-bench covering 15 languages available dimensions. In contrast, Qwen3-4B and Qwen3-8B show only minor differences in NER because both adopt the same compact KV configuration (8 heads 128 per head). Rank collapse in low-resource languages. Certain languages with limited pretraining coverage (e.g., Arabic, Slovenian, Finnish) exhibit unusually low NER, especially in the value cache. This phenomenon may reflect undertrained token embeddings collapsing into low-dimensional subspaces. Beyond compressibility, such rank collapse may serve as diagnostic signal for identifying under-represented languages in multilingual pretraining corpora. 4.2.2 Layer-wise Compressibility Patterns Figure 2 presents the layer-wise NER of Qwen3-4B on the five datasets and on three selected language subsets of VisR-Bench, with additional plots for other models included in Appendix B.1. The results show that NER is not uniform across layers. Middle layers often exhibit higher NER, suggesting they make fuller use of their representational capacity, while early and late layers are typically more compressible. This heterogeneity indicates that future KV-cache compression should be layer-aware: applying uniform compression ratio risks overly degrading high-rank layers while missing opportunities for more aggressive reduction in lower-rank ones. Moreover, the relative positions of highand low-rank layers are consistent across datasets, suggesting that compressibility is partly structural property of the model rather than purely data-driven. At the same time, subtle differences between tasks and languages (e.g., Arabic vs. English subsets) highlight that dataset characteristics can modulate layer usage, pointing to potential for data-aware compression strategies. 7 Figure 2: Layer-wise NER of key and value representations in Qwen3-4B, evaluated on 5 datasets and 3 languages from the VisR-Bench benchmark."
        },
        {
            "title": "4.3 Performance Impact of KV-Cache Compression",
            "content": "We evaluate the performance degradation of compressed models using both perplexity (PPL) and GPT-score. Figure 3 presents PPL heatmaps of Qwen3-4B and LLaMA-2-7B on the Alpaca dataset across grid of KV-cache compression ratios, with additional results provided in Appendix B.2. We can see that LLaMA-2-7B remains relatively stable, showing only modest PPL increases even under aggressive compression, whereas Qwen3-4B is more sensitive, exhibiting substantial degradation. These results suggest that models with lower NER values are generally more compressible, as reflected by smaller changes in PPL. Figure 3: PPL heatmap of Qwen3-4B and LLaMA-2-7b on the Alpaca dataset. In addition to PPL, we also report average GPT-score in Figure 4, which directly measures the quality difference between compressed and original model responses and provides metric more closely aligned with user experience. Due to computational cost, GPT-scores are computed as averages over 100 instructions from the Alpaca dataset. Consistent with PPL trends, LLaMA-2-7B again proves more compressible than Qwen3-4B. Importantly, GPT-score further reveals smoother and more continuous trajectory of performance degradation, even in regions where PPL remains relatively 8 unchanged. Figure 4: GPT score heatmap of Qwen3-4B and LLaMA-2-7b on the Alpaca dataset."
        },
        {
            "title": "4.4 Dataset Compressibility Comparison",
            "content": "To quantitatively assess how well the NER reflects end-to-end robustness under compression, we employ the ND-PPL metrics. As reported in Figure 5, NER and ND-PPL are positively correlated, with Pearson = 0.88 for values and = 0.64 for keys. These results establish NER as reliable predictor of performance sensitivity under compression. Moreover, the scatter plots reveal systematic dataset-level patterns. Multilingual datasets (e.g., Arabic, Portuguese, Finnish) cluster toward the bottom-left, with both low NER and low ND-PPL, indicating higher resilience to compression. In contrast, English-domain datasets such as Alpaca, MedAlpaca, and FunctionCall appear toward the upper-right, showing higher NER and greater sensitivity to compression. This separation suggests that KV-cache compressibility can serve as diagnostic of datamodel alignment: under-trained or poorly covered datasets tend to yield low NER, while well-represented domains exhibit higher NER and are more sensitive to aggressive truncation. Figure 5: Correlation between dataset-level NER and ND-PPL computed by Qwen3-4B."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced KV-CoRE, an SVD-based framework for dataset-level analysis of KV-cache compressibility in large language models. KV-CoRE directly decomposes cached key/value activations with low memory overhead, yielding globally optimal low-rank approximations and enabling systematic evaluation of rank utilization across layers and datasets. Through extensive experiments across multiple model families, domains, and languages, we showed that NER serves as lightweight and reliable indicator of compressibility, correlating closely with perplexityand GPT-based performance under compression. We further introduced ND-PPL as an end-to-end robustness measure, establishing clear empirical link between NER and model sensitivity to truncation. Our analysis uncovers consistent patterns that tie compressibility to architectural design, training data, and language coverage. These findings position KV-CoRE as both diagnostic tool for understanding representational efficiency and benchmark for guiding the development of dynamic, data-aware KV-cache compression strategies and data-centric model improvements."
        },
        {
            "title": "References",
            "content": "[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [2] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038, 2019. [3] Myunghyun Rhee, Joonseop Sim, Taeyoung Ahn, Seungyong Lee, Daegun Yoon, Euiseok Kim, Kyoung Park, Youngpyo Joo, and Hosik Kim. Hpu: High-bandwidth processing unit for scalable, cost-effective llm inference via gpu co-processing. arXiv preprint arXiv:2504.16112, 2025. [4] Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, and Hai Zhao. Keep the cost down: review on methods to optimize llms kv-cache consumption. arXiv preprint arXiv:2407.18003, 2024. [5] Tao Ji, Bin Guo, Yuanbin Wu, Qipeng Guo, Lixing Shen, Zhan Chen, Xipeng Qiu, Qi Zhang, and Tao Gui. Towards economical inference: Enabling deepseeks multi-head latent attention in any transformer-based llms. arXiv preprint arXiv:2502.14837, 2025. [6] Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze, Mohamed Abdelfattah, and Kai-Chiang Wu. Palu: Compressing kv-cache with low-rank projection. arXiv preprint arXiv:2407.21118, 2024. [7] Hao Yu and Jianxin Wu. Compressing transformers: features are low-rank, but weights are not! In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1100711015, 2023. [8] Xin Wang, Yu Zheng, Zhongwei Wan, and Mi Zhang. Svd-llm: Truncation-aware singular value decomposition for large language model compression. arXiv preprint arXiv:2403.07378, 2024. [9] Patrick Chen, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh. Drone: Data-aware low-rank compression for large nlp models. Advances in neural information processing systems, 34:2932129334, 2021. [10] Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. Asvd: Activation-aware singular value decomposition for compressing large language models, 2024. [11] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211218, 1936. [12] Olivier Roy and Martin Vetterli. The effective rank: measure of effective dimensionality. In 2007 15th European signal processing conference, pages 606610. IEEE, 2007. [13] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems, 34:1741317426, 2021. 10 [14] Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin. Language model compression with weighted low-rank factorization. arXiv preprint arXiv:2207.00112, 2022. [15] Habib Hajimolahoseini, Walid Ahmed, Mehdi Rezagholizadeh, Vahid Partovinia, and Yang Liu. Strategies for applying low rank decomposition to transformer-based models. In 36th Conference on Neural Information Processing Systems (NeurIPS2022), volume 6, 2022. [16] Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, and Tuo Zhao. Losparse: Structured compression of large language models based on low-rank and sparse approximation. In International Conference on Machine Learning, pages 2033620350. PMLR, 2023. [17] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International conference on machine learning, pages 27932803. PMLR, 2021. [18] Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:2719827211, 2022. [19] Can Yaras, Peng Wang, Laura Balzano, and Qing Qu. Compressible dynamics in deep overparameterized low-rank learning & adaptation. arXiv preprint arXiv:2406.04112, 2024. [20] Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. In International conference on machine learning, pages 864873. PMLR, 2020. [21] Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind. Transformers learn through gradual rank increase. Advances in Neural Information Processing Systems, 36:2451924551, 2023. [22] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [23] Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth, 1(9), 2024. [24] Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, and Abhinav Bhatele. Loki: Low-rank keys for efficient sparse attention. Advances in Neural Information Processing Systems, 37:1669216723, 2024. [25] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: distributed serving system for {Transformer-Based} generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521538, 2022. [26] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [27] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. [28] Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. [29] Hongjun An, Sida Huang, Siqi Huang, Ruanjun Li, Yuanzhi Liang, Jiawei Shao, Zihan Wang, Cheng Yuan, Chi Zhang, Hongyuan Zhang, Wenhao Zhuang, and Xuelong Li. AI Flow: Perspectives, scenarios, and approaches. CoRR, abs/2506.12479, 2025. [30] Jinyang Chen, Haolun Wu, Jianhong Pang, Yihua Wang, Dell Zhang, and Changzhi Sun. Tool learning with language models: comprehensive survey of methods, pipelines, and benchmarks. Vicinagearth, 2(16), 2025. [31] Xiang Li, Ge Wu, Lingfeng Yang, Wenhai Wang, Renjie Song, Ming-Ming Cheng, and Jian Yang. survey of historical learning: learning models with learning history. Vicinagearth, 2(5), 2025. [32] Carl Meyer. Matrix analysis and applied linear algebra. SIAM, 2023. [33] Qwen Team. Qwen3 technical report, 2025. [34] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. [35] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [36] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [37] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/ tatsu-lab/stanford_alpaca, 2023. [38] Tianyu Han, Lisa Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno Bressem. Medalpacaan open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023. [39] Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. https://github. com/sahil280114/codealpaca, 2023. [40] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct, 2025. [41] Jian Chen, Ming Li, Jihyung Kil, Chenguang Wang, Tong Yu, Ryan Rossi, Tianyi Zhou, Changyou Chen, and Ruiyi Zhang. Visr-bench: An empirical study on visual retrieval-augmented generation for multilingual long document understanding. arXiv preprint arXiv:2508.07493, 2025. [42] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. neural probabilistic language model. Journal of machine learning research, 3(Feb):11371155, 2003."
        },
        {
            "title": "A Preliminary",
            "content": "A.1 MHA, MQA and GQA We first introduce the standard MHA and two of its variantsMQA and GQA. Given an input embedding vector, MHA project it into key and value vectors for each attention head, causing the KV cache size to scale linearly with the number of heads. In contrast, MQA and GQA reduce the KV cache size by grouping heads and sharing the same key and value vectors within each group. Focusing on GQA, the most general technique among the three, we define de, mh, dh and mg as the embedding dimension, number of heads, dimension per head and number of groups, respectively. Given an input embedding vector xt R1de corresponding to the t-th token, GQA divides the mh attention heads into mg groups. Formally, this grouping can be described by helper function g, which maps from head indices {1, ..., mh} to group indices {1, ..., mg} as, g(i) = (cid:24) (cid:30) mh mg (cid:25) , {1, ..., mh} Then it projects xt into query qt R1mhdh, key kt R1mgdh , and value vector vt R1mgdh as follows: [qt,1, ..., qt,mh ] = qt = xtW [kt,1, ..., kt,mg ] = kt = xtW [vt,1, ..., vt,mg ] = vt = xtW (A.1) (A.2) (A.3) (A.4) where qt,i, {1, ..., mh} denote the query vector for each attention head, and kt,i, vt,i, {1, ..., mg} represent the key and value vector for each group. The matrices Rdemhdh and K, Rdemgdhdenote learnable model parameters. The attention of each head and the final projected output are computed as, ot,i = (cid:88) j=1 Softmaxj (cid:33) (cid:32) qt,ikT j,g(i) dh yt = [ot,1, ..., ot,mh ]W vj,g(i), {1, ..., mh} (A.5) (A.6) where ot,i R1dh, Rmhdhde and yt R1de denote the attention output for i-th head, the output projection matrix and the projected output respectively. Note that when mg = mh, GQA reduces to standard MHA, and when mg = 1, it specializes MQA. A.2 MLA Unlike MHA and its variants, MLA projects the input embedding xt Rde of t-th token into two distinct spaces: joint latent KV space and decoupled key space designed to incorporate RoPE. Formally, this can be expressed as: & = xtW DKV cKV kR & = RoPE(xtW KR) (A.7) (A.8) R1dc and kR R1dR denote the joint latent KV vector and the RoPE-encoded decoupled key where cKV vector, receptively. The projection matrices DKV Rdedc and KR RdedR handle the corresponding downprojections. During attention calculation, cKV is up-projected to get the key and value vectors, while the query vector is computed directly from the input embedding xt. These operations are described by following equations equation A.9 and equation A.10: 13 [kC t,1, ..., kC t,n] = kC [vC t,1, ..., vC t,mh ] = vC = cKV kt,i = [kC = cKV K t,i, kR ] V (A.9) [qC [qR t,1, ..., qC t,1, ..., qR t,mh t,mh cQ = xtW DQ = cQ ] = qC Q = RoPE(cQ QR) ] = qR t,i, qR t,i] qt,i = [qC (A.10) where K, Rdcmhdh are up-projection matrices for key and value vectors, respectively. The matrices DQ Rded cmhdh is the up-projection matrix used to incorporate RoPE for the decoupled query vector. Note that both kt,i and qt,i are concatenations of their NoPE and RoPE components. cmhdh serve as the downand up-projection for queries, while QR Rd and Rd Using qt,i, kt,i and vC t,i, the attention of each head and the final projected output are computed as, ot,i = (cid:88) j="
        },
        {
            "title": "Softmaxj",
            "content": "(cid:33) (cid:32) qt,ikT j,i dh + dR yt = [ot,1, ..., ot,mh ]W vC j,i, {1, ..., mh} (A.11) (A.12) key merit of MLA lies in its caching efficiency during token generation: only cKV need to be cached, resulting in cache size of dc + dR. Since both dc << mhdh and dR << mhdh, MLA reduces KV cache size significantly compared to MHA, which requires caching kt R1mhdh and vt R1mhdh for t-th token. and kR t"
        },
        {
            "title": "B Additional Experiment Results",
            "content": "B.1 Layer-wise NER Results Figure B.1: Layer-wise NER of key and value representations in Qwen3-4B evaluated on 5 datasets and 3 languages from the VisR-Bench benchmark. 14 Figure B.2: Layer-wise NER of key and value representations in Qwen3-8B, Phi-3-mini, and mistral evaluated on 5 datasets and 3 languages from the VisR-Bench benchmark. 15 Figure B.3: Layer-wise NER of key and value representations in gemma1.1 and gemma-1.1-7b-it evaluated on 5 datasets and 3 languages from the VisR-Bench benchmark. 16 B.2 PPL Heatmap Figure B.4: PPL heatmap of LLaMA-2-7B on 6 datasets. 17 Figure B.5: PPL heatmap of Qwen3-4B on 6 datasets."
        },
        {
            "title": "C GPT evaluation system prompt",
            "content": "System Prompt 1. You are an automatic evaluator for LLM responses. Your job: compare two candidate answers (A = original model, = compressed model) to the same user prompt and output binary score. - Scoring Rules Output 1 if and are roughly equal in quality (not necessarily the same wording or level of detail, but comparable usefulness for the task). Output 0 otherwise. - Roughly equal means: Both and provide reasonable, relevant answer to the users prompt. They satisfy the intent to similar degree, with no material difference in correctness, completeness, usefulness, or safety. Differences in style, verbosity, order, or minor details do not matter if they dont affect usefulness. If both are equally poor or equally failed (e.g., both empty, both nonsense, both refuse without reason, or both severely hallucinated to similar extent), score 1. If one contains nonsense, large repetition, emptiness, or serious hallucinations while the other does not, score 0. Special rule: The COMPRESSED MODELs answer must itself be reasonable response to the user prompt. If the compressed model gives an empty output, nonsense, off-topic text, or fails to address the question, score 0, even if the original answer is also poor. - Evaluation checklist (internal only): 1. Task fulfillment & correctness: Does each answer address the users ask accurately? 2. Coherence & specificity: Is each answer clear, non-contradictory, minimally redundant? 3. Grounding & hallucinations: Any invented facts or unsupported claims? 4. Completeness at the needed granularity: Are the essentials present? 5. Harm & policy: Safety/compliance roughly comparable? 6. Degenerate behaviors: empty output, nonsense, repetition, prompt copy, or off-topic. - Decision rules: Score 1 if both are reasonable answers and land in the same quality band (Excellent/Adequate/Poor/Fail), close enough that reasonable user would find them similarly useful (or similarly useless). Score 0 if any material gap exists, or if the compressed model fails to provide reasonable answer. - Formatting: Return only single JSON object with no extra text: \"score\": 0 or \"score\": 1 Example: [INPUT PROMPT] Explain why the sky is blue. [ANSWER A: ORIGINAL MODEL] \"The sky looks blue due to Rayleigh scattering of sunlight in Earths atmosphere, which scatters shorter wavelengths like blue more strongly.\" [ANSWER B: COMPRESSED MODEL] \"\" Expected output: \"score\":"
        }
    ],
    "affiliations": [
        "ByteDance",
        "Delft University of Technology",
        "Dolby Laboratories",
        "Institute of Artificial Intelligence (TeleAI), China Telecom",
        "University at Buffalo",
        "University of Maryland"
    ]
}