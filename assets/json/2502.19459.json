{
    "paper_title": "Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting",
    "authors": [
        "Yu Liu",
        "Baoxiong Jia",
        "Ruijie Lu",
        "Junfeng Ni",
        "Song-Chun Zhu",
        "Siyuan Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Building articulated objects is a key challenge in computer vision. Existing methods often fail to effectively integrate information across different object states, limiting the accuracy of part-mesh reconstruction and part dynamics modeling, particularly for complex multi-part articulated objects. We introduce ArtGS, a novel approach that leverages 3D Gaussians as a flexible and efficient representation to address these issues. Our method incorporates canonical Gaussians with coarse-to-fine initialization and updates for aligning articulated part information across different object states, and employs a skinning-inspired part dynamics modeling module to improve both part-mesh reconstruction and articulation learning. Extensive experiments on both synthetic and real-world datasets, including a new benchmark for complex multi-part objects, demonstrate that ArtGS achieves state-of-the-art performance in joint parameter estimation and part mesh reconstruction. Our approach significantly improves reconstruction quality and efficiency, especially for multi-part articulated objects. Additionally, we provide comprehensive analyses of our design choices, validating the effectiveness of each component to highlight potential areas for future improvement."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 5 4 9 1 . 2 0 5 2 : r Published as conference paper at ICLR"
        },
        {
            "title": "BUILDING INTERACTABLE REPLICAS OF COMPLEX\nARTICULATED OBJECTS VIA GAUSSIAN SPLATTING",
            "content": "Yu Liu1,2,,;, Baoxiong Jia2,, Ruijie Lu2,3, Junfeng Ni1,2, Song-Chun Zhu1,2,3, Siyuan Huang2 1Tsinghua University 2State Key Laboratory of General Artificial Intelligence, BIGAI 3Peking University"
        },
        {
            "title": "ABSTRACT",
            "content": "Building interactable replicas of articulated objects is key challenge in computer vision. Existing methods often fail to effectively integrate information across different object states, limiting the accuracy of part-mesh reconstruction and part dynamics modeling, particularly for complex multi-part articulated objects. We introduce ArtGS, novel approach that leverages 3D Gaussians as flexible and efficient representation to address these issues. Our method incorporates canonical Gaussians with coarse-to-fine initialization and updates for aligning articulated part information across different object states, and employs skinning-inspired part dynamics modeling module to improve both part-mesh reconstruction and articulation learning. Extensive experiments on both synthetic and real-world datasets, including new benchmark for complex multi-part objects, demonstrate that ArtGS achieves state-of-the-art performance in joint parameter estimation and part mesh reconstruction. Our approach significantly improves reconstruction quality and efficiency, especially for multi-part articulated objects. Additionally, we provide comprehensive analyses of our design choices, validating the effectiveness of each component to highlight potential areas for future improvement. Our work is made publicly available at: https://articulate-gs.github.io."
        },
        {
            "title": "INTRODUCTION",
            "content": "Articulated objects, central to everyday human-environment interactions, have become key focus in computer vision research (Yang et al., 2023a; Weng et al., 2024; Luo et al., 2025; Liu et al., 2024; Deng et al., 2024). Accurately reconstructing real-world scenes (Chen et al., 2024b; Ni et al., 2024; Lu et al., 2024b) and creating interactable digital replicas of these objects are essential for various applications, including scene understanding (Jia et al., 2024; Huang et al., 2024b; Zhu et al., 2024; Linghu et al., 2024) and robotics learning (Liu et al., 2022a; Geng et al., 2023b;a; Gong et al., 2023; Yang et al., 2024b; Zhao et al., 2024; Lu et al., 2024a). By building high-fidelity digital twins of articulated objects, we bridge the gap between synthetic and real-world scenarios, thus facilitating the sim-to-real transfer of robotic systems (Torne et al., 2024; Kerr et al., 2024). As we advance towards more sophisticated robotic systems and immersive virtual environments, there is growing need for improved and efficient modeling techniques for the reconstruction of articulated objects. The problem of reconstructing articulated objects has been extensively studied (Liu et al., 2023a;b; Weng et al., 2024; Deng et al., 2024; Yang et al., 2023a), with key challenge being the learning of object geometry when only partial views of the object are available at any given state. To accurately reconstruct object parts (e.g., closed drawer), it is essential to integrate observations from multiple object states during interactions (e.g., the opening process of the drawer). This necessitates the simultaneous learning and alignment of fine-grained object parts across different states, which must be achieved jointly during the reconstruction of object geometries. Such requirement presents significant challenges in object modeling, especially for complex everyday articulated objects that often consist of multiple interactable parts. Additionally, uncertainties in object geometry reconstruction introduce further challenges in modeling articulation, as errors in geometry modeling can result in inaccurate learning of articulation parameters. These challenges highlight the need for improved models that handle the complexities of multi-part articulated objects. Equal contribution. ;Work done as an intern at BIGAI. 1 Published as conference paper at ICLR 2025 Recent approaches attempt to address these challenges using part priors from pre-trained models. These models provide either part segmentation masks via models like SAM (Kirillov et al., 2023), or 2D pixel correspondences for aligning pixels across states (Sun et al., 2021). However, these methods rely heavily on priors from pre-trained models, often using single-state inputs and neglecting critical motion information (Mandi et al., 2024) and struggling with the complexity of multi-part objects when accurately matching pixels across states becomes difficult (Weng et al., 2024). These limitations result in unstable and inconsistent learning of object parts, posing significant challenges to the joint learning of part motion and geometry. To address these challenges, we propose ArtGS, which introduces several key innovations for handling complex multi-part articulated objects. Specifically, we adopt the commonly used two-state setting for learning articulated objects, as established in prior works (Liu et al., 2023a; Weng et al., 2024). Central to our approach is the use of 3D Gaussians (Kerbl et al., 2023) as the foundational representation, chosen for their ability to explicitly maintain spatial information while offering efficiency and high reconstruction quality. To effectively model object dynamics and integrate information across multiple object states, we employ canonical Gaussians with carefully designed coarse-to-fine initialization and update scheme. These Gaussians act as bridge between different input object states, enabling accurate deformation modeling that improves both mesh reconstruction and articulation learning. Building on the canonical Gaussians, we draw inspiration from Gaussian skinning (Song et al., 2024) and introduce center-based clustering module for part and dynamics learning. This approach leverages motion priors of Gaussians, which are summarized during the learning process, serving as guide to better align object parts between states and improve articulation learning. These designs allow our method to achieve state-of-the-art performance in joint parameter estimation and part mesh reconstruction, excelling on both existing benchmarks and our newly curated complex multi-part articulated object reconstruction benchmark. Our approach outperforms existing methods in both synthetic and real-world scenarios, with significant improvements in axis modeling and overall efficiency. Through extensive experiments, we demonstrate the effectiveness of our model in efficiently delivering high-quality reconstruction of complex multi-part articulated objects. We also provide comprehensive analyses of our design choices, highlighting the critical role of these modules and identifying areas for future improvement. Contributions Our main contributions of this work can be summarized as follows: We propose ArtGS, novel and efficient method for articulated object reconstruction that achieves state-of-the-art performance, particularly for complex multi-part objects. We introduce coarse-to-fine canonical Gaussian initialization and skinning-inspired part dynamics modeling with self-guided motion priors to improve object part and articulation learning, effectively addressing the limitations of existing methods in using object motion information. We conduct extensive experiments on both synthetic and real-world articulated objects, demonstrating the effectiveness, efficiency, scalability, and robustness of our approach. We also provide comprehensive ablation studies to validate our designs and highlight areas for future improvement."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Dynamic Gaussian Modeling Recent advancements have shown the potential of Gaussian Splatting (Kerbl et al., 2023) for 4D reconstruction (Jung et al., 2023; Katsumata et al., 2023; Wu et al., 2024; Luiten et al., 2024; Li et al., 2024; Lu et al., 2024c; Lei et al., 2024; Guo et al., 2024; Qian et al., 2024; Bae et al., 2024; Wan et al., 2024b). central focus of these efforts is the deformation modeling of 3D Gaussians. While effective for dynamics capturing, most approaches learn transformations implicitly, limiting their capability for controllable dynamics modeling. To address this issue, recent studies use superpoints (Huang et al., 2024c; Wan et al., 2024a) for improved dynamics modeling and control. However, as superpoint learning is based primarily on rendering without considering object physics, these methods fail to reliably capture accurate physical parameters (e.g., joints and axes). Another line of works (Xie et al., 2024; Jiang et al., 2024) introduce controllable Gaussians by integrating physics-based modeling for graphics simulations. These models require intricate priors of objects (e.g., material properties), making them impractical for reconstructing everyday articulated objects. To overcome these challenges, our work combines the explicit 3D Gaussian modeling with articulation modeling, enabling efficient and high-quality reconstruction with precise articulation parameter estimation for more practical digital-twin construction of articulated objects. 2 Published as conference paper at ICLR 2025 Articulation Parameter Estimation Estimating joint articulation parameters for articulated objects has been extensively studied, with approaches broadly categorized into two main categories. First, prediction-based methods estimate joint parameters from sensory inputs of different object configurations (Huang et al., 2014; Katz et al., 2013) or use end-to-end models (Hu et al., 2017; Yi et al., 2018; Li et al., 2020; Wang et al., 2019; Sun et al., 2023; Liu et al., 2022b; Weng et al., 2021; Sturm et al., 2011; Chu et al., 2023; Martín-Martín et al., 2016; Liu et al., 2023c; Gadre et al., 2021; Mo et al., 2021; Jain et al., 2021; Yan et al., 2020; Lei et al., 2023) to predict part segmentation, kinematic structure, as well as joint parameters. Second, reconstruction-based methods optimize articulation parameters by reconstructing multi-view images or videos (Wei et al., 2022; Tseng et al., 2022; Mu et al., 2021; Lewis et al., 2022; Liu et al., 2023a; Lei et al., 2024; Deng et al., 2024; Swaminathan et al., 2024; Noguchi et al., 2022; Zhang et al., 2021; Pillai et al., 2015; Liu et al., 2023b). Most of these methods treat articulation parameter estimation as separate task, without generating high-quality, interactable part-mesh reconstructions. ArtGS aims to address this gap by integrating part-mesh reconstruction and articulation parameter estimation, enabling the creation of high-quality, interactable replicas. Articulated Object Reconstruction Articulated object reconstruction, differing from human and animal motion modeling (Joo et al., 2018; Loper et al., 2023; Mihajlovic et al., 2021; Noguchi et al., 2021; Yang et al., 2021b;a; Romero et al., 2022; Zuffi et al., 2017; Yang et al., 2024a; Xu et al., 2020; Tan et al., 2023; Yang et al., 2022; 2023b; Song et al., 2023a; Yang et al., 2023a; Song et al., 2023b), focus on the piece-wise rigidity of each part, requiring both part-level geometry reconstruction and joint articulation parameter estimation. While end-to-end models predict joint parameters and segment object parts from single-stage (Heppert et al., 2023; Wei et al., 2022; Kawana et al., 2021) or interaction observations(Jiang et al., 2022; Ma et al., 2023; Nie et al., 2022; Hsu et al., 2023), they struggle to generalize to unseen objects. Per-object optimization approaches (Liu et al., 2023a;b; Weng et al., 2024; Deng et al., 2024; Swaminathan et al., 2024), using multi-state observations for articulation modeling, offer better adaptability to unknown objects but face scaling issues of multiple joints. Methods like DTA (Weng et al., 2024) attempt to handle multi-part objects but still struggle with those having more than three movable parts. We address the reliability, flexibility, and scalability issues of previous works with our canonical Gaussian design and skinning-inspired part dynamics modeling, achieving higher accuracy, robustness, and efficiency for articulated object reconstruction."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "3D Gaussian Splatting 3D Gaussian Splatting (3DGS) represents static 3D scene using 3D Gaussians (Kerbl et al., 2023). Each Gaussian Gi is associated with center µi, covariance matrix Σi, opacity σi and spherical harmonics coefficients hi. The final opacity of 3D Gaussian at spatial point can be calculated as: ˆ αipxq σi exp px µiqT Σ1 px µiq , where Σi RiSiST RT . (1) 1 2 As the physical meaning of covariance matrix is only valid if it is positive semi-definite, we decompose the covariance matrix Σi following Eq. (1) into scaling diagonal matrix Si and rotation matrix Ri parameterized by quaternion ri. scene is then described with collection of such Gaussians tGi : µi, ri, si, σi, hiuN i1. We render an image and optionally its depth image from the 3D scene by projecting each Gaussian onto the 2D image plane and aggregating them using α-blending: Nÿ Tiα2D SHphi, viq, Nÿ Tiα2D di, where Ti i1ź p1 α2D q. (2) j1 is 2D version of Eq. (1), with µi, Σi, replaced by the projected µ2D α2D , and the pixel coordinate u. SHpq is the spherical harmonic function, vi is the view direction from the camera to µi, di is the depth of the i-th Gaussian. Given Nv input view images Ii, DiuNv i1, 3DGS learns Gaussians with: , Σ2D i1 Lrender p1 λSSIMqLI ` λSSIMLD-SSIM ` LD, (3) where LI I1 is the L1-loss, LD-SSIM is the D-SSIM loss (Kerbl et al., 2023), λSSIM is the 1 ` D1 weight of D-SSIM loss, and LD log is the optional depth supervision. ` 3 Published as conference paper at ICLR 2025 Figure 1: The overview of ArtGS. Our method is divided into two stages: (i) obtaining coarse canonical Gaussians Gc single trained with each single-state individually and initializing the part assignment module with clustered centers, (ii) jointly optimizing canonical Gaussians Gc and articulation model (including the articulation parameters Ψ and the part assignment module in Sec. 4.2). init by matching the Gaussians single and G1 Mesh Extraction from Gaussians To extract meshes from Gaussian splats G, we can render depth maps and utilize Truncated Signed Distance Function (TSDF) to fuse the reconstructed depth maps, and extract the object mesh with marching cubes (Huang et al., 2024a). This process can be done with Open3D (Zhou et al., 2018) with proper choice of voxel size and truncated threshold."
        },
        {
            "title": "4 METHOD",
            "content": "Given Nv RGB-D images of an unknown articulated object i1 at two joint states t0, 1u, we aim to reconstruct its part-level meshes and joint articulation parameters Ψ. We define set of learnable canonical Gaussians Gc which can be transformed into joint state Gaussians Gt via per-Gaussian SE(3) transformation cÑt, parameterized by Ψ. Formally, , Dt uNv Gt cÑt Gc and Gc pT cÑtq1 Gt (4) We impose the continuity of motion between the joint states by setting the canonical Gaussians Gc at the mid-state (c : = 0.5), enforcing that cÑ0 pT cÑ1q1. This simplifies the articulation learning and connects the two input joint states through the canonical Gaussians Gc, solving potential issues of occlusion and misinformation when reconstructing object meshes separately on the two joint states. t0, 1u. for Using this motion model, we leverage multi-view RGB-D images from the two input states to learn both the canonical Gaussian Gc, the transformation cÑ1 or equivalently the joint parameters Ψ, and extract object meshes Mt for different joint states following Sec. 3. An overview of ArtGS is presented in Fig. 1, with details on key designs provided in the following sections. 4.1 COARSE-TO-FINE CANONICAL GAUSSIAN INITIALIZATION WITH MOTION ANALYSIS The initialization of the canonical Gaussians Gc is crucial for articulation learning. good initialization leverages the consistency between input joint states, improving mesh reconstruction and articulation modeling. In contrast, random initialization leads to undesirable local minima, adversely affecting the learning process (see in Fig. 4). To tackle this issue, we propose coarse-to-fine strategy for the canonical Gaussian initialization, incorporating preliminary motion information from the two input joint states to enhance subsequent articulation modeling. Coarse Initialization by Matching Single-state Gaussians In this phase, we first separately train two sets of single-state Gaussians Gt single with input multi-view images following Eq. (3). We then apply Hungarian Matching to obtain matched Gaussian pairs between G0 single, based on the distance between Gaussian centers. We take the mean of each pair of matched Gaussians as the coarse canonical Gaussian initialization Gc coarse. To reduce the significant computation time associated single and 4 Published as conference paper at ICLR 2025 with matching large number of Gaussians, we use Farthest Point Sampling (FPS) to downsample the learned single-state Gaussians to set of 5K Gaussians prior to matching. Initialization Refinement with Motion Analysis To support geometry reconstruction and articulation modeling, relying solely on 5K matched coarse Gaussians alone is insufficient. Therefore, we refine the coarse initialization Gc coarse guided by the motion information of object parts. Intuitively, single-state Gaussians, G0 single, should exhibit consistency for static object parts discrepancies for movable parts, i.e., the static parts of these Gaussians are well-learned. Based on this insight, we refine the set of coarse canonical Gaussians Gc coarse by including Gaussians corresponding to static parts, allowing more focused learning of movable parts during articulation modeling. In practice, we classify each Gaussian Gi in joint state as static or dynamic by calculating its minimum Chamfer Distance to all Gaussians in the opposite state t: single and G1 CDtÑt min µt µt j2, Gi Gt single, Gj t single and CDtÑt Mean CDtÑt . (5) If the distance CDtÑt exceeds threshold ϵstatic, Gi is classified as dynamic; otherwise it is static. To determine which state, or t, contains more motion information, we compare the mean distance CDtÑt of all Gaussians in state following Eq. (5) and classify the higher state as the more motion informative state. For instance, cabinet with open drawers provides clearer identification of movable parts than one with closed drawers. With this information, we add the static Gaussians from the more motion informative state to refine Gc init. coarse into the final initialization of the canonical Gaussian Gc 4.2 PART DISCOVERY FOR ARTICULATION MODELING Following Eq. (4), we use part-based formulation for articulation modeling. Specifically, given the number of parts K, we aim to decompose the Gaussians into parts and learn the articulation uK paramerters Ψ tT cÑ1 k1. In contrast to existing works that leverage prior information for part discovery (Mandi et al., 2024; Weng et al., 2024), we discover parts in an unsupervised manner during learning. Center-based Part Modeling and Assignment Given input canonical Gaussians Gc tGiuN i1, our objective is to compute part-level masks RN ˆK that assign each Gaussian Gi to specific part. common approach to generating these assignment masks is through unsupervised segmentation modules using MLPs or slot-attention (Locatello et al., 2020; Jia et al., 2023; Liu et al., 2025). However, these models implicitly segment parts and fail to leverage the explicit spatial and dynamic information present in 3D Gaussians. We observe that such methods struggle with parts that exhibit similar motion patterns, leading to incorrect assignments. To address this issue, we adopt centerbased part modeling approach that explicitly utilizes spatial information, inspired by sparse control points from SC-GS (Huang et al., 2024c) and quasi-rigid blend skinning in REACTO (Song et al., 2024). Specifically, we define learnable centers Ck ppk, Vk, λkq with center location pk R3, rotation matrix Vk R3ˆ3, and scale vector λk R3. For given Gaussian Gi Gc, we compute the Mahalanobis distance Dik between Gi and center Ck as: Xk rVkpµc pkqs λk Dk pXk qT Xk and GumbelSoftmax ` τ ˆ (6) where Dk is the distance matrix for part assignment. One challenge of using the distance matrix for part assignment is identifying sharp boundaries when two parts overlap spatially (e.g., in the case of closed drawer). To improve boundary identification, we introduce residual term MLPpµ, X, Dq, predicted by shallow MLP that concatenates the absolute position of each Gaussian and the distance matrix as input. This residual is added to the original distance matrix to refine the part assignment mask following Eq. (6). Notably, we use Gumbel Softmax to ensure that each Gaussian is assigned to only one part, which simplifies the optimization of joint parameters. Detailed implementation can be found in Appendix A. Center Initialization by Clustering Coarse Gaussians We empirically find that the initialization of centers pk and scale λk have great impacts on the correctness of part discovery in later learning 5 Published as conference paper at ICLR 2025 process (see Fig. 4). Therefore, similar to the canonical Gaussian initialization described in Sec. 4.1, we utilize the motion type of each joint as additional information for providing good initializations of part centers. Specifically, we select the input joint state with more motion information to identify static and dynamic parts. For static parts, we take the mean of the Gaussians as the part center. For movable parts, we do spectral clustering on the positions of movable Gaussians (K 1 clusters) and take the mean of each cluster for part center initialization. We use the distance from the farthest point to the center of each cluster as the initial scale."
        },
        {
            "title": "4.3 SELF-GUIDED ARTICULATION TYPE AND PARAMETER LEARNING",
            "content": "After obtaining object part representations, we define the per-part articulation parameters via dual-quaternions. Formally, the joints articulation parameters Ψ tT cÑ1 : pqk,r, qk,dquK k1, where qk,r and qk,d are the real and dual part of the dual-quaternion that determine the rotation and translation of the joint transformation respectively. For notational simplicity, we use qt kq1 in the following texts. With the mid-state assumption in Sec. 4, we have q0 for qcÑt is the inverse of dual-quaternion q1 k. Given object masks obtained in Sec. 4.2, the per-gaussian dual-quaternion qi for Gaussian Gi Gc is given by: k1 tqcÑ1 uK pq1 k Kÿ qt k1 Mik qt k,r, Kÿ k1 Mik qt k,dq. (7) kq1 is the inverse of dual-quaternion q1 where pq1 belongs to part k. With the per-gaussian transformation given qt Gaussian Gc to get the two joint state Gaussians Gt with: qt rt µt RcÑt ` tcÑt µc i,r rc , , (8) where RcÑt , and denotes quaternion multiplication operation. We assume that the scale si and opacity σi of the Gaussian Gi remains consistent under transformation. is the per-gaussian rotation matrix and translation vector derived from qt and tcÑt k and Mik denotes the probability of Gaussian , we transform the canonical kuK To enhance the learning of articulation parameters, we adopt warm-up strategy for predicting the joint type of each part. During the warm-up stage, we optimize the articulation parameters Ψ tq1 k1 without any constraints. Next, we develop heuristic for joint type prediction based on the learned rotation qk,r. Specifically, we classify the joint as revolute if the rotation degree of qk,r exceeds threshold ϵrevol, and otherwise prismatic. With predicted joint types, we constrain the joint transformation for each part. Specifically, we manually set the rotation quaternion qk,r of prismatic joints as identity quaternion. This operation allows the model to focus on optimizing the translation term qk,d of the prismatic joint, thereby obtaining more accurate estimate of the joint parameters. 4.4 OPTIMIZATION We train our model using the rendering loss with depth supervision Lrender described in Sec. 3 on the reconstructed Gt for the two joint states as discussed in Sec. 4.3. To reduce the chances of learning artifacts during update, we use the single-state reconstructed Gaussians Gt single as an additional supervision: LCD 1 Nÿ i1 min µt µt j2 , Gi Gt, and Gj Gt single, (9) where we calculate the single-direction Chamfer Distance between the deformed Gaussians Gt and single-state reconstructed Gaussians Gt single as the loss signal. As these single-state Gaussians are only rough estimate, we only introduce this loss in the first 1K to 5K steps. Additionally, to regularize the learning of part centers pk, we add another regularization loss as: Lreg 1 Kÿ k1 pk ˆpk2, where ˆpk Nÿ Mikř i1 Mik µi, (10) which enforces that the centers pk should be close to the average spatial position of Gaussians in canonical Gaussians Gc that belong to part k. Above all, our supervision could be summarized as: Lrender ` λCDLCD ` λregLreg. (11) We provide more implementation and model training details in Appendix A. 6 Published as conference paper at ICLR"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Datasets We evaluate our method on three datasets: (1) PARIS, two-part dataset proposed by Liu et al. (2023a), which features articulated objects consisting of one static and one movable part. It includes 10 synthetic objects from the PartNet-Mobility dataset (Xiang et al., 2020) and 2 real-world objects captured using the MultiScan (Mao et al., 2022) toolset. (2) DTA-Multi, dataset proposed by Weng et al. (2024), containing 2 synthetic multi-part articulated objects from PartNet-Mobility, each with one static part and two movable parts. (3) ArtGS-Multi, our newly curated dataset, featuring 5 complex articulated objects from PartNet-Mobility with 3 to 6 movable parts. Metrics Following the evaluation protocols of PARIS (Liu et al., 2023a) and DTA (Weng et al., 2024), we assess the performance of all methods using both mesh reconstruction and articulation estimation metrics. For mesh reconstruction, we compute the bi-directional Chamfer Distance between the reconstructed mesh and the ground truth mesh with 10K uniformly sampled points from each mesh. We report the Chamfer Distance for the whole object (CD-w), the static parts (CD-s), and the movable parts (CD-m). For articulation estimation, we evaluate the predicted articulation using the angular error (Axis Ang.) and the distance (Axis Pos., revolute joint only) between the predicted and ground-truth joint axes. We also report the part motion error (Part Motion) which measures the rotation geodesic distance error (in degrees) for revolute joints and Euclidean distance error (in meters) for prismatic joints. 5.1 RESULTS ON SIMPLE ARTICULATED OBJECTS Experimental Setup We use the PARIS dataset as the benchmark and select Ditto (Hsu et al., 2023), PARIS (Liu et al., 2023a), CSG-reg (Weng et al., 2024), 3Dseg-reg (Weng et al., 2024), and DTA (Weng et al., 2024) as baselines for quantitative evaluation. Following the evaluation setting from DTA (Weng et al., 2024), we report all metrics with mean std over 10 trials calculated at the high-visibility joint state. We re-train DTA on the same device (NVIDIA RTX 3090) for training time comparison. Additional results on all joint states are provided in Tab. A.1. Results As shown in Tab. 1, our method significantly outperforms existing approaches across all metrics, especially for joint articulation parameter estimation, where ArtGS achieves substantially lower errors. This improvement stems from our motion model with Gaussian Splatting, which explicitly deforms Gaussians for more precise part transformation modeling, leading to more precise joint parameter estimation. For mesh reconstruction, ArtGS excels in reconstructing movable parts, yielding lower CD-m values, especially for real-world objects. While DTA performs well on CDw and CD-s due to its state-by-state reconstruction, we show in Fig. 2 that it struggles with the low-visibility state. In contrast, ArtGS achieves significantly better results on the low-visibility state while maintaining competitive results on the high-visibility state. This is attributed to the canonical Gaussians modeling that connects the two input joint states for mutually improved mesh reconstruction. Additionally, ArtGS shows consistently better results on real-world objects with significantly faster training time, positioning it as an efficient solution for building digital twins of real-world articulated objects. 5.2 RESULTS ON COMPLEX ARTICULATED OBJECTS WITH MULTIPLE MOVABLE PARTS Experimental Setup We use DTA-Multi and ArtGS-Multi as benchmarks for evaluating complex articulated object reconstruction. On DTA-Multi, we compare our model against PARIS and DTA, while on ArtGS-Multi we use DTA as the main baseline given its strong performance. Similar to Sec. 5.1, we report all metrics with mean over 10 trials for DTA-Multi and 3 trials for ArtGSmulti because of the training time required for baselines. For ArtGS-multi, we report the average of all movable parts for articulation estimation and mesh reconstruction due to the large number of parts. Considering the potential error prediction with no mesh for one of the parts, we manually set the Chamfer Distance of the empty prediction to 1000. Results As demonstrated in Tab. 2 and Tab. 3, our method consistently outperforms existing methods by large margin in both mesh reconstruction and articulation estimation. Notably, on ArtGS-Multi, the baseline model DTA struggles with movable part identification and axis prediction 7 Published as conference paper at ICLR 2025 Table 1: Quantitative evaluation on PARIS. Metrics are reported as mean std over 10 trials at the joint state with higher visibility, following (Weng et al., 2024). PARIS (Liu et al., 2023a) is augmented with depth for fair comparison. DTA is re-trained for time efficiency comparison. Lower (Ó) is better on all metrics and we highlight best and second best results. Objects with : are seen categories trained in Ditto. indicates wrong motion type predictions. Axis Pos. is omitted for prismatic joints (Blade, Storage, and Real Storage). Metric Method Ditto PARIS* CSG-reg 3Dseg-reg DTA Ours Ditto PARIS* CSG-reg 3Dseg-reg DTA Ours Ditto PARIS* CSG-reg 3Dseg-reg DTA Ours Ditto PARIS* CSG-reg 3Dseg-reg DTA Ours Ditto PARIS* CSG-reg 3Dseg-reg DTA Ours Ditto PARIS* CSG-reg 3Dseg-reg DTA Ours DTA Ours Axis Ang Axis Pos Part Motion CD-s CD-m CD-w Time (min) FoldChair 89.35 15.7929.3 0.100.0 - 0.030.0 0.010.0 3.77 0.250.5 0.020.0 - 0.010.0 0.000.0 99.36 127.3475.0 0.130.0 - 0.100.0 0.030.0 33.79 10.205.8 1.69 - 0.180.0 0.260.3 141.11 17.9724.9 1.91 - 0.150.0 0.540.1 6.80 4.376.4 0.48 - 0.270.0 0.430.2 29 9 Fridge 89.30 2.935.3 0.270.0 - 0.090.0 0.030.0 1.02 1.132.6 0.000.0 - 0.010.0 0.000.0 45.2658.5 0.290.0 - 0.120.0 0.040.0 3.05 8.8212.0 1.45 - 0.620.0 0.520.0 0.99 7.2311.5 21.71 - 0.270.0 0.210.0 2.16 5.534.7 0.98 - 0.700.0 0.580.0 30 8 Laptop: 3.12 0.030.0 0.470.0 2.340.11 0.070.0 0.010.0 0.01 0.000.0 0.200.2 0.100.0 0.010.0 0.010.0 5.18 0.030.0 0.350.0 1.610.1 0.110.0 0.020.0 0.25 0.160.0 0.32 0.76 0.300.0 0.630.0 0.19 0.150.0 0.42 1.01 0.130.0 0.130.0 0.31 0.260.0 0.40 0.81 0.320.0 0.500.0 31 Oven: 0.96 7.4323.4 0.350.1 - 0.220.1 0.010.0 0.13 0.050.2 0.180.0 - 0.010.0 0.000.0 2.09 9.1328.8 0.580.0 - 0.120.0 0.020.0 2.52 3.180.3 3.93 - 4.600.1 3.880.0 0.94 6.5410.6 256.99 - 0.440.0 0.890.2 2.51 3.180.3 3.00 - 4.240.1 3.580.0 29 7 Scissor 4.50 16.6232.1 0.280.0 - 0.100.0 0.050.0 5.70 1.591.7 0.010.0 - 0.020.0 0.000.0 19.28 68.3664.8 0.200.0 - 0.370.6 0.040.0 39.07 15.5813.3 3.26 - 3.556.1 0.610.3 20.68 16.6516.6 1.95 - 10.1119.4 0.640.4 1.70 3.903.6 1.70 - 0.410.0 0.670.3 28 7 Synthetic Objects Stapler 89.86 8.1715.3 0.300.0 - 0.070.0 0.010.0 0.20 4.673.9 0.020.0 - 0.020.0 0.010.0 56.61 107.7668.1 0.440.0 - 0.080.0 0.010.0 41.64 2.481.9 2.22 - 2.910.1 3.830.1 31.21 30.4637.0 6.36 - 1.130.5 0.520.1 2.38 5.275.9 1.99 - 1.920.0 2.630.0 29 7 USB 89.77 0.710.8 11.7810.5 - 0.110.0 0.040.0 5.41 3.353.1 0.010.0 - 0.000.0 0.000.0 80.60 96.9367.8 10.489.3 - 0.150.0 0.030.0 2.64 1.950.5 1.95 - 2.320.1 2.250.2 15.88 10.176.9 29.78 - 1.470.0 1.220.1 2.09 1.780.2 1.20 - 1.170.0 1.280.0 31 7 Washer 89.51 18.4023.3 71.936.3 - 0.360.1 0.020.0 0.66 3.283.1 2.131.5 - 0.050.0 0.000.0 55.72 49.7726.5 158.998.8 - 0.280.1 0.030.0 10.32 12.193.7 4.53 - 4.560.1 6.430.1 12.89 265.27248.7 436.42 - 0.450.0 0.450.2 7.29 10.112.8 4.48 - 4.480.2 5.990.1 28 Blade 79.54 41.2831.4 7.645.0 9.407.5 0.200.1 0.030.0 - - - - - - 0.360.2 0.050.0 0.150.0 0.000.0 0.000.0 46.90 1.400.7 0.59 66.31 0.550.0 0.540.0 195.93 117.99213.0 26.62 6.23 2.050.3 1.120.2 42.04 0.580.1 0.56 0.78 0.360.0 0.610.0 27 7 Storage: 6.32 0.030.0 2.822.5 - 0.090.0 0.010.0 - - - - - - 0.09 0.300.0 0.040.0 - 0.000.0 0.000.0 9.18 8.670.8 7.06 - 4.900.5 7.310.2 2.20 52.3411.0 1.39 - 0.360.0 1.020.4 3.91 7.800.4 4.00 - 3.990.4 5.210.1 28 8 All 54.22 11.1416.1 9.602.4 - 0.130.0 0.020.0 2.11 1.791.5 0.320.2 - 0.020.0 0.000.0 39.87 50.5239.0 17.161.8 - 0.130.1 0.020.0 18.94 6.463.9 2.70 - 2.450.7 2.630.1 42.20 52.4858.0 78.36 - 1.662.0 0.670.2 7.12 4.282.4 1.88 - 1.790.1 2.150.1 29 8 Fridge 1.71 1.900.0 8.920.9 - 2.080.0 2.090.0 1.84 0.500.0 1.461.1 - 0.590.0 0.470.0 8.43 1.580.0 14.820.1 - 1.850.0 1.940.0 47.01 11.641.5 6.33 - 2.360.1 1.640.2 50.60 77.8526.8 442.17 - 1.120.0 0.660.2 6.50 8.991.4 5.71 - 2.080.1 1.290.1 29 9 Real Objects Storage 5.88 30.1010.4 69.719.6 - 13.643.6 3.470.3 - - - - - - 0.38 0.570.1 0.640.1 - 0.140.0 0.040.0 16.09 20.252.8 12.55 - 10.980.1 2.930.3 20.35 474.57227.2 521.49 - 30.782.6 6.283.6 14.08 32.108.2 14.29 - 8.980.1 3.230.1 29 9 All 3.80 16.005.2 39.315.2 - 7.861.8 2.780.2 1.84 0.500.0 1.461.1 - 0.590.0 0.470.0 4.41 1.070.1 7.730.1 - 1.000.0 0.990.0 31.55 15.942.1 9.44 - 6.670.1 2.290.3 35.48 276.21127.0 481.83 - 15.951.3 3.471.9 10.29 20.554.8 10.00 - 5.530.1 2.260.1 29 Figure 2: Qualitative visualizations of PARIS objects. We present reconstruction comparisons between DTA and our model on Real Storage (Top) and Synthetic Blade (Bottom). DTA struggles with mesh reconstruction at the low-visibility state, as it processes each state separately. In contrast, our method leverages the connection between states to improve the reconstruction for both lowand high-visibility states. as the number of parts increases, whereas our model maintains high performance regardless of part count. We also provide qualitative comparison in Fig. 3 for better visualization. Moreover, our method maintains the same time efficiency while the training time of existing methods scales with the number of parts. These results underscore the robustness and effectiveness of our method in modeling complex, multi-part articulated objects. 5.3 ABLATIVE STUDIES Experimental Setup To verify the effectiveness of our model design, we meticulously design four ablations of ArtGS to identify the impact of key components in our method: (i) Randomly initializing canonical Gaussians (w/o Cano. Init.), (ii) predicting part assignments with MLP (w/ MLP Seg) or Slot-Attention (w/ SA Seg), (iii) randomly initializing part centers Ck (w/o Center Init.), (iv) 8 Published as conference paper at ICLR Table 2: Quantitative evaluation on DTA-Multi. We report averaged metrics over 10 trials with different random seeds. Lower (Ó) is better on all metrics. Joint 1 of Storage-m is prismatic with no Axis Pos. Object Method Axis Ang 0 Axis Ang 1 Axis Pos 0 Axis Pos 1 Part Motion 0 Part Motion 1 CD-s CD-m 0 CD-m 1 CD-w Time (min) Fridge-m Storage-m PARIS DTA Ours PARIS DTA Ours 34.52 0.25 0.02 43.26 0.17 0.01 15.91 0.06 0. 26.18 0.40 0.02 3.60 0.01 0.00 10.42 0.04 0.01 1.63 0.01 0.00 - - - 86.21 0.23 0. 79.84 0.13 0.01 105.86 0.08 0.03 0.64 0.00 0.00 8.52 0.63 0.62 8.56 0.86 0.78 526.19 0.44 0. 128.62 0.20 0.19 160.86 0.53 0.18 266.71 0.25 0.27 15.00 0.88 0.75 8.66 0.97 0.93 - 32 - 32 8 Table 3: Quantitative evaluation on ArtGS-Multi. Metrics are averaged over 3 trials. Due to the large number of parts, we report the average metric for all movable parts. Lower (Ó) is better on all metrics. Table-31249 has 3 prismatic joints with no Axis Pos. Object Method Axis Ang Axis Pos Part Motion CD-s CD-m CD-w Time (min) Table 25493 (4 parts) Table 31249 (5 parts) Storage 45503 (4 parts) Storage 47468 (7 parts) Oven 101908 (4 parts) DTA Ours DTA Ours DTA Ours DTA Ours DTA Ours 24.35 1.16 20.62 0.04 51.18 0. 19.07 0.14 17.83 0.04 - - 4.2 0.00 2.44 0.00 0.31 0. 6.51 0.01 0.12 0.00 30.8 0.01 43.77 0.03 10.67 0.62 31.80 0. 0.59 0.74 1.39 1.22 5.74 0.75 0.82 0.67 1.17 1.08 104.38 3. 230.38 3.09 246.63 0.13 476.91 3.70 359.16 0.25 0.55 0.74 1.00 1. 0.88 0.88 0.71 0.70 1.01 1.03 34 8 37 8 35 45 8 35 8 Figure 3: Qualitative results on multi-part objects. We present reconstruction comparisons between DTA and our model on Storage-47648 (Left) and Table-31249 (Bottom). On ArtGS-Multi, DTA struggles with movable part identification and axis prediction as the number of parts increases, whereas our model maintains high performance regardless of part count, achieving high-quality reconstruction of part mesh and joint articulation. clustering all Gaussians instead of clustering movable Gaussians for part center initialization (w/o Motion Prior), and (v) learning articulation parameters without the joint prediction warmup stage (w/o Joint Pred.). We select two representative objects: Storage-47648 with 4 revolute and 2 prismatic joints and Oven-101908 with 3 revolute joints for ablative analysis. Similar to Sec. 5.2, we report the average of all parts over 10 trials for all metrics. Results and Discussions As shown in Tab. 4 and Fig. 4, we make the following observations: Canonical Gaussians Initialization. Omitting this initialization strategy significantly degrades the model performance across all metrics, particularly for movable parts. As illustrated in Fig. 9 Published as conference paper at ICLR 2025 Table 4: Ablative experiments. Lower (Ó) is better on all metrics. Storage 47648 (7 parts) Oven 101908 (4 parts) Axis Ang Axis Pos Part Motion CD-s CD-m CD-w Axis Ang Axis Pos Part Motion CD-s CD-m CD-w 0.14 24.15 52.78 26.74 0.16 21.84 25.43 0.02 0.73 0.83 0.22 0.02 3.46 0.7 0.62 20.61 33.04 21.16 0.72 31.43 23.22 0.67 0.83 1.09 258.23 0.67 1.82 1.52 3.70 495.07 344.19 599.46 3.90 664.25 459.89 0.70 1.25 1.69 1.15 0.71 1.28 1. 0.04 57.87 28.94 40.08 0.04 12.08 58.04 0.01 2.95 2.36 0.98 0.01 3.33 4.53 0.23 54.45 22.46 41.06 0.23 27.28 51.28 1.08 1.73 1.41 1.75 1.08 7.78 1.26 0.25 1030.19 8.86 503.44 0.25 126.95 496.64 1.03 2.36 2.13 2.35 1.03 2.19 2. Method Full w/o Cano. init. w/o Center Init. w/o Motion Prior w/o Joint Pred. w/ MLP Seg w/ SA Seg Figure 4: Abaltion Studies. We visualize the initialized and optimized canonical Gaussians with their part assignment and centers for the full model, w/o Motion Prior and w/o Cano. Init. We highlight center error, part assignment error, and canonical Gaussian error with red, green, and blue bounding boxes separately. (5), the absence of our initialization strategy leads to malformed canonical Gaussians, making the model converge to suboptimal local minima during optimization. Center-based Part Modeling and Assignment. Replacing our center-based part assignment module with MLP or Slot-Attention (\"w/ MLP Seg\" and \"w/ SA Seg\") leads to substantial performance drops, especially in joint parameter estimation and movable part reconstruction. This demonstrates the superiority of our center-based approach in accurately segmenting articulated parts. Center Initialization. Random center initialization performs well for static parts but poorly for movable parts. Clustering all Gaussians fails to reconstruct both static and movable parts due to incorrect center initialization. As illustrated in Fig. 4 (1), clustering on movable Gaussians still produces an incorrect center but provides good starting point for optimization. Our ArtGS will refine the centers in the optimization process as shown in Fig. 4 (3). In contrast, clustering on all Gaussians results in entirely wrong center initialization (Fig. 4 (2)), which is difficult to correct (Fig. 4 (4)), leading to even worse performance than random initialization. This highlights the importance of our center initialization strategy in achieving accurate part articulation modeling. Joint Prediction Warmup. This technique primarily affects prismatic joints, as we do not constrain the transformation of revolute joints. As shown in Tab. 4, predicting the joint type and then refining joint parameters with type constraints slightly improves the articulation reconstruction. In summary, these ablation studies confirm that each component contributes significantly to its overall performance, playing crucial roles in achieving accurate joint parameter estimation and high-quality part mesh reconstruction. We provide further discussions in Appendix and Appendix C."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In conclusion, we propose ArtGS, novel approach for reconstructing articulated objects from two states of multi-view images. By leveraging 3D Gaussians and introducing novel techniques for state alignment and part dynamics modeling, our approach overcomes key limitations of existing methods. The performance improvements in joint parameter estimation and part mesh reconstruction, particularly for complex multi-part objects, demonstrate the effectiveness of our innovations. Our comprehensive experiments across synthetic and real-world datasets validate the robustness and efficiency of ArtGS, while also revealing promising directions for future research. As the demand for accurate digital replicas of articulated objects continues to grow in fields such as robotics and augmented reality, ArtGS provides solid foundation for bridging the gap between physical and virtual environments. Moving forward, we anticipate that the principles introduced in this work will inspire further advancements in the field, ultimately enabling more sophisticated and realistic simulations for wide range of applications. 10 Published as conference paper at ICLR"
        },
        {
            "title": "REFERENCES",
            "content": "Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, and Youngjung Uh. Pergaussian embedding-based deformation for deformable 3d gaussian splatting. arXiv preprint arXiv:2404.03613, 2024. 2 Danpeng Chen, Hai Li, Weicai Ye, Yifan Wang, Weijian Xie, Shangjin Zhai, Nan Wang, Haomin Liu, Hujun Bao, and Guofeng Zhang. Pgsr: Planar-based gaussian splatting for efficient and high-fidelity surface reconstruction. arXiv preprint arXiv:2406.06521, 2024a. 20 Yixin Chen, Junfeng Ni, Nan Jiang, Yaowei Zhang, Yixin Zhu, and Siyuan Huang. Single-view 3d scene reconstruction with high-fidelity shape and texture. In Proceedings of International Conference on 3D Vision (3DV), 2024b. 1 Ruihang Chu, Zhengzhe Liu, Xiaoqing Ye, Xiao Tan, Xiaojuan Qi, Chi-Wing Fu, and Jiaya Jia. Command-driven articulated object understanding and manipulation. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 Jianning Deng, Kartic Subr, and Hakan Bilen. Articulate your nerf: Unsupervised articulated object modeling via conditional view synthesis. arXiv preprint arXiv:2406.16623, 2024. 1, Samir Yitzhak Gadre, Kiana Ehsani, and Shuran Song. Act the part: Learning interaction strategies for articulated object part discovery. In Proceedings of International Conference on Computer Vision (ICCV), 2021. 3 Haoran Geng, Ziming Li, Yiran Geng, Jiayi Chen, Hao Dong, and He Wang. Partmanip: Learning cross-category generalizable part manipulation policy from point cloud observations. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2023a. 1 Haoran Geng, Helin Xu, Chengyang Zhao, Chao Xu, Li Yi, Siyuan Huang, and He Wang. Gapartnet: Cross-category domain-generalizable object perception and manipulation via generalizable and actionable parts. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2023b. 1 Ran Gong, Jiangyong Huang, Yizhou Zhao, Haoran Geng, Xiaofeng Gao, Qingyang Wu, Wensi Ai, Ziheng Zhou, Demetri Terzopoulos, Song-Chun Zhu, et al. Arnold: benchmark for languagegrounded task learning with continuous states in realistic 3d scenes. In Proceedings of International Conference on Computer Vision (ICCV), 2023. 1 Zhiyang Guo, Wengang Zhou, Li Li, Min Wang, and Houqiang Li. Motion-aware 3d gaussian splatting for efficient dynamic scene reconstruction. arXiv preprint arXiv:2403.11447, 2024. 2 Nick Heppert, Muhammad Zubair Irshad, Sergey Zakharov, Katherine Liu, Rares Andrei Ambrus, Jeannette Bohg, Abhinav Valada, and Thomas Kollar. Carto: Category and joint agnostic reconstruction of articulated objects. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Cheng-Chun Hsu, Zhenyu Jiang, and Yuke Zhu. Ditto in the house: Building articulation models of indoor scenes through interactive perception. In Proceedings of International Conference on Robotics and Automation (ICRA), 2023. 3, 7 Ruizhen Hu, Wenchao Li, Oliver Van Kaick, Ariel Shamir, Hao Zhang, and Hui Huang. Learning to predict part mobility from single static snapshot. ACM Transactions on Graphics (TOG), 36(6): 113, 2017. 3 Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 Conference Papers, 2024a. 4, 20 Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. In Proceedings of International Conference on Machine Learning (ICML), 2024b. 1 Xiaoxia Huang, Ian Walker, and Stan Birchfield. Occlusion-aware multi-view reconstruction of articulated objects for manipulation. Robotics and Autonomous Systems, 62(4):497505, 2014. 3 Published as conference paper at ICLR 2025 Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2024c. 2, 5, 18 Ajinkya Jain, Rudolf Lioutikov, Caleb Chuck, and Scott Niekum. Screwnet: Category-independent articulation model estimation from depth images using screw theory. In Proceedings of International Conference on Robotics and Automation (ICRA), 2021. 3 Baoxiong Jia, Yu Liu, and Siyuan Huang. Improving object-centric learning with query optimization. In Proceedings of International Conference on Learning Representations (ICLR), 2023. 5 Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, and Siyuan Huang. Sceneverse: Scaling 3d vision-language learning for grounded scene understanding. In Proceedings of European Conference on Computer Vision (ECCV), 2024. Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, et al. Vr-gs: physical dynamics-aware interactive gaussian splatting system in virtual reality. In ACM SIGGRAPH 2024 Conference Papers, 2024. 2 Zhenyu Jiang, Cheng-Chun Hsu, and Yuke Zhu. Ditto: Building digital twins of articulated objects from interaction. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3 Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total capture: 3d deformation model for tracking faces, hands, and bodies. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 3 HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, and Benjamin Busam. Deformable 3d gaussian splatting for animatable human avatars. arXiv preprint arXiv:2312.15059, 2023. 2 Kai Katsumata, Duc Minh Vo, and Hideki Nakayama. An efficient 3d gaussian representation for monocular/multi-view dynamic scenes. arXiv preprint arXiv:2311.12897, 2023. Dov Katz, Moslem Kazemi, Andrew Bagnell, and Anthony Stentz. Interactive segmentation, tracking, and kinematic modeling of unknown 3d articulated objects. In Proceedings of International Conference on Robotics and Automation (ICRA), 2013. 3 Yuki Kawana, Yusuke Mukuta, and Tatsuya Harada. Unsupervised pose-aware part decomposition for 3d articulated objects. arXiv preprint arXiv:2110.04411, 2021. 3 Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3, 18 Justin Kerr, Chung Min Kim, Mingxuan Wu, Brent Yi, Qianqian Wang, Ken Goldberg, and Angjoo Kanazawa. Robot see robot do: Imitating articulated object manipulation with monocular 4d reconstruction. In Conference on Robot Learning (CoRL), 2024. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of International Conference on Computer Vision (ICCV), 2023. 2, 20 Jiahui Lei, Congyue Deng, William Shen, Leonidas Guibas, and Kostas Daniilidis. Nap: Neural 3d articulated object prior. 2023. 3 Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, and Kostas Daniilidis. Gart: Gaussian In Proceedings of Conference on Computer Vision and Pattern articulated template models. Recognition (CVPR), 2024. 2, 3 Stanley Lewis, Jana Pavlasek, and Odest Chadwicke Jenkins. Narf22: Neural articulated radiance fields for configuration-aware rendering. In Proceedings of International Conference on Intelligent Robots and Systems (IROS), 2022. 12 Published as conference paper at ICLR 2025 Xiaolong Li, He Wang, Li Yi, Leonidas Guibas, Lynn Abbott, and Shuran Song. Category-level articulated object pose estimation. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3 Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Ma, Baoxiong Jia, and Siyuan Huang. Multi-modal situated reasoning in 3d scenes. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2024. 1 Jiayi Liu, Ali Mahdavi-Amiri, and Manolis Savva. Paris: Part-level reconstruction and motion analysis for articulated objects. In Proceedings of International Conference on Computer Vision (ICCV), 2023a. 1, 2, 3, 7, 8, Jiayi Liu, Hou In Ivan Tam, Ali Mahdavi-Amiri, and Manolis Savva. Cage: Controllable articulation generation. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1 Liu Liu, Wenqiang Xu, Haoyuan Fu, Sucheng Qian, Qiaojun Yu, Yang Han, and Cewu Lu. Akb-48: real-world articulated object knowledge base. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2022a. 1 Liu Liu, Han Xue, Wenqiang Xu, Haoyuan Fu, and Cewu Lu. Toward real-world category-level articulation pose estimation. Proceedings of Transactions on Image Processing (TIP), 31:1072 1083, 2022b. 3 Shaowei Liu, Saurabh Gupta, and Shenlong Wang. Building rearticulable models for arbitrary 3d objects from 4d point clouds. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2023b. 1, 3 Xueyi Liu, Ji Zhang, Ruizhen Hu, Haibin Huang, He Wang, and Li Yi. Self-supervised categorylevel articulated object pose estimation with part-level se (3) equivariance. In Proceedings of International Conference on Learning Representations (ICLR), 2023c. 3 Yu Liu, Baoxiong Jia, Yixin Chen, and Siyuan Huang. Slotlifter: Slot-guided feature lifting for learning object-centric radiance fields. In Proceedings of European Conference on Computer Vision (ECCV), 2025. Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2020. 5 Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multi-person linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pp. 851866. 2023. 3 Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, and Yansong Tang. Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation. In Proceedings of European Conference on Computer Vision (ECCV), 2024a. 1 Ruijie Lu, Yixin Chen, Junfeng Ni, Baoxiong Jia, Yu Liu, Diwen Wan, Gang Zeng, and Siyuan Huang. Movis: Enhancing multi-object novel view synthesis for indoor scenes. arXiv preprint arXiv:2412.11457, 2024b. 1 Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, and Yuchao Dai. 3d geometry-aware deformable gaussian splatting for dynamic view synthesis. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2024c. 2 Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In Proceedings of International Conference on 3D Vision (3DV), 2024. 13 Published as conference paper at ICLR 2025 Rundong Luo, Haoran Geng, Congyue Deng, Puhao Li, Zan Wang, Baoxiong Jia, Leonidas Guibas, and Siyuang Huang. Physpart: Physically plausible part completion for interactable objects. In Proceedings of International Conference on Robotics and Automation (ICRA), 2025. 1 Liqian Ma, Jiaojiao Meng, Shuntao Liu, Weihang Chen, Jing Xu, and Rui Chen. Sim2real 2: Actively building explicit physics model for precise articulated object manipulation. In Proceedings of International Conference on Robotics and Automation (ICRA), 2023. 3 Zhao Mandi, Yijia Weng, Dominik Bauer, and Shuran Song. Real2code: Reconstruct articulated objects via code generation. arXiv preprint arXiv:2406.08474, 2024. 2, Yongsen Mao, Yiming Zhang, Hanxiao Jiang, Angel Chang, and Manolis Savva. Multiscan: Scalable rgbd scanning for 3d environments with articulated objects. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 7 Roberto Martín-Martín, Sebastian Höfer, and Oliver Brock. An integrated approach to visual perception of articulated objects. In Proceedings of International Conference on Robotics and Automation (ICRA), 2016. 3 Marko Mihajlovic, Yan Zhang, Michael Black, and Siyu Tang. Leap: Learning articulated occupancy of people. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3 Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani. Where2act: From pixels to actions for articulated 3d objects. In Proceedings of International Conference on Computer Vision (ICCV), 2021. 3 Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, and Xiaolong Wang. A-sdf: Learning disentangled signed distance functions for articulated shape representation. In Proceedings of International Conference on Computer Vision (ICCV), 2021. 3 Junfeng Ni, Yixin Chen, Bohan Jing, Nan Jiang, Bin Wang, Bo Dai, Puhao Li, Yixin Zhu, Song-Chun Zhu, and Siyuan Huang. Phyrecon: Physically plausible neural scene reconstruction. 2024. Neil Nie, Samir Yitzhak Gadre, Kiana Ehsani, and Shuran Song. Structure from action: Learning interactions for articulated object 3d structure discovery. arXiv preprint arXiv:2207.08997, 2022. 3 Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. Neural articulated radiance field. In Proceedings of International Conference on Computer Vision (ICCV), 2021. 3 Atsuhiro Noguchi, Umar Iqbal, Jonathan Tremblay, Tatsuya Harada, and Orazio Gallo. Watch it move: Unsupervised discovery of 3d joints for re-posing of articulated objects. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3 Sudeep Pillai, Matthew Walter, and Seth Teller. Learning articulated motions from visual demonstration. arXiv preprint arXiv:1502.01659, 2015. Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, and Siyu Tang. 3dgs-avatar: In Proceedings of Conference on Animatable avatars via deformable 3d gaussian splatting. Computer Vision and Pattern Recognition (CVPR), 2024. 2 Javier Romero, Dimitrios Tzionas, and Michael Black. Embodied hands: Modeling and capturing hands and bodies together. arXiv preprint arXiv:2201.02610, 2022. 3 Chaoyue Song, Tianyi Chen, Yiwen Chen, Jiacheng Wei, Chuan Sheng Foo, Fayao Liu, and Guosheng Lin. Moda: Modeling deformable 3d objects from casual videos. arXiv preprint arXiv:2304.08279, 2023a. 3 Chaoyue Song, Jiacheng Wei, Chuan Sheng Foo, Guosheng Lin, and Fayao Liu. Reacto: Reconstructing articulated objects from single video. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 14 Published as conference paper at ICLR 2025 Chonghyuk Song, Gengshan Yang, Kangle Deng, Jun-Yan Zhu, and Deva Ramanan. Total-recon: Deformable scene reconstruction for embodied view synthesis. In Proceedings of International Conference on Computer Vision (ICCV), 2023b. 3 Jürgen Sturm, Cyrill Stachniss, and Wolfram Burgard. probabilistic framework for learning kinematic models of articulated objects. Journal of Artificial Intelligence Research, 41, 2011. 3 Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2 Xiaohao Sun, Hanxiao Jiang, Manolis Savva, and Angel Xuan Chang. Opdmulti: Openable part detection for multiple objects. arXiv preprint arXiv:2303.14087, 2023. 3 Archana Swaminathan, Anubhav Gupta, Kamal Gupta, Shishira Maiya, Vatsal Agarwal, and Abhinav Shrivastava. Leia: Latent view-invariant embeddings for implicit 3d articulation. arXiv preprint arXiv:2409.06703, 2024. 3 Jeff Tan, Gengshan Yang, and Deva Ramanan. Distilling neural fields for real-time articulated shape reconstruction. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 Marcel Torne, Anthony Simeonov, Zechu Li, April Chan, Tao Chen, Abhishek Gupta, and Pulkit Agrawal. Reconciling reality through simulation: real-to-sim-to-real approach for robust manipulation. arXiv preprint arXiv:2403.03949, 2024. 1 Wei-Cheng Tseng, Hung-Ju Liao, Lin Yen-Chen, and Min Sun. Cla-nerf: Category-level articulated neural radiance field. In Proceedings of International Conference on Robotics and Automation (ICRA), 2022. 3 Diwen Wan, Ruijie Lu, and Gang Zeng. Superpoint gaussian splatting for real-time high-fidelity dynamic scene reconstruction. arXiv preprint arXiv:2406.03697, 2024a. 2 Diwen Wan, Yuxiang Wang, Ruijie Lu, and Gang Zeng. Template-free articulated gaussian splatting for real-time reposable dynamic view synthesis. arXiv preprint arXiv:2412.05570, 2024b. 2 Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. 20 Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qinping Zhao, and Kai Xu. Shape2motion: Joint analysis of motion parts and attributes from 3d shapes. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3 Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. Proceedings of Transactions on Image Processing (TIP), 13 (4):600612, 2004. 20 Fangyin Wei, Rohan Chabra, Lingni Ma, Christoph Lassner, Michael Zollhöfer, Szymon Rusinkiewicz, Chris Sweeney, Richard Newcombe, and Mira Slavcheva. Self-supervised neural articulated shape and appearance models. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Müller, Alex Evans, Dieter Fox, Jan Kautz, and Stan Birchfield. Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 20 Yijia Weng, He Wang, Qiang Zhou, Yuzhe Qin, Yueqi Duan, Qingnan Fan, Baoquan Chen, Hao Su, and Leonidas Guibas. Captra: Category-level pose tracking for rigid and articulated objects from point clouds. In Proceedings of International Conference on Computer Vision (ICCV), 2021. 3 15 Published as conference paper at ICLR 2025 Yijia Weng, Bowen Wen, Jonathan Tremblay, Valts Blukis, Dieter Fox, Leonidas Guibas, and Stan Birchfield. Neural implicit representation for building digital twins of unknown articulated objects. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1, 2, 3, 5, 7, 8 Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: simulated part-based interactive environment. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 7 Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physics-integrated 3d gaussians for generative dynamics. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, William Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Ghum & ghuml: Generative 3d human shape and articulated pose models. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3 Zihao Yan, Ruizhen Hu, Xingguang Yan, Luanmin Chen, Oliver Van Kaick, Hao Zhang, and Hui Huang. Rpm-net: recurrent prediction of motion and parts from point cloud. arXiv preprint arXiv:2006.14865, 2020. 3 Fan Yang, Tianyi Chen, Xiaosheng He, Zhongang Cai, Lei Yang, Si Wu, and Guosheng Lin. Attrihuman-3d: Editable 3d human avatar generation with attribute decomposition and indexing. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2024a. 3 Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, Deva Ramanan, William Freeman, and Ce Liu. Lasr: Learning articulated shape reconstruction from monocular video. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2021a. Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Ce Liu, and Deva Ramanan. Viser: Video-specific surface embeddings for articulated 3d shape reconstruction. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2021b. 3 Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, and Hanbyul Joo. Banmo: Building animatable 3d neural models from many casual videos. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3 Gengshan Yang, Chaoyang Wang, Dinesh Reddy, and Deva Ramanan. Reconstructing animatable categories from videos. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2023a. 1, 3 Gengshan Yang, Shuo Yang, John Zhang, Zachary Manchester, and Deva Ramanan. Ppr: Physically plausible reconstruction from monocular videos. In Proceedings of International Conference on Computer Vision (ICCV), 2023b. 3 Yandan Yang, Baoxiong Jia, Peiyuan Zhi, and Siyuan Huang. Physcene: Physically interactable 3d scene synthesis for embodied ai. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2024b. 1 Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2021. 20 Li Yi, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su, and Leonidas Guibas. Deep part induction from articulated object pairs. arXiv preprint arXiv:1809.07417, 2018. 3 Ge Zhang, Or Litany, Srinath Sridhar, and Leonidas Guibas. Strobenet: Category-level multiview reconstruction of articulated objects. arXiv preprint arXiv:2105.08016, 2021. 3 Published as conference paper at ICLR 2025 Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 20 Zihang Zhao, Yuyang Li, Wanlin Li, Zhenghao Qi, Lecheng Ruan, Yixin Zhu, and Kaspar Althoefer. Tac-man: Tactile-informed prior-free manipulation of articulated objects. Transactions on Robotics (T-RO), 2024. 1 Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3d: modern library for 3d data processing. arXiv preprint arXiv:1801.09847, 2018. 4 Ziyu Zhu, Zhuofan Zhang, Xiaojian Ma, Xuesong Niu, Yixin Chen, Baoxiong Jia, Zhidong Deng, Siyuan Huang, and Qing Li. Unifying 3d vision-language understanding via promptable queries. In Proceedings of European Conference on Computer Vision (ECCV), 2024. Silvia Zuffi, Angjoo Kanazawa, David Jacobs, and Michael Black. 3d menagerie: Modeling the 3d shape and pose of animals. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 3 17 Published as conference paper at ICLR"
        },
        {
            "title": "A IMPLEMENTATION AND TRAINING DETAILS",
            "content": "Canonical Gaussian Initialzation We train single-state Gaussians G0 and G1 for 10K steps with loss p1λSSIMqLI `λSSIMLD-SSIM`λoLo, where λSSIM 0.2, λo 0.01 is used in experiments and Lo is an opacity entropy loss calculated as: ˆσi 1tσi ą 0.5u, Lo Nÿ rˆσiσi ` p1 ˆσiq logp1 σiqs,"
        },
        {
            "title": "1\nN",
            "content": "i1 which encourages Gaussian opacities σi to approach either 0 or 1, controlling Gaussian count and accelerating training. We then obtain coarse canonical Gaussians by matching G0 and G1 as described in Sec. 4.1. This stage takes about 2 minutes per object. Part Discovery for Articulation Modeling As described in Sec. 4.2, given canonical Gaussians Gc tGiuN i1 and learnable part centers Ck ppk, Rk, λkq, we calculate part-level masks using Eq. (6). We use learnable hash grid to encode Gaussian positions and predict the residual term in Eq. (6) as: Xk rRkpµc pkqs λk , Dik pXk qT Xk Wik MLPpµc , Hpµc q, tX uK k1, tDikuK k1q, GumbelSoftmax ˆ ` τ Since the part assignment and articulation parameters are far from optimal at the beginning of training, using hard assignment for Gumbel-Softmax hinders the joint optimization of the part assignment and articulation parameters. To address this problem, we anneal the temperature τ from 1 to 0.1 over 10K steps, using soft assignment that is similar to Softmax when τ ą 0.1 and hard assignment otherwise for training stability. This approach allows for more flexible assignments during the early stages of training, facilitating better joint optimization, and gradually transitioning to decisive part assignment as the model converges. for two-part objects and ϵstatic 0.05 maxi CDtÑt Optimization To enhance the learning of articulation parameters, we adopt warm-up strategy to predict the joint type of each part. This process requires 3K-5K steps that take 30 to 50 seconds. Then we train ArtGS with joint type constraint for 20K steps, taking 5-7 minutes per object. For hyper-parameters, we set the threshold ϵstatic to identify static/movable Gaussians as ϵstatic 0.02 maxi CDtÑt for multi-part objects. We use ϵrevol 100 for predicting joint types following PARIS (Liu et al., 2023a). λcd and λreg are set as 100 and 0.1 separately. In addition, the CD loss in Eq. (9) aims to decrease the distance between deformed Gaussian Gt single. Since the deformed Gaussians and canonical Gaussians for prismatic joint have large overlap, the nearest Gaussian may be in the opposite direction of the ideal one, making it ineffective for prismatic joints. Thus the CD loss is only used for regularizing the objects that only have revolute joints. Moreover, the densification strategy of Gaussians is cloning or splitting one Gaussian when the gradient of its center µ is greater than threshold ϵdensify. This is effective for static scenes but meets challenges for dynamic scenes. In the early stage of training, the large gradient is often due to deformation error. To prevent excessive increase of Gaussian quantity due to deformation error, we raised this threshold ϵdensify from 0.0002 used in previous works (Kerbl et al., 2023; Huang et al., 2024c) to 0.001. and its nearest Gaussians ˆGt in Gt i"
        },
        {
            "title": "B ADDITIONAL DISCUSSIONS",
            "content": "We present comprehensive analysis of ArtGS and DTA through additional quantitative and qualitative results. Visibility Problem Our results uncover an intriguing inconsistency in DTAs performance across different states of the same object. As illustrated in Tab. A.1, DTA demonstrates good reconstruction quality in the high-visibility state but shows markedly poor performance in the low-visibility state. This limitation is particularly pronounced for objects with prismatic joints, such as real storage and blade. In these cases, DTA struggles to accurately capture the geometry and articulation of partially 18 Published as conference paper at ICLR 2025 Table A.1: Quantitative evaluation of each state on PARIS data. We report the average of metrics over 10 trials of each state. \"metric-0/1\" represents the metric evaluated at state 0/1 and \"metric-m\" is the average of two states. We highlight best results on average of two states. Axis Pos. is omitted for prismatic joints (Blade, Storage, and Real Storage). Metric Method Axis Ang Axis Pos Part Motion CD-s CD-m CD-w DTA-0 Ours-0 DTA-1 Ours-1 DTA-m Ours-m DTA-0 Ours-0 DTA-1 Ours-1 DTA-m Ours-m DTA-0 Ours-0 DTA-1 Ours-1 DTA-m Ours-m DTA-0 Ours-0 DTA-1 Ours-1 DTA-m Ours-m DTA-0 Ours-0 DTA-1 Ours-1 DTA-m Ours-m DTA-0 Ours-0 DTA-1 Ours-1 DTA-m Ours-m Synthetic Objects FoldChair 0.03 0.01 0.04 0.01 0.04 0.01 0.01 0.00 0.01 0.00 0.01 0.00 0.10 0.03 0.09 0.03 0.09 0.03 0.18 0.26 0.19 0.26 0.19 0.26 0.15 0.54 0.13 0.12 0.14 0.33 0.27 0.43 0.26 0.30 0.26 0.36 Fridge Laptop Oven 0.22 0.07 0.09 0.01 0.01 0.03 0.23 0.07 0.10 0.01 0.01 0.03 0.22 0.07 0.10 0.01 0.01 0.03 0.01 0.01 0.01 0.00 0.01 0.00 0.01 0.01 0.01 0.00 0.01 0.00 0.01 0.01 0.01 0.00 0.01 0.00 0.12 0.11 0.12 0.02 0.02 0.04 0.13 0.11 0.13 0.02 0.02 0.04 0.12 0.11 0.12 0.02 0.02 0.04 4.60 0.32 0.62 3.88 0.59 0.52 4.58 0.30 0.63 4.00 0.63 0.48 0.31 4.59 0.62 3.94 0.50 0.61 0.44 0.16 0.27 0.89 0.14 0.21 0.45 0.13 0.30 0.76 0.13 0.21 0.44 0.15 0.28 0.14 0.21 0.82 4.24 0.35 0.70 3.58 0.47 0.58 4.27 0.32 0.70 3.71 0.50 0.59 0.34 4.25 0.70 3.64 0.59 0.48 Scissor 0.10 0.05 0.10 0.05 0.10 0.05 0.03 0.00 0.02 0.00 0.03 0.00 0.38 0.04 0.37 0.04 0.38 0.04 3.30 0.62 3.55 0.61 3.43 0.61 17.38 0.65 10.11 0.64 13.75 0.65 0.42 0.69 0.41 0.67 0.41 0.68 Stapler USB Washer Blade 0.20 0.03 0.26 0.03 0.23 0.03 - - - - - - 0.00 0.00 0.00 0.00 0.00 0.00 0.55 0.54 0.45 0.54 0.50 0.54 2.05 1.12 61.38 1.01 31.72 1.06 0.36 0.61 0.38 0.65 0.37 0. 0.36 0.02 0.36 0.02 0.36 0.02 0.04 0.00 0.05 0.00 0.04 0.00 0.28 0.03 0.28 0.03 0.28 0.03 4.77 6.41 4.56 6.43 4.66 6.42 0.37 1.54 0.45 0.45 0.41 1.00 4.59 6.12 4.48 5.99 4.53 6.05 0.11 0.04 0.11 0.04 0.11 0.04 0.00 0.00 0.00 0.00 0.00 0.00 0.15 0.03 0.14 0.03 0.15 0.03 2.32 2.25 2.90 2.56 2.61 2.41 1.47 1.22 1.51 1.43 1.49 1.33 1.17 1.28 1.52 1.87 1.34 1.58 0.06 0.01 0.07 0.01 0.06 0.01 0.02 0.01 0.02 0.01 0.02 0.01 0.08 0.01 0.08 0.01 0.08 0.01 2.68 3.85 2.91 3.83 2.79 3.84 2.34 0.88 1.13 0.52 1.73 0.70 2.13 3.13 1.92 2.63 2.02 2.88 Storage All 0.13 0.02 0.14 0.02 0.14 0.02 0.02 0.00 0.02 0.00 0.02 0.00 0.13 0.02 0.13 0.02 0.13 0.02 2.41 2.64 2.50 2.67 2.46 2.65 2.50 0.82 7.60 0.63 5.05 0.73 1.83 2.20 1.83 2.21 1.83 2.21 0.07 0.01 0.09 0.01 0.08 0.01 - - - - - - 0.00 0.00 0.00 0.00 0.00 0.00 4.71 7.47 4.90 7.31 4.80 7.39 0.36 1.03 0.36 1.02 0.36 1.02 4.09 5.13 3.99 5.21 4.04 5.17 Real Objects Storage 13.64 3.47 8.08 3.47 10.86 3.47 - - - - - - 0.14 0.04 0.09 0.04 0.12 0.04 10.98 2.93 9.60 4.02 10.29 3.48 30.78 6.28 365.74 87.81 198.26 47.05 8.98 3.23 9.03 2.45 9.01 2. All 7.86 2.78 5.08 2.78 6.47 2.78 0.59 0.47 0.59 0.47 0.59 0.47 1.00 0.99 0.97 0.99 0.99 0.99 6.67 2.29 6.10 3.02 6.39 2.65 15.95 3.47 183.80 44.56 99.88 24.02 5.53 2.26 5.61 1.95 5.57 2.11 Fridge 2.08 2.09 2.07 2.09 2.08 2.09 0.59 0.47 0.59 0.47 0.59 0.47 1.85 1.94 1.85 1.94 1.85 1.94 2.36 1.64 2.59 2.01 2.48 1.82 1.12 0.66 1.85 1.31 1.48 0.99 2.08 1.29 2.19 1.45 2.13 1.37 occluded parts. The observed inconsistency and state-dependent performance fluctuations underscore the necessity for more robust approach that effectively connects and leverages information from multiple states. This is precisely where ArtGSs strengths become evident. By establishing connections between different articulation states, ArtGS achieves more consistent and high-quality reconstructions across varying object configurations. Jointly optimizing over multiple states allows ArtGS to: 1) Leverage complementary information from different articulation states, 2) Maintain consistency in part assignment and geometry across states, 3) Better handle occlusions and low-visibility scenarios by inferring occluded geometries from other states. These capabilities enable ArtGS to produce more accurate and reliable reconstructions, particularly in challenging scenarios. The superior performance of ArtGS demonstrates its potential for robust articulated object reconstruction in real-world applications. Significance of Part Assignment Through analysis of both qualitative (Fig. A.3) and quantitative (Tab. 3) results, we have identified that the models ultimate performance is primarily determined by the accuracy of part assignment. When the model fails to correctly divide an object into parts, it becomes impossible to obtain reasonable joint parameter estimation. Conversely, even when joint parameter estimation is inaccurate, the model may still correctly separate the objects parts. This insight reveals that accurate part assignment is crucial prerequisite for high-quality articulated object reconstruction. Our findings emphasize that to enhance the reconstruction of articulated objects, the ability to reasonably separate parts is of paramount importance. ArtGS addresses this challenge through the center-based segmentation and improved initialization by clustering. These techniques work in synergy to significantly improve the part segmentation capabilities of ArtGS. By enhancing the models ability to correctly identify and separate object parts, we lay solid foundation for 19 Published as conference paper at ICLR Table A.2: Quantitative evaluation of Axis Pos metric on PARIS. Metrics are reported as mean std over 10 trials on average of 2 states. We report the value timed by 1000 and highlight the best results. Metric Method Axis Pos DTA Ours FoldChair 0.530.3 0.480.2 Fridge 0.620.3 0.440.2 Laptop 1.100.7 0.390. Oven 1.491.0 0.550.4 Scissor 2.482.8 0.160.1 Stapler 2.211.8 0.930.4 USB Washer 4.532.8 0.330.3 0.350.2 0.080.1 All 1.661.2 0.420. subsequent stages of the reconstruction process, including joint parameter estimation and final mesh reconstruction."
        },
        {
            "title": "C LIMITATIONS",
            "content": "Stability of Randomness. ArtGS exhibits enhanced robustness and stability across different random seeds, primarily due to our innovative initialization strategy for canonical Gaussians and our part assignment module. We observe that stability issues often stem from the initialization of three key components: canonical Gaussians Gc, part centers in the part assignment module, and joint articulation parameters Ψ. As demonstrated in Sec. 5.3, faulty initialization of Gc and can lead to significant performance degradation, particularly for complex objects with multiple movable parts. While our current initialization strategy has greatly improved stability, severe initialization errors in center may still result in part mis-segmentation. We can integrate prior models such as SAM (Kirillov et al., 2023) to enhance the ability to correct center initialization errors. Although ArtGS works with randomly initialized Ψ, we have observed that improved initialization of Ψ brings enhanced performance. Future work could explore the integration of heuristic algorithms or feed-forward articulation estimation models to provide better initial estimation for Ψ. Limited States Our current approach is limited to modeling articulated objects using only two states, which may not fully capture the complexity of real-world multi-part objects. Moreover, as the number of parts increases, distinguishing parts with similar joint axes and motion patterns (such as parallel drawers) becomes increasingly challenging, complicating the segmentation process. To address this, two main avenues could be explored for future research: 1) Multi-state Extension: Develop methodology to extend ArtGS to handle multiple states that interact with different parts, potentially by identifying movable parts with sequential state update mechanism. This would involve iteratively updating the model as new state information becomes available, allowing for more comprehensive representation of the objects articulation space. 2) Continuous Temporal Reconstruction: Adapt ArtGS to reconstruct articulated objects from monocular video sequences. This approach would leverage temporal information to infer continuous range of articulation states, providing more nuanced understanding of the objects movement capabilities. Mesh Reconstruction Fidelity Our current implementation utilizes the original Gaussian Splatting technique, which, while effective, has limitations in terms of mesh reconstruction quality compared with NeRF-based methods(Wang et al., 2021; Yariv et al., 2021; Wen et al., 2023). Integrating recent advancements in reconstruction with Gaussian Splatting (Huang et al., 2024a; Chen et al., 2024a) may help to improve the reconstruction fidelity of ArtGS."
        },
        {
            "title": "D ADDITIONAL EXPERIMENTS",
            "content": "D.1 ADDITIONAL QUANTITATIVE COMPARISONS We provide additional comparisons with previous methods in this section. Scaled Axis Pos Metric. Following DTA and PARIS, we multiply the Axis Pos metric by 10 in Tab. 1 and Tab. A.1. While this metric shows minimal variation among current methods for synthetic objects, we also report the Axis Pos metric multiplied by 1000. As shown in Tab. A.2, ArtGS demonstrates superior performance compared to DTA. Perception-based Metrics. To evaluate rendering quality, we assess perception-based metrics including LPIPS Zhang et al. (2018), SSIM Wang et al. (2004), and PSNR, with results shown 20 Published as conference paper at ICLR Table A.3: Quantitative evaluation for perception-based metrics on PARIS data. We report the results on average of two states. We highlight best results. Metric Method PSNR SSIM LPIPSvgg PARIS Ours PARIS Ours PARIS Ours Synthetic Objects FoldChair 31.50 34.46 0.985 0.997 0.045 0.036 Fridge Laptop Oven 37.26 37.67 35.30 37.06 34.09 37.11 0.991 0.994 0.980 0.995 0.988 0.993 0.045 0.020 0.032 0.054 0.045 0.041 Scissor 38.37 38.29 0.996 0.998 0.015 0.014 Stapler USB Washer Blade 38.29 38.49 41.16 39.13 0.996 0.995 0.999 0.999 0.017 0.019 0.004 0. 39.07 39.64 0.992 0.998 0.029 0.016 40.08 38.50 0.991 0.995 0.029 0.052 Storage 36.18 37.24 0.993 0.992 0.095 0.097 All 37.22 37.67 0.991 0.995 0.035 0.037 Real Objects Storage 27.13 25.38 0.953 0.930 0.139 0.188 Fridge 25.29 27.05 0.898 0.939 0.188 0. All 26.21 26.22 0.926 0.935 0.164 0.151 Table A.4: Quantitative comparison for whole mesh reconstruction on PARIS data. We report the average of CD-w over 10 trials. We bold best results on average of two states. Metric Method CD-w DTA TSDF with gt depth Ours Synthetic Objects FoldChair 0.26 0.30 0.36 Fridge Laptop Oven 0.34 4.25 0.70 3.60 0.56 0.47 3.64 0.48 0.59 Scissor 0.41 0.49 0.68 Stapler USB Washer Blade 0.37 0.54 0.63 1.34 1.60 1. 4.53 5.73 6.05 2.02 2.78 2.88 Storage All 1.83 2.12 2.21 4.04 5.13 5.17 Real Objects Storage 9.01 131.86 2.84 Fridge 2.13 3.15 1. All 5.57 67.51 2.11 in Tab. A.3. While our primary focus aligns with previous methods on mesh reconstruction and articulation estimation, ArtGS achieves comparable or superior performance relative to PARIS. Limited Improvement for CD-w on Simple Synthetic Objects. Our methods performance on simple synthetic objects, particularly in terms of CD-w metric, is constrained by our use of TSDF for mesh extraction from Gaussian Splatting-rendered depths. To analyze this limitation, we compare against meshes reconstructed using ground-truth depth with TSDF. As shown in Tab. A.4, even with ground-truth depth input, TSDF-based reconstruction cannot surpass algorithms using marching cubes with NeRF, primarily due to the fundamental differences between TSDF and marching cubes algorithms on simple geometries. However, for complex or real-world objects where articulation reconstruction becomes more critical, the advantages of our model become evident. Additionally, TSDF with ground truth depth on real-world objects may produce poor-quality meshes (e.g., real_storage) due to depth sensor noise, while our ArtGS achieves high-quality reconstruction. Importantly, our primary objective is to create digital twins of real-world articulated objects, where ArtGS demonstrates significant performance improvements, particularly for complex and real-world scenarios. D.2 FAILURE CASES Incorrect Initialization of Part Centers. For real-world objects with multiple parts, clusteringderived part centers may be inaccurate (Fig. A.1 (a)) due to sensor noise, occlusion, and varying illumination conditions. These incorrectly initialized centers often persist through optimization, degrading performance for parts with misaligned centers (Fig. A.1 (c)). Manual correction of erroneous part centers prior to training (Fig. A.1 (b)) yields improved results (Fig. A.1 (d)). As discussed in Appendix C, incorporating prior models like SAM for automatic, accurate part center initialization remains promising direction for future work. Similar Motions. Our method exhibits limitations when handling parts with identical motion across states, as demonstrated in case 2 of Fig. A.1 where two drawers are pulled with the same distance. In such scenarios, the model tends to learn single joint to fit both parts, failing to distinguish between the independently movable parts. As discussed in Appendix C, expanding ArtGS to incorporate additional states would provide richer motion information, potentially enabling better part separation. D.3 EVOLUTION OF CANONICAL GAUSSIANS We visualize the evolution of canonical Gaussians in Fig. A.2, showing both their part assignments and centers. Our initialization strategy begins with dense static Gaussians and sparse dynamic Gaussians. As training progresses, the Gaussians undergo densification while simultaneously refining their part centers and assignments. These visualization results demonstrate the effectiveness of ArtGS. D.4 ADDITIONAL QUALITATIVE COMPARISONS We provide additional qualitative comparisons on different datasets in the following pages. 21 Published as conference paper at ICLR Figure A.1: Failure cases. We illustrate failure cases of our ArtGS. Init./Opt. Cano. represents initialized and optimized Canonical Gaussians, while the prefix indicates manual correction of erroneous part centers. Figure A.2: Evolution of canonical Gaussians. We visualize the evolution of canonical Gaussians, showing both their part assignments and centers. Our initialization strategy begins with dense static Gaussians and sparse dynamic Gaussians. As training progresses, the Gaussians undergo densification while simultaneously refining their part centers and assignments. These visualization results demonstrate the effectiveness of ArtGS. 22 Published as conference paper at ICLR 2025 Figure A.3: Additional qualitative results on ArtGS-Multi. Published as conference paper at ICLR 2025 Figure A.4: Interpolation results on PARIS data."
        }
    ],
    "affiliations": [
        "Peking University",
        "State Key Laboratory of General Artificial Intelligence, BIGAI",
        "Tsinghua University"
    ]
}