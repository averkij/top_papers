{
    "paper_title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
    "authors": [
        "Ziling Gong",
        "Yunyan Ouyang",
        "Iram Kamdar",
        "Melody Ma",
        "Hongjie Chen",
        "Franck Dernoncourt",
        "Ryan A. Rossi",
        "Nesreen K. Ahmed"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems."
        },
        {
            "title": "Start",
            "content": "Segment Length Matters: Study of Segment Lengths on Audio Fingerprinting Performance Ziling Gong Data Science Institute Columbia University New York, USA zg2532@columbia.edu Yunyan Ouyang Data Science Institute Columbia University New York, USA yo2384@columbia.edu Iram Kamdar Data Science Institute Columbia University New York, USA ik2594@columbia.edu Melody Ma Data Science Institute Columbia University New York, USA ym3065@columbia.edu Hongjie Chen Dolby Laboratories Atlanta, USA 0000-0002-8755Franck Dernoncourt* Adobe Research Seattle, USA dernonco@adobe.com Ryan A. Rossi* Adobe Research San Jose, USA ryrossi@adobe.com Nesreen K. Ahmed Cisco Research San Jose, USA nesahmed@cisco.com AbstractAudio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems. Index Termsaudio segmentation, audio fingerprinting, audio segment length I. INTRODUCTION Audio segmentation is widely adopted in various audio tasks, including audio fingerprinting, speaker diarization, keyword spotting, speech translation, among others [1][5]. While each task has conventional range of segment lengths that the widespread use of subsequent work typically follows, length is different settings implicitly reflects that segment determining factor in the task performance. For example, speaker identification commonly uses 3-second segments [6]. Keyword spotting models operate on segments of 0.51.0 seconds [7], [8]. Speaker diarization systems typically adopt 1.53 seconds to avoid multi-speaker overlap while at the same time preserving speaker-discriminative information [3], [9], [10]. In the task of ASR, the segmentation exists as context windows, which are often 5-30 seconds but can be up to hours long [11], [12]. Moreover, segmentation for generating speech *Acted in an advisor capacity only; did not process, store, or direct the use of project data. Fig. 1: Our motivation question: What is the optimal segment length for the audio fingerprinting task? language tokens has also been explored [13]. On the task of audio fingerprinting, however, it remains unexplored how segment length affects model performance. To bridge the gap, this paper investigates how different segment lengths impact audio fingerprinting performance, as depicted in Fig. 1. Specifically, we consider audio fingerprinting in the context of audio identification, where large collection of audio recordings are segmented and fingerprinted to construct database. In the following query stage, query audio is segmented and fingerprinted. The resulting fingerprints are used to retrieve the closest segments from the database, with the goal of identifying the original version of the query audio. We experiment with popular neural audio fingerprinter, NAFP [14], which applies 1-second segmentation to audio in both training and query stages. In order to adopt different segment lengths, we propose NAFP variant, named NAFP+. We design four sets of experiments, including NAFP reproduction and three other experiments using {0.5, 1, 2}- second segment lengths with NAFP+. For each experiment set, we train an individual model with different segment 6 2 0 2 5 2 ] . [ 1 0 9 6 7 1 . 1 0 6 2 : r TABLE I: Summary of notation"
        },
        {
            "title": "Description",
            "content": "a wstf Dref audio mel spectrogram fingerprint dimension STFT window size hop size whole audio dataset reference set L Ddist sampling rate #STFT frames segment length query length #query segments query set distractor set length and test with various query lengths. Our results show that for short query lengths (shorter than 3 seconds), small segment lengths (0.5 seconds) achieve significantly better performance. Moreover, we investigate the capacity of Large Language Models (LLMs) in suggesting segment lengths for audio fingerprinting. We evaluate three LLMs, namely, GPT-5mini, Gemini-2.5-flash, and Claude-Sonnet-4.5. We carefully design five prompts and collect responses from LLMs, which show that GPT-5-mini suggests the best segment length for audio fingerprinting. To summarize, this paper has two key findings: (1) short segment length yields better audio fingerprinting performance and (2) GPT-5-mini provides the best segment length suggestions among the three studied LLMs. Related Work. Audio fingerprinting aims to generate compact and distinctive representation of an audio signal that enables efficient identification or retrieval from large databases. Traditional fingerprinting systems rely on handcrafted features, such as spectral peaks [15], while recent deep learning approaches directly extract embeddings from audio signals [16] [18]. For example, recent Neural Audio FingerPrinter (NAFP) model leverages contrastive learning to generate segment embeddings [14], [19]. following work builds on top of it with segment aggregation strategy, resulting in coarselevel segments [16]. The coarser chunk corresponds to tens of seconds (1530 seconds), yielding fewer embeddings per track. This saves storage and retrieval time, however, at the cost of lower accuracy. Another work focuses on improving neural fingerprinting robustness through systematic training practices using large degraded datasets [20]. Their results underscore the influence of training methodology on retrieval accuracy given fixed segmentation. However, these papers have not investigated the impact of different segment lengths, which motivates our study on how different segment lengths may impact the performance of audio fingerprinting. II. EXPERIMENTS WITH NAFP AND NAFP+ This section describes the experimental setups with both the original NAFP and our proposed model variant NAFP+. A. Preliminary Let denote an audio recording and its sampling rate. We use to denote the segment length. For evaluation, we let = Dref Ddist denote collection of audio recordings, where Dref denotes reference set that derives an audio query set Q, while Ddist denotes distractor set providing unrelated candidates. Each audio is queried with query length L. moving window size of and hop of is applied to this L-second (sub)audio to generate query segments. Let denote the number of resulting query segments, hence, q1, q2, . . . , qS. B. NAFP+ Design Given an audio a, we apply sliding window size of {0.5, 1, 2} seconds and hop of = 0.5 seconds to obtain training segments. We then apply mel-spectrogram transformation to each segment, resulting in an input RF , where = 256 is user-selected number of Melfrequency bins. is the number of resulting frames from Short-Time Fourier Transform with window wstf = 256, = wstf + 1 = 8000 256 + 1 (1) Specifically, when is 0.5, 1, and 2 seconds, the corresponding is 16, 32, and 63. For NAFP, the mel spectrogram RF goes through chain of eight convolution blocks. Each block consists of temporal convolutional layer, frequency convolution layer, ELU activation, and normalization layer. L2-normalization is applied to the final fingerprint of selected dimension (d = 128). We follow the same augmentation and training scheme as NAFP [14], where an audio and its distorted replica are considered positive pair in contrastive learning. In order to adopt segment lengths other than 1-second, we propose model variant named NAFP+. To minimize changes to the convolutional blocks in NAFP, we derive NAFP+ by adding fully-connected layer with ELU activation before each block. This modification normalizes the number of audio frames from to fixed number T0 = 32. C. Evaluation We evaluate model performance on an audio identification task. We consider an audio dataset = Dref Ddist and partition it into reference set Dref and distractor set Ddist. The reference set Dref is used to derive query set Q, while Ddist serves as unrelated candidates to prevent the retrieval task from being trivial. Our objective is to identify the correct original audio of from D, particularly from Dref . For query audio, instead of using all of its audio frames, query length of seconds is applied to each query audio to select (sub)audio for the query. The selected portion is further segmented with -second windows and h-second hop size, resulting in segments where = (cid:23) (cid:22) + 1. (2) For instance, if the query length is = 3 seconds, there are = (3 1)/0.5 + 1 = 5 segments, with = 1 and = 0.5 seconds. For each of the resulting segments, namely, q1, q2, . . . , qS, we retrieve its closest segments and retrieve the corresponding audio from weighted segment majority voting. TABLE II: Hit Rate (%) across Segment Lengths and Query Lengths L. Bold indicates the best result within each metric group for fixed query length L. Win reports the number of query lengths where method achieves the best performance. Metric Model Segment Length Top1 Exact Top3 Exact Top10 Exact Top1 Near NAFP NAFP+ NAFP NAFP+ NAFP NAFP+ NAFP NAFP+ 1 0.5 1 2 1 0.5 1 1 0.5 1 2 1 0.5 1 2 Query Length 0.5 - 68.50 - - - 75.45 - - - 78.80 - - - 72.10 - - 1 2 3 5 6 7 8 9 73.05 85.70 75.55 - 81.00 89.10 81.65 - 84.10 90.40 84.40 - 78.00 87.20 78.35 - 90.60 94.30 91.65 72.05 93.47 95.80 93.45 78. 96.50 96.45 94.65 81.05 91.70 94.70 92.35 76.25 95.76 96.70 96.40 85.45 96.90 97.80 97.40 87.75 97.55 98.00 97.65 89.15 96.30 96.80 96.40 86. 97.35 97.95 97.70 89.60 98.45 98.60 98.45 91.65 98.80 98.75 98.70 92.55 97.55 98.00 97.70 90.15 98.45 98.75 98.45 92.00 99.00 99.05 98.80 93. 99.25 99.20 99.00 93.90 98.55 98.75 98.45 92.35 99.00 98.90 98.90 93.20 99.30 99.30 99.15 94.50 99.35 99.55 99.30 94.80 99.00 98.95 98.90 93. 99.30 99.35 99.20 94.15 99.50 99.55 99.35 95.05 99.50 99.80 99.45 95.30 99.30 99.35 99.25 94.35 99.45 99.55 99.40 95.15 99.60 99.70 99.55 95. 99.60 99.85 99.55 95.85 99.50 99.55 99.45 95.20 99.65 99.65 99.60 95.40 99.75 99.75 99.65 95.95 99.80 99.90 99.70 95.95 99.65 99.65 99.60 95. 99.70 99.65 99.70 95.60 99.75 99.75 99.75 96.00 99.80 99.85 99.75 96.05 99.70 99.65 99.70 95.70 Win (#/10) - 8.5 / 10 1.5 / 10 0 / - 9.5 / 10 0.5 / 10 0 / 10 - 10 / 10 0 / 10 0 / 10 - 9 / 10 1 / 10 0 / 10 Fig. 2: Hit Rate (%) of Various Segment Lengths (Green: = 0.5, Blue: = 1, Orange: = 2) and Query Lengths L. lasts for 30 seconds. The dataset Dataset. We use derived music dataset from the Free Music Archive collection [14], [21]. Each audio in the dataset is musical piece that is partitioned into three sets: training set with 10,000 audio clips (83.3 hours), reference audio set Dref with 500 audio clips (4.2 hours), and distractor set Ddist with 9,978 audio clips (83.3 hours). query set is derived from the reference set by synthesizing with held-out subset of background noises, augmenting with time offsets, and simulating of room impulse responses. Metrics. We measure Top-K Exact Hit, which considers whether the correct retrieval is within top-K predictions, and Top-K Near Hit, which allows an extra 1 frame misalignment. We report the Top-K Hit Rate across query segments, i.e., average Top-K-Exact Hit for {1, 3, 10} and Top1-Near Hit. Higher Hit Rate means better performance and hence more favorable. III. RESULTS We report results of NAFP and NAFP+ on segment lengths {0.5, 1, 2} seconds and query lengths {1, 2, . . . , 10} seconds, as shown in Table II. We have multiple observations: length = 0.5 seconds achieves the best (1) Segment performance on most query lengths, with higher than 8 / 10 win rate in any metric group. In contrast, = 2 seconds has the worst performance across query lengths. (2) For small query lengths (L 3), shorter segment lengths consistently achieve significantly higher Hit Rates across metrics. As query length increases beyond 3 seconds, the performance gap between = 0.5 seconds and = 1 second narrows substantially. This suggests that additional temporal context compensates for coarser segmentation. (3) We further visualize the results in Fig. 2. The figure shows that performance improves rapidly with additional segments and begins to saturate after 4 seconds, and only very slight improvement afterwards. A. Segment Length Selection with LLMs We examine the capacity of LLMs in suggesting the best segment lengths, i.e., the segment length that yields the highest hit rates. We evaluate three models, namely, GPT-5-mini, Gemini-2.5-flash, and Claude-Sonnet-4.5, with five different prompts. Each prompt is constructed as combination of fixed context prompt and question prompt. The fixed context prompt includes baseline task description, dataset context based on FMA and large-scale retrieval, and concrete TABLE III: LLM-recommended segment lengths across 5 prompts across different LLMs. # Prompt GPT-5-mini Gemini-2.5-flash Claude-Sonnet-4.5 1 What segment duration best balances temporal resolution and discriminative power in this setup? 2 When audio segments are affected by background noise and small time shifts, what segment duration yields the most stable fingerprints without losing detail? Pick segment duration in the 12 second range. It gives good tradeoff between temporal resolution and discriminative power for music fingerprinting Aim for about 1 second per segment (a practical range 0.81.5 s, with 1.0 the usual sweet spot). In large-scale contrastive audio fingerprinting system, what segment duration provides the best trade-off between retrieval accuracy, speed, and storage cost? second is 1 trade-off for high-specificity audio-fingerprinting system. the best default large-scale, contrastive Optimal Range: 3-5 seconds. This duration is typically long enough to capture meaningful musical phrases, harmonic progressions, and rhythmic grooves that define unique musical identity. segment duration of 2 to 3 seconds is often good starting point, providing strong balance between stability, detail, and computational efficiency. The optimal segment duration for audio fingerprinting typically falls between 1-3 seconds, with many systems using around 2 seconds as good balance point. segment duration of around 1-2 seconds typically provides the best balance between fingerprint stability and temporal resolution. 3 to 5 seconds is strong starting point. This range offers good compromise between capturing sufficient musical context, managing storage costs, and maintaining acceptable indexing speeds. The optimal segment duration in contrastive audio fingerprinting system typically falls between 1-3 seconds, with many systems using around 2 seconds as good compromise. In large-scale contrastive system, what duration provides the best trade-off between accuracy, speed, and storage cost? Use segments in the 12 second range (practical sweet spot about 1.01.5 s; 2 if you prefer fewer embeddings/storage at small accuracy cost). segment duration in the range of 2 to 5 seconds with 50% overlap often provides the best trade-off. 5 When training with fixed-duration segments but testing with variableduration queries, which segment duration generalizes best across query lengths? Short, roughly 1-second segments (or at most 12 s) give the best overall generalization when you train with single fixed length but must handle segment variable-length queries. 3 to 8 seconds is commonly cited and empirically supported sweet spot, offering the best generalization across range of query lengths due to its balance of musical context and flexibility. The optimal segment duration in contrastive neural audio fingerprinting typically falls between 1-3 seconds, with many systems using 1.5-2 seconds. The 1-2s range typically provides the best generalization across varying query lengths while maintaining practical efficiency. 3 4 Summary 1-2 seconds (consistent) 28 seconds (fluctuating) 13 seconds (centered at 2 secs) Prompt Template Segment duration refers to the duration of each fixed-length audio segment measured in seconds in contrastive neural audio fingerprinting model designed for high-specific retrieval. The model generates one embedding per segment, which is later used for similarity-based matching in large-scale music database. Dataset context: Training uses 30-second clips from the Free Music Archive (FMA) dataset with strict train, validation, and test splits. Evaluation involves large-scale retrieval, where each track in the database is segmented into fixed windows and the query is matched by nearest-neighbor search in the embedding space. Example: Consider 1-second segments with 0.5-second hop capturing short melodic phrase or drum pattern used to identify its source track in large database under background noise and small time shifts. {Question Prompt} Fig. 3: Prompt Template: Fixed Context Prompt + One of the Five Question Prompts from Table III. segmentation example illustrating short overlapping segments under background noise and small time shifts, as depicted in Fig. 3. The question prompts are designed from five considerations, including (1) balancing temporal resolution and discriminative power, (2) stability under background noise and small time shifts, (3) the trade-off between retrieval accuracy, speed, and storage cost in large-scale systems, (4) simplified formulation of the accuracyspeedstorage trade-off, and (5) generalization when training on fixed-duration segments but testing on variable-duration queries. The corresponding question wording prompts are provided in Table III. We compare LLM recommendations across the five question prompts and observe that Gemini exhibits higher run-to-run variability under the same prompt. In contrast, GPT-5-mini consistently recommends segment length of approximately 1 second across different question wordings, whereas Gemini and Claude more frequently suggest longer durations around or above 2 seconds. When compared with our empirical findings, the 1-second recommendation aligns more closely with the optimal accuracy, which indicates that GPT-5-mini gives the best segment length recommendations. IV. CONCLUSION In this paper, we investigate the impact of different segment lengths on the performance of the audio fingerprinting task. The performance gain saturates after 4-second queries regardless of the selected segment lengths. We further investigate LLM capacity in suggesting segment lengths with [17] A. Bhattacharjee, S. Singh, and E. Benetos, Grafprint: gnn-based approach for audio identification, in ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025, pp. 15. [18] A. Singh, K. Demuynck, and V. Arora, Attention-based audio embeddings for query-by-example, arXiv preprint arXiv:2210.08624, 2022. [19] Y. Fujita and T. Komatsu, Audio fingerprinting with holographic reduced representations, arXiv preprint arXiv:2406.13139, 2024. [20] R. O. Araz, G. Cortes-Sebastia, E. Molina, J. Serra, X. Serra, Y. Mitsufuji, and D. Bogdanov, Enhancing neural audio fingerprint robustness to audio degradation for music identification, arXiv preprint arXiv:2506.22661, 2025. [21] M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson, Fma: dataset for music analysis, arXiv preprint arXiv:1612.01840, 2016. well-designed prompts. GPT-5-mini consistently recommends 1-second segment length across different question prompts, aligning closely with our empirical results, whereas Gemini2.5-flash and Claude-Sonnet-4.5 exhibit higher variability and tend to suggest longer durations. These results indicate that appropriately contextualized LLMs reflect empirically grounded design choices, though their reliability varies across models. Future research directions include examining whether these findings generalize to other audio domains beyond music, such as speech or environmental sounds. Moreover, segment lengths may also impact other audio tasks, such as speaker verification, keyword spotting, etc., which can be further explored. Finally, research potential also lies in developing automated strategies for segment length selection based on query length or audio content, with LLMs or lightweight predictors as tools."
        },
        {
            "title": "REFERENCES",
            "content": "[1] T. Theodorou, I. Mporas, and N. Fakotakis, An overview of automatic audio segmentation, International Journal of Information Technology and Computer Science (IJITCS), vol. 6, no. 11, p. 1, 2014. [2] I. Tsiamas, G. I. Gallego, J. A. R. Fonollosa, and M. R. Costa-juss`a, SHAS: Approaching optimal Segmentation for End-to-End Speech Translation, in Proc. Interspeech 2022, 2022, pp. 106110. [3] A. Plaquet, N. Tawara, M. Delcroix, S. Horiguchi, A. Ando, and S. Araki, Mamba-based segmentation model for speaker diarization, in ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025, pp. 15. [4] W. S. Teo and Y. Minami, Self-information guided speech segmentation for efficient streaming asr, in ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025, pp. 15. [5] J. Lee, S. Kim, H. Kim, and J. S. Chung, Lightweight audio segmentation for long-form speech translation, arXiv preprint arXiv:2406.10549, 2024. [6] A. Nagrani, J. S. Chung, and A. Zisserman, Voxceleb: large-scale speaker identification dataset, arXiv preprint arXiv:1706.08612, 2017. [7] O. Rybakov, N. Kononenko, N. Subrahmanya, M. Visontai, and S. Laurenzo, Streaming keyword spotting on mobile devices, arXiv preprint arXiv:2005.06720, 2020. [8] Y. Zhang, N. Suda, L. Lai, and V. Chandra, Hello edge: Keyword spotting on microcontrollers, arXiv preprint arXiv:1711.07128, 2017. [9] C. Zhang, L. Luo, H. Peng, and W. Wen, Variable segment length and domain-adapted feature optimization for speaker diarization, in Proc. Interspeech 2024, 2024, pp. 37443748. [10] A. O. Hogg, C. Evers, and P. A. Naylor, Multichannel overlapping speaker segmentation using multiple hypothesis tracking of acoustic and spatial features, in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 2630. [11] W. Wang, T. Park, I. Medennikov, J. Wang, K. Dhawan, H. Huang, N. R. Koluguri, J. Balam, and B. Ginsburg, Speaker targeting via self-speaker adaptation for multi-talker asr, arXiv preprint arXiv:2506.22646, 2025. [12] R. Flynn and A. Ragni, How much context does my attention-based asr system need? arXiv preprint arXiv:2310.15672, 2023. [13] S. Kando, Y. Miyao, and S. Takamichi, Exploring the effect of segmentation and vocabulary size on speech tokenization for speech language models, arXiv preprint arXiv:2505.17446, 2025. [14] S. Chang, D. Lee, J. Park, H. Lim, K. Lee, K. Ko, and Y. Han, Neural audio fingerprint for high-specific audio retrieval based on contrastive learning, in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 30253029. [15] M. C. McCallum, Foreground harmonic noise reduction for robust audio fingerprinting, in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 31463150. [16] Y. Su, W. Hu, F. Zhang, and Q. Xu, AMG-embedding: selfsupervised embedding approach for audio identification, in ACM Multimedia 2024, 2024. [Online]. Available: https://openreview.net/ forum?id=H7etFJugLW"
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Cisco Research",
        "Data Science Institute Columbia University",
        "Dolby Laboratories"
    ]
}