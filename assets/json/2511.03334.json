{
    "paper_title": "UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions",
    "authors": [
        "Guozhen Zhang",
        "Zixiang Zhou",
        "Teng Hu",
        "Ziqiao Peng",
        "Youliang Zhang",
        "Yi Chen",
        "Yuan Zhou",
        "Qinglin Lu",
        "Limin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 4 3 3 3 0 . 1 1 5 2 : r UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions Guozhen Zhang1,,* Zixiang Zhou2, Teng Hu3 Ziqiao Peng4 Youliang Zhang5 Yi Chen2 Yuan Zhou2 Qinglin Lu2 Limin Wang1,6, 1State Key Laboratory for Novel Software Technology, Nanjing University 2Tencent Hunyuan 3Shanghai Jiao Tong University 5Tsinghua University 4Renmin University of China 6Shanghai AI Lab zgzaacm@gmail.com lmwang@nju.edu.cn https://mcg-nju.github.io/UniAVGen/"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, unified framework for joint audio and video generation. UniAVGen is anchored in dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by Face-Aware Modulation (FAM) module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance (MA-CFG), novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGens robust joint synthesis design enables seamless unification of pivotal audio-video tasks within single model, such as joint audio-video generation and continuation, video-toaudio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency. *Work is done during internship at Tencent Hunyuan. Guozhen Zhang and Zixiang Zhou contribute equally to this work. Corresponding author. 1 Joint audio-visual generation has emerged as pivotal trend in state-of-the-art generative AI. Commercial solutions such as Veo3 [10], Sora2 [29], and Wan2.5 [8] have achieved exceptional generation fidelity and demonstrated notable practical utility. However, most existing open-source methods [4, 5, 18, 23, 35, 39] still rely on decoupled pipelines, often leveraging two-stage paradigm. One paradigm first generates silent video, then performs separate audio synthesis for post-hoc dubbing [5, 39]; the other first generates an audio track to drive subsequent video synthesis [4, 18, 23]. Regardless of the order, such sequential frameworks inherently suffer from critical limitations: modality decoupling impedes cross-modal interplay during generation, resulting in inadequate timbre consistency and emotional alignment. Consequently, designing effective audio-video alignment grows overly complex, often yielding suboptimal performance. Recent works have also explored end-to-end joint audiovideo generation [15, 20, 25, 27, 38, 42]. However, existing methods are either confined to generating ambient sounds and fail to synthesize natural human speech [15, 20, 25, 38], or struggle to attain robust audio-visual alignment [27] and produce content lacking fine-grained temporal audio-visual synchronization [42]. Taken together, to date, there remains lack of open-source, highly generalizable, and wellaligned audio-video generation methods. To address the aforementioned challenges, we introduce UniAVGena unified framework tailored for joint audiovideo generation. We prioritize human audio generation not only because this direction remains underexplored in existing works, but also because of its superior controllability and significant practical utility. As shown in Fig. 1, our framework seamlessly and efficiently adapts to related conditional generation tasks, such as video-to-audio dubbing mechanism, incorporating modal-specific designs to enhance cross-modal consistency in joint generation. We propose face-aware modulation module to dynamically constrain the regions of cross-modal interaction for more efficient and aligned cross-modal learning. We present modality-aware classifier-free guidance, novel strategy that selectively amplifies cross-modal dependencies during inference. Leveraging its robust architectural design, UniAVGen can be seamlessly extended to multiple audio-video generation tasks and demonstrates state-of-the-art performance. 2. Related Work To enable aligned audio-video generation, the research community has explored three primary paradigms: audiodriven video synthesis, video-to-audio synthesis, and joint audio-video generation. Audio-driven video synthesis. This dominant paradigm typically adopts two-stage pipeline. First, Text-toSpeech (TTS) model [2, 3, 12, 22, 32] synthesizes desired audio waveforms from speech content. Subsequently, separate video synthesis model generates video frames conditioned on this audio, with focus on precise lip synchronization [4, 9, 23, 33, 34, 48]. While effective for lip synchronization, this cascaded design suffers from inherent modal decoupling: audio is generated without non-verbal cues, leading to poor semantic consistency between audio and video. Video-to-audio synthesis. The reverse paradigm [5, 21, 28, 36, 39, 44, 49] aims to generate spatiotemporally aligned audio for silent videos guided by captions. However, it retains two key limitations: first, current methods primarily focus on ambient audio dubbing and lack the ability to synthesize natural human audio; second, it inherits the critical flaw of modal decouplingvideos are generated in an auditory vacuum, unaware of the audio they will eventually pair with. Joint audio-video generation. The most holistic paradigm synthesizes audio and video simultaneously within single unified framework [15, 20, 25, 27, 38, 42, 43, 47, 50]. Unfortunately, most prior open-source works [15, 20, 25, 38, 43] target general joint audio-video generation rather than human audio specificallyfailing to produce highquality human audio and offering limited practical value. Notably, recent concurrent worksUniVerse-1 [42] and Ovi [27]have begun to support human audio generation. UniVerse-1 stitches two pre-trained audio and video generation models; due to architectural asymmetry, this stitching is complex and yields limited overall performance. Ovi employs symmetric dual-tower architecture for joint generation, delivering strong performance. However, it lacks modal-specific cross-modal interaction designs and humanspecific modulation, resulting in limited generalization in Figure 1. Multi-task compatibility of UniAVGen. Leveraging its robust design, UniAVGen can simultaneously tackle three pivotal tasks: (a) joint audio-video generation, (b) video-to-audio dubbing, and (c) audio-driven video synthesis. and audio-driven video synthesis. This versatility enables us to unify pivotal audio-video generation tasks under single paradigm, eliminating the need for task-specific model designs. Specifically, UniAVGen is anchored in symmetric dual-branch joint synthesis architecture, featuring two parallel Diffusion Transformer (DiT) [31, 45] streamsone dedicated to video, the other to audiowith identical architectural designs. Crucially, this symmetry establishes representational parity and fosters cohesive latent space, pivotal for synchronizing joint audio-video generation. To better tackle the intricacies of audio-video joint synthesis task, we augment this core architecture with three targeted innovations, as detailed below. First and foremost, at the core of our framework lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-modal attention. Equipped with two modal-specifically designed alignersaudio-to-video and video-to-audiothis mechanism injects fine-grained audio semantics into the video stream for precise synchronization, while imparting the temporal dynamics and identity details from video to the audio. To further strengthen this cross-modal synergy and ground it in human-related features, we introduce FaceAware Modulation module. Specifically, this component dynamically infers mask for facial regions using decaying supervision signals, and constrains cross-modal interaction with gradually relaxed scope. Additionally, to enhance the expressive fidelity of generated content, we propose Modality-Aware Classifier-Free Guidancea novel strategy that explicitly amplifies cross-modal correlation signals during the classifier-free guidance stage. This targeted enhancement significantly boosts emotional intensity in audio and motion dynamics in video, enhancing the overall realism of the generated content. Our key contributions are summarized as follows: We present UniAVGen, unified audio-video generation framework anchored in dual-branch joint synthesis architecture and an asymmetric cross-modal interaction 2 Figure 2. Architecture of UniAVGen: dual-branch joint synthesis framework with asymmetric cross-modal interaction, augmented by face-aware modulation. Taking reference image and text prompt as input, it enables coherent audio-video generation. In contrast, our UniAVGen adout-of-domain testing. dresses these gaps by integrating asymmetric cross-modal interaction and face-aware modulation; it thus achieves superior semantic synchronization and robust generalization capabilities. 3. Method Our proposed method, UniAVGen, is unified framework for high-fidelity audio-video generation. UniAVGen takes as input reference speaker image ref, video prompt (a caption describing the desired motion or expression), and speech content (the text to be spoken). Additionally, it supports specifying target voice via an optional reference audio clip aref, and enables audio-video continuation by conditioning on existing audio acond and video vcond . 3.1. Overview The architecture of UniAVGen is illustrated in Fig. 2. First, we introduce dual-branch joint synthesis framework grounded in symmetric design. For efficient training, we directly adopt the Wan 2.2-5B video generation model [41] as the backbone for the video branch. For the audio branch, we employ the architectural template of Wan 2.1-1.3Bthis shares an identical overall structure with Wan 2.2-5B, differing only in the number of channels. This symmetric strategy ensures both branches start with equivalent representational capacity and establishes natural correspondence between feature maps across all levels. 3 Such structural parity serves as the cornerstone for enabling effective cross-modal interactions, thereby boosting both audio-video synchronization and overall generative quality. Video branch. The video branch operates entirely in the latent space. Specifically, videos are first processed at 16 frames per second and encoded into latent representations zv using the pre-trained Variational Autoencoder (VAE) from [41]. The reference speaker image ref and conditional video are also encoded into latent embeddings zvref and zvcond , respectivelywith the video branchs input formed by concatenating these three latent components = [zvref ]. For the video caption v, it is ˆv encoded via umT5 [6] into ev, and its embeddings are fed into the Diffusion Transformer (DiT) through crossattention. Following [41], we adopt the Flow Matching paradigm [24]: here, the model uθv is trained to predict the vector field vt, with the training objective formulated as: 0 , zvcond , zv Lv = (cid:13) (cid:13)vt(zv ) uθv (z ˆv , t, ev)(cid:13) 2 (cid:13) . (1) Audio branch. Following the common practice in text-toaudio (TTS) [3], audios are first sampled at 24,000 Hz and converted into Mel spectrograms, which serve as the audio latent representation za. Similarly, the reference audio aref and conditional audio acond are also transformed into their respective latent counterparts zaref and zacond . These three latent components are then concatenated along the temporal dimension to form the audio branchs input zˆa = [zaref ]. The training objective for the audio branch 0 , zacond , za 0 Figure 3. Comparison of cross-modal interaction mechanisms: (a) Global Interaction is simple but poses challenges for convergence; (b) Symmetric Time-Aligned Interaction converges quickly but has limited context utilization; (c) Our Asymmetric Cross-Modal Interaction achieves superior balance between convergence speed and performance through modal-specific interaction design. is formulated as: La = (cid:13) (cid:13)vt(za ) uθa (zˆa , t, ea)(cid:13) 2 (cid:13) , (2) where ea denotes the features of the speech content Ta extracted via ConvNeXt [46] Blocks. These features are further injected into the DiT layers through cross-attention, ensuring the audio generation process is tightly coupled with the acoustic information of the target audio. 3.2. Asymmetric Cross-Modal Interaction While the dual-branch structure establishes structural parity, achieving robust audio-video synchronization demands deep cross-modal interaction. Prior works have primarily employed two designs for this: The first is global interaction [27, 42], as shown in Fig. 3(a), where each token of the current modality interacts with all tokens of the other. While simple, it requires high training costs to converge to strong performance due to lacking explicit temporal alignment. The second is symmetric time-aligned interaction [38], as shown in Fig. 3(b), where each video token reciprocally interacts with audio tokens in its corresponding interval. Such methods typically converge faster but access limited contextual information during interaction. To better balance convergence speed and performance, we introduce novel Asymmetric Cross-Modal Interaction mechanism, comprising two specialized aligners tailored to each modalitys unique characteristics. Audio-to-video (A2V) aligner. The A2V aligner ensures precise semantic synchronization by injecting fine-grained audio cues into the video branch. We first reshape the hidthe video den features to align their temporal structure: tokens RLvD are reshaped to ˆH RT vD (where denotes the number of video latent frames and Nv is the number of spatial tokens per frame), and the audio tokens RLaD are reshaped to ˆH RT aD. Unlike Fig. 3(b), we create contextualized audio representation for each video frame, recognizing that visual 4 = [ ˆH iw, . . . , ˆH articulation is influenced by preceding and succeeding phonemes. For the i-th video latent, we construct an audio context window i+w] by concatenating audio tokens from neighboring frames within window of size w. Boundary frames are padded by replicating the features of the first or last frame. Subsequently, we perform frame-wise cross-attention, where the video latent for each frame queries the corresponding contextualized audio latent: , . . . , ˆH i = [ ˆH + CrossAttention(Q = Wq ˆH , k )]. , = = (3) Video-to-audio (V2A) aligner. Conversely, the V2A aligner embeds the generated audio with attributes derived from the speakers visual identity (e.g., timbre, emotion). Here, temporal mismatch is more pronounced, as each video latent maps to block of audio tokens. To achieve granular alignment that captures smooth visual transitions, we adopt temporal neighbor interpolation strategy. For each audio token (corresponding to video latent = j/k), we compute unique interpolated video context a weighted average of latents from two temporally adjacent video latents: frame and the subsequent frame i+1: = (1 α) ˆH i + α ˆH i+1, where α = (j mod k)/k. (4) For the final block of audio tokens, we simply use = ˆH 1. This interpolated context provides smooth, timeaware visual signal. Finally, we perform cross-attention where each audio latent queries its corresponding interpolated video context: (cid:104) ˆH (cid:16) = j + CrossAttention K = = v , = ˆH , (cid:1)(cid:3) . (5) Finally, and are reshaped to match the dimensions of and v, respectively, and injected back as additional features: = + v, = + a. (6) (7) To avoid compromising the generative capability of each modality at the start of training, the output matrices and are both zero-initialized. 3.3. Face-aware modulation For audio-video joint generation, the critical semantic coupling is overwhelmingly concentrated in the facial region. Forcing the interaction to process the entire scene is inefficient and risks introducing spurious correlations that destabilize background elements during early training. To address this, we propose Face-Aware Modulation module that dynamically steers interaction toward the facial area. Dynamic mask prediction. We introduce lightweight auxiliary mask-prediction head operating on video features vl within each interaction layer of the denoising network. This head applies layer normalization [1], learned affine transformation [30], linear projection, and sigmoid activation to generate soft mask (0, 1)T Nv : = σ (Wm (γ LayerNorm(H vl ) + β) + bm) , (8) (cid:13)M gt(cid:13) (cid:13) 2 (cid:13) where is the element-wise product. To ensure the predicted mask provides human-aware guide for interaction, we supervise it not only via the final denoising loss but also with an additional mask loss λmLm = λm (cid:80) using the ground-truth face mask gt [11]. Meanwhile, to avoid over-constraining crossmodal interaction in later training stages, λm gradually decays to 0 over time. Mask-guided cross-modal interaction. The predicted face mask refines cross-modal attention in our asymmetric aligners through two distinct mechanisms: (1) A2V interaction: We employ the mask for selective updates: vl = vl + H vl, (9) where vl denotes the output of A2V cross-attention at layer l. This ensures audio information precisely modulates salient regions without disrupting backgrounds during early training. (2) V2A interaction: To enable to strengthen information transfer from the videos salient regions to the audio branch, we modulate the video features ˆH vl as ˆH vl = ˆH vl prior to computing Eq. (4). 3.4. Modality-aware classifier-free guidance Classifier-Free Guidance (CFG) [17] is cornerstone technique for enhancing conditional fidelity in generative models. However, its conventional design is inherently unimodal. Naively applying it to joint synthesiswhere each branch is independently guided by its text promptfails to amplify critical cross-modal dependencies. The guidance signal for audio-driven video or video-influenced audio is not explicitly enhanced, limiting the models audioTo address this, we propose visual synchronization. Modality-Aware Classifier-Free Guidance (MA-CFG), novel scheme that repurposes the guidance mechanism to strengthen cross-modal conditioning. Our key insight is that single, shared unconditional estimate can serve as the baseline for guiding both modalities simultaneously. This is achieved by performing one forward pass where the conditioning signals for both cross-modal interactions are nullified, which is equivalent to unimodal inference. Specifically, we define the unconditional estimate for the audio and video modalities (without cross-modal interaction) as uθa and uθv , and the estimate with cross-modal interaction as uθa,v . Then, MA-CFG for each modalities can be formulated as: ˆuv = uθv + sv(uθa,v uθv ), ˆua = uθa + sa(uθa,v uθa ), (10) (11) where sv and sa are coefficients controlling the guidance strength for the video and audio modalities, respectively. 3.5. Multi-task unification As shown in Fig. 2, leveraging the symmetry and flexibility of UniAVGens overall design, we support multiple input combinations to handle distinct tasks: (1) Joint audiovideo generation: The default core task, which takes only text and reference image as input to generate aligned audio and video. (2) Joint generation with reference audio: Compared to (1), it supports input of custom reference audio to control the speakers timbre. Notably, latents of the reference audio skip cross-modal interaction to preserve the timbre consistency. (3) Joint audio-video continuation: It performs continuation given conditional audio and conditional video. For this task, conditional information also participates in cross-modal interaction to ensure temporal continuity, while its features remain unaffected by interaction to preserve conditional information. (4) Video-to-audio dubbing: When only conditional video is provided to the video branch, the model generates corresponding emotionand expression-aligned audio based on the video and text. reference audio can be optionally provided to anchor timbre, and the reference image for the video branch is filled (5) audiowith the first frame of the conditional video. driven video synthesis: When only conditional audio is provided to the audio branch, the model generates expressionand motion-aligned video based on the audio and text. 5 Figure 4. Visual comparisons of UniAVGen against concurrent methods Ovi and UniVerse-1. Specifically, Example (a) uses an indistribution real human image: UniAVGen and Ovi generate high-fidelity, well-aligned audio-video, while UniVerse-1 is nearly static. Example (b) uses an out-of-distribution (OOD) anime image: Ovi lacks aligned lip/motions (poor generalization), UniVerse-1 stays static with noisy audio; in contrast, our model shows strong generalization, producing coherent, aligned audio-video matching the anime input. Table 1. Quantitative comparison with different methods. Methods Joint Training Samples Parameters (S+V) Audio Quality Video Quality Audio-Video Consistency PQ() CU() WER() SC() DD() IQ() LS() TC() EC() 21.1B 16.6B 3.7B 7.1B 10.9B 7.1B 8.15 8.15 5.21 4.56 6.03 7.00 7.41 7. 3.93 4.29 6.01 6.62 0.152 0.152 0.987 0.991 0 0.13 0.721 0.750 0.986 0.296 0.216 0. 0.965 0.985 0.972 0.973 0.373 0.08 0.360 0.410 0.716 0.733 0.774 0.779 6.34 6.35 1.23 1.21 6.48 5.95 0.454 0. 0.349 0.375 0.776 0.573 0.828 0.832 0.388 0.300 0.558 0.573 Two-stage Generation OmniAvatar [13] Wan-S2V [14] - - Joint Generation JavisDiT [25] Universe-1 [42] Ovi [27] UniAVGen 10.1M 6.4M 30.7M 1.3M 4. Experiment 4.1. Implementation details UniAVGen is trained in three stages. Stage 1 focuses on training the audio branch in isolation: here, we only optimize the audio network using its dedicated objective La. The training data uses the English subset of the multilingual audio dataset Emilia [16]. We adopt batch size of 256, learning rate of 2 105, and the AdamW [26] optimizer with parameters β1 = 0.9, β2 = 0.999, ϵ = 1e8, for total of 160k training steps. Once the audio branch achieves robust generative performance, we proceed to Stage 2endto-end joint training. In this phase, both branches are cooptimized via composite loss Ljoint = Lv + La + λmLm, where λm is initialized to 0.1 and decays linearly to 0. The training data here uses an internally collected real human audio-video dataset. We use batch size of 32, learning 6 rate of 5e6, and the same optimizer settings as Stage 1, for total of 30k training steps. Stage 3 involves multi-task learning built on Stage 2, with training configurations consistent with Stage 2. In the training process, the ratio of the 5 tasks mentioned in Sec. 3.5 is set to 4:1:1:2:2, with total of 10k training steps. Inference details are provided in the supplementary materials. 4.2. Comparison with previous methods Compared methods. We select representative methods from two categories of paradigms for comparison: (1) Twostage Generation: Since we focus on joint audio-video generation, we first generate audio using F5-TTS [3], then generate video from audio with state-of-the-art OmniAvatar [13] and Wan-S2V [14]. (2) Joint Generation: We select several latest open-source models for comparison: JavisDiT [25] focuses on general audio-video joint generation without human audio optimization, UniVerse-1 [42] adopts dual pre-trained model stitching, and Ovi [27] employs symmetric dual-tower architecture with symmetric global cross-model interactions. Evaluation setting. To mitigate test set leakage and better align with the objectives of audio-video generation, we constructed 100 test samples that are not sampled from existing videos. Each sample comprises reference image, video caption, and audio content. To comprehensively validate the models generalization capabilityparticularly across diverse visual domainshalf of these reference images are real-world captures, while the remaining half consists of AIGC-generated content or anime-style visuals. For evaluation, we measure model performance across three critical dimensions: (1) Audio Quality: Following [42], we adopt AudioBox-Aesthetics [40] to evaluate two core metrics: Production Quality (PQ) and Content Usefulness (CU). Additionally, we leverage the Whisper-largev3 [37] model to compute the Word Error Rate (WER) (2) Video Quality: We utilize of the generated audio. VBench [19]a widely recognized video evaluation benchmarkto assess video generation quality, focusing on three key metrics: Subject Consistency (SC), Dynamic Degree (DD), and Imaging Quality (IQ). (3) Audio-Video Consistency: Notably, this dimension encompasses three subaspects: lip synchronization (LS), timbre consistency (TC), and emotion consistency (EC). Specifically, we employ SyncNet [7]s confidence score to evaluate lip-sync consistency. For timbre and emotion consistency, as no opensource methodologies currently exist to quantify such crossmodal alignment, we instead leverage the multi-modal large language model Gemini-2.5-Pro for evaluation. We set the outputs scores within the range [0, 1]. detailed system prompt (with implementation specifics provided in the supplementary materials) defines the scoring criteria, and the final score for each of these two metrics is computed as the average of three independent evaluations. Quantitative comparison. Tab. 1 summarizes quantitative comparisons between our method and existing baselines: For audio quality, our method demonstrates significant superiority over other joint generation approaches in both acoustic quality and aesthetic metrics, with its WER further outperforming F5-TTSunderscoring stronger alignment with linguistic content. Turning to video quality, while twostage methods exhibit stronger identity consistency, their dynamism scores are near-zero, reflecting their inability to generate actions congruent with audio-driven emotions; in contrast, our method achieves the highest dynamism and aesthetic quality while retaining identity consistency comparable to state-of-the-art alternatives. Notably, for the critical audio-video consistency metric, our methoddespite utilizing the fewest effective training samplesshows clear advantages over competitors in timbre and emotion alignTable 2. Ablation studies on the design of interaction. Interaction A2V V2A LS() TC() EC() (1) (2) (3) (4) (5)"
        },
        {
            "title": "SGI\nSTI\nATI\nSTI\nATI",
            "content": "3.46 3.73 3.88 3.97 4.09 0.667 0.685 0.705 0.691 0.725 0.459 0.472 0.492 0.483 0.504 ment, while maintaining lip-sync performance on par with leading methods. This enhanced training efficiency stems from our asymmetric cross-modal interaction mechanism and face-aware modulation module. Qualitative comparison. Fig. 4 presents visual comparisons of UniAVGen against recent concurrent methods Ovi and UniVerse-1. Specifically, Example (a) uses real human image aligned with the training distribution: both UniAVGen and Ovi generate high-fidelity audio and videos, with motions and emotions tightly aligned to the audio, whereas UniVerse-1 exhibits near-static behavior. Example (b) employs an anime imageout-of-distribution (OOD) relative to the training set: Ovi fails to produce lip movements and motions aligned with the audio, highlighting its constrained generalization capacity; UniVerse-1 remains static and generates noisy audio. In contrast, our model exhibits robust generalization, generating coherent audio and motions that align with the input anime image. 4.3. Ablations For efficient ablation studies, unless otherwise specified, the following ablation results default to those from the first 10k steps of Stage 2 training. The colored background indicates our default setting. 4.3.1. Cross-modal interaction design As core architectural component, we perform detailed ablation studies on the design of the cross-modal interaction module, as shown in Tab. 2. Consistent with the three mechanisms depicted in Fig. 3, this table denotes Symmetric Global Interaction as SGI, Symmetric TemporalAligned Interaction as STI, and our proposed Asymmetric Temporal-Aligned Interaction as ATI. SGI exhibits substantial performance deficits compared to STI with the same number of training stepsthis confirms that temporalaligned designs more effectively facilitate model convergence. Relative to STI, our proposed ATI delivers significant improvements in both A2V and V2A tasks: For A2V, ATI more robustly enhances timbre and emotion consistency between audio and video, validating that it indeed strengthens audios perception of facial expressions and movements across adjacent video frames; for V2A, it fur7 Figure 5. Visual comparisons of predicted masks with fixed λm and decaying λm. Zoom in for the best view. Table 3. Ablation studies on the face-aware modulation. Settings LS() TC() EC() (a) without FAM (b) unsupervised FAM (c) FAM with fixed λm (d) FAM with decaying λm 3.89 3.92 4.11 4.09 0.705 0.701 0.719 0. 0.489 0.492 0.497 0.504 ther boosts lip synchronization accuracy, confirming that it enables video frames to better capture information from adjacent audio segments. 4.3.2. Effectiveness of face-aware modulation We evaluate the effectiveness of Face-aware Modulation (FAM) through two key analyses. First, to confirm that our lightweight dynamic mask prediction module can reliably localize valid facial regions, we visualize average face masks predicted across layers with fixed λm in Fig. 5. This visualization demonstrates that our module effectively pinpoints face-salient regions. Additionally, when trained with decaying λm, the predicted masks still effectively capture facial regions while increasing weights on body regionsthereby enhancing the flexibility of cross-modal interactions. To further validate the FAM strategy, we compare performance under four configurations in Table 4: without FAM, unsupervised FAM, FAM with fixed λm, and FAM with decaying λm. Two critical insights emerge: (1) Supervised FAM yields significant improvements in overall audio-video consistency, indicating that constrained masks facilitate training convergence; (2) Decaying loss weights outperform fixed weights, indicating that gradually relaxing constraints on interaction locations during training further enhances the timbre and emotion consistency. 4.3.3. Modality-aware classifier-free guidance To demonstrate the effectiveness of MA-CFG, we provide visual comparisons in Fig. 6. Without MA-CFG, while audio and video remain generally consistent, the generated characters emotions and body movements are insufficiently aligned with the audios emotional cues. With MA-CFG, by contrast, the jointly generated character exhibits facial Figure 6. Visual comparisons of joint generation results with and without MA-CFG. Zoom in for the best view. Figure 7. Comparisons of different training strategies. expressions and body movements more tightly aligned with audio emotions, alongside more natural lip synchronization. 4.3.4. Analysis of training strategies As shown in Fig. 7, we compare the LC metric of our model under three distinct training strategies: train joint generation only (denoted as JGO), train joint generation first then multi-task learning (denoted as JFML), and multi-task training throughout (denoted as MTO). First, JGO exhibits lower performance ceiling than JFML, which we attribute to the ability of multi-task joint training to further strengthen cross-modal interaction. For instance, video-to-audio dubbing enhances the audio branchs capture of conditional information from video, while audio-driven video synthesis deepens the video branchs perception of the audio branch. Second, MTO demonstrates slower convergence speed than both JGO and JFML. This likely stems from the fact that joint generation is more task-intensive than conditional generation taskstraining the model with conditional tasks from the start may cause it to get trapped in local optima. In contrast, pre-training with joint generation lays solid foundation for subsequent conditional tasks, allowing JFML to achieve the best overall performance. 5. Conclusion This work has introduced UniAVGen, unified framework for generating high-quality audio and video jointly At its 8 core lies the asymmetric cross-modal interaction mechanism (ATI). Unlike symmetric or global interaction designs, ATI enables modality-specific temporal alignment: it allows audio to efficiently perceive dynamics across adjacent video frames while empowering video frames to capture audio cues from neighboring audio segments. Complementing ATI, we further propose the Face-aware Modulation (FAM) module, which dynamically localizes facial regions and enhances interaction precision without incurring significant computational overhead. Additionally, we introduce Modality-Aware CFG during inference to explicitly strengthen cross-modal influences. Overall, UniAVGen sets new benchmark for audio-video generation by unifying efficient training, strong cross-modal alignment, and broad generalization, paving the way for more practical and versatile multi-modal generation systems."
        },
        {
            "title": "References",
            "content": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 5 [2] Edresson Casanova, Kelly Davis, Eren Golge, Gorkem Goknar, Iulian Gulea, Logan Hart, Aya Aljafari, Joshua Meyer, Reuben Morais, Samuel Olayemi, et al. Xtts: massively multilingual zero-shot text-to-speech model. arXiv preprint arXiv:2406.04904, 2024. 2 [3] Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, and Xie Chen. F5-tts: fairytaler that fakes fluent and faithful speech with flow matching. arXiv preprint arXiv:2410.06885, 2024. 2, 3, 6 [4] Yi Chen, Sen Liang, Zixiang Zhou, Ziyao Huang, Yifeng Ma, Junshu Tang, Qin Lin, Yuan Zhou, and Qinglin Lu. Hunyuanvideo-avatar: High-fidelity audio-driven huarXiv preprint man animation for multiple characters. arXiv:2505.20156, 2025. 1, 2 [5] Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, and Yuki Mitsufuji. Mmaudio: Taming multimodal joint training for high-quality video-toaudio synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2890128911, 2025. 1, 2 [6] Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151, 2023. [7] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Asian conference on computer vision, pages 251263. Springer, 2016. 7 [8] Alibaba Cloud. Wan2.5. https://wan.video/, 2025. 1 [9] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with diffusion transformer networks. arXiv e-prints, pages arXiv2412, 2024. 2 [10] Google DeepMind. Veo3. https : / / deepmind . google/models/veo/, 2025. 1 [11] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-shot multiIn Proceedings of level face localisation in the wild. the IEEE/CVF conference on computer vision and pattern recognition, pages 52035212, 2020. 5 [12] Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024. 2 [13] Qijun Gan, Ruizi Yang, Jianke Zhu, Shaofei Xue, and Steven Hoi. Omniavatar: Efficient audio-driven avatar video generation with adaptive body animation. arXiv preprint arXiv:2506.18866, 2025. 6 [14] Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, et al. Wan-s2v: Audio-driven cinematic video generation. arXiv preprint arXiv:2508.18621, 2025. [15] Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Alper Canberk, Kwot Sin Lee, Vicente Ordonez, and Sergey Tulyakov. Av-link: Temporally-aligned diffusion features for cross-modal audio-video generation. arXiv preprint arXiv:2412.15191, 2024. 1, 2 [16] Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, et al. Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation. In 2024 IEEE Spoken Language Technology Workshop (SLT), pages 885890. IEEE, 2024. 6 [17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [18] Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. Hunyuancustom: multimodal-driven architecture for customized video generation. arXiv preprint arXiv:2505.04512, 2025. 1 [19] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 7 [20] Masato Ishii, Akio Hayakawa, Takashi Shibuya, and Yuki Mitsufuji. simple but strong baseline for sounding video generation: Effective adaptation of audio and video arXiv preprint diffusion models for joint generation. arXiv:2409.17550, 2024. 1, 2 [21] Yujin Jeong, Yunji Kim, Sanghyuk Chun, and Jiyoung Lee. Read, watch and scream! sound generation from text and video. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1759017598, 2025. [22] Yinghao Aaron Li, Cong Han, and Nima Mesgarani. Styletts: style-based generative model for natural and diverse textto-speech synthesis. IEEE Journal of Selected Topics in Signal Processing, 2025. 2 [23] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up 9 of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. 1, 2 [24] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [25] Kai Liu, Wei Li, Lai Chen, Shengqiong Wu, Yanhao Zheng, Jiayi Ji, Fan Zhou, Rongxin Jiang, Jiebo Luo, Hao Fei, et al. Javisdit: Joint audio-video diffusion transformer with hierarchical spatio-temporal prior synchronization. arXiv preprint arXiv:2503.23377, 2025. 1, 2, [26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [27] Chetwin Low, Weimin Wang, and Calder Katyal. Ovi: Twin backbone cross-modal fusion for audio-video generation. arXiv preprint arXiv:2510.01284, 2025. 1, 2, 4, 6, 7 [28] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. Advances in Neural Information Processing Systems, 36:4885548876, 2023. 2 [29] Openai. Sora2. https://openai.com/zhHansCN/index/sora-2/, 2025. [30] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 23372346, 2019. 5 [31] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [32] Puyuan Peng, Po-Yao Huang, Shang-Wen Li, Abdelrahman Mohamed, and David Harwath. Voicecraft: Zero-shot speech editing and text-to-speech in the wild. arXiv preprint arXiv:2403.16973, 2024. 2 [33] Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, and Zhaoxin Fan. Synctalk: The devil is in the synchronization for talking head synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 666676, 2024. 2 [34] Ziqiao Peng, Wentao Hu, Junyuan Ma, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Hui Tian, Jun He, Hongyan Liu, and Zhaoxin Fan. Synctalk++: High-fidelity and efficient synchronized talking heads synthesis using gaussian splatting. arXiv preprint arXiv:2506.14742, 2025. 2 [35] Ziqiao Peng, Jiwen Liu, Haoxian Zhang, Xiaoqiang Liu, Songlin Tang, Pengfei Wan, Di Zhang, Hongyan Liu, and Jun He. Omnisync: Towards universal lip synchronization via diffusion transformers. arXiv preprint arXiv:2505.21448, 2025. [36] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2 [37] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech In Internarecognition via large-scale weak supervision. tional conference on machine learning, pages 2849228518. PMLR, 2023. 7 [38] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1021910228, 2023. 1, 2, 4 [39] Sizhe Shan, Qiulin Li, Yutao Cui, Miles Yang, Yuehai Wang, Qun Yang, Jin Zhou, and Zhao Zhong. Hunyuanvideofoley: Multimodal diffusion with representation alignment arXiv preprint for high-fidelity foley audio generation. arXiv:2508.16930, 2025. 1, 2 [40] Andros Tjandra, Yi-Chiao Wu, Baishan Guo, John Hoffman, Brian Ellis, Apoorv Vyas, Bowen Shi, Sanyuan Chen, Matt Le, Nick Zacharov, et al. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. arXiv preprint arXiv:2502.05139, 2025. [41] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3 [42] Duomin Wang, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao Liao, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, and Gang Yu. Universe-1: Unified audio-video generation via stitching of experts. arXiv preprint arXiv:2509.06155, 2025. 1, 2, 4, 6, 7 [43] Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, and Yapeng Tian. Av-dit: Taming image diffusion transformers for efficient joint audio and video generation. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 1048610495, 2025. 2 [44] Le Wang, Jun Wang, Chunyu Qiang, Feng Deng, Chen Zhang, Di Zhang, and Kun Gai. Audiogen-omni: unified multimodal diffusion transformer for video-synchronized arXiv preprint audio, arXiv:2508.00733, 2025. 2 speech, and song generation. [45] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. arXiv preprint Ddt: Decoupled diffusion transformer. arXiv:2504.05741, 2025. 2 [46] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16133 16142, 2023. 4 [47] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing: Open-domain visualaudio generation with diffusion latent aligners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71517161, 2024. 2 [48] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, and Yossi Adi. Diverse and aligned audio-to-video generation via text-to-video model adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 6639 6647, 2024. 2 10 [49] Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing, Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. arXiv preprint arXiv:2407.01494, 2024. 2 [50] Youliang Zhang, Zhaoyang Li, Duomin Wang, Jiahe Zhang, Deyu Zhou, Zixin Yin, Xili Dai, Gang Yu, and Xiu Li. Speakervid-5m: large-scale high-quality dataset for audiovisual dyadic interactive human generation. arXiv preprint arXiv:2507.09862, 2025."
        }
    ],
    "affiliations": [
        "Renmin University of China",
        "Shanghai AI Lab",
        "Shanghai Jiao Tong University",
        "State Key Laboratory for Novel Software Technology, Nanjing University",
        "Tencent Hunyuan",
        "Tsinghua University"
    ]
}