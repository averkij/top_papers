{
    "paper_title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
    "authors": [
        "Enxin Song",
        "Wenhao Chai",
        "Shusheng Yang",
        "Ethan Armand",
        "Xiaojun Shan",
        "Haiyang Xu",
        "Jianwen Xie",
        "Zhuowen Tu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, adapts Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We employ a hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video. Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal global-local attention allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks."
        },
        {
            "title": "Start",
            "content": "VideoNSA: Native Sparse Attention Scales Video Understanding VIDEONSA: NATIVE SPARSE ATTENTION SCALES VIDEO UNDERSTANDING Enxin Song1, Wenhao Chai2, Shusheng Yang3, Ethan Armand1, Xiaojun Shan1, Haiyang Xu1, Jianwen Xie4, and Zhuowen Tu1 1University of California, San Diego 2Princeton University 4Lambda, Inc 3New York University 5 2 0 2 2 ] . [ 1 5 9 2 2 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, adapts Qwen2.5VL through end-to-end training on 216K video instruction dataset. We employ hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video. Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal globallocal attention allocation at fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks. Project page: https://enxinsong.com/VideoNSA-web/ Code: https://github.com/Espere-1119-Song/VideoNSA Model: https://huggingface.co/Enxin/VideoNSA"
        },
        {
            "title": "INTRODUCTION",
            "content": "Key moments of video can occur at any time, exemplified by soccer where game deciding moments typically span seconds of 90 minute game. Within those game deciding moments split second actions define the outcome: an assist, missed tackle, the movement of the keeper. Multimodal large language models (MLLMs)(Team, 2025; Team et al., 2025b;a) have achieved substantial progress in vision-language perception and reasoning, but still cannot match humans ability to extract and reason about salient moments in videos. While humans naturally sample color visuals around 60hz, (Kalloniatis & Luu, 2007) across large contexts, existing VLMs often sample single frame per second. Intuitively, increasing the context for these models by sampling more frames improves accuracy (Cai et al., 2024; Wu et al., 2024), particularly for long videos and complex reasoning tasks. However, this approach pays for improvement with additional tokens, increasing computational complexity and pushing against fundamental limits of model context. To address these challenges, many approaches (Wang et al., 2024; Li et al., 2024b; Jin et al., 2024; Wang et al., 2025a; Yang et al., 2024) adopt token compression to reduce redundancy and increase informative context. However, when applied to complex reasoning tasks, these compression-based models perform worse compared to full-token methods (Song et al., 2025a). Moreover, compression strategies often limit generalization through reduced perception and reasoning capacity (Wen et al., 2025). In contrast, sparse attention mechanisms preserve tokens, but focus the models capabilities on relevant dependencies between tokens. Numerous sparse attention methods have already been employed in large language models (LLMs), but most are inadequate for video complexity (detailed in Appendix A). Therefore, we present VideoNSA, which adopts Native Sparse Attention (Yuan et al., 2025b), learnable hardware-aware sparse attention mechanism proven to be effective in longcontext modeling. VideoNSA is the first learnable and hardware-aware sparse attention framework 1 VideoNSA: Native Sparse Attention Scales Video Understanding tailored for video understanding, effectively scaling to ultra-long vision-text context. We apply the learnable sparse attention to video token sequences, while preserving grouped-query attention for text tokens. Following this pattern, our experiments show that using only 3.6% of the attention budget on 128K context length while improving performance on various tasks We further conduct massive experiments and analyses of VideoNSA , revealing several important findings: (1) VideoNSA extrapolates effectively to contexts beyond its training length, and the optimal balance between temporal density and spatial resolution is highly task dependent. (2) VideoNSA is also sensitive by attention scaling, with results remaining strongest near the training configuration. (3) The gating distribution evolves dynamically across layers, and the selection and sliding-window branches gradually lose importance in deeper layers. (4) The compression branch emerges as the main computational bottleneck. (5)Moreover, the learned sparse attention weights remain beneficial even under dense attention settings. (6) Learnable sparse attention induces distinctive attention sink behaviors across branches, with very few sinks in the selection branch and periodic sink formation in the compression branch. In particular, our paper makes the following contributions: We propose VideoNSA, hardware-aware native sparse attention mechanism, and systematically investigate its effectiveness for video understanding, scaling up to 128K vision context length. We introduce hybrid sparse attention in VideoNSA, enabling flexible allocation of information and attention budgets to achieve optimal performance across diverse task. We dynamically combine global and local attention through three complementary branches, which effectively reduce attention sinks in long vision contexts."
        },
        {
            "title": "2 VIDEONSA",
            "content": "2.1 PRELIMINARIES Native sparse attention. Existing training-free sparse attention methods are rarely hardware aligned, and typically dont increase training efficiency. Native Sparse Attention (Yuan et al., 2025b) (NSA) avoids computing attention between all key-value pairs (Kt, Vt), instead, for each query qt, NSA dynamically constructs an information-dense KV cache subset. NSA combines three complementary cache branches with learnable gate gc ot = (cid:88) adaptively weighting each branch yielding ot: (cid:1). Attn(cid:0)qt, gc , t (1) c{cmp, slc, win} Token Compression (CMP) branch aggregates sequential blocks of keys into more coarse-grained, single block-level representations Kcmp via learnable MLP φ: Kcmp = {φ(K[id+1:id+m]) 0 < }, (2) where is the block length, is the stride. Token Selection (SLC) branch preserves the most salient key-value blocks by computing importance scores pslc and selecting the indices of the top-n blocks: It = {i rank(pslc [i]) n}. The final set of selected keys is formed by concatenating these top-ranked blocks: Kslc = Cat({K[im+1:(i+1)m] It}), (3) (4) where It is the set of selected indices, is the number of blocks to retain. Sliding Window (SWA) branch simply applies the standard sliding window attention, which retains the fixed most recent key-value pairs: Kswa = Ktw+1:t, Vswa = Vtw+1:t. (5) 2 VideoNSA: Native Sparse Attention Scales Video Understanding Figure 1: Overview of VideoNSA. Video frames are encoded into frame-level KV blocks. VideoNSA utilizes three sparse attention branches during prefilling stage: compression branch reduces redundancy via token averaging, selection branch identifies top-k important tokens, and sliding window branch enforces local temporal coverage. The outputs are combined through dynamic gating before integration with text tokens for LLM decoding. Grouped query attention. In Multi-Head Attention (MHA), each query head has dedicated key value (KV) projections, which makes the KV cache scale with the number of heads and increases inference cost. Grouped-Query Attention (GQA) (Ainslie et al., 2023) mitigates this by letting multiple query heads share fewer KV heads. For each input {xi}L i=1, GQA partitions the query heads into groups (1 h). At given timestep t, the output o(s) for the s-th query head with group index m(s) = sg/h is computed by applying attention to the shared keys and values as: t = Attention(q(s) o(s) , (m(s)) , (m(s)) ) = softmax (cid:32) (q(s) (cid:33) )K (m(s)) dk (m(s)) , (6) , k(m(s)) , v(m(s)) where q(s) = xiW (s) . The outputs ot from all heads are concatenated by ot = [o(1) ]. VideoNSA utilizes Qwen2.5-VL-7B (Bai et al., 2025) as the backbone, with Qwen2.5-7B (Qwen et al., 2025) as the LLM decoder, which employs GQA for efficient KV cache utilization using 28 query heads and 4 shared key/value heads. = xiW (m(s)) , . . . , o(h) , o(2) = xiW (m(s)) t"
        },
        {
            "title": "2.2 ARCHITECTURE",
            "content": "Existing token compression methods (Yang et al., 2025d; Zhang et al., 2025c; Hyun et al., 2025; Zhang et al., 2025h) suffer from irreversible information loss on complex tasks and dont address computational and latency bottlenecks in LLM video understanding. From the perspective of attention as message passing in Graph Neural Network (Joshi, 2025; Pappone, 2025), its clear this bottleneck is fundamental. Standard attention propagates information between nodes (tokens) through edges (attention weights), with each token being updated by aggregating features from its neighbors, weighted by attention scores. Training-free sparse attention often imposes static adjacency matrix whose fixed subgraph connectivity restricts information flow. Conversely, NSA (Yuan et al., 2025b) provides data-dependent sparsity that preserves edges necessary for particular task. We build VideoNSA upon Qwen2.5-VL-7B (Qwen et al., 2025), which incorporates vision encoder and adopts Qwen2.5-7B (Bai et al., 2025) as the LLM. As illustrated in Figure 1, VideoNSA introduces hybrid attention mechanism in the LLM across different modalities. At each layer l, we split the input tokens X(l1) into vision tokens X(l1) according to their position IDs. For vision tokens, VideoNSA applies NSA (Yuan et al., 2025b) with dedicated gate gc on each head. We set the block size equal to the token number per frame, and obtain the block-level representation by averaging all tokens within the block. The vision attention output oV is dynamically and text tokens X(l1) 3 VideoNSA: Native Sparse Attention Scales Video Understanding weighted by the compression, selection, and sliding window branches as: o(l) = (cid:88) c{cmp,slc,win} Attn(cid:0)qt, Kc gc , Vc (cid:1), where gc is implemented as two-layer MLP with sigmoid activation. The text attention output o(l) instruction following capabilities. We obtain the final output o(l) of the layer by concatenating: is computed using standard GQA (Ainslie et al., 2023) to preserve o(l) = [ o(l) ; o(l) ]."
        },
        {
            "title": "2.3 TRAINING RECIPE",
            "content": "We conduct end-to-end training to adapt vision features for data-dependent sparse connectivity in the language model. The training dataset of VideoNSA is constructed from LLaVA-Video-178K (Zhang et al., 2024c) by filtering for question answer pairs at 4 fps and retaining videos with 350550 frames, for subset of 216K pairs. To emphasize sparse attention for temporal redundancy, we constrain the maximum pixels per frame to 50,176, and the maximum context length per training instance to 36K tokens. In VideoNSA, block size is set to 64, block is set to 32, and sliding window size is set to 256. We trained using SWIFT (Zhao et al., 2024), adapting the NSA (Yuan et al., 2025b) implementation from FLA (Yang & Zhang, 2024) and (Pai et al., 2025b). The complete training process requires 4600 H100 GPU hours. More training details including hyper-parameters selection can be found in Appendix B."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 EFFECTIVENESS ON VIDEO UNDERSTANDING Baselines Our primary baseline is Qwen2.5-VL-7B (Qwen et al., 2025) with dense FlashAttention (Dao, 2023). We compare VideoNSA against several strong baselines, including the quantization model AWQ (Team, 2024), training-free token compression models (Yang et al., 2025c; Zhang et al., 2025b; Chen et al., 2024a), and training-free sparse attention methods (Jiang et al., 2024; Xu et al., 2025a; Lai et al., 2025; Li et al., 2024c). All methods employ their official configuration without additional training and using Qwen2.5-VL-7B (Qwen et al., 2025) as base. For token compression baselines, we use the token kept ratio and sampling fps from the original papers that yield the best accuracy, while for sparse attention baselines, we use the same configuration as VideoNSA. In addition, we fine-tune Qwen2.5-VL-7B (Qwen et al., 2025) using the same training dataset as VideoNSA to serve as competitive baseline. We also include models with different backbones for broad comparison. We evaluate VideoNSA across three domains including long video understanding, temporal reasoning, and spatial understanding using LMMs-Eval (Zhang et al., 2024a) and VLMEvalKit (Duan et al., 2024). Table 1 indicates that sparse attention methods consistently outperform token compression approaches. We empirically evaluate the effectiveness of VideoNSA based on several popular long video understanding benchmarks, including LongVideoBench (Wu et al., 2024), MLVU (Zhou et al., 2024), TimeScope (Zohar et al., 2025) and LongTimeScope (Zohar et al., 2025). VideoNSA achieves competitive results, narrowing the gap with state-of-the-art methods. We observe that VideoNSA shows clear advantages on tasks involving order-sensitive temporal reasoning and ultra-long video settings (10 hours in LongTimeScope (Zohar et al., 2025)). To evaluate the visual temporal reasoning capbility of VideoNSA, we evaluate VideoNSA on Tomato (Shangguan et al., 2024), benchmark spanning six reasoning types and three video scenarios. VideoNSA attains the highest accuracy on Tomato (Shangguan et al., 2024), substantially outperforming compression-based methods, underscoring their limitations in fine-grained temporal inference. VSIBench (Yang et al., 2025a) focuses on spatial reasoning allowing us to test whether efficient models can preserve local fidelity while achieving efficiency. VideoNSA matches the strongest sparse attention baselines and significantly surpasses token compression methods in spatial understanding, confirming that it preserves spatial fidelity. All detailed evaluation settings and subset results can be found in Appendix C, Appendix D, Appendix E, and Appendix F. 4 VideoNSA: Native Sparse Attention Scales Video Understanding Table 1: Results on long video understanding, temporal reasoning and spatial understanding tasks. LVB, LTS for LongVideobench (Wu et al., 2024) and LongTimeScope (Zohar et al., 2025). Model Long-form Video Temporal Spatial LVB MLVUtest TimeScope LLaVA-OneVision-7B (Li et al., 2024a) LLaVA-Video-7B (Zhang et al., 2024b) VideoLLaMA3-8B (Zhang et al., 2025a) InternVL2.5-8B (Chen et al., 2024b) Video-XL-2 (Qin et al., 2025b) Qwen2.5-VL-7B (Qwen et al., 2025) Qwen2.5-VL-7B-AWQ (Team, 2024) Qwen2.5-VL-7B-SFT Token Compression Methods + FastV (Chen et al., 2024a) + VScan (Zhang et al., 2025b) + VisionZip (Yang et al., 2025c) Sparse Attention Methods + Tri-Shape (Li et al., 2024c) + MInference (Jiang et al., 2024) + FlexPrefill (Lai et al., 2025) + XAttention (Xu et al., 2025a) VideoNSA 56.3 58.2 59.8 60.0 61.0 58.7 59.0 57.8 57.3 58.7 52. 59.5 59.2 58.4 59.1 60.0 47.7 52.2 51.2 46.0 51.2 41.8 48.1 33.1 49.2 49.2 46.0 50. 51.8 74.1 69.5 55.8 81.0 76.8 46.5 80.3 43.5 82.7 82.7 83.0 83.1 83. LTS 34.0 40.7 40.2 35.6 31.1 40.4 28.4 44.4 39.1 41.1 44. Tomato VSIBench 25.5 22.6 21.7 21.6 19.1 23.6 22.1 23.0 23.7 21.4 26. 32.4 35.6 29.7 35.0 30.5 32.0 34.4 32.1 34.9 36.5 34.0 36.6 36.1 Table 2: Ablation study on branch selection across different LongVideobench (Wu et al., 2024) and LongTimeScope (Zohar et al., 2025). tasks. LVB, LTS for Branch Long Video Understanding Temporal Reasoning Spatial Understanding CMP SLC SWD LVB MLVUtest TimeScope LTS Tomato VSIBench (cid:33) (cid:33) 48.1 48.4 (cid:33) 49.1 (cid:33) (cid:33) (cid:33) 49.4 (cid:33) 49.3 (cid:33) (cid:33) 48.8 (cid:33) (cid:33) (cid:33) 60.0 43.9 47.7 40.2 42.7 42.4 43.4 51.8 41.5 63.7 59.3 57.3 65.2 57. 83.7 25.1 37.1 29.8 32.4 34.4 31.6 44.4 23.3 24.0 24.0 23.5 23.0 24. 26.5 3.2 ABLATION STUDY 29.2 27.6 29.8 29.4 29.1 30.3 36.1 To further analyze the components of VideoNSA, we visualize attention pattern in each branch in Appendix and assess the effectiveness of different branches. Table 2 shows that single-branch models suffer significant degradation, and even two-branch combinations remain inferior to the full VideoNSA, highlighting the necessity of integrating all three branches with dynamic gating. Detailed results of different branch combination can be found in Appendix H."
        },
        {
            "title": "4 SCALING ANALYSIS AND FINDINGS",
            "content": "Finding 1. Do learned sparse attention weights remain beneficial in dense attention settings? Table 3: Ablation study on transferring sparse attention weights to dense attention across tasks. Model Long Video Understanding Temporal Reasoning Spatial Understanding LongVideoBench MLVUT est TimeScope LongTimeScope Tomato Qwen2.5-VL-7B Dense-SFT Dense-NSA VideoNSA 58.7 57.8 (-1.5%) 56.1 (-4.4%) 59.4 (+1.1%) 81.0 51.2 51.2 (+0.0%) 76.8 (-5.2%) 51.6 (+0.8%) 83.0 (+2.5%) 51.8 (+1.2%) 82.7 (+2.1%) 40.7 40.2 (-1.2%) 40.9 (+0.5%) 44.4 (+9.1%) 22.6 21.7 (-4.0%) 23.4 (+3.5%) 26.2 (+15.9%) VSIBench 29.7 30.6 (+2.1%) 33.1 (+10.7%) 36.1 (+20.3%) 5 VideoNSA: Native Sparse Attention Scales Video Understanding (a) Information Scaling of LongVideoBench (b) Information Scaling of TimeScope (c) Information Scaling of Tomato (d) Information Scaling of VSIBench Figure 2: Scaling Performance of VideoNSA under Different Context Allocation Strategies. We highlight the Token Budget Constraint to indicate settings with equal context length, and annotate the best-performing configuration under each benchmark. Since videos in Tomato (Shangguan et al., 2024), we vary FPS instead of total frames, with FPS TPF = 128 denoted as K0. We further examine whether the learned QKV weights of VideoNSA can imrpove performance in dense attention inference. Table 3 reports the relative performance change over the Qwen2.5-VL7B (Qwen et al., 2025). Due to the limited quality of the training data, our fine-tuned Qwen2.5VL-7B (Dense-SFT) exhibits slight performance drops on most benchmarks. We observe that the transferred model (Dense-NSA) allows the dense variant to recover and surpass the baseline on several benchmarks suggesting that sparse-trained weights provides inductive bias towards more effective attention distributions. However, the effect remains limited on LongVideoBench (Wu et al., 2024). VideoNSA significantly outperforms Dense-NSA on most tasks, highlighting the importance of runtime sparsity and dynamic gating. Finding 2. How far can VideoNSA scale in context length? The effective vision context length is jointly determined by the number of vision tokens per frame and the total number of input frames . VideoNSA is trained with maximum context length of = 36K tokens, corresponding to = 64 tokens per frame. We conduct an information budget study under fixed context length, by varying tokens per frame and frame rate. We then scale up the context length beyond the training budget, evaluating up to the maximum 128K tokens supported by the language model. As observed in Figure 2, the model consistently achieves higher performance when scaled to longer contexts beyond its training length across benchmarks. However, the ideal allocation of same token budget is highly task-dependent. LongVideoBench (Wu et al., 2024) favors allocating more tokens per frame, while Tomato (Shangguan et al., 2024) and TimeScope (Zohar et al., 2025) benefit more from increasing the number of frames, emphasizing temporal coverage. VSIBench (Yang et al., 2025a) shows mixed preferences depending on context length, reflecting balance between spatial and temporal sampling. Additional results on information scaling are reported in Appendix I. Finding 3. How to allocate the attention budget? 6 VideoNSA: Native Sparse Attention Scales Video Understanding (a) Attention Scaling of MLVU (b) Attention Scaling of LongTimeScope (c) Attention Scaling of Tomato (d) Attention Scaling of VSIBench Figure 3: Scaling Performance of VideoNSA under Different Attention Allocation Strategies. Scatter points from small to large and from light to dark indicate increasing performance. We annotate the point corresponding to the same attention allocation strategy as used during training and connect configurations with equal attention budgets using solid orange lines. We further scale the best configuration using dashed lines. Percentages show attention relative to full attention. We define the Attention Budget as the total number of key-value pairs visible to each query, denoted by Kvis. It is composed of global sparse component and local sliding-window component as: Kattn = + w, where and denote the number and size of global blocks, and is the slidingwindow width. With context length L, compared to causal dense attention with L(L1) edges, the fraction of attention used γ is 2 γ = L(cS + w) L(L1) 2 = 2(cS + w) 1 , To determine the optimal attention allocation, we first fix the total sequence length L, the attention budget Kvis, and the block size = 64, while systematically varying the local attention ratio α = . We then employ the optimal allocation ratio α for attention budget scaling. As shown in Kattn Figure 3, scatter points denote different allocation strategies, with their size and color reflecting performance. We highlight the point corresponding to the training configuration, connect equal-budget settings with solid orange lines, and extend the best-performing configuration with dashed lines, where the annotated values indicate the fraction of attention used γ. Results show that model performance is highly sensitive to attention allocation. Although the optimal ratio between global and local attention varies across tasks, configurations close to the training allocation generally yield better results. Under the same budget, fine-tuning around the training setting often improves performance, whereas simply enlarging the overall budget does not consistently bring further gains. Moreover, across most benchmarks, increasing global attention (enlarging the block count) tends to outperform increasing local attention (enlarging the sliding window). Remarkably, VideoNSA achieves leading performance using only 3.6% of the full attention budget. More results are in Appendix J. Finding 4. What roles do compression, selection, and sliding-window gates play in VideoNSA? We analyze the gating distribution of VideoNSA across Tomato (Shangguan et al., 2024), VSIBench (Yang et al., 2025a), and LongVideoBench (Wu et al., 2024), and aggregate the average routing gate weights over 100 examples from each. As illustrated in Figure 4, where shaded bars denote the interquartile range and horizontal lines represent mean values, each head in VideoNSA exhibits distinct and diverse preferences across branches throughout its full depth. The diversity allows different layers to specialize in distinct modes of the context-dependent information flow. The compres7 VideoNSA: Native Sparse Attention Scales Video Understanding Figure 4: Gate weights across layers in VideoNSA. Compression remains dominant, while selection and sliding-window weaken in later layers. Figure 5: Inter-head similarities of gates in VideoNSA. Selection and sliding-window gates show high similarity in middle layers. sion branch maintains relatively high average weights across most layers, underscoring its primary role in reducing redundancy while preserving salient features. The selection and sliding window gates fluctuate more strongly, occasionally surpassing the compression branch in early and middle layers. However, their contributions diminish in the final layers (e.g., L22L26), demonstrating that the focus shifts towards aggregating high-level features. We also note strange behavior in the last layer, where all three branches are fully active despite selection and sliding window being inactive in the layers before. Full gate values distribution in Appendix K. We further dive into the inter-head gate similarity of each layer in Figure 5. In the middle layers, both selection and sliding window gates exhibit pronounced increases in inter-head similarity. This indicates that multiple mid-layer heads converge to highly consistent gating behaviors when the model performs block selection and local temporal integration. However, the compression gate shows consistently low inter-head similarity, indicating that it operates largely in head-independent manner. At both the initial and final layers of VideoNSA, inter-head similarity remains weak across all gates, reflecting the need to maintain diversity in early representations and to support mixing information in higher-level abstractions. More inter-head gate similarites visualization in Appendix L. Finding 5. Where does the efficiency bottleneck come from? of the latency Ideally, inference We measure each branch in VideoNSA using wall-clock time across varying context lengths from 1K to 128K. The compression branch dominates runtime as the context grows, while the selection and sliding window branches contribute relatively little at longer contexts. the compression branch grows approximately linearly with L, and the sliding window branch has complexity of O(L w), which results in linear scaling for fixed window size w. The selection branch requires computing importance scores over all L/b blocks per query, leading to computational complexity of O(L2/b). However, wall-clock latency deviates from these estimates due to hardware parallelism, memory access patterns, and kernel launch overheads. Overall, the compression branch emerges as the primary bottleneck, highlighting the need for further optimization of its kernel design and memory efficiency. Figure 6: Inference latency of each branch in VideoNSA. Finding 6. Do learnable sparse mechanisms induce dynamic attention sinks? 8 VideoNSA: Native Sparse Attention Scales Video Understanding In decoder-only transformers, disproportionate amount of attention is often allocated to the first few tokens, which act as attention sinks and absorb excessive attention mass as byproduct of softmax normalization. Prior studies (Gu et al., 2024; Xiao et al., 2023) show that attention sinks arise from massive activations and unusually small key and value norms, so attention directed to these tokens contributes little to the residual state. This raises an important question in learnable sparse attention: whether sparsity patterns amplify or mitigate such sinks. Figure 7: Attention sinks distribution of different branches. VideoNSA maintains low overall sink ratio, with pink points indicating identified sinks. Figure 8: Layer-wise attention sink ratio distribution in different branches and Flash Attention. Figure 9: Relative positions of attention sinks in different branches and Flash Attention. We follow the attention sink defination in (Pai et al., 2025a): Attention Sink = α > 0.1 < median(v) 2 IQR(v) (cid:110) (cid:111) , where α is the average attention score received by the key, and is the value norm of the token. Figure 7 illustrates the average distribution of attention sinks across the three branches of VideoNSA. Each frame is encoded into 256 tokens, and we adopt the same sparse attention configuration as used during training. The three branches exhibit markedly different sink behaviors. The compression branch produces the most sinks, with distinct banded concentrations along the value norm axis caused by token merging that amplifies some token norms while suppressing others. Conversely, the selection branch yields almost no sinks, as its top-k block filtering mechanism enforces smoother value norm distribution. Notably, the sliding window branch demonstrates clearer separation between sink and non-sink tokens along the value norm axis. Critically, dynamic gating allows VideoNSA to counteract the negative effects of the compression branch, achieving stable model with low overall sink ratio of 0.3%. Figure 8 indicates that VideoNSA maintains low sink ratios overall, with only minor fluctuations across layers. However, Flash Attention exhibits gradual increase in sink ratios toward deeper layers. The compression branch maintains relatively high sink levels across most layers. The selection branch remains consistently close to zero, while the sliding window branch occasionally shows higher peaks in the middle-to-late layers, indicating that locality constraints may still introduce bias in long-sequence settings. From the perspective of positional distribution in Figure 9, Flash Attention produces sinks that are uniformly spread across the entire sequence due to its fully connected dense attention. Under dynamic gating,VideoNSA achieves smoother temporal coverage, alleviating over-reliance on early positions while avoiding the global diffusion characteristic of dense attention. In contrast, the compression branch exhibits strong accumulation at the beginning with an even steeper decay, indicating that token merging exerts its strongest impact on early-stage representations. The selection branch yields very few sinks across the sequence, while the sliding window branch produces sparse peaks at periodic boundaries of local neighborhoods. More analysis about attention sinks on various sparse attention settings can be found in Appendix M. 9 VideoNSA: Native Sparse Attention Scales Video Understanding"
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we present VideoNSA, hybrid hardware-aware sparse attention model that significantly advances video understanding across various tasks. By dynamically fusing block-wise compression, salient block selection, and sliding window, VideoNSA effectively preserves critical information while achieving near-linear scalability in efficiency and memory. Our experiments demonstrate that VideoNSA consistently outperforms existing methods on key tasks including long video understanding, temporal reasoning, and spatial understanding. While the prefill stage remains the primary bottleneck, our findings confirm that this hybrid sparse approach provides powerful and scalable framework, paving the way for more capable video foundation models."
        },
        {
            "title": "6 ACKNOWLEDGEMENT",
            "content": "This work is supported by NSF award IIS-2127544 and NSF award IIS-2433768. We thank Lambda, Inc. for their compute resource help on this project."
        },
        {
            "title": "7 ETHICS STATEMENT",
            "content": "This research on video understanding utilizes publicly available datasets, ensuring that all data complies with privacy regulations. We acknowledge the potential biases that can arise in automatic answer generation, particularly concerning gender, race, or other characteristics. We have taken measures to evaluate and minimize such biases, while remaining committed to further improvements. Additionally, we recognize the potential risks of misuse, such as generating misleading answers, and have checked the training dataset with safeguards against such applications."
        },
        {
            "title": "8 REPRODUCIBILITY STATEMENT",
            "content": "We have made several efforts to ensure the reproducibility of our work. All the key implementation details, including the architecture of our model, the training procedures, and hyperparameter settings, are described in supplementary meterial Section B. The settings of the used evaluation benchmarks are in Section to further support reproducibility."
        },
        {
            "title": "9 THE USE OF LARGE LANGUAGE MODELS",
            "content": "Large language models (LLMs) were used only for light editorial purposes, such as minor grammar checking and language polishing. They were not used for generating scientific content, research ideation, experiment design, or analysis. The authors take full responsibility for the entirety of the paper, and LLMs are not considered contributors or eligible for authorship."
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, and Alexey Tumanov. Rocketkv: Accelerating long-context llm inference via two-stage kv cache compression, 2025. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, et al. Temporalbench: Benchmarking fine-grained temporal understanding for multimodal video models. arXiv preprint arXiv:2410.10818, 2024. 10 VideoNSA: Native Sparse Attention Scales Video Understanding Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, JenqNeng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models. In European Conference on Computer Vision, pp. 1935. Springer, 2024a. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Sixun Dong, Juhua Hu, Mian Zhang, Ming Yin, Yanjie Fu, and Qi Qian. Mmtok: Multimodal coverage maximization for efficient inference of vlms, 2025. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pp. 1119811201, 2024. Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Peiyuan Zhou, Jiaxing Qi, Junjie Lai, Hayden Kwok-Hay So, Ting Cao, Fan Yang, et al. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. Yizhao Gao, Shuming Guo, Shijie Cao, Yuqing Xia, Yu Cheng, Lei Wang, Lingxiao Ma, Yutao Sun, Tianzhu Ye, Li Dong, Hayden Kwok-Hay So, Yu Hua, Ting Cao, Fan Yang, and Mao Yang. Seerattention-r: Sparse attention adaptation for long reasoning, 2025. Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and Min Lin. When attention sink emerges in language models: An empirical view. arXiv preprint arXiv:2410.10781, 2024. Zhiyuan He, Yike Zhang, Chengruidong Zhang, Huiqiang Jiang, Yuqing Yang, and Lili Qiu. Trianglemix: lossless and efficient attention pattern for long context prefilling, 2025. Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, JoonYoung Lee, Seon Joo Kim, and Minho Shim. Multi-granular spatio-temporal token merging for training-free acceleration of video llms. arXiv preprint arXiv:2507.07990, 2025. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. Jindong Jiang, Xiuyu Li, Zhijian Liu, Muyang Li, Guo Chen, Zhiqi Li, De-An Huang, Guilin Liu, Zhiding Yu, Kurt Keutzer, Sungjin Ahn, Jan Kautz, Hongxu Yin, Yao Lu, Song Han, and Wonmin Byeon. Storm: Token-efficient long video understanding for multimodal llms, 2025a. Pengfei Jiang, Hanjun Li, Linglan Zhao, Fei Chao, Ke Yan, Shouhong Ding, and Rongrong Ji. Visa: Group-wise visual token selection and aggregation via graph summarization for efficient mllms inference. 2025b. doi: 10.1145/3746027.3755792. Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13700 13710, 2024. Chaitanya Joshi. Transformers are graph neural networks. arXiv preprint arXiv:2506.22084, 2025. VideoNSA: Native Sparse Attention Scales Video Understanding Michael Kalloniatis and Charles Luu. Temporal resolution. In Helga Kolb, Eduardo Fernandez, Ralph Nelson, and Bryan Jones (eds.), Webvision: The Organization of the Retina and Visual System. University of Utah Health Sciences Center, Salt Lake City (UT), 2007. URL https: //www.ncbi.nlm.nih.gov/books/NBK11559/. Accessed September 24, 2025. Minsoo Kim, Kyuhong Shim, Jungwook Choi, and Simyung Chang. Infinipot-v: Memoryconstrained kv cache compression for streaming video understanding, 2025. Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, and Xun Zhou. Flexprefill: context-aware sparse attention mechanism for efficient long-sequence inference. arXiv preprint arXiv:2502.20766, 2025. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Handong Li, Yiyuan Zhang, Longteng Guo, Xiangyu Yue, and Jing Liu. Breaking the encoder barrier for seamless video-language understanding, 2025a. Hongyu Li, Songhao Han, Yue Liao, Junfeng Luo, Jialin Gao, Shuicheng Yan, and Si Liu. Reinforcement learning tuning for videollms: Reward design and data efficiency. arXiv preprint arXiv:2506.01908, 2025b. Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024b. Yixuan Li, Changli Tang, Jimin Zhuang, Yudong Yang, Guangzhi Sun, Wei Li, Zejun Ma, and Chao Zhang. Improving llm video understanding with 16 frames per second, 2025c. Yucheng Li, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn, Chengruidong Zhang, Amir Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, et al. Scbench: kv cache-centric analysis of long-context methods. arXiv preprint arXiv:2412.10319, 2024c. Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, and Lili Qiu. Mminference: Accelerating pre-filling for long-context vlms via modality-aware permutation sparse attention, 2025d. Manlai Liang, Wanyi Huang, Mandi Liu, Huaijun Li, and Jinlong Li. Lag-relative sparse attention in long context training, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Zhenyu Ning, Guangda Liu, Qihao Jin, Wenchao Ding, Minyi Guo, and Jieru Zhao. Livevlm: Efficient online video understanding via streaming-oriented kv cache and retrieval, 2025. Dhruv Pai, Timor Averbuch, Mason Wang, and Ben Keigwin. Sparsity is cool. Blog post, Tilde Research, June 2025a. URL https://www.tilderesearch.com/blog/sparse-attn. Tilde; correspondence to dhruv@tilderesearch.com. Dhruv Pai, Timor Averbuch, Mason Wang, and Ben Keigwin. Sparsity is cool. https://www. tilderesearch.com/blog/sparse-attn, June 25 2025b. Tilde Research Blog. Francesco Pappone. Attention sinks from the graph perspective. https://publish. obsidian.md/the-tensor-throne/Transformers+as+GNNs/Attention+ sinks+from+the+graph+perspective, August 2025. Blogpost. Minghao Qin, Xiangrui Liu, Zhengyang Liang, Yan Shu, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, and Zheng Liu. Video-xl-2: Towards very long-video understanding through task-aware kv sparsification, 2025a. Minghao Qin, Xiangrui Liu, Zhengyang Liang, Yan Shu, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, and Zheng Liu. Video-xl-2: Towards very long-video understanding through task-aware kv sparsification. arXiv preprint arXiv:2506.19225, 2025b. 12 VideoNSA: Native Sparse Attention Scales Video Understanding Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, and Wenhu Chen. Vamba: Understanding hour-long videos with hybrid mamba-transformers, 2025. Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan. Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models. arXiv preprint arXiv:2410.23266, 2024. Kele Shao, Keda Tao, Kejia Zhang, Sicheng Feng, Mu Cai, Yuzhang Shang, Haoxuan You, Can Qin, Yang Sui, and Huan Wang. When tokens talk too much: survey of multimodal long-context token compression across images, videos, and audios, 2025. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1822118232, 2024. Enxin Song, Wenhao Chai, Weili Xu, Jianwen Xie, Yuxuan Liu, and Gaoang Wang. Video-mmlu: massive multi-discipline lecture understanding benchmark. arXiv preprint arXiv:2504.14693, 2025a. Enxin Song, Wenhao Chai, Tian Ye, Jenq-Neng Hwang, Xi Li, and Gaoang Wang. Moviechat+: Question-aware sparse memory for long video question answering. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025b. Core Team, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, and Bingquan Xia. Mimo-vl technical report, 2025a. URL https://arxiv.org/abs/2506.03569. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025b. Kwai Keye Team. Kwai keye-vl technical report, 2025. URL https://arxiv.org/abs/ 2507.01949. Qwen Team. Qwen2.5-vl-7b-instruct-awq. https://huggingface.co/Qwen/Qwen2. 5-VL-7B-Instruct-AWQ, 2024. Abhishek Tyagi, Arjun Iyer, William Renninger, Christopher Kanan, and Yuhao Zhu. Dynamic sparse training of diagonally sparse networks, 2025. Pavlo Vasylenko, Marcos Treviso, and Andre F. T. Martins. Long-context generalization with sparse attention, 2025. Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and Liqiang Nie. Retake: Reducing temporal and knowledge redundancy for long video understanding. arXiv preprint arXiv:2412.20504, 2024. 13 VideoNSA: Native Sparse Attention Scales Video Understanding Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025a. Yixuan Wang, Huang He, Siqi Bao, Hua Wu, Haifeng Wang, Qingfu Zhu, and Wanxiang Che. Proxyattn: Guided sparse attention via representative heads. arXiv preprint arXiv:2509.24745, 2025b. Ziyi Wang, Haoran Wu, Yiming Rong, Deyang Jiang, Yixin Zhang, Yunlong Zhao, Shuang Xu, and Bo XU. Lvc: lightweight compression framework for enhancing vlms in long video understanding, 2025c. Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, and Linfeng Zhang. Token pruning in multimodal large language models: Are we solving the right problem? arXiv preprint arXiv:2502.11501, 2025. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. Xattention: Block sparse attention with antidiagonal scoring. arXiv preprint arXiv:2503.16428, 2025a. Weili Xu, Enxin Song, Wenhao Chai, Xuexiang Wen, Tian Ye, and Gaoang Wang. AuroarXiv preprint ralong: Bringing rnns back to efficient open-ended video understanding. arXiv:2507.02591, 2025b. Ran Yan, Youhe Jiang, and Binhang Yuan. Flash sparse attention: An alternative efficient implementation of native sparse attention kernel, 2025. Chenyu Yang, Xuan Dong, Xizhou Zhu, Weijie Su, Jiahao Wang, Hao Tian, Zhe Chen, Wenhai Wang, Lewei Lu, , and Jifeng Dai. Pvc: Progressive visual token compression for unified image and video processing in large vision-language models. arXiv preprint arXiv:2412.09613, 2024. Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces, 2025a. URL https://arxiv.org/abs/2412.14171. Lijie Yang, Zhihao Zhang, Arti Jain, Shijie Cao, Baihong Yuan, Yiwei Chen, Zhihao Jia, and Ravi Netravali. Less is more: Training-free sparse attention with global locality for efficient reasoning, 2025b. Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1979219802, 2025c. Sihan Yang, Runsen Xu, Chenhang Cui, Tai Wang, Dahua Lin, and Jiangmiao Pang. Vflowopt: token pruning framework for lmms with visual information flow-guided optimization. arXiv preprint arXiv:2508.05211, 2025d. Songlin Yang and Yu Zhang. Fla: triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/fla-org/ flash-linear-attention. Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, and Mengye Ren. Streammem: Query-agnostic kv cache memory for streaming video understanding, 2025e. 14 VideoNSA: Native Sparse Attention Scales Video Understanding Linli Yao, Yicheng Li, Yuancheng Wei, Lei Li, Shuhuai Ren, Yuanxin Liu, Kun Ouyang, Lean Wang, Shicheng Li, Sida Li, Lingpeng Kong, Qi Liu, Yuanxing Zhang, and Xu Sun. Timechat-online: 80 Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. Native sparse attention: Hardware-aligned and natively trainable sparse attention, 2025a. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025b. Zhihao Zhan, Jianan Zhao, Zhaocheng Zhu, and Jian Tang. Overcoming long-context limitations of state-space models via context-dependent sparse attention, 2025. Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025a. Ce Zhang, Kaixin Ma, Tianqing Fang, Wenhao Yu, Hongming Zhang, Zhisong Zhang, Yaqi Xie, Katia Sycara, Haitao Mi, and Dong Yu. Vscan: Rethinking visual token reduction for efficient large vision-language models. arXiv preprint arXiv:2505.22654, 2025b. Dell Zhang, Xiangyu Chen, Jixiang Luo, Mengxi Jia, Changzhi Sun, Ruilong Ren, Jingren Liu, Hao Sun, and Xuelong Li. Infinite video understanding. arXiv preprint arXiv:2507.09068, 2025c. Haichao Zhang and Yun Fu. Neural discrete token representation learning for extreme token reduction in video large language models, 2025. Hanzhi Zhang, Heng Fan, Kewei Sha, Yan Huang, and Yunhe Feng. Dam: Dynamic attention mask for long-context large language model inference acceleration, 2025d. Jintao Zhang, Rundong Su, Chunyu Liu, Jia Wei, Ziteng Wang, Haoxu Wang, Pengle Zhang, Huiqiang Jiang, Haofeng Huang, Chendong Xiang, Haocheng Xi, Shuo Yang, Xingyang Li, Yuezhou Hu, Tianyu Fu, Tianchen Zhao, Yicheng Zhang, Boqun Cao, Youhe Jiang, Chang Chen, Kai Jiang, Huayu Chen, Min Zhao, Xiaoming Xu, Yi Wu, Fan Bao, Jun Zhu, and Jianfei Chen. survey of efficient attention methods: Hardware-efficient, sparse, compact, and linear attention. 2025e. Jintao Zhang, Haoxu Wang, Kai Jiang, Shuo Yang, Kaiwen Zheng, Haocheng Xi, Ziteng Wang, Hongzhou Zhu, Min Zhao, Ion Stoica, et al. Sla: Beyond sparsity in diffusion transformers via fine-tunable sparse-linear attention. arXiv preprint arXiv:2509.24006, 2025f. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024a. Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention, 2025g. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024b. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024c. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024d. URL https://arxiv.org/abs/2410.02713. Yunzhu Zhang, Yu Lu, Tianyi Wang, Fengyun Rao, Yi Yang, and Linchao Zhu. Flexselect: Flexible token selection for efficient long video understanding. arXiv preprint arXiv:2506.00993, 2025h. 15 VideoNSA: Native Sparse Attention Scales Video Understanding Weilin Zhao, Zihan Zhou, Zhou Su, Chaojun Xiao, Yuxuan Li, Yanghao Li, Yudi Zhang, Weilun Zhao, Zhen Li, Yuxiang Huang, et al. Infllm-v2: Dense-sparse switchable attention for seamless short-to-long adaptation. arXiv preprint arXiv:2509.24663, 2025. Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. URL https://arxiv.org/abs/2408.05517. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv e-prints, pp. arXiv2406, 2024. Orr Zohar, Xiaohan Wang, Rui Li, Andres Marafioti, Miquel Farre, Merve Noyan, Leandro von Werra, Serena Yeung-Levy, and Thomas Wolf. Apollo2: Exploring the long-video frontier of large multimodal models, 2025. 16 VideoNSA: Native Sparse Attention Scales Video Understanding The supplementary material is structured as follows:"
        },
        {
            "title": "Appendix",
            "content": "Iiterature review about the related works in Section A. The training settings for VideoNSA in Section B. The introduction of the used evaluation benchmarks and settings in Section C. More results on long-form video benchmarks in Section D. More results on temporal reasoning benchmarks in Section E. More results on spatial understanding benchmarks in Section F. Visualization of attention pattern in each branch in Section G. More results on branch combination in Section H. More results on information scaling study in Section I. More results on attention scaling study in Section J. Full gate values distribution in Section K. More inter-head gate similarites visualization in Section L. More analysis about attention sinks on various sparse attention settings can be found in Section M. Visualization of attention sinks in dense attention in Section N."
        },
        {
            "title": "A RELATED WORK",
            "content": "A.1 EFFICIENT VIDEO UNDERSTANDING Video understanding systems typically convert videos into long sequences of vision tokens, which can easily exceed GPU memory and slow down inference as the video length grows. To address this, existing work mainly address this by token compression, alternative sequence modeling, and KVcache compression. One important line of work emphasizes token compression. Spatial or temporal token merging methods (Wang et al., 2025c; Zhang & Fu, 2025; Li et al., 2025c; Jiang et al., 2025a; Li et al., 2025a; Shao et al., 2025; Song et al., 2024; Chai et al., 2024) progressively discard redundant content, while question-/task-aware strategies (Jiang et al., 2025b; Dong et al., 2025; Yao et al., 2025; Song et al., 2025b) tailor retained tokens to the query. These approaches substantially lower FLOPs but still rely on dense attention once tokens are merged. Beyond pure self-attention, Mamba-based or hybrid architectures (Jiang et al., 2025a; Ren et al., 2025; Xu et al., 2025b) inject state-space or recurrent modules to approach linear-time inference while preserving long-range dependencies. Also, there exists approach to design data efficient systems for further fine-tuning (Li et al., 2025b). Another direction targets the keyvalue cache during decoding via task-aware sparsification and streaming-friendly memory (Qin et al., 2025a; Ning et al., 2025; Kim et al., 2025; Yang et al., 2025e) reduce memory and improve throughput, yet prefill still scales quadratically with sequence length. In contrast to methods that mostly decide where to drop or compress tokens, our approach systematically probe the effectiveness of native sparse attention (Yuan et al., 2025a) that restructures attention itself to be learnable and sparse from the ground up. VideoNSA attains near-linear scalability up to 128K tokens and processes over 10,000 frames on single GPU, outperforming compression-only pipelines on long-video understanding, temporal reasoning, and spatial understanding tasks. A.2 SPARSE ATTENTION MECHANISM Sparse attention is central strategy for efficient long-context modeling in language and multimodal systems. Surveys (Zhang et al., 2025e) categorize approaches into pattern-based vs. dynamic/learned. Pattern-based sparsity. Methods such as Longformer (Beltagy et al., 2020), StreamingLLM (Xiao et al., 2024), and TriangleMix (He et al., 2025) prescribe fixed local/strided patterns that can be applied training-free; recent multimodal works (Zhang et al., 2025d; Yang et al., 2025b) follow similar principles, while hardware-efficient kernels like Flash Sparse Attention (Yan et al., 2025) further reduce prefill latency. InfLLM-V2 (Zhao et al., 2025) uses switchable dense 17 VideoNSA: Native Sparse Attention Scales Video Understanding sparse attention to smoothly adapt models from short to long sequences while maintaining consistency and achieving efficient acceleration with high performance. ProxyAttn (Wang et al., 2025b) uses representative heads for fine-grained block importance estimation, enabling faster sparse attention with minimal performance loss. Dynamic and trainable sparsity. Contentor gradientadaptive mechanisms select important connections (e.g., diagonal selection (Tyagi et al., 2025) or lag-relative strategies (Liang et al., 2025)); trainable sparse attention improves long-context reasoning (Gao et al., 2025; Vasylenko et al., 2025; Gao et al., 2024), diffusion-based video generation (Zhang et al., 2025g), and state-space models (Zhan et al., 2025). SLA (Zhang et al., 2025f) decomposes attention weights into critical, marginal, and negligible parts, combining sparse and low-rank acceleration to greatly reduce computation while preserving generation quality. Hybrid approaches such as RocketKV (Behnam et al., 2025) combine token/cache compression with learned sparsity, and MMInference (Li et al., 2025d) accelerates modality-aware sparse prefill for VLMs. Despite these advances, most techniques are optimized for text or short multimodal contexts and do not directly address the ultra-long, highly redundant spatio-temporal structure of videos. VideoNSA unifies block-wise compression, salient block selection, and sliding-window branch under learnable gates that dynamically allocate computation across three native sparse branches (Yuan et al., 2025a). This end-to-end, data-driven design preserves critical global/local dependencies while scaling nearly linearly in both time and memory."
        },
        {
            "title": "B DETAILED TRAINING SETTINGS",
            "content": "Training hyperparameters for VideoNSA are shown in Table 4. We filter subset of LLaVA-Video178K (Zhang et al., 2024d) as the training data. For each video, we uniformly sample at 4 frames per second and retain only those with 350550 frames, resulting in 216K video questionanswer pairs from the original 961K pairs in LLaVA-Video-178K (Zhang et al., 2024d). Table 4: Training hyper-parameters for VideoNSA. Hyper-parameters Fine-tuning trainable parameters warmup schedule warmup start factor warmup ratio learning rate schedule optimizer optimizer hyper-parameters weight decay max norm epoch peak learning rate total equivalent batch size ViT + MLP + LLM linear 1e-5 0.1 cosine AdamW (Loshchilov & Hutter, 2017) β1, β2 = (0.9, 0.999) 0.01 1 1 1e-"
        },
        {
            "title": "C EVALUATION BENCHMARKS AND SETTINGS",
            "content": "We list all the hyper-parameters and prompt used for evaluation as shown in Table 5. MORE RESULTS ON LONG-FORM VIDEO BENCHMARKS We take LongVideoBench (Wu et al., 2024), LongTimeScope (Zohar et al., 2025), MLVU (Zhou et al., 2024), and TimeScope (Zohar et al., 2025) as representative long-video benchmarks and compare against existing token compression and sparse attention methods. As shown in Table 6, Table 7, Table 8, and Table 9, VideoNSA achieves comparable performance without specialized designs. Moreover, we observe that VideoNSA significantly outperforms the baselines on subtasks related to temporal reasoning and on videos of extended length. 18 VideoNSA: Native Sparse Attention Scales Video Understanding Table 5: Evaluation settings summary for each benchmarks. For all benchmarks we set temperature, top p, number of beams to 0, 0, 1 respectively. # TPF stands for the vision tokens per frame, and # stands for the number of sampling frames. Benchmark # TPF # # Max New Tokens LongVideoBench (Wu et al., 2024) LongTimeScope (Zohar et al., 2025) TimeScope (Zohar et al., 2025) MLVUtest (Zhou et al., 2024) Tomato (Shangguan et al., 2024) VSIBench (Yang et al., 2025a) 512 128 64 128 4FPS 256 512 2048 512 256 128 32 16 16 16 1024 16 Table 6: LongTimeScope results across baselines. Metrics include overall accuracy and task-specific scores across different steps. Flash Attn stands for Qwen2.5-VL-7B (Qwen et al., 2025) accelerated by Flash Infer, and Flash Attn + SFT stands for our fine-tuning version. Method Overall 28800 36000 OCR QA Temporal OCR QA Temporal OCR QA Temporal Flash Attn 40.7 Flash Attn + SFT 40.2 AWQ XAttn MInference tri-shape FlexPrefill FastV VisionZip VScan VideoNSA 41.1 44.4 28.4 39.1 35.6 31.1 40.4 44.4 54.0 42.0 46.0 30. 52.0 56.0 64.0 56.0 34.0 36.0 52.0 46.0 36.0 50.0 38.0 32.0 48.0 52.0 50.0 54.0 22.0 34.0 30.0 26.0 12.0 24.0 16.0 14.0 24.0 30.0 48.0 60.0 46.0 44.0 54.0 52.0 58.0 60.0 48.0 48.0 46.0 56.0 44.0 50.0 56.0 46.0 50.0 52.0 54.0 72.0 24.0 36.0 6.0 8.0 0.0 14.0 4.0 0.0 22.0 0.0 48.0 58.0 52.0 44.0 52.0 64.0 56.0 66.0 44.0 32.0 46.0 66.0 44.0 64.0 44.0 46.0 46.0 64.0 48.0 76.0 10.0 20.0 4.0 6.0 2.0 2.0 12.0 4.0 6.0 16."
        },
        {
            "title": "E MORE RESULTS ON TEMPORAL REASONING BENCHMARKS",
            "content": "We take Tomato (Shangguan et al., 2024) as the representative temporal reasoning benchmark and compare against existing token compression and sparse attention methods. As shown in Table 10, VideoNSAachieves comparable performance without specialized designs. Moreover, we observe that VideoNSA significantly outperforms the baselines on subtasks including object counting, shape description, and human actions."
        },
        {
            "title": "F MORE RESULTS ON SPATIAL UNDERSTANDING BENCHMARKS",
            "content": "We take VSIBench (Yang et al., 2025a) as the representative spatial understanding benchmark and compare against existing token compression and sparse attention methods. As shown in Table 11, VideoNSA achieves comparable performance without specialized designs. Moreover, we observe that VideoNSA significantly outperforms the baselines on subtasks including object relative direction, route planning, and object size estimation."
        },
        {
            "title": "G VISUALIZATION OF ATTENTION PATTERN IN EACH BRANCH",
            "content": "We visualize the attention patterns of the last layer across the three branches in Figure 10, Figure 11, Figure 12, and Figure 13, together with the final attention output, as representative examples. The compression branch reduces redundancy to preserve salient information, the selection branch highlights task-relevant regions with sparse activations, and the sliding window branch enforces local temporal coverage by focusing on short-range dependencies. These complementary roles collectively shape the final attention output. 19 VideoNSA: Native Sparse Attention Scales Video Understanding Table 7: LongVideoBench results across baselines. Metrics include overall accuracy and taskspecific scores across different steps. Flash Attn stands for Qwen2.5-VL-7B (Qwen et al., 2025) accelerated by Flash Infer, and Flash Attn + SFT stands for our fine-tuning version. Method Overall 600 TOS S2E E3E S2A SAA O3O T3O T3E O2E T2O S2O TAA T2E E2O SSS T2A 60 SOS 15 Flash Attn 58.7 Flash Attn + SFT 57.8 59.0 AWQ 59.1 XAttn 59.2 MInference 59.5 tri-shape 58.4 FlexPrefill 57.3 FastV 52.4 VisionZip 58.7 VScan 60.2 VideoNSA 58.5 38.4 69.9 66.0 70.5 56.9 57.6 58.1 53.4 63.2 63.2 55.6 52.4 63.1 63.1 39.2 64.6 72.7 61.7 65.6 52.3 55.8 38.4 65.6 61.7 73.9 56.9 65.2 55.4 57.5 57.5 56.6 62.5 51.2 55.4 66.2 39.2 63.3 74.4 58.0 64.6 52.0 60.0 34.2 72.0 64.9 68.2 59.7 57.6 52.7 50.7 69.0 52.6 63.9 57.3 61.5 67.7 43.3 62.0 73.8 63.0 67.7 50.9 59.4 36.0 70.0 66.0 67.2 57.3 58.1 55.8 53.8 64.5 62.2 64.3 56.3 65.2 66.7 41.3 58.5 75.2 60.7 68.8 50.6 60.6 34.6 74.3 66.0 68.3 58.7 56.6 53.1 52.4 68.0 56.9 60.1 58.8 60.5 65.2 39.2 63.6 74.6 66.9 67.3 50.8 60.9 34.6 73.2 66.0 69.5 58.7 58.1 55.8 52.4 68.0 55.6 61.5 60.0 60.5 66.7 38.2 63.6 74.6 66.9 67.8 51.1 61.7 31.5 65.6 62.8 71.6 59.7 59.1 58.1 52.1 65.5 51.3 62.5 48.8 61.5 72.3 42.3 63.3 71.5 65.4 58.2 52.1 57.3 43.8 64.5 60.6 70.5 52.8 56.1 52.7 48.0 59.8 67.1 56.9 48.8 67.7 66.2 40.2 58.2 69.8 61.7 70.9 48.9 53.2 32.9 63.4 66.0 58.0 54.2 50.0 51.4 42.5 57.5 47.4 58.3 45.1 56.9 61.5 30.9 51.9 62.2 61.7 58.2 46.8 57.0 29.5 69.0 65.0 69.6 56.3 54.1 56.1 55.5 61.2 58.5 61.9 60.2 58.0 73.4 41.4 61.3 74.2 65.9 73.7 50.3 59.9 48.1 65.1 67.6 74.1 55.6 55.5 58.4 56.3 62.2 57.0 63.9 53.3 56.2 71.6 35.9 62.7 67.5 72.4 66.3 55.1 Table 8: MLVU results across baselines. Metrics include overall accuracy and task-specific scores across different steps. Flash Attn stands for Qwen2.5-VL-7B (Qwen et al., 2025) accelerated by Flash Infer, and Flash Attn + SFT stands for our fine-tuning version. Method Overall PlotQA Needle Ego Count Order Anomaly Reco Topic Reason. SportsQA TutorialQA 51.2 Flash Attn Flash Attn + SFT 51.2 46.0 AWQ 50.2 XAttn 49.2 MInference 49.2 tri-shape 46.0 FlexPrefill 41.8 FastV 33.1 VisionZip 48.1 VScan 51.8 VideoNSA 58.0 58.0 42.7 60.0 56.0 56.0 54.0 44.0 30.0 58.0 48.0 68.3 58.3 53.0 64.7 64.7 64.7 54.7 45.0 26.7 63.3 69.3 52.8 31.7 58.5 23.3 40.9 27.2 56.5 28.0 48.9 29.7 48.9 29.7 42.7 24.7 47.2 18.3 30.2 6.7 50.9 28.3 51.3 27.7 25.7 40.0 50.2 29.4 26.6 26.6 40.9 30.0 22.9 24.3 34.6 46.2 43.6 57.0 41.6 41.6 41.6 36.6 46.2 41.0 43.6 44.5 79.1 81.3 65.0 74.9 77.1 77.1 72.6 84.6 68.1 78.0 86. 38.9 36.1 38.3 39.7 39.7 39.7 29.7 28.6 19.7 47.2 47.7 48.8 37.2 39.2 39.9 39.9 39.9 32.2 32.2 26.4 39.5 31."
        },
        {
            "title": "H MORE RESULTS ON BRANCH COMBINATION",
            "content": "In this section, we report detailed results of different branch combinations across three domains, including long video understanding (Table 12, Tavke 13, Table 14, and Table 15), temporal reasoning  (Table 16)  , and spatial understanding  (Table 17)  . The corresponding performances are summarized in the table, which highlights how the use of individual branches or their combinations affects downstream tasks."
        },
        {
            "title": "I MORE RESULTS ON INFORMATION SCALING STUDY",
            "content": "Figure 15 shows the scaling performance of VideoNSA under different context allocation strategies on LongTimeScope and MLVU. Both benchmarks were trained with maximum context length of 32K tokens, yet their performance consistently improves when scaled to 64K, beyond the training budget. On LongTimeScope (Zohar et al., 2025), the best results emerge around 512 frames with 128 TPF at 64K tokens, underscoring the datasets reliance on extended temporal coverage for long-horizon reasoning. In contrast, MLVU (Zhou et al., 2024) also peaks at 64K with the same allocation, but its contours are smoother, and competitive performance extends across broader range of frametoken trade-offs. This suggests that while LongTimeScope demands aggressive temporal scaling, MLVU benefits from more balanced distribution of temporal and spatial information. In addition to the overall scaling trends, we further report detailed subtask-level results under different allocation settings in Table 18, Table 19, Table 20, Table 21, Table 22, and Table 23."
        },
        {
            "title": "J MORE RESULTS ON ATTENTION SCALING STUDY",
            "content": "Figure 15 evaluates the scaling behavior of VideoNSA under different attention allocation strategies, where the x-axis denotes the sliding window size (log scale), the y-axis shows the block count, and the size and color of each marker reflect performance, with the dashed blue curve indicating configurations of equal attention budget and arrows marking the training setting as well as reduced-budget 20 VideoNSA: Native Sparse Attention Scales Video Understanding Table 9: TimeScope results across baselines. Metrics include overall accuracy and task-specific scores across different steps. Flash Attn stands for Qwen2.5-VL-7B (Qwen et al., 2025) accelerated by Flash Infer, and Flash Attn + SFT stands for our fine-tuning version."
        },
        {
            "title": "Method",
            "content": "Overall 60 120 180 300 600 1200 1800 3600 7200 10800 Flash Attn 81.0 Flash Attn + SFT 76.8 AWQ XAttn MInference tri-shape FlexPrefill FastV VisionZip VScan VideoNSA 83.1 82.7 82.7 83.0 46.5 43.5 80.3 83.7 96.7 96.0 96.0 94.7 94.0 88.0 82.0 68.7 52.7 96.7 96.7 96.0 95.3 90.7 78.0 78.0 54.7 41.3 94.0 93.4 93.4 92.0 92.7 89.4 82.7 72.7 70.7 93.4 94.0 93.4 92.0 92.7 87.4 80.0 74.0 70.0 93.4 94.0 93.4 92.0 92.7 87.4 80.0 74.0 70.0 96.7 96.0 96.7 95.3 96.0 95.3 86.0 77.3 55.3 82.7 76.0 74.0 54.0 32.7 32.7 29.3 29.3 34.0 92.0 66.7 60.0 43.3 35.3 26.0 30.7 29.3 28.0 96.7 96.7 96.0 93.3 92.7 89.3 81.3 60.0 55.3 96.7 96.0 97.4 92.0 85.4 91.6 89.3 73.3 63. 41.3 40.7 50.7 50.0 50.0 35.3 20.0 23.3 41.3 52.0 Table 10: Tomato results across baselines. Metrics include overall accuracy and task-specific scores across different steps. Flash Attn stands for Qwen2.5-VL-7B (Qwen et al., 2025) accelerated by Flash Infer, and Flash Attn + SFT stands for our fine-tuning version. Method Overall Direction Count Rotation Shape & Trend Vel. & Freq. Visual Cues Human Simulated Object Flash Attn 22.6 Flash Attn + SFT 21.7 21.4 XAttn 23.0 MInference 23.7 FlexPrefill 21.6 FastV 19.1 VisionZip 23.6 VScan 26.5 VideoNSA 23.6 19.6 22.1 22.6 23.3 20.6 17.6 25.3 21.6 23.3 23.3 22.9 27.1 25.0 26.0 16.8 21.9 31.5 16.1 18.2 19.6 18.9 22.7 20.3 21.0 19.9 22.0 22.9 26.0 17.9 22.0 22.0 23.3 19.3 24.2 25.6 21.9 18.1 17.1 20.0 21.4 12.7 19.0 20.5 23.3 42.9 38.6 42.9 37.1 35.7 30.0 42.9 40. 18.0 18.8 15.5 16.6 17.1 17.1 14.8 18.7 21.7 19.7 18.0 21.5 20.6 22.7 24.2 21.5 21.9 23.6 27.9 25.6 26.8 29.6 29.9 25.6 22.3 28.7 29.3 configurations (3.6% and 1.8%); on LongVideoBench, performance peaks near the training configuration and degrades when allocating excessive budget to local attention through larger sliding windows, while the best configuration achieves strong results with only 3.6% of the full budget, and on TimeScope, performance is even more sensitive, with larger sliding windows quickly reducing accuracy whereas maintaining more global blocks yields superior outcomes, and overall the results confirm that training allocations are well balanced, that prioritizing global attention is consistently more effective than enlarging local windows under equal budget, and that VideoNSA sustains leading performance with as little as 3.6% or less of the full attention cost, demonstrating both efficiency and hardware awareness. In addition to the overall scaling trends, we further report detailed subtask-level results under different allocation settings in Table 18, Table 19, Table 20, Table 21, Table 22, and Table 23."
        },
        {
            "title": "K FULL GATE VALUES DISTRIBUTION",
            "content": "L MORE INTER-HEAD GATE SIMILARITES VISUALIZATION L0 L1 21 VideoNSA: Native Sparse Attention Scales Video Understanding Table 11: VSIBench results across baselines. Metrics include overall accuracy and task-specific scores across different steps. Flash Attn stands for Qwen2.5-VL-7B (Qwen et al., 2025) accelerated by Flash Infer, and Flash Attn + SFT stands for our fine-tuning version. Method Overall Obj. Order Abs. Dist. Counting Rel. Dist. Size Est. Room Est. Route Plan. Rel. Dir. Flash Attn 29.7 Flash Attn + SFT 30.6 AWQ XAttn MInference tri-shape FlexPrefill FastV VisionZip VScan VideoNSA 35.0 36.6 36.5 34.9 34.0 32.1 34.4 36.0 25.7 31.9 32.7 36.5 35.7 34.1 31.7 28.8 33.0 25.5 16.0 14.2 18.1 18.2 18.2 21.6 21.7 17.9 21.9 19. 20.5 12.3 39.7 43.9 44.3 35.1 26.1 28.8 33.0 42.5 34.7 40.4 37.6 39.4 39.8 39.3 36.2 36.5 40.0 35.4 49.5 46.6 52.1 48.5 48.6 51.8 47.8 48.9 51.9 54.0 22.5 30.4 30.0 38.8 38.8 29.7 35.0 26.9 28.5 30.1 30.4 30.9 32.5 30.0 29.0 30.4 33.5 29.4 30.4 37.5 38.5 37.8 37.4 37.7 37.7 36.8 40.1 39.3 36.6 43. Figure 10: Attention pattern of the compression branch in the final layer of VideoNSA. L4 L8 L12 L5 L13 22 VideoNSA: Native Sparse Attention Scales Video Understanding Figure 11: Attention pattern of the selection branch in the final layer of VideoNSA. Figure 12: Attention pattern of the sliding window branch in the final layer of VideoNSA. L20 L17 L21 23 VideoNSA: Native Sparse Attention Scales Video Understanding Figure 13: Attention pattern of the final vision attention output in the final layer of VideoNSA. Table 12: LongVideoBranch results across different branch selection strategy. Metrics include overall accuracy and task-specific scores across different steps. Method Overall 600 TOS S2E E3E S2A SAA O3O T3O T3E O2E T2O S2O TAA T2E E2O SSS T2A 60 SOS 15 3600 VideoNSA + Test SFT 56.1 48.1 NSA-CMP 48.4 NSA-SLC 49.1 NSA-SWA 49.4 NSA-CMPSLC 49.3 NSA-SLCSWA 48.8 NSA-CMPSWA 57.0 46.6 59.1 61.7 69.3 56.9 63.6 52.7 50.7 56.3 59.2 59.7 43.9 55.4 64.6 38.1 58.2 70.4 60.5 65.1 48.1 50.5 38.4 51.6 56.4 53.4 50.0 45.5 51.4 41.1 54.0 42.1 43.1 45.1 47.7 53.9 26.8 53.2 55.2 64.2 47.6 44.3 49.0 32.9 61.3 59.6 58.0 52.8 48.5 46.0 43.8 52.9 36.8 47.2 42.7 44.6 55.4 33.0 48.1 53.5 55.6 50.3 45.7 50.7 37.0 52.7 56.4 59.1 51.4 48.5 43.2 45.2 55.2 42.1 48.6 45.1 46.2 61.5 30.9 45.6 54.1 65.4 48.7 46.5 49.5 34.3 55.9 61.7 58.0 56.9 48.5 47.3 41.1 56.3 35.5 52.8 47.6 46.2 55.4 34.0 41.8 54.1 63.0 48.2 48.2 48.8 32.9 58.1 61.7 55.7 52.8 47.0 46.0 46.6 54.0 34.2 48.6 47.6 47.7 54.0 35.1 48.1 54.1 64.2 49.2 48.2 49.3 34.3 53.8 59.6 54.6 52.8 50.0 48.7 42.5 57.5 40.8 51.4 42.7 46.2 55.4 29.9 45.6 57.6 64.2 48.7 45.9 (a) Information Scaling of LongTimeScope (b) Information Scaling of MLVU Figure 14: Scaling Performance of VideoNSA under Different Context Allocation Strategies. We highlight the token budget constraint to indicate settings with equal context length, and annotate the best-performing configuration under each benchmark. L24 L25 24 VideoNSA: Native Sparse Attention Scales Video Understanding Table 13: LongTimeScope results across different branch selection strategy. Metrics include overall accuracy and task-specific scores across different steps. Method Overall 18000 28800 OCR QA Temporal OCR QA Temporal OCR QA Temporal VideoNSA + Test SFT 40.9 25.1 NSA-CMP 37.1 NSA-SLC 29.8 NSA-SWA 32.4 NSA-CMPSLC 34.4 NSA-SLCSWA 31.6 NSA-CMPSWA 44.4 VideoNSA 52.0 42.0 20.0 22.0 30.0 38.0 34.0 34.0 36.0 34.0 38.0 36.0 30.0 38.0 50.0 54.0 42.0 24.0 40.0 22.0 24.0 36.0 20.0 30.0 48.0 62.0 38.0 40.0 50.0 58.0 36.0 46.0 46.0 54.0 46.0 56.0 40.0 52.0 54.0 72.0 18.0 0.0 12.0 4.0 8.0 8.0 16.0 0. 42.0 50.0 34.0 34.0 42.0 44.0 34.0 46.0 42.0 36.0 38.0 36.0 36.0 36.0 48.0 76.0 12.0 14.0 20.0 12.0 12.0 16.0 16.0 16.0 Table 14: TimeScope results across different branch selection strategy. Metrics include overall accuracy and task-specific scores across different steps."
        },
        {
            "title": "Method",
            "content": "Overall 60 120 180 300 600 1200 1800 3600"
        },
        {
            "title": "81.0\nFull Attn\nFlash Attn\n81.0\nFlash Attn + SFT 76.8\nAWQ\nXAttn\nMInference\ntri-shape\nFlexPrefill\nFastV\nVisionZip\nVScan\nRetake\nAdaRetake\nSFT + Test NSA\nNSA + Test SFT\nNSA-CMP\nNSA-SLC\nNSA-SWA\nNSA-CMPSLC\nNSA-SLCSWA\nNSA-CMPSWA\nVideoNSA",
            "content": "83.1 82.7 82.7 83.0 46.5 43.5 80.3 81.0 83.0 41.5 63.7 59.3 57.3 65.2 57.3 83.7 96.7 96.0 96.0 94.7 94.0 88.0 82.0 68.7 52.7 96.7 96.0 96.0 94.7 94.0 88.0 82.0 68.7 52.7 96.7 96.7 96.0 95.3 90.7 78.0 78.0 54.7 41.3 94.0 93.4 93.4 92.0 92.7 89.4 82.7 72.7 70.7 93.4 94.0 93.4 92.0 92.7 87.4 80.0 74.0 70.0 93.4 94.0 93.4 92.0 92.7 87.4 80.0 74.0 70.0 96.7 96.0 96.7 95.3 96.0 95.3 86.0 77.3 55.3 82.7 76.0 74.0 54.0 32.7 32.7 29.3 29.3 34.0 92.0 66.7 60.0 43.3 35.3 26.0 30.7 29.3 28.0 96.7 96.7 96.0 93.3 92.7 89.3 81.3 60.0 55.3 96.7 96.0 96.0 94.7 94.0 88.0 82.0 68.7 52.7 96.7 95.3 94.0 93.3 94.0 90.7 87.3 76.7 54.7 82.0 74.0 65.3 59.3 17.3 25.3 19.3 26.7 27.3 92.0 86.0 86.7 78.0 66.7 57.3 51.3 40.7 38.0 88.7 80.0 73.3 73.3 46.7 44.7 48.7 42.7 43.3 92.0 89.3 89.3 79.3 66.0 59.3 50.0 41.3 40.7 88.7 80.0 73.3 73.3 46.7 44.7 48.7 42.7 43.3 96.7 96.0 97.4 92.0 85.4 91.6 89.3 73.3 63.3 41.3 41.3 40.7 50.7 50.0 50.0 35.3 20.0 23.3 41.3 41.3 47.3 18.0 40.0 32.0 44.7 32.0 52.0 (a) Attention Scaling of LongVideoBench (b) Attention Scaling of TimeScope Figure 15: Scaling Performance of VideoNSA under Different Attention Allocation Strategies. We highlight the attention budget constraint to indicate settings with equal attention budget, and annotate the best-performing configuration under each benchmark. 25 VideoNSA: Native Sparse Attention Scales Video Understanding Table 15: MLVU results across different branch selection strategy. Metrics include overall accuracy and task-specific scores across different steps. Method Overall PlotQA Needle Ego Count Order Anomaly Reco Topic Reason. SportsQA TutorialQA NSA + Test SFT 51.6 43.9 NSA-CMP 47.7 NSA-SLC 40.2 NSA-SWA NSA-SLCSWA 42.4 NSA-CMPSWA 43. 56.0 36.0 50.0 40.0 42.0 46.0 61.7 35.0 50.0 40.0 48.3 40.0 66.0 31.7 42.9 52.4 41.5 15.0 45.3 16.7 43.4 18.3 28.6 24.3 22.9 24.3 25.7 35.7 51.3 30.8 33.3 30.8 38.5 33. 80.2 80.2 74.7 76.9 75.8 82.4 36.1 30.6 33.3 36.1 33.3 27.8 32.6 34.9 34.9 32.6 Table 16: Tomato results across different branch selection strategy. Metrics include overall accuracy and task-specific scores across different steps. Method Overall Direction Count Rotation Shape & Trend Vel. & Freq. Visual Cues Human Simulated Object NSA + Test SFT 23.4 23.3 NSA-CMP 24.0 NSA-SLC 24.0 NSA-SWA NSA-CMPSLC 23.5 23.0 NSA-SLCSWA NSA-CMPSWA 24.5 21.3 22.1 21.3 21.3 20.8 20.6 23.1 29.1 29.5 32.2 32.2 29.8 27.4 30.8 17.5 17.1 16.4 16.4 18.5 18.5 18.9 25.1 24.7 26.0 26.0 22.9 22.4 25.1 20.0 20.5 22.9 22.9 23.8 24.8 22. 40.0 34.3 32.9 32.9 34.3 32.9 32.9 19.3 19.2 19.8 19.8 19.0 19.5 21.0 19.3 22.7 21.5 21.5 26.8 21.0 23.6 28.5 27.3 28.7 28.7 25.8 26.8 28."
        },
        {
            "title": "ATTENTION SETTINGS",
            "content": "Figure 17a indicates that in the compression branch, smaller blocks produce sharper and higher sink peaks at the sequence start, while larger blocks used in training reduce the initial peak but introduce broader low-density diffusion with periodic boundary spikes. The selection sinks in 17b remain at consistently low densities under different configurations, suggesting that the top-k filtering mechanism robustly suppresses sink formation across different settings. Figure 17 shows the distribution of attention sinks under different sparse attention settings. When varying the window size, sinks are concentrated near the beginning and decay rapidly with position. Overall, larger windows yield lower sink density but broader coverage, while the training configuration (w = 256) strikes middle ground and exhibits sparse periodic clusters in the mid-to-late sequence, reflecting sensitivity to local boundaries learned during training."
        },
        {
            "title": "N DENSE ATTENTION SINK VISUALIZATION",
            "content": "26 VideoNSA: Native Sparse Attention Scales Video Understanding Table 17: VSIBench results across different branch selection strategy. Metrics include overall accuracy and task-specific scores across different steps. Method Overall Obj. Order Abs. Dist. Counting Rel. Dist. Size Est. Room Est. Route Plan. Rel. Dir. NSA + Test SFT 33.1 29.2 NSA-CMP 27.6 NSA-SLC 29.8 NSA-SWA 29.4 NSA-CMPSLC NSA-SLCSWA 29.1 NSA-CMPSWA 30. 24.3 19.9 18.0 22.8 19.9 19.9 22.5 19.8 16.3 10.9 15.6 16.3 12.2 15.6 31.2 12.6 17.3 17.4 15.1 18.5 15.3 38.0 29.3 32.0 32.3 31.0 31.4 31.1 49.8 48.7 47.8 49.8 51.1 49.6 52.5 32.2 26.7 24.8 27.2 25.5 26.5 26. 32.5 38.1 32.0 33.5 33.5 34.0 35.1 37.2 41.7 38.1 39.4 42.6 40.4 43.3 Table 18: Ablation study results on information scaling of LongTimeScope (Zohar et al., 2025). Metrics include overall accuracy and task-specific scores across different steps. # TPF stands for token per frame, and # stands for sampling frame number. # TPF # Overall"
        },
        {
            "title": "OCR QA Temporal OCR QA Temporal OCR QA Temporal",
            "content": "256 512 128 256 512 64 128 256 64 128 64 128 128 256 256 256 512 512 512 1024 1024 2048 42.9 41.1 42.0 41.3 41.6 40.2 44.4 38.7 41.6 41.1 38.4 54.0 48.0 54.0 60.0 58.0 56.0 58.0 52.0 54.0 56.0 52.0 52.0 50.0 54.0 48.0 50.0 54.0 56.0 50.0 46.0 50.0 62.0 36.0 28.0 26.0 36.0 32.0 26.0 30.0 30.0 22.0 32.0 26.0 46.0 62.0 42.0 62.0 46.0 62.0 48.0 62.0 46.0 60.0 44.0 64.0 54.0 72.0 52.0 56.0 46.0 66.0 46.0 62.0 40.0 60. 6.0 4.0 2.0 0.0 2.0 2.0 0.0 8.0 4.0 14.0 2.0 40.0 80.0 40.0 78.0 40.0 78.0 40.0 70.0 40.0 78.0 44.0 76.0 48.0 76.0 36.0 60.0 36.0 72.0 38.0 54.0 38.0 42.0 14.0 2.0 10.0 6.0 6.0 2.0 16.0 8.0 18.0 28.0 26.0 27 VideoNSA: Native Sparse Attention Scales Video Understanding Table 19: Ablation study results on information scaling of TimeScope (Zohar et al., 2025). Metrics include overall accuracy and task-specific scores across different steps. # TPF stands for token per frame, and # stands for sampling frame number. # TPF # Overall 60 120 180 300 600 1200 1800 3600 7200 10800 256 512 128 256 512 64 128 256 64 128 64 128 128 256 256 256 512 512 512 1024 1024 2048 73.1 72.5 76.5 76.1 75.8 78.5 76.5 77.3 81.7 81.8 82.7 96.7 94.7 93.4 85.4 72.0 62.6 57.3 56.6 54.0 95.4 94.0 92.7 82.0 72.7 64.6 57.3 56.3 53.3 98.0 97.4 96.0 86.7 78.7 78.0 63.3 58.6 54.0 96.7 96.7 91.4 86.7 76.0 74.6 64.0 62.0 56.0 95.4 94.7 90.7 86.7 76.0 75.3 66.0 62.6 53.3 96.7 95.4 94.7 88.0 80.7 82.6 71.3 62.0 58.0 98.0 97.4 96.0 86.7 78.7 78.0 63.3 58.6 54.0 96.7 96.7 90.7 83.4 76.7 78.0 72.0 66.6 59.3 96.7 95.4 94.7 90.0 84.7 88.0 78.0 69.3 64.0 98.0 97.4 94.0 85.4 80.7 92.0 78.0 72.6 66.0 96.7 95.4 94.7 90.0 82.0 91.3 88.0 73.3 63. 58.6 56.6 54.0 56.6 57.3 55.3 54.0 52.6 56.6 54.0 52.0 Table 20: Ablation study results on information scaling of LongVideoBench (Wu et al., 2024). Metrics include overall accuracy and task-specific scores across different steps. # TPF stands for token per frame, and # stands for sampling frame number. # TPF # Overall 600.0 TOS S2E E3E S2A SAA O3O T3O T3E O2E T2O S2O TAA T2E E2O SSS T2A 60.0 SOS 15.0 3600.0 512 128 256 512 128 256 512 64 128 256 64 128 64 128 128 128 256 256 256 512 512 512 1024 1024 58.3 57.4 57.9 59.0 58.7 58.2 59.4 57.7 58.5 58.3 58.4 58. 55.5 44.0 67.2 66.6 71.8 57.0 54.0 48.9 50.8 59.9 55.7 63.9 53.3 57.8 70.1 37.9 60.2 68.1 71.2 65.7 56.8 45.4 66.1 66.6 69.5 61.2 55.5 53.0 49.5 59.9 51.7 54.2 47.2 57.8 68.5 40.0 58.9 68.6 70.0 63.6 58.5 48.1 69.4 66.6 70.6 58.4 55.5 53.0 48.1 57.6 47.8 54.2 49.7 59.3 68.5 40.0 65.2 68.1 70.0 63.1 59.4 49.5 68.3 66.6 72.9 63.9 54.0 53.0 49.5 61.0 51.7 61.2 50.9 56.2 68.5 39.0 64.0 68.6 71.2 63.6 52.7 46.7 68.3 65.5 69.5 57.0 52.5 53.0 48.1 56.4 46.5 51.4 48.5 57.8 67.0 37.9 65.2 63.4 70.0 58.3 58.7 39.9 64.0 66.6 71.8 58.4 54.0 59.7 52.2 59.9 55.7 58.4 50.9 56.2 70.1 40.0 61.4 66.9 68.7 63.1 60.4 52.2 67.2 65.5 75.2 61.2 54.0 55.7 52.2 62.2 53.1 62.6 49.7 56.2 68.5 35.9 65.2 67.5 72.4 65.7 58.2 41.3 67.2 68.7 65.0 58.4 58.5 54.3 52.2 62.2 49.1 58.4 53.3 62.4 71.6 35.9 55.1 66.3 68.7 61.5 59.4 42.6 68.3 66.6 69.5 59.8 60.0 57.0 52.2 64.5 50.4 59.8 52.1 59.3 68.5 35.9 60.2 65.7 68.7 63.6 59.2 44.0 64.0 64.5 71.8 65.3 52.5 55.7 55.0 61.0 54.4 62.6 49.7 59.3 71.6 37.9 56.4 66.9 67.5 63.1 59.4 42.6 65.1 68.7 66.1 62.6 55.5 58.4 49.5 64.5 54.4 58.4 50.9 59.3 74.7 36.9 57.6 66.3 68.7 61.5 58.5 41.3 67.2 68.7 71.8 65.3 60.0 59.7 52.2 59.9 53.1 62.6 47.2 59.3 71.6 32.8 60.2 65.7 67.5 63.6 54.9 52.4 52.6 54.2 52.7 53.5 54.0 53.5 54.0 53.5 54.2 55.1 28 VideoNSA: Native Sparse Attention Scales Video Understanding Table 21: Ablation study results on information scaling of MLVU (Zhou et al., 2024). Metrics include overall accuracy and task-specific scores across different steps. # TPF stands for token per frame, and # stands for sampling frame number. # TPF # Overall PlotQA Needle Ego Count Order Anomaly Reco Topic Reason. SportsQA TutorialQA 256 512 128 256 512 64 128 256 64 128 128 128 256 256 256 512 512 512 1024 1024 49.6 49.2 50.6 51.2 48.0 51.2 51.8 48.6 51.8 48.0 46.0 52.0 50.0 50.0 54.0 50.0 48.0 50.0 56.0 52.0 52.7 51.0 57.7 56.0 49.3 62.7 69.3 51.0 66.0 51.0 53.2 24.3 47.5 24.3 60.7 24.3 56.9 27.7 49.4 22.7 55.1 24.3 51.3 27.7 47.5 24.3 53.2 26.0 49.4 29. 36.0 37.4 33.1 38.9 37.4 34.6 34.6 33.1 36.0 33.1 47.0 39.3 39.3 41.9 39.3 47.0 44.5 52.2 47.0 44.5 87.3 87.3 86.2 85.1 86.2 84.0 86.2 84.0 84.0 80.7 36.6 42.1 42.1 42.1 33.8 42.1 47.7 47.7 42.1 44.9 36.2 33.9 36.2 36.2 29.3 38.6 31.6 26.9 31.6 24.6 Table 22: Ablation study results on information scaling of Tomato (Shangguan et al., 2024). Metrics include overall accuracy and task-specific scores across different steps. # TPF stands for token per frame, and # stands for sampling frame number. FPS TPF Overall Direction Count Rotation Shape&Trend Velocity&Freq. Visual Cues Human Simulated Object 1 1 1 1 2 2 2 2 4 4 4 4 64 128 256 512 64 128 256 512 64 128 256 512 24.7 23.9 24.7 23.9 24.5 24.3 24.4 24.7 25.1 25.8 26.2 26.5 22.8 20.6 22.3 20.6 21.1 20.6 21.3 19.4 22.1 21.8 23.1 21.6 26.7 29.8 29.5 29.8 31.8 30.5 29.5 32.2 31.5 33.2 32.5 31. 20.6 19.9 19.6 19.9 19.6 20.3 18.5 21.3 19.6 21.3 20.6 22.0 25.1 24.7 25.1 24.7 22.4 24.7 26.5 25.6 26.5 25.6 26.9 25.6 27.1 23.8 25.2 22.9 25.7 23.3 24.8 23.8 23.3 25.7 26.7 23.3 34.3 32.9 35.7 34.3 35.7 38.6 37.1 37.1 38.6 37.1 37.1 40.0 21.2 19.8 20.8 20.3 20.7 20.7 20.3 20.0 21.2 21.5 21.8 21.7 23.2 24.0 23.3 21.5 21.9 22.3 24.0 24.0 25.0 25.3 25.3 23. 28.4 27.6 28.7 27.9 28.8 28.4 28.2 29.1 29.0 29.9 30.5 29.3 Table 23: Ablation study results on information scaling of VSIBench (Yang et al., 2025a). Metrics include overall accuracy and task-specific scores across different steps. # TPF stands for token per frame, and # stands for sampling frame number. TPF # Max Frames Overall Obj. Order Abs. Dist. Counting Rel. Dist. Size Est. Room Est. Route Plan. Rel. Dir. 512 512 256 512 128 256 512 64 128 256 64 128 32 64 128 128 256 256 256 512 512 512 1024 1024 34.9 34.8 36.0 34.6 35.6 35.5 34.8 34.2 36.0 33.9 35.8 35. 27.6 29.1 24.7 27.4 26.8 27.8 28.2 29.1 25.5 28.2 24.4 26.6 16.2 17.5 17.6 17.2 17.0 17.0 16.5 15.8 19.0 15.8 18.5 18.4 31.4 34.9 41.3 37.3 42.0 42.4 40.3 42.1 42.5 42.9 46.4 45.3 35.1 33.0 37.5 34.4 36.8 33.7 33.9 33.9 35.4 31.3 34.4 32.7 52.2 52.2 53.9 50.3 51.8 51.3 48.8 45.5 54.0 43.6 52.4 50.3 31.5 31.0 30.7 30.6 31.2 31.6 30.7 27.7 30.1 29.9 29.7 31. 40.1 36.5 39.1 35.5 35.0 35.5 36.5 37.0 37.5 36.5 37.5 37.0 44.6 43.9 43.3 43.8 44.2 44.5 43.3 42.9 43.6 43.1 43.0 43.7 Table 24: LongTimeScope (Zohar et al., 2025) results across different attention budget strategy. Metrics include overall accuracy and task-specific scores across different steps. Block Count Window Size Overall 18000 36000 OCR QA Temporal OCR QA Temporal OCR QA Temporal 36 35 34 28 20 4 16 64 32 64 128 512 1024 1900 128 512 44.0 44.0 41.8 42.0 40.9 0.0 41.6 42.4 56.0 50.0 54.0 58.0 50.0 56.0 50.0 56.0 52.0 56.0 0.0 0.0 52.0 56.0 52.0 56. 46.0 66.0 46.0 68.0 44.0 64.0 48.0 64.0 48.0 64.0 0.0 0.0 46.0 62.0 48.0 64.0 16.0 12.0 6.0 6.0 0.0 0.0 10.0 8.0 46.0 72.0 46.0 74.0 46.0 74.0 46.0 76.0 44.0 76.0 0.0 0.0 42.0 76.0 46.0 76.0 16.0 12.0 8.0 4.0 0.0 0.0 4.0 4.0 28.0 26.0 28.0 28.0 28.0 0.0 26.0 28.0 VideoNSA: Native Sparse Attention Scales Video Understanding Table 25: TimeScope (Zohar et al., 2025) results across different attention budget strategy. Metrics include overall accuracy and task-specific scores across different steps. Block Count Window Size Overall 60 120 180 300 600 1200 1800 3600 7200 10800 36 35 34 28 20 4 10 40 32 64 128 512 1024 1900 512 81.6 82.4 82.2 82.8 83.2 8.6 59.9 83.7 97.4 97.4 96.7 85.4 82.7 86.3 85.3 71.3 60.6 92.7 91.3 92.7 91.4 83.4 91.6 91.3 74.6 63.3 96.7 96.7 94.7 89.4 82.0 88.3 85.3 76.6 60.0 94.7 97.4 94.7 90.7 82.7 91.6 88.6 74.0 63.3 96.7 97.4 97.4 88.7 85.4 89.0 84.6 78.6 58.6 4.7 4.7 4.7 12.3 13.3 13.3 13.3 4.7 4.7 94.7 88.7 79.4 85.6 80.0 68.0 58.0 96.7 96.0 97.4 92.0 85.4 91.6 89.3 73.3 63.3 4.7 4.7 52.7 52.0 52.7 50.0 55.3 10.0 35.3 52.0 Table 26: LongVideoBench (Wu et al., 2024) results across different attention budget strategy. Metrics include overall accuracy and task-specific scores across different steps. Block Count Window Size Overall 600.0 TOS S2E E3E S2A SAA O3O T3O T3E O2E T2O S2O TAA T2E E2O SSS T2A 60.0 SOS 15.0 3600.0 36 35 34 28 20 4 17 32 64 128 512 1024 1900 64 59.9 60.1 60.2 59.4 59.6 28.3 59.4 57.7 48.1 64.0 66.6 72.9 55.6 52.5 58.4 57.7 59.9 54.4 70.9 52.1 56.2 70.1 32.8 66.5 67.5 72.4 65.2 58.7 48.1 64.0 65.5 75.2 57.0 55.5 59.7 59.1 58.7 55.7 66.7 49.7 56.2 70.1 34.8 65.2 66.9 73.7 65.7 59.9 48.1 65.1 67.6 74.1 55.6 55.5 58.4 56.3 62.2 57.0 63.9 53.3 56.2 71.6 35.9 62.7 67.5 72.4 66.3 60.4 46.7 62.9 66.6 74.1 58.4 52.5 54.3 56.3 64.5 55.7 59.8 49.7 56.2 79.3 34.8 61.4 67.5 68.7 64.1 60.6 45.4 66.1 66.6 72.9 59.8 54.0 54.3 55.0 64.5 57.0 58.4 50.9 56.2 80.9 32.8 62.7 69.2 67.5 63.6 27.6 23.4 24.2 35.7 25.2 32.0 29.7 27.3 24.8 31.1 30.7 33.4 35.1 27.0 28.5 26.6 19.7 27.4 28.0 27.1 58.0 46.7 62.9 65.5 74.1 55.6 52.5 58.4 57.7 58.7 57.0 66.7 50.9 56.2 71.6 30.7 64.0 68.1 72.4 65.7 56.1 55.9 55.1 53.3 53.1 29.7 54. Table 27: MLVU (Zhou et al., 2024) results across different attention budget strategy. Metrics include overall accuracy and task-specific scores across different steps. Block Count Window Size Overall Direction Count Rotation Shape&Trend Velocity&Freq. Visual Cues Human Simulated Object 32 36 35 34 28 20 64 4 16 256 32 64 128 512 1024 512 2048 128 26.5 25.9 27.1 27.2 26.1 25.1 25.3 26.4 21.4 21.6 21.6 23.8 23.8 21.8 20.6 21.3 21.8 19. 31.5 32.5 33.9 34.2 32.2 30.8 30.5 33.6 17.5 22.0 19.2 20.6 20.3 19.2 17.5 19.6 20.3 20.2 25.6 25.1 25.6 25.1 24.7 23.3 24.2 25.6 21.0 23.3 25.5 25.5 25.5 27.5 29.4 27.5 27.5 30.0 40.0 37.1 37.1 38.6 37.1 34.3 32.9 32.9 28.8 21.7 21.4 22.1 21.9 22.1 21.4 21.4 21.8 17. 23.6 21.5 24.2 24.2 22.4 23.3 22.9 24.7 17.6 29.3 29.2 30.8 30.8 28.2 25.6 27.4 29.4 20.6 Table 28: Tomato (Shangguan et al., 2024) results across different attention budget strategy. Metrics include overall accuracy and task-specific scores across different steps. Block Count Window Size Overall Direction Count Rotation Shape&Trend Velocity&Freq. Visual Cues Human Simulated Object 36 35 34 28 20 64 4 16 32 64 128 512 1024 512 2048 25.9 27.1 27.2 26.1 25.1 25.3 26.4 21.4 21.6 23.8 23.8 21.8 20.6 21.3 21.8 19.5 32.5 33.9 34.2 32.2 30.8 30.5 33.6 17.5 19.2 20.6 20.3 19.2 17.5 19.6 20.3 20.2 25.1 25.6 25.1 24.7 23.3 24.2 25.6 21.0 25.5 25.5 25.5 27.5 29.4 27.5 27.5 30. 37.1 37.1 38.6 37.1 34.3 32.9 32.9 28.8 21.4 22.1 21.9 22.1 21.4 21.4 21.8 17.8 21.5 24.2 24.2 22.4 23.3 22.9 24.7 17.6 29.2 30.8 30.8 28.2 25.6 27.4 29.4 20.6 Table 29: VSIBench (Yang et al., 2025a) results across different attention budget strategy. Metrics include overall accuracy and task-specific scores across different steps. Block Count Window Size Overall Appearance Abs. Dist. Counting Rel. Dist. Size Est. Room Est. Route Plan. Rel. Dir. 28 20 4 34 35 36 36 16 64 512 1024 1900 128 64 32 62 128 512 36.0 35.9 0.0 35.5 35.4 34.9 35.3 0.0 35.8 23.9 24.0 0.0 24.7 25.5 25.3 25.2 0.0 23.6 18.2 18.5 0.0 18.7 20.4 20.6 20.5 0.0 18.3 45.5 46.8 0.0 37.8 33.2 28.4 28.3 0.0 45. 30 36.9 36.7 0.0 36.2 35.4 36.0 35.8 0.0 36.8 54.1 53.6 0.0 53.9 53.6 54.4 54.7 0.0 54.2 29.8 29.0 0.0 31.7 31.5 30.9 31.0 0.0 29.3 36.0 36.5 0.0 36.0 38.1 38.1 41.1 0.0 36.5 43.4 42.0 0.0 45.2 46.0 45.9 46.1 0.0 42. VideoNSA: Native Sparse Attention Scales Video Understanding Figure 16: Gate weight distribution of each layer. (a) Compression sinks across block size and counts. (b) Selection sinks across block size and counts. (c) Sliding window sinks across window sizes. Figure 17: Attention sink distributions across the three branches under different sparse settings."
        }
    ],
    "affiliations": [
        "Lambda, Inc",
        "New York University",
        "Princeton University",
        "University of California, San Diego"
    ]
}