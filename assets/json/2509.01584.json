{
    "paper_title": "ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association",
    "authors": [
        "Ganlin Zhang",
        "Shenhan Qian",
        "Xi Wang",
        "Daniel Cremers"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present ViSTA-SLAM as a real-time monocular visual SLAM system that operates without requiring camera intrinsics, making it broadly applicable across diverse camera setups. At its core, the system employs a lightweight symmetric two-view association (STA) model as the frontend, which simultaneously estimates relative camera poses and regresses local pointmaps from only two RGB images. This design reduces model complexity significantly, the size of our frontend is only 35\\% that of comparable state-of-the-art methods, while enhancing the quality of two-view constraints used in the pipeline. In the backend, we construct a specially designed Sim(3) pose graph that incorporates loop closures to address accumulated drift. Extensive experiments demonstrate that our approach achieves superior performance in both camera tracking and dense 3D reconstruction quality compared to current methods. Github repository: https://github.com/zhangganlin/vista-slam"
        },
        {
            "title": "Start",
            "content": "ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association Ganlin Zhang 1,2 Shenhan Qian 1,2 Xi Wang 1,2,3 Daniel Cremers 1,2 1 TU Munich 2 MCML 3 ETH Zurich 5 2 0 2 1 ] . [ 1 4 8 5 1 0 . 9 0 5 2 : r Figure 1. ViSTA-SLAM Results on Multi-room Scene [8]. By combining the proposed lightweight frontend Symmetric Two-view Association (STA) model with Sim(3) pose graph optimization and loop closuring as the backend, ViSTA-SLAM achieves high-quality reconstruction and accurate trajectory estimation on challenging scenes while running in real time."
        },
        {
            "title": "Abstract",
            "content": "We present ViSTA-SLAM as real-time monocular visual SLAM system that operates without requiring camera intrinsics, making it broadly applicable across diverse camera setups. At its core, the system employs lightweight symmetric two-view association (STA) model as the frontend, which simultaneously estimates relative camera poses and regresses local pointmaps from only two RGB images. This design reduces model complexity significantly, the size of our frontend is only 35% that of comparable state-ofthe-art methods, while enhancing the quality of two-view constraints used in the pipeline. In the backend, we construct specially designed Sim(3) pose graph that incorporates loop closures to address accumulated drift. Extensive experiments demonstrate that our approach achieves superior performance in both camera tracking and dense 3D reconstruction quality compared to current methods. Github repository: https://github.com/zhangganlin/ vista-slam 1. Introduction 1.1. Real-time Monocular Dense SLAM Simultaneous Localization and Mapping (SLAM) jointly estimates an agents pose and the surrounding 3D scene from sensor observations. In the monocular dense setting, single RGB camera is used to reconstruct continuous 3D map, enabling both geometric accuracy and visual realism. The camera poses and dense reconstruction outputs underpin downstream tasks [2, 19, 31, 32, 42] such as semantic perception, object interaction, and scene editing, and are crucial for applications in VR/AR, robotics, and autonomous driving, where accurate, low-latency 3D perception is essential. 1.2. Related Work Classical Visual SLAM Classical visual SLAM methods can be broadly categorized into two types. The first, similar to incremental SfM [1, 26, 41], is feature-based SLAM [6, 10, 34, 35], relying on keypoint extraction and descriptor matching to provide constraints for triangulation and PnP [21, 22] pose estimation. The second, known as direct methods, such as LSD-SLAM [15] and DSO [16], optimizes camera poses by minimizing photometric error from pixel intensities while estimating per-frame depth map. Both categories typically adopt frontend (feature-based or direct) and backend for optimization, most often bundle adjustment [49] to jointly refine poses and structure. However, they rely heavily on accurate camera calibration and are generally limited to sparse 3D reconstructions. Dense Visual SLAM To enable denser reconstructions, recent works have incorporated deep learning into either 1 the frontend or the scene representation. For example, DROID-SLAM [48] uses RAFT [47] for dense optical flow in the frontend and performs dense bundle adjustment on the GPU, while methods such as SuperPrimitive [30] and COMO [12] leverage monocular priors (e.g., surface normals, depth distributions) from pretrained models with direct photometric optimization. BA-Track [7] augments point-based tracking with scale-grid deformation of monocular depth priors. Neural scene representations have also been adopted, with NICER-SLAM [61] and MonoGS [29] optimizing camera poses and 3D structure using NeRF [33] and 3D Gaussian Splatting [20], respectively. Tracking robustness and geometric accuracy have been further improved by integrating depth priors and auxiliary trackers [40, 5760]. However, most approaches still require accurate camera intrinsics and many struggle to achieve true real-time performance due to the computational demands of dense optimization and neural rendering. SLAM with 3D Foundation Models All aforementioned methods require known and accurate camera intrinsics. With the advent of 3D foundation models [23, 51, 53], several intrinsic-free SLAM frameworks have emerged, aiming to produce dense outputs without calibration. Methods such as Spann3R [50] and others [5, 27, 52] extend the two-view DUSt3R [53] model to sequential inputs, directly regressing point clouds in unified global coordinate system. Reloc3r [13] instead regresses only relative poses and performs offline global optimization using SfM techniques [37, 46, 56]. MASt3R-SLAM [36] extracts dense correspondences from MASt3R [23] and feeds them into classical optimization pipeline, while submap-based methods [11, 28] employs the multiview model VGGT [51] to regress local submaps before stitching them via pose graph optimization. While these approaches address some classical limitations, they still face notable drawbacks: 1. Current two-view models [23, 53] use asymmetric architectures that regress pointmaps of both views to the first views coordinates, making it difficult to decouple views for backend optimization (e.g. loop closure). 2. Pure regression methods [27, 50, 52] predict incoming frames with previous memory, but suffer from drift and start forgetting once the trajectory gets longer. 3. Methods [27, 36, 50, 52] built on current two-view models inherit the asymmetric architecture with two separate decoders, resulting in large model size. Submapbased methods [11, 28] employ an even larger multiview model [51] to build submaps, which further increases the size of the frontend model. 1.3. Contributions of ViSTA-SLAM To address these concerns, we propose ViSTA-SLAM, novel real-time monocular visual SLAM pipeline based on symmetric two-view association. At its core is lightweight Symmetric Two-view Association (STA) model frontend, which takes two RGB images as input and simultaneously regresses two pointmaps in their respective local coordinate frames, along with the relative camera pose between them. During training, we enforce cycle consistency on relative poses and geometric consistency on pointmaps to improve accuracy and stability. Unlike prior 3D models [23, 51, 53], STA is fully symmetric with respect to its inputs: neither view is designated as reference, and the same encoderdecoder architecture is applied to both. In the backend, we perform Sim(3) pose graph optimization with loop closures to mitigate drift and ensure global consistency. To further enhance robustness, each view is represented by multiple nodes rather than single one, which are connected by scale-only edges to handle scale inconsistencies across different forward passes. This symmetric design makes our frontend substantially more lightweight than existing methods, with STA being only 64% the size of MASt3R [23] and 35% the size of VGGT [51]. Unlike prior approaches [11, 28] that group multiple views into single submap node, our method assigns each view its own nodes in the pose graph. Leveraging the local pointmaps produced by the STA frontend, each node can be represented independently, connected to others solely through relative transformations. Compared to submap-based methods [28], this design yields more flexible graph structure and greater robustness. The combination of this flexibility and lightweight architecture underpins our choice of symmetric two-view model as the frontend. In summary, our main contributions are as follows: We design and train lightweight, symmetric two-view association network as the frontend, which takes only two RGB images as input and regresses their pointmaps in local coordinates along with the relative camera pose. We construct robust Sim(3) pose graph with loop closures, optimize it using the LevenbergMarquardt algorithm for fast and stable convergence. By integrating these components, we present real-time monocular dense visual SLAM framework that operates without requiring any camera intrinsics. Our method achieves state-of-the-art performance on the real-world 7-Scenes [43] and TUM-RGBD [45] datasets for both camera trajectory estimation and dense 3D reconstruction. 2. ViSTA-SLAM Pipeline As monocular dense SLAM pipeline  (Fig. 2)  , our aim is to simultaneously track camera poses and reconstruct the recorded scene online using dense pointcloud. To achieve this, we propose lightweight and novel symmetric twoview association model as the frontend of our pipeline, which extracts the relative pose and local point maps of two neighboring input frames (Sec. 2.1 and Sec. 2.2). In 2 Figure 2. ViSTA-SLAM Overview. Given sequential video frames without intrinsics as the input, our frontend model takes in view pairs and predicts local pointmaps and relative poses within each pair. We then use the pair-wise predictions to construct Sim(3) pose graph with loop closure and optimize it via LevenbergMarquardt algorithm. The frontend model employs fully symmetric design, making the model lightweight and supporting more flexible pose graph optimization. The blue edges in the pose graph and final results represent connections between neighboring nodes (views), while the orange edges correspond to loop closures. the backend, sparse and efficient Sim(3) pose graph optimization with loop closure is performed to mitigate drift accumulation (Sec. 2.3). 2.1. Symmetric Two-view Association Model In classical monocular SLAM pipelines, the two-view estimation is one of the most critical building block, as it establishes geometric constraints that allow for further optimization. In this work, we follow the same principle; however, instead of relying on traditional methods, we propose deep learning based Symmetric Two-view Association (STA) model that eliminates the need for camera intrinsics in the SLAM process. Encoder Our STA model takes in two images Ii, Ij as input. It uses shared ViT encoder [14] to divide input images into patches and encode them into features Ei/j = Encoder (cid:0)Ii/j (cid:1) RKC, where represents tokens and denotes the dimensions. Subsequently, we insert camera pose embedding to the encoding features of each view, forming i/j = (cid:0)p, Ei/j (cid:1) R(K+1)C. and Decoder The decoder further processes and fuses information between the encoding features j. It contains sequence of decoder blocks, each conducting self-attention operation followed by cross-attention operation, producing decoding features i/j = DecoderBlock(b) (cid:16) D(b) where {1, 2, . . . , B} is the index of decoder block, and D(0) Symmetric Formulation Prior approaches [23, 53] regress both point maps into the coordinate frame of the R(K+1)C , D(b1) j/i i/j = D(b1) i/j i/j. (cid:17) first view, thus requiring two separate decoders. In contrast, our model predicts only local point maps and relative poses between views. This fully symmetric formulation makes it possible to use only one decoder. As result, the number of parameters for decoding is effectively reduced by half (shown in Fig. 3), forming more compact model for realtime applications. Moreover, producing local view outputs in their own coordinate systems is better suited for the subsequent pose graph optimization, see Sec. 2.3 for details. Local Point Maps Given decoding features D(b) i/j, we use DPT head [38] to regress the local point maps and corresponding confidence maps : Pi/j, Wi/j = PointHead (cid:17) (cid:16) D(b) i/j , i.e., the camera pose embedding p(B) Relative Poses Given the first embedding of the decoding feature D(B) R1C , we use an MLP to regress the relative transformation from view to . Specifically, the MLP outputs matrix Mij R33 for rotation, translation vector tij R31, and confidence score wij [0, 1]: Mij, tij, wij = PoseHead(p(B) ) Since Mij is not guaranteed to lie on the SO(3) manifold, we apply SVD orthogonalization [24] to it to obtain valid rotation matrix Rij. Then, the relative transformation is Tij = [Rijtij]. With our symmetric formulation, we could also input pj into the pose head to regress Tji. But in practice we only regress one of them for building pose graph. 2.2. Training Objective There are three loss terms to supervise the training of our STA model. Pointmap Loss compares the regressed local pointmap with ground-truth points. Relative Pose Loss , penalizes errors in relative rotation and translation, with cycle-consistency term ensuring the two predicted poses are mutual inverses. Geometric Consistency Loss enforces alignment of the two local point maps after applying the predicted relative transformation, improving local reconstruction consistency. Local Pointmap Loss Following DUSt3R [53], we apply the confidence-weighted regression loss for all predicted point maps with valid ground truths. Since the reconstruction is up-to-scale, we also normalize the regressed pointmap and the ground-truth pointmap according to their mean Euclidean distance to the origin, and ˆn: (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) αpoint log (Wv(x)) , (cid:32) ˆPv(x) ˆn Pv(x) (cid:33)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Wv(x) Lpmap = v{i,j} (cid:88) (cid:88) xIv (1) where is the pixel coordinate. Note that all the points are regressed in the local coordinate space of each view. Relative Pose Loss Relative pose loss consists of three parts: rotation loss, translation loss and identity loss. The rotation loss LR evaluates the angle between the regressed rotation Rij and the ground-truth rotation ˆRij, LR(R, ˆR) = arccos (cid:32) tr(R1 ˆR) 1 2 (cid:33) . (2) The translation loss Lt evaluates the euclidean distance between the predicted translation tij and the ground truth ˆtij, which are normalized by the same factors and ˆn as for the pointmap loss in Eq. (1): Lt(t, ˆt) = (cid:13) (cid:13) (cid:13) (cid:13) ˆt ˆn 2 (cid:13) (cid:13) (cid:13) (cid:13) . (3) The pose identity loss Lid minimizes the difference of TijTji and the identity transformation I, essentially constraining Tij and Tji to be the inverse of each other to improves the consistency of our pose prediction: Lid = LR(RijRji, I) + Lt(Rijtji + tij, 0). (4)"
        },
        {
            "title": "Then the complete relative pose loss is defined as",
            "content": "Lpose =wij (cid:16) LR(Rij, ˆRij) + Lt(tij, ˆtij) + Lid (cid:17) α log(wij), (5) weighted by separate confidence score wij for pose regression. Geometrical Consistency Loss To ensure that the predicted point maps of each pair are spatially consistent when placed in the same coordinate space, we introduce geometric consistency loss. Given pair of images with groundtruth intrinsics, depths, and relative poses, each pixel in 4 Figure 3. Asymmetric vs. Symmetric Architectures. Asymmetric architectures [23, 53] use two decoders to regress point maps in shared coordinate space. our symmetric formulation regresses relative pose and local point maps with only single decoder, reducing over 36% of the parameters ( 0.4 vs. 0.7 billion), while achieving higher accuracy and enabling pose graph optimization in the backend. Figure 4. An Example Pose Graph for 5 views (N = 2). Nodes of the same view are grouped and connected to the first processed node of that view via scale edges. Our two-type-edge design enhances optimization robustness, yielding more accurate poses. view can be accurately projected into view j, yielding its ground-truth corresponding pixel Cij(x) in view j. Then the geometric consistency loss Lgc is defined as: Lgc = (cid:88) xIi TijPi(x) Pj (Cij(x)) /n, (6) where Tij is the predicted relative transformation and is the same normalization factor as in Eq. (1)."
        },
        {
            "title": "The total training objective function L is the weighted",
            "content": "sum of the above three losses, = λpmapLpmap + λposeLpose + λgcLgc, (7) where λpmap, λpose and λgc are weighting factors. 2.3. Backend Pose Graph Optimization Notation In the backend, Sim(3) pose graph = (V, E) is maintained and optimized to mitigate accumulated errors introduced by the two-view estimations. The vertex set and edge set are defined as = {vj i, N}, = {eij i, N}, (8) , eij Sim(3), each vertex vj where vj represents an absolute camera pose with scale, of view i, with view and viewj as the input of STA; each edge eij encodes the relative transformation between pair of connected vertices vj and vi and eij have rigid transformation part SE(3) and scale part R+. Graph Construction To construct the pose graph, for given view i, the STA model performs 2N forward passes j. Both vj Method chess fire heads office pumpkin kitchen stairs . a . l U NICER-SLAM [61] DROID-SLAM [48] GlORIE-SLAM [57] CUT3R [52] SLAM3R [27] MASt3R-SLAM [36] VGGT-SLAM [28] ViSTA-SLAM 0.033 0.036 0.036 0.743 0.089 0.063 0.037 0.073 0.069 0.027 0. 0.226 0.048 0.046 0.026 0.035 0.042 0.025 0.014 0.363 0.036 0.029 0.022 0.028 0.108 0.066 0.094 0.664 0.088 0.103 0.103 0.055 0.200 0.127 0. 0.546 0.196 0.112 0.147 0.129 0.039 0.040 0.053 0.381 0.102 0.074 0.063 0.035 0.108 0.026 0.020 0.413 0.126 0.032 0.095 0.029 Avg. 0.086 0.049 0.056 0.477 0.098 0.066 0.070 0.055 Table 1. Camera Trajectory Estimation (ATE RMSE) on 7-Scenes [43]. ViSTA-SLAM performs the best on average. with neighboring views [i N, 1] [i + 1, + ]. The predicted pointmap for each view in each forward pass corresponds to node in the graph, resulting in multiple nodes per view since each view is processed multiple times by the frontend model. As shown in Fig. 4, we define two types of edges in the graph. Pose edges connect two nodes generated from the same forward pass, using the estimated relative camera pose and an identity relative scale, based on the assumption that the two local point maps from single forward pass share the same scale. Scale edges connect nodes belonging to the same view but obtained in different forward passes (paired with different neighboring views), with the rigid transformation component set to identity, and the scale component solved via weighted least squares between the predicted point maps of different forward passes. Among all nodes of the same view, scale edges are constructed only between the first processed node and the others for sparsity and simplicity. Loop Closure We use Bag of Words [17] to detect loop candidates, which form new pairs. Then we can feed each candidates pair into our STA model to confirm these proximity. If the predicted confidence score of the relative pose is higher than predefined threshold τp, this pair is accepted as valid loop, and two new nodes connected by pose edge are added. Each new node is also connected to the first processed node of its corresponding view via scale edge. Optimization We perform pose graph optimization using the LevenbergMarquardt algorithm in the space of Lie algebra sim(3). min V} {vj (cid:88) eij (cid:13) (cid:13) (cid:13)logSim(3) (cid:16) eij (vj )1 vi (cid:17)(cid:13) 2 (cid:13) (cid:13) Ωij (9) where Ωij represents the covariance matrix, derived from the confidence score predicted by the STA model. The optimization process takes less than 5 iterations to converge in most cases. Using the optimized camera pose and scale, the reconstructed pointcloud i in global coordinate is, i = sj where Rj and sj are the orientation, position, scale of vj respectively. To avoid redundancy, for each view i, we + tj , , tj i Rj (10) only keep the pointcloud with largest confidence among all i in the final result. 3. Experiments Evaluation Datasets and Metrics Following VGGTSLAM [28], we evaluate our method on standard monocular SLAM benchmarks for camera tracking accuracy and reconstruction quality. We report root mean square error (RMSE) of absolute trajectory error (ATE, in meters) on real-world 7-Scenes [43] and TUM-RGBD [45] datasets using the evo toolkit [18]. Reconstruction quality on 7-Scenes is assessed via RMSE of accuracy, completion, and Chamfer distance (meters), leveraging its ground-truth 3D scenes. Implementation Details The frontend STA model is initialized from the weights of DUSt3R [53], and trained on ScanNet [8], ScanNet++[54], ARKitScenes[4], CO3D [39], Aria Synthetic Environments [3], and Replica [44] for 7 days using 8 NVIDIA H100 GPUs. We use AdamW optimizer to train our STA model with learning rate 1.5e5, weight decay 0.01, = 12, αpoint = 0.2, αpose = 0.05, λpmap = 1, λpose = 1, λgc = 1, and τp = 0.75. We conducted evaluations on machine with an NVIDIA RTX 4090 GPU and an Intel i9-14900KF CPU, with = 2 for 7-Scenes and = 3 for TUM-RGBD. Baselines ViSTA-SLAM is primarily compared with state-of-the-art (SOTA) learning-based SLAM methods in uncalibrated scenarios: VGGT-SLAM [28], MASt3RSLAM [36], SLAM3R [27], and CUT3R [52]. To reduce randomness from VGGT-SLAMs RANSAC, we run it 5 times per scene as suggested in their paper; results for other methods, including ours, are deterministic. We also compare with SOTA methods using known camera intrinsics [6, 25, 48, 57, 61]. Some results are taken from [28, 36]. For MASt3R-SLAM and VGGT-SLAM, we keep their original keyframe selection; for ours, CUT3R, and SLAM3R, we use frame strides of 5 (7-Scenes) and 3 (TUM-RGBD). Calibrated methods are shown in gray. Best results are highlighted as first , second , and third . 3.1. Camera Trajectory Evaluation In Tab. 1 and Tab. 2, we report ATE RMSE. ViSTA-SLAM achieves the best average performance on both datasets, outMethod 360 desk desk2 floor plant room rpy teddy xyz a l e b c ORB-SLAM3 [6] DPV-SLAM [25] DPV-SLAM++ [25] DROID-SLAM [48] GlORIE-SLAM [57] CUT3R [52] SLAM3R [27] MASt3R-SLAM [36] VGGT-SLAM [28] ViSTA-SLAM 0.112 0.132 0.111 0.128 0.174 0.211 0.070 0.063 0.104 0.017 0.018 0.018 0.018 0.016 0.592 0.861 0.032 0.031 0. 0.210 0.029 0.029 0.042 0.028 0.546 0.967 0.055 0.048 0.030 0.057 0.050 0.021 0.021 0.662 0.790 0.056 0.152 0.070 0.034 0.021 0.022 0.016 0.021 0.467 0.755 0.035 0.023 0. 0.330 0.096 0.049 0.042 0.911 1.013 0.118 0.133 0.067 0.030 0.032 0.026 0.020 0.051 0.063 0.041 0.038 0.023 0.084 0.098 0.048 0.035 0.845 0.986 0.116 0.039 0. 0.009 0.010 0.010 0.012 0.010 0.129 0.185 0.020 0.020 0.015 Avg. N/A 0.076 0.054 0.038 0.036 0.486 0.648 0.060 0.061 0.052 Table 2. Camera Trajectory Estimation (ATE RMSE) on TUM-RGBD [45]. ViSTA-SLAM performs the best on average. performing current SOTA [36] by 17% (0.055 vs. 0.066) and 13% (0.052 vs. 0.060), and surpasses some calibrated methods [25, 61]. ViSTA-SLAM performs less effectively on TUM-RGBD 360 scene due to predominantly rotational camera motion that leads to frontend ambiguity and degrading performance. Other methods [28, 36] use either heavier multi-view frontend or more intensive optimization to reduce the influence. Pure regression-based methods [27, 52] struggle to maintain consistent registration over long sequences with large camera motion due to forgetting effects. In Fig. 5, we show the estimated trajectories from different methods on 7-Scenes [43] office and TUMRGBD [45] room. CUT3R [52] suffers from severe forgetting issues on long sequences; SLAM3R [27] has bad point registration on the challenged scene TUM-RGBD room, thus, does not produce correct camera poses. Compared to pure regression-based methods, MASt3R-SLAM [36] and VGGT-SLAM [28] work well, while ViSTA-SLAM achieves even higher trajectory accuracy. 3.2. Dense Reconstruction Evaluation In Tab. 3, we evaluate reconstruction quality across methods. Leveraging accurate camera poses and consistent local point clouds, ViSTA-SLAM achieves the best Chamfer distance among all approaches. Despite using lightweight two-view frontend, ViSTA-SLAM, combined with tailored Sim(3) pose graph optimization, significantly outperforms multi-view-frontend methods [27, 28] in accuracy (0.45 vs. 0.52) while matching or exceeding completeness. To demonstrate the effectiveness of our lightweight frontend, we add another strong baseline, replacing our STA model with two-view VGGT [51] as the frontend and conducting the same pose graph optimization. ViSTA-SLAM still achieves better performance in Chamfer distance, completeness, and absolute trajectory error, highlighting the effectiveness of our lightweight symmetric frontend over larger multiview models like VGGT for SLAM tasks In Fig. 6, we show qualitative reconstruction results on 7-Scenes redkitchen, TUM-RGBD [45] room, and BundleFusion [9] apt1. CUT3R [52] fails to reconstruct Method ATE Acc. Comp. Chamfer DROID-SLAM [48] MASt3R-SLAM [36] 0.049 0.047 N/A Spann3R @20 [50] N/A Spann3R @2 [50] 0.477 CUT3R [52] 0.098 SLAM3R [27] 0.066 MASt3R-SLAM [36] VGGT-SLAM [28] 0.070 2-view VGGT w/ PGO 0.065 0.055 ViSTA-SLAM 0.141 0.089 0.069 0.124 0.276 0.053 0.059 0.052 0.039 0.045 0.048 0. 0.047 0.043 0.303 0.059 0.056 0.060 0.077 0.056 0.094 0.087 0.058 0.084 0.290 0.056 0.057 0.056 0.058 0.051 Table 3. Tracking and Reconstruction Evaluation on 7Scenes [43]. @n indicates keyframe every images. 2-view VGGT w/ PGO uses the 2-view VGGT frontend with the same pose graph optimization as ours. ViSTA-SLAM achieves the best trajectory estimation and reconstruction performance on 7-Scenes. CUT3R SLAM3R MASt3R [27] [52] SLAM [36] SLAM [28] VGGT 2-view VGGT ViSTA SLAM w/ PGO Size FPS 0.79 34.2 0.76 45.8 0.69 30. 1.26 93.3 1.26 12.6 0.44 78. Table 4. Model Size and Running Time Evaluation on 7Scenes [43] redkitchen. Model sizes are reported in billions of parameters. FPS indicates the average number of frames processed per second over three runs. ViSTA-SLAM shows highly competitive real-time speed with the smallest model size among baselines. correctly due to forgetting issues, while SLAM3R [27] struggles in scenes with large camera perspective changes. MASt3R-SLAM [36] and VGGT-SLAM [28] produce artifacts on object boundaries, failing to clearly separate foreground from background, and show misalignment across In contrast, ViSTA-SLAM overcomes these chalviews. lenges through geometric consistency constraints during training. Notably, VGGT-SLAM fails midway through the apt1 scene as backend optimization diverges, which stems from the unstable RANSAC-based 3D homography estimation, which can sample planar regions and cause ambiguity in their proposed SL(4) pose graph optimization. 6 Figure 5. Trajectory estimation results on 7-Scenes office (top) and TUM-RGBD room (bottom). Estimated camera trajectories are projected onto the xy plane, with ground-truth shown as dashed lines. The trajectory color encodes ATE RMSE: higher errors in red, lower in blue. For MASt3R-SLAM [36] and VGGT-SLAM [28], only the poses of their selected keyframes are estimated. load data encoder decoder detect loop construct graph optimize graph time (s) % 1.88 0.89 14.7% 7.0% 3.38 2.93 0.41 26.4% 3.18% 22.9% 3.31 25.9% Table 5. Time Spent on Each Component of ViSTA-SLAM for 7-Scenes [43] redkitchen (in seconds and percentage). STA Model Pose Graph Settings w/o Lgc w/o Lid w/o pose graph opt. w/o loop closure w/o two edge types ViSTA-SLAM with full features ATE Chamfer 0.056 0. 0.105 0.103 0.057 0.055 0.057 0.059 0.070 0.072 0.051 0.051 Table 6. Ablation Study on 7-Scenes [43]. w/o pose graph opt. simply accumulates relative poses for absolute poses. w/o two edge types uses the classical pose graph in which each view is represented by single node. 3.3. Model Size and Speed We compare the frontend model size and processing speed across methods in Tab. 4. Owing to our symmetric design, the decoder and regression heads use only half the parameters of existing feedforward models [23, 27, 50, 53]. Consequently, our model is far more compact: only 64% the size of MASt3R [23] (used in MASt3R-SLAM [36]) and 35% the size of VGGT [51] (used in VGGT-SLAM [28]). The speed evaluation further confirms that ViSTASLAM achieves real-time performance. Benefiting from both the compact frontend and the sparse pose graph, our approach is highly competitive in runtimefaster than the pure regression-based methods CUT3R [52] and SLAM3R [27], and comparable to VGGT-SLAM [28]. It is worth noting that VGGT-SLAM performs inference only once every 32 keyframes, reducing the total number of inference steps. When replacing our STA model with twoview VGGT that takes two views as input at time like STA, the running speed is significantly slower, further demonstrating the effectiveness of our lightweight frontend. Tab. 5 shows the percentage of runtime spent on major pipeline components. Decoding two-view information and pose graph optimization dominate the processing time. 3.4. Ablation Study In Tab. 6, we present ablations by selectively disabling components of ViSTA-SLAM. Incorporating all proposed features yields the best performance on both camera trajectory estimation and 3D reconstruction. Both Lgc and Lid contribute substantially to reconstruction quality. Lgc improves consistency between the reconstructed local pointmaps of the two-view input pair, while Lid further refines the estimated relative camera poses by enforcing cycle consistency constraint on the model. Pose graph optimization with loop closure is also highly effective, bringing 48% improvement in the trajectory accuracy (0.105 0.055), as it introduces simple yet powerful constraints through the edges of the pose graph, preventing error accumulation from two-view estimations as the trajectory grows longer. These findings further support the symmetric formulation of our frontend to regress local pointclouds and relative poses, which maximizes the effectiveness of pose graph optimization since the pointmaps of each view are tightly coupled with their corresponding camera poses in the graph. On the contrary, if pointmaps were regressed in shared coordinate system inside submap, as in previous works [5, 23, 27, 50, 52, 53], pose updates would not be able to fix misalignments inside submaps. Our two-edge-type pose graph design improves camera 7 ] 2 5 [ 3 ] 7 2 [ 3 S ] 6 3 [ S - 3 M ] 8 2 [ S - V S - i Figure 6. Reconstruction results on 7-Scenes redkitchen (left), TUM-RGBD room (middle), and BundleFusion apt1 (right). Purple boxes highlight reconstruction artifacts near the edges (background points wrongly mapped to the edge of the foreground). Red boxes indicate misalignments. Green boxes highlights ViSTA-SLAMs competitive results. VGGT-SLAM fails to complete reconstruction on apt1 due to divergence in pose graph optimization. trajectory estimation by representing each view with multiple nodes connected by scale and pose edges, rather than single node with standard edges. This structure better averages out uncertainty, particularly relative scale variations in pointmaps from different forward passes, improving the robustness of pose graph optimization and the accuracy of trajectory estimation. 4. Conclusion We propose novel monocular intrinsics-free SLAM pipeline, ViSTA-SLAM, which features lightweight frontend (Symmetric Two-View Association) and Sim(3) pose graph optimization with loop closure as the backend. Experimental results demonstrate the superior camera tracking accuracy and 3D reconstruction quality of ViSTA-SLAM. Meanwhile, it is significantly more lightweight and operates at faster or comparable speed comparing current state-ofthe-art methods. Limitation and Future Work Our method omits optimization of point clouds in the backend for efficiency consideration. Therefore, it can suffer from misalignments caused by imperfect pointmap prediction by the frontend model. Future work could explore incorporating implicit camera information from previous estimates or aligning latent features across views to enhance local consistency across forward passes."
        },
        {
            "title": "References",
            "content": "[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven Seitz, and Richard Szeliski. Building rome in day. Communications of the ACM, 54 (10):105112, 2011. 1 [2] Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R. Zamir, Martin Fischer, Jitendra Malik, and Silvio Savarese. 3D scene graph: structure for unified semantics, 3D space, and camera. In ICCV, 2019. 1 [3] Armen Avetisyan, Christopher Xie, Henry Howard-Jenkins, Tsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang, Duncan Frost, Luke Holland, Campbell Orme, et al. Scenescript: Reconstructing scenes with an autoregressive structured language model. In ECCV, pages 247263. Springer, 2024. 5 [4] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. ARKitscenes - diverse real-world dataset for 3D indoor scene understanding using mobile RGB-D data. In NeurIPS, 2021. 5 [5] Yohann Cabon, Lucas Stoffl, Leonid Antsfeld, Gabriela Csurka, Boris Chidlovskii, Jerome Revaud, and Vincent Leroy. MUSt3R: Multi-view network for stereo 3D reconstruction. In CVPR, pages 10501060, 2025. 2, 7 [6] Carlos Campos, Richard Elvira, Juan Gomez Rodrıguez, Jose MM Montiel, and Juan Tardos. ORB-SLAM3: An accurate open-source library for visual, visualinertial, and IEEE transactions on robotics, 37(6): multimap SLAM. 18741890, 2021. 1, 5, [7] Weirong Chen, Ganlin Zhang, Felix Wimbauer, Rui Wang, Nikita Araslanov, Andrea Vedaldi, and Daniel Cremers. Back on track: Bundle adjustment for dynamic scene reconstruction. arXiv preprint arXiv:2504.14516, 2025. 2 [8] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. ScanNet: In Richly-annotated 3D reconstructions of indoor scenes. CVPR, 2017. 1, 5, 2 [9] Angela Dai, Matthias Nießner, Michael Zollofer, Shahram Izadi, and Christian Theobalt. BundleFusion: Real-time globally consistent 3D reconstruction using on-the-fly surface re-integration. ACM TOG, 2017. 6, 3 [10] Andrew Davison, Ian Reid, Nicholas Molton, and Olivier Stasse. MonoSLAM: Real-time single camera SLAM. IEEE TPAMI, 29(6):10521067, 2007. 1 [11] Kai Deng, Zexin Ti, Jiawei Xu, Jian Yang, and Jin Xie. VGGT-Long: Chunk it, loop it, align itpushing VGGTs arXiv limits on kilometer-scale long RGB sequences. preprint arXiv:2507.16443, 2025. 2 [12] Eric Dexheimer and Andrew Davison. COMO: Compact mapping and odometry. In ECCV, pages 349365. Springer, 2024. [13] Siyan Dong, Shuzhe Wang, Shaohui Liu, Lulu Cai, Qingnan Fan, Juho Kannala, and Yanchao Yang. Reloc3r: Largescale training of relative camera pose regression for generalizable, fast, and accurate visual localization. In CVPR, pages 1673916752, 2025. 2 [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 3 [15] Jakob Engel, Thomas Schops, and Daniel Cremers. LSDIn ECCV, SLAM: Large-scale direct monocular SLAM. pages 834849. Springer, 2014. 1 [16] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE TPAMI, 40(3):611625, 2017. 1 [17] Dorian Galvez-Lopez and J. D. Tardos. Bags of binary words for fast place recognition in image sequences. IEEE Transactions on Robotics, 28(5):11881197, 2012. 5 [18] Michael Grupp. evo: Python package for the evaluation https : / / github . com / of odometry and SLAM. MichaelGrupp/evo, 2017. 5 [19] Anna-Maria Halacheva, Yang Miao, Jan-Nico Zaech, Xi Wang, Luc Van Gool, and Danda Pani Paudel. Holistic understanding of 3D scenes as universal scene description. arXiv preprint arXiv:2412.01398, 2024. 1 [20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3D gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2 [21] Laurent Kneip, Davide Scaramuzza, and Roland Siegwart. novel parametrization of the perspective-three-point problem for direct computation of absolute camera position and orientation. In CVPR, pages 29692976. IEEE, 2011. 1 [22] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua. EPnP: An accurate O(n) solution to the PnP problem. IJCV, 81(2):155166, 2009. 1 [23] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3D with MASt3R, 2024. 2, 3, 4, [24] Jake Levinson, Carlos Esteves, Kefan Chen, Noah Snavely, Angjoo Kanazawa, Afshin Rostamizadeh, and Ameesh Makadia. An analysis of SVD for deep rotation estimation. NeurIPS, 33:2255422565, 2020. 3 [25] Lahav Lipson, Zachary Teed, and Jia Deng. Deep patch visual SLAM. In ECCV, pages 424440. Springer, 2024. 5, 6 [26] Shaohui Liu, Yidan Gao, Tianyi Zhang, Remi Pautrat, Johannes Schonberger, Viktor Larsson, and Marc Pollefeys. Robust incremental structure-from-motion with hybrid features. In ECCV, pages 249269. Springer, 2024. 1 [27] Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yingda Yin, Yanchao Yang, Qingnan Fan, and Baoquan Chen. SLAM3R: Real-time dense scene reconstruction from monocular RGB videos. In CVPR, pages 1665116662, 2025. 2, 5, 6, 7, 8, 1 [28] Dominic Maggio, Hyungtae Lim, and Luca Carlone. VGGTSLAM: Dense RGB SLAM optimized on the SL(4) manifold. arXiv preprint arXiv:2505.12549, 2025. 2, 5, 6, 7, 8 [29] Hidenobu Matsuki, Riku Murai, Paul HJ Kelly, and Andrew Davison. Gaussian splatting SLAM. In CVPR, pages 18039 18048, 2024. 2 [30] Kirill Mazur, Gwangbin Bae, and Andrew Davison. SuperPrimitive: Scene reconstruction at primitive level. In CVPR, pages 49794989, 2024. 9 [31] Yang Miao, Iro Armeni, Marc Pollefeys, and Daniel Barath. Volumetric semantically consistent 3d panoptic mapping. In IROS, pages 1292412931. IEEE, 2024. 1 [32] Yang Miao, Francis Engelmann, Olga Vysotska, Federico Tombari, Marc Pollefeys, and Daniel Bela Barath. Scenegraphloc: Cross-modal coarse visual localization on 3D scene graphs. In ECCV, pages 127150. Springer, 2024. 1 [33] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [34] Raul Mur-Artal and Juan Tardos. ORB-SLAM2: An opensource SLAM system for monocular, stereo, and RGB-D cameras. IEEE transactions on robotics, 33(5):12551262, 2017. 1 [35] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan Tardos. ORB-SLAM: versatile and accurate monocular SLAM system. IEEE transactions on robotics, 31(5):1147 1163, 2015. [36] Riku Murai, Eric Dexheimer, and Andrew Davison. MASt3R-SLAM: Real-time dense SLAM with 3D reconIn CVPR, pages 1669516705, 2025. 2, struction priors. 5, 6, 7, 8 [37] Linfei Pan, Daniel Barath, Marc Pollefeys, and Johannes Schonberger. Global structure-from-motion revisited. In ECCV, pages 5877. Springer, 2024. 2 [38] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. ViIn ICCV, pages sion transformers for dense prediction. 1217912188, 2021. 3 [39] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3D: Large-scale learning and evaluation of real-life 3D category reconstruction. In ICCV, 2021. 5 [40] Erik Sandstrom, Ganlin Zhang, Keisuke Tateno, Michael Oechsle, Michael Niemeyer, Youmin Zhang, Manthan Patel, Luc Van Gool, Martin Oswald, and Federico Tombari. Splat-SLAM: Globally optimized rgb-only SLAM with 3D gaussians. In CVPRW, pages 16801691, 2025. 2 [41] Johannes Schonberger and Jan-Michael Frahm. StructureIn CVPR, pages 41044113, 2016. from-motion revisited. 1 [42] Sunando Sengupta, Eric Greveson, Ali Shahrokni, and Philip HS Torr. Urban 3D semantic modelling using stereo vision. In ICRA, pages 580585. IEEE, 2013. 1 [43] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression forests for camera relocalization in RGBD images. In CVPR, pages 29302937, 2013. 2, 5, 6, 7, 1, 3 [44] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard Newcombe. The Replica dataset: digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019. 5 [45] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. benchmark for the evaluation of RGB-D SLAM systems. In IROS, 2012. 2, 5, 6, [46] Christopher Sweeney, Tobias Hollerer, and Matthew Turk. Theia: fast and scalable structure-from-motion library. In ACM MM, pages 693696, 2015. 2 [47] Zachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In ECCV, pages 402419. Springer, 2020. 2 [48] Zachary Teed and Jia Deng. DROID-SLAM: Deep visual SLAM for monocular, stereo, and RGB-D cameras. NeurIPS, 34:1655816569, 2021. 2, 5, 6 [49] Bill Triggs, Philip McLauchlan, Richard Hartley, and Andrew Fitzgibbon. Bundle adjustmenta modern synthesis. In International workshop on vision algorithms, pages 298372. Springer, 1999. 1 [50] Hengyi Wang and Lourdes Agapito. 3D reconstruction with spatial memory. In 3DV, 2025. 2, 6, [51] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. VGGT: Visual geometry grounded transformer. In CVPR, 2025. 2, 6, 7 [52] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3D perception model with persistent state. In CVPR, pages 10510 10522, 2025. 2, 5, 6, 7, 8 [53] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. DUSt3R: Geometric 3D vision made easy. In CVPR, pages 2069720709, 2024. 2, 3, 4, 5, 7 [54] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. ScanNet++: high-fidelity dataset of 3D indoor scenes. In ICCV, 2023. 5 [55] Jie Yin, Ang Li, Tao Li, Wenxian Yu, and Danping Zou. M2DGR: multi-sensor and multi-scenario SLAM dataset for ground robots. IEEE Robotics and Automation Letters, 7 (2):22662273, 2021. 3 [56] Ganlin Zhang, Viktor Larsson, and Daniel Barath. Revisiting rotation averaging: Uncertainties and robust losses. In CVPR, pages 1721517224, 2023. [57] Ganlin Zhang, Erik Sandstrom, Youmin Zhang, Manthan Patel, Luc Van Gool, and Martin Oswald. GlORIE-SLAM: Globally optimized RGB-only implicit encoding point cloud SLAM. arXiv preprint arXiv:2403.19549, 2024. 2, 5, 6 [58] Wei Zhang, Tiecheng Sun, Sen Wang, Qing Cheng, and Norbert Haala. HI-SLAM: Monocular real-time dense mapping with hybrid implicit fields. IEEE Robotics and Automation Letters, 9(2):15481555, 2023. [59] Wei Zhang, Qing Cheng, David Skuddis, Niclas Zeller, Daniel Cremers, and Norbert Haala. HI-SLAM2: Geometryaware gaussian SLAM for fast monocular scene reconstruction. arXiv preprint arXiv:2411.17982, 2024. [60] Youmin Zhang, Fabio Tosi, Stefano Mattoccia, and Matteo Poggi. GO-SLAM: Global optimization for consistent 3D instant reconstruction. In ICCV, pages 37273737, 2023. 2 10 [61] Zihan Zhu, Songyou Peng, Viktor Larsson, Zhaopeng Cui, Martin Oswald, Andreas Geiger, and Marc Pollefeys. NICER-SLAM: Neural implicit scene encoding for RGB SLAM. In 3DV, pages 4252. IEEE, 2024. 2, 5, 6 ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association"
        },
        {
            "title": "Metric",
            "content": "chess fire heads office pumpkin redkitchen stairs Avg CUT3R SLAM3R MASt3R-SLAM VGGT-SLAM"
        },
        {
            "title": "Accuracy\nCompleteness\nChamfer\nATE",
            "content": "0.274 0.303 0.289 0.743 0.043 0.030 0.037 0.089 0.090 0.055 0.073 0.063 0.029 0.052 0.040 0.037 0.065 0.063 0.064 0.073 0.102 0.081 0.091 0. 0.022 0.013 0.018 0.048 0.037 0.024 0.031 0.046 0.014 0.064 0.039 0.026 0.015 0.022 0.019 0.035 0.106 0.093 0.100 0.363 0.020 0.015 0.017 0. 0.027 0.021 0.024 0.029 0.031 0.021 0.026 0.022 0.031 0.040 0.035 0.028 0.326 0.441 0.384 0.664 0.035 0.030 0.033 0.088 0.047 0.053 0.050 0. 0.041 0.066 0.054 0.103 0.036 0.048 0.042 0.055 0.452 0.459 0.456 0.546 0.072 0.055 0.064 0.196 0.097 0.054 0.075 0.112 0.128 0.054 0.091 0. 0.061 0.037 0.049 0.129 0.325 0.358 0.342 0.381 0.062 0.061 0.061 0.102 0.070 0.036 0.053 0.074 0.036 0.057 0.047 0.063 0.035 0.030 0.033 0. 0.345 0.389 0.367 0.413 0.116 0.209 0.162 0.126 0.045 0.149 0.097 0.032 0.087 0.110 0.098 0.095 0.074 0.154 0.114 0.029 0.276 0.303 0.290 0. 0.053 0.059 0.056 0.098 0.059 0.056 0.057 0.066 0.052 0.061 0.056 0.070 0.045 0.056 0.051 0.055 Table 7. Per Scene Evaluation on 7-Scenes [43]. Comparison of accuracy, completeness, Chamfer distance, and trajectory error on the 7-Scenes dataset. Lower is better. Best results are bold, second best are underlined. 5. Relative Scale in Pose Graph then, the relative scale sjk can be computed as, As mentioned in Sec. 2.3, the scale edges connect nodes corresponding to the same view but obtained from different forwarding passes. Since the training supervision of the frontend STA model uses only normalized pointmaps, the scales of the same view across different passes are not consistent. Therefore, estimating the relative scale is crucial for pose graph construction. Given two pointmaps and of the same view (obtained from forwarding passes with input view and view k, respectively), along with their confidence maps Cj , we first get the confidence score wx of the point pair for pixel x, and Ck wx =Cj (x) Ck (x), sjk = min (cid:88) wxP i (x) sP (x)2 (cid:80) = wx(P (cid:80) (x) (x) wxP (x)) . (11) 6. Per Scene Evaluation Results on 7-Scenes In Sec. 3.2, only the average reconstruction evaluation is provided. In Tab. 7, we present more detailed per-scene results on 7-Scenes [43] to offer deeper insights. The pure regression method SLAM3R [27] performs well in scenes where the camera primarily focuses on single corner, 1 result w/o PGO result w/ PGO Figure 7. Qualitative Comparison for Pose Graph Optimization. Red boxes highlight regions with misalignments, while green boxes indicate areas where these misalignments have been corrected after pose graph optimization. Figure 8. Qualitative Comparison of Wrong Loop Filtering. Keeping wrong loop candidates decreases the performance lot. duce false positives, which may significantly degrade performance by introducing misleading edges into the pose graph. As shown in Fig. 8, rejecting incorrect loop candidates using the relative pose confidence score provided by STA results in much more stable performance. 7.3. More Results In Fig. 9, we present the results of ViSTA-SLAM across various datasets. ViSTA-SLAM demonstrates stable performance despite differing camera motions in these scenes. As before, light blue frustums represent camera poses, blue lines connect neighboring views, while orange lines indicate loop closures. such as chess, fire, and heads. However, in scenes involving longer camera trajectories, like pumpkin and redkitchen, its performance degrades due to difficulties in accurately registering points. Our method, ViSTASLAM, achieves the best performance in average across all four metrics. 7. More Qualitative Results 7.1. Pose Graph Optimization In Fig. 7, we compare the reconstruction and trajectory estimation results with and without pose graph optimization on ScanNet [8] scene0000 00. Pose graph optimization effectively corrects misaligned areas and averages out the errors from the frontend. 7.2. Wrong Loop Filtering In Sec. 2.3, we describe feeding each loop candidate pair into our STA model to verify their spatial proximity. This is necessary because Bag of Words loop detection can pro2 BundleFusion [9] apt2 TUM-RGBD [45] floor BundleFusion [9] apt 7-Scenes [43] pumpkin M2DGR [55] room 03 M2DGR [55] room 01 Figure 9. More Qualitative Results. Reconstructions and camera trajectories from different datasets."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "MCML",
        "TU Munich"
    ]
}