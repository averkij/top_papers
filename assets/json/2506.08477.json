{
    "paper_title": "Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning",
    "authors": [
        "Fengjun Pan",
        "Anh Tuan Luu",
        "Xiaobao Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Detecting harmful memes is essential for maintaining the integrity of online environments. However, current approaches often struggle with resource efficiency, flexibility, or explainability, limiting their practical deployment in content moderation systems. To address these challenges, we introduce U-CoT+, a novel framework for harmful meme detection. Instead of relying solely on prompting or fine-tuning multimodal models, we first develop a high-fidelity meme-to-text pipeline that converts visual memes into detail-preserving textual descriptions. This design decouples meme interpretation from meme classification, thus avoiding immediate reasoning over complex raw visual content and enabling resource-efficient harmful meme detection with general large language models (LLMs). Building on these textual descriptions, we further incorporate targeted, interpretable human-crafted guidelines to guide models' reasoning under zero-shot CoT prompting. As such, this framework allows for easy adaptation to different harmfulness detection criteria across platforms, regions, and over time, offering high flexibility and explainability. Extensive experiments on seven benchmark datasets validate the effectiveness of our framework, highlighting its potential for explainable and low-resource harmful meme detection using small-scale LLMs. Codes and data are available at: https://anonymous.4open.science/r/HMC-AF2B/README.md."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 7 7 4 8 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Detecting Harmful Memes with Decoupled Understanding and Guided CoT\nReasoning",
            "content": "Fengjun Pan Anh Tuan Luu * Xiaobao Wu * Nanyang Technological University {fengjun001, anhtuan.luu, xiaobao002}@ntu.edu.sg"
        },
        {
            "title": "Abstract",
            "content": "Detecting harmful memes is essential for maintaining the integrity of online environments. However, current approaches often struggle with resource efficiency, flexibility, or explainability, limiting their practical deployment in content moderation systems. To address these challenges, we introduce U-CoT+, novel framework for harmful meme detection. Instead of relying solely on prompting or fine-tuning multimodal models, we first develop high-fidelity meme-to-text pipeline that converts visual memes into detail-preserving textual descriptions. This design decouples meme interpretation from meme classification, thus avoiding immediate reasoning over complex raw visual content and enabling resourceefficient harmful meme detection with general large language models (LLMs). Building on these textual descriptions, we further incorporate targeted, interpretable human-crafted guidelines to guide models reasoning under zero-shot CoT prompting. As such, this framework allows for easy adaptation to different harmfulness detection criteria across platforms, regions, and over time, offering high flexibility and explainability. Extensive experiments on seven benchmark datasets validate the effectiveness of our framework, highlighting its potential for explainable and low-resource harmful meme detection using small-scale LLMs. 1 This paper includes content for demonstration purposes that some readers may find disturbing."
        },
        {
            "title": "Introduction",
            "content": "Memes have emerged to be popular communication medium on social media platforms (Nguyen and Ng, 2024; Joshi et al., 2024). By combining images with overlaid text (Dancygier and Vandelanotte, 2017), they can convey rich information in *Corresponding Authors. 1Codes and data are available at: https://anonymous. 4open.science/r/HMC-AF2B/README.md. 1 visually engaging and immediate manner. However, despite their often humorous nature, memes can be exploited to disseminate harmful content, such as hate speech (Kiela et al., 2020; Davidson et al., 2019), toxic misinformation (Lu et al., 2024; Attanasio et al., 2022), or inflammatory opinions (Liu et al., 2022) etc. under the guise of humor. As such, harmful memes pose threat to the safety and harmony of online environments (Pandiani et al., 2024; Sharma et al., 2022), thus motivating the development of automated harmful meme detection methods, which typically follow supervised fine-tuning (SFT). However, the adoption of these methods in realworld content moderation systems is limited by three critical issues: (i) Low resource efficiency. Supervised approaches require large amounts of labeled training data to perform well (Cao et al., 2023b; Burbi et al., 2023; Shah et al., 2024; Lin et al., 2024a; Cao et al., 2023a; Kumar and Nandakumar, 2022; Pramanick et al., 2021b), but collecting such data is often time-consuming, costly, and potentially biased, as harmful memes with nuanced and implicit meanings can be difficult for human annotators to label consistently without being influenced by their individual knowledge and subjective perceptions. Besides, previous methods often rely on expensive advanced multimodal models (Huang et al., 2024; Ji et al., 2024; Lin et al., 2024a) such as GPT-4(Hurst et al., 2024), posing significant accessibility challenges for practitioners with limited resources. (ii) Low flexibility. The classification criteria distinguishing harmful and harmless memes often vary across sociocultural contexts (Pramanick et al., 2021a; Sharma et al., 2022) and differ significantly across platforms, regions, and over time. Once trained, supervised models becomes difficult to adapt to new criteria and out-of-domain meme types without undergoing costly data collection and retraining (Mei et al., (iii) Low explainabil2025; Cao et al., 2024). ity. Most existing supervised methods operate as black boxes, offering no human-interpretable reasoning behind their predictions (Burkart and Huber, 2021; Yang et al., 2024; Mei et al., 2023; Burbi et al., 2023). This lack of transparency in decisionmaking undermines user trust and the reliability for deployment in sensitive or regulated scenarios (Sasikala and Sachan, 2024), where rationales are essential to support subsequent verification and moderation feedback. To address these limitations, in this paper, we propose U-CoT+, an unimodal guided CoT-based zero-shot harmful meme detection framework. It converts the default multimodal classification setup back to unimodal text-based inference that enables general Large Language Models (LLMs) to perform zero-shot harmful meme detection solely based on natural language input. Specifically, we first propose High-Fidelity Meme2Text pipeline. Instead of directly prompting Large Multimodal Models (LMMs) to make classification decisions as before, we query LMMs multiple times to extract fine-grained visual details from memes (Cao et al., 2023a; Ji et al., 2024), including sensitive identity characteristics of human subjects e.g., race, gender, appearance, and disability status. We then leverage an LLM to integrate these visual details into unified and coherent description of the memes content. As such, we effectively decouple meme understanding from the reasoning of meme classification. This reduces the task complexity by avoiding direct reasoning over complicated visual elements, thereby enabling zero-shot harmful meme detection using general-purpose LLMs. Furthermore, we introduce Guided CoT (Chain-of-Thoughts (Wei et al., 2022)) prompting approach. Rather than relying solely on the inherent knowledge embedded in LLMs, we provide explicit, human-crafted guidelines that define the fine-grained, targeted criteria for detecting harmful content in memes. LLMs perform classification by reasoning over meme descriptions in conjunction with the classification guidelines. This encourages consistent, interpretable, and aligned classifications, and more importantly, enables the framework to easily and effectively adapt to varying harmful meme criteria and wide range of memes from diverse sociocultural contexts. Through extensive experiments on benchmark datasets, we show that our proposed LLMs-based unimodal harmful meme detection framework under zero-shot setting can achieve performance highly comparable to stateof-the-art full-shot supervised fine-tuned baselines. Notably, with guided CoT prompting, small-scale LLMs (such as Mistral-12B (Mistral, 2024) and Qwen2.5-14B (Qwen et al., 2025)) perform competitively through zero-shot inference, matching and even outperforming GPT-4o and other lowresource baselines. We summarize our main contributions as three-fold: We propose resource-efficient, unimodal harmful meme detection framework that effectively leverages the inherent reasoning abilities of pretrained LLMs to enable interpretable meme classification in low-resource settings. By performing zero-shot inference on high-fidelity meme descriptions under guided CoT prompting, our framework can easily adapt to different harmful meme criteria across diverse contexts. We contribute set of interpretable and targeted harmful meme detection guidelines that capture diverse features of harmful memes across different contexts. We demonstrate that these high-quality, human-crafted guidelines can significantly enhance the zero-shot performance of LLMs in harmful meme detection. We conduct extensive experiments and demonstrate that our method achieves comparable or better performance than advanced multimodal models, especially showing higher resource efficiency, flexibility, and explainability."
        },
        {
            "title": "2 Related Work",
            "content": "Harmful Meme Detection aims to classify memes between harmful and harmfless. Contrastive learning-based models e.g., CLIP (Radford et al., 2021) and frameworks have been extensively explored to train multimodal vision language models to fuse cross-modal features and learn unified meme representations in embedding space for classification (Mei et al., 2025; Yang et al., 2024; Su et al., 2024; Burbi et al., 2023; Tzelepi and Mezaris, 2025; Kumar and Nandakumar, 2022). The others convert the original multimodal task to unimodal text classification setting (Ji et al., 2024; Suryawanshi et al., 2023; Cao et al., 2023b) by combining OCR text in memes along with meme image captions generated by pre-trained vision language models e.g., BLIP-2 (Li et al., 2023) to train text classifiers based on pure language models such as RoBERTa (Liu et al., 2019), FLAN-T5 (Chung et al., 2024). To introduce explainability while ensuring classification accuracy, some proposed to 2 Figure 1: Illustration of previous methods and our U-CoT+. Previous methods follow either fully supervised or low-resource settings, fine-tuning LMMs/LLMs with labeled data or prompting advanced LMMs (e.g., GPT-4) with (or without) few-shot examples or retrieval-augmented mechanisms. These methods do not necessarily guarantee predictions that include explicit reasoning. U-CoT+ employs High-fidelity Meme2Text pipeline to convert the multimodal harmful meme detection task into unimodal, text-only setting, and further enhances LLM performance through Unimodal Guided CoT Prompting. An output example given by Qwen2.5-14B under U-CoT+ is presented in Step-by-step Reasoning. explicitly prompt capable LMMs (Hee and Lee, 2025; Lin et al., 2024a) or LLMs(Kumari et al., 2024) to perform reasoning before training small MLP or LLM-based classifiers over the generated textual interpretations or rationales with or without other multimodal features. However, existing literature on zeroor few-shot harmful meme detection remains limited. Huang et al. (2024) leverages LLaVA-34B (Liu et al., 2023b) and GPT-4o (Hurst et al., 2024) as multimodal agents for retrievalaugmented zero-shot harmful meme classification based on small portion of annotated memes. Despite the potential capabilities seen in large-scale LMMs under such low-resource setting, smaller LMMs with much less parameters yet struggle in zero-shot harmful meme detection (Rizwan et al., 2024; Abdullakutty et al., 2024). LLMs with CoT Reasoning Thanks to the extensive pretraining on universal knowledge, Large Language Models (LLMs) such as GPT-4, Llama etc., have demonstrated remarkable zero-shot inference abilities over wide range of reasoning tasks (Kojima et al., 2022; Wei et al., 2021) from commonsense, causality and logical reasoning (Pan et al., 2023; Qi et al., 2023; Pan et al., 2024; Wu et al., 2024e,f; Wu, 2025) to numerical, symbolic and metathetical reasoning (Gruver et al., 2023; Miao et al., 2023). The Chain-of-Thought (CoT) approach (Wei et al., 2022) improves inference performance by prompting LLMs to explicitly generate intermediate reasoning steps before arriving at final answer. Recent studies have shown the effectiveness of combining CoT prompting with supervised fine-tuning for meme metaphor detection (Xu et al., 2024) and misogynistic meme detection (Kumari et al., 2024). Also, Lin et al., 2024b show that some LMMs such as GPT-4V and CogVLM, can benefit from zero-shot CoT prompts for harmful meme classification. However, the capability of unimodal LLMs to perform harmful meme classification under zero-shot CoT prompting remains largely unexplored."
        },
        {
            "title": "3 Methodology",
            "content": "Since human perceptions and definitions of harmful content (including content that is considered hateful or offensive, etc.) are often subjective and vary widely with sociocultural contexts, the criteria for detecting harmful memes can differ across platforms and regions and may even evolve over time. Consequently, in real-world applications, it is both natural and reasonable for practitioners to define 3 specific criteria for harmful content they intend to flag and block. This means that the decision boundary between harmful and harmless memes can often be described in natural language. In fact, such criteria often already exist during the data annotation process (Pramanick et al., 2021a,b), guiding human annotators in determining whether meme should be considered harmful. As result, effective harmful meme detection requires not only commonsense knowledge of widely accepted values, social norms, moral standards, and the historical, cultural, and sociopolitical context of human society but also an understanding of additional context-specific criteria. General-purpose LLMs, pre-trained on largescale web corpora and further aligned with human preferences via reinforcement learning, inherently possess extensive world knowledge and fundamental reasoning abilities that enable them to perform well on wide range of reasoning tasks. Building on this, we hypothesize that even small-scale LLMs internalize sufficient general knowledge and reasoning skills to understand memes. When provided with explicit classification criteria as guidelines, these models can function like human annotators, detecting harmful memes by relying on their inherent capabilities without the need for resourceintensive fine-tuning. With this motivation, we propose U-CoT+, unimodal guided CoT-based framework that empowers LLMs to perform harmful meme detection via zero-shot inference. Our approach converts the default multimodal detection setup into unimodal, text-based commonsense reasoning task, allowing LLMs to process textual meme descriptions alongside human-crafted guidelines under zero-shot CoT prompting. This framework comprises two key components: (1) High-Fidelity Meme2Text pipeline and (2) Unimodal Guided CoT Prompting, as demonstrated in Figure 1."
        },
        {
            "title": "3.1 High-Fidelity Meme2Text",
            "content": "Harmful memes often target people based on their identity. However, most open-source LMMs tend to produce identity-neutral descriptions when directly prompted to describe images (Ji et al., 2024), using ambiguous references such as person or some people. This behavior likely results from preference alignment focused on safety concerns. While this may help avoid bias in general applications, it can be problematic for harmful content detection, where identity characteristics of human subjects often provide essential clues. In fact, existing harmful meme datasets cover only limited range of identity characteristics. We identify few commonly targeted and vulnerable aspects including race, gender, disability, physical appearance, and celebrity status etc., that are critical for detecting harmfulness. To acquire these details, we decompose the task of meme description generation into discrete steps of visual information extraction through basic visual question answering (VQA), which does not necessarily require the underlying models to be highly capable. By repeatedly querying small LMM (such as Llava1.6-7B), each time focusing on specific aspect, we can gather all the key visual details of meme without resorting to more resource-intensive LMMs, while still ensuring comprehensive information coverage necessary for harmful content detection. (See Appendix C.1 for details.) In this way, we effectively decouple meme content understanding from meme classification reasoning. This helps reduce task complexity by avoiding direct reasoning over complicated, entangled visual elements, which can be prone to hallucination. The proposed high-fidelity meme2text pipeline is demonstrated in Figure 14. Specifically, for each meme , we obtain basic low-fidelity description Dl through general Describe prompt such as What is shown in the meme?: Dl = LMM(M, D) (1) We then prompt LMMs to identify whether there is any human subject present in the image. For memes that do portray humans, LMMs are futher prompted to provide identify-related information Ii, where denotes one of the specific characteristics: Ii = LMM(M, ) (2) To remove verbose expressions and warnings in the outputs of LMMs, rather than merely combining the information, we further leverage unimodal LLMs to integrate all LMM outputs into coherent and unified description of the meme content. Dh = LLM(Dl, , , d , , ) (3) This informative high-fidelity description Dh then serves as input for the subsequent text-based classification."
        },
        {
            "title": "3.2.1 Guideline Crafting",
            "content": "The knowledge required for detecting harmful memes is primarily general commonsense. Unlike specialized scientific knowledge that demands expert insights to ensure accuracy, commonsense knowledge is widely accessible and does not require significant cost to acquire or verify. Consequently, crafting harmful meme detection guidelines manually is both intuitive and cost-effective, eliminating the need for heavy reliance on experts to identify patterns of harmful memes in existing datasets. Grounded in this empirical commonsense, we first collect and analyze the definitions, explanations, and examples of harmful memes provided by the authors in each dataset paper, along with their reported data annotation guidelines as initial references. From this analysis, we identify several types of guidelines that are shared across sociocultural contexts and can support LLMs in reasoning more effectively when detecting harmfulness in memes. Implicitness Memes containing derogatory language, abusive slurs, offensive label-calling, direct personal attacks or indication of hate are explicitly harmful, making them easy to flag by LLMs that have been pre-trained to warn about the presence of profanity or swearing. However, as highlighted in Huang et al. (2024), many harmful memes included in existing datasets can be implicit, with their harmful messages wrapped under the deceptive disguise of benign visuals and neutral language. Therefore, this guideline requires LLMs to carefully reflect on memes implication to check whether it is deliberately crafted to evoke negative contextual interpretations e.g., associations to harmful stereotypes, sensitive controversies in societal, cultural or political topics etc. , that are actually meant to be malicious. Tone & Intent We observe that both multiand uni-modal LLMs tend to interpret an arbitrary meme positively if they are not explicitly reminded. Such behavior is probably the result of human preference alignment that aims to ensure appropriate and safe responses, but is undesirable for harmful meme classification. To this end, we include this guideline to remind LLMs not to presume the nature of memes tone and intent as humorous, playful or light-hearted in the first place, but to interpret from neutral perspective. Fine-grained Taxonomy Although harmfulness classification criteria across different contexts are disparate, we can find patterns of harmful content within each specific domain. These patterns can be summarized into fine-grained taxonomies of target or type. The target of harmful meme is the entity at which the harmful message is directed, such as vulnerable minority or protected group, an organization or specific public figure, etc.The type of harmful meme refers to sub-category of harmfulness. For example, misogynistic meme can be further divided into stereotypes, body shaming, objectification, violence, etc. (Fersini et al., 2022). By prompting LLMs to first identify potential target involved in meme, or to classify whether meme falls into fine-grained type instead of the general harmful type, we can narrow the problem, reduce uncertainty, and guide the reasoning of LLMs to be more focused. Examples & Exception We articulate the patterns found in harmful memes in natural language as examples of harmful content, e.g., Perpetuating stereotypes related to females domestic roles, Anti-semitic contents that make light of the Holocaust etc. We can further prompt LLMs to generate more examples and summarize them into more structured heuristics. These examples aim to encourage and support analogical reasoning of LLMs. Moreover, for classification task, it is also important not to sacrifice precision solely for high recall. Exception guidelines define the circumstances when meme should be considered harmless, and may vary with datasets, usually reflecting the inherent bias and specificity in the original annotation."
        },
        {
            "title": "3.2.2 CoT Prompting",
            "content": "Following the widely adopted implementation of chain-of-thought (CoT) reasoning introduced by Wei et al., 2022, we prompt unimodal LLMs in zero-shot setting using heuristic prompt CoT such as Now, lets think step by step or Now, lets apply the guidelines one by one. This encourages LLMs to produce the final classification decision CLS through detailed step-by-step rationales by reasoning over the high-fidelity meme description Dh together with our human-crafted harmful meme detection guidelines GL: CLS, = LLMU-CoT+(Dh, GL, CoT ) (4)"
        },
        {
            "title": "4 Experiments",
            "content": "We report the Accuracy (Acc) and macro-F1 (F1) score for performance evaluation in line with recent studies (Mei et al., 2025; Huang et al., 2024). See Appendix for implementation details. our (ii) MAMI Datasets We proposed evaluate framework on seven widely used harmful (i) FHM (Kiela meme detection datasets: et al., 2020), (Fersini et al., (iii) PrideMM (Shah et al., 2024), 2022), (iv) HarMeme (Pramanick et al., 2021a), (v) Harm-P Pramanick et al., 2021b, (vi) MultiOFF (Suryawanshi et al., 2020) and (vii) GoatBench (Lin et al., 2024b). These datasets and benchmarks cover common harmful content in online memes, addressing different targets and topics across broad range of sociocultural contexts. Table 5 summarizes the dataset statistics. More dataset details are in Appendix A. Baselines We consider the following baselines under two settings. For full-short supervised finetuning-based methods, we consider (i) LMMRGCL (Mei et al., 2025), (ii) UMR (Yang et al., 2024), (iii) Pro-Cap (Cao et al., 2023a), (iv) CapAlign (Ji et al., 2024), (v) MemeCLIP (Shah et al., 2024), (vi) M3H-p-CoT (Kumari et al., (vii) ExplainHM (Lin et al., 2024a), 2024), (viii) IntMeme (Hee and Lee, 2025). For lowresource and training-free baselines, we include (ix) LoReHM (Huang et al., 2024), (x) ModHATE (Cao et al., 2024), (xi) GPT-4o-mini (Hurst et al., 2024). Since we extract LLMs prediction based on exact token match, the probability-based Area Under the Receiver Operating Characteristic Curve (AUC) score is not applicable. For baselines that evaluates fine-tuned classifiers on AUC, we only include their reported Acc results. LMMs & LLMs We experiment with Llava1.67B (Liu et al., 2023a) and Qwen2VL-7B (Wang et al., 2024) as the representative small-scale LMMs for visual information extraction and generation of low-fidelity meme descriptions based on visual question answering. For small-scale LLMs, we evaluate the unimodal harmful meme detection performance of instruction-tuned Qwen2.514B (Qwen et al., 2025), Mistral-12B (Mistral, 2024), Qwen2.5-7B (Qwen et al., 2025), and Llama3.1-8B (Grattafiori et al., 2024), as they represent state-of-the-art reasoning capabilities among 6 open-source models with comparable parameter counts. Table 1 presents the best zero-shot meme classification performance achieved by the bestperforming Qwen2.5-14B and Mistral-12B models on the respective benchmarks, compared to range of strong state-of-the-art baselines under both full-shot supervised fine-tuning and lowresource settings. The performance of Qwen2.57B and Llama3.1-8B are presented in Table 6. By default, each LLM reasons over visual information extracted exclusively by either Llava1.6-7B or Qwen2VL-7B. However, for the FHM dataset, we experiment with specific collaborative setup between the two models: Qwen2VL-7B is responsible for extracting disability-related visual cues, while Llava1.6-7B handles the rest. This design is motivated by Qwen2VL-7Bs lower tendency to hallucinate when identifying such information. This FHM-specific configuration is denoted as LLaVa+Qwen in the subsequent illustrations. We denote the traditional multimodal meme classification scheme based on LMMs under zero-shot CoT settings as M-CoT, where LMMs are directly prompted with the meme image along with the textual prompt. Our proposed unimodal guided CoT-based harmful meme detection scheme is denoted as U-CoT+, while U-CoT denotes the ablation variant, in which the LLMs reason solely based on meme descriptions without referring to our human-crafted guidelines."
        },
        {
            "title": "Baselines",
            "content": "As shown in Table 1, although performance gaps remain between zero-shot guided CoT prompted small-scale LLMs and state-of-the-art fully supervised fine-tuned models, however, on FHM, HarMeme, MAMI and PrideMM, their zero-shot classification performance is nearly comparable to that of SOTA full-shot SFT baselines. For example, Mistral-12B achieves 72.90 (Acc) and 72.87 (F1) on FHM, which has surpassed IntMeme and come within 3 points of the performance achieved by ProCap and ExplainHM. Notably, its performance on HarMeme (83.90/82.41) is comparable to most SFT baselines, surpassing IntMeme and CapAlign, achieving notably higher macro-F1. Both LLMs perform well on the MAMI dataset, with Qwen2.5-14B achieving an accuracy of 79.90 and macro-F1 score of 78.90, which is on par with the performance of LMM-RGCL. While slightly lower,"
        },
        {
            "title": "Base Model",
            "content": "Full-shot Supervised Fine-tuned Classifiers"
        },
        {
            "title": "HarMeme",
            "content": "Harm-P"
        },
        {
            "title": "PrideMM",
            "content": "Acc. F1 Acc. F1 Acc. Acc. F1 Acc. F1 Acc. RGCL UMR Pro-Cap CapAlign MemeCLIP ExplainHM IntMeme M3H-p-CoT Qwen2VL-7B BLIP RoBERTa ChatGPT+FLAN-T5 CLIP GPT3.5+FLAN-T5 FLAVA+RoBERTa Mistral-7B 82.10 77.20 75.10 75.60 71.52 75.39 88.10 87.85 85.03 82.20 87.00 82.66 86.99 68.63 86.41 91.60 92.11 93.80 90.73 91.10 92.11 93.11 90.72 71.10 64.80 69.96 79.90 73.63 72.44 80.28 78.10 76.06 78.40 75.09 Low-resource Baselines w/ Retrieval Augmented Classification LOREHM LOREHM RGCL+RKC LLaVA-34B GPT-4o Qwen2VL-7B 65.60 70.20 69. 65.59 70.14 73.73 74.54 81.90 70.86 72.98 67.30 67.80 63. 55.60 75.40 83.00 75.60 75.28 82.98 69.30 69.30 Low-resource Baselines w/o Retrieval Augmented Classification GPT-4o-mini RGCL+Few-shot Qwen2VL-7B Mod-HATE 67.60 63.50 BLIP-2+LLaMA-7B 57.60 65.51 53.88 70.90 68.10 71.19 69.46 69.64 65.35 65.35 65.77 64.86 77.40 69.05 76.59 68.78 72.39 72.28 Ours: Unimodal Guided CoT U-CoT+ Qwen2.5-14Bf Mistral-12Bf 72.50 72.90 72.41 72. 83.62 83.90 82.00 82.41 65.35 63.10 65.35 62.01 69.13 64.43 68.37 61. 79.90 75.00 79.89 74.72 71.60 69.03 71.37 68.07 Table 1: Comparison with existing SOTA baselines under full-shot SFT or low-resource settings. The best SFT performance is in bold. The best zero-/few-shot performance is in red bold, with the second-best underlined. : LMM-RGCL (Mei et al., 2025). RGCL+RKC/Few-shot is the out-of-distribution performance of the fine-tuned LMM-RGCL model with retrieval augmentation and few-shot examples, respectively. Model Scheme Hateful Acc F1 Harmful F1 Acc Misogyny F1 Acc Offensive F1 Acc GPT-4V M-CoT 71.90 71.37 66. 64.97 81.60 81.57 61.37 60.99 Qwen2.5 (14B) Mistral (12B) U-CoT U-CoT+ U-CoT U-CoT+ 70.80 71.95 70.20 71.45 69.17 71. 68.21 70.43 62.64 71.16 61.45 70.76 60.51 70.69 58.93 68.32 77.40 80. 75.50 75.10 77.38 80.09 75.48 74.83 60.97 61.78 63.53 60.70 59.76 60. 60.62 58.86 Table 2: Performance comparison of small LLMs against GPT-4V on the Goat-Bench dataset. GPT-4V results are given in Lin et al., 2024b. Our best performance is highlighted in bold. the best zero-shot performance of small LLMs on the challenging PrideMM dataset remains comparable to that of SFT baselines, with performance gap within 7 points. The performance disparity between small LLMs and fully supervised baselines is most pronounced on datasets of U.S. political memes. Fully supervised models achieve similarly high accuracy on Harm-P, surpassing 90%, while small LLMs struggle to exceed 66%. Although MultiOFF and HarmP share similar contexts and meme styles, SFT baselines do not exhibit equally strong performance on MultiOFF. Specifically, UMR and LMM-RGCL achieve F1 scores of 69.96 and 64.80, respectively, making the zero-shot performance of Qwen2.514B (69.13/68.37) competitive by comparison. The 7 performance drop of SFT baselines on MultiOFF is likely due to the reduced amount of training data, as shown in Table 5, with only 445 instances which are significantly fewer than the 2,939 available in Harm-P. This highlights the inherent limitations of supervised fine-tuning methods under low-resource conditions, where insufficient training data leads to degraded performance."
        },
        {
            "title": "4.2 U-CoT+ vs. Low-Resource Baselines",
            "content": "Although operating under zero-shot setting, lowresource baselines with retrieval-based augmentation such as LOREHM and LMM-RGCL+RKC, further benefit from majority voting mechanism that leverages the ground-truth labels of the topK most similar memes retrieved in the embedding space. However, the small-scale LLMs in our proposed framework rely solely on humancrafted guidelines, without the need for any auxiliary ground-truth signals or few-shot examples for additional reference. Notably, the performance of the supervised fine-tuned LMM-RGCL degrades significantly in out-of-distribution zero-shot inference settings, even when supported by the retrievalbased KNN classifier (RKC). Table 1 shows that both Qwen2.5-14B and retrievalMistral-12B outperform the two LLM LMM Scheme FHM Acc F1 HarMeme F1 Acc Harm-P Acc F1 MultiOFF F1 Acc MAMI Acc F1 PrideMM F1 Acc GPT-4o-mini 67.60 65.51 70.90 69.46 65.35 65. 65.77 64.86 77.40 76.59 72.39 72. Qwen2VL-7B LLaVa1.6-7B M-CoT . Qwen2VL-7B LLaVa1.6-7B 4 1 LLaVa+Qwen - 5 2 Q Qwen2VL-7B LLaVa1.6-7B LLaVa+Qwen B 2 1 - t Qwen2VL-7B LLaVa1.6-7B LLaVa+Qwen Qwen2VL-7B LLaVa1.6-7B LLaVa+Qwen U-CoT U-CoT+ U-CoT U-CoT+ 64.20 60.40 70.10 68.00 69.10 72.50 71.50 72.00 68.30 65.30 67. 71.20 70.00 72.90 62.68 57.85 70.02 67.70 68.83 72.41 71.48 71.98 68.09 64.60 66.70 71.19 69.88 72. 67.51 66.38 61.02 59.60 81.92 83.62 65.54 62.43 83.90 81.07 60.29 61. 57.92 51.46 81.00 82.00 63.48 54.60 82.41 78.57 56.34 56.34 60.00 63.94 65.35 63.94 58.87 58.03 63.10 59.72 53.05 53.62 59.91 63.84 65.35 63.67 58.32 56.53 62.01 56.40 71.14 59.73 53.69 64.43 63.09 69.13 65.10 61.74 59.06 64.43 64.61 57.38 53.15 62.93 62.41 68.37 62.53 56.89 57.33 61.36 68.10 67.80 75.10 76.40 78.60 79.90 73.00 76.40 74.90 75.00 66.03 67. 75.10 76.38 78.59 79.89 72.99 76.38 74.81 74.72 68.44 60.16 68.44 66.47 70.41 71.60 62.72 62.33 69.03 68.05 68.43 59.99 68.42 66.47 70.04 71.37 62.16 61.68 68.07 67.82 Table 3: Ablation studies comparing different prompting schemes for zero-shot harmful meme detection. Performance improvements from multimodal (M-CoT) to unimodal text-only (U-CoT) settings are highlighted by , while improvements from plain to guided CoT (U-CoT+) are highlighted by . The best unimodal results are in bold. Model Scheme FHM Acc F1 HarMeme Acc F1 MAMI Acc F1 GPT-4o M-CoT 67.60 65.51 70.90 69. 77.40 76.59 Qwen2.5 (14B) U-CoT 70.10 U-CoT+L 70.50 72.50 U-CoT+ 70.02 70.49 72. 61.02 67.23 83.62 57.92 65.90 82.00 76.40 76.70 79.90 76.38 76.70 79.89 Mistral (12B) U-CoT U-CoT+L U-CoT+ 68.30 65.80 72.90 68.09 63.78 72.87 65.54 65.54 83.90 63.48 59.15 82.41 76.40 67.50 75.00 76.38 65.24 74. Table 4: Ablation study comparing the effectiveness of our human-crafted guidelines (U-CoT+) against GPT4o-generated meme insights proposed in Huang et al., 2024 (U-CoT+L) in improving the performance of LLMs under unimodal zero-shot CoT prompting (UCoT). The best unimodal results are in bold, with the second best underlined. Improved results from U-CoT to U-CoT+L are indicated by . : GPT-4o-mini. augmented baselines on FHM and HarMeme, even surpassing LOREHMs performance based on the more advanced GPT-4o. Specifically, Qwen2.514B can further outperform LMM-RGCL+RKC on MultiOFF, MAMI and PrideMM. Compared to low-resource baselines without retrieval augmentation, small LLMs under our guided zero-shot CoT prompting can also significantly outperform both LMM-RGCL and Mod-HATE that are evaluated under few-shot settings. We report the zero-shot performance of GPT-4o-mini based on direct multimodal inference, while the performance of GPT-4o is sourced from Huang et al., 2024. As shown, small LLMs under U-CoT+ consistently outperform or match GPT-4o(-mini) on five out of six datasets in Table 1, although showing slightly weaker performance on PrideMM. Table 2 compares the best zero-shot results achieved by Qwen2.5-14B and Mistral-12B models to those by GPT-4V on Goat-Bench (Lin et al., 2024b) (See Table 7 for complete results). The performance of GPT-4V is based on multimodal zero-shot CoT prompting (M-CoT), while we report our best results both with and without providing guidelines to the LLMs. As can be seen, small LLMs achieve highly comparable performance to, and in some cases even outperform, GPT-4V across all four sub-tasks. These results highlights the advantage of our method in terms of resource efficiency and flexibility, especially when compared to approaches based on supervised learning or more advanced multimodal models. It demonstrates the inherent capabilities of LLMs to detect harmful memes across diverse domains and contexts in zero-shot settings by effectively following explicit guidelines, whereas supervised models often struggle with poor out-ofdistribution generalization. Despite results given by pretrained LLMs with more than 10 billion parameters, we do not observe equally competitive performance on smaller LLMs (e.g., Qwen2.5-7B and Llama3.1-8B). This showcases that harmful meme classification in text-only unimodal setting still places certain demands on the overall reasoning capabilities of the underlying LLMs."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "We present the results of ablation studies in Table 3 and Table 6, focusing on the performance comparison of different prompting schemes: M-CoT, 8 U-CoT, and U-CoT+. Figure 2 presents the detailed confusion matrices under the three different zeroshot CoT schemes. To ensure fair comparisons between multimodal and unimodal settings, we only replace the input images of memes with our highfidelity meme descriptions while keeping all textual prompts identical across the compared schemes. As shown, under M-CoT, the 7B Qwen2VL and Llava1.6 models perform similarly on HarMeme, Harm-P, and MAMI, while Qwen2VL outperforms Llava1.6 on the other three datasets. As shown in Figure 2, except for PrideMM, the two LMMs exhibit an inherent bias toward the negative class across most datasets, frequently overclassifying memes as harmless. This reflects the limited capability of small LMMs in detecting harmfulness in memes based solely on visual inputs. Converting the default multimodal settings into unimodal, text-only settings using more capable LLMs can, in most cases, improve meme classification performance by balancing predictions, leading to more examples being classified as harmful. Consistent improvements from M-CoT to U-CoT are observed on FHM, Harm-P, and MAMI. However, on HarMeme and MultiOFF, only meme descriptions based on Qwen2VL-7B helps Qwen2.5-14B better detect harmful memes. Comparing U-CoT to U-CoT+ demonstrates the overall effectiveness of our human-crafted guidelines, which is particularly prominent for HarMeme, MAMI and PrideMM. For example, our human-crafted guidelines tend to make Qwen2.5-14B produce more predictions of harmful on FHM and more predictions of harmless on PrideMM, but have limited impact on its classification decisions for the two political meme datasets, MultiOFF and Harm-P. Huang et al., 2024 presents GPT-4o-generated insights for harmful meme detection, which serve similar purpose to our human-crafted guidelines by providing additional instructions to guide model reasoning. To evaluate their effectiveness relative to ours, we conduct an ablation study in which we replace our human-crafted guidelines with these GPT-4o-generated insights (results shown in Table 4). We observe that GPT-4o-generated insights do not significantly improve the classification performance of small LLMs and may even lead to performance drops when applied to Mistral-12B. This suggests that small LLMs fail to fully leverage GPTgenerated insights, which are often too general and verbose and lack clear context-specific relevance and distinctions between individual rules. This Figure 2: Comparison of classification confusion matrices under three different zero-shot CoT prompting schemes, with Llava1.6-7B and Qwen2VL-7B as the base LMMs for Meme2Text, and Qwen2.5-14B as the base LLM for unimodal, text-only inference. further highlights the effectiveness of our humancrafted guidelines."
        },
        {
            "title": "4.4 Error Analysis",
            "content": "Our method explicitly provides detailed rationales from which predicted labels are derived, and this transparency in the decision-making process allows us to analyze the inherent behaviors of LLMs for the task of harmful meme detection. We randomly sample 30 balanced error cases (False Negative:False Positive = 1:1) generated by Qwen2.514B under U-CoT+ from each of the three relatively underperformed datasets: FHM, Harm-P, and PrideMM. We identify and summarize the common causes of misclassifications by closely examining the step-by-step reasoning outputs (see Appendix for detailed case study): (i) Incorrect or missing visual details: Due to limited image understanding and text recognition capabilities, 7B LMMs often struggle to generate accurate details for visually noisy, text-heavy memes such as edited screenshots of chats or tweets. They also show limited robustness in recognizing celebrities, including historically significant figures such as (ii) Excessive censorship: LLMs Anne Frank. tend to overinterpret meme content or overstate the potential negative effects when they interpret and apply the guidelines too rigidly or adopt an overly 9 strict perspective, such as regarding sarcasms or parodies on political debates as trivializing serious issues. (iii) Misinterpretation: On one hand, LLMs may misinterpret the memes tone or intent, either in an opposite or skewed direction or fail to grasp the intended meaning when the meme is subtle, ambiguous or poorly framed. On the other hand, discrepancies may exist between LLMs and human interpretations of the guidelines, leading the models to overlook or fail to apply the guidelines consistently. (iv) Different interpretations: LLMs may arrive at differing yet reasonable interpretations, often due to discrepancies between the LLMs and annotators perceptions of the degree of harm. This means that LLMs classifications are not necessarily incorrect, even when they deviate from annotator labels. For instance, LLMs often classify some examples annotated as harmful as harmless, interpreting them as merely mild ridicule or satire, which is both common and acceptable in political memes. (v) Misclassification of targets: Since the predicted target determines which subset of guidelines is applied, inaccurate predictions of the memes target subject can lead to guideline mismatches and, consequently, to misclassifications. (vi) Blind Spot: LLMs can still be limited by types of harmfulness not covered by the guidelines, particularly when encountering rare examples that deviate from the main context."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we focus on the task of harmful meme detection and address the limitations of existing methods in resource efficiency, flexibility, and explainability. We introduce U-CoT+, framework that converts the default multimodal setting into unimodal, text-only task using high-fidelity meme-to-text pipeline, and further boosts the zeroshot performance of LLMs through novel guided CoT prompting scheme. By incorporating highquality, targeted, and interpretable human-crafted guidelines, U-CoT+ enables small-scale LLMs to achieve strong performance comparable to fully supervised baselines and, to match or outperform existing low-resource baselines and advanced GPT4-based models across seven datasets. These results demonstrate the effectiveness of our framework for low-resource, explainable harmful meme detection."
        },
        {
            "title": "Limitations",
            "content": "We consider the following limitations for future work. First, we focus solely on the zero-shot harmful meme classification performance of individual models i.e., we report results based only on each LLMs independent predictions. We may later explore whether ensemble methods, such as majority voting across the decisions given by multiple small-scale LLMs and LMMs, can further improve zero-shot performance. Second, we currently only rely on LLMs internalized knowledge for zero-shot classification, it is yet to explored whether small-scale LLMs under our proposed guided CoT prompting can further benefit from Retrieval Augmented Generation (Anaissi et al., 2025) that equips LLMs with additional knowledge."
        },
        {
            "title": "Ethics Statement",
            "content": "This research exclusively utilizes publicly available datasets and does not involve any personally identifiable information. We have taken steps to minimize potential biases by implementing explicit, human-crafted guidelines to guide unimodal LLMs behaviors in detecting harmful content in memes. Our approach prioritizes transparency and interpretability, enabling human oversight of model predictions and reasoning steps. All code and data used in our experiments are released to support reproducibility and community validation. We acknowledge that some readers may find certain content disturbing, and we advise appropriate caution when reviewing examples included for demonstration purposes."
        },
        {
            "title": "References",
            "content": "Faseela Abdullakutty, Somaya Al-Maadeed, and Usman Naseem. 2024. Context-aware offensive meme detection: multi-modal zero-shot approach with caption-enhanced classification. In 2024 IEEE International Conference on Data Mining Workshops (ICDMW), pages 137145. IEEE. Ali Anaissi, Junaid Akram, Kunal Chaturvedi, and Ali Braytee. 2025. Detecting and understanding hateful contents in memes through captioning and visual question-answering. arXiv preprint arXiv:2504.16723. Giuseppe Attanasio, Debora Nozza, Federico Bianchi, and 1 others. 2022. Milanlp at semeval-2022 task 5: Using perceiver io for detecting misogynous memes with text and image modalities. In Proceedings of 10 the 16th International Workshop on Semantic Evaluation (SemEval-2022). Association for Computational Linguistics. Giovanni Burbi, Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Alberto Del Bimbo. 2023. Mapping memes to words for multimodal hateful meme classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 28322836. Nadia Burkart and Marco Huber. 2021. survey on the explainability of supervised machine learning. Journal of Artificial Intelligence Research, 70:245 317. Rui Cao, Ming Shan Hee, Adriel Kuek, Wen-Haw Chong, Roy Ka-Wei Lee, and Jing Jiang. 2023a. Procap: Leveraging frozen vision-language model for hateful meme detection. In Proceedings of the 31st ACM International Conference on Multimedia, pages 52445252. Rui Cao, Roy Ka-Wei Lee, Wen-Haw Chong, and Prompting for multimodal arXiv preprint Jing Jiang. 2023b. hateful meme classification. arXiv:2302.04156. Rui Cao, Roy Ka-Wei Lee, and Jing Jiang. 2024. Modularized networks for few-shot hateful meme detection. In Proceedings of the ACM Web Conference 2024, pages 45754584. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, and 1 others. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. Barbara Dancygier and Lieven Vandelanotte. 2017. Internet memes as multimodal constructions. Cognitive Linguistics, 28(3):565598. Thomas Davidson, Debasmita Bhattacharya, and Ingmar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. arXiv preprint arXiv:1905.12516. Elisabetta Fersini, Francesca Gasparini, Giulia Rizzi, Aurora Saibene, Berta Chulvi, Paolo Rosso, Alyssa Lees, and Jeffrey Sorensen. 2022. SemEval-2022 task 5: Multimedia automatic misogyny identifiIn Proceedings of the 16th International cation. Workshop on Semantic Evaluation (SemEval-2022), pages 533549, Seattle, United States. Association for Computational Linguistics. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Wilson. 2023. Large language models are zero-shot time series forecasters. Advances in Neural Information Processing Systems, 36:1962219635. Ming Shan Hee and Roy Ka-Wei Lee. 2025. Demystifying hateful content: Leveraging large multimodal models for hateful meme detection with explainable decisions. arXiv preprint arXiv:2502.11073. Jianzhao Huang, Hongzhan Lin, Ziyan Liu, Ziyang Luo, Guang Chen, and Jing Ma. 2024. Towards lowresource harmful meme detection with lmm agents. arXiv preprint arXiv:2411.05383. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Junhui Ji, Xuanrui Lin, and Usman Naseem. 2024. Capalign: Improving cross modal alignment via informative captioning for harmful meme detection. In Proceedings of the ACM Web Conference 2024, pages 45854594. Saurav Joshi, Filip Ilievski, and Luca Luceri. 2024. Contextualizing internet memes across social media platforms. In Companion Proceedings of the ACM Web Conference 2024, pages 18311840. Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances in neural information processing systems, 33:26112624. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Gokul Karthik Kumar and Karthik Nandakumar. 2022. Hate-clipper: Multimodal hateful meme classification based on cross-modal interaction of clip features. arXiv preprint arXiv:2210.05916. Gitanjali Kumari, Kirtan Jain, and Asif Ekbal. 2024. M3hop-cot: Misogynous meme identification with arXiv multimodal multi-hop chain-of-thought. preprint arXiv:2410.09220. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, and Ruichao Yang. 2024a. Towards explainable harmful meme detection through multimodal debate between large language models. In Proceedings of the ACM Web Conference 2024, pages 23592370. 11 Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang, and Jing Ma. 2024b. Goat-bench: Safety insights to large multimodal models through meme-based social abuse. ACM Transactions on Intelligent Systems and Technology. Chen Liu, Gregor Geigle, Robin Krebs, and Iryna Gurevych. 2022. Figmemes: dataset for figurative language identification in politically-opinionated memes. In Proceedings of the 2022 conference on empirical methods in natural language processing, pages 70697086. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. Preprint, arXiv:2310.03744. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. Advances in neural information processing systems, 36:34892 34916. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Junyu Lu, Bo Xu, Xiaokun Zhang, Hongbo Wang, Haohao Zhu, Dongyu Zhang, Liang Yang, and Hongfei Lin. 2024. Towards comprehensive detection of chinese harmful memes. Advances in Neural Information Processing Systems, 37:1330213320. Jingbiao Mei, Jinghong Chen, Weizhe Lin, Bill Byrne, and Marcus Tomalin. 2023. Improving hateful meme detection through retrieval-guided contrastive learning. arXiv preprint arXiv:2311.08110. Jingbiao Mei, Jinghong Chen, Guangyu Yang, Weizhe Lin, and Bill Byrne. 2025. Improved fine-tuning of large multimodal models for hateful meme detection. arXiv preprint arXiv:2502.13061. Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. arXiv preprint arXiv:2308.00436. Mistral. 2024. Mistral. Khoi Nguyen and Vincent Ng. 2024. Computational meme understanding: survey. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2125121267. Fengjun Pan, Xiaobao Wu, Zongrui Li, and Anh Tuan Luu. 2024. Are llms good zero-shot fallacy classifiers? arXiv preprint arXiv:2410.15050. 69817004, Toronto, Canada. Association for Computational Linguistics. Delfina Sol Martinez Pandiani, Erik Tjong Kim Sang, and Davide Ceolin. 2024. Toxic memes: survey of computational perspectives on the detection and explanation of meme toxicities. arXiv preprint arXiv:2406.07353. Shraman Pramanick, Dimitar Dimitrov, Rituparna Mukherjee, Shivam Sharma, Md. Shad Akhtar, Preslav Nakov, and Tanmoy Chakraborty. 2021a. Detecting harmful memes and their targets. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 27832796, Online. Association for Computational Linguistics. Shraman Pramanick, Shivam Sharma, Dimitar Dimitrov, Md Shad Akhtar, Preslav Nakov, and Tanmoy Chakraborty. 2021b. Momenta: multimodal framework for detecting harmful memes and their targets. arXiv preprint arXiv:2109.05184. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, and Bowen Zhou. 2023. Large language models are zero shot hypothesis proposers. arXiv preprint arXiv:2311.05965. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and 1 others. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR. Naquee Rizwan, Paramananda Bhaskar, Mithun Das, Swadhin Satyaprakash Majhi, Punyajoy Saha, and Animesh Mukherjee. 2024. Zero shot vlms for hate meme detection: Are we there yet? arXiv preprint arXiv:2402.12198. Sasikala and Shubham Sachan. 2024. Decoding decision-making: embracing explainable ai for trust and transparency. Exploring the frontiers of artificial intelligence and machine learning technologies, 42. Siddhant Bikram Shah, Shuvam Shiwakoti, Maheep Chaudhary, and Haohan Wang. 2024. Memeclip: Leveraging clip representations for mularXiv preprint timodal meme classification. arXiv:2409.14703. Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. 2023. Fact-checking complex claims with program-guided reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages Shivam Sharma, Firoj Alam, Md Shad Akhtar, Dimitar Dimitrov, Giovanni Da San Martino, Hamed Firooz, Alon Halevy, Fabrizio Silvestri, Preslav Nakov, and Tanmoy Chakraborty. 2022. Detecting and understanding harmful memes: survey. arXiv preprint arXiv:2205.04274. Xuanyu Su, Yansong Li, Diana Inkpen, and Nathalie Japkowicz. 2024. Hatesieve: contrastive learning framework for detecting and segmenting hateful content in multimodal memes. arXiv preprint arXiv:2408.05794. Shardul Suryawanshi, Mihael Arcan, Suzanne Little, and Paul Buitelaar. 2023. Multimodal offensive meme classification with natural language inference. In Proceedings of the 4th Conference on Language, Data and Knowledge, pages 134145. Shardul Suryawanshi, Bharathi Raja Chakravarthi, Mihael Arcan, and Paul Buitelaar. 2020. Multimodal meme dataset (MultiOFF) for identifying offensive In Proceedings of the content in image and text. Second Workshop on Trolling, Aggression and Cyberbullying, pages 3241, Marseille, France. European Language Resources Association (ELRA). Maria Tzelepi and Vasileios Mezaris. 2025. Improving multimodal hateful meme detection exploiting lmm-generated knowledge. arXiv preprint arXiv:2504.09914. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. Preprint, arXiv:2409.12191. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Xiaobao Wu. 2025. Sailing ai by the stars: survey of learning from rewards in post-training and test-time scaling of large language models. arXiv preprint arXiv:2505.02686. Xiaobao Wu, Xinshuai Dong, Thong Nguyen, Chaoqun Liu, Liang-Ming Pan, and Anh Tuan Luu. 2023a. Infoctm: mutual information maximization perspective of cross-lingual topic modeling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1376313771. Xiaobao Wu, Xinshuai Dong, Thong Nguyen, and Anh Tuan Luu. 2023b. Effective neural topic modeling with embedding clustering regularization. In International Conference on Machine Learning. PMLR. Xiaobao Wu, Chunping Li, Yan Zhu, and Yishu Miao. 2020. Short text topic modeling with topic distribution quantization and negative sampling decoder. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 17721782, Online. Xiaobao Wu, Anh Tuan Luu, and Xinshuai Dong. 2022. Mitigating data sparsity for short text topic modeling by topic-semantic contrastive learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 27482760, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Xiaobao Wu, Thong Nguyen, and Anh Tuan Luu. 2024a. survey on neural topic models: Methods, applications, and challenges. Artificial Intelligence Review. Xiaobao Wu, Thong Thanh Nguyen, Delvin Ce Zhang, William Yang Wang, and Anh Tuan Luu. 2024b. Fastopic: Pretrained transformer is fast, adaptive, stable, and transferable topic model. In The Thirtyeighth Annual Conference on Neural Information Processing Systems. Xiaobao Wu, Fengjun Pan, and Anh Tuan Luu. 2024c. Towards the TopMost: topic modeling system toolkit. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 3141, Bangkok, Thailand. Association for Computational Linguistics. Xiaobao Wu, Fengjun Pan, Thong Nguyen, Yichao Feng, Chaoqun Liu, Cong-Duy Nguyen, and Anh Tuan Luu. 2024d. On the affinity, rationality, and diversity of hierarchical topic modeling. In Proceedings of the AAAI Conference on Artificial Intelligence. Xiaobao Wu, Liangming Pan, William Yang Wang, and Anh Tuan Luu. 2024e. AKEW: Assessing knowlIn Proceedings of the edge editing in the wild. 2024 Conference on Empirical Methods in Natural Language Processing, pages 1511815133, Miami, Florida, USA. Association for Computational Linguistics. Xiaobao Wu, Liangming Pan, Yuxi Xie, Ruiwen Zhou, Shuai Zhao, Yubo Ma, Mingzhe Du, Rui Mao, Anh Tuan Luu, and William Yang Wang. 2024f. Antileak-bench: Preventing data contamination by automatically constructing benchmarks with updated real-world knowledge. arXiv preprint arXiv:2412.13670. Yanzhi Xu, Yueying Hua, Shichen Li, and Zhongqing Wang. 2024. Exploring chain-of-thought for multiIn Proceedings of the modal metaphor detection. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 91101. 13 Chuanpeng Yang, Yaxin Liu, Fuqing Zhu, Jizhong Han, and Songlin Hu. 2024. Uncertainty-guided modal rebalance for hateful memes detection. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 43614371."
        },
        {
            "title": "Datasets",
            "content": "Test #Harm #Benign"
        },
        {
            "title": "Train",
            "content": "FHM (Kiela et al., 2020) HarMeme (Pramanick et al., 2021a) Harm-P (Pramanick et al., 2021b) MultiOFF (Suryawanshi et al., 2020) MAMI (Fersini et al., 2022) PrideMM (Shah et al., 2024) GB(Hateful) (Lin et al., 2024b) GB(Harmful) GB(Offensive) GB(Misogynistic) 490 124 171 58 500 247 750 420 303 500 510 8500 230 3013 184 2939 91 445 500 9000 260 4328 - - - - 1250 589 440 500 Table 5: Dataset Statistics. GB: The Goat-Bench dataset (Lin et al., 2024b). The sizes of the training splits are provided as reference for the training data scale used in SFT-based models."
        },
        {
            "title": "A Datasets",
            "content": "We experiment with the following datasets: FHM (Kiela et al., 2020) The Facebook Hateful Memes (FHM) dataset was released in 2021 as challenge set for multimodal classification, focusing on detecting hate speech in multimodal memes. The dataset was synthetically constructed to examine model performance in distinguishing real hateful memes from harmless ones that include their benign confounders, which are constructed with flipped labels through minimal changes to one of the modalities (either images or overlaid text). The hatefulness of meme can be determined by either both or only one of the modalities (Wu et al., 2020, 2022, 2023b,a, 2024c,d,b,a). Hateful speech under the context of this dataset is defined as: direct or indirect attack on people based on characteristics, including ethnicity, race, nationality, immigration status, religion, caste, sex, gender identity, sexual orientation, and disability or disease. We define attack as violent or dehumanizing (comparing people to non-human things, e.g. animals) speech, statements of inferiority, and calls for exclusion or segregation. Mocking hate crime is also considered hate speech. HarMeme (Pramanick et al., 2021a) contains 3,544 online memes related to COVID-19 pandemic. They define harmful in broader way different from hateful or offensive as the potential to cause mental abuse, defamation, psycho-physiological injury, proprietary damage, emotional disturbance, and compensated public image. The targets of these harmful memes can be any individual, organization, community, or the general society. Given the inherently ambiguous nature of such definition, which could be different from general criteria internalized in most LLMs, we find that annotated labels in this dataset are highly biased to favor specific types of memes as harmful, particularly those that may harm the public image of Donald Trump. HarmP (Pramanick et al., 2021b) is follow-up work of HarMeme but has shift in attention to broader collection of U.S. political memes, which are labelled into three categories i.e., very harmful, somewhat harmful and not harmful, following the same annotation procedure in HarMeme. Following prior works (Mei et al., 2025), we incorporate very and somewhat harmful memes into unified harmful class. Yet, the ambiguity in harmfulness definition may have resulted in unclear criteria and inconsistent annotation in the data. In addition, unlike synthetic memes that usually follow standard creation format, online political memes can be highly free-form or noisy, e.g., digitally altered, distorted, masked, blurred, filled with hardly identifiable text etc. Such low-quality images pose great challenges on existing small LMMs. MultiOFF (Suryawanshi et al., 2020) is small collection of political memes similar to those in Harm-P but are mainly related to 2016 U.S. presidential election in terms of context. This datasets highlights specific taxonomy of multiple types of offensive memes, including racial abuse, attacking minorities, personal attacks and homophobic abuse. MAMI (Fersini et al., 2022) specifically focuses on misogynistic memes that cause harm to women. Misogynistic memes are generally more explicit in language and imagery compared to other types of harmful memes. PrideMM (Shah et al., 2024) contains 5,063 online memes in the context of LGBTQ+ pride movements. Each meme is annotated with labels indicating its stance toward the goal of LGBTQ+ movements, whether it is considered hateful or humorous. Apart from the binary labels for hatefulness, the dataset defines taxonomy of hate targets: Undirected, Individual, Community and Organization. Whether meme can be regarded as hateful varies with respect to the perspective of different targets. Therefore, 15 this dataset also includes target classification task. then trains T5-based model with multimodal fusion to assess meme harmfulness. GoatBench (Lin et al., 2024b) is comprehensive meme benchmark comprising over 6K varied memes encapsulating themes such as implicit hate speech, sexism, and cyberbullying, etc., curated from the above datasets. In this work, we only evaluate on their harmfulness-related subtasks, without considering the task of Sarcasm."
        },
        {
            "title": "B Baselines",
            "content": "We summarize the key techniques and backbone models of the baselines as follows: LMM-RGCL (Mei et al., 2025) learns hateaware vision and language representations through contrastive learning objective applied to pre-trained CLIP encoder, achieving state-ofthe-art performance on the MultiOFF dataset. UMR (Yang et al., 2024) integrates uncertaintyguided modal rebalance with Gaussian-based stochastic embeddings and improved cosine loss on frozen CLIP/ALBEF/BLIP/BLIP-2 backbones to handle modality imbalance in multimodal hate detection. Pro-Cap (Cao et al., 2023a) leverages frozen BLIP-2 for zero-shot VQA to generate probingbased captions, which are then combined with meme texts for training BERT and PromptHate models on hateful meme detection. CapAlign (Ji et al., 2024) uses LLaMA-2-7B as the base model and fine-tunes it with LoRA on instruction-following and synthetic alignment conversations, enabling efficient and aligned caption generation for multimodal tasks. MemeCLIP (Shah et al., 2024) builds on frozen CLIP ViT-L/14 encoders and introduces lightweight linear projections, feature adapters, and cosine classifier with semantic-aware initialization for multi-aspect meme classification. M3H-p-CoT (Kumari et al., 2024) proposes multi-hop reasoning framework combining EOR features and meme text to prompt CLIPbased classifier through hierarchical CoT modules trained with cross-attention and contrastive learning. IntMeme (Hee and Lee, 2025) uses frozen InstructBLIP and mPLUG-Owl to generate meme interpretations in zero-shot setting, then trains RoBERTa and FLAVA encoders on meme content and interpretations for hateful meme classification. LoReHM (Huang et al., 2024) adopts an agentbased LMM framework (experimenting on Llava34B and GPT-4o) that leverages few-shot incontext learning and self-improvement capabilities for low-resource hateful meme detection. Mod-HATE (Cao et al., 2024) trains suite of LoRA modules and utilizes few-shot demonstration examples to train module composer, which assigns weights to the LoRA modules for effective low-resource hateful meme detection. GPT-4o (Hurst et al., 2024) is cutting-edge multimodal model."
        },
        {
            "title": "C Implementation Details",
            "content": "We run the zero-shot inference experiments on two NVIDIA A6000 48GB GPUs and six NVIDIA A40 48GB GPUs. All LMM-based multimodal inference are conducted with batch size of 1. Qwen2.5-based LLMs and Mistral are run with batch size of 16. LLama3.1 is run with batch size of 32. We obtain GPT-4o-mini inference results using the OpenAI API with the model version gpt-4o-mini-2024-07-18. For reproducibility, we adhere to greedy decoding without sampling (with do_sample set to False and temperature as 0) to ensure fully deterministic generation. The max_new_tokens is set as 256/356 for LMMs and 1024/1536 for LLMs depending on datasets. C.1 Visual Information Extraction We experiment with two widely used open-source LMMs with 7 billion parameters i.e., Llava1.6Mistral-7B (Liu et al., 2023a) and Qwen2-VL7B (Wang et al., 2024). The two models have been extensively pre-trained to gain strong capabilities in image understanding, embedded text recognition and multimodal inference, and are generally accessible under most computational resource conditions. ExplainHM (Lin et al., 2024a) fine-tunes LLaVA (Liu et al., 2023b) to generate multimodal rationales under debate-style prompts and"
        },
        {
            "title": "We identify various types of visual clues that are\ncritical for downstream harmful meme detection\nand design the questions accordingly to prompt",
            "content": "16 LLM LMM Scheme FHM Acc F1 HarMeme F1 Acc Harm-P Acc MultiOFF F1 Acc MAMI Acc F1 PrideMM Acc GPT-4o-mini 67.60 65.51 70. 69.46 65.35 65.35 65.77 64.86 77. 76.59 72.39 72.28 Qwen2VL-7B LLaVa1.6-7B Qwen2VL-7B LLaVa1.6-7B LLaVa+Qwen Qwen2VL-7B LLaVa1.6-7B LLaVa+Qwen Qwen2VL-7B LLaVa1.6-7B LLaVa+Qwen Qwen2VL-7B LLaVa1.6-7B LLaVa+Qwen M-CoT U-CoT U-CoT+ U-CoT U-CoT+ 64.20 60.40 63.50 63.00 62.80 68.00 67.00 65.70 66.70 62.70 63.80 67.50 65.70 66. 62.68 57.85 61.44 60.36 60.15 66.98 65.51 64.10 66.45 61.94 63.23 67.50 65.53 66.55 67.51 66. 62.43 65.25 71.19 72.03 48.59 59.89 70.34 77.12 60.29 61.05 50.64 46.66 61.44 60.88 47.07 59.82 69.97 75.26 56.34 56.34 62.25 59.72 61.13 57.75 58.59 58.03 62.54 59.15 53.05 53.62 58.78 56.40 56.21 51.09 57.46 57.80 60.20 54.99 71.14 59.73 63.76 63.09 68.46 63.76 57.05 63.76 62.42 67.11 64.61 57.38 57.55 51.84 62.35 53.86 56.81 63.72 62.13 66.63 68.10 67. 69.80 68.70 71.90 73.50 71.80 70.60 73.60 74.40 66.03 67.46 69.09 68.10 71.86 73.50 71.80 70.59 73.39 74.40 68.44 60.16 61.34 60.55 63.51 64.50 63.51 63.31 67.26 64.30 68.43 59.99 57.90 58.49 60.86 63.26 62.38 61.98 66.90 64.00 . 8 - 1 3 l . 7 - 5 2 Q Table 6: Complete results comparing different prompting schemes for zero-shot harmful meme detection using Llama3.1-8B and Qwen2.5-7B. Performance improvements from multimodal (M-CoT) to unimodal text-only (U-CoT) settings are highlighted by , while improvements from plain to guided CoT (U-CoT+) are highlighted by . The best results achieved by each LLM are in bold."
        },
        {
            "title": "Scheme",
            "content": "GPT-4V M-CoT 4 1 - 5 . 2 Q 2 1 - t M Qwen2VL-7B LLaVa1.6-7B LLaVa+Qwen Qwen2VL-7B LLaVa1.6-7B LLaVa+Qwen Qwen2VL-7B LLaVa1.6-7B LLaVa+Qwen Qwen2VL-7B LLaVa1.6-7B LLaVa+Qwen U-CoT U-CoT+ U-CoT U-CoT+ Hatefulness F1 Acc Harmfulness F1 Acc 71. 70.80 70.15 69.70 71.25 71.95 70.80 70.2 68.15 68.2 71.45 70.05 71.75 71.37 69.17 67.44 67. 70.75 71.05 69.95 68.21 64.45 64.67 70.43 68.15 70.15 66.90 60.16 62.64 71.16 71.56 61.45 60.46 70.76 70.66 64.97 59.43 60.51 70.69 70.53 58.93 56.84 68.32 67."
        },
        {
            "title": "Acc",
            "content": "81.60 77.20 77.40 79.30 80.10 74.9 75.5 73.8 75.1 81.57 77.20 77.38 79.28 80.09 74.9 75.48 73.73 74.83 Offensiveness F1 Acc 61.37 58.95 60.97 59.49 61.78 63.53 60.57 60.70 61.1 60. 58.39 59.76 58.91 60.90 60.62 56.22 58.86 58.07 Table 7: Complete harmful meme detection results on Goat-Bench (Lin et al., 2024b). LMMs to extract the information through visual question answering (VQA). In particular, for the synthetic FHM dataset, since the visual style of its images is more like that of regular photographs other than that of authentic online memes, the captions of some memes do not directly relate to the visual contents. Therefore, we explicitly add the instruction Ignore any overlaid text or caption., denoted by [Ignore], after the questions to force LMMs to focus solely on the visual content without being distracted by the overlaid captions. For most binary questions, we adopt this template for unified output format: Start your response with Yes, or No, before giving the explanation., denoted by [Fm]. The overlaid text or caption in meme is denoted by [OCR]. Question templates: Human Is there any human subject in the given image? ([Ignore]) [Fm] #Human Does the image include more than one human subject? ([Ignore]) [Fm] Gender What is/are the gender(s) of the human subject(s) in the image? Race What is/are the perceived race(s) of the human subject(s) in the image? Appearances What are the distinctive physical appearance characteristics of the human subject(s) in the image? Disability Does any of the human subjects in the image have any disability? 17 (i) Who is/are the human subject(s) in Celebrity the image? (ii) Is any celebrity or historical figure portrayed in the image? If yes, who are they? If no, just output No..[Fm](iii) FHM-specific: a. Does the image portray Adolf Hitler? If yes, output Yes, Adolf Hitler is portrayed in the image. Otherwise, if you are not sure about the identity of human subject, just output No, cant tell. b. Does the image portray Anne Frank, the Jewish girl who hid from the Nazis during World War II? If yes, output Yes, Anne Frank is portrayed in the image. Otherwise, if you are not sure about the identity of human subject, just output No, cant tell. We design the following misogyny-specific questions to extract visual cues that are crucial for detecting potential misogynistic contents in memes: Adult Content Does the images visual content contain adult content?[Ignore][Fm] Female image female?[Ignore][Fm] Is/Are there any human subject(s) in the (i) Does the images visual content highSexual light the sexiness of the females figure in way that is sexually provocative?[Ignore][Fm](ii) Does this image highlight sexual body parts of the female subject(s), such as the breast, the hip/buttock, or the genital?[Ignore][Fm](iii) Does/Do the female subject(s) in the image appear to be overweight?[Ignore][Fm](iv) Does/Do the female subject(s) in the image appear to be of large body size (considered as fat)?[Ignore][Fm] The following questions are used to extract politics-related visual cues (e.g., politicians, political parties, sensitive topics) in political memes i.e., those in HarMeme, Harm-P and MultiOFF: (i) Is any politician or celebrity porPoliticians trayed in the image? If yes, who?[Fm](ii) Is any head of state portrayed in the image?[Fm](iii) Is Donald Trump depicted in the image?[Fm](iv) Is Joe Biden depicted in the image?[Fm](v) Is Barack Obama depicted in the image?[Fm](vi) Is Hillary Clinton depicted in the image?[Fm](vii) Is Bernie Sanders depicted in the image?[Fm](viii) Is Gary Johnson depicted in the image?[Fm](ix) Does this image feature Joe Biden and Barack Obama?[Fm] (i) Is any political party explicPolitical Issues itly involved in this image?[Fm](ii) Is LGBTQ+ community or LGBTQ+ individual portrayed in this image?[Fm](iii) Is any individual of Middle Eastern descent portrayed in the image?[Fm](iv) Is any protected racial or minority group (such as African Americans or other colored people) portrayed in this image?[Fm] Prompts designed for LMMs to generate descriptions of memes, either including or ignoring the overlaid text: (i) FHM: What is shown in the image? Describe Describe within two sentences, ignoring any overlaid text or caption. (ii) HarMeme and Harm-P: What is shown in the meme? (iii) MultiOff: What is shown in this image? (iv) MAMI: The overlaid text on the image reads:[OCR]. Question: What is shown in the image? Describe within three sentences. DO NOT assume the nature of the images tone or intent as humorous, comical, playful or lighthearted in your description. (v) PrideMM: This is an online meme related to LGBTQ+ pride movement. What is this meme about? Note: DO NOT ASSUME the nature of the memes tone and intent as humorous or lighthearted. Describe in neutral tone. C."
        },
        {
            "title": "Information Integration",
            "content": "After acquiring visual information leveraging LMMs, we prompt unimodal text-only LLMs to integrate the extracted visual cues (along with the overlaid caption [OCR] for some datasets) into coherent, unified description of the content of each meme. We experiment with four open-source small-scale LLMs with no more than 14 billion parameters i.e., Qwen2.5-14B (Qwen et al., 2025), Mistral-12B (Mistral, 2024), Qwen2.5-7B (Qwen et al., 2025) and Llama3.1-8B (Grattafiori et al., 2024). All LLMs are the instruction-tuned versions. The visual information gathered is denoted by [VIG]. Examples of integration prompt templates: FHM Given the following information provided about an image, and disregarding any information about overlaid text or captions, synthesize and rephrase these information into coherent and unified description of the images content. Information:[VIG] PrideMM Given the following information provided about an online meme, synthesize and rephrase these information into unified, coherent, and neutral description of the meme content. DO NOT mention the memes tone and intent as humorous or light-hearted in the description. 18 Information:[VIG]. The overlaid caption or text recognized in the meme reads:[OCR] Appendix C.2 converts memes to text. The following sections (including the proposed guided CoT reasoning) are based on the generated meme description, denoted by [M2T]. Chain-of-Thought prompting instruction (e.g., Now, lets think step by step:, Now, lets analyze by applying all the guidelines one by one:) is denoted by [CoT]. C.3 Fine-grained Target Identification For FHM, we include three target-oriented reasoning steps prior to the final hateful classification to prepare for target-specific guidelines: Protected Group Detection Given the following image-caption content which may or may not be an online meme, analyze: what vulnerable protected group(s) might be involved in the content? Here are some guidelines for your reference: A. If the content does not appear to involve any specific protected group, just outputNo specific protected group involved. B. If the image content does involve specific protected group(s), choose your answer(s) from the specified list (you may choose multiple options if there are more than one protected groups involved): 1. Women (Female); 2. LGBTQ Community; 3. People with Disabilities; 4. Muslims and Islamic culture; 5. Individuals of Middle Eastern descent; 6. Jewish individuals; 7. Individuals of African descent; 8. African Americans; 9. Individuals of East Asian descent; 10. Individuals of South Asian descent; 11. Native Americans; 12. Other protected groups not listed. Here is the imagecaption content you need to analyze:[M2T]. The caption overlaid on the image reads[OCR].[CoT] We denote the target protected groups identified as G1, G2,. . . ,T Gn. The following template prompts LLMs with few-shot human-defined hateful examples [FS] to sequentially generate more hateful forms targeting each protected group: Hateful Forms Generation 1. Provide examples of commonly found harmful stereotypes and forms of offensive, hateful content against G1 in online memes. Provide only phrases or terms without detailed explanations e.g.,[FS]. 2. Provide examples of commonly found . . . against G2. . . Target-oriented reasoning steps for PrideMM: Entity Identification Given the following description of an online meme related to LGBTQ+ movements, {(i) Country and Region: does the meme explicitly reference any country or region where LGBTQ+ identities or advocacy are discouraged? (ii) Politics: does the meme explicitly involve or mention politicians, political figures, political parties, ideologies, or groups? (iii) Company: does the meme explicitly touch on topics about corporate involvement in LGBTQ+ movements? (iv) Specific Individual: analyze: Does the meme content involve any specific individual? <Definition of Specific Individual> (v) Organization: analyze: Does the meme content address or discuss organizational involvement related to LGBTQ+ issues? <Definition of Organization> (vi) LGBTQ+ Subgroup: analyze: what specific subgroup(s) within the LGBTQ+ community is/are particularly mentioned, referenced or implicitly implied in the meme content? <Subgroup Identification Guidelines> }**Description of the meme content**: [M2T]{[Fm][CoT]} Target Classification Given the following description of an online meme related to LGBTQ+ movements, analyze: What is the memes target subject? Select the most appropriate category from these options: 1. Undirected; 2. Specific Individual; 3. LGBTQ+ Community; 4. Organization. <Target Classification Guidelines> Description of the meme content:[M2T][CoT] C.4 CoT+: CoT Prompting with Guidelines The proposed guided CoT reasoning prompts are listed below, where the interpretable human-crafted task-specific guidelines are denoted by <GL>. Detailed guidelines are released in our code. C.4.1 FHM Prompt Given the following image-caption content, which may or may not be an online meme, classify the content as either hateful or non-hateful based on common values, knowledge and social norms.<GL>Image-caption content you need to classify:[M2T].The caption overlaid on the image reads:\"[OCR]\".[CoT]"
        },
        {
            "title": "Guidelines",
            "content": "Some image-caption contents perceived as hateful may be implicit, which means they may not contain explicit derogatory language, offensive speech, or direct indications of hatred toward individuals or groups. However, they may be deliberately crafted to evoke negative contextual associations, such as harmful stereotypes against 19 protected groups, painful historical events, sensitive cultural, religious, or political controversies, thereby reinforcing biases, discrimination, and potential hatefulness toward the human targets. The vulnerable protected groups within the scope of this task include: women, LGBTQ+ community, people with disabilities, Muslims and Islamic culture, individuals of Middle Eastern descent, Jewish individuals, all colored people (e.g., individuals of African descent, African Americans, East Asian or South Asian individuals, native Americans, etc.) and other similarly vulnerable communities. Stereotypes and topics involving these protected groups are especially sensitive and serious, whereas other stereotypes or mildly negative implications that do not concern these protected groups could be considered harmless. If the caption merely describes, states, or explains the visual facts of the image (e.g., providing context about what is going on in the image) in neutral tone from an observers perspective without expressing any sentiment inclination or personal opinion, avoid overinterpreting for negative associations or implications. Such captions, if being objective or illustrative statements, should be considered innocent. Take into account the level of potential hate the content may pose to the relevant audience, as well as the sensitivity and seriousness of the topic based on common social norms. Content that carries only mildly negative implications but does not target any specific protected group might be considered innocent. Using derogatory language, mocking, or advocating violence and extremism toward non-human animals is not considered hateful within the scope of this task. The discussion of hatefulness here pertains only to humans. If the content does not explicitly target any specific protected groups and is unlikely to cause significant harms or negative impacts, rhetorical metaphor, extreme or exaggeration should not be overinterpreted and might be considered innocent. Hateful examples generated from Appendix C.3. C.4.2 Political Memes Prompt Given the following description of an online meme related to {HarMeme: COVID-19 pandemic; Harm-P: U.S. politics; MultiOFF: 2016 U.S. Presidential Election}, classify the meme content as either {HarMeme and Harm-P: harmful or harmless; MultiOFF: offensive or nonoffensive} based on widely accepted values, established knowledge and social norms.<GL>Meme content you need to classify:[M2T].[CoT] Guidelines: HarMeme Meme contents perceived as harmful can be implicit. While they may not contain explicit derogatory language, slurs, offensive speech, or direct expressions of hate toward specific politicians in the image or caption, they can still intentionally provoke negative contextual interpretations or associations that contribute to negative portrayal of the target politicians, maliciously harm their reputation and public image through critical satires, mockeries or exaggerated caricature against their statements, behaviors, actions or policies, reinforce distrust, harmful stereotypes, unfair biases, or even hatred against them, thus being harmful to the politicians involved. Try to analyze the intent and implication of the meme from neutral perspective first without presuming the nature of its tone as humorous. Commonly found harmful contents in online memes targeting different politicians during Covid-19 pandemic include: Donald Trump: Taking Trumps controversial public statements or tweets out of context to mock, embarrass or make fun of him. . . . Joe Biden: Parodies that portray Joe Biden as childlike, juvenile, forgetful or disconnected. . . . (See the released code for more details.) Meme contents that make fun of coronavirus and the pandemic but do not involve any politician or celebrity might be considered harmless. Guidelines: Harm-P Commonly found harmful contents in political memes include: Spreading or reinforcing harmful misinformation associated with politicians, political parties or groups; Encouraging misleading, inflammatory, extremist or harmful viewpoints or ideologies. . . (See the released code for more details.) Commonly found harmful contents against politicians include: Leveraging sarcastic/satirical personal attacks intended to insult, humiliate, discredit, or ridicule public figures; Taking political 20 statements out of context and attributing them to politicians to mislead or provoke; Using sensitive topics such as sexual scandals as punchlines for mockery. tributing extreme or offensive beliefs to Muslims; Spreading dangerous misinformation that touch on Islamophobia, immigration policies in an inflammatory manner, etc. Commonly found harmful contents targeting political parties include: Perpetuating offensive, exaggerated, oversimplified, or misleading stereotypes about political parties or groups; Provoking partisan distrust or hostility through manipulative framing, taking out of context, oversimplification, etc. Commonly found offensive contents targeting Joe Biden include: Parodies that portray this elder as childlike, juvenile, forgetful or disconnected in fictional scenarios; Implicitly mocking his perceived ineptitude, mental or physical decline. Guidelines: MultiOFF Try to interpret the image content from neutral perspective by combining both the visual content and any overlaid text as whole without presuming the nature of tone or intent as humorous or lighthearted. DO NOT let any single aspect dominate your determination. Commonly found offensive contents in political memes include: Spreading or reinforcing misinformation associated with politicians, political parties, or racial groups; Reinforcing superficial stereotypes about political groups. . . . (See the released code for more details.) Commonly found offensive contents against politicians include: Using dismissive, aggressive or vulgar language or humors that are disrespectful and crude; Leveraging sarcastic/satirical personal attacks intended to insult, humiliate, discredit, embarrass, or ridicule public figures. . . . (See the released code for more details.) Commonly found offensive contents against political parties or groups include: Perpetuating exaggerated, demeaning, or offensive stereotypes about political parties, groups or political opponents/supporters; Encouraging partisan hostility, partisan distrust through manipulative framing, lack of context, etc. Offensive contents against Muslims or Islam include: Reinforcing harmful stereotypes about Muslims or Islam, such as associations with extremism or terrorism, or portraying them as targets of discriminatory immigration policies; AtOffensive contents against LGBTQ community include: Stereotyping LGBTQ individuals as with certain appearance traits (such as dyed hair); Promoting homophobia, transphobia speech, etc. Other offensive contents include: Perpetuating harmful racist speech or stereotypes; Using explicitly derogatory racially charged language, etc. C.4.3 MAMI Prompt Given the following image-caption content, which may or may not be an online meme, classify the content as either misogynistic or nonmisogynistic based on common values, knowledge, social norms and the provided guidelines.<GL>Image-caption content you need to classify:[M2T]. The caption overlaid on the image reads:\"[OCR]\".[CoT]"
        },
        {
            "title": "Guidelines",
            "content": "Try to interpret the content by combining both the image and caption as whole. DO NOT let any single aspect dominate your classification. Try to interpret the implications of the imagecaption contents from neutral perspective without presuming the nature of tone or intent as humorous, playful or lighthearted. Some image-caption contents that might be perceived as potentially misogynistic can be implicit, which means their images or captions may not contain explicit derogatory language, offensive speech, indication of discrimination, dislike or hatred against women. However, they may intentionally trigger audiences contextual interpretations with negative associations such as harmful stereotypes, body shaming, objectification or sexualization of women and even violence against women, thus reinforcing harmful biases, inequality, and potential hatefulness against women. <Caution e.g., Harmful Stereotypes: NOT ALL stereotypes are deemed \"harmful\". Within the scope of this task, beyond the following provided examples of \"harmful\" stereotypes against women, other contents that might be interpreted as portraying women in slightly negative light should not be automatically regarded as \"harmful\" stereotypes, and therefore should be considered as harmless>, therefore, if the given 21 image-caption content aligns with or intentionally implies any of the following commonly found misogynistic contents, the content should be classified as misogynistic. Such misogynistic contents include: <Examples>. (See our released code for more details.) Harmful Stereotypes Associating or comparing women with household appliances such as dishwasheres or washing machines; Intentionally highlighting women in traditional domestic roles . . . Body Shaming Making offensive jokes about or critising satirically on womens appearance, especially women who appear to have large body size (often considered as overweight or fat) . . . Objectification of Women Comparing women to household appliances like dishwasheres or washing machines; Treating women as mere objects, instruments or commodities (such as food or household appliances) for mens use or sexual enjoyment . . . Sexualization of Women Highlighting certain body parts of women for sexual appeal like womens breasts, chests, hips, buttocks, genitals, etc.; Portraying or treating women as objects to satisfy sexual desire . . . Advocating Violence against Women Implying the use of violence (to punish or control womens behaviors); Making light of domestic abuse/violence towards women . . . Mocking Feminists Contents that explicitly involve feminists in the image or caption to mock, satirize, question or reinforce negative stereotypes against feminists. . . Unless the content stereotypes women as prone to cheating, avoid overinterpreting content that features dynamics or interactions in \"husbandwife\" or \"boyfriend-girlfriend\" relationships to assume negative stereotypes against women. Women vs. men (or boys vs. girls) comparisons are not considered as \"harmful\" stereotypes against women and should be considered nonmisogynistic when such comparison remarks focus on non-sexual daily topics or aspects (e.g., hobbies, interests, attitudes, lifestyles, etc.). If the content does not contain explicit adult content, content that involves women but has captions that are inherently neutral with respect to gender should be regarded as non-misogynistic. C.4.4 PrideMM Prompt Given the following description of an online meme related to LGBTQ+ pride movements, A. If targeting LGBTQ+ community or supporters: classify the content as either harmful or harmless to LGBTQ+ community and supporters, B. If targeting LGBTQ+ individuals: classify the content as either hurtful or non-hurtful to the specific LGBTQ+ individual involved, C. If targeting specific individual: classify the content as either harmful or harmless to the specific individual involved, D. If targeting organizations: classify the content as either harmful or harmless to the public image of the organization(s) involved, according to widely accepted social norms, values, cultural understanding, and the provided guidelines.<GL>Meme content you need to classify:[M2T].[CoT]"
        },
        {
            "title": "Guidelines",
            "content": "Interpret the meme content by combining both the visual elements and the overlaid caption as whole. DO NOT let any single aspect dominate your classification. Maintain neutral perspective when interpreting the contents implications. Try to analyze the memes accurate stance toward LGBTQ+ community, supporters and movements. Is the memes stance \"neutral\", \"support\" or \"oppose\"? Meme contents that contain explicit derogatory language, offensive speech, direct personal attacks, dehumanizing imagery, demeaning, discriminatory or abusive remarks, slurs, or indication of hatred towards individuals or groups of LGBTQ+ community and supporters in the image or caption are explicitly harmful. Some meme contents perceived as harmful may be implicit, which means they may not contain explicit derogatory language, slurs, offensive speech, or direct indications of hatred toward LGBTQ+ community or movements. However, such content may be deliberately crafted in implicit xenophobic undertone to evoke negative contextual associations-such as harmful stereotypes against LGBTQ+, connotations of mockery, dismisiveness or hostility-that reinforce bias, discrimination, stigmatization and even hatefulness toward the LGBTQ+ community, undermining the efforts of inclusion movements. Commonly found harmful contents towards LGBTQ+ community and supporters include: transphobia Speech reinforcing homophobia, e.g., criticizing LGBTQ+ as violation of religious beliefs; Mocking, satirizing, criticizing or questioning LGBTQ+ movements. . . (See the released code for more details.) Commonly found hurtful or harmful contents towards LGBTQ+ individuals include: Speech reinforcing homophobia, transphobia e.g., criticizing LGBTQ+ individuals as violation of religious beliefs; Mocking, satirizing, criticizing or questioning LGBTQ+ individuals. . . Commonly found harmful contents towards the public image of organizations in LGBTQ+ context include: Mocking, satirizing or criticizing corporate involvement for LGBTQ+ support (e.g., inclusive actions or participation) as excessive, performative, superficial or insincere. . . If the content is neither mocking, dismissive nor containing extremist or violence, but instead empathetic and relatable, speaking from the perspective of LGBTQ+ individuals-aimed at fostering understanding and acceptance by validating and affirming common queer experiences such as self-doubt, introspective struggles, internal conflicts, gender identity exploration, self-awareness or self-discovery, etc., it should be classified as harmless. If the memes caption merely describes, states, or explains the facts about the images visual content (e.g., providing context about what is going on in the image) in neutral tone (neither satirical nor critical) from an observers perspective without any rhetorics, sentiment inclination or personal viewpoints, avoid inferring for negative associations or implications. Such captions, if being objective or illustrative statements, should be considered as innocent."
        },
        {
            "title": "D Case Study",
            "content": "We present examples of correctly classified memes, along with the corresponding LLM-generated reasoning outputs from Figure 4 to Figure 9. As can be seen, for these correct predictions, the rationales provided by Qwen2.5-14B and Mistral-12B are comprehensive and accurate, effectively capturing the directly related sociocultural contexts, commonsense knowledge, and the specific reasons why the memes are regarded harmful. This further verifies that the inherent understanding of small-scale LLMs regarding social norms, moral standards, 23 Figure 3: Distribution of error types within the thirty randomly sampled error cases from each of FHM, PrideMM, and Harm-P. and the perception of harmful content is generally aligned with human preferences. Figure 3 shows the distribution of different error types across the three datasets of FHM, Harm-P and PrideMM. Notably, we find varying degrees of annotation errors across these three datasets, highlighting the potential bias in human annotation. Examples of such potential annotation errors are shown in Figure 13. Examples of detailed reasoning outputs of error cases are demonstrated in Figure 10, Figure 11 and Figure 12. Figure 4: Example LLM reasoning outputs for correctly detected harmful memes in FHM. 24 Figure 5: Example LLM reasoning outputs for correctly detected harmful memes in Harm-C. 25 Figure 6: Example LLM reasoning outputs for correctly detected harmful memes in Harm-P. 26 Figure 7: Example LLM reasoning outputs for correctly detected harmful memes in MultiOFF. 27 Figure 8: Example LLM reasoning outputs for correctly detected harmful memes in PrideMM. 28 Figure 9: Example LLM reasoning outputs for correctly detected harmful memes in MAMI. 29 Figure 10: Error Analysis on FHM. 30 Figure 11: Error Analysis on PrideMM. 31 Figure 12: Error Analysis on Harm-P. 32 Figure 13: Examples of potential annotation errors in FHM, PrideMM and Harm-P. 33 Figure 14: Our proposed High-fidelity Meme2Text pipeline."
        }
    ],
    "affiliations": [
        "Nanyang Technological University"
    ]
}