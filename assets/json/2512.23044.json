{
    "paper_title": "Video-BrowseComp: Benchmarking Agentic Video Research on Open Web",
    "authors": [
        "Zhengyang Liang",
        "Yan Shu",
        "Xiangrui Liu",
        "Minghao Qin",
        "Kaixin Liang",
        "Paolo Rota",
        "Nicu Sebe",
        "Zheng Liu",
        "Lizi Liao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web's most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present \\textbf{Video-BrowseComp}, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 4 0 3 2 . 2 1 5 2 : r Video-BrowseComp: Benchmarking Agentic Video Research on Open Web Zhengyang Liang1,, Yan Shu2,, Xiangrui Liu3, Minghao Qin3, Kaixin Liang4, Paolo Rota2, Nicu Sebe2, Zheng Liu5, Lizi Liao1 1Singapore Management University 2University of Trento 3Beijing Academy of Artificial Intelligence 4Beijing University of Posts and Telecommunications 5Hong Kong Polytechnic University Equal contribution (zyliang@smu.edu.sg; yan.shu@unitn.it; lzliao@smu.edu.sg) The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, significant modality gap remains in processing the webs most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present Video-BrowseComp, challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadatarich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning. Date: December 30, 2025 Website for Benchmark: https://liang-zhengyang.github.io/video-browsecomp/"
        },
        {
            "title": "Introduction",
            "content": "The rapid evolution of Large Language Models (LLMs) has catalyzed paradigm shift from static questionanswering to autonomous agents capable of actively navigating the web to solve complex problems (Nakano et al., 2021b; He et al., 2024). This transition towards agentic web browsing has become dominant trend in AI research. Pioneering benchmarks such as GAIA (Mialon et al., 2023), BrowseComp (Wei et al., 2025), and MM-BrowseComp (Li et al., 2025a) have established standards for agents operating within textual and static multimodal environments. These works reflect an emerging consensus: real-world agents must proactively seek, retrieve, and reason over information rather than passively receiving it. Despite these advancements, the current landscape of deep research agents faces fundamental limitation: modality blind spot. While the community has focused heavily on static web information like text and images (Jin et al., 2025; Geng et al., 2025), it has largely overlooked the most information-dense and dynamic modality on the web: video. In real-world information seeking, video is not merely entertainment, but critical repository of knowledge, ranging from product reviews that demonstrate dynamic usage to tutorials explaining complex procedures. However, relying on existing video evaluation protocols is insufficient to address this need. Current video benchmarks, such as VideoMME (Fu et al., 2024), MLVU (Zhou et al., 2025a), and MVBench (Li et al., 2024), predominantly focus on passive perception. They typically input curated clip to model and query its 1 content without requiring external information retrieval. This setting fails to reflect the agentic nature of real-world research, where video serves as dynamic source of evidence that must be actively interrogated. In real-world scenarios, users do not simply watch video; they seek verification by jumping between timestamps, comparing conflicting narratives across multiple sources, and triangulating visual evidence with external web text. Existing benchmarks fail to measure such active retrieval, multi-step planning, and cross-source reasoning capabilities. To bridge this gap, we introduce Video-BrowseComp, challenging benchmark designed to evaluate whether agents can perform holistic deep research that integrates dynamic video content. Unlike prior benchmarks, Video-BrowseComp enforces mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived via text search alone but require navigating video timelines and interpreting visual dynamics. To systematically assess the limits of agentic capabilities, we stratify the benchmark into three distinct difficulty levels (Level 1 to Level 3) based on the required search breadth and reasoning depth. Specifically, Level 1 questions define clear search domains, requiring models to locate and understand videos within specified sources. Level 2 questions demand active video search across websites, comparing multiple candidates to identify and comprehend the target video. Level 3 questions build upon Level 2 by further requiring cross-video reasoning to aggregate and verify information across multiple sources. Moreover, to reflect real-world scenarios, the benchmark spans 8 diverse video genres ranging from film to documentary. To ensure rigorous evaluation in this open-ended setting, we employ objective short-form answers and structured verification checklists, adhering to the hard-to-solve, easy-to-verify principle. We conducted extensive evaluations on state-of-the-art models, from Tool-Free to Search-Augmented models. Our experiments reveal stark reality: while current models excel at passive recognition, they exhibit significant deficiencies in research-level reasoning, often failing to maintain coherence across long-context video sources or to accurately ground external claims in video. Through Video-BrowseComp, we aim to expand the scope of autonomous agent evaluation, pushing the community to recognize that true deep research requires not just passive comprehension, but actively exploring and verifying the dynamic world of video."
        },
        {
            "title": "2.1 Video Understanding",
            "content": "The field of video understanding has witnessed rapid advancements with the emergence of Multimodal Large Language Models (MLLMs) (Comanici et al., 2025; OpenAI et al., 2024; Liu et al., 2023; Zhu et al., 2025). Video LLMs (Shu et al., 2025; Qin et al., 2025; Li et al., 2025d; Liu et al., 2025; Chen et al., 2024; Shen et al., 2024) have demonstrated impressive capabilities in processing long-context visual inputs. Concurrently, benchmarks (Fu et al., 2024; Zhou et al., 2025a; Li et al., 2024; Wang et al., 2025) have been established to rigorously evaluate these models on traditional video QA tasks. However, these existing works predominantly operate under paradigm of passive perception. In typical evaluation settings, model is fed curated video clip and queried about its internal content in closed-world manner. This approach remains at the perception level, falling short of research-oriented investigation. In real-world scenarios, answering complex queries often requires more than just watching single video (Peng et al., 2025; He et al., 2025; Jang et al., 2025; Fu et al., 2025). It necessitates combining internal video evidence with external knowledge or actively searching across multiple videos to triangulate facts. Current benchmarks inability to measure external verification and cross-source reasoning underscores the need for agents with active search capabilities."
        },
        {
            "title": "2.2 Web Browsing Agents",
            "content": "To address the limitations of passive models, the community has shifted focus towards autonomous browsing agents. This evolution began with text-based agents, where pioneers (Nakano et al., 2021a; Jin et al., 2025; Li et al., 2025c; Wu et al., 2025a; Li et al., 2025b) demonstrated that LLMs could effectively utilize search engine tools to solve complex text-based questions (Wei et al., 2025; Zhou et al., 2025b; Mialon et al., 2023; Phan et al., 2025; Chen et al., 2025). Recently, this paradigm has expanded into the multimodal domain. Models and benchmarks (Li et al., 2025a; Wu et al., 2025b; Jiang et al., 2024) have pushed the boundary 2 by requiring agents to process static web information, including text and images, marking significant step towards multimodal information seeking. Despite these advancements, critical gap remains: current agents lack native capabilities for dynamic video interaction. While models can now process text and static images on the web, they struggle to actively navigate, search, or verify information within dynamic video streams. Existing browsing agents often treat video merely as static metadata, losing the rich temporal nuances essential for deep research. Video-BrowseComp bridges this gap as the first benchmark specifically designed to evaluate agents ability to actively browse, perceive, and reason over dynamic video content."
        },
        {
            "title": "3.1 Task Definition",
            "content": "We formulate the video browsing task as an open-ended information seeking problem constrained by multimodal evidence verification. Given natural language query Q, an agent is provided access to the open web W. The agent must autonomously plan sequence of actions = {a1, a2, ..., at}, such as searching the web, retrieving specific video timestamps, or reading subtitles, to generate final result."
        },
        {
            "title": "3.2 Principles and Scope",
            "content": "To ensure Video-BrowseComp serves as rigorous benchmark for agentic video research, we adhere to three foundational design principles during benchmark construction: Mandatory Video Dependency. core objective of this benchmark is to evaluate video perception and reasoning, not merely text-based web search. To prevent models from solving questions via textual shortcuts (e.g., retrieving movie plot from Wikipedia without watching the video). Ensuring that the benchmark evaluates true video research rather than the ability to retrieve textual shortcuts. Hard-to-Find, Easy-to-Verify. The questions are intentionally complex, often requiring multi-step navigation, cross-referencing, or long-context scanning. However, the output format is constrained to short, objective strings, such as specific entity names, counts, or colors, allowing for automated and unambiguous verification against the ground truth. Answer Uniqueness. To guarantee evaluation reliability, we ensure that every question has single answer. Questions are grounded in objective spatio-temporal attributes rather than subjective interpretations (e.g., What is the color of the car behind the protagonist at 05:20? rather than Is the car beautiful?)."
        },
        {
            "title": "3.3 Difficulty Stratification",
            "content": "To systematically assess the limits of agentic capabilities, we stratify Video-BrowseComp into three distinct difficulty levels based on the complexity of the search space and the depth of reasoning required. Level 1: Explicit Retrieval & Perception. Questions in this level provide explicit constraints regarding the search domain, such as specific publication dates, platform names, or title keywords (e.g., In video published on the official channel in Feb 2016...) The agent is not required to perform open-ended exploration; instead, the challenge lies in Instruction Following to locate the target video and Temporal Grounding to extract visual details (e.g., identifying jersey number at specific event). This level serves as baseline to evaluate fundamental video retrieval and perception skills. Level 2: Implicit Retrieval & Entity Grounding. This level removes explicit search keywords, forcing agents to operate in an open-world setting. The target video is referenced only through indirect descriptions (e.g., team logo on scarf or specific building in photo). Agents must first demonstrate Visual Entity Recognition to bridge the gap between visual cues and textual search queries, and then filter through multiple candidate videos to find the one matching the event description. Level 3: Cross-Source Reasoning & Multi-Hop Verification. Representing the highest tier of difficulty, these questions cannot be answered by viewing single video. They require Multi-Hop Reasoning where information 3 Figure 1 Overview of the Video-BrowseComp benchmark dataset showing sample questions, ground truth answers, and verification checklists for the three difficulty levels (Level 1: Explicit Retrieval, Level 2: Implicit Retrieval, and Level 3: Cross-Source Reasoning). retrieved from one source serves as prerequisite for locating the next. For example, an agent may need to identify an actor from scene in Movie A, leverage that identity to find specific scene in Movie B, and finally extract visual attributes (e.g., clothing color) from the second video. This level rigorously tests the agents ability to maintain long-horizon memory and aggregate fragmentary evidence across disparate video sources."
        },
        {
            "title": "3.4 Dataset Construction",
            "content": "To curate Video-BrowseComp, we recruited 8 expert annotators (holding Masters or PhD degrees) with extensive experience in LLM usage, web search, and video understanding. The construction process follows rigorous annotation pipeline with two-stage quality control. Annotation Pipeline. We adopt reverse construction strategy that begins with video exploration and culminates in complex query formulation. First, each annotator is assigned 2 video categories and encouraged to explore relevant topics through web browsing. After identifying candidate videos, annotators watch the complete video and design an initial question-answer pair focused on specific clip. These simple questions are then expanded into complex queries following two principles: Indirect & Compositional Queries: Questions are augmented with compositional conditions to ensure the target video cannot be easily retrieved through simple search. Annotators use indirect descriptions to reference video content rather than explicit keywords. Multi-Source & Multi-Hop Reasoning: Level 1 and Level 2 samples can be derived from the above steps with single-video evidence. For Level 3 samples, annotators first identify set of related videos covering the same topic or event, then design questions that require aggregating complementary information across these videos, such as comparing statistics from different matches, verifying conflicting claims across multiple sources, or tracing an events progression through several recordings. Finally, annotators document the complete evidence chain or authoritative source video URLs, ensuring full traceability of the ground-truth answers. Quality Control. We implement two-stage quality control protocol to ensure the quality of Video-BrowseComp. Easy Question Filtering. Although annotators are required to design questions based on the visual content of videos, some samples may still be answerable through commonsense knowledge without watching the video. To identify and remove such cases, we use the Google text search api1 to return 5 relevant results, input it to GPT-5 to filter out questions that can be answered correctly, ensuring that the remaining questions faithfully require video understanding. Answer Uniqueness Validation. This step ensures that each question has only one correct and unambiguous answer. We employ cross-validation strategy among the same 8 annotators: each annotator answers questions annotated by others following the evidence chain, and we compare their responses against the original ground-truth answers. If an alternative answer satisfies all task constraints but differs from the original, the sample is considered ambiguous and discarded. Through this process, we filter 91 low-quality samples, resulting in final benchmark of 210 validated questions that maintain high difficulty and answer verifiability."
        },
        {
            "title": "3.5 Dataset Statistics",
            "content": "The final benchmark consists of 210 validated questions spanning 8 diverse genres. As illustrated in Figure 2, the distribution prioritizes complex visual narratives, with Film (29%) and Sports (21%) forming the core, ensuring agents are evaluated on dynamic events rather than just static metadata. The difficulty is stratified into pyramidal structure: while Level 1 (125 samples) tests explicit retrieval, Level 2 (62) and Level 3 (23) specifically target advanced capabilities in implicit retrieval and cross-source reasoning."
        },
        {
            "title": "4.1 Metrics",
            "content": "Accuracy. We employ Accuracy (Acc) as the primary metric to evaluate model performance. Accuracy measures the percentage of test instances where the models generated answer is semantically equivalent to the ground truth reference answer. Since the reference answers in Video-BrowseComp are designed to be short and verifiable (e.g., specific entities, colors, or counts), we utilize gpt-5-mini-2025-08-07 as an automated judge to assess correctness. The judge is prompted2 to compare the models prediction with the ground truth and determine if they represent the same factual information, thereby handling minor variations in formatting or phrasing that rigid string matching might misclassify. 1https://serper.dev 2The judge prompt is provided in Appendix A. 5 Figure 2 Distribution statistics of the benchmark, illustrating the breakdown of the 210 validated questions across eight video categories and the three difficulty levels. Calibration Error. Beyond mere accuracy, it is critical for an autonomous research agent to accurately assess its own uncertainty, particularly when verifying evidence across complex video streams. To quantify this reliability, we prompt3 the model to provide self-assigned confidence score [0, 1] alongside its final answer. We then calculate the Expected Calibration Error (ECE) to measure the alignment between the models predicted confidence and its actual accuracy. Following standard practice, we partition the predicted confidence scores into = 5 bins (i.e., [0, 0.2), . . . , [0.8, 1.0]). The ECE is calculated as the weighted average of the absolute difference between the accuracy and the average confidence within each bin: ECE = (cid:88) i=1 ni acc(i) conf(i) (1) Where is the total number of samples, ni is the number of samples in the i-th bin, acc(i) is the empirical accuracy of the samples in the i-th bin, and conf(i) is the average predicted confidence of the samples in that bin. lower ECE indicates more reliable agent that is less prone to overconfidence when hallucinating or retrieving incorrect video evidence."
        },
        {
            "title": "4.2 Baselines",
            "content": "To systematically evaluate the agentic capabilities for video-based research, we categorize our baselines into three distinct groups: Tool-Free Models, Search-Augmented Models. Tool-Free Models. This category evaluates the capability of state-of-the-art Multimodal Large Language Models (MLLMs), including Qwen3-VL-8B-Thinking, Qwen3-VL-235B-A22B-Instruct(Yang et al., 2025), GLM-4.6V(Team et al., 2025), gpt-4o-2024-11-20, gpt-4o-mini-2025-0807(OpenAI et al., 2024), gemini-2.5flash-2025-06, gemini-2.5-pro-2025-06(Comanici et al., 2025), to answer questions relying solely on their internal parametric knowledge and the provided context. By restricting access to external tools or the internet, this baseline serves as lower bound to determine if questions can be solved via hallucination or rote memorization of popular video content. 3The evaluation prompt is provided in Appendix A. 6 Table 1 Main results on Video-BrowseComp. We report Overall Accuracy (OA), accuracy across three difficulty levels (L1, L2, L3), and Calibration Error (CE). Bold indicates the best performance."
        },
        {
            "title": "Model",
            "content": "Tool-Free Models Qwen3-VL-8B-Thinking Qwen3-VL-235B-A22B-Instruct GLM-4.6V gpt-4o-2024-11-20 gpt-4o-mini-2024-07-18 gpt-5-mini-2025-08-07 gemini-2.5-flash-2025-06 gemini-2.5-pro-2025-06 Search Models gemini-2.5-flash-2025-06 (w/ Search) gemini-2.5-pro-2025-06 (w/ Search) gpt-5.1-2025-11-13 (w/ Search) o4-mini-deep-research-2025-06-26 Overall Difficulty Accuracy (%) Calibration Acc (%) Level 1 Level 2 Level 3 Error (%) 7.14 13.33 10.95 17.62 9.52 15.71 16.67 19.52 20.95 23.81 15.24 22. 12.00 22.40 16.80 28.00 16.00 26.40 27.20 31.20 32.80 37.60 21.60 30.40 0.00 0.00 3.23 3.23 0.00 0.00 1.61 3.23 4.84 4.84 6.45 12.90 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 4.35 8. 52.49 77.64 44.40 58.81 63.55 37.47 77.79 79.18 35.98 31.45 30.20 42.55 Search-Augmented Models. This category evaluates the official, Search-Augmented model services provided by major vendors, including gemini-2.5-flash-2025-06 (w/ Search), gemini-2.5-pro-2025-06 (w/ Search), gpt-5.12025-11-13 (w/ Search)(OpenAI, 2025b), o4-mini-deep-research-2025-06-26(OpenAI, 2025a). Unlike passive models, these systems possess integrated web search capabilities that enable them to query the internet for up-to-date information autonomously during inference. Note on Implementation: As of late 2025, most commercial browsing tools process web content primarily via textual indices (HTML, metadata, captions) and do not natively stream video data from URLs. Therefore, this baseline represents the current State-of-thePractice: attempting to solve video tasks via Textual Proxies (metadata, reviews, logs). This setup allows us to quantify the Modality Gap, the performance penalty incurred when agents cannot actively perceive the visual dynamics of the web."
        },
        {
            "title": "4.3 Performance",
            "content": "4.3.1 Accuracy As shown in Table 1, Tool-Free models relying solely on internal parametric knowledge struggle to address the proposed tasks, with baselines like Qwen3-VL-8B-Thinking and GLM-4.6V achieving only 7.14% and 10.95% accuracy, respectively. Even the high-performing Gemini-2.5-pro plateaus at 19.52% without external tools. breakdown by difficulty reveals that these models achieve marginal success in Level 1 tasks merely by leveraging parametric memory to identify popular videos and recall associated details. However, performance collapses in Level 2 and Level 3 tasks. The multi-source nature of these higher-difficulty levels precludes solutions derived from single-video memorization, confirming that internal knowledge alone is insufficient for complex questions. Integrating web search capabilities yields tangible performance gain, as evidenced by Gemini-2.5-pro (w/ Search) in Figure 3, which improves overall accuracy to 23.81%. However, while search augments the ability to locate text-based content with significant accuracy jump in Level 1 (37.60%), it cannot yet search video evidence or parse video. In Level 2 and Level 3, where the challenge shifts from locating single video to synthesizing evidence across multiple sources, the performance of search models collapses. For instance, Gemini-2.5-pro (w/ Search) drops to 4.84% in Level 2 and fails completely (0.00%) in Level 3. Unlike the simple Search-Augmented method, the deep research agent, like o4-mini-deep-research. It achieves emergent capabilities in Level 2 (12.90%) and Level 3 (8.70%). The improvement implies that the bottleneck in agentic video research is not merely information search, but the capacity for long-horizon planning, highlighting 7 the necessity of specialized agentic workflows over simple tool augmentation. Figure 3 Performance comparison of Gemini-2.5 models (Flash and Pro) in Tool-Free versus Search-Augmented settings. Figure 4 Comparison of Expected Calibration Error between Tool-Free and Search-Augmented models. 4.3.2 Calibration Error In Table 1, Tool-Free models exhibit severe overconfidence, with Calibration Errors (CE) soaring as high as 79.18% for Gemini-2.5-pro and 77.64% for Qwen3-VL-235B-A22B-Instruct. This high error rate stems from parametric hallucination, where models confidently generate plausible but incorrect details based on outdated or non-existent internal memories. In contrast, Search-Augmented models significantly mitigate this issue. As shown in Figure 4, the CE for Gemini-2.5-pro drops from 79.18% to 31.45%, and that of Gemini-2.5-flash drops from 77.79% to 35.98%, indicating that access to external search tools serves as critical grounding mechanism. Current search tools act more as Hallucination Dampeners than Problem Solvers. They dont massively boost reasoning accuracy (the answer is often still wrong), but they significantly improve the models ability to know when its wrong (grounding). 4.3.3 Modality Gap To strictly isolate the impact of blind web searching versus visual reasoning, we conducted an experiment on subset of 20 samples where the baseline gemini-2.5-pro (w/ Search) achieved only 5.0% accuracy. In this comparison setting, we input the ground-truth video to model, bypassing the search step entirely. The results in Figure 5 were striking: accuracy surged from 5.0% to 45.0%. This 40% performance jump confirms that the primary bottleneck for current search models is the Modality Gap, the inability to retrieve and perceive dynamic video information from the web. However, the fact that accuracy plateaued at 45.0% even with perfect visual input validates the intrinsic difficulty of Video-BrowseComp. The remaining 55% error rate stems from the complex, multi-hop reasoning required by the benchmark, which challenges even state-ofthe-art multimodal models. 4.3.4 Categories Analysis Figure 5 Input video boosts accuracy by 40%. To investigate the source of agentic failures, we analyze performance across the eight video categories defined in Section 3.3. As illustrated in Table 2 and heatmap in Figure 10, model performance is not uniform but Table 2 Performance breakdown across 8 video genres. We report the accuracy (%) for each category. Abbreviations: Doc: Documentary, Edu: Education & Knowledge, Game: Games & Esports, Music: Music & Variety, TV: TV Series, Video: Video & Shorts. Bold indicates the best performance."
        },
        {
            "title": "Film Game Music Sport TV Video",
            "content": "Tool-Free Models Qwen3-VL-8B-Thinking Qwen3-VL-235B-Instruct GLM-4.6V gpt-4o-2024-11-20 gpt-4o-mini-2024-07-18 gpt-5-mini-2025-08-07 gemini-2.5-flash-2025-06 gemini-2.5-pro-2025-06 Search Models gemini-2.5-flash (w/ Search) gemini-2.5-pro (w/ Search) gpt-5.1 (w/ Search) o4-mini-deep-research 0.00 10.00 12.00 0.00 12.00 0.00 16.00 10.00 12.00 0.00 0.00 28.00 20.00 20.00 28.00 10.00 13.11 19.67 11.48 26.23 18.03 22.95 24.59 29.51 0.00 13.64 13.64 13.64 9.09 13.64 4.55 13.64 20.00 0.00 10.00 0.00 0.00 10.00 0.00 0. 0.00 2.22 2.22 4.44 0.00 0.00 2.22 2.22 5.26 16.67 21.05 27.78 26.32 16.67 31.58 27.78 5.56 15.79 22.22 21.05 22.22 36.84 22.22 36.84 4.55 0.00 36.00 29.51 9.09 20.00 32.00 31.15 18.18 21.31 8.00 0.00 29.51 27.27 28.00 10.00 10.00 10.00 10.00 10.00 36.84 16.67 11.11 57.89 16.67 8.89 22.22 21.05 8.89 16.67 17.78 21.05 exhibits strong correlation with the availability of external textual metadata. We observe that Search-Augmented models achieve disproportionately high accuracy in metadata-rich domains. Specifically, Gemini-2.5-Pro (w/ Search) attains peak accuracy of 57.89% on TV Series. We hypothesize this is due to the abundance of textual plot summaries, episode guides, and fan wikis available on the open web. In these instances, the agent likely bypasses visual processing entirely, retrieving the answer via text search rather than temporal grounding. In contrast, performance collapses in domains characterized by dynamic, stochastic content that lacks detailed textual indexing. In the Games and Sports categories, where answers typically depend on specific gameplay moments or match events not described in text. Gemini-2.5-Pro (w/ Search)s accuracy drops to 9.09% and 8.89%, respectively. This gap highlights critical limitation in current Deep Research agents: their success is often predicated on finding textual proxy for the video content. When forced to rely on Temporal Grounding in metadata-sparse domains (Level 2 and Level 3 tasks), even state-of-the-art agents fail to bridge the modality gap."
        },
        {
            "title": "5 Qualitative Case Studies",
            "content": "Our evaluation reveals critical reliance on textual metadata. In dynamic scenarios like sports, where specific gameplay moments (e.g., specific foul sequence or buzzer-beater in overtime) are not indexed in search engine results, models fail to answer even when the visual evidence is clear. Consider the example of identifying two NBA teams based on double-overtime sequence in Figure 6. The visual evidence explicitly shows the Houston Rockets and Oklahoma City Thunder. However, because this specific game narrative does not appear in textual game logs, both GPT-4o and Gemini-2.5-Pro refuse to answer, stating that the information is impossible to determine. Similarly, in Figure 7 involving technical fouls and baseline altercations , models like GPT-5.1 (w/ Search) default to Unknown. This demonstrates that current Search-Augmented models often function merely as text search, incapable of using temporal grounding to fill information gaps when the open web falls silent. In Figure 8, the agent is asked to read the temperature on digital display inside space module. The visual ground truth is 85. However, o4-mini-deep-search retrieves an external news report stating the ovens design specification is 190C and confidently outputs that incorrect value. The model successfully retrieved relevant text but failed to verify it against the visual reality. 9 Figure 6 Level 2 case, NBA teams. Figure 7 Level 2 case, NBA player. 10 Figure 8 Level 2 case, space module. Figure 9 Level 3 case, cross-film. 11 In Figure 9, complex query requiring the identification of comic villain (Loki), related film (Iron Man), and specific visual detail (a food stain), models struggle to maintain coherence. While GPT-5.1 correctly identifies the Cheeseburger by linking the text clues to the correct movie scenes, Gemini-2.5-Pro fails the reasoning chain. It incorrectly identifies the stain as an Acai popsicle by hallucinating scene from Avengers: Infinity War rather than the target scene from Iron Man . This highlights the difficulty agents face in maintaining context across disparate video sources."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced Video-BrowseComp, the first benchmark designed to evaluate autonomous agents on open-ended video research tasks. Unlike prior benchmarks focusing on passive perception of single video , Video-BrowseComp enforces mandatory dependency on dynamic visual evidence, requiring agents to actively navigate, retrieve, and cross-reference information from the open web. Our extensive evaluation of state-of-the-art models reveals critical dichotomy in current capabilities. While Search-Augmented models excel at explicit retrieval tasks (Level 1) and significantly reduce calibration error by grounding responses in textual web data, they still struggle with complex, multi-hop reasoning challenges. The collapse of performance in Level 2&3 tasks highlights the necessity of cognitive architectures capable of long-horizon planning and cross-source synthesis. We observe that emerging Deep Research agents offer preliminary solution to this bottleneck, yet significant gaps remain in efficiency and video grounding. We hope Video-BrowseComp serves as catalyst for the community to move beyond passive viewers toward agents capable of genuine, verifiable video research."
        },
        {
            "title": "References",
            "content": "Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos, 2024. https://arxiv.org/abs/2408.10188. Zijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green, Kshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong, Xinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, and Jimmy Lin. Browsecomp-plus: more fair and transparent evaluation benchmark of deep-research agent, 2025. https://arxiv.org/abs/2508.06600. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, and etc. Ori Ram. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. https://arxiv.org/abs/2507.06261. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Mingyang Fu, Yuyang Peng, Dongping Chen, Zetong Zhou, Benlin Liu, Yao Wan, Zhou Zhao, Philip S. Yu, and Ranjay Krishna. Seeking and updating with live visual knowledge, 2025. https://arxiv.org/abs/2504.05288. Xinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong Wu, Yida Zhao, Kuan Li, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webwatcher: Breaking new frontier of vision-language deep research agent, 2025. https://arxiv.org/abs/2508.05748. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. Zhihao He, Tianyao He, Yun Xu, Tieyuan Chen, Huabin Liu, Chaofan Gan, Zuxuan Wu, and Weiyao Lin. Enhancing video large language models with structured multi-video collaborative reasoning, 2025. https://arxiv.org/abs/ 2509.13161. Lawrence Keunho Jang, Yinheng Li, Dan Zhao, Charles Ding, Justin Lin, Paul Pu Liang, Rogerio Bonatti, and Kazuhito Koishida. Videowebarena: Evaluating long context multimodal agents with video understanding web tasks. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview.net/forum? id=unDQOUah0F. 12 Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, and Hongsheng Li. Mmsearch: Benchmarking the potential of large models as multi-modal search engines, 2024. https://arxiv.org/abs/2409.12959. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning, 2025. https://arxiv. org/abs/2503.09516. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2219522206, June 2024. Shilong Li, Xingyuan Bu, Wenjie Wang, Jiaheng Liu, Jun Dong, Haoyang He, Hao Lu, Haozhe Zhang, Chenchen Jing, Zhen Li, Chuanhao Li, Jiayi Tian, Chenchen Zhang, Tianhao Peng, Yancheng He, Jihao Gu, Yuanxing Zhang, Jian Yang, Ge Zhang, Wenhao Huang, Wangchunshu Zhou, Zhaoxiang Zhang, Ruizhe Ding, and Shilei Wen. Mm-browsecomp: comprehensive benchmark for multimodal browsing agents, 2025a. https://arxiv.org/abs/ 2508.13186. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models, 2025b. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yongkang Wu, Ji-Rong Wen, Yutao Zhu, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025c. Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, and Limin Wang. Videochat-flash: Hierarchical compression for long-context video modeling, 2025d. https://arxiv.org/abs/2501.00574. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. https://arxiv.org/ abs/2304.08485. Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Haotian Tang, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Jinyi Hu, Sifei Liu, Ranjay Krishna, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, and Yao Lu. Nvila: Efficient frontier visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 41224134, June 2025. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants, 2023. https://arxiv.org/abs/2311.12983. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. CoRR, abs/2112.09332, 2021a. https://arxiv.org/abs/2112.09332. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021b. OpenAI. Deep Research System Card, February 2025a. https://cdn.openai.com/deep-research-system-card.pdf. OpenAI. GPT-5.1 Instant and GPT-5.1 Thinking System Card Addendum, November 2025b. https://openai.com/ index/gpt-5-system-card-addendum-gpt-5-1/. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, and etc. Alex Beutel. Gpt-4o system card, 2024. https://arxiv.org/abs/2410.21276. Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Gavin Chang, Jian Yang, Shihao Li, Yanghai Wang, Xintao Wang, Houyi Li, Wei Ji, Pengfei Wan, Steven Huang, Zhaoxiang Zhang, and Jiaheng Liu. Mvu-eval: Towards multi-video understanding evaluation for multimodal llms, 2025. https://arxiv.org/abs/2511.07250. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, and etc Richard Ren. Humanitys last exam, 2025. 13 Minghao Qin, Xiangrui Liu, Zhengyang Liang, Yan Shu, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, and Zheng Liu. Video-xl-2: Towards very long-video understanding through task-aware kv sparsification, 2025. https://arxiv.org/abs/2506.19225. Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2616026169, June 2025. Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. https://arxiv.org/abs/2507.01006. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Lvbench: An extreme long video understanding benchmark, 2025. https://arxiv.org/abs/2406.08035. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents, 2025. https://arxiv.org/abs/2504.12516. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025a. Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearch-r1: Incentivizing lmms to search, 2025b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. https://arxiv.org/abs/2505.09388. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: Benchmarking multi-task long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1369113701, June 2025a. Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, Yuxin Gu, Sixin Hong, Jing Ren, Jian Chen, Chao Liu, and Y. Hua. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese, 2025b. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, and etc. Jie Shao. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. https://arxiv.org/abs/2504.10479."
        },
        {
            "title": "Appendix",
            "content": "This supplementary material includes the following sections: Evaluation Prompts (cf. A) Limitations and Future Work (cf. B)"
        },
        {
            "title": "A Evaluation Prompts",
            "content": "We provide the prompts used in evaluation and judge process. Evaluation Prompt Your response should be in the following JSON format: { \"Explanation\": \"your explanation for your final answer\", \"Answer\": \"your succinct, final answer\", \"Confidence\": \"your confidence score between 0% and 100% for your answer\" } Judge Prompt Question: {question} Ground Truth: {ground_truth} Model Prediction: {prediction} Evaluate if the Prediction matches the Ground Truth. Return JSON: {{\"is_correct\": true}} or {{\"is_correct\": false}}"
        },
        {
            "title": "B Detailed Performance Analysis",
            "content": "B.1 Category-Wise Performance Figure 10 Performance heatmap. Figure 10 provides granular breakdown of model accuracy across the eight video genres. clear performance dichotomy is observable based on the availability of external textual metadata. In categories like TV Series and Education, where transcripts, wikis, and plot summaries are abundant on the open web, search-augmented models achieve their highest scores. For example, Gemini-2.5-Pro (w/ Search) reaches 57.9% accuracy on TV Series. Conversely, performance collapses in dynamic, stochastic environments like Games and Sports. These categories require temporal grounding in specific visual moments (e.g., specific foul or gameplay sequence) that are rarely indexed by text search engines. Consequently, the same models accuracy drops to 9.1% in Games and 8.9% in Sports , highlighting the \"Modality Gap\" where agents fail to process visual information when text shortcuts are unavailable. 16 B.2 Cost-Efficiency Analysis (Token Usage) Figure 11 Token usage. Figure 11 visualizes the trade-off between computational cost (measured in average token usage per query) and performance (accuracy). As indicated by the directional arrow, the top-right quadrant represents the ideal balance of High Accuracy and Low Token Usage. This is the target frontier for future agentic development. The plot reveals that current high-performing agents like o4-mini-deep-research achieve superior accuracy but at the expense of significantly higher token consumption, likely due to extensive iterative browsing and multi-step reasoning. In contrast, lightweight models like gpt-4o-mini are token-efficient but lack the deep reasoning capabilities required for complex video tasks. The arrow suggests that the goal of future research is to push these agents \"upwards\" towards the ideal zone, optimizing the reasoning process to be both accurate and token-efficient."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Beijing University of Posts and Telecommunications",
        "Hong Kong Polytechnic University",
        "Singapore Management University",
        "University of Trento"
    ]
}