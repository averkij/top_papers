{
    "paper_title": "Value Residual Learning For Alleviating Attention Concentration In Transformers",
    "authors": [
        "Zhanchao Zhou",
        "Tianyi Wu",
        "Zhiyun Jiang",
        "Zhenzhong Lan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. Further visualization results suggest that Resformer alleviates attention sinks through avoiding value-state drains. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate."
        },
        {
            "title": "Start",
            "content": "VALUE RESIDUAL LEARNING FOR ALLEVIATING ATTENTION CONCENTRATION IN TRANSFORMERS Tianyi Wu3 Zhiyun Jiang4 2Westlake University Zhanchao Zhou1,2 1Zhejiang University 3University of Electronic Science and Technology of China 4China University of Mining and Technology 5Research Center for Industries of the Future, Westlake University Zhenzhong Lan2,5 4 2 0 2 4 ] . [ 2 7 9 8 7 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the KV cache by nearly 50%. Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. Further visualization results suggest that Resformer alleviates attention sinks through avoiding value-state drains. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate."
        },
        {
            "title": "INTRODUCTION",
            "content": "The Transformer (Vaswani, 2017) model has become one of the leading architectures in recent years, excelling in both language modeling (Devlin, 2018; Lan, 2019; Brown, 2020) and computer vision tasks (Dosovitskiy, 2020). The discovery of scaling laws (Hoffmann et al., 2022; Kaplan et al., 2020) has driven the pursuit of larger Transformer models by increasing network depth and width. Training large models presents significant challenges. Balancing the depth and width of Transformer model within fixed parameter budget is particularly difficult. While research indicates that deeper models generalize more compositionally than shallower ones (Petty et al., 2024), the training and deployment of deep models remain problematic. Although Transformers use residual connections (He et al., 2016) to address the vanishing gradient issue, training very deep Transformers is still challenging. For example, 32-layer Vision Transformer (ViT) may perform worse than 24layer one (Zhou et al., 2021). This is mainly due to the smoothing mechanism of attention (Shi et al., 2022), which can lead to an over-smoothing effect (Nguyen et al., 2023) where the token representations become the same as the models depth increases. Existing solutions to alleviate the over-smoothing problem in Transformer include adding extra regularizers (Nguyen et al., 2023; Shi et al., 2022) and optimizing the information flow within the model (Pagliardini et al., 2024). During the era of convolutional neural network architectures, Stochastic Depth (Huang et al., 2016) reduces the likelihood of over-smoothing by randomly dropping layers during training and DenseNet (Huang et al., 2017) mitigates the impact of over-smoothing by allowing each layer to directly access the hidden states of all preceding layers. Recently, DenseFormer Equal Contribution; Work done during internship at Westlake University; Corresponding author. 1Code is available at https://github.com/Zcchill/Value-Residual-Learning. 1 Figure 1: (Left) Illustration of the relative training loss (loss of target model - loss of vanilla Transformer) curve between different Transformer variants; model size is fixed to be 82M. (Middle) The average entropy of token importance across layers in ResFormer vs. the vanilla Transformer, where token importance is derived from the attention matrix. Lower entropy indicates more focused attention on specific tokens. More details can be found in Eqn. 11. (Right) The average entropy of token importance across layers in Llama (8B) (Dubey et al., 2024) and Mistral (7B) (Jiang et al., 2023). (Pagliardini et al., 2024) adopts the idea of DenseNet when training Transformer. Additionally, NeuTRENO (Nguyen et al., 2023) alleviates over-smoothing through incorporating the difference between the value states of the first layer and the current layer to the attention output. In this paper, we address the problem of multi-layer attention from novel perspective. We introduce the phenomenon of attention concentration, which describes how models attention increasingly focuses on fewer tokens as the network depth increases, as illustrated in Fig. 1 (Right). We quantify the degree of attention concentration using the entropy of the distribution of token importance, where lower entropy indicates more pronounced concentration. In deep model, the attention mechanism exhibits pattern of concentration - dispersion - concentration, where the model could potentially lose useful information during the concentrated phase. See Fig. 18 for analysis of over-smoothing. To alleviate the attention concentration from stacking multiple attention layers, an effective method is to use cross-layer attention on information from earlier layers. Given the high computational cost of cross-layer attention, we propose ResFormer as an approximation. ResFormer achieves similar effect by applying residual connection between the value states of the current layer and the first layer before the attention operation. Experimental results show that ResFormer mitigates the attention concentration effect in deeper layers, maintaining attention dispersion throughout the sequence, as shown in Fig. 1 (Middle). Further visualization results show that Resformer effectively alleviates attention sinks (Xiao et al., 2023) through avoiding value-state drains (Guo et al., 2024b). During inference, deep networks require substantial KV cache, severely impacting model deployment (Xiao et al., 2023). Existing KV -efficient methods often process keys and values simultaneously. Building on ResFormer, we decouple the value from the attention operation and propose new kind of Transformer with single layer value (SVFormer). In SVFormer, the queries and keys of all layers share the value from the first layer. Consequently, SVFormer can save nearly half of the KV cache. Additionally, SVFormer is orthogonal to the classical method GQA, and they can be used concurrently. Experimental results show that SVFormer trains significantly faster than the vanilla Transformer. Its performance mainly depends on factors like training sequence length and cumulative learning rate. We experiment on 20B SlimPajama sub-sampled dataset, using settings similar to popular large language models (Wei et al., 2023; Dubey et al., 2024; Kaplan et al., 2020). We compare different models by their relative training curves against the vanilla Transformer. Results show that ResFormer outperforms the vanilla Transformer, DenseFormer, and NeuTRENO across all settings. Furthermore, SVFormer trains faster than the vanilla Transformer and performs better when the sequence length is longer."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 SHORTCUT CONNECTIONS FOR BETTER INFORMATION FLOW Deep learning models often consist of multiple layers, posing challenge to minimize information loss during transmission. ResNet (He et al., 2016) mitigates the vanishing gradient problem with 2 (a) Transformer (b) NeuTRENO (c) DenseFormer (d) ResFormer (e) SVFormer Figure 2: Simplified illustration of the vanilla Transformer, NeuTRENO, DenseFormer, ResFormer, and SVFormer, with only three-layer structures and no operations other than attention. Ai, i, and denote the attention matrix, value states, and attention outputs at the i-th layer, respectively. , , and represent standard matrix addition, subtraction, and multiplication, respectively. identity connections. Stochastic Depth (Huang et al., 2016) enhances training by randomly dropping layers. DenseNet (Huang et al., 2017) allows subsequent layers to directly access the hidden states of all preceding layers. These two methods further enhance the information flow after ResNet. Related research indicates that for advanced Transformer architectures, although increasing depth continues to yield performance improvements in language modeling tasks, the gains become less significant with further increases (Petty et al., 2024). Furthermore, Zhou et al. (2021) illustrates that 32-layer ViT underperforms 24-layer ViT. Depth-Wise Attention (ElNokrashy et al., 2024) allows each query to access the key and value at the same position from previous layers through an attention-like mechanism before the output layer. DenseFormer (Pagliardini et al., 2024) integrates weighted fusion of outputs from all preceding layers after each layer. To explore why increasing depth in Transformers does not yield expected gains, Wang et al. (2022) finds that self-attention acts as low-pass filter, smoothing token representations in ViTs. Additionally, Shi et al. (2022) investigates over-smoothing from graph perspective in BERT-based language modeling tasks. NeuTRENO (Nguyen et al., 2023) adds the difference between the value states of the first and current layers to each layers attention output and significantly alleviates the over-smoothing problem. In contrast to these methods, ResFormer accesses and integrates information from previous layers prior to the attention operation, as illustrated in Fig. 2. Moreover, it does not require the selection or tuning of additional hyperparameters. 2.2 KV CACHE COMPRESSING The KV cache is key factor limiting the efficiency of long-text model inference. Research in this area can be broadly classified into Transformer-based methods, which target redundant information in Transformer models, and non-Transformer methods, which mainly addresses the quadratic time complexity of attention with respect to sequence length. For non-Transformer methods, Mamba (Gu & Dao, 2023) and RWKV (Peng et al., 2023) are two popular works. They replace the original softmax-based attention with SSM (Gu et al., 2021) and AFT (Zhai et al., 2021) mechanisms, respectively. Besides, several approaches have been proposed to enhance models ability to process long texts while reducing the reliance on KV cache. Dai (2019) advocates segmenting long texts into smaller parts for attention computation. Building upon this, Munkhdalai et al. (2024) proposes using fixed-size memory matrix for storing and retrieving historical information. Transformer-based methods can be categorized into three main groups. The first group consists of post-training methods like SnapKV (Li et al., 2024) and ThinK (Xu et al., 2024), which compress 3 KV cache during inference based on attention matrices at token or hidden dimension levels. The second group focuses on quantization and adopts low-precision KV cache quantization rather than completely eliminating them (Hooper et al., 2024). The third group aims to maximize the efficiency of attention-based models via parameter or activation value sharing. The most representative works include Multi-Query Attention (Shazeer, 2019) and Grouped-Query Attention (Ainslie et al., 2023) which suggest to share key and value across group of queries. MLKV (Zuhri et al., 2024) further suggest to share keys and values for queries across layers and MLA (Liu et al., 2024) introduces low-rank projection when processing keys and values. Besides, CLA (Brandon et al., 2024) and LISA (Mu et al., 2024) respectively point out that we can reuse keys, values, or the attention matrix across layers to reduce redundancy between layers. While these methods typically process both key and value simultaneously, SVFormer is the first approach to decouple value from query and key. It shares values across all layers and is compatible with other methods like GQA."
        },
        {
            "title": "3.1 MOTIVATION: INFORMATION TRANSFER VIA CROSS LAYER ATTENTION",
            "content": "Let Hn Rld be the input hidden state of the n-th layer, where denotes the sequence length and is the dimension size. In standard attention, the hidden state Hn will be firstly projected into Qn, Kn, Vn Rld through three linear projections WQ, WK, WV Rdd respectively. For simplicity, we introduce dot-product attention of layer as Attention(Qn, Kn, Vn) = Softmax( QnKT )Vn. (1) An ideal way to incorporate previous layers information is cross layer attention. The attention mechanism naturally extracts relevant information from previous layers. If these layers contain lowquality information, the similarity between the current layers query and the previous layers keys will be low, thus minimizing negative impacts. Given < and the information (Qm, Km, Vm) of m-th layer, the cross layer mechanism calculates the attention output Un of n-th layer by the following attention formula: Un = Softmax (cid:16) Qn Concat(Kn, Km)T / (cid:17) Concat(Vn, Vm). (2) In practice, cross-layer attention enhances feature fusion by allowing information to flow between layers, capturing both intra-layer and inter-layer dependencies. However, this approach introduces additional computational overhead due to the concatenation of keys and values from multiple layers. For example, in scenarios described by Eqn. 2, the overall computational complexity of the model nearly doubles compared with vanilla attention described in Eqn. 1."
        },
        {
            "title": "3.2 EFFICIENT CROSS LAYER ATTENTION",
            "content": "To solve this problem, we propose to replace the Km with Kn in Eqn. 2, as shown in Eqn. 3. Un Softmax (cid:16) Qn Concat(Kn, Kn)T / (cid:16) (cid:17) Softmax QnKT / (Vn + Vm). = 1 (cid:17) Concat(Vn, Vm) (3) (4) Utilizing the concept of block matrices, Eqn. 3 can be further simplified into Eqn. 4. This simplification converts the concatenation operation of the two value matrices into an addition operation. Compared to Eqn. 1, this new method only brings minimal increase in computational complexity while still leveraging the information from the m-th layer in the n-th layer. Furthermore, Eqn. 4 can be generalized to incorporate cross-layer attention across all preceding 1 layers as follows: Un 1 An (cid:88) i= Vi. (5) where An denotes the original attention matrix for layer n. From the perspective of information 4 Figure 3: Average token similarity between the outputs of different mapping methods and that of Eqn. 2. Figure 4: Ablation study on sharing keys or values in every two layers, with CLAttention denoting sharing both. propagation, model described by Eqn. 3 projects the historical values into the current layers embedding space using the current layers attention as weight matrix. For example, naive approach would be to perform identity mapping, as described by Un = AnVn + 1 1 n1 (cid:88) i=1 Vi. (6) To evaluate the approximation effect of replacing the Km with Kn, we randomly select 1,000 pretraining data samples. For each layer of trained baseline model, assuming cross-layer attention defined by Eqn. 2 is required for each layer with respect to the previous one, we calculate the cosine similarity between the outputs from Eqn. 2 and Eqn. 3. We also calculate the cosine similarity between the outputs from Eqn. 2 and Eqn. 6 for comparison. Fig. 3 shows that using current attention as mapping matrix provides better approximation for cross-layer attention. 3.3 TRANSFORMER WITH RESIDUAL VALUE Based on Eqn. 5, we propose variant of Transformer with residual value (ResFormer) which only chooses first layer as the target of cross layer attention since the first layer contains all basic information of each token. The analysis of entropy in Fig. 1 (Right) supports this point, indicating that attention tends to be relatively dispersed across different tokens in the initial layers of the model. The attention mechanism of ResFormer can be formulated as Un = 1 An(Vn + V1). (7) where 2 and standard attention is applied in the first layer. From the training perspective, it explicitly learns residual mapping instead of directly learning the desired underlying mapping and thats why we call it ResFormer. 3.4 UNIFIED VIEW OF NEUTRENO AND DENSEFORMER Using our framework, the NeuTRENO can be defined as (cid:16) Un = An λJ (cid:17) Vn + λV1. (8) where denotes matrix of ones and λ is hyper-parameter. It can be found that the term of λJ may have certain negative impact on the learning of original attention. If we ignore the attention output projections and the MLP layers, DenseFormer can also be modeled within our framework as Un = (cid:88) i=1 αiAiVi. (9) where {αi}n i=1 is set of hyper-parameters. DenseFormer uses attention matrix of previous layer as the weight matrix of projecting values but this is not aligned with the concept of the efficient cross layer attention shown in Eqn. 3. 5 Figure 5: (Left) The relative training curve between 82M ResFormer and Transformer across different training sequence lengths. (Middle) Average training loss for the final 50 steps across different model sizes and the corresponding fitted curves. (Right) The relative training curve across different model size for fixed 2,048 training sequence length."
        },
        {
            "title": "3.5 SVFORMER: SINGLE-LAYER VALUE FOR HALF KV CACHE",
            "content": "After ResFormer, natural idea is whether we can remove the value states in each layer and have all layers share the value states from the first layer. We call this method SVFormer. Similar to ResFormer, SVFormer still adopts standard attention in the first layer and obtain the attention output Un for n-th layer where 2 through Un = AnV1. (10) Compared to previous methods, SVFormer is the first method that decouple value states from attention. Its main advantage is that it only requires computing and storing the value states for the first layer, saving nearly half of the KV cache during inference. Similar methods like CLA reduce KV cache by sharing both of the key and value states every two layers, but the results in Fig. 4 show that sharing values has less negative impact on performance compared with sharing keys."
        },
        {
            "title": "4 PRETRAIN EXPERIMENTS",
            "content": "4.1 SETTING 4.1.1 TRAINING DETAILS Following Brandon et al. (2024), we choose the Llama-like architecture and SlimPajama (Soboleva et al., 2023) data for main experiments. Specifically, the architecture includes pre-normalization, SwiGLU activations (Shazeer, 2020), rotary position embedding (Su et al., 2024), and no dropout. For slimpajama, we randomly sample nearly 20B tokens according to the original data distribution of seven domains during training and adopt tokenizer used for RedPajama-INCITE-7B-Base. The details of training data can be found in Table 2 in Appendix. Unless otherwise noted, we train all models using AdamW optimizer with 0.1 weight decay, β1 = 0.9, β2 = 0.95 and the max grad norm 1.0. The batch size is set to be around 2M tokens (Zhang et al., 2024) with sequence length of 2,048 and the total steps is fixed 10,000 steps (Kaplan et al., 2020). We adopt linear learning rate warmup for the first 1,200 steps with the initial learning rate and the peak learning rate to be 1e-7 and 6e-4 respectively. The cosine decay schedule gradually decays to 10% of the peak learning rate by the end of training (Brandon et al., 2024; Zhou et al., 2024; Wei et al., 2023). See Table 4 and Table 3 in Appendix for more details. All models are trained with 8 Nvidia A100 80G GPUs using mixed-precision training in FP16. We adopt deepspeed zero-2 optimizer and flash attention mechanism. 4.1.2 RELATIVE TRAINING LOSS CURVE ON SLIMPAJAMA We trained all models for only one epoch on SlimPajama subsets, and primarily use training loss to compare different models. Furthermore, we use the relative training loss curve for better visualizing the difference among different models from the perspective of loss landscape. Specifically, for each method, we will subtract the smoothed training curve of the vanilla Transformer, obtained under the same experimental settings, from the smoothed training curves of the method. The smoothing is done using window size of 10 steps or 100 steps. 6 Figure 7: Ablation study of adding residual connection to queries or keys. Figure 8: Ablation study of adding residual connection using different mapping matrix. Figure 9: Ablation studies on which historical layers value to include in residual connections."
        },
        {
            "title": "4.1.3 ENTROPY FOR ANALYZING ATTENTION CONCENTRATION EFFECTS",
            "content": "Given the attention matrix Rll at one layer, we use entropy to represent its concentration effect. To obtain entropy e, calculate the importance vector = 1 j=1 Aij firstly where is lower triangular matrix. The entropy can be formulated as follows: (cid:80)l = (cid:88) i=1 log a i. (11) (cid:16)(cid:80)l (cid:17) = ai/ where of clustering in a, i.e., attention matrix is more likely to focus on several specific tokens. for = 1, 2, . . . , and the higher the entropy e, the greater the degree i=1 ai 4.1.4 SPECTRAL DECOMPOSITION FOR ANALYZING REPRESENTATIONS Spectral Decomposition is classical method to analyze the representations of models. Zhu et al. (2021) suggests that the eigenvectors with larger eigenvalues are more transferable. Here we use spectral decomposition to analyze the feature space of value of one layer as following: 1 (cid:88) i= vivT = (cid:88) i=j wjλjwT . (12) where wj is the j-th eigenvector with eigenvalue λj for = 1, 2, . . . , and is the dimensionality of the values feature space. 4.2 RESFORMER vs. VANILLA TRANSFORMER We trained ResFormer and vanilla Transformer with different model size on data with difIn Fig. 5, ResFormer consistently outperforms vanilla Transferent sequence lengths. the results in Fig. 1 former throughout illustrate that ResFormer outperforms DenseFormer and NeuTRENO. Furthermore, (Left) to additional performance improvements. integrating ResFormer with NeuTRENO leads training for different models sizes. Additionally, We further test the variant of Resformer defined as Un = An(Vn + λV1). As shown in Fig.6, Resformer can accommodate wide range of λ values and the performance improves as λ increases, achieving the best results at λ = 2. Regardless of the value of λ, ResFormer consistently outperforms Transformers. It suggests that the success of Resformer lies in the use of V1 and the transformation through An. The ablation study of different hyperparameters λ for NeuTRENO, as defined in Equation 8, can be found in the Appendix A.3. Figure 6: Ablation study of different λ for Resformer. (a) Token importance. (b) Norms of value states. (c) Norms of hidden states. Figure 10: The token importance, value-state norms, and hidden-state norms of the first token in each sequence across layers in ResFormer vs. the vanilla Transformer. (a) Transformer. (b) NeuTRENO. (c) Resformer. Figure 11: The distribution of token importance for different models at different layers. 4.3 ABLATION STUDY OF RESIDUAL CONNECTION In Eqn. 4, we employ residual connections for the values. We compare this approach with models that add residual connections to queries or keys. The results, shown in Fig. 7, indicate that only residual connections for values yield positive effects. One possible explanation is that attention mechanisms are sensitive to perturbations, and modifying queries or keys can significantly impact attention matrix. Moreover, we compare with the models based on Eqn. 2 and Eqn. 6. From the cross layer attention perspective, the former uses gold attention matrix in Eqn. 2 to map historical values, while the latter uses an all-ones matrix in Eqn. 6. The results in Fig. 8 align with Fig. 3, showing that identity mapping causes significant perturbations, leading to poor performance. Interestingly, using current attention as the mapping matrix results in an even lower final loss than using gold attention. When determining the mapping method and target value, it is crucial to consider which historical layers values should be included in the residual connection. Fig. 9 shows that each Transformer layer should add shortcut to the first layers value rather than to the nearest preceding layer or all previous layers, highlighting the first-layer values critical importance. potential explanation is that incorporating values from other layers may dilute the impact of the first-layer value. 4.4 DOWNSTREAM EVALUATIONS We compare the different models on several classical reasoning tasks following (Zhang et al., 2024) in zero-shot way. The tasks include Hellaswag (Zellers et al., 2019), OpenBookQA (Mihaylov et al., 2018), WinoGrande (Sakaguchi et al., 2019), ARC-Easy and ARC-Challenge (Clark et al., 2018) and PIQA (Bisk et al., 2020). The results in Table 1 show that ResFormer (82M) achieves an average accuracy improvement of nearly 3% compared to the vanilla Transformer (82M). 4.5 VISUALIZATION OF RESFORMER To figure out why ResFormer can achieve better performance on language modeling tasks than vanilla Transformer, we conduct visualization based on the eigenvalue decomposition discussed in Section 4.1.4. After sorting the eigenvalues in descending order, we compute the average eigenvalue Model Max Length HellaSwag Obqa WinoGrande ARC-c ARC-e PIQA Avg. Transformer ResFormer Transformer ResFormer 2,048 2,048 64,000 64, 0.263 0.273 0.267 0.274 0.142 0.148 0.142 0.136 0.492 0.512 0.485 0.513 0.199 0.182 0.179 0.184 0.331 0.414 0.322 0.407 0.572 0.604 0.570 0. 0.333 0.355 0.328 0.350 Table 1: Zero-shot accuracy on commonsense reasoning tasks. Figure 12: Left: Distribution of eigenvalues for the value states in the first layer of ResFormer and Transformer. Right: Maximum eigenvalue for each layer of ResFormer and Transformer. for each layer across 1,000 randomly sampled pre-train data examples. The results in Fig. 12 indicate that the value states generated by most layers of the ResFormer exhibit stronger representational capacity compared to those of the vanilla Transformer. Besides, we analyze the attention concentration effects mentioned in Section 4.1.3 using the same batch of test data. Fig. 1 (Middle) illustrates that the clustering effect of attention increases significantly with the number of layers for the vanilla Transformer, whereas the clustering effect is relatively less pronounced for the ResFormer. We further visualize the attention weights, value-state norms v2, and hidden-state norms h2 of tokens at different layers and positions, with detailed results in Appendix A.2. Given that attention clustering often occurs on the first token, we primarily show its results in Fig. 10. The results indicate that using ResFormer significantly mitigates attention sinks (Xiao et al., 2023), value-state drains (Guo et al., 2024b) and residual-state peaks (Sun et al., 2024). Guo et al. (2024a) attributes these phenomena to the mutual reinforcement mechanism of model and we suggest that the value shortcut disrupts this mechanism by alleviating value-state drains. Specifically, for tokens lacking semantic information, such as the first token, large value state magnitude can adversely affect the prediction of future tokens if it is overly attended to, resulting in higher loss. 4.6 SVFORMER vs. GQA In the Fig. 13, at training sequence length of 64,000, SVFormer demonstrates lower final loss compared to existing KV-efficient methods such as CLA and GQA. Moreover, it can be used concurrently with GQA to enhance KV efficiency further. However, we observed that with training sequence length of 2,048, SVFormer underperforms compared to GQA. The results indicate that sequence length significantly affects SVFormers performance. Thus, we conducted more comprehensive experiments on sequence length. Results in Fig. 14 (Left) demonstrate that SVFormer will always be gradually surpassed by vanilla attention during training while its training speed is much faster than vanilla attention. However, as the training sequence length increases, the SVFormer model performs better. In this way, we focus on the critical point, defined as the number of training steps exceeded. Fig. 14 (Right) illustrates that the relationship between the critical point and sequence length exhibits an exponential trend. We argue that its due to the challenge deep models face in fully optimizing the increasingly larger first-layer value matrix as the training sequence length grows. 9 Figure 13: The relative training loss for SVFormer and other KV efficient model compared with vanilla attention. The numbers in parentheses represent the training sequence length. Left: Model with nearly 1/2 KV cache. Right: Model with nearly 1/8 KV cache. Figure 14: Left: The relative training loss for SVFormer under different sequence lengths with fixed batch size of 2M tokens. Right: Analysis of critical point, and we predict it for length 64,000 using linear regression with the last 1,000 data points. (a) Learning Rate. (b) Warmup Steps. (c) Model Size. (d) Architecture. Figure 15: The relative training loss for SVFormer under different hyper-parameter setting. 4.7 OTHER FACTORS INFLUENCING SVFORMER Intuitively, the training effectiveness of SVFormer is influenced by factors such as the maximum learning rate, warmup steps, model size, and other factors beyond just the training sequence length. We conducted experiments to explore these relationships. Based on the results shown in Fig. 15a and Fig. 15b, smaller learning rate benefits SVFormer more, with warmups impact being comparatively small. This could be attributed to the models outcomes being closely tied to the total summed learning rate, which has weak connection with warmup steps (Kaplan et al., 2020). Moreover, larger models often require smaller learning rates to ensure training stability, making them more suitable for using SVFormer. Except for the 2M model, Llama-like models ranging from 82M to 468M, as well as models with the GPT2 architecture, exhibit similar critical points and final losses (see Fig. 15c and Fig. 15d). This suggests that the difference between SVFormer and the vanilla Transformer is not sensitive to model size and architecture."
        },
        {
            "title": "4.8 ABLATION STUDY OF SVFORMER",
            "content": "Figure 16: Ablation study of sharing first layers query(key) across all layers. Figure 17: Ablation study on sharing values from different numbers of layers. To better understand SVFormer, we conduct several ablation experiments. We first observe the effects of sharing the first layers queries or keys across all layers in Fig. 16, finding that this significantly impacts model performance, similar to the results in Fig. 4. Additionally, sharing the first layers values in multi-layer network may reduce the networks effective depth. By updating the shared values using intermediate layers as anchors, we find that increasing the number of anchors improves performance, as shown in Fig. 17."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we propose the concept of attention concentration, problem that arises from stacking multiple attention layers. From the perspective of cross-layer attention, we derive ResFormer, which adds residual connection between the value states of the current layer and those of the first layer before the attention operation to alleviate attention concentration. Additionally, we introduce SVFormer, based on ResFormer, which reduces the KV cache by nearly half. We conducted comprehensive experiments on the language modeling task to validate the advantages of these two Transformer variants in different scenarios."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "This work was supported by the Research Center for Industries of the Future at Westlake University (Grant No. WU2023C017) and the Key Research."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "On the one hand, the data employed in this paper is sourced from publicly available datasets provided by the company, which have undergone certain level of filtering. On the other hand, the models trained in our study are solely utilized for experimental analysis and will not be publicly deployed."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have detailed the complete experiments setup such as batch size, optimizer and learning rates in Section 4.1.1. Besides, we have released the source codes. These resources should be sufficient to reproduce the results of the paper."
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. 11 Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 23972430. PMLR, 2023. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning In The Thirty-Fourth AAAI Conference on about physical commonsense in natural language. Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 74327439. AAAI Press, 2020. doi: 10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai. v34i05.6239. William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and Jonathan Ragan Kelly. Reducing transformer key-value cache size with cross-layer attention. arXiv preprint arXiv:2405.12981, 2024. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457. Zihang Dai. Transformer-xl: Attentive language models beyond fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Muhammad ElNokrashy, Badr AlKhamissi, and Mona Diab. Depth-wise attention (dwatt): layer In Proceedings of the 2024 Joint International fusion method for data-efficient classification. Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 46654674, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael Jordan, and Song Mei. Active-dormant attention heads: Mechanistically demystifying extreme-token phenomena in llms. arXiv preprint arXiv:2410.13835, 2024a. Zhiyu Guo, Hidetaka Kamigaito, and Taro Watanabe. Attention score is not all you need for token importance indicator in kv cache reduction: Value also matters. arXiv preprint arXiv:2406.12335, 2024b. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pp. 646661. Springer, 2016. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 47004708, 2017. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Lan. Albert: lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469, 2024. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixtureof-experts language model. arXiv preprint arXiv:2405.04434, 2024. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering, 2018. Yongyu Mu, Yuzhang Wu, Yuchun Fan, Chenglong Wang, Hengyu Li, Qiaozhi He, Murun Yang, Tong Xiao, and Jingbo Zhu. Cross-layer attention sharing for large language models. arXiv preprint arXiv:2408.01890, 2024. Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143, 2024. Tam Nguyen, Tan Nguyen, and Richard Baraniuk. Mitigating over-smoothing in transformers via regularized nonlocal functionals. Advances in Neural Information Processing Systems, 36:80233 80256, 2023. Matteo Pagliardini, Amirkeivan Mohtashami, Francois Fleuret, and Martin Jaggi. Denseformer: arXiv preprint Enhancing information flow in transformers via depth weighted averaging. arXiv:2402.02622, 2024. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Jackson Petty, Sjoerd Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette, and Tal Linzen. The impact of depth on compositional generalization in transformer language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 72327245, 2024. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. 13 Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen Lee, and James Kwok. Revisiting over-smoothing in bert from the perspective of graph. arXiv preprint arXiv:2202.08625, 2022."
        },
        {
            "title": "Daria",
            "content": "Soboleva, Faisal Al-Khateeb, and Nolan Dey. Hestness, deduplicated slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. 627B token SlimPajama: RedPajama. cleaned version of Jacob Steeves, Joel and https://cerebras.ai/blog/ Robert Myers, Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Mingjie Sun, Xinlei Chen, Zico Kolter, and Zhuang Liu. Massive activations in large language models. arXiv preprint arXiv:2402.17762, 2024. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. arXiv preprint arXiv:2203.05962, 2022. Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu, Rui Hu, et al. Skywork: more open bilingual foundation model. arXiv preprint arXiv:2310.19341, 2023. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36, 2024. Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, and Doyen Sahoo. Think: Thinner key cache by query-driven pruning. arXiv preprint arXiv:2407.21018, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can maIn Anna Korhonen, David R. Traum, and Lluıs M`arquez chine really finish your sentence? (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pp. 4791 4800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p19-1472. Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021. Fei Zhu, Zhen Cheng, Xu-Yao Zhang, and Cheng-lin Liu. Class-incremental learning via dual augmentation. Advances in Neural Information Processing Systems, 34:1430614318, 2021. Zayd Muhammad Kawakibi Zuhri, Muhammad Farid Adilazuarda, Ayu Purwarianti, and Alham Fikri Aji. Mlkv: Multi-layer key-value heads for memory efficient transformer decoding. arXiv preprint arXiv:2406.09297, 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 TOKEN SIMILARITY ANALYSIS Attention concentration tends to make embeddings of different tokens more similar, resulting in over-smoothing. The extent of over-smoothing can be assessed by calculating the average token similarity of the hidden states using the following formula: = 1 l(l 1) (cid:88)l (cid:88)l i=1 j=1,j=i (cid:16)"
        },
        {
            "title": "Sim",
            "content": "hi, hj (cid:17) . (13) where {hi}l i=1 is the hidden state of the i-th token and Sim() denotes the operation of cosine similarity. The results in Fig. 18 are align with the results in Fig. 1. In the case of Llama and Mistral, the average token similarity demonstrates an M-shaped pattern with increasing network depth, while entropy follows W-shaped pattern at corresponding positions. These trends suggest that attention concentration indeed leads to the phenomenon of over-smoothing. Figure 18: The average token similarity of hidden states across layers in Llama and Mistral. A.2 ATTENTION CONCENTRATION VISUALIZATION We visualize the token importance, norms of value states and norms of hidden states for tokens at different position across layers. The results are averaged from 1,000 different sequences so that only the start token is the same and special across all sequences. Fig. 19 (First row) demonstrates that the start token easily attracts massive attention despite lacking semantic information for Transformer and NeuTRENO. For Resformer, the importance of the start token is less than 10 times that of tokens at other positions, indicating that tokens carrying semantic information receive more attention. Moreover, both Transformer and NeuTRENO exhibit significant value-state drains (Guo et al., 2024b) and residual-state peaks (Guo et al., 2024a; Sun et al., 2024) on the start token at certain layers. In contrast, for Resformer, the value state norm of the start token is approximately 0.7 times that of other tokens, while the peak hidden state norm is about three times larger. Moreover, We conduct an entropy analysis on test examples of varying lengths. The results in Fig.20 indicate that Resformer is more effective in mitigating attention concentration when the sequences are shorter. A.3 ABLATION STUDY OF NEUTRENO NeuTRENO is sensitive to the choice of hyperparameter λ which is task-dependent. In the appendix of Nguyen et al. (2023), it is reported that λ is set to 0.6 for image classification and segmentation tasks, and 0.4 for language modeling tasks. Fig. 21 indicates that λ = 0.4 achieves the best results in our training dataset so that we choose λ = 0.4 for comparison. Besides, we empirically choose λ = 0.2 for NeuTRENO when combined with Resformer. A.4 PRE-TRAIN DATASET Based on the equation 5000 0.74 (Kaplan et al., 2020) where is data size and is the number of non-embedding parameters, we need to collect at least 17.5B for model has = 700M"
        },
        {
            "title": "Norms of\nvalue states",
            "content": "Norms of hidden states (a) Transformer. (b) NeuTRENO. (c) Resformer. Figure 19: Visualization of the token importance, norms of value states and norms of hidden states. (a) Length: 20 tokens. (b) Length: 100 tokens. (c) Length: 500 tokens. (d) Length: 2,048 tokens. Figure 20: The average entropy of token importance across layers in ResFormer vs. Transformer for sequence with different lengths. the vanilla Figure 21: Ablation study of different λ for NeuTRENO. 16 Data source proportions Tokens Commoncrawl C4 GitHub Books ArXiv Wikpedia StackExchange 50% 20% 10% 5% 5% 5% 5% 10 4 2 1 1 1 1 Table 2: The details of pre-train dataset. non-embedding parameters (corresponding to complete 1B model with 2,048 hidden size, 50,277 vocab size and 2,048 sequence length) to avoid over-fitting. Besides, Xie et al. (2024) indicates that the mixture proportions of pre-training data domains significantly affects the training results. In this way, we sampled 20B tokens data from original 627B data based on the original data proportions shown in the Table 2. A.5 TRAINING DETAILS Max Sequence Length 512 2, 8,192 32,000 64,000 Total Batch Size Per-GPU Batch Size Gradient Accumulation Step GPUs 4,096 128 1,024 256 8 32 8 64 2 32 1 Table 3: Training details for training dataset with different sequence length. Section 4.1.1 introduces the main experimental hyperparameters used in the paper. This section further details the training parameters for various model sizes and training sequence lengths. Table 4 demonstrates the differences among models of various sizes. The configurations for the number of layers, attention heads, hidden dimensions, and FFN dimensions are based on Biderman et al. (2023). Additionally, the λ in Eqn. 8 is set to be 0.4 for NeuTRENO. Moreover, as reported in Table 3, the batch size that single GPU can accommodate varies depending on the length of the training sequences. Note that the total number of tokens in each batch is consistently 2 million. A.6 VALIDATION LOSS ON SLIMPAJAMA Section 4.1.2 introduces to use relative training loss as main evaluation matrix. Table 5 reports the validation loss for differnt model on the whole validation split of slimpajama."
        },
        {
            "title": "Model Size",
            "content": "Layers Attention Heads Hidden Dimension FFN Dimension Tie Word Embedding (Peak Learning Rate, Final Learning Rate) Learning Rate Schedule Vocabulary Size Activation Function Position Embedding Batch Size Data Size (Warmup Steps, Training Steps) Adam β Dropout Weight Decay 2M 82M 180M 468M 4 2 16 56 8 8 512 1,792 12 12 768 2,688 24 16 1,024 3, False (6e 4, 6e 5) Cosine Decay 50,277 SwiGLU RoPE (θ = 10,000) 2M tokens 20B tokens (120, 10,000) (0.9, 0.95) 0.0 0.1 Table 4: Training details for models with different size. Model Transformer (82M) Transformer (180M) Transformer (468M) Resformer (82M) Resformer (180M) Resformer (468M) Common Crawl 3.3595 3.0961 2.8514 3.3362 3.0631 2. C4 Github 3.5388 3.2834 3.0430 3.5191 3.2504 3.0115 1.4247 1.2451 1.0908 1.3941 1.2200 1.0730 Stack Exchange 2.3872 2.1651 1.9628 2.3592 2.1350 1. Wikipedia Book Arxiv Avg. 2.9047 2.5897 2.2821 2.8646 2.5435 2.2477 3.3797 3.1309 2.8979 3.3572 3.0994 2. 2.1779 2.0001 1.8362 2.1518 1.9732 1.8142 2.7389 2.5015 2.2806 2.7117 2.4692 2.2537 Table 5: Validation loss on slimpajama."
        }
    ],
    "affiliations": [
        "China University of Mining and Technology",
        "Research Center for Industries of the Future, Westlake University",
        "University of Electronic Science and Technology of China",
        "Westlake University",
        "Zhejiang University"
    ]
}