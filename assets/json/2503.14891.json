{
    "paper_title": "MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer",
    "authors": [
        "Honglin Lin",
        "Zhuoshi Pan",
        "Yu Li",
        "Qizhi Pei",
        "Xin Gao",
        "Mengzhang Cai",
        "Conghui He",
        "Lijun Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated promising capabilities in solving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as a vital component in guiding answer generation. Current paradigms typically generate CoT and answers directly for a given problem, diverging from human problem-solving strategies to some extent. Humans often solve problems by recalling analogous cases and leveraging their solutions to reason about the current task. Inspired by this cognitive process, we propose \\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall and reflect on meta-problems, those structurally or semantically analogous problems, alongside their CoT solutions before addressing the target problem. Additionally, we introduce a problem-restating mechanism to enhance the model's comprehension of the target problem by regenerating the original question, which further improves reasoning accuracy. Therefore, the model can achieve reasoning transfer from analogical problems, mimicking human-like \"learning from examples\" and generalization abilities. Extensive experiments on mathematical benchmarks demonstrate that our MetaLadder significantly boosts LLMs' problem-solving accuracy, largely outperforming standard CoT-based methods (\\textbf{10.3\\%} accuracy gain) and other methods. Our code and data has been released at https://github.com/LHL3341/MetaLadder."
        },
        {
            "title": "Start",
            "content": "MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer Honglin Lin1, Zhuoshi Pan1,2, Yu Li1, Qizhi Pei1,3, Xin Gao1, Mengzhang Cai1, Conghui He1, Lijun Wu1* 1Shanghai AI Laboratory 2Tsinghua University 3Renmin University of China {linhonglin,wulijun}@pjlab.org.cn 5 2 0 2 9 1 ] . [ 1 1 9 8 4 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated promising capabilities in solving mathematical reasoning tasks, leveraging Chain-ofThought (CoT) data as vital component in guiding answer generation. Current paradigms typically generate CoT and answers directly for given problem, diverging from human problem-solving strategies to some extent. Humans often solve problems by recalling analogous cases and leveraging their solutions to reason about the current task. Inspired by this cognitive process, we propose MetaLadder, novel framework that explicitly prompts LLMs to recall and reflect on meta-problems, those structurally or semantically analogous problems, alongside their CoT solutions before addressing the target problem. Additionally, we introduce problem-restating mechanism to enhance the models comprehension of the target problem by regenerating the original question, which further improves reasoning accuracy. Therefore, the model can achieve reasoning transfer from analogical problems, mimicking human-like learning from examples and generalization abilities. Extensive experiments on mathematical benchmarks demonstrate that our MetaLadder significantly boosts LLMs problem-solving accuracy, largely outperforming standard CoT-based methods (10.3% accuracy gain) and other methods. Our code and data has been released at https://github. com/LHL3341/MetaLadder."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have achieved remarkable success in mathematical reasoning tasks by leveraging Chain-of-Thought (CoT) data, which explicitly guides models to decompose problems into intermediate reasoning steps before producing final answers (OpenAI, 2024b; Guo et al., 2025; Team, 2024). Pioneering works such as (Wei et al., * Corresponding author. 1 Figure 1: Compare our MetaLadder with other methods (Standard CoT, Question & Answer Augmentation, Reflective Augmentation) on training data construction. 2022) demonstrated that training LLMs on CoTstyle solutions significantly improves their ability to solve complex problems, with subsequent studies (Fu et al., 2022; Zhou et al., 2022) further refining this paradigm. For instance, models like Minerva and GPT-4 (Lewkowycz et al., 2022; OpenAI et al., 2023) have showcased near-human performance by distilling high-quality CoT trajectories from expert demonstrations. These methods typically follow straightforward template: given problem, the model generates CoT explanation step-by-step, which then leads to the correct answer. While effective, such approaches align only partially with the nuanced cognitive processes (Daniel, 2011) humans employ during problem-solving. Despite their success, existing CoT-based finetuning methods rely on rigid Problem CoT Answer framework (Fu et al., 2023; Yu et al., 2024), which diverges from how humans approach challenging mathematical tasks. When solving problems, humans rarely generate solutions in isolation; instead, they actively recall analogous problems and their solutions, especially for difficult or unfamiliar questions (Vosniadou, 1988; Daugherty and Mentzer, 2008). For example, encountering combinatorics problem, student might recall similar problems involving permutations or recursive strategies, using their structures to guide the current solution. This ability to leverage prior analogical experiences is critical for generalizing knowledge and tackling novel challenges. However, current LLM training paradigms largely overlook this aspect, treating each problem as an independent instance without encouraging cross-problem reasoning. This limitation constrains models capacity to transfer learned reasoning patterns, particularly for problems requiring abstract structural or semantic similarities to prior examples. To bridge this gap, we propose MetaLadder, framework inspired by human-like analogical reasoning and problem comprehension. MetaLadder explicitly guides LLMs to recall and reflect on meta-problemsstructurally or semantically analogous problems with known CoT solutionsbefore generating answers for the target problem. These meta-problems and their CoT trajectories serve as scaffolding to derive the current solution, mirroring how humans stand on the shoulders of past experiences. Additionally, we introduce problem-restating mechanism: before reasoning, the model regenerates the original question in its own words, enhancing its comprehension of the problems core components and constraints. This dual mechanismanalogical recall and active restatementenables the model to decompose complex problems into familiar reasoning patterns, effectively mimicking the human ability to learn from examples and generalize solutions across analogous contexts. By integrating these steps, MetaLadder successfully transforms the traditional linear CoT process into dynamic, context-aware reasoning ladder, where each rung represents retrieved meta-problem or refined understanding of the target task. Extensive experiments validate MetaLadders effectiveness. On mathematical benchmarks like GSM8K and MATH, models trained with MetaLadder achieve significant improvements over standard CoT fine-tuning baselines, with accuracy gains of 12.4% and 11.3%, respectively, surpassing recent advanced methods. Further analysis reveals that MetaLadder-trained models exhibit stronger generalization to structurally novel problems, solving 9.3% more out-of-distribution test cases than vanilla CoT models. Qualitative examples demonstrate that the model not only retrieves relevant meta-problems but also adapts their solutions creatively. These results collectively highlight that emulating human-like analogical reasoning and active comprehension is powerful yet underexplored direction for advancing LLMs mathematical reasoning capabilities."
        },
        {
            "title": "2.1 Data Synthesis for Math Reasoning",
            "content": "Data synthesis has significantly contributed to the development of LLMs mathematical reasoning abilities (Yang et al., 2024; Shao et al., 2024). Some studies focus on expanding the dataset and its diversity by rewriting questions or answers (Yu et al., 2024; Yuan et al., 2023; Liu et al., 2024; Tang et al., 2024; Luo et al., 2023). For example, MetaMath (Yu et al., 2024) diversifies the data through various enhancement methods, including question rephrasing, answer augmentation, and the generation of inverse problems. Another line of research focuses on improving the quality and difficulty of the data (Wang et al., 2024a; Luo et al., 2023; Tong et al., 2024; Zhang et al., 2024b; Fan et al., 2024). For instance, WizardMath (Luo et al., 2023) generates more challenging data through RLEIF, while RefAug (Zhang et al., 2024b) adds reflective information after the original CoT process to encourage enhancing the reasoning process. Our method differs by augmenting data to activate the models analogical reasoning capabilities, enabling the model to generate and apply solutions based on analogous problems rather than relying solely on paraphrased data, even enabling self-evolution by generating analogous data."
        },
        {
            "title": "2.2 RAG for Problem Solving",
            "content": "Retrieval-Augmented Generation (RAG) systems enhance the performance of LLMs by integrating an external search engine for knowledge retrieval (Khandelwal et al., 2019; Lewis et al., 2020; Gao et al., 2023). When user poses question, the RAG system first retrieves relevant knowledge fragments through the search engine and then uses these answers along with the original query to generate an answer. To address more complex problems, such as mathematical reasoning, some works incorporate the reasoning capabilities of LLMs into the RAG framework, achieving retrieval-augmented reasoning (RAR) (Melz, 2023). For example, IRCoT (Trivedi et al., 2022) 2 Figure 2: Overview of the MetaLadder framework for generating reflective data. The process starts with the original problem Q, followed by the problem type and solution method S, and the generation of analogous problems and solutions . Afterward, the original problem is reintroduced to prompt the model to restate the problem. These components are then combined with the solution of the original problem to form the training data. combines RAG with multi-step CoT by using the question and previous reasoning steps as queries, retrieving relevant documents to generate the next reasoning step. RAT (Wang et al., 2024b) generates complete CoT first and then refines each reasoning step iteratively using RAG. Recent work, such as Search-o1 (Li et al., 2025), extends the RAG paradigm by applying it to o1-like models, further enhancing the models reasoning capabilities. While RAG emphasizes enhancing the models performance by retrieving external knowledge without updating the models parameters, our approach differs in that it internalizes analogical reasoning through model fine-tuning, allowing the model to generate reflective information during reasoning without relying on external data."
        },
        {
            "title": "3 MetaLadder",
            "content": "We first introduce the overall MetaLadder framework for enhancing mathematical problem-solving (Section 3.1). Then we present each our method in detail by explaining the generation of reflective data to guide the models reasoning process (Section 3.2), describing the composition of training data to activate the models analogical reasoning (Section 3.3). Besides, we also add selfevolve process in our framework to enable the models ability to autonomously generate data for self-improvement (Section 3.4). Finally, we also incorporate shortcut inference mechanism for fast and effective generation (Section 3.5)."
        },
        {
            "title": "3.1 Overview",
            "content": "The overview of MetaLadder framework is illustrated in Fig. 2. Given the original data consisting of target problem and its solution C. We first generate additional reflective data by synthesizing an problem analysis and solution strategy for the target problem, then along with analogous problem and its corresponding solution C, which are structurally or semantically similar to the original data. Moreover, we introduce problem-restating mechanism, where the target problem is inserted before the final solution to enhance the models understanding of the target problem. After training on the generated data sequence QSQCQC, the model is able to recall analogous problems, restate the target problem and apply analogical reasoning to find the final solution. Notably, because of MetaLadders analogical reasoning capability, the model can autonomously generate similar problems for training itself, which enables the self-evolution ability of the model. We now introduce the details in the following sections."
        },
        {
            "title": "3.2 Reflective Data Generation",
            "content": "MetaLadder improves the models mathematical problem-solving by incorporating reflective data during training. This approach simulates human learning, encouraging the model to recall and reflect on analogous problems, using their solutions to inform reasoning on the target problem. To achieve this, the model requires structured guidance to first understand the problem-solving strategy, then recall analogous problems, and finally 3 apply the solutions from those analogous problems to the current task. Therefore, the reflective data consists of three key components: 1) Problem Type and Solution Method S. Each problem is categorized into mathematical domain, with an explanation of the relevant concepts and methods (e.g., This is simple arithmetic problem involving addition and subtraction of discount. The solution method involves calculating the total cost of items and then applying the discount.). This helps the model grasp the problem-solving framework for future similar problems. 2) Analogous Problem Q. The model generates an analogous problem by modifying the context, numbers, or variables, while keeping the core structure intact, offering new learning context. For example, as shown in Fig 2: Original Problem: Michael buys his suit for $430 and shoes for $190. So, if he gets $100 discount, what was the amount that he paid to the seller? The generated analogy problem is then: Similar Problem: Sarah buys laptop for $850 and mouse for $50. If she receives $150 discount on her total purchase, how much does she pay in total? 3) Solution to Analogous Problem C. The model provides solution for the analogous problem, reinforcing the application of similar strategies across different problems. For instance, the solution to the above analogy problem is: 1. Calculate the total cost before the discount: 850 + 50 = 900. 2. Apply the discount: 900 - 150 = 750. Thus, the total amount Sarah pays is $750. To generate the above annotation data, we carefully design prompts for data generation. The detailed prompts are provided in Table 9."
        },
        {
            "title": "3.3 Analogical Reasoning Activation",
            "content": "To activate the analogical reasoning capabilities of the model, we compose the training data with the generated reflective data in format as described in Figure 1 (d). In traditional approaches, as shown in Fig. 1, CoT directly generates the solution from the problem Q, while question and answer augmentation directly rephrase the problem and solution into QC. RefAug (Zhang et al., 2024b) adds additional reflection after the solution C. In contrast, our MetaLadder introduces an analogical reasoning process SQCQ between and C, involving the generation of analogous problem and the transfer of knowledge from similar problem to the target problem. Specifically, our analogical reasoning process SQCQ consists of: Problem Type and Solution Method S. This is the problem and solution analysis, which is generated in Section 3.2. Analogical Problem and Corresponding Solution QC. This is the analogy problem and the solution of the analogy problem, which is also generated in Section 3.2. Problem-restating mechanism. After presenting QC, we reintroduce the original problem Q. This step ensures that the model revisits the target problem and apply the knowledge gained from solving the analogous problem, engaging in analogical reasoning and transferring the learned solution to solve the original problem. Overall, the enhanced training data format is QSQCQC: Original Problem Problem Type and Solution Strategy Analogous Problem and its Solution Original Problem and its Solution. By training on the above formatted data, we aim to improve the models mathematical reasoning by activating its analogical reasoning from similar problem and solution with deeper understanding of the target problem."
        },
        {
            "title": "3.4 Analogical Self-evolution",
            "content": "Similar to other works (Zelikman et al., 2022; Luong et al., 2024; Guan et al., 2025), after training, our model gains the ability to autonomously generate analogous problems that are related to the target problem. This capability facilitates self-evolving data augmentation process, where the model can iteratively bootstrap its own dataset. Specifically, after making predictions for given problem, the model is used to generate new problem instances based on its own outputs. These generated problems, being structurally or conceptually similar to the original ones, are then fed back into the training loop for further refinement. The self-evolution process significantly enhances the models ability to expand its knowledge base. As the model generates new problem instances, it creates novel variations of existing problems by modifying key componentssuch as numbers, variables, or contextswhile preserving the underlying structure. This process not only reinforces the models understanding of the problem-solving strategies but also improves its generalization ability across new, previously unseen problem types."
        },
        {
            "title": "3.5 Shortcut Inference",
            "content": "During training, the model learns to implicitly encode analogy-problem reasoning schemas via the 4 QSQCQC paradigm, where explicit generation of analogical problems and their solutions establishes robust neural pathways for structural pattern transfer. At inference time, we try to propose shortcut inference method that enables streamlined QSQC process that bypasses QC generation for fast inference. Specifically, after the model generates S, we force it to directly restate the original problem and output the answer by inserting after S. Surprisingly, skipping the QC generation not only reduces inference cost but also boost the performance significantly (see results in Section 4.2). This clearly demonstrates MetaLadder can successfully learn analogy knowledge transfer through analogy problem solving."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Datasets/Benchmarks. We use the training sets from GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) for our experiments. The augmented parts in each problem (problem type and solution method, the analogy problem, the solution to the analogy problem) are generated by GPT-4o-mini (OpenAI, 2024a). The details of the data generation process can be found in the Appendix A.1. For evaluation, besides the test sets from GSM8K and MATH as in-distribution evaluation, we also include out-of-distribution test sets from ASDiv (Miao et al., 2020), College Math (Tang et al., 2024), GaoKao (Chinese College Entrance Exam) En 2023 (Liao et al., 2024) and DM (Saxton et al., 2019) for verification. Training and Evaluation. We primarily use two popular LLMs for our experiments, covering the general focused LLM and the math-focused LLM: the widely used Llama3-8B (Zhang et al., 2024a) and DeepSeekMath-7B (Shao et al., 2024). For fair comparison, all the models are trained for 1 epoch. During inference, greedy decoding is applied to get the outputs. As for evaluation metrics, we report Pass@1 accuracy for all the models and baselines. More experimental details can be found in the Appendix A.2 and A.3. Baselines. We first introduce the following baseline methods that we adopt for comparison, which are also shown in Figure 1: (i) CoT: The original CoT data from GSM8K and MATH, which is the standard setting (Figure 1(a)). (ii) AnalogyAug: This combines our augmented analogy problem/- solution (as used in MetaLadder) with the original CoT data in batch-level training, making the training data twice as large as the original data (also known as question&answer augmentation, shown in Figure 1(b)). (iii) RefAug: state-of-the-art (SOTA) data augmentation method that enhances model reasoning by appending reflective data to the end of the CoT chain (Figure 1(c)). For our MetaLadder-based settings, we enhance the original CoT data as described in Section 3.3 to train 1) MetaLadder, the basic setting, and derive the following variant 2) MetaLadder + Selfevolve: We use the MetaLadder model after one round of training to greedily sample one data point from each problem, and then filter out the correct answers to add back to the training data used in the first round. Since we have generated the analogy problem for helping solve the target problem Q, we add reverse training setting in this section. Specifically, we train 3) MetaLadder + Reverse, which simply swaps the target problem and the analogous problem to be new training sample, expanding the training data to twice the size of the original data. Besides, we also experiment on variant, 4) MetaLadder + Reverse + Self-evolve, which further incorporates self-evolve data. Besides, for our MetaLadder-related experiments, we also include the shortcut inference mechanism as comparison. The \"-cut\" suffix after the method name indicates the use of the shortcut inference."
        },
        {
            "title": "4.2 Main Results",
            "content": "Our experimental results, as shown in Table 1, reveal the following key findings: MetaLadder Outperforms Strong Methods. The main experimental results demonstrate the effectiveness of the MetaLadder framework across multiple mathematical benchmarks. MetaLadder consistently outperforms baseline methods on both in-domain and out-of-domain datasets. On the LLaMA3-8B and DeepSeekMath-7B models, MetaLadder surpasses CoT by an average accuracy improvement of 6.7 (36.1 vs. 42.8) and 10.3 (47.3 vs. 57.6) points, respectively, and outperforms RefAug by 5.4 and 9.5 points in accuracy. These results highlight MetaLadders significant advantage in enhancing the models reasoning ability, particularly when tackling challenging mathematical problems. MetaLadder Enhances the Model Beyond BatchLevel Augmentation. Compared to AnalogyAug, which performs question and answer augmentation at the batch level, MetaLadder achieves higher scores on GSM8K and comparable performances 5 Method # Sample GSM8K MATH ASDiv CM GE DM Average In-Domain Out-of-Domain CoT (Wei et al., 2022) RefAug (Zhang et al., 2024b) RefAug+CoT (Zhang et al., 2024b) AnalogyAug MetaLadder MetaLadder-cut MetaLadder+Self-evolve MetaLadder+Self-evolve-cut MetaLadder+Reverse MetaLadder+Reverse+Self-evolve MetaLadder+Reverse+Self-evolve-cut CoT (Wei et al., 2022) RefAug (Zhang et al., 2024b) RefAug+CoT (Zhang et al., 2024b) AnalogyAug MetaLadder MetaLadder-cut MetaLadder+Self-evolve MetaLadder+Self-evolve-cut MetaLadder+Reverse MetaLadder+Reverse+Self-evolve MetaLadder+Reverse+Self-evolve-cut LLaMA3-8B 61.5 59.7 64.9 63.8 66.2 69.4 66.5 73.5 71.5 71.1 70.5 DeepSeekMath-7B 64.2 67.4 67.2 67.7 69.4 71.5 70.5 74.1 72.3 72.6 76.6 15K 15K 30K 30K 15K 15K 22K 22K 30K 44K 44K 15K 15K 30K 30K 15K 15K 23K 23K 30K 46K 46K 19.4 20.3 21.8 22.6 22.4 24.0 24.8 26.0 25.6 26.7 27.7 34.3 35.1 34.9 38.9 38.6 40.0 39.3 41.3 40.5 40.7 45.6 73.2 74.3 74.4 76.1 76.7 78.8 76.6 81.6 77.2 77.2 77.2 81.5 83.0 80.4 83.2 85.9 87.1 86.3 87.3 85.2 85.2 89.3 16.6 17.6 16.5 18.2 17.2 18.2 19.1 20.3 19.0 19.2 19.4 31.4 30.3 31.8 30.5 32.6 35.2 33.3 35.7 32.1 33.7 35. 19.5 18.4 21.0 20.8 23.9 26.0 26.8 24.4 24.7 27.3 28.3 29.4 35.6 29.4 36.9 37.4 40.5 35.1 40.5 37.4 38.2 43.1 26.3 23.2 24.8 27.2 29.7 29.4 31.7 30.7 29.0 31.4 31.9 43.0 43.5 45.1 49.6 48.4 49.1 50.8 50.5 51.3 51.9 54.8 36.1 35.6 37.4 38.1 39.4 41.0 40.9 42.8 41.2 42.2 42.5 47.3 49.2 48.1 51.1 52.1 53.9 52.6 54.9 53.1 53.7 57. Table 1: Accuracy on in-domain and out-of-domain mathematical benchmarks. The bold and underlined values represent the first and second best performances, respectively. CM, GE, DM denotes College Math, Gaokao En 2023, DeepMind-Mathematics, respectively. on MATH. On both models, MetaLadder improves by 4.7 and 6.5 points, respectively. This suggests that MetaLadder effectively enhances the models reasoning ability by activating its analogical reasoning capabilities, rather than simply adding more augmented data. Particularly, MetaLadder + Reverse, which swaps the target problem with analogous problems to double the dataset, outperforms AnalogyAug by 8.1 and 4.6 points on the two models, respectively. This further validates the effectiveness of MetaLadders data generation strategy. MetaLadders Self-Evolution Boosts Model Performance. Self-evolution provides further improvements in model performance, with significant gains observed across all datasets. After one round of self-evolution, MetaLadder+Selfevolve improves by 1.5 points on LLaMA3-8B and 0.5 points on DeepSeekMath-7B, demonstrating that single round of self-training effectively enhances the models reasoning ability. Additionally, MetaLadder+Reverse+Self-evolve improves by 1.0 points and 0.6 points on LLaMA38B and DeepSeekMath-7B in accuracy across all datasets, respectively, confirming the benefits of data augmentation through problem swapping. Ultimately, MetaLadder + Reverse + Self-evolve exceeds CoT by 6.1 points and RefAug by 4.8 points on LLaMA3-8B and achieves the best score of 53.7 on DeepSeekMath-7B. Shortcut Inference Reduces Inference Cost and Improves Performance. Surprisingly, shortcut inference not only reduces the inference cost by skipping the analogy problem reasoning during inference (e.g., as shown in Table 2, DeepSeekMetaLadder-cut on MATH achieves 1343.74 seconds, faster than DeepSeek-MetaLadders 2181.13 seconds and close to CoTs 1253.26 seconds), but also boosts the model performance by clear margin, e.g., 1.6 accuracy points on LLaMA3-8B and 1.8 points on DeepSeekMath-7B. The results demonstrate MetaLadder has transferred analogy problem-solving knowledge. These results underscore MetaLadders outstanding performance in solving both in-domain and outof-domain problems, further validating the frameworks effectiveness in enhancing mathematical problem-solving abilities and cross-domain generalization. Through its data augmentation and self-evolution strategies, MetaLadder not only excels in reasoning tasks within known domains but also demonstrates strong adaptability when facing unfamiliar data."
        },
        {
            "title": "5.2 Multi-round Self-evolve",
            "content": "To thoroughly evaluate the contribution of each component in the MetaLadder framework, we conducted an ablation study that systematically examined the impact of its core elements, where w/o Strategy, w/o Analogy, and w/o Restate refer to the absence of the problem type and solution method, analogy meta-problem, and problem restating mechanism, respectively."
        },
        {
            "title": "Method",
            "content": "GSM8K MATH Average w/o Strategy w/o Analogy w/o Restate"
        },
        {
            "title": "MetaLadder",
            "content": "64.9 64.7 61.6 66.2 22.2 21.0 22.0 22.4 43.6 42.9 41.8 44. Table 2: Ablation study on GSM8K and MATH, where w/o Strategy, w/o Meta-problem, and w/o Restate refer to the absence of the problem type and solution method, analogy meta-problem, and problem restating mechanism, respectively. As shown in Table 2, we observed that removing any of these components resulted in significant performance drop across both datasets. Specifically, removing the strategy component caused 0.7% average decrease in performance on both datasets, indicating that the strategy is important in guiding the model toward more accurate and efficient solutions. Furthermore, excluding the analogy meta-problem or the problem restating mechanism led to even greater performance degradation, with decreases of 1.4 and 2.5 points, respectively. This highlights the crucial role of these components in enhancing the models reasoning ability."
        },
        {
            "title": "5.1 The Amount of MetaLadder Data",
            "content": "We first investigate the impact of different amounts of MetaLadder-enhanced data on model performance across two models. As shown in Figure 3(a), in our experiments, we gradually replace the original CoT data with MetaLadder-enhanced data. As the proportion of MetaLadder-augmented data increased, the models performance steadily improves, reaching its peak when the original data is completely replaced with augmented data. This demonstrates the effectiveness and scalability of the augmentation method. To further explore the impact of multiple rounds of self-evolution, we examine the performance improvements with each additional round of selftraining. As shown in Figure 3(b), the performance of MetaLadder steadily improves with the increasing number of self-evolution rounds. On the LLaMA3-8B model, after one round of selfevolution, the average accuracy across all testsets increases from 39.4 to 40.9. After two rounds, the accuracy further improves to 41.7, with an increase of 2.3 points. On the DeepSeekMath-7B model, after two and three rounds of self-evolution, the average accuracy increases to 53.1 and 53.9, respectively, with improvements of 1.0 points and 1.8 points. This demonstrates that multi-round selfevolution significantly enhances the models reasoning ability. Further experimental results can be found in the Table 5. 5."
        },
        {
            "title": "Impact of Reflection on Train and Test",
            "content": "To investigate the impact of reflection on model performance, we compared the effects of performing reflection before and after generating the final answer. We train Pre-RefAug by shifting the reflection component of RefAug to the training stage, and Post-MetaLadder by placing the reflection component of MetaLadder after answer generation. As shown in Table 3, Pre-RefAug outperformed RefAug by 1.4 and 2.1 points on two models, while MetaLadder achieved scores 1.3 and 0.3 points higher than Post-MetaLadder. Our results demonstrate that allowing reflection before providing the final answer leads to further performance improvement. This suggests that reflection data not only enhances the models reflective capabilities during training but also guides its reasoning during testing in an in-context learning manner, ultimately boosting performance."
        },
        {
            "title": "5.4 Combine with Other Augmentation",
            "content": "We further investigate the effectiveness of our method when combined with other augmentation approaches. We compare the performance before and after applying the MetaLadder method on two augmented training sets: 1) In the AugCoT method, we enhance the original solutions using prompts that are almost identical to those used for generating MetaLadders analogous data, aiming to better align the distribution and complexity of the answers with the meta-problem. 2) We also use data gen7 Figure 3: Left: Performance of enhancing different amounts of original data. Middle: Results of 3 rounds of evolution on LLaMA and DeepSeek. Right: Combination of MetaLadder with AugCoT method. Method Train Inference QCR QC QRC QRC RefAug Pre-RefAug Post-Metaladder QQCSQC QQC MetaLadder QSQCQC QSQCQC LLaMA DeepSeek GSM8K MATH GSM8K MATH 59.7 61. 64.1 66.2 20.3 21.7 21.9 22.4 67.4 69.1 70.3 69.4 35.1 37. 37.1 38.6 Table 3: Comparison of model performance across different methods with varying placements of reflections before and after the answer. erated by the mainstream MetaMath method (Yu et al., 2024), which employs both question rephrasing and answer augmentation to generate more diverse set of problems. The results in Figure 3(c) demonstrate the effectiveness of the MetaLadder framework in improving the performance of DeepSeekMath-7B on mathematical benchmark tasks. Compared to the AugCoT baseline, AugCoT-MetaLadder shows more significant improvement across all datasets, with an average increase of 5.4 points in accuracy. This result suggests that the performance boost brought by MetaLadder is not mainly because of the enhanced data only, but rather due to the structured, reflective data generation process within the MetaLadder framework. Additionally, as shown in Table 6, we present the experimental results based on the augmented data from MetaMath20K and MetaMath40K. After being enhanced with MetaLadder, MetaMath20K-MetaLadder outperforms the original MetaMath20K with an average performance improvement of 1.3 points, which highlights the positive impact of MetaLadder on model accuracy. These results further suggest that combining MetaLadder with other augmentation methods can more effectively boost the models performance, demonstrating the potential of structured data augmentation in improving mathematical reasoning."
        },
        {
            "title": "5.5 Case Study",
            "content": "To have more straightforward understanding of the advantage of our MetaLadder, we show some cases and make discussions in this section. In case 4 (in Appendix, with more cases in Appendix C.), we examine the performance of MetaLadder and the standard CoT approach on root-finding problem. The problem is: Compute + + c, given that a, b, and are the roots of 1 + 5x2 = 6x 24. CoT solves the problem directly, focusing on converting the equation to 4x3 5x2 + 5x 1 = 0 and immediately applying Vietas formulas to obtain + + = 5 4 . This approach involves minimal abstraction and no explicit reuse of methods from other problems. It delivers direct result but provides limited insight into the generalization of the solution method. In contrast, the MetaLadder framework explicitly identifies this task as part of general class of polynomial-related problems and uses Vietas formulas as the backbone of the solution. In addition, it builds reusable methodology, highlights similarities with the original problem ( 1 + 5x2 = 6x 24), and emphasizes systematic computation techniques. The solution still delivers an accurate result (a + + = 5 4 ), but it also builds more structured understanding of the type of problem."
        },
        {
            "title": "This example highlights how MetaLadder can\nimprove the accuracy and reliability of the model\nin solving conceptually rich and broadly applicable",
            "content": "8 Example 5.1: Case in Math: Polynomial Roots Problem"
        },
        {
            "title": "Limitations",
            "content": "71 71 10 71 13+i 169240 10 + 5x2 = 6x 24. Question: Compute + + c, given that a, b, and are the roots of 1 CoT: We can rewrite the equation as 5x3 6x2 x+24 = 0. By the rational root theorem, the only possible rational roots are 1, 2, 3, 4, 6, 8, 12, and 24. We can check that = 2 is root, so 5x2 13x + 12 = 0. The roots of this quadratic are = 13 = 13 . Thus, the sum of the roots is 2 + = 3. = 13i 10 10 + 13i 10 The answer is 3 . MetaLadder: This is polynomial equation problem where we need to find the sum of the roots of polynomial. The solution method involves rearranging the equation to standard form and using Vietas formulas to find the sum of the roots. Similar Problem: Compute + + c, given that a, b, and + 4x2 = 5x 15. are the roots of 1 Solution to the Similar Problem: We can rewrite the equation as 4x3 5x2 + 5x 1 = 0. By Vietas formulas, the sum of the roots is 5 4 . Original Problem: ... Solution to the Original Problem: We can rewrite the equation as 5x3 6x2 + 6x 1 = 0. By Vietas formulas, the sum of the roots is 6 5 . Thus, the answer is 6 5 . Table 4: An example of model prediction on polynomial roots problem in MATH dataset, demonstrating the process of solving for the sum of the roots using algebraic manipulation and Vietas formulas. problems, further underscoring the value of reflective reasoning in enhancing the models overall problem-solving capabilities."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce MetaLadder, novel framework that enhances the mathematical problem-solving abilities of LLMs. By explicitly prompting the model to reflect on analogical problems and their solutions, MetaLadder enables it to transfer reasoning across similar tasks, mimicking human learning. Additionally, our problem-restating mechanism further enhances the models reasoning accuracy. Experimental results across multiple mathematical benchmarks demonstrate that MetaLadder significantly improves LLM performance, surpassing both standard Chain-of-Thought (CoT) methods and other state-of-the-art approaches. Our work highlights the importance of integrating analogical reasoning and meta-cognitive strategies into LLMs for complex reasoning tasks. 9 Although the MetaLadder framework has shown promising progress in mathematical problem solving, there are still some limitations worth further exploration and improvement. For instance, the performance of MetaLadder relies on the quality of the analogy problems and their corresponding solutions C. During the generation of analogy problems, data augmentation biases may be introduced, especially when the analogy problems are generated with strong reliance on certain problem types or solution methods. The model may overfit to common problem types or solutions present in the training data, potentially impacting its ability to generalize to novel problems. Future work could focus on improving the quality of generated analogy problems, enhancing the models ability to handle wider variety of problem types, and further investigating the trade-off between inference efficiency and reasoning depth."
        },
        {
            "title": "References",
            "content": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Kahneman Daniel. 2011. Thinking, fast and slow. Macmillan. Jenny Lynn Daugherty and Nathan Mentzer. 2008. Analogical reasoning in the engineering design process and technology education applications. Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, and Pengfei Liu. 2024. Reformatted alignment. Arxiv preprint, 2402.12219. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023. Specializing smaller language models towards multi-step reasoning. In ICML 2023. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. CoRR, abs/2312.10997. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843 3857. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic search-enhanced large reasoning models. Preprint, arXiv:2501.05366. Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. 2024. Mario: Math reasoning with code interpreter output reproducible pipeline. Preprint, arXiv:2401.08190. Haoxiong Liu, Yifan Zhang, Yifan Luo, and Andrew Chi-Chih Yao. 2024. Augmenting math word problems via iterative question composing. Preprint, arXiv:2401.09003. Loshchilov. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. 2024. Reft: Reasoning with reinforced fine-tuning. Preprint, arXiv:2401.08967. Eric Melz. 2023. Enhancing llm intelligence with armrag: Auxiliary rationale memory for retrieval augmented generation. Preprint, arXiv:2311.04177. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975984. OpenAI. 2024a. Gpt-4o mini: advancing cost-efficient intelligence. OpenAI. 2024b. Learning to reason with llms. Josh OpenAI, Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical Preprint, reasoning abilities of neural models. arXiv:1904.01557. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. 2024. Mathscale: Scaling instruction tuning for mathematical reasoning. In ICML. OpenReview.net. Qwen Team. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. 2024. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. In NeurIPS. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Interleaving retrieval with chain-of-thought reasoning for knowledgeIn Proceedings of intensive multi-step questions. the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Stella Vosniadou. 1988. Analogical reasoning as mechanism in knowledge acquisition: developmental perspective. Center for the Study of Reading Technical Report; no. 438. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. 2024a. Mathcoder: Seamless code integration in llms for enhanced mathIn The Twelfth International ematical reasoning. Conference on Learning Representations. 10 Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. 2024b. Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation. Preprint, arXiv:2403.05313. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024. Metamath: Bootstrap your own mathematical questions for large language models. In ICLR 2024. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems, volume 35, pages 1547615488. Curran Associates, Inc. Di Zhang, Jiatong Li, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. 2024a. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394. Zhihan Zhang, Tao Ge, Zhenwen Liang, Wenhao Yu, Dian Yu, Mengzhao Jia, Dong Yu, and Meng Jiang. 2024b. Learn beyond the answer: Training language models with reflection for mathematical reasoning. In EMNLP, pages 1472014738. Association for Computational Linguistics. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022. Least-to-most prompting enables complex reasonarXiv preprint ing in large language models. arXiv:2205.10625."
        },
        {
            "title": "A Experimental Details",
            "content": "A.1 Annotation Details Data annotation was performed using the GPT-4omini-2024-07-18 model API with sampling temperature of 0.7. The full prompt used for annotation is shown in Block 9. In Experiment 6, we use prompt shown in Block 10 to rephrase the original solutions in GSM8K and MATH. A.2 Train Details Model training was conducted using the LLaMA Factory 1 on 8 NVIDIA A100 GPUs. We train all models for one epoch with batch size of 128, using the AdamW optimizer (Loshchilov, 2017) with learning rate of 5e-6 and cosine learning rate decay. The training prompt is shown in Block 11. A.3 Evaluation Details GSM8K (Cobbe et al., 2021): GSM8K consists of grade-school arithmetic tasks with relatively low difficulty, primarily used to evaluate basic mathematical reasoning abilities. The testset includes 1319 basic math word problems, covering simple arithmetic operations such as addition, subtraction, multiplication, and division. Compared to other more complex datasets, the problems in GSM8K are straightforward and suitable for testing models performance on solving basic mathematical problems. MATH (Hendrycks et al., 2021): The MATH testset contains 5000 challenging competition-level math problems. These problems are designed to be complex and require the model to possess higherlevel mathematical reasoning capabilities, far surpassing the simpler problems found in GSM8K. MATH spans multiple mathematical domains, including algebra, geometry, and number theory, making it an ideal benchmark for evaluating models performance on complex mathematical reasoning tasks. ASDiv (Miao et al., 2020): ASDiv (Academia Sinica Diverse MWP Dataset) is diverse English math word problem dataset intended to evaluate the capabilities of various MWP solvers. This dataset includes 2,305 math word problems that cover wide range of language patterns and problem types, offering more diversity than existing MWP datasets. It includes problems commonly found in elementary school and is annotated with problem types 1https://github.com/hiyouga/LLaMA-Factory and grade levels to help assess the difficulty and complexity of each problem. Gaokao 2023 EN (Liao et al., 2024): Gaokao 2023 EN contains 385 math problems from the 2023 Chinese National College Entrance Examination (Gaokao), which are primarily high schoollevel open-ended problems. These problems cover wide range of mathematical topics and include content taught during high school in China. The Gaokao EN2023 dataset is designed to assess students ability to apply mathematical reasoning in real-world situations, containing both basic problems and more complex ones. It serves as an important benchmark for evaluating models performance on Gaokao-style math problems. CollegeMath (Tang et al., 2024): The CollegeMath dataset includes 2,818 college-level math problems extracted from 9 textbooks, spanning 7 mathematical domains such as linear algebra and differential equations. CollegeMath is designed to test models ability to reason across diverse mathematical topics, with particular focus on generalization to complex mathematical reasoning tasks at the college level. The problems are more difficult, making the dataset well-suited for evaluating models ability to solve advanced mathematical problems. DeepMind-Mathematics (Saxton et al., 2019): The DeepMind-Mathematics test set containing 1000 problems from variety of problem types, based on national school mathematics curriculum (up to age 16), designed to assess basic mathematical reasoning across different domains. The dataset generates question and answer pairs of varying types, generally at school-level difficulty, and aims to test the mathematical learning and algebraic reasoning abilities of learning models."
        },
        {
            "title": "All",
            "content": "model evaluation was provided carried at the using framework out https://github.com/ZubinGou/math-evaluationharness/tree/main with zero-shot evaluation, greedy sampling and maximum generation length of 2048 tokens. To validate that the improvement from the shortcut method was not due to avoiding the truncation of MetaLadders output, we also conducted the main experiment with maximum length of 4096 tokens, and no significant changes in the metrics were observed."
        },
        {
            "title": "B More Experiments",
            "content": ""
        },
        {
            "title": "Method",
            "content": "In-Domain # Sample GSM8K MATH ASDiv Out-of-Domain"
        },
        {
            "title": "DM Average",
            "content": "MetaLadder MetaLadder+Self-evolve MetaLadder+Self-evolve2 MetaLadder+Self-evolve3 MetaLadder MetaLadder+Self-evolve MetaLadder+Self-evolve2 MetaLadder+Self-evolve3 15K 22K 30K 38K 15K 23K 31K 40K LLaMA3-8B 22.4 24.8 25.1 25.9 66.2 66.5 68.9 69.2 DeepSeekMath-7B 69.4 70.5 71.9 72. 38.6 39.3 39.8 40.6 76.7 76.6 79.3 77.9 85.9 86.3 86.1 86.3 17.2 19.1 19.1 19.8 32.6 33.3 33.6 34.0 23.9 26.8 25.7 26. 37.4 35.1 37.9 38.7 29.7 31.7 32.2 31.6 48.4 50.8 49.2 51.5 39.4 40.9 41.7 41.7 52.1 52.6 53.1 53.9 Table 5: Accuracy of self-evolution on in-domain and out-of-domain mathematical benchmarks. The bold and underlined values represent the first and second best performances, respectively. CM, GE, DM denotes College Math, Gaokao En 2023, DeepMind-Mathematics, respectively."
        },
        {
            "title": "Method",
            "content": "GSM8K MATH ASDiv CM GE DM Average MetaMath20K MetaMath20K-MetaLadder MetaMath40K MetaMath40K-MetaLadder AugCoT AugCoT-MetaLadder 71.1 72.9 73.9 75.7 65.3 75.5 38.4 39.8 38.5 40.0 40.7 42.8 84.5 86.5 84.6 87.0 73.5 85.9 32.0 31.6 32.0 30.5 30.0 35. 32.5 34.3 34.3 34.8 35.1 38.7 46.9 44.9 48.2 45.8 54.0 52.8 50.9 51.7 51.9 52.3 49.8 55.2 Table 6: DeepSeekMath-7B performance on two augmented datasets. MetaMath20K constructed by uniformly sampling half of the data from MetaMath40K, and AugCoT representing original solutions rephrased by 4o-mini to match the style of analogous data used in MetaLadder."
        },
        {
            "title": "Cos",
            "content": "JD LD GSM8K MATH"
        },
        {
            "title": "1.00\nOriginal Problem\nAnalogous Problem\n0.93\nEnhanced Analogous Problem 0.91",
            "content": "0.00 0.48 0.61 0.00 101.00 131.30 64.0 66.2 65.3 21.1 22.4 23.3 Table 7: The impact of repeating the original problem and using analogous problems with greater divergence on model performance. Cos refers to Cosine Similarity, JD refers to Jaccard Distance, and LD refers to Levenshtein Distance."
        },
        {
            "title": "Method",
            "content": "GSM8K MATH LLaMA-CoT LLaMA-MetaLadder LLaMA-MetaLadder-cut DeepSeek-CoT DeepSeek-MetaLadder DeepSeek-MetaLadder-cut 61.86 172.62 111.26 65.9 210.15 113.96 749.26 1479.85 1081. 1253.26 2181.13 1343.74 Table 8: Total time cost of inference on the whole GSM8K and MATH testsets, in seconds. Our MetaLadder, combined with shortcut reasoning, significantly reduces inference time, achieving speeds close to CoT."
        },
        {
            "title": "Data Annotation Prompt",
            "content": "You are professional math teacher, and your goal is to teach your student to learn given math problem. Identify the type of the given problem and the name of the solution method. Then, generate similar problem with its solution. ## Example 1: ### Original Problem: Youngsville had population of 684 people. The town had growth spurt and the population increased by 25% then they witnessed that 40% of the population moved away. What is the current population? ### Solution to the Original Problem: The town had 684 people, and then had 25% growth spurt, so the population increased by 684 * 0.25 = 171 people. This increase brought the population to 684 + 171 = 855 people. 40% of the population moved away, so 855 * 0.40 = 342 people moved away. The new population is 855 - 342 = 513 people. The answer is 513. ### Type of Problem and Solution Method: This is consecutive percentage change problem. The solution method involves applying the growth factor first, then applying the reduction factor. The key to solve the problem is to understand the concept of relative increase and decrease percentages. ### Similar Problem: company started with 800 employees. After one year, the workforce increased by 20%, and then 30% of the new workforce decided to quit due to relocation. How many employees remain at the company? ### Solution to the Similar Problem: 1. Initial employees: 800. 2. Increase by 20%: 20% of 800 is 160, so the workforce grows to 800 + 160 = 960. 3. Then, 30% of these 960 employees quit: 30% of 960 is 288, so 288 employees leave. 4. Remaining employees: 960 - 288 = 672. Therefore, the final number of employees is 672. ## Example 2: ### Original Problem: Solve the equation (x - 99)(x - 101) = 8. ### Solution to the Original Problem: Let t=x-100. Then the equation becomes (t - 1)(t + 1) = 8, which transforms into t2 - 1 = 8. Therefore, = 3 or = -3, and accordingly we get = 97 or = 103. ### Type of Problem and Solution Method: This is quadratic equation problem solved by introducing substitution to simplify the expression. The solution method involves recognizing suitable substitution that transforms the equation into simpler form. The key to solving the problem is understanding how to use algebraic manipulation to facilitate solving equations. ### Similar Problem: Solve the equation (x - 50)(x - 52) = 4. ### Solution to the Similar Problem: 1. Notice that the middle point between 50 and 52 is 51, so let = - 51. 2. Then (x - 50) = (t + 1) and (x - 52) = (t - 1). 3. The equation becomes (t + 1)(t - 1) = 4 t2 - 1 = 4 t2 = 5 = 4. Substituting back: = + 51. 5. - If = - If = 5. Hence, the solutions are = 51 + 5, then = 51 + 5, then = 51 5 or = 51 5. 5 or = 5. ### Original Problem: {problem} ### Solution to the Original Problem: {solution} Table 9: Data annotation prompt for generating and solving similar math problems. The prompt provides structured approach for identifying problem types, solution methods, and creating analogous problems with detailed solutions."
        },
        {
            "title": "Answer Augmentation Prompt",
            "content": "You are professional math teacher, and your goal is to teach your student to learn given math problem. Identify the type of the given problem and the name of the solution method. Then, generate similar problem with its solution. ## Example 1: ### Problem: Youngsville had population of 684 people. The town had growth spurt and the population increased by 25% then they witnessed that 40% of the population moved away. What is the current population? ### Original Solution: The town had 684 people, and then had 25% growth spurt, so the population increased by 684 * 0.25 = 171 people. This increase brought the population to 684 + 171 = 855 people. 40% of the population moved away, so 855 * 0.40 = 342 people moved away. The new population is 855 - 342 = 513 people. The answer is 513. ### Augmented Solution: 1. Initial population: 684. 2. Growth by 25%: 25% of 684 is 171, so the population increases to 684 + 171 = 855. 3. 40% of 855 moved away: 40% of 855 is 342, so 342 people moved away. 4. The remaining population is 855 - 342 = 513. Therefore, the final population is 513. ## Example 2: ### Problem: Solve the equation (x - 99)(x - 101) = 8. ### Original Solution: Let = 100. Then the equation becomes (t 1)(t + 1) = 8, which transforms into t2 1 = 8. Therefore, = 3 or = 3, and accordingly we get = 97 or = 103. ### Augmented Solution: 1. Notice that the middle point between 99 and 101 is 100, so let = - 100. 2. Then (x 99) = (t + 1) and (x 101) = (t 1). 3. The equation becomes (t + 1)(t 1) = 8 t2 1 = 8 t2 = 9 = 3ort = 3. 4. Substituting back: = + 100. - If = 3, then = 100 + 3. - If = -3, then = 100 - 3. Hence, the solutions are = 103 or = 97. ### Problem: {problem} ### Original Solution: {solution} Table 10: Answer augmentation prompt designed for creating detailed step-by-step solutions to math problems.In the AugCoT method, we enhance the original solutions using this prompt, which is almost identical to those used for generating MetaLadders analogous data, aiming to better align the distribution and complexity of the answers with the meta-problem."
        },
        {
            "title": "Training and Evaluation Prompt",
            "content": "### Instruction: {question} ### Response: Lets think step by step. Table 11: Training and evaluation prompt designed to guide the model in generating step-by-step responses to math problems."
        },
        {
            "title": "C More cases",
            "content": "We present more cases in this section to show the generated predictions of our MetaLadder trained model. 17 Example C.1: Case in MATH Question: Exits on highway are numbered consecutively from 1 to 50. The distance from exit 41 to exit 50 is 100 km. If each exit is at least 6 km from the next exit, what is the longest possible distance, in kilometers, between exit 47 and exit 48? CoT: The distance between exit 41 and exit 50 is 100 km. distance between exit 47 and exit 48 is at least 12 km. The longest possible distance between exit 47 and exit 48 is 12 km. Final Answer: 12 . If each exit is at least 6 km from the next exit, the MetaLadder: This is problem involving the arrangement of objects (exits) with constraints on the distances between them. The solution method involves setting up inequalities based on the given conditions and then solving for the maximum distance between two specific exits. Similar Problem: Exits on highway are numbered consecutively from 1 to 40. The distance from exit 31 to exit 40 is 80 km. If each exit is at least 5 km from the next exit, what is the longest possible distance, in kilometers, between exit 37 and exit 38? Solution to the Similar Problem: 1. The distance from exit 31 to exit 40 is 80 km, which means the distance from exit 31 to exit 32 is at least 5 km. 2. The distance from exit 32 to exit 33 is at least 5 km, and so on. 3. The total distance from exit 31 to exit 38 is: From exit 31 to exit 32: 5 km From exit 32 to exit 33: 5 km From exit 33 to exit 34: 5 km From exit 34 to exit 35: 5 km From exit 35 to exit 36: 5 km From exit 36 to exit 37: 5 km From exit 37 to exit 38: 5 km 4. Therefore, the total distance from exit 31 to exit 38 is: 5. The distance from exit 38 to exit 40 is: 6. The distance from exit 37 to exit 38 is: 5 km 7 = 35 km 80 km 35 km = 45 km 45 km 5 km = 40 km Thus, the longest possible distance between exit 37 and exit 38 is **40 km**. Original Problem: Exits on highway are numbered consecutively from 1 to 50. The distance from exit 41 to exit 50 is 100 km. If each exit is at least 6 km from the next exit, what is the longest possible distance, in kilometers, between exit 47 and exit 48? Solution to the Original Problem: The distance from exit 41 to exit 50 is 100 km, which means the distance from exit 41 to exit 42 is at least 6 km. The distance from exit 42 to exit 43 is at least 6 km, and so on. The total distance from exit 41 to exit 48 is: From exit 41 to exit 42: 6 km From exit 42 to exit 43: 6 km From exit 43 to exit 44: 6 km From exit 44 to exit 45: 6 km From exit 45 to exit 46: 6 km From exit 46 to exit 47: 6 km From exit 47 to exit 48: 6 km Therefore, the total distance from exit 41 to exit 48 is: 6 km * 7 = 42 km The distance from exit 48 to exit 50 is: 100 km - 42 km = 58 km The distance from exit 47 to exit 48 is: 58 km - 6 km = 52 km Final Answer: 52 . 18 Example C.2: Case in MATH: Trigonometry Question: Find csc(120). CoT: We have csc(120) = 1 sin(120) = Final Answer: 2 3 . 1 sin(120) = 1 3 3 2 3 . = MetaLadder: This is trigonometric function evaluation problem. The solution method involves using the properties of the cosecant function and the reference angle to find the value of the cosecant at given angle. Similar Problem: Find csc(135). Solution to the Similar Problem: 1. The angle 135 is in the third quadrant, where the cosecant function is negative. 2. The reference angle for 135 is 135 180 = 45. 3. The cosecant of 45 is csc(45) = 4. Therefore, csc(135) = 2. 2. Thus, the final answer is 2 . Original Problem: Find csc(120). Solution to the Original Problem: The angle 120 is in the third quadrant, where the cosecant function is negative. The reference angle for 120 is 120 180 = 60. The cosecant of 60 is csc(60) = 2 . Therefore, csc(120) = 2 3 . Final Answer: 2 3 . 19 Example C.3: Case in GSM8K: Algebra Question: football team played 22 games. They won 8 more than they lost. How many did they win? CoT: If they won 8 more than they lost, then they lost 22-8=14 games. So they won 22-14=8 games. Final Answer: 8 . MetaLadder: This is problem involving system of equations based on relationships between wins, losses, and total games played. The solution method involves setting up equations based on the given information and solving for the unknowns. Similar Problem: basketball team played 30 games. They won 10 more than they lost. How many did they win? Solution to the Similar Problem: 1. Let the number of games won be w. 2. The number of games lost is 10. 3. The total number of games played is 30, so we have the equation: 4. Simplifying this gives: 5. Adding 10 to both sides: 6. Dividing by 2: Thus, the team won 20 games. + (w 10) = 2w 10 = 30 2w = 40 = 20 Original Problem: football team played 22 games. They won 8 more than they lost. How many did they win? Solution to the Original Problem: Let the number of games won be w. The number of games lost is 8. The total number of games played is 22, so we have the equation: Final Answer: 15 . + (w 8) = 22 2w 8 = 22 2w = 30 = 15 20 Example C.4: Case in Math Question: Simplify [ 1 1 tan2 + 1 1 cot2 ]. CoT: We have[ 1 1 tan2 + 1 1 cot2 ] = 1 cos2 sin2 + 1 sin2 cos2 = 1 cos2 sin2 1 cos2x sin2x = ]. Final Answer: 0 MetaLadder: This is trigonometric simplification problem. The solution method involves using the Pythagorean identities to express the given expression in terms of sine and cosine, and then simplifying the resulting expression. Similar Problem: 1 Simplify [ 1 sin2 + 1 1 cos2 ]. Solution to the Similar Problem: Using the Pythagorean identity, we have 1 sin2 = cos2 and 1 cos2 = sin2 x. Thus, 1 cos2 + 1 sin2 = sin2 + cos2 sin2 cos2 = 1 sin2 cos2 . Original Problem: (cid:20) 1 1 tan2 Simplify + 1 1 cot2 (cid:21) . 1 1 sin2 + 1 1 cos2 = Solution to the Original Problem 1. Start with the given expression: [ 1 1 tan2 2. Use the Pythagorean identity: [1 tan2 = 3. Substitute these identities into the expression: ]. + 1 1 cot2 1 cos2 1 1 cos2 + 1 1 sin2 and 1 cot2 = 1 sin2 ]. = cos2 + sin2 x. 4. Since cos2 + sin2 = 1, we have: cos2 + sin2 = 1. 5.Final Answer: 1 21 Example C.5: Case in MATH: Exponentiation Question: Compute (23)(55) in your head. CoT: We have (23)(55) = (23)(53)(52) = (103)(25) = 100000. Final Answer: 100000 . MetaLadder: This is multiplication problem involving powers of different bases. The solution method involves simplifying the expression by breaking it down into smaller parts and then multiplying the results. Similar Problem: Compute (32)(43) in your head. Solution to the Similar Problem: 1. First, calculate 32: 32 = 9 2. Next, calculate 43: 43 = 4 4 4 = 16 4 = 3. Now, multiply the results: 9 64 = 576 Thus, the final answer is 576 . Original Problem: Compute (23)(55) in your head. Solution to the Original Problem: We have 23 = 8 and 55 = 5 25 25 = 5 625 = 3125. Thus, the answer is 8 3125 = 25000 . Final Answer: 25000 . Example C.6: Case in GSM8K: Basic Algebra Question: treasure hunter found buried treasure chest filled with gems. There were 175 diamonds, 35 fewer rubies than diamonds, and twice the number of emeralds than the rubies. How many of the gems were there in the chest? CoT: There were 175 35 = 140 rubies. There were 140 2 = 280 emeralds. There were 175 + 140 + 280 = 600 gems in the chest. Final Answer: 600 . MetaLadder: This is problem involving basic arithmetic operations (addition and multiplication) to find the total number of items based on given relationships. The solution method involves calculating the quantities of each type of gem based on the relationships provided. Similar Problem: collector found collection of antique coins. There were 200 silver coins, 40 fewer gold coins than silver coins, and three times the number of copper coins than the gold coins. How many coins were there in total? Solution to the Similar Problem: 1. Silver coins: 200. 2. Gold coins: 200 40 = 160. 3. Copper coins: 160 3 = 480. 4. Total coins: 200 + 160 + 480 = 840. Therefore, the total number of coins in the collection is 840 . Original Problem: treasure hunter found buried treasure chest filled with gems. There were 175 diamonds, 35 fewer rubies than diamonds, and twice the number of emeralds than the rubies. How many of the gems were there in the chest? Solution to the Original Problem: There were 175 35 = 140 rubies. There were 140 2 = 280 emeralds. There were 175 + 140 + 280 = 595 gems in the chest. Final Answer: 595 ."
        }
    ],
    "affiliations": [
        "Renmin University of China",
        "Shanghai AI Laboratory",
        "Tsinghua University"
    ]
}