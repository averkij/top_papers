{
    "paper_title": "Scaling Image Tokenizers with Grouped Spherical Quantization",
    "authors": [
        "Jiangtao Wang",
        "Zhen Qin",
        "Yifan Zhang",
        "Vincent Tao Hu",
        "Björn Ommer",
        "Rania Briq",
        "Stefan Kesselheim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 2 3 6 2 0 . 2 1 4 2 : r Scaling Image Tokenizers with Grouped Spherical Quantization Jiangtao Wang1 , Zhen Qin2, Yifan Zhang3, Vincent Tao Hu4, Björn Ommer4, Rania Briq1, Stefan Kesselheim Jülich Supercomputing Centre1, TapTap2, Tsinghua University3, CompVis @ LMU Munich, MCML4 Training Code & Checkpoints"
        },
        {
            "title": "Abstract",
            "content": "Vision tokenizers have gained lot of attraction due to their scalability and compactness; previous works depend on old school GAN-based hyperparameters, biased comparisons, and lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQspecifically in latent dimensionality, codebook size, and compression ratiosand their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As result, GSQ-GAN achieves 16 down-sampling with reconstruction FID (rFID) of 0.50."
        },
        {
            "title": "1 Introduction",
            "content": "Recent advancements in generative models for images and videos have seen substantial success, with approaches like autoregressive models Sun et al. (2024); Kondratyuk et al. (2024); Wang et al. (2024b), masked language models Yu et al. (2024b; 2023); Chang et al. (2022); Weber et al. (2024), and diffusion-based methods (including score-matching and flow-matching) Rombach et al. (2022); Yang et al. (2024); Hu et al. (2024); Gao et al. (2024) and surpass GAN-based Kang et al. (2023); Sauer et al. (2023) models. common factor in many of these models is the reliance on latent discrete representations of images, especially within language-model-based, where continuous feature maps are quantized into discrete tokens. This quantization has become critical for high-fidelity generation, as tokenized images facilitate model efficiency and enhance generative quality, avoiding the need to work on high-resolution images directly. Recent studies Yu et al. (2024b); Wang et al. (2024b) confirm image tokenizer directly translates to generative quality, and the effectiveness of generative models is closely tied to the performance of image tokenizers. The fundamental challenge in training image tokenizers is balancing compression efficiency with reconstruction accuracy. While recent methods show progress, several critical issues remain unresolved: (1) Many current tokenizers still depend on outdated GAN-based hyperparameters, often resulting in suboptimal, even negative, performance due to inconsistencies between generation and reconstruction objectives. (2) Benchmarking efforts frequently rely on legacy VQ-GAN implementations with outdated configurations, leading to biased comparisons and limited assessment accuracy. (3) Although various quantization models 1 (a) Reconstruction performance of GSQ with latent dimension of 16 at 16ˆ spatial compression, compared to the state-of-the-art. (b) Scaling behaviour of the latent dimension v.s. spatial compression factor in GSQ; 16 is fixed while groups increase to expand latent space. Figure 1: The top figure shows GSQ-GANs reconstruction performance compared to state-of-the-art methods, demonstrating superior results even without latent decomposition. Training with larger G, which is more composed of groups, can further optimize the use of latent space, enhancing reconstruction quality. The bottom figure illustrates GSQ-GANs efficient scaling behaviour, where expanded latent capacity effectively manages increased spatial compression, thus achieving higher fidelity reconstructions on highly spatial compressed latent. Notably, GSQ-GAN achieves these results with only 20 training epochs on ImageNet at 2562 resolution, while methods, such as Luo et al. (2024); Yu et al. (2024b), require over 270 epochs. have been introduced, comprehensive analyses of their relative performance and scalability are limited, hindering the development of efficient, streamlined training methodologies for image tokenizers. Additionally, some methods, such as FSQ Mentzer et al. (2024) and LFQ Yu et al. (2024b), rigidly bind latent dimension and codebook size, making independent scaling of either latent dimension or codebook size infeasible. To address these challenges, we propose the following contributions: 1. Grouped Spherical QuAnTization (GSQ): We introduce novel approach featuring spherical codebook initialization and lookup regularization. With optimised configurations, GSQ outperforms state-of-the-art image tokenizers, achieving high performance with fewer training steps and without the need for auxiliary losses or GAN regularization. 2. Efficient Latent Space Utilization: GSQ achieves superior reconstruction performance with compact latent dimensions and large codebook sizes. Scaling studies reveal that latent space is often un2 derutilized in lower spatial compression scenarios, underscoring the need for efficient latent space usage, which GSQ can address. 3. Scalability with Latent Dimensions: GSQ scales effectively with increasing latent dimensions by decomposing and grouping latents. Our spatial scaling studies indicate that latent space saturation occurs at larger spatial reduction scenarios. GSQ enables greater spatial reductions and leverages an expanded latent space to maximize the quantizers capacity. These insights lay foundation for more efficient and scalable training protocols in image tokenizers, advancing the potential of downstream tasks such as generative models for high-fidelity image generation tasks. We also demonstrate that our training approach can easily train up to 32ˆ spatial downsampling image tokenizer."
        },
        {
            "title": "2 Related Work",
            "content": "The Variational AutoEncoder Kingma (2013) is the foundational approach for image tokenization, initially developed to compress images into continuous latent space, while later one more work focuses on refining continuous representations Higgins et al. (2017); Vahdat & Kautz (2020); Kim et al. (2019); Luhman & Luhman (2022); Bhalodia et al. (2020); Egorov et al. (2021); Su & Wu (2018); Qin & Huang (2024). Despite their strengths, however, these image encodings, often constrained by strong KL regularization, are rarely applied as image tokenizers within generative models. Instead, the VAE with vector quantization (VQ-VAE) Van Den Oord et al. (2017); Razavi et al. (2019) have become the preferred choice due to their effective use of codebook for latent distribution regularization. Alternative variance is Residual Vector Quantizer (RVQ) Zeghidour et al. (2021) that can achieve image compression and discrete quantization simultaneously. Building on the success of VQ-VAE, the VQ-GAN model Esser et al. (2021) further advanced image tokenizer training by incorporating perceptual loss Zhang et al. (2018) and an adversarial loss, enhancing the quality of generated images. Subsequent research has extended VQ-GAN through (1) architectural improvements, such as transformer-based structures Yu et al. (2022) and Layer Normalization Chang et al. (2022); (2) novel vector quantizers like Finite Scalar Quantization Mentzer et al. (2024), Lookup-Free Quantizer Yu et al. (2024b) and so on Zhao et al. (2024); Zheng et al. (2022a); Zhu & Soricut (2024); Sadat et al. (2024); Adiban et al. (2023); Yu et al. (2024a); Cao et al. (2023); You et al. (2022); Lee et al. (2022); Adiban et al. (2022); Kumar et al. (2024); Zheng et al. (2022b); Kumar et al. (2024); Li et al. (2024); Luo et al. (2024); Tian et al. (2024); Fifty et al. (2024); and (3) refined loss functions with perceptual enhancements, for example, using ResNet-based perceptual loss Weber et al. (2024); Yu et al. (2023) and incorporating StyleGAN discriminators Yu et al. (2022; 2024b). Our work primarily focuses on this stream of compression-oriented image tokenizer training, examining scaling behaviours and their influence on reconstruction quality. An alternative line of research in image tokenization focuses on embedding semantic visual representations in the latent space, rather than maximising compression rates. This approach typically leverages pretrained visual foundation models, such as DINO Oquab et al. (2024), CLIP Radford et al. (2021), and MAE He et al. (2022), by transferring their learned representations into the latent space of image tokenizers or quantizing their latent representations. Early studies Peng et al. (2022); Hu et al. (2023); Park et al. (2023) demonstrated the feasibility of this strategy, though these models traditionally underperform in reconstruction quality compared to compression-driven tokenizers. Recent advancements have narrowed this gap by optimizing codebook initialization, refining network architectures, and employing advanced knowledge distillation methods, resulting in models that achieve competitive reconstruction fidelity while preserving strong semantic representation capabilities Yu et al. (2024c); Zhu et al. (2024a;b); Li et al. (2024)."
        },
        {
            "title": "3.1 Preliminary: VQ Image Tokenizer",
            "content": "The image tokenizer consists of an encoder Enc and decoder Dec. The encoder compresses the highresolution input image RHˆW ˆ3 into continuous latent maps: EncpIq tzi RDuhˆw i1 . (1) and the decoder reconstructs the image from the latent representation, ˆI DecpZq. The down-sampling factor denotes spatial reduction, and the compression ratio is given by 3f 2 , where H, is the height and width of input image and h, w, is the height, width and dimension of latent. With vector quantizer, the latent space is discretised by mapping to indices in the codebook tci RDuV i1, where is the vocabulary size. Each latent vector zi from is quantized to the nearest codebook entry using look-up operation, often based on Euclidean distance: VQpziq lookuppzi, Cq arg min zi cj2. (2) 3.2 Simple Scaling with GSQ Pursuing higher spatial reduction requires increasing the latent dimensionality to maintain R, thus preserving reconstruction fidelity. However, increasing introduces high-dimensionality challenges, making distance computations less effective and limiting achievable compression ratios. One of the solutions is using product quantizer Vahdat & Kautz (2020); Zheng et al. (2022a;b); Jegou et al. (2010), hence we decompose each latent vector zi into groups: GSQpziq tlookuppzpgq , CpgqquG g1, (3) Here, each zpgq represents sub-group of zi with channels, where ˆ enables efficient compression without compromising reconstruction fidelity. To improve stability and performance, we propose to initialize codebook entries from spherical uniform distribution and same as Yu et al. (2022); Zhao et al. (2024), apply ℓ2 normalization during lookup: cpgq ℓ2pN p0, 1qq, lookuppzi, Cq arg min ℓ2pziq ℓ2pcjq2. (4) (5) We employ shared codebook among all groups and omit ℓ2 when G{D t1, 2u, in which case GSQ reduces to LFQ Yu et al. (2024b), and the spherical space significantly collapsed, which requires additional entropy loss during training Yu et al. (2024b); Zhao et al. (2024). Further discussion is provided in Appendix C."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Optimized Training for GSQ-VAE We first investigate the efficacy of our proposed improvements to GSQ on VAE-based tokenizers, including impacts of training configurations, auxiliary losses, model architecture, and hyperparameter settings. We set 1 for all modes, they were trained on 1282 resolution ImageNet Deng et al. (2009) with down-sampling factor 8, vocabulary size 8,192, latent dimensionality 8, with batch size 256, and learning rate of 1e4 for 100k steps (20 epochs). Specific hyperparameters are reported in Appendix D. All tokenizers adopted an exponential moving average with decay rate of 0.999. We utilized the LPIPS perceptual loss Zhang et al. (2018) as proposed in Esser et al. (2021) with weight of 1.0 in training. 4 Codebook Init Norm rFID Ó Up1{V, 1{V Up1{V, 1{V ℓ2pN p0, 1qq ℓ2pN p0, 1qq ℓ2pN p0, 1qq ℓ ℓ1 ℓ2 11.37 5.343 5.343 8.312 5.375 IS Ò LPIPS Ó PSNR Ò SSIM Ò Usage Ò PPL Ò 84 113 113 94 113 3.38% 100% 100% 33.9% 100% 22.3 23.7 23.9 22.1 23.59 237 8077 7408 566 0.64 0.71 0.72 0.66 0.71 0.12 0.10 0.12 0.12 0.11 Table 1: Ablation of spherical codebook initialization and lookup normalization for GSQ-VAE-F8 models, trained on ImageNet with 1282 resolution for 20 epochs. PPL is the perplexity."
        },
        {
            "title": "4.1.1 Effectiveness of Spherical Quantization",
            "content": "Baseline and codebook initialization. table 1 demonstrates that our spherical uniform distribution codebook initialization significantly improved codebook usage to nearly 100% during training. Using ℓ2 normalization, mentioned with previous studies Yu et al. (2022); Zhao et al. (2024), is crucial for stabilizing codebook usage (especially in larger codebooks) and ensuring all codes are usually equal. As illustrated in fig. 7, our approach maintained approximately 100% codebook utilization throughout training, which enabled the reduction of the rFID from 11.37 to 5.375, and with ℓ2 the perplexity of codebook usage is close to the vocabulary size. Figure 2: Comparisons of quantizers for VAE-F8 training. VQ is initialized with uniform distribution; all models have the same backbone, latent dimension, and vocabulary size. Quantizer Comparisons. Taking the proposed spherical codebook initialization method and ℓ2 normalized lookup, GSQ (similar to VQ, when is 1) can outperform FSQ Mentzer et al. (2024), and by scaling to 8, GSQ can beat RVQ Zeghidour et al. (2021), as we reported in fig. 2, all model here has same latent dimension eight and vocabulary size 8,192. Codebook auxiliary loss. We investigated the effectiveness of codebook auxiliary losses, e.g. entropy loss Yu et al. (2024b); Luo et al. (2024) and TCR loss Zhang et al. (2023). table 2 reveals that these losses negatively impacted the tokenizer performance and impeded codebook usage. Entropy loss only provided marginal improvement with minimal weight (0.01). Given their limited utility and computational cost on large vocab size during training, we opted not to use them. Also, the later results show that our method maintained 100% codebook usage for vocabulary sizes up to 512k without these losses. 4.1.2 Ablation of Network Backbone We explored variations in baseline architectures, including the effect of Adaptive Group Normalization (as known as AdaLN) Huang & Belongie (2017) and Depth2Scale Yu et al. (2024b). As detailed in table 3, surprisingly, these modules degraded the reconstructions perceptual quality, increasing the rFID but decreasing the pixel-wise error. We use Adaptive Group Normalization as the default and further invested Depth2Scale in GANs training in section 4.2.4."
        },
        {
            "title": "Entropy Loss TCR Loss",
            "content": "0.01 0.1 0.5 0.01 rFID Ó 5.281 5.687 7.906 9.937 5.375 IS Ò LPIPS Ó PSNR Ò SSIM Ò Usage Ò PPL Ò 114 112 97 82 113 99.8% 73.5% 8.83% 81.1% 100% 23.9 23.7 22.8 22.5 23.59 7397 5399 620 830 8062 0.12 0.12 0.11 0.15 0.11 0.72 0.71 0.67 0.65 0.71 Table 2: Ablation of codebook auxiliary loss for GSQ-VAE-F8. Our methods enable the codebook usage to always be full; there is no need to use this auxiliary loss for training. AGN Depth2Scale rFID Ó 5.375 5.406 5.562 5.531 IS Ò LPIPS Ó PSNR Ò SSIM Ò Usage Ò PPL Ò 113 113 113 100% 100% 100% 100% 23.59 23.85 23.93 23.94 8062 7457 7410 7452 0.71 0.71 0.72 0.72 0.11 0.10 0.11 0.11 Table 3: Ablation of using Adaptive Group Norm (AGN) and Depth2Scale for GSQ-VAE-F8. Type LPIPS Dino ResNet VGG-16 λp 0.1 0.1 1.0 1.0 10 0.1 0.1 0.7 0.1 0.1 0.7 0.1 0.1 0. λrec 1.0 5.0 1.0 5.0 1.0 1.0 5.0 4.0 1.0 5.0 4.0 1.0 5.0 4.0 rFID Ó 7.062 12.18 5.406 6.156 6.093 7.312 4.250 4.343 31.37 9.625 204 4.468 5.031 4.906 IS Ò LPIPS Ó PSNR Ò SSIM Ò Usage Ò PPL Ò 98 73 113 105 115 90 112 110 53 84 1.60 112 111 103 100% 87% 100% 100% 99% 100% 100% 100% 37% 73% 77% 100% 100% 100% 25.26 25.68 23.85 24.93 22.41 24.91 23.12 23.66 21.70 23.91 20.16 22.64 21.97 24.17 7013 5673 7457 7192 7417 6457 7004 6887 2657 5001 5028 6926 6986 0.75 0.75 0.71 0.74 0.68 0.72 0.65 0.67 0.57 0.68 0.41 0.63 0.61 0.69 0.12 0.14 0.10 0,11 0.11 0.15 0.12 0.13 0.19 0.15 0.56 0.14 0.14 0.15 Table 4: Ablation of perceptual loss and weights for VAE-F8 training. λp and λrec are weights of perceptual and reconstruction loss. 4.1.3 Ablation of Perceptual Loss Selection We explored various perceptual loss configurations, including LPIPS Zhang et al. (2018) and logit-based perceptual loss with different backbone architectures: ResNet He et al. (2016), VGG Simonyan & Zisserman (2015), and Dino Oquab et al. (2024). As presented in table 4, our findings indicate that ResNet-based logit loss is ineffective as perceptual loss, which contradicts earlier findings Weber et al. (2024). In contrast, Dino and VGG-based logit losses yielded lower rFID scores, demonstrating their potential. However, we opted for LPIPS due to its ability to effectively balance rFID and pixel-wise error. We anticipate that further optimisation through detailed hyperparameter tuning could enhance the performance of stronger perceptual losses. 4.1.4 Hyper-parameters optimization for GSQ-VAE Optimizers. The choice of hyper-parameters specifically β in Adam, significantly affects training dynamics. We evaluated combinations of β values, ranging from 0 to 0.9, and reported results in table 5. Our experiments reveal higher β always brings better reconstruction performance by promoting stable training. 6 We also assessed weight decay values of 5e2 and 1e4, and results show that when higher β is used, weight decay with 5e2 performing best overall. Therefore, we use β r0.9, 0.99s with weight decay of 0.05 for optimal training stability. β (0, 0.99) (0.5, 0.99) (0.9, 0.95) (0.9, 0.99) (0.9, 0.999) Weight Decay 5e2 1e4 5e2 1e4 5e2 1e4 5e2 1e4 5e2 1e4 rFID Ó 5.562 5.812 5.750 5.375 5.406 5.562 5.343 5.562 5.406 5.468 IS Ò LPIPS Ó PSNR Ò SSIM Ò Usage Ò PPL Ò 113 107 111 109 113 113 113 112 112 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 23.9 23.9 23.85 23.85 23.85 23.85 23.89 23.86 23.87 23.88 7410 7393 7492 7421 7457 7407 7462 7404 7472 7411 0.72 0.71 0.71 0.71 0.71 0.71 0.71 0.71 0.71 0.71 0.11 0.11 0.10 0.09 0.10 0.10 0.10 0.10 0.10 0.10 Table 5: Optimizers β and weight decay ablations for GSQ-VAE-F8 training. The codebook usage is 100% for all models. Warm-up Decay Final L.R. 0 5k 5k 5k 5k 5k 75k 95k 95k 10% at 75k 1e4 1e4 1e5 1e5 0 1e rFID IS Ò 113 114 110 109 111 112 Ó 5.343 5.406 5.750 5.781 5.625 5.468 LPIPS PSNR SSIM Usage PPL Ó 0.10 0.10 0.10 0.09 0.10 0.10 Ò 23.89 23.78 23.67 23.76 23.73 23.83 Ò 0.71 0.72 0.71 0.71 0.71 0. Ò Ò 100% 7462 100% 7429 100% 7344 100% 7355 100% 7343 100% 7389 Table 6: Learning rate scheduler ablations for GSQ-VAE-F8 training, the maximal learning rate is 1e4. The codebook usage is 100% for all models. Learning rate scheduler. Recent studies used diverse learning rate schedulers for training tokenizers. We compared fixed learning rate training against the other five schedulers, each with 5k steps warm-up period and varied decay strategies, as plotted in fig. 9 in Appendix D. The results are reported in table 6, showing that substantial learning rate decay negatively impacted model performance, and there are no advantages from warm-up training. Therefore, we opted for constant learning rate throughout training to maintain the GAN training and simplicity of hyper-parameter optimization. 4.2 Optimized Training for GSQ-GAN Next, we incorporated discriminator and adversarial loss to ablate training configurations for GSQ-GAN training on ImageNet Deng et al. (2009) at 1282 resolution for up to 80k steps; the VAE and discriminator have learning rate of 1e4. Detailed hyperparameters are reported in Appendix E. 4.2.1 Ablations of Discriminator and Combinations of Adversarial Loss We evaluated three types of discriminator: N-Layer Discriminator (NLD) Isola et al. (2017), StyleGAN Discriminator (SGD) Karras et al. (2019), and Dino Discriminator (DD) Sauer et al. (2023). We also compared three adversarial loss types: vanilla non-saturating (V ), hinge (H ), and improved non-saturating (N ), resulting in six combinations of adversarial-discriminator loss setups. Choosing an improper GAN loss led to negative performance for N-Layer and Dino Discriminators. As shown in table 7. All GAN models trained with Dino Discriminators consistently outperformed GAN with"
        },
        {
            "title": "Discriminator",
            "content": "NLD Isola et al. (2017) SGD (1k) Karras et al. (2019) DD Sauer et al. (2023) Adv. loss Hinge Hinge Hinge Non-Sat. Non-Sat. Non-Sat. Non-Sat. Discr. loss Vanilla Hinge Non-Sat. Vanilla Hinge Hinge Non-Sat. Non-Sat. Hinge Non-Sat. Non-Sat."
        },
        {
            "title": "Hinge\nVanilla\nHinge\nHinge\nVanilla\nHinge",
            "content": "OpenMagViT2 w/ 1.75M steps rFID IS PSNR SSIM Usage PPL Ó 5.343 45.2 24.0 68.5 9.562 11.3 23.7 18.1 19.1 27.1 1.976 1.906 1.867 1.180 Ò 113 25 49 14 86 80 50 63 62 46 116 117 117 Ò 23.89 20.6 21.4 19.3 22.08 22.0 21 21.65 21.57 21.42 21.78 22.01 22.12 Ò 0.71 0.58 0.62 0.51 0.66 0.66 0.62 0.64 0.64 64.96 0.64 0.65 0. Ò Ò 100% 7462 96.4% 6976 98.5% 7424 58.2% 4069 100% 7558 100% 7516 99.0% 7451 100% 6104 100% 6061 100% 5514 100% 7546 100% 7533 100% 7525 Luo et al. (2024) Table 7: GSQ-GAN-F8 model trained on 1282 ImageNet, 80k training step. The SGD-GAN model is evaluated at the 1k training step due to the failure of NaN loss in training. the N-Layer one. The best losses for N-Layer Discriminator are with NV losses, achieving an rFID of 9.562, and NH for Dino Discriminator, which reached 1.867 rFID. Additionally, we ablate the data augmentation Sauer et al. (2023) in Dino Discriminator, as shown in table 8, using combination of colour augmentation, translation, and cutout led to improved reconstruction performance. Discr. Data Aug. Color+Trans Cutout+Color+Trans Resize+Color+Trans rFID-1282 Ó 1.953 2.000 1.867 2.000 rFID-2562 Ó 0.824 0.783 0.824 0.832 Table 8: Ablation on data augmentation in Dino-Discriminator. 4.2.2 Hyper-parameters Optimization for GSQ-GAN Discriminator optimizer and adversarial loss weights. We performed ablation studies on optimizer hyper-parameters (β) for N-Layer and Dino Discriminator. The results, presented in table 9, indicate that higher β values (β r0.9, 0.99s) led to more stable training dynamics for both discriminator types. We used this configuration for the remainder of the experiments. Additionally, varying the weight of adversarial loss did not show significant benefits, leading us to set the adversarial loss weight to 0.1. Learning Rates and Batch Size. We investigated the batch size and learning rate configurations, comparing three different batch sizes and learning rates. The results, shown in table 10, indicate that larger batch sizes and increased learning rates improved stability and convergence speed and thus allowed us to speed up GAN training with larger batch sizes. 4.2.3 GAN Regularization Ablations We explored several regularization techniques for stabilizing discriminator training: gradient penalty Gulrajani et al. (2017), LeCAM regularisation Yu et al. (2023), and autoencoder warm-up, as well as adaptive discriminator loss weights Yu et al. (2022), weight decay, and gradient clipping. table 11 summaries our findings. 8 Discr. Loss NH NLD NH NLD NH NLD NH NLD NH NLD NH NLD NV NLD NV NLD NV NLD NH DD NH DD NH DD NV DD NV DD NV DD β (0, 0.99) (0.5, 0.9) (0.5, 0.9) (0.9, 0.95) (0.9, 0.99) (0.9, 0.99) (0.5, 0.9) (0.9, 0.99) (0.9, 0.99) (0.5, 0.9) (0.9, 0.99) (0.9, 0.99) (0.5, 0.9) (0.9, 0.99) (0.9, 0.99) λadv 0.1 0.1 0.5 0.1 0.1 0.5 0.1 0.1 0.5 0.1 0.1 0.5 0.1 0.1 0.5 rFID Ó 6.687 11.31 106 3.578 3.515 3.718 9.562 3.390 3.515 1.867 1.859 2.453 1.906 1.820 2.671 IS Ò PSNR Ò SSIM Ò 96.5 80.0 8.68 114 114 114 86 102 114 117 118 106 117 117 102 22.35 22.01 15.40 22.74 22.85 22.83 22.08 22.88 22.86 22.12 22.12 20.66 22.01 22.02 20.28 0.67 0.66 0.29 0.69 0.69 0.69 0.66 0.69 0.69 0.66 0.66 0.59 0.65 0.65 0.57 Table 9: Ablation of Adams β and adversarial loss weights for GSQ-GAN-F8 training. λadv is the weight of adversarial loss. Batch size Learning rate 256 256 256 512 512 768 768 1e4 2e4 3e4 1e4 2e4 2e4 3e4 rFID Ó 1.859 1.796 1.890 1.671 1.578 1.593 1.648 IS Ò LPIPS Ó PSNR Ò SSIM Ò Usage Ò PPL Ò 118 119 118 120 122 121 122 100% 100% 100% 100% 100% 100% 100% 22.12 22.28 22.36 22.08 22.25 22.32 22.31 7528 7525 7544 7494 7538 7513 7520 0.66 0.66 0.67 0.66 0.66 0.67 0.67 0.08 0.07 0.07 0.08 0.07 0.07 0.07 Table 10: Batch size and learning rate ablations of GSQ-GAN-F8 training, with DD-NH discriminator and loss combination. Using constant λadv performed best, with no advantages observed from adaptive weighting Esser et al. (2021). The Gradient penalty added for N-Layer Discriminator was ineffective, and LeCAM only slightly improved results. Autoencoder warm-up (discriminator training starts after 20k steps) did not improve stability or performance; gradient clipping at 2.0 (by default) was more effective than at 1.0, and weight decay of 1e4 improved the N-Layer Discriminator but slightly degraded the Dino Discriminator. Training StyleGAN Discriminator with regularization could not address NaN issues. We also tested combination of StyleGAN Discriminator and gradient penalty. But training with gradient penalty was also roughly four times slower, so we could not finish the training within 80k step training wall time, see more details of StyleGAN Discriminator in Appendix E). 4.2.4 Analysis of Attention Integration We conducted ablation studies on the attention module and Depth2Scale layers. Recent works such as Luo et al. (2024); Yu et al. (2024b) omit attention layers, but as seen in table 12, incorporating attention into mid-blocks improved model performance. We also re-evaluated Depth2Scale, observing that it enhanced GANs performance under adversarial training. The models performance across different resolutions is also reported in table 12, showing the models cross-resolution inference capabilities. We take Depth2Scale, as it generally benefits the GAN training; the model trained with Depth2Scale has rFID 1.53 with 80k training steps. Including attention modules can further boost reconstruction, though it may introduce instability during training. 9 Discr. NLD-NV NLD-NV + GC 1.0 NLD-NV NLD-NV NLD-NV + GP NLD-NV + LeCAM DD-NH DD-NH DD-NH DD-NH + AE-warmup DD-NH + LeCAM SGD-NH WD AW rFID Ó 5e2 5e2 1e4 5e2 5e2 5e2 5e2 1e4 5e2 5e2 5e2 5e2 3.390 3.453 3.296 4.437 5.750 3.546 1.859 1.914 2.687 2.000 5.250 3.593 IS Ò LPIPS Ó PSNR Ò SSIM Ò PPL Ò 22.8 114 22.8 114 22.86 115 23.34 112 23.78 110 22.89 113 22.12 118 22.12 118 23.40 117 22.22 116 23.79 111 23.61 110 7594 7483 7494 7476 7447 7455 7528 7514 7464 7484 7437 0.69 0.69 0.69 0.70 0.71 0.69 0.66 0.66 0.70 0.66 0.71 0.70 0.06 0.06 0.06 0.07 0.09 0.07 0.08 0.08 0.07 0.08 0.08 0.07 Table 11: Ablation studies of GANs regularization technologies for GSQ-GAN-F8 training, WD is weight decay, AW is adversarial loss adaptive weight Esser et al. (2021), GC is gradient clip. All modes are trained with gradient clip 2.0 by default, GP is gradient penalty, and LeCAMs weight is 0.001 if enabled; when warmup is used, the discriminator starts to be updated after 20k iterations. Data Aug D2S Attention OpenMagViT2 Luo et al. (2024) w/ 1.75M steps rFIDÓ 128 1.609 1.578 1.570 1.531 1.421 1.539 1.523 1.180 rFIDÓ 256 0.675 0.652 0.660 0.605 0.605 0.585 0.660 0.34 Table 12: Ablation of discriminator data augmentation, integration of attention and Depth2Scale for GSQGAN-F8 training. D2S is the short for Depth2scale. 4.3 Scaling Behaviors of GSQ-GAN This section investigates how variations influence reconstruction quality in latent dimensions and codebook vocabulary size. All models in this study were trained at 2562 resolution with batch size of 512 over 50k steps (20 epochs). Detailed hyper-parameters are provided in Appendix F. Figure 3: GSQ-GAN ablations on wider and deeper networks w/ and w/o attention blocks. Models are trained on 2562 resolution on ImageNet."
        },
        {
            "title": "4.3.1 Network Capacity.",
            "content": "We examine the effects of network capacity on reconstruction fidelity, specifically looking at the width and depth. Width scaling was implemented by increasing the number of channels in convolution layers, while depth scaling involved adding additional convolution blocksYu et al. (2024b). The results, summarized in fig. 3, demonstrate consistent improvements in reconstruction as network width and depth increase. Integrating attention modules within wider networks yielded further gains as used in Esser et al. (2021). (a) Scaling of latent dimension and vocabulary size for GSQ at 8ˆ spatial compression. (b) Same scaling behaviour as the top figure with vocabulary size in logarithmic scale. Figure 4: The top figure illustrates the scaling of latent dimension and codebook size for GSQ at 8ˆ spatial compression, where smaller latent dimension improves reconstruction, suggesting the latent space is not saturated for F8 downsampling. Optimising latent space size further enhances performance. The bottom figure shows the same trend with vocabulary size in logarithmic scale, indicating effective scaling as vocabulary size increases. All models are trained with 1 and no latent decomposition, making this equivalent to VQ-based methods. All models are trained on ImageNet at 2562 resolution. 4.3.2 Scaling of Latent Space and Vocabulary. Next, we investigate the impact of scaling latent dimensionality and codebook vocabulary size. Models were trained with latent dimensions of 23, 24, 25, and 26, each paired with vocabulary sizes of 8k, 16k, 64k, 256k, and 512k. Results in fig. 4a and fig. 4b indicate that larger vocabulary sizes, combined with lower latent dimensions, consistently yielded superior reconstruction performance. Remarkably, model with latent 11 dimension of 8 and vocabulary size 512k outperformed the state-of-the-art image tokenizers, achieving notable results within just 50k training steps (20 epochs). These findings underscore the significance of large codebook vocabulary in enhancing quantizer representational capacity. This trend aligns with theoretical expectations, as the representational capacity of GSQ-GAN is fundamentally bounded by log as shown in fig. 4b, where is the vocabulary size. The pattern holds consistently across configurations and provides point of contrast with prior studies with VQ (e.g., Yu et al. (2024b) Yu et al. (2022) Sun et al. (2024)), as they did not employ optimized configurations for VQ-GAN training that the model training degradation has bias on their scaling behaviours observation. Our experiments reveal that lower-dimensional latent spaces result in improved reconstruction fidelity. As detailed in Appendix C, low-dimensional latent spaces are advantageous for computing precise Euclidean distances used for codebook updates. This insight supports the success of decomposed vector quantization approaches, such as LFQ Yu et al. (2024b), FSQ Mentzer et al. (2024), and our own proposed GSQ. Interestingly, one might intuitively expect larger latent dimension to yield better performance because of the huge latent space. Our results suggest that high-dimensional spaces are often underutilized. This is important since effective compression at higher spatial down-sampling ratios requires larger latent dimensionality. However, normal VQ-like models cannot effectively scale latent dimensions against high spatial compression challenges. As illustrated in fig. 5, increasing latent dimensionality enhances reconstruction quality when moving from F8 to F16. However, beyond certain point (here is F16-D16), the model encounters the well-known limitations imposed by the curse of dimensionality. By contrast, when using the dimension decomposition in GSQ, even with 2, the reconstruction performance gains fascinating improvement. 4.3.3 Latent Space and Downsample Factor, and Better Scaling with GSQ To address the limitations regarding the difficulty of scaling attend dimension. We use GSQ to decompose large latent dimensions into low dimensions, thus maximizing reconstruction fidelity more effectively. As demonstrated in table 13, by decomposing latent vectors into multiple groups, GSQ significantly enhances reconstruction performance without changing the overall latent dimensionality or vocabulary size. This result confirms GSQs ability to harness the representational power of high-dimensional latent spaces, leading to substantial gains in model fidelity. Figure 5: Latent dimension scaling for GSQ-GAN-F16 training, the latent space is saturated for F16 spatial compression; we expect to enhance reconstruction performance by increasing the latent dimension to increase the latent capacity. Only GSQ with latent decomposition can scale to higher latent dimension. Notably, the model achieves near-lossless reconstruction with 64 and 16, approaching theoretical maximum performance. Although the compression ratio is very low and lacks practical value, it highlights GSQs remarkable scalability and representational power. Scaling Down-sample Factor. With GSQ optimizing latent space utilization, we further investigate the impact of varying down-sampling factors on reconstruction quality. We conducted experiments across 12 Models Luo et al. (2024) LFQ F16-D18 256k GSQ F8-D64 8k GSQ F16-D16 256k GSQ F32-D32 256k ˆ rFID Ó IS Ò LPIPS Ó PSNR Ò SSIM Ò Usage Ò PPL Ò 18 ˆ 1 1.17 1 ˆ 64 2 ˆ 32 4 ˆ 16 16 ˆ 4 1 ˆ 16 2 ˆ 8 4 ˆ 4 8 ˆ 2 16 ˆ 1 16 ˆ 1 1 ˆ 32 2 ˆ 16 4 ˆ 8 8 ˆ 4 16 ˆ 2 32 ˆ 1 0.63 0.32 0.18 0.03 1.42 0.82 0.74 0.50 0.52 0.51 6.84 3.31 1.77 1.67 1.13 1.21 205 220 226 233 179 199 202 211 210 210 95 139 173 176 190 187 0.08 0.05 0.03 0.004 0.13 0.09 0.08 0.06 0.06 0.06 0.24 0.18 0.13 0.12 0.10 0. 22.95 25.42 28.02 34.61 20.70 22.20 22.75 23.62 23.54 23.52 17.83 19.01 20.60 20.88 21.73 21.64 0.67 0.76 0.08 0.91 0.56 0.63 0.63 0.66 0.66 0.66 0.40 0.47 0.53 0.54 0.57 0.57 99.87% 100% 100% 99.98% 100% 100% 62.46% 46.83% 50.81% 52.64% 100% 100% 100% 59% 46% 54% 8,055 8,157 8,143 6,775 254,044 257,273 43,767 22,181 181 748 245,715 253,369 253,199 40,307 30,302 247 Table 13: Ablation studies of group decomposition with 8, 16 and 32 spatial downsample, vocabulary size is 8k, 256k and 256k respectively. GSQ outperforms LFQ with 3ˆ lower rFID. is the number of groups, and is latent dimension in each group. 16 ˆ 1 is trained with clip instead of ℓ2 normalization. different configurations of latent dimensions and down-sampling factors. As illustrated in 1b, models trained with down-sampling factor of 8{16{32 showed consistent improvement in reconstruction as latent dimensions increased (with 16 and group count adjusted accordingly). These results align with theoretical expectations and further validate the effectiveness of GSQ in fully utilizing the latent space."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce novel quantization method, Grouped Spherical Quantization(GSQ), incorporating spherical codebook initialization, lookup normalization, and latent decomposition. We systematically investigate training strategies and optimizations for the proposed GSQ-GAN, identifying key configurations that enhance reconstruction quality with significantly fewer training iterations. We highlight critical scaling behaviours related to the model, latent space, and codebook vocabulary size, emphasizing the role of compact latent spaces in achieving high-fidelity reconstruction. Our results demonstrate that GSQ efficiently scales in high-dimensional latent spaces, leverating latent decomposition and spherical normalization for improved compression and reconstruction."
        },
        {
            "title": "6 Acknowledgment",
            "content": "In alphabetical order, we thank Erik, Ismail, Jan, Lijun, and Oleg for their insightful input and feedback on this manuscript. This work was supported by the German Federal Ministry for Economic Affairs and Climate Action under the project NXT GEN AI METHODS: Generative Methods for Perception, Prediction, and Planning, the bidt project KLIMA-MEMES, Bayer AG, and the German Research Foundation (DFG) project 421703927. We also appreciate the Gauss Centre for Supercomputing e.V. for granting access to computing resources on the JUWELS and JURECA supercomputers at the Jülich Supercomputing Centre (JSC). The German AI Service Centre WestAI provided additional computational resources."
        },
        {
            "title": "References",
            "content": "Mohammad Adiban, Marco Siniscalchi, Kalin Stefanov, and Giampiero Salvi. Hierarchical residual learning based vector quantized variational autoencorder for image reconstruction and generation. In 33rd British Machine Vision Conference, 2022. Mohammad Adiban, Kalin Stefanov, Sabato Marco Siniscalchi, and Giampiero Salvi. S-hr-vqvae: Sequential hierarchical residual learning vector quantized variational autoencoder for video prediction. arXiv preprint arXiv:2307.06701, 2023. Riddhish Bhalodia, Iain Lee, and Shireen Elhabian. dpvaes: Fixing sample generation for regularized vaes. In Proceedings of the Asian Conference on Computer Vision, 2020. Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, and Kaigi Huang. Efficientvqgan: Towards high-resolution image generation with efficient vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 73687377, 2023. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Evgenii Egorov, Anna Kuzina, and Evgeny Burnaev. Boovae: Boosting approach for continual learning of vae. Advances in Neural Information Processing Systems, 34:1788917901, 2021. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Christopher Fifty, Ronald G. Junkins, Dennis Duan, Aniketh Iger, Jerry W. Liu, Ehsan Amid, Sebastian Thrun, and Christopher Ré. Restructuring vector quantization with the rotation trick, 2024. Peng Gao, Le Zhuo, Chris Liu, , Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with constrained variational framework. ICLR (Poster), 3, 2017. Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving, 2023. Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Björn Ommer. Zigma: dit-style zigzag mamba diffusion model. In ECCV, 2024. Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pp. 15011510, 2017. 14 Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117128, 2010. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1012410134, 2023. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition networks. (CVPR), June 2019. Minyoung Kim, Yuting Wang, Pritish Sahu, and Vladimir Pavlovic. Bayes-factor-vae: Hierarchical bayesian In Proceedings of the IEEE/CVF international deep auto-encoder models for factor disentanglement. conference on computer vision, pp. 29792987, 2019. Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Joshua V. Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David Ross, Bryan Seybold, and Lu Jiang. Videopoet: large language model for zero-shot video generation. In Forty-first International Conference on Machine Learning, 2024. Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved rvqgan. Advances in Neural Information Processing Systems, 36, 2024. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1152311532, 2022. Xiang Li, Hao Chen, Kai Qiu, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024. Eric Luhman and Troy Luhman. Optimizing hierarchical image vaes for sample quality. arXiv preprint arXiv:2210.10205, 2022. Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An opensource project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: VQ-VAE made simple. In The Twelfth International Conference on Learning Representations, 2024. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=a68SUt6zFt. Song Park, Sanghyuk Chun, Byeongho Heo, Wonjae Kim, and Sangdoo Yun. Seit: Storage-efficient vision training with tokens using 1% of pixel storage. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1724817259, October 2023. 15 Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with vector-quantized visual tokenizers, 2022. Tian Qin and Wei-Min Huang. Epanechnikov variational autoencoder. arXiv preprint arXiv:2405.12783, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann Weber. LitearXiv preprint vae: Lightweight and efficient variational autoencoders for latent diffusion models. arXiv:2405.14477, 2024. Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In International conference on machine learning, pp. 3010530118. PMLR, 2023. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR, 2015. Jianlin Su and Guang Wu. f-vaes: Improve vaes with conditional flows. arXiv preprint arXiv:1809.05861, 2018. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. NeurIPS, 2024. Arash Vahdat and Jan Kautz. Nvae: deep hierarchical variational autoencoder. Advances in neural information processing systems, 33:1966719679, 2020. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation, 2024a. URL https://arxiv.org/abs/2406.09399. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b. Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: Embedding-free image generation via bit tokens. arXiv preprint arXiv:2409.16211, 2024. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Tackgeun You, Saehoon Kim, Chiheon Kim, Doyup Lee, and Bohyung Han. Locally hierarchical autoregressive modeling for image generation. Advances in Neural Information Processing Systems, 35:16360 16372, 2022. 16 Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=pfNyExj7z2. Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1045910469, 2023. Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al. Spae: Semantic pyramid autoencoder for multimodal generation with frozen llms. Advances in Neural Information Processing Systems, 36, 2024a. Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key to visual generation. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview. net/forum?id=gzqrANCF4g. Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. NeurIPS, 2024c. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30: 495507, 2021. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Yifan Zhang, Zhiquan Tan, Jingqin Yang, Weiran Huang, and Yang Yuan. Matrix information theory for self-supervised learning. arXiv preprint arXiv:2305.17326, 2023. Yue Zhao, Yuanjun Xiong, and Philipp Krähenbühl. Image and video tokenization with binary spherical quantization. arXiv preprint arXiv:2406.07548, 2024. Chuanxia Zheng, Guoxian Song, Tat-Jen Cham, Jianfei Cai, Dinh Phung, and Linjie Luo. High-quality pluralistic image completion via code shared vqgan. arXiv preprint arXiv:2204.01931, 2022a. Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems, 35:2341223425, 2022b. Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of VQ-GAN to 100,000 with In The Thirty-eighth Annual Conference on Neural Information Processing utilization rate of 99%. Systems, 2024a. URL https://openreview.net/forum?id=RbU10yvkk6. Yongxin Zhu, Bocheng Li, Hang Zhang, Xin Li, Linli Xu, and Lidong Bing. Stabilize the latent space for image autoregressive modeling: unified perspective, 2024b. Zhenhai Zhu and Radu Soricut. Wavelet-based image tokenizer for vision transformers. arXiv preprint arXiv:2405.18616, 2024."
        },
        {
            "title": "3.1 Preliminary: VQ Image Tokenizer",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4.1.1 Effectiveness of Spherical Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "4.1.2 Ablation of Network Backbone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.3 Ablation of Perceptual Loss Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.4 Hyper-parameters optimization for GSQ-VAE . . . . . . . . . . . . . . . . . . . . . . . 4.2 Optimized Training for GSQ-GAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.1 Ablations of Discriminator and Combinations of Adversarial Loss . . . . . . . . . . . . 4.2.2 Hyper-parameters Optimization for GSQ-GAN . . . . . . . . . . . . . . . . . . . . . . 4.2.3 GAN Regularization Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.4 Analysis of Attention Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Scaling Behaviors of GSQ-GAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.1 Network Capacity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3. Scaling of Latent Space and Vocabulary. . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.3 Latent Space and Downsample Factor, and Better Scaling with GSQ . . . . . . . . . . 5 Conclusion 6 Acknowledgment Performance of State-of-the-Art Image Tokenizers Networks GSQ and Other Quantizers C.1 Discussion of Euclidean Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Scaling Without Dimension Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . Ablation Studies of VAE D.1 VAE Training configurations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Usage of Codebook Initialization Ablation Studies. . . . . . . . . . . . . . . . . . . . . . . . 18 1 3 4 4 4 4 5 6 6 7 7 8 9 10 11 11 12 13 20 20 21 22 24 24 24 D.3 Learning Rate Scheduler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 VAE Reconstruction Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Ablation Studies of GAN",
            "content": "E.1 GAN Training configurations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Discriminator Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Adversarial and Discriminator Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Failed Style-GAN Discriminator GANs Training . . . . . . . . . . . . . . . . . . . . . . . . . E.5 GAN Reconstruction Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "F Scaling Behaviors",
            "content": "25 26 27 27 28 29 31 32 19 Performance of State-of-the-Art Image Tokenizers We list additional comparisons of reconstruction performance among various state-of-the-art image tokenizers, including the model trained in this study. The evaluation is conducted on ImageNet at resolution of 256 ˆ 256. Ours-GSQ Ours-GSQ Ours-GSQ Ours-GSQ VQ-GAN Esser et al. (2021) VQGAN-LC Zhu et al. (2024a) VIT-VQGAN_SL Yu et al. (2022) OmniTokenizer Wang et al. (2024a) OmniTokenizer Wang et al. (2024a) LlamaGen Sun et al. (2024) BSQ Zhao et al. (2024) Open-MAGVIT2 Luo et al. (2024) Ours-GSQ w/ attention Ours-GSQ Ours-GSQ VQGAN-LC Zhu et al. (2024a) MASKGIT Chang et al. (2022) LlamaGen Sun et al. (2024) Titok-B Yu et al. (2024c) MASKBIT Weber et al. (2024) ImageFolder Li et al. (2024) MAGVIT2 Yu et al. (2024b) Open-MAGVIT2Luo et al. (2024) 8 8 8 8 8 8 8 8 8 8 8 8 16 16 16 16 16 16 16 16 16 16 16 8 (d 8,G 1) 8 (d 8,G 1) 16(d 16,G 1) 64(d 4,G 16) Latent-size 32 ˆ 32 32 ˆ 32 32 ˆ 32 32 ˆ 32 32 ˆ 32 1024 32 ˆ 32 32 ˆ 32 32 ˆ 32 32 ˆ 32 32 ˆ 32 32 ˆ 8 32 8 8 8 36 18 16 ˆ 16 16 ˆ 16 16 ˆ 16 256 16 ˆ 16 16 ˆ 16 128 16 ˆ 16 265 16 ˆ 16 16 ˆ 16 8(d 1,G 8) 16(d 16,G 1) 16(d 1,G 16) 8 256 8 256 18 18 8k 256k 256k 8k 8k 100,000 8k 8k 8 16k 236 256k 512k 256k 256k 100,000 1k 16k 4k 1024 4k 256k 256k rFID 0.48 0.36 0.51 0.03 1.49 1.29 1.28 1.11 0.69 0.59 0.41 0.34 0.95 1.42 0.52 2.62 2.28 2.19 1.70 1.66 1.57 1.15 1.17 Table 14: Reconstruction performance comparison of the proposed model against other state-of-the-art methods on ImageNet (256 ˆ 256 resolution)."
        },
        {
            "title": "B Networks",
            "content": "The network backbone is derived from VQ-GAN Yu et al. (2022), and MagVit2 Yu et al. (2024b). The encoder and decoder backbones are classified into two primary components: up/down-sampling resolution blocks (grey blocks in fig. 6) and mid-blocks (green blocks in fig. 6). We build down-sampling resolution blocks in the encoder with such rules: 1) For spatial down-sampling factor of 2N , the encoder includes ` 1 down-sampling blocks, each containing ResBlock. The first blocks are followed by down-sampling operation using stride convolutions. 2) the convolutional channels in each ResBlock and the number of ResBlocks within each down-sampling block are determined by the Channel Multipliers and Encoder Layer Configurations. 3) an additional ResBlock is introduced to match the channel dimensions if the channel multiplier doubles at specific layer. The decoder follows analogous principles, adding Adaptive GroupNorm layers before each up-sampling operation. For mid-blocks each of the mid-blocks consists of specified number of ResBlocks, with their channel dimensions determined by the output channels of the preceding layer. When mid-block attention mechanisms are used, attention is inserted between any two consecutive ResBlocks within the mid-blocks. 20 Figure 6: Architecture of the GSQ tokenizer. The backbone follows the 2D convolutional version of MagVit2 Yu et al. (2024b), with variations in the number of blocks."
        },
        {
            "title": "C GSQ and Other Quantizers",
            "content": "This section discusses the relationship between GSQ and other tokenizers. GSQ provides unified framework for tokenizers, excluding the specific spherical codebook initialization proposed in this work. Other tokenizers can be derived by appropriate configurations, as outlined in table 15. 21 VQ VQGAN-ViT LFQ FSQ BSQ ˆ GSQ 2 1 1 1 1 Cpgq 2 2 g"
        },
        {
            "title": "V\nV",
            "content": "Finite t1, 1u Codebook-Sharing ą 2 ℓ2 Fixed-Codebook Effective 2D gPG Cpgq g ś Table 15: The effective configurations of other tokenizers in GSQs view. VQ VQ Van Den Oord et al. (2017) and GSQ are identical when the latent space is not decomposed into groups (G 1) and without ℓ2 normalization. BSQ BSQ Zhao et al. (2024) represents the 2 case of GSQ, where the number of groups is set as 2 . Codebooks are shared across groups, and BSQs codebook is fixed. FSQ FSQ Mentzer et al. (2024) is specific case of GSQ, where D, and each group has its own unshared, finite codebook. The term \"finite\" here refers to small vocabulary size . with each latent variable in the codebook Cpgq constrained as follows: Sigmoidpzq t0, 1 1 , 2 1 , ..., 1u (6) In FSQ, typical values for pgq are 5, 6, 7, or 8, representing very small vocabulary size. LFQ LFQ Mentzer et al. (2024) can be interpreted from multiple perspectives. Within the GSQ framework, the simplest interpretation is to set 1. For any 1-dimensional latent variable zi, the ℓ2 normalization reduces to two possible outputs, 1 or 1: # ℓ2pziq zi zi2 1, if zi ą 0 1, if zi ă 0 (7) Special cases, such as zi 0, are handled by setting ℓ2pziq 1 in alignment with Mentzer et al. (2024). In this scenario, the 1-dimensional sphere degenerates into two discrete points, reducing the vocabulary size to 2. Prior studies Yu et al. (2024b); Luo et al. (2024); Zhao et al. (2024) have shown the necessity of additional auxiliary objectives, such as entropy loss, to ensure effective codebook usage during training. However, in LFQ, codebook indices are not explicitly used; instead, the computational cost is transferred to entropy calculations. For large codebooks, even modern entropy computation kernels introduce significant memory and computational overhead. There are two possible ways to address these challenges for 1 with shared codebook: Avoid applying ℓ2 normalization, thereby eliminating the vocabulary size degradation and the need for entropy loss and expensive entropy computations in large codebooks. Alternatively, we can enable ℓ2 normalization but use different codebooks among groups (very similar to LFQ). Both approaches generalize the 1-dimensional sphere into 1-dimensional manifold, equivalent to the 2 case of GSQ without ℓ2 normalization. We take the first solution in for 1 case. C.1 Discussion of Euclidean Distance The squared Euclidean distance between an n-dimensional vector and vector in the codebook is given by: C2 2 z2 2 ` C2 2 2pz Cq, (8) where denotes the dot product. p0, σqboth the mean and variance of the distances scale linearly with dimension n: In high-dimension spaces, (assuming and are drawn from Erz C2 Varrz C2 2s 2nσ2 2s 4nσ4. (9) (10) By normalizing both and with ℓ2 normalization (i.e., z2 C2 1), the distance calculation simplifies to: ℓ2pzq ℓ2Cq2 2 2p1 cos θq (11) where cos θ represents the cosine similarity between and C. For ℓ2-normalized vectors, the expectation and variance of the squared Euclidean distance are as follows: Erℓ2pzq ℓ2Cq2 2s 2 Varrℓ2pzq ℓ2Cq2 2s 4 1 Op 1 q. (12) (13) In high-dimensional spaces, most vectors in the codebook become nearly orthogonal to the query vector z. This results in similar distances from to most codebook vectors, converging towards 2 as the dimension increases. However, the rate of this convergence is relatively slow. As dimensionality increases, the differences between the query vector and the vectors in the codebook become centralized around 2, with variance proportional to 1 . This highlights the inefficiency of directly quantifying high-dimensional vectors. Instead, quantifying individual components of high-dimensional vectors separately is more effective in preserving representational diversity and accuracy. C.2 Scaling Without Dimension Decomposition When dimension decomposition is not applied (i.e., GSQ with 1), we explored the relationship between vocabulary size (V ) and latent dimensionality (D) by tuning these parameters  (fig. 4)  . The relationship between the rFID and the parameters log and can be modeled as: rFID log α ` Dβ 411.63 plog q2.8375 ` 0.1601 D0. (14) (15) (16)"
        },
        {
            "title": "D Ablation Studies of VAE",
            "content": "D.1 VAE Training configurations We list full training parameters here and highlight the optimized parameters that can improve the models performance. Parameter Training Parameters Image Resolution Num Train Steps Gradient Clip Mixed Precision Train Batch Size Exponential Moving Average Beta Model Configuration Down-sample-factor (f ) Hidden Channels Channel Multipliers Encoder Layer Configs Decoder Layer Configs Quantizer Settings Embed Dimension (D) Codebook Vocabulary (V ) Group (G) Codebook Initialization Look-up Normalization Loss weights Reconstruction Loss Perceptual Loss (LPIPS) Commitment Loss VAE Optimizer Base Learning Rate Learning Rate Scheduler Weight Decay Betas Epsilon"
        },
        {
            "title": "Value",
            "content": "128ˆ 128 100,000 (20 epochs) 2 BF16 256 0.999 8 128 [1, 2, 2, 4] [2, 2, 2, 2, 2] [2, 2, 2, 2, 2] 8 8192 1 ℓ2pN p0, 1qq ℓ2 1.0 1.0 0.25 1 ˆ 104 Fixed 0.05 [0.9, 0.95] Ñ [0.9, 0.99] 1 ˆ 108 Table 16: VAE-F8 Training Hyperparameters D.2 Usage of Codebook Initialization Ablation Studies. In section 4.1.1, we compared various codebook initialization methods and observed that ℓ2-normalized lookup in VAE achieves superior reconstruction performance and higher codebook usage. The detailed codebook usage during training is shown in fig. 7. Notably, the proposed spherical initialisation ensures 100% codebook usage throughout the training process, unlike uniform initialisation. To further analyse the impact, we trained an additional model, GSQ-GAN-F16, with 4 and 256k vocabulary size, using codebook initialised with uniform distribution. As summarised in table 13, the rFID of our proposed method is 0.52, while the uniform distribution case exhibits degraded rFID of 0.66. More critically, the codebook usage drops significantly to just 3.68% with uniform initialisation, as illustrated in fig. 8. 24 Figure 7: Codebook usage during training for GSQ-VAE-F8. Our proposed ℓ2pN p0, 1qq codebook initialisation, both with and without ℓ2, ensures consistent full codebook usage. Figure 8: Codebook usage for GSQ-GAN-F16-D16G4 training with uniformly initialised codebook. D.3 Learning Rate Scheduler Figure 9: The learning rate schedules for GSQ-VAE-F8 training. In section 4.1.4, we compared five different learning rate schedulers against constant learning rate for GSQ-VAE-F8. The detailed learning rate schedules relative to training steps are depicted in fig. 9. 25 D.4 VAE Reconstruction Visualization (a) Original images (128ˆ128 resolution) (b) Reconstruction results by VAE-F8 (c) With Depth2Scale (d) With Adaptive Normalization (e) With Depth2Scale and Adaptive Normalization Table 17: Reconstruction results of the VAE-F8 model (in Section 4.1.2) with ablation of Depth2Scale and Adaptive Normalization."
        },
        {
            "title": "E Ablation Studies of GAN",
            "content": "E.1 GAN Training configurations We list the full training parameters for GAN in table 18, with highlighted optimised parameters that significantly improve the models performance. To achieve high rFID without group decomposition, ℓ2 normalisation was omitted. Parameter Training Parameters Image Resolution Num Train Steps Gradient Clip Mixed Precision Train Batch Size Exponential Moving Average Beta Model Configuration Down-sample-factor (f ) Hidden Channels Channel Multipliers Encoder Layer Configs Decoder Layer Configs Quantizer Settings Embed Dimension (D) Codebook Vocabulary (V ) Group (G) Codebook Initialization Look-up Normalization Discriminator Name Generator Loss Discriminator Loss Dino-D Data Augmentation Loss weights Reconstruction Loss Perceptual Loss (LPIPS) Commitment Loss Adversarial Loss Discriminator Loss VAE Optimizer Base Learning Rate Learning Rate Scheduler Weight Decay Betas Epsilon Discriminator Optimizer Base Learning Rate Learning Rate Scheduler Weight Decay Betas Epsilon"
        },
        {
            "title": "Value",
            "content": "128ˆ 128 80,000 (16 epochs) 2 BF16 256 0.999 8 128 [1, 2, 2, 4] [2, 2, 2, 2, 2] [2, 2, 2, 2, 2] 8 8192 1 ℓ2pN p0, 1qq Dino Discriminator Non-Saturate Hinge Cutout+Color+Translation 1.0 1.0 0.25 0.1 1.0 1 ˆ 104 Ñ 2 ˆ 104 Fixed 0.05 [0.9, 0.99] 1 ˆ 10 1 ˆ 104 Ñ 2 ˆ 104 Fixed 0.05 [0.5, 0.9] Ñ [0.9, 0.99] 1 ˆ 108 Table 18: GAN-F8 Training Hyperparameters 27 E.2 Discriminator Architecture We listed the network configurations of N-Layer, Dino and StyleGAN discriminators we used in GANs ablation studies as follows: Parameter N-Layer Discriminators (NLD) Input Channels Number of Channels Number of Layers Style-GAN Discriminators (SGD) Input Channels Number of Channels Channels Multiplier DINO Discriminators (DD) Base Model Channels Multiplier Features from layer"
        },
        {
            "title": "Value",
            "content": "3 64 3 3 128 [2, 4, 4, 4, 4] DinoV2_vits14_reg [2, 4, 4, 4, 4] [2, 5, 8, 11] Table 19: Discriminator configurations E.3 Adversarial and Discriminator Loss We define the adversarial and discrimination loss as follows: the ℓreal and ℓf ake are logits of real and reconstructed images obtained by passing corresponding images to the discriminator. Vanilla Discriminator Loss Lvanilla_discr ` 1 logp1 ` eℓrealq ` logp1 ` eℓf ake Vanilla Generator Loss Hinge Generator Loss Hinge Discriminator Loss Lvanilla_gen logp1 ` eℓf ake Lhinge_gen rℓf akes Lhinge_discr 1 2 pE rmaxp0, 1 ℓrealqs ` rmaxp0, 1 ` ℓf akeqsq Non-Saturate Generator Loss Lnon_saturate_gen ReLUpℓf akeq ℓf ake 1 ` log 1 ` eℓf ake ı Non-Saturate Discriminator Loss Lreal ReLUpℓrealq ℓreal 1 ` log 1 ` eℓreal ı Lfake ReLUpℓf akeq ℓf ake 0 ` log 1 ` eℓf ake ı Lnon_saturate_discr 1 2 pLreal ` Lfakeq (17) (18) (19) (20) (21) (22) (23) (24) E.4 Failed Style-GAN Discriminator GANs Training As discussed in the main paper, extensive ablations were conducted on Style-GAN Discriminator (SGD) training. However, most experiments encountered numerical instability, resulting in NaN errors. We provide qualitative analysis of these failed runs by plotting training loss and evaluation rFID. We compare three combinations of discriminator losses: NV, HH, and NH. These combinations were chosen based on their relatively better performance in the NLD ablation studies (see table 7). (a) The summation of the generator (VAE) training loss of GSQGAN training with Style-GAN Discriminator. (b) The RFID of GSQ-GAN trained with Style-GAN Discriminator and different combinations of discriminator loss and regularization. Figure 10: Style-GAN Discriminator training models training loss and rFID are trained with different discriminator loss combinations and GAN regularization technologies. As shown in fig. 10, training with NV achieves the lowest rFID and exhibits more stable numerical behaviour than the other combinations. During this short training period, NV performs better than NH, achieving both lower rFID and lower training loss, consistent with the results of NLD. However, SGD-NV training fails abruptly at 10k steps due to NaN errors. Training with NH using the optimizer configuration β r0.9, 0.99s also fails before reaching the 10k step, previous studies (NLD and DD) suggesting that higher β values boost model performance. We further conducted ablation studies on GAN regularization techniques, including adaptive discriminator loss weights, LeCAM regularization, gradient penalty, and generator warmup. The results are presented in fig. 10. Training with gradient penalty regularization demonstrates robust and stable dynamic, with the models loss decreasing smoothly and achieving lower rFID than other methods. In contrast, training with LeCAM regularization shows significantly unstable behaviour, as reflected by sharp peaks in the loss curves. Gradient penalty and adaptive Weights perform best for Style-GAN Discriminator training among all the regularization methods, but when these two work together, the training will be highly unstable. Meanwhile, 29 due to the high parameter count and computational FLOPs of SGD, gradient penalty regularization and adaptive weights become computationally expensive, requiring additional backward passes during training. Consequently, it makes SGD an impractical choice for efficient GAN training. 30 E.5 GAN Reconstruction Visualization (a) Orignal images (128ˆ128 resolution) (b) Reconstruction results by with NLD-NV discriminators (c) Reconstruction results by with DD-NH discriminators (d) Reconstruction results by with NLD-NV discriminators and β r0.9, 0.99s (e) Reconstruction results by with DD-NH discriminators and β r0.9, 0.99s Table 20: Reconstruction results of the GAN-F8 models (see Section 4.2.1) , trained with different discriminators."
        },
        {
            "title": "F Scaling Behaviors",
            "content": "This section details the training parameters for the GAN scaling experiments. All models were trained on the 256 ˆ 256 resolution ImageNet dataset. Each scaling ablation study focuses on the latent dimension and codebook vocabulary size. Parameter Training Parameters Image Resolution Num Train Steps Gradient Clip Mixed Precision Train Batch Size Exponential Moving Average Beta Model Configuration Down-sample-factor (f ) Hidden Channels Channel Multipliers Encoder Layer Configs Decoder Layer Configs Discriminator Name Generator Loss Discriminator Loss Dino-D Data Augmentation Loss weights Reconstruction Loss Perceptual Loss (LPIPS) Commitment Loss Adversarial Loss Discriminator Loss VAE and Discriminator Optimizer Base Learning Rate Learning Rate Scheduler Weight Decay Betas Epsilon"
        },
        {
            "title": "Value",
            "content": "256ˆ 256 50,000 (20 epochs) 2 BF16 512 0.999 8 128 [1, 2, 2, 4] [2, 2, 2, 2, 2] [2, 2, 2, 2, 2] Dino Discriminator Non-Saturate Hinge Cutout+Color+Translation 1.0 1.0 0.25 0.1 1.0 2 ˆ 104 Fixed 0.05 [0.9, 0.99] 1 ˆ 108 Table 21: GAN-F8 Training Hyperparameters In the network capacity scaling experiments described in section 4.3.1, the model names correspond to their respective Channel Multipliers. The default depth of the network is set to two for each block (Encoder Layer Configs and Decoder Layer Configs). For the Deeper network configuration, the Encoder Layer Configs are set to r4, 3, 4, 3, 4, 4s and the Decoder Layer Configs to r3, 4, 3, 4, 4, 4s, following the architectural design principles outlined in MagVit2 Yu et al. (2024b). 32 (a) Orignal images (256ˆ256 resolution) (b) GSQ-GAN-F8, 16, 1 (b) GSQ-GAN-F8, 32, 2 Table 22: Scaling latent dimension for GSQ-GAN-F8 model. The models are detailed in fig. 1b. (b) GSQ-GAN-F8, 64, 4 33 (a) Orignal images (256ˆ256 resolution) (b) GSQ-GAN-F16, 16, 1 (c) GSQ-GAN-F16, 32, 2 Table 23: Scaling latent dimension for GSQ-GAN-F16 model. The models are detailed in fig. 1b. (c) GSQ-GAN-F16, 64, 4 34 (a) Orignal images (256ˆ256 resolution) (b) GSQ-GAN-F32, 16, 1 (c) GSQ-GAN-F32, 32, 2 (d) GSQ-GAN-F32, 64, Table 24: Scaling latent dimension for GSQ-GAN-F32 model. The models are detailed in fig. 1b. (e) GSQ-GAN-F32, 128, 8 35 (a) Orignal images (256ˆ256 resolution) (b) GSQ-GAN-F8, 64, 1 (c) GSQ-GAN-F8, 64, (c) GSQ-GAN-F8, 64, 4 Table 25: Scaling latent dimension for GSQ-GAN-F8-D64 model. The models are detailed in Section 4.3.3. (c) GSQ-GAN-F8, 64,"
        }
    ],
    "affiliations": [
        "CompVis @ LMU Munich",
        "Jülich Supercomputing Centre",
        "TapTap",
        "Tsinghua University"
    ]
}