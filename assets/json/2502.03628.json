{
    "paper_title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering",
    "authors": [
        "Zhuowei Li",
        "Haizhou Shi",
        "Yunhe Gao",
        "Di Liu",
        "Zhenting Wang",
        "Yuxiao Chen",
        "Ting Liu",
        "Long Zhao",
        "Hao Wang",
        "Dimitris N. Metaxas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss -- visually grounded tokens gradually become less favored throughout generation, and (2) early excitation -- semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information -- visually grounded tokens though not being eventually decided still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by abount 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 8 2 6 3 0 . 2 0 5 2 : r The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering Zhuowei Li 1 Haizhou Shi 1 Yunhe Gao 2 Di Liu 1 Zhenting Wang 1 Yuxiao Chen 1 Ting Liu 3 Long Zhao 3 Hao Wang 1 Dimitris N. Metaxas"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits ranking throughout the generation process, revealing three key patterns in how LVLMs pro- (1) gradual visual informacess information: tion loss visually grounded tokens gradually become less favored throughout generation, and (2) early excitation semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information visually grounded tokens though not being eventually decoded still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by about 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies. Code is available at: https: //github.com/LzVv123456/VISTA 1Rutgers University 2Stanford University 3Google DeepMind. Correspondence to: Zhuowei Li <zhuowei.li@rutgers.edu>, Haizhou Shi <haizhou.shi@rutgers.edu>. Pre-print, work in progress 1 Large Vision-Language Models (LVLMs) (Dai et al., 2023; Bai et al., 2023; Chen et al., 2023; Zhu et al., 2023; Liu et al., 2024c) have revolutionized multimodal AI by enabling seamless integration of visual and textual information, powering applications from interactive assistance to autonomous systems (Lin et al., 2023; Yang et al., 2024; Lai et al., 2024). However, LVLMs frequently hallucinate semantically coherent yet visually ungrounded contents, hindering their reliability in real-world applications. Though LVLM hallucination is considered multifaceted (Liu et al., 2024d), critical cause stems from the overwhelming influence of language priors over visual contexts, and has been studied from the perspective of attention patterns (Huang et al., 2024; Liu et al., 2024f) and distribution divergence within logits space (Leng et al., 2024; Favero et al., 2024). Despite these insights, it remains unclear how hallucination emerges and propagates during the generation process. Inspecting Token Dynamics in LVLMs. In this work, we take novel perspective by examining LVLMs generation dynamics through the lens of token logits ranking. Given an image its corresponding description (produced by an LVLM), we identify three categories of tokens (elaborated in Sec. 3.4): Hidden Genuine Tokens tokens that are missing in generated contents yet clearly visible from visual input; Decoded Genuine Tokens tokens that appear in continuation with visual groundings; Hallucinated Tokens tokens extracted from the hallucinated contents within generation. We then track each token types corresponding logits rankings throughout the generation across temporal (Fig. 1 left) and layer sequences (Fig. 1 right). Our analysis makes three prominent observations: (OBS-1) Gradual Visual Information Loss. As generation progresses, genuine token rankings gradually decline while hallucinated tokens are surfaced (see Fig. 1 left and Fig. 2). This aligns with recent findings (Yue VISTA: Visual Information Steering with Token-logit Augmentation Figure 1. Analysis of token logits ranking patterns across 500 randomly selected images from MSCOCO dataset. Higher ranking indicates higher generation probability. Left: Average token ranking from the last five layers, showing temporal progression across early, mid, and late generation stages. Right: Layer-wise evolution of token rankings averaged across all time steps, demonstrating early-excitation phenomenon. et al., 2024; Favero et al., 2024), exhibiting increasing hallucination in late generation phase. We hypothesize that this occurs as accumulated language priors in residual streams progressively dilute visual information, leading to syntactically coherent but visually ungrounded generation. (OBS-2) Early Excitation of Semantically Meaningful Tokens. Semantically meaningful tokens 1 exhibit peak excitation in penultimate layer (Fig. 1 right) or within window of layers preceding the final layer  (Fig. 2)  . In contrast, the final layer prioritizes functional tokens like this, a, and other stopwords, suggesting that the models decision process may overemphasize syntactic elements in its final stage. (OBS-3) Hidden Genuine Information. LVLMs may perceive more visual clues than they express. We observe that hidden genuine tokens, though not eventually decoded, consistently maintain relatively high rankings (around 5K in 32K vocabulary) during the course of generation (see Fig. 1). Reducing Hallucination of LVLMs. Inspired by above findings, we propose VISTA (Visual Information Steering with Token-logit Augmentation), simple and novel training-free framework that can be applied on top of various decoding methods to reduce hallucination while promoting genuine information. VISTA introduces two complementary modules: Visual Steering Vector (VSV) that counteracts gradual visual information loss by extracting and reinforcing visual cues in activation space, and SelfLogits Augmentation (SLA) which utilizes early excitation patterns to prioritize semantically meaningful tokens. VSV and SLA work synergistically and can effectively mitigate hallucination in LVLMs. Technical Contributions. This study presents the first systematic investigation of token dynamics in LVLMs through the lens of token logits ranking, revealing novel insights into how visual information is processed and potentially 1We refer to categorized tokens as semantically meaningful tokens since we primarily focus on identifying objects, attributes, and relations as detailed in Appendix A. 2 Figure 2. Token ranking heatmaps for representative image, demonstrating the evolution of token rankings across model layers (vertical axis) and generation steps (horizontal axis). Darker colors indicate higher ranking. The visualization reveals both gradual visual information loss and early excitation phenomena. lost during generation. Building on these insights, we propose VISTA, an inference-time intervention framework that incorporates two complementary modules. The effectiveness of VISTA is validated through comprehensive experiments across multiple architectures (e.g., LLaVA, Shikra, MiniGPT-4, InstructBLIP) and evaluation protocols (openended generation, visual question answering), demonstrating significant reduction in hallucination (up to around 40% for open-ended generation). Our approach is notably efficient as it requires no additional training or model modifications, making it readily applicable to existing LVLM deployments. 2. Methodology In this section, we first establish the notions and knowledge foundation in Sec. 2.1, followed by the elaboration of token ranking analysis in Sec. 2.2. We then present the proposed VSV and SLA methods in Sec. 2.3 and 2.4, respectively. 2.1. Preliminaries Conditional Generation of LVLMs. Suppose that an LVLM consists of vision encoder and cross-modal interface that projects visual inputs into sequence of visual tokens Xv. Given an input image, the complete prompt tokens Xc are constructed by concatenating system message tokens Xs (can be empty), visual tokens Xv, and query tokens Xq: Xc = concat(Xs, Xv, Xq). At each time step t, the model samples new token xt according to the probability distribution conditioned on both the input context Xc and previously generated tokens X<t = {xi}t1 i=1: xt p(xtXc, X<t) = softmax(H(hL t1)), (1) where denotes the models head layer and hL t1 indicates the hidden state from the last layer at time step t1. Above formulation suggests that the dilution or insufficiency of visual information in hL t1 can bias the generation towards hallucination. VISTA: Visual Information Steering with Token-logit Augmentation Residual Stream. Taking the mathematical interpretation from Elhage et al. (2021), we view layer-wise hidden states as residual streams that evolve recursively: = hl1 hl + al + ml t, (2) and ml where is the layer index, al represent the output activation of integrated multi-head attention (MHA) layer and feed-forward network (FFN), respectively. Within this framework, MHAs facilitate information fusion across different residual streams, while FFNs access and integrate learned parametric knowledge (Geva et al., 2021; Dai et al., 2022). Residual stream provides natural interface for monitoring and controlling information flow, making it particularly suitable for hallucination analysis and mitigation. Logit Lens. The head layer is by default applied on top of last layer hidden states hL. However, thanks to the gradual evolvement of hidden states within residual streams (Chuang et al., 2024), applying to hidden states of earlier layers < remains effective, even without additional training (Gurney, 2023). This practice is commonly referred to as logit lens and can be used to decipher intermediate states. 2.2. Token Ranking Analysis To systematically investigate how visual information is processed during generation, we propose token ranking analysis framework that tracks the relative importance of different tokens throughout the generation process. Identification of Target Tokens. For each given imagedescription pair where the text description is generated by an LVLM (e.g., LLAVA-1.5 (Liu et al., 2024a)), we utilize gpt-4o (Hurst et al., 2024) as the oracle model to identify three categories of words referencing both visual and textual contents. word is decoded genuine word if it appears in the continuation and align with visual evidence; hidden genuine word if it is visually evident but not included in the continuation; hallucinated word if it appears in continuation but lacks visual grounding. Collected words are then tokenized to form our analysis sets. Implementation details are included in Appendix A. Token Ranking via Logit Lens. To analyze token dynamics during generation, we apply the logit lens H(hl t) to each layer and time step t. Given token x, we calculate its ranking position among all possible tokens according to: and layer l. lower rank indicates higher probability. This operation produces 2D ranking matrix for each token with vertical and horizontal axes indicating layers and time steps, respectively. We aggregate these matrices across tokens within each category to obtain category-specific ranking patterns, as shown in Fig. 2. For temporal analysis, we quantize time sequence into three equal-sized buckets, i.e., early, mid, and late, and compute average rankings within each bucket across 500 randomly sampled images from MS COCO dataset (Lin et al., 2014). This temporal view (Fig. 1 left) reveals that visually grounded information is gradually sinking while hallucinated contents are surfaced. We further average rankings across all time steps within layer to provide layer-wise perspective (Fig. 1 right), which exhibits that semantically meaningful tokens achieve peak excitation in the penultimate layer. We refer readers to Appendix A.3 for discussion of why token ranking analysis is desirable and its limitations. 2.3. Visual Steering Vector (VSV) Being aware of the challenge from gradual visual information loss, it is of critical importance to retain visual cues throughout the generation. promising method involves increasing attention weights distributed on visual tokens (Liu et al., 2024f). Nevertheless, this operation simultaneously introduces undesired parametric priors cumulated in residual streams of visual tokens. Drawing inspiration from steering vectors in LLMs (Turner et al., 2023; Zou et al., 2023; Liu et al., 2024e; Li et al., 2024), we propose Visual Steering Vector (VSV) to steer the generation of LVLM towards the direction with visual groundings without amplifying inherent language biases. VSV Construction. The core logic behind VSV is to extract directional vector within activation space without introducing disturbing language priors. To this end, we construct VSV via contrastive process using paired context sequences: positive context Xp = concat(Xs, Xv, Xq) containing visual tokens Xv, and negative counterpart Xn = concat(Xs, Xq) that discards visual tokens while preserving other elements. Both sequences are processed by vectorization function F, which forwards the given token sequence through the LVLM and takes the residual stream from the last token. Visual steering vector (VSV) can be computed as: Vsteer = Vp Vn = {vl steer}L l=1, (4) Rl t(x) = rank(H(hl t), x), (3) where Vp = F(Xp) and Vn = F(Xn). Here, vl to the steering vector for layer l. steer refers where Rl t(x) represents the position of token in the probability-ordered sequence of all tokens at time step Inference-time Intervention. During inference, we inject the visual steering vector into the residual stream at each 3 VISTA: Visual Information Steering with Token-logit Augmentation Augmentation Logits. To elicit the rich semantic information present in the late layers, we calculate augmentation logits oaug at each decoding time step t. These are obtained by applying the head layer to the hidden states of the layers prior to the final layer and averaging their logits: oaug = 1 L1 (cid:88) l=Lw H(hl t), (7) where indicates the window size. In practice, larger leads to improved performance (see Table 4). We hypothesize that this is due to the ever-evolving logits distributions within late layers, especially for those semantically meaningful tokens (Chuang et al., 2024), and applying larger smooths the distribution and provides nuanced information. Logits Ensemble. Augmentation logits are then integrated with the final layer logits through weighted aggregation: ot = (1 γ) oL + γ oaug , (8) = H(hL where oL ) is the logits of the last layer, and γ [0, 1] is constant coefficient controlling the influence of early-layer informationwhen γ = 0, the model reduces to standard generation, while γ = 1 would rely entirely on early-layer logits. The next token is then sampled from the output distribution xt softmax( ot1). Notably, SLA seeks to promote the decoding of more semantically meaningful tokens than truthful tokens as done in DoLa (Chuang et al., 2024). It works synergistically with the VSV module, enhancing overall performance (see Fig. 6). 3. Experiments In this section, we empirically validate VISTA across four architectures, three decoding strategies, and four benchmarks. We first present the experimental configuration (Sec. 3.1), followed by an extensive evaluation on hallucinationspecific and general-purpose benchmarks (Sec. 3.2 and 3.3). We then analyze VISTAs effectiveness in addressing the observed phenomena (Sec. 3.4) and conclude with comprehensive ablation studies (Sec. 3.5). 3.1. Experimental Setup Model Architectures. We evaluate VISTA on four representative LVLMs with distinct architectural designs: LLAVA1.5 (Liu et al., 2024a) and Shikra (Chen et al., 2023), which employ linear projections for visual-textual alignment, and MiniGPT-4 (Zhu et al., 2023) and InstructBLIP (Dai et al., 2023), which utilize Q-former (Li et al., 2023a) for crossmodal interaction. Decoding Strategies. To demonstrate VISTAs versatility as an inference-time intervention method, we verify it across three widely used decoding protocols: (1) greedy decoding, Figure 3. Architectural overview of VISTA. VISTA introduces two complementary mechanisms: VSV extracts and reinforces visual grounding information (Vs) at inference, and SLA leverages earlylayer semantic information to guide token generation. Note: While three separate forward passes are shown for illustration purpose, they can be avoided in implementation. generation step: hl = hl + λvl s, [1, L], (5) with λ controlling the intervention strength, balancing visual fidelity against natural generation. To maintain stability, we normalize the modified hidden states: = hl hl hl hl t2 t2 , [1, L]. (6) Unlike steering vectors in LLMs that capture abstract concept in an amortized fashion leveraging group of contrastive pairs (Zou et al., 2023), VSV computes the steering vector per image basis to preserve crucial visual details unique to each input image. Remark on Difference with Existing Contrastive Strategy. Existing contrastive strategy (Li et al., 2022; Leng et al., 2024) necessitates second negative logits distribution to contrast the final layer logits at all generation steps. VSV, on the other hand, extracts steering vector priori using only the context tokens, and is applied to all layers  (Fig. 3)  . In practice, formatting prompt and general negative prompt (e.g., Describe the image in detail.) can be forwarded only once and cached for future usage, making VSV remarkably efficient (see Table 5). 2.4. Self-Logits Augmentation (SLA) Motivated by early excitation phenomenon (Fig. 1 right), where semantically meaningful tokens show stronger activation in penultimate layer, we propose Self-Logits Augmentation (SLA) to promote the decoding of such tokens. 4 VISTA: Visual Information Steering with Token-logit Augmentation Table 1. CHAIR hallucination evaluation results. We compare VISTA to state-of-the-art training-free methods that do not rely on external supervision. Maximum new token is set to 512. Best and second best results are bolded and underlined, respectively. - indicates the result is not supported by released implementation. Decoding Method LLAVA-1.5 (Liu et al., 2024a) MiniGPT-4 (Zhu et al., 2023) Shikra (Chen et al., 2023) InstructBLIP (Dai et al., 2023) CHAIRS CHAIRI CHAIRS CHAIRI CHAIRS CHAIRI CHAIRS CHAIRI Greedy Beam Search Nucleus Sampling Vanilla DoLa (Chuang et al., 2024) VCD (Leng et al., 2024) PAI (Liu et al., 2024f) VISTA (ours) Vanilla VCD (Leng et al., 2024) OPERA (Huang et al., 2024) PAI (Liu et al., 2024f) VISTA (ours) Vanilla DoLa (Chuang et al., 2024) VCD (Leng et al., 2024) PAI (Liu et al., 2024f) VISTA (ours) 46.4 45.4 47.4 22.8 20.4 49.0 49.8 45.2 22.3 17.4 53.2 47.2 60.8 30.2 24.0 12.1 11.9 13.0 7.0 6.9 12.5 12.4 12.4 6.8 6. 15.1 14.0 16.2 10.3 8.2 35.2 - - 29.2 19.8 33.0 - 26.8 31.6 18.4 34.8 - - 31.8 18.4 10.7 - - 10.9 6.0 11.0 - 9.3 11.2 6. 11.2 - - 13.2 6.4 56.8 60.0 - 40.8 31.4 53.8 - 39.6 41.6 32.2 56.4 56.6 - 43.2 31.8 14.8 15.1 - 11.0 9.7 14.4 - 12.2 10.4 9. 15.9 16.3 - 12.0 9.7 38.0 - 45.8 - 27.4 37.8 49.2 50.2 - 26.8 46.6 - 57.0 - 29.4 10.7 - 12.8 - 8.1 10.7 13.7 13.9 - 7. 13.1 - 16.0 - 9.1 which selects the highest probability token at each step, (2) beam search with beam size of 5, maintaining multiple generation hypotheses, and (3) nucleus sampling with topp=0.9. Temperature is fixed at 1.0 for all scenarios. Baselines. Besides three vanilla decoding strategies, we set aside VISTA with several SoTA hallucination mitigation methods that can operate without external supervision. DoLa (Chuang et al., 2024) signifies internal (across layers) contrastive strategy within logits space, while VCD (Leng et al., 2024) presents parallel contrastive method across time steps. OPERA (Huang et al., 2024) is powerful technique tailored for the beam search. PAI (Liu et al., 2024f) is another inference-time intervention method that is most comparable to ours. We reproduce all baseline results using identical evaluation data and settings (e.g., prompt, temperature). Methods without official supports for certain architectures and decoding strategies are omitted to prevent implementation bias. Implementation Details. We employ the following configuration across all experiments unless stated otherwise. The VSV strength parameter λ is set to 0.17 for both LLAVA-1.5 and InstructBLIP, 0.1 for MiniGPT-4, and 0.12 for Shikra. The SLA mixing coefficient γ is consistently set to 0.3, with window size = 5 for aggregating early-layer logits. We search the hyperparameters of λ and γ on holdout validation set containing 100 images from MSCOCO to balance between generation quality and hallucination reduction. 3.2. Results on Object Hallucination Benchmarks We first evaluate VISTA on two widely adopted benchmarks that assess object hallucination: CHAIR (Rohrbach et al., 2018) for open-ended generation and POPE (Rohrbach et al., 2018) for targeted visual question answering. CHAIR Evaluation. Caption Hallucination Assessment with Image Relevance (CHAIR) (Rohrbach et al., 2018) provides systematic framework for evaluating object hallucination in image captioning tasks. CHAIR assesses caption accuracy by comparing mentioned objects against ground-truth labels, with hallucinations defined as objects present in captions but absent from ground truth. The metinstance-level (CHAIRI) and ric operates at two levels: sentence-level (CHAIRS): CHAIRI = {hallucinated object} {object} and CHAIRS = {caption w/ hallucinated objects} . Following established protocol (Huang et al., 2024; Liu et al., 2024f), we evaluate on 500 randomly sampled images from MSCOCO 2014 validation set, using the prompt Please help me describe the image in detail with maximum generation length of 512 tokens. {caption} Results in Table 1 show that VISTA significantly reduces hallucination in open-ended generation task, outperforming existing inference-time intervention method and other contrastive decoding methods by substantial margin. VISTA brings around 40% relative improvement upon corresponding vanilla decoding methods. Notably, while PAI (Liu et al., 2024f) shows less efficacy for sampling-based decoding, VISTA excels across all decoding strategies. We attribute this robust performance to the contrastive design and the choice of activation space steering which is not hinged with any specific decoding strategy. POPE Evaluation. The Polling-based Object Probing Evaluation (POPE) (Rohrbach et al., 2018) examines object hallucination through targeted visual questions of the form Is there <object> in the image?. The benchmark comprises three splits of increasing difficulty: random objects selected from general vocabulary, frequently occurring objects chosen from common categories, and adversarially selected objects that are contextually plausible but absent from images. We evaluate on the COCO subset, reporting average accuracy and F1 scores across all splits. Since POPE evaluation is formulated as short VQA format and the response is simply Yes or No, the gradual visual VISTA: Visual Information Steering with Token-logit Augmentation Table 2. Evaluation results on POPE benchmark across four LVLMs. Results show averaged accuracy and F1 scores computed across random, popular, and adversarial object splits. Best and second best results are bolded and underlined, respectively. Decoding Method LLAVA-1.5 (Liu et al., 2024a) MiniGPT-4 (Zhu et al., 2023) Shikra (Chen et al., 2023) InstructBLIP (Dai et al., 2023) Avg. Accuracy Avg. F1 Avg. Accuracy Avg. F1 Avg. Accuracy Avg. F1 Avg. Accuracy Avg. F1 Greedy Beam Search Nucleus Sampling Vanilla DoLa (Chuang et al., 2024) VCD (Leng et al., 2024) PAI (Liu et al., 2024f) VISTA (ours) Vanilla VCD (Leng et al., 2024) OPERA (Huang et al., 2024) PAI (Liu et al., 2024f) VISTA (ours) Vanilla DoLa (Chuang et al., 2024) VCD (Leng et al., 2024) PAI (Liu et al., 2024f) VISTA (ours) 84.79 84.92 84.80 85.85 86.15 85.45 85.85 85.68 86.27 85.83 81.26 81.20 81.08 81.92 85.35 85.61 85.67 85.65 86.08 86. 84.93 85.90 85.83 85.91 85.95 82.40 82.44 82.22 83.16 85.54 76.76 - - 75.64 77.06 73.68 - 74.81 73.83 75.96 60.56 - - 61.26 66.96 76.82 - - 77.57 77. 72.40 - 75.42 74.63 77.17 62.04 - - 63.40 68.05 81.32 81.13 - 81.30 82.44 81.73 - 82.18 81.90 82.54 78.94 79.49 - 79.25 81.01 82.01 81.94 - 80.81 82. 82.10 - 82.49 81.08 82.52 80.18 80.72 - 79.87 81.15 84.36 - 84.81 - 84.87 84.38 84.90 85.31 - 85.78 78.83 - 79.61 - 83.11 84.64 - 85.28 - 84. 83.71 84.43 85.51 - 85.74 79.74 - 80.43 - 83.27 Table 3. Overall performance scores on MME full evaluation set. Higher scores indicate better general capability across perception, reasoning, and knowledge-based tasks. Decoding Method LLAVA-1.5 MiniGPT-4 Shikra InstructBLIP Greedy Beam Nucleus Vanilla VISTA Vanilla VISTA Vanilla VISTA 1752.35 1771.87 1749.57 1763.15 1625.22 1738.56 969.93 1041.66 869.74 1062.48 845.30 1069. 1101.50 1256.22 1223.44 1323.25 1069.60 1254.31 1355.25 1364.05 1357.02 1366.57 1397.71 1447. information loss is not evident. We therefore adjust VSV strength to λ = 0.01 to reduce VSVs impact. Results in Table 2 demonstrate VISTAs consistent superiority across models and decoding strategies. Under greedy decoding, VISTA achieves average accuracies of 86.15%, 77.06%, 82.44%, and 84.87% for LLAVA-1.5, MiniGPT-4, Shikra, and InstructBLIP respectively, consistently outperforming both vanilla decoding and PAI. The improvements are particularly pronounced in nucleus sampling, where VISTA achieves significant gains over vanilla decoding across all models: from 81.26% to 85.35% (+4.09%) for LLAVA-1.5, 60.56% to 66.96% (+6.40%) for MiniGPT-4, and similarly substantial improvements for other architectures. These results highlight VISTAs particular efficacy in excavating genuine information under stochastic sampling settings, while maintaining strong performance in deterministic decoding strategies like greedy and beam search. 3.3. Results on Comprehensive Benchmarks We further validate VISTA on MMHal-Bench (Sun et al., 2023) and MME (Fu et al., 2023), two challenging benchmarks that examine diverse aspects of model behavior. MMHal-Bench Evaluation. MMHal-Bench (Sun et al., 2023) provides specialized framework for assessing hallucination in LVLMs through 96 carefully designed imagequestion pairs. The benchmark spans eight distinct categories: object attributes (ATTR), adversarial objects (ADV), comparisons (COMP), counting (COUNT), spatial relations (SPAT), environmental inferences (ENV), holistic descriptions (HOL), and others (OTHER). Unlike conventional VQA evaluations, MMHal-Bench emphasizes logical reasoning and complex visual understanding, providing rigorous test of hallucination mitigation in challenging scenarios. Model responses are evaluated using GPT-4 for alignment with ground-truth answers. We compare VISTA with PAI and vanilla decoding methods for this evaluation. The results in Fig. 4 demonstrate VISTAs consistent effectiveness across all evaluated LVLMs. Compared to vanilla methods, VISTA achieves substantial improvements on average scores, with LLAVA-1.5 and InstructBLIP showing the most pronounced gains (20% and 30% relative improvement, respectively). VISTA particularly excels in challenging categories such as environmental inference (ENV), attribute perception (ATTR) and counting (COUNT), where visual grounding is crucial for accurate responses. While PAI shows competitive performance on specific architectures, VISTA maintains more consistent improvements across both model architectures and question types. This robust performance across diverse tasks indicates that VISTA effectively addresses hallucination while preserving general visual-language capabilities. Results under beam search and nucleus sampling (see Appendix B) exhibit similar performance patterns, confirming VISTAs robustness across different decoding strategies. MME Evaluation. To assess whether hallucination mitigation affects general model capabilities, we evaluate VISTA on MME (Fu et al., 2023), comprehensive benchmark encompassing 14 distinct visual-language abilities. MME tests perception, reasoning, and knowledge integration through carefully curated image-question pairs, providing holistic view of model performance. We conduct full-set evaluation across all architectures and decoding strategies. As shown in Table 3, VISTA, although initially crafted for addressing hallucination, demonstrates broad benefits beyond its primary objective. Performance improvements are 6 VISTA: Visual Information Steering with Token-logit Augmentation Figure 4. Performance comparison on MMHal-Bench across different question categories: attributes (ATTR), adversarial objects (ADV), comparisons (COMP), counting (COUNT), spatial relations (SPAT), environmental inference (ENV), holistic descriptions (HOL), and others (OTHER). Scores are computed using GPT-4 evaluation protocol. Table 4. Impact of window size on SLA performance. Layer ranges (X-31) indicate the span of layers used for logit augmentation, where varies from 27 to 31. CS and CI denote CHAIRS and CHAIRI metrics, respectively. γ 0.1 0.2 0.3 0.4 31-31 CI 13.9 16.4 18.8 18. CS 48.2 56.6 62.0 61.2 F1 76.2 75.3 72.9 73.3 CS 48.6 49.4 55.4 57. 30-31 CI 12.7 14.4 15.7 15.7 F1 77.7 76.5 75.9 75.3 CS 46.8 47.4 49.2 52.6 29-31 CI 12.6 12.7 14.2 14.5 F1 77.4 77.3 76.5 76. CS 46.2 46.8 45.8 48.8 28-31 CI 12.2 12.1 12.4 13.5 74.4 77.7 77.9 77.0 CS 45.8 43.2 42.8 46.6 27-31 CI 11.3 11.7 11.3 12. F1 77.6 77.6 78.4 77.2 3.5. Ablation Study To thoroughly investigate the effectiveness of VISTA, we gauge the practical latency of VISTA, and analyze how different VSV strength (λ) and SLA mixing ratio (γ) affect the models performance in terms of hallucination reduction (CHAIR-S and CHAIR-I metrics) and overall quality (F1 score). Results are in Fig. 6. Additional ablation results are deferred to Appendix B.2. Table 5. Measure of throughput and latency on LLAVA-1.5. Greedy decoding strategy is applied and listed as baseline. Methods Greedy VCD PAI VISTA (ours) Latency (ms/token) Throughput (token/s) 28.54 (1.0) 35.04 (1.0) 58.34 (2.04) 17.14 (0.49) 57.78 (2.02) 17.31 (0.49) 36.32 (1.27) 27.53 (0.79) Efficiency. The greedy decoding latency in Table 5 shows that VISTA is more efficient than the other test-time intervention strategy PAI and contrastive decoding strategy like VCD. VSV Strength (λ). As we vary injection strength λ from 0.0 to 0.18, both CHAIR-S and CHAIR-I scores improve significantly. However, using inappropriate scale (see Fig. 6) will cause clear degradation in F1, suggesting overemphasis on visual features can harm generation quality. SLA Mixing Ratio (γ). The impact of γ is studied across values from 0 to 0.4, revealing that moderate values of γ (0.2-0.3) yield the best balance between hallucination reduction and generation quality, and higher γ values ( 0.4) leads to degraded performance, hinting on the importance of syntactic information. Synergy & Robustness. As visualized in Fig. 6, there exists synergistic relation between λ and γ. Moderate values of both parameters consistently outperform extreme settings of either component alone. Notably, generation Figure 5. Cross-stage token ranking comparison between greedy and VISTA on LLAVA-1.5. VISTA effectively promotes the ranking of genuine tokens while depressing hallucination tokens. consistent across architectures and decoding methods, with particularly striking gains in challenging scenarios. Under nucleus sampling, MiniGPT-4s performance improves substantially from 845.30 to 1069.37, while LLAVA-1.5 maintains strong performance with significant improvement from 1625.22 to 1738.56. These comprehensive gains suggest that VISTAs approach to maintaining visual grounding enhances fundamental visual-language integration mechanisms, leading to better overall model behavior. 3.4. On Solving Gradual Visual Information Loss Our analysis in Sec. 2.2 identifies gradual visual information loss as substantial challenge for long sequence generation. To validate VISTAs effectiveness in addressing this issue, we compare token logits rankings w/ and w/o VISTA throughout generation. According to Fig. 5, VISTA not only improves the average ranking of hidden genuine tokens throughout generation but also reverses the concerning trend of hallucinated tokens, reducing their prominence in mid and late stages where hallucination typically occurs. Not surprisingly, VISTA also maintains high ranking for decoded genuine tokens since VSV captures all visual clues of an image and reinforces them at all time steps. This quantitative evidence directly demonstrates VISTAs capability in maintaining visual grounding throughout the generation process. (see Appendix A.4 for additional results). Case Study. Qualitative analysis further supports our quantitative findings above. We kindly refer readers to Appendix for collection of qualitative examples. 7 VISTA: Visual Information Steering with Token-logit Augmentation Figure 6. Ablation matrices for VSV strength (λ) and SLA mixing ratio (γ) on Shikra. Brighter color signifies the better performance. Red boxes highlight the parameter combinations we used. F1 score is included to demonstrate the overall generation quality. quality remains stable (F1>72.0) across broad range of parameter combinations, demonstrating the robustness of our approach. This complementarity suggests that VSV and SLA address different aspects of the hallucination problem. Window Size in SLA (w). In Table 4, we explore window size from one layer up to five layers across different mixing ratios. As shown, larger window size generally reduces hallucination, and the optimal configuration achieves both strong hallucination reduction and the highest F1 score (78.4). Interestingly, we observe an inverse relationship between window size and optimal γ values, suggesting that while broader layer spans capture richer visual information, they require more conservative mixing ratios to maintain generation stability. 4. Related Work Hallucination Mitigation in LVLMs. Hallucination the generation of content that is irrelevant, factually incorrect, or inconsistent with visual inputs (Bai et al., 2024) represents fundamental challenge in LVLM development. Research has identified three primary sources: limitations in visual encoder capabilities (Tong et al., 2024; Liu et al., 2024b; Shi et al., 2024), excessive reliance on learned parametric knowledge (Li et al., 2023b; Zhou et al., 2023; Leng et al., 2024; Huang et al., 2024), and noisy training data (Liu et al., 2023; Yu et al., 2024). Mitigation approaches span training-based solutions with refined datasets (Yue et al., 2024; Jiang et al., 2024), post-processing techniques including revision (Yin et al., 2023; Zhou et al., 2023) and verification (Chen et al., 2024; Sun et al., 2023), and inference-time interventions like Visual Contrastive Decoding (Leng et al., 2024) and enhanced attention methods (Liu et al., 2024f). Recent studies revealing text inertia (Liu et al., 2024f), where models generate similar hallucinations without visual input, highlight concerning reliance on learned text patterns. While these findings advance our understanding, how hallucination propagates through model architectures remains elusive, and existing solutions often require external supervision and are hinged with specific decoding strategies. Contrastive Decoding in LVLMs. Contrastive decoding, originally introduced in NLP (Li et al., 2022; Shi et al., 2023), has emerged as promising approach for reducing hallucination in LVLMs. Recent adaptations of this technique have explored various contrasting strategies: VCD (Leng et al., 2024) introduces visual-specific contrasts by crafting noisy visual tokens as negative samples, while DoLa (Chuang et al., 2024) innovates by contrasting logits distributions from different layers within the same model, using divergence measurements to dynamically select contrasting layers. Taking temporal perspective, M3ID (Favero et al., 2024) proposes horizontal strategy that contrasts current logits with those from previous timesteps. Other approaches extend contrastive techniques to attention mechanisms (Woo et al., 2024). While these methods primarily operate in the logits space, our VISTA takes different approach by performing contrasts in the activation space and intervening at residual streams. This earlier-stage intervention strategy offers an efficient alternative that can complement existing decoding methods. 5. Conclusion and Limitations This study investigates the hidden life of tokens in Large Vision Language Models (LVLMs) and introduces VISTA (Visual Information Steering with Token-logit Augmentation), lightweight approach to mitigate hallucination. Through systematic analysis, we reveal that visual information gradually attenuates during text generation, but can be effectively restored through our frameworks visual information steering and strategic use of early-layer logits. Extensive experimentation across diverse architectures and decoding strategies demonstrates that our framework significantly reduces hallucination while preserving generation quality. These findings not only illuminate the hidden dynamics of LVLM behavior but also establish visual information steering as promising direction for enhancing the reliability of multimodal AI systems. Limitations. VISTA is subject to several limitations. First, while VISTA demonstrates robustness across range of hyperparameter values, optimal settings may vary across different architectures. Second, the effectiveness of VSV relies on the quality of visual cues extracted by the LVLMs vision encoder models with weak visual encoding capabilities may see reduced benefits. Third, the current implementation focuses on addressing hallucination in single-round tasks; adaptation to interactive scenarios like visual dialogue may require additional considerations. 8 VISTA: Visual Information Steering with Token-logit Augmentation"
        },
        {
            "title": "Impact Statement",
            "content": "This research advances methods for making large visionlanguage models more trustworthy and reliable through mitigating hallucination. While the proposed method demonstrates promising results, its effectiveness is subject to the inherent capability of large vision-language model, and improper usage may adversely affect models performance. To the best of our knowledge, there are no ethical or other concerns that need to be addressed."
        },
        {
            "title": "References",
            "content": "Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: frontier large visionlanguage model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. Bai, Z., Wang, P., Xiao, T., He, T., Han, Z., Zhang, Z., and Shou, M. Z. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930, 2024. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. Chen, Z., Zhao, Z., Luo, H., Yao, H., Li, B., and Zhou, J. Halc: Object hallucination reduction via adaptive focalcontrast decoding. arXiv preprint arXiv:2403.00425, 2024. Chuang, Y.-S., Xie, Y., Luo, H., Kim, Y., Glass, J. R., and He, P. Dola: Decoding by contrasting layers improves factuality in large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=Th6NyL07na. Dai, D., Dong, L., Hao, Y., Sui, Z., Chang, B., and Wei, F. Knowledge neurons in pretrained transformers. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 84938502, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. acl-long.581. URL https://aclanthology.org/ 2022.acl-long.581. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. InstructBLIP: Towards general-purpose vision-language models with instrucIn Thirty-seventh Conference on Neural tion tuning. Information Processing Systems, 2023. URL https: //openreview.net/forum?id=vvoWPYqZJA. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., et al. mathematical framework for transformer circuits. Transformer Circuits Thread, 1:1, 2021. Favero, A., Zancato, L., Trager, M., Choudhary, S., Perera, P., Achille, A., Swaminathan, A., and Soatto, S. Multimodal hallucination control by visual information grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1430314312, 2024. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Geva, M., Schuster, R., Berant, J., and Levy, O. Transformer feed-forward layers are key-value memories. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 54845495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.446. URL https:// aclanthology.org/2021.emnlp-main.446. Gurney, A. Interpreting logit https://www.lesswrong. gpt: The lens. com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens, tober 2023. com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens. cessed: 2024-10-22. OcURL https://www.lesswrong. AcHuang, Q., Dong, X., Zhang, P., Wang, B., He, C., Wang, J., Lin, D., Zhang, W., and Yu, N. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1341813427, 2024. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Jiang, C., Xu, H., Dong, M., Chen, J., Ye, W., Yan, M., Ye, Q., Zhang, J., Huang, F., and Zhang, S. Hallucination augmented contrastive learning for multimodal large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2703627046, 2024. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., and Jia, J. Lisa: Reasoning segmentation via large language VISTA: Visual Information Steering with Token-logit Augmentation In Proceedings of the IEEE/CVF Conference model. on Computer Vision and Pattern Recognition, pp. 9579 9589, 2024. Leng, S., Zhang, H., Chen, G., Li, X., Lu, S., Miao, C., and Bing, L. Mitigating object hallucinations in large visionlanguage models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1387213882, 2024. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023a. Li, X. L., Holtzman, A., Fried, D., Liang, P., Eisner, J., Hashimoto, T., Zettlemoyer, L., and Lewis, M. Contrastive decoding: Open-ended text generation as optimization. arXiv preprint arXiv:2210.15097, 2022. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, X., and Wen, J.-R. Evaluating object hallucination in large visionlanguage models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 292305, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 20. URL https://aclanthology.org/2023. emnlp-main.20. Li, Z., Xu, Z., Han, L., Gao, Y., Wen, S., Liu, D., Wang, H., and Metaxas, D. N. Implicit in-context learning. arXiv preprint arXiv:2405.14660, 2024. Lin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., and Yuan, L. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740 755. Springer, 2014. Liu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., and Wang, L. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2023. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Improved reasonJanuary 2024b. https://llava-vl.github.io/blog/ and Lee, Y. J. ing, ocr, URL 2024-01-30-llava-next/. and world knowledge, Llava-next: Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024c. Liu, H., Xue, W., Chen, Y., Chen, D., Zhao, X., Wang, K., Hou, L., Li, R., and Peng, W. survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253, 2024d. Liu, S., Ye, H., Xing, L., and Zou, J. In-context vectors: Making in context learning more effective and controllable through latent space steering, 2024e. Liu, S., Zheng, K., and Chen, W. Paying more attention to image: training-free method for alleviating hallucination in lvlms. arXiv preprint arXiv:2407.21771, 2024f. Rohrbach, A., Hendricks, L. A., Burns, K., Darrell, T., and Saenko, K. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018. Shi, M., Liu, F., Wang, S., Liao, S., Radhakrishnan, S., Huang, D.-A., Yin, H., Sapra, K., Yacoob, Y., Shi, H., et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. Shi, W., Han, X., Lewis, M., Tsvetkov, Y., Zettlemoyer, L., and Yih, S. W.-t. Trusting your evidence: Hallucinate less with context-aware decoding. arXiv preprint arXiv:2305.14739, 2023. Sun, Z., Shen, S., Cao, S., Liu, H., Li, C., Shen, Y., Gan, C., Gui, L.-Y., Wang, Y.-X., Yang, Y., et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., and Xie, S. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024. Turner, A. M., Thiergart, L., Udell, D., Leech, G., Mini, U., and MacDiarmid, M. Activation addition: Steering language models without optimization, 2023. Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines In Proceedings of the with visual instruction tuning. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024a. Wang, L., Li, L., Dai, D., Chen, D., Zhou, H., Meng, F., Zhou, J., and Sun, X. Label words are anchors: An information flow perspective for understanding in-context learning. In Bouamor, H., Pino, J., and Bali, K. (eds.), 10 VISTA: Visual Information Steering with Token-logit Augmentation Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 98409855, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 609. URL https://aclanthology.org/2023. emnlp-main.609. Woo, S., Kim, D., Jang, J., Choi, Y., and Kim, C. Dont miss the forest for the trees: Attentional vision calibration for large vision language models. arXiv preprint arXiv:2405.17820, 2024. Yang, R., Song, L., Li, Y., Zhao, S., Ge, Y., Li, X., and Shan, Y. Gpt4tools: Teaching large language model to use tools via self-instruction. Advances in Neural Information Processing Systems, 36, 2024. Yin, S., Fu, C., Zhao, S., Xu, T., Wang, H., Sui, D., Shen, Y., Li, K., Sun, X., and Chen, E. Woodpecker: Hallucination correction for multimodal large language models. arXiv preprint arXiv:2310.16045, 2023. Yu, Q., Li, J., Wei, L., Pang, L., Ye, W., Qin, B., Tang, S., Tian, Q., and Zhuang, Y. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1294412953, 2024. Yue, Z., Zhang, L., and Jin, Q. Less is more: Mitigating multimodal hallucination from an eos decision perspective. arXiv preprint arXiv:2402.14545, 2024. Zhou, Y., Cui, C., Yoon, J., Zhang, L., Deng, Z., Finn, C., Bansal, M., and Yao, H. Analyzing and mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2310.00754, 2023. Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Zou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren, R., Pan, A., Yin, X., Mazeika, M., Dombrowski, A.-K., Goel, S., Li, N., Byun, M. J., Wang, Z., Mallen, A., Basart, S., Koyejo, S., Song, D., Fredrikson, M., Kolter, J. Z., and Hendrycks, D. Representation engineering: top-down approach to ai transparency, 2023. 11 VISTA: Visual Information Steering with Token-logit Augmentation GPT-4o Prompt You are vision-language evaluator. Given an image and an AI-generated description, perform the following tasks: 1. List clearly visible contents in the image that are not mentioned in the description. 2. List hallucinated contents in the description that are not present in the image. 3. List contents accurately described in the description that match the image. For each task, include objects, object properties (e.g., color, count, position), and relationships between objects. You must answer each content with single word, separating different contents by commas. If no contents apply, write None. Make sure there is no overlapping words between three tasks. Answer 1: [Missing contents] Answer 2: [Hallucinated contents] Answer 3: [Accurate contents] Table 6. The prompt used for GPT-4o to identify genuine and hallucinated words. A. Implementation Details for Token Ranking Analysis A.1. Prompt for GPT-4o We employ GPT-4o as our oracle model for identifying three categories of tokens: hidden genuine tokens, decoded genuine tokens, and hallucinated tokens. The precise prompting strategy used to elicit these classifications is detailed in Table 6. A.2. Additional Implementation Details detailed token analysis algorithm is provided in Algorithm 1. Token Processing. As shown in Algorithm 1, we derive genuine and hallucinated tokens from their corresponding word-level classifications. In cases where single word decomposes into multiple tokens under the models tokenization scheme, we adopt the first token as representative proxy for the entire word. This approach ensures consistent handling of multi-token words while maintaining analytical tractability. Ranking Aggregation Protocol. When computing cross-stage token rankings (visualized in Fig. 1 left), we implement focused aggregation strategy that considers only the final five layers of the model, deliberately excluding rankings from earlier layers. This methodological choice mitigates the inherent embedding disparity between the models decoding layer and preceding layers. Since the LLMs decoding head is specifically trained on final-layer hidden states, the reliability of token rankings decreases with distance from this layer. Consequently, we restrict our analysis to window of layers proximate to the final layer to ensure robust and meaningful ranking estimates. A.3. Advantages of Token Ranking Analysis Why Not Attention? While attention matrices have been extensively studied in LVLM hallucination research to understand information flow patterns and inform mitigation strategies (Huang et al., 2024; Liu et al., 2024f), our token ranking methodology offers several distinct advantages. Previous work, such as PAI (Liu et al., 2024f), has attributed hallucination phenomena like text inertia to insufficient attention allocation to visual tokens. However, this interpretation is potentially confounded by the presence of anchor tokens (Wang et al., 2023; Huang et al., 2024) that aggregate and redistribute information across the network. The existence of these information hubs means that reduced attention weights on visual tokens, particularly in later layers where visual information can be accessed indirectly through anchor positions, may not necessarily indicate information loss. Token ranking analysis, by contrast, provides more direct insights into the models processing of visual information. Through explicit tracking at the token level, this approach enables quantitative measurement of visual information preservation throughout the generation process. The methodology reveals gradual visual information degradation patterns that might be obscured in attention-based analyses. Furthermore, token ranking analysis uncovers previously unobserved phenomena such as hidden genuine information and early excitation patterns, which are not readily distinguishable through attention analysis alone. These capabilities make token ranking analysis particularly well-suited for investigating the mechanisms underlying hallucination in LVLMs. 12 VISTA: Visual Information Steering with Token-logit Augmentation for = 1 to do for = 1 to do hl GetHiddenState(M, l, t) {Eq. 3} logits H(hl t) {Eq. 4} for each token in Tc do Algorithm 1 Token Ranking Analysis Framework Require: Image I, LVLM model , Oracle model Ensure: Token ranking matrices for each category 1: /* Generate description using LVLM */ 2: (I) 3: /* Classify tokens using Oracle */ 4: Wdec, Whid, Whal O(I, D) {decoded, hidden, hallucinated} 5: Tdec, Thid, Thal Tokenize(Wdec, Whid, Whal) 6: for each token category Tc in {Tdec, Thid, Thal} do 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for 17: /* Compute stage-wise aggregation */ 18: for each stage in {early, mid, late} do 19: 20: 21: 22: 23: 24: end for output {Ranking matrices for all categories} Ts GetTimeSteps(s) {timesteps in stage s} Lf GetFinalLayers() {final 5 layers} Rc end for t(x) rank(logits, x) {per Eq. 4} for each category in {dec, hid, hal} do t(x) Ts, Lf , Tc}) mean({Rl"
        },
        {
            "title": "Rl\nend for",
            "content": "end for end for Limitations. Our token ranking analysis approach also pose certain limitations: 1. Oracle Model Reliability: While GPT-4o serves as our oracle for identifying genuine and hallucinated content, this process can introduce potential biases and uncertainties. The oracle models classifications may not perfectly align with human judgments, and its own biases could influence the categorization of tokens. This is particularly challenging for nuanced cases where the distinction between genuine and hallucinated content is subtle. 2. Embedding Space Discrepancy: Another limitation arises from applying the LLMs decoding head to hidden states from earlier layers. Since the decoding head is specifically trained on final-layer representations, there exists an embedding space misalignment when analyzing preceding layers. This discrepancy becomes more pronounced for layers distant from the final layer, potentially leading to less reliable token rankings in earlier stages of the network. While our analysis mitigates this by focusing on layers proximate to the last layer, the issue remains inherent to the methodology. Despite above limitations, our analysis provides valuable insights into LVLMs behavior and has proven effective in motivating our hallucination mitigation approach. A.4. Additional Token Ranking Analysis In the main text, we present token ranking analysis results for LLAVA-1.5. Here, we extend this analysis to other architectures to demonstrate the generalizability of our observations. As shown in Fig. 7, the cross-stage token ranking analysis on Shikra exhibits similar patterns to those observed in LLAVA-1.5, with genuine tokens experiencing gradual rank degradation while hallucinated tokens become increasingly prioritized across generation stages. The layer-wise analysis presented in Fig. 8 further corroborates the early excitation phenomenon, where semantic tokens achieve peak activation in layers preceding the 13 VISTA: Visual Information Steering with Token-logit Augmentation Figure 7. Cross-stage token ranking on Shikra. Figure 8. Layer-wise token rankings on Shikra. final decoding layer. Fig. 9 presents comparative analysis between vanilla decoding and VISTA (greedy-based) on Shikra. The results demonstrate VISTAs effectiveness in maintaining the ranking of genuine tokens throughout the generation process while simultaneously suppressing the promotion of hallucinated tokens. This pattern is consistent with our findings for LLAVA-1.5, suggesting that the phenomena we identified and the effectiveness of our mitigation strategy generalize across different LVLM architectures. The consistency of these patterns across architectures with distinct design choices (linear projector in Shikra versus Q-former in other models) provides strong evidence for the fundamental nature of these phenomena in LVLM generation dynamics. B. Additional Experiments B.1. MMHal-Bench Results For Other Decoding Strategies We further report results of MMHal-Bench under beam search  (Fig. 10)  and nucleus sampling  (Fig. 11)  . As demonstrated in figures, VISTA consistently improves overall performance across all evaluated LVLMs under both decoding strategies. The performance trends remain consistent with those observed under greedy decoding in the main text, further validating the robustness of our approach across different inference strategies. These comprehensive results demonstrate that VISTAs mechanisms for maintaining visual grounding and promoting semantic richness are effective regardless of the chosen decoding strategy. 14 VISTA: Visual Information Steering with Token-logit Augmentation Figure 9. Cross-stage token ranking comparison between greedy and VISTA (greedy-based) on Shikra. VISTA effectively promotes the ranking of genuine tokens while depressing hallucination tokens. Figure 10. Performance comparison on MMHal-Bench using beam search. Figure 11. Performance comparison on MMHal-Bench using nucleus sampling. 15 VISTA: Visual Information Steering with Token-logit Augmentation Figure 12. Ablation matrices for VSV strength (λ) and SLA mixing ratio (γ) on MiniGPT-4. Brighter color signifies the better performance, and red boxes highlight the parameter combinations used in Table 1. F1 score is included to indicate the overall generation quality. Figure 13. Ablation matrices for VSV strength (λ) and SLA mixing ratio (γ) on LLAVA-1.5. Brighter color signifies the better performance, and red boxes highlight the parameter combinations used in Table 1. F1 score is included to indicate the overall generation quality. Figure 14. Ablation matrices for VSV strength (λ) and SLA mixing ratio (γ) on InstructBLIP. Brighter color signifies the better performance, and red boxes highlight the parameter combinations used in Table 1. F1 score is included to indicate the overall generation quality. 16 VISTA: Visual Information Steering with Token-logit Augmentation B.2. Additional Ablation Results In addition to the ablation studies presented in the main text, we further provide detailed ablation results on LLAVA1.5, MiniGPT-4, and InstructBLIP to validate our hyperparameter choices. Figure 12 presents the ablation matrices for MiniGPT-4, examining the impact of VSV strength (λ) and SLA mixing ratio (γ) on CHAIRS, CHAIRI, and F1 scores. The results reveal similar trends to those observed in Shikra, though with slightly lower optimal λ value of 0.1, suggesting architecture-specific sensitivity to visual steering. For LLAVA-1.5 (Figure 13), the ablation matrices demonstrate particularly strong performance improvements with stronger VSV value (λ = 0.17). The InstructBLIP results (Figure 14) show robust performance across broader range of parameter combinations, with optimal performance achieved at λ = 0.17 and γ = 0.3, matching the configuration used for LLAVA-1.5. Across all architectures, we observe consistent pattern where moderate values of both VSV strength and SLA mixing ratio yield the best balance between hallucination reduction and generation quality. This consistency, despite architectural differences, validates the generality of our approach while highlighting the need for architecture-specific fine-tuning of hyperparameters for optimal performance. C. Case Study In this section, we extend our qualitative evaluation across all four architectures to demonstrate VISTAs effectiveness in reducing hallucination while promoting genuine information. Figures 15-18 present comparative examples between vanilla decoding and VISTA for LLAVA-1.5, MiniGPT-4, Shikra, and InstructBLIP respectively. For each example, hallucinated content is highlighted in red, while genuine information that was previously omitted or wrongly described but successfully recovered by VISTA is marked in blue. The examples demonstrate VISTAs consistent ability to reduce hallucination across different architectures while eliciting pertinent visual details that vanilla decoding fails to capture. 17 VISTA: Visual Information Steering with Token-logit Augmentation Figure 15. Case study for LLAVA-1.5. 18 VISTA: Visual Information Steering with Token-logit Augmentation Figure 16. Case study for MiniGPT-4. 19 VISTA: Visual Information Steering with Token-logit Augmentation Figure 17. Case study for Shikra. 20 VISTA: Visual Information Steering with Token-logit Augmentation Figure 18. Case study for InstructBLIP."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Rutgers University",
        "Stanford University"
    ]
}