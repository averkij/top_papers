{
    "paper_title": "Bridging Text and Video Generation: A Survey",
    "authors": [
        "Nilay Kumar",
        "Priyansh Bhandari",
        "G. Maragatham"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-video (T2V) generation technology holds potential to transform multiple domains such as education, marketing, entertainment, and assistive technologies for individuals with visual or reading comprehension challenges, by creating coherent visual content from natural language prompts. From its inception, the field has advanced from adversarial models to diffusion-based models, yielding higher-fidelity, temporally consistent outputs. Yet challenges persist, such as alignment, long-range coherence, and computational efficiency. Addressing this evolving landscape, we present a comprehensive survey of text-to-video generative models, tracing their development from early GANs and VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these models work, what limitations they addressed in their predecessors, and why shifts toward new architectural paradigms were necessary to overcome challenges in quality, coherence, and control. We provide a systematic account of the datasets, which the surveyed text-to-video models were trained and evaluated on, and, to support reproducibility and assess the accessibility of training such models, we detail their training configurations, including their hardware specifications, GPU counts, batch sizes, learning rates, optimizers, epochs, and other key hyperparameters. Further, we outline the evaluation metrics commonly used for evaluating such models and present their performance across standard benchmarks, while also discussing the limitations of these metrics and the emerging shift toward more holistic, perception-aligned evaluation strategies. Finally, drawing from our analysis, we outline the current open challenges and propose a few promising future directions, laying out a perspective for future researchers to explore and build upon in advancing T2V research and applications."
        },
        {
            "title": "Start",
            "content": "Preprint Bridging Text and Video Generation: Survey Nilay Kumar , Priyansh Bhandari , G. Maragatham Department of Computational Intelligence SRM Institute of Science and Technology, KTR {nl9459, pr6479, maragatg}@srmist.edu.in 5 2 0 O 6 ] . [ 1 9 9 9 4 0 . 0 1 5 2 : r Abstract Text-to-video (T2V) generation technology holds immense potential to transform multiple domains such as education, marketing, entertainment, and assistive technologies for individuals with visual or reading comprehension challenges, by creating coherent visual content from natural language prompts. From its inception, the field has advanced from adversarial models to diffusion-based models, yielding higher-fidelity, more temporally consistent outputs. Yet multiple challenges still persist in these domains, such as alignment, long-range coherence, and computational efficiency. Addressing this evolving landscape, we present comprehensive survey of text-to-video generative models, tracing their development from early GANs and VAEs to current hybrid Diffusion-Transformer (DiT) architectures, detailing how these models work internally, what limitations they addressed in their predecessors, and why shifts toward new architectural paradigms were necessary to overcome challenges in quality, coherence, and control. We provide systematic account of the datasets, which the surveyed text-to-video models were trained and evaluated on, and, to support reproducibility and assess the accessibility of training such models, we detail their training configurations, including their hardware specifications, GPU counts, batch sizes, learning rates, optimizers, epochs, and other key hyperparameters. Further, we outline the evaluation metrics commonly used for evaluating such models and present their performance across standard benchmarks, while also discussing the limitations of these metrics and the emerging shift toward more holistic, perception-aligned evaluation strategies. Finally, drawing from our analysis, we outline the current open challenges and propose few promising future directions, laying out perspective for future researchers to explore and build upon in advancing T2V research and applications. Authors Note (October 2025): This paper surveys text-to-video generation models and benchmarks up to Q1 2025. Since its initial completion, several new models have emerged (e.g., the Veo series by Google, the Sora series by OpenAI, and further models released by Luma Labs, Runway, Kling AI, and others). An updated survey is planned for future release to incorporate these developments. Until then, this version provides detailed foundation covered in the rest of the paper."
        },
        {
            "title": "1 Introduction",
            "content": "Deep learning has brought about paradigm shift in generative modeling, enabling significant advancements across vision, language, and cross-modal domains. particularly impactful area is text-to-image (T2I) synthesis, where models such as DALLE [1], CLIP [2], and Stable Diffusion [3] have shown remarkable capabilities in generating high-fidelity images from natural language descriptions. Building on the success of T2I, recent research efforts have begun addressing the more complex task of text-to-video (T2V) generation, where the goal is to synthesize temporally coherent and semantically aligned video sequences from textual Corresponding author 1 Preprint prompts. T2V extends beyond static visual depiction, requiring the modeling of motion dynamics, object persistence, temporal transitions, and scene continuity over time. This inherently makes the problem more demanding than T2I, both in terms of model architecture and computational overhead. Additionally, the alignment between natural language and visual content becomes significantly more intricate when extended to dynamic scenes, often involving multiple interacting objects and evolving spatial contexts. Despite these challenges, T2V presents immense potential across range of real-world applications. In education, it can facilitate the visualization of abstract processes or physical phenomena, thereby enhancing comprehension and learner engagement. In accessibility contexts, it offers means to convey information visually for users with reading difficulties or visual impairments. Within marketing and entertainment, T2V enables scalable production of product demonstrations, personalized media, and animated storytelling. However, several critical obstacles still impede its broader adoption. Ensuring smooth temporal consistency is vital to avoid visually jarring transitions [4]. Semantic alignment between text and video remains core challenge, particularly in multi-object or action-rich scenes [5]. The task is further compounded by the high computational costs associated with video generation [6] and the limited availability of large-scale, high-quality text-video datasets [7], both of which restrict scalability and generalization. The field has introduced wide range of models aimed at addressing the challenges of text-to-video (T2V) generation by extending techniques from text-to-image (T2I) synthesis to incorporate temporal dynamics. GODIVA [8], one of the earlier models, employed autoencoders and attention mechanisms to efficiently generate short video clips. UWA [9] proposed unified framework that considered not only text but also image and video modalities, albeit with significantly increased computational demands. In response to the scarcity of large-scale paired video datasets, models such as CogVideo [10] utilized extensive text-image datasets; however, aligning textual descriptions with temporally evolving visual content remained substantial challenge. Make-A-Video [11] explored unsupervised zero-shot generation, but the resulting visual quality was often poor and lacked consistency. More recent research has focused on improving efficiency, temporal coherence, and output diversity. VideoFusion [12] introduced probabilistic framework to minimize redundancy while preserving coherent motion across frames. LatentShift [13] incorporated temporal modeling within the latent space of T2I models, offering significant reductions in computational complexity. FreeBloom [14] integrated large language models to enhance narrative quality, although maintaining visual consistency across frames remains persistent challenge. VideoTetris [15] pushed the boundaries of multisubject video generation through improved compositionality and identity preservation. Similarly, models like FIFO-Diffusion [16] and Pyramidal Flow Matching [17] proposed hierarchical generation strategies for long-form videos, aiming to balance high-resolution quality with temporal continuity. Evaluating the performance of T2V models continues to be complex issue. While quantitative metrics offer scalable benchmarking, they often fail to capture nuanced aspects of visual realism, semantic alignment, and temIn contrast, systematic human evaluation provides deeper insight into these subjective poral coherence. qualities, particularly in assessing alignment with textual prompts. Establishing standard evaluation protocols, combining both human and metric-based methods, will be essential to ensure fair comparisons and consistent progress across the field. Training T2V models also demands substantial computational resources and high-quality, well-curated datasets. This paper analyzes the commonly used datasets, evaluating their scale, diversity, and suitability for supporting realistic and expressive video generation. The availability of richly annotated, diverse datasets remains critical factor in achieving high-quality T2V outputs. Despite rapid advancements, the field still lacks comprehensive and unified overview that brings together model architectures, evaluation strategies, training protocols, and data resources. Existing surveys [18, 19] address individual components but fall short of delivering an integrated, in-depth review. This survey addresses that gap by systematically analyzing state-of-the-art models, evaluation methods, benchmark datasets, and training configurations. The main contributions of this paper include: detailed overview of the development of T2V models, covering their core architectures, training 2 Preprint methodologies, and design principles. The paper outlines the progression from early autoencoderbased approaches to recent diffusion-based frameworks, highlighting key innovations and their impact on the field. comprehensive review of the datasets commonly used for T2V model training, focusing on their scale, diversity, and content characteristics. The discussion also includes typical training setups and computational requirements, emphasizing the infrastructure necessary for effective model development. An analysis of the evaluation metrics employed in the T2V domain, with attention to current benchmarking practices. The paper also examines the role of human evaluations in assessing realism and semantic alignment, and reviews emerging protocols aimed at improving evaluation reliability. discussion of the central challenges facing T2V research, including limited data availability, high computational costs, and difficulties in modeling temporal consistency. The paper outlines potential solutions, such as synthetic data generation, architectural adjustments, and improved temporal modeling, while emphasizing the need to broaden real-world applications. The rest of the paper is organized as follows: Section 2 covers the preliminaries. Section 3 presents the methodologies. Section 4 details the datasets and training configurations. Section 5 outlines the evaluation metrics and benchmarks. Section 6 discusses key challenges and future directions, and finally, Section 7 concludes the paper."
        },
        {
            "title": "2 Preliminaries",
            "content": "The development of text-to-video (T2V) models has been deeply shaped by progress in foundational architectures originally designed for image synthesis and sequence modeling. To understand how T2V systems generate coherent videos from text prompts, it is important to first establish the key architectural and learning paradigms that underpin this field. These include encoderdecoder frameworks for spatial understanding, probabilistic methods for latent representation learning, adversarial strategies for realism, diffusion-based sampling for high-fidelity generation, and attention mechanisms for capturing long-range dependencies across modalities and time. Each of these foundations has played pivotal role in enabling the transition from static image generation to dynamic, temporally consistent video synthesis. In the subsections that follow, we briefly introduce these core components to provide the necessary context for understanding the evolution and design of modern T2V models."
        },
        {
            "title": "2.1 U-Net",
            "content": "U-Net [20] belongs to the class of convolutional neural networks [21] that were specifically designed for dense prediction tasks that require both global context and local detail preservation. U-Net is built up of two core components: (1) an Encoder and (2) Decoder, that work together to transform input features into detailed output maps. In the U-Net framework, the network architecture is symmetric in the shape of U. The network is built of an encoder module, which represents the contracting path, and decoder module, which represents the expanding path. In this approach, the encoder gathers contextual information by continuously decreasing the input images spatial scale and simultaneously enhancing the richness of feature representations through subsequent layers of convolution, followed by pooling operations, as shown in Eqs. (1)(2): 3 Preprint 𝑓𝑙 = 𝜎(𝑊𝑙 𝑓𝑙1 + 𝑏𝑙) 𝑓𝑙 = Pool( 𝑓𝑙) (1) (2) where 𝜎() is nonlinear activation function like ReLU [22], 𝑊𝑙 and 𝑏𝑙 are the weights and biases, and Pool typically refers to max pooling [21]. The decoder reconstructs the spatial dimensions and refines feature maps for precise localization. Upsampling is performed by concatenating the upsampled features with corresponding encoder features via skip connections [23], followed by convolutional operations, as shown in Eqs. (3)(5): 𝑓 𝑙 = Up( 𝑓𝑙+1) 𝑓 concat 𝑙 , 𝑓𝐸𝑙 ] 𝑙 = [ 𝑓 𝑓𝑙 = 𝜎(cid:0)𝑊 𝑙 𝑓 concat 𝑙 + 𝑏 𝑙 (cid:1) (3) (4) (5) where Up() denotes upsampling (e.g., transposed convolution [24]), and 𝑓𝐸𝑙 are the encoder feature maps at level 𝑙. U-Net learns mapping from an input image RHW to an output segmentation map RHW by minimizing the cross-entropy loss, denoted in Eq. (6): (𝜃; x, y) = HW 𝑖=1 𝑘=1 y𝑖,𝑘 log p𝑖,𝑘 (6) where 𝑝𝑖,𝑘 is the predicted probability for pixel 𝑖 and class 𝑘. By minimizing this loss, U-Net effectively captures abstract semantic information through its encoder and retains detailed spatial features via its decoder and skip connections. This dual capability makes the U-Net an extremely strong architecture for tasks that call for both global understanding and pixel-level precision, extending its application from segmentation into wide array of image transformation and generation tasks as demonstrated in [2529]."
        },
        {
            "title": "2.2 Variational Auto-Encoders (VAEs)",
            "content": "Variational Auto-Encoders (VAEs) [30] are class of deep generative models built on the principles of Bayes theorem [31], aiming to learn continuous latent representations of data. VAE is composed of two neural networksan encoder and decoderthat collaboratively model complex data distributions. In the VAE framework, the goal is to model an unknown data distribution p(x) by introducing latent variables that capture the underlying structure of the data. The encoder network q𝜙 (zx), parameterized by 𝜙, approximates the intractable posterior over latent variables conditioned on the observed data. It encodes the input into latent distribution, effectively capturing salient features in compressed form. The decoder network p𝜃 (xz), parameterized by 𝜃, models the likelihood of the data conditioned on latent variables and reconstructs the input from its encoding. The joint probability of the observed data and the latent variables is defined in Eq. (7): p𝜃 (x, z) = p𝜃 (xz) p(z) (7) Here, p(z) denotes the prior over the latent space, usually assumed to follow standard normal distribution (0, I). The term p𝜃 (xz) defines the generative process through which the decoder reconstructs the input data. Training VAE involves maximizing the Evidence Lower Bound (ELBO) [32], variational objective that serves as proxy for the marginal log-likelihood of the observed data, defined in Eq. (8): L(𝜃, 𝜙; x) = Eq𝜙 (zx) (cid:2) log p𝜃 (x z)(cid:3) KL(cid:0)q𝜙 (z x) p(z)(cid:1) (8) 4 Preprint The first term, Eq𝜙 (z x) [log p𝜃 (x z)], represents the expected reconstruction accuracy, guiding the decoder to produce outputs that resemble the original input x. The second term, KL(q𝜙 (z x) p(z)), is the Kullback-Leibler divergence, acting as regularizer that aligns the learned posterior with the prior and encourages smoothness in the latent space. By optimizing the ELBO with respect to both 𝜙 and 𝜃, VAEs balance reconstruction quality and latent space regularity. Once trained, they can generate new samples by sampling from the prior p(z) and decoding these samples through p𝜃 (x z), thereby capturing the essential structure of the original data distribution."
        },
        {
            "title": "2.3 Generative Adversarial Networks (GANs)",
            "content": "GANs [33] are type of deep generative model that frames data creation as an interplay between two neural nets: the neural nets being the (1) Generator and the (2) Discriminator. These networks engage in competitive process, with each network refining its ability to generate or distinguish data. GANs utilize adversarial training to learn complex data distributions without explicitly modelling probability density functions. The generator G𝜃 (z), parameterized by 𝜃, transforms random noise vectors from prior distribution pz(z) (typically standard normal distribution) into synthetic data samples x. The discriminator D𝜙 (x), parameterized by 𝜙, evaluates whether given sample is real (from pdata(x)) or fake (produced by G𝜃 ). The GAN framework aims to have the generators distribution pg(x) closely approximate the true data distribution pdata(x). This is achieved through minimax optimization objective, denoted in Eq. (9): Ladv(𝜃, 𝜙) = min𝜃 max𝜙 (cid:2)E𝑥 𝑝data ( 𝑥) log 𝐷 𝜙 (𝑥) + E𝑧 𝑝𝑧 (𝑧) log(cid:0)1 𝐷 𝜙 (𝐺 𝜃 (𝑧))(cid:1)(cid:3) (9) In this setup, the discriminator D𝜙 gets better in maximizing its accuracy in distinguishing real from fake samples, while the generator G𝜃 strives to minimize the discriminators ability to make correct classifications. Training involves iteratively updating both networks: the discriminator maximizes its objective LD(𝜙), and the generator minimizes its objective LG(𝜃). However, to improve gradient flow and enhance training stability, the generators loss is often redefined as in Eq. (10): G(𝜃) = Ezpz (z) (cid:2)log D𝜙 (G𝜃 (z))(cid:3) (10) By repetitively optimizing this objective, GANs learn to generate realistic data samples that can hardly be distinguished from the real data. While the adversarial training allows the generator to capture highly intricate data distributions, the discriminator continuously adapts to detect extremely subtle differences between generated and real data. This has opened up numerous use cases in the fields of computer vision and image synthesis like image generation [33], style transfer [28], image-super-resolution [26], powered by the strong generative capabilities of GANs."
        },
        {
            "title": "2.4 Denoising Diffusion Probabilistic Models (DDPMs)",
            "content": "DDPMs [34] are sophisticated family of deep learning based generative models that generate data using progressive denoising, drawing inspiration from non-equilibrium thermodynamics. DDPM functions in two stages: (a) forward diffusion phase where Gaussian noise is gradually introduced to the input, and (b) backward phase that aims to reconstruct the original input by incrementally removing the noise. It leverages the simplicity of the Gaussian distribution together with the representation power of neural networks to model complex distributions without explicit likelihood estimation."
        },
        {
            "title": "2.4.1 Unconditioned DDPMs",
            "content": "In unconditioned DDPMs, the forward diffusion process gradually converts an initial data point into sequence of noisy latent variables by adding Gaussian noise at each timestep. This process is regulated by 5 Preprint variance schedule, where each value in the schedule determines the noise intensity at specific timestep, denoted in Eq. (11): q(𝑥𝑡 𝑥𝑡 1) = (𝑥𝑡 ; (11) Here, q(𝑥𝑡 𝑥𝑡 1) denotes the probability distribution of the noisy variable at timestep 𝑡 given 𝑡1; is Gaussian distribution; 𝑥𝑡 and 𝑥𝑡 1 are the noisy latent variables; 𝛽𝑡 controls the noise added; and is the identity matrix for uniform noise application. The aggregate spread at any timestep 𝑡 is defined as in Eq. (12): 1 𝛽𝑡 𝑥𝑡 1, 𝛽𝑡 I) (12) Here, q(𝑥𝑡 𝑥0) represents the aggregate spread of 𝑥𝑡 at timestep 𝑡 given 𝑥0, where 𝛼𝑡 = 1 𝛽𝑡 and 𝛼𝑠, which control the noise schedule. The reverse denoising process reconstructs the initial data 𝛼𝑡 = (cid:206)𝑡 point from sequence of noisy variables, as defined in Eq. (13): 𝑠=1 𝛼𝑡 𝑥0, (1 𝛼𝑡 )I) q(𝑥𝑡 𝑥0) = (𝑥𝑡 ; (13) Here, p𝜃 (𝑥𝑡 1𝑥𝑡 ) models the reverse denoising to estimate 𝑥𝑡 1 from 𝑥𝑡 . 𝜇 𝜃 (𝑥𝑡 , 𝑡) is mean predicted 𝑡 is variance, either fixed or learnable, that adjusts noise reduction at each step. p𝜃 (𝑥𝑡 1𝑥𝑡 ) = (𝑥𝑡 1; 𝜇 𝜃 (𝑥𝑡 , 𝑡), 𝜎2 by neural network, and 𝜎2 Training involves minimizing the loss function, denoted in Eq. (14): 𝑡 I) where x𝑡 = 𝛼𝑡 x0 + Lsimple(𝜃) = Ex0, 𝜖 ,t (cid:2)𝜖 𝜖 𝜃 (x𝑡 , t)2(cid:3) (14) 1 𝛼𝑡 𝜖 and 𝜖 (0, I). This objective trains the model to accurately predict and remove the added noise, enabling the generation of high-fidelity samples from pure noise. 2.4.2 Conditioned DDPMs Conditioned DDPMs extend the unconditioned framework by incorporating additional information to guide the generation process, enabling the creation of data that aligns with specific attributes or inputs. This conditioning is achieved through two primary methods: classifier guidance [35] and classifier-free guidance [36]. (i) Classifier Guidance: In Classifier Guidance, the model estimates the likelihood of target attribute based on noisy inputs. During the reverse diffusion process, it uses gradient information to refine predictions, as shown in Eq. (15): 𝑝 𝜃 (𝑥𝑡 1 𝑥𝑡 , 𝑦) = (cid:0)𝑥𝑡 1 ; 𝜇 𝜃 (𝑥𝑡 , 𝑡) + 𝑠 𝜎 𝑡 𝑥𝑡 log 𝑝 𝜙 (𝑦 𝑥𝑡 ) , 𝜎2 𝑡 𝐼(cid:1) (15) where p𝜙 (y x𝑡 ) serves as an external classifier to guide the prediction, adjusts the strength of guidance, and the gradient x𝑡 log p𝜙 (y x𝑡 ) steers sampling toward regions aligned with the target attribute y. (ii) Classifier-Free Guidance: Classifier-free guidance integrates conditioning directly into the denoising model, removing the need for an external classifier. The model 𝜖 𝜃 (x𝑡 , t, y) is trained to predict noise with and without conditioning information by randomly omitting during training. At inference, the prediction is adjusted as in Eq. (16): ˆ𝜖 𝜃 (𝑥𝑡 , 𝑡 𝑦) = (1 + 𝑤) 𝜖 𝜃 (𝑥𝑡 , 𝑡 𝑦) 𝑤 𝜖 𝜃 (𝑥𝑡 , 𝑡) (16) Here 𝑤 denotes the guidance scale, which fine-tunes the balance between sample diversity and fidelity to the conditioning information. Both guidance methods enhance DDPMs ability to generate conditioned data, enabling applications like conditional image synthesis and text-to-image generation [3] by effectively steering the denoising process toward desired attributes. 6 Preprint"
        },
        {
            "title": "2.5 Transformers",
            "content": "Transformers [37] represent revolutionary class of deep models that have brought sea change in NLP and now find growing applications in computer vision and studies on generation [3739]. Transformer models utilize mechanisms of self-attention so as to model dependencies between input and output sequences. This enables the parallel computation of the input and allows for capturing long-range interactions efficiently. This architecture is made up of two major modules: Firstly, The Encoder and Secondly, The Decoder, both of which include multiple layers; commonly these are made up of self-attention modules and feed-forward neural network layers. The encoder is designed to identify contextual relationships using self-attention, whereas the decoder is tasked with creating output sequences based on attention to the encoder representations and previously generated outputs. At the core of Transformers is the self-attention mechanism, defined as in Eq. (17): Attention(Q, K, V) = softmax (cid:19) (cid:18) QK d𝑘 (17) The formula in Eq. (17) defines how the attention mechanism computes the interaction between query (Q), key (K), and value (V) matrices, which originate from input embeddings. Here, d𝑘 represents the size of the key vectors. This module enables the model to dynamically assess and prioritize different elements in sequence. The multi-head attention mechanism improves model performance by distributing inputs across various subspaces, which helps the model identify wide range of relationships. The formula for this is shown in Eqs. (18)(19). MultiHead(Q, K, V) = Concat(head1, . . . , headℎ) W𝑂 head𝑛 = Attention(QW 𝑄 𝑛 is the projection matrix for queries, 𝐾 𝑄 𝑛 , KW 𝐾 𝑛 , VW𝑉 𝑛 ) 𝑛 for keys, and 𝑉 Here, 𝑛 for values, transforming the respective inputs to align with specific attention functions. W𝑂 is the final projection matrix that combines all attention heads outputs into single output vector. Training Transformers involves minimizing the cross-entropy loss for sequence generation, denoted in Eq. (20): (𝜃) = 𝑇 𝑡=1 log 𝑃(y𝑡 y<𝑡 , x) (20) In Eq. (20), y𝑡 represents the token generated at position 𝑡, y<𝑡 denotes the tokens that come before it, and refers to the original series of input data. By refining this loss function, Transformers can efficiently grasp complex dependencies and produce outputs of superior quality. These models excel at interpreting the nuanced meanings of input data, resulting in well-organized outputs, as result, making Transformers essentially indispensable for lot of the sequence-to-sequence applications like translation [37], summarization [40], and generative modelling [38]. Its with this flexibility and scalability, that Transformers have become the foundational architectures in modern deep learning applications."
        },
        {
            "title": "3 Methodologies",
            "content": "Various deep learning approaches are contributing remarkably towards in-text-to-video generation with each solving unique aspects of synthesizing temporally coherent videos from textual descriptions. Broadly, host of such approaches can be put under the umbrella of three such model architecture classes. Ordered by the evolution of such model classes are (a) GANs, then (b) VAEs and finally (c) DDPMs. GAN-based 7 (18) (19) Preprint approaches were one of the first to be applied to generating text-to-video problems, relying on an adversarial training process between its generator and discriminator networks in order to create realistic video content. Although such models showed early video synthesis success, they are generally plagued with issues of stability during training and scaling to higher resolutions. Another point of view was introduced by VAEsbased models: they learn compact latent representation of videos that enables more controlled generation by probabilistic modeling. Such models appeared to be more stable than GANs but sometimes struggled with the quality of their output and fine details. More recently, the dominant paradigm has been the emergence of diffusion-based models for the synthesis of text-based video generation that exhibit superior quality and have more temporal consistency. These models which have extended successful results of diffusion-based approaches for image generation have achieved high-quality video generations with semantic alignments to input text. The overriding trend for using diffusion models is captured in many recent developments in this class of methods, as will be seen by the lengthy list of methods in this survey based on diffusion. Each of the paradigms enjoys different advantages and issues while solving the inherent problems of video generation, namely temporal consistency, visual quality, and text-video alignment. In the following sections, thorough review of representative models from each category is provided, including discussions of architecture, methodology, and contribution."
        },
        {
            "title": "3.1 GANs Based Models",
            "content": "GANs were introduced in 2014 by Goodfellow et al. in [33], immediately causing revolution in generative modeling by introducing the adversarial training paradigm. It is made up of two competing neural networks: one, the generator, generates data samples; the other, the discriminator, distinguishes between real and generated samples. When they were first designed for generating images, GANs immediately started working art wonders, managing to generate several high-quality realistic images. This immediately called for significant changes in the architecture of GANs, including temporal dynamics and consistency between frames. The different approaches and novelties authors have developed to extend GANs for video generation are discussed as follows. 3.1.1 MoCoGAN MoCoGAN [6], as illustrated in Fig. 1, makes use of decomposed GAN architecture for video generation where the latent space gets decomposed into content subspace and motion subspace. Such separation enables the model to handle static elements, i.e., content, and dynamic variations, i.e., motion, separately. This is controlled by single content vector, sampled from Gaussian distribution and kept constant throughout the video, encoding features that remain consistent across frames, like the object identity or background. The changes with respect to time are described by the motion subspace through the generation, on each frame, of motion vectors via the RNN [41] with Gated Recurrent Unit [42], introducing recurrent path to be able to emulate smooth, sequential motion. MoCoGAN treats the video sequences as trajectories in latent space, where each point in that latent space takes frame, and variations in path length allow for videos of variable duration. It contains one generator that composites single fixed content vector with frame-specific motion vectors and two discriminators: an image discriminator for ensuring coherence in individual frames and video discriminator for temporal coherence across sequences of frames. The discriminator in the video also includes categorical dynamics through predefined motion categories that can generate MoCoGAN with specific actionssuch as walking and jumpinginput, given categorical variable. 8 Preprint Figure 1: Model Architecture of MoCoGAN from [6]."
        },
        {
            "title": "3.1.2 N ¨UWA",
            "content": "N UWA presented in [9] has unified architecture of 3D Transformer encoder-decoder, which leverages tokenization from the 2D VQ-GAN [43] and uses 3D Nearby Self-Attention (3DNA) for holistic visual Images and videos are synthesis across images, videos, and textual descriptions, displayed in Fig. 2. represented as 3D spatial-temporal cube where each token in the cube corresponds to patch of the visual input. Synchronized by VQ-GAN, this is patch-based approach that tokenizes frames into discrete latent codes mapped to shared codebook, hence allowing unified representation of both images and videos but decreased data complexity while preserving essential information in space and time. The 3D Nearby Self-Attention mechanism in UWA is focused on enhancing efficiency by considering only the spatial and temporal neighborhoodsthat is, where each token attends to only the tokens in the near neighborhood inside the cube. In addition, the attention model in this work conveys local dependencies while considering coherency across frames as one of the majors for generating realistic visuals for long sequences. Training of the model is incorporated with multi-task objectives for supporting tasks like video prediction, text-toimage, and text-to-video generation where conditioning generation is either based on the conditioning text or previous frames through masked token prediction within the 3D spatial-temporal cube. This allows the model to create the missing components by capturing contextual dependencies within the spatial domain as well as in the temporal dimension. Figure 2: Model Architecture of UWA from [9]."
        },
        {
            "title": "3.2 VAEs Based Models",
            "content": "VAEs, First proposed in [30], are suite of deep generative models which try to learn to compress data to lower latent space and later try to rebuild it back as similar as it can to the original data, trying to maintain the most important features. Unlike GANs, VAEs provide probabilistic framework for generation and explicitly model the data distribution through variational inference. Their ability to learn structured latent 9 Preprint representations made them particularly attractive for complex generative tasks. The next sections discuss how researchers have adapted and extended the VAE framework to video generation tasks by incorporating temporally modelling and text conditioning mechanisms."
        },
        {
            "title": "3.2.1 VideoGPT",
            "content": "VideoGPT [44] implements hybrid architecture, which merges VQ-VAE [45] with GPT-style transformers [46] for efficient naturalistic video generation. It makes use of 3D convolutions [47] in the VQ-VAE framework for extracting deep spatial and temporal features from the videos and compressing them into discrete latent space. Such compression reduces the data dimensionality extensively but preserves the most salient information that is critical for maintaining quality in generated videos. These 3D convolutions are used by the encoder component to transform the high-dimensional video data into lower-dimension and discrete latent representation. The decoder reconstructs video from this compressed format using transposed 3D convolutions that guarantee detailed and accurate visual outputs. The model treats the latent space by first discretizing raw video into discrete tokens through some process of vector quantization, as shown in Fig. 3. The encoder outputs get matched to learned codebook to reduce the video data into simple, manipulable codes. This projects such dense video representations into finite set of discrete codes; therefore, it summarizes video content in very effective way. The integration then involves GPT-style autoregressive transformer model which predicts these latent codes sequentially with the goal of generating video frames that are coherent temporally. This involves learned spatio-temporal position encodings [46] to help maintain continuity both of space and time between frames. Figure 3: Model Architecture of VideoGPT from [44]. 3.2.2 GODIVA GODIVA, introduced in [8], integrates frame-wise VQ-VAE with 3D sparse attention [48] for video generation conditioned on natural language descriptions, as shown in Fig. 4. In the model, the internal VQ-VAE framework processes frames independently: each frame is encoded into latent variables via 3D convolutions accurately quantized by matching encoded regions against codebook. This results in discrete and compact latent representation, which is reconstructed by the decoder through transposed 3D convolutions [49] with the preservation of the critical spatial and temporal information. VQ-VAEs training objective in GODIVA contains loss functions such as reconstruction loss, codebook loss, and commitment loss to ensure proper frame encoding and decoding. GODIVA generates video using text based on the modeled conditional probability 𝑃(𝑧 𝑡), where 𝑧 stands for the discrete latent code, and 𝑡 stands for the input text. Text embeddings combine the outputs from pre-trained embeddings with positional encodings to temporally align text and video. GODIVA processes the temporal, row, and column information of the encoded frames separately with its three-dimensional sparse attention mechanism that optimizes computational efficiency by attending only to the relevant spatio-temporal features. 10 Preprint Figure 4: Model Architecture of GODIVA from [8]. 3.2.3 CogVideo CogVideo [10] implements dual-channel transformer-based architecture for large-scale text-to-video generation, as shown in Fig. 5. Within its framework, the spatial and temporal information is separately treated by the dual-channel attention blocks. It contains two major types of attention channels in its architecture: an attention-base layer pre-trained for spatial features and an attention-plus layer designed for temporal processing. Further, these channels dynamically combine through learnable mixture factor, which effectively weights the adaptation between spatial and temporal components. This transformer-based architecture effectively generates aligned videos through the incorporation of several multi-modal attention mechanisms. The generation of the video pipeline, designed to work in multi-stage process, first goes through sequential generation phase at low frame rates, creating keyframes, and is followed by recursive interpolation phase Its architecture features two important to eventually create intermediate frames for temporal coherence. components, including the 3D Shifted Window attention [50] that earmarks the efficient processing of the spatial-temporal cube by applying localized attention to neighboring spatial and temporal tokens. This rewindowing strategy minimizes memory usage while allowing parallelization of spatial and temporal dimensions. The architecture includes mechanisms for conditioning on the frame rate, giving control on the output frame rate to secure alignment with textual specifications."
        },
        {
            "title": "3.3 Diffusion Models",
            "content": "Building upon the theoretical developments of Sohl-Dickstein et al. in 2015, recently emerged as one of the successful generative models. These models work progressively to denoise random Gaussian noise in manner guided by learned reverse diffusion process. Their success in generating images through models like DALLE [1] and Stable Diffusion [3] has led to their rapid adoption for video synthesis. Of course, diffusion models boast several advantages, such as stable training dynamics and generation of high quality. In the subsequent sections, the paper describes the various approaches in relation to extending diffusion models towards video generation by considering temporal consistency, computational efficiency, and alignment between input texts and generated videos. 11 Preprint Figure 5: Model Architecture of CogVideo from [10]. 3.3.1 Make-A-Video Make-A-Video [11] follows multi-stage diffusion-based architecture that generates videos leveraging pre-trained text-to-image (T2I) model and has no need for paired text-video datasets, as shown in Fig. 6. The architecture uses pre-trained T2I diffusion model, spatiotemporal layers to capture temporal dynamics, and frame interpolation network to better the continuity between frames. At the beginning of the process, the model follows pre-trained T2I diffusion model to generate base frames at 64 64 resolution. These frames are later enhanced by two specialized super-resolution networks: SRt is spatiotemporal network that increases the resolution of the frames to 256 256 and ensures temporal coherence, whereas SRh is used for spatial super-resolution, increasing the resolution further up to 768 768. The architecture extends T2I capabilities into video synthesis through the use of Pseudo-3D convolution and attention layers. This design efficiently processes spatiotemporal relationships while optimizing computational resources. The attention mechanism makes use of Pseudo-3D attention layers that apply spatial and temporal attentions in sequence, creating trade-off between memory efficiency and temporal coherence. Finally, performs masked frame insertion for free frame rate control and transitions. Figure 6: Model Architecture of Make-A-Video from [11]."
        },
        {
            "title": "3.3.2 VideoFusion",
            "content": "VideoFusion [12] proposes Decomposed Diffusion Probabilistic Model architecture for generating highquality, temporally coherent video, as shown in Fig. 7. It is defined through the decomposition of video 12 Preprint Figure 7: Model Architecture of VideoFusion from [12]. synthesis into two complementary generators. It depends on hierarchical noise decomposition, emitting base noise shared between frames and residual noise with only time-specific variations, hence enabling the modeling of both spatial consistency and dynamic frame-to-frame variation. The architecture mainly consists of two parts: (a) base generator utilizing pretrained image DPM to handle the static, shared visual content across frames, and (b) residual generator, introducing temporal details by adding residual noise which adapts the base output to capture the frame-specific motion. During training, both generators are jointly trained to equilibrate the content generated by the pretrained image DPM with temporal refinement from the residual generator so that every frame in the video sequence is coherent with the previous one. The residual generator works complementarily with the base generator to predict frame-wise noise that selectively changes each frame while keeping the continuous coherence of the base content, balancing computational efficiency with temporal dynamics. 3.3.3 Latent-Shift Latent-Shift [13] adapts pretrained latent diffusion model [3] towards text-to-video generation through parameter-free temporal shift module included in the models architecture, as shown in Fig. 8. The system first trains VAE-based autoencoder over images to learn the latent representation and then adapts toward encoding and decoding video frames independently, thus efficiently encoding the spatial information across frames in the latent space. At the heart of Latent-Shift is Temporal Shift U-Net to learn the denoising of latent video representations. During training, the U-Net processes the latent representation of each frame by sampling different steps of diffusion, and during inference, it progressively denoises from an initial noise distribution until clean frame sequence is achieved. The architecture is made up of two key modules: 2D ResNet [23] blocks that include convolutional layers, and transformer modules which are additionally modified with spatial attention mechanisms. The proposed temporal shift module is adjusted within the residual branch of each ResNet block, which allows the feature maps to move across the temporal dimension. This setup allows for temporal coherence between frames without requiring additional parameters. Text inputs condition the generated video. With cross-attention mechanism in transformer blocks, it incorporates text embeddings into spatial attention layers and aligns the generated video content with the provided prompts."
        },
        {
            "title": "3.3.4 Free-Bloom",
            "content": "Free-Bloom [14] presents zero-shot, training-free pipeline with large language models and pretrained latent diffusion models that transform text prompts into high-quality videos without any video data or extra training, as shown in Fig. 9. The overall architecture of the model is divided into three sequential modules: serial prompting, video generation, and interpolation empowerment. During the Serial Prompting stage, it is the director LLM that turns single-text prompt into detailed frame-by-frame serialization describing action development through time while keeping semantic coherence across frames. pre-trained LDM during Video Generation acts as the animator: it generates individual frames based on the generated prompt sequence. To achieve the dual goals of temporal and identical coherence, the backward diffusion process is 13 Preprint Figure 8: Model Architecture of Latent-Shift from [13]. modified in two main ways: first, joint noise sampling, reaching progressive joint for the noise in order to more effectively balance the content coherence with variation; second, step-aware attention shift, shifting the LDMs U-Net architecture modification of self-attention layers between contextual frames and the current frame, with the variation determined by the denoising time step, to favor identical coherence with semantic alignment. The Interpolation Empowerment module further improves temporal resolution and continuity and interpolates intermediate frames with the Dual-Path Interpolation strategy, comprising Contextual Path that performs linear interpolation between the latent variables of neighboring frames and Denoising Path that applies DDIM denoising conditioned on interpolated text embeddings. Figure 9: Model Architecture of Free-Bloom from [14]. 3.3.5 LaVie LaVie [51] presents cascaded framework of Video Latent Diffusion Models (V-LDMs) that leverages pre-trained Text-to-Image (T2I) models for generating high-quality videos, coherent over time, without losing much creative generation capability, as shown in Fig. 10. This model architecture is ordered into three sequential modules: the Base Text-to-Video (T2V) model, which creates the initial video frames from text input; the Temporal Interpolation (TI) module, which is designed to enhance temporal consistency between frames; and the Video Super-Resolution (VSR) module, which improves the resolution and quality of the generated video. In the Base T2V stage, LaVie extends the pre-trained T2I model, namely Stable Diffusion, to support video data with minimal architectural changes: pseudo-3D convolutions that inflate the 2D kernel to include temporal dimension and the Spatio-Temporal Transformer (STTransformer) enhanced with Rotary Positional Embedding (RoPE) [52], capable of capturing rich temporal correlations in video sequences. It jointly trains the model on image and video data to avoid catastrophic forgetting, retaining the richness of creative diversity the T2I model provides. For Temporal Interpolation, the TI model increases temporal resolution by raising the frame rate of the base video. text-conditioned diffusion U-Net In the VSR model, interpolates between keyframes to yield smooth motion with richer temporal detail. 14 Preprint super-resolution of the videos is boosted by incorporating temporal attention along with 3D convolutional layers into diffusion-based image upscaler. By doing this, the model performs fine-tuning by adjusting the newly added temporal layers while keeping the pre-trained spatial layers fixedthus improving visual quality while maintaining temporal coherence. Figure 10: Model Architecture of LaVie from [51]."
        },
        {
            "title": "3.3.6 DreamVideo",
            "content": "DreamVideo [53] presents an image-to-video generation framework that effectively combines an Image Retention block with pre-trained VLDM for video generation from single reference image with textual prompts, as shown in Fig. 11. The main components of the architecture are divided into two parts: the T2V model and the Image Retention block. T2V expands pre-trained VLDM with Temporal Attention modules (Temp-Attn) and Temporal Convolutional layers (Temp-Conv) inside the U-Net architecture to enable the model to process spatio-temporal relationships in video sequences. Down-sampling and middle layers are followed by the Image Retention block, which adds more convolutional layers to process the It merges the image features into the denoising process by concatenating them with the input image. latent video representations at various levels of U-Net. Classifier-Free Guidance is used with double session to balance image and text conditions during training and inference. This is done by nullifying each condition independently during training, avoiding overfitting, while enabling adjustable guidance scales during inference to modulate the influence of each modality. The framework also supports two-stage inference to generate longer video sequences or new actions by reusing the final frame of one video as the starting frame for the next with updated prompts. Figure 11: Model Architecture of DreamVideo from [53]."
        },
        {
            "title": "3.3.7 Grid-Diffusion",
            "content": "Grid-Diffusion [54] is diffusion-based framework that reformulates video synthesis as an image generation problem by adopting grid-based representation, as shown in Fig. 12. The model is structured around two main modules: keyframe grid generation and autoregressive grid image interpolation. Both modules 15 Preprint Figure 12: Model Architecture of Grid-Diffusion from [54]. collaborate to translate textual descriptions into temporally coherent videos. The key grid image generation component uses fine-tuned Stable Diffusion model to generate 22 grid of chronologically ordered frames summarizing the temporal flow of the video. This grid effectively anchors motion and content information using T2I capabilities. The Autoregressive Grid Image Interpolation module contains two sub-models: 1-Step and 2-Step Interpolation Model. These operate over masked grid images in an autoregressive fashion, conditioned on previously generated frames, to create intermediate frames and preserve temporal continuity. This setup ensures temporal consistency while avoiding visual artifacts or discontinuities in the output. 3.3.8 FIFO-Diffusion FIFO-Diffusion [16] proposes training-free inference framework for pre-trained diffusion models that enables infinite-length video generation from text prompt, as shown in Fig. 13. The architecture comprises three main modules: Diagonal Denoising, Latent Partitioning, and Look-ahead Denoising. In the Diagonal Denoising module, frames are processed in queue-based mechanism following first-in-first-out (FIFO) order. As fully denoised frames exit the queue, new noisy frames are added, keeping memory usage constant. Latent Partitioning splits the queue into multiple frame blocks to reduce inter-frame noise level variation, improving alignment with the models training distribution and allowing parallel computation. Lookahead Denoising enhances forward referencing capabilities by selectively updating frames and incorporating downsampled future frames, giving cleaner context during denoising. Figure 13: Model Architecture of FIFO-Diffusion from [16]."
        },
        {
            "title": "3.3.9 VideoTetris",
            "content": "VideoTetris [15] is compositional diffusion-based text-to-video framework capable of generating multiobject videos with control over attributes, spatial layout, and temporal dynamics, as shown in Fig. 14. It is composed of three major modules: Spatio-Temporal Compositional Diffusion, Enhanced Video Data 16 Preprint Figure 14: Model Architecture of VideoTetris from [15]. Preprocessing, and Consistency Regularization. The Spatio-Temporal Compositional Diffusion module performs: (i) Prompt Decomposition into frame-wise sub-prompts with region masks; (ii) Cross-Attention Manipulation for sub-prompt alignment; (iii) Spatio-Temporal Composition for coherent temporal attention integration. Enhanced Video Preprocessing supports extended video generation through optical flow scoring for motion diversity and language model-based prompt expansion for semantic enrichment. Consistency Regularization ensures object fidelity using reference frame encoding and reference frame attention mechanism, maintaining spatial integrity across frames via region-specific cross-attention. 3.3.10 GVDIFF GVDIFF [55] is text-to-video generation framework that integrates both discrete and continuous grounding conditions, as shown in Fig. 15. Its architecture comprises: Uncertainty-Based Grounding Injection, the Spatial-Temporal Grounding Layer (STGL), and Dynamic Gate Network (DGN), built atop pretrained Latent Diffusion Model (LDM). Discrete grounding elements include layouts and keypoints, while continuous ones involve depth and edge maps (e.g., HED, Canny). These are encoded into uncertaintybased representations that guide the U-Nets attention via modified self-attention layers. The STGL updates transformer layers to handle spatial and temporal grounding, linking grounding features with visual tokens and ensuring alignment across frames. The DGN adaptively skips or engages grounding attention per layer, allowing low-level details in early layers and semantic features in deeper ones. This results in both efficiency and high-fidelity grounded video generation."
        },
        {
            "title": "3.3.11 CogVideoX",
            "content": "CogVideoX [56] implements diffusion-transformer pipeline for text-to-video generation, converting textual prompts into extended-duration videos without requiring additional video data or training, as shown in Fig. 16. Its architecture comprises several core components: 3D Causal VAE, an Expert Transformer with Expert Adaptive LayerNorm, and Progressive Training framework enhanced with Multi-Resolution Frame Packing. The 3D Causal VAE compresses spatial and temporal content through causal convolutions, which ensure that information from future frames is not used in generating past framespreserving temporal causality. This results in efficient video encoding while maintaining high fidelity. The Expert Transformer fuses 17 Preprint Figure 15: Model Architecture of GVDIFF from [55]. modalities by concatenating text embeddings and video latents along the sequence dimension. It employs Expert Adaptive LayerNorm modules that adjust normalization parameters differently for each modality, modulated by diffusion timestep inputs. This aligns the numerical scale and feature space between text and video inputs, enabling effective cross-modal fusion. The Progressive Training framework, combined with Multi-Resolution Frame Packing, enables the model to process videos of varying lengths and resolutions within single batch. 3D Rotary Positional Embedding (3D-RoPE) is employed to encode relative positions across spatial and temporal dimensions, enhancing coherence and scalability. Figure 16: Model Architecture of CogVideoX from [56]."
        },
        {
            "title": "3.3.12 Pyramidal Flow",
            "content": "Pyramidal Flow [17] introduces diffusion-transformer video generation framework that integrates pyramidal visual representations with flow-matching mechanisms to improve computational efficiency, as shown 18 Preprint in Fig. 17. The model architecture is built around two main modules: the Spatial Pyramid and the Temporal Pyramid, all within unified Diffusion Transformer backbone. The Spatial Pyramid decomposes the denoising trajectory spatially across hierarchical stages. Instead of operating on full-resolution video throughout, the model performs denoising at progressively increasing spatial scales, reaching full resolution only in the final pyramid stage. To ensure continuity and consistency across resolutions, Pyramidal Flow Matching is employed to match piecewise flows between noisy and clean latent representations, aligning distributions and enabling smoother transitions between pyramid levels. Corrective Renoising scheme is applied at each stage to ensure distribution alignment and stability during denoising. The Temporal Pyramid addresses autoregressive video generations temporal challenges using Pyramidal Temporal Conditioning. It compresses temporal history into progressively coarser representations, which are used as conditioning signals for future frame prediction. This pyramidal temporal structure reduces both memory usage and computational complexity while maintaining temporal fidelity across long sequences. Figure 17: Model Architecture of Pyramidal Flow from [17]."
        },
        {
            "title": "4 Training Datasets and Configurations",
            "content": "The diversity and quality of the training datasets, along with proper training configurations, serve as the stepping stones in effectively training text-to-video generation models. The given section provides summary of the datasets which were utilized to train such T2V models and also provides details of their training configurations."
        },
        {
            "title": "4.1 Training Datasets",
            "content": "The diversity and quality of datasets are of immense importance in developing text-to-video generation models, as it provides foundational data necessary for the training and the performance assessment of such models. Through this section, the paper mentions the datasets used across different papers and gives detailed descriptions of the datasets that aided the training efforts of the various models."
        },
        {
            "title": "4.1.1 WebVid-10M",
            "content": "WebVid-10M [57] dataset comprises total of 10.7 million videos tagged with their corresponding text pairs, amounting to close to 52,000 hours of video data. Sourced primarily from stock footage sites such as Shutterstock, each short video is captioned with the respective textual description. First introduced by Bain et al. in the work Frozen in Time, the dataset allows for joint video and image encoding for retrieval tasks. Due to copyright restrictions, the dataset originally provided URLs and captions for independent downloading; however, as of February 23, 2024, these resources may no longer be available following cease-and-desist request from Shutterstock. 19 Preprint"
        },
        {
            "title": "4.1.2 UCF-101",
            "content": "UCF-101 [58] dataset comprises close to 13,320 videos which cover actions performed by humans and are classified into 101 classes that range from sports and musical performances to human-object interactions. Extracted from YouTube, it provides wide range of dynamic camera movements, object appearances, poses, scales, viewpoints, background clutter, and lighting conditions, making it challenging benchmark for training and evaluating action recognition models."
        },
        {
            "title": "4.1.3 HowTo100M",
            "content": "HowTo100M [59] dataset is made up of approximately 136 million video segments created from 1.22 million narrated tutorial videos, covering over 23,000 unique tasks, such as culinary arts, handicrafts, and physical fitness. Each clip includes automatically transcribed subtitles from YouTube, enabling models to learn text-video embeddings from real-world instructional content. Due to copyright restrictions, only URLs and captions were provided for independent downloading, but these resources are no longer accessible as of February 23, 2024. 4.1.4 LAION-5B LAION-5B [60] dataset is an extensive collection of 5.85 billion image-text pairs curated to democratize access to large-scale training data for image-text models. It includes 2.3 billion English-language pairs and 2.2 billion pairs in over 100 other languages. Sourced from the Common Crawl repository [61], it focuses on images with associated alt-text descriptions, filtered using OpenAIs CLIP model [2] to retain pairs with high image-text similarity. 4.1.5 VATEX VATEX [62] dataset is large-scale multimodal dataset, made up of over 41,250 unique videos. Each video includes annotations with 10 captions in English and 10 in Chinese, resulting in total of 825,000 captions. The dataset covers breadth of 600 categories of human activities and supports both multilingual video captioning and video-guided machine translation. 4.1.6 HD-VILA-100M HD-VILA-100M [63] dataset includes approximately 100 million high-definition video clips along with their textual descriptions, totaling about 371,500 hours at 720p each. It spans 15 distinct categories, including sports, cooking, and travel, offering rich spatiotemporal contextualization for training video-language models. Each clip averages 13.4 seconds in length with an average of 32.5 words in its description."
        },
        {
            "title": "4.1.7 Tai-Chi-HD",
            "content": "Tai-Chi-HD [64] is collection of high-resolution videos (180 in total) showing sequences of Tai Chi moves running between 128 and 1,024 frames. The videos, sourced from YouTube, are cropped to focus on complete human bodies performing Tai Chi, serving as benchmark for high-quality motion transfer and video generation models."
        },
        {
            "title": "4.1.8 MUG Facial Expression",
            "content": "MUG Facial Expression [65] dataset contains high-resolution image sequences from 86 subjects performing six basic facial expressions: disgust, fear, anger, happiness, sadness, and surprise. Each sequence includes 20 Preprint Table 1: Training Configurations of Survey Models Model Name MoCoGAN [6] VideoGPT [44] GODIVA [8] UWA [9] CogVideo [10] Make-A-Video [11] VideoFusion [12] Latent-Shift [13] Free-Bloom [14] LaVie [51] DreamVideo [53] Grid Diffusion [54] FIFO-Diffusion [16] VideoTetris [15] GVDIFF [55] CogVideoX [56] Pyramidal Flow [17] GPU - - V100 - A100 A100 - - RTX 3090 Ti - - A100 80GB - A800 - - GPU Count - - 64 - 32 2048 - - 1 - 8 2 - 4 - - 128 Batch Size - 128 512 - 256 - - 256 - - 9 28/20 - 2 - - 1536/768/384 Learning Rate - 1 104 1 104 - 1 104 - - 1 105 - - - - - 1 105 - - 1 104, 5 105 Optimizer Adam Adam Adam - Adam - - Adam - - - - - - - - AdamW Steps/Epochs - - 200k steps - 1M steps - - - - - 5 epochs (340k samples) 82k/54k steps - 16k steps - - 300k total steps Loss Function Adversarial Cross-entropy Cross-entropy - Cross-entropy Standard diffusion - MSE (latent space) - MSE (latent space) MSE - - - - L1+LPIPS+KL+GAN Flow matching Dropout Rate - - - - - - - 0 - - - - - 0.05 - - - onset, apex, and offset stages, starting and ending with neutral expression. The dataset includes 1,462 sequences at 896896 resolution, along with facial landmark annotations. 4.1.9 Weizmann Action Weizmann Action [66] dataset consists of 90 low-resolution videos featuring nine subjects performing ten different actions such as bending, jumping, running, and waving. Captured using static camera and uniform backgrounds, the dataset supports fundamental human action recognition research. 4.1.10 BAIR BAIR [67] dataset comprises video sequences of robotic arm interacting with tabletop objects. It contains about 44,000 sequences of 30 frames each at 6464 resolution, capturing the dynamics of physical interactions. It has been widely used in action-conditioned video prediction studies. 4.1.11 TGIF (Tumblr GIF) TGIF (Tumblr GIF) [68] dataset is collection of 100,000 animated GIFs sourced from Tumblr, each captioned with relevant text description, totaling 120,000 sentences. This dataset supports research in video captioning, retrieval, and text-to-video synthesis by offering animated visual content with associated natural language annotations."
        },
        {
            "title": "4.1.12 Moments in Time",
            "content": "Moments in Time [69] dataset includes one million videos, each lasting three seconds, labeled with one of 339 action classes involving activities like opening, running, and cooking. Encompassing events involving people, animals, objects, and natural phenomena, it provides both visual and audio information that is highly valuable in the development of models that understand complex events."
        },
        {
            "title": "4.2 Training Configurations",
            "content": "Understanding the complexities of training configurations becomes essential for researchers entering the field of text-to-video generation. These configurations serve as practical guidelines for evaluating the feasibility 21 Preprint of implementing or enhancing existing approaches under specific resource constraints. Familiarity with computational requirements, optimization strategies, and architectural decisions not only aids reproducibility but also highlights potential bottlenecks and opportunities for improvement. Table 1 catalogs the essential training parameters reported across surveyed models, including GPU type and count, batch sizes, learning rates, optimizer choices, training steps or epochs, loss functions, and dropout rates. Together, these parameters offer comprehensive overview of the computational and architectural needs required to train text-to-video models. They are thus crucial reference points for future research and implementation planning. While several papers do not provide complete training configurations [914, 16], insights can still be extracted through available metadata, ablation studies, and cross-referencing within the papers. These partial details, along with identifiable patterns across model families, allow for preliminary estimates of resource needs and can help guide researchers in assessing implementation feasibility under various computational budgets."
        },
        {
            "title": "5 Performance Evaluation",
            "content": "The fast development of text-to-video generation models calls for an equally strong framework that provides evidence for their effectiveness and applicability in real-world applications. The performance evaluation includes both quantitative metrics for objective measurement over various generated video aspects and qualitative assessments, often through human evaluations, for capturing perceptual qualities not satisfactorily addressed by metrics alone. Added to that, new benchmarks are being developed that tackle the changing challenges in the field, which allow for more comprehensive and varied evaluations. The current section will give an overview of the current standard evaluation metrics and benchmarks, discuss the place of human evaluations in qualitative assessment, and review one of the newer benchmarks that seek to provide more nuanced and comprehensive evaluations of model performance."
        },
        {
            "title": "5.1 Current Standard Evaluation Metrics and Benchmarks",
            "content": "5.1.1 Evaluation Metrics Evaluation of generative video models depends on set of currently standardized, quantitative metrics that objectively assess generated content. Such uniformity in evaluation metrics facilitates benchmarking and monitoring progress in respect of such generative models. The most widely adopted quantitative metrics are: (i) Inception Score (IS) [70]: The Inception Score provides quality and diversity assessments of generated videos by employing pre-trained Inception network [71] used to classify frames. The higher the IS is, the better the quality and diversity of videos are, hence making the generated easily recognizable by the classification model, denoted in Eq. (21): IS = exp (E𝑥 [𝐷KL ( 𝑝(𝑦𝑥) 𝑝(𝑦))]) (21) Here, 𝑝(𝑦𝑥) is the probability distribution over classes for generated video x, and 𝑝(𝑦) is the marginal distribution over classes. 𝐷KL denotes the Kullback-Leibler divergence [72], measuring the difference between these distributions. (ii) Frechet Inception Distance (FID) [73]: FID measures the similarity between the distributions of generated and real videos by comparing their feature representations. Originally designed for images, FID captures both quality and diversity, serving as foundation for the Frechet Video Distance (FVD), denoted in Eq. (22): FID = (cid:13) (cid:13)𝜇𝑔 𝜇𝑟 (cid:13) 2 + Tr (cid:13) (cid:16) Σ𝑔 + Σ𝑟 2 (cid:0)Σ𝑔Σ𝑟 (cid:1) 1/2(cid:17) (22) 22 Preprint In this formula, 𝜇𝑔 and 𝜇𝑟 are the mean feature vectors of generated and real videos, respectively. Σ𝑔 and Σ𝑟 are their covariance matrices. Tr denotes the trace of matrix, representing the sum of its diagonal elements. (iii) Frechet Video Distance (FVD) [74]: FVD assesses both quality and temporal coherence by comparing feature distributions over time. lower FVD score indicates that generated videos closely resemble real videos in both visual quality and motion dynamics, denoted in Eq. (23): FVD = (cid:13) (cid:13)𝜇𝑔 𝜇𝑟 (cid:13) 2 + Tr (cid:13) (cid:16) 𝛴𝑔 + 𝛴𝑟 2 (cid:0)𝛴𝑔𝛴𝑟 (cid:1) 1/2(cid:17) (23) Here, 𝜇𝑔 and 𝜇𝑟 represent the mean feature vectors, while 𝛴𝑔 and 𝛴𝑟 are the covariance matrices of the generated and real videos. The term (cid:0)𝛴𝑔𝛴𝑟 (cid:1) 1/2 is the matrix square root of the product of the covariance matrices. (iv) CLIP Similarity (CLIP-SIM) [2]: CLIP-SIM measures how well the generated videos align semantically to their provided corresponding text descriptions, utilizing the CLIP model [2] for this purpose. higher CLIP-SIM score indicates better semantic coherence between video content and text prompts, denoted in Eq. (24): CLIP-SIM = CLIP(𝑉), CLIP(𝑇) CLIP(𝑉) CLIP(𝑇) (24) In this equation, CLIP(𝑉, 𝑇) is the cosine similarity between video and text prompt embeddings from the CLIP model. CLIP(𝑉) and CLIP(𝑇) are the Euclidean norms of the video and text embeddings, respectively. (v) Kernel Video Distance (KVD) [75]: KVD evaluates the similarity between the feature distributions of real and generated videos, focusing on both spatial and temporal consistency. It is based on the Maximum Mean Discrepancy (MMD), applied to features extracted from pre-trained networks that capture spatiotemporal information in video data. reduced KVD score means that the created video aligns well with real video data, both in terms of visual appearance and motion fluidity. First, MMD2 is calculated as, denoted in Eq. (25): MMD2(X, Y) = 1 m(m 1) 𝑘 (x𝑖, 𝑗) 𝑖 𝑗 + 1 n(n 1) 𝑖 𝑗 𝑖=1 𝑗=1 2 mn 𝑘 (x𝑖, 𝑗) 𝑘 (y𝑖, 𝑗) (25) In this equation, = {x1, x2, . . . , x𝑚} represents feature vectors gathered from real videos, while = {y1, y2, . . . , y𝑛} corresponds to feature vectors from generated videos. The kernel function 𝑘 (x, y) (e.g., Gaussian) measures similarity between feature vectors. Finally, KVD is obtained by taking the square root of MMD2, denoted in Eq. (26): KVD = MMD2(X, Y) (26) reduced KVD score ensures higher quality in the created video, translating to realistic visual appearance and smooth motion alignment with actual video data. Performance evaluation of these models on the mentioned metric [70, 74, 76] and datasets is provided in Table 2. 23 Preprint Table 2: Performance Comparison of Text-to-Video Generation Models on Standard Benchmarks Model Name UCF-101 MSRVTT BAIR Kinetics-600 IS () FVD () CLIPSIM () IS () FVD () CLIPSIM () FVD () FVD () CLIPSIM () - - - - MoCoGAN [6] 12.480.03 VideoGPT [44] 24.690. GODIVA [8] UWA [9] CogVideo [10] Make-A-Video [11] - - 50. 33 626 367.23 VideoFusion [12] 71.67 Latent-Shift [13] Free-Bloom [14] LaVie [51] DreamVideo [53] Grid Diffusion [54] FIFO-Diffusion [16] VideoTetris [15] GVDIFF [55] CogVideoX [56] Pyramidal Flow [17] - - - - - - 54.39 62. 197.66 340 - - - - - - - - - - 5.1.2 Evaluation Benchmarks - - - - 0. - - - - - - - - - - 1294 - - - - - - - - - - - - - - - - 0.2402 0. 0.2631 0.3049 0.293 0.2773 - 0. 15.25 149.18 - 0.3282 375 - - - - - - - - - - - - - - - - 0.3096 - - - - - 503 103.3 - - - 86.9 109.23 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 0.3012 - - - - - - - - - - - The systematic evaluation of text-to-video generation models needs some standardized datasets that can very effectively assess the technical capabilities and the creative aspects of video synthesis. These benchmark datasets are crucial testing grounds, enabling quantitative comparison across different architectures while challenging models across diverse scenarios, temporal dynamics, and semantic complexity. The following benchmarks are the current standard evaluation frameworks, each of which targets specific aspects of video generation competency, from action recognition to temporal coherence and semantic alignment. (i) UCF-101 [58]: UCF-101 contains 13,320 video clips covering total of 101 action categories. These action categories include sports, daily actions, and other complex motions, challenging models to generate coherent and contextually relevant videos across various action types. (ii) MSR-VTT [77]: The dataset is made up of close to 10,000 videos which are tagged with their corresponding 200,000 textual descriptions. It is made in way to evaluate how well the models can align generated video content with provided textual prompts, allowing for the assessment of semantic coherence and content relevancy. (iii) Kinetics-600 [78]: Kinetics-600 contains over 500,000 video clips across 600 action classes. This thorough dataset covers broad range of activities, testing models ability to generate detailed and dynamic videos that accurately show wide variety of complex actions. (iv) BAIR [67]: The dataset comprises of video sequences of robotic arm interacting with objects on tabletop. Its training data consists of approximately 44,000 video sequences, each with 30 frames in resolution 6464 pixels that capture the dynamic relations between robots and objects. It has been used in video prediction and action-conditioned video generation research, furnishing the models with baseline to learn the dynamics of physical interactions. Preprint"
        },
        {
            "title": "5.2 Human Evaluations",
            "content": "While quantitative metrics provide measures objectively, they just cannot capture subjective quality aspects of the generated video, which are crucial for user satisfaction and real-world usage. Thus, human evaluations, in different forms and metrics, have been introduced in multiple papers to assess characteristics like visual realism, semantic consistency, and general aesthetic appeal. They hint at how well model meets the expectations or preferences of human being. Human evaluations are purely subjective methods of qualitatively assessing the dimensions of the generation models of videos that capture those aspects, such as realism, coherence, semantic alignment, and others, which could be missed by automated metrics. These ensure that the generated videos meet human expectations and preferences, thus confirming that models produce content that is both technically proficient and subjectively satisfying. This section outlines the four main core human evaluation metrics that have been used across various papers to measure video quality based on human rankings: Fidelity to Text/Alignment to Text, Motion Realism, Aesthetic/Quality, and Overall Quality/Preference."
        },
        {
            "title": "5.2.1 Fidelity to Text/Text Alignment",
            "content": "Fidelity to Text, also at times referred to as Alignment to Text, refers to the degree of representation that the generated video exhibits with respect to the input textual description. This metric ensures not only that the content, context, and actions shown in the video are semantically consistent but also that they are appropriate to the given prompts. Different papers use different terminologies to present this metric. For example, some papers refer to it as Semantic Relevance [44], whilst others refer to this metric as Semantic Consistency (SC) [8], Faithfulness [11], Instruction Following [56], and Fidelity to Text [55]. 5.2.2 Realism of Motion Motion Realism measures the coherence and smoothness of the motion in the generated video, which should be natural, logical, and continuous in both action and transition, with fast change or transition of frames. This metric is important to assess the believability and engagement of the motion of objects and subjects within the videos. Papers introduce this metric with various terms and different measures. Metrics Motion Realism [44], Plausibility of Motion [12,55], Temporal Coherence [14], Motion Awareness Evaluation (MAWE) [15], and Motion [16, 54] are some of the other names by which this metric goes by across various papers. 5.2.3 Aesthetic/Quality Aesthetic Quality or Quality evaluations are subjective measures of the generated videos visual appeal, including the richness of senses involved and overall stylistic coherence. It includes information about colour harmony, the quality of textures appearing in the video, and artistic expressions. Other names by which this metric goes in various papers are: Visual Realisticity [8], Aesthetic Quality [12, 15, 16, 51, 54, 56], Sensory Quality and Cover Quality [56]."
        },
        {
            "title": "5.2.4 Overall Quality/Preference",
            "content": "Overall Quality/Preference is used to serve as thorough measure for the generated video to assess the overall desirability and visual excellence from human perspective. This metric typically encompasses various qualitative dimensions to determine the general desiredness of the videos. Different Papers refer to this metric using different terminologies. For instance, some of the papers refer to it as User Preference 25 Preprint Score [6], whereas other papers use the terms like Overall Quality Score [44], Visual Realisticity [8], Overall Rank [14], and Overall Preference [11, 12, 15, 16, 53, 55, 56]."
        },
        {
            "title": "5.3 Towards A Standardized Video Generation Evaluation Benchmark",
            "content": "While being widely adapted, the conventional metrics like Inception Score [70], Frechet Inception Distance [73], Frechet Video Distance [74], and CLIPSIM [2] are limited in the holistic assessment of video generative models. They mostly focus on the statistical similarities between generated and real data distributions, while they fundamentally cannot capture key qualitative aspects that match human perception. Such simplification of video quality to single numerical scores not only avoids the rich complexity of video assessment but also does not provide sufficient granularity to allow for effective model evaluation. Metrics initially designed for static images and traditional video analysis cannot capture challenges particular to video, such as temporal coherence or semantic consistency. More critically, they are not able to quantify key human-centered characteristics such as identity preservation, motion fluidity, and temporal stability, which are essential for any standard quality assessment in video. To address these limitations, VBench introduces an extensive benchmarking methodology for generative video models [79]. This framework systematically decomposes video quality assessment into 16 different dimensions of evaluation, offering fine-grained assessment of video generation quality closer to human perception. By surpassing traditional single-score metrics, VBench supplies researchers with diagnostic tools and, with the inclusion of human preference annotations, allows for the identification of strengths and weaknesses of model, fostering focused improvements in video generation capabilities, detailed in Fig. 18. Figure 18: Evaluation Pipeline of VBench from [79] The assessment process of this framework is based on its hierarchically organized Evaluation Dimension Suite, which classifies the various assessment criteria into two large domains: Video Quality and VideoCondition Consistency. For Video Quality, this framework evaluates several important temporal dimensions, including subject consistency, background stability, temporal flickering, motion smoothness, and dynamic degree, along with frame-like attributes that involve aesthetic and imaging qualities. At the semantic level, Video-Condition Consistency evaluations include object accuracy, multi-object composition, correctness of human actions, consistency in colors, relationships among objects in space, scene correctness, adherence to style, and alignment between video and its corresponding text. VBench follows comprehensive Prompt Suite of about 100 creatively designed prompts across eight different content categories to ensure that the evaluation is thorough. The framework uses specialized evaluation methods for background consistency, and motion priors for smoothness evaluation, to objectively measure each quality aspect. These automated 26 Preprint evaluations are validated through systematic human preference annotations: annotators compare video pairs across specific dimensions, ensuring alignment with human perception. Given the holistic and systematic nature of the review, VBench has the potential to become the go-to standard framework for evaluation of T2V models. Additionally, because of its thorough human validation, its detailed assessment method can show stronger correspondence with viewers perceptions, making it valuable for real-world applications. VBench allows for fair model comparisons across the research community and promotes consistent practices for model evaluations through open sourcing and extensibility. Its multi-dimensional approach serves as both an analytical tool and guide for future research, allowing developers to focus on selected quality aspects most relevant to their use case. This versatility, coupled with its comprehensive methodology, positions VBench as foundational framework that could guide the development and standardization of future video generation evaluation technology."
        },
        {
            "title": "6 Future Direction",
            "content": "While Text-to-Video Generation has demonstrated some encouraging progress through deep learning approaches, most of the fundamental challenges remain open. The existing models, reporting initial success in generating simple video clips from text descriptions, still struggle with their computational efficiency, dataset limitations, and output quality. In this section, the article discusses some of the possible research avenues that may help overcome these challenges. It also considers the evolution of the creation process of datasets, model architectures, and application domains. Although the suggestions are far from exhaustive, they do hope to contribute something to the oncoming discussion about the progression of such text-to-video technology."
        },
        {
            "title": "6.1 Dataset Enrichment",
            "content": "Currently available datasets for T2V synthesis have severe weakness because of their limited size, quality, or legal conditions restricting the development of strong models capable of generalization outside particular setting to generate high-quality video. One possible avenue of research might be the synthesis of datasets using game engines like Unity or Unreal Engine in order to create large-scale, high-resolution, and diverse datasets with no copyright infringement. It does this by developing generalized prompt framework for overall development, classifying textual descriptions into principal subject, adjectives and attributes, verbs and actions, background, theme and style, and metadata of the camera. Generalization hierarchically allows comprehensive coverage of the subject matter, like people, animals, and objects while avoiding biases due to the use of different attributes and settings. These can range from person running through futuristic cityscape with cyberpunk aesthetics to an animal exploring fantasy forest in anime style. Feeding such structured prompts into game engines allows the generation of videos corresponding to each scenario with broad avenues for customization. Automating the pipelines of these game engines will efficiently generate large-scale datasets, improving quality and quantity. This approach would not only overcome any kind of limitation that might have existed in existing datasets but also keep it ethical and legal, avoiding any use of copyrighted material. Dataset augmentation this way may finally allow the training of models that generate semantically correct and visually realistic videos which is major push forward within the realm of text-to-video synthesis."
        },
        {
            "title": "6.2 Model Architectures and Optimization",
            "content": "Text-to-video generation models currently face several significant limitations that hinder their effectiveness and scalability. Training is computationally intensive because the current architectures are not optimized to sequentially process video data efficiently. Novel model architectures or algorithms that can better handle 27 Preprint temporal sequences therefore need to be investigated to achieve more economy in resource usage and training times. These models also tend to produce only short output videos, which is highly limiting for scenarios with longer content. Hence, in the future, research is required for allowing the generation of longer videos by enhancing these models temporal modelling. It is also limited to the generation of content with little diversity, partly because the training data itself lacks variety, making the models unable to create wide variations of scenes or subjects. This could be improved with the addition of more diverse datasets or the use of advanced augmentation techniques. Moreover, most generated videos suffer from temporal and spatial incoherence: objects often shift, appear, or disappear unnaturally. Such unrealistic simulations make viewers break immersion; hence, there is need for further development of temporal consistency and spatial coherence by better modelling of the sequence and imposing physical constraints. The interaction of physics also, in most cases, is unrealistic, hence making such generated videos less plausible; there is great need for models to understand and, consequently do better job of simulating the physics of real-world interactions. Other limitations include difficulties in capturing complex scenes with multiple interacting elements and problems related to maintaining high-resolution outputs on long sequences. In general, future directions will be in the direction of integrating advanced mechanisms of attention, leveraging multi-modal data, modifying the loss function to focus more on coherence and realism even more effectively, and optimizing models for better computational efficiency. Naturally, as architectures are being proposed, improvements in grounded algorithms become possible, and training data becomes more diverse and richer, advanced text-to-video generation models can generate longer and more diverse videos that are highly realistic and exhibit temporal and spatial coherence with plausible physical interactions."
        },
        {
            "title": "6.3 Possible Applications and Implications",
            "content": "Text-to-video generation holds transformative potential across various industries by converting textual descriptions into engaging visual content, thereby enriching communication, accessibility, and creativity. In education and e-learning, this technology facilitates pedagogical shift, enabling teachers to explain concepts in detail through customized videos, which enhances engagement and learning [80, 81]. Within the accessibility and assistive technology sectors, it offers significant assistance to individuals with disabilities, providing visual materials for the visually impaired and sign language videos for the deaf and hard-of-hearing, thereby promoting inclusiveness [82]. For content creation and marketing industries, text-to-video generation serves as cost-effective solution, allowing enterprises to produce promotional materials, personalized ads, and engaging social media content without the need for expensive, resource-intensive production processes [83]. In parental control and customized content applications, it enables parents to create personalized educational and entertainment videos that are safe for their children and reflect family values [84]. Additionally, this technology can enhance the preservation and promotion of cultural heritage by transforming folkloric materials, historical accounts, and endangered languages into accessible visual narratives, thereby contributing to cultural preservation and education [85]. In legal and forensic applications, the capability to reconstruct events and simulations from textual reports can aid investigations, court presentations, and legal education [86]. Text-to-video applications also support the generation of synthetic data for training and validating AI algorithms, advancing artificial intelligence development and machine learning research [87]. In the gaming industry, it facilitates the creation of sequences, such as automated cutscenes and adaptive storytelling based on player input [88]. Finally, virtual reality experiences can be enhanced by automatically generating immersive environments and narratives from textual descriptions, offering personalized and interactive VR content for training, simulation, and entertainment purposes [89]. In summary, this technology is poised to impact wide range of industry verticals by accelerating workflows, promoting inclusivity, and opening new creative horizons toward future where content creation is continually evolving, personalized, and accessible. 28 Preprint"
        },
        {
            "title": "7 Conclusion",
            "content": "In this survey, we reviewed the development of text-to-video (T2V) generation models, from early GANbased and VAE-based models to current diffusion-based architectures, detailing their internal mechanisms, the limitations addressed by successive approaches, and the reasons behind major architectural shifts. We provided review of the datasets used to train and evaluate these models, and to support reproducibility and assess the accessibility of training such models, we reported their hardware specifications, GPU counts, batch sizes, learning rates, optimizers, epochs, and other key hyperparameters. We went over the standard evaluation metrics commonly used to assess these models, presented their performance scores across these benchmarks, and discussed both the limitations of existing metrics and the emergence of more perceptionaligned evaluation benchmarks. Finally, we identified the main open challenges, including alignment, long-range coherence, and computational efficiency, and outlined promising directions for future research in text-to-video generation."
        },
        {
            "title": "References",
            "content": "[1] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, Zero-shot text-to-image generation, arXiv preprint arXiv:2102.12092, 2021. [2] A. Radford et al., Learning transferable visual models from natural language supervision, arXiv preprint arXiv:2103.00020, 2021. [3] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with latent diffusion models, arXiv preprint arXiv:2112.10752, 2022. [4] R. Villegas, J. Yang, Y. Zou, K. Sohn, X. Lin, and H. Lee, Learning to generate long-term future via hierarchical prediction, in International Conference on Machine Learning (ICML), 2017. [5] Y. Li, S. Mandt, and Y. Wang, Disentangled sequential autoencoder, in International Conference on Machine Learning (ICML), 2018. [6] S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz, Mocogan: Decomposing motion and content for video generation, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [7] Y. Hong, J. Yang, H. Chang, and J. Shi, The unsupervised learning of visual representations by predicting scene dynamics, in International Conference on Computer Vision (ICCV), 2018. [8] C. Wu, L. Huang, Q. Zhang, B. Li, L. Ji, F. Yang, G. Sapiro, and N. Duan, Godiva: Generating open-domain videos from natural descriptions, arXiv preprint arXiv:2104.14806, 2021. [9] C. Wu, J. Liang, L. Ji, F. Yang, Y. Fang, D. Jiang, and N. Duan, Nuwa: Visual synthesis pre-training for neural visual world creation, arXiv preprint arXiv:2111.12417, 2021. [10] W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang, Cogvideo: Large-scale pretraining for text-to-video generation via transformers, arXiv preprint arXiv:2205.15868, 2022. [11] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni et al., Make-a-video: Text-to-video generation without text-video data, arXiv preprint arXiv:2209.14792, 2022. 29 Preprint [12] Z. Luo, D. Chen, Y. Zhang, Y. Huang, L. Wang, Y. Shen, D. Zhao, J. Zhou, and T. Tan, Videofusion: Decomposed diffusion models for high-quality video generation, arXiv preprint arXiv:2303.08320, 2023. [13] J. An, S. Zhang, H. Yang, S. Gupta, J.-B. Huang, J. Luo, and X. Yin, Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation, arXiv preprint arXiv:2304.08477, 2023. [14] H. Huang et al., Free-bloom: Zero-shot text-to-video generator with llm director and ldm animator, arXiv preprint arXiv:2309.14494, 2023. [15] Y. Tian, L. Yang, H. Yang, Y. Gao, Y. Deng, J. Chen, X. Wang, Z. Yu, X. Tao, P. Wan, D. Zhang, and B. Cui, Videotetris: Towards compositional text-to-video generation, arXiv preprint arXiv:2406.04277, 2024. [16] J. Kim, J. Kang, J. Choi, and B. Han, Fifo-diffusion: Generating infinite videos from text without training, arXiv preprint arXiv:2405.11473, 2024. [17] Y. Jin, Z. Sun, N. Li, K. Xu, K. Xu, H. Jiang, N. Zhuang, Q. Huang, Y. Song, Y. Mu, and Z. Lin, Pyramidal flow matching for efficient video generative modeling, arXiv preprint arXiv:2410.05954, 2024. [18] A. Singh, survey of ai arXiv:2311.06329, 2023. text-to-image and ai text-to-video generators, arXiv preprint [19] R. Sun, Y. Zhang, T. Shah, J. Sun, S. Zhang, W. Li, H. Duan, B. Wei, and R. Ranjan, From sora what we can see: survey of text-to-video generation, arXiv preprint arXiv:2405.10674, 2024. [20] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomedical image segmentation, arXiv preprint arXiv:1505.04597, 2015. [21] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, Backpropagation applied to handwritten zip code recognition, Neural Computation, vol. 1, no. 4, pp. 541551, 1989. [22] X. Glorot, A. Bordes, and Y. Bengio, Deep sparse rectifier neural networks, in Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 315323. [23] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770778. [24] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, Deconvolutional networks, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010, pp. 25282535. [25] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, Image-to-image translation with conditional adversarial networks, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [26] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, Photo-realistic single image super-resolution using generative adversarial network, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 30 Preprint [27] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, Context encoders: Feature learning by inpainting, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [28] J. Johnson, A. Alahi, and L. Fei-Fei, Perceptual losses for real-time style transfer and super-resolution, in European Conference on Computer Vision (ECCV), 2016. [29] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, Unpaired image-to-image translation using cycleconsistent adversarial networks, in Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. [30] D. P. Kingma and M. Welling, Auto-encoding variational bayes, arXiv preprint arXiv:1312.6114, 2013. [31] T. Bayes, An essay towards solving problem in the doctrine of chances, Philosophical Transactions of the Royal Society of London, vol. 53, pp. 370418, 1763. [32] C. M. Bishop, Variational principal components, in Proceedings of the 9th International Conference on Artificial Neural Networks, 1995, pp. 509514. [33] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, Generative adversarial nets, in Advances in Neural Information Processing Systems (NeurIPS), vol. 27, 2014, pp. 26722680. [34] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, arXiv preprint arXiv:2006.11239, 2020. [35] P. Dhariwal and A. Nichol, Diffusion models beat gans on image synthesis, arXiv preprint arXiv:2105.05233, 2021. [36] J. Ho and T. Salimans, Classifier-free diffusion guidance, arXiv preprint arXiv:2207.12598, 2022. [37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in Advances in Neural Information Processing Systems (NeurIPS), vol. 30, 2017. [38] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, and N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, in International Conference on Learning Representations (ICLR), 2020. [39] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, Language models are unsupervised multitask learners, OpenAI, Technical Report, 2019. [40] Y. Liu and M. Lapata, Text summarization with pretrained encoders, arXiv preprint arXiv:1908.08345, 2019. [41] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Learning representations by back-propagating errors, Nature, vol. 323, no. 6088, pp. 533536, 1986. [42] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, Learning phrase representations using rnn encoder-decoder for statistical machine translation, arXiv preprint arXiv:1406.1078, 2014. 31 Preprint [43] P. Esser, R. Rombach, and B. Ommer, Taming transformers for high-resolution image synthesis, arXiv preprint arXiv:2012.09841, 2020. [44] W. Yan, Y. Zhang, P. Abbeel, and A. Srinivas, Videogpt: Video generation using vq-vae and transformers, arXiv preprint arXiv:2104.10157, 2021. [45] A. van den Oord, O. Vinyals, and K. Kavukcuoglu, Neural discrete representation learning, arXiv preprint arXiv:1711.00937, 2017. [46] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, Improving language understanding by generative pre-training, arXiv preprint arXiv:1801.06146, 2018. [47] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, Learning spatiotemporal features with 3d convolutional networks, arXiv preprint arXiv:1412.0767, 2014. [48] Y. Zhang et al., Detr4d: Direct multi-view 3d object detection with sparse attention, arXiv preprint arXiv:2212.07849, 2022. [49] A. Dosovitskiy et al., Deconvolution and checkerboard artifacts, arXiv preprint arXiv:1505.04366, 2015. [50] C. Li et al., R3d-swin: Use shifted window attention for single-view 3d reconstruction, arXiv preprint arXiv:2312.02725, 2023. [51] Y. Wang et al., Lavie: High-quality video generation with cascaded latent diffusion models, arXiv preprint arXiv:2309.15103, 2023. [52] J. Su et al., Roformer: Enhanced transformer with rotary position embedding, arXiv preprint arXiv:2104.09864, 2021. [53] Y. Wei et al., Dreamvideo: Composing your dream videos with customized subject and motion, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, pp. 65376549. [54] T. Lee et al., Grid diffusion models for text-to-video generation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, pp. 12341243. [55] H. Dou et al., Gvdiff: Grounded text-to-video generation with diffusion models, arXiv preprint arXiv:2407.01921, 2024. [56] Z. Yang et al., Cogvideox: Text-to-video diffusion models with an expert transformer, arXiv preprint arXiv:2408.06072, 2024. [57] M. Bain et al., Frozen in time: joint video and image encoder for end-to-end retrieval, arXiv preprint arXiv:2104.00650, 2021. [58] K. Soomro, A. R. Zamir, and M. Shah, Ucf101: dataset of 101 human action classes from videos in the wild, arXiv preprint arXiv:1212.0402, 2012. [59] A. Miech et al., Howto100m: Learning text-video embedding by watching hundred million narrated video clips, arXiv preprint arXiv:1906.03327, 2019. [60] C. Schuhmann et al., Laion-5b: An open large-scale dataset for training next generation image-text models, arXiv preprint arXiv:2210.08402, 2022. 32 Preprint [61] G. Mishne et al., Common crawl: massive, open-access web crawl, in Proceedings of the 20th International Conference on World Wide Web (WWW), 2011, pp. 12. [62] X. Wang et al., Vatex: large-scale, high-quality multilingual dataset for video-and-language research, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 45804589. [63] H. Xue et al., Advancing high-resolution video-language representation with large-scale video transcriptions, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 10 63410 643. [64] A. Siarohin et al., First order motion model for image animation, arXiv preprint arXiv:1905.08233, 2019. [65] N. Aifanti, C. Papachristou, and A. Delopoulos, The mug facial expression database, in Proceedings of the 11th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS), 2010, pp. 14. [66] L. Gorelick et al., Actions as space-time shapes, in Proceedings of the Tenth IEEE International Conference on Computer Vision (ICCV), 2005, pp. 13951402. [67] C. Finn et al., Unsupervised learning for physical interaction through video prediction, in Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS), 2017, pp. 6472. [68] Y. Li et al., Tgif: new dataset and benchmark on animated gif description, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 46414650. [69] M. Monfort et al., Moments in time dataset: One million videos for event understanding, arXiv preprint arXiv:1801.03150, 2018. [70] T. Salimans et al., Improved techniques for training gans, arXiv preprint arXiv:1606.03498, 2016. [71] C. Szegedy et al., Going deeper with convolutions, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 19. [72] S. Kullback and R. A. Leibler, On information and sufficiency, The Annals of Mathematical Statistics, vol. 22, no. 1, pp. 7986, 1951. [73] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, Gans trained by two timescale update rule converge to local nash equilibrium, in Advances in Neural Information Processing Systems (NeurIPS), vol. 30, 2017, pp. 66266637. [74] T. Unterthiner et al., Fvd: new metric for video generation, arXiv preprint arXiv:1906.08976, 2019. [75] , Towards accurate generative models of video: new metric & challenges, arXiv preprint arXiv:1812.01717, 2018. [76] M. Z. Hossain and A. Imteaj, Sim-clip: Unsupervised siamese adversarial fine-tuning for robust and semantically-rich vision-language models, arXiv preprint arXiv:2407.14971, 2024. [77] J. Xu et al., Msr-vtt: large video description dataset for bridging video and language, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 52885296. Preprint [78] J. Carreira et al., short note about kinetics-600, arXiv preprint arXiv:1808.01340, 2018. [79] Z. Huang et al., Vbench: Comprehensive benchmark suite for video generative models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, pp. 12341243. [80] A. Ghimire et al., Generative ai in education: study of educators awareness, sentiments, and influencing factors, arXiv preprint arXiv:2403.15586, 2024. [81] F. Kamalov, D. S. Calonge, and I. Gurrib, New era of artificial intelligence in education: Towards sustainable multifaceted revolution, arXiv preprint arXiv:2305.18303, 2023. [82] S. Trewin, Ai fairness for people with disabilities: Point of view, arXiv preprint arXiv:1811.10670, 2018. [83] J. Wu, W. Gan, Z. Chen, S. Wan, and H. Lin, Ai-generated content (aigc): survey, arXiv preprint arXiv:2304.06632, 2023. [84] S. Viswanathan et al., The interaction layer: An exploration for co-designing user-llm interactions in parental wellbeing support systems, arXiv preprint arXiv:2411.01228, 2024. [85] P. Jaramillo and I. Sipiran, Cultural heritage 3d reconstruction with diffusion networks, arXiv preprint arXiv:2410.10927, 2024. [86] A. Zappal`a, L. Guarnera, V. Rinaldi, S. Livatino, and S. Battiato, Enhancing crime scene investigations through virtual reality and deep learning techniques, arXiv preprint arXiv:2409.18458, 2024. [87] J. W. Anderson, M. Ziolkowski, K. Kennedy, and A. W. Apon, Synthetic image data for deep learning, arXiv preprint arXiv:2212.06232, 2022. [88] D. Valevski et al., Diffusion models are real-time game engines, arXiv preprint arXiv:2408.14837, 2024. [89] O. Aydin, E. Karaarslan, and N. G. Narin, Artificial intelligence, vr, ar and metaverse technologies for human resources management, arXiv preprint arXiv:2406.15383, 2024."
        }
    ],
    "affiliations": [
        "Department of Computational Intelligence SRM Institute of Science and Technology"
    ]
}