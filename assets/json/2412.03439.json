{
    "paper_title": "CleanDIFT: Diffusion Features without Noise",
    "authors": [
        "Nick Stracke",
        "Stefan Andreas Baumann",
        "Kolja Bauer",
        "Frank Fundel",
        "Björn Ommer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 9 3 4 3 0 . 2 1 4 2 : r CleanDIFT: Diffusion Features without Noise Nick Stracke* Stefan Andreas Baumann* Kolja Bauer*"
        },
        {
            "title": "Frank Fundel",
            "content": "Bjorn Ommer CompVis @ LMU Munich, MCML {nick.stracke,b.ommer}@lmu.de compvis.github.io/cleandift Figure 1. Our proposed CleanDIFT feature extraction method yields noise-free, timestep-independent, general-purpose features that significantly outperform standard diffusion features. CleanDIFT operates on clean images, while extracting diffusion features with existing approaches requires adding noise to an image before passing it through the model. Adding noise reduces the information present in the image and requires tuning timestep per downstream task."
        },
        {
            "title": "Abstract",
            "content": "features from large-scale pre-trained diffusion Internal models have recently been established as powerful semantic descriptors for wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide highquality, noise-free semantic features. We show that these features readily outperform previous diffusion features by *Equal Contribution wide margin in wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at fraction of the cost. 1. Introduction Learning meaningful visual representations that capture vast amount of world knowledge remains key problem in the field of computer vision. Diffusion models can be trained at scale in self-supervised manner and have rapidly advanced the state of the art in image [11, 35, 44] and video generation [12, 22, 50], making them good candidate to learn visual representations. Many early works have already achieved impressive results using internal features from large-scale pretrained diffusion models for wide variety of tasks, such as semantic correspondence detection [54, 58, 59], semantic segmentation [2, 33, 55], panoptic segmentation [56], object detection [7], and classification [26]. However, the optimal approach to extract this world knowledge from diffusion model remains uncertain. To understand why that is the case, we take look at how diffusion models are trained: varying amount of noise is added to clean input image (forward process) and the model is tasked to remove the noise from the image (backward process). The amount of added noise is dependent on the diffusion timestep. As result, the model learns to operate on noisy images and also becomes dependent on the noise timestep as different noise levels require the model to perform different tasks [1, 4]. Since noisy images inherently contain less information than clean images (c.f. Figure 2), we hypothesize that this harms the internal feature representation of diffusion models [8] and, thus, the extractable world knowledge. Furthermore, the timestep acts as hyperparameter that influences the internal feature representation and needs to be picked independently for every downstream application (c.f. Figure 13). We propose novel feature extraction method that (1) eliminates the need to destroy information by adding noise to the input; and (2) produces timestep-independent generic diffusion features useful for wide range of down-stream tasks, alleviating the need to tune noising timestep per down-stream task. We show how to adapt an off-the-shelf large-scale pre-trained diffusion backbone to provide these features at minimal cost (approximately 30 minutes of finetuning on single A100 GPU) and demonstrate improved performance across wide range of downstream tasks. We achieve this by viewing diffusion model as family of feature extractors that operate on images with different noise levels and provide features with different characteristics. We consolidate all feature extraction functions in our feature extractor by aligning internal representations. Specifically, we initialize our feature extractor as trainable copy of the diffusion model; fine-tune it with clean images and no timestep as input; and align its features with all timestep-dependent feature extractors of the diffusion model. We evaluate our improved features across wide variety of downstream tasks, such as semantic correspondence matching, monocular depth estimation, semantic segmentation, and classification, and find that they consistently improve upon approaches based on standard diffusion features. These improvements are most evident for dense visual tasks such as semantic correspondence matching, where our features show substantial performance gains across wide variety of setups [54, 58, 59] and set new state-of-the-art for unsupervised semantic correspondence matching. Additionally, our proposed method eliminates the need for noise or timestep ensembling [54], offering substantial speed gains (e.g., 8ˆ over DIFT [54]), on top of improved quality. Our method is generic and integrates (a) Reconstruction without Noise (b) Diffusion Model Reconstruction with added noise (t 261 [54]) Figure 2. Deterioration of Diffusion Features. As current methods need to pass noisy images to the model to obtain useful features, they significantly reduce the information available. We alleviate this problem by obtaining useful features without noise, improving the performance of downstream tasks. easily with established methods, such as fusing diffusion and DINOv2 features [58, 59]. Our main contributions are as follows: 1. We propose CleanDIFT, finetuning approach for diffusion models that enables them to operate on clean images and makes the inherent world knowledge of these models more accessible. 2. We show how to consolidate information from all diffusion timesteps into single feature prediction, removing the need for task-specific timestep tuning. 3. We demonstrate significant performance gains of our diffusion feature extraction technique across wide range of down-stream tasks, notably surpassing the current state of the art in zero-shot unsupervised semantic correspondence detection. We further demonstrate the generality of our enhanced features by showing that these performance gains transfer to advanced methods that fuse diffusion features or operate in supervised setting. 4. Our proposed approach is significantly more efficient than previous methods that tried to address this problem by noise ensembling or supervised training. 2. Related Work Self-Supervised Representation Learning Features from large, pre-trained foundation models have been shown to yield competitive performance to supervised models for variety of downstream tasks, both in zero-shot and fine-tuning settings [17, 37, 41]. These foundation models are trained on different pre-text tasks like inpainting [17], predicting transformations [16], patch reordering [32, 36], and discriminative tasks [6, 37]. DINOv2 [37] uses discriminative objective combined with self-distillation to learn general-purpose visual features that have proven useful for variety of downstream tasks [9]. CLIP [41] 2 learns such features by employing contrastive objective on text-image pairs. Masked Autoencoders [17] (MAEs) are trained to reconstruct masked out patches of the input, also resulting in general-purpose visual features. from different models, with being the number of discrete diffusion timesteps, because the features are different at every timestep. Diffusion Models as Self-Supervised Learners Diffusion models [21, 52, 53] are generative models that have defined the state-of-the-art in image generation [11, 13, 35, 39, 44, 46], video generation [12, 40], and audio generation [14, 25] in recent years. Their primary purpose is to generate high-quality samples (images, videos, etc.). However, generation can also be interpreted as pretext task for learning expressive features, since the model has to build up comprehensive world knowledge in order to generate plausible samples [54]. Features from diffusion models (typically referred to as diffusion features) are obtained by passing noised image through the diffusion model and extracting intermediate feature representations. They have been shown to be useful for variety of tasks such as finding semantic correspondences [18, 29, 54], semantic and panoptic segmentation [2, 33, 55, 56], classification [26], and object detection [7]. For semantic correspondence matching, features are leveraged to identify semantically matching regions across images. Existing approaches utilize diffusion features either in zero-shot setting [18, 54] or fine-tune them for the semantic correspondence task [27, 29, 59]. Some zero-shot approaches do not fine-tune on semantic correspondence but still require tuning prompt to activate attention maps at the query location of the correspondence [18]. In contrast, our approach aims to provide universal features that can be used for various down-stream tasks in true zero-shot manner. Fused Features It has been shown that diffusion features and features from other self-supervised learning methods such as DINOv2 [37] complement each other [58]. DINOv2 features provide sparse but accurate semantic information, while Diffusion features provide dense spatial information albeit with sometimes inaccurate semantics. State-of-theart approaches for semantic correspondence detection exploit this and fuse features [58, 59]. Compared to DINOv2 features, diffusion features yield smoother, spatially more coherent correspondences [58]. Distillation for Diffusion Models Knowledge Distillation [19] is technique used to distill knowledge from teacher model into student model. In the context of diffusion models, this is typically applied either to reduce the required denoising steps [47] or to distill classifier-free guided [20] model into one without CFG [30]. While our approach is inspired by distillation, we consolidate features 3. Method 3.1. Preliminaries Diffusion Models Diffusion models are trained to predict clean image x0 given noisy image xt, either explicitly or implicitly. The noisy image xt is weighted sum 1 αtϵ with with random Gaussian noise xt timestep-dependent coefficients αt and noise ϵ p0, Iq. r0, denotes the time step of the diffusion process, with 0 corresponding to the clean image and corresponding to pure noise. αtx0 ` ? ? Intuitively, the model faces different objectives for different noise levels [1, 4]: for very high noise, there is little information in the input, and the model is first generating the coarse structure of the image [43]. At lower noise levels, more highand medium-frequency information is available and the task shifts to generating finer details and intricate structures. This multi-objective nature intuitively explains why previous methods found diffusion features extracted from different timesteps to provide information with differing semantics. Diffusion Feature Extraction Typically, diffusion feature extraction happens after first adding noise to an image and passing the resulting xt to U-Net [45] denoiser. Features are then extracted at multiple hand-picked locations of the U-Net decoder [2, 33, 54, 58, 59]. Different levels of noise added to the input image result in features beneficial for different downstream applications. Typical diffusion timesteps are 261 [54], 100 [58] or 50 [59]. By adding noise to the input image, these methods bottleneck the perceptual information the model can extract. To illustrate this, we show Stable Diffusion 2.1s reconstruction of an image noised at 261 [54] in Figure 2. Figure 3. Fraction of variance of diffusion features explained by 1) encoding the clean image at 0 (no additive noise), and 2) encoding just the added noise ϵ at 999. Even at relatively low timesteps such as 261 as used by DIFT [54], substantial part of the features directly depends only on the added noise. 3 Extraction Setup We train our feature extraction model to match the diffusion models internal representations. We initialize the feature extraction model as trainable copy of the diffusion model. Crucially, the feature extraction model is given the clean input image, while the diffusion model receives the noisy image and the corresponding timestep as input. Our goal is to obtain single, noise-free feature map from the feature extraction model that consolidates the information of the diffusion models timestep-dependent internal representations into single one. To align our models representations with the timestep-dependent diffusion model features during training, we introduce pointwise timestep-conditioned feature projection heads. The feature maps predicted by these projection heads are then aligned to the diffusion models features. For feature extraction at inference time, we usually discard the projection heads and directly use the feature extraction models internal representations. However, the projection heads can also be used to efficiently obtain feature maps for specific timesteps by reusing the feature extraction models internal representations and passing them through the projection heads for different values. Training Objective We regard the diffusion model as family of feature extraction functions featp, ϵ, tq for timestep r1, 999s and noise ϵ p0, Iq. Each of these functions maps an image to feature vector featpx, ϵ, tq. We aim to consolidate the information provided by all feature extraction functions into single joint function featcpq with the same dimensionality: Stable Diffusion featpx, ϵ, 1q featpx, ϵ, 2q ... featpx, ϵ, 999q , ///. ///- CleanDIFT consolidate information featcpxq To align our models features with the diffusion models features, we maximize the similarity between the diffusion models features and the projected features of our feature extraction model: $ & % featcpxq projp , 1q ÝÑ ÐÝ featpx, ϵ, 1q projp , 2q ÝÑ ÐÝ featpx, ϵ, 2q ... projp , 999q ÝÑ ÐÝ featpx, ϵ, 999q Specifically, we minimize the negative cosine similarity between the diffusion models features and our models features extracted at stages t1, ..., Ku in the network. Figure 4. Our training setup. We train our model to predict features from clean input image, while the frozen diffusion model is fed the noisy image. The projection heads project our models features onto the noisy diffusion model features, given the noising timestep t. For downstream tasks, we discard the projection heads and directly use our models internal representations as features. ? αtx0 ` Diffusion Features Encode Noise We hypothesize that diffusion features extracted from noisy images xt encode the noise ϵ in addition to information from the image. We investigate this hypothesis in very simple setting by examining how well features featpϵ; extracted from the additive noise ϵ by itself approximate the features featpxt ? 1 αtϵ; tq. Using least squares, we fit single scalar approximation coefficient to obtain the optimal reconstruction. We then quantify how much of the variance of the overall features is explained by this approximation (see Figure 3). Even at relatively low timesteps, such as 261 used by DIFT [54], encoding pure noise explains substantial fraction of the features variance. Current diffusion feature methods extract this information jointly with the image information. Our proposed method addresses this issue by eliminating the noise from the feature extraction process. We further analyze the residual and similarly decompose it via the features predicted at 0 (the clean image). We find that they do not fully explain the remainder of the features either. Instead, substantial part of the feature variance at medium noise timesteps falls to components not present at 0 or . This matches observations by previous works [54] that found diffusion features at higher timesteps offer better semantics, despite the added noise at the image input. 3.2. CleanDIFT: Noise-Free Diffusion Features In this section, we present our method to extract noise-free general-purpose diffusion features from pretrained diffusion backbone through lightweight fine-tuning process. We call our approach CleanDIFT because it extracts clean DIffusion FeaTures. An overview of our setup is shown in Figure 4. 4 Given clean image x0, the feature extraction models output for feature map is denoted as featpkq px0q. Our CleanDIFT feature map is then adapted by the learned projection heads projpkqpfeatpkq px0q, tq, where projpkqp, is the projection head for feature map k. The diffusion model receives the noisy image xt corresponding to the same x0 and timestep t. The projection head then learns timestepdependent alignment from CleanDIFT features to the diffusion models features featpkqpxt; tq. Putting it all together, our loss function is defined as: Kÿ simpprojpkqpfeatpkq px0q; tq, featpkqpxt; tqq. k1 (1) For each training image x0, we sample different noising timesteps ti in stratified manner, with each timestep ti Up q, where is the maximum timestep. By sampling multiple timesteps per image we incentivize the feature extraction model to match the diffusion models features across the entire noise spectrum. T, i`1 4. Experiments In this section, we investigate the utility of our extracted CleanDIFT features. We test our hypothesis that the proposed extraction setup enables us to leverage more of the world knowledge inherent in diffusion models compared to existing diffusion feature extraction methods while being task-agnostic and timestep-independent. To that end, we evaluate our features on wide range of downstream tasks: unsupervised zero-shot semantic correspondence, monocular depth estimation, semantic segmentation, and classification. We compare our features against standard diffusion features, methods that combine diffusion features with additional features, and non-diffusion-based approaches. 4.1. Experimental Setup Implementation Details Following previous works [54, 58, 59], we evaluate our method on Stable Diffusion (SD) backbone [44]. We apply our method to SD 1.5 and SD 2.1 to enable fair comparisons with existing methods that use either. We fine-tune our feature extraction model for only 400 steps, taking 30 minutes on single A100 GPU. We extract features after the U-Nets middle block and after each of the U-Nets decoder blocks, except the two final blocks. detailed visualization of where we extract features is provided in the Appendix. This yields total of 11 feature maps that we align between the diffusion model and the feature extraction model. Our point-wise feature projection heads consist of three stacked Feed Forward Networks (FFNs). We study the effect of different projection head architectures in the Appendix. We train with batch size of 8, learning rate of 2e-6 with linear warmup, and 5 use Adam [24] as our optimizer. For the stratified timestep sampling, we utilize 3 stratification bins across all our experiments, i.e. three different noise levels per training image. Datasets We fine-tune our feature extraction model on subset of COYO-700M [5], which is similar to the LAION [48] dataset on which Stable Diffusion 1.5 and 2.1 were trained on originally. That way, we can ensure that all performance improvements originate from the feature extraction model consolidating the diffusion models internal feature representations over time and not from choosing different dataset that matches the test dataset distribution more closely. The subset consists only of images with minimum size of 5122. We crop and resize all images to match the corresponding input resolution of the underlying diffusion model. 4.2. Unsupervised Semantic Correspondence As many previous diffusion feature methods focus on (unsupervised) semantic correspondence matching [29, 54, 58, 59], we perform an extensive evaluation of our method on this task. Following previous works on semantic correspondence matching [54, 58, 59], we measure our performance in Percentage of Correct Keypoints (PCK). We average PCK directly across all keypoints, not over images. We use α 0.1 as threshold and report both PCK values with error margins relative to the image size and to the bounding box size, denoted as PCKimg and PCKbbox respectively. We evaluate the performance on the test split of the SPair-71k dataset, which consists of approximately 12k image pairs from 18 categories. Some existing works [54, 58, 59] evaluate on additional datasets but find SPair-71k to be the most challenging and therefore the most informative benchmark. Results We first compare our extracted features to DIFT [54], an approach that detects semantic correspondences using standard diffusion features. Substituting these with our CleanDIFT features yields performance increase of 1.79 percentage points for PCKimg and 1.86 percentage points for PCKbbox. Notably, DIFT averages the extracted feature maps across 8 different noise samples. Without this averaging over noise samples, our performance gain is even larger. This indicates that our feature extraction model learns more than mere averaging over the noise in the diffusion models feature maps. We present an extended version of the time-step dependent performance analysis conducted by [54] in Figure 5: We evaluate the diffusion models performance for different timesteps in two settings. In the first setting, we provide the diffusion model with noisy input image xt as usual. In the second setting, we demonstrate that feeding the clean image along with Figure 5. Following [54], we evaluate semantic correspondence matching accuracy for different noise levels. Our feature extractor outperforms the standard noisy diffusion features across all timesteps t. We additionally demonstrate that simply providing the diffusion model with clean image and non-zero timestep does not result in improved performance. non-zero timestep is not viable solution to obtain meaningful features: We provide the diffusion model with the clean input image x0 for all timesteps t. We observe that the models performance for the clean input image degrades faster and has lower peak than for the noisy input. This is to be expected, as the diffusion model was trained on noisy images, not clean images. Our CleanDIFT features are timestep-independent and significantly outperform the standard diffusion features, even after tuning for an optimal noising timestep. Tale of Two Features [58] extends the approach of DIFT by combining diffusion features with DINOv2 [37] features. Again, we replace the standard diffusion features with our CleanDIFT features and observe that the performance gain transfers when combining our features with DINOv2 features. Telling Left from Right [59] further improves upon the results of Tale of Two Features by introducing test-time adaptive pose alignment strategy. We observe that the performance gain transfers to this setting as well. To the best of our knowledge, Telling Left from Right combined with our CleanDIFT features sets new state-ofthe-art in unsupervised zero-shot semantic correspondence In summary, replacing standard diffusion feamatching. tures with our CleanDIFT features consistently results in significant performance improvement across all three methods. An overview of the results is provided in Tab. 1 and more extensive evaluation per category is provided in the Appendix. We also investigate the performance of our features in supervised fine-tuning setting for semantic correspondence matching. Following [29], we train an aggregation network that uses all extracted feature maps and learns to aggregate them into single task-specific feature map for semantic correspondence matching. In contrast to [29], we do not have to perform costly DDIM inversion [51] to obInstead, tain matching noisy image for every timestep. we directly feed the clean image to our feature extraction model. Therefore, extracting features with our CleanDIFT Figure 6. Semantic correspondence results using DIFT [54] features with the standard SD 2.1 (t 261) and our CleanDIFT features. Our clean features show significantly less incorrect matches than the base diffusion model. Method General Approaches DINOv2+NN Diff. Feat.-based Approaches DIFT [54] Tale of Two Features [58] Telling Left from Right [59] Our Features PCK@α pÒq αimg = 0.1 αbbox = 0.1 - - 55.6 66.53 68.32Ĳ1.79 72.31 73.35Ĳ1.04 77.07 78.40Ĳ1.33 59.57 61.43Ĳ1.86 63.73 64.81Ĳ1.08 68.64 69.99Ĳ1.35 Table 1. Zero-shot unsupervised semantic correspondence matching performance comparison on SPair71k [31]. Our improved features consistently lead to substantial improvements in matching performance. We report PCK on the test split of SPair71k, aggregated per point. Numbers are reproduced, for discussion and comparison to reported numbers view Tab. 5. approach is 50x faster, since we perform single denoiser forward pass while [29] perform 50 for the inversion. Our model achieves PCKimg value of 72.48 vs their 72.75 and PCKbbox value of 64.37 vs their 64.53. We observe slight performance regression compared to their approach, however, at speedup of 50ˆ. Luo et al. [29] also present single-step ablation of their full method that only requires single forward pass which makes it more comparable to ours. We outperform this single-step version by wide margin of 9.0 percentage points for PCKimg and 9.1 percentage points for PCKbbox. 6 4.3. Depth Estimation Input Ours SD 2.1t Ground Truth We also investigate monocular depth estimation on NYUv2 [34]. Similar to [37], we follow the evaluation protocol from [28]. We use SD 2.1 as the base model and resize the input to the models native resolution of 7682. We extract features from the same location as [54] and obtain feature map of dimension 482. Unlike [37], we do not upsample the features and directly apply the linear probe. The probe predicts depth in 256 uniform bins which we combine with classification loss after linear normalization following [3]. We train one probe for our CleanDIFT features and one for standard diffusion features at 299, as that timestep minimizes the error in our settings. Our qualitative results (see Figure 7) show substantial fidelity gap in the estimated depth maps between the features from the standard SD 2.1 backbone and the features from our feature extraction model. This is reflected in substantial improvement in quantitative metrics over the baseline as seen in Tab. 2. Lastly, we reuse the probe trained on standard diffusion features and apply it on the CleanDIFT features. While this does not match the performance of the CleanDIFT probe, it still achieves significantly better results when compared to using standard diffusion features. This indicates that our features can be used as drop-in replacement for the original diffusion features and offer improved performance on downstream applications. Method Backbone Self-Supervised Methods OpenCLIP [23] ViT-G/14 ViT-H/14 MAE [17] ViT-B/8 DINO [6] ViT-L/16 iBOT [60] ViT-g/14 DINOv2 [37] Diffusion Features RMSE (Ó) 0.541 0.517 0.555 0.417 0.344 DIFT-like [54] SD 2.1 [44] Ours + Probes from noisy features 0.469 0.444İ0.025 0.453İ0.016 Table 2. Monocular Depth Estimation. Following [37], we evaluate metric depth prediction on NYUv2 [34] using linear probe. Our clean features outperform the noisy features by significant margin. Probes trained on the noisy features can be reused for the clean features, but incur smaller performance gain. 4.4. Semantic Segmentation To further investigate the difference between standard noisy diffusion features and our CleanDIFT features, we evaluate on the semantic segmentation task by training linear probes on our CleanDIFT features and on standard diffusion features. We utilize SD2.1 as the diffusion backbone and extract features at the same location as [54]. This procedure yields feature maps of size 482. We train our linear probe Figure 7. Qualitative results for depth estimation using linear probe on diffusion features on NYUv2 [34]. Our CleanDIFT features enable substantially better depth estimation than standard diffusion features. Note how the CleanDIFT features are far less noisy when compared to the standard diffusion features. on the 482 feature maps and upscale the obtained segmentation masks using nearest neighbor upsampling. We train and evaluate on the PASCAL VOC dataset [15]. Following common practice [33], we use mean Intersection over Union (mIOU) as the evaluation metric. Qualitative results are shown in Figure 9. Using our features, we observe significantly less noisy feature maps than with standard diffusion features. quantitative comparison of our CleanDIFT features performance against standard diffusion features across timesteps is provided in Figure 8. Notably, the optimal timestep appears to be around 100, in contrast to the optimal timestep for semantic correspondences, which [54] found to be 261. This highlights the need for tuning timestep individually per downstream task. Our method both alleviates the need for such timestep tuning and outperforms the standard diffusion features for the optimal timestep. Figure 8. Performance on semantic segmentation using linear probes. Our clean features outperform the noisy diffusion features for the best noising timestep t. Semantic segmentation performance of standard diffusion model heavily depends on the used noising timestep. Unlike for semantic correspondence matching, the optimal value appears to be around 100. Input Ours SD 2.1 Input Ours SD 2. Figure 9. Qualitative results for semantic segmentation from diffusion features on Pascal VOC [15]. Standard SD features use 100 as the timestep, which we found to perform best quantitatively (c.f. Figure 8). Note how the CleanDIFT segmentation maps are far less noisy compared to those of the standard diffusion features. 4.5. Classification To assess the impact of our method on non-spatial tasks, we evaluate classification performance using pooled features. Pooling mitigates the influence of localized noise, so we anticipate classification performance to remain on par with standard diffusion features unless our setup introduces detrimental effects. We perform k-Nearest Neighbor (kNN) classification with 10 on ImageNet1k [10], using SD 1.5 as the diffusion backbone. We sweep across feature maps and timesteps for the base model, with results presented in Figure 10. Our analysis shows that the feature map with the lowest spatial resolution, i.e., feature map #0 (see Figure 11), achieves the highest classification accuracy. Furthermore, the optimal timestep for the base model varies between feature maps. For the best-performing feature map #0, timestep 100 yields the highest classification accuracy. Importantly, CleanDIFT features slightly outperform the standard diffusion features even when using an optimal timestep for the base model showing that it does not introduce any detrimental effects. 4.6. Ablation Studies For simplicity, we perform our ablation studies using DIFT [54] and evaluate the performance for unsupervised zeroshot semantic correspondence matching on subset of the SPair71k test split. Training Objective During training, we maximize the cosine similarity between projected outputs of our feature extraction model and standard diffusion features to align them. To investigate the influence of the employed similarity metric, we compare feature extraction models trained on three different alignment objectives commonly used in similar contexts: mean absolute error (L1), mean squared error (L2), and cosine similarity. Quantitative results are Figure 10. Classification performance on ImageNet1k [10], using kNN classifier with 10 and cosine similarity as the distance metric. We sweep over different timesteps and feature maps. We find that the feature map with the lowest spatial resolution (feature map #0) yields the highest classification accuracy. provided in Tab. 3. While all objectives result in feature extraction models that outperform standard diffusion features, cosine similarity consistently performs the best across the alignment objectives by significant margin, confirming our choice of similarity metric."
        },
        {
            "title": "Projection\nHeads",
            "content": "PCK@α pÒq αimg 0.1 αbbox 0.1 Cosine Sim. L2 L1 SD 2. - 67.61 67.37 66.71 66. 66.18 66.07 63.41 60.28 60.22 59.30 59.34 58.79 59.01 55. Table 3. Ablation Study Results. We evaluate the feature extraction models performance for zero-shot semantic correspondence matching on subset of the SPair71k test split. PCK is aggregated per point. Projection Heads We investigate the influence of our proposed projection heads that are used to project CleanDIFT features onto standard diffusion features. The alignment of feature extraction model and diffusion model is determined by the utilized similarity metric and the projection heads. Therefore, we evaluate our feature extraction models performance with and without the projection heads in combination with all three similarity metrics. An overview of the comparison is given in Tab. 3. In our main configuration that uses cosine similarity, using the projection heads yields slight performance improvements of 0.24 8 percentage points for PCKimg and 0.06 percentage points for PCKbbox compared to fine-tuning without the projection heads. As the projection heads are typically not used for inference, they add computational overhead only during the lightweight fine-tuning. Therefore, we argue that it is worthwhile to include them and leverage the small performance gain. Additionally, they can be reused to efficiently obtain feature maps for specific timesteps. 5. Conclusion In this paper, we have proposed CleanDIFT, novel feature extraction method for diffusion features. CleanDIFT timestep-independent, general-purpose yields noise-free, diffusion features. We achieve this by consolidating the timestep-dependent feature representations of pre-trained diffusion backbone into single feature representation. To this end, we align the features of our feature extraction model and the pre-trained diffusion backbone in lightweight fine-tuning process. The training takes approximately 30 min on single A100 GPU. Our feature extraction model operates on clean images, thus eliminating the need to destroy information by adding noise to the input image. Additionally, our method alleviates the need to tune timestep per downstream task. In addition, our model does not require ensembling over noise or timesteps, instead it only requires single forward pass at inference time to extract features. This reduces the inference cost to fraction compared to methods that employ ensembling or inversion. We evaluate our CleanDIFT features on wide variety of downstream tasks and demonstrate significant performance improvements over standard diffusion features."
        },
        {
            "title": "Acknowledgments",
            "content": "This project has been supported by the German Federal Ministry for Economic Affairs and Climate Action within the project NXT GEN AI METHODS Generative Methoden fur Perzeption, Pradiktion und Planung, the German Research Foundation (DFG) project 421703927, and the bidt project KLIMA-MEMES. The authors gratefully acknowledge the Gauss Center for Supercomputing for providing compute through the NIC on JUWELS at JSC and the HPC resources supplied by the Erlangen National High Performance Computing Center (NHR@FAU funded by DFG). Further, we would like to thank Owen Vincent for continuous technical support."
        },
        {
            "title": "References",
            "content": "[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 2, 3 [2] Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models. In International Conference on Learning Representations, 2022. 1, 3 [3] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 40094018, 2021. 7 [4] Giulio Biroli, Tony Bonnaire, Valentin De Bortoli, and Marc Mezard. Dynamical regimes of diffusion models. arXiv preprint arXiv:2402.18491, 2024. 2, 3 [5] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: https : / / github . com / Image-text pair dataset. kakaobrain/coyo-dataset, 2022. [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 2, 7 [7] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Diffusiondet: Diffusion model for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1983019843, 2023. 2, 3 [8] Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He. Deconstructing denoising diffusion models for self-supervised learning. arXiv preprint arXiv:2401.14404, 2024. 2 [9] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr In The Bojanowski. Vision transformers need registers. Twelfth International Conference on Learning Representations, 2024. 2 [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. 8 [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1, [12] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73467356, 2023. 1, 3 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 3 [14] Zach Evans, CJ Carr, Josiah Taylor, Scott Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. In Forty-first International Conference on Machine Learning, 2024. 3 [15] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2): 303338, 2010. 7, 8, 14 [16] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In International Conference on Learning Representations, 2018. 2 [17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 2, 3, 7 [18] Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion. Advances in Neural Information Processing Systems, 36, 2023. 3 [19] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015. 3 [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [23] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. 7 [24] Diederik P. Kingma and Jimmy Lei Ba. Adam: method In Proceedings of the 3rd Infor stochastic optimization. ternational Conference on Learning Representations (ICLR), 2015. 5 [25] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: versatile diffusion model for In International Conference on Learning audio synthesis. Representations, 2021. 3 [26] Alexander Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22062217, 2023. 2, 3 [27] Xinghui Li, Jingyi Lu, Kai Han, and Victor Adrian Prisacariu. Sd4match: Learning to prompt stable diffuIn Proceedings of the sion model for semantic matching. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2755827568, 2024. [28] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for monocular depth estimation. IEEE Transactions on Image Processing, 2024. 7 [29] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. In Advances in Neural Information Processing Systems, pages 4750047510. Curran Associates, Inc., 2023. 3, 5, 6 [30] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. 3 [31] Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho. Spair-71k: large-scale benchmark for semantic correspondence. arXiv preprint arXiv:1908.10543, 2019. 6, 12, 13 [32] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67076717, 2020. 2 [33] Koichi Namekata, Amirmojtaba Sabour, Sanja Fidler, and Seung Wook Kim. Emerdiff: Emerging pixel-level semantic knowledge in diffusion models. In The Twelfth International Conference on Learning Representations, 2023. 1, 3, 7 [34] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Indoor segmentation and support inference from Fergus. rgbd images. In ECCV, 2012. 7, [35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 1, 3 [36] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of In Eurovisual representations by solving jigsaw puzzles. pean conference on computer vision, pages 6984. Springer, 2016. 2 [37] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. 2, 3, 6, 7 [38] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, 2018. 12 [39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 3 [40] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 3 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 10 tional Conference on Computer Vision, pages 12061217, 2023. 1, 3 [56] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29552966, 2023. 2, 3, 13, 14 [57] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. 12 [58] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements In Advances dino for zero-shot semantic correspondence. in Neural Information Processing Systems, pages 45533 45547. Curran Associates, Inc., 2023. 1, 2, 3, 5, 6, 12, 13, 14 [59] Junyi Zhang, Charles Herrmann, Junhwa Hur, Eric Chen, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. Telling left from right: Identifying geometry-aware semantic correIn Proceedings of the IEEE/CVF Conference spondence. on Computer Vision and Pattern Recognition, pages 3076 3085, 2024. 1, 2, 3, 5, 6, 12, 13, [60] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image BERT pre-training with online tokenizer. In International Conference on Learning Representations, 2022. 7 [42] Prajit Ramachandran, Barret Zoph, and Quoc Le. arXiv preprint Searching for activation functions. arXiv:1710.05941, 2017. 13 [43] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. arXiv preprint arXiv:2206.13397, 2022. 3 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 3, 5, 7 [45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention MICCAI 2015, pages 234241, Cham, 2015. Springer International Publishing. [46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 3 [47] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 3 [48] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 5 [49] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 13 [50] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations, 2023. [51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 6 [52] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 3 [53] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [54] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence In Advances in Neural Information from image diffusion. Processing Systems, pages 13631389. Curran Associates, Inc., 2023. 1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14 [55] Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen. Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using difIn Proceedings of the IEEE/CVF Internafusion models. CleanDIFT: Diffusion Features without Noise"
        },
        {
            "title": "Supplementary Material",
            "content": "B. Additional Quantitative Evaluations Unsupervised Semantic Correspondence In Tab. 4, we provide an extended version of Tab. 1, in which we report the PCK metric per category of the SPair71k dataset [31]. The categories for which we observe the largest gain when comparing our CleanDIFT features to standard diffusion features are TV, Plant, and Chair. These classes are characterized by long, texture-less edges: the bezel of TV monitor, the pot of plant, and the legs of chair. Supporting this observation, the performance gain for samples from the Plant class mostly comes from keypoints not on the plant itself but on the pot of the plant. This is further illustrated in Figure 6. We conclude that our CleanDIFT features are particularly effective for matching corresponding keypoints located along texture-less edges. C. Additional Qualitative Samples We provide additional qualitative samples for semantic correspondence matching in Figure 12 and for semantic segmentation in Figure 14. D. Projection Heads Projected Feature Maps We show more thorough analysis of the depth probe experiment presented in Sec. 4.3. We show the full set of linear probes for depth prediction on our projected features, i.e. the outputs of the projection heads for different timesteps. comparison of the performance across timesteps is provided in Figure 15. We observe that the performance of linear probes trained on the projected features decreases for large timesteps, albeit significantly less severe than for standard diffusion features due to the absence of noise. The best performance on projected features is achieved at timestep 499, while the best performance for standard diffusion features is achieved at timestep 299. Adapter Ablations We show the remaining results of our ablation study from Sec. 4.6, including our investigation of the influence of different components of the projection head architecture and the influence of pre-training the projection heads. In our main configuration, we use FiLM layer [38] in each FFN block to adaptively scale activations depending on the timestep t. We replace the FiLM layers with Adaptive RMS (AdaRMS) norm layers [57] and observe performance degradation of 0.1 percentage points for PCKbbox. We conclude that removing the scale and shift information Figure 11. We align the feature maps between our feature extractor and the diffusion model at multiple stages within the network to enable the usage of multiple feature maps for downstream tasks. In total, we extract and align features at 11 stages of the SD U-Net decoder. The downsampling factor for the different blocks is denoted as DS and the channel dimension is shown on the right side of each block. A. Architecture Details An illustration of where exactly we extract and align feature maps is provided in Figure 11. The decoder architecture is identical for SD 1.5 and SD 2.1, therefore Figure 11 applies to both models. DIFT [54] extracts feature map #6. Tale of Two Features [58] and Telling Left from Right [59] both extract feature maps #2, #6, and #8. 12 Method DIFT [54] Tale of Two Features [58] Telling Left from Right [59] Our Features PCK@αbbox = 0.1 per category (Ò) Aero Bike Bird Boat Bottle Bus Car Cat Chair Cow Dog Horse Motor Person Plant Sheep Train TV All 63.41 55.10 80.40 34.55 46.15 52.26 48.02 75.86 39.46 75.57 55.00 61.71 53.32 46.53 56.36 57.68 71.30 63.63 59.57 63.72 55.90 80.50 35.40 49.36 53.46 48.08 75.78 43.10 76.20 55.69 61.01 54.17 49.14 62.56 58.37 74.63 71.54 61.43Ĳ1.86 71.26 62.23 87.01 37.24 53.78 54.32 51.20 78.61 46.50 78.93 64.43 69.47 62.23 69.27 59.28 68.03 65.40 53.81 63.73 71.12 62.70 87.42 38.33 54.78 54.67 51.20 78.52 47.86 79.38 64.88 69.18 62.61 69.72 62.82 68.87 67.51 59.04 64.81Ĳ1.08 78.14 66.37 89.60 43.74 53.29 66.61 59.94 82.66 51.75 82.79 68.95 74.91 65.84 71.67 57.71 72.24 83.46 49.66 68.64 77.17 65.65 89.58 44.24 54.27 67.24 60.63 82.33 56.57 82.53 68.37 75.91 65.99 71.37 62.29 70.42 84.58 59.84 69.99Ĳ1.35 Table 4. Reproduced results for zero-shot unsupervised semantic correspondence matching, evaluated on SPair71k [31]. The three categories for which we observe the largest overall gains are marked in blue. We report PCK@α 0.1 with an error margin relative to bounding box sizes on the test split of SPair71k, aggregated per point and per category. We compare our reproductions against the papers reported numbers in Tab. 5 Method DIFT [54] Tale of Two Features [58] Telling Left from Right [59] Eval Method reported reproduced reported reproduced reported reproduced PCK@α pÒq αimg = 0.1 αbbox = 0. - 66.53 - 72.31 - 77.07 59.50 59.57 64.00 63.73 69.60 68.64 Table 5. Reproduced vs reported numbers for zero-shot semantic correspondences, evaluated on SPair71k [31]. Tale of Two Features [58] and Telling Left from Right [59] report higher PCK values than our reproduction because they utilize conditioning mechanism on CLIP image embeddings from [56] that was finetuned for panoptic segmentation. As this task is related to semantic correspondence matching, we do not consider using this conditioning mechanism fair in comparison to other zero-shot approaches for semantic correspondences. Therefore, we exclude it from our reproductions. of the models feature by normalization is harmful and cannot be recovered by subsequent scaling and shifting. Our main configuration for the projection heads uses the SwiGLU [49] gating mechanism as an activation function in each FFN block. We investigate the influence of removing this gating mechanism from our FFNs, effectively leaving us with Swish layers [42]. When removing the gating mechanism, PCKbbox slightly decreases by 0.12 percentage points. Additionally, we experiment with fine-tuning the projection heads before training our feature extraction model. After pre-training the projection heads, we fine-tune them in two settings: fine-tuning both the feature extraction model and projection heads, and training only the feature extraction model while locking the pre-trained projection heads. When fine-tuning both the feature extraction model and projection heads, we achieve the exact same performance as our main configuration which does not use pretraining. When locking the projection heads during finetuning, the performance slightly decreases by 0.16 percentage points for PCKbbox. Figure 12. Additional qualitative results for semantic correspondence matching using DIFT [54] with the standard SD 2.1 (t 261) and our CleanDIFT features. Our clean features show significantly less incorrect matches than the standard diffusion features, especially along texture-less edges. E. Evaluation Design Choices During our efforts to reproduce the numbers reported by [54, 58, 59], we found variety of differences between evaluation pipelines that influence the results. We list them here to provide some clarity for future comparisons. 13 from [56] that was fine-tuned for panoptic segmentation. Specifically, they multiply the CLIP image embedding element-wise with learned tensor. This reweighed CLIP conditioning is then added to the embedding of an empty prompt and subsequently passed to the U-Net as the prompt embedding. Additionally, another learned tensor is used to element-wise scale the CLIP image embedding and then add it to the timestep embedding. Sliding Window Both Tale of Two Features [58] and Telling Left from Right [59] use sliding window approach to account for input resolutions higher than the native model input resolution. Specifically, they perform forward passes for overlapping patches, each patch having the model input resolution. Additionally, the methods use different resizing strategies to handle non-square images. DIFT [54] simply resizes the non-square input images to the square input resolution of the evaluation pipeline. Tale of Two Features [58] and Telling Left from Right [59] resize the input image such that the longer side matches the evaluation resolution and pad the remaining part of the square image with zeros. Figure 13. Depending on the downstream application, different diffusion timesteps result in optimal feature representations. For semantic segmentation, 100 is optimal resulting in much cleaner segmentation map compared to higher timesteps. However, for depth estimation, the low timestep yields inaccurate depth estimates and higher timestep is necessary (t 300). CleanDIFT remedies the dependence on the timestep and yields optimal features for every downstream task without additional tuning (Figure 1). Input Ours SD 2.1 Input Ours SD 2. Figure 14. Additional qualitative results for semantic segmentation from diffusion features on Pascal VOC [15]. Standard SD features use 100 as the timestep, which we found to perform best quantitatively (c.f. Figure 8). Figure 15. Metric depth prediction for NYUv2 [34] using linear probes. We investigate our proposed projection heads outputs by training linear probes for depth prediction on them, following the procedure described in Sec. 4.3. This figure extends the results presented in Tab. 2 by showcasing the performance over timesteps. CLIP Image Embedding Conditioning Tale of Two Features [58] and Telling Left from Right [59] employ conditioning mechanism on CLIP image embeddings"
        }
    ],
    "affiliations": [
        "CompVis @ LMU Munich"
    ]
}