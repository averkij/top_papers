{
    "paper_title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation",
    "authors": [
        "Jie Jiang",
        "Yangru Huang",
        "Zeyu Wang",
        "Changping Wang",
        "Yuling Xiong",
        "Jun Zhang",
        "Huan Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints."
        },
        {
            "title": "Start",
            "content": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation Jie Jiang, Yangru Huang, Yuze Wang, Changping Wang, Yuling Xiong, Jun Zhang, Huan Yu Tencent Inc., China {zeus,yarayrhuang,peterzywang,terracewang,whitnyxiong,neoxzhang,huanyu}@tencent.com 6 2 0 2 1 1 ] . [ 1 9 9 6 0 1 . 2 0 6 2 : r Abstract Generative recommendation via autoregressive models has unified retrieval and ranking into single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from fundamental probabilityreward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding weak comparative signal for RL. To address these challenges, we propose V-STAR, Value-guided Sampling and Treestructured Advantage Reinforcement framework. V-STAR forms self-evolving loop via two synergistic components. First, ValueGuided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints. Keywords Generative Recommendation, Reinforcement Learning, ValueGuided Efficient Decoding, Sibling-GRPO"
        },
        {
            "title": "1 Introduction\nRecommender systems are shifting from retrieve-and-rank to end-\nto-end generative recommendation (GR) [21, 26, 50], which has\nbeen deployed in e-commerce and short-video platforms [26, 50].\nBy reformulating item identifiers into a hierarchical search space\nof Semantic IDs (SIDs), large-scale candidate generation has be-\ncome computationally practical with autoregressive language mod-\nels [9, 38, 39, 46]. To further bridge the gap between generation\nlikelihood and real-world utility, these GR models are usually fine-\ntuned with reinforcement learning (RL) [4, 6, 30] techniques, e.g.,\non-policy GRPO-style groupwise updates [9, 14, 20, 43, 52]. How-\never, such a training mechanism is bottlenecked by a fundamental\nprobability-reward misalignment. During training, candidate items\nare typically sampled by probability-driven decoding strategies",
            "content": "Equal contribution: Jie Jiang and Yangru Huang. Corresponding author: Jun Zhang. Figure 1: Comparison of Beams Search and V-STAR. Left: probability-based pruning removes high-reward items and produces homogeneous candidates. Right: V-STAR expands high-value prefixes under fixed budget and strengthens within-group learning with Sibling-GRPO. (e.g., beam search). In contrast, the recommendation objective aims to maximize the expected reward of the full generated SIDs. This misalignment creates tension between the greedy nature of generation and the long-term goal of exploration, manifesting in two critical structural failures: (1) insufficient exploration of high-reward items: Many high-reward items begin with lowerprobability tokens because they have limited historical interactions. Standard beam search prunes branches based on likelihood and can irreversibly discard such paths early in generation. Consequently, the model becomes blind to high-reward candidates simply because their prefixes are not immediately likely. (2) advantage compression within the generated group: Guided by likelihood, decoding tends to over-expand multiple branches that share high-probability prefixes, producing candidate set dominated by siblings, i.e., near-duplicate items with similar rewards. However, effective RL relies on reward contrast: policy gradients are driven by differences between good and bad actions. In such sibling-heavy groups, rewards become highly correlated and concentrate in narrow band, while few outliers can set the overall reward scale. As result, GRPOs group-wise normalization is dominated by the global reward range and shrinks the relative advantage differences among siblings, weakening the learning signal at the decisions that actually matter. Previous methods attempt to mitigate these issues, but each incurs hard trade-off. One line of work increases the sampling budget [11, 42, 45], yet indiscriminate expansion largely spends compute on redundant, high-probability candidates. Another line relies on heuristic sampling (e.g., temperature scaling or nucleus sampling) to increase sample diversity [13, 24, 36], but the added Conference17, July 2017, Washington, DC, USA Trovato et al. randomness is difficult to control, often hurting relevance and making high-reward outcomes less reliable. More structured approaches, such as Monte Carlo Tree Search or tree-of-thought style search [23, 32, 47], can improve exploration in LLM inference, but their computational cost is prohibitive for broad use in recommendation. critical gap therefore remains: we lack mechanism that reliably determines when exploration is worth paying for and where limited compute should be allocated to maximize reward. Motivated by this gap, we make simple observation: under fixed decoding budget, extra exploration does not help equally at every generation step. Many prefixes are clearly low-potential, or their next token is already obvious, so additional compute there is largely wasted. In contrast, exploration matters most at small number of decisive prefixes where several plausible branches compete and which branch to choose strongly affects the final reward (e.g., whether long-tail but high-reward items can be reached). We detect these decisive points using two signals: (1) high value, meaning the prefix is promising in expected reward, and (2) high ambiguity given high value, meaning the next-step choice is uncertain enough that extra search can change the outcome. This perspective turns exploration into targeted budgeting problem: decide where to spend compute so that it most improves reward, which directly motivates value-guided decoding strategy and tree-structured credit assignment objective. Building on these insights, we propose V-STAR (Value-guided Sampling and Tree-structured Advantage Reinforcement), selfevolving sampling-and-learning framework where better sampling provides clearer training signals, and the resulting updates further improve sampling. V-STAR comprises two key components: ValueGuided Efficient Decoding (VED) for candidate sampling and Sibling-GRPO for tree-structured policy optimization. VED tackles insufficient exploration by performing budgeted value-guided expansion on the SID prefix tree. It initializes shallow prefix tree with likelihood-guided beam search to cheaply reveal where hypotheses concentrate and where promising branches are pruned early. It then combines lightweight value estimator with an uncertainty signal (e.g., policy entropy) to select small set of high-potential prefixes and spends additional compute only at these divergence points, enabling local expansion and backup under strict depth and branching constraints. This targeted allocation corrects premature pruning and improves the reachability of long-tail, high-reward items without incurring exhaustive tree search. Sibling-GRPO addresses advantage compression in prefix-coupled generation by introducing structured group-relative objective over genealogically related candidates. Rather than applying single global normalization over the entire candidate set, it forms sibling groups under each parent prefix and performs within-group relative advantage learning. This concentrates gradients on the decisive branching actions where candidates diverge yet remain highly correlated, mitigating advantage compression and recovering informative learning signals inside homogenized candidate clusters. Together, VED improves candidate quality and diversity, while Sibling-GRPO turns these gains into more stable and informative updates. In summary, our contributions are as follows: (1) We formalize probabilityreward misalignment in generative recommendation and analyze its impact on sample decoding and RL fine-tuning. Building on this analysis, we propose V-STAR to address the issue via value-guided decoding and tree-structured credit assignment. (2) We develop Value-Guided Efficient Decoding (VED) for budgeted candidate construction by allocating decoding compute to high-value and high-ambiguity nodes. (3) We introduce Sibling-GRPO, tree-structured objective that uses sibling groups to mitigate advantage compression under correlated candidates. (4) Experiments on both offline datasets and online settings show consistent gains over strong generative baselines."
        },
        {
            "title": "2.3 RL for Generative Alignment\nRL is widely used to align generative models with non-differentiable\nobjectives (e.g., CTR and diversity). Common approaches in-\nclude REINFORCE [41], PPO [28], GRPO [29], and recent direct",
            "content": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation Conference17, July 2017, Washington, DC, USA preference-optimization methods such as DPO [25]. In recommendation, RL has been applied to optimize slate generation (e.g., SlateQ [16]) and to correct exposure bias [5]. However, existing generative RL methods often implicitly treat candidate samples as independent and identically distributed (i.i.d.) [9, 39]. In autoregressive generation, candidates frequently share long prefixes and therefore form sibling groups with highly correlated features and rewards. Standard policy-gradient updates can be ineffective in this setting: subtracting baseline from set of nearly identical rewards yields weak and noisy advantage signal, which can lead to advantage collapse [27]. Our proposed Sibling-GRPO addresses this issue by explicitly modeling relative advantages within sibling groups, which is related in spirit to listwise ranking objectives [3] but tailored to the hierarchical structure of the generation tree."
        },
        {
            "title": "3 Preliminaries\n3.1 Problem Formulation\nGenerative Recommendation with Semantic IDs. We formu-\nlate recommendation as a conditional sequence generation task [10,\n34]. Under the SID paradigm [26], each item is represented by a\nunique, fixed-length sequence of discrete tokens ğ‘¦ = (ğ‘¦1, . . . , ğ‘¦ğ¿) âˆˆ\nVğ¿, where V is a finite token vocabulary and ğ¿ is the (fixed) SID\nlength. Given a user context ğ‘¥ (e.g., interaction history and pro-\nfile features), an autoregressive policy ğœ‹ğœƒ models the conditional\nprobability of an item sequence via the probability chain rule:\nğœ‹ğœƒ (ğ‘¦ | ğ‘¥) = (cid:206)ğ¿\n| ğ‘¥, ğ‘¦â‰¤ğ‘™ âˆ’1) , where ğ‘¦â‰¤ğ‘™ âˆ’1 = (ğ‘¦1, . . . , ğ‘¦ğ‘™ âˆ’1)\nğ‘™=1\ndenotes the already generated prefix before step ğ‘™, with ğ‘¦â‰¤0 = âˆ…\n(empty prefix, i.e., generation starts from the user context ğ‘¥ alone).\nDecoding Operator. We abstract decoding process as an operator\nD that maps policy ğœ‹ğœƒ and context ğ‘¥ to a candidate set C(ğ‘¥):",
            "content": "ğœ‹ğœƒ (ğ‘¦ğ‘™ C(ğ‘¥) Vğ¿. C(ğ‘¥) = (ğœ‹ğœƒ , ğ‘¥), (1) Ideally, should return candidates that maximize the groundtruth reward ğ‘…(ğ‘¥, ğ‘¦). In practice, standard decoding strategies (e.g., beam search [45]) approximate this objective by maximizing model probability ğœ‹ğœƒ (ğ‘¦ ğ‘¥) through heuristic prefix pruning. Policy Refinement via GRPO. To align the pre-trained policy with non-differentiable ranking objectives, we apply GRPO [29] to the decoded set C(ğ‘¥) as the empirical reference set for reward comparison. The standard group-normalized advantage is used: ğ´(ğ‘¥, ğ‘¦) = ğ‘…(ğ‘¥, ğ‘¦) ğœ‡ğ‘… (ğ‘¥) ğœğ‘… (ğ‘¥) + ğœ– , (2) where ğœ‡ğ‘… (ğ‘¥) and ğœğ‘… (ğ‘¥) are the mean and standard deviation of rewards over C(ğ‘¥) and ğœ– > 0 ensures numerical stability. In addition to standard GRPO, our method also applies sibling GRPO to further enhance learning signals, which will be introduced in Sec. 4.3."
        },
        {
            "title": "3.2 Probability-Driven Decoding Bias\nInsufficient exploration on high-reward items. Autoregressive\ndecoding process can be viewed as traversing a prefix tree [40]: SIDs\nsharing a common prefix follow the same branch until they diverge.\nUnder a fixed budget, pruning-based methods [7, 24, 36] (e.g., beam\nsearch or top-K sampling) keep only a small set of active prefixes\nand discard the rest by local likelihood. Pruning is irreversible in this\ncase: once a prefix is removed, all items under that branch will never\nbe evaluated. This is problematic because likelihood and reward can",
            "content": "be misaligned: there may exist ğ‘¦ğ‘, ğ‘¦ğ‘ with ğœ‹ğœƒ (ğ‘¦ğ‘ ğ‘¥) > ğœ‹ğœƒ (ğ‘¦ğ‘ ğ‘¥) but ğ‘…(ğ‘¥, ğ‘¦ğ‘) < ğ‘…(ğ‘¥, ğ‘¦ğ‘ ). As result, likelihood-only pruning may eliminate low-probability prefixes that lead to high-reward items, and those branches are rarely revisited. Advantage compression within the generated group. Given candidate set C(ğ‘¥), we define the within-group reward range as: (3) ğ‘…(ğ‘¥, ğ‘¦). Î”ğ‘… (ğ‘¥) max ğ‘¦ (ğ‘¥ ) ğ‘…(ğ‘¥, ğ‘¦) min ğ‘¦ (ğ‘¥ ) . (4) ğ´(ğ‘¥, ğ‘¦) = Î”ğ‘… (ğ‘¥) ğœ– (cid:12) (cid:12) (cid:12) (cid:12) Î”ğ‘… (ğ‘¥ ) ğœ– Since ğ‘…(ğ‘¥, ğ‘¦) ğœ‡ğ‘… (ğ‘¥) Î”ğ‘… (ğ‘¥) for all ğ‘¦ C(ğ‘¥), the within-group advantage magnitude is bounded by: (cid:12) (cid:12) (cid:12) (cid:12) Î”ğ‘… (ğ‘¥) ğ‘…(ğ‘¥, ğ‘¦) ğœ‡ğ‘… (ğ‘¥) ğœğ‘… (ğ‘¥) + ğœ– ğœğ‘… (ğ‘¥) + ğœ– , Î”ğ‘… (ğ‘¥ ) Eq. 4 implies that ğ´(ğ‘¥, ğ‘¦) [ ]. By Popovicius inğœ– Î”ğ‘… (ğ‘¥ ) equality on variances, Varğ‘¦ (ğ‘¥ ) [ğ´(ğ‘¥, ğ‘¦)] ( )2, so the stanğœ– Î”ğ‘… (ğ‘¥ ) dard deviation ğœğ´ (ğ‘¥) Varğ‘¦ (ğ‘¥ ) [ğ´(ğ‘¥, ğ‘¦)] . Therefore, ğœ– when candidate collapse in C(ğ‘¥) yields small Î”ğ‘… (ğ‘¥), it forces ğœğ´ (ğ‘¥) to be small, making advantages nearly indistinguishable. We refer to this degeneration of learning signal as advantage compression. Implication. Together, insufficient exploration and advantage compression show that decoding is not mere inference-time heuristic [31]: it controls both reachability (which high-reward items are ever surfaced) and learnability (whether candidates provide enough reward contrast for RL). We therefore need decoder that avoids irreversible pruning of low-probability yet high-reward branches and yields candidate groups with sufficient reward dispersion, motivating our dynamic decoding framework."
        },
        {
            "title": "4.1 Semantic-aware Value Model Learning\nVED relies on lookahead value estimation to mitigate probability-\ndriven bias on the SID prefix tree. To this end, we first learn a\nprefix value function ğ‘‰ğœ™ that maps each prefix to its expected down-\nstream return [28]. At generation step â„“ âˆˆ {1, . . . , ğ¿}, we define\nthe decoding state ğ‘ â„“ as ğ‘ â„“ â‰œ (ğ‘¥, ğ‘¦â‰¤â„“ ), where ğ‘¥ is the user context\nand ğ‘¦â‰¤â„“ = (ğ‘¦1, . . . , ğ‘¦â„“ ) is the current SID prefix. The value function\nestimates the discounted return of continuing generation from ğ‘ â„“ :\n(cid:34) ğ¿\nâˆ‘ï¸",
            "content": "(cid:35) ğ‘‰ğœ™ (ğ‘ â„“ ) ğ›¾ ğ‘¡ â„“ ğ‘Ÿğ‘¡ ğ‘ â„“ , (5) ğ‘¡ =â„“ where ğ›¾ (0, 1] is the discount factor. We parameterize ğ‘‰ğœ™ as lightweight value head on top of the policy backbone: single shallow Transformer block (operating on the prefix hidden states) followed by an MLP regressor, enabling frequent value queries with negligible overhead. Semantic-aware dense supervision. Learning ğ‘‰ğœ™ from sparse terminal returns (non-zero only when the generated item exactly matches the ground truth) provides weak, delayed signals for intermediate prefixes. We therefore build dense semantic returns from pre-computed item embeddings, obtained by encoding each items (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Conference17, July 2017, Washington, DC, USA Trovato et al. Figure 2: Overview of the proposed self-evolving decoding-and-learning framework V-STAR. textual description with frozen text encoder. Given context ğ‘¥, let C(ğ‘¥) = (ğœ‹ğœƒ , ğ‘¥) be the sampled candidate set. For prefix ğ‘¦â„“ , we define the sampled prefix bucket to be the subset of items in C(ğ‘¥) that shares this prefix: ğ‘¦â„“ = C(ğ‘¥; ğ‘¦â„“ ) (cid:8) ğ‘¦ C(ğ‘¥) ğ‘¦ (6) where ğ‘¦ â„“ denotes the SID prefix of the candidate item ğ‘¦. We then define context-conditional prefix embedding by averaging embeddings over this bucket: â„“ = ğ‘¦â„“ (cid:9) , e(ğ‘¥, ğ‘¦â„“ ) 1 C(ğ‘¥; ğ‘¦â„“ ) where e(ğ‘–) is the embedding of candidate item ğ‘–. The step-wise dense return ğ‘Ÿâ„“ for prefix ğ‘¦â„“ is then defined as: ğ‘– (ğ‘¥;ğ‘¦â„“ ) e(ğ‘–), (7) ğ‘Ÿâ„“ = ğ‘¤â„“, ğ‘¤â„“ (cid:0)1 cos(cid:0)e(ğ‘¥, ğ‘¦â„“ ), e(ğ‘¦)(cid:1)(cid:1) , if ğ‘¦â„“ = ğ‘¦ â„“, otherwise, (8) where ğ‘¤â„“ > 0 is monotonically increasing weight with respect to â„“, cos denotes the cosine similarity between the embeddings, and e(ğ‘¦) represents the embedding of the ground-truth item ğ‘¦ for context ğ‘¥. Compared to sparse exact-match supervision, the proposed semantic-aware dense supervision provide informative feedback on mismatched prefixes by leveraging their semantic proximity to the ground-truth item, thereby improving value estimation for intermediate decoding states. Temporal-difference learning. Given the stepwise signal in (8), we train the value function by one-step temporal-difference learning, enforcing Bellman-style consistency across successive prefixes. Concretely, the TD target and regression loss are: ğ‘Ÿâ„“ + ğ›¾ ğ‘‰ğœ™ (ğ‘ â„“+1), ğ‘Ÿğ¿, â„“ < ğ¿, â„“ = ğ¿, Lğ‘‰ (ğœ™) = E(cid:104) (cid:0)ğ‘‰ğœ™ (ğ‘ â„“ ) ğ‘¦â„“ (cid:1) 2(cid:105) , (9) where ğ‘ â„“+1 = (ğ‘¥, ğ‘¦â„“+1). The resulting ğ‘‰ğœ™ provides an efficient, reward-aligned lookahead over prefixes, which is later used to guide budgeted expansion toward decisive prefix nodes in Sec. 4.2."
        },
        {
            "title": "4.2 Value-Guided Efficient Decoding\nWith the value model as guidance, we cast semantic-ID decoding as\nconstructing a candidate set under a strict compute budget on the\nSID prefix tree. Given context ğ‘¥, the decoder returns C(ğ‘¥) âŠ‚ Vğ¿\nthat (i) contains high-reward items and (ii) enables informative\nwithin-group comparisons for groupwise RL.\nA budgeted objective for candidate sets. Let Cost(C(ğ‘¥)) denote\nthe compute required to construct C(ğ‘¥), measured by the number of\nbackbone forward tokens incurred during decoding. We formulate\ncandidate construction as a constrained set optimization problem:",
            "content": "max (ğ‘¥ ) Vğ¿ Eğ‘¦ (ğ‘¥ ) [ğ‘…(ğ‘¥, ğ‘¦)] (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) (cid:124) reward s.t. Cost(C(ğ‘¥)) ğµ. + ğœ† Contrast(C(ğ‘¥)) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:123)(cid:122) dispersion (10) where Contrast() is set-level regularizer that encourages withingroup discriminability, e.g., by increasing reward dispersion and mitigating prefix coupling (Sec. 3.2). The budget ğµ captures the computational constraint in recommendation. Value-based scoring with entropy regularization. Directly optimizing Eq. (10) is intractable due to the exponential SID space. VED therefore allocates the decoding budget to small set of decisive prefixes. The key question is how to rank prefix states ğ‘  = (ğ‘¥, ğ‘¦â„“ ) for further expansion. To maximize the expected improvement Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation Conference17, July 2017, Washington, DC, USA per unit cost, prefix expansion should balance exploitation and exploration. We therefore define prefix priority score: ğº (ğ‘ ) ğ‘‰ğœ™ (ğ‘ ) + ğœ† Hğœƒ (ğ‘ ), ğ‘‰ğœ™ (ğ‘ ), â„“ < ğ¿, â„“ = ğ¿, (11) where ğ‘‰ğœ™ (ğ‘ ) is the predicted downstream return from prefix ğ‘ , ğœ† is the regularization weight, and Hğœƒ (ğ‘ ) is the entropy term that measures how uncertain the policy is about the next token: ğœ‹ğœƒ (ğ‘¦â„“+1 ğ‘¥, ğ‘¦â„“ ) log ğœ‹ğœƒ (ğ‘¦â„“+1 ğ‘¥, ğ‘¦â„“ ). Hğœƒ (ğ‘ ) (12) ğ‘¦â„“ +1 For terminal prefixes (â„“ = ğ¿), the priority reduces to ğ‘‰ ğœ™ (ğ‘ ). Score-based efficient decoding. With the above value-based prefix score, the decoding process performs budgeted search and expansion on search tree over prefix states ğ‘  = (ğ‘¥, ğ‘¦â„“ ). The procedure consists of four stages: 1 initialization, 2 selection and traversal, 3 gated expansion, and 4 backpropagation. 1 Initialization. We initialize using prefixes obtained by low-cost probability-guided beam search under ğœ‹ğœƒ . The beam width in this warm start is set smaller than the final candidate set size to limit overhead. The resulting prefix tree reveals which prefixes the policy prefers and which branches are pruned early. We evaluate each node in the initial tree once to populate {ğ‘‰ğœ™ (ğ‘ ), Hğœƒ (ğ‘ ), ğº (ğ‘ )} and record whether the node admits unexpanded valid children. 2 Selection and traversal. After initialization, we repeatedly select nodes and traverse root-to-leaf path. Let ğ‘ (ğ‘ ) be the visit count of node ğ‘  and ğ‘root the number of traversals so far, we select nodes with UCB-style score [18]: ğ‘ˆ (ğ‘ ) ğº (ğ‘ ) + ğ›½ ln(cid:0)ğ‘root + 1(cid:1) ğ‘ (ğ‘ ) + 1 , (13) where ğ›½ controls exploration. Starting from the root, we recursively choose the child with the largest ğ‘ˆ (ğ‘ ) until reaching leaf or terminal node. 3 Gated expansion. Along the selected path, we expand only prefixes that are decisive relative to other prefixes at the same depth. Let Tâ„“ denote the set of nodes in the current tree at depth â„“, and define the depth-wise average priority ğºâ„“ 1 Tâ„“ ğ‘¢ Tâ„“ ğº (ğ‘¢). (14) For any visited node ğ‘  = (ğ‘¥, ğ‘¦â„“ ) on the path that is not fully expanded, we trigger one-step expansion if and only if ğº (ğ‘ ) ğºâ„“ . When triggered, we add one new child to ğ‘  by sampling token from its yet-unexpanded valid children according to their normalized policy probabilities. 4 Backpropagation. For each newly added child ğ‘ new, we evaluate (ğ‘‰ğœ™, Hğœƒ , ğº) and update statistics along its ancestor chain, including visit count ğ‘ () and depth-wise average priority scores. The above stages 2 4 are repeated until Cost(T ) > ğµ, where Cost() counts the number of backbone forward tokens. After termination, we extract candidate set C(ğ‘¥) Vğ¿ from the search tree in value-aware manner. In practice, we sample top-valued SIDs from depth-ğ¿ leaf nodes. In rare cases, if insufficient leaf nodes exist, we fill the remainder by completing high-value prefixes in the tree using cached ğœ‹ğœƒ , without additional backbone forwards. The resultant candidate set improves reachability by exploring low-probability but high-value prefixes."
        },
        {
            "title": "4.3 Sibling-GRPO\nWhile VED improves reachability and increases candidate reward\ndispersion, hierarchical SIDs still induce common-prefix sibling\nnode groups in C(ğ‘¥). Sibling-GRPO exploits these groups to recover\nstrong learning signals exactly at the decisive branching actions.\nSibling groups and sibling nodes. An SID ğ‘¦ = (ğ‘¦1, . . . , ğ‘¦ğ¿) cor-\nresponds to a root-to-leaf path on a depth-ğ¿ prefix tree. Fix a depth\nâ„“ âˆˆ {1, . . . , ğ¿} and let â„ âˆˆ Vâ„“ âˆ’1 denote a parent prefix at depth â„“ âˆ’ 1.\nWe first define the sibling group under â„ as the sampled items that\nshare this parent prefix:",
            "content": "G(â„) { ğ‘¦ C(ğ‘¥) ğ‘¦â„“ 1 = â„ } . (15) Next, we define the sibling node set under â„ as the set of child nodes at depth â„“ that extend â„ by one token: S(â„) { ğ‘£ ğ‘£ V, ğ‘¦ G(â„) s.t. ğ‘¦â„“ = ğ‘£ } . (16) Relative advantages over sibling nodes. Sibling-GRPO is computed within S(â„) by assigning each child node ğ‘£ S(â„) nodelevel score defined as the average reward of candidates routed through that child: ğ‘…(ğ‘¥; â„, ğ‘£) 1 G(â„, ğ‘£) ğ‘…(ğ‘¥, ğ‘¦), ğ‘¦ (â„,ğ‘£) (17) G(â„, ğ‘£) { ğ‘¦ G(â„) ğ‘¦â„“ = ğ‘£ }. This construction yields localized, sibling-level comparisons that focus learning on the branching decision at depth â„“. Let ğœ‡â„ (ğ‘¥) and ğœâ„ (ğ‘¥) denote the mean and standard deviation of { ğ‘…(ğ‘¥; â„, ğ‘£)}ğ‘£ (â„) , the sibling-relative node advantage for child ğ‘£ S(â„) is ğ´node (ğ‘¥; â„, ğ‘£) = ğ‘…(ğ‘¥; â„, ğ‘£) ğœ‡â„ (ğ‘¥) ğœâ„ (ğ‘¥) + ğœ– . (18) Compared with global normalization over C(ğ‘¥), this sibling-node advantage prevents within-parent advantages from being washed out by coarse variations across unrelated prefixes, and it concentrates learning signals on the branching action ğ‘¦â„“ that differentiates siblings under the same parent prefix. Objective. Sibling-GRPO retains the GRPO update form but applies it to the sibling nodes within each depth. For parent prefix â„ Vâ„“ 1 at depth â„“ 1 and child node ğ‘£ S(â„). We define the token-level importance ratio as ğœŒğœƒ (ğ‘£ ğ‘¥, â„) ğœ‹ğœƒ (ğ‘£ ğ‘¥,â„) , where (ğ‘£ ğ‘¥,â„) ğœ‹ğœƒold denotes the frozen behavior policy used to sample C(ğ‘¥). We then maximize the following sibling-node GRPO objective aggregated over depths: ğœ‹ğœƒ old ğ¿ 1 C(ğ‘¥) Jsib (ğœƒ ) = Eğ‘¥ (19) This formulation performs GRPO-style [29] updates on branching tokens at each depth, with sibling-normalized advantages computed across competing child nodes under the same parent prefix. ğ´node (ğ‘¥; â„, ğ‘£) ğœŒğœƒ (ğ‘£ ğ‘¥, â„) ğ‘£ (â„) â„“=1 â„ ."
        },
        {
            "title": "4.4 Training and Testing\nTraining-Time Self-Evolution. Our V-STAR primarily focuses\non training-time improvement. During training, VED and Sibling-\nGRPO form a closed loop: VED uses the value ğ‘‰ğœ™ together with\nthe entropy signal to selectively refine search and generate struc-\ntured candidate groups with better reachability and clearer reward\ncontrast. Sibling-GRPO then converts these structured groups into",
            "content": "Conference17, July 2017, Washington, DC, USA Trovato et al. Table 1: Performance comparison on Industrial and Office. Gen. means Generative model. Best results are bolded. Model Gen. SID RL Industrial Office HR@3 HR@5 HR@10 NDCG@3 NDCG@5 NDCG@10 HR@3 HR@5 HR@10 NDCG@3 NDCG@5 NDCG@10 GRU4Rec Caser SASRec HSTU BIGRec TIGER LCRec D3 S-DPO MiniOneRec Ours-Train 0.0638 0.0618 0. 0.0927 0.0931 0.0852 0.0915 0.1024 0.1032 0.1143 0.0774 0.0717 0.0909 0.1037 0.1092 0.1010 0.1057 0. 0.1238 0.1321 0.0999 0.0942 0.1088 0.1163 0.1370 0.1321 0.1332 0.1500 0.1524 0.1586 0.0542 0.0514 0. 0.0885 0.0841 0.0742 0.0805 0.0991 0.0906 0.1011 0.0598 0.0555 0.0748 0.0918 0.0907 0.0807 0.0862 0. 0.0991 0.1084 0.0669 0.0628 0.0806 0.0958 0.0997 0.0908 0.0952 0.1082 0.1082 0.1167 0.0629 0.0748 0. 0.1134 0.1069 0.0986 0.0921 0.1204 0.1169 0.1217 0.0789 0.0865 0.0949 0.1252 0.1204 0.1163 0.1048 0. 0.1356 0.1420 0.1019 0.1093 0.1120 0.1400 0.1434 0.1408 0.1237 0.1634 0.1587 0.1634 0.0528 0.0615 0. 0.1031 0.0961 0.0852 0.0807 0.1055 0.1033 0.1088 0.0595 0.0664 0.0805 0.1079 0.1017 0.0960 0.0859 0. 0.1110 0.1172 0.0669 0.0737 0.0858 0.1126 0.1091 0.1002 0.0920 0.1213 0.1255 0.1242 0.1189 0.1361 0. 0.1057 0.1128 0.1217 0.1344 0.1500 0.1746 0.1196 0. 0.1340 stable, high-quality policy updates by comparing sibling alternatives at each branching level. As training progresses, both ğœ‹ğœƒ and ğ‘‰ğœ™ improve, which sharpens the acquisition score in Eq. (11) and further improves subsequent candidate constructionyielding self-evolving decoding-and-learning loop. Testing-Time Decoding. At test time, we use standard beam search [34] decoding for efficiency and compatibility with existing serving pipelines [9]. Optionally, decoding with VED can also be applied at inference: the learned ğ‘‰ğœ™ and the same acquisition rule Eq. (11) guide the decoder to select small number of high-potential prefixes under fixed compute budget, improving long-tail coverage and candidate diversity when additional search is allowed."
        },
        {
            "title": "5 Experiments\n5.1 Experimental Setup\nDatasets and Evaluation. Experiments are conducted on both\noffline public dataset and online data. For offline evaluation, we\nuse Amazon Review dataset [15] with two subsets Industrial and\nOffice Products. Following the standard next-item prediction proto-\ncol, interactions are ordered chronologically per user. The model\ntakes the entire historical sequence as input and predicts the next\ninteracted item. For data preprocessing, we follow the procedure\ndescribed in [20]. Each item is represented by a 3-level SID derived\nfrom the RQ-VAE codebook [26], and all generative decoders op-\nerate under a hard constraint that restricts outputs to valid SIDs,\nensuring that each generated sequence maps to a unique catalog\nitem. We report standard metrics Hit Rate (HR) and Normalized Dis-\ncounted Cumulative Gain (NDCG). HR@K measures whether the\nground-truth next item appears in the Top-ğ¾ list, while NDCG@K\nfurther accounts for its ranked position. Unless otherwise specified,\nwe set ğ¾ = 3, 5, 10 in the main tables. For online evaluation, we use\nreal traffic data, which will be detailed in Sec. 5.3.\nImplementation Details. We use Qwen2.5-1.5B [44] as the model\nbackbone. All models are trained in two stages: supervised fine-\ntuning (SFT) followed by RL with GRPO. SFT is run with data\nparallelism on 8 GPUs with a global batch size of 1024 and a fixed\nrandom seed. RL is trained on 8 GPUs with a per-GPU batch size 64\nand gradient accumulation step 2. The learning rate is set to 1Ã—10âˆ’5,\nand KL regularization coefficient is 10âˆ’3. During training, VED is",
            "content": "utilized to construct 16 candidates per query, with prefix length ğ¿ = 3 and hierarchical weights ğ‘¤ğ‘™ = [0.3, 0.5, 1.0]. The discount factor ğ›¾ in Eq. 9 is set to 0.99 and the exploration coefficient ğœ† in Eq. 11 is set to 0.1. The beam width for VED prefix-tree initialization is set to 8. To ensure serving efficiency, we adopt standard beam search as the default decoder during inference, effectively decoupling the exploration-intensive training from latency-critical deployment. 5.2 Performance on Offline Dataset The proposed method is compared with recent state-of-the-art baselines under the same setting, including traditional embeddingbased sequential recommenders (GRU4Rec [12], Caser [35], and SASRec [17]), generative non-SID baselines (HSTU [49], BIGRec [1]), SID-based generative recommenders without RL-style optimization (TIGER [26], LCRec [51], D3 [2]), and SID-based methods with RL or preference optimization (S-DPO [19], MiniOneRec [20]). Table 1 summarizes results on Industrial and Office. Our method consistently achieve the highest performance among all methods. Compared with the strongest RL baseline MiniOneRec, our method achieves 4.0% relative improvement in HR@3 and 4.3% relative improvement in NDCG@10 on Industrial. On Office, the gain is more pronounced, reaching 10.4% relative improvement in HR@3. These consistent gains indicate that value-guided, budgeted exploration can reliably surface higher-reward SID candidates under strict prefix constraints, while tree-structured advantage reinforcement strengthens training by concentrating gradients on the branching decisions that actually differentiate sibling items. 5.3 Online Performance To validate V-STAR in real business environment, we conduct online A/B testing for 5 days on 5% of live request traffic on WeChat Channels. We use GMV (Gross Merchandise Volume) as the primary metric, i.e., the total transaction value attributed to advertising recommendations and core indicator of commercial revenue. We compare against the BeamSearch+GRPO baseline. The online results show that V-STAR achieves 1.23% relative improvement in GMV and 1.87% relative improvement in GMV-Normal (GMV from click/conversion-optimized ad). These gains suggest that valueaware candidate construction, together with Sibling-GRPO, helps VSTAR more reliably surface high-commercial-value items, thereby increasing transaction volume. Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation Conference17, July 2017, Washington, DC, USA Table 2: Ablation on decoding strategies. Table 4: Ablation on training objective. Decoding Strategy Beam Search Top-K Ours Industrial Office NDCG@10 HR@10 NDCG@10 HR@10 Training Objective Industrial Office NDCG@10 HR@10 NDCG@10 HR@10 0.1194 0.1090 0.1217 0.1606 0.1538 0. 0.1299 0.1226 0.1340 0.1684 0.1644 0.1746 GRPO Sibling-GRPO Joint 0.1189 0.1204 0.1217 0.1598 0.1640 0.1641 0.1302 0.1335 0. 0.1712 0.1749 0.1746 Table 3: Ablation of expand rules in VED. Expansion Industrial Office Rule ğ‘‰ğœ™ (ğ‘  ) Hğœƒ (ğ‘  ) ğº (ğ‘  ) NDCG@10 HR@10 NDCG@10 HR@10 0.1202 0.1198 0. 0.1627 0.1604 0.1641 0.1333 0.1259 0.1340 0.1742 0.1688 0."
        },
        {
            "title": "5.4 Ablation Study\nWe conduct targeted ablation studies to disentangle the contribu-\ntions of (i) the decoding strategy used to construct candidate sets,\nand (ii) the training objective used to optimize the policy.\nImpact of the Decoding Strategy. To isolate decoding effects,\nwe fix the trained way and vary only the inference-time candi-\ndate construction method. On Industrial and Office, we compare\n(i) beam search, (ii) top-K (stochastic sampling from the ğ¾ highest-\nprobability tokens at each step), and (iii) our VED.1 Table 2 shows\nthat VED consistently achieves the best performance across datasets\nand metrics. Beam search is probability-dominated and may prune\nlow-prior yet high-reward branches, while top-ğ¾ improves diver-\nsity but yields less stable ranking due to untargeted exploration. In\ncontrast, VED reallocates compute to a small set of decisive prefixes\nwith high value and high uncertainty, mitigating early pruning and\nimproving reachability of valuable long-tail items.",
            "content": "To isolate what drives the gain in VED, we keep the sampling procedure and candidate budget, and vary only the prefix priority score for selecting expansion nodes. Table 3 compares three rules: value-only (ğ‘‰ğœ™ ), entropy-only (Hğœƒ ), and the joint score ğº (ğ‘ ) (Eq. 11) combining ğ‘‰ğœ™ and Hğœƒ . Value-only may over-exploit and miss lowprior yet high-reward branches, whereas entropy-only may overexplore high-uncertainty but low-reward regions. The joint score is consistently best, indicating that uncertainty mainly serves as an expansion gate: when value is high but confidence is already saturated, further expansion is redundant; uncertainty redirects budget to high-value yet under-resolved prefixes where exploration can still improve reachability. Impact of Sibling-GRPO. To evaluate the impact of the training objective, we fix VED candidate construction ğ¶ (ğ‘¥) and compare three RL objectives that differ only in how advantages are normalized on the VED-generated prefix tree: (i) GRPO with global groupbased normalization over ğ¶ (ğ‘¥); (ii) Sibling-GRPO with siblingrelative normalization at shared-prefix branching points; and (iii) GRPO+Sibling-GRPO that jointly optimizes both losses. Table 4 shows that standard GRPO performs worst, as global normalization is dominated by inter-cluster variance and compresses intra-group 1All methods enforce the same SID-validity constraint and return the same number of final items. Figure 3: Spearman Correlation ğœŒ of Probability(P) and Value (V) Signals with Ground-truth Rewards. advantages. Sibling-GRPO improves performance by aligning updates with the prefix-tree topology, focusing gradients on decisive branching nodes. Finally, the combined objective achieves the highest NDCG and HR. This suggests that global sequence-level alignment from GRPO and structure-aware, branch-level credit assignment from Sibling-GRPO are mutually reinforcing."
        },
        {
            "title": "5.5 Further Analysis\nHow well do probability and value align with ground-truth\nreward across SID levels? To answer this, we form a candi-\ndate pool C(ğ‘¥) of 64 SIDs for each query ğ‘¥ by temperature sam-\npling from 3 different models: SFT model, RL model trained with\nbeam search+standard GRPO, and our V-STAR model. The tem-\nperature is set to 1.5 to cover various probability range. At each\nlevel â„“ âˆˆ {1, 2, 3}, we compute Spearman correlation ğœŒ between\nprefix log-probability log ğœ‹ğœƒ (ğ‘¦â‰¤â„“ | ğ‘¥) and reward ğ‘…(ğ‘¥, ğ‘¦â‰¤â„“ ), where\nğ‘…(ğ‘¥, ğ‘¦â‰¤â„“ ) is the average reward of candidate in C(ğ‘¥) that share\nprefix ğ‘¦â‰¤â„“ . For V-STAR, we also compute ğœŒ between value predic-\ntion ğ‘‰ğœ™ (ğ‘¥, ğ‘¦â‰¤â„“ ) and ğ‘…(ğ‘¥, ğ‘¦â‰¤â„“ ). We report ğœŒ averaged over queries\nin Fig. 3. From the figure we can see that: (i) the predicted value\nğ‘‰ğœ™ (ğ‘¥, ğ‘¦â‰¤â„“ ) aligns best with ğ‘…(ğ‘¥, ğ‘¦â‰¤â„“ ), (ii) for probabilityâ€“reward\nalignment, V-STAR > beam search+GRPO > SFT, indicating that\nRL improves the probability landscape and V-STAR further en-\nhances such improvement, (iii) the correlation gap is largest at\nâ„“=3, where semantically similar leaf items reduce discriminability\nand weaken likelihoodâ€“reward correlation, while the value model\nremains strongly reward-aligned.\nDoes VED increase candidate-set diversity without sacrific-\ning quality? We measure candidate-set diversity in SID space by\ncalculating the average pairwise dissimilarity in candidate pool\nC(ğ‘¥). For two SIDs ğ‘£ğ‘– and ğ‘£ ğ‘— (each with ğ¿ tokens), we define their\nsimilarity as the normalized length of the longest common prefix\n(LCP), i.e., SimSID (ğ‘£ğ‘–, ğ‘£ ğ‘— ) = LCP(ğ‘£ğ‘–, ğ‘£ ğ‘— )/ğ¿. Diversity is then com-\nputed as the average of 1 âˆ’ SimSID (ğ‘£ğ‘–, ğ‘£ ğ‘— ) over all candidate pairs",
            "content": "Conference17, July 2017, Washington, DC, USA Trovato et al. Figure 4: Training time scaling with decoding token budget. Table 5: Case Study of different decoding strategies on Office Products. SIDs in red share prefixes with history items. SIDs in teal represent novel explorations. SIDs in green represent ground truth. VED shows higher candidate diversity and successfully discovers the ground-truth (GT) item in novel branch (<a_20>), while baselines remain biased toward historical prefixes. Context Key Items & Candidate SIDs (For non-GT candidates, only the first SID are shown for simplicity) History Pencil Sharpener (<a_250>), Postcards (<a_225>), Bubble Mailer (<a_118>), Thank-you Stickers (<a_186>), Sharpie Pen (<a_102>) GT Item ACCUTECK Digital Scale (<a_20><b_181><c_107>) Beam Search Stickers (<a_186>), Label Tape (<a_117>), Sharpie Pen (<a_102>), Postcards (<a_225>), Bubble Mailer (<a_118>), Stickers (<a_186>), Label Printer (<a_117>), Address Labels (<a_186>), Shipping Labels (<a_117>), Label Tape (<a_117>) (High redundancy: 6/10 items are historical prefixes; GT is missed) Top-K Ours (VED) Sharpie Pen (<a_102>), Thank-you Labels (<a_186>), Thank-you Stickers (<a_186>), Sharpie Marker (<a_102>), Sharpie Marker (<a_102>), Foam Board (<a_242>), EXPO Markers (<a_178>), Bubble Mailer (<a_118>), Glue Stick (<a_131>), File Folders (<a_204>) (Prefix-biased: 6/10 items are historical prefixes; GT is missed) (<a_186>), Thank-you Labels (<a_186>), Sharpie Pen (<a_102>), ACCUTECK Digital Scale (GT) Thank-you Stickers (<a_20><b_181><c_107>), Round Labels (<a_186>), File Folders (<a_187>), Sharpie Marker (<a_102>), EXPO Markers (<a_178>), Poly Mailers (<a_118>), Glue (<a_242>) (Finds GT via novel <a_20> branch while keeping diverse useful neighbors) Table 6: Diversity and max reward of candidate sets. Method Industrial Office Div( C) MaxReward Div( C) MaxReward Beam search Top-K Ours (VED) 0.7949 0.8089 0. 0.2303 0.2246 0.2475 0.8624 0.8856 0.8852 0.2321 0.2160 0.2473 in C(ğ‘¥), where larger value indicates fewer near-duplicate SIDs and broader coverage across SID branches. To ensure diversity is not obtained by sacrificing strong candidates, we also report the best-in-set reward maxğ‘£ ğ‘…(ğ‘£), averaged over test queries. Three decoding strategies are tested: beam search, top-K, and our VED. Table 6 reports results with C(ğ‘¥) = 64. VED yields the highest (or matched-highest) diversity across datasets while consistently improving the best-in-set reward, indicating reduced SID redundancy and better discovery of high-reward candidates without trading off candidate quality. detailed case study is given in Table 5. Does VED exhibit more favorable scaling property than beam search under training-time decoding budgets? We compare VED with beam search under matched training-time decoding budgets. Specifically, we scale the decoding token budget by the number of model-evaluated tokens during candidate construction (excluding prompt prefill). For ğ¿=3 with beam width ğµ = 16, the 1 budget is 1+2ğµ, and we proportionally expand it to 2 (1+4ğµ), 3 (1+6ğµ), and 4 (1+8ğµ). Similarly, VED is constrained to the same budgets at each scale. Figure 4 shows that VED consistently outperforms beam search across budgets and metrics. On Industrial, VED yields larger gains at low budgets (1 2), whereas widening the beam provides smaller increments, indicating higher compute efficiency from value-guided expansion. On Office, VED maintains clear margin at every scale, and 1 VED often matches or exceeds 3 4 beam search. This suggests that value-guided expansion is more compute-efficient than indiscriminately widening the beam. Overall, VED converts additional training-time decoding compute into larger improvements than likelihood-driven widening by reducing redundant expansions and prioritizing high-value branches."
        },
        {
            "title": "6 Conclusion\nThis paper studies candidate construction for Semantic-ID gen-\nerative recommendation, where probability-driven decoding (e.g.,\nbeam search) tends to over-exploit high-probability sibling branches\nand provides limited reward contrast for RL. We propose V-STAR,\nwhich couples Value-Guided Efficient Decoding (VED) for budgeted\ncandidate construction with Sibling-GRPO for structure-aware pol-\nicy optimization. VED allocates a fixed decoding budget to a small\nset of high-potential branching points, improving reachability and\ncandidate diversity without exhaustive tree search. Sibling-GRPO\nleverages the prefix-tree structure to compute sibling-relative learn-\ning signals, stabilizing optimization under prefix coupling. Exten-\nsive experiments show that V-STAR consistently improvements\nover strong generative baselines under strict token budgets, and\nadditional analyses verify better valueâ€“reward alignment and more\neffective use of extra decoding compute.",
            "content": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation Conference17, July 2017, Washington, DC, USA References [1] Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yanchen Luo, Chong Chen, Fuli Feng, and Qi Tian. 2025. bi-step grounding paradigm for large language models in recommendation systems. ACM Transactions on Recommender Systems 3, 4 (2025), 127. [2] Keqin Bao, Jizhi Zhang, Yang Zhang, Xinyue Huo, Chong Chen, and Fuli Feng. 2024. Decoding matters: Addressing amplification bias and homogeneity issue in recommendations for large language models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). 1054010552. [3] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to Rank: From Pairwise Approach to Listwise Approach. In Proceedings of the International Conference on Machine Learning (ICML). 129136. [4] Ben Chen, Xian Guo, Siyuan Wang, Zihan Liang, Yue Lv, Yufei Ma, Xinlong Xiao, Bowen Xue, Xuxin Zhang, Ying Yang, et al. 2025. Onesearch: preliminary exploration of the unified end-to-end generative framework for e-commerce search. arXiv preprint arXiv:2509.03236 (2025). [5] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed Chi. 2019. Top-k off-policy correction for REINFORCE recommender system. In Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM). 456464. [6] Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, and Tat-Seng Chua. 2024. On softmax direct preference optimization for recommendation. Advances in Neural Information Processing Systems (NeurIPS) 37 (2024), 2746327489. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021). [8] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In Proceedings of the ACM Conference on Recommender Systems (RecSys). 191198. [9] Jiaxin Deng, Shiyao Wang, Kuo Cai, Lejian Ren, Qigen Hu, Weifeng Ding, Qiang Luo, and Guorui Zhou. 2025. Onerec: Unifying retrieve and rank with generative recommender and iterative preference alignment. arXiv preprint arXiv:2502.18965 (2025). [10] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as Language Processing (RLP): Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). In Proceedings of the ACM Conference on Recommender Systems (RecSys). 299315. [11] Kartik Goyal, Graham Neubig, Chris Dyer, and Taylor Berg-Kirkpatrick. 2018. continuous relaxation of beam search for end-to-end training of neural sequence models. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), Vol. 32. 30453052. [12] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015). [13] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The Curious Case of Neural Text Degeneration. In International Conference on Learning Representations (ICLR). [14] Minjie Hong, Zetong Zhou, Zirun Guo, Ziang Zhang, Ruofan Hu, Weinan Gan, Jieming Zhu, and Zhou Zhao. 2025. Generative Reasoning Recommendation via LLMs. arXiv preprint arXiv:2510.20815 (2025). [15] Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley. 2024. Bridging language and items for retrieval and recommendation. arXiv preprint arXiv:2403.03952 (2024). [16] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng, et al. 2019. Reinforcement Learning for Slate-based Recommender Systems: Tractable Decomposition. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI). 23682375. [17] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In IEEE International Conference on Data Mining (ICDM). IEEE, 197206. [18] Levente Kocsis and Csaba SzepesvÃ¡ri. 2006. Bandit based monte-carlo planning. In Proceedings of the European Conference on Machine Learning (ECML). Springer, 282293. [19] Aobo Kong, Wentao Ma, Shiwan Zhao, Yongbin Li, Yuchuan Wu, Ke Wang, Xiaoqian Liu, Qicheng Li, Yong Qin, and Fei Huang. 2025. Sdpo: Segment-level direct preference optimization for social agents. arXiv preprint arXiv:2501.01821 (2025). [20] Xiaoyu Kong, Leheng Sheng, Junfei Tan, Yuxin Chen, Jiancan Wu, An Zhang, Xiang Wang, and Xiangnan He. 2025. Minionerec: An open-source framework for scaling generative recommendation. arXiv preprint arXiv:2510.24431 (2025). [21] Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni. 2023. GPT4Rec: generative framework for personalized recommendation and user interests interpretation. arXiv preprint arXiv:2304.03879 (2023). [22] Shuai Li, Xinyu Luo, Xiang Wang, Yongfeng Zhang, and Tat-Seng Chua. 2023. Large Language Models for Generative Recommendation: Survey and Visionary Discussions. arXiv preprint arXiv:2309.01157 (2023). [23] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, RÃ©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624 (2022), 10921097. [24] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems (NeurIPS) 35 (2022), 2773027744. [25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct Preference Optimization: Your Language Model is Secretly Reward Model. In Advances in Neural Information Processing Systems (NeurIPS). [26] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Tran, Jonah Samost, et al. 2023. Recommender systems with generative retrieval. Advances in Neural Information Processing Systems (NeurIPS) 36 (2023), 1029910315. [27] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, KiantÃ© Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. 2023. Is Reinforcement Learning (Not) for Natural Language Generation?. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). 844862. [28] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. arXiv preprint arXiv:1707.06347 (2017). [29] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). [30] Amit Sharma, Hua Li, Xue Li, and Jian Jiao. 2024. Optimizing novelty of top-k recommendations using large language models and reinforcement learning. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). 56695679. [31] Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, and Wai Lam. 2024. thorough examination of decoding methods in the era of llms. arXiv preprint arXiv:2402.06925 (2024). [32] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. 2017. Mastering the game of go without human knowledge. nature 550, 7676 (2017), 354359. [33] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM). 14411450. [34] Ilya Sutskever, Oriol Vinyals, and Quoc Le. 2014. Sequence to sequence learning with neural networks. Advances in Neural Information Processing Systems (NeurIPS) 27 (2014). [35] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM). 565573. [36] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [37] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In International Conference on Learning Representations (ICLR). [38] Ye Wang, Jiahao Xun, Minjie Hong, Jieming Zhu, Tao Jin, Wang Lin, Haoyuan Li, Linjun Li, Yan Xia, Zhou Zhao, et al. 2024. Eager: Two-stream generative recommender with behavior-semantic collaboration. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). 32453254. [39] Zhipeng Wei, Kuo Cai, Junda She, Jie Chen, Minghao Chen, Yang Zeng, Qiang Luo, Wencong Zeng, Ruiming Tang, Kun Gai, et al. 2025. Oneloc: Geo-aware generative recommender systems for local life service. arXiv preprint arXiv:2508.14646 (2025). [40] Yepeng Weng, Qiao Hu, Xujie Chen, Li Liu, Dianwen Mei, Huishi Qiu, Jiang Tian, and Zhongchao Shi. 2025. Traversal Verification for Speculative Tree Decoding. arXiv preprint arXiv:2505.12398 (2025). [41] Ronald Williams. 1992. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Machine Learning 8 (1992), 229256. [42] Sam Wiseman and Alexander Rush. 2016. Sequence-to-sequence learning as beam-search optimization. arXiv preprint arXiv:1606.02960 (2016). [43] Haibo Xing, Hao Deng, Yucheng Mao, Jinxin Hu, Yi Xu, Hao Zhang, Jiahao Wang, Shizhun Wang, Yu Zhang, Xiaoyi Zeng, et al. 2025. Reg4rec: Reasoningenhanced generative model for large-scale recommendation systems. arXiv preprint arXiv:2508.15308 (2025). [44] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Conference17, July 2017, Washington, DC, USA Trovato et al. Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115 (2024). [45] Weiqin Yang, Bohao Wang, Zhenxiang Xu, Jiawei Chen, Shengjia Zhang, Jingbang Chen, Canghong Jin, and Can Wang. 2026. BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models. arXiv preprint arXiv:2601.22925 (2026). https://arxiv.org/abs/2601.22925 Preprint. [46] Yuhao Yang, Zhi Ji, Zhaopeng Li, Yi Li, Zhonglin Mo, Yue Ding, Kai Chen, Zijian Zhang, Jie Li, Shuanglong Li, et al. 2025. Sparse meets dense: Unified generative recommendations with cascaded sparse-dense representations. arXiv preprint arXiv:2503.02453 (2025). [47] Shunyu Yao, Dian Yu, Jeffrey Zhao, Tom Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems (NeurIPS) 36 (2023), 1180911822. [48] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. 2021. SoundStream: An End-to-End Neural Audio Codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing 30 (2021), 495507. [49] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Jiayuan He, et al. 2024. Actions Speak Louder than Words: Trillion-parameter Sequential Transducers for Generative Recommendations. In Proceedings of the International Conference on Machine Learning (ICML). 5848458509. [50] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, et al. 2024. Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations. arXiv preprint arXiv:2402.17152 (2024). [51] Bowen Zheng, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, Ming Chen, and Ji-Rong Wen. 2024. Adapting large language models by integrating collaborative semantics for recommendation. In IEEE International Conference on Data Engineering (ICDE). 14351448. [52] Guorui Zhou, Hengrui Hu, Hongtao Cheng, Huanjie Wang, Jiaxin Deng, Jinghao Zhang, Kuo Cai, Lejian Ren, Lu Ren, Liao Yu, et al. 2025. Onerec-v2 technical report. arXiv preprint arXiv:2508.20900 (2025)."
        }
    ],
    "affiliations": [
        "Tencent Inc., China"
    ]
}