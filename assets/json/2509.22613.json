{
    "paper_title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective",
    "authors": [
        "Siwei Wang",
        "Yifei Shen",
        "Haoran Sun",
        "Shi Feng",
        "Shang-Hua Teng",
        "Li Dong",
        "Yaru Hao",
        "Wei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 3 1 6 2 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "BENEFITS AND PITFALLS OF REINFORCEMENT LEARNING FOR LANGUAGE MODEL PLANNING: THEORETICAL PERSPECTIVE Siwei Wang1, Yifei Shen1, Haoran Sun2, Shi Feng3, Shang-Hua Teng4, Li Dong1, Yaru Hao1, Wei Chen1 1Microsoft Research Asia, 2Peking University, 3Harvard University, 4University of Southern California"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RLs benefits and limitations through tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring explorations role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice."
        },
        {
            "title": "INTRODUCTION",
            "content": "Planning is fundamental cognitive construct that underpins human intelligence, shaping our ability to organize tasks, coordinate activities, and formulate complex solutions such as mathematical proofs. It enables humans to decompose complex goals into manageable steps, anticipate potential challenges, and maintain coherence during problem solving. Similarly, planning plays pivotal role in state-of-the-art Large Language Models (LLMs), enhancing their ability to address structured and long-horizon tasks with greater accuracy and reliability. Early generations of LLMs primarily relied on next-token prediction and passive statistical learning, which limited their planning capabilities to short-horizon, reactive responses. The o1 family of models represents major advance in planning by incorporating reinforcement learning (RL) objectives that reward accurate, multi-step reasoning and penalize errors. Inspired by the success of o1, RL has been applied to enhance planning capabilities in various settings, including task decomposition for tool use (Wu et al., 2024a; Luo et al., 2025) and gaming (Yang et al., 2024), visual-language spatial navigation (Chu et al., 2025), and long-horizon robotics tasks (Dalal et al., 2024). These approaches have demonstrated significantly better performance than their supervised fine-tuning (SFT) counterparts. For more related works, please refer to Appendix B. Despite recent successes, the theoretical basis underlying RLs advantage over SFT in planning tasks and the limitations of current RL methods remain to be established. To enable tractable analysis of the gradient dynamics, we adopt the data generation model from (Wang et al., 2024b). Within their framework, planning is abstracted as path-finding problem over graph structure. For example, tool-use scenario can be modeled as identifying valid call sequence within an API call graph (Wu et al., 2024b). denotes equal contribution. Corresponding author (weic@microsoft.com)."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "To capture the fundamental limitations of SFT in planning, we begin by presenting structural characterization of its stable point for path planning (Section 3). Our analyses, expanding the observation of Wang et al. (2024b) that transformer-based LLM architectures cannot identify reachability relationships through transitivity in SFT, show that it introduces co-occurrence-based spurious solutions into planning tasks. This characterization provides basis for comparison with and motivation for using the RL-based learning approach in language model planning. Focusing on the behaviors of RL-based learning dynamics, we first consider policy gradient (PG), widely adopted algorithm for tuning large language models (Section 4). Our analysis yields three key findings. First, with only 0-1 outcome rewards, each iteration of PG equivalently corresponds to an SFT process on the exploration data; however, PG empirically outperforms SFT due to the exploration-driven data augmentation it enables. Second, although PG converges to model that outputs correct paths for all sourcetarget pairs seen during training, we uncover diversity collapse phenomenon: the models output diversity steadily declines throughout training and continues to diminish even after achieving 100% training accuracy. Third, we show that KL regularization acts as an explicit diversity-preserving term, but at the expense of accuracy. We then analyze Q-learning, paradigm well known in game playing but rarely applied to LLMs (Mnih et al., 2013) (Section 5). Our analysis yields two key findings. First, when trained with only an outcome reward signal, Q-learning suffers from reward hacking; however, incorporating process rewards eliminates this issue. Second, once this issue is addressed, Q-learning offers two theoretical advantages over PG: it converges to solution that preserves output diversity when achieving optimal training accuracy, and it naturally supports off-policy learning. The latter is particularly important in practice, since rollouts performed with quantized model or large batch sizes are effectively off-policy, as exemplified by the VeRL framework (Sheng et al., 2024). Finally, we validate all these theoretical findings through experiments. To summarize, our main contribution is theoretical treatment of the impact of reinforcement learning on language model planning. Our mathematical analysis of learning dynamics sheds light on phenomena observed in practicefor example, SFT tends to memorize while RL promotes generalization; PG methods often suffer from diversity collapse; and KL regularization helps mitigate diversity degradation, albeit at the cost of reduced accuracy. Other findings point to promising future directions, such as leveraging Q-learning to achieve both diversity and accuracy, as well as enabling off-policy learning. Taken together, these results provide principled foundation for understanding and advancing reinforcement learning methods in language model planning."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "2.1 PATH PLANNING DATASET: SYNTAX AND DATA SOURCES Following (Wang et al., 2024b), we abstract planning in large language models as path planning over an unknown directed graph = (V, E), where represents the set of nodes and represents the set of edges. Each node is represented by unique token. The language models vocabulary consists of these node tokens and special end-of-sequence token, n. An edge (u, v) signifies directed connection from node to node v. node is reachable from node if directed path from to exists in G. We denote by {0, 1}VV the adjacency matrix of G, where A[u, v] = 1 if and only if (u, v) E, and by {0, 1}VV the reachability matrix, where R[t, s] = 1 if and only if is reachable from s. The set of all reachable source-target pairs (s, t) is partitioned into training set DTrain and test set DTest. We define three corresponding data stages: SFT Training Data: We construct training dataset DSFT for supervised fine-tuning by sampling multiple (K) paths for each reachable pair (s, t) DTrain by random walk. Each training data in DSFT is sequence in the format s t n, where c are tokens for nodes in valid path from to t, and indicates the end of the sequence. We call the model after SFT training the base model. RL Training Data: We sample pairs (s, t) from DTrain and let the model itself (on-policy) or the base model (off-policy) generate the remaining tokens in the sequence. When the model outputs"
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "n or the generation reaches the maximum length, an outcome reward or some step rewards will be given, depending on the used reward format. Test Data: When testing, we provide pairs (s, t) from DTest, which are never encountered in either SFT or RL training. The model is tasked with generating valid path from to t. Throughout the empirical study, we use one-layer, single-head Transformer as the backbone model. The embedding size is set to = 120. The graph in our main empirical validation is generated using the Erdos-Renyi model with = 100 nodes and an edge probability of 0.15. The ratio of the sets DTrain/DTest is approximately 0.25 (approximately 20% pairs are in DTrain). The number of paths sampled for each reachable pair in DTrain is = 10. We also consider the graph GBW that characterizes the transition between different block configurations in Blocksworld, which is proposed by Valmeekam et al. (2023a) to evaluate the LLMs planning ability. The details for the graph construction are presented in Appendix G."
        },
        {
            "title": "2.2 REINFORCEMENT LEARNING ALGORITHMS",
            "content": "We first define the notation. Given vector x, we denote its m-th element by x[m]. For given sequence s t n, we represent it as = (u1, u2, , um, ). We denote by ˆum the output probability vector of the current model at the m-th position, and by ˆubase that of the base model before RL. The model parameters are denoted by θ. Policy Gradient. Let be the set of valid paths. The outcome reward is only given at the end of the path and is defined by R(u) = δuP + p, where > 0 and are constants, and δ denotes the indicator function that is 1 if condition is true and 0 otherwise. For an individual trajectory, the loss function is ℓ = (cid:32) (cid:88) R(u) log ˆum[um+1] (cid:124) (cid:123)(cid:122) (cid:125) Policy Gradient +λ log ˆum[um+1] (cid:26) log (cid:124) (cid:123)(cid:122) KL Divergence ˆum[um+1] ˆubase [um+1] (cid:27) (cid:33) , (cid:125) (1) where λ controls the KL regularization strength, and {} means the term is detached and will not contribute to the gradient. Q-Learning. The goal of Q-learning is to approximate the Q-function with the model logits. Let Qθ(sm, am) be the Q-function where sm = (u1, u2, , um) is the state, am is the action, and (cid:0)Qθ(sm, am) [R(sm, am) + = (u1, u2, , um, am) is their next state. The objective is (cid:80) . We denote the logits at step by um. For an individual trajectory, the loss maxa is given by m)](cid:1)2 Qθ(s m, ℓ = (cid:18) (cid:88) m3 um[um+1] R(u, m) (cid:26) max um+1[k] (cid:27)(cid:19)2 . (2) For Q-learnings reward R(u, m), we study two scenarios: (i) outcome reward, where the reward depends on whether the path is correct, and (ii) process reward, where intermediate rewards are given based on adjacency and target checks. Specifically, R(u, m) = δuP δum+1=u2 , If outcome reward, δum+1=u2 (cid:124) (cid:125) (cid:123)(cid:122) Target check δ(um,um+1)E (cid:125) (cid:123)(cid:122) (cid:124) Adjacency check , If process reward. (3) That is, in the outcome reward setting, reward of 1 is given only if the entire path is valid, and it is assigned at the step when the target is reached. In contrast, in the process reward setting, we do not check whether the entire path is valid or not. The model is always rewarded upon reaching the target, but it is also penalized at any step that transitions to non-adjacent node."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "(a) Edge Frequency (b) SFT (c) SFT + PG (d) SFT + Q-Learning Figure 1: Frequency of edge occurrences in the SFT training data DSFT and the adjacency structures learned by different models. The underlying graph represents transitions between block configurations in Blocksworld (Valmeekam et al., 2023a)."
        },
        {
            "title": "3 LIMITATIONS OF SUPERVISED FINE-TUNING IN PLANNING",
            "content": "Focusing on the stationarity of the training dynamics, we present basic structural characterization that captures fundamental limitation of SFT in planning. Our analysis builds on an early finding of Wang et al. (2024b), which showed that transformer-based SFT planning approaches lack transitivity-learning mechanisms needed to obtain complete reachability structures. The new characterization expands and complements the earlier results and provides theoretical explanation for why SFT-based planning tends to rely on memorization. More importantly, this result establishes theoretical basis for comparison with RL-based planning frameworks and highlights the role of exploration in achieving better generalization during the adaptive learning process. 3.1 DISCUSSIONS ON EXISTING FINDINGS To set up our characterization, we first review the analysis framework of Wang et al. (2024b), which examines the training dynamics of one-layer, single-head Transformer under an autoregressive loss function. Their analysis shows that, during training, the model encodes both the adjacency and reachability structures of the underlying graph in its learnable parameters. The model then predicts the next node in sequence by ensuring that it is adjacent to the current node and lies along path toward the target node. full description of their approach is given in Algorithm 1 in Appendix C. Wang et al. (2024b) showed, both theoretically and experimentally, that the adjacency and reachability information stored in models weights is generally incomplete. To formalize this, consider training dataset DSFT. The observed adjacency matrix Aobs(DSFT) contains exactly those edges (j, k) that appear in at least one path from DSFT. Similarly, the observed reachability matrix Robs(DSFT) records that target node is reachable from an intermediate node if DSFT contains sequence with target in which occurs as non-source node. We refer to such pairs (t, k) as observed reachable pairs. However, we find that even when an adjacency relation appears in DSFT, the SFT model may not learn high weight for it. To illustrate this, we run experiments on the Blockworld dataset, and the results are presented in Figure 1. In Figure 1a, we show the frequency of all adjacency relationships in the training set (every adjacency relationship appears at least once), where brighter regions indicate higher frequencies. Then Figure 1b displays the corresponding weights learned after SFT. By comparing them, we observe that some adjacency relationships present in the data are not well captured by the model, especially those with low frequency. This observation motivates us to further investigate the models stable (optimal) points. 3.2 CHARACTERIZATION OF THE STABLE POINT IN SFT-BASED LEARNING DYNAMICS Building on the observation of Wang et al. (2024b) that next-node prediction depends mainly on the current and target nodes, we adopt the following natural assumption about model expressiveness for our structural characterization. Recall that u2 and um denote the target node and the current node at position m, respectively."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "Assumption 3.1. The models predicted logits for the next token can be expressed as function of the (target, current) node pair, i.e., there exists function such that the logits um = (u2, um). Note that in the assumption, can be an arbitrary function. We now characterize the structure of the stable point achieved by SFT. Due to space limitations, we defer all the proofs in this paper to the appendix. Theorem 3.1 (Optimal Solution of SFT). Assume Assumption 3.1 holds. Let Nu2,um,k denote the number of occurrences in the training dataset where the target node is u2, the current node is um, and the next node is k. The optimal solution of SFT satisfies: exp(f (u2, um)[k]) exp(f (u2, um)[k]) (cid:80) = Nu2,um,k Nu2,um,k (cid:80) (cid:88) if Nu2,um,k > 0. If (cid:80) Nu2,um,k = 0, output can be any probability distribution. Takeaway 1: SFT memorizes co-occurrence relationships in the training dataset. Theorem 3.1 extends the findings of Wang et al. (2024b), which showed that SFT-based mechanisms may fail to learn the complete adjacency and reachability matrices, leading to spurious correlations. However, those earlier results did not specify the nature of the solutions to which the model converges. Complementing their work, Theorem 3.1 clarifies this by showing that SFT essentially memorizes co-occurrence relationships among the target node, the current node, and the immediate next node based on their frequencies in DSFT. Hence, SFT will fail to exploit transitivity information (which never appears in DSFT) to capture the true graph connectivity required for path planning. In Figure 1, we further compare the weights of models trained by two RL approaches, PG and Q-learning. Both RL approaches capture the adjacency relationships better. Similar findings are reported by Chu et al. (2025), who empirically observe that SFT tends to memorize while RL exhibits better generalization. Our structural analysis in Theorem 3.1 provides theoretical explanation for the first part of this phenomenon, namely, why SFT memorizes. In the following sections, we examine the two RL-based approaches, PG and Q-learning, and provide theoretical explanation of the second part, i.e., why RL generalizes."
        },
        {
            "title": "4 PATH PLANNING CAPACITIES OF POLICY GRADIENT",
            "content": "In this section, we examine the path-planning capacity of the policy gradient, the core principle behind advanced RL algorithms such as PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024). Understanding the strengths and limitations of the basic policy gradient provides theoretical insights into its behavior, highlights the mechanisms that enable effective path planning, and clarifies the challenges that motivate more sophisticated approaches. 4.1 THEORETICAL ANALYSIS We first establish the connection between policy gradient (PG) and supervised fine-tuning (SFT), highlighting the potential advantages of PG over SFT. We then analyze PGs training dynamics and show that, without KL regularization, the model can achieve 100% training accuracy (under temperature sampling) while progressively losing output diversity. Finally, we demonstrate that, when initialized with reasonably capable base model, adding KL regularization helps preserve diversity and thereby enhances generalization, albeit sometimes at the cost of accuracy. To make this connection precise, we show that the PG loss function closely resembles the SFT loss, restricted to the subset of data generated during RL training that corresponds to correct paths. Theorem 4.1 (Connections between PG and SFT). Assume Assumption 3.1 holds. Let DRL,t denote the set of data generated during the RL training step t. When = 1, = 0 (i.e., reward 1 for correct path and reward 0 otherwise) and λ = 0 (i.e., without KL regularization), the loss function of Policy Gradient is the same as the loss function of using SFT only on correct paths in DRL,t. As shown by Wang et al. (2024b), SFT can learn the adjacency and reachability relations. Thus, Theorem 4.1 shows that PG can capture these relations presented in the dataset (T t=1DRL,t) P."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "However, unlike SFT, which relies on fixed training dataset, PG generates data on-policy during training. As the model improves, it can explore and discover new correct paths that were absent from the initial training set. This exploration-driven data augmentation enables PG to achieve stronger performance beyond what SFT alone can provide. Takeaway 2: PG outperforms SFT primarily because its iterative data generation process encourages exploration and effectively expands the training dataset. Building on the loss function, we analyze the gradient and identify two distinctive properties of on-policy PG updates. Theorem 4.2 (Convergence of PG without KL regularization). Assume Assumption 3.1 holds. For any i, pair, let C(i, j) denote the set of nodes that can reach and are adjacent to j. The following then holds: If = 1, = 0 and λ = 0, then (i) the gradient (i,j)[k] for / C(i, j) is always positive, and (ii) the total sum of gradient (cid:80) (i,j)[k] = 0. ℓ ℓ Theorem 4.2 shows that the logits (i, j)[k] corresponding to incorrect tuples (i, j, k), i.e., cases where node cannot reach node through node k, will continue to decrease, while some other logits will not converge to . Consequently, under gradient descent, the probability that the model outputs wrong path in DTrain converges to zero. Next, we analyze how the models output diversity evolves. Intuitively, the most diverse model that still achieves 100% accuracy is one that produces uniform distribution over C(i, j) for each target node and current node j. We now analyze the evolution of the KL divergence between this uniform distribution and the models output distribution during PG training without KL regularization. Theorem 4.3 (Diversity Collapse of PG without KL regularization). Assume Assumption 3.1 holds. Let UC(i,j) denote the uniform probability distribution on support C(i, j). When = 1, = 0 and λ = 0, and logits t(i, j)[k] for / C(i, j) is , where t(i, j) denotes the logits value of (i, j) at time step t. For any such PG gradient descent step t, we have that KL(UC(i,j)softmax(f t(i, j)) E[KL(UC(i,j)softmax(f t+1(i, j))]. Note that the metric KL(UC(i,j)softmax(f t(i, j))) takes minimum value when softmax(f t(i, j)) is also the uniform distribution on C(i, j), and takes maximum value when softmax(f t(i, j)) is one-hot vector. Thus, Theorem 4.3 demonstrates that even after attaining 100% accuracy on DTrain, the model continues to exhibit declining output diversity. Takeaway 3: In the absence of KL divergence, output diversity continuously declines. This diversity-collapse phenomenon has been reported in the literature (Cui et al., 2025) and can impair models ability to generalize. To address it, many techniques have been proposed, the most common being KL regularization. To better understand its role, we analyze the stable point of the model under KL regularization, highlighting both its advantages and limitations. Theorem 4.4 (The effect of KL regularization). When = 1, = 0 and λ > 0, the stable point of the PG model satisfies the following, under Assumption 3.1: For any fixed i, j, either q(i, j)[k] = 0 or q(i, j)[k] qbase(i, j)[k] exp(p(i, j)[k]/λ). Here q(i, j)[k] is the probability of outcome in softmax(f (i, j)), qbase(i, j)[k] is the probability of outcome in the base model, and p(i, j)[k] is the probability of tuple i, j, belonging to valid path given output probability {q(i, j)[k]}i,j,k. This result shows that KL regularization constrains the trained model to remain close to the base model, thereby preserving some of its diversity. This effect is double-edged sword. Consider valid next node for which the base model assigns low probability, i.e., qbase(i, j)[k] is small. On the one hand, KL regularization prevents q(i, j)[k] from becoming arbitrarily small, increasing the chance of generating valid paths involving k. On the other hand, it also prevents q(i, j)[k] from becoming very large, limiting potential gains when the base models prior is suboptimal. This tradeoff explains seemingly contradictory findings in recent literature: when the base model is already capable, KL regularization preserves diversity and improves generalization, but when the base model is weak, the regularization may hinder learning by overly constraining policy updates. Takeaway 4: KL regularization explicitly acts as diversity-preserving mechanism, provided that the base model is reasonably capable, but this comes at the cost of reduced train accuracy."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "(a) Test Accuracy (b) Train Accuracy (c) Output Diversity (d) Influence of KL Figure 2: Empirical results of PG training. Both PG and continual SFT are initialized from the same base model. Figures (a)-(c) illustrate the training dynamics of test accuracy (under greedy decoding), training accuracy (under temperature sampling), and response diversity (under temperature sampling). Figure (d) shows how different KL regularization strengths affect the final models."
        },
        {
            "title": "4.2 EMPIRICAL VALIDATIONS",
            "content": "The results are presented in Figure 2, where we compare PG with different KL regularization factor λ against continual SFT. All models are initialized from the same base model after SFT training, while continual SFT means training the model for more time steps on the same SFT dataset DSFT. The empirical results match the takeaways we summarized from our theoretical findings, as detailed below. Takeaway 2: In Figure 2a, as the training progresses, the test accuracy of Continual SFT constantly decreases, while all the PG methods can achieve an improvement, since they benefit from exploration-driven training data. Takeaway 3: In Figure 2b and 2c, we can see that PG without KL regularization progressively achieves and maintains 100% training accuracy, but its output diversity, i.e., the average number of distinct correct paths generated over 100 sampling trials for the same source-target pair, keeps decreasing during training. In the end, the model eventually produces only one path per pair. Moreover, as shown in Figure 2a, when the diversity diminishes, continued training degrades test accuracy. Takeaway 4: As comparison, PG with KL regularization maintains high output diversity in the end, but their training accuracy is limited. This trade-off is further stated in Figure 2d: with higher factor λ, the model can have higher output diversity and lower training accuracy. Along with Figure 2a, it is shown that KL regularization prevents the model from deviating too far from the base model in terms of both diversity and training accuracy. This mitigates overfitting but also caps potential gains in test accuracy."
        },
        {
            "title": "5 ANALYSIS OF THE Q-LEARNING-BASED PLANNING MECHANISM",
            "content": "In this section, we analyze the Q-learning mechanism for language-model planning under two different reward designs. We show that stepwise process rewards enable convergence, preserve diversity, and remain valid under off-policy sampling, whereas outcome rewards collapse to trivial solutions. Our analysis begins under Assumption 3.1 for both reward types, and we then extend the processreward analysis to more concrete linear Transformer model without this assumption. 5.1 THEORETICAL ANALYSIS To analyze the structure and convergence of the Q-learning stable point, we introduce mild assumption, which we call the persistent exploration assumption about the RL-based learning dynamics. Assumption 5.1 (Persistent exploration). At training step t, let it = u2, jt = um, kt, respectively, denote the target, current, and next nodes. We assume for every (i, j, k), prop i,j,k > 0 such that lim inf 1 T 1 (cid:88) t=0 δ(it,jt,kt)=(i,j,k) prop i,j,k . Under the persistent exploration assumption, every coordinate is updated frequently enough to allow convergence analysis. In practice, this assumption is usually satisfied, for instance:"
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "Lemma 5.1. Training with ϵ-exploration (i.e., exploring each alternative action with probability proportional to ϵ) satisfies the persistent exploration assumption. With the outcome reward, the signal merely verifies whether the entire sequence constitutes valid path ending at target i. It does not differentiate between current nodes or candidate next nodes when = i. As result, at stable point, all logits collapse to the same constant ci for each fixed target i, causing the parameters to lose structural information, as stated in the theorem below. Theorem 5.1 (Stable points of outcome reward). Assume the RL-training uses the outcome reward R(u, m) = δuP δum+1=u2 , and stable point exists under persistent exploration (Assumption 3.1). Then, at any stable point of the Q-learning model, for each fixed target and = i, all logits (i, j)[k] take the same value depending only on i. With the process reward, the update rule accounts for both adjacency and target conditions. The next theorem establishes that the process converges to well-defined limits that capture the underlying graph structure. Theorem 5.2 (Stable points of process reward). Assume Assumption 3.1 holds and the process reward is used, i.e. R(u, m) = δum+1=u2 δ(um,um+1) /E . Suppose the score vector (i, j) Rn is initialized at zero and updated under the persistent exploration assumption with learning rate η. Then, in the Q-learning model, as , (t)(i, j)[i] A[j, i], and for = i, (t)(i, j)[k] A[j, k] = 1 and R[i, k] = 1, exactly one of (A[j, k] = 1) or (R[i, k] = 1), 1, 0, 1, A[j, k] = 0 and R[i, k] = 0. Here denotes convergence or tend to. Moreover, the convergence is linear; the effective rate depends on η and the update proportions prop i,j,k . Takeaway 5: Different from PG methods, in Q-learning, relying solely on the outcome reward signal can cause reward hacking, whereas introducing process rewards mitigates this issue. To gain further insight in setting closer to practice, we analyze simplified but concrete one-layer, single-head linear Transformer without the abstraction of Assumption 3.1. Assumption 5.2 (Linear transformer Wang et al. (2024b)). We work under the simplified Transformer setting in Wang et al. (2024b): (1) The token embedding matrix and the output weight matrix are both set to the identity; (2) Attention is fixed entirely on the target node u2, so the attention block contributes only the value lookup WV [u2, ]; (3) All layer normalizations are removed, and the feedforward block is replaced by linear map of the form FFN(X) = XWM . Under these assumptions, the logit decomposes as um+1[k] = WM [um, k] + WV [u2, k], where WM arises from the feed-forward weights and WV from the value matrix of the attention block. Despite this simplification, the analyzed results remain consistent with the experiments of real Transformers, as demonstrated in Wang et al. (2024b). This formulation aligns with the actual 1-layer 1-head Transformer architecture, offering greater practical utility compared to the abstract function (i, j)[k]. The subsequent result characterizes the set of stable points under this decomposition and is consistent with the structural limits established in Theorem 5.2. Theorem 5.3 (Stable points of process reward). Assume Assumption 5.2 holds. For linear transformer, assume training uses the process reward, and the persistent exploration condition holds. At stable point of the Q-learning model, for each there exists ck such that WM [j, k] = A[j, k] 1 + ck, WV [i, k] = R[i, k] ck. Conversely, any such (WM , WV ) is stable point. Hence, the set of stable points is {(WM , WV ) : ck R, [V]}. Theorem 5.1 shows that if only outcome reward is used, the learned logits collapse to constant across all states for given target. In contrast, Theorem 5.2 and Theorem 5.3 show that with persistent exploration, process rewards can preserve adjacency and reachability (note that in Theorem 5.3, the constant ck is immaterial in terms of path planning, since for any stable point, um+1[k] = WM [um, k] + WV [u2, k] = A[j, k] + R[i, k] 1, which is the same as Theorem 5.2)."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "(a) Train Accuracy and Test Accuracy of Q-Learning (b) Output Diversity vs. Accuracy Figure 3: Empirical comparison between Q-learning and PG. Figure (a) shows the training dynamics of training and test accuracy (under greedy decoding). Figure (b) compares the Pareto frontiers of output diversity and accuracy on the training and test sets (under temperature decoding). (a) Epoch 10000 (b) Epoch 30000 (c) Epoch (d) Epoch 300000 Figure 4: Heatmap of normalized logits from the Q-learning model with process reward. For each row , green blocks indicate valid next nodes given the current node 0 and target node i. The logits corresponding to these valid actions consistently increase during training. Moreover, the convergence holds even under off-policy sampling, and action diversity is preserved because all feasible next nodes converge to the logit value 1. Takeaway 6: Compared to PG methods, Q-learning can operate off-policy and better maintains output diversity. 5.2 EMPIRICAL VALIDATIONS We first examine the training and test accuracy results in Figure 3a, where we compare Q-learning under different reward designs and sampling policies. All models are initialized from the same base model. Figure 3b states the diversity-accuracy trade-off of Q-learning models, policy gradient models, and the continual SFT model (under different temperatures). Figure 4 illustrates the logits of an on-policy Q-learning model with process rewards and fixed attention (attention fixed on the target node u2). The model is initialized from the same base model as in Figure 3, and is further trained with reinforcement steps on all pairs (s, t) DSFT where [20]. In each row i, we plot the logits for nodes 020 (normalized to [0, 1]) when the current node is 0 and the target node is i. White indicates larger logits, black indicates smaller logits, and green frames highlight nodes that are both children of node 0 and ancestors of i, corresponding to valid outputs. The empirical results are consistent with the takeaways introduced above, as detailed below. Takeaway 5: In Figure 3a, Q-learning with process rewards achieves comparable training accuracy and significantly better test accuracy than the PG model, while Q-learning with outcome rewards collapses and converges to near-zero accuracy on both training and test sets. Examining each row of Figure 4, we observe that the logits of feasible nodes gradually increase and converge to the largest values within their respective rows, which aligns with Theorem 5.2 and 5.3 and confirms that process rewards enable the model to recover the correct graph structure. Takeaway 6: In Figure 3a, offpolicy Q-learning with process rewards attains training and test accuracy comparable to on-policy Q-learning with process rewards, demonstrating that Q-learning can operate off-policy. Finally, Figure 3b further highlights that the Q-learning process rewards preserve output diversity. Figure 4 also reflects this phenomenon: within each row, the logits of feasible nodes become increasingly close to one another (approaching white) over time, indicating convergence to diverse but correct transitions."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we analyze the benefits and limitations of reinforcement learning in language model planning through the lens of learning dynamics. Our theoretical analysis shows that supervised fine-tuning introduces spurious co-occurrence solutions, while policy gradient and Q-learning outperform SFT primarily through exploration. We further identify critical drawback of basic policy gradientdiversity collapseand show that Q-learning mitigates this issue while supporting off-policy learning. These insights clarify the mechanisms behind the recent success of RL-based approaches and highlight principled research directions, such as leveraging Q-learning for robust, scalable, and generalizable planning in language models."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We attach all the source code necessary to reproduce our experimental results in the supplementary materials. As for the theoretical results, complete proofs of all the theorems are provided in Appendix C, D, and for full transparency and verification."
        },
        {
            "title": "REFERENCES",
            "content": "Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, and Yang Yang. Graphllm: Boosting graph reasoning ability of large language model. arXiv preprint arXiv:2310.05845, 2023. Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. Graphwiz: An instruction-following language model for graph computational problems. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 353364, 2024a. Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, and Zhangyang Wang. Llaga: Large language and graph assistant. arXiv preprint arXiv:2402.08170, 2024b. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Andrew Cohen, Andrey Gromov, Kaiyu Yang, and Yuandong Tian. Spectral journey: How transformers predict the shortest path. arXiv preprint arXiv:2502.08794, 2025. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Xinnan Dai, Haohao Qu, Yifen Shen, Bohang Zhang, Qihao Wen, Wenqi Fan, Dongsheng Li, Jiliang Tang, and Caihua Shan. How do large language models understand graph patterns? benchmark for graph pattern comprehension. arXiv preprint arXiv:2410.05298, 2024. Xinnan Dai, Kai Yang, Jay Revolinsky, Kai Guo, Aoran Wang, Bohang Zhang, and Jiliang Tang. From sequence to structure: Uncovering substructure reasoning in transformers. arXiv preprint arXiv:2507.10435, 2025. Murtaza Dalal, Tarun Chiruvolu, Devendra Chaplot, and Ruslan Salakhutdinov. learn: Language model guided rl for solving long horizon robotics tasks. arXiv:2405.01534, 2024. Plan-seqarXiv preprint Artur Back De Luca and Kimon Fountoulakis. Simulation of graph algorithms with looped transformers. arXiv preprint arXiv:2402.01107, 2024. Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. arXiv preprint arXiv:2305.15066, 2023. Xiaojun Guo, Ang Li, Yifei Wang, Stefanie Jegelka, and Yisen Wang. G1: Teaching llms to reason on graphs with reinforcement learning. arXiv preprint arXiv:2505.18499, 2025. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Paul Saldyt, and Anil Murthy. Position: LLMs cant plan, but can help planning in LLM-modulo frameworks. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=Th8JPEmH4z. Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna Qiu, and Yuqing Yang. Agent lightning: Train any ai agents with reinforcement learning. arXiv preprint arXiv:2508.03680, 2025."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi Jiang, and Xing Xie. Graphinstruct: Empowering large language models with graph understanding and reasoning capability. arXiv preprint arXiv:2403.04483, 2024. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Ida Momennejad, Hosein Hasanbeig, Felipe Vieira Frujeri, Hiteshi Sharma, Nebojsa Jojic, Hamid Palangi, Robert Ness, and Jonathan Larson. Evaluating cognitive maps and planning in large language models with CogEval. Advances in Neural Information Processing Systems, 36, 2023. Nanda Neel, Chan Lawrence, Lieberum Tom, Smith Jess, and Steinhardt Jacob. Progress measures for grokking via mechanistic interpretability. In International Conference on Learning Representations, 2023. Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, and Jonathan Halcrow. Let your graph do the talking: Encoding structured data for llms. arXiv preprint arXiv:2402.05862, 2024. Clayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow, Bryan Perozzi, and Vahab Mirrokni. Understanding transformer reasoning capabilities via graph algorithms. Advances in Neural Information Processing Systems, 37:7832078370, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Amrith Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar. Scaling test-time compute without verification or rl is suboptimal. arXiv preprint arXiv:2502.12118, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving AI tasks with ChatGPT and its friends in Huggingface. Advances in Neural Information Processing Systems, 36, 2023. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving Olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Planbench: An extensible benchmark for evaluating large language Kambhampati. models on planning and reasoning about change. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 3897538987. Curran Associates, URL Inc., 2023a. https://proceedings.neurips.cc/paper_files/paper/2023/file/ 7a92bcdede88c7afd108072faf5485c8-Paper-Datasets_and_Benchmarks. pdf. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36, 2023b. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language? Advances in Neural Information Processing Systems, 36, 2023b. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):126, 2024a. Siwei Wang, Yifei Shen, Shi Feng, Haoran Sun, Shang-Hua Teng, and Wei Chen. Alpine: Unveiling the planning capability of autoregressive learning in language models. Advances in neural information processing systems, 37:119662119688, 2024b. Qinzhuo Wu, Wei Liu, Jian Luan, and Bin Wang. Toolplanner: tool augmented llm for multi granularity instructions with path planning and feedback. arXiv preprint arXiv:2409.14826, 2024a. Xixi Wu, Yifei Shen, Caihua Shan, Kaitao Song, Siwei Wang, Bohang Zhang, Jiarui Feng, Hong Cheng, Wei Chen, Yun Xiong, et al. Can graph learning improve planning in llm-based agents? Advances in Neural Information Processing Systems, 37:53385383, 2024b. Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Haoran Tan, Chencheng Jiang, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmental feedback. In European conference on computer vision, pp. 2038. Springer, 2024. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, et al. The landscape of agentic reinforcement learning for llms: survey. arXiv preprint arXiv:2509.02547, 2025a. Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025b. Hanlin Zhu, Baihe Huang, Shaolun Zhang, Michael Jordan, Jiantao Jiao, Yuandong Tian, and Stuart Russell. Towards theoretical understanding of thereversal cursevia training dynamics. Advances in Neural Information Processing Systems, 37:9047390513, 2024."
        },
        {
            "title": "A THE USE OF LARGE LANGUAGE MODELS",
            "content": "In this paper, the core conceptual framework and its iterative development were driven by human researchers. LLMs served strictly in supporting capacity, primarily employed for linguistic refinement of the manuscript to enhance readability while preserving original technical content. The whole paper is carefully supervised, reviewed, and modified by the authors who maintain complete responsibility for the scientific validity, technical accuracy, and ethical integrity of this work."
        },
        {
            "title": "B MORE RELATED WORKS",
            "content": "B.1 PLANNING OF LLMS Planning is fundamental component of human intelligence and autonomous agents. Several studies have evaluated the planning capabilities of LLMs trained without reinforcement learning, such as CogEval (Momennejad et al., 2023) and Blockworlds (Valmeekam et al., 2023b). These works consistently report negative results, suggesting that LLMs lack inherent planning abilities. In contrast, models such as o1 show the ability to solve such problems, though the underlying mechanisms remain unclear. On the other hand, LLM-based agents have demonstrated remarkable competence in performing real-world planning tasks, even without RL training (Wang et al., 2024a). Many of these planning tasks can be naturally abstracted as path planning problems on graph. For example, in tool-augmented agents (Shen et al., 2023), tool dependencies can be modeled as graph where nodes represent tools and edges represent dependency relations (Wu et al., 2024b). Planning, in this context, involves finding path of tools to fulfill the users request. Similarly, in mathematical reasoning agents (Trinh et al., 2024), theorem dependencies form graph where constructing proof is equivalent to finding path. In game-playing agents such as Voyager (Wang et al., 2023a), skill dependencies create graph structure where planning determines the sequence of skills needed to accomplish tasks. These observations motivate our abstraction of planning as path planning problem in this work. Agents trained without RL face two key challenges: (1) supervised fine-tuning loss is misaligned with the agents ultimate objectives, and (2) real-world data is scarce. RL addresses the first issue by explicitly optimizing for the end goal through reward signal, and the second by generating exploratory data. Consequently, RL significantly mitigates these limitations and improves performance (Zhang et al., 2025a). Our paper further examines the benefits of RL over SFT, as well as the limitations of RL, providing insights for future research directions. B.2 RL FOR LLMS Recently, RL has been widely adopted to enhance reasoning capabilities in language models, exemplified by milestone systems such as OpenAIs o1 and DeepSeek-R1. This paradigm has inspired new wave of reasoning-focused models, including Qwen-3 and Phi-4 Reasoning (Zhang et al., 2025b). State-of-the-art LLM-based agents also commonly employ RL (Zhang et al., 2025a). Despite its empirical success, the mechanisms by which RL improves LLM performance remain an active area of research, with current understanding scattered across multiple works. For instance, Chu et al. (2025) empirically compares SFT and RL on reasoning benchmarks, concluding that RL provides better generalization. Theoretical analysis in (Setlur et al., 2025) further shows that any verification-free approach, such as SFT, is suboptimal. Additionally, Yue et al. (2025) identifies an entropy mechanism, establishing and empirically validating trade-off between entropy and accuracy during RL training. In this paper, we focus on path planning as case study and derive results consistent with prior work: (1) SFT tends to memorize training data and produce co-occurrence-driven outputs; (2) RL surpasses SFT primarily through exploration; and (3) diversity collapse occurs during PG training. Beyond these findings, we uncover evidence suggesting that Q-learning may offer advantages over policy gradient methods, introducing new perspective on RL for LLMs."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "B.3 GRAPH PROBLEMS WITH LANGUAGE MODELS Graph problems serve as valuable testbed for analyzing the reasoning capabilities of language models. From an empirical standpoint, several benchmarks have been proposed (Guo et al., 2023; Wang et al., 2023b; Dai et al., 2024), spanning spectrum of tasks: classic graph problems (e.g., connectivity, path-finding, and pattern detection), graph neural network (GNN) benchmarks (e.g., node and graph classification), and semantic graph-based question answering (e.g., on knowledge graphs). Without additional training, LLMs generally underperform on these tasks. To improve performance, early approaches leverage instruction tuning and DPO (Luo et al., 2024; Chen et al., 2024a; Perozzi et al., 2024; Chai et al., 2023; Chen et al., 2024b), while later methods employ RL (Guo et al., 2025), which consistently achieves superior results. There are three major paradigms for analyzing how transformers solve graph-related reasoning tasks. The first is mechanistic interpretability, which reverse-engineers trained model weights (Neel et al., 2023). For example, Cohen et al. (2025) observed that transformers implement spectral algorithm to compute shortest paths. However, this paradigm largely relies on empirical observation without theoretical grounding. The second paradigm is based on expressiveness analysis (Dai et al., 2024; Sanford et al., 2024; Dai et al., 2025; De Luca & Fountoulakis, 2024), constructing weight configurations that enable transformers to simulate algorithms. Yet, such configurations are often unrealistic for transformers trained via SGD (e.g., embedding vectors explicitly set to 1, 2, . . . , (Dai et al., 2024)). The third paradigm investigates gradient dynamics, which is both practical and challenging due to the non-convexity of the optimization landscape. Prior work has analyzed path-finding in directed graphs (Wang et al., 2024b) and compositionality of paths (Zhu et al., 2024). To the best of our knowledge, this work presents the first analysis of RL gradient dynamics in LLMs. Our results explain why RL-based methods outperform SFT approaches and highlight the potential advantages of Q-learningdriven methods, opening promising directions for future research."
        },
        {
            "title": "C APPENDIX FOR SFT",
            "content": "C.1 PATH PLANNING ALGORITHM IN TRANSFORMER Algorithm 1 handcrafted path planning algorithm 1: Input: Adjacency matrix A, reachability matrix R, source node s, target node 2: Set path = [s s] and set current node = 3: while = do 4: 5: 6: 7: end while 8: output path Obtain = {kA(i,k) = 1 and R(t,k) = 1} Randomly sample next node from Append to path , and set = C.2 PROOF OF THEOREM 3.1 Proof. The next-token prediction cross-entropy loss can be written as ℓ = (cid:88) (cid:88) (cid:88) δk=um+1 log ˆum[k]. uDSFT m3 Under the assumption that the output distribution ˆum depends only on the target node u2 and the current node um, we can aggregate identical terms, and express the loss as (cid:32) (cid:88) (cid:88) u2,um Nu2,um,k (cid:33) (cid:32) (cid:88) Nu2,um,k Nu2,um,k (cid:80) log exp(f (u2, um)[k]) exp(f (u2, um)[k]) (cid:80) (cid:33) ."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "If (cid:80) bution and the distribution of doing softmax on vector (u2, um). It is minimized when Nu2,um,k = 0, the expression in brackets is the cross-entropy between the empirical distriexp(f (u2, um)[k]) exp(f (u2, um)[k]) (cid:80) = Nu2,um,k Nu2,um,k (cid:80) . If (cid:80) distribution. Nu2,um,k = 0, the loss does not depend on (u2, um), so it can be any valid probability"
        },
        {
            "title": "D APPENDIX FOR POLICY GRADIENT",
            "content": "D.1 PROOF OF THEOREM 4.1 Proof. In this case, the loss function of policy gradient is (cid:88) ℓ = uDRL,t δuP (cid:88) m3 log ˆum[um+1] = (cid:88) (cid:88) uDRL,tP m3 log ˆum[um+1] . D.2 PROOF OF THEOREM 4.2 Proof. We first rewrite the loss function as ℓ = (cid:88) (cid:88) uDRL,tP m3 log ˆum[um+1] = R,P,t i,j,k (cid:18) log exp(f (i, j)[k]) exp(f (i, j)[k]) (cid:80) (cid:19) , (cid:88) i,j,k where R,P,t set DRL,t P. Then we can take the gradient and get i,j,k denote the number of times that u[2] = i, u[m] = and u[m + 1] = for 3 in ℓ (i, j)[k] = R,P,t i,j,k + exp(f (i, j)[k]) exp(f (i, j)[k]) (cid:80) (cid:88) R,P,t i,j,k . For wrong tuple i, j, (where is not adjacent with or cannot reach i), R,P,t Thus, the gradient is always positive. On the other hand, we will also have i,j,k is always zero. ℓ (i, j)[k] (cid:88) (cid:88) = R,P,t i,j,k + (cid:80) (cid:80) exp(f (i, j)[k]) exp(f (i, j)[k]) (cid:88) N R,P,t i,j,k = 0. D.3 PROOF OF THEOREM 4.3 Proof. Note that KL(UC(i,j)softmax(f (i, j)) = (cid:88) kC(i,j) 1 C(i, j) (cid:32) log C(i, j) log exp(f (i, j)[k]) kC(i,j) exp(f (i, j)[k]) (cid:80) (cid:33) = log C(i, j) 1 C(i, j) (cid:88) kC(i,j) log exp(f (i, j)[k]) kC(i,j) exp(f (i, j)[k]) (cid:80) . Thus, it is sufficient to prove (cid:88) kC(i,j) log exp(f t(i, j)[k]) kC(i,j) exp(f t(i, j)[k]) (cid:80) (cid:88) kC(i,j) log (cid:80) exp(f t+1(i, j)[k]) kC(i,j) exp(f t+1(i, j)[k]) ."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "According to the gradient, we have that t+1(i, j)[k] = t(i, j)[k] η R,P,t i,j,k + exp(f (i, j)[k]) kC(i,j) exp(f (i, j)[k]) (cid:80) (cid:88) R,P,t i,j,k , kC(i,j) Here η is the step size. Let R,P,t i,j that R,P,t = (cid:80) kC(i,j) R,P,t i,j,k is the counter of outcome for R,P,t i,j i,j,k , then due to the on-policy updating in policy gradient, we know independent multi-nomial random variables with parameters (cid:110) exp(f t(i,j)[k]) C(i,j) exp(f t(i,j)[k]) (cid:80) (cid:111) . This means that kC(i,j) Moreover, since log E[f t+1(i, j)[k]] = t(i, j)[k]. (cid:17) kC(i,j) exp(f t(i, j)[k]) (cid:16)(cid:80) is convex function, we also have log (cid:88) kC(i,j) exp(f t+1(i, j)[k]) log = log (cid:88) kC(i,j) (cid:88) kC(i,j) exp(E[f t+1(i, j)[k]]) exp(f t(i, j)[k]) Because of this, we have (cid:88) kC(i,j) log exp(f t(i, j)[k]) kC(i,j) exp(f t(i, j)[k]) (cid:80) (cid:88) = t(i, j)[k] (cid:88) (cid:88) log kC(i,j) (cid:80) exp(f t+1(i, j)[k]) kC(i,j) exp(f t+1(i, j)[k]) t+1(i, j)[k] C(i, j) log (cid:88) kC(i,j) exp(f t(i, j)[k]) kC(i,j) kC(i,j) +C(i, j)E log (cid:88) kC(i,j) exp(f t+1(i, j)[k]) 0. D.4 PROOF OF THEOREM 4. Proof. When λ > 0, the loss function is ℓ = (cid:88) i,j,k R,P,t i,j,k ( log q(i, j)[k]) + λ R,t i,j,k (cid:88) i,j,k (cid:18) (cid:26) log q(i, j)[k] log q(i, j)[k] qbase(i, j)[k] (cid:27)(cid:19) , where R,t set DRL,t. i,j,k denote the number of times that u[2] = i, u[m] = and u[m + 1] = for 3 in We can take the gradient and get: ℓ (i, j)[k] = R,P,t i,j,k + q(i, j)[k] (cid:88) N R,P,t i,j,k + λN R,t i,j,k(1 q(i, j)[k]) log q(i, j)[k] qbase(i, j)[k] λ (cid:88) k=k R,t i,j,kq(i, j)[k] log q(i, j)[k] qbase(i, j)[k] ."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "Taking expectation, we can get: (cid:21) (cid:20) ℓ (i, j)[k] = E[N R,P,t i,j,k ] + q(i, j)[k] E[N R,P,t i,j,k ] + λE[N R,t i,j,k](1 q(i, j)[k]) log q(i, j)[k] qbase(i, j)[k] (cid:88) λ (cid:88) k=k E[N R,t i,j,k]q(i, j)[k] log q(i, j)[k] qbase(i, j)[k] . Letting R,t R,t i,j = (cid:80) R,t i,j,k, i,j q(i, j)[k]p(i, j)[k], and E[N R,t then due to on-policy training, we have that E[N R,P,t i,j,k] = R,t i,j q(i, j)[k]. i,j,k ] ="
        },
        {
            "title": "Hence",
            "content": "(cid:20) (cid:21) ℓ (i, j)[k] i,j q(i, j)[k]p(i, j)[k] + R,t = R,t i,j q(i, j)[k] q(i, j)[k]p(i, j)[k] (cid:88) +λN R,t i,j q(i, j)[k](1 q(i, j)[k]) log q(i, j)[k] qbase(i, j)[k] λN R,t i,j (cid:88) k=k q(i, j)[k]q(i, j)[k] log q(i, j)[k] qbase(i, j)[k] = R,t i,j q(i, j)[k] (cid:18) q(i, j)[k] (cid:88) p(i, j)[k] p(i, j)[k] + λ log q(i, j)[k] qbase(i, j)[k] λ log q(i, j)[k] qbase(i, j)[k] (cid:19) . The stable point must satisfy that, for any tuple i, j, k, case, for fixed i, and any such that q(i, j)[k] > 0, their p(i, j)[k] + λ log q(i,j)[k] equal. Otherwise we can always look for = arg mink:q(i,j)[k]>0 p(i, j)[k] + λ log q(i,j)[k] and its expected gradient is strict negative. qbase(i,j)[k] should qbase(i,j)[k] , = 0. And we claim that in this ℓ (i,j)[k] (cid:105) (cid:104) ℓ (i,j)[k] (cid:104) (cid:105) APPENDIX FOR Q-LEARNING E.1 PROOF OF LEMMA 5.1 Proof. Fix any triple (i, j, k). Consider training sequences whose first two nodes satisfy u1 and u2 = i. By the definition of the training process, P(u1 V, u2 = i) > 0. Under ϵ-exploration uniform over V, : P(next node = v) ϵ/V. Condition on the event {u1 V, u2 = i}. Then in the next two decisions, P(u4 = u1 V, u2 = i) ϵ/V, P(u5 = u1 V, u2 = i, u4 = j) ϵ/V. Hence P(u2 = i, u4 = j, u5 = k) p0(ϵ/V)2 > 0. Each occurrence of (u2, u4, u5) = (i, j, k) triggers one update of (i, j)[k]. Since each occurrence yields an update of (i, j)[k], we obtain lim inf 1 1 (cid:88) t=0 δ(it,jt,kt)=(i,j,k) prop i,j,k > 0, which is the persistent exploration condition. E.2 PROOF OF THEOREM 5.1 Proof. At stable point, the expected update of each coordinate vanishes. The per-step loss is ℓ = (cid:0)f (u2, um)[um+1] δuP δum+1=u2 {max (u2, um+1)[k]}(cid:1) ."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "Taking the gradient with respect to (u2, um)[um+1] and setting the expectation to zero yields, for every triple (i, j, k), (i, j)[k] = E[δuP δk=i] + max (i, k)[k]. (4) If = i, the expectation term in equation 4 vanishes, so (i, j)[k] = max (i, k)[k], which does not depend on j. Thus, for each = i, we have (i, j)[k] = max (i, k)[k] = max k=i (cid:0) max (i, k)[k], (i, k)[i](cid:1) = max k=i max (i, k)[k]. This expression no longer depends on or j. Therefore, for each fixed and = i, all (i, j)[k] take common value ci, independent of and k. E.3 PROOF OF THEOREM 5.2 Proof. For clarity, we introduce two notations that will be used repeatedly. First, for i, [n] and iteration t, define S(t) i,k := max (t)(i, k)[k]. Second, we write Anc(i) if there exists 1 such that (Am)[k, i] = 1, i.e. is an ancestor of i. The per-step loss under the process reward is ℓ = (cid:0)f (u2, um)[um+1] (δum+1=u2 δ(um,um+1) /E ) {max (u2, um+1)[k]}(cid:1)2 . Taking the gradient with respect to the active coordinate (u2, um)[um+1] gives ℓ (u2, um)[um+1] = 2(cid:0)f (u2, um)[um+1] δum+1=u2 + δ(um,um+1) /E max (u2, um+1)[k](cid:1). Applying gradient descent with learning rate η yields (u2, um)[um+1] (12η) (u2, um)[um+1]+2η(cid:0)δum+1=u2 δ(um,um+1) /E +max f (u2, um+1)[k](cid:1). Renaming (i, j, k) = (u2, um, um+1) and writing S(t) i,k = maxk (t)(i, k)[k], the recursion is (t+1)(i, j)[k] = (1 2η)f (t)(i, j)[k] + 2η(δk=i + (A[j, k] 1) + S(t) i,k). () When = i, by convention S(t) i,i = 0, so the update is (t+1)(i, j)[i] = (1 2η)f (t)(i, j)[i] + 2η A[j, i]. This linear recursion has fixed point A[j, i] and solution (t)(i, j)[i] = (1 (1 2η)t) A[j, i], which converges to 1 if A[j, i] = 1 and to 0 otherwise. The contraction factor is 1 2η. i,k must be analyzed. For = i, the limit of S(t) of i, in which case (t)(i, k)[i] 1 and hence S(t) Inductively S(t) all children of also satisfy / Anc(i), and inductively S(t) (12η)f (t)(i, k)[r]+2ηS(t) i,r 0. Thus S(t) and S(t) i,k 0 otherwise. If Anc(i) {i}, then either is parent i,k 1, or has child with Anc(i). i,k 1. If / Anc(i), then i,r 0, giving (t+1)(i, k)[r] = i,k 1 if Anc(i){i} i,r 1, and then () implies (t)(i, k)[r] 1, so S(t) i,k 0. Therefore the limit is S(t)"
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "For = i, substituting the limiting S(t) i,k into () gives (t+1)(i, j)[k] = (1 2η)f (t)(i, j)[k] + 2η((A[j, k] 1) + S(t) i,k). i,k 0, so (t)(i, j)[k] 0. If A[j, k] = 0 and Anc(i), then S(t) If A[j, k] = 1 and Anc(i), then S(t) i,k 1, so (t)(i, j)[k] 1. If A[j, k] = 1 and / Anc(i), then S(t) i,k 1, so the recursion is (t+1)(i, j)[k] = (1 2η)f (t)(i, j)[k], implying (t)(i, j)[k] 0. If A[j, k] = 0 and / Anc(i), then S(t) i,k 0, so the recursion is (t+1)(i, j)[k] = (1 2η)f (t)(i, j)[k] 2η, which converges to 1. These limits match the cases in the theorem. To establish rates, define the weight error eW (i, k) = S(t) eS (i, j, k) = (t)(i, j)[k] (i, j)[k] and the max error i,k. When = i, the recursion is i,k t+1(i, j, i) = (1 2η) eW eW (i, j, i), so each update contracts the error by 1 2η. Under persistent exploration, the coordinate (i, j, i) is updated with positive frequency prop i,j,i , so in global time (i, j, i) (1 2η prop eW i,j,i ε)t for any ε > 0 and large enough t. When = i, the recursion is t+1(i, j, k) = (1 2η) eW eW (i, j, k) + 2η eS (i, k). (i, k) depends only on {eW The error eS (i, k, r) : is child of k}. Along any directed path = v0 v1 vm = i, the error at can decay only after the error at v1 has already decayed, and so on. Thus, the effective contraction factor for eW (i, j, k) is the product of the peredge contraction rates (1 2η prop i,vn,vn+1 ). m1 (cid:89) n=0 Formally, by induction, for any ε > 0 and sufficiently large we have (cid:32) eW (i, j, k) max paths p:ki (cid:89) 1 2η prop i,vn ,vn+1 ε (cid:33)t . (vn,vn+1)p Therefore, all iterates converge linearly in global time, with effective rates determined jointly by η and the update proportions prop i,j,k . This completes the proof. E.4 PROOF OF THEOREM 5.3 Proof. Let prop i,j,k denote the asymptotic proportion of triples (u2, um, um+1) = (i, j, k) occurring in the generated sequences at the stable point, under the given sampling method and the persistent exploration condition. Equivalently, prop i,j,k is the limiting frequency with which state transitions to with target in the trajectories sampled by the model. By definition prop i,j,k > 0 for all i, j, k. At stable point of the updates, the expected gradient with respect to each parameter must vanish. Averaging the stationarity conditions with weights prop i,j,k yields, for all i, j, (cid:16) (cid:16) prop i,j,k prop i,j,k (cid:88) (cid:88) WM [j, k] + WV [i, k] A[j, k] + 1 δi=k max WM [j, k] + WV [i, k] A[j, k] + 1 δi=k max (WM [k, k] + WV [i, k]) (cid:17) (WM [k, k] + WV [i, k]) (cid:17) = 0, = 0. Introduce centered variables Sj,k := WM [j, k] A[j, k] + 1, Ti,k := WV [i, k] δi=k max (WM [k, k] + WV [i, k]),"
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "so that normalizing each sum by its positive denominator gives the block system Sk + Pk Tk = 0, where Sk = (Sj,k)j[n], Tk = (Ti,k)i[n], and Tk + Qk Sk = 0, (5) (Pk)[j, i] = prop i,j,k prop i,j,k (cid:80) , (Qk)[i, j] = prop i,j,k prop i,j,k (cid:80) . Since prop i,j,k > 0, every entry of Pk, Qk is strictly positive, and both are row-stochastic. Hence PkQk and QkPk are strictly positive stochastic matrices. By the PerronFrobenius theorem, both have simple eigenvalue 1 with eigenvector 1, and all other eigenvalues satisfy λ < 1. Thus ker(I PkQk) = span{1}, ker(I QkPk) = span{1}. From equation 5, eliminating Tk yields Sk = (PkQk)Sk, so Sk = ck1 for some ck R, and then Tk = QkSk = ck1. Returning to the definitions, WM [j, k] = A[j, k] 1 + ck, WV [i, k] = δi=k + max (WM [k, k] + WV [i, k]) ck. Substituting WM [k, k] = A[k, k] 1 + ck and writing Vi,k := WV [i, k] ck gives Vi,k = δi=k + max Vi,k. k: A[k,k]=1 On DAG, the unique {0, 1} solution of this recursion is the reachability indicator Ri,k. An induction over topological order shows Vi,k = Ri,k for all i, k. Therefore WV [i, k] = R[i, k] ck. Finally, note that if (Sk, Tk) solves equation 5, then so does (Sk + c1, Tk c1) for any R, since Pk1 = Qk1 = 1. Hence, the solution set for each is exactly one-dimensional affine line parametrized by ck. Conversely, if (WM , WV ) is of the above form, then plugging it into the update equations shows that the expected increment is identically zero: both sides of the gradient equations cancel by construction, so the point is stationary. Therefore, these conditions are not only necessary but also sufficient for stability."
        },
        {
            "title": "F EQUIVALENCE OF UNCLIPPED PPO AND POLICY GRADIENT",
            "content": "For sequence u, the policy gradient objective is ℓPG(u) = (cid:88) m3 R(u) log ˆum[um+1], where R(u) = δuP + p. Taking the gradient gives θℓPG(u) = = (cid:88) m3 (cid:88) R(u)θ log ˆum[um+1] R(u) θ ˆum[um+1] ˆum[um+1] . For unclipped PPO, the ratio between new and old probabilities is formed, with the denominator detached. The loss is ℓPPO-uc(u) = (cid:88) m3 R(u) ˆum[um+1] {ˆum[um+1]} . Since the denominator {ˆum[um+1]} is treated as constant, its gradient vanishes. Thus θℓPPO-uc(u) = (cid:88) m3 R(u) θ ˆum[um+1] {ˆum[um+1]} . Comparing with the policy gradient expression, we see the two gradients coincide. Therefore, for any fixed sequence u, unclipped PPO with stop-gradient denominator is exactly equivalent to vanilla policy gradient."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "Figure 5: The test accuracy of PG with different KL coefficients on four data splits after fine-tuning the SFT model on DRL-Train. All accuracies are evaluated with greedy decoding. Figure 6: The test accuracy of Q-learning on four data splits after fine-tuning the SFT model on DRL-Train. All accuracies are evaluated with greedy decoding."
        },
        {
            "title": "G ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "G.1 EXPERIMENTS ON ERD OS-R ENYI GRAPHS Beyond the setup in Section 2, we conduct additional experiments to compare RL methods with SFT. The graph construction and initial SFT stage remain unchanged. After SFT, we split all reachable pairs into an RL training set DRL-Train and an RL test set DRL-Test. This yields four intersections: DTrain2Train := DTrain DRL-Train, DTrain2Test := DTrain DRL-Test, DTest2Train := DTest DRL-Train, and DTest2Test := DTest DRL-Test. During the RL process, the model generates paths for pairs in DRL-Train and receives reward signals. The main difference between this setup and that in Section 2 is that the RL training set now contains new pairs that were unseen during SFT (DTest2Train). Therefore, the initial model is not perfect on these new training pairs. Additionally, some pairs from the SFT training set are not used for RL training (DTrain2Test), which allows us to measure the extent of forgetting. We consider the same RL algorithms introduced in Section 2: PG and Q-learning, whose training curves are presented in Figures 5 and 6, respectively. All accuracies are evaluated using greedy decoding. From Figure 5, we observe the opposing effects of KL regularization on DTrain2Test and DTest2Train. PG without KL regularization (λ = 0) and less regularized PG (λ = 0.0001) achieve significantly higher accuracy on DTest2Train. Stronger KL regularization hinders the models ability to learn new pairs, which aligns with Takeaway 4: KL regularization reduces training accuracy. Conversely, PG without KL regularization (λ = 0) tends to overfit the training data and exhibits continual forgetting of previous knowledge learned during SFT. Results on DTest2Test further demonstrate that overly strong KL regularization can hinder PGs improvement. Among all settings, λ = 104 achieves the best balance, indicating that well-chosen KL weight can improve generalization with minimal sacrifice in training accuracy. Compared to PG and the performance observed in Section 5, Q-learning exhibits slower convergence in this setting. One possible explanation is that the initial model performs poorly on the new training pairs, generating more failure cases and causing stronger re-instantiation."
        },
        {
            "title": "Preprint as an Arxiv Paper",
            "content": "G.2 EXPERIMENTS ON GRAPH REPRESENTED FOR BLOCKSWORLD We also run experiments on Blocksworld (Valmeekam et al., 2023a), benchmark for evaluating LLM planning ability (Kambhampati et al., 2024). The environment consists of blocks stacked on table, and the goal is to rearrange the blocks from an initial configuration to target configuration using sequence of actions. We model this into path-finding task, in which each configuration is node in graph, and an edge connects two nodes if one configuration can be transformed into the other by single valid action, such as moving block from one stack to another. We consider Blocksworld with four blocks and construct an undirected graph with 73 nodes: 24 configurations of single stack of four blocks, 24 configurations with three blocks in one stack and one block on the table, 12 configurations with two stacks of two blocks, 12 configurations with one stack of two blocks and two blocks on the table, and one configuration with all blocks on the table. Since accuracy comparison is not our focus, all node pairs are used for SFT training. The SFT dataset contains 50,000 paths sampled from the graph, with source and target nodes drawn uniformly from the 73 nodes. During RL training, the model generates paths for, and is updated on, all node pairs. We use policy gradient and Q-learning as introduced in Section 2. After training, we evaluate the learned weights using the metric of Wang et al. (2024b), which measures the models understanding of graph adjacency. As shown in Figure 1, with fixed training data, SFT may not learn the complete adjacency very well. In contrast, both PG and Q-learning improve the learned adjacency. In particular, Q-learning nearly recovers the complete adjacency, consistent with the results in Section 5."
        }
    ],
    "affiliations": [
        "Harvard University",
        "Microsoft Research Asia",
        "Peking University",
        "University of Southern California"
    ]
}