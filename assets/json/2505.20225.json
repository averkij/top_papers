{
    "paper_title": "FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models",
    "authors": [
        "Hao Kang",
        "Zichun Yu",
        "Chenyan Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token. Yet academic researchers still lack a fully open, end-to-end MoE platform for investigating scaling, routing, and expert behavior. We release FLAME-MoE, a completely open-source research suite composed of seven decoder-only models, ranging from 38M to 1.7B active parameters, whose architecture--64 experts with top-8 gating and 2 shared experts--closely reflects modern production LLMs. All training data pipelines, scripts, logs, and checkpoints are publicly available to enable reproducible experimentation. Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs. Leveraging full training trace transparency, we present initial analyses showing that (i) experts increasingly specialize on distinct token subsets, (ii) co-activation matrices remain sparse, reflecting diverse expert usage, and (iii) routing behavior stabilizes early in training. All code, training logs, and model checkpoints are available at https://github.com/cmu-flame/FLAME-MoE."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 5 2 2 0 2 . 5 0 5 2 : r FLAME-MoE: Transparent End-to-End Research Platform for Mixture-of-Experts Language Models Hao Kang2 Zichun Yu1 Chenyan Xiong1,3 1Language Technologies Institute 2School of Computer Science 3Foundation and Language Model Center Carnegie Mellon University {haok, zichunyu, cx}@andrew.cmu.edu"
        },
        {
            "title": "Abstract",
            "content": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only fraction of the model per token. Yet academic researchers still lack fully open, end-to-end MoE platform for investigating scaling, routing, and expert behavior. We release FLAME-MoE, completely open-source research suite composed of seven decoder-only models, ranging from 38M to 1.7B active parameters, whose architecture64 experts with top-8 gating and 2 shared expertsclosely reflects modern production LLMs. All training data pipelines, scripts, logs, and checkpoints are publicly available to enable reproducible experimentation. Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs. Leveraging full training trace transparency, we present initial analyses showing that (i) experts increasingly specialize on distinct token subsets, (ii) co-activation matrices remain sparse, reflecting diverse expert usage, and (iii) routing behavior stabilizes early in training. All code, training logs, and model checkpoints are available at https://github.com/cmu-flame/FLAME-MoE."
        },
        {
            "title": "Introduction",
            "content": "Mixture-of-Experts (MoE) architectures have emerged as powerful paradigm for scaling large language models efficiently. By activating only sparse subset of expert modules per input token, MoE models significantly expand the number of trainable parameters without commensurate increase in training or inference cost. This approach enables high effective capacity with strong computational efficiency, and has been adopted in range of state-of-the-art systemsboth open and closed-sourceincluding Gemini-1.5, DeepSeek-V3, and Llama-4 [21, 23, 34]. As these architectures gain traction, there is growing need for transparent, reproducible frameworks to facilitate systematic study of MoE behavior, limitations, and scaling dynamics. Recent open-source efforts such as OLMoE and OpenMoE have made notable progress toward building accessible and performant MoE models, releasing architectural variants, pretrained checkpoints, and analyses of routing and expert specialization [25, 36]. These contributions represent valuable steps in expanding the empirical foundation for MoE research. However, they primarily emphasize architectural design and downstream performance, offering limited support for systematic, end-to-end investigation of MoE training dynamics, scaling behavior, and routing evolution. As result, unlike the dense model communitywhich has benefited from comprehensive frameworks Equal contribution. Author order determined alphabetically. Preprint. Table 1: Openness comparison between open-source MoE models. Model Code Data All Ckpts Logs #Scales Active/Total Parameters JetMoE [30] OpenMoE [36] OLMoE [25] FLAME-MoE 1 3 1 7 2B/8B 339M/650M-6.8B/34B 1.3B/6.9B 38M/100M-1.7B/10.3B such as Pythia [3]the MoE research landscape has yet to converge around shared experimental platform that enables reproducible and extensible analysis across model scales. To address the gap in accessible and reproducible research on MoE language models, we introduce FLAME-MoE: transparent, robust platform for controlled experimentation across model scales, sparsity levels, and architectures. It includes seven decoder-only MoE models (38M1.7B active parameters), each with 64 experts per layer, top-8 gating, and two shared experts, following DeepSeekV2 [20] and OLMoE [25]. Using empirical scaling laws, we allocate compute-optimal training budgets to ensure fair, efficient pretraining. As summarized in Table 1, FLAME-MoE is the only MoE platform offering full opennesscode, data, checkpoints, routing logs, and evaluation resultsacross wide range of model sizes, supporting reproducibility, accessibility, and cross-scale analysis. Empirical evaluations on 6 downstream tasks show that FLAME-MoE consistently outperforms dense counterparts trained under equal compute, with accuracy gains ranging from 1.8 to 3.4 points. More importantly, our training traces reveal nuanced dynamics of MoE models: expert specialization emerges early and intensifies over time; expert co-activation remains sparse and stable; and routing behaviors quickly converge during early pretraining. These findings underscore FLAME-MoEs value as tool for in-depth analysis of expert dynamics, sparsity effects, and routing evolution. FLAME-MoE is not merely model release, but comprehensive platform for advancing research on sparse language models. Our main contributions include: (1) fully transparent infrastructure for end-to-end MoE experimentation, including models, training scripts, routing traces, and evaluation pipelines; (2) suite of consistent, modular architectures spanning wide range of active parameters to facilitate rigorous comparison across scales and with dense baselines; and (3) analytical capabilities that enable fine-grained investigation of routing behavior, expert specialization, load balancing, and parallelization strategiesquestions that remain difficult to address without full access to the training process. By opening every stage of model development to the research community, we hope FLAMEMoE could establish principled and extensible foundation for future work on Mixture-of-Experts."
        },
        {
            "title": "2 Related Work",
            "content": "Mixture-of-Experts models aim to increase model capacity without commensurate rise in computational cost by selectively activating sparse subset of expert modules per input [15, 29]. Foundational works such as GShard [17] and Switch Transformer [7] demonstrated the efficacy of sparse expert routing in enabling the training of large-scale models with substantially reduced floating point operations (FLOPs), while maintaining competitive performance. These models introduced top-k routing mechanisms alongside auxiliary losses to ensure balanced expert utilization, thus establishing key design principles for scalable sparse training. Subsequent developments, including BASE Layers [18], extended this paradigm by employing global routing via optimal transport to enhance load balancing and training stability. Later models such as DeepSeek-V3 [21] and M6-T [37] scaled these approaches further, incorporating mechanisms such as shared experts and expert prototyping to improve convergence and efficiency, particularly in multilingual and long-context scenarios. Recent work has also examined the robustness and specialization of expert routing in MoE architectures [5, 25, 36]. For instance, Chi et al. [5] address the issue of representation collapsewhere token embeddings become overly concentrated around expert centroidsby projecting tokens and expert keys onto hypersphere prior to computing routing scores. This modification enhances assignment diversity and yields improved performance on multilingual benchmarks. Other studies have investigated the evolution of routing dynamics throughout training [25, 36], noting that expert activation patterns frequently correlate with input position and frequency. Collectively, these findings underscore the importance of not only architectural efficiency but also the stability of sparse computation. 2 In parallel, large-scale dense model platforms have played critical role in advancing empirical research by enhancing reproducibility and transparency. Pythia [3], for example, comprises suite of 16 decoder-only models trained on common dataset, with full checkpoint availability and precise training reconstructions. This design enables fine-grained analyses of scaling laws, memorization behaviors, and training dynamics. Similarly, OPT [39] released dense models ranging from 125M to 175B parameters, along with training code, hyperparameters, and logs, setting precedent for open large-scale experimentation. Pythia has served as the foundation for range of downstream studies and systems, including instruction-tuned models such as Dolly 2.0 [33]. Researchers have leveraged the Pythia suite to examine memorization [2, 13], convergence patterns [22], and internal circuit development [35]. Its consistent data ordering across model sizes enables rigorous scaling analyses, supporting investigations into privacy risks [1], interpretability [32], and alignment techniques such as Direct Preference Optimization (DPO) [26]. While early MoE systems were predominantly developed on proprietary infrastructure, recent opensource initiatives have significantly broadened accessibility. Notable efforts such as OpenMoE [36] and OLMoE [25] have released models, training frameworks, and diagnostic tools, facilitating the study of expert specialization and routing behavior while narrowing the reproducibility gap. From systems perspective, multiple frameworks have emerged to support efficient MoE training and deployment. DeepSpeed-MoE [27] and Tutel [14] offer optimized all-to-all communication, expert parallelism, and high-throughput inference, supporting models at the trillion-parameter scale. FastMoE [9] provides lightweight PyTorch extension with minimal integration overhead, while Megatron-LM [31] integrates MoE support alongside tensor, expert, and pipeline parallelism. These systems collectively enable scalable, production-grade sparse modeling via optimized infrastructure."
        },
        {
            "title": "3 Model Architecture",
            "content": "Our model architecture builds upon recent advancements in MoE models, particularly drawing inspirations from DeepSeek-V2 [20] and OLMoE [25]. Specifically, we adopt decoder-only transformer architecture consisting of NL layers, where all feedforward network (FFN) sublayers except for the first one are replaced by an MoE layer. 3.1 MoE Layer Each MoE layer comprises NE expert networks (FFNs) and router mechanism. For an input token representation x, we employ Top-k routing, where only the experts with the highest routing scores are selected to process the token: MoE(x) = (cid:88) softmax(r(x))iEi(x) (1) iTop-k(r(x)) where Ei denotes the i-th expert network and is router network that computes router scores r(x) for each of the NE experts based on the input token representation x. The Top-k function selects the indices of the experts yielding the highest scores. These scores are normalized via softmax function, resulting in gating weights softmax(r(x))i. The final output of the MoE layer is weighted sum: the output Ei(x) of each selected expert Top-k(r(x)) is multiplied by its corresponding gating weight, and these products are summed together. Following best practice and findings from OLMoE and DeepSeek-V2, we set the total number of experts NE = 64 per MoE layer and activate = 8 experts per token. Adapting the strategy from DeepSeek-V2, 2 of these 8 active experts are designated as shared experts, meaning they are activated for every token, providing baseline computation path. The remaining 6 experts are routed experts, selected dynamically by the router based on the input token. 3.2 Training Loss Training MoE models effectively often requires auxiliary loss functions to encourage balanced load across experts and prevent routing collapse. First, we adopt load balancing loss from Shazeer et al. [29] to ensure that tokens are distributed as evenly as possible among the NE experts, which improves both model utilization and parallelism. For each routing operation, given NE experts and 3 (a) IsoFLOP profiles. (b) Parametric loss. (c) Fitted scaling law. (d) Generalization ability. Figure 1: Scaling law experiments: (a) IsoFLOP profiles; (b) parametric loss function fitting; (c) fitted scaling law; (d) generalization from validation loss to downstream performance. Active-Total Layers Hidden Size #FLOPs H100 Hours Tokens Table 2: Model cards of 7 FLAME-MoE models. FFN Hidden MoE FFN Hidden Total/Active/Shared 38M-100M 98M-349M 115M-459M 290M-1.3B 419M-2.2B 721M-3.8B 1.7B-10.3B 9 9 12 9 15 12 18 256 512 512 1024 1024 1536 1368 2736 2736 5472 5472 8208 10944 176 352 352 704 704 1056 1408 64/8/2 64/8/2 64/8/2 64/8/2 64/8/2 64/8/2 64/8/2 1.0e18 3.0e18 6.0e18 2.0e19 3.0e19 8.0e19 2.4e20 5.7 8.2 20.9 63.4 66.4 172.8 560.5 4.4B 5.0B 8.7B 11.4B 11.9B 18.4B 23.1B batch of tokens, the load balancing loss LLB is defined as: LLB = NE NE(cid:88) i=1 mi Pi, where mi = 1 (cid:88) 1(xj)i and Pi = j=1 1 (cid:88) j=1 softmax(r(xj))i. (2) (3) Here, mi is the fraction of tokens dispatched to expert i; Pi is the average gating weight assigned to expert by the router over the batch; 1(xj)i is an indicator function that equals 1 if token xj is routed to expert i, and 0 otherwise. This loss encourages the router to assign tokens more uniformly across all experts, mitigating the risk of expert underutilization and routing collapse. In addition to the load balancing loss, we incorporate router z-loss [40] to further stabilize MoE training. The z-loss penalizes large-magnitude router logits, encouraging the router to produce outputs with smaller absolute values, which helps reduce numerical instability and round-off errors when passing through the gating function. Formally, the router z-loss is defined as: LRZ(x) = 1 (cid:88) i=1 log NE(cid:88) j=1 2 exp(r(xi)j) , (4) where r(xi)j denotes the router logit for token xi and expert j. This loss encourages the router logits to remain well-behaved, improving the stability and efficacy of MoE training. The final training objective combines the standard cross-entropy loss (LCE) with two auxiliary losses: Ltrain = LCE + γLLB + ηLRZ, (5) where γ and η are two hyperparameters controlling the strengths of the load balancing loss and the router z-loss. Following OLMoE, we set γ = 0.01 and η = 0.001 for all experiments."
        },
        {
            "title": "4 Scaling Law Study for FLAME-MoE",
            "content": "Determining the optimal allocation between the model size and the number of training tokens for fixed compute budget (FLOPs) is crucial to training LLMs efficiently. As we fix the total number of experts and the number of active experts per token for FLAME-MoE, we concentrate on the number of active parameters as the primary variable for our scaling law study. This simplification allows us to adapt established methodologies in dense models for finding compute-optimal MoE configurations. 4 Specifically, we investigate two primary approaches employed in Chinchilla [11], to determine the optimal Nactive for our MoE models given fixed FLOPs budget, IsoFLOP profiles (4.1) and parametric loss function fitting (4.2). Finally, we apply our fitted scaling law to predict the optimal number of parameters to configure our released FLAME-MoE model family (4.3). In all following experiments, we use the data sampled from the state-of-the-art open-source pretraining corpus, DataComp-LM (DCLM) [19], to train our models. 4.1 IsoFLOP Profiles The first approach generates IsoFLOP profiles by varying the number of active parameters Nactive while keeping the computational budget fixed. For each fixed FLOPs budget, we calculate the number of training tokens as function of Nactive using the formula: = κNactive (6) where κ is constant commonly set to 6 in previous works [11]. By plotting the final validation loss achieved by each model configuration against its Nactive for given FLOPs budget, we obtain an IsoFLOP curve. We then fit parabolic function to each of these curves. The minimum of this parabola provides an estimate of the optimal Nactive for that specific computational budget. Finally, we fit power law to these data points to model the relationship active(C) and D(C) b, where active is the optimal Nactive, is the optimal number of training tokens, is the computational budget, and a, are exponents to be determined. In our experiments, we choose 4 computational budgets (1e18, 3e18, 6e18, and 3e19). For each budget, we train 16 models with different Nactive, ranging from 33.4M to 1.7B. The plotted IsoFLOP profiles are shown in Figure 1a, where the parabolas decently fit the data points. 4.2 Parametric Loss Function Fitting The second approach fits single parametric function to all experimental data points (final validation loss, Nactive, number of training tokens) collected from the IsoFLOP experiments. Following [11], we model the final validation loss Lval as function of Nactive and D: Lval(Nactive, D) = (Nactive)α + Dβ + L0 (7) where A, B, α, β, and L0 (an irreducible loss term) are parameters to be fitted from the experimental data. To ensure the robustness of our fitted parametric loss function, we account for potential local minima by employing grid search over initial parameter values for the fitting algorithm. Furthermore, we utilize Huber loss function with δ = 103 for the fitting. The Huber loss is less sensitive to outliers in the experimental data compared to standard mean squared error loss, which is crucial for achieving generalized predictive performance when extrapolating to larger computational budgets. As illustrated in Figure 1b, the fitted parametric loss function precisely predicts loss values, demonstrating its effectiveness in capturing the underlying relationship between Nactive, D, and Lval. Once this parametric loss function is fitted, we can use it to predict the optimal Nactive and training tokens for any given computational budget C. By minimizing Lval(Nactive, D) subject to the active(C) and D(C). Similar to the IsoFLOP constraint = κNactiveD, we can derive the optimal approach, we can then fit power laws to these derived optimal values as function of C. 4.3 FLAME-MoE Model Family We present our fitted scaling law in Figure 1c and Table 4, where the results from both the IsoFLOP and parametric loss approaches closely align, demonstrating the robustness and consistency of our experimental methodology. The generalization ability of our scaling law is also confirmed in Figure 1d, where we observe strong correlation (Spearman=0.89) between the validation loss and the performance on representative downstream task, HellaSwag [38]. We choose three additional #FLOPs from DCLM (2e19, 8e19, and 2.4e20) and apply our fitted scaling law to predict the optimal number of parameters under these #FLOPs. The resulting compute-optimal models are FLAME-MoE-290M-1.3B, 721M-3.8B, and 1.7B-10.3B, where the first number denotes 5 Table 3: Performance of FLAME-MoE on downstream tasks across different compute budgets. 1: DCLM 400M-1x; 2: DCLM 400M-4x; 3: DCLM 1B-1x. The other 4 #FLOPs are what we used in our scaling law experiments. We report accuracy for each task. Best performances are marked bold. Model #FLOPs ARC-E ARC-C OBQA HellaSwag PIQA WinoGrande Average 1.0e18 Dense-77M FLAME-MoE-38M-100M 1.0e18 Dense-77M 3.0e18 FLAME-MoE-98M-349M 3.0e18 Dense-191M 6.0e18 FLAME-MoE-115M-459M 6.0e18 2.0e191 2.0e191 Dense-411M FLAME-MoE-290M-1.3B Dense-411M FLAME-MoE-419M-2.2B Dense-411M FLAME-MoE-721M-3.8B Dense-1.4B FLAME-MoE-1.7B-10.3B 3.0e19 3.0e19 8.0e192 8.0e192 2.4e203 2.4e203 0.3266 0.3607 0.3754 0.4217 0.4318 0. 0.4802 0.5198 0.5046 0.5476 0.5484 0.5955 0.6035 0.6469 0.2193 0.2235 0.2065 0. 0.2099 0.2406 0.2415 0.2474 0.2568 0.2841 0.2568 0.2884 0.3020 0.3174 0.2460 0. 0.2440 0.2660 0.2780 0.2700 0.2940 0.3020 0.2940 0.3100 0.2940 0.3200 0.3360 0. 0.2588 0.2582 0.2632 0.2790 0.2769 0.3103 0.3237 0.3567 0.3353 0.3936 0.3776 0. 0.4653 0.5304 0.5582 0.5615 0.5832 0.6137 0.6023 0.6240 0.6398 0.6676 0.6387 0. 0.6654 0.6986 0.6844 0.7236 0.4949 0.4949 0.5043 0.5241 0.4917 0.5201 0.5130 0. 0.5099 0.5059 0.5257 0.5130 0.5178 0.5359 0.3506 0.3575 0.3628 0.3866 0.3818 0. 0.4154 0.4329 0.4232 0.4536 0.4447 0.4782 0.4848 0.5174 active parameters, and the second denotes total parameters. We then combine these three with the 4 compute-optimal models from our IsoFLOP experiments (FLAME-MoE-38M-100M, 98M-349M, 115M-459M, and 419M-2.2B) to form FLAME-MoE model family. The detailed configurations for each model scale within the FLAME-MoE family are presented in Table 2."
        },
        {
            "title": "5 Pretraining FLAME-MoE",
            "content": "In this section, we present the setup (5.1), evaluation results (5.2), and throughput (5.3) of our pretraining experiments. 5.1 Experimental Setup Our pretraining implementations are based on Megatron-LM [31], highly optimized and integrated platform designed for large-scale training. We compare the performance of FLAME-MoE with dense baseline models of similar sizes, which are configured following the same architectures used in Pythia [3] (Dense-77M and Dense-191M) or DCLM [19] (Dense-411M and Dense-1.4B). We train these baselines ourselves using the same codebase as FLAME-MoE to facilitate fair comparison. For training, we use the Adam optimizer [16] with maximum learning rate of 3e-4, global batch size of 1024, and sequence length of 2048. The learning rate is configured using WSD scheduler [12], with warmup ratio of 0.01 and decay ratio of 0.1 relative to the total number of training steps. We use 32 NVIDIA H100 GPUs for training and store 10 checkpoints across evenly splitted training steps to study its performance trends. We provide summary of all training configurations in Table 5. For evaluation, we adapt lm-evaluation-harness [8] to work with models trained using Megatron-LM. We assess the performance of FLAME-MoE on 6 downstream tasks from OLMoE [25]2, including ARC-E [6], ARC-C [6], OBQA [24], HellaSwag [38], PIQA [4], and WinoGrande [28]. Following DCLM, we use 10-shot evaluation for ARC-E, ARC-C, HellaSwag and PIQA, and 0-shot for OBQA and WinoGrande. The evaluation metric for all tasks is accuracy. 5.2 Evaluation Results We present the evaluation results of FLAME-MoE in Table 3. FLAME-MoE significantly outperforms the dense counterparts with the same pretraining FLOPs on almost every task, demonstrating the effectiveness of our scaling law and the superiority of MoE models. The advantages of FLAME-MoE are more pronounced at larger scales, with more than 3 points of average accuracy improvements over dense baselines under both 8.0e19 and 2.4e20 budgets. 2We replace MMLU [10] with OBQA as the MMLU performance of small models is near random guess. 6 (a) 400M-1x (2.0e19 #FLOPs). (b) 400M-4x (8.0e19 #FLOPs). (c) 1B-1x (2.4e20 #FLOPs). Figure 2: Downstream comparison between FLAME-MoE and dense models during pretraining. Figure 3: Training efficiency of FLAME-MoE-1.7B-10.3B under different parallelization strategies (EP = Expert Parallel, PP = Pipeline Parallel). Dense-1.4B is also included here as comparison. We further illustrate the scaling curves of FLAME-MoE and dense models in Figure 2. We observe that the performance gap between FLAME-MoE and dense models continuously improves with the increase of pretraining FLOPs, and FLAME-MoE can match or even outperform dense models trained with 2x FLOPs (e.g., in 400M-4x). These results demonstrate that FLAME-MoE substantially improves pretraining efficiency, achieving better speed-quality frontier. 5.3 Training Throughput In addition to task performance, we also evaluate the efficiency of training in various parallelization strategies. We profile both throughput (TFLOPs/s/GPU) and elapsed time per training step under different combinations of pipeline parallelism (PP) and expert parallelism (EP) that can fit into one node (8 GPUs). As shown in Figure 3, increasing EP generally improves utilization and reduces latency, while deeper pipeline parallelism (e.g., PP=2) can further enhance scalability. Based on these findings, we adopt the best-performing configuration of PP=1 and EP=8 for training our FLAME-MoE models, ensuring that our experiments efficiently utilize compute resources. However, while MoE models demonstrate great utilization under EP=8 as presented in Appendix A, the overall FLOPs throughput still lags behind dense models. This discrepancy primarily arises from the inherent sparsity and communication overheads introduced by MoE architectures, which pose unique infrastructure challenges. These limitations highlight an area for improvement in open-source MoE implementations, such as those in Megatron-LM. Despite being among the most optimized open MoE frameworks available, current performance still trails that of proprietary systems with tightly integrated hardware-software co-design."
        },
        {
            "title": "6 Empirical Analyses",
            "content": "A central advantage of the FLAME-MoE suite is the release of full pretraining checkpoints, which enables fine-grained analysis of model behavior across all stages of trainingnot just at convergence. In this section, we use this capability to explore key MoE-specific behaviors, including expert specialization (6.1), co-activation (6.2), and router saturation (6.3). These analyses exemplify how FLAME-MoE can support the broader academic community in studying the training dynamics of large-scale MoE models. 7 Figure 4: Evolution of specialization scores for the top-2 most specialized tokens across Experts 0, 1, 6, 9 at the final layer in FLAME-MoE-1.7B-10.3B on the validation set. Figure 5: Expert co-activation in FLAME-MoE-1.7B-10.3B at the final checkpoint on the validation set. The heatmap shows pairwise co-activation scores among the 16 experts with the highest coactivation across layers 2, 6, 12, and 18. Expert IDs are shown on the axes. 6.1 Expert Specialization To characterize expert behavior during pretraining, we analyze the routing patterns of individual tokens over time. In particular, we focus on whether certain experts consistently process specific tokens, which may indicate form of specialization. We define expert specialization as the fraction of times given token is routed to specific expert, normalized by the tokens total frequency in the evaluation corpus. Formally, for token and an expert e, the specialization score is: Specializatione(t) = # times expert activates on # times appears in the evaluation set (8) This formulation quantifies how responsible an expert becomes for handling particular token. The definition aligns closely with vocabulary specialization explored in OLMoE [25], but is evaluated here across full pretraining trajectories. To track specialization over time, we fix the top-2 most specialized tokens for each expert at the end of pretraining and retrospectively evaluate their scores at earlier checkpoints. As shown in Figure 4, we observe consistent upward trend for these specialization scores across all experts analyzed. This indicates that token-level specialization gradually emerges and solidifies during pretraining. Additional details are presented in Appendix B. 6.2 Expert Co-activation To understand expert interactions under top-k routing, we analyze expert co-activationhow often expert pairs are selected together for the same token. This reveals whether experts behave independently or tend to co-operate. Following OLMoE [25], we define the directional co-activation score from expert Ei to expert Ej as: CoAct(Ei, Ej) = {Ei Top-k(x), Ej Top-k(x) } {Ei Top-k(x) } (9) where is the evaluation token set. CoAct measures the conditional likelihood that Ej is co-activated with Ei. high score indicates tight coupling between the two experts, whereas low score suggests independence. Importantly, this formulation is asymmetric, i.e. CoAct(i, j) = CoAct(j, i) in general. As shown in Figure 5, co-activation is generally sparse, with most expert pairs exhibiting low scores. This suggests limited redundancy and indicates that experts are learning to be diverse instead of 8 Figure 6: Router saturation across training for FLAME-MoE-1.7B-10.3B. Each subplot shows the average expert selection overlap with the final checkpoint using different top-k values (1, 2, 4, 8). frequently overlapping in activation. In addition, co-activation increases with depth: the maximum score rises from 0.38 (Layer 2) and 0.39 (Layer 6) to 0.50 (Layer 12) and 0.70 (Layer 18). This pattern becomes more pronounced as training progresses; in Layer 18, the peak score grows from 0.51 to 0.70 between 10% and 100% of pretraining. Shallower layers show weaker trends. Additional results across different training steps and model scales are provided in Appendix C. 6.3 Router Saturation Another key question in understanding MoE behavior is how early and how consistently the router converges on its expert selection patterns. Unlike expert co-activation, which focuses on the interactions among experts within single forward pass, router saturation examines the temporal stability of routing decisions throughout pretraining. Understanding the saturation dynamics offers insight into how MoE models balance early specialization with continued learning capacity. We define saturation as the average overlap between the top-k experts selected for each token at step and at convergence. For token and layer l, let Top-k(t) (x) denote the top-k experts at step t, and Top-k(T ) (x) the final selection. Then, Saturationl(t) = 1 (cid:88) xT Top-k(t) (x) Top-k(T ) (x) (10) Although FLAME-MoE was trained with top-k = 8 routing, we report saturation under different evaluation settings (k = 1, 2, 4, 8) to capture finer-grained changes in expert preference. As shown in Figure 6, saturation increases steadily with training, with most layers reaching over 70% agreement by the midpoint of training. Notably, saturation rises sharply within the first few thousand steps, suggesting that the router converges to stable expert assignments early in pretraining. This pattern is consistent across all top-k settings, though absolute scores are higher for smaller k, reflecting greater selection consistency among the most preferred experts. Deeper layers generally saturate faster than shallower ones, indicating more stable routing behavior as depth increases. Additional results across different model scales are provided in Appendix D."
        },
        {
            "title": "7 Conclusion",
            "content": "We present FLAME-MoE, transparent and reproducible research platform built to advance the study of Mixture-of-Experts language models. By releasing family of seven compute-optimal models along with complete training artifactsincluding logs, checkpoints, routing traces, and evaluation scripts we enable rigorous, end-to-end study of MoE systems. Empirically, our models significantly outperform dense baselines under the same compute budgets, validating the effectiveness of our scaling law methodology that offers principled guide for resource allocation. FLAME-MoE is not limited to isolated benchmark evaluation; rather, it offers an infrastructure to study every facet of MoE models, including expert specialization, routing dynamics, scaling behavior, training stability, parallel efficiency, and generalization performance. We believe that FLAME-MoE lays the groundwork for systematic, transparent exploration of sparse model architectures and invites broader community engagement in understanding and advancing MoE systems."
        },
        {
            "title": "Acknowledgements",
            "content": "We sincerely thank CMU Foundation and Language Model (FLAME) Center for providing support of computational resources. We sincerely thank Tianqi Chen, Ruihang Lai, Zihao Ye, Hongyi Jin, and Bohan Hou for discussing ideas and providing helpful feedback on this work."
        },
        {
            "title": "References",
            "content": "[1] Atilla Akkus, Mingjie Li, Junjie Chu, Michael Backes, Yang Zhang, and Sinem Sav. Generated data with fake privacy: Hidden dangers of fine-tuning large language models on generated data. arXiv preprint arXiv:2409.11423, 2024. [2] Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. Emergent and predictable memorization in large language models. Advances in Neural Information Processing Systems, 36:2807228090, 2023. [3] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: suite for analyzing large language models across training and scaling. In Proc. of ICML, 2023. [4] Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about physical commonsense in natural language. In Proc. of AAAI, 2020. [5] Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, et al. On the representation collapse of sparse mixture of experts. Proc. of NeurIPS, 2022. [6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the ai2 reasoning challenge. ArXiv preprint, 2018. [7] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. JMLR, 2022. [8] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 2023. [9] Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, and Jie Tang. Fastmoe: fast mixture-of-expert training system. ArXiv preprint, 2021. [10] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In Proc. of ICLR, 2021. [11] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Proc. of NeurIPS, 2022. [12] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. MiniCPM: Unveiling the potential of small language models with scalable training strategies. ArXiv preprint, 2024. [13] Jing Huang, Diyi Yang, and Christopher Potts. Demystifying verbatim memorization in large language models. arXiv preprint arXiv:2407.17817, 2024. [14] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, et al. Tutel: Adaptive mixture-of-experts at scale. Proc. of MLSys, 2023. [15] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. ArXiv preprint, 2024. [16] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Proc. of ICLR, 2015. [17] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. ArXiv preprint, 2020. [18] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In Proc. of ICML, 2021. [19] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al. DataComp-LM: In search of the next generation of training sets for language models. In Proc. of NeurIPS, 2024. [20] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. DeepSeek-V2: strong, economical, and efficient mixture-of-experts language model. ArXiv preprint, 2024. [21] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. ArXiv preprint, 2024. [22] Richard Diehl Martinez, Pietro Lesci, and Paula Buttery. Tending towards stability: Convergence challenges in small language models. arXiv preprint arXiv:2410.11451, 2024. [23] Meta AI. The llama 4 herd: The beginning of new era of natively multimodal ai innovation, 2025. [24] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proc. of EMNLP, 2018. [25] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. OLMoE: Open mixture-ofexperts language models. In Proc. of ICLR, 2025. [26] Laura OMahony, Leo Grinsztajn, Hailey Schoelkopf, and Stella Biderman. Attributing mode collapse in the fine-tuning of large language models. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024. [27] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-ofexperts inference and training to power next-generation ai scale. In Proc. of ICML, 2022. [28] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proc. of AAAI, 2020. [29] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. ArXiv preprint, 2017. [30] Yikang Shen, Zhen Guo, Tianle Cai, and Zengyi Qin. JetMoE: Reaching llama2 performance with 0.1 dollars. ArXiv preprint, 2024. [31] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. ArXiv preprint, 2019. [32] Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, and Ravid Shwartz-Ziv. Layer by layer: Uncovering hidden representations in language models. arXiv preprint arXiv:2502.02013, 2025. [33] Databricks Team."
        },
        {
            "title": "Free",
            "content": "instruction-tuned dolly-first-open-commercially-viable-instruction-tuned-llm, 2023. llm. dolly:"
        },
        {
            "title": "Introducing",
            "content": "open https://www.databricks.com/blog/2023/04/12/ the worlds first truly [34] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint, 2024. [35] Curt Tigges, Michael Hanna, Qinan Yu, and Stella Biderman. Llm circuit analyses are consistent across training and scale. arXiv preprint arXiv:2407.10827, 2024. [36] Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. OpenMoE: An early effort on open mixture-of-experts language models. ArXiv preprint, 2024. [37] An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, et al. M6-T: Exploring sparse expert models and beyond. arXiv preprint, 2021. [38] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Proc. of ACL, 2019. [39] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: Open pre-trained transformer language models. ArXiv preprint, 2022. [40] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. ST-MoE: Designing stable and transferable sparse expert models. ArXiv preprint, 2022. 12 Table 4: Scaling law parameters of FLAME-MoE. Name Value L0 α β 148.413257 3269017.372472 2.241716 0.279702 0.715500 0.689902 0."
        },
        {
            "title": "A Training Efficiency",
            "content": "Table 5: Training details of FLAME-MoE. Name Batch Size Sequence Length Optimizer Scheduler Max Learning Rate Min Learning Rate Warmup Ratio Decay Ratio Pipeline Parallel Expert Parallel Device Value 1024 2048 Adam WSD 3e-4 3e-5 0.01 0.1 1 8 32xH100 (a) EP=2, PP= (b) EP=8, PP=1 Figure 7: GPU utilization under different parallelization strategies (EP = Expert Parallel, PP = Pipeline Parallel) using Megatron-LM, collected by Wandb. Figure 7 illustrates the variation in GPU utilization under different levels of expert parallelism while keeping the pipeline parallelism constant. As observed, increasing the number of expert parallel groups (EP) from 2 to 8 leads to marked improvement in GPU utilization. Specifically, in the configuration with EP=8 and PP=1, the majority of GPU utilization samples exceed 90%, indicating consistently high utilization across devices."
        },
        {
            "title": "B Expert Specialization",
            "content": "Figure 8 shows how expert specialization develops over training in the final layer of FLAME-MoE1.7B-10.3B. Each plot represents an individual experts specialization score trajectory, computed for its top-1 and top-2 most frequently routed tokens. The results indicate that specialization progress is heterogeneous across expertssome develop clear specialization early, while others follow more gradual or fluctuating trajectory. Nonetheless, general upward trend is evident in the majority of experts, particularly for the top-1 and top-2 tokens. This suggests that, over time, most experts become more confidently and consistently associated with subset of tokens, supporting the emergence of functional specialization within the MoE architecture. Expert Co-activation Figure 9 presents expert co-activation heatmaps at 10%, 50%, and 100% of pretraining (corresponding to steps 1,100, 5,500; and 11,029, respectively). These visualizations illustrate the evolution of routing dynamics across training. As pretraining progresses, deeper layers display increasingly sharp coactivation patterns, with only small subset of expert pairs being consistently co-activated. In contrast, co-activation in shallower layers remains more diffuse throughout. Notably, we do not observe global 13 Figure 8: Evolution of expert specialization scores over the course of training in the final layer of FLAME-MoE-1.7B-10.3B. Each subplot tracks the specialization score of one expert across training steps, measured for its top-1 and top-2 most specialized tokens. The score reflects how consistently each expert is selected for specific tokens. or widespread co-activationmost heatmaps are sparsely activated, indicating that the model does not route inputs uniformly across all expert pairs. Figures 10 and 11 illustrate expert co-activation patterns across FLAME-MoE models of different scalesFLAME-MoE-290M-1.3B, FLAME-MoE-721M-3.8B, and FLAME-MoE-1.7B-10.3Bat both shallow and deep layers. The visualizations present pairwise co-activation scores among the 16 most frequently co-activated experts within the first and final Mixture-of-Experts (MoE) layers for each model, respectively. Darker shades in the heatmaps denote stronger co-activation, with expert indices shown along both axes. At shallow layers (e.g., layer 2), larger models exhibit broader and more intense expert co-activation, as seen in the FLAME-MoE-1.7B-10.3B model, which shows darker and more widespread patterns. This suggests that larger models tend to engage more experts early in processing, likely to capture more diverse or complex input features. In contrast, at deeper layers, the co-activation patterns appear relatively similar across model scales, with no pronounced differences, indicating more consistent expert utilization behavior regardless of model size. 14 Figure 9: Evolution of expert co-activation in FLAME-MoE-1.7B-10.3B at 10%, 50%, and 100% of pretraining (steps 1,100, 5,500, and 11,029), shown across layers 2, 6, 12, and 18. Each heatmap visualizes pairwise co-activation frequencies among the 16 most frequently co-activated experts at that layer and training stage. Figure 10: Expert co-activation patterns in FLAME-MoE models of varying scales at the final checkpoint on the validation set. Each heatmap shows pairwise co-activation scores among the 16 most frequently co-activated experts in the first MoE layer."
        },
        {
            "title": "D Router Saturation",
            "content": "Figures 12 and 13 show how router saturation evolves during training across FLAME-MoE models of different scales under top-1 and top-6 routing, respectively. Each line represents the saturation trend of specific MoE layer, measured as the average overlap in expert selection with the final checkpoint. We observe that larger models exhibit slower saturation, particularly in early layers. For example, in FLAME-MoE-1.7B-10.3B under top-1 routing, the saturation in layer 2 (blue line) starts notably lower compared to smaller models, indicating greater degree of route plasticity early in training. 15 Figure 11: Expert co-activation patterns in FLAME-MoE models of varying scales at the final checkpoint on the validation set. Each heatmap shows pairwise co-activation scores among the 16 most frequently co-activated experts in the final MoE layer. Figure 12: Router saturation over the course of training for FLAME-MoE models of different scales under top-1 routing. Each line represents different MoE layer, showing the trend of average expert selection overlap with the final checkpoint. Figure 13: Router saturation over the course of training for FLAME-MoE models of different scales under top-6 routing. Each line represents different MoE layer, showing the trend of average expert selection overlap with the final checkpoint. 16 More broadly, as model scale increases, greater number of shallow layers show delayed saturationevidenced by wider set of lines starting at lower valuessuggesting increased flexibility in routing decisions during early stages of training. These trends imply that larger models maintain more potential for route refinement throughout training, especially in initial layers, which may contribute to their improved capacity for learning complex patterns and adapting expert utilization over time."
        }
    ],
    "affiliations": [
        "Foundation and Language Model Center Carnegie Mellon University",
        "Language Technologies Institute",
        "School of Computer Science"
    ]
}