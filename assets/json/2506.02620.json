{
    "paper_title": "FlexPainter: Flexible and Multi-View Consistent Texture Generation",
    "authors": [
        "Dongyu Yan",
        "Leyi Wu",
        "Jiantao Lin",
        "Luozhou Wang",
        "Tianshuo Xu",
        "Zhifei Chen",
        "Zhen Yang",
        "Lie Xu",
        "Shunsi Zhang",
        "Yingcong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Texture map production is an important part of 3D modeling and determines the rendering quality. Recently, diffusion-based methods have opened a new way for texture generation. However, restricted control flexibility and limited prompt modalities may prevent creators from producing desired results. Furthermore, inconsistencies between generated multi-view images often lead to poor texture generation quality. To address these issues, we introduce \\textbf{FlexPainter}, a novel texture generation pipeline that enables flexible multi-modal conditional guidance and achieves highly consistent texture generation. A shared conditional embedding space is constructed to perform flexible aggregation between different input modalities. Utilizing such embedding space, we present an image-based CFG method to decompose structural and style information, achieving reference image-based stylization. Leveraging the 3D knowledge within the image diffusion prior, we first generate multi-view images simultaneously using a grid representation to enhance global understanding. Meanwhile, we propose a view synchronization and adaptive weighting module during diffusion sampling to further ensure local consistency. Finally, a 3D-aware texture completion model combined with a texture enhancement model is used to generate seamless, high-resolution texture maps. Comprehensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods in both flexibility and generation quality."
        },
        {
            "title": "Start",
            "content": "FlexPainter: Flexible and Multi-View Consistent Texture Generation Dongyu Yan1,, Leyi Wu1,, Jiantao Lin1, Luozhou Wang1, Tianshuo Xu1, Zhifei Chen1, Zhen Yang1, Lie Xu2, Shunsi Zhang2, Yingcong Chen1,3, 1HKUST(GZ), 2Quwan, 3HKUST {starydyxyz@gmail.com; lwu398@connect.hkust-gz.edu.cn; yingcong.ian.chen@gmail.com} 5 2 0 2 3 ] . [ 1 0 2 6 2 0 . 6 0 5 2 : r Figure 1. FlexPainter generates diverse, high-quality textures based on various flexible user prompts."
        },
        {
            "title": "Abstract",
            "content": "Texture map production is an important part of 3D modeling and determines the rendering quality. Recently, diffusionbased methods have opened new way for texture generation. However, restricted control flexibility and limited prompt modalities may prevent creators from producing desired results. Furthermore, inconsistencies between generated multi-view images often lead to poor texture generation quality. To address these issues, we introduce FlexPainter, novel texture generation pipeline that enables flexible multi-modal conditional guidance and achieves highly consistent texture generation. shared conditional embedding space is constructed to perform flexible aggregation between different input modalities. Utilizing such embedding space, we present an image-based CFG method to decompose structural and style information, achieving reference image-based stylization. Leveraging the 3D knowledge within the image diffusion prior, we first generate multi-view images simultaneously using grid representation to enhance global understanding. Meanwhile, we propose view synchronization and adaptive weighting module during diffusion sampling to further ensure local consistency. Finally, 3D-aware texture completion model combined with texture enhancement model is used to generate seamless, high-resolution texture maps. Comprehensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods in both flexibility and generation quality. 1. Introduction Both authors contributed equally to this research. Corresponding Author. Texturing is crucial step in 3D modeling and an important aspect of computer graphics and vision. It has significant applications in games, film, VR/AR, and animation. With the development of diffusion models [16], works utilizing image generation models for texture generation have demonstrated strong potential [712]. However, extending these methods from general image generation to texture creation remains challenging. key challenge in multi-view-based texture generation is precisely controlling the generated results according to user expectations. Most of the existing texture generation methods, like TEXTure [7], Text2Tex [8], and MVPaint [13] use text as the condition. Other works like Paint3D [14] and FlexiTex [15] utilize IP-Adapter [16] to allow image-conditioned generation. Although high-quality texture maps can be generated, these methods only rely on single modality as condition, which may cause ambiguity. For example, text descriptions may fail to convey detailed color, style, or material information accurately. Meanwhile, finding an image condition that perfectly meets the users demand is difficult; mixed information within it may lead to unclear semantic intent and missing contextual details. Differently, StyleTex [17] uses reference style image to generate stylized textures. However, using an additional GPT model for content-style separation increases the systems complexity, and the style feature obtained may preserve unwanted structural information of the input image. Another important challenge of texture generation is ensuring inter-view consistency, as global and local discrepancies can significantly degrade the plausibility and detail quality of the generated texture. Previous works [7, 8] usually generate one viewpoint at time sequentially to paint the mesh. These generated views are independent, which may easily lead to inconsistencies. To overcome such problems, methods like SyncMVD [18] and TexFusion [19] merge different views in UV space during diffusion sampling and achieve view-synchronized generation. Although these methods can produce locally consistent images, there is still no information exchange between non-overlapping views, which causes the Janus problem. To gain holistic information across views, Meta TextureGen [20] and MVPaint [13] leverages multi-view image grid as the generation target. However, after reprojecting into UV space, misalignments in the details may still occur, resulting in ghosting artifacts. Moreover, both types of the above methods employ heuristic weight map to merge partial textures reprojected from different views. Such simple weighting function can cause low robustness and adaptability to different inputs. To address the aforementioned challenges, we introduce FlexPainter, novel texture-generation pipeline with flexible input and consistent generation. Leveraging shared, linear-structured conditional embedding space, we achieve the complementary integration of high-level semantics from texts and low-level details from images. Such multi-granularity control helps generate textures that better meet designers needs in real-world production, reducing the time spent on repeated adjustments. With such embedding space, we further inject image embedding into the classifier-free guidance (CFG) [21] module. Such imagebased CFG not only improves generation quality but also provides flexible control. It can eliminate the structure and content information in the image prompt by feeding grayscaled versions as negative prompts, yielding aesthetic texture stylization. For inconsistency, our method handles it from both global and local perspectives. We apply the multi-view image grid representation as the generation target to gain the model with global understanding. This way, attention between views can ensure holistic understanding of the object, thereby maintaining global coherence. As for the local alignment, we apply view synchronization during diffusion sampling. At each denoising step, we reproject views back to UV space, aggregate them into unified UV map, and then rasterize it into the image grid representation for the next step of generation. We use an additional adaptive WeighterNet to aggregate the partial UV maps. Compared with the traditional heuristic weighting function used in [13, 20, 22], our method can handle various generated inputs at different timesteps and output robust results. Finally, diffusion-based texture completion module and texture enhancement network are applied to obtain the full texture map with high-resolution details. In summary, our contributions are as follows: (1) We propose FlexPainter, carefully designed pipeline for generating high-quality and consistent textures, offering in- (2) We conduct linear operatuitive granularity control. tions and image-based CFG in the conditional embedding space, achieving flexible generation and coherent stylization based on different user demands and prompt modalities. (3) We realize global and local consistent texture generation through unified image grid representation combined with view synchronization and weighting module. 2. Related Work 2.1. Multi-View Generation Multi-view generation tasks aim to produce multiple perspectives of an object with the reference image or text. Zero-1-to-3 [23] and Consistent-1-to-3 [24] generate novel views using viewpoint-conditioned manner. Zero123++ [25], MVDream [26], and other works [2730] manage to generate multi-view grid images simultaneously for better consistency. Some works [3136] incorporate various modules, including video diffusion priors, iterative view synchronization with 3D proxy, and flexible prompt-based input to accomplish the task. Unlike the above methods, which mainly focus on subsequent 3D generation, multi2 view generation for texturing incorporates additional geometry constraints. It requires multi-view information to be projected and merged into UV space to obtain the final texture map, imposing stricter requirements on multi-view consistency. 2.2. Texture Generation Texture generation aims to produce UV texture maps of the target mesh with the given prompts as conditions. Traditional learning-free methods [3740] rely on manual or algorithm-specific procedures to generate textures. Approaches like [4144] performs texture generation based on GAN structure [45, 46]. With the rapid development of diffusion models, their adoption in texture generation has been explored. Approaches like [4751] optimize texture fields based on Score Distillation Sampling (SDS). Other works [710] use pre-trained diffusion models to generate in the image domain and apply predefined rules to fuse images from different views into the UV domain. Condition flexibility. Many texture generation methods attempt to allow multi-modality input to better guide texture generation. TEXTure [7] and TextureDreamer [51] achieve concept transfer using textual inversion [52] and DreamBooth [53] techniques. In addition to basic text input, FlexiTex [15] and Paint3D [14] allows for image conditioning by adopting IP-Adapter [16] module. However, these works still use fixed inputs and cannot integrate multiple conditional controls flexibly. Differently, StyleTex [17] proposes to operate embeddings in the CLIP [54] space to achieve stylized texture generation. While aiming to disentangle content and style, the additional language model introduces overhead and may undesirably preserve structural information from the input image. UV space consistency. Direct back-projection of the diffusion-generated multi-view images may cause misalignments in UV space. To address the UV inconsistencies from different generated views, methods like TexFusion [19], SyncMVD [18], and others [9, 11, 12, 15, 22, 55] share noisy content across views via UV projection during denoising loop. However, these methods struggle to maintain global consistency due to insufficient holistic information, often producing artifacts and Janus Problem. Other works [13, 20, 56] leverage image grid representation for simultaneous view generation. Although attention between views can enhance global consistency, it may lead to local misalignment, affecting aggregated UV results. Point-UV [57], and TEXGen [58] take different approach by directly generating texture maps using point and UV hybrid structures. However, they are trained from scratch, relying on limited 3D datasets, leading to limited generalizability. 3. Preliminaries 3.1. Latent Flow Matching Method The flow matching method [59] is generative method that transforms noise distribution into target distribution. It is modeled by velocity prediction that guides sampling along smooth trajectory in data space. Specifically, the most commonly used rectified flow defines straight line as the path: xt = (1 t)x0 + tϵ, dx dt = vt(xt, Θ), (1) where xt is sample point on the flow trajectory in time step t, x0, ϵ are the destinations from two distributions, and vt represents the velocity field modeled by neural network with parameters Θ. Flow matching-based image generation is typically extended to the latent space to improve scalability and efficiency. To this end, variational autoencoder (VAE) [60] is used for the mapping between image space and latent space: x0 = E(I), = D(x0), (2) where and denotes the encoder and decoder of the VAE, and is sample image in the original space. 3.2. Texture Rasterization and ReProjection Texture rasterization and reprojection are fundamental techniques in computer graphics, enabling mapping between rendered images and UV textures. Rasterization involves projecting 3D model onto 2D image plane, determining visibility, and assigning texture values from UV space to image pixels. In contrast, reprojection involves backprojecting images from certain view onto the 3D object, which reconstructs textures based on known depth and camera information. With camera and texture map , they can be noted as: = R(T, c), = R1(I, c). (3) 4. Method Given 3D object mesh, we aim to generate high-fidelity textures that reflect users creative intent through diverse prompts. We implement prompt aggregation method on multi-modal embedding space to achieve flexible conditioning. We also achieve aesthetic texture stylization with an image-based CFG method (Sec. 4.1). As for the generation pipeline, we use multi-view image grid representation to strengthen global understanding. We propose reprojection-based view synchronization and weighting module during diffusion sampling for better view consistency (Sec. 4.2). Finally, we perform texture completion and enhancement in the UV space to generate complete and detailed results (Sec. 4.3). The full pipeline of our method is shown in Fig. 3. ploying image embeddings yields better conditional control and higher-quality generation results than textual-only embeddings as negative prompts. Moreover, we can utilize images containing undesired structures or specific colors as negative prompts to explicitly suppress their presence in the generated outputs. Building on this property, we further achieve reference image-conditioned texture stylization. Due to the entanglement between content and style, directly using the original reference image as prompt may introduce unintended structural artifacts into the generated texture. To address this, we employ grayscaled version of the reference image as negative prompt to eliminate structural information while preserving stylistic features. This disentanglement of content and style enables more robust and flexible texture stylization, allowing the model to focus on transferring color and tone without copying shape or layout from the reference. 4.2. Generation Pipeline Representation of the multi-view images. We first perform generation in the image space by leveraging the strong priors in current state-of-the-art 2D diffusion models. However, generating single image per sample can cause global inconsistencies due to the lack of communication between perspectives. We adopt image grid representation for synchronized multi-view generation in single sample to facilitate information exchange between different views and maximize the use of 3D prior information from existing image diffusion models. Specifically, we arrange four evenly spaced surround views of the object in two-by-two grid as our generation target. For geometry alignment, depth maps are rendered and concatenated with the noisy color images to form the input to the diffusion network. In particular. we use LoRA [61] to retarget the pre-trained model to generate pure background and coherent multi-view grid images. The image grid representation and simultaneous generation strategy allow attention between views, which can better ensure global understanding and consistency. The 3D knowledge within the models large amount of training data makes it much easier to adapt to the new task. Reprojection-based view synchronization. The use of image grid representation allows us to generate globally consistent multi-views. However, local misalignments can still occur when the images are reprojected into UV space, affecting the overall texture quality. To address this problem, we propose reprojection-based view-synchronization module to align different views in UV space at each sampling step. Specifically, after obtaining the predicted flow vt from the noisy latent xt at denoising step t, we first use it to predict the fully denoised latent xt 0: xt 0 = xt tvt(xt). (5) Figure 2. Example of modality aggregation and semantic manipulation by controlling the image embedding strength α. Each cases first and second rows show text-to-image and image-to-image interpolation, respectively. 4.1. Flexible Conditioning Aggregation of multi-modal prompts. Accommodating multi-modal inputs to achieve desired results flexibly is challenging topic. To enable text, images, and their interactions as conditions, we need shared embedding space to align the information from each modality. Therefore, we use separated image and text embedders within the same embedding space to process inputs of different modalities. By concatenating the output embeddings, unified condition can be formed to guide the generation process using cross-attention. Moreover, since our embedding space retains the superior properties of linear space, we can further manipulate the strength and direction of the image embeddings to provide more flexible control. The final embedding for the model can be obtained through the following equation: = (wcond) (cid:77) (cid:88) αiI(I cond), (4) i=1 where and represents the text and image embedder respectively, wcond and cond are the text and i-th image condition. Such manipulation in embedding space results in meaningful generations, as illustrated in Fig. 2. Even with single-modal training, the linear structure and combination property can be well preserved, allowing advanced tasks such as text-assisted image prompt refinement and generation using reference-style images to be effectively implemented without special design. Image-based CFG Traditional CFG is usually carried out using blank text, which lacks strong and precise control. Utilizing the above multi-modal embedding space, we can inject image information into the diffusion network to enhance its generative capabilities. Furthermore, we incorporate image embeddings into the negative prompts used in the CFG framework [21] to improve generation control. Em4 Figure 3. Pipeline of our method. We first generate multi-view images using the conditional input from the user. The top two cases show the generation of using text-only or image-only conditions. Leveraging the linear operation in Equ.(4), we can also perform text-guided image refinement (shown in green) and stylization using reference image (shown in blue). The right side shows our view-synchronization and weighting module. Consistent multi-view images can be generated by reprojection and weighting during each sampling step. Then, the latent xt 0 is decoded, split into independent images, and reprojected into UV space, producing four partial texture maps 1:4: 1:4 = S(D(xt , ci), 0)), = R1(I i = 1, 2, 3, 4, (6) where denotes splitting operation that turns grid image into individuals images 1:4, and ci represents the i-th camera. Next, weighting function is applied on the partial texture maps 1:4 to generate the fused texture map t: = W(T 1:4). (7) From t, the view-synchronized latent xt through operations inverse to the above process: 0 can be obtained 1:4 = R( t, c1:4), 0 = E(F( xt 1:4)). (8) Finally, view-synchronized flow prediction vt is computed to replace the original vt, and the subsequent sampling steps continues: vt = xt xt 0 . (9) This reprojection-based view synchronization ensures that, at each diffusion step, the multi-view images to be denoised are derived from the same UV map. This way, the consistency between views can be further maintained. Adaptive Weighting. Although multi-view consistency can be ensured using view synchronization, the generation quality can be greatly affected by the choice of the weighting function W. Traditionally, the weights assigned to partial textures from different views are determined heuristically, using the cosine of the angle between the camera rays and the rendered normals of the mesh as the weighting factor. While such weighting function provides simple way to detect observation quality, it can not dynamically adjust to different input variations. It may introduce poorly generated regions into the final result. Therefore, we train WeighterNet as the weighting function W, allowing it to discern generation quality and achieve robust weighting results. Due to the discontinuities in UV space, incorporating 3D structures to enhance the networks spatial awareness is beneficial. Thus, we combine the UV blocks and point blocks by the mapping between 2D and 3D space. The WeighterNet also considers the input variation of different time steps by taking an additional as input. The final format can be represented as: = W(T 1:4, R1:4, N1:4, P, t, Θ), (10) where represents the position map in the UV space. We use ground truth textures and its simulated denoised texture at time step as training pair. For supervision, perceptual loss and cycle loss are used: Lpec =eαt 4 (cid:88) i=1 vgg(R( , ci)) vgg(R(W(T 1:4, Θ), ci)), Lcyc = 4 (cid:88) i=1 5 R(T i , ci) R(W(T 1:4, Θ), ci), (11) where eαt balances the penalties for different noise levels. As result, our adaptive weighting function can provide adaptable view aggregation in training-based manner. Table 1. Quantitative evaluation result of text-to-texture generation. The best results are in bold, while the second-best ones are underlined. KID multiplied by 104. 4.3. Texture Completion and Enhancement We generate and blend the multi-view images in the previous stage, resulting in single UV map 0. Although most of the object surface is covered, some regions still require texture completion due to occlusion. straightforward approach is to use an image diffusion model to perform texture inpainting in the UV space. However, due to the discontinuity inherent in UV space, this method lacks an understanding of the spatial relationships, potentially leading to suboptimal results. Leveraging the 3D-aware architecture of TEXGen [58], we finetune it with our aggregated four-view UV map. Due to the explicitly introduced 3D knowledge and the texel-aligned input, our texture completion network produces reasonable and spatially correlated inpainting in the UV space. Although our method can generate high-quality, 1K (10242 pixels) resolution texture maps through the above pipeline, some scenarios require higher resolution for more detailed rendering. To address this limitation, we perform super-resolution on the generated texture maps. We finetune Real-ESRGAN [62] model using our baked texture maps to perform 4 times up-scale. At can be produced. At last, precise and detailed texture maps at resolution of 4K (40962 pixels) 5. Experiment We conduct qualitative and quantitative experiments to verify the superiority of our method. Ablation studies are also performed to prove the effectiveness of our proposed modules. Furthermore, we demonstrate examples of our method in practical applications to illustrate its real-world significance. 5.1. Implementation Details We choose FLUX.1-dev [6] as our base generation model and utilize LoRA [61] to retarget it for multi-view image grid generation. We use T5 [63] text encoder and Redux [6] image encoder to align multi-modal information in shared embedding space. For the WeighterNet, we use Vision Transformer (ViT) [64] combined with Point Transformer [65] to achieve UV and spatial perception. For texture completion and enhancement, we leverage the inpainting capabilities of TEXGen [58] and finetune it on our synthesized partial texture data. We then deploy Real-ESRGAN [62] finetuned on the UV map dataset to achieve texture refinement and super-resolution. Data preparation. We select 100K 3D objects from the Objaverse dataset [66] as our training set. During training, FID 78.269 TEXTure Text2Tex 89.008 SyncMVD 73.385 77.801 72.900 71.621 Paint3D TEXGen Ours KID 107.062 94.423 41.737 123.605 61.729 58.465 User Preference 4.4% 11.3% 22.5% 17.8% 15.7% 28.3% Table 2. Quantitative evaluation result of image-to-texture generation. The best results are indicated in bold. KID multiplied by 104. FID Paint3D 83.977 59.492 Ours KID 267.132 62.089 User Preference 28.6% 71.4% we randomly select rendered images and GPT-4V labeled caption as prompt. Four surrounding, relatively fixed depth and albedo images are also rendered as training data. For WeighterNet training, we simulate denoised texture maps at time step by first adding noise on the ground-truth rendered views to get xt. Then, we can use Equ. (5) and (6) to get the simulate as training input. choose TEXTure Baselines and metrics. We [7], Text2Tex [8], SyncMVD [18], Paint3D [14] and TEXGen [58] as our comparison baseline In particular, we use depth-controlled Flux [6] model to generate the meshaligned front view required by TEXGen to ensure fairness. We randomly select 100 unseen 3D objects from Google Scanned Objects (GSO) [67] as the evaluation dataset to better demonstrate our generalization capabilities. Common metrics, including FID [68] and KID [69], are used for quantitative comparison. To further prove our effectiveness, we conduct user study by asking users to select the texture generation result with the highest quality that best matches the text or image prompt. The results are represented as the proportion of user preference. 5.2. Comparisons on Text-to-Texture We conduct experiments on text-to-texture generation. We only use text as the condition for this task and set all αi to 0. The results are shown in Tab. 1 and Fig. 4. Our model achieves the best FID score and user preference in the quantitative comparison. The qualitative experiments show that our model can produce text-coherent results without ghost artifacts. 6 Figure 4. Qualitative results on text-to-texture generation. 5.4. Ablation Studies We conduct ablation studies to demonstrate the effect of our view synchronization, WeighterNet (replaced by simple cosine weight), and the image-based CFG (replaced by normal text-based CFG) modules. Quantitative and qualitative results can be seen in Table 3 and Fig. 7, respectively. Although our multi-view generation model may still exhibit local misalignment in the UV space, our synchronization strategy and the 3D-aware weighting network can solve such problem and generate flawless texture maps. Due to multiple encode-decode operations during the sampling process, the original text-based CFG can lead to decline in generation quality. Meanwhile, our image-based CFG method can enhance condition consistency and generate higher-quality results. 5.5. Applications We show the applications of our method, including tasks of text-to-texture, image-to-texture, text-guided image refinement, and stylization using reference image. The results are shown in Fig. 6. These results show that our method can cope with different user needs with different prompts in actual production and flexibly generate high-quality textures that meet user intentions. 6. Conclusion We introduce FlexPainter, novel framework for generating high-quality, consistent texture maps with flexible generation. We achieve multi-modality mixing and flexible conditional control by constructing shared semantic space. The proposed image-based CFG also enables structure supFigure 5. Qualitative results on image-to-texture generation. 5.3. Comparisons of Image-to-Texture We conduct quantitative experiments on image-to-texture generation using the rendered images as input. We use fixed trigger words as text prompts and set α0 = 1 for the image embedding. The results presented in Tab. 2 show that our method significantly outperforms others, indicating better image understanding and stronger generative capabilities. For the qualitative experiments, we use imperfectly-aligned, in-the-wild images as conditions to test the generalizability of our method. From the results shown in Fig. 5, we can tell that our method can infer semantic correspondences from the image information to generate results that meet both image and geometry conditions. 7 Figure 6. Our Applications includes tasks of text-to-texture, image-to-texture, text-guided image refinement, and reference image-based stylization."
        },
        {
            "title": "References",
            "content": "[1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [2] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [3] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [4] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. [5] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [6] Black-Forest-Labs. Flux. https://github.com/ black-forest-labs/flux, 2023. 2, 6, 14 [7] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. 2, 3, 6 [8] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nießner. Text2tex: Text-driven texIn Proceedings of the ture synthesis via diffusion models. Figure 7. Qualitative results of our ablation study. Our full model achieves consistent and high-quality generation results, while the ablated methods suffer from ghosting artifacts and degraded quality. Table 3. Quantitative ablation studies on our view synchronization, WeighterNet, and image-based CFG. The best results are indicated in bold. KID multiplied by 104. w/o view-sync 80.635 77.883 w/o WeighterNet 76.688 59.674 w/o image CFG 79.041 65.482 our full model 71.621 58.465 FID KID pression of the reference image and achieves high-fidelity stylization. For texture coherency, we use multi-view image grid as generation target for global understanding. The reprojection-based view synchronization and weighting module enhances multi-view alignment for local consistency. Finally, texture completion and enhancement module produces high-resolution textures. Extensive experiments show that FlexPainter outperforms existing methods, offering superior semantic alignment and multi-view consistency, making it practical solution for 3D artists and designers. Finally, texture completion and enhancement module generates full high-resolution textures, significantly improving the visual quality and detail of the final output. 8 IEEE/CVF International Conference on Computer Vision, pages 1855818568, 2023. 2, 6 [9] Jinbo Wu, Xing Liu, Chenming Wu, Xiaobo Gao, Jialun Liu, Xinqi Liu, Chen Zhao, Haocheng Feng, Errui Ding, and Jingdong Wang. Texro: Generating delicate textures arXiv preprint of 3d models by recursive optimization. arXiv:2403.15009, 2024. 3 [10] Xiaoyu Xiang, Liat Sless Gorelik, Yuchen Fan, Omri Armstrong, Forrest Iandola, Yilei Li, Ita Lifshitz, and Rakesh Ranjan. Make-a-texture: Fast shape-aware texture generation in 3 seconds. arXiv preprint arXiv:2412.07766, 2024. 3 [11] Chenjian Gao, Boyan Jiang, Xinghui Li, Yingpeng Zhang, and Qian Yu. Genesistex: Adapting image denoising diffusion to texture space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 46204629, 2024. 3 [12] Dong Huo, Zixin Guo, Xinxin Zuo, Zhihao Shi, Juwei Lu, Peng Dai, Songcen Xu, Li Cheng, and Yee-Hong Yang. Texgen: Text-guided 3d texture generation with multi-view sampling and resampling. In European Conference on Computer Vision, pages 352368. Springer, 2025. 2, [13] Wei Cheng, Juncheng Mu, Xianfang Zeng, Xin Chen, Anqi Pang, Chi Zhang, Zhibin Wang, Bin Fu, Gang Yu, Ziwei Liu, et al. Mvpaint: Synchronized multi-view diffusion for painting anything 3d. arXiv preprint arXiv:2411.02336, 2024. 2, 3 [14] Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, and Gang Yu. Paint3d: Paint anything 3d with lighting-less texture diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42524262, 2024. 2, 3, 6 [15] DaDong Jiang, Xianghui Yang, Zibo Zhao, Sheng Zhang, Jiaao Yu, Zeqiang Lai, Shaoxiong Yang, Chunchao Guo, Xiaobo Zhou, and Zhihui Ke. Flexitex: Enhancing texarXiv preprint ture generation with visual guidance. arXiv:2409.12431, 2024. 2, 3 [16] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3 [17] Zhiyu Xie, Yuqing Zhang, Xiangjun Tang, Yiqian Wu, Dehan Chen, Gongsheng Li, and Xiaogang Jin. Styletex: Style image-guided texture generation for 3d models. ACM Transactions on Graphics (TOG), 43(6):114, 2024. 2, 3 [18] Yuxin Liu, Minshan Xie, Hanyuan Liu, and Tien-Tsin Wong. Text-guided texturing by synchronized multi-view diffusion. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2, 3, [19] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, and Kangxue Yin. Texfusion: Synthesizing 3d textures with text-guided image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41694181, 2023. 2, 3 [20] Raphael Bensadoun, Yanir Kleiman, Idan Azuri, Omri Harosh, Andrea Vedaldi, Natalia Neverova, and Oran Gafni. Meta 3d texturegen: Fast and consistent texture generation for 3d objects. arXiv preprint arXiv:2407.02430, 2024. 2, 3 [21] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, 4, 14 [22] Hongkun Zhang, Zherong Pan, Congyi Zhang, Lifeng Zhu, and Xifeng Gao. Texpainter: Generative mesh texturing with multi-view consistency. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2, [23] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 2 [24] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng Wang. Consistent-1-to-3: Consistent image to 3d view synthesis via geometry-aware diffusion models. In 2024 International Conference on 3D Vision (3DV), pages 664674. IEEE, 2024. 2 [25] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 2 [26] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 2 [27] Sixu Li, Chaojian Li, Wenbo Zhu, Boyang Yu, Yang Zhao, Cheng Wan, Haoran You, Huihong Shi, and Yingyan Lin. Instant-3d: Instant neural radiance field training towards onIn Proceedings of the 50th device ar/vr 3d reconstruction. Annual International Symposium on Computer Architecture, pages 113, 2023. 2 [28] Peng Wang and Yichun Shi. multi-view diffusion for 3d generation. arXiv:2312.02201, 2023. Imagedream: Image-prompt arXiv preprint [29] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99709980, 2024. [30] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2 [31] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2025. 2 [32] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. arXiv preprint arXiv:2311.09217, 2023. 9 [33] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, and Filippos Kokkinos. Iterative multiview diffusion and reconstruction for high-quality 3d generation. arXiv preprint arXiv:2402.08682, 2024. Im-3d: [34] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. [35] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong Zhang, CL Chen, and Lei Zhang. Consistent123: Improve consistency for one image to 3d object synthesis. arXiv preprint arXiv:2310.08092, 2023. [36] Xinli Xu, Wenhang Ge, Jiantao Lin, Jiawei Feng, Lie Xu, HanFeng Zhao, Shunsi Zhang, and Ying-Cong Chen. Flexgen: Flexible multi-view generation from text and image inputs. arXiv preprint arXiv:2410.10745, 2024. [37] Johannes Kopf, Chi-Wing Fu, Daniel Cohen-Or, Oliver Deussen, Dani Lischinski, and Tien-Tsin Wong. Solid texture synthesis from 2d exemplars. In ACM SIGGRAPH 2007 papers, pages 2es. 2007. 3 [38] Sylvain Lefebvre and Hugues Hoppe. Appearance-space texture synthesis. ACM Transactions on Graphics (TOG), 25(3):541548, 2006. [39] Greg Turk. Texture synthesis on surfaces. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 347354, 2001. [40] Li-Yi Wei, Sylvain Lefebvre, Vivek Kwatra, and Greg Turk. State of the art in example-based texture synthesis. Eurographics 2009, State of the Art Report, EG-STAR, pages 93 117, 2009. 3 [41] Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, and Andreas Geiger. Texture fields: Learning texture representations in function space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 45314540, 2019. 3 [42] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan, Matthias Nießner, and Angela Dai. Texturify: Generating textures on 3d shape surfaces. In European Conference on Computer Vision, pages 7288. Springer, 2022. [43] Alexey Bokhovkin, Shubham Tulsiani, and Angela Dai. Mesh2tex: Generating mesh textures from image queries. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 89188928, 2023. [44] An-Chieh Cheng, Xueting Li, Sifei Liu, and Xiaolong Wang. Tuvf: Learning generalizable texture uv radiance fields. arXiv preprint arXiv:2305.03040, 2023. 3 [45] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 3 [46] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improvIn Proceedings of ing the image quality of stylegan. the IEEE/CVF conference on computer vision and pattern recognition, pages 81108119, 2020. 3 [47] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [48] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300309, 2023. [49] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highIn Proceedings of the quality text-to-3d content creation. IEEE/CVF international conference on computer vision, pages 2224622256, 2023. [50] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1266312673, 2023. [51] Yu-Ying Yeh, Jia-Bin Huang, Changil Kim, Lei Xiao, Thu Nguyen-Phuoc, Numair Khan, Cheng Zhang, Manmohan Chandraker, Carl Marshall, Zhao Dong, et al. Texturedreamer: Image-guided texture synthesis through geometryaware diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43044314, 2024. 3 [52] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 [53] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 3 [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [55] Shang Liu, Chaohui Yu, Chenjie Cao, Wen Qian, and Fan Wang. Vcd-texture: Variance alignment based 3d-2d codenoising for text-guided texturing. In European Conference on Computer Vision, pages 373389. Springer, 2024. 3 [56] Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, and Maneesh Agrawala. Flashtex: Fast relightable mesh texturing with In European Conference on Computer Vilightcontrolnet. sion, pages 90107. Springer, 2025. 3 [57] Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and Xiaojuan Qi. Texture generation on 3d meshes with pointuv diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42064216, 2023. 3 [58] Xin Yu, Ze Yuan, Yuan-Chen Guo, Ying-Tian Liu, Jianhui Liu, Yangguang Li, Yan-Pei Cao, Ding Liang, and Xiaojuan 10 Qi. Texgen: generative diffusion model for mesh textures. ACM Transactions on Graphics (TOG), 43(6):114, 2024. 3, 6, 14 [59] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [60] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. 3 [61] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 4, 6 [62] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In Proceedings of the IEEE/CVF international conference on computer vision, pages 19051914, 2021. 6, 14 [63] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 6 [64] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 6 [65] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler faster stronger. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48404851, 2024. [66] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 6, 14 [67] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: highquality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 25532560. IEEE, 2022. 6 [68] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [69] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018. 6 [70] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 11 Figure 8. More cases of our text-to-texture generation. Figure 9. More cases of our imgae-to-texture generation. 12 Figure 10. More cases of our applications. Supplementary Material for FlexPainter: Flexible and Multi-View Consistent Texture Generation A. Additional Implementation Details A.1. Baking Texture When preparing for texture data, we found one problem in Objaverse[66]. Instead of single unified texture, some meshes are composed of multiple parts, each with its texture image. For those meshes, we use Blenders Smart UV Project function to re-unfold the UVs and then bake the diffuse color to get unified texture image. For the texture completion net, we bake 10K texture maps at 1K (10242 pixels) resolution; for the texture enhancement net, we bake the same texture maps at 4K (40962 pixels) resolution. A.2. Training of LoRA. During LoRA training, we use Adam [70] optimizer with learning rate of 105. The LoRA rank is set to 256. We train the model for 20K steps with 8 NVIDIA-A800 GPUs and batch size of 4. We use grid of 2x2 multi-view image. white background. for the trigger word. A.3. Training of WeighterNet. For the training of WeighterNet, we use Adam optimizer with learning rate of 104. The weight of the supervision loss are set to: λpec = 1, λcyc = 0.5. Additionally, smooth loss Lsm with weight λsm = 0.2 is used to regularize the training process: (cid:88) Lsm = ((xI)2 i,j + (yI) i,j), (12) and the full supervision loss is set to: = λpercLperc + λcycLcyc + λsmLsm (13) baked texture maps, we generate corresponding low-quality images with the degradation process described in RealESRGAN[62]. During training, we use Adam [70] optimizer with learning rate of 104. Additionally, we utilize L1 Loss, GAN Loss, and Perceptual Loss for pixel level, content level, and style level supervision: = λpercLperc + λpLp + λgLg (14) where λperc, λp, λg are set to 1.0, 1.0, 0.1 respectively. We train the model for 400K step with 1 NVIDIA-A800 GPU and batchsize of 12. Detailed pipeline of the texture completion and enhancement module is shown in Fig.S1. A.5. Implementaion of CFG Since the original checkpoint from FLUX.1-dev [6] uses distilled CFG mechanism, we implement an explicit CFG method following [21] during sampling. For all of our experiments, the distilled CFG scale is set to 6, and the explicit CFG scale is set to 2. We use the embedding of white image as the negative embedding for the explicit CFG. B. More Qualitative Results Our main paper introduces various applications with our model, including text-to-texture, image-to-texture, and texture stylization. We demonstrate more results in Fig.S2, Fig.S3. Additionally in Fig.S4 to Fig. Fig.S6, we detailed conditions and generation results shown in the teaser of the main paper. Also, we attached video to the supplementary file to present the texture results using dynamic approach. We train the model for 100K step with 8 NVIDIA-A800 GPU and batchsize of 2. C. Details of our user study A.4. Training of Completion and Enhancement Net. For texture completion net, we start from the checkpoint provided by TEXGen [58]. However, since the original TEXGen is trained using partial texture map reprojected from single view, direct extension into larger partial UV map from four views may result in domain variance. To this end, we generate partial texture map using our four-view camera setting and finetune the original model. We use an Adam optimizer with learning rate of 104 and train the model for 20K steps on 8 NVIDIA-A800 GPUs with batch size 2. For choose Realenhancement net, we ESRGAN[62] as our base model. Given 4K (4096 4096) texture In our user study, we use 120 cases for text-to-texture generation and 50 cases for image-to-texture generation. We shuffle all the cases and separate them randomly into 6 questionnaires and distribute them to 100 interviewees. We showcase examples of our questionnaires from Fig.S7 to Fig.S12. Notice that the cases are shown in video. D. Limitations and Future Work Although our model has already achieved superior results, some challenges remain to be solved. While utilizing depth maps as geometric conditions is effective, it often results in the loss of geometric detail from the original mesh. This discrepancy can lead to the generation results in Stage 14 Figure S1. Detailed pipeline of the texture completion and enhancement module. We use 3D-aware texture completion module containing UV blocks and point blocks to complete the partial texture. Then, we use texture enhancement module to generate the super-resolution results. not fully aligning with the original mesh. Moreover, we are still exploring whether explicit 3D information can be used as guidance to steer the inpainting process in Stage II. Due to the albedo data used during training, our generated textures are free from ambient lighting. However, we have observed that applying these textures in practical 3D generation pipelines still requires designers to spend considerable time adjusting the lighting. Therefore, our future work will explore methods to generate textures that align with designers expected lighting effects to achieve more flexible, thereby reducing design time. 15 Figure S2. More cases of our text-to-texture generation. Figure S3. More cases of our image-to-texture generation. Figure S4. More cases of our applications. 17 Figure S5. More cases of our applications. Figure S6. More cases of our applications. Figure S7. Our user study interface. Figure S8. Our user study interface. 19 Figure S9. Our user study interface. Figure S10. Our user study interface. Figure S11. Our user study interface. Figure S12. Our user study interface."
        }
    ],
    "affiliations": [
        "HKUST",
        "HKUST(GZ)",
        "Quwan"
    ]
}