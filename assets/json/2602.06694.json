{
    "paper_title": "NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models",
    "authors": [
        "Hyochan Chong",
        "Dongkyu Kim",
        "Changdong Kim",
        "Minseop Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8$\\times$ in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 4 9 6 6 0 . 2 0 6 2 : r NANOQUANT: Efficient Sub-1-Bit Quantization of Large Language Models Hyochan Chong * 1 Dongkyu Kim * 1 Changdong Kim 1 Minseop Choi 1 Abstract Weight-only quantization has become standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NANOQUANT, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NANOQUANT formulates quantization as low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through block and model reconstruction process. Consequently, NANOQUANT establishes new Pareto frontier in low-memory post-training quantization, achieving state-of-theart accuracy even at sub-1-bit compression rates. NANOQUANT makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8 in just 13 hours on single H100, enabling 70B model to operate on consumer 8 GB GPU. 1. Introduction Large language models (LLMs) have demonstrated remarkable performance across wide variety of tasks. However, their extremely large size makes deployment costly. Weightonly quantization offers standard route to alleviate these bottlenecks (Frantar et al., 2022; Lin et al., 2024; Shao et al., 2023; Liu et al., 2024). This has led to its widespread adoption within production-grade inference engines, such as vLLM (Kwon et al., 2023) and SGLang (Zheng et al., 2024). Recent post-training quantization (PTQ) efforts have suc- *Equal contribution 1Samsung Research, Seoul, Korea. Correspondence to: Dongkyu Kim <dongkyu.k@samsung.com>. Preprint. February 9, 2026. 1 Table 1. Comparison of LLM quantization frameworks. Methods are categorized by quantization scheme (PTQ vs. QAT), scalability to 70B+ models, and sub-1-bit capability. NANOQUANT is the only PTQ method enabling sub-1-bit compression. Quantization Method Scheme Compression 70B+ LLMs 1-Bit Sub-1-Bit BiLLM (Huang et al., 2024) STBLLM (Dong et al., 2024) ARB-LLM (Li et al., 2024) HB-LLM (Chen et al., 2025b) OneBit (Xu et al., 2024) BinaryMoS (Jo et al., 2024) DBF (Boža & Macko, 2025) ParetoQ (Liu et al., 2025) LittleBit (Lee et al., 2025) NANOQUANT (Ours) PTQ PTQ PTQ PTQ QAT QAT QAT QAT QAT PTQ cessfully pushed weight compression toward 2-bit (Chee et al., 2023; Tseng et al., 2024b) and even 1-bit (Huang et al., 2024; Li et al., 2024; Chen et al., 2025b). However, breaking the sub-1-bit barrier remains challenge for current PTQ frameworks for two distinct reasons. First, current binary PTQ methods utilize in-place binarization with fullprecision scales (e.g., αB1), an approach that is structurally bounded by minimum of 1 bit per parameter. Moreover, these techniques require complex weightgrouping metadata (Huang et al., 2024; Dong et al., 2024; Zhao et al., 2025; Chen et al., 2025b), resulting in effective bitrates to exceed 2 and even 3 bits (Dong et al., 2024). Thus, key challenge for sub-1-bit PTQ is to efficiently represent model parameters, to overcome both the structural and storage limitations of current methods. In contrast, binary quantization-aware training (QAT) methods successfully compress LLMs to binary (1-bit) and even sub-1-bit levels with low-rank representations (Boža & Macko, 2025; Lee et al., 2025), overcoming both the structural and additional storage limitations inherent in binary PTQ methods. Through an end-to-end training process, such binary QAT methods replace linear layer weights with compact, low-rank binary matrices and scales. However, unlike PTQ methods, these QAT methods require hundreds of millions or billions of tokens, and utilize multiple GPUs over multiple days. These demands are impractical for resourceconstrained environments and limit such QAT methods from compressing larger, 70B parameter models. Therefore, deriving data-efficient and compute-efficient sub-1-bit PTQ method remains an open challenge. NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models Figure 1. Perplexity comparison on WikiText-2. NANOQUANT achieves state-of-the-art results among post-training quantization (PTQ) methods and is the only framework effectively enabling sub-1-bit compression while outperforming existing binary baselines. To bridge the gap between binary PTQ and QAT methods, we propose NANOQUANT, an efficient and accurate PTQ method that can compress LLMs to sub-1-bit levels. By directly addressing multiple shortcomings of conventional post-training quantization methods, NANOQUANT precisely initializes latent binary matrices and scales via robust Hessian-aware alternating direction method of multipliers (ADMM). Then, NANOQUANT utilizes hierarchical reconstruction pipeline that optimizes parameters at the block level, and subsequently calibrates scaling factors at the model level for enhanced global activation alignment. With only 128 calibration samples (0.26M tokens) and 1 GPU, NANOQUANT is the first PTQ method to achieve subbinary compression and shows state-of-the-art performance in low-memory regimes. NANOQUANT enables compressing 70B LLM from 138.04 GB to 5.35 GB using 1 GPU, and running the quantized 70B LLM on consumer 8GB GPU at up to 20.11 tokens per second, making LLM compression and inference accessible in resource-constrained environments. Contributions. Our main contributions are as follows: We propose NANOQUANT, the first post-training quantization (PTQ) method to compress LLMs to both 1-bit and sub-1-bit levels. This approach addresses the structural limitations of existing binary quantization frameworks. We provide theoretical justification for our initialization strategy, confirming that precise low-rank binary matrix initialization is critical factor for establishing new sub-1-bit quantization frontier. We conduct extensive experiments across diverse model families and language tasks, demonstrating that NANOQUANT achieves competitive performance with higher-bit PTQ and binary quantization-aware training (QAT) methods, despite using limited calibration data. We implement custom binary GEMV and GEMM CUDA kernels for NANOQUANT, enabling significantly faster inference throughput, reduced memory footprints, and enhanced energy efficiency for datacenter GPUs, consumer GPUs, and edge devices. 2. Related Work Binary Post-Training Quantization. State-of-the-art binary post-training quantization (PTQ) methods often adopt in-place binarization and full-precision scales to preserve the sign and magnitude of weights, respectively (Huang et al., 2024; Li et al., 2024; Chen et al., 2025b). Other methods introduce sparsity to further reduce the memory footprint of binary weights (Dong et al., 2024). However, although such methods show respectable performance, these binary PTQ algorithms incur additional storage requirements (e.g. scaling factors and grouping bit-masks), causing them to fall short of their intended binary compression rates, requiring at least 2 or 3 bits per weight (Huang et al., 2024; Li et al., 2024; Zhao et al., 2025; Chen et al., 2025b). Binary Quantization-Aware Training. In contrast, binary quantization-aware training (QAT) methods successfully reach binary compression rates through end-to-end training on larger datasets. Many previous binary QAT methods utilize in-place binarization to compress LLM weights to binary levels (Wang et al., 2023; Xu et al., 2024; Jo et al., 2024; Liu et al., 2025). More recent methods compress weights to low-rank binary matrices to reach binary and sub-1-bit compression levels (Boža & Macko, 2025; Lee et al., 2025). However, although such low-rank binary methods display promising performance, they require copious amounts of data and compute, requiring multiple GPUs for multiple days to train on hundreds of millions or billions of tokens. Such resource demands have also limited these methods to binarize only relatively smaller models, such as Llama2-7B. 3. NANOQUANT This section introduces NANOQUANT, post-training quantization (PTQ) method capable of compressing LLM 2 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models Figure 2. Illustration of the NANOQUANT compression scheme. The process proceeds in three stages: (a) Factorization, where the weight matrix is decomposed into continuous latent factors (UFP, VFP) and floating-point scales (s1, s2) which are fine-tuned to minimize reconstruction error; (b) Binarization, where these optimized factors are quantized into binary matrices (U1, V1) containing {1, +1} values; and (c) Packing, where these values are mapped to bits (1 0, +1 1) and efficiently packed into integer formats (e.g., 8-bit blocks) for memory efficiency. weights to sub-1-bit levels. NANOQUANT derives highfidelity low-rank binary representations by integrating precise initialization subroutine directly into block-wise reconstruction loop, followed by lightweight global calibration. 3.1. Quantization Scheme We formulate sub-1-bit weight compression as low-rank binary factorization problem. Let = {1, +1} denote the set of binary values. For each linear layer weight Rdoutdin in the transformer, we approximate the dense matrix using two low-rank binary matrices U1 Bdoutr and V1 Bdinr, alongside two full-precision scaling vectors: an output channel scale s1 Rdout and an input channel scale s2 Rdin . The decomposition structure is defined as cW = s1 (U1V 1) 2 , (1) where denotes element-wise multiplication with broadcasting. Figure 2 visualizes this scheme, illustrating how the dense weight matrix decomposes into continuous latent factors and scales before binarization and packing. Direct optimization of binary parameters constitutes nonconvex, combinatorial problem that is NP-hard (Froese & Hertrich, 2023). To address this within strict PTQ budget, NANOQUANT employs sequential block reconstruction pipeline that incorporates precise initialization and latent optimization. 3.2. Block Reconstruction Pipeline We sequentially compress each linear layer in each transformer block. Unlike methods that treat initialization as separate pre-processing phase, NANOQUANT integrates initialization as subroutine within the block reconstruction loop. As depicted in Figure 3, each block undergoes threestep optimization process: (1) error propagation mitigation, (2) low-rank binary initialization via ADMM and magnitude Figure 3. The NANOQUANT block reconstruction pipeline for compressing linear layers. The process sequentially optimizes each transformer block through three key phases: (1) Error Propagation Mitigation to adjust full-precision weights for accumulated errors; (2) Low-Rank Binary Initialization, which utilizes Latent Binary ADMM (LB-ADMM) to precisely generate latent binary factors and scales; and (3) Factorized Component Refinement., which fine-tunes the continuous latent matrices and scales using Straight-Through Estimators (STE) before final packing. balancing, and (3) factorized component refinement. Step 1: Error Propagation Mitigation. Quantization error accumulates as the reconstruction proceeds through the network (Frantar et al., 2022). We tune the full-precision weights of the current block to minimize the error introduced by the quantization of preceding blocks, as well as previously factorized layers in the current block. This comprehensive strategy is in line with recent quantization methods that adopt this method for either some (Boža & Macko, 2025) or all (Tseng et al., 2024a; Egiazarian et al., 2024; Arai & Ichikawa, 2025) linear layers, and NANOQUANT falls in the latter. Step 2: Low-Rank Binary Initialization. Because PTQ relies on small calibration set, the stability of initialization is critical (Hubara et al., 2021; Nagel et al., 2020). We initialize the low-rank binary parameters and scales through an activation-aware process involving preconditioning, factorization via alternating direction method of multipliers (ADMM), and magnitude balancing. Step 2-1: Hessian-Aware Preconditioning. To minimize quantization error, we adopt the formulation from DBF (Boža & Macko, 2025) and consider the secondorder Taylor expansion of the task loss. The objective minimizes the Hessian-weighted distortion approximated via Kronecker-factored approximate curvature (K3 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models FAC) (Martens & Grosse, 2015): L(cW) eDout(W cW) eDin2 . (2) Here, eDin and eDout are diagonal preconditioners constructed from activation and gradient statistics. These values are computed during global calibration phase prior to the block-wise reconstruction loop, as outlined in Algorithm 1. Given limited calibration data, empirical estimates can be sensitive to outliers. To mitigate this, we employ shrinkage (Ledoit & Wolf, 2004) regularization on the diagonal entries, [ eD()]ii (1 γ)[D()]ii + γ mean(D()). (3) The shrinkage coefficient γ [0, 1] plays pivotal role in regulating the trade-off between preserving feature-specific curvature information and maintaining global robustness against calibration noise. We empirically find that smaller values (e.g.,, 0.2) are optimal for Llama and Qwen models, and larger values (e.g.,, 0.6) are optimal for Gemma and Rnj models. Step 2-2: Latent Binary Factorization (LB-ADMM). We formulate initialization as finding factors and that approximate the preconditioned target fWtarget. To handle the non-convex landscape, we employ ADMM with ridge regularization λ, introducing auxiliary variables and scaled dual variables Λ to decouple constraints. The optimization problem is defined as: Algorithm 1 The NANOQUANT algorithm. Input: FP teacher M, calib set , rank r, (τ, γ), (K, ρ, ϵ), (Tpre, Tpost, Tglob) Output: cM with packed binaries {U(ℓ) 1, V(ℓ) 1} and float scales {s(ℓ) 1 , s(ℓ) 2 } 1: cM 2: # Phase 1: Global Calibration 3: for each linear layer ℓ do eD(ℓ) 4: 5: end for in , eD(ℓ) out ROBUSTDIAG(z(ℓ) in , z(ℓ) out; τ, γ) Step 1: Error Propagation Mitigation TUNEFP(b, X, Y; Tpre) Step 2: Low-Rank Binary Initialization for linear layer ℓ with weight W(ℓ) do 6: # Phase 2: Block Reconstruction Pipeline 7: for block = 1, . . . , do 8: cM<b(X ) 9: B (X) 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: end for end for Step 3: Factorized Component Refinement TUNELATENTSTE(b, X, Y; Tpost) for linear layer ℓ do 1 sign(U (ℓ)), V(ℓ) 1 sign(V (ℓ)) 1, V(ℓ) 1) U(ℓ) PACKBINARY(U(ℓ) fW eD(ℓ) (U (ℓ), (ℓ), s(ℓ) out W(ℓ) eD(ℓ) 1 , s(ℓ) end for in 2 ) ADMM(fW; r, K, ρ, ϵ) min U,V,Z 1 2 fWtarget UV2 + λ 2 s.t. = ZU, = ZV. (cid:0)U + V2 (cid:1) 24: # Phase 3: Model Reconstruction 25: TUNESCALESKD( cM, M, ; Tglob) 26: return cM (4) The solver alternates between updating continuous factors, auxiliary proxies, and dual variables. First, we update (symmetrically V) by solving linear system regularized by penalty ρ and λ: (VV+(ρ+λ)I)U = fW target+ρ(ZUΛU). (5) We employ stabilized Cholesky decomposition for this step, reducing the computational complexity to O(r3/3) compared to general LU factorization, which scales as O(2r3/3). This optimization is pivotal, as it enables NANOQUANT to scale efficiently to massive architectures (e.g.,, Llama-2-70B) within limited computational budgets. Second, we update the auxiliary variable using the consensus variable PU + Λ. We apply Sign-Value Independent Decomposition (SVID) (Pouransari et al., 2020; Xu et al., 2024) to derive the optimal rank-1 approximation that preserves the sign structure: (k+1) = SVID(P (k+1) ). (6) Finally, we update the dual variables to enforce consensus, computed as Λ (k) + U(k+1) (k+1) (k+1) = Λ . Step 2-3: Latent Magnitude Balancing. Upon con- (K) and vergence of ADMM, the pre-binary variables (K) possess inherent scale ambiguity, resulting in illP conditioned proxies. To rectify this, we first recover the eD1 unscaled continuous proxies, defined as bU = out eD1 and bV = in , and compute an equilibrium factor η to equalize their Frobenius norms: (K) (K) η = bVF (cid:14) bUF . (7) The scaling vectors s1 and s2 are computed directly from the balanced projections of these proxies to capture the magnitude information via the mean absolute value: [s1]i = mean(η bui), [s2]j = mean(η1 bvj), (8) where bui and bvj denote the row vectors of bU and bV, respectively. Following scale extraction, we define the final latent variables and to serve as well-conditioned initializers NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models for the subsequent fine-tuning phase: (K) := η bU = ηP := η1 bV = η1P eD1 out , (K) eD1 in . (9) This separation allows the explicit scales to handle magnitude at the input and output boundaries, ensuring that the core linear transformation proceeds sequentially without intervening scalar operations, thereby reducing computational overhead on hardware accelerators. Step 3: Factorized Component Refinement. Following initialization, we refine the factorized components to align with the full-precision block outputs. Unlike approaches that defer binary optimization to global stage (Boža & Macko, 2025) through PV-tuning (Malinovskii et al., 2024), we locally optimize these parameters during the block reconstruction phase. We jointly tune the continuous latent proxies U, and the scaling vectors s1, s2 using the StraightThrough Estimator (STE) (Bengio et al., 2013). Let B() and bB() denote the full-precision and quantized mappings of the current transformer block (with all previously processed blocks fixed), respectively. The optimization objective is formulated as: min ,V,s1,s2 B(Xin) bB(Xin; sign(U), sign(V), s1, s2)2 . (10) This formulation allows gradients to propagate through the quantization function, enabling local identification of optimal sign structures while concurrently adjusting channelwise magnitudes. Upon convergence, we fix U1 = sign(U) and V1 = sign(V) as the final binary values, and pack the binary weights into int values. 3.3. Model Reconstruction With the block-wise optimization concluded, the binary parameters are frozen and packed into efficient integer formats. Consequently, the final model reconstruction phase focuses exclusively on optimizing the floating-point scaling vectors Sglobal = {s1, s2}l to align the logits of the quantized model with the original predictions (Kwon et al., 2022). The objective function minimizes the Kullback-Leibler (KL) divergence: min Sglobal Logits(M(X)) Logits( cM(X; Sglobal))KL. (11) Unlike prior methods that require extensive memory resources for global fine-tuning (Chen et al., 2025a), this approach maintains fixed bit-packed binary weights throughout the process. This constraint substantially reduces the memory footprint, and it enables calibration of massive models, such as Llama-2-70B, feasible on single GPU. 5 4. Experiments 4.1. Experimental Setup Implementation and Environment. The implementation of NANOQUANT relies on PyTorch (Paszke et al., 2019) and the Transformers library (Wolf et al., 2020). Primary quantization and evaluation experiments used single NVIDIA H100 (80GB) GPU to ensure consistency across model scales up to the 70B parameter regime. To assess deployment viability in resource-constrained environments, inference latency and memory footprints were analyzed on consumer-grade hardware, specifically an NVIDIA RTX 3050, and an edge device, an NVIDIA Jetson TX2. Models and Datasets. Evaluations included diverse set of LLM families, including Llama2 (Touvron et al., 2023), Llama3 (Grattafiori et al., 2024), Gemma3 (Team et al., 2025), Qwen3 (Yang et al., 2025), and Rnj-1 (Essential AI, 2025), with sizes ranging from 0.6B to 70B parameters. This range addresses the sensitivity of smaller models to quantization noise (Li et al., 2020; Gong et al., 2024) and challenges the compression latency limits of larger architectures. Calibration used 128 samples from the WikiText-2 dataset (Merity et al., 2016) with sequence length of 2048. Evaluation metrics included perplexity for next-token prediction and zero-shot accuracy across six commonsense reasoning tasks: WinoGrande (Sakaguchi et al., 2021), HellaSwag (Zellers et al., 2019), BoolQ (Clark et al., 2019), ARC-Easy, ARC-Challenge (Clark et al., 2018), and PIQA (Bisk et al., 2020). Baselines. We benchmark NANOQUANT against state-ofthe-art binary post-training quantization (PTQ) methods, specifically BiLLM (Huang et al., 2024), ARB-LLM (Li et al., 2024), STBLLM (Dong et al., 2024), and HBLLM (Chen et al., 2025b). Comparisons also include binary quantization-aware training (QAT) methods such as OneBit (Xu et al., 2024), BinaryMoS (Jo et al., 2024), LittleBit (Lee et al., 2025), and DBF (Boža & Macko, 2025). We utilize official open-source implementations for PTQ baselines and select the highest-performing variants, such as ARB-LLMRC and HBLLMcol. Regarding QAT methods, we report the performance metrics for OneBit and BinaryMoS directly from their original literature. Conversely, we reproduce specific components of DBF and LittleBit to validate initialization strategies. 4.2. Accuracy Analysis Next Token Prediction. Table 2 presents the perplexity comparison between NANOQUANT and existing binary PTQ baselines. The results indicate that NANOQUANT maintains functional perplexity across diverse model families while using fewer bits than competing methods. Prior NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models Table 2. WikiText-2 perplexity () results of 1-bit and sub-1-bit post-training quantization methods. The evaluation encompasses pre-trained models from the Llama-2 (L2), Llama-3 (L3), Gemma-3 (G3), Qwen-3 (Q3), and Rnj-1 (R1) families. In these abbreviations, the numerical suffix denotes the parameter count in billions (e.g., L3-8 represents Llama-3-8B). NANOQUANT demonstrates performance competitive with higher-bit baselines across these architectures. Bits Total Bits Method L2-7 L2-13 L2-70 L3-1 16. - - 5.47 4.88 3.32 9. L3-3 7.81 L3-8 L3-70 G3-1 G3G3-12 G3-27 Q3-0.6 Q3-1.7 Q3-4 Q3-8 Q3-14 R1-8 6.24 2.86 10.60 7. 5.86 4.88 12.66 9.39 7.89 7. 6.37 8.19 1.00 1.00 2.88 4.13 2.51 3.25 1.00 4.00 0.80 3.50 0.55 1. 0.80 0.55 RTN XNOR BiLLM STBLLM ARB-LLMRC HBLLMR NANOQUANT 1.63e5 4.82e4 1.57e5 5.39e8 1.82e13 4.41e5 3.98e5 3.64e22 2.96e17 1.90e24 6.29e21 2.58e7 5.14e8 6.59e4 9.80e3 1.37e4 1.15e5 323.16 19.87 187.40 10.12 66.36 11.80 7.60 36.00 25.59 10.34 5.12e6 7.05e9 7.26e5 3.27e7 1.60e6 1.56e10 1.37e8 1.63e8 6.25e4 20.71 3.17e3 858.09 14.78 329.76 1.32e3 15.13 49.51 129.52 11.90 35.14 78.58 19.21 27.56 15.45 8.50e5 8.61e5 31.20 93.36 16.68 155.43 7.89 19.06 11.82 8.88 11.32 14. 5.00e6 262.83 63.45 32.47 19.22 24.70 1.91e8 144.72 80.54 67.43 28.58 35.30 3.32e6 31.21 16.46 16.80 9.08 32.98 1.78e6 55.43 25.46 23.87 15.99 17.90 4.50e6 37.08 21.97 23.37 12.92 19.64 13.50 10.72 10.25 8.37 10. 29.62 20.04 12.74 10.51 12.47 13.29 8.08 8.43 6.27 8.71 78.36 35.03 18.04 14.73 14.29 8.75 5.26 5.20 4.56 6.52 1.45e6 STBLLM (6:8) NANOQUANT STBLLM (4:8) NANOQUANT 11.24 12.20 20.27 16.66 8.97 10.14 15.22 13.46 5.94 7. 9.27 9.82 314.19 33.08 6.69e3 49.01 39.00 22.09 381.77 32.33 20.19 18. 88.84 25.69 78.21 13.75 1.83e3 19.69 123.60 50.15 592.31 78.22 26.01 25. 69.63 40.69 95.83 32.84 489.64 45.29 26.14 32.84 83.29 45.59 3.63e3 1.96e3 25.31 33. 44.99 19.33 24.19 14.83 12.50 12.88 5.74e5 5.18e4 33.74 52.94 1.30e3 32.86 109.40 28.50 17.06 20. 18.07 20.06 67.60 32.62 Table 3. Zero-shot accuracy comparison on commonsense reasoning tasks using Llama-3 (L3) and Qwen-3 (Q3) models. NANOQUANT maintains competitive accuracy against higher-bit binary PTQ baselines, despite utilizing 1-bit representation. Table 4. Comparing the compression and resource efficiency of various quantization methods, when compressing Llama-2 7B on NVIDIA H100 GPUs. NANOQUANT requires multiple orders of magnitudes less data and compute to achieve binary quantization. Model Bits Method ARC-e ARC-c BoolQ Hella. Wino. PIQA Avg. Method PTQ/QAT Bits Model Size Data GPU Hours PPL () L3-8 Q3-8 16.00 BF16 4.13 STBLLM 3.25 HBLLMcol 2.88 BiLLM 2.51 ARB-LLMRC 2.28 GPTQ(w2g64) 1.00 NANOQUANT 16.00 BF 4.13 STBLLM 3.25 HBLLMcol 2.88 BiLLM 2.51 ARB-LLMRC 2.28 GPTQ(w2g64) 1.00 NANOQUANT 81.57 36.87 60.02 36.32 49.71 28.24 43.69 81. 52.78 68.43 28.96 68.18 28.62 49.45 51.45 19.97 29.10 18.34 22.95 20.14 20.31 52. 27.13 36.77 22.35 34.64 20.99 24.32 81.96 60.01 73.56 80. 71.44 48.01 63.03 56.36 64.28 41.87 36.47 43.46 30.16 34.73 27.00 57.62 63.77 51.93 56.04 50.59 61.48 70.35 57.56 63.28 54.08 39.83 50.45 38.16 44.23 36. 61.47 33.81 55.96 60.45 45.95 83. 58.81 72.38 79.11 71.22 62.84 68.53 62.23 66.30 43.64 38.38 45.68 32.42 40.83 29. 57.54 63.85 51.30 59.12 50.04 64.15 71.65 55.01 68.17 54.68 46.15 54.30 38.18 51.86 37.92 62.17 36.34 58. 63.32 48.94 Full-Precision GPTQ (W2g64) STBLLM HBLLMR BiLLM ARB-LLMRC OneBit BinaryMoS DBF LittleBit NanoQuant NanoQuant PTQ PTQ PTQ PTQ PTQ QAT QAT QAT QAT PTQ PTQ 12.55 GB 2.37 GB 4.07 GB 3.16 GB 2.85 GB 2.55 GB 1.37 GB 1.40 GB 1.25 GB 1.37 GB 1.24 GB 1.24 GB 2.28 4.13 3.25 2.88 2.51 1.04 1.08 1.00 1.04 1.00 1. 0.26M 0.26M 0.26M 0.26M 0.26M 155.46M 196.00M 1.38B 196.00M 0.26M 2.10M 0.1 0.9 2.2 1.1 1.3 700.7 92.8 37.6 123.6 1.7 2.5 5.47 21.00 10.12 7.60 19.87 11.80 9.73 7.88 9.25 9.08 10.34 8.85 binary PTQ approaches often struggle to break the 1-bit barrier due to structural overhead, but NANOQUANT achieves sub-1-bit compression without catastrophic degradation in the predictive distribution. This finding suggests that the proposed low-rank factorization effectively captures the salient weight information required for language modeling, even at extreme compression rates. Zero-Shot Reasoning. The evaluation of commonsense reasoning tasks in Table 3 reveals that NANOQUANT yields performance competitive with higher-bit binary PTQ baselines. Furthermore, the method approaches the zero-shot accuracy of binary QAT methods. This result is notable because QAT approaches typically require extensive training on billions of tokens. In contrast, NANOQUANT achieves comparable fidelity using orders of magnitude less data and compute. This efficiency suggests that precise initialization and block-wise reconstruction may substitute for the expensive end-to-end retraining traditionally required for binary quantization. 4.3. Compression vs. Model Size We analyze the storage efficiency of various quantization paradigms using Llama-2-7B as case study, as shown in Table 4. The analysis reveals that standard binary PTQ methods often incur substantial overhead due to auxiliary parameters. Consequently, their effective storage requirements exceed those of 2-bit quantization methods such as GPTQ (Frantar et al., 2022). For instance, BiLLM and STBLLM require 2.88 and 4.13 bits per weight, respectively, whereas GPTQ (W2g64) uses only 2.28 bits. NANOQUANT overcomes this limitation and emerges as the only binary PTQ method to achieve genuine 1-bit and sub-1-bit compression. By minimizing metadata overhead, it offers storage solution that is strictly more efficient than existing PTQ baselines while maintaining perplexity levels competitive with resource-intensive QAT methods. 4.4. Inference Efficiency The extreme compression ratio achieved by NANOQUANT translates directly into reduced memory footprints and enhanced throughput, particularly in memory-bound regimes. To quantify this, we compared the decoding performance of NANOQUANT against PyTorch BF16 baseline. We NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models Figure 4. On 1 NVIDIA RTX 3050 (8GB), NANOQUANT delivers up to 3.6 higher decoding throughput, 5.4 lower peak memory usage, and 3.9 greater energy efficiency compared to BF16 baselines for Llama-3.2-1B and 3B models. Figure 5. Datacenter inference efficiency on single NVIDIA H100 (80GB). NANOQUANT enables faster decoding throughput while maintaining superior memory and energy efficiency for Llama-2-13B and Qwen-3-32B, compared to the PyTorch BF16 baseline. focused on this comparison because open-source kernels for other binary PTQ baselines are currently unavailable. Table 5. Initialization via latent-binary ADMM (LB-ADMM) from NANOQUANT outperforms other low-rank binary initialization strategies, when compressing Rnj-1 (Essential AI, 2025) to 0.8 bits Consumer Hardware. On an NVIDIA RTX 3050 (8GB), NANOQUANT enables 4.02 speedup in inference throughput for Llama-3.2-3B. Additionally, the method achieves 5.4 reduction in peak memory usage and 3.9 improvement in energy efficiency per token, as shown in Figure 4. Beyond speed, the method fundamentally expands accessibility. NANOQUANT compresses the Llama2-70B model from 138.04 GB to 5.35 GB, representing 25.8 compression factor. This reduction allows 70B parameter model to fit entirely within the VRAM of consumer-grade 8GB GPU and effectively lowers the barrier to entry for large-scale model deployment. To evaluate deployment viability in even more constrained environments, we extended our analysis to embedded systems. Detailed performance metrics on the NVIDIA Jetson TX2 are provided in Appendix E. Datacenter Hardware. On high-end hardware (NVIDIA H100 80GB), NANOQUANT alleviates memory bandwidth bottlenecks and demonstrates up to 10 lower memory usage during inference. As illustrated in Figure 5, this results in faster single-batch inference and superior energy efficiency compared to the BF16 baseline. Additional kernel implementation details are provided in Appendix E. 4.5. Ablation Studies Initialization Strategy. We investigate the hypothesis that precise initialization of low-rank binary matrices is critical for convergence in PTQ. We integrated initialization strategies from prominent QAT methods, specifically LittleBit (Lee et al., 2025) and DBF (Boža & Macko, 2025), into our Initialization Method PPL () Zero-shot () Dual-SVID (Lee et al., 2025) DBF ADMM (Boža & Macko, 2025) LB-ADMM (Ours) 167.73 30.27 20.06 35.11 37.20 39. reconstruction pipeline. Table 5 demonstrates that LatentBinary ADMM (LB-ADMM) outperforms these alternatives in both perplexity and zero-shot tasks. This result indicates that solving the combinatorial problem of binary factorization prior to fine-tuning provides more stable optimization landscape than the initialization schemes used in existing QAT frameworks. Component Efficacy. Table 6 dissects the contribution of each algorithmic component. Comparison against the baseline DBF architecture highlights that the integrated NANOQUANT pipeline yields superior fidelity. This improvement stems from the combination of robust initialization and block-wise reconstruction. Each module contributes distinctly to preserving the models representational capacity under extreme compression. Pareto Optimality. Finally, we analyze the trade-off between model size and performance across the Qwen3 family, as shown in Figure 6. NANOQUANT establishes new Pareto frontier in the low-memory regime and consistently outperforms previous binary PTQ baselines. This suggests that sub-1-bit quantization may be viable alternative to low-bit integer quantization for memory-critical applications. 7 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models Figure 6. Pareto optimality analysis for models in the Qwen3 family. NANOQUANT establishes new efficiency frontier in the low-bit regime, offering superior accuracy-per-bit trade-offs compared to existing state-of-the-art binary PTQ methods. Table 6. Component-wise efficacy analysis of the NANOQUANT pipeline on Qwen3-8B. The table demonstrates the contribution of each moduleInitialization, Error Mitigation, Factorized Component Refinement, and Model Reconstructiontowards enhancing performance. Block Reconstruction Initialization Error Mitigation Fact. Refinement Model Recon. PPL () Zero-shot () 206.03 15.07 15.00 13.58 12.47 36.89 46.40 47.88 46.75 48.94 Comparison with Low-Rank Binary QAT. Table 7 contrasts NANOQUANT with state-of-the-art QAT methods. DBF (Boža & Macko, 2025) and LittleBit (Lee et al., 2025) rely on training over 1 billion and 100 million tokens respectively. We find that with 512 calibration samples, NANOQUANT achieves comparable predictive performance with binary QAT methods. This data efficiency validates the effectiveness of the proposed PTQ formulation for scenarios where full-scale retraining is impractical. 4.6. Limitations and Future Work Although our experiments demonstrate data efficiency using small calibration set, scaling the data and compute budget could enhance performance on more complex reasoning tasks. Regarding inference, the custom CUDA kernels demonstrate promising results, as detailed in Section 4.4. However, further optimization for next-generation architectures such as NVIDIA Blackwell GPUs or edge-specific hardware could yield additional speedups. Additionally, while NANOQUANT outperforms 2-bit baselines, further enhancing capabilities to outperform higher-bit 2 or 3-bit PTQ performance remains an open challenge for the sub8 Table 7. NANOQUANT achieves comparable performance with QAT methods DBF and LittleBit, while using orders of magnitude less data and compute time, when compressing Qwen3-4B and Llama2-7B to 1 bit. Model Method Data GPU Hours PPL () Zero-shot () LittleBit Q3-4B DBF NANOQUANT LittleBit L2-7B DBF NANOQUANT 169.50M 1.19B 1.05M 196.00M 1.38B 1.05M 92.5 25.3 2.3 123.6 37.6 2. 14.79 14.62 12.62 9.08 9.25 9.01 47.32 52.30 50.63 54.92 54.24 51.01 binary regime. Future work will focus on optimizing the compression runtime and exploring the scalability of the method to larger calibration datasets. 5. Conclusion We propose NANOQUANT, an efficient and accurate posttraining quantization method to enable 1-bit and sub-1-bit weight quantization of LLMs of up to 70B parameters, with only single GPU. NANOQUANT enables rapid binarization and extremely compact weight storage, significantly reducing the memory footprint of LLM inference. Custom binary CUDA kernels further improve energy efficiency and decoding speed. Our approach achieves up to 25.8 model compression, making it feasible to run 70B-parameter LLM on an 8GB GPU. NANOQUANT democratizes access to large-scale language models by enabling fast, efficient compression and inference for researchers and developers in resource-constrained settings, and advances the frontier of extreme LLM quantization. NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models"
        },
        {
            "title": "Impact Statement",
            "content": "This work presents NANOQUANT, sub-1-bit post-training quantization algorithm for large language models (LLMs). By enabling the deployment of massive models (e.g., Llama2-70B) on consumer hardware (e.g., single 8GB GPU) and edge devices, our method significantly lowers the barrier to entry for advanced AI research and application. This contributes to the democratization of AI, allowing individuals and institutions with limited computational resources to utilize state-of-the-art LLMs. Furthermore, NANOQUANT is environmentally and resource friendly, as it drastically reduces the memory bandwidth and energy consumption required for inference, as demonstrated by our energy-efficiency experiments."
        },
        {
            "title": "References",
            "content": "Arai, Y. and Ichikawa, Y. Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization. arXiv preprint arXiv:2504.09629, 2025. Bengio, Y., Léonard, N., and Courville, A. Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. arXiv:1308.3432, 2013. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. PIQA: Reasoning about Physical Commonsense in Natural Language. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020. Boža, V. and Macko, V. Addition Is Almost All You Need: Compressing Neural Networks with Double Binary Factorization. arXiv preprint arXiv:2505.11076, 2025. Chee, J., Cai, Y., Kuleshov, V., and De Sa, C. M. QuIP: 2-Bit Quantization of Large Language Models with Guarantees. Advances in Neural Information Processing Systems, 36: 43964429, 2023. Chen, M., Shao, W., Xu, P., Wang, J., Gao, P., Zhang, K., and Luo, P. EfficientQAT: Efficient Quantization-Aware Training for Large Language Models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10081 10100, 2025a. Chen, N., Ye, W., and Jiang, Y. HBLLM: WaveletEnhanced High-Fidelity 1-Bit Quantization for LLMs. arXiv preprint arXiv:2512.00862, 2025b. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. arXiv preprint arXiv:1905.10044, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think You Have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457, 2018. Dong, P., Li, L., Zhong, Y., Du, D., Fan, R., Chen, Y., Tang, Z., Wang, Q., Xue, W., Guo, Y., et al. STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs. arXiv preprint arXiv:2408.01803, 2024. Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar, E., Babenko, A., and Alistarh, D. Extreme Compression of Large Language Models via Additive Quantization. arXiv preprint arXiv:2401.06118, 2024. Essential AI. Rnj-1: Building Instruments of Intelligence, 2025. URL https://essential.ai/research/ rnj-1. Research blog post, Essential AI. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers. arXiv preprint arXiv:2210.17323, 2022. Frantar, E., Castro, R. L., Chen, J., Hoefler, T., and Alistarh, D. MARLIN: Mixed-Precision Auto-Regressive Parallel Inference on Large Language Models. In Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, pp. 239251, 2025. Froese, V. and Hertrich, C. Training Neural Networks is NP-Hard in Fixed Dimension. Advances in Neural Information Processing Systems, 36:4403944049, 2023. Gong, Z., Liu, J., Wang, J., Cai, X., Zhao, D., and Yan, R. What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1808218089, 2024. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783, 2024. Huang, W., Liu, Y., Qin, H., Li, Y., Zhang, S., Liu, X., Magno, M., and Qi, X. BiLLM: Pushing the Limit of Post-Training Quantization for LLMs. arXiv preprint arXiv:2402.04291, 2024. Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry, D. Accurate post training quantization with small calibration sets. In International Conference on Machine Learning, pp. 44664475. PMLR, 2021. Jo, D., Kim, T., Kim, Y., et al. Mixture of Scales: MemoryEfficient Token-Adaptive Binarization for Large Language Models. Advances in Neural Information Processing Systems, 37:137474137494, 2024. 9 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models Kim, J., Halabi, M. E., Park, W., Schaefer, C. J., Lee, D., Park, Y., Lee, J. W., and Song, H. O. GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance. arXiv preprint arXiv:2505.07004, 2025. Martens, J. and Grosse, R. Optimizing Neural Networks In with Kronecker-Factored Approximate Curvature. International Conference on Machine Learning, pp. 2408 2417. PMLR, 2015. Kwon, S. J., Kim, J., Bae, J., Yoo, K. M., Kim, J.-H., Park, B., Kim, B., Ha, J.-W., Sung, N., and Lee, D. AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models. arXiv preprint arXiv:2210.03858, 2022. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Ledoit, O. and Wolf, M. Well-Conditioned Estimator for Large-Dimensional Covariance Matrices. Journal of Multivariate Analysis, 88(2):365411, 2004. Lee, B., Kim, D., You, Y., and Kim, Y. LittleBit: Ultra Low-Bit Quantization via Latent Factorization. Advances in Neural Information Processing Systems, 2025. Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., and Gonzalez, J. Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers. In International Conference on Machine Learning, pp. 59585968. PMLR, 2020. Li, Z., Yan, X., Zhang, T., Qin, H., Xie, D., Tian, J., Kong, L., Zhang, Y., Yang, X., et al. ARB-LLM: Alternating Refined Binarizations for Large Language Models. arXiv preprint arXiv:2410.03129, 2024. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. AWQ: Activation-Aware Weight Quantization for On-Device LLM Compression and Acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. Liu, Z., Zhao, C., Fedorov, I., Soran, B., Choudhary, D., Krishnamoorthi, R., Chandra, V., Tian, Y., and Blankevoort, T. SpinQuant: LLM Quantization with Learned Rotations. arXiv preprint arXiv:2405.16406, 2024. Liu, Z., Zhao, C., Huang, H., Chen, S., Zhang, J., Zhao, J., Roy, S., Jin, L., Xiong, Y., Shi, Y., et al. ParetoQ: Scaling Laws in Extremely Low-Bit LLM Quantization. arXiv preprint arXiv:2502.02631, 2025. Malinovskii, V., Mazur, D., Ilin, I., Kuznedelev, D., Burlachenko, K., Yi, K., Alistarh, D., and Richtarik, P. PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression. Advances in Neural Information Processing Systems, 37:50745121, 2024. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer Sentinel Mixture Models, 2016. Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or Down? Adaptive Rounding for Post-Training Quantization. In International Conference on Machine Learning, pp. 71977206. PMLR, 2020. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. PyTorch: An Imperative Style, HighPerformance Deep Learning Library. Advances in Neural Information Processing Systems, 32, 2019. Pouransari, H., Tu, Z., and Tuzel, O. Least Squares Binary Quantization of Neural Networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 698699, 2020. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. Communications of the ACM, 64(9):99 106, 2021. Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang, K., Gao, P., Qiao, Y., and Luo, P. OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. arXiv preprint arXiv:2308.13137, 2023. Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Ramé, A., Rivière, M., et al. Gemma 3 Technical Report. arXiv preprint arXiv:2503.19786, 2025. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open Foundation and FineTuned Chat Models. arXiv preprint arXiv:2307.09288, 2023. Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and De Sa, C. QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks. arXiv preprint arXiv:2402.04396, 2024a. Tseng, A., Sun, Q., Hou, D., and De Sa, C. M. QTIP: Quantization with Trellises and Incoherence Processing. Advances in Neural Information Processing Systems, 37: 5959759620, 2024b. Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., Yang, F., Wang, R., Wu, Y., and Wei, F. BitNet: Scaling 1-Bit Transformers for Large Language Models. arXiv preprint arXiv:2310.11453, 2023. NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models Wijmans, E., Huval, B., Hertzberg, A., Koltun, V., and Krähenbühl, P. Cut Your Losses in Large-Vocabulary Language Models. arXiv preprint arXiv:2411.09009, 2024. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Transformers: State-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, 2020. Xu, Y., Han, X., Yang, Z., Wang, S., Zhu, Q., Liu, Z., Liu, W., and Che, W. OneBit: Towards Extremely Low-Bit Large Language Models. Advances in Neural Information Processing Systems, 37:6635766382, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388, 2025. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can Machine Really Finish Your Sentence? arXiv preprint arXiv:1905.07830, 2019. Zhao, J., Zhang, M., Wang, M., Shang, Y., Zhang, K., Guan, W., Wang, Y., and Zhang, M. PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models. arXiv preprint arXiv:2502.13179, 2025. Zheng, L., Yin, L., Xie, Z., Sun, C. L., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., et al. SGLang: Efficient Execution of Structured Language Model Programs. Advances in Neural Information Processing Systems, 37:6255762583, 2024. 11 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models A. Theoretical Analysis: Magnitude Balancing In this section, we provide the theoretical justification for the Magnitude Balancing strategy employed in NANOQUANT. We analyze the necessity of controlling the magnitude of latent factors from two complementary perspectives: maximizing the representational capacity of the quantized structure (static balancing), and ensuring numerical stability during the alternating optimization process (dynamic balancing via normalization). Let the target weight approximation be represented as UV . In our formulation, the continuous latent variables are decomposed into scales and binary-like directions: diag(s1)U1, diag(s2)V1, (12) where s1 Rm and s2 Rn capture the row-wise and column-wise magnitudes, respectively. A.1. Static Balancing: Optimal Scale Distribution First, we explicitly show how balancing the continuous latent variables and ensures the optimal derivation of the output scale s1 and input scale s2. Proposition 1 (Optimality of Balanced Scales). Let the target weight matrix possess the intrinsic structure defined by Singular Value Decomposition (SVD) as = LΣR. We analyze the optimal energy distribution by introducing parameter γ [0, 1]: Uγ = LΣγ, Vγ = RΣ1γ. (13) The condition that balances the magnitude distribution to avoid numerical extremes is UF = VF . Proof. The magnitudes of the derived scales are proportional to the norms of their corresponding latent variables. To minimize the relative reconstruction error bound and ensure numerical stability, we minimize the total magnitude energy: (γ) 1 (Uγ2 + Vγ2 ) = 1 2 i= (σ2γ + σ2(1γ) ). (14) By the Arithmetic-Geometric Mean (AM-GM) inequality, the minimum is achieved if and only if σ2γ implies γ = 0.5. Thus, U0.5F = V0.5F . Conclusion: Balancing the latent variables effectively embeds Σ0.5 into both s1 and s2. In the context of low-precision floating-point formats (e.g., FP16), this balance is critical. By ensuring that neither scale vector becomes numerically negligible nor explodes, we maximize the utilization of the limited dynamic range provided by the exponent bits, thereby preserving effective precision. = σ2(1γ) , which A.2. Dynamic Balancing: Normalization for Stability While Proposition 1 defines the optimal final state, maintaining this balance during the iterative optimization is challenging due to scale ambiguity. In NANOQUANT, we enforce dynamic balancing via Iterative Normalization. Remark 1 (Numerical Stability via Normalization). The ADMM update step involves solving linear system with the system matrix = VV + ρI. Explicitly normalizing the fixed factor (e.g., VF 1) at each iteration acts as dynamic constraint that bounds the condition number κ(H) within stable range, thereby preventing algorithmic instability. Analysis of Conditioning. The numerical stability of the linear solve is governed by the condition number κ(H) = λmax+ρ λmin+ρ , where λ are the eigenvalues of VV. Without normalization, scale ambiguity leads to two extremes: 1. Vanishing Scale (VF 0): Here, ρI. While well-conditioned, the update becomes dominated by the regularizer, causing gradients from the data term to vanish. 2. Exploding Scale (VF ): Here, λmin ρ. The regularization term becomes negligible, and κ(H) approaches that of the ill-conditioned Gram matrix VV. By normalizing V, we enforce the eigenvalues λ to remain in controlled range relative to the penalty parameter ρ. Consequently, κ(H) is prevented from exploding, ensuring accurate Cholesky decompositions throughout the ADMM process. NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models B. Convergence Analysis of Low-Rank Binary Initialization This section provides stability analysis of Robust Hessian Preconditioning and establishes the convergence properties of the Latent Binary ADMM (LB-ADMM) solver employed in NANOQUANT. We demonstrate that the algorithm ensures monotonic reduction in the objective function, thereby providing stable initialization for the subsequent fine-tuning phase. B.1. Problem Setup Let WFP Rmn denote the full-precision weight matrix. We define the robustly preconditioned target as: eDoutWFP eDin Rmn, (15) where eDin and eDout are diagonal preconditioners derived from robust activation statistics. We seek rank-R binary factorization UV with RmR and RnR. To enable efficient optimization via ADMM, we introduce auxiliary low-rank proxy variables ZU, ZV. The optimization problem is formulated as follows: min U,V,ZU,ZV (U, V) + IS (ZU) + IS (ZV) s.t. = ZU, = ZV, (16) where (U, V) 1 rank-1 approximations solvable via SVID. 2 UV2 is the smooth reconstruction loss, and IS denotes the indicator function for the set of B.2. Robust Hessian Preconditioning The stability of the optimization relies on the spectral properties of W. NANOQUANT employs robust estimator Φ() based on percentile clipping to limit the influence of outliers. Lemma 1 (Uniform Bound Induced by Percentile Clipping). By construction, the diagonal entries of the preconditioners are bounded by the final cumulative threshold τmax. That is, for every coordinate j, Φ(v)j τmax. Consequently, the spectral norms satisfy: eDin2 τmax, eDout2 τmax. (17) Proof. The sequence of thresholds {τt} is non-decreasing due to the cumulative maximum update rule τt = max(τt1, qt). Therefore, τmax qt for all time steps t. Since the estimator Φ() applies dynamic clipping based on this threshold, the diagonal entries are explicitly constrained such that eDjj τmax. As diagonal matrices, their spectral norm equals the maximum absolute diagonal entry, which proves the bound. Corollary 2 (Spectral Control of the Preconditioned Target). The preconditioned target weight matrix satisfies: W2 eDout2 WFP2 eDin2 τ maxWFP2. (18) This spectral control ensures that the Lipschitz constant Lf of the gradient remains finite, satisfying necessary condition for the stability of the subsequent ADMM steps. B.3. ADMM Formulation and Updates We introduce dual variables YU, YV and penalty parameter ρ > 0. The Augmented Lagrangian Lρ is defined as: Lρ(U, V, ZU, ZV, YU, YV) 1 + UV2 (cid:16) IS (ZX) + YX, ZX + X{U,V} 13 ZX2 (cid:17) . ρ 2 (19) NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models The algorithm proceeds by alternating between the following updates: Uk+1 := arg min Vk+1 := arg min Lρ(U, Vk, Zk U, Zk V, Yk U, Yk Lρ(Uk+1, V, Zk U, Zk V, Yk V), U, Yk V), Zk+1 := arg min ZUS := arg min ZVS Zk+1 Lρ(Uk+1, Vk+1, ZU, Zk V, Yk U, Yk V), Lρ(Uk+1, Vk+1, Zk+ , ZV, Yk U, Yk V), Yk+1 := Yk + ρ(Uk+1 Zk+ ), Yk+1 := Yk + ρ(Vk+1 Zk+1 ). (20) (21) (22) (23) (24) Lemma 2 (SPD Structure and Uniqueness). For any ρ > 0, the system matrices for the continuous updates, VkVk + ρI and Uk+1Uk+1 + ρI, are Symmetric Positive Definite (SPD). Consequently, the sub-problems for and are strongly convex and admit unique closed-form solutions. Proof. For any nonzero vector a, the quadratic form satisfies a(VkVk + ρI)a = Vka2 2 + ρa2 quantity is strictly positive, confirming that the matrix is PD. The same logic applies to the update for U. 2. Since ρ > 0, this B.4. Convergence Analysis: Monotonic Descent Property The presence of the non-convex set renders the optimization problem non-convex and non-smooth. Standard ADMM convergence theory, which relies on convex sets or smooth manifolds, does not directly apply to this formulation. However, we establish that the proposed algorithm satisfies Monotonic Descent Property, ensuring that the augmented objective function improves or remains stable at each iteration. Theorem 3 (Monotonic Descent of Augmented Lagrangian). Let Lf denote the Lipschitz constant of . If the penalty parameter is chosen such that ρ > Lf , the sequence of iterates generated by the LB-ADMM algorithm satisfies: Lρ(Uk+1, Vk+1, Zk+1, Yk+1) Lρ(Uk, Vk, Zk, Yk). (25) Proof. We analyze the sufficient decrease provided by each step of the alternating minimization: 1. Continuous Updates (U, V): By Lemma 2, the sub-problems for and are strongly convex quadratic functions. The updates Uk+1 and Vk+1 are the unique global minimizers given the other fixed variables. This ensures strict decrease in Lρ proportional to the squared norm of the primal step. 2. Proxy Updates (Z): The update for corresponds to finding the closest element in the set via SVID: Zk+1 = SVID (cid:18) Uk+1 + (cid:19) . Yk 1 ρ (26) By defining the constraint set as the image of the SVID operator, this step constitutes an exact projection. Therefore, the value of Lρ is minimized with respect to over at each iteration and does not increase. 3. Dual Update and Total Descent: The increase in the Lagrangian due to the dual ascent step is bounded by the primal residual. Assuming the iterates (Uk, Vk) remain within compact set due to the regularization term, the gradient is Lipschitz continuous with constant Lf . By selecting ρ > Lf , the descent in the primal variables dominates the ascent in the dual variables, ensuring that the total Augmented Lagrangian is non-increasing. Since Lρ is bounded from below by zero, the sequence of Lagrangian values converges. Connection to Relaxed Stationarity. While the LB-ADMM algorithm guarantees monotonic descent, theoretical firstorder stationarity (KKT conditions) is formally defined on the smooth manifold of fixed-rank matrices. In this relaxed setting, our formulation satisfies standard assumptions for non-convex ADMM convergence to stationary point. The use of the rank-1 SVID constraint set in NANOQUANT functions as heuristic strengthening of this relaxation, prioritizing the low-rank structure required for quantization over smooth stationarity. The monotonic descent property confirms that this structural enforcement does not destabilize the optimization trajectory. 14 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models C. Implementation Details All compression experiments for NANOQUANT were conducted on 1 NVIDIA H100 80GB, and we utilize unified hyperparameters when compressing models with NANOQUANT. When tuning pre-factorized parameters to absorb quantization error, we used learning rate of 1e-4 and batch size of 4. For tuning factorized parameters (low-rank, latent binary and full-precision scales), we used unified learning rate of 1e-5 and batch size of 1. For global scale reconstruction, we used learning rate of 1e-6 and batch size of 1. Pre-factorized, factorized, and global tuning stages all consist of 8 epochs and utilize cosine learning rate scheduler. We employed linear ADMM penalty scheduler for 400 factorization steps, for each weight matrix across all models. For calibration data, we used 128 samples with sequence length of 2048 from the WikiText-2 dataset (Merity et al., 2016), and used random seed value of 0 for data selection. For all experiments, we used torch=2.6.0, transformers=4.51.3, datasets=4.0.0, lm_eval=0.4.9, and CUDA 12.4. To derive the activation-based diagonal preconditioners, we utilized gradient checkpointing and used memory-efficient implementation of the cross-entropy function (Wijmans et al., 2024). During block reconstruction, we employed weighted MSE function, utilized in previous quantization works (Boža & Macko, 2025; Kim et al., 2025). Official open-source implementations or quantized models were used to evaluate baselines for binary PTQ (Huang et al., 2024; Dong et al., 2024; Li et al., 2024; Chen et al., 2025b), vector quantization (Egiazarian et al., 2024; Malinovskii et al., 2024; Tseng et al., 2024b), and low-rank binary QAT (Boža & Macko, 2025; Lee et al., 2025). Since all binary PTQ baselines (BiLLM (Huang et al., 2024), STBLLM (Dong et al., 2024), ARB-LLM (Li et al., 2024), HBLLM (Chen et al., 2025b)) do not compress quantized models into memory-efficient formats, we utilize the main text and appendices of such methods to calculate both effective bits and model checkpoint sizes (Huang et al., 2024; Li et al., 2024; Chen et al., 2025b). Further details on effective bit calculation and model checkpoint sizes can be found in Appendix F. 15 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models D. Further Ablations D.1. NANOQUANT vs Vector Quantization We compare the performance of NANOQUANT with state-of-the-art 2-bit vector quantization methods (Egiazarian et al., 2024), PV-tuning (Malinovskii et al., 2024), and QTIP (Tseng et al., 2024b). These methods utilize significantly more data and compute than other low-bit PTQ methods, as in Table 8. Notably, compressing Llama2-7B with NANOQUANT takes less than 3 hours on 1 NVIDIA H100 GPU, while AQLM is reported to take at least 1 day on multiple A100 GPUs, and PV-Tuning even longer. Furthermore, 2-bit NANOQUANT shows up to 1.45 reduced memory footprint, compared to vector quantization baselines. Nevertheless, NANOQUANT shows competitive performance with data and compute intensive vector quantization baselines, and makes NANOQUANT lucrative compression method for resource-constrained environments. Table 8. Comparison of NANOQUANT with state-of-the-art vector quantization methods AQLM (Egiazarian et al., 2024), PV-Tuning (Malinovskii et al., 2024), and QTIP (Tseng et al., 2024b) for compressing Llama2-7B. NANOQUANT shows competitive data, compute, and storage efficiency with comparable performance. Method Variant Model Size Data PPL () Zero-shot () QTIP AQLM AQLM + PV NANOQUANT 2-bit 2.15 GB 12.58M 2-bit-1x16 2-bit-2x8 2-bit-1x16 2-bit-2x8 1-bit 2-bit 2.38 GB 2.15 GB 2.38 GB 2.15 GB 1.24 GB 1.68 GB 8.00M 8.00M 8.00M 8.00M 1.05M 1.05M 6.29 6.34 7.24 6.08 6. 9.01 7.35 67.18 63.46 59.53 64.04 63.68 51.01 56.90 Figure 7. LLM decoding performance of NANOQUANT, compared with PyTorch BF16 and vector quantization methods (AQLM, PV-Tuning, QTIP) for 128 input tokens and various output sequence lengths, on 1 NVIDIA H100 GPU. NANOQUANT shows superior inference speed, memory efficiency, and energy efficiency, compared to vector quantization methods and BF16. D.2. Different Data Budgets We test with different calibration data budgets for the block and model reconstruction stages. As in Table 9, utilizing more data during block reconstruction leads to greater performance gains. D.3. Analysis of Latent Weight Dynamics To validate the efficacy of the Factorized Component Refinement phase (Step 3), we analyze the trajectory of the continuous latent variables, and V, before and after fine-tuning. Figure 8 visualizes the distribution shifts and sign flip ratios for all linear layers within the first transformer block of Llama3.2-1B. The abscissa represents the magnitude of the latent 16 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models Table 9. Utilizing different data budgets for the block and model reconstruction stages of NANOQUANT, when compressing Llama2 7B to 1-bit. Block Recon. Samples 32 64 128 256 512 Model Recon. Samples 32 128 256 512 14.16 12.13 10.56 10.25 9.24 13.24 11.66 10.40 10.24 9.16 12.72 11.23 10.35 10.17 9. 12.22 10.94 10.06 9.77 9.07 11.91 10.69 9.89 9.60 9.07 weights initialized by LB-ADMM (Step 2), while the ordinate in the right-hand panels denotes the magnitude of change after refinement (Step 3). Stability of Initialization. As illustrated in Figure 8, predominant proportion of latent weights retain their original signs throughout the refinement process. The sign flip ratio remains consistently low, ranging from 0.47% in the k_proj layer to 6.82% in the gate_proj layer. This stability indicates that the LB-ADMM initialization establishes parameter configuration proximate to local optimum, thereby mitigating the necessity for substantial updates during the fine-tuning phase. Refinement of Boundary Weights. Despite the low flip ratio, the refinement step functions as critical margin maximization process. The interaction density plots reveal an inverse correlation between the initial magnitude and the degree of change. Specifically, weights with initial magnitudes near zero, corresponding to the decision boundary, exhibit the highest mobility and likelihood of sign flipping. This behavior suggests that the refinement phase selectively rectifies the signs of ambiguous weights near the zero boundary while preserving the confident decisions established during the initialization. Consequently, this targeted adjustment compensates for discretization errors introduced during the initial factorization. NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models (a) self_attn.q_proj (Flip: 0.60%) (b) self_attn.k_proj (Flip: 0.47%) (c) self_attn.v_proj (Flip: 2.26%) (d) self_attn.o_proj (Flip: 2.11%) (e) mlp.gate_proj (Flip: 6.82%) (f) mlp.up_proj (Flip: 6.22%) (g) mlp.down_proj (Flip: 1.73%) Figure 8. Visualization of latent variable dynamics between the initialization (LB-ADMM) and refinement (STE Tuning) phases for Llama3.2-1B (Block 0). Blue points denote weights that retained their sign, while red points denote sign flips. The density plots in each panel illustrate that sign flips and large magnitude updates are concentrated around weights with near-zero initial magnitude. This demonstrates that the refinement step selectively optimizes decision boundaries for features with high uncertainty. 18 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models E. Inference Ablations E.1. Kernel Benchmarking Details We benchmark custom CUDA kernels using torch.compile from torch 2.6.0 (Paszke et al., 2019) and StaticCache from the transformers library (Wolf et al., 2020), with CUDA 12.4. The decoding script is based on the open-source implementation for QTIP (Tseng et al., 2024b), and is used for all kernel evaluations. For GEMV decoding (batch size = 1), we vary the number of output tokens. For GEMM inference, we evaluate performance under increasing batch sizes. We fix the input tokens to 128, output tokens for batched inference to 512, temperature to 0.8, and the top-k value to 32. We utilize the open-source ml-energy/zeus library for all energy measurements. We benchmark our kernels on 4 different GPUs, as listed in Table 10. Notably, we test on high-end GPUs, consumer GPU, and an edge device with no NVIDIA Tensor Cores, to test the decoding and efficiency limits of our custom binary CUDA kernels. Table 10. Hardware specifications of devices we benchmark our custom GPU kernels on."
        },
        {
            "title": "Device",
            "content": "NVIDIA Jetson TX2 NVIDIA RTX 3050 NVIDIA A100 NVIDIA H"
        },
        {
            "title": "Compute",
            "content": "GPU Memory (GB)"
        },
        {
            "title": "Type",
            "content": "Bandwidth (GB/s) CUDA Cores Tensor Cores 8 8 80 80 LPDDR4 GDDR6 HBM2e HBM3 59.7 224 2039 2000 256 2,560 6,912 16,896 0 80 432 E.2. Binary GEMV Inference Figure 9. On the NVIDIA Jetson TX2, our custom GEMV kernels show significantly faster inference speeds than PyTorch FP16 for various matrix shapes, even for batch sizes up to 16. GEMV CUDA kernel details. The GEMV kernel implements two-stage, 1-bit quantized matrixvector multiplication for float16 and bfloat16 tensors. In each stage, the input (or intermediate) vector is multiplied by weight matrix whose signs are stored as packed bitfield: each weight occupies single bit in uint32 array. Because the binary matrices are low-rank, the effective reduction in memory traffic is typically less than the theoretical 16, but the packing still 19 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models Table 11. Throughput (tokens/s) and peak memory (GB) for varying sequence lengths, for Llama-2 models compressed to 0.55 bits with NANOQUANT. Extreme compression with NANOQUANT enables fast and memory-efficient inference on an NVIDIA RTX 3050 8GB."
        },
        {
            "title": "Sequence Length",
            "content": "32 64 128 256 512 Llama-2-7B Llama-2-13B Llama-2-70B Tokens/s Peak Mem (GB) Tokens/s Peak Mem (GB) Tokens/s Peak Mem (GB) 134.10 133.40 127.04 122.52 108.44 86.27 1.59 1.09 1.32 1.07 1.12 1. 83.83 1.70 20.11 5.86 83.35 1.73 19.74 5.87 81.43 1.78 19.18 5. 75.32 1.88 17.68 5.93 65.31 2.09 15.37 6.02 51.63 2.57 12.13 6. yields substantial bandwidth saving. During execution the bits are unpacked on-the-fly with lightweight mask operation, after which fused-multiply-add (FMA) is performed using vectorized float16 or bfloat16 intrinsics that process two low-precision values per instruction. Per-column scaling factors are incorporated directly into the FMA, and an optional per-row scaling is applied before writing the intermediate or final result. The kernel does not invoke Tensor-Core WMMA operations; instead it relies on standard FP16 arithmetic, making it matmul-free implementation that avoids the overhead of explicit matrix-multiply APIs. Results. We first benchmark our binary GEMV CUDA kernels against PyTorch BF16 and 2 state-of-the-art vector quantization methods, QTIP (Tseng et al., 2024b) and AQLM (Egiazarian et al., 2024), on single NVIDIA H100 GPU, as shown in Figure 7. To fully encompass the performance of the kernels, we measure the throughput (tokens per second), peak allocated GPU memory, and average energy per token during the single batch, end-to-end decoding process. We utilize open-source models provided by QTIP and AQLM, and utilize models from the Llama2 (Touvron et al., 2023) and Llama3 (Grattafiori et al., 2024) families. Next, we benchmark our binary GEMV CUDA kernels on NVIDIA Jetson TX2, which does not have any NVIDIA Tensor Cores, as in Table 10. We find that the extreme compression capability of NANOQUANT enables up to 12.2 speedup in inference throughput, compared to PyTorch FP16, as shown in Figure 9. E.3. Binary GEMM Inference Figure 10. Custom GEMM kernels for NANOQUANT achieve competitive batched inference performance with BF16 PyTorch on single NVIDIA A100 80GB GPU. NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models GEMM Kernel Details Our GEMM kernel is highly optimized CUDA GEMM implementation specifically designed for efficient low-rank binary matrix multiplication in quantized neural networks. We base our implementation on the Marlin GEMM kernel (Frantar et al., 2025), which leverages NVIDIA Tensor Cores for matrix multiplication through inline PTX assembly, processing matrix (e.g.,. 16 8 16) tiles with mma.sync operations. The kernel employs multi-stage pipeline (default 4 stages) with asynchronous memory operations (cp.async) to overlap data transfers with computation, effectively hiding memory latency. It efficiently handles binary matrices by packing multiple 1-bit values into 32-bit words and using bit manipulation for fast dequantization, while maintaining computation in FP16/BF16 precision for accuracy. While GEMV excels in low-batch, memory-bound scenarios, large-scale deployments benefit from batched GEMM operations that saturate tensor core throughput. Therefore, Binary GEMM kernels are necessary for compute-bound LLM serving operations, especially for datacenter GPUs to fully utilize matrix-multiplication computation units, such as NVIDIA Tensor Cores. The pipelined execution of our binary GEMM kernels keep compute units busy by overlapping the data loading, computation, and storing phases, while the warp-level parallelism with optimized thread scheduling maximizes GPU utilization. These optimizations result in high arithmetic intensity and efficient use of hardware resources, making the kernel particularly well-suited for quantized neural network inference where binary low-rank matrices significantly decrease memory requirements and bandwidth usage without sacrificing model accuracy. 21 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models F. Detailed Model Size Analysis of Binary Weight Quantization Methods We analyze the storage cost of binary weight representation methods, encompassing both post-training quantization (PTQ) and quantization-aware training (QAT) via low-rank factorization. We count all stored bits including binarized weights (or binary factor matrices), reconstruction coefficients (typically FP16), and any auxiliary flags or bitmaps required to accurately decode the quantized model. This allows for fair assessment of the memory requirements across diverse binary compression paradigms. F.1. Unified Storage Metric Bits Per Weight (BPW). Let BWQ and BWFP be the total number of bits required to represent quantized weights and full-precision weights, respectively. We derive BPW, the average number of bits per full-precision weight in the quantized model, as: BPW = 16. (27)"
        },
        {
            "title": "BWQ\nBWFP",
            "content": "For instance, if we binarize all weight values in full-precision matrix WFP Rnm to 1 without additional metadata, the BPW value would be nm 16nm 16 = 1. If we assume WQ is two-dimensional matrix, BWFP = 16mn, and thus BPW = BWQ 16mn 16 = BWQ mn . F.2. Overview of Memory Requirements of Binary Quantization Methods Notation. We consider weight matrix Rnm, where is the number of rows and is the number of columns. We define as the block size (typically 128), and as the number of salient columns. The number of blocks per row is denoted as m/k. We assume reconstruction scales and means are stored in FP16 (16 bits). Additionally, denotes the storage cost for the salient column bitmap (often compressed). Methods. We analyze the memory requirements of state-of-the-art binary PTQ methods (BiLLM (Huang et al., 2024), ARB-LLM (Li et al., 2024), STBLLM (Dong et al., 2024), HBLLM (Chen et al., 2025b)), binary QAT methods using low-rank binary matrices (DBF (Boža & Macko, 2025), LittleBit (Lee et al., 2025)), and NANOQUANT. F.3. Binary PTQ Methods BiLLM. BiLLM (Huang et al., 2024) partitions the weight matrix into salient and non-salient parts. It employs secondorder binarization for salient columns and first-order binarization with two quantization groups for non-salient columns. Based on the analysis in (Chen et al., 2025b), the total memory requirement MBILLM is formulated as: MBiLLM = 2nc + m/k 3n 16 } {z Second-order binarization (Salient) + n(m c) + m/k 2n 16 2 } {z First-order binarization (Non-salient, 2 groups) + nm {z} Non-salient group bitmap + {z} Salient column bitmap = n(2m + c) + + 112n m/k , (28) where is the number of salient columns. The term 3n in the second-order part accounts for parameters α1, α2 and the combined mean µ. With MBILLM, we can derive the BPW equation as BPWBiLLM = MBILLM mn = 2 + nc + + 112n m/k mn . (29) 22 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models STBLLM STBLLM (Dong et al., 2024) extends the BiLLM framework by introducing : sparsity and finer-grained grouping. Unlike BiLLM, which uses 2 groups, STBLLM categorizes non-salient weights into 3 groups (sparse, intermediate, dense) using trisection search, requiring 2-bit group bitmap per stored element. Additionally, STBLLM employs : structured sparsity (e.g., 4:8 or 6:8) for the non-salient weights. This requires storing the indices of the non-zero elements. For standard : pattern, the index storage MIndices is determined by the combinatorics of choosing positions out of , typically log2 (cid:1) bits per block of weights. (cid:0)M The total memory requirement MSTBLLM is formulated as: MSTBLLM = 2nc + m/k 3n 16 } {z Second-order binarization (Salient) + + [n(m c) + 2nm] {z Binarized non-zero weights and Group bitmap n(m c) (cid:18)M log2 (cid:19)(cid:25) (cid:24) } {z Sparsity Indices (Metadata) } + m/k 2n 16 3 } {z FP16 scales/means (3 groups) + {z} Salient column bitmap (30) where and are the matrix dimensions, is the number of salient columns, and is the block size. Dividing by the total original parameters mn yields the Bit-Width Per Weight (BPW) equation: BPWSTBLLM = = + MSTBLLM mn (cid:16) (cid:17) 1 + 2 M 144nm/k + mn + 2c + 1 (cid:16) 1 (cid:17) (cid:24) log2 (cid:19)(cid:25) (cid:18)M (31) ARB-LLM ARB-LLM (Li et al., 2024) utilizes alternating refined binarization. We analyze the storage for the ARB-LLMRC variant, as derived in (Li et al., 2024). This method applies second-order binarization to both salient and non-salient parts using 2 groups: MARBLLM-RC = 2nc + (m/k 2n + 2c) 16 } {z Second-order binarization (Salient, 2 groups) + n(m c) + (m/k + (m c)) 16 2 } {z First-order binarization (Non-salient, 2 groups) + nm {z} Group bitmap + {z} Salient column bitmap = n(2m + c) + 33m + 64n m/k . With MARBLLM-RC, we can derive the BPW equation as BPWARBLLM-RC = MARBLLM-RC mn = 2 + nc + 33m + 64n m/k mn . 23 (32) (33) NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models HBLLM HBLLM (Chen et al., 2025b) introduces structure-aware grouping with two primary variants: HBLLM-row and HBLLM-col. HBLLM-row employs neighborhood averaging strategy for non-salient weights and utilizes four subgroups per row for coefficients: MHBLLM-ROW = nm + m/k 3n 16 2 } {z Unsalient weights (2 groups) + nc + m/k 2n 16 2 } {z Salient weights (2 groups) + n(m + c) } {z Group bitmap + {z} Salient column bitmap = 2n(m + c) + + 160n m/k ."
        },
        {
            "title": "We can derive the BPW equation as",
            "content": "BPWHBLLM-row = MHBLLM-ROW mn = 2 + 2nc + + 160n m/k mn . (34) (35) HBLLM-col shares subgroups across two rows and applies intra-band mean sharing, reducing the coefficient overhead: MHBLLM-COL = n(m c) + m/k 1.5n 16 2 } {z Unsalient weights (2 groups) + nc + m/k 2n 16 2 } {z Salient weights (2 groups) + nm {z} Group bitmap + {z} Salient column bitmap = 2nm + + 112n m/k . We can derive the BPW equation as BPWHBLLM-col = MHBLLM-COL mn = 2 + + 112n m/k mn . F.4. Binary QAT Methods Using Low-Rank Binary Matrices (36) (37) DBF and LittleBit Double Binary Factorization (DBF) (Boža & Macko, 2025) and Littlebit (Lee et al., 2025) approximate the weight matrix as: cW = Diag(s1) (cid:0) U1 Diag(smid) 1 (cid:1) Diag(s2), (38) where U1 {1}nr and V1 {1}mr are stored as 1-bit entries, and the scales s1 Rn, smid Rr, s2 Rm are stored in FP16. The total storage is: We can derive the BPW equation as MDBF = r(n + m) + 16(n + + m). BPWDBF = r(n + m) + 16(n + + m) mn 24 (39) (40) NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models F.5. NANOQUANT (Ours) Our method simplifies the factorization structure, by removing the rank-wise scale smid via 2-scale system: The total storage required is: cW = Diag(s1) U1 1 Diag(s2). MNANOQUANT = r(n + m) + 16(n + m). This reduction in scalar overhead contributes to lower BPW compared to DBF at the same rank r."
        },
        {
            "title": "We can derive the BPW equation as",
            "content": "BPWNANOQUANT = r(n + m) + 16(n + m) mn (41) (42) (43) F.6. Compression Comparison Compressed Model Comparison. To evaluate the compression capability of each quantization method, we compute the BPWmodel for model containing linear layers {Wℓ}L l=1 in LLM decoder blocks, with dimensions nℓ mℓ. The total memory bits are given by Mtotal = PL ℓ=1 Mℓ, where Mℓ is calculated using the formulas of each respective method. The effective bits per weight is: BPWmodel = PL ℓ=1 Mℓ ℓ=1 nℓmℓ PL . (44) Notably, all open-source implementations of the baseline binary PTQ methods have maximum value of 50 salient columns (c 50) and unified block size value of = 128. With these constraints, we can derive the theoretical lower and upper bounds of the compression rate of all baselines. Table 12. Upper and lower bounds of quantized model size (GB) for various binary post-training quantization baseline methods, represented as (min, max). Model BF16 NANOQUANT BiLLM STBLLM4: STBLLM6:8 STBLLM8:8 ARB-LLMRC HBLLMR L2-7 L2-13 L2-70 12.55 24.24 128. 2.30 L3-1 5.98 L3-3 14.96 L3-8 L3-70 131.42 L3-405 755.96 G3-1 G3-4 G3-12 G3-27 Q3-0.6 Q3-1.7 Q3-4 Q3-8 Q3-14 1.86 7.26 21.95 50.35 1.11 3.20 7.49 15.26 27.51 1.24 2.08 8. 0.60 1.06 2.77 11.86 65.50 0.64 1.62 3.12 5.59 0.34 0.74 1.14 3.12 4.43 (2.85, 2.86) (5.22, 5.23) (25.65, 25.69) (3.36, 3.36) (6.21, 6.22) (31.00, 31.03) (3.76, 3.77) (7.00, 7.01) (35.28, 35.30) (3.86, 3.87) (7.20, 7.21) (36.35, 36.39) (2.55, 2.56) (4.63, 4.64) (22.47, 22.51) (3.16, 3.17) (5.81, 5.84) (28.86, 28.94) (1.40, 1.40) (2.59, 2.59) (4.61, 4.62) (28.81, 28.85) (152.76, 152.88) (1.48, 1.48) (2.81, 2.81) (5.16, 5.16) (34.15, 34.18) (184.14, 184.21) (1.54, 1.54) (2.99, 2.99) (5.59, 5.60) (38.43, 38.46) (209.24, 209.32) (1.55, 1.56) (3.03, 3.04) (5.70, 5.71) (39.50, 39.54) (215.52, 215.64) (1.36, 1.36) (2.46, 2.47) (4.29, 4.30) (25.62, 25.66) (134.01, 134.13) (171.58, 171.83) (1.45, 1.45) (2.72, 2.73) (4.94, 4.96) (32.01, 32.10) (1.46, 1.46) (3.84, 3.85) (7.90, 7.91) (14.84, 14.87) (1.51, 1.52) (4.09, 4.09) (8.74, 8.75) (16.84, 16.86) (1.56, 1.56) (4.29, 4.29) (9.41, 9.42) (18.44, 18.46) (1.57, 1.57) (4.34, 4.35) (9.58, 9.59) (18.84, 18.87) (1.43, 1.43) (3.69, 3.70) (7.40, 7.41) (13.65, 13.68) (1.49, 1.50) (3.99, 4.00) (8.40, 8.43) (16.04, 16.09) (0.78, 0.78) (1.75, 1.76) (2.86, 2.87) (4.99, 5.00) (7.86, 7.87) (0.82, 0.82) (1.86, 1.86) (3.15, 3.15) (5.53, 5.53) (8.89, 8.90) (0.84, 0.84) (1.95, 1.95) (3.37, 3.38) (5.96, 5.97) (9.72, 9.73) (0.85, 0.85) (1.97, 1.98) (3.43, 3.44) (6.07, 6.08) (9.93, 9.94) (0.76, 0.76) (1.69, 1.69) (2.70, 2.70) (4.67, 4.68) (7.25, 7.26) (0.80, 0.81) (1.82, 1.83) (3.03, 3.05) (5.31, 5.33) (8.48, 8.51) 25 NANOQUANT : Efficient Sub-1-Bit Quantization of Large Language Models Table 13. Upper and lower bounds of bits-per-weight (BPW) for quantized models of various binary post-training quantization baseline methods, represented as (min, max). Model BF16 NANOQUANT BiLLM STBLLM4:8 STBLLM6:8 STBLLM8:8 ARB-LLMRC HBLLMR L2-7 L2-13 L2-70 16.00 16.00 16.00 16.00 L3-1 16.00 L3-3 16.00 L3-8 L3-70 16.00 L3-405 16.00 G3-1 G3-4 G3-12 G316.00 16.00 16.00 16.00 Q3-0.6 16.00 Q3-1.7 16.00 16.00 Q3-4 16.00 Q3-8 16.00 Q3-14 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1. (2.88, 2.89) (2.88, 2.88) (2.88, 2.88) (2.88, 2.90) (2.88, 2.89) (2.88, 2.89) (2.88, 2.88) (2.88, 2.88) (2.88, 2.91) (2.88, 2.89) (2.88, 2.89) (2.88, 2.88) (2.88, 2.92) (2.88, 2.90) (2.88, 2.89) (2.88, 2.89) (2.88, 2.88) (3.50, 3.51) (3.50, 3.51) (3.50, 3.50) (3.50, 3.51) (3.50, 3.51) (3.50, 3.51) (3.50, 3.50) (3.50, 3.50) (3.50, 3.52) (3.50, 3.51) (3.50, 3.51) (3.50, 3.51) (3.50, 3.53) (3.50, 3.51) (3.50, 3.51) (3.50, 3.51) (3.50, 3.51) (4.00, 4.01) (4.00, 4.01) (4.00, 4.00) (4.00, 4.01) (4.00, 4.01) (4.00, 4.01) (4.00, 4.00) (4.00, 4.00) (4.00, 4.02) (4.00, 4.01) (4.00, 4.01) (4.00, 4.01) (4.00, 4.03) (4.00, 4.01) (4.00, 4.01) (4.00, 4.01) (4.00, 4.01) (4.13, 4.14) (4.13, 4.13) (4.13, 4.13) (4.13, 4.15) (4.13, 4.14) (4.13, 4.14) (4.13, 4.13) (4.13, 4.13) (4.13, 4.16) (4.13, 4.14) (4.13, 4.14) (4.13, 4.13) (4.13, 4.17) (4.13, 4.15) (4.13, 4.14) (4.13, 4.14) (4.13, 4.13) (2.51, 2.52) (2.51, 2.51) (2.50, 2.51) (2.51, 2.53) (2.51, 2.52) (2.51, 2.52) (2.50, 2.51) (2.50, 2.50) (2.52, 2.55) (2.51, 2.53) (2.51, 2.52) (2.50, 2.51) (2.52, 2.56) (2.51, 2.53) (2.51, 2.52) (2.51, 2.52) (2.51, 2.51) (3.25, 3.27) (3.25, 3.27) (3.25, 3.26) (3.25, 3.29) (3.25, 3.28) (3.25, 3.27) (3.25, 3.26) (3.25, 3.25) (3.25, 3.32) (3.25, 3.28) (3.25, 3.27) (3.25, 3.27) (3.25, 3.33) (3.25, 3.29) (3.25, 3.28) (3.25, 3.27) (3.25, 3.27)"
        }
    ],
    "affiliations": [
        "Samsung Research, Seoul, Korea"
    ]
}