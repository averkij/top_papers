{
    "paper_title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
    "authors": [
        "Michael Tschannen",
        "Alexey Gritsenko",
        "Xiao Wang",
        "Muhammad Ferjad Naeem",
        "Ibrahim Alabdulmohsin",
        "Nikhil Parthasarathy",
        "Talfan Evans",
        "Lucas Beyer",
        "Ye Xia",
        "Basil Mustafa",
        "Olivier Hénaff",
        "Jeremiah Harmsen",
        "Andreas Steiner",
        "Xiaohua Zhai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed techniques into a unified recipe -- this includes captioning-based pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, including zero-shot classification, image-text retrieval, and transfer performance when extracting visual representations for Vision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements on localization and dense prediction tasks. We also train variants which support multiple resolutions and preserve the input's native aspect ratio. Finally, we train on a more diverse data-mixture that includes de-biasing techniques, leading to much better multilingual understanding and improved fairness. To allow users to trade off inference cost with performance, we release model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M), and g (1B)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 6 8 7 4 1 . 2 0 5 2 : r February SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features Michael Tschannen*,, Alexey Gritsenko*, Xiao Wang*, Muhammad Ferjad Naeem*, Ibrahim Alabdulmohsin*, Nikhil Parthasarathy*, Talfan Evans*,, Lucas Beyer*,, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner and Xiaohua Zhai*,, Google DeepMind, *Core contributor, Project lead, Work done while at Google DeepMind We introduce SigLIP 2, family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed techniques into unified recipethis includes captioningbased pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, including zero-shot classification, image-text retrieval, and transfer performance when extracting visual representations for Vision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements on localization and dense prediction tasks. We also train variants which support multiple resolutions and preserve the inputs native aspect ratio. Finally, we train on more diverse data-mixture that includes de-biasing techniques, leading to much better multilingual understanding and improved fairness. To allow users to trade off inference cost with performance, we release model checkpoints at four sizes: ViT-B (86M), (303M), So400m (400M), and (1B). 1. Introduction image-text Contrastive embedding models trained on billion-scale datasets, as pioneered by CLIP [50] and ALIGN [28], have become the mainstream approach for high-level, semantic understanding of visual data. These models zero-shot classification enable fine-grained, rivaling the quality of supervised methods and enable efficient text-to-image and image-to-text retrieval. Furthermore, they lead to excellent vision-language understanding capabilities when combined with Large Language Models (LLMs) to build Vision-Language Models (VLMs). Developing on the success of CLIP, several improvements have been proposed such as recaptioning images [38], adding image-only selfsupervised losses [38, 45], and training with small decoder for auxiliary tasks such as captioning and localization [32, 62, 67]. At the same time, several groups have released model checkpoints for the open-source community [19, 27, 50, 57, 70]. However, these releases do not include the full breadth of latest improvements into single model, as they all relatively closely follow CLIPs original approach. Here, building on the SigLIP training recipe [71], we incorporate several improvements from prior work and release new family of open models1 that both excel on CLIPs core capabilitieszero-shot classification, retrieval, and feature extraction for VLMsand improve areas where vanilla CLIP-style models lag behind, including localization and extracting dense, semantic representations. In summary, SigLIP 2 models provide the following: Strong multilingual vision-language encoders: SigLIP 2 shows excellent performance on English-focused vision-language tasks while providing strong results on multilingual benchmarks with single model. This enables use in wide range of languages and cultural contexts. Dense features: We incorporate self1Model checkpoints are available at https://github.com/google-research/big_vision/tree/main/ big_vision/configs/proj/image_text/README_siglip2.md Corresponding author(s): tschannen@google.com 2025 Google DeepMind. All rights reserved SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features supervised losses as well as decoder-based loss, which result in better dense features (e.g. for segmentation and depth estimation) and improve localization tasks (such as referring expression comprehension). Backward compatibility: SigLIP 2 is designed to be backward compatible with SigLIP by relying on the same architecture. This allows existing users to simply swap out the model weights and tokenizer (which is now multilingual) to get improvements on wide range of tasks. Native aspect ratio and variable resolution: SigLIP 2 also includes NaFlex variant, which supports multiple resolutions and preserves the native image aspect ratio. These models have the potential to improve aspect sensitive applications such as document understanding. Strong small models: SigLIP 2 further optimizes performance of smaller models (B/16 and B/32 models), by using techniques in distillation via active data curation. In the next section we provide detailed description of the SigLIP 2 training recipe. Sec. 3 presents evaluations of SigLIP 2 models and baselines across variety of tasks and benchmarks. Finally, Sec. 4 gives short overview of related work, and conclusions can be found in Sec. 5. 2. Training recipe We combine the original SigLIP training [71] with decoder-based pretrainrecipe in addition to self-distillation ing [60, 62], and masked prediction as in the DINO line of work [9, 47] (see Fig. 1 for an overview). Pretraining an image encoder with language decoder for captioning and referring expression comprehension was shown to improve OCR capabilities and localization [62], whereas self-distillation and masked prediction leads to better features for dense prediction tasks, zero-shot classification and retrieval [38, 45]. Rather than combining all these techniques in single run we follow staged approach as outlined below to manage the computational and memory overhead compared to SigLIP training. Figure 1 SigLIP 2 adds the captioning-based pretraining from LocCa [62] as well as selfdistillation and masked prediction from SILC [45] and TIPS [38] (during the last 20% of training) to the sigmoid loss from SigLIP [71]. For some variants, the recipe additionally involves fine-tuning with data curation [61] or adaptation to native aspect ratio and variable sequence length [6, 12]. In addition to training set of models and adapting each model separately to different resolutions while distorting the aspect ratio, we also train variants which process images while largely preserving their native aspect ratio like NaViT [12] and support different sequence lengths as FlexiViT [6]. We call this variant NaFlex, described in Sec. 2.4.2. Finally, to improve the quality of the smallest models we fine-tune those with implicit distillation via active sample selection, following the approach from [61]. 2.1. Architecture, training data, optimizer For the architecture, we follow SigLIP [71] so that existing users can simply swap out the encoder weights. Specifically, the fixed-resolution variant relies on the standard ViT architecture [15] with learned positional embedding. We use the same architecture for the image and text tower, except for the g-sized vision encoder which is paired with an So400m-sized [1] text encoder. Vision and text representations are pooled using MAP head (at2 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features tention pooling) [69]. We set the text length to 64 and use the multilingual Gemma tokenizer [22] with vocabulary size 256k, transforming the text to lower case before tokenization. We use the WebLI dataset [10] containing 10 billion images and 12 billion alt-texts covering 109 languages. To strike good balance between quality on English and multilingual visionlanguage benchmarks we compose the mixture such that 90% of the training image-text pairs is sourced from English web pages, and the remaining 10% from non-English web pages, as recommended in [49]. We further apply the filtering techniques from [2] to mitigate data biases in representation and association with respect to sensitive attributes. Unless noted otherwise, we use the Adam optimizer with learning rate 103, decoupled weight decay 104 [37], and gradient clipping to norm 1. We set the batch size to 32k and use cosine schedule with 20k warmup steps, training for total of 40B examples. Our models are trained on up to 2048 TPUv5e chips [24] using fullysharded data-parallel strategy (FSDP [72]). ordinates for captions describing specific image regions, whereas the latter involves predicting region-specific captions given bounding box coordinates. Region-caption pairs are automatically annotated by first extracting n-grams from the alttexts and then applying open-vocabulary detection using the recipe from [41]. Additionally, we use the fixed set of object categories from [10] instead of n-grams. For each example, the decoder is trained to predict all three targets (amounting to three decoder forward-passes). The captioning target is predicted with parallel prediction [60] with probability of 50%, i.e. all caption tokens are predicted in parallel from mask tokens, without causal attention mask. Please refer to [62] for more detail. Finally, to reduce memory consumption due to the large vocabulary, we implement chunked version of the decoder loss. For all model sizes, we set the vision encoder patch size to 16 and the image resolution to 256 (resulting in an image representation sequence length of 256). Finally, we note that the decoder only serves for representation learning here and is not part of the model release. 2.2. Training with Sigmoid loss and decoder 2.3. Training with self-distillation and masked In the first step of pretraining, we combine SigLIP [71] with LocCa [62] by simply combining the two losses with equal weight. Unlike CLIP [50], which relies on contrastive loss, SigLIP creates binary classification problems by combining every image embedding with every text embedding in the mini-batch and trains the embeddings to classify matching and nonmatching pairs via logistic regression (sigmoid loss). We use the original implementation and refer to [71] for details. prediction Following SILC [45] and TIPS [38], we augment the training setup described in Sec. 2.2 with local-to-global correspondence learning with self-distillation and masked prediction losses [9, 47, 75] to improve the local semantics of the (un-pooled) feature representation. This representation is typically used for dense prediction tasks like segmentation, depth estimation etc. Concretely, we add two terms to the losses described in Sec. 2.2 as detailed next. For LocCa, we attach standard transformer decoder with cross-attention to the un-pooled vision encoder representation (before applying the MAP head). The decoder follows the shapes of the text encoder except that we add crossattention layers and reduce the number of layers by factor of two. Besides image captioning, LocCa also trains for automatic referring expression prediction and grounded captioning. The former amounts to predicting bounding box coThe first term is the local-to-global consistency loss from [45], in which the vision encoder becomes the student network, which gets partial (local) view of the training image, and is trained to match the teachers representation, derived from the full image. This auxiliary matching task is performed in high-dimensional feature space computed with separate MLP head. As is common in the literature, the teacher parameters are obtained as an exponential moving average SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features (EMA) of the student parameters over the previous iterations. We rely on single global (teacher) view and 8 local (student) views and otherwise follow the augmentations, loss and hyper parameters from [45]. The second loss term is the masked prediction objective from [38]. We replace 50% of the embedded image patches in the student network with mask tokens and train the student to match the features of the teacher at masked locations. The loss is then defined identically to the first term (consistency loss), but applied to per-patch features rather than the pooled, image-level representation. Further, both the student and the teacher see the same, global view (up to masking in the student). We add these losses at 80% of training completion, initializing the teacher with the student parameters and the remaining additional parameters (heads, mask token and corresponding optimizer parameters) randomly. We use the original image for computing the SigLIP and LocCa losses from the previous section and apply the additional losses on additional augmented views. This is done to ensure that data augmentation does not negatively impact the image-text alignment as recommended by [45]. The weights of the first and the second loss terms are set to 1 and 0.25. Further, to balance model quality on global/semantic and dense tasks, we re-weight the two loss terms by another factor of 0.25, 0.5, 1.0, and 0.5 for the B, L, So400m and g, model sizes, respectively. 2.4. Adaptation to different resolutions 2.4.1. Fixed-resolution variant To obtain fixed-resolution checkpoints at multiple resolutions, we resume the checkpoints (with sequence length 256 and patch size 16) at 95% of training, resize the positional embedding to the target sequences length (and in some cases the patch embedding from patch size 16 to 14 with the pseudoinverse (PI)-resize strategy from [6]), and resume the training at the target resolution with all losses. We opt for this approach as the common strategy of fine-tuning the final checkpoint with smaller learning rate and without weight decay [71] did not lead to good results across all sizes and resolutions. 2.4.2. Variable aspect and resolution (NaFlex) NaFlex combines ideas from FlexiViT [6], i.e. supporting multiple, predefined sequence lengths with single ViT model, and NaViT [12], namely processing images at their native aspect ratio. This enables processing different types of images at appropriate resolution, e.g. using larger resolution to process document images, while at the same time minimizing the impact of aspect ratio distortion on certain inference tasks, e.g. on OCR. Given patch size and target sequence length, NaFlex preprocesses the data by first resizing the input image such that the height and width after resizing are multiples of the patch size, while 1) keeping the aspect ratio distortion as small as possible and 2) producing sequence length of at most the desired target sequence length. The resulting distortion in width and height is at most (patch_size-1)/width and (patch_size-1)/height, respectively, which tends to be small for common resolutions and aspect ratios. Note that NaViT incurs the same type of distortion. After resizing, the image is split into sequence of patches, and patch coordinates as well as mask with padding information is added (to handle the case where the actual sequence length is smaller than the target length). To process different sequence lengths (and aspect ratios) with ViT, we bilinearly resize (with anti-aliasing) the learned positional embedding to the target, non-square patch grid for the resized input image. We set the length of the learned positional embedding to 256, assuming 16 16 patch grid before resizing. When the sequence length after resizing is smaller than the target sequence length, the attention layers (including the MAP head) are masked to ignore the extra padding tokens. As for the fixed-resolution, adapted variants, we start from the default checkpoints trained with the setup described in Sec. 2.2, i.e. with nonaspect preserving resize to 256px, resulting in sequence length of 256. We take the checkpoint at 90% training completion, then switch to aspect4 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features ViT Res. Seq. Model val v2 ReaL ObjNet 10s. TI IT TI IT TI IT ImageNet-1k COCO Flickr XM B/32 B/16 L/14 L/16 So/14 So/ H/14 g/16 64 224 256 384 224 224 576 256 196 512 1024 49 MetaCLIP [66] 59.6 67.7 64.8 OpenCLIP [27] 72.8 74.0 66.9 81.4 SigLIP 2 61.9 CLIP [50] 68.3 62.3 OpenCLIP [27] 70.2 65.1 72.4 MetaCLIP [66] 67.0 74.7 EVA-CLIP [57] 82.8 69.5 76.2 SigLIP [71] 68.2 76.2 DFN [19] 84.8 71.4 78.2 SigLIP 2 83.1 70.1 76.7 SigLIP [71] 85.4 72.5 79.1 SigLIP 2 84.6 72.0 78.6 SigLIP [71] 86.2 73.8 80.6 SigLIP 2 79.2 SigLIP [71] 84.9 72.9 81.2 74.5 86.7 SigLIP 2 61.1 OpenCLIP [27] 74.0 69.0 75.5 CLIP [50] 72.6 79.2 MetaCLIP [66] 72.8 79.7 CLIPA-v2 [33] 72.9 79.8 EVA-CLIP [57] 82.2 75.7 DFN [19] 85.9 74.2 80.5 SigLIP [71] 87.3 76.8 82.5 SigLIP 2 87.1 75.9 82.1 SigLIP [71] 83.1 87.6 77.4 SigLIP 2 83.5 77.8 87.7 512 1024 SigLIP 2 87.1 76.0 82.2 87.8 77.7 83.2 83.2 87.5 77.1 84.1 78.7 88.1 86.1 74.1 80.8 83.4 87.7 77.8 78.4 88.1 84.1 84.3 79.1 88.1 80.5 74.1 83.4 77.3 88.3 79.2 84.5 85.0 79.8 88.5 SigLIP [71] SigLIP 2 SigLIP [71] SigLIP 2 mSigLIP [71] SigLIP 2 SigLIP 2 384 512 1024 SigLIP MetaCLIP [66] DFN [19] SigLIP 2 SigLIP 2 256 576 256 384 256 576 729 256 256 256 224 256 384 256 224 52.8 59.6 66.1 55.3 56.0 60.0 62.3 70.7 63.2 73.6 71.3 74.5 73.8 77.1 74.8 77.8 66.4 69.9 74.6 71.1 75.3 74.8 77.9 83.0 80.9 84.4 84.6 80.5 84.6 82.9 86.0 79.5 84.8 85.8 86.2 76.5 76.5 87.1 88.0 84.8 57.9 46.6 39.9 22.4 40.3 22.5 40.7 23.3 41.2 23. 81.9 86.3 85.7 89.6 93.0 91.1 94.4 92.2 94.9 92.5 72.9 64.9 66.6 47.2 63.7 75.5 89.3 38.3 49.0 62.1 52.4 33.1 69.8 59.4 42.3 77.1 48.9 71.2 58.7 42.2 29.3 77.9 64.5 47.2 69.9 77.3 51.9 50.7 80.7 68.9 52.1 72.1 29.9 78.3 65.1 47.4 70.3 51.0 81.7 69.7 53.2 73.1 30.3 49.7 80.7 67.5 72.7 51.6 54.6 71.4 83.8 74.7 73.3 30.5 81.6 67.6 50.4 75.2 55.2 71.2 84.5 95.5 41.4 52.0 88.7 75.0 62.1 46.1 85.2 65.2 56.3 36.5 55.7 83.3 89.1 46.3 64.1 73.0 77.3 89.7 63.7 47.5 84.7 59.6 40.1 81.3 76.8 69.6 51.2 56.5 84.1 78.8 54.7 71.5 39.7 82.6 70.5 52.8 78.7 79.5 55.3 71.4 56.3 85.0 79.6 55.2 72.1 85.3 95.8 47.4 56.7 22.8 76.6 78.2 57.5 84.3 79.5 79.4 26.6 80.5 80.4 55.8 71.7 85.7 94.9 48.4 57.5 92.1 50.0 62.8 49.4 77.1 80.0 68.6 57.5 79.7 48.1 94.2 55.4 71.5 84.4 57.5 85.3 95.9 48.3 80.4 56.0 71.2 57.6 48.3 80.5 56.0 57.5 63.1 55.7 71.3 85.5 95.4 85.0 86.5 48.2 58.2 85.3 82.1 82.5 56.1 72.8 86.0 95.4 48.6 57.9 30.9 46.5 31.4 47.1 92.0 94.5 92.9 95.2 69.0 71.5 70. 90.7 94.6 93.5 16.0 47.9 17.8 50.8 55.1 52.0 95.3 72.5 Table 1 Zero-shot classification, 10-shot (10s) classification (on the validation set), and retrieval performance (recall@1) of SigLIP 2 along with several baselines. SigLIP 2 outperforms the baselines often by large margindespite being multilingual. Note that DFN [19] relies on data filtering network fine-tuned on ImageNet, COCO, and Flickr. preserving resizing and uniformly sampling sequence length from {128, 256, 576, 784, 1024} per mini-batch. At the same time we stretch the learning rate schedule corresponding to the last 10% by factor 3.75 to ensure that each resolution is trained for sufficiently many examples. For the largest sequence length we further half the batch size and double the number of training steps to avoid out-of-memory errors. To keep implementation and computation complexity manageable, we do not apply selfdistillation and masked prediction from Sec. 2.3. 2.5. Distillation via active data curation To maximize performance of the smallest fixedresolution models (ViT-B/16 and ViT-B/32), we distill knowledge from teacher (reference) 5 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features Figure 2 Per-language image-text retrieval performance for SigLIP, SigLIP 2 and mSigLIP on Crossmodal-3600 [58]. SigLIP 2 almost matches the performance of mSigLIP (SigLIP trained on multilingual data) despite performing substantially better on English vision-language tasks  (Table 1)  . model during short fine-tuning stage. We lower the learning rate to 105, remove weight-decay, and continue training these models for an additional 4B examples using just the sigmoid imagetext loss. During this stage, we perform implicit distillation through data using the ACID method proposed in [61]. Briefly, at every training step, the teacher model and the current learner model are used to score examples by their learnability [42]. These scores are then used to jointly select an optimal batch of size 32k from larger super-batch [16]. Here, we select data with filtering ratio of 0.5 (i.e. super-batch size of 64k) to balance gains from curation with training compute. For the B/32 model, we find leveraging filtering ratio of 0.75 is worth the extra cost. We note that the authors in [61] suggest that the best performance is achieved with ACED, method that combines ACID with explicit softmaxdistillation (using second teacher trained on more diverse data). However, here we propose way to adapt ACID to capture these benefits without the need for explicit distillation, saving significant amounts of compute. Specifically, instead of utilizing two separate teacher models, we take single strong teacher trained on the diverse data (in this case, the SigLIP 2 So400m model) and fine-tune it for 1B examples on the high-quality curated dataset from [16]. We then use this fine-tuned teacher model in the ACID method, as described above. Because this teacher blends diverse knowledge of concepts from pretraining, with knowledge of what is high-quality (from the curated dataset), the implicit distillation of ACID alone is sufficient to recover the benefits of ACED. 3. Experiments and results 3.1. Zero-shot classification and retrieval In Table 1 we report the performance of SigLIP 2 along with baselines on common zero-shot classification (ImageNet [13] ObjectNet [4], ImageNetv2 [53], ImageNet ReaL [5]) and image-text retrieval benchmarks. SigLIP 2 performs better than SigLIP and other (open-weight) baselines across the board, despite supporting many languages unlike the baselines (except mSigLIP [71]). Note that DFN [19], which comes closest to SigLIP 2 on these benchmarks, uses network fine-tuned on ImageNet, COCO, and Flickr (i.e. the main benchmarks in Table 1) as filter to improve data quality. SigLIP 2s improvements over the baselines are particularly significant for the B-sized models owing to distillation (Sec. 2.5). Moreover, we observe the common scaling trends as function of image resolution and model size. 6 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features Figure 3 Comparing the NaFlex (a single checkpoint per model size supporting native aspect ratio and variable sequence length/resolution) and the standard square-input SigLIP 2 variants which use separate checkpoint for each sequence length/resolution. The sequence lengths annotated on the x-axis correspond to training sequence lengths for NaFlex. NaFlex interpolates fairly well between training resolutions, but does not extrapolate well (not shown). Table 1 and Figure 2 further show the multilingual retrieval performance on Crossmodal-3600 (XM3600) [58] covering 36 languages. SigLIP 2s recall exceeds that of SigLIP by large margin, while only lagging slightly behind mSigLIP, which in turn performs substantially worse than SigLIP and SigLIP 2 on English-focused benchmarks. in particular for small sequence lengths (and hence resolutions) which tend to suffer more from aspect ratio distortion. On benchmarks predominantly based on natural images, the standard B-sized variant outperforms NaFlex, arguably thanks to the distillation step, whereas for the So400m architecture the two are on par. This is remarkable since the standard variant also benefits from the self-distillation stage (Sec. 2.3). 3.1.1. NaFlex variant Fig. 3 compares the fixed-resolution squareaspect ratio (standard) SigLIP 2 with the aspectpreserving NaFlex variant (one checkpoint for all sequence lengths) as function of the sequence length. In addition to the retrieval benchmarks listed in the previous section, we add range of OCR/document/screen-focused imagetext benchmarks, namely TextCaps [55], HierText [36], SciCap [26] and Screen2Words [63]. The NaFlex variant outperforms the standard variant on the majority of these retrieval benchmarks, 3.2. SigLIP 2 as vision encoder for VLMs popular use case for vision encoders like CLIP and SigLIP is to extract visual representations for VLMs [3, 7, 32, 35, 39, 48, 59]. The common paradigm combines pretrained vision encoder with pretrained LLM and does multimodal training on rich mixture of vision language tasks. To evaluate the performance of SigLIP 2 in this application, we develop recipe similar to that of PaliGemma 2 [56]. Concretely, we combine 7 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features Figure 4 Comparison of different vision encoders after training Gemma 2 LLM for 50M steps with frozen vision encoder (PaliGemma [7] stage 1), followed by fine-tuning the VLM on individual datasets (PaliGemma stage 3). SigLIP 2 performs better than SigLIP and AIMv2 [20] for different model sizes and resolutions. Same data as in Table 6. SigLIP 2 vision encoders and baselines with the Gemma 2 2B LLM [23] and train the LLM on 50M examples of the Stage 1 training mix from [7, 56] involving captioning, OCR, grounded captioning, visual question answering, detection, and instance segmentation (the annotations for the last 4 tasks are machine-generated, see [7, Sec. 3.2.5] for details). We keep the vision encoder frozen (which has essentially no impact on quality [7, Sec. 5.4]) and reduce training duration to reflect typical open model use case. The resulting VLM is then fine-tuned on broad range of down8 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features Segmentation Depth Normals Model ViT Res. PASCAL ADE20k NYUv2 NAVI NYUv2 NAVI CLIP [50] OpenCLIP [27] SigLIP [71] SigLIP 2 SigLIP [71] SigLIP 2 L/14 G/14 So/14 So/ So/14 So/14 224 224 224 224 384 384 74.5 71.4 72.0 77.1 73.8 78.1 39.0 39.3 37.6 41. 40.8 45.4 0.553 0.541 0.576 0.493 0.563 0.466 0.073 0.083 0.067 0.069 0.064 24.3 25.9 24. 24.1 23.0 25.5 26.0 25.4 25.4 25.0 Table 2 Probing the frozen SigLIP 2 representation for range of dense prediction tasks (metrics: segmentation: mIoU; depth: RMSE; normals; angular RMSE). SigLIP 2 outperforms several other popular open-weight models, often by significant margin. stream tasks with the transfer settings from [56]. To understand the effect of the input resolution we perform experiments at resolution 224 or 256 (for models with patch size 14 and 16, respectively, to extract 256 image tokens) and 384px, but unlike [7, 56] we repeat stage 1 at 384px rather than starting from the 224px variant. Fig. 4 shows the results after fine-tuning for each dataset. Overall, SigLIP 2 clearly outperforms SigLIP across resolutions and model size. For an L-sized vision encoder, SigLIP 2 also outperforms the recently released AIMv2 model [20]. The data from Fig. 4 can also be found in Table 6. 3.3. Dense prediction tasks 3.3.2. Open-vocabulary segmentation Open-vocabulary segmentation aims to develop models that can segment any novel classes beyond fixed training vocabulary. Here, we evaluate SigLIP 2s performance on this task. We use Cat-Seg [11] as framework and compare performance across different models as proposed in [45]. We train Cat-Seg on COCOStuff-164k [8] with 172 classes and then test it on various representative datasets with different vocabularies: ADE20k [73, 74] with 847 or 150 classes (A-847/A-150), Pascal Context (PC459/PC-59) [43], and Pascal VOC (VOC-20/VOC21) [17]. The results can be found in Table 3. We observe that the SigLIP 2 at L/16 improves on SigLIP and even surpasses the much bigger OpenCLIP G/14 model [27]. 3.3.1. Semantic segmentation, depth estimation, surface normal estimation 3.4. Localization tasks We adopt the evaluation protocol from [38] and probe the frozen SigLIP 2 representation, either with linear layer or with DPT decoder [52], on six benchmarks spanning semantic segmentation, monocular depth estimation, and surface normal estimation (see [38, Sec. 4.1] for details on the protocol and hyper parameters). Note, we make one (necessary) change: where the original method concatenates the CLS token to each of the patch feature vectors, we concatenate the output embedding of the MAP head instead, as we use MAP head instead of CLS token. The results in Table 2 indicate that SigLIP 2 outperforms several previous open, CLIP-style vision encoders, including SigLIP, often by significant margin. 3.4.1. Referring expression comprehension To probe the referring expression comprehension capabilities of SigLIP 2 on different RefCOCO variants [29, 68] we apply the evaluation protocol from [62]. We attach 6-layer transformer decoder to the un-pooled, frozen vision encoder representation via cross-attention and train it from scratch on mix of all RefCOCO variants (see [62] for details). The results in Table 5 show that SigLIP 2 outperforms SigLIP as well as CLIP and pretraining via image captioning (Cap) by large margin, across resolutions and model sizes. This can be attributed to the decoder-based pretraining, as described in Sec. 2.2. SigLIP 2 is only outperformed LocCa, which we hypothesize 9 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features Model ViT A-847 PC-459 A-150 PC-59 VOC-20 VOCCLIP [50] L/16 OpenCLIP [27] G/14 L/16 SigLIP [71] L/16 SigLIP 2 10.8 13.3 14.0 14.3 20.4 21.4 23.9 24.1 31.5 36.2 37.5 38.8 62.0 61.5 61.6 62.4 96.6 97.1 96.1 97. 81.8 81.4 81.1 82.3 Table 3 We use Cat-Seg [11] to compare open-vocabulary segmentation performance (mIoU) of several models similar to [45]. We observe that SigLIP 2 offers respectable improvements over comparable and even bigger models. might be due to the fact that SigLIP 2 is pretrained on multilingual data. LocCa, on the other hand, is trained on text only from English web sites. Finally, note that we expect significant improvements when using the decoder from pretraining as observed for LocCa. 3.4.2. Open-vocabulary detection OWL-ViT [40] is popular method to adapt CLIPstyle vision-language models to open-vocabulary detection. Here, we apply this approach to SigLIP and SigLIP 2 models, closely following the data and optimizer configuration from [40]. The results in Table 4 show that SigLIP 2 achieves better performance than SigLIP on the two popular benchmarks COCO [34] and LVIS [25]. The relative improvement is most pronounced for the LVIS rare categories. Further, the results here are better than those in [40] which is likely because [40] used CLIP rather than SigLIP. 3.5. Cultural diversity and fairness Besides the improvement in model quality in SigLIP 2 compared to its predecessor, SigLIP 2 is also more inclusive in two aspects. First, we follow the recommendations of [49] and utilize training mixture comprising both English and multilingual data to enhance cultural diversity. Second, to address potential societal biases in the training data, we integrate the data de-biasing techniques from [2]. These techniques are applied to mitigate biases in both first-order statistics, such as disparities in gender representation, and second-order statistics, such as biased associations between gender and occupation. Next, we present the evaluation results. ViT Model COCO (AP) LVIS (AP) LVIS (APr) B/16 So/ SigLIP SigLIP 2 SigLIP SigLIP 2 42.2 42.8 44.3 45.2 33.0 34.4 39.5 40. 31.0 32.7 40.9 42.3 Table 4 Fine-tuned SigLIP and SigLIP 2 for openvocabulary detection via OWL-ViT [40]. Cultural Diversity To evaluate for cultural diversity, we report the zero-shot classification accuracy results using Dollar Street [54], GeoDE [51], and Google Landmarks Dataset v2 (GLDv2) [65]. We also include 10-shot geolocalization using Dollar Street and GeoDE, as proposed in [49]. For zero-shot evaluation on Dollar Street, we implement the methodology outlined in [54], mapping 96 topics within the dataset to corresponding ImageNet classes. This process results in subset of 21K images for our analysis. Fig. 5 shows set of representative results (full results are shown in Appendix C). We observe an improvement in these metrics in SigLIP 2 compared to SigLIP for the same model size and resolution, and the improvements are particularly significant in geolocalization tasks. For instance, 10-shot geolocalization accuracy in GeoDE (region) improves from 36.2% for SigLIP L/16 at 256px to 44.4% in SigLIP 2. Similarly, 0-shot accuracy on Dollar Street improves from 52.1% to 55.2% in the same models. In terms of fairness, we report two Fairness metrics. The first is representation bias, as defined in [2], which measures the tendency in the model to associate random object (such as cars) with particular gender group. As shown in Fig. 6, SigLIP 2 is significantly better than SigLIP. 10 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features RefCOCO RefCOCO+ RefCOCOg ViT Seq. Model val testA testB val testA testB val-u test-u 256 576 576 256 729 256 576 SigLIP [71] SigLIP 2 SigLIP [71] SigLIP 2 Cap [60] CapPa [60] CLIP [50] SigLIP [71] SigLIP 2 LocCa [62] SigLIP [71] SigLIP 2 SigLIP [71] SigLIP 2 SigLIP [71] SigLIP 2 SigLIP 2 SigLIP 2 64.05 83.76 67.17 85.18 60.64 64.17 65.21 67.33 86.04 88.34 70.76 87.28 64.68 86.42 67.66 87.88 87.31 88. 70.10 86.21 72.94 87.92 65.47 69.90 71.28 72.40 89.02 91.20 76.32 90.29 71.23 89.41 74.12 91.13 90.24 91.53 57.89 79.57 60.94 80.53 56.17 58.25 58.17 61.21 81.85 85.10 63.79 82.85 58.40 82.48 62.36 83.59 83.25 84.95 55.77 74.26 59.09 76.08 52.56 56.14 57.53 59.57 77.29 79.39 63.38 79.00 57.43 77.81 60.74 80.06 79.25 80.44 63.57 79.85 67.26 82.17 58.32 63.68 66.44 67.09 83.28 85.13 71.48 85.00 66.06 84.36 69.73 86.30 85.23 87.09 47.51 65.83 50.22 67.10 45.99 48.18 47.77 51.08 70.16 72.61 54.65 70.92 49.38 70.67 52.12 72.66 71.60 73.53 59.06 77.25 61.98 79.08 56.75 58.90 59.32 61.89 80.11 81.69 64.73 81.84 59.66 80.83 62.61 82.68 81.48 83. 60.33 77.83 62.64 79.60 57.99 59.91 60.24 62.90 80.78 82.64 65.74 82.15 60.88 81.27 63.24 83.63 82.14 84.14 So Table 5 Comparing SigLIP 2 models with SigLIP and other baselines from the literature on referring expression comprehension (Acc@0.5). For matching model size and sequence length (seq.) SigLIP 2 models outperform SigLIP models substantially. SigLIP 2 is only outperformed by LocCa, which uses the same decoder-based loss, but is trained on captions from English language websites only. For instance, while SigLIP L/16 at 256px has representation bias of about 35.5%meaning it prefers to associate random images with men over women more than 85.5% of the time SigLIP 2 of the same size and resolution has representation bias of 7.3% only. In addition, larger models tend to exhibit less representation bias than smaller models, in agreement with the earlier findings in [2]. and open-vocabulary tasks including detection [30, 40, 41] and segmentation [11, 14]. Besides the original CLIP release, several projects have released open-weight contrastive models [19, 27, 33, 57, 66, 71]. At high level, these works follow training methods that are relatively close to the original CLIP method, mainly [71] proposing modified loss functions and [19, 66] targeting data quality and filtering. We also investigate the Dollar Street 0-shot results by income level and the GeoDE results by geographic region as [49]. However, in this context we only observe very minor benefits, or no benefits when comparing SigLIP and SigLIP 2 models of matching size and resolution (some results shown in Table 9). 4. Related work Contrastive pretraining as popularized by CLIP [50] and ALIGN [28] has become the dominant approach for learning high-level, semantic, visual representations that perform well on classification and retrieval, as vision encoders for VLMs [3, 7, 32, 35, 39, 48, 59] More generally, large number of modifications and improvements to contrastive training have been proposed in the literature. [16, 19, 21, 61, 66] study filtering techniques to improve data quality. With similar motivation, [18, 31, 38, 46] re-caption training images with VLMs to improve the caption quality and hence the quality of the training signal. Another promising area has been to modify or augment the loss function. [38, 44, 45] combine CLIP with selfsupervised losses. Another popular approach is to add language decoder to train with captioning as an auxiliary task [32, 67]. Captioning as standalone representation learning task has attracted less attention, but can produce visual representations competitive with contrastive training [20, 60, 62, 64]. 11 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features Figure 5 10-shot and 0-shot accuracy for geographically diverse object classification tasks (Dollar Street, GeoDE), as well as geolocalization (GeoDE country/region) and landmark localization (GLDv2) tasks. SigLIP 2 consistently performs better than SigLIP (see Table 8 for additional results). 5. Conclusion In this work, we introduced SigLIP 2, family of open-weight multilingual vision-language encoders that builds on the success of SigLIP. By incorporating combination of techniques such as decoder-based pretraining, self-supervised losses, and active data curation, SigLIP 2 achieves significant improvements in zero-shot classification, transfer performance as vision encoder in VLMs, and in localization and dense prediction tasks. Furthermore, thanks to training on multilingual data and applying de-biasing filters, SigLIP 2 attains more balanced quality across culturally diverse data. Finally, the NaFlex variant enables the model to support multiple resolutions with single model checkpoint, while preserving the native image aspect ratio. We hope that our SigLIP 2 release will enable many exciting applications within the open-source community. Acknowledgments We would like to thank Josip Djolonga, Neil Houlsby, Andre Araujo, Kevis Maninis, and Phoebe Kirk for discussions and feedback on this project. We also thank Joan Puigcerver, André Susano Pinto, and Alex Bewley for infrastructure contributions to the big_vision code base, which were helpful for this project. Figure 6 Representation bias (association of random objects with gender; lower is better) for different models."
        },
        {
            "title": "References",
            "content": "[1] I. Alabdulmohsin, X. Zhai, A. Kolesnikov, and L. Beyer. Getting vit in shape: Scaling laws for compute-optimal model design. In NeurIPS, 2023. [2] I. Alabdulmohsin, X. Wang, A. P. Steiner, P. Goyal, A. DAmour, and X. Zhai. Clip the bias: How useful is balancing data in multimodal learning? In ICLR, 2024. [3] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. QwenVL: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv:2308.12966, 2023. [4] A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz. Objectnet: large-scale biascontrolled dataset for pushing the limits of object recognition models. NeurIPS, 2019. [5] L. Beyer, O. J. Hénaff, A. Kolesnikov, X. Zhai, and A. v. d. Oord. Are we done with imagenet? arXiv:2006.07159, 2020. [6] L. Beyer, P. Izmailov, A. Kolesnikov, M. Caron, S. Kornblith, X. Zhai, M. Minderer, M. Tschannen, I. Alabdulmohsin, and F. Pavetic. Flexivit: One model for all patch sizes. In CVPR, 2023. [7] L. Beyer, A. Steiner, A. S. Pinto, A. Kolesnikov, X. Wang, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello, T. Unterthiner, D. Keysers, 12 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features S. Koppula, F. Liu, A. Grycner, A. Gritsenko, N. Houlsby, M. Kumar, K. Rong, J. Eisenschlos, R. Kabra, M. Bauer, M. Bošnjak, X. Chen, M. Minderer, P. Voigtlaender, I. Bica, I. Balazevic, J. Puigcerver, P. Papalampidi, O. Henaff, X. Xiong, R. Soricut, J. Harmsen, and X. Zhai. PaliGemma: versatile 3B VLM for transfer. arXiv:2407.07726, 2024. [8] H. Caesar, J. Uijlings, and V. Ferrari. Cocostuff: Thing and stuff classes in context. In CVPR, 2018. [9] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In CVPR, pages 9650 9660, 2021. [10] X. Chen, X. Wang, S. Changpinyo, A. J. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan, C. Riquelme, A. Steiner, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut. PaLI: jointly-scaled multilingual language-image model. arXiv:2209.06794, 2022. [11] S. Cho, H. Shin, S. Hong, A. Arnab, P. H. Seo, and S. Kim. Cat-seg: Cost aggregation for open-vocabulary semantic segmentation. In CVPR, pages 41134123, 2024. [12] M. Dehghani, B. Mustafa, J. Djolonga, J. Heek, M. Minderer, M. Caron, A. Steiner, J. Puigcerver, R. Geirhos, I. M. Alabdulmohsin, et al. Patch npack: NaViT, vision transformer for any aspect ratio and resolution. NeurIPS, 2024. [13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. [14] J. Ding, N. Xue, G.-S. Xia, and D. Dai. Decoupling zero-shot semantic segmentation. In CVPR, pages 1158311592, 2022. [15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [16] T. Evans, N. Parthasarathy, H. Merzic, and O. J. Henaff. Data curation via joint example selection further accelerates multimodal learning. In NeurIPS Datasets and Benchmarks Track, 2024. [17] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010. [18] L. Fan, D. Krishnan, P. Isola, D. Katabi, and Y. Tian. Improving clip training with language rewrites. NeurIPS, pages 35544 35575, 2023. [19] A. Fang, A. M. Jose, A. Jain, L. Schmidt, A. T. Toshev, and V. Shankar. Data filtering networks. In ICLR, 2024. [20] E. Fini, M. Shukor, X. Li, P. Dufter, M. Klein, D. Haldimann, S. Aitharaju, V. G. T. da Costa, L. Béthune, Z. Gan, A. T. Toshev, M. Eichner, M. Nabi, Y. Yang, J. M. Susskind, and A. El-Nouby. Multimodal autoregressive pre-training of large vision encoders. arXiv:2411.14402, 2024. [21] S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. NeurIPS, 36, 2024. [22] Gemma Team. Gemma: Open models based on gemini research and technology. arXiv:2403.08295, 2024. [23] Gemma Team. Gemma 2: Improving open language models at practical size. arXiv:2408.00118, 2024. [24] Google Cloud. Introduction to Cloud https://cloud.google.com/ AcTPU. tpu/docs/intro-to-tpu, 20xx. cessed: 2024-07-04. 13 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features [25] A. Gupta, P. Dollar, and R. Girshick. Lvis: dataset for large vocabulary instance segIn CVPR, pages 53565364, mentation. 2019. [26] T.-Y. Hsu, C. L. Giles, and T.-H. Huang. Scicap: Generating captions for scientific figures. arXiv:2110.11624, 2021. [27] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar, H. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. OpenCLIP, 2021. [28] C. Jia, Y. Yang, Y. Xia, Y. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig. Scaling up visual and visionlanguage representation learning with noisy text supervision. In ICML, 2021. [29] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. ReferItGame: Referring to objects in photographs of natural scenes. In EMNLP, Oct. 2014. [30] W. Kuo, Y. Cui, X. Gu, A. Piergiovanni, and A. Angelova. Open-vocabulary object detection upon frozen vision and language models. In ICLR, 2023. [31] Z. Lai, H. Zhang, B. Zhang, W. Wu, H. Bai, A. Timofeev, X. Du, Z. Gan, J. Shan, C.-N. Chuah, Y. Yang, and M. Cao. VeCLIP: Improving clip training via visual-enriched captions. arXiv:2310.07699, 2024. [32] J. Li, D. Li, S. Savarese, and S. C. H. Hoi. BLIP-2: bootstrapping language-image pretraining with frozen image encoders and large language models. In ICML, 2023. [33] X. Li, Z. Wang, and C. Xie. Clipa-v2: Scaling clip training with 81.1% zero-shot imagenet accuracy within $10,000 budget; an extra $4,000 unlocks 81.8% accuracy. arXiv:2306.15658, 2023. [35] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In NeurIPS, 2023. [36] S. Long, S. Qin, D. Panteleev, A. Bissacco, Y. Fujii, and M. Raptis. ICDAR 2023 competition on hierarchical text detection and recognition. In ICDAR, 2023. [37] I. Loshchilov, F. Hutter, et al. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5, 2017. [38] K.-K. Maninis, K. Chen, S. Ghosh, A. Karpur, K. Chen, Y. Xia, B. Cao, D. Salz, G. Han, J. Dlabal, et al. TIPS: Text-image pretraining with spatial awareness. In ICLR, 2025. [39] B. McKinzie, Z. Gan, J. Fauconnier, S. Dodge, B. Zhang, P. Dufter, D. Shah, X. Du, F. Peng, F. Weers, A. Belyi, H. Zhang, K. Singh, D. Kang, A. Jain, H. Hè, M. Schwarzer, T. Gunter, X. Kong, A. Zhang, J. Wang, C. Wang, N. Du, T. Lei, S. Wiseman, G. Yin, M. Lee, Z. Wang, R. Pang, P. Grasch, A. Toshev, and Y. Yang. MM1: methods, analysis & insights from multimodal LLM pretraining. arXiv:2403.09611, 2024. [40] M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Mahendran, A. Arnab, M. Dehghani, Z. Shen, et al. Simple open-vocabulary obIn ECCV, pages 728755, ject detection. 2022. [41] M. Minderer, A. A. Gritsenko, and N. Houlsby. Scaling open-vocabulary object detection. In NeurIPS, 2023. [42] S. Mindermann, J. M. Brauner, M. T. Razzak, M. Sharma, A. Kirsch, W. Xu, B. Höltgen, A. N. Gomez, A. Morisot, S. Farquhar, et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In ICML, pages 1563015649, 2022. [34] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays, P. Perona, D. Ramanan, P. Dolla r, and C. L. Zitnick. Microsoft COCO: common objects in context. arXiv:1405.0312, 2014. [43] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.- W. Lee, S. Fidler, R. Urtasun, and A. Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014. SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features [44] N. Mu, A. Kirillov, D. Wagner, and S. Xie. SLIP: Self-supervision meets languageimage pre-training. In ECCV, pages 529 544, 2022. [45] M. F. Naeem, Y. Xian, X. Zhai, L. Hoyer, L. Van Gool, and F. Tombari. SILC: Improving vision language pretraining with selfdistillation. In ECCV, pages 3855, 2024. [46] T. Nguyen, S. Y. Gadre, G. Ilharco, S. Oh, Improving multimodal and L. Schmidt. datasets with image captioning. NeurIPS, 36, 2024. [47] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. TMLR, 2024. [48] Z. Peng, W. Wang, L. Dong, Y. Hao, S. Huang, S. Ma, and F. Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv:2306.14824, 2023. [49] A. Pouget, L. Beyer, E. Bugliarello, X. Wang, A. P. Steiner, X. Zhai, and I. Alabdulmohsin. No filter: Cultural and socioeconomic diversityin contrastive vision-language models. arXiv:2405.13777, 2024. [50] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. [51] V. V. Ramaswamy, S. Y. Lin, D. Zhao, A. Adcock, L. van der Maaten, D. Ghadiyaram, and O. Russakovsky. Geode: geographically diverse evaluation dataset for object recognition. NeurIPS, 36, 2024. [52] R. Ranftl, A. Bochkovskiy, and V. Koltun. Vision transformers for dense prediction. In CVPR, pages 1217912188, 2021. [53] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In ICML, pages 5389 5400, 2019. [54] W. A. G. Rojas, S. Diamos, K. R. Kini, D. Kanter, V. J. Reddi, and C. Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. In NeurIPS Datasets and Benchmarks Track, 2022. [55] O. Sidorov, R. Hu, M. Rohrbach, and A. Singh. TextCaps: dataset for image captioning with reading comprehension. In ECCV, 2020. [56] A. Steiner, A. S. Pinto, M. Tschannen, D. Keysers, X. Wang, Y. Bitton, A. Gritsenko, M. Minderer, A. Sherbondy, S. Long, et al. Paligemma 2: family of versatile vlms for transfer. arXiv:2412.03555, 2024. [57] Q. Sun, Y. Fang, L. Wu, X. Wang, and Y. Cao. EVA-CLIP: Improved training techniques for clip at scale. arXiv:2303.15389, 2023. [58] A. V. Thapliyal, J. Pont Tuset, X. Chen, and R. Soricut. Crossmodal-3600: massively multilingual multimodal evaluation dataset. In EMNLP, 2022. [59] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer, X. Pan, A. Wang, R. Fergus, Y. LeCun, and S. Xie. Cambrian-1: Fully Open, VisionCentric Exploration of Multimodal LLMs. arXiv:2406.16860, 2024. [60] M. Tschannen, M. Kumar, A. Steiner, X. Zhai, N. Houlsby, and L. Beyer. Image captioners are scalable vision learners too. In NeurIPS, 2023. [61] V. Udandarao, N. Parthasarathy, M. F. Naeem, T. Evans, S. Albanie, F. Tombari, Y. Xian, A. Tonioni, and O. J. Hénaff. Active data curation effectively distills large-scale arXiv:2411.18674, multimodal models. 2024. [62] B. Wan, M. Tschannen, Y. Xian, F. Pavetic, I. Alabdulmohsin, X. Wang, A. S. Pinto, A. Steiner, L. Beyer, and X. Zhai. LocCa: Visual pretraining with location-aware captioners. In NeurIPS, 2024. 15 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features [63] B. Wang, G. Li, X. Zhou, Z. Chen, T. Grossman, and Y. Li. Screen2words: Automatic mobile ui summarization with multimodal learning. In Symposium on User Interface Software and Technology, 2021. [64] Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao. SimVLM: Simple visual language model pretraining with weak supervision. In ICLR, 2022. [65] T. Weyand, A. Araujo, B. Cao, and J. Sim. Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval. In CVPR, pages 25752584, 2020. [66] H. Xu, S. Xie, X. Tan, P.-Y. Huang, R. Howes, V. Sharma, S.-W. Li, G. Ghosh, L. Zettlemoyer, and C. Feichtenhofer. Demystifying clip data. In ICLR, 2024. [67] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. CoCa: Contrastive captioners are image-text foundation models. TMLR, 2022. [68] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling context in referring expressions. In ECCV, pages 6985, 2016. [69] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision transformers. CVPR, 2022. [70] X. Zhai, X. Wang, B. Mustafa, A. Steiner, D. Keysers, A. Kolesnikov, and L. Beyer. Lit: Zero-shot transfer with locked-image text tuning. In CVPR, 2022. [71] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. [72] Y. Zhao, A. Gu, R. Varma, L. Luo, C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer, A. Desmaison, C. Balioglu, P. Damania, B. Nguyen, G. Chauhan, Y. Hao, A. Mathews, and S. Li. Pytorch FSDP: experiences on scaling fully sharded data parallel. VLDB, 2023. [73] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. BarScene parsing riuso, and A. Torralba. through ade20k dataset. In CVPR, 2017. [74] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and A. Torralba. Semantic understanding of scenes through the ade20k dataset. IJCV, 2019. [75] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, Image BERT preIn ICLR, A. Yuille, and T. Kong. training with online tokenizer. 2022. 16 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features"
        },
        {
            "title": "Appendix",
            "content": "A. Full PaliGemma results Large 224/256px So400m/14 224px So400m 384px SigLIP AIMv SigLIP2 SigLIP SigLIP2 SigLIP SigLIP2 AI2D AOKVQA-DA (val) AOKVQA-MC (val) COCO-35L (avg34) COCO-35L (en) COCOcap CountBenchQA DocVQA (val) GQA InfoVQA (val) NLVR2 NoCaps OCR-VQA OKVQA RefCOCO (testA) RefCOCO (testB) RefCOCO (val) RefCOCO+ (testA) RefCOCO+ (testB) RefCOCO+ (val) RefCOCOg (test) RefCOCOg (val) ST-VQA (val) SciCap ScienceQA Screen2Words TallyQA (complex) TallyQA (simple) TextCaps TextVQA (val) VQAv2 (minival) VizWizVQA (val) WidgetCap XM3600 (avg35) XM3600 (en) 75.2 60.3 78.3 109.9 136.7 138.6 75.3 33.0 65.2 25.3 90.7 117.7 70.6 62.4 71.0 66.0 68.7 67.5 59.6 63.6 63.9 63.3 54.0 161.1 96.1 108.7 67.6 79.9 116.5 51.9 81.5 74.4 132.8 39.0 77.7 73.2 62.3 78.4 111.4 138.3 139.9 83.1 32.3 65.6 25.1 91.3 121.7 71.8 62.7 71.9 67.8 69.5 69.0 61.5 65.1 65.4 64.3 53.9 156.4 96.1 106.9 69.4 81.0 116.8 53.9 82.1 74.4 133.0 39.6 78.0 75.9 61.7 77.6 112.2 139.4 141.3 82.2 35.4 66.1 26.3 91.1 120.3 72.5 63.3 74.3 70.3 72.4 70.8 63.3 67.6 67.5 66.8 59.8 165.5 96.2 114.3 69.3 82.0 126.1 57.3 82.1 76.0 139.1 39.7 79.1 75.3 62.0 79.0 111.9 139.0 141.4 78.2 34.3 65.5 25.1 91.0 120.1 71.3 63.1 72.4 67.5 69.9 69.0 60.8 64.9 64.7 64.5 56.7 162.3 95.4 111.3 68.4 80.4 121.7 54.5 81.9 75.5 134.4 39.8 77.8 74.8 62.8 80.5 113.2 139.4 142.7 84.7 35.9 65.7 26.0 91.4 120.9 72.7 63.4 74.5 70.5 72.5 71.4 63.3 67.8 67.9 67.3 60.1 161.8 96.3 110.6 70.0 82.2 123.8 59.4 82.8 76.0 142.0 40.1 79.2 76.7 64.9 82.5 113.6 140.3 142.2 80.8 62.7 67.0 34.7 91.7 120.8 74.4 63.7 76.6 71.4 74.3 74.1 65.4 70.0 69.9 69.5 75.0 177.2 96.2 115.3 71.0 83.5 145.0 69.7 84.3 76.8 147.0 40.8 80. 78.3 64.7 83.1 114.8 141.1 143.8 83.9 65.9 67.8 37.1 91.8 121.9 75.2 64.5 78.2 74.5 76.1 75.9 67.6 72.0 72.1 71.7 77.3 179.3 96.1 116.1 72.5 85.4 150.9 74.0 85.2 77.6 151.1 41.1 81.0 Table 6 The first three columns compare Large-sized models with 256 tokens each (thats 224px for the AIMv2 model with patch size 14, and 256px for the SigLIP models with patch size 16). The last four columns compare So400M-sized SigLIP models with patch size 14 at two different resolutions (and hence tokens). Same data as in Figure 4. 17 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features B. Full NaFlex results ViT Seq. Model val v2 ReaL ObjNet TI IT TI IT TI IT TI IT TI IT ImageNet-1k COCO R@1 TC R@1 HT R@1 SC R@ S2W R@1 B/16 So/16 1024 576 64 SigLIP 2 (NaF.) 71.2 63.2 78.3 144 SigLIP 2 (NaF.) 76.2 69.4 82.9 78.2 71.4 84.8 196 SigLIP 2 79.1 72.5 85.4 SigLIP 2 SigLIP 2 (NaF.) 78.5 71.9 84.6 SigLIP 2 80.6 73.8 86.2 SigLIP 2 (NaF.) 80.0 73.1 85.6 676 SigLIP 2 (NaF.) 80.1 73.5 85.7 784 SigLIP 2 (NaF.) 80.2 73.5 85.9 900 SigLIP 2 (NaF.) 80.3 73.6 85.9 SigLIP 2 81.2 74.5 86.7 SigLIP 2 (NaF.) 80.4 73.5 85.9 64 SigLIP 2 (NaF.) 78.5 71.0 84.2 144 SigLIP 2 (NaF.) 81.8 75.2 86.7 SigLIP 2 83.4 77.8 87.7 SigLIP 2 (NaF.) 83.5 77.5 87.7 SigLIP 2 84.1 78.4 88.1 SigLIP 2 (NaF.) 84.1 78.6 88.0 676 SigLIP 2 (NaF.) 84.2 78.5 88.0 784 SigLIP 2 (NaF.) 84.3 78.6 88.0 900 SigLIP 2 (NaF.) 84.3 78.6 88.1 84.3 79.1 88.1 SigLIP 2 SigLIP 2 (NaF.) 84.4 78.8 88.1 576 256 1024 62.1 70.2 73.6 74.5 74.6 77.1 76.4 76.5 76.9 76.6 77.8 76.6 73.8 79.8 84.8 83.8 85.8 85.7 85.7 85.9 85.8 86.2 85.8 5. 4.0 6.4 43.6 60.4 30.4 57.5 3.4 6.4 11.0 49.0 65.7 36.5 65.8 5.7 10.3 13.5 11.8 13.9 25.4 9.5 13.3 10.9 10.8 18.7 52.1 68.9 38.9 68.0 5.5 9.8 17.1 14.2 12.9 22.9 53.2 69.7 40.5 69.4 6.1 51.1 67.3 39.5 69.0 7.4 12.9 19.7 17.1 14.8 26.6 54.6 71.4 43.6 73.0 7.5 12.0 23.3 19.4 14.1 24.8 52.5 69.1 41.6 71.8 8.7 14.1 24.3 21.0 15.3 26.7 52.9 68.6 41.8 73.0 8.8 13.9 24.3 21.4 15.2 26.2 53.1 68.8 42.5 72.9 8.7 14.0 24.8 21.5 15.2 26.4 52.9 69.2 42.3 72.6 8.6 15.0 24.8 21.6 15.0 25.8 55.2 71.2 44.7 74.7 8.1 14.6 25.2 20.7 14.5 25.3 52.9 68.9 42.5 73.2 9.1 14.4 25.1 21.5 14.9 26.4 49.6 67.4 37.0 65.5 5.6 10.3 11.8 10.9 12.1 21.4 53.4 70.4 42.8 71.0 8.0 14.6 22.2 23.1 17.1 29.0 55.4 71.5 44.8 72.9 7.9 13.9 29.7 28.8 17.4 28.7 55.1 71.2 44.9 73.6 9.2 15.7 29.8 29.2 17.5 29.2 56.0 71.2 47.0 74.9 9.7 16.3 34.5 32.4 17.8 28.0 55.9 71.4 46.5 75.1 11.3 18.4 32.9 32.0 17.7 28.8 55.8 71.7 46.9 74.9 11.3 18.5 33.3 32.2 17.7 29.8 55.9 71.3 46.7 74.9 11.5 18.5 33.0 32.3 17.6 29.5 55.8 71.2 46.8 75.4 11.7 18.5 32.9 32.5 17.7 29.4 56.0 71.3 47.3 76.0 10.3 18.3 35.9 33.5 17.9 28.1 55.8 71.0 46.9 74.9 11.7 18.4 32.6 32.4 17.8 29.4 Table 7 Comparing the NaFlex (supporting native aspect ratio and variable sequence length (Seq.)) and the standard square-input SigLIP variants which use separate checkpoint per sequence length. Numerical data corresponding to the plots in Fig. 3. TC: TextCaps, HT: HierText, SC: SciCap, S2W: Screen2Words. 18 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features C. Full cultural diversity and fairness results ViT B/32 B/16 L/16 So400m/ So400m/16 g-opt/16 Res. Model Dollar Street GeoDE (country) GeoDE (region) Dollar Street GLDv2 GeoDE 10-shot 0-shot 256 224 256 384 512 384 512 224 384 256 384 512 256 SigLIP 2 SigLIP SigLIP 2 SigLIP SigLIP 2 SigLIP SigLIP 2 SigLIP SigLIP 2 SigLIP SigLIP 2 SigLIP SigLIP 2 SigLIP 2 SigLIP SigLIP 2 SigLIP SigLIP 2 SigLIP 2 mSigLIP SigLIP 2 SigLIP 2 SigLIP 2 SigLIP 2 13.1 13.8 16.2 15.0 17.7 16.1 19.8 16.6 21.7 18.8 26.8 22.8 30.4 32.5 26.6 31.9 32.1 38.3 33.2 27.1 38.2 40.8 37.6 44.5 13.9 12.7 20.0 13.3 22.7 16.4 25.6 17.7 28.2 22.1 34.5 26.0 39.3 42.5 31.9 38.1 36.5 45.2 39.8 33.3 44.1 47.6 46.6 52.0 29.3 27.3 34.9 29.3 36.3 31.5 41.4 32.3 43.1 36.2 44.4 41.7 48.0 50.6 45.8 49.1 51.6 56.1 50.9 48.5 54.4 58.6 54.0 58.7 50.5 50.1 53.4 50.3 54.2 51.5 54.8 51.3 54.9 52.1 55.2 52.9 55.4 55.2 55.1 55.4 56.3 56.6 55.8 54.2 56.5 56.6 56.9 57.2 44.7 48.5 50.8 47.7 52.5 51.9 55.2 53.1 57.6 56.7 64.5 60.5 66.1 67.6 74.1 65.6 71.7 68.6 66.7 57.5 67.8 69.2 71.2 72. 90.6 92.4 92.9 92.8 93.3 93.6 93.9 94.1 94.2 93.6 94.9 94.3 95.1 95.3 94.7 94.8 94.9 95.2 95.0 94.3 95.3 95.3 95.4 95.7 Table 8 10-shot and 0-shot accuracy for geographically diverse object classification tasks (Dollar Street, GeoDE), as well as geolocalization (GeoDE country/region) and landmark localization (GLDv2) tasks. SigLIP 2 consistently outperforms SigLIP on most benchmarks. 19 SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features ViT B/ B/16 L/16 So400m/14 So400m/16 g-opt/16 Res. Model Disparity Rep. bias 256 224 256 512 256 384 512 224 256 384 512 256 384 SigLIP 2 SigLIP SigLIP 2 SigLIP SigLIP 2 SigLIP SigLIP 2 SigLIP SigLIP 2 SigLIP SigLIP 2 SigLIP SigLIP 2 SigLIP 2 SigLIP SigLIP 2 SigLIP SigLIP 2 SigLIP 2 mSigLIP SigLIP 2 SigLIP 2 SigLIP 2 SigLIP 2 33.3 31.2 31.0 30.2 29.7 30.9 30.6 31.5 30.8 32.0 31.1 32.0 30.4 29.2 30.5 29.7 29.2 28.1 28.4 31.6 29.0 28.2 28.1 28.3 16.6 36.6 17.2 35.6 19.4 35.8 18.0 35.4 20.0 35.5 7.3 34.8 6.6 6.8 33.3 7.4 33.9 7.5 7.2 37.3 11.0 10.8 7.9 4.9 Table 9 Disparity: Corresponds to the maximum difference in 0-shot accuracy on Dollar Street when disaggregating the accuracy by income level: We observe that SigLIP 2 slightly reduces the performance disparity. Rep. bias: Representation bias; lower values are better. SigLIP2, which is trained on de-biased data, exhibits significantly reduced representation bias than its predecessor. In addition, larger models are better than smaller models, in agreement with the earlier findings in [2]."
        }
    ],
    "affiliations": [
        "Google DeepMind"
    ]
}