{
    "paper_title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback",
    "authors": [
        "Hao Zhu",
        "Phil Cuvin",
        "Xinkai Yu",
        "Charlotte Ka Yee Yan",
        "Jason Zhang",
        "Diyi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., \"If you find that the button is disabled, don't click it again\", or \"This agent has too much autonomy to decide what to do on its own\", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: \"coverage\" and \"redundancy\". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 0 2 8 2 0 . 5 0 5 2 : r Preprint. Under review. AutoLibra: Agent Metric Induction from Open-Ended Feedback"
        },
        {
            "title": "Stanford University University of Toronto University of Pennsylvania",
            "content": "{zhuhao,ckyy,jasonbz,diyiy}@stanford.edu philippe.cuvin@mail.utoronto.ca, xinkaiyu@sas.upenn.edu Code Data Website: https://autolibra.opensocial.world"
        },
        {
            "title": "Abstract",
            "content": "Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra , framework for agent evaluation, that transforms open-ended human feedback e.g. If you find that the button is disabled, dont click it again, or This agent has too much autonomy to decide what to do on its own into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agents behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLMas-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of set of (induced) metrics with open feedback: coverage and redundancy. Through optimizing these meta-metrics, we experimentally demonstrate AutoLibras ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on wide range of text game tasks, improving agent performance over baseline by mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is powerful task-agnostic tool for evaluating and improving language agents."
        },
        {
            "title": "Introduction",
            "content": "Humans readily acquire skills from open-ended instructions and feedback from others (Tomasello et al., 1993). These instructions and feedback are internalized for self-regulated learning (Pintrich & Zusho, 2002; Nicol & Macfarlane-Dick, 2006), providing internal signals for continuous improvement. Drawing inspiration from this process, we investigate how well AI agents can benefit from open-ended human feedback through induction of generalizable metrics. We can characterize existing evaluations and rewards for AI agents into two main classes: (1) goal-oriented evaluation whether the agents have fulfilled the given task, e.g. benchmarks (Zhou et al., 2024a; Jimenez et al., 2024; Chan et al., 2024; Paglieri et al., 2024) and reward modeling approaches (Pan et al., 2024; Chen et al., 2025b; Choudhury, 2025) and (2) behavior evaluation the manner the agents perform the task, e.g. social agent and human-agent interaction benchmarks (Zhou et al., 2024b; Shao et al., 2024) and agent failure mode analysis (Pan et al., 2025; Zhang et al., 2023; Yang et al., 2023).1 Goal-oriented evaluation 1Note that the difference between the two classes is not whether the evaluation is based on the outcome or the process. Both evaluations can be performed at every step or in the final step. 1 Preprint. Under review. Figure 1: AutoLibra induces agent evaluation metrics from human feedback, and uses these metrics to evaluate agents, which can be meta-evaluated via evaluating the coverage on unseen human feedback. Here we show real examples of agent trajectories, human feedback, aspects, induced metrics, evaluation results on WebVoyager (He et al., 2024). 2 Preprint. Under review. is often designed to be verifiable, but is not sufficiently fine-grained or comprehensive to diagnose agents behavior problems or locate improvement bottlenecks (Yehudai et al., 2025). Although the above can be complemented with behavior evaluation, it requires manual design of metrics based on top-down heuristics (Zhou et al., 2024b), or thematic analysis of the agents behavior (Shao et al., 2024; Pan et al., 2025). This manual design process requires the labor of experts in annotations and classification, which limits its scalability. In this paper, we introduce AutoLibra , metric induction method, as novel agent evaluation framework that mitigates the limitations of current evaluation paradigms. AutoLibra is an evaluation tool that induces interpretable metrics for AI agents from open-ended human feedback, which can be collected from end users of AI agents or experts. This offers two key advantages: (1) It is much easier to provide concrete feedback for trajectories than creating metrics, and (2) AutoLibra allows us to evaluate agents from the perspective of the users. AutoLibra-induced metrics provide concrete definitions of behaviors that the model-based evaluation method should look for when evaluating AI agents, which could be used to understand agent behavior, as well as optimization targets to improve agents. Inspired by the code-theme steps of thematic analysis conducted by experts in social sciences (Braun & Clarke, 2006), we design the AutoLibra induction process (2.2) as two steps: (1) feedback grounding: where we ground every aspect of human feedback on some behavior in the entire agent trajectory, and (2) behavior clustering: where we cluster the aspects into multiple clusters of similar behaviors to summarize into metrics. As illustrated in Fig. 1, the user gives web agent feedback the agent did not choose iPhone 14/15 which is grounded to the agents behavior, choosing iPhone 16 Pro from the drop-down menu. Similar behaviors are clustered into common cluster, summarized as Element Interaction Accuracy. The AutoLibra evaluation process is designed to provide closed-loop feedback signal for the induction process. The agent trajectories used in the induction process are scored by LLM-as-a-Judge (Zheng et al., 2023) on the induced metrics. The evaluation process (2.3) then tries to match the feedback aspects, e.g. recipe does not contain quinoa, with the traits, e.g. task-requirement-achievement. In this way, we can meta-evaluate the quality of the metrics: (i) coverage (what proportion of feedback aspects can be matched with an agent trait), and (ii) redundancy of the metrics (what proportion of the detected traits are not mentioned by humans). These two metrics provide an overall statistical picture of the quality of the induced metrics. Based on these two metrics, we can search for the set of metrics with the lowest redundancy within those with the highest coverage. As shown in 3.1, we find that as the number of metrics increases, the redundancy increases, and the coverage ultimately converges to the maximum coverage. With AutoLibra, our aim is to answer the following research questions: RQ1: How aligned are the results from AutoLibra for each step with human judgment? RQ2: Can AutoLibra provide more insight into agent behavior that were not provided by expert designed metrics? RQ3: Can AutoLibra provide optimization signals for human agent engineers and agent learning algorithms to improve agents performance? Key findings: Experiments within multiple agent domains, including collaborative agents (Shao et al., 2024), social agents (Zhou et al., 2024b), web agents (Zhou et al., 2024a; He et al., 2024), and text game agents (Paglieri et al., 2024; Cloos et al., 2024), demonstrate that AutoLibra is able to induce fine-grained and interpretable metrics with high coverage and low redundancy in unseen human feedback with 80 trajectories annotated with one feedback for each trajectory per dataset. These metrics are more concrete, and some of them were even overlooked in expert designed metrics or error analysis (4). The fact that AutoLibra can iteratively discover new, emergent metrics (3.2) throughout the agent optimization process, and provide salient and human-interpretable optimization signals helps improve the performance in Baba-Is-AI by over 20% (5.1), and web-browsing task WebVoyager by 5% on top of previous model-based rejection fine-tuning methods (5.2) with only 18 trajectories annotated with one feedback for each trajectory in each improvement iteration. 3 Preprint. Under review."
        },
        {
            "title": "2 AutoLibra",
            "content": "To address the limitations of existing evaluation paradigms, AutoLibra is designed to meet the following desiderata: (1) induced from agent behavior: This ensures that metrics are grounded in agent trajectories rather than predefined by human experts, (2) self-validating: Allows choosing minimal set of metrics that cover unseen human feedback with sufficient abstraction to be useful across different tasks, and (3) generalizable: Applicable to various agent environments, independent of domain-specific design. Based on feedback data collected from humans (2.1), AutoLibra achieves these desiderata through closed-loop pipeline consisting of two processes: Induction Process that converts agent behaviors and corresponding feedback into metrics, (2.2) and Evaluation Process that predicts ratings and quality of new agent behaviors on the induced metrics (2.3)."
        },
        {
            "title": "2.1 Collecting human feedback",
            "content": "In this paper, we use human feedback from two groups: (1) End-users for agents that interact directly with humans, we use the feedback from the users who interact and converse with the agents. CoGym (Shao et al., 2024) is the environment that belongs to this category, and we use the user comments collected in their study, resulting in 197 trajectories with feedback. (2) Experts for agents that do not directly interact with humans, we use the feedback from human annotators (five authors in this paper) who observe agent trajectories. All other environments belong to this category, these being Sotopia (Zhou et al., 2024b), WebArena (Zhou et al., 2024a), WebVoyager (He et al., 2024), Baba-is-ai (Cloos et al., 2024), and MiniHack (Samvelyan et al., 2021). For each trajectory, we collect only one element of feedback; feedback is given based on the complete agent trajectories.2 Annotators are instructed to explicitly indicate the aspects of agent behavior that they classify as good or bad, and to avoid general comments such as \"The agent is good at solving the task\". The annotators can also choose from TTY (TeleTYpewriter) or web interface; in both cases the annotator is provided with the agents task and then view the agents observation and actions step by step, in text form. 3 For multi-agent tasks, we annotate each agents trajectory in given interaction separately. For Sotopia (Zhou et al., 2024b), WebArena (Zhou et al., 2024a), and WebVoyager (He et al., 2024), we annotate 100 trajectories of agents based on GPT-4 (Achiam et al., 2023) with feedback for each dataset. For experiments in 5 we annotate 18 trajectories for each dataset in each iteration. The annotation process is fast: Human annotators spend less than 5 minutes to provide feedback for each trajectory; 4, we randomly hold out 20% of the trajectories for validation. Figure 2: Feedback Grounding 2.2 Induction Process: agent trajectories and human feedback evaluation metrics Feedback Grounding The feedback of human annotators can contain multiple aspects; e.g. AI agent was pretty good at giving me consistent itinerary and vacation plan, although it froze on the last couple of minutes., collected from human annotators in CoGym (Shao et al., 2024), contains positive aspect about the agents ability to generate consistent itinerary, and negative aspect about the agent freezing at the end. Here we define an aspect as triple (behavior, feedback, sign). In the positive aspect of the previous example, the 2While in theory we can leverage feedback on specific steps to achieve better feedback grounding and multiple feedback for single trajectory, we leave it as future work. 3While viewing screenshots is standard for web navigation tasks, we keep the observation format consistent across agents and humans to encourage more grounded feedback. 4 Preprint. Under review. behavior is the agents actions to create 20-day itinerary for the Maldives, the feedback is that the created itinerary is consistent and the sign is positive. This grounding procedure is similar to the coding procedure in thematic analysis by humans. Illustrated in Fig. 2, in this step, we feed the trajectory and the feedback into the LLM (we use GPT-4o (OpenAI et al., 2024) as it yields good results in our pilot experiments) and prompt the LLM with the following instructions: (1) break down the feedback into bullet points; (2) for each bullet point, find the corresponding part of the trajectory to which the feedback refers. Finally, we use constrained decoding to force GPT-4o to output the aspects in the previous format. In our experiments, we find that on most datasets, for each trajectory, the LLM can generate one to five aspects, with mean of one to two aspects. Behavior Clustering The second step of the extraction process is to group the aspects into metrics. To illustrate this step, we consider another example in the same dataset The AI responds quickly to write and run the Python script where the behavior is the agents action to quickly write and run Python script, the feedback is that the agent responds quickly, and the sign is positive. Although this aspect is positive aspect, it reflects the same dimension of the agents behavior as the previous negative aspect, with an opposite value. Each metric is cluster of aspects, with definition summarizing the criteria of positive behaviors, list of positive behavior examples, and list of negative behavior examples. This clustering procedure is similar to the theme induction step in thematic analysis. Figure 3: Behavior Clustering However, clustering similar agent behaviors together is challenging for statistical clustering methods.4 Inspired by LLM-based semantic clustering and concept induction methods Viswanathan et al. (2024); Lam et al. (2024), we prompt an LLM (o3-mini high5, as it produces the most accurate coverage and redundancy scores as evaluated later) to cluster the aspects into metrics. As illustrated in Fig. 3, we gather all the aspects of trajectories and cluster into metrics, where is parameter set through the optimization process (3.1). We provide the LLM with the following instructions: The granularity of the grouping should be minimal; only very similar behaviors are grouped together; but dont limit to one particular website or one particular character, which empirically makes the metrics more concrete but still applicable across different tasks."
        },
        {
            "title": "2.3 Evaluation Process: evaluating agents and the quality of the induced metrics",
            "content": "Evaluating agents with induced metrics LLM-as-a-Judge (Zheng et al., 2023), or more broadly, model-based evaluation (Zhang et al., 2019; Celikyilmaz et al., 2021) is method to use machine learning models to evaluate the output of other machine learning models. The success of LLM-as-a-Judge depends on the gap between the difficulty of evaluation or verification and that of generation and action. In agentic tasks, this gap is often large, as the policy model must perform multiple steps in decision-making, while the evaluation model must only classify the trajectories, which make LLM-as-a-Judge widely used (Zhou et al., 2024a; He et al., 2024; Zhou et al., 2024b). In AutoLibra, we employ LLM-as-a-Judge to evaluate the agent trajectories configured with the induced metrics. However, LLM-as-aJudge can be replaced by any other evaluation methods implementing the induced metrics; e.g. an interact-valid-element metric could be evaluated by rule-based evaluator that checks if the agent interacts with valid elements on the webpage. We note that AutoLibra could be used with other evaluation methods, such as programmatic evaluation (Ma et al., 2024); we leave generating programs for the induced metrics for future work. 4In preliminary experiments, we tried to use K-means clustering on the aspect vectors generated by text-embedding-3-large, but the clusters are mostly based on tasks and not on the behaviors. 5https://openai.com/index/openai-o3-mini/ 5 Preprint. Under review. As illustrated in Fig. 4, taking the induced metrics as input, an LLM (we use o3-mini medium, as it provides similar results in this step to o3-mini high) is prompted to rate the agent trajectories to {+ 1, -1, N/A} for each metric. For an agent trajectory, the metrics labeled +1 are the positive traits, and the ones labeled -1 are the negative traits. When we calculate the scores of the metrics, we use the ratio of agent trajectories rated as positive to the ones that are rated as positive or negative, ignoring those rated as N/A, since not all metrics are applicable to all trajectories (some metrics like valid-search-terms are only applicable when the task involves searching). Figure 4: Evaluating agents with induced metrics Meta evaluation The last component of the loop is the meta-evaluation, i.e. evaluating the evaluation metrics induced by AutoLibra. This step matches the traits detected by the LLM-as-a-Judge with aspects grounded from the human feedback. The goal is to verify whether (1) the induced metrics cover the behaviors the human annotators care about, and (2) LLM-as-a-Judge can produce accurate evaluation results based on the induced metrics. In the previous example, if the respond-promptly is extracted as metric, and the LLM-as-a-Judge has the same opinion as the human annotators, then this aspect is considered as successfully covered. If either similar metric was not extracted, or the LLM-as-a-Judge assigns different score, then this aspect is considered as not covered. As illustrated in Fig. 5, we perform meta-evaluation for each trajectory-feedback pair by classifying the aspects into positive and negative aspects, classifying traits into positive and negative traits based on rating, then matching the positive aspects with positive traits and the negative aspects with negative traits. We prompt an LLM (we use GPT-4o (OpenAI et al., 2024)) with list of aspects and another list of traits and ask the LLM to find the best matching trait for each aspect or decide that there is no matching trait. The coverage of the whole dataset is calculated as the proportion of aspects of all instances that have matching trait, and the redundancy is calculated as the proportion of traits of all instances that have not been matched with any aspect. Figure 5: Meta evaluation"
        },
        {
            "title": "3 Optimizing and validating AutoLibra",
            "content": "AutoLibra is designed to be self-validating through the evaluation process, which allows us to search the optimal set of metrics that cover the human opinion the best (3.1). This optimization process can also be applied iteratively throughout the agent improvement process. As the agent is optimized, new metrics can be added to existing metrics (3.2), which is similar to how unit tests are kept throughout software development to prevent new features from interfere with existing features. In the last part of this section, we study the alignment between each step of AutoLibra and human judgment."
        },
        {
            "title": "3.1 Metric Optimization",
            "content": "As illustrated in Fig. 6, we optimize the metric induction process to maximize coverage and minimize redundancy. Among the two, we prioritize coverage of the metrics to provide comprehensive evaluation of the agent behavior, while minimizing overlap within the metrics to avoid redundancy, thus maximizing the utility of induced metrics. 6 Preprint. Under review. Figure 6: Metric optimization: optimizing the induction process through maximizing the coverage while minimizing redundancy of the metrics, calculated via the evaluation process. To optimize for this objective, we generate 20 different metrics, with ranging from 4 to 13, and calculate the coverage and redundancy of the metrics in human feedback. We then select metrics with coverage of at least the highest coverage minus 1%, and the lowest redundancy. This is performed iteratively, by resetting the range of to the number of metrics selected previously 2, repeating until the coverage and redundancy of the selected metrics converge, normally within 3 iterations. While this optimization process is simple, experiments with various other optimization strategies, including genetic algorithms and iterative clustering saw none of them yield better results than the simple strategy. Fig. 7 shows the highest coverages of the metrics of size N, which converge around = 6 to 10 depending on the datasets. The best coverage on Sotopia (Zhou et al., 2024b) is the lowest among all four datasets, 60%, likely due to the diversity of the tasks in the dataset, while coverage on WebArena (Zhou et al., 2024a) and WebVoyager (He et al., 2024) are the highest, 88%. We also find that the coverage of the held-out trajectories is only slightly worse (< 5%) than the trajectories we use to induce the metrics, which is expected since we use the exact examples extracted from the latter. Lastly, we show that the good and bad behaviors are crucial in the metrics, dropping which resulting in up to 30% coverage decrease (on CoGym (Shao et al., 2024)). Figure 7: Coverage and redundancy of AutoLibra metrics on four agentic datasets. Circles indicate coverage and redundancy for different induced metrics; stars indicate the the best metrics coverage and redundancy on held-out human feedback; squares show an ablation test, indicating the effect when good and bad behavior examples are removed from metrics, demonstrating the criticality of concrete behavior examples 3."
        },
        {
            "title": "Iterative Metric Induction",
            "content": "When applying AutoLibra to agent optimization, we can iteratively induce new metrics, as agents develop new failure modes or new behaviors as they improve, which is useful for tracking agents progress across different iterations.6 To do this, we modify the behavior clustering step, by providing the LLM with the existing metrics and their definitions, and ask the LLM not to change the definitions of the existing 6Alternatively, new set of metrics can be induced from scratch for each iteration - in practice, we do not find that this results in any coverage loss, but we choose the former method for consistency 7 Preprint. Under review. Steps CoGym Sotopia WebArena WebVoyager Baba-is-AI Grounding LLM-as-a-Judge Meta-Evaluation 0.95 0.90 0.98 0.95 0.85 0. 0.98 0.95 0.85 0.93 1.00 0.83 0.93 0.90 0.95 Average 0.95 (0.03) 0.92 (0.04) 0.90 (0.04) Table 1: The ratio of instances marked as fully correct in human validation. For each step and each task, we randomly sample 40 instances to reach relatively small confidence interval of 0.04 and ask human annotators to label them as completely correct or not. Although the agreement scores vary across tasks and steps, the average agreement for each step and dataset is above 0.85 significantly. metrics, to only add new behaviors to the existing metrics, and add new metrics if necessary. We apply the same optimization strategy as in the metric optimization step ensure the newly induced metrics cover emerging behaviors and do not overlap with existing metrics."
        },
        {
            "title": "3.3 How aligned are the steps in AutoLibra with human judgment?",
            "content": "Since AutoLibra uses LLMs in each step, we first ask whether LLM outputs are reliable or aligned with human judgment. To measure the alignment of AutoLibra metric induction with human judgment, we validate the feedback grounding, agent evaluation, and meta evaluation steps by having human experts manually review each step (with exception of the behavior clustering step, as it is prohibitively time-intensive for human annotators to process and cluster more than 400 aspects), scoring (1/0) based on whether they agree with the outcomes of each iteration. The coverage and redundancy scores, in combination with the validation results of the other steps in the loop, thus serve as an indirect validation for the behavior clustering step. Table 1 shows the agreement rate of human annotators in AutoLibra steps. It should be noted that these tasks are significantly different; e.g., grounding for WebVoyager (He et al., 2024) is challenging due to the length and wide action space of the trajectory, and LLM-as-a-Judge for Sotopia (Zhou et al., 2024b) is difficult due to the complexity of the evaluation of social interactions. Our results show that the majority (significantly over 85%) of results in AutoLibra are reliable according to human validation."
        },
        {
            "title": "4 AutoLibra as a lens",
            "content": ": agent evaluation with AutoLibra In this section, we use AutoLibra as lens to provide grounded, behavior-salient insights into agent trajectories. In three data sets, CoGym (Shao et al., 2024), Sotopia (Zhou et al., 2024b), and WebVoyager (He et al., 2024), we compare induced metrics with heuristically proposed evaluation dimensions and failure modes summarized by the authors. We find that AutoLibra can discover more concrete metrics than heuristically defined categories, and novel metrics that are overlooked by experts. Tab. 2 summarizes the comparison between AutoLibra-induced metrics and evaluation criteria across the three aforementioned datasets. CoGym For CoGym (Shao et al., 2024), AutoLibra induces 9 metrics from feedback from end users, which can correspond to the five failure categories proposed by the authors. The failure rate (frequency of metric score of -1) measured by AutoLibra also roughly matches the failure rate of the manually labeled CoGym categories by the authors. This shows that AutoLibra induces metrics that reflect human-expert categorization and provide an automated measurement of agent failures. Sotopia In Sotopia, the authors (Zhou et al., 2024b) proposed 7 dimensions for evaluating social intelligence in AI agents. With AutoLibra, we recover the exact dimension Goal Completion, and 3 metrics as the subdimensions of Believability, indicating that Believability could be too high-level, while AutoLibra provides more concrete breakdown metrics. The failure rate (frequency of score of -1 metric rating, indicating the agent performs poorly on that metric) measured by AutoLibra in these two categories roughly matches the score of the Sotopia dimensions of the agent we studied. AutoLibra induces another four metrics 8 Preprint. Under review. AutoLibra -induced metrics"
        },
        {
            "title": "Matched metrics and failure categories",
            "content": "Responsiveness and Efficiency (75%) Communication Clarity & Notification (8%) Instruction Adherence & Follow-Through (24%) Iterative Refinement and Adaptability (47%) Autonomy and Proactiveness (28%) Content Quality and Coherence (16%) Search and Retrieval Accuracy (13%) Data Analysis Competence (2%) Interface and User Experience (23%) Communication (65%) Situational Awareness (40%) Planning (39%) Environmental Awareness (28%) Personalization (16%)"
        },
        {
            "title": "Matched metrics and social dimensions",
            "content": "Goal Achievement & Outcome Effectiveness (19%) Goal Completion (14%) Conversational Naturalness & Efficiency (5%) Personality Consistency and Alignment (2%) Contextual Integration of Identity (1%) Believability (4%) Unmatched AutoLibra -induced metrics Negotiation Tactics and Strategic Adaptability (14%), Responsiveness and Conversational Termination (5%), Adaptability and Flexibility in Dialogue (7%) Unmatched Sotopia-Eval dimensions Relationship, Knowledge, Secret, Financial and Material Benefits, Social Rules"
        },
        {
            "title": "Matched metrics and failure reasons",
            "content": "Error Recovery & Adjustment (15%) Step Efficiency & Action Redundancy (13%) Navigation Accuracy (11%) Access Barrier Handling (2%) Information & Verification Accuracy (16%) Result Relevance Accuracy (9%) Navigation Stuck (44%) Hallucination (22%) Prompt Misalignment (9%) Unmatched AutoLibra -induced metrics Query and Search Strategy Efficiency (7%), Final Output and Summarization Quality (18%)"
        },
        {
            "title": "Unmatched WebVoyager fail reasons",
            "content": "Visual Grounding Issue (25%) ) 4 2 0 2 , . e S ( o ) 4 2 0 2 , . e Z ( o ) 4 2 0 2 , . e ( g V Table 2: Comparison between AutoLibra-induced metrics and expert proposed evaluation dimensions and failure categories. The percentages in parenthesis denote frequency of -1 in AutoLibra-induced metrics, and failure frequency or score from the original papers. that are overlooked in the heuristically proposed Sotopia-Eval dimensions. We note that the other five dimensions in Sotopia are still valuable evaluation dimensions for social intelligence. However, behaviors captured by dimensions Financial and Material Benefits, Knowledge, and Secret are often also captured by Goal Completion and Believability. As result, AutoLibra produces the single Goal Achievement and Outcome Effectiveness by minimizing redundancy. Whereas, Relationship and Social Rules captures long-tailed behaviors not captured by AutoLibra. 9 Preprint. Under review. WebVoyager Similarly, for web navigation tasks, AutoLibra also discovers metrics such as Access Barrier Handling, Error Recovery and Adjustment, Step Efficiency and Action Redundancy, and Navigation Accuracy, which much more closely reflect concrete agent behavior than the failure analysis categories proposed in previous work (He et al., 2024; Zhou et al., 2024c), where they are often simply classified as navigation stuck. We also find additional metrics that are not mentioned in the failure analysis, such as Query and Search Strategy Efficiency and Final Output and Summarization Quality, which are frequent issues (with frequencies of 7% and 18%). Since AutoLibra only observes the behavior of the agents, it cannot interpret the neural representation, not able to capture the visual grounding issues, which are mentioned in the WebVoyager paper. This further demonstrates AutoLibras utility in extracting behavior-salient metrics, and particularly demonstrates its ability to obtain fine-grained metrics that expert design would not have been able to extract."
        },
        {
            "title": "5 AutoLibra as a ladder",
            "content": ": Agent improvement with AutoLibra Baba-Is-AI (5.1) WebVoyager (5.2) Avg. Metrics Success Rate Avg. Metrics"
        },
        {
            "title": "Success Rate",
            "content": "Iter 0 Iter 1 Iter 2 Iter 3 15.8 28.2 40.7 56.7 33.3 40.0 44.4 52.7 45.9 53.8 61.3 69.9 34.8 35.5 38.1 39.7 Table 3: Using AutoLibra-induced metrics as optimization targets for prompt engineering and finetuning leads to improvements in the average metric positive rates (Avg. Metrics) and task success rate (Success Rate) defined by the two environments respectively. As AutoLibra can automatically induce metrics from human feedback, natural question to ask is whether it can enable self-regulated improvement in agents through iterative feedback. In this section, we will study can AutoLibra be used ladder, inducing metrics as new optimization targets in each iteration to improve agent performance. We investigate two methods for optimizing agents: prompt engineering and fine-tuning, across two domains: text game and web browsing. Across all tested methods and domains, AutoLibra induces new metrics in each iteration to cover emergent agent behaviors, and enables continuous improvements on both induced metrics and task success. Fig. 8 illustrates the results."
        },
        {
            "title": "5.1 Can AutoLibra induce good optimization targets for prompt engineering?",
            "content": "To answer this question, we use Baba-Is-AI (Cloos et al., 2024; Paglieri et al., 2024) as case study, since it is not yet saturated by state-of-the-art LLM agents. This game requires metacognitive skills, such as breaking and building new rules and planning ordered subtasks; full list of game rules is listed in App. C. We use the top model on the leaderboard7 (which when we started the experiments, was GPT-4o) (OpenAI et al., 2024) for the agent policy, while we observe similar improvements in Claude-3.5-Sonnet8 as well. To avoid overfitting to the tasks used in optimization, we fine-tune on 6 of 40 tasks in BabaIs-AI (Paglieri et al., 2024), but report performance on all 40 tasks. In one Iteration, the human prompt engineers (two of the authors) generate 18 agent trajectories (3 trajectories / task), use AutoLibra to induce metrics, then optimize the prompt to achieve higher performance on the induced metrics without considering the task success rate, and start new iteration when one or more metrics improved significantly (over 30%). The procedure and experiment setup are detailed in App. Alg. and App. B. Results As shown in Fig. 8 and Tab. 3, the performance on Babaisai task increased from 25% to 55% between Iteration 0 and Iteration 3, with each iteration resulting in greater 7https://balrogai.com 8https://www.anthropic.com/news/claude-3-5-sonnet 10 Preprint. Under review. Figure 8: AutoLibra iteratively discovers new optimization targets throughout the agent optimization process. Each emoji pair represents an induced metric; the arrows and text between iterations represents the changes made to prompts informed by metrics. Purple boxes indicate the newly emerging metrics, and green arrows indicate significant improvements over the previous iteration. The metric names can be found in Tab. 4 and App. Tab. 10. performance than the previous; this is replicated across the held-out tasks alone, (complete task results tabulated in App. D). This represents substantial increase compared to the highest base model performance of 33% on Baba-Is-AI (Paglieri et al., 2024). With only 6 tasks used in inducing the metrics, the improvement on all 40 tasks showing the metrics induced by AutoLibra are generalizable to unseen tasks. Among induced metrics (App. Tab. 4), agent performance increases correspondingly to prompt changes targeting those metrics, an example being Win Condition Recognition , which increases from 28% to 50% from Iterations 0-1 after few-shot prompting (prompt in App. G) is introduced to teach the identification of win condition rule. The complete list of observations and reasoning for prompt engineering is in the App. H. With AutoLibra, prompt engineers could receive feedback on whether their prompt changes lead to desired behavior changes in agents. Qualitative observation of agent trajectories reveals behaviors commensurate with induced metric scores; the agent random-walks in Iteration 0, navigates towards specific objectives but gets stuck or trapped in loop on long-horizon tasks in Iteration 1 and Iteration 2, and fully understands basic subtasks (atomic goals on the critical path to environment completion) and the correct order of subtasks to successfully complete an environment in Iteration 3. permetric score breakdown is listed in Appendix E, and full periteration documentation of code changes and results is presented in Appendix H. Emoji It. Metric 0 Win Condition Recognition Rule Modification 0 Direct Navigation Efficiency 0 Context-Sensitive Decision Making 1 Win Rule Construction 1 2 3 Selective Interaction w/ Relevant Obj. Rule Manipulation and Execution Subtask Coordination Immovable Interaction Table 4: Metrics and Turn of Inductionfor Baba-Is-AI 11 Preprint. Under review. Generalization to other models When replacing GPT-4o with Claude-3.5 as the LLM used in the agent (Appendix D), performance was found to be similar across all iterations, with held-out task performance increasing by 20% to 55% between Iteration 0 and Iteration 3 and qualitatively similar trajectory performance and agent behaviors observed between the two LLMs. This demonstrates that the improvements realized by AutoLibra are generalizable to other LLMs, and that the induced metrics are robust to changes in the underlying LLM. Generalization to other tasks To evaluate whether this pipeline can be generalized to other domains, we conduct the same pipeline to MiniHack (Samvelyan et al., 2021), an environment whose tasks and action space are more complex than Baba-Is-AI. Similarly to Baba-Is-AI, in three iterations, the task completion rate was observed to increase to 25%, an improvement of 15% versus the baseline of 10%, validating AutoLibras general utility in improving agent performance; full results are available in the Appendices I-N."
        },
        {
            "title": "5.2 Can AutoLibra induce good optimization targets for agent fine-tuning?",
            "content": "To answer this question, we use web browsing (via the WebVoyager (He et al., 2024) benchmark) to compare the effectiveness of AutoLibra with existing goal-oriented LLMas-a-Judge methods in rejection behavior cloning methods for agent fine-tuning. We use diverse dataset (NNetNav-live) composed of agent trajectories of open-end exploration on 20 live websites (Murty et al., 2025). NNetNav-live tasks are assigned task label post-hoc, to ensure they have no overlap with and are more complex than the WebVoyager evaluation datasets used in this subsection. Using the state-of-the-art agent Llama-8b-nnetnav-live, which was finetuned with trajectories selected with LLM-as-a-Judge instructed to evaluate trajectories based on goal completion, and following similar pipeline to 5.1, we improve the agents for 3 iterations, obtaining feedback on 18 trajectories in each iteration. Instead of prompt engineering, we use the induced metrics to choose the sampled trajectories with top 10% sum scores of all metrics for fine-tuning the agent policy. Throughout the training procedure, we use the tasks in NNetNav-live (Murty et al., 2025) for metric induction and agent finetuning. This process yields 4.9% task completion rate improvement on WebVoyager, shown in Fig. 8 and Tab. 3, while using naive LLM-as-a-Judge instructed to evaluate goal completion does not yield significant improvement in metrics or success rate. This shows that AutoLibra-induced metrics serve as better optimization targets than the goal-oriented evaluation metric alone; theseinduced metrics are shown in App. Tab. 10."
        },
        {
            "title": "6 Related Work",
            "content": "AutoLibra unifies three areas of research: it draws inspiration from thematic analysis to create nautral language-derived evaluation metrics to evaluate and reward AI agents. Evaluating AI agents Much of the work in AI agent evaluation focuses around benchmarks which contains both task suites and evaluation metrics. In addition to the datasets we used in this paper, SWE-Bench (Jimenez et al., 2024) uses human-written unit tests as evaluation metrics; Embodied Agent Interface (Li et al., 2024) provides fine-grained evaluation for LLMbased embodied agents; τ-Bench (Yao et al., 2024) compares database states for evaluation; concurrent work AgentRewardBench (Lù et al., 2025) builds benchmark for reward models for web agents. Recently, there are observatory tools including Galileo (Galileo, 2025), Vertex AI Gen AI (Cloud, 2025), and Docent (Meng et al., 2025) which provide user interfaces to visualize agent failure modes. Generating intrinsic rewards have also been studied in the reinforcement learning community (Du et al., 2019; Pathak et al., 2017; Laskin et al., 2022) to encourage exploration, sub-task completion, or skill discovery. In contrast to these, AutoLibra is pure data-driven task-agnostic method without predefined failure taxonomy for generating interpretable metrics for agents. Learning from natural language and human feedback Researchers have been studying reinforcement learning with language feedback to provide dense reward to agents (Goyal et al., 2019). Since LLM agents are even harder to train with sparse reward, there is substantial interest in training LLM agents from natural language feedback. Chen et al. (2024) 12 Preprint. Under review. propose an imitation learning method for learning from human feedback; Text2Reward (Xie et al., 2024) uses code generation to generate robot reward functions from open-ended human feedback; our work (Chen et al., 2025a) uses feedback to the improvement agent policy with prompting and then align the unprompted agent policy with the prompted one; Shi et al. (2024) propose new model architecture to incorporate human feedback into policy learning. On the other hand, human non-open-ended feedback is also incorporated in training agents, including rating feedback (Nguyen et al., 2017), preference feedback (Christiano et al., 2017), demonstrative feedback (Shaikh et al., 2025). Unlike these papers, AutoLibra induces metrics from feedback from all annotated instances and generates metrics that are generalizable to different tasks and useful for both evaluation and agent fine-tuning. Automatic thematic analysis Thematic analysis is powerful tool for qualitative study through coding and iterative creation of themes. Gauthier & Wallace (2022) provide computational tools to aid this process; Hong et al. (2022) and Gebreegziabher et al. (2023) explore human-AI collaboration in thematic analysis; LLooM (Lam et al., 2024), is an automatic method for concept induction, is closest to and influences our approach. This paper completes the loop of concept induction by using the meta-evaluation step to optimize the induced metrics, and apply this social science technique to agent evaluation."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "In this paper, we propose AutoLibra, new paradigm for agent evaluation, one of the first works to explore adaptable trajectory-derived evaluation heuristics, offering substantial advantages in agent training over traditional end-to-end evaluation. We find that this framework is generalizable to diverse range of agent tasks, provides new insights into agent behaviors, and identifies strong optimization targets for agent improvement. There are few directions for further extending and applying this framework. Behavior-centric evaluation AutoLibra leads paradigm shift from end-to-end agent evaluation (analogous to integration tests in software development) to evaluation with granular metrics that measure agents concrete behaviors (analogous to unit tests). Future work can study whether this process can be improved through better human-AI collaboration. Sub-trajectory feedback from humans In AutoLibra, we label each trajectory with one piece of feedback, and ground it into the agents concrete behavior which is at the sub-trajectory level. In the future, researchers can let users directly give feedback for one or multiple steps in the trajectory, which should lead to better feedback grounding results. Similarly, user feedback can be collected during the interaction instead of after the agent has completed the tasks, which is more user-friendly way to gather high quality feedback data. Wider exploration of agent improvement methods In this paper, we only explored two methods for agent improvement to show the utility of AutoLibra. In the future, researchers can use AutoLibra to provide dense rewards for individual steps, and use reinforcement learning to train LLM agents with these dense rewards."
        },
        {
            "title": "Limitation and Ethics Statement",
            "content": "The scope of this paper is text-based agents, which does not include agents with multimodal observation or action spaces. Within human-aided experiments, we are also limited by the diversity of human annotators. The annotation of the data in this paper, except for the feedback collected by Shao et al. (2024), is done by the authors, who are experts in AI agents. We do not address the effect of the experience of the annotators on the induced metrics. Finally, induced metrics should be used with caution, these could reflect the internal biases of the LLMs used to extract them. 13 Preprint. Under review."
        },
        {
            "title": "Acknowledgment",
            "content": "This work is supported by ONR grant N000142412532 and NSF grant IIS-2247357. We thank Google Cloud Platform and Modal Platform for their credits. We thank Yutong Zhang, Hayley Zhang, Yijia Shao, Michelle Lam, Manling Li, Ryan Louie, Yanzhe Zhang, Xuhui Zhou, Maarten Sap, Sherry Tongshuang Wu, Shikhar Murty, Saujas Vaduguru, Chenghao Yang, Xizhi Xiao, Anant Sinha and all members of Stanford SALT Lab for their help and feedback throughout this project."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Virginia Braun and Victoria Clarke. Using thematic analysis in psychology. Qualitative research in psychology, 3(2):77101, 2006. Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. Evaluation of text generation: survey, 2021. URL https://arxiv.org/abs/2006.14799. Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. Mle-bench: Evaluating machine learning agents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024. Angelica Chen, Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, and Ethan Perez. Learning from natural language feedback. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https: //openreview.net/forum?id=xo3hI5MwvU. Wentse Chen, Jiayu Chen, Fahim Tajwar, Hao Zhu, Xintong Duan, Russ Salakhutdinov, and Jeff Schneider. Fine-tuning llm agents with retrospective in-context online learning. In Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning, 2025a. Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, and Chuang Gan. Scaling autonomous In The Thirteenth International agents via automatic reward modeling and planning. Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id= womU9cEwcO. Sanjiban Choudhury. Process reward models for llm agents: Practical framework and directions. arXiv preprint arXiv:2502.10325, 2025. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Nathan Cloos, Meagan Jens, Michelangelo Naim, Yen-Ling Kuo, Ignacio Cases, Andrei Barbu, and Christopher J. Cueva. Baba is ai: Break the rules to beat the benchmark, 2024. URL https://arxiv.org/abs/2407.13729. Google Cloud. Introducing agent evaluation in vertex ai gen ai evaluation service, 2025. URL https://cloud.google.com/blog/products/ai-machine-learning/ introducing-agent-evaluation-in-vertex-ai-gen-ai-evaluation-service. Accessed: 2025-04-24. Yali Du, Lei Han, Meng Fang, Ji Liu, Tianhong Dai, and Dacheng Tao. Liir: Learning individual intrinsic reward in multi-agent reinforcement learning. Advances in neural information processing systems, 32, 2019. Galileo. Introducing agentic evaluations, 2025. URL https://www.galileo.ai/blog/ introducing-agentic-evaluations. Accessed: 2025-04-24. 14 Preprint. Under review. Robert Gauthier and James Wallace. The computational thematic analysis toolkit. Proceedings of the ACM on Human-Computer Interaction, 6(GROUP):115, 2022. Simret Araya Gebreegziabher, Zheng Zhang, Xiaohang Tang, Yihao Meng, Elena Glassman, and Toby Jia-Jun Li. Patat: Human-ai collaborative qualitative coding with explainable interactive rule synthesis. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pp. 119, 2023. Prasoon Goyal, Scott Niekum, and Raymond Mooney. Using natural language for reward shaping in reinforcement learning. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pp. 23852391, 2019. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 68646890, 2024. Matt-Heun Hong, Lauren Marsh, Jessica Feuston, Janet Ruppert, Jed Brubaker, and Danielle Albers Szafir. Scholastic: Graphical human-ai collaboration for inductive and In Proceedings of the 35th Annual ACM Symposium on User interpretive text analysis. Interface Software and Technology, pp. 112, 2022. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. Michelle Lam, Janice Teoh, James Landay, Jeffrey Heer, and Michael Bernstein. Concept induction: Analyzing unstructured text with high-level concepts using lloom. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pp. 128, 2024. Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and Pieter Abbeel. Cic: Contrastive intrinsic control for unsupervised skill discovery. arXiv preprint arXiv:2202.00161, 2022. Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Erran Li Li, Ruohan Zhang, et al. Embodied agent interface: Benchmarking llms for embodied decision making. Advances in Neural Information Processing Systems, 37:100428100534, 2024. Xing Han Lù, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Sta nczak, Peter Shaw, Christopher J. Pal, and Siva Reddy. Agentrewardbench: Evaluating automatic evaluations of web agent trajectories, 2025. URL https://arxiv.org/abs/2504.08942. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. In The Twelfth International Conference on Learning Representations, 2024. Kevin Meng, Vincent Huang, Jacob Steinhardt, and Sarah Schwettmann. Introducing docent. https://transluce.org/introducing-docent, March 2025. Shikhar Murty, Hao Zhu, Dzmitry Bahdanau, and Christopher Manning. Nnetnav: Unsupervised learning of browser agents through environment interaction in the wild. In NNetNav, 2025. Khanh Nguyen, Hal Daumé III, and Jordan Boyd-Graber. Reinforcement learning for bandit neural machine translation with simulated human feedback. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 14641474, 2017. David Nicol and Debra Macfarlane-Dick. Formative assessment and self-regulated learning: model and seven principles of good feedback practice. Studies in Higher Education, 31(2): 199218, April 2006. 15 Preprint. Under review. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Preprint. Under review. Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kuci nski, Lerrel Pinto, Rob Fergus, et al. Balrog: Benchmarking agentic llm and vlm reasoning on games. arXiv preprint arXiv:2411.13543, 2024. Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=NPAQ6FKSmK. Melissa Pan, Mert Cemri, Lakshya Agrawal, Shuyi Yang, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Kannan Ramchandran, Dan Klein, Joseph E. Gonzalez, Matei Zaharia, and Ion Stoica. Why do multiagent systems fail? In ICLR 2025 Workshop on Building Trust in Language Models and Applications, 2025. URL https: //openreview.net/forum?id=wM521FqPvI. Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In ICML, 2017. Paul Pintrich and Akene Zusho. The development of academic self-regulation: The role of cognitive and motivational factors. In Development of achievement motivation, pp. 249284. Elsevier, 2002. Mikayel Samvelyan, Robert Kirk, Vitaly Kurin, Jack Parker-Holder, Minqi Jiang, Eric Hambro, Fabio Petroni, Heinrich Küttler, Edward Grefenstette, and Tim Rocktäschel. Minihack the planet: sandbox for open-ended reinforcement learning research, 2021. URL https://arxiv.org/abs/2109.13202. Omar Shaikh, Michelle Lam, Joey Hejna, Yijia Shao, Hyundong Justin Cho, Michael Bernstein, and Diyi Yang. Aligning language models with demonstrated feedback. In The Thirteenth International Conference on Learning Representations, 2025. Yijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, and Diyi Yang. Collaborative gym: framework for enabling and evaluating human-agent collaboration. arXiv preprint arXiv:2412.15701, 2024. Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, and Chelsea Finn. Yell at your robot: Improving on-the-fly from language corrections. arXiv preprint arXiv: 2403.12910, 2024. Mirac Suzgun and Adam Tauman Kalai. Meta-prompting: Enhancing language models with task-agnostic scaffolding. arXiv, 2024. doi: 10.48550/ARXIV.2401.12954. URL https://arxiv.org/abs/2401.12954. Michael Tomasello, Ann Cale Kruger, and Hilary Horn Ratner. Cultural learning. Behavioral and brain sciences, 16(3):495511, 1993. 17 Preprint. Under review. Vijay Viswanathan, Kiril Gashteovski, Kiril Gashteovski, Carolin Lawrence, Tongshuang Wu, and Graham Neubig. Large language models enable few-shot clustering. Transactions of the Association for Computational Linguistics, 12:321333, 2024. Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Reward shaping with language models for reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. Zijiao Yang, Arjun Majumdar, and Stefan Lee. Behavioral analysis of vision-and-language navigation agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 25742582, 2023. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ-bench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. Asaf Yehudai, Lilach Eden, Alan Li, Guy Uziel, Yilun Zhao, Roy Bar-Haim, Arman Cohan, and Michal Shmueli-Scheuer. Survey on evaluation of llm-based agents. arXiv preprint arXiv:2503.16416, 2025. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. Xinyi Zhang, Sun Kyong Lee, Hoyoung Maeng, and Sowon Hahn. Effects of failure types on trust repairs in humanrobot interactions. International Journal of Social Robotics, 15(9): 16191635, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595 46623, 2023. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, 2024a. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, LouisPhilippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al. Sotopia: Interactive evaluation for social intelligence in language agents. In The Twelfth International Conference on Learning Representations, 2024b. Yifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine, and Erran Li. Proposer-agent-evaluator(pae): Autonomous skill discovery for foundation model internet agents, 2024c. URL https://arxiv.org/abs/2412.13194. Preprint. Under review."
        },
        {
            "title": "Content of Appendix",
            "content": "1. AutoLibra Ladder Methodology Algorithm of AutoLibra Ladder AutoLibra Ladder Experiment Configuration 2. Baba-is-ai Rules and Environment Details Experiment Results Metric Scores Metric Examples Prompts Qualitative Observations of Agent Performance 3. MiniHack Rules and Environment Details Experiment Results Metric Scores Metric Examples Prompts Qualitative Observations of Agent Performance 4. WebVoyager NNetNav-Live Induced Metrics 19 Preprint. Under review."
        },
        {
            "title": "A Algorithm of AutoLibra Ladder Experiment",
            "content": "Algorithm 1 Pseudocode for iterative agent improvement with AutoLibra for task in selected_tasks do traji, eval_scorei+ = agent.play(task, prompt) annotationsi+ = editor.annotate(traji) end for metricsi = AutoLibra.extractmetrics(traji, annotationsi) traj_scoresi = AutoLibra.llmeval(metricsi, traji) curr_scores = eval_scorei while curr_scores <= eval_scorei do prompt = updated_promptk for task in selected_tasks do _, currscores = agent.play(task, prompt) 1: for in range(n_iters) do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for end for end while"
        },
        {
            "title": "B AutoLibra Ladder Experiment Setup",
            "content": "This section describes the test configuration used during AutoLibra Ladder experiments, as described in 5. For each environment experiment, one unmodified agent is used as baseline for comparison (Iteration 0), and three complete iterations of iterative agent improvement with AutoLibra are performed (Iterations 1-3), for total of four iterations. Six representative tasks for baba-is-ai are used in to induce metrics within the AutoLibra pipeline, with the remaining 34 tasks held out for evaluation. The AutoLibra Ladder pipeline is evaluated on the baba-is-ai and MiniHack environments; any changes to the agent code, the environment score, trajectory performance, and other metrics are recorded at the end of each iteration. GPT-4o-241120 OpenAI et al. (2024) is used as the agent model in all experiments; the human-in-the-loop configuration of the AutoLibra pipeline is used. Baba-is-ai Rules and Environment Configuration This section discusses the rules and implementation details of Baba is AI, the task environment used in experiments in Section 4. Baba is AI is derived from Baba is You, grid-based puzzle video game, and was originally implemented as part of the BALROG Paglieri et al. (2024) agent benchmark. Baba Is You is puzzle game where the player controls character that can navigate and modify the rules of the game by pushing blocks containing words such as you, is key, that, when combined, define the games rules. The game field consists of rectangular 4-connected grid with number of objects, rules blocks, and obstacles present. Words (represented as rule blocks) can be combined to form sentences that define the properties of objects, rules, and obstacles in the game, with rules becoming active when full sequence of rule blocks are joined in 3-block line horizontally or vertically. Active rules are built in the form [<subject> <verb> <object>], where the subject and object are the names of objects in the game, and the verb is one of the following: \"is\", \"has\", \"can\", or \"not\". The goal of the game is to reach win condition, by manipulating the rules of the game to create new win conditions, modify the properties of objects, or change the behavior or identity of the player character. 20 Preprint. Under review. Figure 9: Example of the baba-is-ai environment. In this task (two_room-break_stop-make_windistr_obj-irrelevant_rule), the agent has to break the \"wall is stop\" rule by pushing either wall, is, or stop out of alignment with the other blocks, and must then push the \"key\" rule block next to \"is win\" at (9, 1) to assemble the win rule, then touch the key at (7, 7) to win. Pushing the \"door\" rule block would be mistake, as no door object is present. The testing implementation we use is the baba-is-ai environment, simplified version of Baba is You originally implemented within the BALROG agent benchmark Cloos et al. (2024); Paglieri et al. (2024). This implementation has several simplifications compared to the original game, including smaller grid size, fewer objects, and limited set of rules. The environment is designed to be easy to use and understand, while still providing challenging testbed for evaluating the performance of reinforcement learning agents on agentic reasoning tasks. single baba-is-ai task is defined by an arbitrary rectangular grid where an exit condition must be reached, with several possible obstacles and rules preventing the player from reaching the exit. Intermediate goals to achieving the exit condition can be defined by the user, and the environment will generate task that requires the agent to learn how to manipulate the rules of the game in order to reach the exit. These intermediate goals are referred to as \"subtasks\" within our paper, and include: Goto-Win: The agent must reach specific location on the grid. Make-Win: The agent must create new win condition by manipulating the rules of the game. Break-Stop: The agent must break or bypass wall or obstacle in order to reach the exit. Change-You: The agent must change the identity of the player character in order to reach the exit, by modifying the \"baba is you\" rule. These subtasks can be arbitrarily combined with distractor objects or rules, immovable game field walls, and each other to generate tasks of varying complexity and length; baba-is-ai implements 40 total unique tasks of gradually increasing difficulty. The agent is provided observations in text form, which includes the current state of the game field, the currently active rules, and relative locations of obstacles to the active player character in terms of shortest Manhattan distance. This is done to make the environment compatible with purely text-based language models. Preprint. Under review. The agent is provided the space of possible actions it can take, which consist moving one space in one of the four cardinal directions (up, down, left, right), but is not given information about whether given move will result in movement; it is thus free to move into immovable walls without effect, to push rule blocks into corners where they cannot be moved, or to make the task unsolvable by breaking the active \"baba is you\" rule without taking control of new object; in this last instance, the environment automatically resets the task to randomized beginning state for that task. The environment is limited to 100 steps per task episode to avoid tasks being solved by random walks. Baba-is-ai Experiment Results"
        },
        {
            "title": "Iteration",
            "content": "0 1"
        },
        {
            "title": "Baseline",
            "content": "Babaisai Score GPT-4o 30% 40% 43% 55% Babaisai Score GPT-4o (Only Held-Out) 33% 40% 44% 53% Babaisai Score Claude 3.5 Sonnet 35% 40% 45% 55% Babaisai Score Claude 3.5 Sonnet (Only Held-Out) 38% 42% 47 % 58% 33% 33% 37% 33% Average Env. Steps 79 63 60 51 - Table 5: Baba-is-ai Scores and Average Environment Steps Baba-is-ai Metric Scores"
        },
        {
            "title": "Iteration",
            "content": "0 1"
        },
        {
            "title": "Win Condition Recognition",
            "content": "35.0% 55.0% 87.5% 87.5%"
        },
        {
            "title": "Rule Modification",
            "content": "0.0% 10.0% 37.5% 61.9%"
        },
        {
            "title": "Direct Navigation Efficiency",
            "content": "5.6% 22.5% 27.5% 37.5% Context-Sensitive Decision Making"
        },
        {
            "title": "Win Rule Construction",
            "content": "Selective Interaction With Relevant Objects Rule Manipulation and Execution"
        },
        {
            "title": "Redundancy",
            "content": "2.5% 27.5% 30.0% 37.5% 0.0% 0.0% 0.0% 5.3% 35.0% 40.0% 45.0% 82.5% 0.0% 12.5% 31.0% 35.5% 2.5% 25.0% 27.5% 35.0% 64.9% 69.7% 69.7% 87.9% 65.0% 83.0% 85.0% 92.0% 58.0% 59.0% 47.0% 59.0% Table 6: Metric Performance for baba-is-ai AutoLibra Iterations 03, Across Full (40) Environment Tasks 22 Preprint. Under review. Baba-is-ai Metric Examples Iteration 0: Context-Sensitive Decision Making Explanation: This metric assesses the agents capacity for context-sensitive decision making. It evaluates whether the agent tailors its actions according to the immediate game conditionsbalancing between direct navigation and rule modification. Positive behaviors in new environments will demonstrate an ability to determine when obstacles require intervention and when direct movement is sufficient, thereby optimizing overall efficiency. Good Behaviors: Accurately gauges the game context by recognizing when obstacles are not an issuesuch as when the win condition is already accessibleand refrains from unnecessary rule modifications. Selects focused, goal-oriented actions that align with observed win conditions, avoiding extraneous exploration. Adapts its strategy based on spatial layout and current rules, ensuring that its actions are timely and appropriate for the situation. Bad Behaviors: Engages in excessive exploratory actions that do not contribute to reaching the win condition. Repeatedly takes ineffective actions (for example, persistently moving into walls) before finally switching strategy, indicating delayed context-sensitive decisions. Alters irrelevant rules or diverts attention from the active win condition when the situation does not demand it. Fails to adjust decision-making based on contextual cues, leading to uncoordinated or delayed progression toward the goal. Iteration 1: Rule Modification for Obstacle Management Explanation: This metric measures the agents competence in managing obstacles through rule modifications. It focuses on the agents ability to detect when game rule (such as blocking wall or an unchangeable character assignment) is hindering progress and to successfully alter that rule to create viable path to victory. In novel scenarios, agents displaying positive behavior will apply targeted rule changes that directly open the path toward the win condition. Good Behaviors: Proactively breaks the wall is stop rule when an obstacle blocks access to the win condition. Effectively modifies rulessuch as replacing baba is you with keyto remove or bypass obstacles. Bad Behaviors: Fails to modify critical rule blocks (for instance, not altering baba is you when required) that prevent access to the win condition. Does not interact with immovable obstacles like the wall is stop rule, neglecting available mechanisms to bypass them. Neglects to rearrange rule blocks to create or build necessary win conditions (e.g., door is win), leaving obstacles unaddressed. Preprint. Under review. Iteration 2: Subtask Coordination and Overall Task Planning Explanation: This metric assesses how well the agent coordinates multiple sub-tasks and plans its overall strategy. It rewards behaviors that demonstrate clear sequencing from recognizing obstacles and manipulating rules to directly advancing toward the final objective. Failures in subtask coordination result in repetitive loops, ineffective transitions between actions, and an inability to achieve meaningful win condition. Good Behaviors: Final movement and approach towards ball after removing the obstacle. Direct movement between objectives as opposed to unrelated exploration. Throughout the trajectory, the agent repeatedly chooses actions that reduce the distance to the door: moving left and down as needed. The trajectory demonstrates efficient movement towards the goal without unnecessary actions. The trajectory shows direct movement to the door without redundant backtracking or circular movement. Throughout the trajectory, the agent follows direct and purposeful path towards its goal. Bad Behaviors: The trajectory shows multiple iterations of upwards movement resulting in no significant progress. In the trajectory, actions such as repeated left moves where no significant progress towards the goal is made. In over multiple steps, the agent moves unsuccessfully against immovable boundaries and objects. There are periods in the trajectory where the agent exhibits loops or repetitive movements without advancing its position strategically. Ultimately, the agents efforts to form victory conditions do not result in meaningful or achievable goal given the maps configuration. 24 Preprint. Under review. Iteration 3: Interaction with Immovable Obstacles Explanation: This metric measures how the agent handles immovable obstacles. Positive behaviors show proper recognition and effective avoidance of fixed objects, whereas negative behaviors involve futile or incorrect push attempts that betray lack of understanding of the environments static features. Good Behaviors: Recognizes that immovable walls or blocks should not be pushed and instead plans to bypass them. Avoids colliding with immovable objects by correctly assessing their fixed nature. Plans actions that account for static obstacles, ensuring safe navigation around them. Bad Behaviors: Repeatedly tries to push into an immovable wall or rule block despite the known constraints. Interacts with stationary obstacles in ways that disregard their immovability, leading to ineffective progress. Executes push commands in the wrong direction on fixed objects, indicating misunderstanding of obstacle dynamics. 25 Preprint. Under review. Baba-is-ai Prompts Iteration 0 Baba-is-ai Prompt Baba Is You is puzzle game where you can manipulate the rules of each level. The following are the possible actions you can take in the game, followed by short description of each action: idle: wait for one step, up: take one step up, right: take one step to the right, down: take one step down, left: take one step to the left. Tips: Examine the level carefully, noting all objects and text blocks present. Identify the current rules, which are formed by text blocks in the format \"[Subject] IS [Property]\" (e.g. \"BABA IS YOU\"). Consider how you can change or create new rules by moving text blocks around. Remember that you can only move objects or text that are not defined as \"STOP\" or similar immovable properties. Your goal is usually to reach an object defined as \"WIN\", but this can be changed. Think creatively about how changing rules can alter the properties and behaviors of objects in unexpected ways. If stuck, try breaking apart existing rules or forming completely new ones. Sometimes the solution involves making yourself different object or changing what counts as the win condition. PLAY! Current Observation: Active rules: ball is win baba is you Objects on the map: rule ball: 5 steps to the left and 1 step up rule is: 4 steps to the left and 1 step up rule win: 3 steps to the left and 1 step up ball: 5 steps to the left and 2 steps down rule baba: 5 steps to the left and 4 steps down rule is: 4 steps to the left and 4 steps down rule you: 3 steps to the left and 4 steps down 26 Preprint. Under review. First, think about the best course of action. Then, you must choose exactly one of the listed actions and output it strictly in the following format: <ACTION>YOUR_CHOSEN_ACTION<END> Replace YOUR_CHOSEN_ACTION with the chosen action. Iteration 1 Baba-is-ai Prompt Baba Is You is puzzle game where you can manipulate the rules of each level. The following are the possible actions you can take in the game, followed by short description of each action: idle: wait for one step, up: take one step up, right: take one step to the right, down: take one step down, left: take one step to the left. Additional Tips: 1. The game is won by identifying win condition and making it true by placing the \"object is win\" rule blocks next to each other in line. 2. First, identify the win condition and where the object corresponding to the win condition is located. 3. If it is blocked, identify the rules that are blocking it and try to remove them, or circumvent them by changing the character you control by changing the \"baba is you\" rule. 4. If the path to the win condition is not blocked, travel directly to the win condition object without distractions. 5. If \"wall is stop\" rule is bounded on two sides by walls, the blocks cannot be moved, and you must find another way to reach the win condition. 6. Ignore any objects not related to the win condition, as they are not necessary to complete the level. Example: If your observation is: Active rules: ball is win wall is stop baba is you Objects on the map: wall 5 steps to the right and 2 steps up rule ball 8 steps to the right and 2 steps up rule is 9 steps to the right and 2 steps up rule win 10 steps to the right and 2 steps up rule wall 1 step up rule is 1 step to the right and 1 step up rule stop 2 steps to the right and 1 step up wall 5 steps to the right and 1 step up rule door 10 steps to the right and 1 step up wall 5 steps to the right ball 6 steps to the right wall 5 steps to the right and 1 step down wall 5 steps to the right and 2 steps down 27 Preprint. Under review. wall 5 steps to the right and 3 steps down rule baba 4 steps down rule is 1 step to the right and 4 steps down rule you 2 steps to the right and 4 steps down wall 5 steps to the right and 4 steps down door 7 steps to the right and 4 steps down You should reason that: The win condition is \"ball is win,\" therefore you should reach the ball to win. The ball is blocked by wall, so you should remove the \"wall is stop\" rule. The \"wall is stop\" rule is not bounded by walls, so you can move the blocks to remove the rule. The door is not necessary to reach the win condition, so you can ignore it. Once the \"wall is stop\" rule is removed, you can move directly to the ball to win. PLAY! Current Observation: Active rules: baba is you Objects on the map: rule is: 2 steps to the left and 3 steps up rule win: 1 step to the left and 3 steps up key: 2 steps to the right and 2 steps up rule key: 1 step to the right and 1 step up rule baba: 3 steps to the left and 2 steps down rule is: 2 steps to the left and 2 steps down rule you: 1 step to the left and 2 steps down ball: 2 steps down rule ball: 2 steps to the right and 2 steps down First, think about the best course of action. Then, you must choose exactly one of the listed actions and output it strictly in the following format: <ACTION>YOUR_CHOSEN_ACTION<END> Preprint. Under review. Iteration 2 Baba-is-ai Prompt Baba Is You is puzzle game where you can manipulate the rules of each level. The following are the possible actions you can take in the game, followed by short description of each action: idle: wait for one step, up: take one step up, right: take one step to the right, down: take one step down, left: take one step to the left. You solve the puzzle by identifying the type of sub-problem, and then applying the following blocks of solution steps: 1. If the win condition is not blocked and its rule is active, move to the win condition object. 2. If the win condition is blocked by wall and \"wall is stop\" rule is active and not bounded by the map boundary: (a) Move into the \"wall is stop\" blocks to remove the rule. (b) Move to the win condition object. 3. If the win condition is blocked by wall and \"wall is stop\" rule is active and bounded by the map boundary: (a) Locate and move to an object rule block on your side of the wall. (b) Push the object rule block towards the \"baba is you\" rule block by moving into it, and use this rule block to push the \"baba\" block out of the \"baba is you\" rule block. 4. If the win condition is not active: (a) Locate the object rule block that can be pushed to the \"is\" rule block to activate the win condition. (b) Push the object rule block to the \"is\" rule block, making sure they are adjacent. (c) Move to the win condition object. 5. If the win condition object is not present: (a) Locate an object rule block that can be pushed to the \"is\" rule block to create the win condition object. (b) Push the object rule block to the \"is\" rule block, making sure they are adjacent. (c) Move to the win condition object. Example: If your observation is: Active rules: wall is stop baba is you Objects on the map: wall 3 steps to the right and 3 step up rule is 7 steps to the right and 3 step up rule win 8 steps to the right and 3 step up rule wall 2 step to the left and 2 step up rule is 1 step to the left and 2 step up rule stop 2 step up wall 3 steps to the right and 2 step up wall 3 steps to the right and 1 step up key 4 steps to the right and 1 step up 29 Preprint. Under review. wall 3 steps to the right rule ball 7 steps to the right wall 3 steps to the right and 1 step down wall 3 steps to the right and 2 steps down rule baba 2 step to the left and 3 steps down rule is 1 step to the left and 3 steps down rule you 3 steps down wall 3 steps to the right and 3 steps down ball 4 steps to the right and 3 steps down You should plan your actions as follows: 1. The win condition is not active, so it needs to be built. 2. The win rule can be built by pushing the \"ball\" block on the other side of the wall next to the \"is\" block. 3. The \"wall is stop\" rule is blocking the path to the ball, so you should remove this rule first. 4. The \"wall is stop\" rule is not bounded by walls, so you can move the blocks to remove the rule. Move 2 steps to the left and 2 steps up to reach the \"wall is stop\" rule, and push the \"wall\" block to remove the rule. 5. Once the \"wall is stop\" rule is removed, you can push the \"ball\" block to the \"is\" block to win. Move 7 steps to the right, 3 steps down to reach the \"ball\" block. Move 3 steps up to push the \"ball\" block to the \"is\" block. 6. Once you detect the \"ball is win\" rule as being active, you can move directly to the ball to win. PLAY! Current Observation: Active rules: ball is win baba is you Objects on the map: rule ball: 3 steps to the left and 3 steps up rule is: 2 steps to the left and 3 steps up rule win: 1 step to the left and 3 steps up ball: 3 steps to the left and 2 steps up key: 3 steps to the left and 1 step up rule baba: 3 steps to the left and 2 steps down rule is: 2 steps to the left and 2 steps down rule you: 1 step to the left and 2 steps down First, think about the best course of action. Then, you must choose exactly one of the listed actions and output it strictly in the following format: <ACTION>YOUR_CHOSEN_ACTION<END> Replace YOUR_CHOSEN_ACTION with the chosen action. 30 Preprint. Under review. Iteration 3 Baba-is-ai Prompt Baba Is You is puzzle game where you can manipulate the rules of each level. The following are the possible actions you can take in the game, followed by short description of each action: idle: wait for one step, up: take one step up, right: take one step to the right, down: take one step down, left: take one step to the left. You solve the puzzle by identifying the type of sub-problem, and then applying the following blocks of solution steps: 1. If the win condition is not blocked and its rule is active, move to the win condition object. 2. If the win condition is blocked by wall and \"wall is stop\" rule is active and not bounded by the map boundary: (a) Move into the \"wall is stop\" blocks to remove the rule. (b) Move to the win condition object. 3. If the win condition is blocked by wall and \"wall is stop\" rule is active and bounded by the map boundary: (a) Locate and move to an object rule block on your side of the wall. (b) Push the object rule block towards the \"baba is you\" rule block by moving into it, and use this rule block to push the \"baba\" block out of the \"baba is you\" rule block. 4. If the win condition is not active: (a) Locate the object rule block that can be pushed to the \"is\" rule block to activate the win condition. (b) Push the object rule block to the \"is\" rule block, making sure they are adjacent. (c) Move to the win condition object. 5. If the win condition object is not present: (a) Locate an object rule block that can be pushed to the \"is\" rule block to create the win condition object. (b) Push the object rule block to the \"is\" rule block, making sure they are adjacent. (c) Move to the win condition object. Example: If your observation is: Active rules: wall is stop baba is you Objects on the map: rule wall at (1, 1) rule is at (2, 1) rule stop at (3, 1) rule is at (7, 3) rule win at (8, 3) 31 Preprint. Under review. rule ball at (7, 0) rule baba at (5, 6) rule is at (6, 6) rule you at (7, 6) wall at (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6) key at (5, 1) ball at (5, 6) Current position: (4, 3) in grid of shape (1, 0) to (8, 6) You should plan your actions as follows: 1. The win condition is not active, so it needs to be built. 2. The win rule can be built by pushing the ball block on the other side of the wall next to the is block. 3. The \"wall is stop\" rule is blocking the path to the ball, so you should remove this rule first. 4. The \"wall is stop\" rule is not bounded by walls, so you can move the blocks to remove the rule. Move 2 steps to the left and 2 steps up to reach the \"wall is stop\" rule, and push the wall block to remove the rule. 5. Once the rule is removed, you can push the ball block to the is block. Move 7 steps to the right and 3 steps down to reach the ball block. Move 3 steps up to push the ball block to the is block. 6. Once you detect the ball is win rule is active, move directly to the ball to win. PLAY! Current Observation: Active rules: baba is you Objects on the map: rule is: at (2, 1) rule win: at (3, 1) key: at (6, 2) rule key: at (3, 4) door: at (6, 5) rule baba: at (1, 6) rule is: at (2, 6) rule you: at (3, 6) rule door: at (6, 6) Current position: (4, 3) in grid of shape (1, 1) to (6, 6) Rule block pushing physics explanation: Avoid moving into any non-wall rule block unless you are ready to interact with it. Do not push \"wall is stop\" blocks if the \"wall is stop\" rule is not currently active. 32 Preprint. Under review. Steps to avoid unwanted interaction: 1. Read all rule block positions. 2. Read your current position and direction. 3. If rule block is in your path, change direction by 90 (e.g., if going up, go left or right). 4. If rule is on the wall of the grid, it can only be pushed parallel to that wall. Example: If you are at (3, 3) and rule block is at (3, 4), you cannot push it up by first moving down. Instead, move to (2, 3) or (4, 3), then move down to (3, 6) to push it upward. Strategic Planning: First, think about your current goal based on the game state. If an active rule changes, your goal likely changes too. Most Important Rule: If \"wall is stop\" is active, find way to remove it or gain control of different object before doing anything else. Pushing Strategy: Always push blocks upward first (maximum 10 spaces), then push them left or right. Rule blocks are written in backticks. Objects are not. Only blocks can be pushed. Never interact directly with the is or win blocks. Rule Construction Example: If win is at (11, 1) and is is at (10, 1), place the object rule at (9, 1) to form valid rule. Coordinate System: Top-left is (1, 1), bottom-right is (x, y). Moving right increases x, moving down increases y. Goal Planning Example: Active rules: wall is stop Must be broken under all circumstances! Steps to achieve the goal: 1. Break \"wall is stop\" by moving into its blocks from below. 2. Build \"ball is win\" rule. 3. ball block is at (7, 4), is is at (8, 1). So: ball needs to move 3 steps up, 1 step right. Push from the left (1 step) and from below (3 steps). 4. If youre at (6, 4), move 1 step down and 1 step right to reach (7, 5), then push from below. 5. Move 3 steps up. Then, you must choose exactly one of the listed actions and output it strictly in the following format: <ACTION>YOUR_CHOSEN_ACTION<END> 33 Preprint. Under review. Qualitative Observations of baba-is-ai Agent Performance The metrics induced by AutoLibra  (Table 4)  capture the behavior of the agent effectively, with coverage increasing from 65% at Iteration 0 to 92% at Iteration 3 and low mean redundancy of 56%. Notably, earlier metrics were found to describe higher-level behaviors than later metrics, reflective of the more complex behaviors that the agent demonstrated in later iterations, as well as the compositionality of induced metrics. Code changes were selected to specifically target given metrics, and as seen in Figure 8, the agents performance on the targeted metric improved significantly in the iteration following the code change. This demonstrates the utility of AutoLibra for fine-grained agent improvement, as well as the human interpretability of the induced metrics. The section below discusses iteration-by-iteration induced metrics, code changes directed by the metrics, and the results of these code changes on agent environment and trajectory performance. Iteration 0 The agent behavior is stochastic and inconsistent, with no clear strategy or goal. Any progress in the environment is the consequence of random walk. Win Condition Recognition was identified as prerequisite to all other metrics induced in this iteration, as progress in the environment is impossible without recognizing the win condition, and was therefore targeted for improvement. Improvements took the form of few-shot prompting, by supplying an example of observations and the corresponding win condition and subtasks, and guidance on subtask-level planning, by mapping specific environmental observations to actions that would progress the agent towards the win condition. Iteration 1 An increase in performance of 22% was observed between Iteration 0 and Iteration 1, indicating that the code changes were effective in improving the agents ability to recognize the win condition. slight but statistically insignificant increase was observed for the other metrics induced in Iteration 0, but the agents overall baba-is-ai environment score increased by 10%, indicating that identified by AutoLibra as key bottleneck to the agents performance in the environment. was correctly The agent now targets specific blocks and objects instead of moving randomly, but gets stuck in loops and on immovable objects, and is unable to consistently complete multi-step tasks. Based on the metrics induced in Iteration 1, several changes to the agent code were implemented: Augmentation of existing subtask-level planning guidance with low-level position instructions, targeting improvement of Rule Modification for Obstacle Management and Direct Navigation Efficiency Meta-prompting Suzgun & Kalai (2024) by providing subtask identification heuristic, targeting improvement of Selective Interaction with Relevant Objects Context-Sensitive Decision Making and Augmentation of single-shot example with movement instructions, targeting improvement of Rule Manipulation and Execution Iteration 2 Substantial improvements in were observed due to more detailed single-shot examples and reasoning templates, and the corresponding increase in other metrics supports idea that is key bottleneck to the agents performance. We further observed significant and , indicating that the changes targeting these metrics were effective in increase in improving the agents reasoning performance and consequent avoidance of irrelevant objects. Subtask Identification map directions and confuses rule blocks with objects. saw no significant improvement, as the agent still misunderstands Qualitatively, the agent now recognizes when wall rules need to be broken and breaks them, but still cannot assemble rules to win, as it gets stuck on immovable blocks and walls and does not understand where rule blocks need to be placed to form win condition. Based on the metrics induced in Iteration 2, several changes to the agent code were implemented: 34 Preprint. Under review. Formatting observations as absolute position (as opposed to relative steps from the agent), and listing of immovable/movable blocks to improve Direct Navigation Efficiency Chain-of-thought reasoning to assemble subtask completion agenda based on iterationto-iteration observations, targeting improvement of Subtask Coordination and Overall Task Planning Augmentation of single-shot example to few-shot example with reasoning templates, targeting improvement of Rule Manipulation and Execution Explicit instructions on navigation to avoid block collisions and assembly of rules, including movement templates, targeting improvement of Rule Manipulation and Execution and Win Rule Construction Iteration 3 We observe large increase in , indicating that the changes targeting these metrics were effective in improving the agents reasoning performance and consequent and avoidance of irrelevant objects. saw further improvement, indicating that the use of absolute position and immovable/movable block listing was effective in improving the agents navigation efficiency. saw slight increase, indicating that the changes targeting this metric were effective in improving the agents ability to manipulate rules to achieve the win condition. saw its first improvement, indicating that more complex metrics are effectively targeted when the base performance and simpler metrics are improved. Qualitatively, nearly every agent always understands when wall rules can be and cannot be broken, as well as when its not necessary to break the wall rule, covers nearly all tasks that involve going to win. The agent understands how to assemble win rules but still struggles with changing block direction and understanding that blocks get stuck against walls. 35 Preprint. Under review."
        },
        {
            "title": "I MiniHack Rules and Environment Configuration",
            "content": "Figure 10: Overview of representative tasks used in MiniHack agent optimization process with AutoLibra. From left to right, up to down: (1) Boxoban, (2) MazeWalk, (3) Corridor Fight, and (4) Quest. This section discusses the rules and implementation details of MiniHack, another task environment used in experiments in Section 4. Similarly to Baba is AI, MiniHack is derived from grid-based puzzle video game (NetHack), and was originally implemented as part of the BALROG Paglieri et al. (2024) agent benchmark. MiniHack is grid navigation game expressed in text similar to baba-is-ai, consisting of procedurally generated environment that requires the agent to navigate space consisting of various agent roles, creatures, items, and tasks to reach goal Samvelyan et al. (2021). Given its plasticity and abundant elements, MiniHack is more complex, challenging, and diversified than baba-is-ai; this is reflected in lower success rate for agents on MiniHack versus Baba is AI, with baseline agent task completion rate of 10% on MiniHack vs 33% on baba-is-ai Paglieri et al. (2024). Similar to our experiments with baba-is-ai, the Ladder improvement process for MiniHack also follows the algorithm detailed in Appendix A. Two full iterations of agent improvement with AutoLibra were performed on MiniHack. Four representative tasks for MiniHack are used in iterative metric improvement, with the remainder held out for evaluation. The agent is evaluated on the MiniHack environment and any changes to the agent code at the beginning of each iteration, and the environment score, trajectory performance, and other metrics are recorded at the end of each iteration. GPT-4o-241120 is used as the agent model. The four selected representative tasks each contain unique subset of subtasks evaluating an agents capability. Figure 10 shows an example for each task, and from left to right, up to down, these maps respectively represent Boxoban, MazeWalk, Corridor Fight, and Quest. Boxoban is box-pushing puzzle game inspired by Sokoban, rendered within the MiniHack environment. To succeed in Boxoban, the agent needs to push the four boulders (orange balls) onto the four fountains (blue icons), and partial credit will be awarded for pushing some of the boulders onto the fountains. Boxoban tests the agents capability in strategic planning and rule-following. MazeWalk is game that requires the agent to explore unknown dark spaces to find the target exit staircase (the icon with downward arrow). Two challenges for MazeWalk are fog of war the agent initially lacks information about areas of the map it hasnt visited, and must explore to discover the map and maze layout, and darkness even if the agent has visited block, the block will become dark as the agent walks away and it passes out of view, retaining information about its layout but not any enemies or items present. Thus, MazeWalk tests the agents capability in map memory and strategic searching. Corridor Fight is game that requires the agent to explore an unknown dark corridor map to find the target exit staircase while engaging or avoiding giant rat enemies. Corridor Fight tests the agents capability in memory, space awareness, hazard awareness, and strategic combat. 36 Preprint. Under review. Quest requires the agent to use tool to help itself cross an otherwise impassable wall of lava, survive randomly generated monsters, and search for the target exit staircase. As the most subtask-rich and randomized game, it tests the agents abilities to recognize and utilize tools, understand its role and special power, and strategically survive from monsters. In all tasks, the agent is provided observations in text form, which includes the current state of the game field, the currently active rules, and relative locations of obstacles to the active player character in terms of shortest Manhattan distance. This is done to make the environment compatible with purely text-based language models. The environment is limited to 100 steps per task episode to avoid tasks being solved by random walks."
        },
        {
            "title": "Turn",
            "content": "0"
        },
        {
            "title": "Baseline",
            "content": "MiniHack Score GPT-4o 0% 12.5% 25% 10% Average Env. Steps 85 88 - Table 7: MiniHack Score and Average Environment Steps"
        },
        {
            "title": "Efficient Exploration and Map Memory\nUtilization",
            "content": "Hazard Awareness and Equipment Utilization"
        },
        {
            "title": "Combat Engagement and Survival",
            "content": "Role-Specific Ability Utilization"
        },
        {
            "title": "Iteration",
            "content": "0 1 2 16.67% 8.33% 41.67% 16.67% 0.00% 25.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 8.33% 8.33% 25.00% 0.00% 0.00% 0.00% - - - 16.67% 58.33% 0.00% 0.00% - 25.00% 82.89% 81.82% 87.84% 61.11% 65.63% 71.30% Table 8: Metric Performance for MiniHack AutoLibra Iterations 02 37 Preprint. Under review."
        },
        {
            "title": "L MiniHack Metric Examples",
            "content": "Iteration 0: Target Navigation Effectiveness Explanation: This metric evaluates the agents ability to accurately identify and navigate toward key goal locations (e.g., the downward stairs). Positive behaviors show clear planning and directional focus, while negative examples reveal distractions or failures in goal-oriented movement. Good Behaviors: Behavior: The agent decides to move south and reaches the downward stairs. Behavior: The agents reasoning shows it plans its action to reach the stairs. Behavior: From its reasoning, the agent appears to recognize the goal of locating the stairs downward. Behavior: From the trajectory and reasoning, the agent consistently aimed toward the general direction of its goalthe stairs down. Bad Behaviors: Behavior: The repeated attempts to kick the goblin instead of progressing towards the stairs. Behavior: The agent repeatedly interacts with adjacent stairs up but does not visibly attempt to identify or locate downward stairs. Behavior: The agent explored the map but did not identify the downward stair in its trajectory. Behavior: Throughout the trajectory, the agent did not manage to locate the downward stairs, despite exploring various places in the environment. Behavior: The agent spends multiple actions traversing the map without making meaningful progress toward the downward stairs. Behavior: Performed in the sequence of actions where the agent explores but eventually doesnt find the stairway downward, ending the session unsuccessfully. Behavior: During its navigation, the agent didnt succeed in reaching the stairs down. Preprint. Under review."
        },
        {
            "title": "M MiniHack Prompts",
            "content": "Iteration 0 MiniHack Prompt You are an agent playing MiniHack. The following are the possible actions you can take in the game, followed by short description of each action: north: move north, east: move east, south: move south, west: move west, northeast: move northeast, southeast: move southeast, southwest: move southwest, northwest: move northwest, far north: move far north, far east: move far east, far south: move far south, far west: move far west, far northeast: move far northeast, far southeast: move far southeast, far southwest: move far southwest, far northwest: move far northwest, up: go up the stairs, down: go down the stairs, wait: rest one move while doing nothing, more: display more of the message, apply: apply (use) tool, close: close an adjacent door, open: open an adjacent door, eat: eat something, force: force lock, kick: kick an enemy or locked door or chest, loot: loot box on the floor, pickup: pick up things at the current location if there are any, pray: pray to the gods for help, puton: put on an accessory, quaff: quaff (drink) something, search: search for hidden doors and passages, zap: zap wand For task which name consists \"corridor\": Your goal is to explore the level and reach the stairs down. For task which name consists quest: Your goal is to explore the level, fight monsters, and navigate rooms and mazes to ultimately reach the stairs down. For task which name consists boxoban: You are playing Boxoban, box-pushing game inspired by Sokoban. Your goal is to push the boulders onto the fountains on the map. You can push the boulders by walking into them, as long as there are no obstacles behind them. 39 Preprint. Under review. For task which name consists mazewalk: Your goal is to explore the level and reach the stairs down. Otherwise: Your goal is to get as far as possible in the game. In moment will present history of actions and observations from the game. Tip: there is no point in outputting the same action over and over if nothing changes. Additional Feedback (if provided): will also present lists of positive and negative feedback based on your previous attempts on this task, use this information to change your strategy and improve your performance. Positive: positive Negative: negative PLAY! Iteration 1 MiniHack Prompt You are an agent playing MiniHack. The following are the possible actions you can take in the game, followed by short description of each action: north: move north, east: move east, south: move south, west: move west, northeast: move northeast, southeast: move southeast, southwest: move southwest, northwest: move northwest, far north: move far north, far east: move far east, far south: move far south, far west: move far west, far northeast: move far northeast, far southeast: move far southeast, far southwest: move far southwest, far northwest: move far northwest, up: go up the stairs, down: go down the stairs, wait: rest one move while doing nothing, more: display more of the message, apply: apply (use) tool, close: close an adjacent door, open: open an adjacent door, eat: eat something, force: force lock, kick: kick an enemy or locked door or chest, loot: loot box on the floor, pickup: pick up things at the current location if there are any, pray: pray to the gods for help, puton: put on an accessory, 40 Preprint. Under review. quaff: quaff (drink) something, search: search for hidden doors and passages, zap: zap wand For task name contains corridor: Your goal is to explore the level and reach the stairs down. Notes: 1. The target stairs down is always at the other room on the map, so you should prioritize exploring the other room. 2. Whenever you encounter monster (giant rat) in room, you should always try to run to the direction of the corridor. 3. Whenever you encounter monster (giant rat) on the corridor, you should always try to kill the monster (giant rat). 4. Once you reach the second room on the map, you should explore the room fully to find the target stairs down, but whenever you encounter monster (giant rat), you should always try to run to the direction of the corridor. Example: --- .< ..@# . For task name contains quest: Your goal is to explore the level, fight monsters, and navigate rooms and mazes to ultimately reach the stairs down. Notes: 1. You should never cross (lava) without necessary ability or equipment. 2. Always explore the area approachable (no need to cross the lava) well enough to find sufficient support. Below is an example: Map Example: -------------- @....}......--- .(...}.......F- .....}..................... .....}........-------- .....}......--- -------------- 3. You should always try to apply the tool you can get, even though you do not know the effect. 4. You should always try to apply your roles ability, even though you do not know the effect. For task name contains boxoban: You are playing Boxoban, box-pushing game inspired by Sokoban. Your goal is to push the boulders onto the fountains on the map. You can push the boulders by walking into them, as long as there are no obstacles behind them. Notes: For this task, you should follow the below rules with priority from top to down: 1. You should remember the locations of the fountains in the beginning. 2. You should never push boulder in direction if in this direction, the next spot next to the boulder is not empty or is not the fountain. 41 Preprint. Under review. 3. You should never push boulder to any L-shaped wall configuration (two adjacent walls). Example (dead corner): ########## #`.....### 4. You should never push boulder next to wall unless, in the direction perpendicular to the line joining the boulder and its neighbor wall, there is at least one fountain left open such that between this fountain and the boulder all spots are empty. 5. You should never push boulder if it is already on the fountain. 6. If boulder and fountain are on the same row or column, and all spots between this boulder and this fountain are empty, and the first spot next to the boulder on the opposite direction of which the fountain locates is empty, you should always push this boulder to the corresponding fountain. Map Example (established path): ########## #......<## #.#@.{`.## #.``##.#.# #..`...{.# #.#.{....# #######.{# #######..# #######..# ########## ########## #`.....### Example (dead corner reminder): For task name contains mazewalk: Your goal is to explore the level and reach the stairs down. Notes: For this task, your action space will be limited to: {\"north\": \"move north\", \"east\": \"move east\", \"south\": \"move south\", \"west\": \"move west\"} You will not choose any actions other than these four. Follow strictly: 1. If the block on your east is empty, move east. 2. If east is blocked and north is open, move north. 3. If east and north are both blocked, and west is open, move west. 4. If east, north, and west are blocked, move south. Otherwise: Your goal is to get as far as possible in the game. In moment will present history of actions and observations from the game. Tip: there is no point in outputting the same action over and over if anything changes. PLAY! 42 Preprint. Under review. Iteration 2 MiniHack Prompt You are an agent playing MiniHack. The following are the possible actions you can take in the game: north: move north, east: move east, south: move south, west: move west, northeast: move northeast, southeast: move southeast, southwest: move southwest, northwest: move northwest, far north: move far north, far east: move far east, far south: move far south, far west: move far west, far northeast: move far northeast, far southeast: move far southeast, far southwest: move far southwest, far northwest: move far northwest, up: go up the stairs, down: go down the stairs, wait: rest one move while doing nothing, more: display more of the message, apply: apply (use) tool, close: close an adjacent door, open: open an adjacent door, eat: eat something, force: force lock, kick: kick an enemy or locked door or chest, loot: loot box on the floor, pickup: pick up things at the current location if there are any, pray: pray to the gods for help, puton: put on an accessory, quaff: quaff (drink) something, search: search for hidden doors and passages, zap: zap wand For task name contains corridor: Your goal: Your goal is to explore the level and reach the stairs down. Notes: Most important notice before you start: \".\" in the map means room, so if your current space is marked by \".\" you are in the room. In the map, \"#\" means corridor. You should never misrecognize the type of location you are currently at as this is crucial for your game. Always be certain about your current location before you decide an action. You should permanently treat < the same as . 43 Preprint. Under review. 1. The target stairs down is always at the other room on the map, so you should prioritize exploring the other room. Remember, the goal spot is always at the far east of the map. 2. Whenever you havent moved onto the corridor (#) yet, find one # as soon as possible and move onto it. Example: Example: --- .< ..@# . ----- ..< @... ... ----- 3. Whenever you encounter monster (giant rat) in room, you should always try to run to the direction of the corridor. As soon as you reach the second block on the corridor, you should stop running away after moving west for max of 3 steps and start fighting the rat. Fight until there are no rats on your adjacent spots. Example: --- ---- ... r...% ....#######@r.... r.... <.. --- ---- You should only fight the rat when there is exactly one rat adjacent (in the 8 surrounding spots). 4. Whenever you encounter monster (giant rat) on the corridor, you should always try to kill the monster immediately. Never retreat back into the room. Example: Example: ----- ... ....##%##@r. ..< ---- ----- ... ....@r##%##. ..< ---- 5. Once you reach the second room on the map, you should explore the room fully to find the target stairs down, but whenever you encounter monster, you should always try to run to the direction of the corridor. Once in the second room, never return to the corridor or first room unless actively fleeing monster. Example: ---- 44 Preprint. Under review. ... .............. <.%.#%#####............@.. ... .............. ---- 6. In case you return to the first room (where you can see <), immediately find the corridor # again and go east as much as possible. For task name contains quest: Your goal: Your goal is to explore the level, fight monsters, and navigate rooms and mazes to ultimately reach the stairs down. Notes: 1. You should never cross }(lava) without necessary ability or equipment. 2. Always explore approachable areas (no need to cross the lava) well enough to find sufficient support. Below is an example: Map Example: -------------- @....}......--- .(...}.......F- .....}..................... .....}........-------- .....}......--- -------------- 3. You should always try to apply the tool you can get, even if you do not know its effect. 4. You should always try to apply your roles ability, even if you do not know its effect. For task name contains boxoban: Your goal: You are playing Boxoban, box-pushing game inspired by Sokoban. Your goal is to push the boulders onto the fountains on the map. You can push the boulders by walking into them, as long as there are no obstacles behind them. Notes: For this task, follow these rules in priority order: 1. Remember the locations of the fountains at the start. 2. Never push boulder if the next spot is not empty or not fountain. 3. Never push boulder into an L-shaped wall configuration (two adjacent walls). Example (dead corner): ########## #`.....### Visual Deadlock Examples (NEVER DO THESE): Case 1: Before Push: ########## ########## #<######## #.######## #.######## #..{.##### #@.``##### #`{.`{.### #......### ########## 45 Action: Push South After Push: Preprint. Under review. ########## ########## #<######## #.######## #.######## #..{.##### #..``##### #@{.`{.### #`.....### ########## Result: Boulder stuck cant move due to walls. Case 2: Before Push: Action: Push WEST After Push: Result: Cant move boulder any more. More Thorough Examples: Example 1 Dead Corner: Before: After pushing west: ########## ####...{`# #..#.`@..# #.{.#.#.<# #.``..##.# ##...{##.# ########.# #####...{# ######...# ########## ########## ####...{`# #..#`@...# #.{.#.#.<# #.``..##.# ##...{##.# ########.# #####...{# ######...# ########## ########## ####...{`# #..#.`@..# #.{.#.#.<# #.``..##.# ##...{##.# ########.# #####...{# ######...# ########## ########## ####...{`# #..#`@...# #.{.#.#.<# #.``..##.# ##...{##.# ########.# 46 Preprint. Under review. Example 2 Dead Corner: Before: After pushing south: Example 3 Dead Corner: Before: After pushing north: Example 4 Dead Corner: Before: #####...{# ######...# ########## ########## ########## #<######## #.######## #.######## #..{.##### #@.``##### #`{.`{.### #......### ########## ########## ########## #<######## #.######## #.######## #..{.##### #..``##### #@{.`{.### #`.....### ########## ########## ####...{`# #..#`....# #.@.#.#.<# #.``..##.# ##...{##.# ########.# #####...{# ######...# ########## ########## ####...{`# #..#`....# #...#.#.<# #.@`..##.# ##`..{##.# ########.# #####...{# ######...# ########## ########## ####...{`# #..{`....# #.{.#.#.<# #..`..##.# ##.@`.##.# 47 Preprint. Under review. After pushing east: ########.# #####...{# ######...# ########## ########## ####...{`# #..{`....# #.{.#.#.<# #..`..##.# ##..@`##.# ########.# #####...{# ######...# ########## Terminal State Check: Remember all fountain locations initially. After each push, if fountain spot is covered by boulder, mark that spot terminated. Example 1 Termination: Before: After pushing north: Example 2 Termination: Before: After pushing west: ########## ####...{`# #..#`....# #.{.#.#.<# #.``..##.# ##@..{##.# ########.# #####...{# ######...# ########## ########## ####...{`# #..#`....# #.`.#.#.<# #.@`..##.# ##...{##.# ########.# #####...{# ######...# ########## ########## ####...{`# #..{`@...# #...#.#.<# #.``..##.# ##...{##.# ########.# #####...{# ######...# ########## ########## ####...{`# 48 Preprint. Under review. #..`@....# #...#.#.<# #.``..##.# ##...{##.# ########.# #####...{# ######...# ########## 4. You should never push boulder next to wall unless, in the perpendicular direction there remains an open fountain with all intermediate spots empty. 5. You should never push boulder if it is already on fountain. 6. If boulder and fountain are aligned in the same row or column, all intermediate spots empty, and the spot behind the boulder (opposite the fountain) is empty, push toward the fountain. Map Example (established path): ########## #......<## #.#@.{`.## #.``##.#.# #..`...{.# #.#.{....# #######.{# #######..# #######..# ########## 7. Always explore to create more paths. 8. If boulder is in dead corner initially, do not waste steps pushing it. Example (dead corner reminder): ########## #`.....### Below are two quick dead-corner examples: Example 1: Example 2: <-- Boulder in dead corner ########## ####...{`# #..#`@...# #.{.#.#.<# #.``..##.# ##...{##.# ########.# #####...{# ######...# ########## ########## ########## #<######## #.######## #.######## #..{.##### #..``##### #@{.`{.### #`.....### <-- Boulder in dead corner Finally, successful trajectory: 49 Preprint. Under review. Example 1 Beginning: move east: move north: move east: move east: ########## #...{..{.# ##.`.{.`.# ####.##### ####....{# ###.`.`..# ###.@.#..# ####<##### ########## ########## ########## #...{..{.# ##.`.{.`.# ####.##### ####....{# ###.`.`..# ###..@#..# ####<##### ########## ########## ########## #...{..{.# ##.`.{.`.# ####.##### ####....{# ###.`@`..# ###...#..# ####<##### ########## ########## ########## #...{..{.# ##.`.{.`.# ####.##### ####....{# ###.`.@`.# ###...#..# ####<##### ########## ########## ########## #...{..{.# ##.`.{.`.# ####.##### ####....{# ###.`..@`# ###...#..# ####<##### ########## ########## 50 Preprint. Under review. move south: move east: move north: move west: move west: ########## #...{..{.# ##.`.{.`.# ####.##### ####....{# ###.`...`# ###...#@.# ####<##### ########## ########## ########## #...{..{.# ##.`.{.`.# ####.##### ####....{# ###.`...`# ###...#.@# ####<##### ########## ########## <----target this fountain ########## #...{..{.# ##.`.{.`.# ####.##### ####....`# <----boulder now on fountain ###.`...@# ###...#..# ####<##### ########## ########## ########## #...{..{.# ##.`.{.`.# ####.##### ####....`# ###.`..@.# ###...#..# ####<##### ########## ########## ########## #...{..{.# ##.`.{.`.# ####.##### ####....`# ###.`.@..# ###...#..# ####<##### ########## ########## Preprint. Under review. move west: move south: move west: move north: move north: ########## #...{..{.# ##.`.{.`.# ####.##### ####....`# ###.`@...# ###...#..# ####<##### ########## ########## ########## #...{..{.# ##.`.{.`.# ####.##### ####....`# ###.`....# ###..@#..# ####<##### ########## ########## ########## #...{..{.# ##.`.{.`.# ####.##### ####....`# ###.`....# ###.@.#..# ####<##### ########## ########## ########## #...{..{.# ##.`.{.`.# ####.##### ####`...`# ###.@....# ###...#..# ####<##### ########## ########## ########## #...{..{.# ##.`.{.`.# ####`##### ####@...`# ###......# ###...#..# ####<##### ########## ########## Preprint. Under review. move north: move north: move east: move north: move east: ########## #...{..{.# ##.``{.`.# ####@##### ####....`# ###......# ###...#..# ####<##### ########## ########## ########## #...`..{.# ##.`@{.`.# ####.##### ####....`# ###......# ###...#..# ####<##### ########## ########## ########## #...`..{.# ##.`.@.`.# ####.##### ####....`# ###......# ###...#..# ####<##### ########## ########## ########## #...`@.{.# ##.`.{.`.# ####.##### ####....`# ###......# ###...#..# ####<##### ########## ########## ########## #...`.@{.# ##.`.{.`.# ####.##### ####....`# ###......# ###...#..# ####<##### ########## ########## Preprint. Under review. move east: move east: move south: move west: move west: ########## #...`..@.# ##.`.{.`.# ####.##### ####....`# ###......# ###...#..# ####<##### ########## ########## ########## #...`..{@# ##.`.{.`.# ####.##### ####....`# ###......# ###...#..# ####<##### ########## ########## ########## #...`..{.# ##.`.{.`@# ####.##### ####....`# ###......# ###...#..# ####<##### ########## ########## ########## #...`..{.# ##.`.{`@.# ####.##### ####....`# ###......# ###...#..# ####<##### ########## ########## ########## #...`..{.# ##.`.`@..# ####.##### ####....`# ###......# ###...#..# ####<##### ########## ########## <--- now boulder on fountain Preprint. Under review. Now we have successfully pushed three boulders to the fountains. For task name contains mazewalk: Your goal: Your goal is to explore the level and reach the stairs down. Notes: For this task, your action space is limited to: {\"north\": \"move north\", \"east\": \"move east\", \"south\": \"move south\", \"west\": \"move west\"} You must only choose these four directions: 1. If the block on your east is empty, move east. 2. If east is blocked and north is open, move north. 3. If east and north are both blocked, and west is open, move west. 4. If east, north, and west are blocked, move south. Absolute Wall-Following Protocol: Core Principle: Follow priority order, probe dark directions ONLY when theyre current priority. Movement Validation: Priority rotation ONLY occurs AFTER SUCCESSFUL MOVEMENT. Wall collisions preserve current priority order. 1. Initialization: CURRENT_PRIORITY = [EAST, NORTH, WEST, SOUTH]; permanent_walls = {}. Move east until first wall. 2. At each position, check directions in CURRENT_PRIORITY: If visible (. or <), move immediately and rotate per rules. If dark, attempt move: Success: continue protocol. Wall collision: mark permanent_walls, skip direction, continue. 3. Rotation rules: (a) 1st priority move: rotate clockwise (e.g. ENWS SENW). (b) 2nd priority: no rotation. (c) 3rd priority: rotate counter-clockwise (ENWS NWSE). (d) 4th priority: reverse (swap 13, 24). 4. Terminal Condition: immediately go to stairs down (\">\") when visible; never treat \"<\" specially. Walkthrough Example: Initial: [E,N,W,S] 1. Move until wall. 2. At start: (wall) (open) move north (no rotation). 3. Next: (open) move east (rotate to SENW). Otherwise: Your goal: Your goal is to get as far as possible in the game. In moment will present history of actions and observations from the game. Tip: there is no point in outputting the same action over and over if nothing changes. PLAY! 55 Preprint. Under review."
        },
        {
            "title": "Description",
            "content": "0 0 0 0 0 1"
        },
        {
            "title": "Combat Engagement and Survival",
            "content": "Role-Specific Ability Utilization"
        },
        {
            "title": "Giant Rats Encounter Handling",
            "content": "Table 9: Metrics and Turn of Induction for MiniHack"
        },
        {
            "title": "N Qualitative Observations of MiniHack Agent Performance",
            "content": "The induced metrics and the agents per-task performance are shown in Table 9 and Table 8, respectively. substantial improvement in the agents task completion performance is observed from Iteration 1 to Iteration 2, with the agent achieving both higher environment score and trajectory performance on the held-out tasks compared to the baseline agent. Specifically, the agents performance on metrics increased correspondingly to code changes, like to 41.67%, demonstrating the utility of AutoLibra for fine-grained agent improvement. This is discussed in more detail in the following sections. improving from 16.67% N.1 Extracted Metrics and Improvements The metrics induced by AutoLibra  (Table 9)  capture the behavior of the agent through all iterations, with coverage of 83% at Iteration 0 to 88% at Iteration 2. Due to the diversity of tasks, metrics pertain to one or two tasks, matching the expectation that AutoLibra should generate fine-grained metrics. Notably, the metrics generated at Iteration 0 are nearly comprehensive as they demonstrate good coverage of all four MiniHack tasks, while the later metrics were found to specifically describe detailed behaviors for targeting one task each. Code changes were selected to specifically target improvements in given metrics, and as seen in Table 8, the agents performance on the targeted metric improved significantly in the iteration following the code change. This demonstrates the utility of AutoLibra for fine-grained agent improvement, as well as the human interpretability of the induced metrics. On the other hand, some metrics remain unchanged over iterations. The unchanged metrics are . These metrics are all tightly related to Boxoban or Quest environments. As these two environments contain high level of randomness, interactions with system, and sophisticated planning, our prompt and example based guidance do not provide obvious improvements. This result also indicates that simply coding might not be sufficient if the gap between the agents capability and the tasks complexity is too significant. , and , , Iteration 0 The agents behavior is stochastic and repetitive, with no utilization of memory, planning, and goal awareness. Although the agent has reasoning before taking action, the decided action shows no correlation with the goal or the environment. Target Navigation Effectiveness identified as the core metric that evaluates the performance of all tasks except Boxoban. As all three other tasks require the agent to explore the target exit stairs, serves as the fundamental test of the agents goal awareness for these tasks. Since Boxoban has different winning condition, Boulder Manipulation Strategy induced in this iteration targets describing the overall goal awareness for was 56 Preprint. Under review. the agent in Boxoban environment. The remaining four metrics each cover two to three tasks on more detailed level. Based on the metrics induced in Iteration 0, several changes to the agent code were implemented: Augmentation of subtask-level behavioral restriction guidance with the single-shot example, targeting improvement of Boulder Manipulation Strategy Survival , and Hazard Awareness and Equipment Utilization . , Combat Engagement and Meta-prompting by providing subtask instructions targeting improvement of Role-Specific Ability Utilization . Augmentation of subtask-level winning strategy with few-shot example, targeting improvement of Target Navigation Effectiveness Map Memory Utilization . and Efficient Exploration and Iteration 1 The changes made after Iteration 0 did not yield substantial improvements in agent task completion performance, but behavior changes matching the metrics were observed and insights on prompt improvements were obtained. decrease in performance of 8.3% was observed between Iteration 0 and Iteration 1, indicating that the code changes confuse the agent which reduces their . Three causes of these capability of goal recognition, and the same reduction is also observed for reductions are recognized. Firstly, the overly simplified strategy for MazeWalk provides no positive intuitions for the agent when loops exist in the map. Secondly, the seemingly straightforward behavioral restrictions with examples do not appear sufficient enough for the agent to avoid false action as the agent failed to comprehend the map thorough enough. Thirdly, the randomness of Quest is so high that existing guidance fails to cover new randomly generated situations. Based on Iteration 1s result, MazeWalk and Corridor Fight both appear solvable given more optimal strategy, and Quest and Boxoban, given their high randomness and complexity, do not appear easily solvable, so providing more examples seem to be the only direction of improvement. Moreover, two new metrics are induced, one focusing on Corridor Fight and the other focusing on Quest. Based on the metrics induced in Iteration 1, new changes are listed below: Augmentation of existing subtask-level advanced strategy with step-by-step decision-making instructions and few-shot examples, targeting improvement of Target Navigation Effectiveness and Efficient Exploration and Map Memory Utilization . Augmentation of existing subtask-level strategy with corner cases handling guidance and few-shot examples, targeting improvement of Combat Engagement and Survival Awareness and Equipment Utilization , Hazard , and Spatial Awareness and Interpretation . Augmentation of existing few-shot examples with full trajectory of the complete winning task, targeting improvement of Boulder Manipulation Strategy . Meta-prompting by providing more in-depth subtask instructions, targeting improvement of Role-Specific Ability Utilization and Object Pickup Efficiency . and and indicating the code changes successfully Iteration 2 We observe significant increase in improve agents ability in goal awareness and goal reaching efficiency. Based on task-level observation, the agent succeeded in most MazeWalk and Corridor Fight tasks. The large increase in shows the agents strong performance in the Corridor Fight task. More specifically, the agent now demonstrates both the capability of choosing optimal action based on its current location and the capability of strategically handling dangerous enemiesgiant ratsin an emergency. The new induced metric also further proves the agents rat-handling ability. For instance, when the agent encounters multiple rats at the same time, it will immediately retreat to the middle of the single-way corridor to engage rats in combat individually. By doing so, the agent can easily beat rats one by one and navigate to the goal without danger. However, as all the remaining metrics show no improvements, we also observe the agents poor performance on Boxoban and Quest. This result shows that our current targeting improvement strategy of adding abundant few-shot examples does not seem effective given the gap between the agents reasoning ability and the high randomness and complexity of Boxoban and Quest. For example, despite more than ten examples of pushing boulder to dead corner, prohibited behavior has been presented, the agent will still randomly push boulder to the L-shaped dead corner formed by two walls which causes this boulder to become immovable. Preprint. Under review. N.2 Held-Out Task Performance For the held-out tasks, the agent task completion rate improved from Iteration 0 to Iteration 2, with scores from 0% to 25%. This result further proves that the improvements realized by AutoLibra are generalizable to unseen tasks. However, some failed cases also provide valuable insights into prospective future iterations. For instance, the complete set of Corridor tasks consists of scenarios where the target downstairs may be located west of the agents starting point, unlike Corridor Fight, where the target always lies to the east. Consequently, an instruction that rigidly directs the agent to explore only eastward fails to generalize to the broader Corridor tasks. This outcome underscores the importance of crafting instructions that are specific yet flexible: they should provide sufficiently detailed, high-level strategies without relying on overly prescriptive, task-specific steps. By focusing on generalizable principles rather than rigid directions, the agent is better equipped to adapt to wider range of scenarios while still benefiting from structured guidance. The agents performance on the held-out tasks is shown in Table 7. NNetNav-Live Induced Metrics It. Metric 0 0 0 0 0 1 1 2 2 Navigation Accuracy Search Term Accuracy Information Extraction Details Task Goal Achievement Trajectory Efficiency Barrier Avoidance Navigation Loop Resolution Subtask Coordination UI Interaction Accuracy Map Search Efficiency Wish Cart Accuracy 3 News and Financial Information Retrieval Table 10: Metrics for WebVoyager in Fig. 8 from bottom to top."
        }
    ],
    "affiliations": [
        "stanford.edu",
        "upenn.edu",
        "utoronto.ca"
    ]
}