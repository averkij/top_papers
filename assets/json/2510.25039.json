{
    "paper_title": "Automating Benchmark Design",
    "authors": [
        "Amanda Dsouza",
        "Harit Vishwakarma",
        "Zhengyang Qi",
        "Justin Bauer",
        "Derek Pham",
        "Thomas Walshe",
        "Armin Parchami",
        "Frederic Sala",
        "Paroma Varma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid progress and widespread deployment of LLMs and LLM-powered agents has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are the primary tool for assessing model capabilities, but these quickly become saturated. In contrast, dynamic benchmarks evolve alongside the models they evaluate, but are expensive to create and continuously update. To address these challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a framework that leverages environment design principles to automate the process of dynamic benchmark design. BeTaL works by parameterizing key design choices in base benchmark templates and uses LLMs to reason through the resulting parameter space to obtain target properties (such as difficulty and realism) in a cost-efficient manner. We validate this approach on its ability to create benchmarks with desired difficulty levels. Using BeTaL, we create two new benchmarks and extend a popular agentic benchmark $\\tau$-bench. Extensive evaluation on these three tasks and multiple target difficulty levels shows that BeTaL produces benchmarks much closer to the desired difficulty, with average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the baselines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 9 3 0 5 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Automating Benchmark Design",
            "content": "Amanda Dsouza 1 Derek Pham 1 Harit Vishwakarma 1 Zhengyang Qi 1 Justin Bauer 1 Thomas Walshe 1 Armin Parchami Frederic Sala 1,2 Paroma Varma 1 1Snorkel AI, Redwood City, CA, USA. 2Dept. of Computer Sciences, University of Wisconsin-Madison, WI, USA. Abstract The rapid progress and widespread deployment of LLMs and LLM-powered agents has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are the primary tool for assessing model capabilities, but these quickly become saturated. In contrast, dynamic benchmarks evolve alongside the models they evaluate, but are expensive to create and continuously update. To address these challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), framework that leverages environment design principles to automate the process of dynamic benchmark design. BeTaL works by parameterizing key design choices in base benchmark templates and uses LLMs to reason through the resulting parameter space to obtain target properties (such as difficulty and realism) in cost-efficient manner. We validate this approach on its ability to create benchmarks with desired difficulty levels. Using BeTaL, we create two new benchmarks and extend popular agentic benchmark τ -bench. Extensive evaluation on these three tasks and multiple target difficulty levels shows that BeTaL produces benchmarks much closer to the desired difficulty, with average deviations ranging from 5.3% to 13.2% 2-4 improvement over the baselines."
        },
        {
            "title": "Introduction",
            "content": "New developments in LLMs, particularly in powering agents via advanced planning, reasoning, and tool-use capabilities [1, 2, 3], have outpaced current methods for evaluation. Static, human-curated benchmarks, such as GPQA [4] or HLE [5], remain popular, but are costly to develop and quickly become obsolete as models continue to improve. This is challenging for model developers, as increasingly saturated benchmarks make it impossible to differentiate between the performance of state-of-the-art models. To address these challenges, researchers have turned to dynamic benchmarks that can be updated over time. These benchmarks avoid saturation via re-calibration or the introduction of new and harder data; this also limits the risk of train-test contamination. For example, LiveBench [6] periodically introduces new questions and harder tasks. However, these types of benchmarks still largely rely on unscalable human authoring and manual updates. Increasingly popular agentic tasks exacerbate this problem, as simulated environments must be carefully crafted; repeatedly designing and implementing new environments promises to be even more labor-intensive. How can we build dynamic benchmarks for frontier LLMs without the expense and inefficiency of ongoing manual design and implementation? Unsupervised environment design (UED) methods [7] work with environments that are built from abstract task templates with set of configurable parameters. These parameters can be tuned to produce new and higher utility versions of the benchmark, thus enabling dynamic re-use. In practice, however, we find that the search space over such parameters is intractable for non-trivial environments. Naively sampling random configurations is inefficient, as many will be trivial or unsolvable. We overcome these obstacles via new approach, Benchmark Tuning with an LLM-in-the-loop (BeTaL), that performs dynamic benchmark design. BeTaL leverages the capabilities of large reasoning models playing the role of designers. Central to our approach is the use of powerful designer LLM tasked with reasoning over the space of possible parameter values, design choices, or tasks. The designer is prompted to consider the various parameters of an under-specified benchmark or environment and to propose instances 1 Automating Benchmark Design Figure 1: BeTaL automates the process of designing and adjusting dynamic benchmarks to meet target criteria. or values that are expected to be high utility. This is set up as an interactive and iterative process: after the designer has specified an environment, simulator creates sample benchmark (problems with ground truth answers), and the model or agent being evaluated attempts this benchmark, with results provided back to the designer. After each round, the designer must reason over choices and results and make changes in the parameter values so that the new parameters will result in benchmark with desired objectives (such as difficulty or realism). This closed-loop multi-round strategy allows the benchmark to dynamically adjust over time to meet the objectives. While the procedure is flexible to incorporate several types of objectives and combinations thereof, here we focus on the objective of creating benchmark with given target difficulty level. We hypothesize that the strong zero-shot or few-shot reasoning capabilities of frontier models enable the designer to understand the factors that influence usefulness (e.g., task difficulty) and design benchmarks that meet all desirable criteria (e.g., tasks that are just outside of weaker models current capabilities). This framework design reduces the burden of designing and continually updating benchmarks to meet the demands of ever-improving models. In addition, it permits re-purposing of existing static benchmarks - breathing new life into datasets long considered outdated. Our contributions are: flexible framework for automating benchmark design. We posit that benchmark properties such as complexity are determined by set of underlying benchmark parameters. With this insight, we formulate the design process as an optimization problem over the space of benchmark parameters to obtain settings that will result in benchmark having desired properties, e.g., difficulty level. An efficient LLM-based procedure to solve the optimization. Leveraging the reasoning capabilities of frontier large language models, we introduce Benchmark Tuning with an LLM-in-theloop (BeTaL) to efficiently solve the above optimization problem for benchmark design. New benchmarks and empirical validation: Using BeTaL, we modify existing benchmarks to meet new requirements for dataset-level difficulty, and we introduce new benchmarks that focus on mathematical and spatial reasoning. Our extensive empirical evaluation of BeTaL on these settings reveals BeTaL consistently obtains benchmarks with low deviation (5.3% - 13.2%) between observed and target difficulty 2-4 improvement over baselines across all tasks."
        },
        {
            "title": "2 Methodology",
            "content": "We propose BeTaL, novel framework that uses an LLM-in-the-loop to iteratively design dynamic benchmarks that achieve user-specified goals. Before describing the algorithm, we outline the key building blocks of the system. 2 Automating Benchmark Design Algorithm 1 Benchmark Tuning with an LLM-in-the-loop (BeTaL) 1: Input: Under-specified Environment Description, Parameter Set , Target Performance ρ, Target Model Mt, Designer Model Md, Number of Iterations I. Prompt Prompt + Summary of previous iterations Prompt Template with Environment description, , ρ if > 1 then 2: Initialize 0, vi , minimum gap ˆgi 3: for = 1 to do 4: 5: 6: 7: 8: 9: end if vi MD(Prompt) vi ProjectToDomain(vi, V) Di InstantiateSimulator(vi) ˆρi EvaluateModel(MT , Di) ˆgi ˆρi ρ Update summary of previous iterations with vi and ˆρi 10: 11: 12: 13: 14: iteration if ˆgi < ˆgi then ˆgi ˆgi 15: 16: 17: 18: end for 19: Return: vi end if Get parameters from Designer Model Generate problems with simulator Evaluate Target Model Step 4: Prepare feedback for next"
        },
        {
            "title": "2.1 Preliminaries",
            "content": "Our approach assumes loosely defined environment template that can be refined and instantiated into concrete benchmarks. The system consists of the following components: Underspecified environment. The user begins with high-level description of the benchmark they want to createfor example, spatial reasoning benchmark where questions involve tracking objects on grid after sequence of transformations. Intuitively, the complexity of problems depends on several factors such as grid size, number of actions, types of operations, etc. We begin with an underspecified environment, wherein the environment is characterized by finite set controllable parameters = {p1, p2, . . . , pk}, pi Vi, so that the overall design space is = V1 V2 . . . Vk. Problem/Task Generator. We assume access to simulator that, given parameter configuration , can instantiate the environment and generate dataset = {(xj, yj)} of problems with groundtruth solutions. It is expected that the simulated problems adhere to the constraints specified by the parameter values. In this work, we focus on environments with verifiable or procedurally generated solutions, allowing us to assume that the generated ground truth is correct. Target model. model or system to be evaluated, e.g., an off-the-shelf LLM, proprietary API, or multi-agent pipeline. Target performance. Along with the target model, the user also specifies target performance level ρ and distance measure d. The objective is to output benchmark on which the target models performance will be close to ρ. The exact definition of ρ is left to the user; for instance ρ could be accuracy, diversity, or an aggregate of multiple measures. In this work, we use target difficulty as our measure of performance of the generated benchmarks, as we seek to overcome the challenge of benchmark saturation. Designer model. sufficiently powerful model, such as large reasoning models (LRMs), that can understand the underspecified environment description, the set of free parameters and constraints that influence the environments complexity. We expect such model to be able to reason about the design space and propose specific values to the parameters that result in an environment of given target complexity. 3 Automating Benchmark Design"
        },
        {
            "title": "2.2 BeTaL: Benchmark Tuning with LLM-in-the-loop",
            "content": "BeTaL is built on two key ideas: first, strengthening grounding through explicit feedback from real rollouts of the designed benchmarks; and second, leveraging LLM reasoning to systematically explore and refine the design space. This process mirrors how humans design benchmarks; through an iterative loop of experimentation and observation, where both elements are essential for effective benchmark creation. We describe the process in Alg. 1, and explain it in detail below. Step 1: Parameter generation (LLM-Guided). In step one of the BeTaL, the designer model, an LRM, is prompted to obtain parameter configuration vi. Since these values are generated by language model, it is possible that they may be out of the domain V. Verification is therefore necessary to ascertain that vi V, and, if not, this process is repeated until the generated vi falls in V. In the end, vi is projected to if it is still out-of-domain. Step 2: Environment instantiation and problem/task generation. simulator is instantiated with the parameter configuration obtained in Step 1, which is then used to generate small set of problems/tasks, with ground truth answers for evaluation, i.e. Di = {(xj, yj)}ns j=1. Step 3: Performance evaluation. The target model is evaluated on Di to yield performance ˆρi. When the ground truth is not available, ˆρi could be estimated by evaluating using LLM-as-a-Judge [8] or Program-as-a-Judge [9]. Step 4: Feedback and iteration. The iteration details, including the parameter choices and the resulting performance, are summarized in natural language to the LRM, including vi and ˆρi. This feedback is appended to the next prompt, enabling the model to reason about the impact of its prior choices and propose improved parameters in subsequent iterations. Step 5: Termination and selection. In each iteration, we keep track of the observed performance gap ˆgi = ˆρi ρ and keep track of the iteration that results in the smallest gap. After iterations, the method exits and returns vi ."
        },
        {
            "title": "3 Experimental Setup",
            "content": "In this section, we describe our setup for the experiments. First, we give high-level details of the benchmarking tasks, then discuss the baseline methods, our choices of designer and target models, evaluation metrics, and the protocol to run the experiments."
        },
        {
            "title": "3.1 Benchmarking Tasks",
            "content": "We consider range of tasks based on arithmetic, spatial reasoning, and airline customer service agents. Each of these settings has rich design space with several free parameters that govern the complexity of the benchmark, making them good candidates for evaluating our method. We briefly discuss these tasks and defer the details to the Appendix A.1. Arithmetic sequences task. Given an input number and an output number R, an agent must return the sequence of arithmetic operations o1, o2 . . . oN that, when applied recursively to the intermediate results, yield := (oN oN 1 o1)(x). At inference time, the target model, an LLM agent, is provided access to the arithmetic operators, as tools, to determine the sequence of operators that transform to y. The predicted operator sequence is verified by executing the sequence and comparing it with the ground truth y. Task difficulty depends on several factors such as operator choice, sequence length, range of the input x, and others. Spatial reasoning task. We design multiple spatial reasoning tasks involving 2D square grid (board) with particles placed on it. The board and particles can both rotate, while the particles can additionally move positions. series of such actions is applied, after which the model is queried about the final positions and orientations of the particles. The target LLM receives description of the environment and 4 Automating Benchmark Design action sequence, and its responses are compared against programmatically computed ground truth. The complexity is controlled by parameters such as board size, the number and types of actions allowed. τ -bench airline task. This is an interactive evaluation environment for customer service agents in simulated airline scenarios, where the agent must use available tools to query and update database to fulfill user requests [10]. The reward is computed by comparing the final database state with the database state following series of golden actions. Building on this setup, we design rule-based task generator that randomly samples action sequences and corresponding user instructions. The generator is parameterized both by tool-related variablessuch as the number of passengers when booking flightand by behavioral parameters derived from real user instructions. On all three problems, our objective is to identify parameter configurations that yield benchmarks with desired difficulty levels. For further details on these tasks and associated parameters, see Appendix A.1."
        },
        {
            "title": "3.2 Baselines",
            "content": "We briefly discuss the baselines for evaluation. Details are provided in Appendix A.2. Random sampling with prioritized parameter replay (RS+PPR). Inspired by Prioritized Level Replay (PLR) [11], we develop baseline RS+PPR, that maintains buffer of favorable environment parameters. In each iteration, it samples parameter configuration vi either uniformly at random (with probability p) or, with probability 1 p, as noisy variant of parameters drawn from the buffer. Then the performance gap ˆgi is estimated with vi, and it is added to the buffer if ˆgi . Best-of-N variations. We use best-of-N (BoN) [12, 13], where responses are sampled and the best response selected according to reward model. We consider the reward for parameter configuration to be the negative of its observed performance gap. In the first variant, we consider BoN-ML, with our verifier as predictive model trained offline using standard machine learning methods on parameter performance-gap pairs. In the second variant, BoN-TM, we collect small number of rollouts with the target model, and select the response with the smallest measured performance gap."
        },
        {
            "title": "3.3 Designer and Target Models",
            "content": "We use the latest reasoning models: GPT-5, Claude Opus 4.1, and Grok 4 as designer models and o4-mini as the target model in all settings. We evaluate the resulting benchmarks on three models: o4-mini, Gemini 2.5 Flash, and Claude 3.7 Sonnet. Whenever applicable, we configure the designer model with temperature 0.5 and reasoning budget of 4096 tokens for exploration, while the other models use temperature 0.0 with reasoning budget of 1024 tokens. Details of model configurations are in Appendix A.3."
        },
        {
            "title": "3.4 Metrics",
            "content": "Each benchmarking task can have its own notion of performance ρ (e.g., accuracy, pass@k, etc.). We assume this measure is inversely proportional to the task difficulty, and define the following metric: Performance gap. If method is run with given target performance level ρ, and say that it results in benchmark on which the target model has performance ˆρ, then its performance gap is ˆg = ˆρ ρ."
        },
        {
            "title": "3.5 Experiment Protocol",
            "content": "We evaluate the methods across two phases: parameter search and evaluation. During parameter search, iterative methods are run for 10 iterations, while non-iterative methods sample 10 configurations. The best parameters obtained from each method are then used to generate larger evaluation dataset. To assess each designers ability to produce benchmarks with controlled difficulty, we define four target performance levels: Hard (ρhard = 0.25), Medium (ρmedium = 0.50), Easy (ρeasy = 0.75), and Trivial (ρtrivial = 0.90). The primary evaluation metric is the average performance gap, ˆg, computed at each level. All experiments are repeated three times with different random seeds, and results are reported with 95% confidence intervals based on the Students-t distribution with three degrees of freedom. 5 Automating Benchmark Design Table 1: BeTaL consistently outperforms the iterative and Best-of-N baselines in both parameter search and evaluation phases across all three tasks and all three designer models. Reported numbers are ˆg(%) with o4-mini as the target model. For parameter search, we run either 10 samples or 10 iterations and report the best result for fair comparison. More experimental details can be found in Appendix A. Designer Method Arith. Seq. Spatial Reasoning τ -Bench Airline Param Search Eval Param Search Eval Param Search Eval N/A RS+PPR 15.82.43 13.11 11.6 6.612. 8.3610.45 18.321 21.310.6 GPT-5 BoN-TM BoN-ML BeTaL 8.34.64 11.677.67 28.3441.19 30.2642.77 12.52. 20.88.0 30.012.63 22.17 17.99 21.6619.67 31.2043.49 21.411. 16.710.4 5.84.77 9.0 8.49 0.40.35 5.3412.77 5.33. 13.210.3 BoN-TM 20.012.12 18.94 18.72 26.9341.32 31.0743.27 3.63. 10.012.4 Opus-4.1 BoN-ML BeTaL 31.76.20 29.17 6. 20.4919.13 32.7643.76 11.77.5 9.77.6 12.54.42 11.7810. 3.825.58 7.355.49 5.02.1 7.75.2 BoN-TM 20.011.70 21.4411. 25.3639.60 29.7643.64 15.011.5 18.57.7 GROK 4 BoN-ML BeTaL 32.515.58 33.1120.22 21.2419.44 33.8146.05 34.214. 20.23.1 4.23.26 8.28 4.30 1.362.72 4.988.13 3.93. 10.312.4 Table 2: Chain-of-thought (CoT) prompting does not consistently yield strong designer-model performance. While Claude Opus-4.1 achieves competitive results on the arithmetic sequence and τ -Bench tasks, state-of-the-art LLMs often struggle to outperform random sampling baseline. Reported values are ˆg(%) with o4-mini as the target model. Method Arith. Seq. Spatial Reasoning τ -Bench Airline Random Sampling 21.1751.5 CoT Prompting (GPT-5) 28.3325.8 CoT Prompting (Opus-4.1) 11.677. CoT Prompting (Grok-4) 20.833.6 25.4 9.6 45.3 26.3 26.1 17.9 39.1 25. 37.317.2 23.616.1 11.910.4 31.913."
        },
        {
            "title": "4 Results and Discussion",
            "content": "In this section, we present our main results and discussion. We provide an in-depth discussion on BeTaLs effectiveness in designing benchmarks for any given target difficulty. C1: BeTaL outperforms baselines in creating benchmarks with any target performance level. Our hypothesis is that while LLMs are highly capable, single round of prompting, even with large reasoning budget, is less effective than an iterative framework like BeTaL, which incorporates feedback from previous rounds. Drawing inspiration from recent work framing LLMs as optimizers [14, 15], we expect BeTaLs feedback-driven search to yield stronger performance than non-iterative baselines. The results in Table 1 strongly support this hypothesis. We summarize the key findings below. i) BeTaL versus other multi-round methods. We compare BeTaL with multi-round baselines, including RS+PPR and the variations of Best-of-N. From our results  (Table 1)  , it is evident that BeTaL outperforms these baselines by wide margin, across benchmarks and designer models. We attribute this advantage to the reasoning capacity of LLM-based designers, which enables them to iteratively refine parameters using feedback from previous rounds. In contrast, other baselines, including those that receive feedback, fail to exploit it as effectively. BeTaLs capabilities in iteratively finding the target parameters can be further seen in Figure 3 and Figure 10 in the Appendix. It shows that BeTaL shrinks the performance gap more strongly than RS+PPR over 10 iterations, with wide margin (more than 20%) on both τ -Bench and Spatial Reasoning. 6 Automating Benchmark Design Figure 2: Evaluation results on o4-mini with BeTaL (with GPT-5 as the designer model, and o4-mini as the target model during parameter search) perform robustly at different target difficulty levels, compared to baselines on Arithmetic Sequences, Spatial Reasoning, and τ -Bench. similar performance is noted using Claude Opus 4.1 and Grok-4 as Designers, in Figure 8 in the Appendix. ii) Performance at target difficulty levels. We expect an effective benchmark designer to optimize for any specified target difficulty level. Figure 2 presents the observed performance gap for each target difficulty level. BeTaL demonstrates strong robustness, consistently outperforming all baselines at each difficulty level. We also observe inherent difficulty differences across benchmark domains, which are reflected in the performance gaps. For example, τ -Bench and Spatial Reasoning are inherently challenging, with the largest gaps appearing at the Trivial difficulty level for all LLM designers. In contrast, the Arithmetic Sequence task, containing several degenerate solutions, shows the largest gap at the Hard difficulty level (see Figure 9 in the Appendix). Figure 3: Convergence of iterative methods during parameter selection on Spatial Reasoning and τ -Bench benchmarks: BeTaL vs. RS+PPR. Performance gap of BeTaL shrinks faster compared to RS+PPR, within 10 iterations, indicating LLMs are more efficient than competing iterative methods at finding favorable environment parameters for benchmark creation. Results are averaged over difficulty levels and designer models. iii) Performance comparison of designer models. While BeTaL achieves strong performance with all three designer (reasoning) models, we find that the choice of reasoning model may depend on the nature of the benchmark being developed. Comparing between the designers, Grok-4 and GPT-5 do well on the mathematical and logical reasoning domains of Arithmetic Sequences and Spatial Reasoning. On the other hand, Claude-Opus-4.1 excels on the real-world agentic benchmark of τ -Bench Airline, with performance gap of 7.7 5.2% compared to 13.2 10.3% and 10.3 12.4% by GPT-5 and Grok-4, respectively  (Table 2)  . C2: Benchmark created by BeTaL for one target model is transferable to other target models. benchmark designed for target model (here, o4-mini) can also be used to evaluate other models. When the target and evaluation models coincide, BeTaL produces benchmarks with minimal performance gaps. However, when evaluated on models different from the target, performance naturally varies with model capability. For instance, benchmark that is hard for the target model may appear of medium difficulty to stronger model, and vice versa. Consequently, models with similar capabilities to the target are expected to exhibit comparable performance gaps, whereas stronger (or weaker) models should follow the same performance trends across target difficulty levels but with larger (or smaller) magnitudes. 7 Automating Benchmark Design Figure 4: Evaluation generalization across designer models and datasets. Observed versus target accuracy for o4-mini target trained by different designers (columns: GPT-5, Grok-4, Opus-4.1) on three benchmarks (rows: Arithmetic Sequence, Spatial Reasoning, τ -Bench). The black dashed line indicates perfect alignment. Our results in Figure 4 and 5 and confirm that benchmarks designed by BeTaL exhibit robust transferability across evaluation models. On τ -Bench, benchmarks generated using o4-mini feedback yield comparable performance when evaluated on Claude 3.7 Sonnet and Gemini 2.5 Flash, with BeTaL consistently outperforming all baselines across evaluation models. This cross-model consistency across different benchmark domains: agentic planning in real-world tasks (τ -Bench) and mathematical reasoning (Arithmetic Sequences) domains provides strong evidence that BeTaL-designed environments test fundamental cognitive capabilities that generalize across different model architectures and families, rather than exploiting model-specific weaknesses. C3: Chain-of-Thought alone is insufficient for efficient benchmark design. Despite the remarkable reasoning capacity and extensive world knowledge of state-of-the-art LLMs, their ability to systematically design benchmarks, using prompting alone, remains unreliable. As shown in Table 2, even with high reasoning budgets, LLMs exhibit high variance when tasked with producing benchmarks of varying complexity. Using o4-mini as the target model, Claude Opus-4.1 surpasses the random baseline only on Arithmetic Sequence and τ -Bench, but fails on Spatial Reasoning. GPT-5 and GROK 4 underperform even further. These results demonstrate that Chain-of-Thought prompting alone does not endow LLMs with robust or generalizable benchmark design capabilities. C4. Can LLMs also generate better parameter spaces? Given LLMs strong ability to generate complex and diverse benchmarks through BeTaL, natural question is whether they can also design the underlying parameter spaces themselves. To test this, Automating Benchmark Design (a) Results averaged over the difficulty levels. (b) Results for BeTaL at different target difficulty levels. Figure 5: Results on different evaluation models. The left figure shows aggregate results for all methods, and the right figure focuses on BeTaLs results, showing the observed accuracies at different target difficulty levels. All results are averaged across Designer Models. Figure 6: Performance of BeTaL on τ -bench parameter space generated by Opus 4.1 versus by human. BeTaL on AI-generated parameter space is an acceptably small performance gap for medium and hard benchmarks, yet still generally underperforms to that generated by humans. we prompt Claude Opus-4.1, the best performing designer model on τ -Bench, to generate complete parameter space for τ -Bench, then manually implement the feasible parameters in the task generator. Opus 4.1 adds additional parameters based on user interactions to the design space including cooperation level, and clarifying preferences (whether explicit or implicit). Detailed parameters and prompts can be seen in Appendix A.1 and Appendix B. As shown in Figure 6, BeTaL applied to the AI-generated parameter space performs comparably well on Medium and Hard benchmarks, achieving ˆg as low as 1.1% and 1.7%, respectively. This demonstrates that LLMs can capture key structural patterns needed to produce challenging and well-calibrated benchmarks. However, substantial gap remains relative to human-designed parameter spaces on Trivial and Easy benchmarks, reaching up to 24.4% and 23.3% performance gaps for GPT-5 and Opus-4.1, compared to 15.6% and 13.3% from the human-generated space. These gaps indicate limited flexibility and controllability in the LLM-generated parameter space, particularly in achieving smooth difficulty scaling across the full range of target performances. Overall, these findings suggest that while current LLMs exhibit partial autonomy in environment design, achieving full self-sufficiency in parameter-space generation remains an open challenge for future systems. 9 Automating Benchmark Design"
        },
        {
            "title": "5 Related Work",
            "content": "Automating benchmark design. Recent work streamlines benchmark creation by automating generation, verification, and evolution. BENCHMAKER [16] and CHASE [17] leverage LLMs for systematic or compositional task construction, with BENCHMAKER emphasizing structured evaluation and CHASE building harder problems from simpler components. In the code domain, graph-based generators validate solutions via loop-derived self-consistency and help train reliable LLM-as-judge proxies [18]. Other approaches extend beyond static generation: tasks can evolve through perturbation, probing, or alternation [19], and multi-agent frameworks coordinate specialized roles for diverse benchmark creation [20]. Despite this progress, most methods operate directly at the task levelfixing difficulty or other heuristics to guide evolutionwithout abstracting the environment design space that underlies task instantiation. This makes it hard to adapt benchmarks across new domains. Our approach instead parameterizes the benchmark and closes the loop with target model feedback, enabling flexible benchmark tuning. Environment design for curriculum learning. Automated benchmark design parallels Unsupervised Environment Design (UED) in reinforcement learning, where tasks must remain solvable yet challenging as agents improve. UED methods adapt environments through adversarial generation [21], replay-based curation [11], or evolutionary mutation [22]. These approaches formalize environment design as optimization or curation to sustain adaptive curricula. Extending this idea, LLM-driven variants such as EnvGen [23] and LLM-POET [24] employ language models to generate or mutate RL environments, while co-evolutionary loops like R-Zero [25] pair Challenger and Solver in an adversarial, self-improving curriculum on language tasks. Although these methods share the goal of adapting difficulty in step with capability, BeTaL avoids the need for training loop, enabling adaptive benchmark generation with open and closed models alike. Scaling environments and datasets. complementary line of work scales environments and datasets to advance agentic intelligence, often through synthetic generation or curated annotations. AgentScaler [26] builds large collections of verifiable, API-derived environments to train function-calling agents, while APIGen [27] and ToolACE [28] synthesize diverse, verifiable function-calling datasets through automated generation and multi-stage verification. More recently, ARE and its Gaia2 benchmark [29] provide scalable, asynchronous environments that test adaptability and robustness. These efforts emphasize agentic capabilities, whereas our focus is on automating evaluation. LLMs as optimizers. Our work fundamentally treats benchmark design as an optimization problem, with reasoning models as optimizers. Similar work has been explored in OPRO [30] and evolutionary variants such as LEO [31] and [32] to solve mathematical tasks and optimize prompts. Our work uniquely applies to benchmark design."
        },
        {
            "title": "6 Conclusion, Limitations and Future Work",
            "content": "We introduced BeTaL, an LLM-in-the-loop framework for dynamic benchmark design. Unlike static or manually maintained live benchmarks, BeTaL adaptively generates benchmarks that evolve with model capabilities. By reasoning over parameterized design spaces, it efficiently achieves target performance levels with minimal human input. Across arithmetic, spatial reasoning, and agentic domains, BeTaL consistently reduces performance gaps by 2-4 compared LLM and non-LLM baselines. These results highlight BeTaLs potential to enable evaluation systems that evolve alongside advancing models. One of the drawbacks of BeTaL is that it assumes access to parameterized and verifiable task generators, which may not always exist. Its effectiveness depends on the reasoning strength of the designer model and careful prompt construction. Moreover, our evaluation is limited to small set of domains, leaving multimodal and more subjective tasks unexplored. Future work could extend BeTaL to optimize multiple objectives including realism and diversity, explore multi-agent or co-evolutionary design loops, and incorporate human-in-the-loop oversight to further enhance adaptability and reliability. Ultimately, we envision adaptive benchmarks that evolve with the systems they evaluate, ensuring robust and meaningful assessment as AI capabilities advance. 10 Automating Benchmark Design References [1] Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Advances in Neural Information Processing Systems, 36:3897538987, 2023. [2] Karthik Valmeekam, Kaya Stechly, and Subbarao Kambhampati. Llms still cant plan; can lrms? preliminary evaluation of openais o1 on planbench. arXiv preprint arXiv:2409.13373, 2024. [3] Mohamed Amine Ferrag, Norbert Tihanyi, and Merouane Debbah. From llm reasoning to autonomous ai agents: comprehensive review. arXiv preprint arXiv:2504.19678, 2025. [4] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [5] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. [6] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid ShwartzZiv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 4, 2024. [7] Minqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob Foerster, Edward Grefenstette, and Tim Rocktäschel. Replay-guided adversarial environment design. Advances in Neural Information Processing Systems, 34:18841897, 2021. [8] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. [9] Tzu-Heng Huang, Harit Vishwakarma, and Frederic Sala. Time to impeach llm-as-a-judge: Programs are the future of evaluation. arXiv preprint arXiv:2506.10403, 2025. [10] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains, 2024. [11] Minqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized level replay. arXiv preprint arXiv:2010.03934, 2021. [12] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. [13] Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander Nicholas DAmour, Jacob Eisenstein, Chirag Nagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment policy. In Forty-second International Conference on Machine Learning, 2025. [14] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, 2023. [15] Allen Nie, Ching-An Cheng, Andrey Kolobov, and Adith Swaminathan. The importance of directional feedback for llm-based optimizers. arXiv preprint arXiv:2405.16434, 2024. [16] Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, and Kan Li. Llm-powered benchmark factory: Reliable, generic, and efficient. arXiv preprint arXiv:2502.01683, 2025. [17] Arkil Patel, Siva Reddy, and Dzmitry Bahdanau. How to get your llm to generate challenging problems for evaluation. arXiv preprint arXiv:2502.14678, 2025. 11 Automating Benchmark Design [18] Eitan Farchi, Shmulik Froimovich, Rami Katan, and Orna Raz. Automatic generation of benchmarks and reliable llm judgment for code tasks. arXiv preprint arXiv:2410.21071, 2024. [19] Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei, and Xuanjing Huang. Benchmark selfevolving: multi-agent framework for dynamic llm evaluation. arXiv preprint arXiv:2402.11443, 2024. [20] Natasha Butt, Varun Chandrasekaran, Neel Joshi, Besmira Nushi, and Vidhisha Balachandran. Benchagents: Automated benchmark creation with agent interaction. arXiv preprint arXiv:2410.22584, 2024. [21] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. arXiv preprint arXiv:2012.02096, 2021. [22] Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, and Tim Rocktäschel. Evolving curricula with regret-based environment design. arXiv preprint arXiv:2203.01302, 2023. [23] Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, and Mohit Bansal. Envgen: Generating and adapting environments via llms for training embodied agents. arXiv preprint arXiv:2403.12014, 2024. [24] Fuma Aki, Riku Ikeda, Takumi Saito, Ciaran Regan, and Mizuki Oka. Llm-poet: Evolving complex environments using large language models. arXiv preprint arXiv:2406.04663, 2024. [25] Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004, 2025. [26] Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Towards general agentic intelligence via environment scaling. arXiv preprint arXiv:2509.13311, 2025. [27] Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles, Huan Wang, Shelby Heinecke, and Caiming Xiong. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. arXiv preprint arXiv:2406.18518, 2024. [28] Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, Zezhong Wang, Yuxian Wang, Wu Ning, Yutai Hou, Bin Wang, Chuhan Wu, Xinzhi Wang, Yong Liu, Yasheng Wang, Duyu Tang, Dandan Tu, Lifeng Shang, Xin Jiang, Ruiming Tang, Defu Lian, Qun Liu, and Enhong Chen. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920, 2025. [29] Pierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja, Ricardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, Jean-Baptiste Gaya, Hugo Laurençon, Maxime Lecanu, Kunal Malkan, Dheeraj Mekala, Pierre Ménard, Grégoire Mialon, and et al. Are: Scaling up agent environments and evaluations. arXiv preprint arXiv:2509.17158, 2025. [30] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers, 2024. [31] Shuvayan Brahmachary, Subodh M. Joshi, Aniruddha Panda, Kaushik Koneripalli, Arun Kumar Sagotra, Harshil Patel, Ankush Sharma, Ameya D. Jagtap, and Kaushic Kalyanaraman. Large language model-based evolutionary optimizer: Reasoning with elitism, 2024. [32] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Evoprompt: Connecting llms with evolutionary algorithms yields powerful prompt optimizers, 2025. 12 Automating Benchmark Design Figure 7: Illustration of particles and actions in spatial reasoning tasks. Here the board is 4x4 and initially oriented towards north (black arrow). There are two particles 1 and 2 oriented towards west and south respectively. The first action moved the particle 1 forward by one step, second action rotated the particle 2 by 90 degrees and the last action shows rotation of the board by 90 degrees. The board rotations are w.r.t. to its center and when board rotates or moves the particles on it also rotate and move along with it."
        },
        {
            "title": "A Additional Experiments and Details",
            "content": "A.1 Details of benchmarking tasks Arithmetic sequences task. Given an input number and an output number R, an agent must return the sequence of arithmetic operations o1, o2 . . . oN that, when applied recursively to the intermediate results, yield y; i.e, = (oN oN 1 o1)(x). The benchmark space is constrained to simple operations of addition (+), subtraction (), multiplication ), and power of two (()2). For binary operators, both operands are (), division (), square root ( the same. At inference time, the target model, an LLM agent, has access to the arithmetic operators, as tools, to determine the sequence of operators that transform to y. The predicted operator sequence , o 1 is verified by executing the sequence to generate 1, . . . , y = (o 1 1)(x), and comparing it with the ground truth y. Task difficulty depends on factors such as operator choice, sequence length, range of the input x, and whether is integer or floating-point. Operators like subtraction or division tend to collapse toward zero, whereas multiplication and exponentiation operators cause exponential growth. Our automated benchmark design evaluates whether reasoning models can strategically select parameters to generate problems at specified difficulty levels. Spatial Reasoning. Figure 7 illustrates an example of sample from the spatial reasoning environment. On such samples, we ask 4 types of queries. i) Absolute location (x,y) co-ordinates of the particle or the board. The boards location is defined as the location of its center. ii) The tile number on which specific particle is located. iii) The orientation of given particle (north, east, west, or south), and iv) the relative location of particle or board with respect to another particle or board. When an LLM is prompted with such problems, we instruct it to produce structured outputs along with its reasoning traces. The structured output is verified easily with the ground truth computed programmatically. The parameter space includes board_size, an integer between 5 and 100. Boolean flags board_rotates, particle_rotates, board_moves, and particle_moves indicating whether board and particle rotations and movements are allowed or not. If particle rotations are allowed, then allowed_particle_rotations should be non-empty subset of {0, 90, 180, 270, 360}, where each of these numbers indicates counterclockwise rotation in degrees. If particle movements are allowed, then allowed_particle_movements should be non-empty subset of {LEFT, RIGHT, FORWARD, BACKWARD}, indicating the entity moves 1 unit in the stipulated direction w.r.t its orientation (see Figure 7). Similarly, allowed_board_rotations and 13 Automating Benchmark Design allowed_board_movements should be set if their corresponding flags are on; otherwise, they should be empty sets. The parameter space also includes the numbers of each kind of actions to be applied, i.e., number_of_board_rotations, number_of_particle_rotations, number_of_board_movements, and number_of_particle_movements. Each of these must range between 0 to 15. Lastly, flag wrap_around indicates whether the boards boundaries allow the overflowing movement of particle to wrap around from the opposite side. The descriptions of parameters and actions are provided in the prompt (Appendix B) for the designer model. Human Designed τ -bench Airline. The parameter descriptions and expected behaviors are specified in the designer prompt (Appendix B). Each sample corresponds to an airline itinerary planning scenario parameterized by small set of discrete controls. The parameter space includes numerical factors such as num_actions (16), num_passengers (13), and num_baggages (03), as well as categorical attributes like booking_strategy (cheapest/earliest_arrival), is_direct, is_round_trip, cabin (economy/business), and insurance (yes/no). These parameters jointly control itinerary complexity: increasing action count, passengers, or bags expands the combinatorial search space, while enabling multiple strategies, connecting flights, or round-trip requirements adds additional reasoning constraints. When prompted with such parameterized tasks, the LLM designer is instructed to output both thought process describing how does the configuration achieve the target failure rate and the final parameter values in structured JSON. This structured output can be programmatically validated against the student models measured failure rate. Opus 4.1 Designed τ -bench Airline. The parameter space in the Opus 4.1 designed τ -Bench extends beyond structural complexity (e.g., num_actions [1, 6], num_passengers [1, 3]) to include behavioral and informational dimensions. Categorical controls specify booking preferences (booking_strategy: cheapest/earliest_arrival), routing options (is_direct, is_round_trip), cabin composition (cabin_mix: economy, business, or mixed), and environment conditions such as information_completeness (whether all data is provided upfront), information_pattern (upfront, gradual, reactive revelation of details), cooperation_level (helpful/demanding/uncooperative agents), and preference_clarity (explicit vs. implicit preferences). Together, these parameters modulate combinatorial difficulty, reasoning burden, and dialogue complexity, allowing fine-grained control of task hardness to steer the target models empirical failure rate towards the target. The designer model receives target failure rate ρfail and is asked to generate task parameters that achieve 1 pass@1 ρfail. Structured outputs include both the parameter configuration and thought process explaining why it should achieve the desired difficulty level. A.2 Detailed Baselines RS+PPR. The parameter is the probability to sample from the buffer of good parameters and is the gap below which the parameters are considered good. We use = 0.5 and = 0.1 in all the settings. BoN-ML Model Training and Selection. As part of the BoN-ML experiments, we trained and compared classical machine learning models to predict regret efficiently. Across all three domains, we explored over 800 different parameter configurations and architectures. Given the relatively small datasets (100 samples per domain), with feature counts ranging from 13 to 74, we applied 5-fold cross-validation to obtain reliable performance estimates. All features were derived directly from the environment parameters, ensuring the predictors remained lightweight and domain-specific. Models were selected based on the highest cross-validation R2 score, and the best candidates were saved for deployment. Performance was domain-specific: small neural networks performed best for Arithmetic Sequences, Random Forests excelled in Spatial Reasoning, and gradient boosting worked best for τ -Bench. This process yielded fast, domain-tailored predictors to guide BoN-ML parameter selection effectively. Table 3 summarizes the cross-validation R2 training results. 14 Automating Benchmark Design Table 3: BoN-ML regret prediction training results (5-fold CV on 100 samples per domain). Domain Features Best Model CV R2 Arithmetic Seq. Spatial Reasoning τ -Bench 74 28 13 Neural Network (74 2 1, α=1.0) Random Forest (n=50, d=10) Gradient Boosting (n=20, lr=0.1, d=2) 0.52 0.08 0.43 0.05 0.17 0.24 A.3 Details of LLM Models LLM Versions GPT-5: undisclosed - the latest GPT-5 version as of Sep 25, 2025 Opus 4.1: claude-opus4-1-20250805 Grok 4: grok-4-0709 o4-mini: o4-mini-2025-04-16 claude3.7: claude-3-7-sonnet-20250219 gemini-2.5-flash: gemini-2.5-flash LLM Inference Parameters The default temperature for designer models is 0.5, and for target models is 0.0. However, claude-opus-4-1-20250805 and claude-3-7-sonnet-20250219 are only available with temperature of 1. On the Arithmetic Sequence, which is an agentic task, the target model uses time horizon of 16 steps. The default reasoning budget for designer models is 4096 tokens, and for target models is 1024. However, grok-4-0709 does not support configurable reasoning budget. A.4 Dataset Sizes During parameter search, the rollout dataset sizes are 10, 30, and 250 for Arithmetic Sequence, τ -Bench, and Spatial Reasoning, respectively. For evaluation, we generate datasets using the selected parameters, with sizes of 75, 50, and 500 for Arithmetic Sequence, τ -Bench, and Spatial Reasoning, respectively. A.5 Additional Results Figure 8 shows the observed average performance gaps when Claude Opus-4.1 and Grok-4 models are used as designer models. These results show BeTaL achieves low performance gaps across different designer models. We also provide comparison of BeTaL with all designer models on all datasets and target difficulty levels in Figure 11. We see all three designer models achieve similar results across the settings. We study the convergence behavior of iterative methods in settings ranging from trivial to hard difficulty levels (Figure 10). Except for few settings, we see BeTaL iteratively improves its parameter estimates and converges to the desired performance gap after few iterations. These results provide further evidence in support of LLMs effectiveness as optimizers [30]. Next, in Figure 9 we show results over multiple evaluation models across different datasets. As the benchmarks were designed with o4-mini as the target model, we see low performance gap when evaluated on o4-mini. In the τ Bench setting, we see similar performance across different evaluation models. In the Spatial reasoning and Arithmetic sequences setups, there is larger performance gap on evaluation models different from o4-mini; however, the range of observed accuracies (or regret) still reflects the relative hardness levels inherent in the benchmarks. We also analyze the evolution of different parameters over the BeTaL iterations. Figures 12, 13, 14, 15 show the parameter evolution in the spatial reasoning setting with hard, medium, easy, and trivial difficulty levels, respectively. The results show the designer models start off with random (generally high) values of the parameters and gradually tweak them so that the performance gap is minimized. The evolution patterns for individual parameters matches with our intuitive understanding of the spatial reasoning environment. The models prefer larger board sizes, larger numbers and types of actions to increase the difficulty, and conversely smaller values to reduce the complexity. They also prioritize reducing/disabling board actions to reduce complexity, since an action on board also triggers actions on the particles. 15 Automating Benchmark Design Figure 8: Evaluation results on o4-mini with BeTaL (with Claude Opus 4.1 or Grok-4 as the designer model, and o4-mini as the target model during parameter search) perform robustly at different target difficulty levels, compared to baselines on Arithmetic Sequences, Spatial Reasoning, and τ -Bench. Figure 9: BeTaL performance by the designer model during parameter search across three benchmark domains. Each panel represents one dataset (Arithmetic Sequence, Spatial Reasoning, τ -Bench) and compares three designer models (GPT-5, Grok-4, Opus-4.1) across four target performance levels: Hard (ρhard = 0.25), Medium (ρmedium = 0.50), Easy (ρeasy = 0.75), and Trivial (ρtrivial = 0.90), shown as grouped bars. Bars show mean performance gap (difference between target and observed target performance), with o4-mini as target model, averaged over training iterations. Error bars show standard error. 16 Automating Benchmark Design Figure 10: BeTaL vs. RS+PPR convergence during parameter search across datasets and target difficulty levels. Each panel shows the mean performance gap (difference between target and observed target performance) over training iterations for two design approaches: BeTaL (our method) and RS+PPR (baseline). Rows indicate target performance (Hard (ρhard = 0.25), Medium (ρmedium = 0.50), Easy (ρeasy = 0.75), and Trivial (ρtrivial = 0.90)). Columns show three benchmark domains (Arithmetic Sequence, Spatial Reasoning, τ -Bench). BeTaL results are averaged across designer models (GPT-5, Grok-4, Opus-4.1). All results use o4-mini as the target model, with shaded regions showing standard error across seeds. 17 Automating Benchmark Design Figure 11: Designer model performance during parameter search across datasets and target difficulty levels. Each panel shows the mean performance gap (difference between target and observed target performance) for different design approaches: BeTaL, BoN-ML, and BoN-TM. Rows indicate target performance (Hard (ρhard = 0.25), Medium (ρmedium = 0.50), Easy (ρeasy = 0.75), and Trivial (ρtrivial = 0.90)). Columns show three benchmark domains (Arithmetic Sequence, Spatial Reasoning, τ -Bench). All results use o4-mini as the target model, averaged over each iteration. 18 Automating Benchmark Design Figure 12: Parameter evolution over iterations in the hard difficulty setting. The subplots show average values of the different design parameters at each iteration chosen by the designer models (GPT-5, Grok4, Opus-4-1). Row 1 shows board_size (width), wrap_around and number_of_board_rotation. Row 2 shows number_of_board_movements, number_of_particle_rotations and number_of_particle_movements. Next row presents the sum of these number of actions (total actions), absolute performance gap as observed on the o4-mini target model and the number of enabled capabilities (types of rotations and movements), here BM, BR are the sizes of sets allowed_board_movements and allowed_board_rotations and PM, PR similarly reflect the sizes of action sets corresponding to the particles. We can see to obtain hard configuration, models generally prefer larger board size and higher number of capabilities and actions. Among the models, GPT-5 does it more aggressively and achieves the lowest performance gap as well. 19 Automating Benchmark Design Figure 13: Parameter evolution over iterations in the medium difficulty setting. The subplots show average values of the different design parameters at each iteration chosen by the designer models (GPT-5, Grok4, Opus-4-1). Row 1 shows board_size (width), wrap_around and number_of_board_rotation. Row 2 shows number_of_board_movements, number_of_particle_rotations and number_of_particle_movements. Next row presents the sum of these number of actions (total actions), absolute performance gap as observed on the o4-mini target model and the number of enabled capabilities (types of rotations and movements), here BM, BR are the sizes of sets allowed_board_movements and allowed_board_rotations and PM, PR similarly reflect the sizes of action sets corresponding to the particles. We can see that, to obtain medium difficulty configuration, models prefer much smaller board sizes and number and types of actions as compared to the hard setting in Figure 12. Also consistent with the expectations, the models reduce the number of board actions close to 0 but allow decent number of particle actions. 20 Automating Benchmark Design Figure 14: Parameter evolution over iterations in the medium difficulty setting. The subplots show average values of the different design parameters at each iteration chosen by the designer models (GPT-5, Grok4, Opus-4-1). Row 1 shows board_size (width), wrap_around and number_of_board_rotation. Row 2 shows number_of_board_movements, number_of_particle_rotations and number_of_particle_movements. Next row presents the sum of these number of actions (total actions), absolute performance gap as observed on the o4-mini target model and the number of enabled capabilities (types of rotations and movements), here BM, BR are the sizes of sets allowed_board_movements and allowed_board_rotations and PM, PR similarly reflect the sizes of action sets corresponding to the particles. We can see, to obtain medium difficulty configuration, models prefer much smaller board sizes and number and types of actions as compared to the hard setting in Figure 12. Also consistent with the expectations the models reduce the number of board actions close to 0 but allow decent number of particles actions. 21 Automating Benchmark Design Figure 15: Parameter evolution over iterations in the trivial difficulty setting. The subplots show average values of the different design parameters at each iteration chosen by the designer models (GPT-5, Grok4, Opus-4-1). Row 1 shows board_size (width), wrap_around and number_of_board_rotation. Row 2 shows number_of_board_movements, number_of_particle_rotations and number_of_particle_movements. Next row presents the sum of these number of actions (total actions), absolute performance gap as observed on the o4-mini target model and the number of enabled capabilities (types of rotations and movements), here BM, BR are the sizes of sets allowed_board_movements and allowed_board_rotations and PM, PR similarly reflect the sizes of action sets corresponding to the particles. We can see that, to obtain an easy difficulty configuration, models prefer smaller board sizes and number and types of actions as compared to the medium and easy settings in Figures 13 and 14. Also consistent with the expectations, the models reduce the number of board actions close to 0, but allow few actions on particles. 22 Automating Benchmark Design"
        },
        {
            "title": "B Prompts",
            "content": "We provide the prompts provided to the designer models across the three tasks considered in the paper."
        },
        {
            "title": "LLM Designer Prompt for Arithmetic Sequence",
            "content": "The math problem is to apply sequence of operators on number to produce final answer. The sequence of operators are applied recursively on intermediate results, i.e., num = operator(num) for each operator in the sequence. The operators only take in one number as input. You should target the given model regret at {target_regret}, so that the parameters can generate math problem for the model at the desired regret level. high regret indicates challenging environment (1 for unsolvable), while low regret indicates an easy environment (0 for easy). Here is the feedback from the previous iterations, which you can use to generate new parameters: {feedback} First, reason about the feedback from previous iterations. Specifically note what parameters/aspects made previous environments challenging or trivial. Then, given list of common math operators {operators}, your task is to generate values for the given parameters: 1. feedback_summary: your summary of the feedback from the previous iterations. 2. thought_process: your thought process for generating the parameters. 3. max_range_of_nums: the upper bound of range the input number can take on, i.e. (1, max_range_of_nums). Pick number between 5 and 50. 4. N: the length of the sequence of operators to apply on number (between 5 and 10) 5. K: The maximum number of times an operator can be repeated in the sequence (between 1 and 5) 6. type_of_nums: the type of numbers in the input (int or float) 7. operator_sequence: select 3 operators from the list above, to generate sequence of operators of length to apply on number, where each operator can be repeated at most times. Output format (JSON): { } , , \" feedback_summary \" : \" u _ c \" : \" max_range_of_nums \" : , \"N \" : , \"K\" : \" type_of_nums \" : , \" r r _ u e \" : t , t [ ] LLM Designer Prompt for the Spatial Reasoning Environment You are an expert in designing spatial reasoning environments. The environment is 2D grid world. It consists of square board and two particles on the board. The boards dimensions can be from 5 to 100. The board is divided into tiles of size 1x1. The particles are at the center of the tiles. Each object (board and particles) in the environment has an orientation and location. The orientation is the direction in which the object is facing, which can be one of the following: NORTH, EAST, SOUTH, WEST. The location of particle is given by the 2D coordinates of the center of the tile on which the particle is located. The orientation and location of particle are initialized randomly. The location of the board is the 2D coordinates of the center of the board. Automating Benchmark Design The orientation of the board is the orientation of its center. It is always initialized to NORTH. The environments complexity can be controlled by the following parameters: - The board size determined by the width parameter. - The board can either allow particles to wrap around the edges or not. It is determined by the wrap_around parameter. If it is true, then the particles can wrap around the edges of the board. If it is false, then the particles cannot wrap around the edges of the board. - The movements allowed for the objects (board and particles). Each object can have subset of the following movements: LEFT, RIGHT, FORWARD, BACKWARD. - The rotations allowed for the objects (board and particles). Each object can have subset of the following rotations: 0, 90, 180, 270, 360. If the rotation is 0, then the object is not rotated. If the rotation is 90, then the object is rotated 90 degrees counter-clockwise. If the rotation is 180, then the object is rotated 180 degrees counter-clockwise. If the rotation is 270, then the object is rotated 270 degrees counter-clockwise. If the rotation is 360, then the object is rotated 360 degrees counter-clockwise. You are given list of parameters for board and list of parameters for particle. You are also given list of parameters for actions that can be performed on the board and the particle. You need to design spatial reasoning environment that is sufficiently challenging and an average language model can achieve target accuracy of <accuracy>. # Response format - JSON schema You must get the final answer and convert it to the following JSON data structure. Follow the schema exactly. Key: thought_process Type: String, Description: Your thought process when designing the environment. Key: wrap_around Type: Boolean, Description: Whether the board can wrap around the edges. Key: width Type: Integer, Description: The width of the board. Key: board_moves Type: Boolean, Description: Whether the board can move. Key: board_allowed_moves Type: List of Strings, Description: The movements allowed for the board, must be subset of: LEFT, RIGHT, FORWARD, BACKWARD. Key: board_rotates Type: Boolean, Description: Whether the board can rotate. Key: board_allowed_rotations Type: List of Integers, Description: The rotations allowed for the board, must be subset of: 0, 90, 180, 270, 360. Key: particle_moves Type: Boolean, Description: Whether the particle can move. Key: particle_allowed_moves Type: List of Strings, Automating Benchmark Design Description: The movements allowed for the particle, must be subset of: LEFT, RIGHT, FORWARD, BACKWARD. Key: particle_rotates Type: Boolean, Description: Whether the particle can rotate. Key: particle_allowed_rotations Type: List of Integers, Description: The rotations allowed for the particle, must be subset of: 0, 90, 180, 270, 360. Key: number_of_board_rotation_actions Type: Integer, Description: The number of times the board can be rotated if board_rotates is true. Key: number_of_particle_rotation_actions Type: Integer, Description: The number of times the particles can be rotated if particle_rotates is true. Key: number_of_board_movement_actions Type: Integer, Description: The number of times the board can be moved if board_moves is true. Key: number_of_particle_movement_actions Type: Integer, Description: The number of times the particles can be moved if particle_moves is true. LLM Designer Prompt for τ -bench Airline Environment You are an expert in designing airline-booking tasks for language-model agents. Your goal is to propose task parameters that drive student model to target failure rate of 0.75. Here, the failure rate is defined as 1 pass@1 for the student model, i.e., the probability that the student fails to solve the task on the first attempt. You are directly rewarded for minimizing the absolute distance to the 0.75 failure rate, so choose parameters that make the task challenging enough to approach this target as closely as possible. The task setting is an airline-shopping environment where an agent must construct an itinerary subject to constraints (e.g., number of actions, bags, cabin class, routing). Controllable parameters and intended complexity effects: - num_actions (16): Upper bound on primitive planning/interaction steps. Start simple with fewer actions; increase to raise difficulty. - num_passengers (13): More passengers typically increases combinatorial constraints and price/timing trade-offs. - num_baggages (03): More bags interact with fare rules and cabin choices; higher values generally increase difficulty. - booking_strategy: Subset of {cheapest, earliest_arrival}. Multiple strategies introduce objective trade-offs. - is_direct: Boolean. Allowing false admits connections and routing search complexity. - is_round_trip: Boolean. Round-trips add coupling between outbound/return constraints. - cabin: Subset of {economy, business}. More options broaden fare/rule search space. - insurance: One of {yes, no}. Insurance interacts with cost-focused strategies and can add goal ambiguity. Tune these parameters to steer the student models 1 pass@1 toward 0.75. Automating Benchmark Design # Response format JSON schema You must get the final answer and convert it to the following JSON data structure. Follow the schema exactly. Key: thought_process Type: String Description: Concise reasoning explaining how the chosen parameters are expected to yield failure rate near 0.75; reference how each parameter affects difficulty. Key: num_actions Type: Integer (range: 16) Description: Maximum number of allowed actions/steps. Key: num_passengers Type: Integer (range: 13) Description: Number of travelers to book. Key: num_baggages Type: Integer (range: 03) Description: Total checked bags across passengers. Key: booking_strategy Type: List of Strings (subset of: {cheapest, earliest_arrival}) Description: Allowed objective(s) for the student; may include one or both. Key: is_direct Type: Boolean Description: If true, only nonstop itineraries are valid; if false, connections are allowed. Key: is_round_trip Type: Boolean Description: Whether the itinerary must include return travel. Key: cabin Type: List of Strings (subset of: {economy, business}) Description: Allowed cabin classes. Key: insurance Type: String (one of: yes, no) Description: Whether trip insurance is part of the task constraints. LLM Designer Prompt for τ -bench Airline Environment (Parameter Space Generated by Opus-4.1) You are an expert in designing airline-booking tasks for language-model agents. Your goal is to propose task parameters that drive student model to target failure rate of {target_failure_rate}. Here, the failure rate is defined as 1 pass@1 for the student model, i.e., the probability that the student fails to solve the task on the first attempt. You are directly rewarded for minimizing the absolute distance to {target_failure_rate}, so choose parameters that make the task challenging enough to approach this target as closely as possible. The task setting is an airline-shopping environment where an agent must construct an itinerary subject to constraints (e.g., number of actions, passengers, bags, cabin class, routing, information flow, and user cooperation). Controllable parameters and intended complexity effects: 26 Automating Benchmark Design - num_actions (16): Upper bound on primitive planning/interaction steps. Fewer actions constrain search; increasing actions raises planning depth and error surface. - num_passengers (13): More travelers increase combinatorial constraints (seat availability, fares), amplifying trade-offs. - num_baggages (03): More bags interact with fare rules and cabin choices; higher values generally increase difficulty. - booking_strategy: Subset of {cheapest, earliest_arrival}. Multiple objectives introduce competing trade-offs and ambiguity. - is_direct: Boolean. Allowing false admits connections and routing search complexity (layovers, MCT). - is_round_trip: Boolean. Round-trips couple outbound/return constraints and calendaring. - cabin_mix: Subset of {economy_only, business_only, mixed}. mixed broadens fare/rule search and cross-cabin reasoning. - information_completeness: Boolean. If false, key facts are omitted initially, forcing clarification steps and robustness to uncertainty. - cooperation_level: Subset of {helpful, demanding, uncooperative}. Less cooperative users increase dialogue turns, constraint changes, and error likelihood. - information_pattern: Subset of {upfront, gradual, reactive}. Non-upfront patterns stagger constraints and increase planning revisions. - preference_clarity: Subset of {explicit, implicit}. implicit requires inference from hints (e.g., times, budgets), increasing ambiguity. Tune these parameters to steer the student models 1 pass@1 toward {target_failure_rate}. # Response format JSON schema You must get the final answer and convert it to the following JSON data structure. Follow the schema exactly. Key: thought_process Type: String Description: Concise reasoning explaining how the chosen parameters are expected to yield failure rate near {target_failure_rate}; reference how each parameter affects difficulty. Key: num_actions Type: Integer (range: 16) Description: Maximum number of allowed actions/steps. Key: num_passengers Type: Integer (range: 13) Description: Number of travelers to book. Key: num_baggages Type: Integer (range: 03) Description: Total checked bags across passengers. Key: booking_strategy Type: List of Strings (subset of: {cheapest, earliest_arrival}) Description: Allowed objective(s) for the student; may include one or both. Key: is_direct Type: Boolean Description: If true, only nonstop itineraries are valid; if false, connections are allowed. Key: is_round_trip Type: Boolean Description: Whether the itinerary must include return travel. Automating Benchmark Design Key: cabin_mix Type: List of Strings (subset of: {economy_only, business_only, mixed}) Description: Allowed cabin configuration scope. Key: information_completeness Type: Boolean Description: If true, all necessary details are provided initially; if false, some are withheld. Key: cooperation_level Type: List of Strings (subset of: {helpful, demanding, uncooperative}) Description: Expected user cooperation profile(s). Key: information_pattern Type: List of Strings (subset of: {upfront, gradual, reactive}) Description: How and when information is revealed during the interaction. Key: preference_clarity Type: List of Strings (subset of: {explicit, implicit}) Description: Whether preferences are stated clearly or must be inferred. Example of Question on the Arithmetic Sequence Task You are an agent that can use tools via tool calling: If you have the final answer, respond with: FINAL <sequence of operators as comma separated list> Given the following input number and final answer, use the functions provided to perform the correct sequence of operations on the input number to get the final answer. Input number: 2.4460677252452125 Final answer: 4.423634456186643 Example of Question in the Spatial Reasoning Setting Following is the description of the spatial reasoning environment. Go through it carefully and then answer the question in the requested format. # Environment ## Setup All locations are pairs of real numbers (x, y). North corresponds to increasing y, and South corresponds to decreasing y. East corresponds to increasing x, and West corresponds to decreasing x. Orientation is direction, and can be one of the following: North, East, South, or West. Orientation is also measured in degrees, and can be one of the following: 0, 90, 180, 270. Where 0 means East, 90 means North, 180 means West, and 270 means South. boards rotation is defined as the rotation of the board around its center. When board rotates, the orientation of the board changes, and the tiles and particles on the board also rotate along with it. particles rotation changes the orientation of the particle, but does not change the location of the particle. As general rule, any entitys rotation can change the orientation of the entity, but does not change the location of the entity. boards location is defined as the location of its center. boards movement changes the location of the board, and the tiles and particles on the board also move along with it. For example, if Automating Benchmark Design board moves forward 1 unit, the center of the board and the tiles and particles on the board all move 1 unit along the orientation of the board. particles movement changes the location of the particle For example, if particle moves forward 1 unit, the location of the particle changes by 1 unit along the orientation of the particle. If the movement of particles results in the particle moving beyond the boundary of the board, then the particle will either wrap around the boundary of the board or remain at the current tile. It depends on the boards wrap around settings, which are described in the description of the board. As general rule, any entitys movement can change the location of the entity, but does not change the orientation of the entity. The orientation of an entity can be thought of as the direction in which the entity is facing. This determines the meaning of forward, backward, left, right, etc., for the entity. ## Entities The environment contains the following entities: # Board ## Setup board is 12.0 units wide and 12.0 units tall, and contains 2 particle(s). It is centered at (0.0, 0.0). Its orientation is defined as the centers orientation, which is NORTH. Initially, the board is oriented NORTH. The board has four sides: SIDE-1, SIDE-2, SIDE-3, SIDE-4 The side from the south west corner to south east corner is the bottom side of the board. It is called SIDE-1 The side from the south east corner to north east corner is the right side of the board. It is called SIDE-2 The side from the north east corner to north west corner is the top side of the board. It is called SIDE-3 The side from the north west corner to south west corner is the left side of the board. It is called SIDE-4 ## Boundaries In the event the particle move results in the particle moving beyond the boundary of the board, the resulting location is decided as follows: When particle is on tile, it means its location is the tiles centroid. The SIDE-1 of the board can be crossed when approaching from the SIDE-3, and the particle(s) will move to the opposite tile on the SIDE-3. The SIDE-2 of the board can be crossed when approaching from the SIDE-4, and the particle(s) will move to the opposite tile on the SIDE-4. The SIDE-3 of the board can be crossed when approaching from the SIDE-1, and the particle(s) will move to the opposite tile on the SIDE-1. The SIDE-4 of the board can be crossed when approaching from the SIDE-2, and the particle(s) will move to the opposite tile on the SIDE-2. ## Tiles on the board The board is divided into square tiles of size 1 units by 1 units. Tiles are numbered from 1 to (width * height), starting from the bottom left corner in zigzag pattern. Going from left to right, then right to left, and so on. For example, for 3x3 board, the tiles are numbered as follows: 9 8 7 6 5 4 1 2 ## Allowed moves The following moves are allowed for the board: FORWARD - board moves forward 1 unit. BACKWARD - board moves backwards 1 unit. Orientation remains the same. LEFT - board sidesteps 1 unit to the left. Orientation remains the same. RIGHT - board sidesteps 1 unit to the right. Orientation remains the same. ## Allowed rotations The following rotations are allowed for the board: 90 - board rotates 90 degrees. 180 - board rotates 180 degrees. 270 - board rotates 270 degrees. 29 Automating Benchmark Design # Particle P1 ## Initial State It is located at (3.5, 3.5), and is facing WEST (180 degrees). It is on tile 111. It is on board B1. ## Allowed moves The following moves are allowed for this particle: FORWARD - particle moves forward 1 unit. BACKWARD - particle moves backwards 1 unit. Orientation remains the same. LEFT - particle sidesteps 1 unit to the left. Orientation remains the same. RIGHT - particle sidesteps 1 unit to the right. Orientation remains the same. ## Allowed rotations The following rotations are allowed for this particle: 90 - particle rotates 90 degrees. 180 - particle rotates 180 degrees. 270 - particle rotates 270 degrees. # Particle P2 ## Initial State It is located at (-0.5, 5.5), and is facing SOUTH (270 degrees). It is on tile 139. It is on board B1. ## Allowed moves The following moves are allowed for this particle: FORWARD - particle moves forward 1 unit. BACKWARD - particle moves backwards 1 unit. Orientation remains the same. LEFT - particle sidesteps 1 unit to the left. Orientation remains the same. RIGHT - particle sidesteps 1 unit to the right. Orientation remains the same. ## Allowed rotations The following rotations are allowed for this particle: 90 - particle rotates 90 degrees. 180 - particle rotates 180 degrees. 270 - particle rotates 270 degrees. # Actions The actions are the following: First, board B1 is rotated by 270 degrees. Then, particle P2 is rotated by 270 degrees. Then, particle P1 is rotated by 270 degrees. Then, particle P1 is rotated by 90 degrees. Finally, move particle P2 BACKWARD by 1 units. # Question What is the location of board B1 after all the actions? # Response format - JSON schema You must get the final answer and convert it to the following JSON data structure. Follow the schema exactly. Key: board_B1_x Type: Float, Description: The x-coordinate of board B1 after all the actions. Key: board_B1_y Type: Float, Description: The y-coordinate of board B1 after all the actions. Example of Task in the τ -bench Airline Setting Following is the description of the airline environment. Go through it carefully and then answer the question in the requested format. Automating Benchmark Design # Environment ## Setup The environment simulates commercial airline booking system. Airports are identified by IATA codes (e.g., SEA, EWR). Dates are formatted YYYY-MM-DD. Times are HH:MM:SS in local (EST) for scheduling metadata. Cabins include basic_economy, economy, and business. Bookings may be one_way or round_trip. Payment instruments include certificate, gift_card, and credit_card. Baggage may be free or non-free depending on fare rules (not shown here). Insurance is optional. ## Capabilities Agents may: Search flights (nonstop or onestop) between an origin and destination on specified date. Book reservations with specified flight legs, cabin, passengers, baggages, insurance, and payment methods (in priority order). Request issuance of travel certificate with specified ID and amount. # Entities ## User U1 User identifier: mohamed_li_7869. The users birthday is present in the profile and should not be requested during the interaction. ## Passenger(s) single passenger is provided and known to the user: first_name: Yusuf, last_name: Thomas, dob: 1966-05-11 ## Payment Instruments (available to U1) gift_card_3525913: amount 27 gift_card_5876000: amount 176 gift_card_7716568: amount 237 credit_card_1922786: amount 139 Preferred payment order: certificate gift card credit card. # Demands ## Demand 1: Flight Search Search for an onestop flight from LGA to DTW on 2024-05-25. ## Demand 2: Booking Book one-stop, one-way itinerary from SEA to EWR on 2024-05-30 in business cabin for 1 passenger with 1 total baggage. Choose the cheapest eligible option. Include insurance. Use payments in the order: certificate(s) first, then gift card(s), then credit card(s). Candidate flights presented (for selection during booking): Leg 1: flight_number: HAT117, origin: SEA, destination: DFW scheduled_departure_time_est: 10:00:00, scheduled_arrival_time_est: 14:00:00 status: available, date: 2024-05-30 Seats available: basic_economy 5, economy 0, business 1 Prices: basic_economy 62, economy 119, business 263 Leg 2: flight_number: HAT063, origin: DFW, destination: EWR scheduled_departure_time_est: 18:00:00, scheduled_arrival_time_est: 21:30:00 status: available, date: 2024-05-30 Seats available: basic_economy 11, economy 15, business 9 Prices: basic_economy 80, economy 137, business 286 ## Demand 3: Certificate Issuance Automating Benchmark Design Request certificate with: certificate_id: certificate_4314319 amount: 170 # Actions The intended agent actions, in order, are as follows: 1. search_onestop_flight with {origin: PHX, destination: DFW, date: 2024-05-18}. 2. book_reservation with the provided passenger, baggage, cabin, flight legs (SEADFW, then DFWEWR on 2024-05-30), one-way, business, cheapest, insurance yes, and payment methods listed above in the stated priority order. 3. send_certificate with {certificate_id: certificate_4314319, amount: 170}. Note. Although Demand 1 specifies LGADTW (2024-05-25) search, the sample action shows PHXDFW (2024-05-18). The agent must honor the stated Demands when resolving inconsistencies (prefer Demands). # Question Produce the exact JSON payload(s) for the three API calls in the correct order that satisfy all Demands above (use LGADTW for the search as specified by Demand 1; for booking, choose the cheapest eligible business one-stop SEAEWR itinerary from the two legs provided; include insurance; and apply payment instruments in the order certificate gift card(s) credit card(s)). # Response format - JSON schema Return single JSON object with the following keys: Key: action_sequence Type: Array of Objects Description: The ordered list of actions. Each object must have: name (string; one of search_onestop_flight, book_reservation, send_certificate) kwargs (object; the exact arguments for the call) Key: notes Type: String Description: Brief justification for flight choice, insurance inclusion, baggage count, and the payment breakdown order. 32 Automating Benchmark Design"
        },
        {
            "title": "C Teacher Model Reasoning Traces",
            "content": "This appendix presents reasoning traces from teacher models designing environments across multiple tasks. Each example shows the Chain-of-Thought reasoning used by the teacher model to select environment parameters targeting specific difficulty levels. C.1 Arithmetic Sequence Example 1: Grok-4 Method: BeTaL Target Accuracy: 25.0% Task: Arithmetic Sequence Teacher Model Reasoning at the 3rd iteration: Feedback Summary: The previous iterations had accuracy of 100.0%, indicating trivial environments. The sequences involved repeating patterns of pow, mul, and sqrt or div, which led to cyclic or stable values that were easy to compute. Thought Process: The previous environments used float numbers with operator sequences that resulted in repetitive and predictable patterns, leading to accuracy of 100.0%. The computations were simple due to cycling between values. To target accuracy of 25%, will use integer numbers and sequence consisting of many squaring operations using 'pow' and 'mul' to produce extremely large numbers that are difficult for the model to compute exactly. choose = 8, = 4, max_range_of_nums = 40, type = int, operators 'pow', 'mul', 'sqrt', but the sequence will use only 'pow' and 'mul' to avoid reducing the number. The sequence will have 4 'pow' and 4 'mul' for 8 squarings, leading to x256, which for 40 has about 411 digits, which should be challenging. 33 Automating Benchmark Design Example 2: GPT-5 Method: BoN-TM Target Accuracy: 25.0% Task: Arithmetic Sequence Teacher Model Reasoning: Goal: Set parameters so that deducing the operator sequence from single input-output example is challenging but solvable (target accuracy 0.25). will iterate up to two times. Iteration 1 (rejected): - Tried ops: {pow, div, sqrt}, = 8, = 2. Sampled sequence mixing pow and sqrt. Observation: pow and sqrt largely cancel or explode values; multiple reverse paths produce identical results, leading to high ambiguity. Estimated accuracy 0.1 (too hard). Iteration 2 (accepted): - Choose ops: {mul, add, sqrt}. Set = 7, = 3 so each operator can appear at most 3 times. This mix gives nonlinearity via sqrt and moderate growth via mul/add, making reverse inference nontrivial but feasible. - For sampling only, assume unary meanings commonly used in such puzzles: add = +1, mul = 2, sqrt = principal square root. Type: float to accommodate non-perfect-square roots. Max input range cap: 20. - Constructed operator_sequence of length 7 with counts within K: [mul, add, sqrt, mul, add, sqrt, mul] (mul3, add2, sqrt2). - Sample forward calculation (input = 9): 1) mul: 9 18 2) add: 18 19 3) sqrt: 19 4.3588989435 4) mul: 4.3589 8.7177978871 5) add: 8.7178 9.7177978871 6) sqrt: 9.7178 3.1189801054 7) mul: 3.1190 6.2379602108 (final) - Reverse reasoning feasibility (given input 9, final 6.23796, = 7, ops {add, mul, sqrt}): Assume last op mul divide by 2: 3.11898; prior sqrt square: 9.7178; prior add subtract 1: 8.7178; prior mul divide by 2: 4.3589; prior sqrt square: 19; prior add subtract 1: 18; prior mul divide by 2: 9 = input. This reconstructs consistent and plausible unique path under constraints, but model would need nontrivial backtracking/search, making it challenging. - Difficulty assessment: Branching factor up to 3 with numeric constraints and float noise; sequence length 7; inverse search with sanity checks is required. This is difficult but solvable. Estimated accuracy 0.25. Conclusion: Use the parameters below with operator_sequence limited to {mul, add, sqrt}, = 7, = 3, float numbers, and input range up to 20 to achieve the target challenge level. 34 Automating Benchmark Design Example 3: Claude Opus 4.1 Method: BoN-ML Target Accuracy: 25.0% Task: Arithmetic Sequence Teacher Model Reasoning: Iteration 1: Starting with moderate complexity - Lets use = 6, = 2 with operators that create some ambiguity. - Testing: ['mul', 'sqrt', 'add'] with sequence ['mul', 'sqrt', 'add', 'mul', 'sqrt', 'add']. - With input 4: 4 8 (mul2) 2.83 (sqrt) 3.83 (add+1) 7.66 (mul2) 2.77 (sqrt) 3.77 (add+1) - This is somewhat traceable, maybe too easy for 0.25 accuracy. Iteration 2: Increasing complexity - = 7, = 3, using operators that interact in complex ways. - Testing: ['pow', 'div', 'sqrt'] with sequence ['pow', 'sqrt', 'div', 'pow', 'sqrt', 'div', 'sqrt']. - With input 4: 4 16 (pow) 4 (sqrt) 2 (div 2) 4 (pow) 2 (sqrt) 1 (div 2) 1 (sqrt) - The pow and sqrt operations create multiple possible paths, better for 0.25 accuracy. Iteration 3: Fine-tuning for target accuracy - = 8, = 3, float type for added precision challenges. - Testing: ['mul', 'sqrt', 'pow'] these operators create exponential growth/reduction patterns. - Sequence: ['mul', 'pow', 'sqrt', 'mul', 'sqrt', 'pow', 'mul', 'sqrt']. - This creates complex transformations that are difficult but not impossible to reverse-engineer. - The combination of squaring (pow) and square root operations creates ambiguity. - Multiple valid paths could potentially lead to the same result. - Estimated accuracy: 0.25 (challenging but solvable with careful analysis). C.2 Spatial Reasoning Example 4: GrokMethod: BeTaL Target Accuracy: 75.0% Task: Spatial Reasoning Teacher Model Reasoning: To achieve target accuracy of 75% for an average language model, designed the environment with 10x10 board and wrap-around enabled to introduce moderate complexity in position calculations. The board is allowed to rotate in 90 and 180 degrees up to 2 times, adding challenge to the reference frame without overwhelming. Particles can move in all four directions up to 5 times and rotate in 90, 180, 270 degrees up to 3 times each. This setup requires tracking multiple transformations, leading to potential errors in about 25% of cases. 35 Automating Benchmark Design Example 5: Claude Opus-4. Method: BoN-TM Target Accuracy: 75.0% Task: Spatial Reasoning Teacher Model Reasoning: Designing for 75% accuracy (0.25 regret): Need moderate complexity with some challenging elements but not overwhelming. Key decisions: 12x12 grid provides good spatial complexity without being too large. Wrap-around adds edge-case complexity. Board rotation (90, 180) creates coordinate transformation challenges. Limited board movement (LEFT, RIGHT) adds another dimension without overwhelming. Particles have full movement but limited rotation for balanced complexity. Action counts moderate (2-3) to require multi-step reasoning but not excessive mental load. Example 6: Claude Opus-4.1 Method: BoN-ML Target Accuracy: 75.0% Task: Spatial Reasoning Teacher Model Reasoning: To achieve 75% accuracy (0.25 regret), need moderate complexity. After iterating through options, Ive selected an 8x8 grid which is large enough to be interesting but small enough to track mentally. The key challenge comes from board rotations (90, 180, 270 degrees) which transform the coordinate system, combined with particle movements (FORWARD/BACKWARD) relative to their orientation. With 2 board rotations and 3 particle movements, problems require tracking coordinate transformations and orientations through 5 steps total, creating moderate difficulty suitable for the target performance level. C.3 τ -bench Example 7: Opus-4.1 Method: BeTaL Target Accuracy: 75.0% Task: τ -bench Teacher Model Reasoning: Based on the previous results, observe that num_actions=4 with 2 passengers and 2 baggages gave failure rate of 0.7, while num_actions=5 with 2 passengers and 3 baggages also gave 0.7. Both are 0.05 away from the target of 0.75. To potentially hit closer to 0.75, Ill try num_actions=4 with 2 passengers but increase baggages to 3. This creates configuration between the tested onesslightly more complex than the 4-2-2 setup (which gave 0.7) but less complex than the 5-2-3 setup (which also gave 0.7). This intermediate complexity might push the failure rate slightly higher toward 0.75. 36 Automating Benchmark Design Example 8: GPT5 Method: BoN-TM Target Accuracy: 75.0% Task: τ -bench Teacher Model Reasoning: To target 0.75 failure rate (pass@1 0.25), increased task complexity via higher action count and maximum passengers, enabled multiple competing criteria and toggles (booking strategy, directness, round-trip, insurance), and limited one dimension (cabin) plus moderate baggage count to avoid overshooting difficulty. Example 9: Grok-4 Method: BoN-ML Target Accuracy: 75.0% Task: τ -bench Teacher Model Reasoning: To target failure rate of 0.75, aim for moderate to high complexity. set num_actions to 4 for some complexity without maxing out. num_passengers at 2 and num_baggages at 2 add moderate difficulty. Including both options in arrays for booking_strategy, is_direct, is_round_trip, cabin, and insurance creates varied and potentially challenging tasks, which could lead to around 75 Note: All reasoning traces show the teacher models explanation of why specific parameters were chosen to achieve the target difficulty level. Different experiments may use scratchpad or thought_process field names due to prompt variations; both contain equivalent teacher model reasoning."
        }
    ],
    "affiliations": [
        "Dept. of Computer Sciences, University of Wisconsin-Madison, WI, USA",
        "Snorkel AI, Redwood City, CA, USA"
    ]
}