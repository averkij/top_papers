{
    "paper_title": "RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection in Complex Orchard Environments Under Label Ambiguity",
    "authors": [
        "Ranjan Sapkota",
        "Rahul Harsha Cheppally",
        "Ajay Sharda",
        "Manoj Karkee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This study conducts a detailed comparison of RF-DETR object detection base model and YOLOv12 object detection model configurations for detecting greenfruits in a complex orchard environment marked by label ambiguity, occlusions, and background blending. A custom dataset was developed featuring both single-class (greenfruit) and multi-class (occluded and non-occluded greenfruits) annotations to assess model performance under dynamic real-world conditions. RF-DETR object detection model, utilizing a DINOv2 backbone and deformable attention, excelled in global context modeling, effectively identifying partially occluded or ambiguous greenfruits. In contrast, YOLOv12 leveraged CNN-based attention for enhanced local feature extraction, optimizing it for computational efficiency and edge deployment. RF-DETR achieved the highest mean Average Precision (mAP50) of 0.9464 in single-class detection, proving its superior ability to localize greenfruits in cluttered scenes. Although YOLOv12N recorded the highest mAP@50:95 of 0.7620, RF-DETR consistently outperformed in complex spatial scenarios. For multi-class detection, RF-DETR led with an mAP@50 of 0.8298, showing its capability to differentiate between occluded and non-occluded fruits, while YOLOv12L scored highest in mAP@50:95 with 0.6622, indicating better classification in detailed occlusion contexts. Training dynamics analysis highlighted RF-DETR's swift convergence, particularly in single-class settings where it plateaued within 10 epochs, demonstrating the efficiency of transformer-based architectures in adapting to dynamic visual data. These findings validate RF-DETR's effectiveness for precision agricultural applications, with YOLOv12 suited for fast-response scenarios. >Index Terms: RF-DETR object detection, YOLOv12, YOLOv13, YOLOv14, YOLOv15, YOLOE, YOLO World, YOLO, You Only Look Once, Roboflow, Detection Transformers, CNNs"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 9 9 0 3 1 . 4 0 5 2 : r RF-DETR Object Detection vs YOLOv12 : Study of Transformer-based and CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection in Complex Orchard Environments Under Label Ambiguity Ranjan Sapkotaa,, Rahul Harsha Cheppallyb, Ajay Shardab, Manoj Karkeea, aBiological & Environmental Engineering, Cornell University, Ithaca, 14850, NY, USA bDepartment of Biological and Agricultural Engineering, Kansas State University, Manhattan, 66502, KS, USA Abstract This study presents comprehensive comparison between RF-DETR object detection and YOLOv12 object detection models for greenfruit recognition in complex orchard environments characterized by label ambiguity, occlusion, and background camouflage. custom dataset was developed featuring both single-class (greenfruit) and multi-class (occluded and non-occluded greenfruits) annotations to assess model performance under real-world conditions. The RF-DETR object detection model, leveraging DINOv2 backbone with deformable attention mechanisms, excelled in global context modeling, which proved particularly effective for identifying partially occluded or visually ambiguous greenfruits. Conversely, the YOLOv12 model employed CNN-based attention mechanisms to enhance local feature extraction, optimizing it for computational efficiency and edge deployment suitability. In the single-class detection scenarios, RF-DETR achieved the highest mean Average Precision (mAP@50) of 0.9464, showcasing its robust capability to accurately localize greenfruits within cluttered scenes. Despite YOLOv12N achieving the highest mAP@50:95 of 0.7620, RF-DETR object detection model consistently outperformed in managing complex spatial scenarios. In multi-class detection, RF-DETR again led with an mAP@50 of 0.8298, demonstrating its effectiveness in distinguishing between occluded and non-occluded fruits, whereas YOLOv12L topped the mAP@50:95 metric with 0.6622, indicating superior classification under detailed occlusion conditions. The analysis of model training dynamics revealed RF-DETRs rapid convergence, particularly in single-class scenarios where it plateaued at fewer than 10 epochs, underscoring the efficiency and adaptability of transformer-based architectures to dynamic visual data. These results confirm RF-DETRs suitability for accuracy-critical agricultural tasks, while YOLOv12 remains ideal for speed-sensitive deployments. Keywords: Object Detection, RF-DETR object detection model (Roboflow Detection Transformer), YOLOv12 object detection model, You Only Look Once, Transformers, CNNs, Greenfruit Detection, Deep Learning , Machine-Vision 1. Introduction As illustrated in Figure 1, the field of object detection over the past decade, propelled by breakthroughs in deep learning, have shifted from basic pattern recognition to sophisticated systems capable of complex image understanding. The object detection approaches can be branched into six primary methodologies as illustrated in Figure 1, each with unique strengths and applications in various domains of technology and automation. This evolution is vital for overcoming common visual recognition challenges in fields requiring high precision and adaptability, such as autonomous driving [1, 2], healthcare [3], security surveillance [4], and notably in agriculture [5, 6], where accurate and efficient object detection supports advancements like automated field monitoring [7] and robotic harvesting [8]. Corresponding Authors: Manoj Karkee and Ranjan Sapkota Email address: mk2684@cornell.edu, rs2672@cornell.edu (Manoj Karkee ) Preprint submitted to Elsevier Figure 1: Classification of object detection methodologies: Top features state-of-the-art CNN-based and Transformer-based methods, widely adopted; Vision Language Models are emerging. Also includes Hybrid, Sparse Coding, and Traditional Feature-based approaches. April 18, The six approaches depicted in Figure 1 are Convolutional Neural Networks (CNNs) [9] , Transformer-based models [10, 11], Vision Language Model-based approaches [12, 13], Hybrid models such as RetinaMask and EfficientDet [14, 15], Sparse Coding and Dictionary Learning models, and Traditional Feature-based approaches. Among them, CNNs, including the YOLO (You Only Look Once) series [16, 17] and R-CNN (Region-based CNN) family such as Mask R-CNN [18], have become staples in practical deployments due to their proficient handling of spatial hierarchies. Transformer-based models like DETR (Detection Transformer) such as dynamic DETR [19] and deformable DETR [20] utilize self-attention mechanisms to treat images as sequences of patches, which helps in integrating global context and eliminates the need for Non-Maximum Suppression (NMS) [21], streamlining postprocessing [22]. Vision Language Models (VLMs) such as CLIP (Contrastive Language-Image Pre-training) represent an emerging area that integrates textual and visual data, aiming to enhance robustness through multimodal learning, though their application in real-world scenarios, particularly in robotics and automation, is still developing. On the other hand, Hybrid models such as RetinaMask, Sparse Coding models like Online Dictionary Learning, and Traditional Feature-based methods such as Histogram of Oriented Gradients (HOG) are increasingly considered obsolete [23, 24]. These have been superseded by more advanced systems that provide not only greater accuracy but also the capability to perform in real-time, critical requirement for latency-sensitive operations such as those found in modern agricultural settings. As object detection continues to evolve, the focus remains on technologies that combine high precision with efficient processing, positioning CNN and Transformer-based models as the current state-of-the-art in the field. Among the six primary object detection approaches illustrated in Figure 1, CNN-based and Transformer-based models have emerged as the most widely adopted and actively developed over the past five years. These two paradigms now dominate both research and real-world applications due to their scalability, accuracy, and adaptability. This ongoing dominance has sparked competitive evolution between the two approaches, particularly with the release of powerful Transformer-based models such as RF-DETR, developed by Roboflow. RF-DETR integrates architectural innovations from Deformable DETR and LW-DETR, and utilizes DINOv2 backbone, offering superior global context modeling and domain adaptability. The model eliminates reliance on anchor boxes and Non-Maximum Suppression (NMS), supporting end-to-end training and realtime inference. With two variants, Base (29M) and Large (128M), RF-DETR offers scalability from edge deployment to high-performance scenarios. The model has been shown to outperform YOLOv11 and is the only model to surpass 60% mAP on the COCO dataset to date. Its performance on COCO and RF100-VL benchmarks is visualized in Figure 2a and 2b. However, despite its promise, RF-DETR has not yet been officially benchmarked against YOLOv12, the latest and most advanced model in the YOLO family. comparative evaluation is warranted, particularly since YOLOv12 builds upon the strengths Figure 2: CNN vs Transformer-based model performance comparison focusing on YOLOv12 (CNN-based) and RF-DETR (Transformer-based) architectures: (a) RF-DETR object detection model benchmark evaluation with YOLO11, YOLOv8 and other DETR-based object detection models ; (b)RF-DETR evaluation on the RF100-VL dataset, highlighting domain adaptability and edge deployment potential. ; and (c) Performance overview of recent CNN-based models, including YOLOv6 through YOLOv12, Gold-YOLO, RT-DETR, RT-DETRv2, and YOLOMS. b) RF-DETR benchmark results on the MS COCO dataset, surpassing 60% mAP of YOLOv11, YOLOv10, and Gold-YOLO RT-DETR, as illustrated in Figure 2c. 2 1.1. CNN-based Object Detection Approaches CNNs have been pivotal in advancing object detection, significantly since AlexNet catalyzed the field in 2012 [25]. These networks utilize hierarchical layers of convolutions, pooling, and nonlinear activations to effectively learn feature representations from images [26]. Unlike transformers that handle global relationships via attention mechanisms, CNNs are structured to excel at extracting local features, which is facilitated by their inherent inductive biases such as translation equivariance and the establishment of spatial hierarchies [27]. This fundamental architectural distinction makes CNNs particularly well-suited for scenarios demanding real-time processing and edge computing deployments, albeit with noted limitation in their ability to model global contextual information comprehensively [28, 29]. The progress in CNN architectures for object detection has been marked by several significant innovations as: R-CNN Family: This series began with R-CNN in 2014 [30], which utilized selective search to generate region proposals that were then processed by CNNs to extract features, achieving 53.3% mAP on the PASCAL VOC dataset but with high computational cost. Subsequent iterations, Fast R-CNN and Faster R-CNN, introduced ROI pooling and Region Proposal Networks (RPNs), respectively, enhancing the efficiency and speed of these models dramatically. Mask R-CNN: An extension of Faster R-CNN that includes branch for predicting segmentation masks on each Region of Interest (ROI), effectively handling instance segmentation with high precision level [18, 31]. YOLO Series: Starting with YOLOv1 [16], which reframed object detection as single regression problem from image pixels to bounding box coordinates and class probabilities, through to YOLOv12 [32], which introduced improvements like anchor-free detection and dynamic label assignment for enhanced accuracy and efficiency [33, 34, 35, 31]. SSD: This model combined multi-scale feature maps with default bounding boxes to perform detections, facilitating direct classification and localization from feature maps without needing separate region proposals [36]. RetinaNet: Known for tackling class imbalance with the focal loss function, which helps focus the model on hardto-classify examples by down-weighting the loss assigned to well-classified examples [37]. EfficientDet: This model utilized scaling method that systematically adjusts the depth, width, and resolution of the network, integrated with BiFPN for feature fusion across different scales, achieving high efficiency and accuracy [15]. processing, into visual recognition tasks [22]. Introduced by Facebook AI in 2020, DETR presents novel approach by treating object detection as direct set prediction problem, eliminating the need for traditional components like anchor boxes and complex post-processing steps such as Non-Maximum Suppression (NMS) [22]. At its core, DETR uses standard CNN backbone, typically ResNet-50, for initial feature extraction. This is followed by transformer that consists of an encoder and decoder where the encoder processes the spatial features across the image and the decoder uses learned object queries to predict the presence of objects along with their categories and bounding boxes, all in parallel. The key architectural variants of DETR have addressed its initial shortcomings such as slow convergence and high computational demands: Deformable DETR: Introduced to tackle the inefficiencies of the standard transformer attention mechanism, it employs deformable attention which focuses on small set of key sampling points around each reference point, significantly reducing the computational load and improving detection of small objects [20]. This variant leverages iterative bounding box refinement and multi-scale features to enhance accuracy and speed up training . RT-DETR: Developed for real-time applications, this variant from Baidu features hybrid encoder that merges CNN and transformer features to optimize both intra-scale interaction and cross-scale fusion, achieving impressive speeds on standard hardware. It introduces IoU-aware query selection, dynamically adjusting the decoding process based on predicted objectness scores [38, 39]. Co-DETR: Enhances training stability and performance by implementing dual supervision strategy that combines traditional one-to-many (like Faster R-CNN) and one-toone (like DETR) label matching [40]. This approach, supported by hierarchical attention mechanisms, significantly improves feature representation, especially in challenging conditions such as occlusions [41]. YOLOS: Stands out by adapting Vision Transformers (ViTs) directly for object detection without any CNNs [42]. It uses sequence of image patches (tokens) along with set of learnable detection tokens, demonstrating that transformers can effectively encode spatial relationships inherent in detection tasks [43]. OWL-ViT: Expands the applicability of transformers to open-vocabulary detection by integrating vision and language, using transformer decoder to align image features with text queries [44]. This model facilitates zero-shot detection, where the system can recognize objects it has never seen during training, described only by text [45, 46]. 1.2. Transformers-based Object Detection Approaches DETR have revolutionized object detection by integrating transformer architectures, traditionally used in natural language DINO (DETR with Improved Denoising Anchor boxes): Focuses on enhancing small object detection through novel training strategy that involves adding noise to 3 ground truth boxes and learning to predict corrective offsets, improving precision and robustness [47]. RF-DETR: Released by Roboflow, RF-DETR is real-time transformer-based object detection model that achieves 60.5 mAP at 25 FPS on an NVIDIA T4 GPU, outperforming models like YOLOv11 and LW-DETR on benchmarks such as COCO and RF100-VL [48]. Its architecture is designed for high-speed edge deployment and domain adaptability, with two variants: RF-DETR-Base (29M parameters) and RF-DETR-Large (128M parameters). 1.3. Objectives Despite significant advances in object detection, the performance of state-of-the-art models in complex, label-ambiguous agricultural environments remains underexplored. The recently released RF-DETR, Transformer-based real-time object detection model developed by Roboflow, has demonstrated remarkable performance by surpassing 60% mAP on the MS COCO dataset, the highest recorded for any Transformer-based detector to date. However, RF-DETRs performance has only been benchmarked against earlier versions of YOLO, including YOLOv11, and few other models like LW-DETR, leaving notable gap in comparative evaluation with YOLOv12, the most recent and advanced CNN-based detector from the YOLO family. This lack of direct comparison has created uncertainty regarding which model, RF-DETR or YOLOv12, offers better detection capabilities in real-world conditions, particularly under occlusion, camouflage, and ambiguous labeling. This study addresses this gap by conducting detailed evaluation of RF-DETR and YOLOv12 for the task of greenfruit detection in commercial apple orchards. Immature green apple fruitlets are critical for early yield estimation and thinning but are notoriously difficult to detect due to their small size, color similarity with the background, and frequent occlusion by foliage or other fruits. This visual complexity results in label ambiguity, making it difficult to determine whether fruitlets are fully visible, partially visible, or completely hidden conditions that challenge both manual annotation and automated detection. To assess the robustness of these two architectures, we developed custom dataset and evaluated both models using identical training protocols and hyperparameters. Performance was assessed in both single-class and multi-class detection tasks using key metrics: Precision, Recall, F1-Score, mAP@50, and mAP@50:95. We also measured inference speed and processing efficiency, aiming to provide clear, evidence-driven comparison of CNN-based versus Transformer-based object detection in precision agriculture. 2. Methods This experiment is performed in four steps, as depicted in Initially, real field images were collected from Figure 3a. commercial orchard under complex conditions, characterized by immature greenfruits camouflaged against green canopy, 4 presenting significant challenges for machine vision due to occlusions. Subsequently, these images were captured using robotic platform and machine vision camera, followed by preprocessing and preparation. In the third step, two deep learning models, RF-DETR and YOLOv12, were implemented using the same dataset, hyperparameters, and number of epochs. Finally, the performance of these models was evaluated in terms of their ability to detect single-class and multi-class greenfruit objects in this challenging orchard environment. 2.1. Study Site and Data Acquisition Data acquisition for this study was conducted in commercial orchard situated in Prosser, Washington State, USA, as illustrated in Figure 3b. The orchard was densely planted with Scifresh apple trees, commonly known as Jazz apples. The selection of this specific orchard was due to its complex environmental conditions characterized by the green color of immature fruitlets blending with the green canopy background, as depicted in Figure 3c. This similarity in color created challenging scenario for accurate image detection due to significant occlusions and visual confusion, typical in complex orchard scenes. Image collection was executed using sophisticated robotic platform that incorporated an Intel RGB-D camera, which was mounted on UR5e robotic arm, as depicted in Figure 3d. This setup enabled the precise capture of RGB images, specifically focusing on the immature Scifresh apple fruitlets. The imagery was collected in May 2024, just prior to the commencement of fruitlet thinning activities. The timing for the collection was carefully chosen based on continuous monitoring of the orchards developmental stages (before thinning, exactly during the fruitlet thinning week) and in consultation with local growers and orchard workers to ensure optimal data relevance for the study. The orchard, established in 2008, was methodically laid out with tree rows spaced 3 meters apart and an intra-row spacing of 1 meter. Throughout the course of this study, total of 857 images were captured utilizing an Intel RealSense D435i camera, as shown in Figure 3d. The selected camera is equipped with depth-sensing system that operates on active infrared (IR) stereo vision, complemented by an inertial measurement unit (IMU). This cameras depth sensor employs structured light technology, which utilizes pattern projector to induce disparities between stereo images captured by two IR cameras. The cameras 3D sensor boasts resolution of 1280 720 pixels, capable of capturing depth information up to distance of 10 meters. It supports frame rate of up to 90 frames per second (fps), and features horizontal field of view (HFOV) of 69.4 and vertical field of view (VFOV) of 42.5. Additionally, the integrated 6-axis IMU provides critical orientation data, significantly enhancing the alignment of depth data with the actual scene, thus improving the overall understanding and analysis of the captured images. This detailed and methodical approach to data collection was fundamental in addressing the visual complexities presented by the orchard environment. Figure 3: Overview of data collection setup and environment: a) Flow diagram showing the methodology of RF-DETR vs YOLOv12 comparision ; b) Map highlighting the study location in Prosser, Washington, USA ; c) of Scifresh apple trees, known as Jazz apples; d) The robotic platform used for image acquisition, featuring an Intel RGB-D camera mounted on UR5e robotic arm, capturing images of immature greenfruits in complex orchard environment. 2.2. Data Preprocessing and Preparation Following data collection, the acquired RGB images underwent systematic preprocessing and annotation pipeline to prepare them for deep learning model training and evaluation. Image annotation was performed manually using the Roboflow platform (Roboflow, Des Moines, Iowa), widely used tool for custom dataset generation in computer vision workflows. The dataset construction involved two labeling schemes: (i) 5 single-class dataset and (ii) multi-class dataset, both of which aimed to capture the inherent complexity of greenfruit detection under real-world orchard conditions. In the first scheme, all visible immature apples were annotated under single class labeled as greenfruit, regardless of their degree of visibility or occlusion. total of 857 high-resolution orchard images were uploaded and processed in Roboflow for this purpose. As illustrated in the center image of Figure 3e, this dataset captured wide range of greenfruit appearances, resulting in 4,125 individual object labels. The uniform labeling in this scheme was suitable for establishing baseline detection performance but did not capture the dynamics of visual challenges like partial occlusions or background blending. To explore these complexities more explicitly, second labeling scheme was developed to create multi-class dataset. In this case, each greenfruit was categorized into one of two classes: occluded greenfruit and non-occluded greenfruit. The classification criteria were based on the degree of visibility. Greenfruits with at least 90% of their surface area clearly visible, unobstructed by foliage, branches, or other fruits, were labeled as non-occluded. Conversely, any fruit partially hidden, whether by overlapping apples, intersecting leaves, or obstructing branches was labeled as occluded. This dynamic annotation approach is depicted in the rightmost image of Figure 3e. However, the labeling process was complicated by label ambiguity, critical issue in computer vision tasks, especially in natural environments. Label ambiguity refers to the uncertainty or subjectivity in assigning label due to unclear visual boundaries, overlapping objects, or inconsistent visibility. In this study, several practical instances of label ambiguity emerged. First, in cases where multiple greenfruits clustered tightly, it was often unclear whether one was partially occluding the other or if they were side-by-side. Second, some fruits appeared occluded due to lighting and shadows rather than actual physical obstruction, leading to inconsistent labeling across images. Third, foliage sometimes mimicked the texture and color of immature fruits, making it difficult to distinguish between the object of interest and the background. Fourth, partial occlusions at the edges of images often left annotators uncertain whether to classify the object as occluded or simply truncated due to the field of view. These examples underscore why greenfruit detection in real orchard environments is particularly prone to labeling inconsistencies. Although classification guidelines were applied rigorously, the complex interplay between object geometry, environmental texture, and visibility made complete objectivity difficult to achieve. Thus, the term label ambiguity is used to describe the datasets inherent subjectivity and the potential variability it introduces during model training and evaluation. 2.3. Training Object Detection Models 2.3.1. Training RF-DETR Object Detection Model RF-DETR is real-time, transformer-based object detection architecture optimized for both accuracy and efficiency 1 2. As illustrated in Figure 4a, RF-DETR builds upon the 1https://blog.roboflow.com/rf-detr/ 2https://github.com/roboflow/rf-detr foundations of Deformable DETR and LW-DETR, integrating pre-trained DINOv2 vision transformer as its backbone. This backbone enhances cross-domain generalization through self-supervised learning, making the model highly adaptable to domain-specific challenges like greenfruit detection in agricultural environments. key innovation of RF-DETR is its ability to eliminate traditional object detection components such as anchor boxes and NMS. Instead, it uses transformer-based encoder-decoder architecture with deformable cross-attention to selectively attend to spatially relevant features, improving detection under occlusion, clutter, and camouflage. Unlike traditional DETR variants, RF-DETR employs single-scale feature extraction strategy to reduce computational overhead, thus enabling faster inference without compromising accuracy. Two variants of the model are available: RF-DETR-Base (29 million parameters) and RF-DETR-Large (128 million parameters). In this study, the **RF-DETR-Base** model was selected due to its balance between computational efficiency and high detection accuracy, making it suitable for real-time processing in field robotics. The RF-DETR-Base model achieves mAP of 53.3 on the COCO benchmark and 86.7 mAP@50 on the RF100-VL dataset, positioning it as one of the few models to exceed 60% mAP@50:95 in real-time settings. Training followed the official Roboflow implementation. The model was initialized with DINOv2-pretrained weights and trained using the AdamW optimizer with learning rate of 1e4 and batch size of 8 for 300 epochs. The training leveraged hybrid encoder optimizations inspired by RT-DETR and deformable attention mechanisms. Loss functions included crossentropy for classification and combination of L1 and GIoU losses for bounding box regression. Additionally, contrastive denoising training was employed to improve detection robustness for partially visible and small objects. RF-DETR also adopted collaborative label assignments for stability in ambiguous annotation conditions, as well as multi-resolution input support (6401280 px), allowing latency-accuracy trade-offs without retraining. This configuration made RF-DETR-Base powerful and efficient model for detecting occluded and camouflaged immature greenfruits in complex orchard environments. 2.3.2. Training YOLOv12 Object Detection Model YOLOv12 represents transformative leap in CNN-based object detection, merging the efficiency of traditional convolutional architectures with attention-inspired mechanisms to address modern computer vision demands [32]. Departing from previous YOLO iterations, the model introduces R-ELAN (Residual Efficient Layer Aggregation Network) as its core backbone as depicted in Figure 4b, combining residual connections with multi-scale feature fusion to resolve gradient bottlenecks while enhancing feature reuse across network depths. novel 77 separable convolution layer replaces standard 33 kernels, preserving spatial context with 60% fewer parameters than conventional large-kernel convolutions while implicitly encoding positional relationships, effectively circumventing the need for explicit positional embeddings used in 6 Figure 4: (a) RF-DETR Architecture diagram for object detection ; (b) YOLOv12 Architecture Diagram for object detection transformer-based detectors. The neck architecture integrates FlashAttention-optimized area attention, dividing feature maps into four horizontal/vertical regions for localized processing without sacrificing global context, achieving 40% reduced memory overhead compared to standard self-attention implementations. These innovations enable state-of-the-art accuracy while maintaining real-time performance, with the YOLOv12S variant outperforming RT-DETR-R18 in both speed (1.2 faster) and precision (62.1 vs 59.3 COCO mAP). The architecture further supports multi-task learning through unified prediction pathways, allowing simultaneous object detection, oriented bounding box (OBB) estimation, and instance segmentation via specialized headsa first for the YOLO series. Hardwareaware optimizations ensure sub-10ms inference on edge devices, with the 12n variant (2.1M parameters) achieving 9.8ms latency while maintaining robust detection of sub-50px objects through lightweight MLP ratios (1.2-2.0 vs traditional 4.0) in task-specific heads. YOLOv12s architectural refinements optimize convolutional operations for contemporary hardware while introducing transformer-like capabilities through innovative attention hybrids. The area attention mechanism processes feature map segments independently through FlashAttentions memoryefficient algorithms, enabling precise region-specific focus without the computational burden of full self-attention. This design philosophy extends to the models scalability, offering four configurations (12n/12s/12m/12x) ranging from 2.1M to 42M parameters to accommodate edge deployments (Jetson Nano) to cloud clusters (A100 GPUs). Unlike previous YOLO versions limited to axis-aligned detection, YOLOv12 introduces an OBB head with angle prediction capabilities, critical for aerial imagery and document analysis. Training stability is enhanced through block-level residual scaling in R-ELAN, which prevents feature degradation in deep networks while maintaining the single-pass efficiency that defines the YOLO series. Benchmark results demonstrate 4-8% higher mAP than YOLOv11 7 across all variants, with the 12x model achieving 68.9 mAP on COCO, surpassing similarly sized transformer hybrids like DINO-DETR in small object detection tasks. The architectures separation of feature extraction (backbone) and attentiondriven refinement (neck) allows targeted optimization, enabling the 12s variant to process 4K video streams at 45 FPS on an NVIDIA T4 GPU. By integrating the parameter efficiency of CNNs with the contextual awareness of attention mechanisms, YOLOv12 establishes new standard for real-time vision systems, particularly in industrial applications requiring simultaneous detection, segmentation, and geometric prediction under strict latency constraints. standardized metrics. Both models underwent training and testing under uniform conditions utilizing the same datasets, number of training epochs, learning rates, optimizers, and batch sizes to ensure an equitable comparison between the CNNbased YOLOv12 and Transformer-based RF-DETR architectures. Detection Evaluation Metrics The evaluation metrics employed were Precision, Recall, F1Score, mean Average Precision (mAP@50 and mAP@50:95), and mean Intersection over Union (mIoU). These metrics quantify the performance based on the interaction between predicted bounding boxes and ground truth annotations: 2.4. Training Methodology The training procedures for both deep learning models RFDETR and YOLOv12 were carried out under identical experimental settings to ensure fair and rigorous comparison. All training was conducted on workstation equipped with an Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz (10 cores, 20 threads), running Ubuntu 24.04.1, and supported by an NVIDIA RTX A5000 GPU with 24 GB VRAM. This highperformance hardware configuration ensured sufficient computational resources for training large-scale object detection models. The RF-DETR object detection model, specifically the Base variant, was trained for 50 epochs on the single-class greenfruit dataset and 100 epochs on the multi-class dataset. Notably, RF-DETR demonstrated rapid convergence in the single-class setting, with performance plateauing at under 20 epochs, highlighting the models efficient learning dynamics and its suitability for low-epoch training regimes. YOLOv12 models, including YOLOv12X, YOLOv12L, and YOLOv12N, were each trained for 100 epochs for both single-class and multi-class datasets to ensure convergence and optimal generalization. RF-DETR was implemented in PyTorch using Roboflows rf-detr framework, which integrates Deformable DETR architecture with pre-trained DINOv2 backbone to enhance global context modeling and cross-domain adaptability. The YOLOv12 models were trained using the official Ultralytics PyTorch framework, optimized for fast detection and efficient edge deployment. For both models, the input image resolution was standardized to 640640 pixels, resolution commonly adopted in orchard-based object detection tasks. The models training was performed using FP32 precision with batch size of approximately 16 images per iteration. The software environment included CUDA 11.7+ and cuDNN 8.4+, ensuring full compatibility with GPU acceleration and deep learning libraries. This standardized setup enabled reliable comparative evaluation of model accuracy, convergence behavior, and training efficiency across both transformerand CNN-based architectures. 2.5. Performance Evaluation To rigorously assess the capabilities of RF-DETR and YOLOv12 in identifying greenfruits in complex orchard environment, comprehensive evaluation was conducted using True Positive (TP): predicted bounding box correctly identifies ground truth fruit with an Intersection over Union (IoU) the defined threshold (commonly 0.50). False Positive (FP): predicted bounding box either insufficiently overlaps with any ground truth box (IoU < 0.50) or erroneously marks non-existent object. False Negative (FN): real fruit is overlooked by the detection model, with no corresponding predicted box sufficiently overlapping it. The metrics were calculated as follows: Precision = P + FP Recall = T + FN F1-Score = 2 Precision Recall Precision + Recall (1) (2) (3) Precision assesses the accuracy of the detected greenfruits, Recall gauges the completeness of the detection, and F1-Score balances both aspects to provide single measure of model efficacy. Intersection over Union (IoU) and Mean IoU (mIoU) IoU = Area of Overlap Area of Union = P + FP + FN (4) IoU quantifies the exactness of the overlap between the predicted and actual bounding boxes, crucial in scenarios with densely packed and overlapping fruits. mIoU averages the IoU across all detections to give holistic measure of spatial accuracy. mAP@50 and mAP@50:95 mAP@50 = 1 N(cid:88) i=1 APi(IoU 0.50) mAP@50:95 = 1 10 0.95(cid:88) t=0.50 mAPt (5) (6) 8 mAP@50 measures the mean Average Precision at an IoU threshold of 0.50, commonly used to evaluate detection effectiveness. mAP@50:95 averages the AP at ten IoU thresholds from 0.50 to 0.95 (in increments of 0.05), offering rigorous assessment of models precision across range of criteria from loose to strict overlaps, reflecting the models robustness in both precise and approximate detection scenarios. Application to Our Dataset For the single-class detection task, all greenfruits were uniformly considered, while the multi-class task also evaluated the accuracy of classifying fruits as occluded or non-occluded. Misclassifications of occlusion status were considered false positives, and undetected fruits, particularly those obscured by occlusion, were counted as false negatives. 3. Results The results for single-class and multi-class greenfruit detection using RF-DETR and YOLOv12 are presented to evaluate their performance in detecting green apples in complex orchard environments. Figure 5 displays three examples that highlight how each model performs in single-class detection scenarios, illustrating their efficacy in challenging conditions characterized by dense foliage and partial occlusions. Similarly, Figure 6 showcases three examples for multi-class detection, focusing on the models ability to handle label ambiguity effectively. Each example includes the original RGB image collected in the orchard (left), detection output from RF-DETR (middle), and detection output from YOLOv12 (right). Key regions of interest are highlighted with yellow dotted circles, focusing on areas where fruitlets were either clustered, camouflaged, or heavily occluded. In Figure 5a, three immature green apples appeared closely clustered within dense canopy, with significant partial occlusions caused by overlapping leaves. The original image, shown on the left, presented challenging scenario due to low fruit-background contrast and complex foliage structure. As illustrated in the middle image of Figure 5a, RFDETR successfully detected all three greenfruit instances, correctly bounding each fruit despite their partial visibility. In contrast, YOLOv12, shown on the right, detected only two out of the three apples and failed to identify the third fruit, which was most heavily occluded. This result highlighted RF-DETRs superior capability in handling complex spatial relationships and occlusions. Figure 5b provided another challenging condition where single green apple within the yellow dotted circle was camouflaged due to its visual similarity with the surrounding canopy. Despite the low contrast between the fruit and the background, RF-DETR accurately identified the greenfruit, as shown in the middle figure. Conversely, YOLOv12 failed to detect this fruit, indicating its limitations in distinguishing camouflaged targets in homogeneous backgrounds. In Figure 5c, different scenario was examined where only small portion (approximately 10%) of the fruits calyx was visible due to heavy occlusion by leaf and low ambient lighting. The original RGB image (left) demonstrated minimal visible surface area of the fruit. Remarkably, RF-DETR still managed to detect the partially exposed fruit in the middle image, whereas YOLOv12 again failed to register the object in its detection output. These examples consistently demonstrated RF-DETRs higher sensitivity and robustness in single-class greenfruit detection, especially under conditions of occlusion, camouflage, and low visibility, compared to the CNN-based YOLOv12 model. Figure 6 presents qualitative comparisons between RFDETR and YOLOv12 for multi-class greenfruit detection, where fruitlets were classified as either occluded or nonoccluded. This evaluation highlights the models performance in handling label ambiguitysituations where visibility is unclear due to clustering, occlusion, or edge truncation. Likewise, in Figure 6a, dense cluster of greenfruits appears near the image edge, creating highly ambiguous scene. As shown in the rightmost image, YOLOv12 detected 7 greenfruit instances in this region. However, ground truth annotation confirmed that only 5 greenfruits were actually present. YOLOv12 misclassified background textures or overlapping canopy feaIn tures as non-occluded apples, resulting in false positives. contrast, RF-DETR, shown in the middle image, correctly detected the 5 actual greenfruits but missed classifying them into occluded/non-occluded categories with high certainty. In this example, YOLOv12 appeared visually more active but less accurate, while RF-DETR provided precise detection with lower misclassifications. Furthermore, in Figure 6 b, yellow circles in the original image (left) highlight true greenfruits. RF-DETR detected 12 apples, including an occluded apple at the bottom of the frame, which was correctly labeled occluded (middle). YOLOv12 detected 11 apples but incorrectly labeled the bottom occluded apple as non-occluded (right), suggesting that RF-DETR object detection model performed better in differentiating occlusion classes, likely due to its global attention modeling. Likewise, Figure 6c presents challenging low-visibility case, where only 10% of greenfruit is visible beneath leaf cover (indicated by blue arrow). RF-DETR successfully detected and classified it as occluded (middle), while YOLOv12 failed to detect the fruit at all (right). This reinforces RFDETRs strength in handling extreme occlusion. 3.1. Evaluation of Precision, Recall and F1-Score Among all the models evaluated, YOLOv12N achieved the highest performance in terms of recall (0.8901) and the F1 score (0.8784) for single-class greenfruit detection, indicating its strong ability to detect almost all greenfruit instances while maintaining balanced precision. However, in terms of precision, YOLOv12L outperformed all other configurations of YOLOv12 and RF-DETR object detection model, achieving top value of 0.8892 in single-class detection. This demonstrates the superior ability of YOLOv12L in reducing false positives and making accurate predictions. Detailed precision, recall, and F1 metrics for all models and detection types are presented in Table 1. 9 Figure 5: Visual comparison of single-class greenfruit detection using RF-DETR and YOLOv12 in complex orchard scenes. a) Three clustered greenfruits partially occluded by dense canopy; RF-DETR detected all, YOLOv12 missed one. b) camouflaged greenfruit blending into the canopy; RF-DETR correctly detected it, YOLOv12 failed. c) heavily occluded greenfruit with only the calyx visible under low light; RF-DETR identified it, YOLOv12 missed detection. Table 1: Comparative analysis of Precision, Recall, and F1 Score for single-class and multi-class greenfruit detection using RF-DETR (Transformer-based) and YOLOv12 (CNN-based) object detection algorithms. The table presents model performance across different YOLOv12 configurations (X, L, N) and highlights their effectiveness in detecting greenfruits under varying complexity and class conditions in orchard environments. Models Single-Class Multi-Class Precision Recall F1 Score Precision Recall F1 Score RF-DETR YOLOv12X YOLOv12L YOLOv12N 0.8663 0.8797 0.8892 0.8671 0.8828 0.8595 0.8631 0.8901 0.8744 0.8694 0.8759 0.8784 0.7652 0.6986 0.7692 0.7569 0.8109 0.8261 0.7827 0. 0.7874 0.7570 0.7759 0.7487 3.2. Analysis of Mean Average Precision (mAP) For single-class greenfruit detection, RF-DETR outperformed all other models with the highest mAP@50 of 0.9464, indicating its superior ability to accurately detect and localize greenfruits with sufficient overlap. Furthermore, RF-DETR achieved mAP@50:95 of 0.7433, the second-highest among the tested models. Although YOLOv12N achieved slightly higher mAP@50:95 of 0.7620, RF-DETRs consistently higher mAP@50 suggests more reliable performance in practical orchard detection scenarios, especially when bounding box precision at the 50% threshold is critical. In the multi-class detection scenario, where greenfruits were labeled as either occluded or 10 Figure 6: Visual comparison of multi-class greenfruit detection by RF-DETR and YOLOv12 under label ambiguity. (a) dense fruit cluster at the image edge; YOLOv12 over-detected with false positives, while RF-DETR correctly detected 5 true greenfruits. (b) An occluded apple at the bottom; RF-DETR correctly labeled it as occluded, while YOLOv12 misclassified it as non-occluded. (c) highly occluded fruit with only 10% (approx) visibility; RF-DETR detected it as occluded, whereas YOLOv12 missed the detection entirely. non-occluded, YOLOv12L achieved the highest mAP@50:95 of 0.6622, slightly outperforming YOLOv12X and RF-DETR object detection, which achieved 0.6609 and 0.6530 respectively. This suggests that YOLOv12L was marginally better in maintaining detection consistency across varying degrees of overlap under label ambiguity. However, RF-DETR object detection model achieved the highest mAP@50 of 0.8298 in the multi-class setting, confirming its strength in confidently detecting objects with at least 50% spatial alignment. These findings indicate that RF-DETR excels in spatially accurate detections, particularly for clearly visible fruits, while YOLOv12L performs slightly better under complex classification scenarios involving occlusion. The complete visualization of these metrics is presented in Figure 7, where Figure 7a shows the mAP@50 for single-class detection, and Figure 7b illustrates mAP@50 and mAP@50:95 for multi-class detection. 3.3. Training Dynamics and Model Convergence Analysis Figure 8 provides detailed visualization of the mean Average Precision (mAP@50) against the number of training epochs for both RF-DETR and YOLOv12X models, shedding light on their learning efficiency and stability during the training In Figure 8a, which tracks performance for singlephase. class greenfruit detection, RF-DETR, transformer-based object detection model, demonstrates an impressive early convergence, plateauing before 10 epochs. This rapid stabilization underscores RF-DETRs swift adaptability to complex orchard scenes, significant advance over traditional CNN-based models like YOLOv12X. Similarly, Figure 8b illustrates the training progression for multi-class detection scenarios. Here, RF-DETR also shows superior convergence, reaching stability at around 20 epochs, far sooner than its CNN counterpart, which continues to seek equilibrium. This faster convergence of RF-DETR in both single 11 Figure 7: Mean Average Precision (mAP) comparison for greenfruit detection using RF-DETR and YOLOv12 object detection models: a) mAP@50 for single-class detection. b) mAP@50 and mAP@50:95 for multi-class detection and multi-class settings is emblematic of the inherent strengths of transformer technology in handling dynamic and visually cluttered environments efficiently. These following five observations highlight several key advantages of employing transformer-based models like RFDETR in object detection tasks: 1. Accelerated Learning Curve: RF-DETRs ability to reach peak performance quickly reduces the computational resources and time required for training, enhancing productivity and reducing operational costs. curacy over time, indicating robustness against overfitting and the ability to generalize well from limited epoch training. 3. Adaptability: RF-DETRs architecture is evidently wellsuited for complex detection environments, such as those in precision agriculture, where occlusions and varying object appearances prevail. 4. Efficient Resource Utilization: By converging quickly, RF-DETR maximizes the utility of computational resources, allowing for more tasks to be performed in the same computational budget. 2. Stable Performance: The model maintains consistent ac5. Edge Deployment: The models quick adaptation and sta12 Figure 8: Training Dynamics and Model Convergence Analysis: mAP@50 vs. Epoch Curves for Object Detection Models. (a) Single-class greenfruit detection showing the performance trajectory of RF-DETR and YOLOv12X models over training epochs. (b) Multi-class greenfruit detection comparing the convergence patterns of both models throughout the training period ble performance make it ideal for deployment in edge devices where computational resources and power are limited. 4. Discussion The progression of greenfruit detection technologies is closely aligned with recent advancements in computer vision, where each new model iteration introduces more nuanced capabilities, especially in complex agricultural settings. Notable contributions include those by [31, 49], which provided comparative analysis of YOLOv11 and YOLOv8, focusing on their efficacy in segmenting occluded and non-occluded immature green fruits. Similarly, [50] explored size estimation techniques using YOLOv8 combined with geometric shape fitting on 3D point cloud data, aiming to enhance yield predictions and crop management decisions. These studies underscore the ongoing efforts to refine the accuracy and efficiency of detection systems in variable orchard environments. In this context, our study leverages the RF-DETR model, which has set new benchmarks in detection performance. RF-DETRs transformer-based architecture has achieved mAP@50 of 0.9464, surpassing YOLOv12 and demonstrating superior spatial detection accuracy, particularly under conditions of partial visibility and camouflage [51]. This models efficiency is further highlighted by its rapid convergence during training, indicating significant advancement over traditional CNN-based models. The integration of Vision-Language Models (VLMs) and open-vocabulary detection also represents pivotal shift towards more dynamic and adaptable detection systems. These technologies, as reviewed in [52, 53], allow for the identification of broader range of fruit types and characteristics without retraining. This adaptability is crucial for managing 13 the diverse conditions typical of agricultural settings, where fruit appearances and environmental factors such as lighting and occlusion vary considerably. the application of multimodal learning approaches that incorporate various sensory data types promises to resolve longstanding challenges such as camouflage and label ambiguity. The exploration of semi-supervised and few-shot learning paradigms could reduce reliance on extensive labeled datasets, facilitating quicker adaptation to new orchard environments [54]. Furthermore, the deployment of lightweight transformer variants and efficient VLMs for realtime field applications will be instrumental. These advancements will enable the development of mobile or edge-based systems that offer real-time analytics, crucial for immediate agricultural decision-making [55]. Continued advancements in these areas will undoubtedly forge detection systems that are not only highly accurate but also capable of semantic and contextual understanding. Such systems will drive the next wave of innovations in precision agriculture, ensuring that detection technologies are not only effective but also robust and adaptable to the complex dynamics of natural orchard environments. 5. Conclusion This study provided an in-depth evaluation of RF-DETR (Transformer-based) and YOLOv12 (CNN-based) object detection models for detecting greenfruits in commercial orchards with complex visual environments. The research process included gathering real-world images, preparing datasets with occlusion-based labels for both single-class and multi-class detection, and assessing the models under standardized conditions. The comparison was based on precision, recall, F1-score, and mean average precision (mAP@50 and mAP@50:95). The analysis extended to training dynamics, revealing that RFDETR demonstrated quicker convergence, achieving stable performance in fewer epochs compared to YOLOv12. This insight highlights RF-DETRs effectiveness in adapting to the variable conditions of orchard environments while maintaining accuracy across extensive training phases. Key Findings: Single-Class Detection: RF-DETR object detection model showcased superior performance with the highest mAP@50 of 0.9464, effectively localizing and detecting greenfruits amidst complex backgrounds. While YOLOv12N achieved the highest mAP@50:95 of 0.7620, RF-DETR remained consistently more accurate in cluttered and occluded scenarios. Multi-Class Detection: RF-DETR excelled in distinguishing between occluded and non-occluded fruits, registering the highest mAP@50 of 0.8298. YOLOv12L performed marginally better in mAP@50:95 with 0.6622, showcasing enhanced classification accuracy in detailed occlusion conditions. Model Training Dynamics and Convergence: The RFDETR object detection model was notable for its rapid training convergence, particularly in single-class scenarios where it plateaued at under 10 epochs, demonstrating both the efficiency and robustness of transformer-based architectures in handling dynamic visual data. Acknowledgment This research is funded by the National Science Foundation and the United States Department of Agriculture, National Institute of Food and Agriculture through the AI Institute for Agriculture Program (Award No.AWD003473). We extend our heartfelt gratitude to Zhichao Meng, Astrid Wimmer, Randall Cason, Diego Lopez, Giulio Diracca for the data preparation; Martin Churuvija and Priyanka Upadhyaya for their invaluable efforts in logistical support during the data collection throughout this project. Special thanks to Dave Allan for granting commercial orchard access for this research experiment. Declarations The authors declare no conflicts of interest. References [1] M. Masmoudi, H. Ghazzai, M. Frikha, Y. Massoud, Object detection learning techniques for autonomous vehicle applications, in: 2019 IEEE international conference on vehicular electronics and safety (ICVES), IEEE, 2019, pp. 15. [2] M. Hnewa, H. Radha, Object detection under rainy conditions for autonomous vehicles: review of state-of-the-art and emerging techniques, IEEE Signal Processing Magazine 38 (2020) 5367. [3] R. Elakkiya, V. Subramaniyaswamy, V. Vijayakumar, A. Mahanti, Cervical cancer diagnostics healthcare system using hybrid object detection adversarial networks, IEEE Journal of Biomedical and Health Informatics 26 (2021) 14641471. [4] P. K. Mishra, G. Saroha, study on video surveillance system for object detection and tracking, in: 2016 3rd international conference on computing for sustainable global development (INDIACom), IEEE, 2016, pp. 221226. [5] C. M. Badgujar, A. Poulose, H. Gan, Agricultural object detection with you only look once (yolo) algorithm: bibliometric and systematic literature review, Computers and Electronics in Agriculture 223 (2024) 109090. [6] I. Sa, Z. Ge, F. Dayoub, B. Upcroft, T. Perez, C. McCool, Deepfruits: fruit detection system using deep neural networks, sensors 16 (2016) 1222. [7] P. Singh, R. Krishnamurthi, Iot-based real-time object detection system for crop protection and agriculture field security, Journal of Real-Time Image Processing 21 (2024) 106. [8] L. Yang, T. Noguchi, Y. Hoshino, Development of pumpkin fruits pickand-place robot using an rgb-d camera and yolo based object detection ai model, Computers and Electronics in Agriculture 227 (2024) 109625. [9] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang, G. Wang, J. Cai, et al., Recent advances in convolutional neural networks, Pattern recognition 77 (2018) 354377. [10] D. Nimma, Z. Zhou, intelligent patch-based pyramid vision transformers for object detection and classification, International Journal of Machine Learning and Cybernetics 15 (2024) 17671778. Intelpvt: [11] H. Liu, Y. Zhan, J. Sun, Q. Mao, T. Wu, transformer-based model with feature compensation and local information enhancement for end-toend pest detection, Computers and Electronics in Agriculture 231 (2025) 109920. [12] Y. Zang, W. Li, J. Han, K. Zhou, C. C. Loy, Contextual object detection with multimodal large language models, International Journal of Computer Vision 133 (2025) 825843. 14 [13] S. Fu, Q. Yang, Q. Mo, J. Yan, X. Wei, J. Meng, X. Xie, W.-S. Zheng, Llmdet: Learning strong open-vocabulary object detectors under the supervision of large language models, arXiv preprint arXiv:2501.18954 (2025). [14] C.-Y. Fu, M. Shvets, A. C. Berg, Retinamask: Learning to predict masks improves state-of-the-art single-shot detection for free, arXiv preprint arXiv:1901.03353 (2019). [15] M. Tan, R. Pang, Q. V. Le, Efficientdet: Scalable and efficient object in: Proceedings of the IEEE/CVF conference on computer detection, vision and pattern recognition, 2020, pp. 1078110790. [16] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look once: Unified, real-time object detection, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779788. [17] R. Sapkota, Z. Meng, M. Churuvija, X. Du, Z. Ma, M. Karkee, Comprehensive performance evaluation of yolo11, yolov10, yolov9 and yolov8 on detecting and counting fruitlet in complex orchard environments, arXiv preprint arXiv:2407.12040 (2024). [18] K. He, G. Gkioxari, P. Dollar, R. Girshick, Mask r-cnn, in: Proceedings of the IEEE international conference on computer vision, 2017, pp. 2961 2969. [19] X. Dai, Y. Chen, J. Yang, P. Zhang, L. Yuan, L. Zhang, Dynamic detr: in: Proceedings of End-to-end object detection with dynamic attention, the IEEE/CVF international conference on computer vision, 2021, pp. 29882997. [20] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, J. Dai, Deformable detr: Deformable transformers for end-to-end object detection, arXiv preprint arXiv:2010.04159 (2020). [21] J. Hosang, R. Benenson, B. Schiele, Learning non-maximum suppresin: Proceedings of the IEEE conference on computer vision and sion, pattern recognition, 2017, pp. 45074515. [22] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, S. Zagoruyko, End-to-end object detection with transformers, in: European conference on computer vision, Springer, 2020, pp. 213229. [23] X. Ren, D. Ramanan, Histograms of sparse codes for object detection, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2013, pp. 32463253. [24] Y. Xie, W. Zhang, C. Li, S. Lin, Y. Qu, Y. Zhang, Discriminative object tracking via sparse representation and online dictionary learning, IEEE transactions on cybernetics 44 (2013) 539553. [25] K. Oshea, R. Nash, An introduction to convolutional neural networks, arXiv preprint arXiv:1511.08458 (2015). [26] D. Soydaner, Attention mechanism in neural networks: where it comes and where it goes, Neural Computing and Applications 34 (2022) 13371 13385. [27] A. Khan, Z. Rauf, A. Sohail, A. R. Khan, H. Asif, A. Asif, U. Farooq, survey of the vision transformers and their cnn-transformer based variants, Artificial Intelligence Review 56 (2023) 29172970. [28] L. Alzubaidi, J. Zhang, A. J. Humaidi, A. Al-Dujaili, Y. Duan, O. AlShamma, J. Santamara, M. A. Fadhel, M. Al-Amidie, L. Farhan, Review of deep learning: concepts, cnn architectures, challenges, applications, future directions, Journal of big Data 8 (2021) 174. [29] S. Chen, Y. Liu, X. Gao, Z. Han, Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices, in: Chinese conference on biometric recognition, Springer, 2018, pp. 428438. [30] R. Girshick, J. Donahue, T. Darrell, J. Malik, Rich feature hierarchies for accurate object detection and semantic segmentation, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 580587. [31] R. Sapkota, M. Karkee, Comparing yolov11 and yolov8 for instance segmentation of occluded and non-occluded immature green fruits in complex orchard environment, arXiv preprint arXiv:2410.19869 (2024). [32] Y. Tian, Q. Ye, D. Doermann, Yolov12: Attention-centric real-time object detectors, arXiv preprint arXiv:2502.12524 (2025). [33] R. Sapkota, R. Qureshi, M. F. Calero, C. Badjugar, U. Nepal, A. Poulose, P. Zeno, U. B. P. Vaddevolu, S. Khan, M. Shoman, et al., Yolov10 to its genesis: decadal and comprehensive review of the you only look once (yolo) series, arXiv preprint arXiv:2406.19407 (2024). [34] R. Sapkota, M. Karkee, Improved yolov12 with llm-generated synthetic data for enhanced apple detection and benchmarking against yolov11 and yolov10, arXiv preprint arXiv:2503.00057 (2025). [35] Z. Meng, X. Du, R. Sapkota, Z. Ma, H. Cheng, Yolov10-pose and yolov915 pose: Real-time strawberry stalk pose detection models, Computers in Industry 165 (2025) 104231. [36] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, A. C. Berg, Ssd: Single shot multibox detector, in: Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, Springer, 2016, pp. 2137. [37] T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar, Focal loss for dense object detection, in: Proceedings of the IEEE international conference on computer vision, 2017, pp. 29802988. [38] D. Wang, Z. Li, X. Du, Z. Ma, X. Liu, Farmland obstacle detection from the perspective of uavs based on non-local deformable detr, Agriculture 12 (2022) 1983. [39] H. Lin, J. Liu, X. Li, L. Wei, Y. Liu, B. Han, Z. Wu, Dcea: Detr with concentrated deformable attention for end-to-end ship detection in sar images, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (2024). [40] Z. Zong, G. Song, Y. Liu, Detrs with collaborative hybrid assignments in: Proceedings of the IEEE/CVF international conference on training, computer vision, 2023, pp. 67486758. [41] Y. Zhang, Y. Wu, H. Xu, Y. Xie, Y. Zhang, Improved co-detr with dropkey and its application to hot work detection, Concurrency and Computation: Practice and Experience 37 (2025) e70020. [42] Y. Fang, B. Liao, X. Wang, J. Fang, J. Qi, R. Wu, J. Niu, W. Liu, You only look at one sequence: Rethinking transformer in vision through object detection, Advances in Neural Information Processing Systems 34 (2021) 2618326197. [43] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, J. Chen, Detrs beat yolos on real-time object detection, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 1696516974. [44] M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Mahendran, A. Arnab, M. Dehghani, Z. Shen, et al., Simple open-vocabulary object detection, in: European conference on computer vision, Springer, 2022, pp. 728755. [45] G. Heigold, M. Minderer, A. Gritsenko, A. Bewley, D. Keysers, M. Lucic, F. Yu, T. Kipf, Video owl-vit: Temporally-consistent open-world localization in video, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 1380213811. [46] B. Wang, K. Huang, B. Li, Y. Yan, L. Zhang, H. Lu, Y. He, Effowt: Transfer visual language models to open-world tracking efficiently and effectively, arXiv preprint arXiv:2504.05141 (2025). [47] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, H.-Y. Shum, Dino: Detr with improved denoising anchor boxes for end-to-end object detection, arXiv preprint arXiv:2203.03605 (2022). [48] P. Robicheaux, M. Popov, A. Madan, I. Robinson, J. Nelson, D. Ramanan, N. Peri, Roboflow100-vl: multi-domain object detection benchmark for vision-language models, Roboflow (2025). [49] R. Sapkota, A. Paudel, M. Karkee, Zero-shot automatic annotation and instance segmentation using llm-generated datasets: Eliminating field imaging and manual annotation for deep learning model development, arXiv preprint arXiv:2411.11285 (2024). [50] R. Sapkota, D. Ahmed, M. Churuvija, M. Karkee, Immature green apple detection and sizing in commercial orchards using yolov8 and shape fitting techniques, IEEE Access 12 (2024) 4343643452. [51] R. Sapkota, M. Karkee, Yolo11 and vision transformers based 3d pose estimation of immature green fruits in commercial apple orchards for robotic thinning, arXiv preprint arXiv:2410.19846 (2024). [52] Q. Liu, H. Meng, R. Zhao, X. Ma, T. Zhang, W. Jia, Green apple detector based on optimized deformable detection transformer, Agriculture 15 (2024) 75. [53] R. Sapkota, S. Raza, M. Shoman, A. Paudel, M. Karkee, Multimodal large language models for image, text, and speech data augmentation: survey, arXiv preprint arXiv:2501.18648 (2025). [54] Q. Liu, J. Lv, C. Zhang, Mae-yolov8-based small object detection of green crisp plum in real complex orchard environments, Computers and Electronics in Agriculture 226 (2024) 109458. [55] J. Lv, Z. Wu, P. Zhou, J. Huang, G. Liu, Y. Gu, H. Rong, L. Zou, Fcaeyolov8n: target detection method for immature grape clusters, New Zealand Journal of Crop and Horticultural Science (2024) 119."
        }
    ],
    "affiliations": [
        "Biological & Environmental Engineering, Cornell University, Ithaca, 14850, NY, USA",
        "Department of Biological and Agricultural Engineering, Kansas State University, Manhattan, 66502, KS, USA"
    ]
}