{
    "paper_title": "Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior",
    "authors": [
        "Sheng Wang",
        "Ruiming Wu",
        "Charles Herndon",
        "Yihang Liu",
        "Shunsuke Koga",
        "Jeanne Shen",
        "Zhi Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diagnosing a whole-slide image is an interactive, multi-stage process involving changes in magnification and movement between fields. Although recent pathology foundation models are strong, practical agentic systems that decide what field to examine next, adjust magnification, and deliver explainable diagnoses are still lacking. The blocker is data: scalable, clinically aligned supervision of expert viewing behavior that is tacit and experience-based, not written in textbooks or online, and therefore absent from large language model training. We introduce the AI Session Recorder, which works with standard WSI viewers to unobtrusively record routine navigation and convert the viewer logs into standardized behavioral commands (inspect or peek at discrete magnifications) and bounding boxes. A lightweight human-in-the-loop review turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired \"where to look\" and \"why it matters\" supervision produced at roughly six times lower labeling time. Using this behavioral data, we build Pathologist-o3, a two-stage agent that first proposes regions of interest and then performs behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection, it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the state-of-the-art OpenAI o3 model and generalizing across backbones. To our knowledge, this constitutes one of the first behavior-grounded agentic systems in pathology. Turning everyday viewer logs into scalable, expert-validated supervision, our framework makes agentic pathology practical and establishes a path to human-aligned, upgradeable clinical AI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 7 8 5 4 0 . 0 1 5 2 : r Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior Sheng Wang1,*, Ruiming Wu2,*, Charles Herndon5, Yihang Liu3, Shunsuke Koga1, Jeanne Shen4, and Zhi Huang1,2, 1Department of Pathology and Laboratory Medicine, University of Pennsylvania 2Department of Biostatistics, Epidemiology & Informatics, University of Pennsylvania 3Department of Electrical and System Engineering, University of Pennsylvania 4Department of Pathology, Stanford University 5Department of Pathology, University of California at San Francisco *These authors contributed equally to this work To whom the correspondence should be addressed: Zhi Huang (zhi.huang@pennmedicine.upenn.edu)"
        },
        {
            "title": "ABSTRACT",
            "content": "Diagnosing whole-slide image diagnosis is an interactive, multi-stage process of changing magnification and moving between fields. Although recent pathology foundation models are strong, practical agentic systems that decide what field to examine next, adjust magnification, and deliver explainable diagnoses are still lacking. The blocker is data: scalable, clinically aligned supervision of expert viewing behavior that is tacit and experience-based, not written in textbooks or online, and therefore absent from LLM training. We introduce the AI Session Recorder, which works with standard WSI viewers to unobtrusively record routine navigation and convert the viewer logs into standardized behavioral commands (inspect/peek at discrete magnifications) and bounding boxes. lightweight human-in-the-loop review turns AI-drafted rationales into the Pathology-CoT dataset, form of paired where to look and why it matters supervision produced at roughly six-fold lower labeling time. Using this behavioral data, we build Pathologist-o3, two-stage agent that first proposes ROIs and then performs behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the state-of-the-art OpenAI o3 model and generalizing across backbones. To our knowledge, this constitutes one of the first behavior-grounded agentic systems in pathology. Turning everyday viewer logs into scalable, expert-validated supervision, our framework makes agentic pathology practical and establishes path to human-aligned, upgradeable clinical AI."
        },
        {
            "title": "1 Introduction",
            "content": "The advancement of digital pathology and its integration of artificial intelligence (AI) has deeply intertwined over the past two decades1, 2. This progress enabled the first generation of computer-aided diagnosis tools, some of which are now incorporated into clinical workflows for tasks such as tumor detection and cell counting36. These early successes demonstrated the value of AI in the treatment of well-defined problems within pathology. More recently, the field has been transformed by the emergence of powerful self-supervised pathology foundation models79 alongside large language models (LLMs). Together, these models combine visual understanding with language-level reasoning, opening the way to the next frontier: AI pathology agents. LLM-based agents have already begun to transform complex biomedical domains such as genetic experiment design and clinical diagnosis1013. In whole-slide image (WSI) analysis, an agentic paradigm is particularly well suited, since diagnosis is dynamic, multi-step process rather than static classification task. Here, an agent refers to system capable of perceiving its environment, leveraging LLM to perform reasoning and experiment orchestration, and executing sequence of actions, such as zooming, panning, and selecting regions, to accomplish complex diagnostic goal. Consequently, an agent that can systematically examine and analyze gigapixel WSIs provides natural foundation for collaborative AI partner for pathologists. Although designing an agent system plays role, recent advances such as OpenAI o314 and its variants15, 16 have already established capable agent frameworks. In digital pathology, to ensure alignment with pathologists routine operations and achieving scalability, the main bottleneck is the availability of high-quality, well-curated training data. Unlike programming or mathematical agents, clinical diagnosis cannot be guided by simple rule-based rewards. Agents instead need to learn how pathologists examine slides and which regions they attend to for each task17, 18. This underscores the 1 urgent need to encode human knowledge and practice into reusable computational forms, which in turn requires direct expert involvement in creating such training data. However, gigapixel WSIs encompass diverse diagnostic scenarios and workflows19, 20, often makes dataset construction especially time-consuming, costly, and difficult to scale. Modern digital pathology systems routinely record navigation events such as panning and zooming, thereby creating continuous record of pathologist behavior20. Although these interaction logs are produced at scale, such user behavior signals are both high-frequency and noisy, and thus require careful post-processing before they can serve as effective supervisory signals21, 22 (Figure 1a). These digital exhaust traces can provide scalable source of supervision for training human-aligned agents, but only if they are accurately captured, curated, and converted into structured, model-ready representations. To address this critical gap, we introduce the AI Session Recorder, which converts raw digital pathology interaction logs into structured, agent system-ready pathology-CoT dataset. The proposed system segments continuous event streams and discretizes pathologists interactions into compact sequence of actions. These actions include exploratory scans at low or medium magnification and brief high-magnification inspections, each linked to LLM-friendly coordinates that define Region of Interest (ROI). For each ROI, visionlanguage model (VLM) will generate concise rationale and impression, which the pathologist can quickly verify or edit through human-in-the-loop interface (Figure 1b). This process extracts signals from noise, producing data that are well suited for training LLM-based agents. By coupling the AI Session Recorder with pathologist oversight, annotation costs are substantially reduced while maintaining clinical fidelity. In the gastrointestinal lymph node metastasis task of the Pathology-CoT dataset, this paradigm achieved an eight-fold increase in the efficiency of labeling. To evaluate the effectiveness of behavioral data, we developed pathologist-o3, task-conditioned agent trained on recorded viewing behavior. The model achieved 84.5% precision, 100.0% recall, and 75.4% accuracya, outperforming the state-of-the-art closed-source commercial thinking-with-image OpenAI o3 model, which reached 46.7% precision, 87.5% recall, and 57.8% accuracy. Across different backbones, incorporating behavioral guidance increased both precision and recall by an average of 11.8% and 17.9%, respectively. In addition, the agent demonstrated greater navigation completeness (61.3%) and efficiency (56.2%), much higher when compared with baseline, unguided VLMs. Our approach also differs fundamentally from PathChat+23, which delegates the selection of where to zoom and why to an LLM \"supervisor.\" The procedural viewing policy that pathologists use is largely tacit and not captured in text corpora, making it difficult for text-trained LLMs to acquire. Consistent with this limitation, PathChat+s navigation actions generated by LLM that diverge from expert trajectories and often lack diagnostic utility  (Fig. 4)  . By directly learning from recorded expert navigation and pairing it with pathologist-verified rationales, we provide explicit supervision for both the \"where\" and the \"why,\" enabling human-aligned behavior and stronger end-to-end performance. We make three key contributions throughout this paper. First, we introduce digital pathology AI Session Recorder, which captures routine viewer navigation and, with human-in-the-loop review, transforms raw logs into standardized, agentready training data. Second, we introduce the pathology-CoT dataset. It provides task-conditioned, expert-validated behavior + reasoning pairs, delivering clinically aligned supervision at scale. Third, we develop the pathologist-o3 agent, it can identify diagnostic regions and reasons over standardized crops, outperforming unguided VLM baselines and transferring successfully to an external validation cohort. With these contributions, we tackle the central bottleneck for innovating and advancing digital pathology agents: obtaining data that is both clinically meaningful and scalable. By converting routine viewing logs into task-conditioned behavioral supervision paired with low-cost, expert-verified reasoning, we supply the supervision required for existing architectures to evolve into capable, human-aligned agents."
        },
        {
            "title": "2.1 A scalable data engine converts viewing behavior into agent-ready supervision\nDiagnosing a whole-slide image for conditions like lymph node metastasis is a dynamic process of visual inquiry.\nPathologists begin with a low-power overview, zoom into suspicious areas for high-power analysis, and iteratively navigate\nacross the gigapixel canvas to build a conclusion (Figure 1a left). This sequence of interactions is highly purposeful;\nthe keyboard and mouse movements embody the pathologist’s tacit knowledge, reflecting an expert’s experience-driven\nstrategy for searching for evidence. While modern digital pathology systems routinely capture this behavior in interaction\nlogs, the raw data exists as a high-frequency, noisy, and enormously long stream. This “digital exhaust” is too complex and\nvoluminous for direct use in training machine learning models, as the chaotic input exceeds the context limits of current AI\nand prevents effective learning. This gap means that the vast expertise exercised in daily clinical practice remains in a",
            "content": "aThe lower accuracy compared to both precision and recall is due to the datasets high imbalance. 2/23 Figure 1. The AI Session Recorder framework for converting expert viewing behavior into agent-ready training data. The AI Session Recorder addresses the core data challenge: raw, continuous user logs are too noisy and complex for AI to learn from. The recorders algorithm transforms this chaotic data stream into discrete, meaningful commands (e.g., 5x-Inspect), inspired by the discrete objective lenses (e.g., 5x, 10x, 40x) on physical microscopes. The recorders novel data generation pipeline creates the Pathology-CoT dataset. For each captured action, an AI drafts rationale explaining the experts focus, which pathologist then efficiently verifies or corrects in human-in-the-loop workflow. The resulting dataset contains 5,222 conversational rounds from pathologists with diverse experience levels. The semi-automated, human-in-the-loop workflow is highly efficient, reducing labeling time by approximately six-fold compared to manual annotation from scratch. 3/23 format that is not amenable to model training. systematic methodology to record, structure, and encode this knowledge is fundamentally required to train sophisticated AI agents that can serve as true diagnostic co-pilots or autopilots. Given this urgent challenge, we developed the pathology AI session recorder, tool that aims at resolving the data bottleneck of training next-generation medical AI agent by collecting pathologists behavior data at scale. AI session recorder processes the behavior data from 8 pathologists at Stanford Medicine when using nuclei.io24 to make diagnosis for adjacent lymph node metastasis. Such raw data contains 25 cases, 137 slides across 8 pathologists (four attending pathologists, two fellows, and two residents) with total of 10.6 hours of inspection. Based on such data, AI session recorder performs two key functions to transform unusable digital pathology image inspection raw logs into structured, agent-ready supervision (Figure 1a). First, it automatically analyzes the complex viewing process to identify and discretize the most important areas of focus (Figure 1b). Second, inspired by the discrete objective lenses of physical microscope, this process converts the chaotic event stream into compact sequence of meaningful behavioral commands. Each command is defined as either broad, low-magnification <inspect> or rapid, high-magnification <peek>, and is paired with standardized region-of-interest (ROI) box. The recorder also captures clinical reasoning by using these expert-identified ROIs to prompt vision-language model (VLM), which drafts rationale explaining why the region was examined and what key findings are present (Figure 1b, left). The model-generated rationales are reviewed and, when necessary, edited by two pathologists, then consolidated into the Pathology-CoT agent-training dataset (Figure 1b, right, shows an example). To implement and verify this AI session recorder, we focused on clinically representative task, namely N-staging of colorectal cancer (CRC) lymph-node metastasis, which is both common and labor-intensive. We collected behavioral data from eight pathologists at Stanford Medicine as they analyzed 137 WSIs using standard viewer that recorded all navigation events24. The raw logs were noisy and high-frequency, with active navigation captured at approximately 10 Hz and averaging more than 257 distinct viewport events per slide (Figure S1). Using this stream directly as supervision was infeasible because feeding the raw viewports into VLM would produce over 500,000 visual tokens per case, far exceeding the current context limits and could also prevent effective learning. In contrast, the AI session recorder reduces excessive context length by filtering noisy, non-informative behavior data, while preserving the fidelity of pathologists behavioral signals. Using this tool, human pathologists can rapidly verify AI-drafted behavior and rationale by accepting, revising, or rejecting them. In this verification setting, two pathologists reviewed the chain-of-thought data derived from the viewing logs of the eight pathologists that comprise Figure 1d, including proposed regions to zoom, per-ROI rationales (why to zoom and what is seen), the synthesized summary, and the final impression. During verification they could accept the generated text, edit it, or indicate that proposed behavior was unnecessary (no zoom here). Each zoom-in constitutes two-round exchange: decision to zoom and description of the findings. In two-user timing study (Figure 1c), verification took 12.1 and 9.7 per round for Users 1 and 2, respectively. When edits were needed, revisions took 55.5 and 40.2 s, respectively. Averaged across rounds, the verify-and-edit workflow required 18.8 (User 1) and 16.3 (User 2). For comparison, annotation from scratch measures the total time to decide where to zoom and to explain (by typing or dictation) why to zoom and what is found; this required 106.2 and 85.2 by typing or 56.7 and 64.9 by verbal dictation. Based on these observations, we conclude that our semi-automated, human-in-the-loop process is roughly 56 faster than typing and 34 faster than dictation, with over 80% of AI-generated text requiring no edits, making the AI session recorder low-cost and scalable data collection engine. See Appendix for details on the software and experiment implementation. Based on the manual filtering, the resulting Pathology-CoT dataset is structured collection of expert-validated, task-conditioned behavior + reasoning pairs. Derived from 10.6 hours of recorded interactions from eight pathologists with diverse experience levels (attendings, fellows, and residents) across 921 sessions (Figure 1d), the dataset contains 5,222 conversational rounds of agent training data. The reasoning is detailed: broad, low-magnification <inspect> actions are accompanied by an average of 152 words of description, while the analysis of fine cellular-level findings from high-magnification <peek> actions averages 82 words. This rich dataset captures not only where and how an expert looks but also embeds the clinical reasoning for why, providing robust, multi-modal foundation for training and evaluating pathology agents. To our knowledge, this is the first chain-of-thought pathology behavior data that captured and established from expert pathologists. Different from PathChat+23 that the knowledge was based on GPT model, the thinking ability is based on expert pathologists."
        },
        {
            "title": "2.2 Behavior-guided supervision enables accurate pathology agents\nAI session recorder enables a scalable way to record tacit diagnostic knowledge from pathologists at scale. By that we are\nable to collect a scalable dataset “Pathology-CoT” that is ready for machine learning. Based on such format of dataset, it\nallows (i) digitize and store pathologists’ behavior; (ii) guiding the action model (such as LLM) to locate important regions",
            "content": "4/23 Figure 2. Overview of pathology-o3 and its performance. Workflow: task prompt initializes the agent, which proposes candidate regions using behavior bank/locating module; cropped ROIs are sent to VLM that generates per-ROI reasoning, then summarized into an impression and final diagnosis. Example thinking with image on one slide. The agent lists planned inspections, provides step-wise descriptions for each ROI (blue boxes), and synthesizes case-level conclusion. Positive and negative findings are indicated for each step. Quantitative comparison on the lymph-node metastasis task. Pathologist-o3 achieves the best overall accuracy with perfect or near-perfect recall, outperforming strong VLM baselines (asterisks denote significance). 5/23 from WSI; and (iii) training an agentic AI system to make clinical diagnosis in the same manner as pathologists. To demonstrate the utility of that, we designed task-conditioned agent pathology-o3 that mimics pathologists workflow. Learned from the training data Pathology-CoT dataset in example of lymph node metastasis, the agent first decides where to look, then analyzes what it sees (Figure 2a). To emulate this first step, it uses task-specific Behavior Predictor trained on the complete set of discretized behavioral commands from aforementioned Pathology-CoT dataset, then learning to propose diagnostic ROIs on new slide in our internal and external validation datasets. Once these ROIs are identified, the system performs behavior-guided thinking: for each region, it auto-crops standardized high-resolution field, injects task context into the prompt (case-level goal and information, and ROI-level intent), and then prompts VLM to generate per-ROI reasoning, which is subsequently synthesized into case-level summary. This capability by combining learned human viewing policy with powerful reasoning AI, allows the agent to produce step-by-step visual chain-of-thought for given whole-slide, demonstrating its ability to systematically analyze slide in human-aligned manner (Figure 2b). We evaluated our framework against panel of state-of-the-art VLMs, with detailed specifications provided in Table S1. On the lymph-node metastasis benchmark, our behavior-guided agent achieved 84.5% precision, 100% recall, and 75.4% accuracy (Figure 2c). The next-best model, OpenAI o3, reached 46.7% precision, 87.5% recall, and 57.8% accuracy. Text-centric VLMs performed notably worse and exhibited imbalanced trade-offs: for example, GPT-4.1 achieved high recall (91.7%) but low precision (42.3%), while Llama-4 showed higher precision (75.0%) but extremely low recall (6.2%). Across the remaining baselines, precision ranged from 33.365.6%, recall from 18.291.7%, and accuracy from 42.265.6%. These results demonstrate the importance of grounding reasoning in expert visual search patterns; for nuanced diagnostic challenges like lymph node metastasis, generic reasoning is insufficient. Performance hinges on first learning where and how to look (i.e., thinking with image), which our behavior-aligned data supplies. 2.3 Pathology-o3 and learned behavior can generalize on external validation dataset To test for generalization, the Behavior Predictor, trained on Stanford Medicine data, was evaluated on completely independent external dataset: the lymph node in colon adenocarcinoma (LNCO2) cohort. This challenging cohort consists of 1,245 whole-slide images from Sweden, introducing domain shifts from different scanners and magnifications. Despite these variations, pathology-o3 trained on United State pathologist behavior maintained robust performance, markedly outperforming other leading VLMs on this benchmark (Figure 1a). Quantitatively, our agent achieved an accuracy of 69.4%, precision of 62.9%, and near-perfect recall of 97.6%. These results demonstrate significant leap in performance; for instance, pathology-o3s precision was more than double that of the next-best model (Gemini at 23.5%), and its exceptional recall highlights strong ability to reliably identify positive cases. For example, as shown in Figure 1b, in the first case (inspects 1 and 2; highlighted in blue squares) and the second case (inspect 3), the Behavior Predictor identified tinctorial differences and architectural disruption. For instance, in inspect 2, Behavior Predictor suggests There are no clear clusters, nests, or glandular structures composed of atypical epithelial cells. Paler clusters were detected that interrupted the normal diffuse pattern of the lymph-node parenchyma, appearing as disorganized aggregates or nodules. In the first case (inspects 4 and 5) and the second case (inspect 1), the Behavior Predictor located the lymph-node capsule edge with tinctorial differences and correctly determined that the node was positive for tumor metastasis. These findings suggest that supervision at the level of how and where to look yields transferable viewing policy that is more robust to domain shifts in image appearance than direct pixel-level supervision."
        },
        {
            "title": "2.4 Pathology-o3 learns where to look\nthe robustness of its learned policy and the\nWe next examined two key properties of our behavior-guided agent:\nfundamental importance of navigation itself. We first evaluated robustness by permuting the order of ROI presentation\n(forward, reverse, and random). The agent’s performance varied only minimally across these settings (Figure 4a, left),\nindicating that it synthesizes evidence effectively without dependence on a fixed sequence. This suggests that as long\nas diagnostically relevant regions are identified, the specific viewing habits of different pathologists can be learned and\nsuccessfully deployed within an agentic system.",
            "content": "Second, we analyzed the agents performance when we capped the maximum number of ROIs it could inspect (k, analogous to CoT length). As shown in Figure 4a (right), performance improved steadily as increased, with notable gains in recall while preserving high precision. This finding demonstrates that when provided with sequence of diagnostically relevant regions, VLMs are highly effective at synthesizing the evidence to reach an accurate conclusion. This proficiency in analysis raises critical question: if VLMs are strong analyzers, what limits their performance in WSI-level tasks? To investigate, we evaluated their out-of-the-box navigation capabilities. Rather than being guided by our Behavior Predictor, we let VLMs to select their own ROIs. We then compared their navigation against senior attendings 6/23 path using two metrics (described in Methods): completeness, which represents the fraction of expert-viewed regions recovered; and efficiency, which represents the proportion of selected regions that were clinically relevant. As shown in Figure 4b (upper), unguided VLMs performed poorly on both measures. Qualitatively, their ROI selections were scattered and low-yield, whereas our behavior-guided agent concentrated on clinically salient sites that closely overlapped with the experts annotations (Figure 4b, lower). These results reveal fundamental gap between analysis and navigation. The strong analytical ability of VLMs reflects training on large corpora of static, pre-cropped images, whereas their failure to navigate stems from the absence of data capturing continuous, visually guided interaction with complex environments. This finding underscores our data-centric approach: building effective agents requires digitizing the tacit, procedural knowledge of expert navigation into structured format suitable for learning. 2.5 Practicality, scalability, and the value of learned behavior Finally, we analyzed the practical viability and scalability of our agentic framework. Figure 5a provides cost and latency snapshot for representative WSI analysis, comparing large, powerful model Gemini-2.5-pro with smaller, faster one Gemini-2.5-flash. The results demonstrate that the agentic workflow is already feasible with current backbones. More importantly, our framework is modular by design, allowing the core reasoning engine (the VLM) to be upgraded as foundation models evolve. This design ensures the system is future-proof: it can readily incorporate next-generation models to become progressively faster, cheaper, and more capable over time. Next, we investigated the value of the behavior-aligned data itself. We compared three conditions across multiple VLM backbones (Figure 5b): (i) Non-agent is baseline without guidance and diagnosis with only look at the image once; (ii) Agent guided by real behavior from senior attending GI pathologist (a practical upper bound) as we shown in . and (iii) Agent guided by learned behavior from the behavior bank; The addition of behavioral guidance yielded profound improvement in performance. Compared to the baseline, guidance from the learned viewing policy increased precision by 11.8% and recall by 17.9%. The gains from real expert behavior were similarly large, boosting precision by 11.9% and recall by 19.1%. This translated to marked increase in overall accuracy, improving it by 9.2% and 7.1%, respectively. Critically, our agents learned behavior closes most of the performance gap to real behavior. This has powerful implication for scalability: instead of requiring continuous, large-scale expert data collection, we can train compact and efficient Behavior Predictor on limited dataset. This predictor effectively distills the experts viewing policy, creating scalable asset that can generate human-aligned guidance for any VLM on any number of slides, reinforcing our central thesis that the most effective path to capable pathology agent is by first building the right data engine."
        },
        {
            "title": "3 Discussion",
            "content": "The rapid maturation of visionlanguage models is creating new frontier for artificial intelligence in medicine, shifting the paradigm from task-specific classifiers to versatile, reasoning-based agents. Digital pathology, with its gigapixel canvases and complex, multi-step diagnostic workflows, is particularly fitting domain for this agentic approach. However, the primary obstacle to building capable pathology agents has since been bottlenecked by high quality data annation rather than modeling: supervision that is not only large-scale but also deeply aligned with the procedural, tacit knowledge of clinical experts. The central challenge is that this expert knowledge is not readily available in structured format. While modern pathology viewers generate vast digital logs of user interactions, this raw behavioral data is high-frequency, noisy, and enormously long. It captures the outcome of an experts thought process but not the discrete, intention-driven steps that are amenable to model training. This results in fundamental \"analysisnavigation gap\": while VLMs are trained on vast corpora of static images and are thus adept at analyzing pre-selected regions, they are not trained on the dynamic, interactive data needed to learn the procedural skill of navigating slide to find those regions in the first place. To bridge this gap, we developed the AI Session Recorder, data engine that digitizes the tacit knowledge of expert navigation into our Pathology-CoT dataset. We demonstrated the value of this behavior-aligned data by building simple agent, Pathologist-o3, as controlled testbed. Our results show that this data provides universal, model-agnostic performance multiplier, significantly improving the diagnostic accuracy of wide range of VLM backbones. We found that the learned viewing policy is robust, generalizing effectively to an entirely different hospital system with different scanners and patient populations. Further analysis revealed that this policy is not only robust to variations in viewing habits but is also essential for effective diagnosis, as leading VLMs without this guidance fail at the fundamental task of navigation. Finally, we established that this data-centric approach is both practical and scalable; the expert viewing policy can be distilled into compact, efficient model that provides nearly all the benefits of real expert guidance, creating powerful asset for building the next generation of capable, human-aligned pathology agents. The study has several limitations, primarily centered on our data-centric methodology. First, our findings on viewing policy, such as robustness to viewing order, are specific to the \"search-and-find\" task of metastasis detection. This may 7/23 Figure 3. Quantitative and qualitative results on the external LNCO2 validation cohort. Bar charts comparing Accuracy, Precision, and Recall of Pathology-CoT with other VLM backbones. Qualitative examples of the models output on two slide images, with highlighted regions and corresponding textual descriptions. 8/23 Figure 4. Analysis of reasoning length and order. The effect of sequential, reverse, and random order of evidence, and the impact of chain-of-thought length on performance. Qualitative and quantitative comparison of different models predictions against senior pathologists behavior. 9/23 Figure 5. Comparison of different thinking modes and cost-benefit analysis. table comparing cost, time, and performance between high-cost reasoning model Gemini-2.5-pro and low cost. Performance comparison of Non-agent, Agent guided by real behavior, and Agent guided by learned behavior modes across various VLMs. 10/23 not generalize to tasks requiring systematic, sequential evaluation, like architectural pattern grading in prostate cancer or counting mitoses along tumor boundary, where the viewing path itself is critical component of the diagnosis. To begin addressing this, we are continuously expanding our public dataset with behavior logs from wider range of pathological tasks, with ongoing data releases available on our project website. Second, our efficient human-in-the-loop workflow for generating reasoning labels introduces potential for AI anchoring bias, where the initial AI-generated text could subtly influence the experts final revision. Finally, practical challenge lies in the data source itself. Unlike standardized web-scale corpora, pathologist viewing logs are heterogeneous, with different software platforms using varied and often unstructured formats. Tapping into this data requires an initial engineering effort for each system. However, the significant advantage of our approach is that once this integration is complete, data collection becomes entirely passive, imposing no additional burden on the pathologists routine clinical workflow. The insights from this study were enabled by our AI Session Recorder, the data engine we developed to curate Pathology-CoT, the first large-scale public dataset of expert pathologist viewing behavior. Using this engine, we are continuously expanding the Pathology-CoT dataset to cover wider range of tasks. We anticipate that our open-source methodology and this growing agentic data resource will benefit the medical AI community by providing new modality of supervisionprocedural behaviorand by offering blueprint for creating similar data engines in other clinical domains where expert interaction is the key to building capable, human-aligned AI."
        },
        {
            "title": "4 Method",
            "content": "Figure 6. Pathology-CoT Dataset Creation Pipeline. Left: Pathologist interaction data (viewports, navigation actions like zoom-in/panning, task context) is collected. Center: Generating the Thinking Label. Contextual information (task, current view, next action) is used to prompt large language model (LLM, e.g., Gemini) to generate candidate reasoning. pathologist then reviews, corrects, or approves this output to create the final Thinking Label. Right: The collected actions and generated thinking labels are merged with corresponding image patches (thumbnails, zoomed regions) and system prompts into structured conversational format suitable for model training or in-context learning."
        },
        {
            "title": "4.1 Data Collection: Pathologist Behavior\nWe collected interaction data from eight pathologists with varying experience levels (four attendings, two fellows, and two\nresidents) analyzing 137 diagnostic Whole Slide Images (WSIs) from 25 cases of colorectal cancer (CRC) lymph node\n(LN) metastasis. The data collection setup, using the nuclei.io software, recorded the following:",
            "content": "11/23 High-Resolution WSIs: The original gigapixel images, which were digitized using Leica Aperio Scanner at 40x magnification (0.25 µm per pixel). Timestamped Viewports and Navigation Actions: The sequence of user activities was recorded into log file with timestamps detailed in milliseconds. These logs captured the Field of View (FoV) coordinates and zoom level changes as pathologists navigated the WSIs. Diagnostic Result: The final annotations marking positive LNs were recorded for each WSI, which formed the basis for the final diagnostic findings. This raw data captures the dynamic, multi-modal behavior of expert pathologists during routine diagnostic tasks (Figure 6, left panel). Figure 7. Sankey Diagram of Diagnostic Attention Flow. The diagram illustrates how two primary examination methods, Inspect (broad-field scan) and Peek (high-magnification look), are applied to analyze various histological and cytological features during lymph node metastasis diagnosis. The flow size represents the relative proportion of diagnostic attention directed from specific method to each feature."
        },
        {
            "title": "4.2 AI Session Recorder: Discretizing Behavior to Extract ROIs\nThe foundation of our approach is the AI Session Recorder, a computational pipeline designed to transform the raw\ninteraction data gathered from pathologists—a high-frequency, continuous, and inherently noisy stream of timestamped\nviewports (Figure 7a) into structured, agent-ready data. Pathologists’ navigation is rapid and complex; a single slide\nanalysis can generate an average of 257.3 viewport events. Directly using this raw data for model training would be\nchaotic and impractical. If each of these 257 viewports were treated as a separate 1024x1024 patch, it would generate\nover 500K tokens from the visual modality alone. This volume is prohibitive for prominent large models, and could severely\ndegrade performance.",
            "content": "The AI Session Recorder processes this raw stream using series of heuristics inspired by the physical use of microscope, which has discrete, not continuous, magnification levels. We analyze the temporal patterns of viewport changes, as shown in Figure 7c, to segment the continuous stream into discrete actions. We define two primary types: <inspect>: This action represents broad, exploratory examination. It is identified when pathologist either holds their viewport static over an area for more than one second or continuously pans across region for more than two seconds. 12/23 <peek>: This action captures quick, high-magnification look at fine cytological details. It is implemented by identifying rapid zoom to the WSIs native resolution and capturing the central 1024x1024 pixel area. These initial actions are then filtered to remove low-magnification overviews, merged if they have high spatial overlap (IoU > 0.8), and pruned to prioritize the most specific, high-magnification views. Finally, each resulting actions magnification level is binned to the nearest standard microscope objective, and its bounding box is normalized to standard size. For instance, 12x view becomes <10x-inspect> action. As shown in Figure 7(b), this converts the raw data into clean, compact sequence of behavioral commands. This abstracted action sequence, composed of command type and standardized ROI box, serves as the basis for both training our agent and generating reasoning labels. 4.3 AI Session Recorder: Rationale Generation After extracting behavioral ROIs, the second function of the AI Session Recorder is to generate corresponding clinical rationale for each one. This is achieved through scalable, semi-automated human-in-the-loop workflow that systematically generates the ground truth components for our agent, specifically the reasoning (think) and findings (answer) behind each action. The process unfolds in three stages: 1. Contextual Prompting: For each ROI, powerful Vision-Language Model (VLM), such as Gemini 2.5 Pro, is prompted with rich context. This includes the overall task description, the low-magnification image of the ROI, high-magnification crop from its center, and, when available, snippets of the pathologists transcribed verbal commentary during the original examination. 2. Guided Generation: The VLM is instructed to synthesize concise rationale that logically explains both why an expert would focus on this region (the thinking process) and what the key morphological findings are within it (the answer). 3. Pathologist Review and Correction: Critically, the AI-generated draft is presented to human pathologist in lightweight interface for rapid validation, as shown in Supplementary Figure S3. The interface displays the relevant thumbnail, the zoomed-in ROI, and an optional cytology-level crop alongside the generated text for \"Thumbnail Impression,\" \"Why Zoom,\" and \"Findings.\" The pathologist can then efficiently accept, edit, or reject the text to ensure its clinical accuracy and alignment with their actual reasoning process. This \"review-over-authoring\" paradigm proved to be 8.6 times faster than manual annotation from scratch. It enables the efficient creation of high-quality dataset composed of paired \"behavior + reasoning\" data, which is crucial for both training our agent and generating its reasoning labels. 4.4 The Pathology-CoT Dataset The curated rationale and impression components are structured alongside the corresponding state information to form the final Pathology-CoT dataset. As illustrated in Figure 6 (right panel), the data can be formatted into sequences of state-action tuples (st, aGT t+1), or arranged in conversational format suitable for prompting VLMs using In-Context Learning. The final output of our pipeline is the Pathology-CoT dataset, structured collection of expert-validated, agent-ready training data. The dataset is derived from 10.6 hours of recorded interactions from eight pathologists at Stanford Hospital (including attendings, fellows, and residents) analyzing WSIs for the task of colorectal cancer lymph node metastasis. The current version contains 921 sessions and 5,222 conversational rounds. Each round consists of behavioral command, standardized ROI, and the corresponding expert-validated reasoning. The reasoning is richly detailed, with broad <inspect> actions accompanied by an average of 152 words of description, and high-magnification <peek> actions averaging 82 words on cellular-level findings."
        },
        {
            "title": "4.5 Pathologist-o3 Agent Framework and Evaluation\nInspired by recent advancements in decoupling complex vision-language tasks with tool-driven visual exploration25, we\nconceptualize WSI diagnosis as a sequential, multi-stage process. For a given WSI and a high-level objective (e.g., “Find\nall positive lymph nodes\"), our pipeline operates in three key stages:",
            "content": "1. Global Thumbnail Analysis: The system first processes downsampled thumbnail of the WSI to gain global context and form an initial impression. 13/23 2. Behavior Prediction: In response to the given task, Large Language Model (LLM) functions as router, selecting the most suitable behavior predictor from predefined behavior bank. This predictor then proposes set of candidate Regions of Interest (ROIs) as we will detail in next section. These ROIs, represented as bounding boxes, act as Location Prompts that direct the systems focus to specific areas for closer inspection, mimicking how human pathologist identifies salient regions for magnification. 3. Reasoned Analysis and Summarization: Each candidate ROI is subsequently cropped at higher magnification and processed by central Vision-Language Model (VLM), termed the Reasoning Module. This module generates an explicit reasoning chain and detailed findings for each region. After analyzing all ROIs, the module synthesizes this information to produce final diagnostic summary. Notably, unlike Pathology-CoT, pathologist-o3 does not require the VLM to rationalize the selection of an ROI, as this task is handled exclusively by the behavior predictor. This decoupled pipeline enables the system to efficiently scan large WSI and concentrate its advanced reasoning capabilities on the most pertinent areas, thereby mirroring an expert pathologists workflow. To determine the optimal VLM for the Global Thumbnail Analysis, Reasoning Module and Summarization, we conducted comparative analysis detailed in Table S1. Gemini-2.5-pro demonstrated the best overall performance and was subsequently used in all of our experiments. Table S1. Performance Comparison using Different VLMs in Pathologist-o3 (Behavior Predictor Fixed). Best performed method(s) were highlighted in bold font. Model (Core VLM Backbone) Gemini QWen GPT-4.1 Llama Claude InternVL Diagnosis Decision Prec. (%) Rec. (%) Acc. (%) 84.48 47.91 43.48 56.25 53.53 46. 100.00 58.33 77.77 61.50 100.00 61.50 75.38 53.33 61.54 50.84 65.21 53.33 4.6 The Pathologist-o3 Agent: Behavior Predictor To validate our dataset, we created an agent named Pathologist-o3. Its core navigational component is task-specific Behavior Predictor, which is trained to propose diagnostically relevant ROIs on new WSI. The Behavior Predictor is not single, monolithic model. Instead, our framework envisions library of predictors, each trained for distinct, high-volume clinical task (e.g., melanoma screening, prostate cancer grading). For this paper, we focus on developing and evaluating one such predictor as proof-of-concept: model trained for the common and time-consuming task of identifying lymph node metastasis in colorectal cancer cases. Our choice of YOLO-based architecture for these individual Behavior Predictors is deliberate and grounded in clinical pragmatism. For this study, the predictor is lightweight YOLOv526 object detection model, chosen for its efficiency and ease of deployment. Its lightweight nature ensures that the model can be readily deployed in typical hospital IT environments, including on standard CPUs or within web-based viewers. This accessibility paves the way for future enhancements like on-the-fly incremental learning, where predictor could be continuously personalized to pathologists specific style. We considered more advanced, open-vocabulary detection models (e.g., based on VLM-grounding or CLIP), but found them less suitable for this clinical application. Their \"one-model-for-all\" design does not align with the sub-specialized reality of pathology. The clinical need is not for single, universal model, but for highly efficient and reliable models for specific, repetitive tasks. Our modular approach of training lightweight, task-specific predictors is more practical and robust to variations in scanning hardware and magnificationa common challenge for end-to-end models. The Behavior Predictor was trained on the 1,521 ROI boxes extracted by the AI Session Recorder from 137 WSIs across all eight pathologists. It learns to identify areas with high diagnostic relevance based on expert behavior, which are not necessarily \"abnormal\" in clinical sense. To ensure clinical validity and prevent data leakage, training was performed using 5-fold cross-validation strategy, with splits made at the patient level. For implementation, the model was trained for 100 epochs using the Adam optimizer with an initial learning rate of 1e-4 and batch size of 32. All input images were resized to 1024x1024 pixels. To improve robustness, we applied 14/23 standard data augmentation techniques, including random flips, rotations, scaling, and color adjustments. The training was guided by standard composite loss function: Localization Loss: regression loss (CIoU loss) that penalizes the discrepancy between the predicted and ground-truth bounding boxes. The Complete Intersection over Union (CIoU) loss is defined as: LCIoU = 1 IoU + ρ2(b, bgt) c2 + αv where IoU is the Intersection over Union, ρ2(b, bgt) is the squared Euclidean distance between the center points of the predicted box (b) and the ground-truth box (bgt), is the diagonal length of the smallest box enclosing both boxes, and the term αv is penalty term for the consistency of the aspect ratio. Classification Loss: classification loss (Binary Cross-Entropy) that trains the model to distinguish relevant regions from the background. The Binary Cross-Entropy (BCE) loss is calculated as: LBCE ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 [yi log(ˆyi) + (1 yi) log(1 ˆyi)] where is the number of predictions, yi is the ground-truth label (e.g., 1 for relevant region, 0 for background), and ˆyi is the models predicted probability that the region is relevant. In the validation of the pathologist-o3 in Fig 2, Fig 4, Fig 5, prediction from 5 test fold is merged and used as Behavior Predictors output. In external validation on LN-CO2 dataset as Fig 3, fold-0 model is used to generate behavior bounding boxes prediction. 4.7 Experimental Setting Details 4.7.1 Classification metrics. For discrete prediction tasks, we report suite of complementary classification metrics. Let denote true positives (correctly identified positive cases), true negatives (correctly identified negative cases), false positives (negative cases incorrectly predicted as positive), and false negatives (positive cases incorrectly predicted as negative). Accuracy is the most widely used classification metric, defined as the proportion of correctly classified samples among all cases: Accuracy = + P + + + . It provides an intuitive single-number summary of overall correctness and is easy to interpret across different tasks. However, accuracy alone can be misleading in settings with severe class imbalance, as model may achieve deceptively high scores by always predicting the majority class. For instance, in cancer screening with 95% healthy cases, trivial model labeling all patients as healthy would still reach 95% accuracy while completely failing to detect diseased individuals. These limitations motivate the complementary use of precision, recall, and F1-score, which better capture diagnostic reliability under clinical imbalance. To complement accuracy, we report precision, recall, and their harmonic mean (F1-score), which together provide more nuanced assessment of diagnostic performance under class imbalance. Precision quantifies the reliability of positive predictions, indicating the proportion of predicted positives that are truly positive. Precision = P + . high precision indicates that most predicted positives are true positives, thereby reducing the risk of over-diagnosis and unnecessary clinical interventions. Recall (sensitivity) captures the models ability to identify all true positive cases, thereby minimizing false negatives. This is crucial in clinical contexts such as cancer detection, where missed diagnoses can delay treatment. Recall = P + . High recall is essential in clinical contexts such as early cancer detection, where missing positive cases (false negatives) could delay treatment and lead to adverse outcomes. 15/23 4.7.2 Comparison to State-of-the-Art Models To benchmark our agent, we compared its diagnostic performance on the CRC lymph node task against panel of leading VLMs (Fig. 2c). These baseline models were evaluated on their ability to perform the same task without the guidance of our Behavior Predictor. Full specifications of the models used and the dates of API access are detailed in Supplementary Table 1. 4.7.3 External Validation To assess generalization, we performed rigorous out-of-distribution test using the independent LN-CO2 cohort  (Fig. 3)  . This dataset from different continental (Sweden) contains images from different scanners (Aperio and Hamamatsu) and at different magnifications (20x/40x), presenting significant domain shift. The Behavior Predictor was trained exclusively on our single-center Stanford data and then evaluated end-to-end on the LN-CO2 dataset without any fine-tuning. 4.7.4 Behavioral Analyses To deconstruct agent performance  (Fig. 4)  , we conducted several analyses using the ground truth navigation path of senior attending pathologist. Robustness to Viewing Order: The complete set of expert-selected ROIs for case was presented to the agent in its original sequence (Forward), in reverse (Reverse), and in randomly shuffled order (Random). Effect of Evidence Quantity: To simulate chain-of-thought of varying length, we capped the maximum number of ROIs the agent could inspect at given limit, k. Unguided Navigation: We evaluated the ability of unguided VLMs to navigate by tasking them to select their own ROIs. We measured their performance using two metrics: completeness (recall), defined as the fraction of expert-viewed ROIs they successfully identified, and efficiency (precision), the fraction of their proposed ROIs that were relevant. prediction was considered successful hit if its IoU with an expert ROI was > 0.3. 4.7.5 Practicality and Scalability Analysis To assess practical viability  (Fig. 5)  , we measured the per-slide cost and latency of our agent using two different VLMs: high-performance model (Gemini 2.5 Pro) and more cost-effective model (Gemini 2.5 Flash). Our cost calculations are based on public API pricing at the time of experimentation, which typically charges for both input and output tokens, with output tokens being more expensive. For image inputs, we standardized all ROIs to 1024x1024 resolution to ensure consistent pricing per region, as many models charge based on pixel count. This analysis demonstrates that by carefully managing prompt length and standardizing image inputs, an agentic workflow can be made cost-effective, particularly when using smaller, faster models for less complex tasks. To analyze scalability, we compared three conditions across multiple VLM backbones: the baseline VLM without guidance, our agent guided by its learned behavior, and an oracle agent guided by the real behavior of an expert pathologist."
        },
        {
            "title": "References",
            "content": "1. Gurcan, M. N. et al. Histopathological image analysis: review. IEEE Rev. Biomed. Eng. 2, 147171, DOI: 10.1109/RBME.2009.2034865 (2009). 2. Echle, A. et al. Deep learning in cancer pathology: new generation of clinical biomarkers. Nat. Rev. Cancer 21, 747762, DOI: 10.1038/s41568-021-00372-6 (2021). 3. Campanella, G. et al. Clinical-grade computational pathology using weakly supervised deep learning on whole slide images. Nat. Medicine 25, 13011309, DOI: 10.1038/s41591-019-0508-1 (2019). 4. Ehteshami Bejnordi, B. et al. Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. JAMA 318, 21992210, DOI: 10.1001/jama.2017.14585 (2017). 5. Coudray, N. et al. Classification and mutation prediction from nonsmall cell lung cancer histopathology images using deep learning. Nat. Medicine 24, 15591567, DOI: 10.1038/s41591-018-0177-5 (2018). 6. Veta, M., van Diest, P. J., Jiwa, N., Al-Janabi, S. & Pluim, J. P. W. Mitosis detection in breast cancer histopathology images: An assessment of algorithms. Med. Image Analysis 20, 237248, DOI: 10.1016/j.media.2014.11.004 (2015). 7. Xu, H. et al. whole-slide foundation model for digital pathology from real-world data. Nature 630, 181188, DOI: 10.1038/s41586-024-07441-w (2024). 8. Vorontsov, E., Bozkurt, A., Casson, A., et al. Virchow: million-slide digital pathology foundation model. arXiv preprint arXiv:2309.07778 (2023). 16/23 Model MMMU (Val) Prec. (%) Rec. (%) Acc. (%) o3 o4-mini Llama 4 Maverick Gemini 2.5 Pro 05-06 Qwen-VL-PLUS 82.9 81.6 73.4 79.6 45.2 46.7 55.0 75.0 33.3 37.3 87.5 45.8 6.2 18.2 52. 57.8 65.6 42.2 60.0 49.2 Table S2. Overlap between VLM general capbility (MMMU) overall scores and Precision/Recall/Accuracy. 9. Chen, R. J. et al. Towards general-purpose foundation model for computational pathology. Nat. Medicine 30, 850862, DOI: 10.1038/s41591-024-02567-8 (2024). 10. Huang, K. et al. Crispr-gpt: An llm agent for automated design of gene-editing experiments. arXiv preprint arXiv:2404.18021 (2024). 11. Yan, Z., Chen, J., Li, Y., Wang, H. & Zhou, J. Enhancing diagnostic capability with multi-agents conversational large language models in medicine. npj Digit. Medicine DOI: 10.1038/s41746-025-01550-0 (2025). 12. Roohani, Y. et al. Biodiscoveryagent: An ai agent for designing genetic perturbation experiments. arXiv preprint arXiv:2405.17631 (2024). 13. Ding, T., Chen, R. J., Lu, M. Y., et al. Cpathagent: An agent-based foundation model for computational pathology. arXiv preprint arXiv:2505.20510 (2025). 14. OpenAI. o3 and o4-mini system card (2025). 15. Zheng, Z. et al. Deepeyes: arXiv:2505.14362 (2025). Incentivizing \"thinking with images\" via reinforcement learning. arXiv preprint 16. Lai, X. et al. Mini-o3: Scaling up reasoning patterns and interaction turns for visual search. arXiv preprint arXiv:2509.07969 (2025). 17. Yu, C., Liu, J., Nemati, S. & Yin, Z. Reinforcement learning in healthcare: survey. arXiv preprint arXiv:1908. (2021). 18. Ross, S., Gordon, G. & Bagnell, D. reduction of imitation learning and structured prediction to no-regret online learning. In ICML (2011). 19. Bankhead, P. et al. Qupath: Open source software for digital pathology image analysis. Sci. Reports 7, 16878, DOI: 10.1038/s41598-017-17204-5 (2017). 20. Niazi, M. K. K., Parwani, A. V. & Gurcan, M. N. Digital pathology and artificial intelligence. The Lancet Oncol. 20, e262e271, DOI: 10.1016/S1470-2045(19)30154-8 (2019). 21. Mercan, E., Aksoy, S., Shapiro, L. G., Weaver, D. L. & Elmore, J. G. Measurement of visual search patterns with eye tracking: application to pathology. J. Pathol. Informatics 7, 38, DOI: 10.4103/2153-3539.192540 (2016). 22. Ismail Fawaz, H., Forestier, G., Weber, J., Idoumghar, L. & Muller, P.-A. Deep learning for time series classification: review. Data Min. Knowl. Discov. 33, 917963, DOI: 10.1007/s10618-019-00619-1 (2019). 23. Chen, C. et al. Evidence-based diagnostic reasoning with multi-agent copilot for human pathology, DOI: 10.48550/ arXiv.2506.20964 (2025). 2506.20964. 24. Huang, Z. et al. pathologistai collaboration framework for enhancing diagnostic accuracies and efficiencies. Nat. Biomed. Eng. 9, 455470 (2025). 25. Su, Z. et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918 (2025). 26. Jocher, G. Ultralytics yolov5, DOI: 10.5281/zenodo.3908559 (2020)."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Supplementary Material: The Behavior Analyze Pipeline In this section, we provide detailed, step-by-step technical description of the Behavior Analyze pipeline used to preprocess raw pathologist interaction logs into structured, VLM-friendly format. The entire pipeline is visually summarized in 17/23 Figure S1, which illustrates the transformation of noisy, overlapping raw viewport data into clean, normalized set of action bounding boxes. The implementation details described below correspond to the logic in the provided generate_via.py script. Figure S1. The Behavior Analyze Pipeline for Preprocessing Pathologist Navigation. The process transforms raw, high-frequency viewport data into clean set of actions. From left to right: (1) Raw Behavior: The initial noisy data with many overlapping viewports. (2) Initial Action Segmentation: Raw events are grouped into StayInspect (blue) and PanInspect (red) actions based on temporal heuristics. (3) Filtering Overview Actions: Very large, low-magnification bounding boxes corresponding to non-diagnostic overviews are removed. (4) Merging Overlapping Actions: Spatially-proximal actions with high IoU are merged into single, consolidated actions. (5) Pruning Containing Actions: Larger, redundant actions that fully contain smaller, more specific inspection actions are pruned. (6) Normalizing Action Bounding Boxes: The final set of actions is resized to standard dimensions corresponding to discrete magnification levels (e.g., 5x, 10x), creating consistent input format for the VLM. A.1.1 Timing study design and fairness adjustment for Fig. 1c We quantified the efficiency of the human-in-the-loop review relative to creating labels from scratch using controlled timing study. Two users (User 1 and User 2) reviewed AI-drafted text for each region of interest (ROI) using the interface in Fig. S3. For every ROI, the system recorded wall-clock time from ROI load to decision. We summarize three per-round quantities: Verify (accepted with no edits), Revise (accepted after any edit), and their across-round Average. The experiment comprised 10 representative cases and 78 total conversational rounds. To align with the chain-of-thought supervision used in the main task, users in the manual baselines were instructed to provide both final diagnosis (e.g., lymph-node positive vs. negative) and short rationale describing (i) abnormal/atypical features supporting the decision and (ii) normal structures present. Most AI drafts were accepted without edits; the observed revision rates were 15.59% for User 1 and 21.65% for User 2, consistent with over 80% of AI-generated text requiring no edits. Exact reproduction of the source pathologists navigation is infeasible for new users, which would confound per-round timing if reviewers were allowed to free-roam. To ensure comparability, we fixed the items of interest to the source pathologists ROIs for all conditions (verify/revise and manual baselines). Because this anchoring removes the navigation 18/23 Figure S2. Comparison of AI Session Recorder generated Inspect ROI results from two sessions of same attending pathologist. Same pathologist generate very similar results across different session, showing the stability and reliability of AI Session Recorder. Figure S3. software for pathology ROI review presents the slide thumbnail with the selected ROI box, zoomed ROI view, and an optional cytology crop. AI-drafted text for Thumbnail Impression, Why Zoom, and Findings (ROI + Cyto) is shown for the pathologist to edit and either Accept or Reject; pressing Next advances and, if not rejected, treats the ROI as accepted. The header displays case ID, ROI index, progress, elapsed time, and decision state. 19/23 effort that typically precedes manual labeling, manual baselines were adjusted to include the source experts recorded navigation time when locating each ROI. For round we compute manual = (r) (r) write + expert,(r) nav , where Twrite is the time user spent composing text from scratch (typing or dictation) for the fixed ROI and expert is the original pathologists navigation time to that ROI. This yields conservative, apples-to-apples comparison with the verify/edit workflow, which requires no new navigation. nav Finally, we note consistent verbosity gradient across conditions: free text written from scratch was shorter when typing than when dictating, and both were shorter than the AI drafts presented for verification. The latter are intentionally comprehensive to minimize missed details during review. A.1.2 Reviewer interface design The reviewer tool (Fig. S3) was engineered to minimize edit latency while preserving an auditable record of decisions. The layout presents, from left to right, slide thumbnail with the selected ROI box, zoomed ROI view (and an optional cytology crop), and editable text panels for Thumbnail Impression, Why Zoom, and Findings (ROI + Cyto). The header shows case identifier, ROI index, progress, elapsed time, and the current decision state. AI-drafted rationales are segmented at sentence boundaries on load, and each sentence is rendered as an individual paragraph with an adjacent one-click delete control (). Typical drafts contain five to seven sentences. This sentence-level granularity enables surgical editsreviewers can remove single over-claiming or inaccurate sentence without retyping the remainderthereby reducing total revision time and contributing to the low observed revision rates. Global controls allow the reviewer to Accept the edited draft or Reject the ROI when zoom-in is deemed unnecessary; advancing to the next ROI records the decision and timing metadata automatically. A.1.3 Step 1: Initial Action Segmentation Goal: To convert the high-frequency stream of viewport events into an initial set of meaningful, low-level actions. Method: This step is handled by the BehaviorProcessor class in our script. It iterates through the sequence of timestamped viewport events (record.zoom_times, record.zoom_chain). It uses predefined temporal thresholds to identify two foundational action types: Stay Inspect: static inspection is identified if the dwell time in single viewport exceeds threshold (set to 1.0s). Pan Inspect: panning inspection is identified if sequence of pan events occurs continuously for cumulative duration exceeding threshold (set to 2.0s). For each identified action, bounding box is created. For StayInspect, the box is centered on the viewport. For PanInspect, minimal bounding box is calculated to encompass all viewports within the continuous panning sequence. The result is an initial list of VLMAction objects, as shown in the \"Stay & Pan\" column of Figure S1. A.1.4 Step 2: Filtering Overview Actions Goal: To remove non-diagnostic, low-magnification overview actions that do not represent detailed inspection. Method: This step is performed by the filter_big_bboxes() function. Pathologists often zoom out to get their bearings, resulting in extremely large bounding boxes covering substantial portion of the WSI. These actions are not useful for training model to find specific ROIs. The function filters the action list by removing any VLMAction whose bounding box width is greater than set fraction (e.g., 40% or 2/5) of the total WSI height (wsi_height). This effectively removes the \"birds-eye view\" actions, focusing the dataset on genuine inspection events. This is depicted in the \"Remove Big\" column of Figure S1. A.1.5 Step 3: Merging Overlapping Actions Goal: To consolidate multiple, highly-overlapping actions that likely correspond to single cognitive task of inspecting one larger region. Method: The merge_similar() function implements this logic. It iteratively compares all pairs of actions in the current list. If the Intersection over Union (IoU) between two action bounding boxes (calculate_iou()) exceeds defined iou_threshold (e.g., 0.5-0.8), the two actions are considered part of the same inspection event. They are then merged into single new VLMAction (merge_two_actions()). The new actions bounding box is the union of the two original boxes, and its duration spans the full time from the start of the first action to the end of the second. This process repeats until no more merges are possible, resulting in smaller set of more distinct actions, as seen in the \"Merge\" column of Figure S1. 20/23 A.1.6 Step 4: Pruning Redundant Containing Actions Goal: To refine the action list by removing larger, less specific actions that are made redundant by smaller, more detailed inspections occurring within them. Method: This refinement is handled by the filter_mostly_contained_actions() function. It addresses cases where pathologist inspects large area (e.g., lymph node) and then zooms in to inspect specific feature within it (e.g., cluster of tumor cells). In this scenario, the smaller, high-magnification action is more informative. The function compares every pair of actions. If smaller actions bounding box is mostly contained within larger actions box (i.e., the intersection area is >90% of the smaller boxs area, checked by check_containment_by_area()), the larger, containing action is marked as redundant and removed from the list. This prioritizes the most detailed and specific points of interest identified by the pathologist, as illustrated in the \"Find Include\" column of Figure S1. A.1.7 Step 5: Normalizing Action Bounding Boxes Goal: To standardize the final action bounding boxes into consistent format, mimicking discrete microscope objectives. Method: The final step, standardize_action_bboxes(), ensures that the input to the VLM is uniform. It maps each actions bounding box to one of two standard sizes based on its original area, corresponding to \"5x\" (larger box) or \"10x\" (smaller box) view. The function calculates the center of the final, pruned bounding box and resizes it to the standard dimension (e.g., wsi_height / 5.0 for 5x view). This process also re-bins the actions magnification label (magnification_bin) to match the standardized size. The output is clean, normalized set of action bounding boxes of consistent sizes, centered on the pathologists key areas of focus. This final, structured output is shown in the \"Normalize\" column of Figure S1 and is ready for use in generating ground truth for model training. A.2 Implementation Details on Dataset Construction Seg-Zero Re-implementation. The evaluation of Seg-Zero?s 7B model is conducted using the official model checkpoint available in the public repository. Due to the unavailability of the 3B checkpoint, we re-train the model using the official codebase under the same experimental settings to ensure fair comparison. The prompts used for both models strictly adhere to the official implementation, as shown below. For consistency, the same prompt format is also adopted in our proposed model. Original Prompt from Seg-Zero Please find {Question} with bbox and points. Compare the differences between objects and find the most closely matched one. Output the thinking process in <think> </think> and final answer in <answer> </answer> tags. Output the one bbox and points of two largest inscribed circles inside the interested object in JSON format. i.e., <think> thinking process here </think> <answer>Bbox: [10, 100, 200, 210], Point 1: [30, 110], Point 2: [35, 180]</answer> A.3 Ablation Study on Implementation A.4 Prompts for VLM Analysis This section details the specific prompts used to guide the Vision-Language Model (VLM) at different stages of the diagnostic workflow."
        },
        {
            "title": "Prompt for Initial WSI Overview Analysis",
            "content": "This is an H&E WSI of CRC case. The task is to find all positive lymph nodes. An expert pathologist has identified the regions of interest marked in the image for closer examination. What is your initial impression of the overall image? Format your response as: <impression>your overall impression of the slide</impression>"
        },
        {
            "title": "Prompt for Region of Interest Analysis",
            "content": "What is your impression on this region? Second image is maximum-magnification crop from the center of the region, intended for observing fine cytological features such as nuclei and cytoplasm. Please consider that space between cells is not always cytoplasm, but could be an artifact from processing. Lymph node that is dead or completely occupied by tumor / or dead tumor cell, this should be called tumor deposit not positive lymph node. 21/23 Prompt for Final Summary and Diagnosis Generation Please provide comprehensive final pathological impression and diagnosis based on all the above analyses. Consider: 1. The overall tissue architecture and morphology 2. Findings from each specific region 3. Any patterns or correlations between regions 4. Your final diagnostic impression, please do not consider suspicious region as positive Format your response EXACTLY as follows: <final_impression>Your comprehensive final diagnostic impression</final_impression> <recommendations>Any additional recommendations</recommendations> <diagnostic_info> PT_or_LN: \"PT\" if this is primary tumor section, \"LN\" if this is lymph node section t_stage: [1-4] if primary tumor, 0 if lymph node lymph_node_positive: true/false positive_regions: if lymph_node_positive is true, give [1,2,3] like list. if false just say [] suspicious_regions: [] </diagnostic_info> A.5 Method for Structuring Semi-Automated Thinking Labels To transform the semi-automated, free-text Thinking Labels into structured, analyzable data, we developed classification pipeline powered by large language model (models/gemini-2.5-flash). This method processes each thinking label to assign multiple, relevant pathological tags for both low-magnification (regional) view and high-magnification (cellular) view simultaneously. The core of this method is carefully designed prompt that instructs the LLM to perform two multi-label classification tasks in single API call. The model is required to return the results as two comma-separated lists, delimited by pipe character (). This structured output format is crucial for robust parsing and minimizes the need for complex post-processing. The classification taxonomy was designed to be comprehensive for colorectal cancer lymph node metastasis. For the low-magnification (box-level) view, classes are grouped into three categories: (1) Tumor-related features (e.g., \"Gland formation\", \"Necrosis\"), (2) Normal Lymph Node Structures (e.g., \"Germinal center\", \"Sinus\"), and (3) Reactive or Pathological Changes (e.g., \"Fibrosis\", \"Hemorrhage\"). This allows for capturing the overall tissue architecture. For the high-magnification (40x-level) view, classes are also grouped into three categories: (1) Tumor Cell-related features (e.g., \"Tumor cell\", \"Mitotic figure\"), (2) Normal Background Cells and Structures (e.g., \"Lymphocyte\", \"Extracellular matrix\"), and (3) Pathological Processes (e.g., \"Apoptotic body\", \"Dead cell\"). This captures fine-grained cytological details. The specific prompt used for this pipeline is detailed below. 22/23 Prompt for Multi-Label Classification of Thinking Labels Based on the conversation text, classify what is observed in this ROI for both low-magnification (box) and high-magnification (40x) views. This is MULTI-LABEL classification - multiple classes can be present simultaneously. Box classes (low-magnification): Tumor deposit, Gland formation, Tumor stroma, Necrosis, Germinal center, Lymphoid follicle, Medullary cord, Sinus, Paracortex, Sinus histiocytosis, Fibrosis, Congestion, Hemorrhage, Fatty replacement, other 40x classes (high-magnification): Tumor cell, Mitotic figure, Atypical glandular cell, Signet ring cell, Lymphocyte, Plasma cell, Macrophage, Endothelial cell, Fibroblast, Erythrocyte, Fat cell, Extracellular matrix, Apoptotic body, Dead cell, Inflammatory cell, other Return ONLY the two classification results in this format: box_classifications40x_classifications Where each classification is comma-separated list of applicable classes. Conversation text: text Instructions: - Choose ALL applicable classes for each view from the provided lists - Multiple classes can be present simultaneously (e.g., \"Gland formation,Fibrosis\") - If none match well, use \"other\" - Return only the two classification lists separated by A.6 Specification of Evaluated Vision-Language Models The Vision-Language Models (VLMs) evaluated in our experiments comprise both closed-source and open-source models. When evaluating closed-source models provided as service through an API (e.g., from Google and OpenAI), ensuring reproducibility is significant challenge. These models are often updated by the provider companies without public versioning, which can lead to performance shifts over time. To address this, we document the specific access timeframes for these models. For open-source or specifically versioned models, the model identifier itself is sufficient for reproducibility. The table below provides these details for each model abbreviation used in our figures. Table S3. Detailed specification of the VLM backbones used for comparison experiments. Abbreviation Model and Access Details Gemini Grok 4o-mini Claude o4-mini GPT-4.1 Qwen-VL Phi-4 Llama-4 InternVL o3 Official Google API, accessed in June 2025. grok-2-vision-1212. Official OpenAI API, accessed in June 2024. claude-3.5-sonnet-20240620. Official OpenAI API, accessed in July 2025. Official OpenAI API, accessed in July 2025. Qwen-VL-Plus. microsoft/phi-4-multimodal-instruct. llama-4-maverick. internvl3-14b. Official OpenAI API, accessed in July 2024. 23/"
        }
    ],
    "affiliations": [
        "Department of Biostatistics, Epidemiology & Informatics, University of Pennsylvania",
        "Department of Electrical and System Engineering, University of Pennsylvania",
        "Department of Pathology and Laboratory Medicine, University of Pennsylvania",
        "Department of Pathology, Stanford University",
        "Department of Pathology, University of California at San Francisco"
    ]
}