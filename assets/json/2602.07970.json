{
    "paper_title": "Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity",
    "authors": [
        "Zheyuan Hu",
        "Weitao Chen",
        "Cengiz Öztireli",
        "Chenliang Zhou",
        "Fangcheng Zhong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 ] . [ 1 0 7 9 7 0 . 2 0 6 2 : r LEARNING-GUIDED KANSA COLLOCATION FOR FORWARD AND INVERSE PDES BEYOND LINEARITY Zheyuan Hu1, Weitao Chen2, Cengiz Oztireli1, Chenliang Zhou1, Fangcheng Zhong1 1 Department of Computer Science and Technology, 2 Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK. {zh369, wc358}@cam.ac.uk, {chenliang.zhou, fangcheng.zhong}@cst.cam.ac.uk Co-corresponding authors."
        },
        {
            "title": "ABSTRACT",
            "content": "Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent Zhong et al. (2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and comprehensive survey of neural PDE solvers and scientific simulation applications."
        },
        {
            "title": "INTRODUCTION",
            "content": "PDEs are useful in different domains of scientific computing, including physics, graphics and biology. They describe how quantities change over space and time, making them essential for modeling phenomena such as fluid dynamics, electromagnetic fields, heat transfer, and population dynamics. Zhong et al. (2023) proposed extension to Kansa method, which is mesh-free Radial Basis Functions (RBFs) PDE solver. They introduced auto-tuning of the shape parameters of RBFs. However, their work focuses only on single-variable linear PDEs. Therefore, this paper extends CNFs backend solver to multi-dependent-variable and nonlinear PDEs, and apply the framework to specific scientific simulation problems, including forward computation and inverse problems. Its unknown how (extended) CNF solvers Zhong et al. (2023) compared with other classical and neural PDE solvers. Hence, we also implement and evaluate selected prior methods on the benchmarks on their effectiveness with different quality metrics (e.g. L1, L2, errors) against ground truth solutions, efficiency, computation resource, convergence speed, method complexity, and finally utility in research, i.e. their scientific simulation applications or integration with other methods, e.g. differentiable rendering to solve inverse physics-related problems in Graphics."
        },
        {
            "title": "2 RELATED WORK",
            "content": "PDE benchmarks. We identified several representative equations Takamoto et al. (2022) in Table 14. They are different in linearity of the operator and solution dimensionality. PDE solvers. Numerical methods, e.g. Finite Difference Method (FDM) and Finite Element Method (FEM), are widely used to solve PDEs. However, they suffer from the curse of dimensionality, high computation costs and domain-specific discretization. Recently, neural network based solvers have shown promising results in addressing these issues. For example, Physics-Informed Neural Networks (PINNs) Raissi et al. (2019) and Fourier Neural Operators (FNOs) Li et al. (2020) have demonstrated the ability to generalize to unseen scenarios and handle high dimension effectively. 1 Inverse problem, i.e. estimating unknown parameters or inputs of variable from given solution observations u, is crucial. However, its unclear how CNFs can be applied to these problems, including connecting with differentiable rendering pipelines Spielberg et al. (2023) in Visual Computing."
        },
        {
            "title": "3.1 GENERAL FORM OF PDES",
            "content": "With spatial domain Ω Rd, where its dimension is d, and the unknown field u(x, t) : defined on the spatio-temporal domain = Ω [t0, tf ] Rd+1, the general form of PDEs is, (cid:26) D[u] = f, Ω, [t0, tf ], Bi[u] = gi, Ωi, [t0, tf ]. (cid:26) D[u](x, t) = (x, t), Ω, [t0, tf ], . Bi[u](x, t) = gi(x, t) Ωi, [t0, tf ]. (1) where : is the differential operator and : Rm is source function, e.g. the external force in dynamics, with being the output dimension of 1. The differential operators include the gradient , Laplace , divergence , etc. For boundary conditions, Bi : Zi is each boundary operator with gi Zi : Ωi [t0, tf ] Rni and ni as the output dimension of gi. 3.2 KANSA COLLOCATION Kernel functions. Radial Basis Function (RBF) inversely relates the distance between the input and fixed origin point to the output value. There are various infinitely smooth RBFs, among which we choose Gaussian RBF for its effectiveness in approximating smooth functions, ψc(r) = ψc(x c). (2) ψc(r) = e(ϵr)2 , 1 1+(ϵr)2 , (cid:112)1 + (ϵr)2, Gaussian, Inverse quadratic, Multiquadrics, (3) where Gaussian shape parameter ϵ = 1 2σ , and σ is the standard deviation. Kansa method Kansa (1990) approximates the solution u(x, t) with linear combination of kernel functions ψi(x xi) Rd centered at each collocation points {xi D}N i=1, u(x, t) ˆu(x) = (cid:88) i=1 αi ψi(x xi), D, (4) where αi are the coefficients to be solved. The time dimension is omitted, which can be treated as an additional spatial dimension here. Equation equation 4 is expressed as, by rewriting the kernel functions into matrix form, (cid:124) ˆu(x1) ˆu(x2) ... ˆu(xN ) (cid:123)(cid:122) uRN ψ1(x1 x1) ψ1(x2 x1) ... = ψ2(x1 x2) ψ2(x2 x2) ... ψ1(xN x1) ψ2(xN x2) ψN (x1 xN ) ψN (x2 xN ) . . . ψN (xN xN ) ... (cid:125) (cid:124) (cid:123)(cid:122) kernel matrix KRN . (5) (cid:125) α1 α2 ... αN (cid:124) (cid:123)(cid:122) (cid:125) aRN The PDE general form equation 1 can be summarized as single equation, F[ˆu](xi) = h(xi), xi D, (6) where the operator = {D, Bi} and = {f, gi} represent both the initial and boundary conditions. 1Note that and are two function spaces, and we require they are Banach spaces."
        },
        {
            "title": "3.2.1 LINEAR OPERATOR CASE",
            "content": "By plugging in the approximation of equation 4 and assuming the operator is linear2, the PDE equation 6 can be simplified as, F[ˆu](xi) = F[ (cid:88) αi ψi](xi) = i=1 (cid:88) i=1 αi F[ψi](xi) = h(xi). (7) By expanding in matrix form, the above equation is, F[ψ1](x1) F[ψ2](x1) F[ψ1](x2) F[ψ2](x2) ... ... F[ψ1](xN ) F[ψ2](xN ) (cid:124) (cid:123)(cid:122) operator-evaluated kernel matrix FRN F[ψN ](x1) F[ψN ](x2) . . . F[ψN ](xN ) ... = (cid:125) α1 α2 ... αN (cid:124) (cid:123)(cid:122) (cid:125) aRN h(x1) h(x2) ... h(xN ) (cid:123)(cid:122) constraint values hRN (cid:125) (cid:124) . (8) Concretely, when the kernel function is Gaussian RBF defined in equation 2, and xi = (xi, ti), ψi = r2 2σ2 , = (x xi)2 + (t ti)2. r2 Take = 3 and by the chain rule, the element Fj,i in the matrix equation 8 is thus, F[ψi](xj) = ψi(xj) = ψi(xj) r2 r2 = 1 2σ2 r2 2σ2 2(tj ti) = tj ti σ2 r2 2σ2 . (9) (10) Simultaneous equations. When there are Neq equations of different constraints to be satisfied, the collocation points {xi D}Ntotal i=1 are distributed among all equations4, where Ntotal = (cid:80)Neq {1, . . . , Neq}, xi D. Fj[ˆu](xi) = hj(xi), j=1 Nj. (11) Equation equation 8 can be extended by stacking each matrix F(j) RNtotalNtotal and constraint vector h(j) RNtotal vertically for all equations Zhong et al. (2023). The block matrix form is, F(1) ... F(Neq) (cid:123)(cid:122) stacked FR(Neq Ntotal )Ntotal (cid:124) (cid:125) α1 ... αNtotal (cid:124) (cid:123)(cid:122) (cid:125) aRNtotal = h(1) ... h(Neq) (cid:123)(cid:122) stacked hRNeq Ntotal (cid:125) (cid:124) . (12) The solution of equation 4 depends on the coefficients = [α1, α2, . . . , αN ], which can be solved by the linear system Fa = h. The general form is given by the least squares approximation, i.e. minimizing the norm of the error vector and setting the gradient to zero, aopt = min (Fa h)2, a(Fa h)T (Fa h) = 0 = (FT F)aopt = FT h. (13) If matrix is full rank, FT is invertible, thus one can derive aopt = (FT F)1FT h. Should the matrix be square and invertible, equation 13 can be further simplified as aopt = F1h. Whichever conditions occurs, the final solution for is approximated by plugging in the optimal coefficients aopt into equation 5, i.e. ˆu(x) = aopt. D}M When testing on unseen data points {x test points and the collocation points {xi D}N j=1, the kernel functions are constructed between the i=1. The solution is thus, u(x, t) ˆu(x) = (cid:88) i=1 αi ψi(x xi), D, (14) 2For linear operators, [α ψ] = α F[ψ], as defined in A. 3 ψi : phi = torch.autograd.grad(phi, t, create graph=True)[0] 4Note that repeated collocation points are forced to be repeated here for distinct constraints. Test-time solution equation 14 can be formulated to matrix form, (cid:124) ˆu(x 1) ˆu(x 2) ... ˆu(x ) (cid:123)(cid:122) uRM (cid:125) = (cid:124) ψ1(x 1 x1) ψ1(x 2 x1) ... x1) ψ2(x ψ2(x 1 x2) ψ2(x 2 x2) ... x2) (cid:123)(cid:122) kernel matrix KRM ψ1(x ψN (x 1 xN ) ψN (x 2 xN ) ... . . . ψN (x xN ) (cid:125) . (15) α1 α2 ... αN (cid:124) (cid:123)(cid:122) (cid:125) aRN"
        },
        {
            "title": "3.2.2 EXTENSION 1: COUPLED SOLUTION FIELDS OF PDES",
            "content": "Coupled multi-dimensional PDE solution fields. Assuming there are ND solution dimensions, i.e. = [u1, u2, . . . , uND ], the Kansa approximation equation 4 for each dimension is, ˆud(x) = (cid:88) i=1 α(d) ψ(d) (x xi), {1, . . . , ND}. (16) The couple PDE equation is thus formulated as applying the coupling, or governing operator on all dimensions of solution, which each has its own operator (d), (cid:16) F (1)[ˆu1], . . . , (ND)[ˆuND ] (cid:17) (xi) = h(xi), xi D. Here we assume the coupling operator is linear with each dimension of solution, (ˆv1, . . . , ˆvND ) (x) = ND(cid:88) d=1 βd ˆvd(x), (17) (18) where βd is the per-dimension weight. Equation equation 8 can be extended by stacking each matrix F(d) RN horizontally for all dimensions of solution. The block matrix form is, [β1IN βND IN ] (cid:125) (cid:123)(cid:122) (cid:124) βRN (ND ) F(ND)(cid:3) (cid:2)F(1) (cid:124) (cid:125) (cid:123)(cid:122) coupling FRN (ND ) a(1) ... a(ND) (cid:125) (cid:123)(cid:122) (cid:124) aR(ND ) = h(x1) ... h(xN ) (cid:124) (cid:125) (cid:123)(cid:122) stacked hRN , (19) where is element-wise or Hadamard product and IN is the identity matrix of size . For simultaneous coupled PDE equations, similar to equation 11, they are indexed by {1, . . . , Neq}, (cid:16) Gj (1) [ˆu1], . . . , (ND) [ˆuND ] (cid:17) (xi) = hj(xi), {1, . . . , Neq}, xi D. (20) With Ntotal defined as in equation 12, the block matrix form is, β(1) ... β(Neq) (cid:123)(cid:122) βR(Neq Ntotal )(ND Ntotal ) (cid:125) (cid:124) (cid:124) F(1,1) ... F(Neq,1) F(1,ND) ... F(Neq,ND) F(j,d) (cid:123)(cid:122) coupling FR(Neq Ntotal)(ND Ntotal ) (cid:125) a(1) ... a(ND) (cid:123)(cid:122) aR(ND Ntotal ) (cid:125) (cid:124) = h(1) ... h(Neq) (cid:123)(cid:122) stacked hR(Neq Ntotal) (cid:125) (cid:124) . 3.2.3 EXTENSION 2: NONLINEAR OPERATOR CASE When the operator is nonlinear, we can no longer simplify equation 6 as in equation 7. However, we can still derive the relation between the solution and its linear transformed version as below. (21) Differentiable matrix helps decompose the general non-linear operator into series of linear operators. Take any linear operator, e.g. , it relates the relation between unknown and its derivative = Dx u. We derive Dx from Kansa equation 4, by linearity and equation 7, (cid:88) u(x) = αi ψi(x xi). (22) x i=1 4 In matrix form, we have = Kx a, where the matrix Kx RN is constructed by evaluating ψi(x xi)x=xj for all i, {1, . . . , } as row and column indices. We can invert equation 5 = K1 u, assuming invertibility from independent basis. By substituting into = Kx a, one gets = Kx K1 u. The differentiable matrix is thus, Dx = Kx K1 RN . (23) For viscous Burgers equation equation 63 F[u] = which follows the same formulation as in equation 23 by replacing the operator accordingly, is, x2 . Its differentiable matrix form, + x ν 2u F[u] = Dt + (Dx u) ν(Dxx u). (24) Here we present two categories of Kansa approaches  (Table 1)  . The first consists of four timestepping schemes, including two per-step linear and another two nonlinear systems. The second employs fully nonlinear solver on the PDE residuals, without explicit time discretization. Table 1: Summary of different non-linear Kansa solver features, is the time step size, Nx and Nt are the number of collocation points in spatial and temporal dimensions respectively. Features Time-step Error Stability Memory forward explicit O(t) unstable O(N 2 ) IMEX backward CrankNicolson fully non-linear semi-explicit O(t) stable O(N 2 ) implicit O(t) stable O(N 2 ) implicit O(t2) stable O(N 2 ) O(1) N/A O(N 2 2 ) Time-stepping approach with linear system. We can remove the non-linearity by discretizing the time derivative via finite difference method, for special case of time-dependent PDEs. One solution is to use the (1) explicit forward Euler scheme, t + D[u] = 0 = un+1 un where is the spatial operator. more stable solution is to use the (2) implicit-explicit (IMEX) scheme, which splits the stiff and non-stiff parts of the operator = Istiff + Enon-stiff, which stiffness means the numerical instability incurred by the operator and needs to be treated implicitly, + O(t) + D[un] = 0, (25) un+1 un + O(t) + Istiff[un+1] + Enon-stiff[un] = 0. (26) Despite the non-linear spatial operator or Enon-stiff, we already know the solution un at time step n. Thus, with differentiable matrices, one can evaluate D[un] or Enon-stiff[un] directly, so as to derive the solution un+1 at the next time step + 1. Time-stepping approach with nonlinear solver. If we discretize the time derivative via the (3) backward Euler scheme, the non-linearity remains in the formulation, t + D[u] = 0 = un+1 un + O(t) + D[un+1] = 0. (27) We can directly replace linear system solver by non-linear system solver, e.g. Newton-Raphson method Ypma (1995), to minimize the residual vector and derive the unknown solution at next time step, rn+1, where rn+1 = un+1 un + D[un+1]. (28) un+1 = arg min un+1 Alternatively, (4) Crank-Nicolson scheme can be used to discretize second-order accurate in time, t + D[u] = 0 = un+1 un + 1 2 (cid:0)D[un+1] + D[un](cid:1) + O(t2) = 0. (29) Similar with equation 28, the unknown solution at next time step is derived by minimizing the residual vector as stated in equation 29. 5 Fully nonlinear solver without time-stepping. This approach directly minimizes the PDE residuals equation 6 over all collocation points, without explicit time discretization. After plugging in the differentiable matrix form of the non-linear operator F, the objective function is therefore, α = arg min α (cid:88) i=1 (F[ˆu](xi) h(xi))2 . (30) By plugging in Kansa approximation equation 4, we derive unknown solution over entire domain."
        },
        {
            "title": "3.2.4 AUTO-TUNING OF KANSA HYPERPARAMETERS",
            "content": "To tune the key Kansa method hyperparameter, kernel shape parameter ϵ in equation 3, Zhong et al. (2023) proposed one of the self-tuning methods for ϵ, by minimizing the variation of the solution field over all collocation points, and the condition number of the kernel matrix F, ϵ = arg min ϵ ω1 cond(F) + ω2 (cid:90) u(x)2dx, (31) where cond(F) is the condition number of matrix defined in equation 8. The integral term can be approximated by summing over all collocation points by Monte Carlo integration. This approach works for linear, including coupled and multi-dimensional, PDEs. For non-linear operator case, the solution depends on ϵ implicitly via the coefficients αi. The matrix no longer exists explicitly. Here, we propose to directly minimize the PDE residuals over all collocation points, the total variation of the solution field u, and the training L2 loss between the predicted solution and the ground truth solution ugt if available, ϵ = arg min ϵ ω1 (cid:88) i=1 (F[ˆu](xi) h(xi))2 + ω2 (cid:90) u(x)2dx + ω3 ugt2, (32) where ω1, ω2, and ω3 are the weights for each penalty term. 3.3 SOLUTIONS OF INVERSE PDE PROBLEMS Inverse PDE problems. When given observations of solution field uobs, we infer the unknown PDE parameters π that minimize the discrepancy between the predicted upred(π) and uobs, π = arg min π L(uobs, upred(π)). (33) We adopt the SciPy implementation of the least squares and root finding algorithms, which are either gradient-based or gradient-free, detailed in the evaluation section."
        },
        {
            "title": "4 EVALUATION",
            "content": "4.1 PERFORMANCE METRICS Accuracy. Given the numerical solution ˆui from PDE solvers, and the corresponding ground truth ui, the L2 risk RL2 is the average discretized error over all Ntest test points, ˆRL2 = 1 Ntest Ntest(cid:88) i= ˆui ui2, ˆRrelative L2 = 1 Ntest Ntest(cid:88) i=1 ˆui ui2 ui . (34) The relative L2 risk is computed from RL2 and normalized by the ground truth ui L2 norm, 4.2 EVALUATION OF SOLVERS FOR THE ADVECTION EQUATION For the 1D advection equation defined in equation 45, we set the number of domain quadrature points NR = 100 10, i.e. initial condition (IC) points Nd = 10 and the boundary condition (BC) points NB = 100 2. The advection equation is initialized as per Table 2. Table 2: 1D advection equation experimental setup. domain time range parameter IC BC x0 = 0, xf = 1 t0 = 0, tf = 1 β = 0.4 u0(x) = sin(2πx) per equation 46 FNO requires multiple instances of PDEs for training. Hence, we generate Npde = 100 instances by varying only the initial condition as, given ck (0, 1), u0(x) := u0(x) maxx u0(x) , where u0(x) = 5 (cid:88) k=1 ck sin(2πkx). (35) For training, PINN and FNO are trained via learning rate η = 103 until convergence, i.e. with epoch iterations Niter = 3000 for PINN and Niter = 100 for FNO. For evaluation, the test points Ntest = 64 8. The error is measured by relative L2 risk ˆRrelative L2 equation 34. 4.2.1 FORWARD PROBLEM Since FNO is trained on Npde = 100 instances of PDEs, we compensate more training data for single-instance solvers for fair comparison. The adjustment factor is defined as Cscale [1, Npde] R+. Hence, the domain points is = Cscale NR and methods denoted as FDMCscale and PINNCscale. The test-time results are summarized in Table 3. Table 3: Models accuracy ˆRrelative L2 103, on 1D advection relative to the data domain resolution. Cscale 1 22 42 Npde = 102 average FDM PINN FNO KM 36.63 17.05 7.478 3.228 16.10 12.87 300.2 20.68 8.654 6.457 83.99 124. 744.3 58.71 37.68 13.37 213.5 306.9 1.918 0.0028 N/A N/A 0.9603 0.9576 From Table 3, we conclude that all solvers are sensitive to the number of training data, where larger Cscale leads to better precision on test points. Kansa outperforms other methods in both accuracy and convergence speed, achieving the least error (up to 106) with only Cscale = 42. However, due to the increasing computational cost above Cscale = 102, memory limit was exceeded. 4.2.2 INVERSE PROBLEM For the 1D advection equation equation 45 initialized in Table 2, we set up the inverse PDE problem to infer the initial parameter β from the observation data uobs at all time steps. All methods are evaluated at their best performance from the forward problem. The results are summarized in Table 4, with the initial parameter β0 set. Table 4: Inverse predictions of β on advection equation, where the ground truth β = 0.4. β0 0.2 FDM PINN FNO 0.402 0. 0.3985 KM β0 1.0 0.402 FDM PINN FNO KM 1. 0.40446 1.2267 0.402 For local optimization methods when searching for the optimal parameter, they stuck at different local minima depending on the initial guess β0. With different runs of initial guesses, they give more precise predictions with more computational cost."
        },
        {
            "title": "4.3 EXTENSION 1: KANSA METHOD FOR COUPLED PDES",
            "content": "The Lotka-Volterra equations equation 47 are initialized as per Table 5, where the number of domain quadrature points NR = 100, and initial condition points Nd = 1. For evaluation, the test points Ntest = 64. The results from Kansa method are summarized in Table 7, where the Gaussian RBF shape parameters, as defined in equation 3, are set as ϵ = 0.2 for both x(t) and y(t). Table 5: 1D Lotka-Volterra equations experimental setup. time range t0 = 0, tf = 200 parameter initial conditions α = 0.1, β = 0.02, δ = 0.01, γ = 0.1 x(0) = 40, y(0) = 9 The 1D Maxwells equations as defined in equation 58 are initialized per Table 6, where the speed of time propagation = 1, the number of domain quadrature points NR = 12 12, and initial condition points Nd = 24. For evaluation, the test points Ntest = 10 10. The shape parameter of Gaussian RBF, as defined in equation 3, is set as ϵx = 0.21 and ϵy = 0.2 for Lotka-Volterra equations and ϵE = 16 and ϵB = 16 for Maxwells equations, respectively. Table 6: 1D Maxwells equations experimental setup. domain x0 = 0, xf = time span parameter t0 = 0, tf = 1 2 = 1 initial conditions Ez(x, 0) = sin(2πx) + 1 By(x, 0) = cos(2πx) + 1 2 sin(4πx) 2 cos(4πx) 4.3.1 FORWARD PROBLEM Table 7: ˆRrelative L2 error of Lotka-Volterra and Maxwells equations using Kansa method. Cscale 1 4 x(t) y(t) 0.1279353 0. 0.055667494 0.06230465 Ez(x, t) 0.8049189 0.4383743 By(x, t) 0.5894967 0.3830594 Accuracy. The results from Kansa method are summarized in Table 7. Both errors converge with increasing Cscale. Efficiency. The training time and inference time of Kansa method on LotkaVolterra equations are 0.4034 and 0.0001 seconds, respectively. The training time and inference time of Kansa method on Maxwells equations are 0.4486 and 0.0005 seconds. 4.3.2 INVERSE PROBLEM For the Lotka-Volterra defined in equation 47 initialized in Table 5, we set up the inverse problem to infer the initial parameter α, β, δ and γ from observation xobs(t) and yobs(t) at all time steps. Table 8: Inverse predictions of α, β, δ and γ on Lotka-Volterra equations. α β δ γ α β δ γ reference 0. 0.02 0.01 0.1 prediction 0.102 0. 0.0100 0.0994 With the initial guess all set to 1, the results are summarized in Table 8. Despite the four-dimensional search space, the optimization algorithm SciPy Powell method successfully infers the parameters with high accuracy and decent computational cost."
        },
        {
            "title": "4.4 EXTENSION 2: KANSA METHOD FOR NONLINEAR PDES",
            "content": "The Burgers equation defined in equation 63 is initialized as per Table 9, where the number of domain quadrature points NR = 6416, i.e. initial condition (IC) points Nd = 64 and the boundary condition (BC) points NB = 16 2. For evaluation, the test points Ntest = 48 12. The Gaussian RBF shape parameter, as defined in equation 3, is set as ϵ = 0.9. Table 9: Burgers equation experimental setup. domain time span param. ICs BCs x0 = 10, xf = 10 t0 = 0, tf = 4 ν = 0.5 per equation 74 u(x0) = 1, u(xf ) ="
        },
        {
            "title": "4.4.1 FORWARD PROBLEM",
            "content": "Stability. For four time discretization schemes, only forward Euler scheme is unstable  (Table 10)  , where time step exceeds the stability limit when scale = 1 and 2 according to CFL condition. Table 10: Stability test of forward Euler Kansa method on Burgers equation. scale ˆRrelative L2 Stability 3.74 1029 unstable 2 NaN unstable 4 10 4.31 103 stable 3.11 103 stable Accuracy. From Table 11, we observe that fully non-linear approach outperforms other timestepping schemes. Its hard to determine whether IMEX or backward Euler is more accurate theoretically. However, Crank-Nicolson scheme is definitely more accurate than both IMEX and backward Euler, since its second-order accurate in time while the other two are only first-order accurate. Table 11: ˆRrelative L2 102 error of Burgers equation using Kansa methods. forward 3.74 1031 IMEX backward CrankNicolson fully non-linear 1.68 1.33 1.29 0.012 Computational efficiency. We measure the training and inference time of different Kansa methods on Burgers equation in Table 12. The non-linear solver used is the SciPy least-squares. Table 12: Train or infer time of Burgers equation using Kansa methods (in seconds). forward IMEX backward CrankNicolson fully non-linear Training Inference 0.34 0.436 0.47 0.272 2.33 1.053 1.66 1.109 99.2 0.005 Training time for fully nonlinear approach is longer because each step involves heavier computation with substantial memory  (Table 1)  , further compounded by nonlinear solvers. Four time-stepping schemes have much less training time. The forward Euler is unstable when the stability condition is not satisfied. Inference time of fully non-linear approach is significantly reduced, due to the reuse of coefficient from the training phase. Despite full test-time recomputation from scratch, the inference time of four time-stepping schemes remains acceptable for most practical applications. 9 4.4."
        },
        {
            "title": "INVERSE PROBLEM",
            "content": "For the Burgers equation defined in equation 63 initialized in Table 9, we set up the inverse PDE problem to infer the initial parameter ν from the observation data uobs at all time steps. The results are summarized in Table 13, with the initial parameter ν0 set as 0.1. Table 13: Inverse predictions of ν on Burgers equation, where the ground truth ν = 0.5. forward 0."
        },
        {
            "title": "IMEX",
            "content": "0.535 backward CrankNicolson fully non-linear 0.467 0. 0.500 Accuracy. Under same optimizer and initial guess, the Crank-Nicolson scheme confirms its theoretical advantage  (Table 1)  over both IMEX and backward Euler, which stuck at local minima. Computational efficiency. Fully non-linear approach requires retraining for each new parameter, which is computationally expensive  (Table 12)  . To speed up the per-run training time, it is trained with maximum iteration. Stability. The forward Euler scheme is unstable when given large t."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "This paper extends Zhong et al. (2023) RBF framework solver beyond original scope of linear PDEs. In particular, we generalize its PDE solver to handle multi-dependent-variable and nonlinear PDEs, addressing the loss property of linear reordering. These broaden the applicability of CNFdriven self-tuning mesh-free solvers to both forward modeling and inverse problem formulations. In addition, this work contributes systematic empirical study of how CNF solvers compare with established classical and neural PDE solvers. By implementing representative prior methods and evaluating them across benchmark problems, we assess their relative performance in terms of solution accuracy, efficiency, convergence and complexity. Such comparisons clarify the strengths and limitations of CNF-based approaches within the broader landscape of PDE solvers. Overall, this paper demonstrates that learning-guided Kansa solvers can serve as promising and flexible tool for coupled or nonlinear PDE systems. Future work includes theoretical analysis of error and convergence properties, application to neural field in computing, and integration with differentiable pipelines in scientific domains."
        },
        {
            "title": "REFERENCES",
            "content": "Robert A. Adams and John J. F. Fournier. Sobolev spaces, volume 140 of Pure and Applied Mathematics. Academic Press, Boston, MA, 2 edition, 2003. ISBN 978-0-12-044143-3. Originally published in 1975. Michael Athanasopoulos, Hassan Ugail, and Gabriela Gonzalez Castro. Parametric design of aircraft geometry using partial differential equations. Advances in Engineering Software, 40(7):479486, 2009. ISSN 0965-9978. doi: https://doi.org/10.1016/j.advengsoft.2008.08.001. URL https: //www.sciencedirect.com/science/article/pii/S0965997808001531. Nicolas Bacaer. Lotka, Volterra and the predatorprey system (19201926). In Short History of Mathematical Population Dynamics, pp. 7176. Springer, London, 2011. doi: 10.1007/ 978-0-85729-115-8 13. URL https://doi.org/10.1007/978-0-85729-115-8_ 13. Adam W. Bargteil and Tamar Shinar. An Introduction to Physics-based Animation. ACM SIGGRAPH 2018 Courses, 1(1):157, August 2018. doi: 10.1145/3214834.3214849. HARRY BATEMAN. Some recent researches on the motion of fluids. Monthly Weather 10.1175/1520-0493(1915)43163:SRROTM2.0. URL https://journals.ametsoc.org/view/journals/mwre/43/4/ Review, 43(4):163 170, 1915. CO;2. 1520-0493_1915_43_163_srrotm_2_0_co_2.xml. doi: Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in Machine Learning: survey. Journal of Machine Learning Research, 18(153):143, 2018. URL http://jmlr.org/papers/v18/17-468.html. Richard E. Bellman. Dynamic programming. Princeton University Press, Princeton, NJ, 1957. ISBN 978-0-691-07951-6. Prepared for the Rand Corporation. Richard Courant, Kurt Friedrichs, and Hans Lewy. der mathematischen Physik. Mathematische Annalen, 100(1):3274, 1928. BF01448839. Uber die partiellen Differenzengleichungen doi: 10.1007/ L.C. Evans. Partial Differential Equations. Graduate studies in mathematics. American Mathematical Society, 2010. ISBN 9780821849743. URL https://books.google.co.uk/books? id=Xnu0o_EJrCQC. Walter Greiner. Maxwells equations, pp. 250275. Springer New York, New York, NY, 1998. ISBN 978-1-4612-0587-6. doi: 10.1007/978-1-4612-0587-6 13. URL https://doi.org/ 10.1007/978-1-4612-0587-6_13. Eberhard Hopf. The partial differential equation ut + ux = µ uxx. Communications on Pure and Applied Mathematics, 3(3):201230, 1950. doi: https://doi.org/10.1002/cpa.3160030302. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160030302. Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are doi: 10.1016/ universal approximators. Neural Networks, 2(5):359366, January 1989. 0893-6080(89)90020-8. Arieh Iserles. first course in the Numerical Analysis of Differential Equations. Cambridge UniISBN 978-0-521-73490-5. URL http://www. versity Press, Cambridge, 2 edition, 2008. cambridge.org/9780521734905. E. J. Kansa. Multiquadricsa scattered data approximation scheme with applications to computational fluid dynamicsII solutions to parabolic, hyperbolic and elliptic partial differential equations. Computers & Mathematics with Applications, 19(89):147161, 1990. doi: 10.1016/0898-1221(90)90271-K. Gitta Kutyniok. The Mathematics of Artificial Intelligence, 2022. URL https://arxiv.org/ abs/2203.08890. 11 Jean le Rond DAlembert. Recherches sur la courbe que forme une corde tenduee mise en vibration. Histoire de lacademie royale des sciences et belles lettres de Berlin, 3:214219, 1747. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural Operator: Graph Kernel Network for Partial Differential Equations. In ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations (ODE/PDE+DL), 2020. URL https://arxiv.org/abs/2003.03485. Poster presentation. Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar. Physics-Informed Neural Operator for learning Partial Differential Equations. ACM / IMS J. Data Sci., 1(3), May 2024. doi: 10.1145/3648506. URL https://doi.org/10.1145/3648506. Giuseppe Orlando and Mario Sportelli. Growth and cycles as struggle: LotkaVolterra, Goodwin and Phillips. In Giuseppe Orlando, Alexander N. Pisarchik, and Ruedi Stoop (eds.), Nonlinearities in Economics: An Interdisciplinary Approach to Economic Dynamics, Growth and Cycles, pp. 191208. Springer International Publishing, Cham, 2021. doi: 10.1007/978-3-030-64234-0 10. URL https://doi.org/10.1007/978-3-030-64234-0_10. S. V. Patankar. Numerical heat transfer and fluid flow. Taylor & Francis, 1980. ISBN 978-0-89116522-4. Patrick Perez, Michel Gangnet, and Andrew Blake. Poisson image editing. In ACM SIGGRAPH 2003 Papers, pp. 313318, New York, NY, USA, 2003. Association for Computing Machinery. ISBN 1-58113-709-5. doi: 10.1145/1201775.882269. URL https://doi.org/10.1145/ 1201775.882269. M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686707, 2019. ISSN 0021-9991. doi: https://doi. org/10.1016/j.jcp.2018.10.045. URL https://www.sciencedirect.com/science/ article/pii/S0021999118307125. Walter Rudin. Principles of Mathematical Analysis. McGraw-Hill, New York, 3 edition, 1976. ISBN 978-0-07-054235-8. Tim De Ryck and Siddhartha Mishra. Error analysis for physics-informed neural networks (PINNs) approximating Kolmogorov PDEs. Advances in Computational Mathematics, 48(6):79, 2022. ISSN 1572-9044. doi: 10.1007/s10444-022-09985-9. URL https://doi.org/10.1007/ s10444-022-09985-9. Andrew Spielberg, Fangcheng Zhong, Kwang Moo Rematas, and et al. Differentiable visual computing for inverse problems and machine learning. Nature Machine Intelligence, 5:1189 doi: 10.1038/s42256-023-00743-0. URL https://doi.org/10.1038/ 1199, 2023. s42256-023-00743-0. Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani, Dirk Pfluger, and Mathias Niepert. PDEBench: an extensive benchmark for scientific machine learning. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. Sifan Wang, Hanwen Wang, and Paris Perdikaris. On the eigenvector bias of fourier feature networks: From regression to solving multi-scale pdes with physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering, 384:113938, 2021. ISSN 0045-7825. doi: https://doi.org/10.1016/j.cma.2021.113938. URL https://www.sciencedirect.com/ science/article/pii/S0045782521002759. Dmitry Yarotsky. Optimal approximation of continuous functions by very deep ReLU networks. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Annual Conference on Learning Theory, volume 75 of Proceedings of Machine Learning Research, pp. 1 11, Stockholm, Sweden, 2018. PMLR. URL https://proceedings.mlr.press/v75/ yarotsky18a.html. 12 Tjalling J. Ypma. Historical development of the newtonraphson method. SIAM Review, 37(4): 531551, 1995. doi: 10.1137/1037125. URL https://doi.org/10.1137/1037125. Fangcheng Zhong, Kyle Fogarty, Param Hanji, Tianhao Wu, Alejandro Sztrajman, Andrew Spielberg, Andrea Tagliasacchi, Petra Bosilj, and Cengiz Oztireli. Neural fields with hard constraints of arbitrary differential order. In Advances in Neural Information Processing Systems (NeurIPS), 2023."
        },
        {
            "title": "A LINEAR OPERATOR",
            "content": "A linear operator Iserles (2008) is function : that maps one vector space to another, or itself5, R, and preserving the operations of vector addition and scalar multiplication, also known as homogeneity. Thus, for all vectors ui and all scalars c, the following features hold: F( (cid:88) i=1 ui) = (cid:88) i=1 F(ui), vector additivity, F(c u) = F(u), scalar multiplication. (36) Linear operators are fundamental in Linear Algebra for processing matrices, Quantum Mechanics for observables, Machine Learning, and Signal Processing. This forms the basis for Kansa method for linear PDEs. Here are several commonly used examples of linear operators below, among which some are used in this work for PDE solver algorithms. Matrix multiplication: For matrix A, the function : Rn Rm is linear operator, F(x) = Ax. (37) Integral operator: The operator that integrates function over fixed interval [a, b] is linear operator, I(f ) = (cid:90) (x) dx. (38) Differentiation: The operator taking the derivative in function space is linear operator, because differentiation preserves addition and scalar multiplication, Dx(f ) = x . (39) Gradient operator: In multivariable calculus, the gradient operator is linear operator that maps scalar field to vector field, = (cid:18) x , y , z (cid:19) . (40) Divergence operator: In vector calculus, the divergence operator is linear operator that maps vector field to scalar field, = Fx + Fy + Fz . (41) Laplace operator: In the context of partial differential equations, the Laplace operator is linear operator that maps scalar field to another scalar field, = (f ) = 2f = 2f x2 + 2f y2 + 2f z2 . (42) Curl operator: In vector calculus, the curl operator is linear operator that maps vector field to another vector field, = (cid:18) Fz Fy , Fx Fz , Fy Fx (cid:19) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ˆk ˆi ˆj y Fx Fy Fz (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) . (43) 5If the domain and codomain are the same vector space, i.e., : , its called linear transformation or operator on ."
        },
        {
            "title": "B PARTIAL DIFFERENTIAL EQUATIONS",
            "content": "B.1 BOUNDARY AND INITIAL CONDITIONS (BCS AND ICS) Since solution to differential equations contain integration constants, which is non-unique, additional conditions are required to enforce uniqueness. The boundary conditions (BCs) specify the function behavior on the domain boundary Ω, whereas the initial conditions (ICs) from time scale perspective are given at = 0. The formulation is defined in equation 1. There are some common boundary conditions, defined over the boundary Ω = [x0, xf ] in 1D space, where {gi}4 i=1 are given closed-form functions, Zero BC: u(x0, t) = 0, u(xf , t) = 0, Dirichlet BC: u(x0, t) = g1(t), u(xf , t) = g2(t), von Neumann BC: x (x0, t) = g3(t), x (xf , t) = g4(t). (44) B.2 SUMMARY OF PDES Table 14: Summary of PDEs with different characteristics. Equation Domains Linearity Solution dim. Advection Wave Lotka-Volterra Biology Physics Maxwell Physics, Graphics Nonlinear Burgers Physics, Graphics Linear Physics, Graphics Linear Linear Linear 1 1 2 2 1 B.3 1D ADVECTION EQUATION The advection equation Takamoto et al. (2022) models the linear transport of scalar quantity u(x, t), which is changed over time and space x, as follows: (cid:40) u(x,t) + β u(x,t) u(x, 0) = u0(x), = 0, [x0, xf ], [t0, tf ], initial condition, [x0, xf ], (45) where parameter β is the advection velocity, and u0(x) is the initial condition given at = 0. The analytical solution of equation 45 is, u(x, t) = u0(x βt). (46) The positivity of parameter β indicates the direction of wave propagation. From equation 46, when β > 0, the wave propagates rightwards, and vice versa. The solution is visualized in Figure 1, with initial condition u0(x) = sin(2πx), [0, 1]. Figure 1: Advection equation solution visualization in 1D, 2D and 3D. B.4 LOTKA-VOLTERRA PREDATOR-PREY MODEL Lotka-Volterra predator-prey model Bacaer (2011) relates the populations of prey x(t) and predators y(t) at time in dynamic biological system via coupled differential equations, also applicable to other fields, e.g. the unemployment rate with respect to wage growth Orlando & Sportelli (2021) and many more, (cid:40) x(t) := dx(t) y(t) := dy(t) dt = αx(t) βx(t) y(t), dt = δx(t) y(t) γy(t). , [t0, tf ]. (47) where α is the prey growth rate, β is the predation rate, δ is the ratio of neonate predators to eaten prey, and γ is the predator death rate. It assumes that there would be unlimited food supply for the prey, and thus exponential growth αx(t). The multiplicative term x(t) y(t) represents the encounters between prey and predators statistically. The system has no explicit analytical solution, but the implicit solution exists. After scaling of variables, δ γ By plugging into equation 47, and dividing the first equation by the second, x(t) = y(t) = τ = αt, x(t), y(t), β α dy dx = γ α y(x 1) x(y 1) , (48) (49) The implicit solution is given by integration separation of variables, for which CL-V is the integration constant, ln(y) [ln(x) x] = CL-V. (50) γ α Figure 2 shows the solution with phase space given by the above implicit solution, which depends on the initial conditions x(0) and y(0). (a) Prey and predator solution populations (b) Phase space trajectory Figure 2: Lotka-Volterra predator-prey model solution and phase space. B.5 MAXWELLS EQUATIONS In electromagnetism, Maxwells equations Greiner (1998) relate the electric field E(r, t) and magnetic field B(r, t) with spatial position R3 and time t, to the electric charge density ρ(r, t) and current density J(r, t) R3. The differential form is as follows, , = ρ ϵ0 = 0, = t , = µ0J + µ0ϵ0 t , Gausss law, Gausss law for magnetism, Faradays law of induction, Amp`ere-Maxwell law, (51) where constants µ0, ϵ0 R+ are the vacuum permeability and permittivity respectively. Their product is the reciprocal of the square of the speed of light 3 108 s1 in vacuum, µ0ϵ0 = 1 c2 . 16 (52) The first two equations state that the electric field sourced by electric charges, and no magnetic monopoles exist. The last two equations depict how time-varying magnetic field induces an electric field E, and vice versa with the addition of current density J. In general, Maxwells equations are linear with respect to and B. Taking the curl of Faradays law and Amp`ere-Maxwell law respectively, (cid:26) ( E) = ( B), ( B) = µ0( J) + µ0ϵ0 ( E). (53) By vector calculus identity of ( E) = ( E) E, and plugging in Amp`ere-Maxwell law on the right-hand side of the first equation, (cid:40) 2E t2 , ( E) = µ0 ( E). ( B) = µ0( J) + µ0ϵ0 t µ0ϵ By substituting Gausss law for in the first equation, Gausss law for magnetism for and Faradays law for in the second equation, the two equations after rearrangement are inhomogeneous, i.e. including source terms F(r, t), wave equations, taking the forms of c2u utt = F, (cid:40) µ0ϵ0 µ0ϵ0 2E t2 = ( ρ ϵ0 2B t2 = µ0( J). ) + µ0 t , To simplify the problem, we take the one-dimensional (1D) electromagnetic wave propagating along x-axis without sources, i.e. ρ = 0 and = 0, with the electric field E(r, t) = (0, 0, Ez(x, t)) along z-axis and magnetic field B(r, t) = (0, By(x, t), 0) along y-axis respectively. By expanding the defintion of curl operators, and removing the zero terms, = (cid:18) Ez Ey , Ex Ez , Ey Ex (cid:19) (cid:18) = 0, (cid:19) , 0 , Ez thus the reduced last two equations of Maxwells equations equation 51 are, (cid:40) Ez = By , By = µ0ϵ0 Ez . By taking partial derivatives and t, and simplifying, the 1D wave solutions are6, (cid:40) 2Ez x2 µ0ϵ0 2By x2 µ0ϵ0 The initial conditions at = 0 are given as follows, 2Ez t2 = 0, 2By t2 = 0. Ez(x, 0) = (x), By(x, 0) = g(x), [x0, xf ]. (56) (57) (58) (59) Let = Ez + By and = Ez By and with change of variables xt = xt=0 ct, where defined in equation 52 is the speed of light in vacuum. According to dAlemberts formula le Rond DAlembert (1747), (cid:26)u(x, t) = u(x ct, 0) = (x ct) + g(x ct), v(x, t) = v(x + ct, 0) = (x + ct) g(x + ct). By reversing the change of variables, the analytical solutions to equation 58 are, (cid:26)Ez = 1 By = 1 2 (u + v) = 1 2 (u v) = 1 2 [f (x ct) + (x + ct)] + 1 2 [f (x ct) (x + ct)] + 1 2 [g(x ct) g(x + ct)], 2 [g(x ct) + g(x + ct)]. The solution is visualized in Figure 3, with initial condition given as, (x) = sin(2πx) + 0.5 sin(4πx), g(x) = cos(2πx) + 0.5 cos(4πx) [0, 1], [0, 0.5]. (62) 6The 1D wave equation aligned with the general inhomogeneous wave equation equation 55, with = 0. 17 (54) (55) (60) (61) Figure 3: Maxwells equations solution visualization in 1D, 2D and 3D. B.6 VISCOUS BURGERS EQUATION Viscous Burgers equation Takamoto et al. (2022) captures both non-linear advection, also known as convection and diffusion phenomena in dynamics, = ν 2u(x,t) x2 [x0, xf ], , initial condition, + u(x, t) u(x,t) [x0, xf ], [t0, tf ], u(x, 0) = u0(x), (cid:40) u(x,t) (63) where viscosity ν R+ is the positive constant, and u0(x) is the initial condition given at = 0. By Cole-Hopf transformation Hopf (1950), unknown function u(x, t) is converted into ϕ(x, t) via, u(x, t) = 2ν ln ϕ(x, t) = 2ν 1 ϕ(x, t) ϕ(x, t) 2ν ϕx ϕ . (64) By chain rule and quotient rule of differentiation, the first-order and second-order spatial or temporal derivatives of u(x, t) are, = 2ν u(x, t) 2u(x, t) x2 = 2ν (cid:19) , (cid:18) ϕ2 ϕxx ϕ2 ϕ (cid:18) 3ϕxϕxx ϕ2 u(x, t) ϕxxx ϕ 2ϕ3 ϕ3 (cid:18) ϕxϕt ϕ2 (cid:19) , ϕxt ϕ = 2ν (cid:19) . (65) By plugging equation 65 into equation 63 and simplifying, 2ν (cid:18) ϕxϕt ϕ2 ϕxt ϕ ν ϕxϕxx ϕ2 + ν ϕxxx ϕ (cid:19) = 0, [x0, xf ], [t0, tf ], (66) With the inversion of quotient rule, equation 66 is rearranged as, 2ν (cid:18) νϕxx ϕt ϕ (cid:19) = 0, [x0, xf ], [t0, tf ]. By integrating equation 67 with respect to and introducing an integration function (t), νϕxx ϕt ϕ = (t), [x0, xf ], [t0, tf ]. Now introduce (t) = dF (t) dt ϕ = eF (t) and ϕ = ϕ eF (t), thus the derivatives of ϕ are, (cid:18) ϕt + ϕ (cid:19) , dF (t) dt 2 ϕ x2 = eF (t)ϕxx, (67) (68) (69) by plugging them into equation 68. The resulting equation is reduced to the standard heat equation, 2 ϕ(x, t) x2 ϕ(x, t) ν = 0, [x0, xf ], [t0, tf ]. (70) The solution of equation 70 is formed by heat kernel Φ(x, t) convolved with the initial condition ϕ0(x) = ϕ(x, 0) Evans (2010), ϕ(x, t) = (cid:90) Φ(x x, t) ϕ0(x) dx, where Φ(x, t) = 1 4πνt x2 4νt . (71) Note that the transformation from ϕ to ϕ does not change the Cole-Hopf transformation equation 64, since the additional multiplicative term eF (t) is independent of x, u(x, t) = 2ν ln ϕ(x, t) = 2ν ln ϕ(x, t). (72) From the Cole-Hopf equation 72 at = 0 and via integration, the initial condition for ϕ(x, 0) is thus, (cid:90) ϕ0(x) = u0(x) dx. 1 2ν 0 (73) The analytical solution of equation 63 is thus plugging equation 73 into equation 71 and then into equation 72. In this work, we consider when u0() and u0() exist and expression BATEMAN (1915) is then steadily propagating wave as below, 0(x) < 0 for all R, the explicit (cid:18) u0 2ν u(x, t) = cu0 tanh u0() u0() 2 (74) The solution is visualized in Figure 4, with initial condition set by equation 74, u0() = 1, u0() = 0, and ν = 0.5. u0() + u0() 2 , where = , u0 = (x ct) . (cid:19) Figure 4: Burgers equation solution visualization in 1D, 2D and 3D. B.7 PDE SOLVERS There are many attempts to solve the PDE solution field u. Among which, we categorize PDE solvers into two types, i.e. numerical analysis methods and neural-based methods. Constrained optimization. In PDE solvers, constraints are boundary conditions, initial conditions, or PDE residuals. They can be in the form of either soft or hard constraints. The former is the cost functions that are penalised, while the latter is that can not be violated, e.g. (in)equality forms. Table 15: Summary of different PDE solvers. method FDM PINN motivation grid-based physics-driven training supervised constraint N/A hard soft method NO - FNO - PINO KM - CNF motivation data-driven data-driven hybrid mesh-free grid KM on neural fields training supervised N/A constraint soft soft soft hard hard B.7.1 FINITE DIFFERENCE METHOD (FDM) Considering discretized sequence RN of the continuous function u(x, t) as in equation 45. Along the spatial dimension and temporal dimension t, there are and sampled points respectively. The finite difference operators Iserles (2008) defined on per element, are as follows: i+1 uj (+u)i = uj , uj (u)i = uj i1, i+1 uj uj 2 backward difference. forward difference. central difference. (0u)i = (u)i = (75) , , for which {0, 1, . . . , 1} is the spatial index, and {0, 1, . . . , 1} is the temporal index of the sequence u. The partial equations often involve full and/or partial derivatives, where differential operators can be discretized into difference operators equation 75 via finite difference method Bargteil & Shinar (2018). By Taylor expansion of u(x x, t) around u(x, t) up to the first order error, the corresponding examples for the spatial derivative are, uj (x, t) = = + O(x) (u)i (u)i + O(x) (+u)i (+u)i (0u)i + O(x2) = = (0u)i = i1 uj uj i+1uj uj i uj i+1 uj 2x i1 if β > 0, if β < 0. , upwind scheme. , central difference. (76) where is the spatial spacing, with spatial index and temporal index defined above. The upwind scheme Patankar (1980) considers where the information comes from, e.g. when β > 0, the wave propagates rightwards, and thus uj i1, and vice versa for downwind scheme. Under the upwind scheme, the advection equation equation 45 is therefore as the following ODE, is influenced by uj where the indicator function Iβ>0 = is for controlling different cases of β7. u(x, t) + β[Iβ>0 (cid:26)1 0 (u)i if β > 0, otherwise. + Iβ<0 (+u)i ] = 0, (77) By forward Euler method for ODEs, the temporal derivative is discretized via the forward difference operator. After which, the advection equation equation 45 is simplified as, with being the temporal spacing, uj uj+1 + β[Iβ>0 (u)i + Iβ<0 (+u)i ] = 0. With algebraic reordering, the upwind scheme update rule is thus, = uj uj+1 βt [Iβ>0(u)i + Iβ<0(+u)i]. (78) (79) Stability condition. For implicit numerical schemes, e.g. the backward Euler method, the solution is unconditionally stable. However, for explicit numerical schemes, e.g. the forward Euler method above, stability conditions must be satisfied to avoid numerical instability, which we briefly introduce below. 7Alternatively, one may use max(β, 0) and min(β, 0) to replace βIβ>0 and βIβ<0 respectively. 20 In the 1D space, the scalar Courant number C, also known as the CFL stability criteria, measures the ratio of how far the wave propagates in one time interval to the spatial spacing x. The CFL condition Courant et al. (1928) states that must satisfy, = βt Cmax, (80) where Cmax is problem-dependent constant. It sets the maximum allowable time step for given x, for numerical stability. B.7.2 PHYSICS-INFORMED NEURAL NETWORK (PINN) Physics-informed neural network (PINN) Raissi et al. (2019) is data-driven approach for functional PDE approximation, which requires large labeled dataset but has the ability to generalize. Consider the general form of PDEs defined in equation 1, PINNs approximate the unknown solution u(x, t) with neural network ˆuθ(x, t) U, i.e. ˆuθ(x, t) u(x, t), parameterized by updatable parameters θ Θ. Residual Rθ of the PDEs is calculated without supervised data for the neural network ˆuθ, which is minimized via automatic differentiation Baydin et al. (2018)8 during training for generalizability, Rθ(x, t) = D[ˆuθ](x, t) (x, t), Ω, [t0, tf ]. (81) 9, also known as the physics-informed loss, is defined to be the p-norm of the The residual loss LR residual Rθ in equation 81. During training, NR quadrature points are sampled, where the integral loss is approximated by the discretized loss LR with weights ωk at each sample index and training error ET (θ), (cid:90) LR := (Rθp)p := [( By quadrature, = (cid:124) NR(cid:88) k=1 (cid:124) Rθp dx dt) 1 ]p = (cid:123)(cid:122) integral LR (cid:125) (cid:90) Rθp dx dt ωkRθ(xk, tk)p +ET (θ) LR, where ET (θ) = LR LR. (82) (cid:123)(cid:122) discretized LR (cid:125) If considering the boundary conditions, the residual for the i-th boundary condition RBi via equation 1 as well, after which the boundary condition loss LBC is defined accordingly, θ is calculated RBi θ (x, t) Zi = Bi[ˆuθ](x, t) gi(x, t), Ωi, [t0, tf ]. (83) As defined in equation 99, the total error between the optimal solution from the network ˆuθ and the ground truth is, by expanding equation 97, EPINN(θ) = (ˆuθ up)p. (84) During training, the network is optimized on supervised dataset {(xn, tn), u(xn, tn)}Nd being the total number of data. The supervised loss Ldata n=1, with Nd 10 approximates the total error equation 84, Ldata = 1 Nd Nd(cid:88) (ˆuθ(xn, tn) u(xn, tn)p). n=1 (85) Training. PINN approximates the solution as ˆuθ = uθopt (x, t). To avoid overfitting due to the limited supervised data, the main goal is to minimize the unsupervised residual error LR equation 82. With the addition of the supervised loss equation 85 and the boundary condition residual 8Example x : create graph=True)[0] of = torch.autograd.grad(outputs=u, inputs=x, 9Note that LR is the same as the risk defined in equation 97, but for the residual Rθ instead of the solution u. 10Note that when the supervised data is only sampled on the boundary, the supervised loss and the boundary condition loss are the same. 21 equation 83, the optimized theta is θopt arg minθΘ L, where the total training loss LPINN is, NR(cid:88) LPINN = ωkRθ(xk, tk)p +λ k=1 (cid:124) (cid:125) (cid:123)(cid:122) Discretized residual loss LR"
        },
        {
            "title": "1\nNd\n(cid:124)",
            "content": "Nd(cid:88) (ˆuθ(xn, tn) u(xn, tn)p) +λ2 NBi(cid:88) (cid:88) n= (cid:123)(cid:122) Supervised loss Ldata b=1 (cid:125) (cid:124) RBi ωBi θ (xb, tb)p , (cid:123)(cid:122) BC loss LBC (cid:125) with weights ωBi λ1, λ2 > 0 for combining different losses. (86) at each sample index for i-th boundary condition and regularization parameters Algorithm 1 Physics-Informed Neural Network training pseudocode. 1: Input: Initial parameters θ for network ˆuθ. 2: Output: Optimized parameters θopt for network ˆuθ. 3: Hyperparameters: Learning rate η, number of training iterations Niter. 4: while number of iterations < Niter do 5: 6: 7: 8: 9: end while Sample PDE points xk Ω, tk [t0, tf ] and boundary points xb Ωi, tb [t0, tf ]. Compute the network outputsˆuθ and their derivatives D[ˆuθ] and boundary Bi[ˆuθ]. Compute loss LPINN = LR + Ldata + LBC by equation 86. By gradient descent, update θ θ η θL. B.7.3 NEURAL OPERATOR (NO) Operator learning. From the general form of PDEs equation 1, we assume that within the operator, the source function or the initial conditions, there is parameter of the same dimension as the solution u. In this subsection, we denote the differential operator as Da, where the PDE is thus Da[u] = . , ), uj i = 1, ..., NR}Npde Given the dataset {(aj j=1 with Npde PDE instances each NR quadrature points, the idea of operator learning Li et al. (2020) is to learn the operator mapping input : to the solution : R, i.e. G(a, ) = connecting two function spaces and with infinite dimensions, which is challenging for neural networks since they are for finite dimensions instead. Solution A. To solve this challenge, one solution is to parameterize the PDE = u(t, x, µ), assuming that is measureable in finite dimension, i.e. = a(µ), µ Rdy . Its techinique widely used in aircraft design and manufacturing Athanasopoulos et al. (2009), image processing Perez et al. (2003). The training process is therefore to minimize the supervised loss Ldata from data with p-norm, which measures how much the predicted solution Gθ(ai, fi) deviates from the ground truth ui for each data point i, Ldata = 1 Npde NR Npde (cid:88) NR(cid:88) j=1 i=1 (Gθ(aj , ) uj p)p. (87) The approximated solution is therefore ˆuθ = Gθopt(a, ), where θopt arg minθΘ Ldata. There is no addition of the PDE residual loss LR equation 81 as in PINNs for basic operator learning. An extension, termed as physics-informed neural operator (PINO) Li et al. (2024), combines the supervised loss Ldata and the residual loss LR as the total training loss, LPINO = Ldata + λ 1 Npde NR (cid:124) Npde (cid:88) NR(cid:88) j=1 i=1 (Da[Gθ(aj , )] i (xi, ti)p)p , (88) (cid:123)(cid:122) Residual loss LR (cid:125) Despite the simplicity in ideas, the parameterization suffers from how to sample from the given space, non-uniqueness, and low generalization to unseen a. Solution B. Interpolation from the discretized grid, including neural network based interpolator or traditional methods (linear, cubic, spline, etc). However, it suffers from inconsistency between the discretized and continuous functions. 22 Solution C. Generalize the neural network from discrete to continuous function space. (Nlv)(x) = σ[Alv(x) + Bl(x) + (cid:90) Kl(x, y)v(y) dy], D. (89) Fast implemenentation via FFT."
        },
        {
            "title": "C ARCHITECTURE DETAILS",
            "content": "We adopt multi-scale feed-forward neural network architecture for PINN Wang et al. (2021), which augments the original feed-forward architecture with input encoding layers of multiple frequency scales. There are three hidden layers each with 64 neurons. For FNO, we adopt the architecture with 16 modes retained in the spectral convolution layer, and the latent feature dimension is 64."
        },
        {
            "title": "D ENVIRONMENT SETUP",
            "content": "All the measurements are conducted on Mac M1, with single-core CPU running at 3.2 GHz. In the following sections, the data points are uniformly sampled unless otherwise specified."
        },
        {
            "title": "E LEARNING THEORY",
            "content": "E.1 FUNCTIONAL ANALYSIS We introduce basic functional analysis concepts here for PDE solvers and later learning theory (Appendix E). The p-norm of function : Ω Rd is defined as, (cid:90) p = ( Ω (x)p dx) 1 , for 1 < . The p-integrable function Lp(Ω), is defined as (f p)p = (cid:90) Ω (x)p dx < . For additional concepts used for learning theory, please refer to Appendix E.2. (90) (91) E.2 FUNCTIONAL ANALYSIS ADDENDUM Smoothness Rudin (1976) of function is defined to be the number of continuous derivatives it has. The class of function with smoothness N+ has at least k-th derivative, and is denoted as k, k(Ω) = {f : Ω Rd α k. αf exists and is continuous}. (92) When = , the function is differentiable at all orders. While not every function is not smooth, there is generalization of smooth functions, i.e. Sobolev functions. The weak derivative generalizes to include functions that are not differentiable, but locally integrable on bounded domain [a, b]. The definition is for all smooth test functions ϕ, with ϕ(a) = ϕ(b) = 0, (cid:90) (x)ϕ(x) dx = [f (x)ϕ(x)]b (cid:90) a (x)ϕ(x) dx, by integration by parts, = (cid:90) f (x)ϕ(x) dx, as ϕ(a) = ϕ(b) = 0. (93) Sobolev spaces Adams & Fournier (2003) k,p(Ω) is function space where all functions having weak derivatives up to order and every derivate is p-integrable via equation 91, k,p(Ω) Lp(Ω) = {f : Ω Rd α k. αf Lp(Ω)}, (94) When = 2, it forms Hilbert space, i.e. k,2(Ω) = k(Ω). 23 E.3 APPROXIMATION THEORY OF NEURAL NETWORKS We quote some known bounds for neural networks from theoretical machine learning field here, which are relevant to PDE solvers analysis later. Universal approximation theorem Hornik et al. (1989) states that neural networks ˆuθ, for which parameters θ Θ, can approximate any continuous functions : Rd with little error ϵ > 0 in the p-norm of function space U, with an extension to their differential operator D, θ Θ. ˆuθ up < ϵ = D[ˆuθ] D[u]p < ϵ. (95) Optimal DNN functions approximation theorem Yarotsky (2018). Assuming continuous function s,p as defined in equation 94, where N+ is the smoothness of u, there exists neural network ˆuθ with parameters, such that the error bound is, ˆuθ up = O(M (96) where is the input dimension of u. It means intuitively that the smoother and lower-dimensional the is, the easier for it to be approximated by neural network ˆuθ. For fixed error ϵ, the required number of parameters is = O(ϵ ), which suffers from the exponential growth of d, i.e. the curse of dimensionality Bellman (1957). ), E.4 ERROR ANALYSIS Error and risk estimation. Define the risk of the approximation ˆuθ against the ground truth function : Ω with p-norm integral, R(ˆuθ) = (ˆuθ up)p := (cid:90) Ω ˆuθ(x) u(x)p dx, (97) For discretized computation, quadrature ˆR is used to approximate the integral risk with sample points from the dataset, where ωk is the weight at each sample index k, (cid:90) ˆuθ(x) u(x)p dx = ωkˆuθ(xk) u(xk)p +ET (θ) ˆR(ˆuθ), ET (θ) := R(ˆuθ) ˆR(ˆuθ), (cid:88) Ω (cid:124) (cid:123)(cid:122) integral R(ˆuθ) (cid:125) k=1 (cid:124) (cid:123)(cid:122) discretized ˆR(ˆuθ) (cid:125) (98) where the training, also known as generalization or out-of-sample, error ET (θ) measures the difference between the integral risk and the discretized risk due to quadrature. Error decomposition Kutyniok (2022). When approximating continuous function with neural network ˆuθ, the total error E(θ)11 is decomposed into three parts, with the risk and its quadrature ˆR defined in equation 97 and equation 98 respectively, E(θ) := R(ˆuθ) inf θΘ (cid:124) R(uθ ) (cid:123)(cid:122) (cid:125) EA(θ) + ˆR(ˆuθ) inf θΘ (cid:123)(cid:122) EO(θ) (cid:124) R(uθ ) (cid:125) + R(ˆuθ) ˆR(ˆuθ) (cid:123)(cid:122) (cid:125) ET (θ) (cid:124) , (99) the approximation error EA measures the risk between the best network approximation uθ and ground truth u, optimization error EO measures the trained network result ˆuθ deviation from the best network approximation, and training error ET defined in equation 98. E.5 PINN LEARNING THEORY We briefly analyze the PINN error bound12. The total error between the optimal solution ˆuθ and the ground truth is shown in equation 84. However, during training, the network doesnt have access to the exact ground truth for u. Therefore, we aim to reduce the PDE residual instead. ER(θ) = (Rθp)p = (D[ˆuθ] p)p, = D[ˆuθ] D[u]p, = ˆf p, by equation 81. by equation 1. by the definition of ˆf , (100) 11inf θ is the infimum over all possible network parameters θ, which might not be attained. 12The PDE residual is considered here, whereas boundary and initial conditions are omitted for simplicity. 24 where ˆf = D[ˆuθ] is the approximated source function. In practice, this integral is approximated via quadrature, with training error defined in equation 82. From the theoretical perspective, the goal is to derive that the total error EPINN equation 84 is sufficiently small. To prove this, sufficient condition is that the total error is bounded by the residual error ER equation 100, i.e. we can prove that the smallest residual error ensures the smallest total error. θ Θ. EPINN(θ) CER(θ), (101) where is constant. By expansion of EPINN equation 84 and ER equation 100, the abovementioned inequality equation 101 is equivalent to the following coercivity condition Ryck & Mishra (2022), θ Θ. ˆuθ C ˆf p, (102) By quadrature bound Iserles (2008), the smallest practical training error ET equation 82 ensures the smallest residual error ER equation 100, where is constant, θ Θ. ER(θ) [ET (θ) + Eu(NR)], (103) and the extra term Eu(NR) converges faster than 1 NR sampled quadrature points NR, , thus can be ignored given the increasing Eu(NR) o( 1 NR ) = Eu(NR) 1 NR = lim NR = 0, , by the definition of little-o notation. NREu(NR) = 0, by the definition of limit. (104) By the above two inequalities equation 101 and equation 103, the total error EPINN equation 84 converges as the training error ET equation 82 converges, θ Θ. EPINN(θ) CC [ET (θ) + o( 1 NR )]. (105) By Universal approximation theorem equation 95, the smoothness of the solution ensures that the residual error ER(θ) < ϵ is sufficiently small. Given sufficient quadrature points NR, and smooth activation functions in the neural network ˆuθ Iserles (2008), min θΘ ET (θ) ER(θ) + o( 1 NR ), (106) Hence, the training error ET (θ) < ϵ + o( 1 NR the total error EPINN(θ) < CC [ϵ + o( 1 )], by equation 105, which concludes the proof. NR ) is sufficiently small, according to equation 104. So is From the practical perspective, the common failure modes, from the above theoretical analysis, are (1) few quadrature points NR leading to large training error ET in equation 98, (2) insufficient training resulting in large optimization error EO in equation 99, (3) violation of the coercivity condition equation 102 for PDEs, and (4) large constant in equation 101 or in equation 103."
        }
    ],
    "affiliations": [
        "University of Cambridge, UK"
    ]
}