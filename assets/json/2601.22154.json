{
    "paper_title": "Exploring Reasoning Reward Model for Agents",
    "authors": [
        "Kaixuan Fan",
        "Kaituo Feng",
        "Manyuan Zhang",
        "Tianshuo Peng",
        "Zhixun Li",
        "Yilei Jiang",
        "Shuang Chen",
        "Peng Pei",
        "Xunliang Cai",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research."
        },
        {
            "title": "Start",
            "content": "Kaixuan Fan1,2 Kaituo Feng1,2 Manyuan Zhang2* Tianshuo Peng1 Zhixun Li3 Yilei Jiang1,2 Shawn Chen2 Peng Pei2 Xunliang Cai2 Xiangyu Yue1 1MMLab, CUHK 2Meituan 3SEEM, CUHK Repository: https://github.com/kxfan2002/Reagent 6 2 0 2 9 2 ] . [ 1 4 5 1 2 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), multifaceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: ReagentC (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning with Verifiable Reward (RLVR) has achieved remarkable success in improving the reasoning capabilities of Large Language Models (LLMs) (Liu et al., 2025b; Feng et al., 2025b; Tang et al., 2025; Chen et al., 2025b,c). Motivated by this progress, recent works have extended this paradigm to agents, demonstrating its potential to handle complex interactions with dynamic environments and external knowledge sources (Jin et al., 2025; Wu et al., 2025a; Li et al., 2025b). *Project Leader. Corresponding Author. However, previous agentic RL methods typically rely on sparse, outcome-based rewards based solely on final correctness (Jin et al., 2025; Wu et al., 2025a; Li et al., 2025b). This design is inherently limiting for long-horizon agentic tasks requiring multi-step tool utilization (Feng et al., 2025c; Liu et al., 2025a; Zhang et al., 2025e). In such settings, outcome-based supervision fails to differentiate high-quality intermediate reasoning from entirely incorrect attempts, for instance, treating trajectory that fails only at the final step as total failure. Consequently this coarse-grained binary supervision obscures the value of successful intermediate steps, resulting in sub-optimal performance (Dong et al., 2025). To provide more granular feedback, recent research has pivoted toward integrating Reward Models into Agentic RL. However, the effective deployment of Reward Models remains hampered by two bottlenecks. First, while step-level rewards offer finer granularity feedback (Xi et al., 2025; Liu et al., 2025a; Xu et al., 2025), they are often plagued by prohibitive annotation costs (Rahman et al., 2025) and susceptibility to reward hacking (Zhang et al., 2025c). Second, existing reasoning-based Reward Models focus on pair-wise preferences (Li et al., 2025c; Liu et al., 2025a; Hu et al., 2025), which frequently introduces inherent biases and fails to capture fine-grained quality gradations between trajectories or provide actionable guidance for refinement (Jian et al., 2025; Zhang et al., 2025d). Furthermore, most of these efforts exclusively rely on numeric reward feedback for training, leaving the natural language critique (Zhang et al., 2025a) largely unexplored, which could provide more granular guidance for agentic policy. To this end, we develop Agent Reasoning Reward Model (Agent-RRM), multi-faceted evaluator designed to provide reasoning-aware feedback for agentic trajectories. Unlike conventional Reward Models that yield merely scalar scores or binary preferences, Agent-RRM conducts explicit reasoning to justify its assessments. For each trajectory, it generates decomposed judgment comprising: (1) an internal reasoning trace that analyzes logical consistency of trajectory; (2) targeted critique identifying specific flaws to guide refinement; and (3) holistic quality score. This hierarchy of signals provides dense, multi-dimensional supervision, combining scalar rewards for global optimization with textual critiques for explicit error correctionall without necessitating ground truth. Building upon these informative signals, we perform systematic investigation into the integration of Agent-RRM and Agentic RL. We formalize this integration through unified scheme with three variants: Text-augmented Refinement, where agents polish trajectories based on AgentRRMs textual feedback; Reward-augmented Guidance, which complements rule-based rewards with model-based signals; and Unified Feedback Integration, which harmonizes multi-source rewards with critique-augmented sampling. We denote the agent policy models of these variants as ReagentC, Reagent-R, and Reagent-U, respectively. Notably, our experiments demonstrate that ReagentU achieves superior performance by synthesizing these feedback modalities, reaching 43.7% on GAIA and 46.2% on WebWalkerQA. Our study provides comprehensive roadmap for harnessing multi-level feedback to accelerate agentic RL. To support this investigation, we curate four specialized datasets that provide high-quality trajectories for both agent reasoning and reward model training. Extensive experiments across 12 diverse benchmarks demonstrate that Reagent models achieve significant performance gains, underscoring the efficacy of multi-level reasoning-based feedback signals in complex agentic tasks. In summary, our contributions are as follows: We introduce Agent-RRM, multi-faceted evaluator that generates structured feedback including explicit reasoning rationales, actionable critiques, and holistic quality scores, providing transparent and granular assessment. We systematically explore three agent variants with Agent-RRM: Text-augmented Refinement (Reagent-C), Reward-augmented Guidance (Reagent-R), and Integrated Feedback Optimization (Reagent-U). This provides roadmap for using reasoning rewards to enhance agent performance. We curate and release four high-quality datasets specifically tailored for training reasoning agent and reward model. These resources provide the community with valuable assets to advance research in multi-granular feedback for agentic reinforcement learning."
        },
        {
            "title": "2.1 Agentic Reinforcement Learning",
            "content": "Agentic Reinforcement Learning (Agentic RL) has emerged as cornerstone for developing agents capable of operating in dynamic, open-ended environments (Dong et al., 2025; Lù et al., 2025). Recent advancements (Jin et al., 2025; Wang et al., 2025; Xia et al., 2025; Wu et al., 2025c; Li et al., 2025f; Song et al., 2025) illustrate that RL can effectively instill multi-step information-seeking and tool-use proficiencies. For example, Search-R1 (Jin et al., 2025) demonstrates that agentic RL enables LLMs to interleave multi-turn web search, substantially improving retrieval-augmented reasoning performance. WebSailor (Li et al., 2025b) further shows that agentic RL can scale to long-horizon web navigation, equipping agents with the ability to reduce extreme uncertainty in complex informationseeking tasks. Agent0 (Xia et al., 2025) introduces co-evolutionary process where tool-aware reasoning behaviors emerge without human-curated supervision. Despite these successes, most existing methods rely heavily on sparse, outcome-based rewards, which often limits training efficacy and hampers agents ability to rectify intricate intermediate errors (Dong et al., 2025; Lin et al., 2025)."
        },
        {
            "title": "2.2 Reward Modeling",
            "content": "Reward Models (RMs) play central role in optimizing complex reasoning tasks by providing learning signals for policy improvement (Wang et al., 2024; Fan et al., 2025; Li et al., 2025a). Motivated by Deepseek-R1 (Guo et al., 2025), reasoningaware reward models are introduced to perform explicit reasoning before reward assignment for delivering higher-quality and more transparent supervision (Whitehouse et al., 2025; Zhang et al., 2025b). For instance, RM-R1 (Chen et al., 2025d) introduces generative reasoning-based reward model to first derive explicit reasoning rubrics and then evaluate candidate responses accordingly. R1Reward (Zhang et al., 2025b) proposes multimodal reasoning reward model and introduces stabilized RL algorithm that improves training roFigure 1: Detailed distribution information of Reagent-SFT-55.6K and Reagent-RL-709K. bustness for multimodal RMs. In agent domain, reasoning-based reward models still remain underexplored. Atom-Searcher (Deng et al., 2025) directly utilizes Qwen3-30B-A3B without training as reward model to assign scores to agent steps, while PPR (Xu et al., 2025) employs process reward model to evaluate trajectory steps based on predefined principle set. However, these methods remain confined to step-level scalar rewards, which are susceptible to reward hacking and fail to provide language-based guidance necessary for rectifying complex logic flaws."
        },
        {
            "title": "3.1 Preliminaries: GRPO Framework",
            "content": "Policy Optimization In Group Relative for query (GRPO) (Shao et al., 2024), sampled from the dataset (q), the policy πθ generates group of outputs {oi}G i=1 such that: (1) oi πθold(oq). πθold Let ri(θ) = πθ(oiq) (oiq) denote the importance sampling ratio. The GRPO objective is formulated as: JGRP O(θ) = EqP (q){oi}πθold (cid:16) (cid:88) min(ri(θ)Ai, clipϵ) βD(i) (cid:17)(cid:21) KL , (cid:20) 1 i=1 (2) where clipϵ denotes clip(ri(θ), 1 ϵ, 1 + ϵ)Ai, and D(i) KL denotes the KL divergence between current policy and reference model πref for the i-th output: D(i) KL = DKL(πθ(oiq)πref (oiq)). The advantage Ai is computed by normalizing the rewards within the group = {R1, . . . , RG}: (3) Ai = Ri mean(R) std(R) . (4) where Ri is the reward for the i-th output assigned by the reward system."
        },
        {
            "title": "3.2 Agentic Tool Design",
            "content": "To enable effective interaction with diverse environments, we equip the agent with suite of six specialized tools covering information retrieval, code execution, and multi-modal perception: Search: Given query, retrieve relevant search results using the Bing search engine. Web Browse: Given URL and query, fetch the webpage content and generate response to the query based on the page information. Python Code Interpreter: Execute provided Python code snippet and return the execution results. File Reader: Access and extract file and return its textual content. Image Descriptor: Given an image and query, generate textual response to the query conditioned on the visual features. Audio Converter: Transcribe an input audio file into text."
        },
        {
            "title": "3.3 Dataset Construction",
            "content": "We curate four specialized datasets spanning mathematical deduction, multimodal understanding, webbased information seeking, and complex tool utilization. These datasets support the distinct training requirements of both Reagent and Agent-RRM. Agent Training Datasets We synthesize diverse QA benchmarks to enhance the agents reasoning and tool-use capabilities. To ensure data quality, we apply rigorous three-stage pipeline: (1) filtering samples with ambiguous ground truths; (2) cross source deduplication; and (3) difficulty-aware samFigure 2: Overview of the Reagent training scheme. We explore three integration variants: Reagent-C (blue arrows), Reagent-R (gray arrows), and Reagent-U (both arrows). pling. This yields Reagent-RL-709K, comprehensive corpus of 709k question-answer pairs for RL training. For Supervised Fine-tuning (SFT), we prioritize the holistic quality of reasoning trajectories. Using DeepSeek-V3.1, we generate and retain only trajectories that lead to correct final answers, resulting in Reagent-SFT-55.6K as high-quality cold-start data. Dataset distribution is shown in Figure 1. Specific selection criteria and filtering thresholds are detailed in Appendix A.2. Reward Model Datasets Training robust Agent-RRM necessitates exposure to wide spectrum of logical error patterns and diverse response styles. Building upon the Reagent-RL709K, we construct two meticulously labeled datasets: Reagent-RRM-SFT-28K and ReagentRRM-RL-90K for SFT and RL stage respectively. We sample reasoning trajectories from an ensemble of models including Qwen3-8B/14B, Qwen3-ARPO-DeepSearch (8B/14B), Qwen2.57B-ARPO, Qwen2.5-WebDancer (7B/32B), and DeepSeekV3.1 to maximize the coverage of potential error patterns. These trajectories are then annotated by GPT-OSS-120B to generate structured three-part judgments: an analytical <think> trace, targeted <critique> of flaws, and holistic <score> (s [0, 1]). See Appendix A.3 for prompts and process details."
        },
        {
            "title": "3.4 Agent-RRM: Reward Model Training",
            "content": "To facilitate granular feedback, we train Agent Reasoning Reward Model (Agent-RRM) to generate multi-dimensional judgments consisting of three components: <think>, an internal reasoning trace analyzing trajectory quality, <critique>, targeted identification of reasoning or execution flaws, and <score>, scalar quality assessment within [0, 1]. Following (Chen et al., 2025d; Zhang et al., 2025b), we adopt two-stage training procedure. First, we conduct SFT on Reagent-RRMSFT-28K to instill the structured output format and foundational evaluative capabilities. Subsequently, we apply GRPO on Reagent-RRM-RL-90K to refine the models evaluative rationales and ensure the calibration of its scalar rewards. This training paradigm ensures that Agent-RRM can generate high-fidelity, self-consistent feedback even in the absence of ground-truth answers, making it highly effective for complex, open-ended agentic tasks."
        },
        {
            "title": "3.5 Reagent: Integrating Reasoning Rewards",
            "content": "into Agents In this section, we introduce our agent policy model Reagent, and present three variants that explore different ways of incorporating reasoning rewards and critiques into agentic policies. To provide robust starting point for RL, we finetune the base model on the Reagent-SFT-55.6K dataset. This stage ensures the agent acquires fundamental reasoning and tool-calling proficiencies. The resulting optimized policy, πθSF , serves as the seed model for subsequent RL investigations in Section 3.5.2 and 3.5.3. We investigate three Reagent variants to explore the synergy between AgentRRM and the agent: (1) Textual-augmented Refinement (Reagent-C), which evaluates the immediate utility of textual critiques via zero-shot, incontext refinement; (2) Reward-augmented Guidance (Reagent-R), which optimizes πθSF by complementing rule-based rewards with model-based scalar signals; and (3) Unified Feedback Integration (Reagent-U), which harmonizes both modalities within joint optimization loop. The overall framework is illustrated in Figure 2. initial generation quality and refinement capability, we investigate whether these objectives can yield synergistic improvements through mutual reinforcement for agent. For each query q, the agent performs two-stage"
        },
        {
            "title": "3.5.1 Textual-augmented Refinement",
            "content": "This variant (Reagent-C) exploits textual critiques from Agent-RRM for training-free refinement, applied directly to the Qwen3-8B via in-context prompting. For each query q, the agent first generates an initial response o(1) πθ(oq). Subsequently, AgentRRM analyzes o(1) to produce targeted critique ci via its <critique> component, identifying specific reasoning flaws or execution errors. The agent then performs refined pass conditioned on feedback: πθ(oq, o(1) o(2) , ci), (5) i where the augmented context (q, o(1) , ci) provides the original task and explicit guidance for correction. Crucially, the policy πθ remains frozen in this variant, allowing us to isolate and evaluate the agents in-context refinement capability. All reported results for Reagent-C correspond to the refined outputs {o(2) }."
        },
        {
            "title": "3.5.2 Reward-augmented Guidance",
            "content": "This variant (Reagent-R) utilizes the scalar score from Agent-RRM to provide fine-grained quality assessments of agent trajectories. Following standard GRPO sampling procedure, the agent generates outputs oi πθold(oq). The reward Ri is defined as combination of rule-based correctness and model-based quality evaluation: Ri = Rrule(q, oi) + λ Rmodel(q, oi), (6) where Rrule validates final answer correctness, Rmodel is extracted from Agent-RRMs <score>, and λ is scaling factor balancing their contributions. This variant alleviates the sparsity of rulebased rewards by providing reasoning-aware feedback. It enables the agent to capture fine-grained spectrum of trajectory quality, effectively rewarding logical merit while penalizing reasoning deficiencies regardless of final answers correctness."
        },
        {
            "title": "3.5.3 Unified Feedback Integration",
            "content": "This variant (Reagent-U) harmonizes scalar rewards and textual critique-driven refinement within unified RL loop. By simultaneously optimizing sampling: o(1) πθold(oq), πθold(oq, o(1) o(2) , ci), (7) where o(1) is the initial attempt and o(2) is the refined response guided by <critique> ci generated by Agent-RRM. We pool all trajectories from both stages into [G], {1, 2}} and compute via Eq. 6. The advantage is Gpool = {o(k) combined reward R(k) computed across this unified pool: A(k) = R(k) mean(Rpool) std(Rpool) , (8) o(k) where Rpool = {R(k) objective is formulated as: (cid:20) 1 2G JU (θ) = (cid:88) 2 (cid:88) (cid:16) k= i=1 Gpool}. The unified min(r(k) (θ)A(k) , clipϵ) (9) βD(i,k) KL (cid:17)(cid:21) , where the importance ratio r(k) (θ) and KL penalty D(i,k) KL are computed relative to their respective contexts. By normalizing advantages across all initial and refined trajectories, Reagent-U encourages the agent to optimize for overall trajectory quality, effectively boosting the agents core reasoning and tool-calling performance. Notably, textual critiques are utilized exclusively during the training phase to internalize reasoning capabilities; at inference time, ReAgent-U operates as standard agent without additional critique refinement or external guidance."
        },
        {
            "title": "4 Experiments",
            "content": "Benchmarks We evaluate comprehensively on (1) Mathemultiple challenging benchmarks. matical Reasoning: AIME24 (Zhang and MathAI, 2024), AIME25 (Zhang and Math-AI, 2025), GSM8K (Cobbe et al., 2021) and MATH500 (Lightman et al., 2023).(2) Knowledge-Intensive Reasoning: HotpotQA (Yang et al., 2018), 2Wiki (Ho et al., 2020), Bamboogle (Press et al., 2023) and MuSiQue (Trivedi et al., 2022). (3) General Agent and Search Reasoning: GAIA (Mialon et al., 2023), WebWalkerQA (Wu et al., 2025b), Humanitys Last Exam (HLE) (Phan et al., 2025) and xbench (Chen et al., 2025a). Table 1: Comprehensive Evaluation on General Agent and Search Benchmarks. Method Proprietary Agents - - - - Open-source Baselines (8B) GAIA (text) WebWalkerQA HLE xbench Backbone Lv.1 Lv.2 Lv.3 Avg. Easy Med. Hard Avg. Avg. Avg. OpenAI-o3 o1-preview Calude-4-Sonnet OpenAI DeepResearch - - - - - - - - - - - - 70.5 - - - 71. - 11.9 10.4 7.9 9.9 68.3 67. - - - - - - 61.7 - 20.2 11.1 20.2 26. 66.0 - 64.0 - WebThinker (Li et al., 2025e) Qwen3-8B 43.6 11.5 0.0 22.3 6.7 13.1 16.9 13.0 6.6 13.0 WebDancer (Wu et al., 2025a) Qwen2.5-7B 41.0 30.7 0.0 31.0 40.6 44.1 28.2 36.0 VerlTool (Jiang et al., 2025) ARPO (Dong et al., 2025) Qwen3-8B Qwen3-8B - - - 34.0 - - - - 53.9 32.7 16.7 38.8 26.7 33.3 29.6 30.5 Open-source Baselines (32B) - - QwQ-32B 30.9 6.5 5.2 18.9 DeepSeek-R1-671B 40.5 21.2 5.2 25.2 7.5 5.0 2.1 4.2 3. 11.8 11.3 10.0 Tree-GRPO (Ji et al., 2025) Qwen2.5-14B 20.8 24.3 7.3 21.0 11.1 15.5 10.8 12.8 ARPO (Dong et al., 2025) Qwen3-14B 56.4 40.4 16.7 43.7 31.1 42.9 31.0 36.0 Search-o1 (Li et al., 2025d) QwQ-32B-Preview 53.8 34.6 16.7 39. 43.1 35.0 27.1 34.1 - 8.4 8.8 6.4 8. - 10.0 10.8 WebDancer (Wu et al., 2025a) Qwen2.5-32B 46.1 44.2 8.3 40. 44.3 46.7 29.2 38.4 - Open-source Baselines with Process Reward - - 25. 10.0 32.0 - 32.0 40.0 38. Atom-Searcher (Deng et al., 2025) Qwen2.5-7B 18.0 21.2 0.0 17.5 31.7 23.7 37.0 27.9 10.0 21.0 Our Agents - Reagent w/o Agent-RRM Reagent-C (Direct Inference) Reagent-R Reagent-U Qwen3-8B Qwen3-8B Qwen3-8B Qwen3-8B Qwen3-8B 28.2 21.2 0.0 21.4 31.1 28.6 28.2 29. 41.0 36.5 0.0 34.0 44.4 45.0 41.3 43.5 30.8 23.1 16.7 25.2 35.6 38.1 32.4 35.5 51.3 30.8 16.7 36.9 47.5 46.0 42.9 45. 59.0 38.5 16.7 43.7 49.2 46.8 43.3 46.2 4.0 6.8 4.6 10. 10.8 9.0 32.0 15.0 41.0 43. Implementation Details Following (Dong et al., 2025; Feng et al., 2025a; Wu et al., 2025a), we employ two-phase training pipeline: Supervised Fine-Tuning followed by Reinforcement Learning. This training protocol mitigates optimization instability in early RL stages and equips the agent with the foundational skills necessary for effective tool interaction. Both agent models and reward model are initialized from Qwen3-8B (Yang et al., 2025). Both our agent models and Agent-RRM are trained on 8 NVIDIA A800-80G GPUs. Batch size is set to 32 for both SFT and RL. We use AdamW optimizer. Learning rate is set to 1 105 for SFT and 5 107 for RL. λ in Eq. 6 is set to 0.3. Detailed hyperparameters and compute resources are deferred to Appendix B. For evaluation metrics, following (Dong et al., 2025), we utilize Qwen2.5-72B-Instruct as judge model to perform binary scoring based on ground truth and agent prediction. To ensure rigorous comparison with prior works (Dong et al., 2025; Wu et al., 2025a), unless otherwise specified, we report pass@1 using decoding temperature of 0.6 and top-p of 0.95. Evaluation details are shown in Appendix C."
        },
        {
            "title": "4.1 Can Textual Critiques Guide\nInference-Time Refinement?",
            "content": "To investigate the direct impact of textual critiques, we evaluate Reagent-Ca training-free varianton Qwen3-8B using Agent-RRM for critique guidance. As shown in Table 1 and Table 2, Reagent-C achieves consistent performance gains across all benchmarks without any parameter updates. Improvements are particularly pronounced in Mathematical Reasoning, while solid advancements are also observed in General Agentic and KnowledgeIntensive tasks. We attribute this versatility to Agent-RRMs diagnostic capacity, which effectively pinpoints logical fallacies and tool-execution errors within complex trajectories. See case study Table 2: Results on Knowledge-Intensive Reasoning and Math Benchmarks. (HQA: HotpotQA) Knowledge-Intensive Reasoning Mathematical Reasoning Backbone HQA 2Wiki Bamboogle MuSiQue AIME24 AIME25 MATH500 GSM8K Method Proprietary Agents - - - 54. 49.5 68.8 24.0 GPT-4o o1-preview Claude-4-Sonnet - - - - 43.3 42. 58.8 - 38.2 39.2 76.1 - - - 43.2 38.4 71.5 - Open-source Baselines (8B) Search-R1 (Jin et al., 2025) VerlTool (Jiang et al., 2025) Qwen2.5-7B Qwen2.5-7B1 ARPO (Dong et al., 2025) Qwen2.5-7B ARPO (Dong et al., 2025) Qwen3-8B AgentFlow (Li et al., 2025f) Qwen2.5-7B 57.0 77. 69.6 Open-source Baselines (32B) Search-o1 (Li et al., 2025d) QwQ-32B-Preview 45.2 Tree-GRPO (Ji et al., 2025) Qwen2.5-14B ARPO (Dong et al., 2025) Qwen3-14B 50.2 - 58.0 50. - Open-source Baselines with Process Reward Atom-Searcher (Deng et al., 2025) Qwen2.5-7B PPR-Instruct (Xu et al., 2025) Qwen2.5-7B Our Agents - Reagent w/o Agent-RRM Qwen3-8B Qwen3-8B Reagent-C (Direct Inference) Qwen3-8B Reagent-R Reagent-U Qwen3-8B Qwen3-8B 57.3 38. 66.9 31.0 52.0 65.8 61.0 67. 68.1 58.0 77.0 68.9 79.0 78. 56.0 54.4 - 70.7 41.2 53. 61.6 61.6 72.8 76.8 - - 19.6 18.0 31.1 - 25.3 16. 25.9 - 27.6 15.5 22.1 28. 25.0 28.3 31.3 13.4 46.7 43. - 36.7 33.3 33.3 40.0 56. - 25.7 - 33.1 - 33. 30.0 30.0 - - - 36. 30.0 - - 46.7 50.0 56. 53.3 60.0 - - 40.0 43. 46.7 50.0 50.0 60.3 85.5 93. - 82.8 88.8 88.4 - 86. - 83.0 - - 90.4 90. 93.8 92.2 93.8 - - - - 92.1 92.2 93.4 - - - 93.6 - - 94.6 94. 94.9 94.1 95.1 in Appendix D. The results confirm that the second response {o(2) } consistently achieves better performance compared to the initial response {o(1) }. Crucially, the widening margin between the first and second response underscores that many initial failures stem from transient execution errors or logical oversights. Our critiques are uniquely positioned to rectify these flaws by offering precise, actionable feedback. Furthermore, since Agent-RRM operates without access to ground-truth answers, these performance gains empirically validate its capacity to diagnose reasoning flaws and tool-execution errors. This highlights that textual critiques offer the high-granularity supervision essential for mastering complex, multi-step agentic tasks."
        },
        {
            "title": "4.2 Does Model-based Reward Improve",
            "content": "Learning? To explore whether dense model-based rewards can alleviate reward sparsity in agentic RL, we evaluate 1VerlTool backbones: Qwen2.5-7B (knowledge) and Qwen2.5-Math-7B (math). Reagent-R, which augments rule-based outcome rewards with holistic reasoning-level scores from Agent-RRM. As shown in Table 1 and Table 2, Reagent-R consistently outperforms rule-based reward baseline (Reagent w/o Agent-RRM) across all benchmarks. Specifically, Reagent-R achieves 72.8% on Bamboogle and 41.0% on xbench, surpassing Reagent w/o Agent-RRM by 11.2 and 9.0 percentage points, respectively. These results suggest that holistic model-based rewards provide more informative feedback for complex, multi-step reasoning scenarios, where sparse binary outcomes often provide overly coarse and limited guidance for learning. Reagent-R serves as critical ablation to isolate the impact of scalar supervision by excluding the textual critiques used in Reagent-U. While Reagent-R consistently outperforms sparse-reward baselines, it remains inferior to Reagent-U across most tasks. This performance gap suggests that while continuous scores better differentiate trajectory quality, they lack the explicit, structural guidance inherent in textual feedback. This indicates clear need for supervision with richer informational Table 3: Performance of Reagent-U on GAIA text set and full set (multi-modal). Model GAIA (text) GAIA (full) pass@1 pass@3 pass@1 pass@3 Qwen3-8B (Yang et al., 2025) 21.4 MCP-R1 (Anonymous, 2025) 39.8 Reagent-U 43.7 24. 52.4 53.4 20.0 37.6 38.8 26. 51.5 53.9 granularity, such as the textual critiques integrated into Reagent-U. Figure 3: Impact of Agent-RRM reward weight λ on task performance."
        },
        {
            "title": "4.3 Does Unified Feedback Synergistically",
            "content": "Boost Performance? Evaluations in Tables 1 and 2 reveal that the unified feedback mechanism in Reagent-U consistently outperforms all baselines across diverse spectrum of reasoning and agentic benchmarks. Specifically, Reagent-U achieves 43.7% on GAIA (text) and 46.2% on WebWalkerQA, surpassing all compared methods. Beyond its excellence in general agentic and search tasks, Reagent-U maintains robustness in knowledge-intensive and mathematical reasoning, securing 76.8% on Bamboogle and 60.0% on AIME24. In contrast, many existing baselines fail to generalize across diverse domains, often suffering from significant performance trade-offs. This balanced proficiency indicates that Reagent-U augments multi-tool, multi-turn reasoning capabilities rather than merely optimizing for web search. Such results demonstrate comprehensive long-horizon decision-making ability, effectively showing that the unified feedback mechanismintegrating both scalar rewards and textual critiquesallows the agent to internalize more sophisticated policy across complex, heterogeneous tasks."
        },
        {
            "title": "4.4 Beyond Text-Only: Cross-Modal\nReasoning and Complex Tool Use",
            "content": "To evaluate Reagent-Us proficiency across diverse modalities and tools, we conduct analysis on full GAIA benchmark. While existing studies (Dong et al., 2025; Jiang et al., 2025; Li et al., 2025b,f) focus on the text subset, which emphasizes web navigation and information retrieval ability, we argue that such narrow scope overlooks the heterogeneous reasoning capabilities required for complex real-world tasks. By evaluating on the GAIA full set, we challenge the agent with tasks requiring the integration of open-domain search, multimodal interpretation, python coding, and file-based reasoning. As shown in Table 3, Reagent-U not only maintains competitive performance on the text subset but also significantly outperforms baselines on the full set. These results confirm that ReagentU fosters versatile agentic intelligence that generalizes across broad task spectrum rather than overfitting to specific text-based requirements."
        },
        {
            "title": "4.5 Parameter Analysis on λ",
            "content": "To evaluate the impact of the Agent-RRM reward weight λ, we conduct parameter analysis on AIME24 (math) and xbench (deep search). Figure 3 shows that agent performance initially increases with rising λ values, demonstrating that the integration of reasoning-based rewards enhances the agents decision-making compared to the baseline (λ = 0). Specifically, performance reaches plateau between λ [0.2, 0.4], followed by slight decline at λ = 0.5. This trend suggests that while moderate reasoning feedback provides essential supervisory signals, disproportionately high weight may over-emphasize intermediate steps at the expense of final task completion. Consequently, balancing Agent-RRM rewards with rule-based outcome reward is crucial to maintain an optimal tradeoff between reasoning and outcome supervision."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce Agent-RRM, multifaceted reasoning reward model designed to provide textual critiques and holistic reasoning-aware reward. Building upon this, we present Reagent, comprehensive scheme designed to explore the efficacy of multi-dimensional feedback in agentic learning. Our systematic evaluations reveal that while textual critiques provide diagnostic guidance for inference-time refinement, model-based rewards serve to mitigate signal sparsity during training. Together, these signals significantly bolster the agents long-horizon reasoning and multi-step tooluse proficiency, leading to consistent gains across diverse complex, multi-modal tasks. Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168."
        },
        {
            "title": "6 Limitations",
            "content": "We discuss the limitations of our work and potential directions for future research as follows: First, our current experiments primarily focus on models at the 8B parameter scale. While this setting demonstrates the efficacy of our Reagent scheme, its scaling behavior on larger-scale models remains to be explored. Future work could investigate how more powerful base models might further amplify the benefits of structured reasoning feedback. Second, moving beyond standardized benchmarks to handle broader toolsets and more intricate reasoning chains is essential. Future works can explore open-ended, real-world applications (e.g., AI for science) that involve more diverse toolsets and unpredictable task environments to further validate the schemes adaptability."
        },
        {
            "title": "References",
            "content": "Anonymous. 2025. MCP-r1: Generalized real-world task agent mastering dozens of tools. In Submitted to The Fourteenth International Conference on Learning Representations. Under review. Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, and 1 others. 2025a. xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations. arXiv preprint arXiv:2506.13651. Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, and Yu Cheng. 2025b. Advancing multimodal reasoning: From optimized cold start to staged reinforcement learning. arXiv preprint arXiv:2506.04207. Shuang Chen, Yue Guo, Yimeng Ye, Shijue Huang, Wenbo Hu, Haoxi Li, Manyuan Zhang, Jiayu Chen, Song Guo, and Nanyun Peng. 2025c. Ares: Multimodal adaptive reasoning via difficulty-aware arXiv preprint token-level entropy shaping. arXiv:2510.08457. Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, and 1 others. 2025d. Rmr1: Reward modeling as reasoning. arXiv preprint arXiv:2505.02387. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, and 1 others. 2025. Atom-searcher: Enhancing agentic deep research via fine-grained atomic thought reward. arXiv preprint arXiv:2508.12800. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, and 1 others. 2025. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849. Kaixuan Fan, Kaituo Feng, Haoming Lyu, Dongzhan Zhou, and Xiangyu Yue. 2025. Sophiavl-r1: Reinforcing mllms reasoning with thinking reward. arXiv preprint arXiv:2505.17018. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. 2025a. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776. Kaituo Feng, Manyuan Zhang, Hongyu Li, Kaixuan Fan, Shuang Chen, Yilei Jiang, Dian Zheng, Peiwen Sun, Yiyuan Zhang, Haoze Sun, and 1 others. 2025b. Onethinker: All-in-one reasoning model for image and video. arXiv preprint arXiv:2512.03043. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. 2025c. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multihop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, Barcelona, Spain (Online). International Committee on Computational Linguistics. Ziyou Hu, Zhengliang Shi, Minghang Zhu, Haitao Li, Teng Sun, Pengjie Ren, Suzan Verberne, and Zhaochun Ren. 2025. Openreward: Learning to reward long-form agentic tasks via reinforcement learning. arXiv preprint arXiv:2510.24636. Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, and Liaoni Wu. 2025. Tree search for llm agent reinforcement learning. arXiv preprint arXiv:2509.21240. Ai Jian, Jingqing Ruan, Xing Ma, Dailin Li, QianLin Zhou, Ke Zeng, and Xunliang Cai. 2025. Patarm: Bridging pairwise and pointwise signals via preference-aware task-adaptive reward modeling. arXiv preprint arXiv:2510.24235. Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, and 1 others. 2025. Verltool: Towards holistic agentic reinforcement learning with tool use. arXiv preprint arXiv:2509.01055. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Hongyu Li, Manyuan Zhang, Dian Zheng, Ziyu Guo, Yimeng Jia, Kaituo Feng, Hao Yu, Yexin Liu, Yan Feng, Peng Pei, and 1 others. 2025a. Editthinker: Unlocking iterative reasoning for any image editor. arXiv preprint arXiv:2512.05965. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, and 1 others. 2025b. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592. Renhao Li, Jianhong Tu, Yang Su, Hamid AlinejadRokny, Derek Wong, Junyang Lin, and Min Yang. 2025c. One model to critique them all: Rewarding agentic tool-use via efficient reasoning. arXiv preprint arXiv:2510.26167. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025d. Search-o1: Agentic searchenhanced large reasoning models. arXiv preprint arXiv:2501.05366. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yongkang Wu, Ji-Rong Wen, Yutao Zhu, and Zhicheng Dou. 2025e. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776. Xiaoqian Liu, Ke Wang, Yuchuan Wu, Fei Huang, Yongbin Li, Junge Zhang, and Jianbin Jiao. 2025a. Agentic reinforcement learning with implicit step rewards. arXiv preprint arXiv:2509.19199. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025b. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783. Xing Han Lù, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Stanczak, Peter Shaw, Christopher Pal, and Siva Reddy. 2025. Agentrewardbench: Evaluating automatic evaluations of web agent trajectories. arXiv preprint arXiv:2504.08942. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, and 1 others. 2025. Humanitys last exam. arXiv preprint arXiv:2501.14249. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 56875711. Salman Rahman, Sruthi Gorantla, Arpit Gupta, Swastik Roy, Nanyun Peng, and Yang Liu. 2025. Spark: referenceStepwise process-aware rewards for arXiv preprint free reinforcement arXiv:2512.03244. learning. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Zhuofeng Li, Haoxiang Zhang, Seungju Han, Sheng Liu, Jianwen Xie, Yu Zhang, Yejin Choi, James Zou, and Pan Lu. 2025f. In-the-flow agentic system optimization for effective planning and tool use. arXiv preprint arXiv:2510.05592. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592. Minhua Lin, Zongyu Wu, Zhichao Xu, Hui Liu, Xianfeng Tang, Qi He, Charu Aggarwal, Xiang Zhang, and Suhang Wang. 2025. comprehensive survey on reinforcement learning-based agentic search: Foundations, roles, optimizations, evaluations, and applications. arXiv preprint arXiv:2510.16724. Sijun Tan, Michael Luo, Colin Cai, Tarun Venkat, Kyle Montgomery, Aaron Hao, Tianhao Wu, Arnav Balyan, Manan Roongta, Chenguang Wang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. 2025. rllm: framework for post-training language agents. https://pretty-radio-b75.notion.site/rLL M-A-Framework-for-Post-Training-Languag e-Agents-21b81902c146819db63cd98a54ba5f31. Notion Blog. Xinyu Tang, Yuliang Zhan, Zhixun Li, Wayne Xin Zhao, Zhenduo Zhang, Zujie Wen, Zhiqiang Zhang, and Jun Zhou. 2025. Rethinking sample polarity in reinforcement learning with verifiable rewards. arXiv preprint arXiv:2512.21625. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics. Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, and 1 others. 2024. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080. Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li, Sicheng Gao, Meng Meng, Xu Zhou, Manyuan Zhang, Yuzhang Shang, Adatooler-v: Adaptive and 1 others. 2025. arXiv preprint tool-use for images and videos. arXiv:2512.16918. Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, and Swarnadeep Saha. 2025. J1: Incentivizing thinking in llm-asa-judge via reinforcement learning. arXiv preprint arXiv:2505.10320. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, and 1 others. 2025a. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and 1 others. 2025b. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572. Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. 2025c. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965. Zhiheng Xi, Chenyang Liao, Guanyu Li, Yajie Yang, Wenxiang Chen, Zhihao Zhang, Binghai Wang, Senjie Jin, Yuhao Zhou, Jian Guan, and 1 others. 2025. Agentprm: Process reward models for llm agents via step-wise promise and progress. arXiv preprint arXiv:2511.08325. Peng Xia, Kaide Zeng, Jiaqi Liu, Can Qin, Fang Wu, Yiyang Zhou, Caiming Xiong, and Huaxiu Yao. 2025. Agent0: Unleashing self-evolving agents from zero data via tool-integrated reasoning. arXiv preprint arXiv:2511.16043. Peiran Xu, Zhuohao Li, Xiaoying Xing, Guannan Zhang, Debiao Li, and Kunyu Shi. 2025. Hybrid reward normalization for process-supervised arXiv preprint non-verifiable agentic tasks. arXiv:2509.25598. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP). Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, and Helen Meng. 2025a. Critique-grpo: Advancing llm reasoning with natural language and numerical feedback. arXiv preprint arXiv:2506.03106. Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, Tianke Zhang, Changyi Liu, Kaiyu Jiang, Kaibing Chen, Kaiyu Tang, and 1 others. 2025b. R1-reward: Training multimodal reward model through stable reinforcement learning. arXiv preprint arXiv:2505.02835. Yifan Zhang and Team Math-AI. 2024. American invitational mathematics examination (aime) 2024. Yifan Zhang and Team Math-AI. 2025. American invitational mathematics examination (aime) 2025. Zheng Zhang, Ziwei Shan, Kaitao Song, Yexin Li, and Kan Ren. 2025c. Linking process to outcome: Conditional reward modeling for llm reasoning. arXiv preprint arXiv:2509.26578. Zhiwei Zhang, Hui Liu, Xiaomin Li, Zhenwei Dai, Jingying Zeng, Fali Wang, Minhua Lin, Ramraj Chandradevan, Zhen Li, Chen Luo, and 1 others. 2025d. Bradley-terry and multi-objective reward modeling are complementary. arXiv preprint arXiv:2507.07375. Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, and Xiaolong Li. 2025e. Rlvmr: Reinforcement learning with verifiable meta-reasoning rewards for robust long-horizon agents. arXiv preprint arXiv:2507.22844. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics."
        },
        {
            "title": "A Dataset Details",
            "content": "All datasets will be publicly released to support future research. A.1 Agent Training Data Distribution We collect 709k question-answer pairs from publicly available datasets as our RL dataset, ReagentRL-709K. The detailed distribution information is shown in Figure 1 (bottom). We randomly select 100k data from RL dataset and utilize DeepSeekV3.1 to collect problem solving trajectories with our 6 tools. The trajectories reach to the correct final answer is saved as the SFT dataset in Figure 1 (top). In total, we collect 55.6k high quality trajectories for SFT training, denoted as Reagent-SFT-55.6K. A.2 Dataset Selection and Filtering DeepMath. Each question in DeepMath is accompanied by three independently generated solutions. We remove samples for which the final answers are inconsistent across the three solutions, as such cases introduce ambiguity in supervision. DeepScaleR. We filter out samples whose provided solutions produce answers inconsistent with the labeled ground-truth answer. SimpleRL-Zoo. To encourage non-trivial reasoning behavior, we subsample questions by difficulty, retaining higher proportion of mediumand hardlevel questions and fewer easy ones. MMK12. We select samples where the visual input consists of charts or tables, which can be reliably processed using OCR-based tool assistance. PixelReasoner. We select questions that require extracting information from textual content or visual avatars within images. These samples are solvable using combination of OCR and image description tools. LiveVQA. We retain questions that ask about identifiable attributes such as titles or authors present in images. Only samples from the imagebased subset are included. ToolVQA. We select questions from the ImageDescription, GoogleSearch, OCR, and Calculator categories to align with the agents available tool set. SimpleDeepSearcher. We convert the original tool-calling format into Qwen-compatible format to ensure consistency with the agents action space. AFM-WebAgent. We transform multi-agent interaction data into single-agent reasoning trajectory by linearizing planning, verification, and reflection steps. These components provide useful reasoning patterns for single-agent reasoning. LongAudio. We select audio samples with durations between 5 and 40 seconds. Audio clips shorter than 5 seconds typically lack sufficient informational content, while longer clips impose excessive computational overhead on the audio-totext model (whisper-large-v3). A.3 Agent-RRM Construction Details The prompt template employed to generate training data for Agent-RRM is detailed in Figure 4. Our primary design objective is to augment the reasoning-driven analytical capabilities of AgentRRM, ensuring it provides reliable and informative feedback at both the semantic and scalar levels. To this end, each training instance is structured into three distinct components to facilitate multigranular reasoning supervision: <think>: reasoning process that evaluates the logical consistency of the given trajectory. By explicitly articulating the rationale behind the assessment, this component provides transparency into how the reward model derives its judgments, thereby enhancing the interpretability of the final reward signal. <critique>: targeted summary that identifies reasoning flaws, with particular focus on global logic and the appropriate invocation of external tools. <score>: holistic scalar value derived from the preceding analysis to quantify the trajectorys overall quality. By condensing complex reasoning evaluations into standardized numerical format, this component serves as the formal reward signal required for advantage calculation within the reinforcement learning optimization loop. To support the development of our model, we curate two specialized datasets: Reagent-RRMSFT-28K, comprising 28,000 high-quality trajectories for initial supervised fine-tuning, and ReagentRRM-RL-90K, consisting of 90,000 instances designed for large-scale RL training."
        },
        {
            "title": "Instruction Prompt for Reward Model Annotation",
            "content": "You are an expert agent tool use evaluator. You must strictly follow the output format below: <think> Provide comprehensive analysis of the entire reasoning trajectory. Focus specifically on the agents reasoning quality and its tool-usage behavior across ANY type of tool. Key points to evaluate (for all tasks and all tools): - Whether the agent correctly decided when to call tools. Over-reliance on tools for trivial reasoning is bad; failing to call tools when necessary is also bad. - Whether the agent misused tools (e.g., calling an irrelevant tool, giving incorrectly formatted arguments, hallucinating tool inputs or filenames, making repeated tool calls without new purpose). - Whether the agent understood tool limitations (e.g., tool outputs may be incomplete, noisy, or partial; tools cannot access nonexistent resources). - Whether the agent improved its reasoning over time (e.g., corrected wrong assumptions, avoided repeated mistakes, verified hypotheses when possible). - Whether the agent avoided unverified guesses. Hypotheses without verification are harmful. - Whether the agent avoided fabricating tool results, file names, object identifiers, or other non-existent content. If uncertain, identify potential harmful reasoning patterns: unnecessary tool calls, missing essential tool calls, uncritical acceptance of tool output, faulty logical jumps, or incorrect assumptions about tool capabilities. Never mention the true answer. Only evaluate the reasoning process and tool use. </think> <critique> Provide succinct, specific, and actionable summary of issues in the agents reasoning and tool use. This section will be shown to the agent, so it must be concise and clearly highlight: - Incorrect, unnecessary, missing, or repeated tool calls. - Incorrect assumptions, unverified reasoning, or blind trust in tool results. - Any improper handling of tool limitations or constraints. - Any hallucinated tool arguments, filenames, or resource identifiers. - Unlogical reasoning. Do NOT provide the correct answer or hints toward it. </critique> <score> single float between 0 and 1 representing the overall quality of the reasoning and tool use. 0 means completely incorrect or harmful reasoning; 1 means flawless reasoning with appropriate, precise, and well-justified tool use. </score> Strict Requirements: Output exactly three blocks; focus solely on reasoning/tool-use; never reveal the correct answer. Figure 4: The prompt used for generating structured judgments of reward model."
        },
        {
            "title": "B Training Details",
            "content": "B.1 Training Codebase We use LLaMA-Factory (Zheng et al., 2024) to implement SFT training of both Agent-RRM and Reagent. Both reward model and agent model are trained for 2 epoches. We use rLLM (Tan et al., 2025) to implement Agentic RL training of Reagent. We use VeRL (Sheng et al., 2024) to implement RL training of Agent-RRM. The hyper-parameters we used during RL training is shown in Table 4. We conduct RL training for 300 steps. B.2 Tools configuration Our models are trained on 8 NVIDIA A800-80G GPUs. We detail the specific implementations of tools integrated into our agentic framework during Table 4: Hyper-parameters for Reinforcement Learning Training."
        },
        {
            "title": "Category",
            "content": "Hyper-parameter"
        },
        {
            "title": "Training Config",
            "content": "Generation & Env Base Model Optimizer Learning Rate Training Batch Size Mini-batch Size Max Agent Steps Temperature Top-p Rollout Samples (n) Lambda (λ) Qwen3-8B AdamW 5 107 64 16 13 0.7 0.95 8 0.3 training: Search: Powered by the Bing Search API. The agent receives the top-k results, including the URL, title, and content snippet for each entry. Browse: Website content is retrieved via the Jina Reader and subsequently condensed using DeepSeek-Chat as summarization model. Image2text: Visual queries and image-based reasoning are handled by GPT-4.1. Audio2text: Audio inputs are transcribed into text using the Whisper-large-v3 model."
        },
        {
            "title": "C Evaluation Details",
            "content": "C.1 Evaluation Benchmarks For GAIA, we report performance on the 103-task Text subset in Table 1 following Dong et al. (2025), while the full set results (165 tasks) are reported as GAIA (Full). For HLE, we evaluate on the 500task subset consistent with Dong et al. (2025), as shown in Table 1. For xbench, we report results specifically on the xbench-DeepSearch set. C.2 Tool Configurations The configurations for evaluation tools are identical to those detailed in Appendix B.2. C.3 Agent Evaluation Settings For all benchmark evaluations, we employ decoding temperature of 0.6 and top_p of 0.95. The maximum response length is constrained to 32,768 tokens, and the agent is permitted maximum of 30 tool-use steps per query. Consistent with Dong et al. (2025), we utilize Qwen2.5-72B-Instruct as the automated judge to perform binary scoring by comparing the agents predictions against the groundtruth answers."
        },
        {
            "title": "D Case Study",
            "content": "We present two case studies of Reagent-C in Figure 5 and Figure 6 to demonstrate the effectiveness of our critique mechanism. The critiques generated by Agent-RRM highlight logical inconsistencies or inappropriate tool usage in the initial responses. These critiques serve as informative signals that facilitate the agents self-correction, leading to successful reasoning refinement and error rectification in the subsequent responses. Figure 5: Case 1: search question from GAIA. Figure 6: Case 2: math question from GSM8K."
        }
    ],
    "affiliations": [
        "MMLab, CUHK",
        "Meituan",
        "SEEM, CUHK"
    ]
}