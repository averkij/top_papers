{
    "paper_title": "ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges",
    "authors": [
        "Jiaxin Ai",
        "Pengfei Zhou",
        "Zhaopan Xu",
        "Ming Li",
        "Fanrui Zhang",
        "Zizhen Li",
        "Jianwen Sun",
        "Yukang Feng",
        "Baojin Huang",
        "Zhongyuan Wang",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting MLLMs as automated process judges has become a common practice. However, the reliability of these model-based judges remains uncertain. To address this, we introduce ProJudgeBench, the first comprehensive benchmark specifically designed for evaluating abilities of MLLM-based process judges. ProJudgeBench comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and multi-modal content. In ProJudgeBench, each step is meticulously annotated by human experts for correctness, error type, and explanation, enabling a systematic evaluation of judges' capabilities to detect, classify and diagnose errors. Evaluation on ProJudgeBench reveals a significant performance gap between open-source and proprietary models. To bridge this gap, we further propose ProJudge-173k, a large-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning strategy that encourages models to explicitly reason through problem-solving before assessing solutions. Both contributions significantly enhance the process evaluation capabilities of open-source models. All the resources will be released to foster future research of reliable multi-modal process evaluation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 3 5 5 6 0 . 3 0 5 2 : r ProJudge: Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges Jiaxin Ai1,2, Pengfei Zhou3, Zhaopan Xu3, Ming Li3, Fanrui Zhang4,2, Zizhen Li5,2, Jianwen Sun5,2, Yukang Feng5,2, Baojin Huang6, Zhongyuan Wang1, Kaipeng Zhang3,2 1WHU, 2Shanghai Innovation Institude, 3Shanghai AI Laboratory, 4USTC, 5NKU, 6HZAU julyai@whu.edu.cn, zhangkaipeng@pjlab.org.cn, https://github.com/jiaxin-ai/ProJudge"
        },
        {
            "title": "Abstract",
            "content": "As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting MLLMs as automated process judges has become common practice. However, the reliability of these model-based judges remains uncertain. To address this, we introduce ProJudgeBench, the first comprehensive benchmark specifically designed for evaluating abilities of MLLM-based process judges. ProJudgeBench comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and multimodal content. In ProJudgeBench, each step is meticulously annotated by human experts for correctness, error type, and explanation, enabling systematic evaluation of judges capabilities to detect, classify and diagnose errors. Evaluation on ProJudgeBench reveals significant performance gap between open-source and proprietary models. To bridge this gap, we further propose ProJudge-173k, large-scale instruction-tuning dataset, and Dynamic Dual-Phase fine-tuning strategy that encourages models to explicitly reason through problem-solving before assessing solutions. Both contributions significantly enhance the process evaluation capabilities of open-source models. All the resources will be released to foster future research of reliable multi-modal process evaluation. 1. Introduction Recent multi-modal large language models (MLLMs) [14] have demonstrated remarkable capabilities in solving scientific problems. While final answer accuracy is commonly used to evaluate performance [58], the validity of the solving process itself is equally critical [911]. Without examinFigure 1. Task definition of process evaluation. For each step, MLLM-based process judges detect errors, classify error types and provide brief explanations. Based on these analyses, we derive insights into model weaknesses, guiding future improvements. ing intermediate steps, models may produce correct answers through flawed reasoning, leading to an overestimation of their true capabilities. Besides, compared with final answer evaluation, fine-grained error analyses of reasoning process can reveal more detailed model weaknesses, providing actionable insights for targeted improvements. These factors highlight the necessity of Process Evaluation. MR-GSM8K [12] CriticBench [13] MathCheck-GSM [14] ProcessBench [15] PRMBench [16] PROJUDGEBENCH Process Judge Benchmark Multi-modal Benchmark Multi-Discipline Benchmark Multi-Difficulty Problems Step-level Annotation Fine-grained Error Analysis Avg. Steps 8.3 - - 7.1 13.4 20.8 Table 1. Comparison between related benchmarks with our ProJudgeBench. As MLLMs become more advanced, evaluating lengthy and intricate reasoning processes becomes increasingly challenging for humans. As result, researchers are turning to prompt MLLMs as automated process judges [8, 1719]. While promising, this also raises critical question: How reliable are these model-based judges? Since MLLMs may be prone to errors and biases, their abilities to conduct accurate and objective process evaluation remains uncertain. However, there is currently lack of comprehensive benchmarks specifically designed to assess the capabilities of these model-based process judges. Although some existing benchmarks [1216] can be adapted for this purpose, they suffer from three critical limitations: (1) Narrow scope: Most are limited to single modalitiy, single discipline, or problems of limited difficulty, failing to capture the diverse challenges of real-world reasoning tasks. (2) Insufficient error analysis: They primarily focus on error detection, while neglecting the assessing of judges nuanced error diagnosing capabilitiesvital for enhancing judging explainability and uncovering model weaknesses. (3) Synthetic or Non-Generalizable Data: Many benchmarks are either tailored to specific policy models, or based on synthetic error data (e.g. GPT-modified errors) that fail to reflect diverse error patterns observed in actual model solutions, limiting their applicability in real-world scenarios. these limitations, we introduce ProJudgeBench, comprehensive benchmark specifically designed for assessing abilities of MLLM-based process judges. ProJudgeBench possesses the following distinctive features: (1) Multi-modal, Multi-discipline, and Multidifficulty: It comprises 2,400 test cases and 50,118 steplevel labels, spanning mathematics, physics, chemistry, and biology, with diverse difficulty levels and multi-modal content. (2) Fine-grained Error Analysis: We define seven error types that encompass common mistakes models may make in long reasoning chains. Each step is meticulously annotated by human experts for correctness, error type, and explanation, enabling systematic evaluation of process judges capabilities to detect, classify, and diagnose errors in scientific problem-solving tasks. (3) Realistic and Diverse Error Patterns: To ensure real-world applicability, we collect solutions from 10 distinct MLLMs of varying sizes, architectures, and design goals, reflecting broad"
        },
        {
            "title": "To address",
            "content": "spectrum of realistic reasoning behaviors and error patterns. We evaluate 11 MLLMs on ProJudgeBench, revealing significant performance gap between proprietary and opensource models. To bridge this gap, we further present ProJudge-173k, large-scale instruction tuning dataset designed for fine-grained evaluation of step-by-step reasoning. The dataset is constructed via two complementary pathways, ensuring both diversity and real-world relevance. Rigorous filtering guarantees high-quality synthetic annotations, providing foundation for fine-tuning open-source models as process judges. Besides, we propose Dynamic Dual-Phase (DDP) fine-tuning strategy that encourages models to explicitly reason through problem-solving steps before assessing solutions, mimicking the behavior of human experts. Such explicit thinking not only deepens the models understanding of the problems but also improves the robustness and generalizability of process judges. By fine-tuning on ProJudge-173k with the proposed DDP strategy, we demonstrate significant improvements in process evaluation capabilities of open-source models, narrowing their performance disparity with proprietary systems. Our contributions can be summarized as follows: We introduce ProJudgeBench, multi-modal, multidiscipline benchmark specifically designed for assessing fine-grained error detection, classification and diagnosis capabilities of MLLM-based process judges. To bridge the gap in process evaluation capabilities between open-source and proprietary models, we further present ProJudge-173k, large-scale instruction tuning dataset, and Dynamic Dual-Phase fine-tuning strategy. Both of these innovations significantly improve the process evaluation capabilities of open-source models. Comprehensive experiments uncover key challenges and limitations in current models, providing valuable insights into multi-modal reasoning and process evaluation. 2. Related Work Multi-Modal Benchmarks. With the fast development of recent MLLMs [1, 3, 2022], it becomes essential to evaluate their performance across variety of tasks to understand their strengths and weaknesses [5, 2336]. For example, MMMU [5] has been proposed to evaluate the scienFigure 2. An overview for data construcion process of ProJudgeBench and ProJudge-173k. tific problem-solving abilities of MLLMs across range of college-level subjects. MathVista [23] is widely used to assess the mathematical capabilities of MLLMs. While considerable progress has been achieved, these benchmarks often focus solely on evaluating final answers, overlooking crucial information of intermediate reasoning steps and potentially leading to unreliable results. MLLM-based Process Judges. MLLM-based process judges have been widely used to automatically evaluate multi-modal reasoning steps of large language models [8, 1719]. For example, MM-Math [17] incorporates MLLMas-a-judge to automatically analyze intermediate steps and identify errors. OlympicArena [8] employs GPT-4V [37] to score the correctness of each solution step, ensuring rigorous assessment. MathVerse [18] utilizes GPT-4V [37] to extract and assess key reasoning steps, providing nuanced scoring. Despite the widespread adoption, the reliability of MLLM-based judges is rarely scrutinized. Besides, current evaluation predominantly relies on proprietary models, suffering from prohibitive costs and reproducibility instability. This necessitates the development of open-source process judges. To address these challenges, we introduce ProJudgeBench, comprehensive benchmark for assessing MLLMs capabilities as process judges, and ProJudge173k, large-scale instruction-tuning dataset designed to enhance open-source MLLMs process evaluation abilities. Process Evaluation Benchmarks. There exist several benchmarks related to assessing process evaluation abilities of LLMs [1216]. For example, ProcessBench [15] measures the ability to identify erroneous steps in mathematical reasoning. PRMBench [16] synthesize erroneous steps based on PRM800k [11], evaluating fine-grained error detection capabilities of PRMs across multiple dimensions. Our ProJudgeBench is distinguished from prior benchmarks in three key aspects, as shown in Table 1. First, it covers four scientific discipline with multi-modal content and varying difficutly levels, reflecting the complexity of realworld reasoning tasks. Second, test cases are curated from diverse model-generated solutions instead of synthetic data, capturing wide range of realistic reasoning behaviors and error patterns. Third, each step is human-annotated for correctness, error type and explanation, enabling fine-grained evaluation of models error-diagnosing capabilities. 3. ProJudgeBench 3.1. Task Definition As shown in Figure 1, given scientific problem with its final answer A, and step-by-step solution = {s0, s1, , sn1} generated by student model, ProJudgeBench requires judge models to perform fine-grained evaluations of each step si by determining its correctness, classifying the error type and providing brief explanation. Specifically, for each step si, the model outputs tuple (ci, ei, ri), where ci {1, 0} indicates whether the step is correct (ci = 1) or not (ci = 0); ei denotes the error type; and ri is the natural language explanation of the error cause. To categorize errors, we define 7 error types based on thorough analysis of common mistakes that models tend to make during long-chain reasoning processes: reasoning error, visual interpretation error, numerical calculation error, symbolic calculation error, knowledge error, question understanding error, and no solution provided. Definitions of each category are displayed in Appendix B. Unlike existing benchmarks that focus solely on identifying the initial error location [11, 15, 17, 38], ProJudgeBench emphasizes fine-grained analysis of the entire reasoning process. This is motivated by the observation that, model-generated solutions often contain multiple errors of diverse types distributed across different steps. By requiring models to not only detect errors but also classify their types and diagnose their root causes, ProJudgeBench can provide more comprehensive judging tool for uncovering the weaknesses of student models and thereby providing actionable insights to guide their improvement. 3.2. Data Construction Problem Curation. We collect problems from two public benchmarks, OlympiadBench [39] and OlympicArena [8], covering four scientific disciplines and various modalities. Since both benchmarks focus on college and competitionlevel problems, we supplement the dataset with problems from primary, middle, and high school (referred to as K12) to ensure more diverse difficulty levels. In total, we curate 400 scientific problems, with balanced distribution across disciplines and difficulty levels, providing comprehensive set for analyzing scientific problem-solving processes. Step-by-Step Solution Generation. We generate solutions using various proprietary and open-source MLLMs, including GPT-4o [1], InternVL series [40], QwenVL series [20], LLaVA series [41] and MiniCPM-V [42], resulting in total of 10 distinct solution generators. Each problem is solved by 5 models randomly selected from the 10 candidates, with strict balancing mechanism to ensure equal usage across all models. The complete list of models and generating prompts are provided in Appendix C.1. To standardize the step granularity of each solution, we prompt Qwen2.5-72BInstruct [43] to split solutions into steps. This reformatting process ensures consistency in step segmentation, which is crucial for accurate human annotation. Solutions that were altered during reformatting (e.g., changes in final answers) are manually segmented to maintain the integrity. Additionally, we collect ground-truth solutions for each problem, where every step is labeled with ci = 1. Expert Annotation. We recruit 6 human experts with domain expertise (at the undergraduate level or above) for annotation. All of them are required to pass mandatory proficiency examination and complete an annotation tutorial. The annotators are tasked with evaluating each step in the model-generated solutions for correctness, error type, and root cause. To reduce annotation difficulty and ensure accuracy, we provide annotators with reference solution for each problem, helping them to first understand the correct problem-solving approach before assessing the modelgenerated solutions. To ensure annotation reliability, we implement rigorous quality control mechanism. Please refer to Appendix C.2 for details about quality control process. 3.3. Statistics The final ProJudgebench comprises 2400 test cases, with detailed statistics presented in Table 2. Generally, higherdifficulty problems often lead to more steps in modelgenerated solutions, with increased error rates and more diverse error types. We analyze the distribution of error types across different disciplines and difficulty levels in Figure 3. Reasoning errors are the most prevalent, accounting for the highest proportion of errors overall. Notably, reasoning errors are even more dominant in high-difficulty problems compared to routine problems, highlighting the chalFigure 3. Distribution of error types across different disciplines and difficulty levels in ProJudgeBench. K12 and Comp represent routine and competition-level problems, respectively. lenges models face in handling complex logical reasoning. Visual interpretation and calculation errors follow in frequency, suggesting that models still struggle with accurately interpreting visual information and performing precise calculations. In biology and chemistry problems, knowledgebased errors are more prominent, reflecting the models limitations in domain-specific understanding. 4. ProJudge-173k for Instruction Tuning 4.1. Data Construction As shown in Figure 2, we adopt two complementary paths to construct the dataset, each addressing different aspects of error generation and evaluation. Path 1: Controlled Error Injection. In this process, we utilize GPT-4o [1] to intentionally inject errors into correct solutions. We first apply few-shot learning to familiarize the model with various error types. Subsequently, we instruct the model to randomly inject multiple errors into correct solutions, creating flawed step-by-step reasoning paths. For this purpose, we select problems from the Camel [44] dataset, which is relatively easy for the model to understand and inject reasonable errors. Camel covers four disciplines: mathematics, physics, chemistry, and biology. For each discipline, we randomly select subset of 2,000 problems with their ground-truth solutions, which are then split into reasoning steps for injection. This process ensures high annotation accuracy since errors are deliberately injected with known types and causes. However, it sacrifices some realism, as the artificially introduced errors may differ from those naturally occurring in the models reasoning process. Path 2: Realistic Error Collection. This path closely mirrors the ProJudgeBench construction process. We begin by collecting competition-level problems from the training set of OlympiadBench [39], including 4,013 mathematics and"
        },
        {
            "title": "Statistic",
            "content": "ProJudgeBench - # Math / Phy. / Chem. / Bio. - # K12 / OlymBench / OlymArena - Avg. / Max. Steps - Avg. / Max. Error Types - Avg. / Max. / % Error Steps ProJudge-173k - # Math / Phy. / Chem. / Bio. - # Camel / K12 / OlymdBench - # Avg. / Max. Steps - Avg. / Max. Error Types - Avg. / Max. / % Error Steps"
        },
        {
            "title": "Number",
            "content": "2,400 600 1,350 / 250 / 625 20.8 / 470 1.5 / 5 6.6 / 226 / 21.6 173,354 58k / 40k / 37k / 35k 26k / 93k / 53k 18.2 / 926 1.4 / 5 5.9 / 402 / 24.7 Table 2. Statistics of ProJudgeBench and ProJudge-173k. 2,071 physics problems. To incorporate broader range of difficulty levels, we compile K12 dataset consisting of 4,000 problems each from mathematics, physics, chemistry, and biology, covering primary, middle and high school levels. To simulate real-world evaluation scenarios, we generate solutions using 9 diverse MLLMs, which are then segmented into step-by-step reasoning paths. GPT-4o [1] is then prompted to evaluate each step for correctness, error types and explanations. While this approach heavily depends on the expertise of the annotation model, it ensures consistency with real-world evaluation tasks by capturing errors commonly encountered during model reasoning. Data Filtering. To ensure data quality, we implement rigorous filtering processes, including format consistency, annotation consistency, and error coverage checks. Please refer to Appendix D.2 for more details. 4.2. Statistics The statistics are presented in Table 2. Our dataset spans wide range of difficulty levels and scientific domains, ensuring balanced representation across various problem types and evaluation scenarios. On average, each solution contains 18 steps, with some solutions extending to over 900 steps, reflecting the complexity of real-world reasoning tasks. The dataset also ensures diversity in error types and modalities, making it comprehensive resource for training MLLM-based process judges. To the best of our knowledge, ProJudge-173k is the first large-scale instruction tuning dataset specifically designed for process evaluation with fine-grained step-level annotations. 5.1. Phase 1: Direct Evaluate and its final answer xj This phase serves as the baseline task, where the model directly evaluates the students solution xS based on the problem xj S, the model generates an evaluation result organized as tuple (si, ci, ei, ri), denoting step description, correctness, error type, and root cause, respectively. A. For each step si in xj A, yj PE)}N j=1, where yj The training set for this task can be expressed as: DDE = {(xj P, xj PE represents the ground-truth step annotations and denotes the dataset size. During training, the model minimizes the cross-entropy loss between its predictions and the ground-truth annotations: yj PE (cid:88) LDE(θ, DDE) = log p(yj PE,txj P, xj A, xj S, yj PE,<t; θ)] (cid:88) ["
        },
        {
            "title": "1\nN",
            "content": "j=1 t=1 where yj quence, yj (1) PE,t denotes the t-th token in the ground-truth sePE,<t represents the preceding tokens. 5.2. Phase 2: Synthesize-then-Evaluate In this phase, the model is encouraged to explicitly reason through problem-solving steps before evaluating student solutions, closely mimicking the behavior of human experts. Formally, the model is first required to reason through the problem xj and synthesize reference solution ˆS, which is expected to reach the correct final answer xj A: ˆS p(xj A, Isolv; θ). Isolv =Lets solve the problem step by step to get the correct final answer. P, xj With ˆS generated, the model gains deeper understanding of the problem, enabling more informed judgments. This also provides reference for subsequent process evaluation. Based on ˆS, the model conducts process evaluaˆP tion on the students solution xj p( ˆS, xj S, Isolv; θ), where Ieval =Based on our solution, lets evaluate the students solution step by step. S, formalized as: S, yj S), ( ˆS, xj A, Isolv, yj PE, Ieval)}N The training set for this phase is structured as: DSE = {(xj P, xj and yj PE denote the ground-truth solution and process evaluation results respectively. During training, the model is optimized on two separate cross-entropy losses to handle both the solution synthesis and the process evaluation tasks: yj (cid:88) j=1, where yj log p(yj S,txj P, xj A, Isolv, yj S,<t; θ) LSE(θ, DSE) = (cid:88) ["
        },
        {
            "title": "1\nN",
            "content": "j=1 t=1 + yj PE (cid:88) t=1 log p(yj PE,t ˆS, xj S, Ieval, yj PE,<t; θ)] (2) 5. Dynamic Dual-Phase Fine-tuning To enhance the robustness of process judges, we propose Dynamic Dual-Phase fine-tuning strategy, which consists of two training phases: Direct Evaluate and Synthesize-thenEvaluate. 5.3. Dynamic Dual-Phase Training During fine-tuning, the model dynamically alternates between the two phases with probability p. This mechanism introduces variability and diversity into the training process, enhancing models robustness and generalizability across wide range of evaluation scenarios. Model Name InternVL2.5-8B InternVL2.5-26B InternVL2.5-38B MiniCPM-V-2 Qwen2.5-VL-3B Qwen2-VL-7B Qwen2-VL-72B Gemini-2.0-flash-exp Gemini-2.0-thinking-exp GPT-4o Step Corr. 25.58 66.72 78.85 23.26 11.47 34.65 77. 72.51 72.61 85.10 Overall RE. VI. NC. SC. KE. QU. NS. Error Types 6. 13.51 17.12 0.13 0.84 0.69 33.87 35.54 35.27 44.89 Open-source MLLMs 8.19 16.44 17.95 0.03 0.89 0.55 40.11 0. 0.64 3. 0. 0. 0. 1.49 Proprietary MLLMs 40.70 40.06 52. 26.60 27.67 9.61 8.18 14.80 27.08 1.68 2.11 1.55 39. 30.18 32.44 40.90 1.75 2.63 8.33 0. 0.43 0.43 30. 5.70 4.82 27.19 1.71 2.85 17.54 0. 0.57 2. 1.07 2.15 25.80 0. 0. 0. 0. 4.16 2. 0. 0. 0. 10.27 4.83 2.08 12. 11.98 16.11 8.06 9.13 30.10 8.33 8.33 2. InternVL2.5-8B Qwen2.5-VL-3B Qwen2-VL-7B Fine-tuned MLLMs on ProJudge-173k 15.12+13.95 3.22+2.15 10.41+10.41 84.50+58.92 45.39+38.62 53.97+45.78 30.98+30.98 22.14+13.96 81.29+70.45 39.09+38.25 46.36+45.47 24.67+24.67 24.54+22.43 4.16+4.16 12.69+12.12 1.61+1.61 83.72+49.07 44.57+43.88 52.29+51.47 31.62+31.62 27.36+25.81 11.40+10.97 14.69+11.98 4.30+4.30 6.25+6.25 8.33+6.58 5.70+5.27 Table 3. Performances comparison of MLLMs on ProJudgeBench. The best performance is in bold, while the second-best is underlined. denotes the models fine-tuned on ProJudge-173k with Dynamic Dual-Phase strategy. In fine-tuned MLLMs, + (or ) indicates the performance gain (or lose) compared to the baseline open-source models. 6. Experiments To provide comprehensive evaluation of various models on ProJudgeBench, we select diverse set of both proprietary and open-source MLLMs, including GPT-4o [1], Gemini-2.0-flash-exp [3], Gemini-2.0-thinking-exp [45], InternVL2.5 [40] (8B, 26B, 38B), MiniCPM-V-2 6 [42] (8B), and Qwen2.5-VL-Instruct [20] (3B, 7B, 72B). Additionally, we evaluate fine-tuned versions of InternVL2.58B, Qwen2.5-VL-3B, and Qwen2.5-VL-7B. To ensure consistency, all models use the same input format and evaluation prompts, with hyperparameters following configurations in VLMEvalKit [46]. The evaluation prompts are provided in Appendix F.4. We use accuracy as the primary evaluation metric, focusing on two aspects: (1) Step Correctness Accuracy, the accuracy of determining whether each step in students solution is correct; and (2) Error Types Classification Accuracy, the accuracy of identifying specific error types, including Reasoning Errors (RE), Visual Interpretation Errors (VI), Numerical Calculation Errors (NC), Symbolic Calculation Errors (SC), Knowledge Errors (KE), Question Understanding Errors (QU), and No Solution Provided (NS). 6.1. Main Results and Findings In this subsection, we compare the performance of 11 opensource and proprietary models and 3 fine-tuned models. The experimental results are presented in Table 3. Finding 1: Model Scale Impacts Process Evaluation Capabilities. Larger models exhibit significantly higher accuracies in both step correctness and error type classification. For example, InternVL series exhibits clear scaling effect, with the step correctness accuracy increasing from 25.58% for the 8B model to 66.72% and 78.85% for the 26B and 38B versions, respectively. In contrast, smaller models like MiniCPM-V-2 6 exhibit weaker reasoning capabilities, with near-zero accuracy in several error categories. Finding 2: Proprietary Models Maintain Performance Lead. GPT-4o achieves the highest step correctness accuracy and excels across multiple error categories. Gemini-2.0 series, while slightly behind GPT-4o, still outperforms most open-source models, with step correctness accuracy around 72.5% and shows particular strength in identifying visual interpretation errors. These results underscore the advanced multi-modal reasoning capabilities of proprietary models. Finding 3: Fine-tuning Significantly Enhances Model Performance. Open-source models like InternVL2.58B and Qwen2.5-VL-3B show significant gains after fine-tuning, with step correctness accuracy increasing by 58.92% and 70.45% respectively, bringing their performance on par with proprietary models. With sufficient fine-tuning, smaller models can approach the performance of much larger counterparts. For instance, the fine-tuned Qwen2.5-VL-3B achieves 81.29% accuracy, surpassing Qwen2.5-VL-72B. These results highlight the transformative impact of domain-specific fine-tuning. Finding 4: Models Exhibit Unique Strengths Across Different Error Types. While GPT-4o leads overall, open Figure 4. Performance of MLLM-based process judges across different disciplines, difficulty levels and modalities. source models like Qwen2.5-VL-72B and InternVL2.5-38B also excel in certain tasks. For example, Qwen2.5-VL-72B performs remarkbly well in identifying reasoning and calculation errors, while InternVL2.5-38B leads in detecting knowledge and question understanding errors, highlighting the nuanced strengths of different model architectures. 6.2. Further Analysis To gain deeper insights into the capabilities of MLLMs as process judges, we explore several key research questions (RQ): (1) How does model performance vary across different domains? (2) How do models perform as judges when assessing solutions from different student models? (3) What is the relationship between models ability to detect errors and its tendency to generate similar errors? 6.2.1. RQ1: Performance Across Different Domains We analyze model performance across different disciplines (math, physics, chemistry, biology), difficulty levels (primary, middle, high school, competition), and modalities (pure text, single image, multiple images). The results, illustrated in Figure 4, reveal several key insights into how MLLM-based process judges perform in diverse scenarios. Most MLLMs exhibit higher accuracy in biology and chemistry, compared to mathematics and physics. For instance, Qwen2.5-VL-72B achieves 84.13% accuracy in biology but only 77.08% in physics, while InternVL2.538B achieves 81.22% in chemistry but 73.14% in math. This discrepancy can be attributed to the nature of the tasks: biology and chemistry often rely on factual knowledge and rule-based reasoning, making them easier for models to evaluate. In contrast, math and physics demand complex logical reasoning, multi-step derivations and the ability to interprete real-world scenarios, which require precise logical validation during evaluation, leading to misjudgements when models fail to grasp subtle errors in reasoning. As task difficulty increases, model accuracy in step evaluation declines. For instance, Gemini-2.0 achieves Figure 5. Model-as-Judge Performance across different models. Each position (x, y) in the heatmap represents the accuracy of model as judge in assessing solutions generated by model y. 80.2% accuracy in high school tasks but drops to 56.2% in OlympiadBench tasks. This trend reflects the challenges for current models to deal with complex, multi-step reasoning tasks, which require both advanced domain knowledge and the ability to synthesize information in novel ways. Interestingly, primary school tasks do not always show the highest accuracy, despite their relative simplicity. This can be attributed to their variablity and context-dependent nature, which may involve unconventional reasoning steps or less structured problem-solving processes, posing challengies for models trained on formal and structured datasets. Open-source models face challenges with tasks involving images. Pure text tasks, which rely solely on textual information, align well with the core strengths of language models, resulting in more accurate step evaluation. In contrast, tasks involving images require models to interpret visual elements and integrate them with textural reasoning, complicating the evaluation process and reducing accuracy. This underscores the limitations of current open-source models in multi-modal reasoning and evaluation. Model Overall OlympicArena InternVL2.5-8B (Base) + FT + FT + DDP (Full) QwenVL2.5-3B (Base) + FT + FT + DDP (Full) QwenVL2.5-7B (Base) + FT + FT + DDP (Full) 25.58 83.37 84.50 11.47 80.57 81.29 34.65 81.91 83.72 22.33 81.71 85.07 11.92 80.38 81.01 38.25 77.88 82. Table 4. Ablation study on Dynamic Dual-Phase fine-tuning strategy. We compare the standard fine-tuning (+ FT) with DDPenhanced training (+ DDP). Overall and OlympicArena represent the results on the full set of ProJudgeBench and its OlympicArena subset respectively. rors. This suggests the potential for reducing such errors by introducing self-checking mechanisms. Besides, both models show low knowledge error generation frequency and detection accuracy, suggesting that models may lack domain knowledge in certain areas, leading to mistakes when encountering related concepts and an inability to identify such errors. Targeted training on relevant domain knowledge could help improve performance in both aspects. 6.3. Ablation Study In this section, we conduct an ablation study to evaluate the effectiveness of our Dynamic Dual-Phase (DDP) fine-tuning strategy. As shown in Table 4, we compare the standard fine-tuning (+FT) with DDP-enhanced training (+DDP). Our evaluation focuses on two aspects: overall step correctness accuracy and results on the unseen OlympicArena test set, which allows us to assess both in-domain performance and generalization capabilities. All models were fine-tuned for one epoch using LoRA [47], with further details provided in Appendix E.1. The results demonstrate that DDP strategy consistently improves performance across all models, with particularly notable gains in generalization to unseen problem types. On the challenging OlymicArena test set, which contains entirely novel competition-level problems, DDP-enhanced models achieve significant performance boostsInternVL2.5-8B improves by 3.36%, while QwenVL2.5-7B gains 4.79%. This suggests that the explicit problem-solving phase in DDP not only deepens the models understanding of scientific reasoning, but also enhances its robustness and generalizability to evaluate unfamiliar solutions. 7. Conclusions In this paper, we introduce ProJudgeBench, multi-modal, multi-discipline benchmark, assessing the fine-grained error detection, classification and diagnosis capabilities of Figure 6. Relationship between error generation and detection. VI, NC/SC, KE, QU and NS represent for visual interpretation errors, calculation errors, knowledge errors, question understanding errors and no solution provided respectively. 6.2.2. RQ2: Cross-Model Evalution Figure 5 illustrates the performance of different models acting as process judges when evaluating solutions generated by other models (student models). We find that InternVL models demonstrate higher accuracy when evaluating themselves or models from the same series. This can be attributed to the shared architectures, training data, and reasoning styles within the same series, enabling them to better understand and assess the reasoning processes and error patterns of their counterparts. Besides, larger models tend to achieve higher accuracy when evaluating smaller models, whereas smaller models struggle to assess larger ones effectively. For example, GPT-4o reaches 84.31% accuracy when evaluating MiniCPM-V-2 6, while MiniCPM-V2 6 achieves only 15.69% when evaluating GPT-4o. This indicates the superior reasoning abilities of larger models, which enable them to better analyse and validate the outputs of smaller models. In contrast, smaller models, constrained by their limited capacity, often struggle to accurately evaluate the more complex reasoning steps of larger models. 6.2.3. RQ3: Error Detection vs. Error Generation Given that models vary in their ability to detect different error types, we aim to explore the relationship between error types and detection performance. We investigate two MLLMs across five error types, examining: their frequency of generating these errors when acting as student models; and their accuracy in detecting these errors as process judges. The results are shown in Figure 6. Both models exhibit high generation rates and low detection accuracy in visual interpretation errors, revealing their limitations in interpreting and reasoning about visual information. For calculation errors, both models have relatively high generation frequency and also notable detection accuracy, indicating that while models are prone to calculation errors, they possess reasonable ability to identify such erMLLM-based process judges. We also present ProJudge173k, large-scale instruction-tuning dataset, and Dynamic Dual-Phase fine-tuning strategy. Both significantly enhance the process evaluation capabilities of open-source models. Through extensive experiments, we identify key challenges in current models and provide actionable insights for future advancements. Our work lays foundation for process evaluation in multi-modal reasoning, and we hope it will inspire future research in this critical area."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Gpt-4o system card. https://cdn.openai. com/gpt4osystemcard.pdf, 2024. Accessed: 2024-09-26. 1, 2, 4, 5, 6 [2] OpenAI. https : Learning to reason with llms. //openai.com/index/learningtoreasonwith-llms/, September 2024. [3] DeepMind. Gemini 2.0 flash experimental. https : / / deepmind . google / technologies / gemini / flash/, 2024. Accessed: 2024-12-25. 2, 6, 1 [4] Qwen Team. Qvq: To see the world with wisdom, December 2024. 1 [5] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 1, 2 [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. [8] Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, et al. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai. Advances in Neural Information Processing Systems, 37:1920919253, 2025. 1, 2, 3, 4 [9] Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. Evaluating mathematical reasoning beyond accuracy. arXiv preprint arXiv:2404.05692, 2024. [10] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [11] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. 1, 3 [12] Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. Mr-gsm8k: meta-reasoning benchmark for large language model evaluation. arXiv preprint arXiv:2312.17080, 2023. 2, 3, 1 [13] Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. CriticBench: Benchmarking LLMs In Findings of the Associafor critique-correct reasoning. tion for Computational Linguistics: ACL 2024, pages 1552 1587, 2024. 2, 1 [14] Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek Wong, Xiaowei Huang, Qiufeng Wang, and Kaizhu Huang. Is your model really good math reasoner? evaluating mathematical reasoning with checklist. arXiv preprint arXiv:2407.08733, 2024. 2, [15] Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. 2, 3, 1 [16] Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: fine-grained and challenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025. 2, 3, 1 [17] Kai Sun, Yushi Bai, Ji Qi, Lei Hou, and Juanzi Li. MMMATH: Advancing multimodal math evaluation with process evaluation and fine-grained classification. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 13581375, 2024. 2, 3, 1 [18] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. 3, 1 [19] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. 2, 3, 1 [20] Qwen Team. Qwen2.5-vl, January 2025. 2, 4, 6, 1 [21] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. [22] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv preprint arXiv:2403.05525, 2024. 2, 1 [23] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations. 2, 3, 1 [24] Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, and Yu Qiao. Mm-safetybench: benchmark for safety evaluation of multimodal large language models. In European Conference on Computer Vision, pages 386403. Springer, 2024. [25] Ming Li, Jike Zhong, Tianle Chen, Yuxiang Lai, and Konstantinos Psounis. Eee-bench: comprehensive multimodal arXiv electrical and electronics engineering benchmark. preprint arXiv:2411.01492, 2024. 1 [26] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: comprehensive evaluation benchmark for large vision-language models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [27] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [28] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. In Forty-first International Conference on Machine Learning. [29] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [30] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. [31] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering In Proceedings benchmark requiring external knowledge. of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. [32] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [33] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. [34] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. [35] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2025. [36] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2025. 2, 1 [37] OpenAI. Gpt-4v(ision) system card. https://cdn. openai.com/papers/GPTV_System_Card.pdf, 2023. Accessed: 2024-09-26. 3 [38] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935, 2023. 3 [39] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, pages 38283850, 2024. [40] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 4, 6 [41] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 4 [42] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 4, 6 [43] Qwen Team. Qwen2.5: party of foundation models, September 2024. 4 [44] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for mind exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [45] DeepMind. https : Gemini 2.0 flash thinking. / / deepmind . google / technologies / gemini / flash-thinking/, 2025. Accessed: 2025-01-21. 6 [46] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. 6 [47] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 8 A. Related Work A.1. Multi-Modal Benchmarks With the fast development of recent MLLMs [1, 3, 2022], it becomes essential to evaluate their performance across variety of tasks to understand their strengths and weaknesses, which is crucial for guiding future developments and enhancements [5, 23, 25, 2736]. For example, MMMU [5] has been proposed to evaluate the scientific problem-solving abilities of MLLMs across range of college-level subjects. MathVista [23] is widely used to assess the mathematical capabilities of MLLMs. While considerable progress has been achieved, these benchmarks often focus solely on evaluating final answers, overlooking crucial information of intermediate reasoning steps and potentially leading to unreliable results. A.2. MLLM-based Process Judges As MLLMs regularly make mistakes when solving scientific problems [5, 18], evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is costly and time-consuming, prompting MLLMs as automated process judges has become common practice. MLLM-based process judges have been widely used to automatically evaluate reasoning steps of multi-modal language models. For example, MMMath [17] incorporates MLLM-as-a-judge to automatically analyze solution steps, identifying and categorizing errors into specific error types. OlympicArena [8] employs GPT-4V to conduct process-level evaluations, scoring the correctness of each reasoning step to ensure rigorous assessment. MathVerse [18] introduces Chain-of-Thought (CoT) evaluation strategy, using GPT-4V to extract and assess key reasoning steps, providing fine-grained error analysis and nuanced scoring that goes beyond binary correctness. MME-CoT [19] extends this approach by evaluating CoT reasoning across multiple domains, while introducing metrics for reasoning quality, robustness, and efficiency. Despite the widespread adoption, reliability of MLLM-based judges themselves is rarely scrutinized. Besides, current evaluation predominantly relies on proprietary models, suffering from prohibitive costs and reproducibility instability. This necessitates the development of open-source process judges. To address these challenges, we introduce ProJudgeBench, comprehensive benchmark for assessing MLLMs capabilities as process judges, and ProJudge-173k, large-scale instruction-tuning dataset designed to enhance open-source MLLMs process evaluation abilities. A.3. Process Evaluation Benchmarks There exist several benchmarks related to assessing process evaluation abilities of LLMs. MR-GSM8K [12] introduces meta-reasoning paradigm, requiring LLMs to transition from solving problems to evaluating the correctness of reasoning steps. MathCheck-GSM [14] presents checklist-based framework, where LLMs are tasked with evaluating both final answers and intermediate reasoning steps. CriticBench [13] assesses the ability of LLMs to critique and correct their reasoning across multiple domains. ProcessBench [15] measures the ability to identify erroneous steps in mathematical reasoning, particularly in competition-level problems. PRMBench [16] synthesize erroneous steps based on PRM800k [11], evaluating fine-grained error detection capabilities of PRMs across multiple dimensions. Our ProJudgeBench is distinguished from prior benchmarks in three key aspects: First, it covers four scientific discipline with multi-modal content and varying difficutly levels, reflecting the complexity of real-world reasoning tasks. Second, test cases are curated from diverse model-generated solutions instead of synthetic data, capturing broad realistic reasoning behaviors and error patterns. Third, each step is human-annotated for correctness, error type and explanation, enabling fine-grained evaluation of models error-diagnosing capabilities. B. Definitions of Error Types As described in Section 3.1, we define seven error types based on comprehensive analysis of common mistakes that models tend to make during long-chain reasoning processes. The definitions of each category are displayed in Table 5. C. Detailed Information of ProJudgeBench In this section, we provide detailed information on ProJudgeBench, including the list of MLLMs used for generating solutions in data construction, instruction for human annotators, our annotation website, quality control process, and breakdown statistics. C.1. Data Construction As described in Section 3.2, we utilize 10 distinct MLLMs to generate solutions in ProJudgeBench. The list of utilized MLLMs are presented in Table 6."
        },
        {
            "title": "No solution provided",
            "content": "Errors in basic arithmetic operations such as addition, subtraction, division, or square roots. Errors in manipulating algebraic expressions, such as incorrect expansion, factoring, simplification, or solving equations with variables. Errors in interpreting graphical data, such as misidentifying coordinates, shapes, spatial relationships, or data within figures. Errors in the logical thinking process that lead to incorrect conclusions, such as flawed arguments, invalid inferences, or gaps in the logical flow of the solution. Errors caused by insufficient understanding or incorrect application of necessary knowledge (e.g., concepts, formulas, theorems, methods), or using outdated or incorrect information. Errors due to misunderstanding or misinterpreting the problems conditions or requirements, such as misreading questions or misapplying given conditions. The model refuses to answer, fails to follow instructions to make solution, or encounters anomalies in generation process such as repetitive responses or incomplete outputs. Table 5. Definitions of seven error types."
        },
        {
            "title": "MLLMs as solution generators in ProJudgeBench",
            "content": "InternVL2.5-8B InternVL2.5-26B InternVL2.5-38B Qwen2.5-VL-Instruct-3B Qwen2.5-VL-Instruct-7B Qwen2.5-VL-Instruct-72B MiniCPM-V-2 6 (8B) QVQ-72B-Preview LLaVA-OneVision (7B) GPT-4o Table 6. List of MLLMs used in ProJudgeBench to generate solutions. C.2. Quality Control To ensure annotation reliability, we implement rigorous quality control mechanism. First, random subset of solutions are selected for repeated annotation to measure inter-annotator agreement. In cases where annotations exhibit significant discrepancies, the solutions are flagged for re-annotation. Additionally, solutions with missing annotations or contradictory annotations (e.g., step marked as correct but assigned an error type) are also flagged for review and re-annotation. Besides, we conduct regular annotation review meetings where annotators discuss challenging cases and resolve ambiguities collaboratively. We also develop detailed annotation guideline document, which is continuously updated based on annotator feedback and edge cases encountered during the annotating process. The multi-layered approach ensures high level of consistency and reliability in the final annotations. C.3. BreakDown Statistics The breakdown statistics of ProJudgeBench is shown in Table 7. We can see that, as the difficulty of the problems increases, the average number of steps per solution also rises. Besides, higher-difficulty problems exhibit greater proportion of erroneous steps and more diverse error types, with competition-level math problems reaching up to 512 steps, indicating that models tend to generate more steps when tackling complex problems. Higher-difficulty problems also exhibit greater proportion of erroneous steps and more diverse error types, with some solutions containing up to 8 erroneous steps and 4 distinct error types. This highlights the importance of fine-grained process evaluation: if the judge model can diagnose all these errors, it will enable more comprehensive analysis of the models weaknesses, offering targeted feedback for improvement. We also plot the frequency of the first occurrence step for different error types in Figure 7 . While the distribution varies across error categories, consistent overall pattern emerges: errors peak in frequency during the earlier steps (e.g., steps 0-3)"
        },
        {
            "title": "Biology",
            "content": "K12 Comp K12 Comp K12 Comp K12 Comp # Samples Avg. Steps Max. Steps % Error Steps Avg. Error Steps Max. Error Types 2400 20.8 470 21.6 6.6 5 450 23.7 470 23.0 8.1 4 150 21.3 215 19.7 7.0 4 300 21.5 199 21.2 7.0 300 26.0 353 23.1 8.4 4 300 20.0 209 18.9 5.7 4 300 19.9 197 24.4 6.2 5 300 15.2 78 15.7 4.2 3 300 18.0 150 23.4 5.7 4 Table 7. Statistics of ProJudgeBench. K12 and Comp represent normal and competition-level problems, respectively. Figure 7. Distribution of the first occurance steps for different error types. Truncated to 16 for better visualization. and gradually decline thereafter. Visual interpretation errors show distinct peak at step 3, highlighting the models initial struggles with accurately interpreting visual elements. Calculation errors, on the other hand, are more evenly distributed across all steps, indicating persistent challenges in performing precise mathematical operations throughout the reasoning process. In contrast, reasoning errors become more frequent in later steps, suggesting that models face increasing difficulty in maintaining logical consistency as the solution progresses. D. Detailed Information of ProJudge-173k In this section, we provide detailed information on ProJudge-173k, including the list of MLLMs used for generating solutions in data construction, data filtering process, and breakdown statistics. D.1. Data Construction As described in Section 4.1, we utilize 9 distinct MLLMs to generate solutions in ProJudge-173k. The list of utilized MLLMs are presented in Table 8. MLLMs as solution generators in ProJudge-173k InternVL2.5-8B InternVL2.5-26B InternVL2.5-38B Qwen2.5-VL-Instruct-3B Qwen2.5-VL-Instruct-7B Qwen2.5-VL-Instruct-72B MiniCPM-V-2 6 (8B) QVQ-72B-Preview LLaVA-OneVision (7B) Table 8. List of MLLMs used in ProJudge-173k to generate solutions."
        },
        {
            "title": "Total",
            "content": "Camel-AI K"
        },
        {
            "title": "Olympiad",
            "content": "# Samples # Problems # Math # Physics # Chemistry # Biology Avg. Steps Max. Steps % Error 173,354 26,084 58,922 40,910 37,789 35,733 18 926 24.72 26,249 8,000 7,196 6,268 6,656 6,129 11 77 20. 93,184 12,000 15,908 16,539 31,133 29,604 17 487 23.55 53,921 6,084 35,818 18,103 0 0 22 926 27.52 Table 9. Statistics of data sources and quantities in ProJudge-173k. D.2. Data Filtering As described in Section 4.1, we conduct rigorous filtering processes to ensure data quality. Specifically, we apply three quality control mechanisms: (1) Format Consistency: We remove samples that deviate from the predefined format, where each step in the student solution must be annotated as tuple containing the step description, correctness, error type, and brief explanation. Additionally, samples with mismatched step counts between annotations and student solutions are discarded. (2) Annotation Consistency: Samples with contradictory or incomplete annotations, such as steps marked as incorrect but lacking error type or cause descriptions, are excluded. (3) Error Coverage: Samples with insufficient error diversity or repetitive error patterns are excluded to maintain dataset variety. D.3. BreakDown Statistics The statistics of the data sources and quantities are presented in Table 9. By providing detailed error analysis, realistic reasoning paths, and diverse problem types, our dataset lays solid foundation for advancing research in process evaluation, particularly for improving models capabilities of error diagnosis in complex reasoning tasks. E. Detailed Information of Process Evaluation E.1. Fine-tuning Details In the experiments, we fine-tune InternVL2.5-8B, Qwen2.5-VL-3B-instruct and Qwen2.5-VL-7B-Instruct on ProJudge-173k with Dynamic Dual-Phase strategy. The training is conducted on 8 H100 GPUS. All the models are fine-tuned using LoRA for one epoch, with the same training set. The global batch size is set to 16, with per-device batch size of 4 and gradient accumulation steps of 2. For InternVL2.5-8B, we employ learning rate of 4e-5, optimized using cosine learning rate scheduler with warmup ratio of 0.03. Additionally, we applied weight decay of 0.05 to regularize the training process and prevent overfitting. For Qwen2.5VL-3B and Qwen2.5-VL-7B, the models are trained with learning rate of 1.0e-4, also using cosine learning rate scheduler and warmup ratio of 0.1. Both fine-tuning processes utilize mixed-precision training (bf16) to accelerate computation and reduce memory usage. For InternVL2.5-8B, we additionally enable gradient checkpointing to further optimize memory usage during training. F. Prompts F.1. Prompts for Injection Errors As discussed in Section 4.1, we utilize GPT-4o to intentionally inject errors into correct solutions. The prompts we use are displayed in Table 10. 1. System Prompt You are highly experienced educator with strong understanding of both solving problems and mimicking realistic, common mistakes made by students or AI. 2. User Content Task: Analyse the following question, tested knowledge points and reference step-by-step solution. Introduce REASONABLE and REALISTIC errors into solution that mimic COMMON mistakes made by students or AI. Instructions: 1. Understand the following error categories and incorporate one or more of the following error types: a. Numerical Calculation Error. Errors in basic arithmetic operations such as addition, subtraction, division, or square roots. b. Symbolic Calculation Error. Errors in manipulating algebraic expressions, such as incorrect expansion, factoring, simplification, or solving equations with variables. c. Visual Interpretation Error. Errors in interpreting graphical data, such as misidentifying coordinates, shapes, spatial relationships, or data within figures. d. Reasoning Error. Errors in the logical thinking process that lead to incorrect conclusions, such as flawed arguments, invalid inferences, or gaps in the logical flow of the solution. e. Knowledge Error. Errors caused by insufficient understanding or incorrect application of necessary knowledge (e.g., concepts, formulas, theorems, methods), or using outdated or incorrect information. f. Question Understanding Error. Errors due to misunderstanding or misinterpreting the problems conditions or requirements, such as misreading questions or misapplying given conditions. 2. Introduce Errors: a. Insert multiple errors into the reference solution steps. b. Errors should be RESONABLE, REALISTIC and resemble COMMON mistakes, NOT arbitrary or overly obvious. c. AVOID REPEATING the same error reason across steps. ENSURE each step is evaluated INDIVIDUALLT. 3. Generate Erroneous Solutions: a. Provide 35 erroneous solutions to cover diverse possibilities. b. Ensure the erroneous solutions align with the question, remain logically consistent and misleading enough to challenge the reader. 4. Response Format: Present each erroneous solution step-by-step in the following format. a. Mark each step as 1 (Correct) or 0 (Incorrect). b. For incorrect steps, specify the error type and brief error description. c. Example: [ [ ], [Step Description 1, 0, Error category,a brief error descrition], [Step Description 2, 1, ,], ] d. Strictly adhere to the format! DO NOT add any explanations, extra content, or annotations outside the specified format! # Problem: {problem} # Tested Knowledge points: {knowledge points} # Reference solution: {step-by-step reference solution} Table 10. Prompt for injecting errors. F.2. Prompts for Solution Generation As described in Section 3.2 and Section 4.1, we use diverse MLLMs to generate solutions, capturing wide range of realistic reasoning behaviors and error patterns. The prompts we use for generating solutions are displayed in Table 11. 1. System Prompt You are highly skilled student proficient in solving scientific problems. 2. User Content Based on the given images, solve the following question. Here is some context information for this question, which might assist you in solving it: {context}* Problem: {problem} Think step by step logically, considering all relevant information before answering. Write out the solution process, and use the same LaTeX format as the question in the solution process. Please end your response with: The final answer is AN SW ER . Table 11. Prompt for generating solutions. . F.3. Prompts for Spliting Solutions into Steps To standardize the step granularity of each solution, we prompt Qwen2.5-72B-Instruct to split solutions into logically complete and progressive steps. The prompts we use for splitting are displayed in Table 12. User Content: Please split the following solution steps into individual steps and return them formatted as Python list. Each step should be separate string within the list. If the solution steps contain only single step or sentence, DO NOT split it further, return it as single element in the list. Only return the list itself, with no additional text or formatting. DO NOT modify the text in any way, simply split it. Example Format: [ First step description, Second step description, Third step description, ... ] Solution steps to split: solution Table 12. Prompt for generating solutions. F.4. Prompts for Process Evaluation As described in Section 6, we use the same evaluation for all models to ensure consistency. The prompts for process evaluation are displayed in Table 13. 1. System Prompt You are teacher skilled in evaluating the intermediate steps of students solution to given problem. 2. User Content You are given scientific problem, its correct final answer, and students solution to evaluate. Your task is to: first, solve the problem yourself, using the correct final answer as hint. Ensure your reasoning leads to the correct answer. Once you have clear understanding of how the problem could be solved, evaluate the correctness of each step in the students solution. Focus exclusively on the scientific, logical, or mathematical correctness of the solution. Ignore differences in formatting, expression style, specific wording, or presentation order, as long as the reasoning and results are valid. For each step, perform: 1. Binary scoring: assign score of 1 for correct steps and 0 for incorrect steps. 2. Error classification (only if the step is incorrect): a. Numerical Calculation Error. Errors in basic arithmetic operations such as addition, subtraction, division, or square roots. b. Symbolic Calculation Error. Errors in manipulating algebraic expressions, such as incorrect expansion, factoring, simplification, or solving equations with variables. c. Visual Interpretation Error. Errors in interpreting graphical data, such as misidentifying coordinates, shapes, spatial relationships, or data within figures. d. Reasoning Error. Errors in the logical thinking process that lead to incorrect conclusions, such as flawed arguments, invalid inferences, or gaps in the logical flow of the solution. e. Knowledge Error. Errors caused by insufficient understanding or incorrect application of necessary knowledge (e.g., concepts, formulas, theorems, methods), or using outdated or incorrect information. f. Question Understanding Error. Errors due to misunderstanding or misinterpreting the problems conditions or requirements, such as misreading questions or misapplying given conditions. g. No solution provided. The model refuses to answer, fails to follow instructions to make solution, or encounters anomalies in generation process such as repetitive responses or incomplete outputs. 3. Provide brief explanation for the identified error. # The given problem: {problem} # The Correct Final Answer: {final answer} # Students solution: step-by-step students solution Finally, your evaluation results should be Python list within <evaluation> and < /evaluation> tags, as follows: <evaluation> [ [The full text of correct step, 1, , ], [The full text of an incorrect step, 0, Error category, Brief error descrition], ] < /evaluation> Strictly adhere to the output format, with no additional text or formatting. Ensure the length of your output list matches the students solution. Table 13. Prompt for process evaluation."
        }
    ],
    "affiliations": [
        "HZAU",
        "NKU",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "USTC",
        "WHU"
    ]
}