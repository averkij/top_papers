{
    "paper_title": "MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation",
    "authors": [
        "Hongpeng Wang",
        "Zeyu Zhang",
        "Wenhao Li",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL."
        },
        {
            "title": "Start",
            "content": "MoRL: Reinforced Reasoning for Unified Motion Understanding and"
        },
        {
            "title": "Generation",
            "content": "Hongpeng Wang1 Zeyu Zhang2 Wenhao Li3 Hao Tang2 1The University of Sydney 2Peking University 3Nanyang Technological University Equal contribution. Project lead. Corresponding author: bjdxtanghao@gmail.com. 6 2 0 2 6 1 ] . [ 1 4 3 5 4 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and testtime planning. We propose MoRL, unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and textmotion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-ofMotion (CoM), test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over stateof-the-art baselines. Code: https://github. com/AIGeeksGroup/MoRL. Website: https: //aigeeksgroup.github.io/MoRL."
        },
        {
            "title": "Introduction",
            "content": "Human motion understanding and generation are fundamental problems in computer vision and robotics. They enable wide range of applications, from interactive character animation and robotics to game development and virtual reality. With the advent of large-scale motion capture datasets and expressive parametric human models such as SMPL (Loper et al., 2023) and SMPL-X (Pavlakos et al., 2019), recent years have witnessed rapid progress in text-to-motion generation (Zhang et al., 2024g,f, 2025, 2024h,e; Zhang et al.; Wang et al., 2026) and motion-language alignment (Zhang et al., 2023a; Guo et al., 2022a). Currently, the success of large language models (LLMs) has inspired multimodal extensions that integrate text, image, and 3D signals, pushing the frontier of motion language mod1 Figure 1: Visualization comparisons with MotionLLM. In the backflip example, MotionLLM fails to maintain coherent takeoff-rotation-landing trajectory, resulting in unstable body orientation, while MoRL completes physically plausible flip. In the Wack-style dance, MotionLLM shows inconsistent rotation direction and fragmented poses, whereas MoRL preserves continuous left-to-right rotation and stylistic coherence. eling toward more scalable and generalizable systems. Existing approaches have begun to explore this space. MotionGPT (Jiang et al., 2023) considers motion as foreign language to establish unified action language framework. MotionRL (Liu et al., 2024) introduces multi-reward optimization to better match human preferences. More recently, Motion-R1 (Ouyang et al., 2025) applies Chain-ofThought reasoning and reinforcement learning to motion generation. Despite these advances, two major challenges remain. First, current models treat user queries as whole, with limited reasoning capability. They struggle to parse prompts into fine-grained steps or to understand or generate detailed motions in step-by-step manner. Second, at test time, most models simply decode outputs in single pass. They lack explicit planning or reflection, and therefore cannot fully exploit the reasoning ability of large language models. To address the first challenge, we propose MoRL, multimodal motion unified model that unifies motion understanding and generation under reinforcement learning framework. MoRL is trained with hierarchical post-training pipeline. We then perform reinforcement learning with verifiable rewards (RLVR). Unlike prior works that rely primarily on generic similarity scores, our reward design is task-specific and dual-headed: for motion understanding, we introduce semantic alignment and novel reasoning coherence reward that enforces logically consistent reasoning traces; for motion generation, we combine textmotion consistency with physical plausibility reward that enforces biomechanical validity. This combination provides simple yet innovative way to align model outputs with both semantic fidelity and human perceptual realism. To address the second challenge, and improve the test-time performance, we introduce Chain-ofMotion (CoM), decoding strategy that explicitly incorporates step-by-step reasoning and reflection. CoM not only improves the robustness of reasoning-based motion understanding but also refines motion generation through iterative selection and correction. Moreover, the same principle guides the synthesis of our CoT datasets, ensuring consistency between training and inference. Specifically, we construct two large-scale synthetic Chain-of-Thinking (CoT) datasets, MoUnd-CoT140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and concise action descriptions. To further showcase the effectiveness, we conduct comprehensive experiments on HumanML3D (Guo et al., 2022a) and KIT-ML (Plappert et al., 2016). Results show that MoRL achieves significant gains over SOTA baselines. In summary, the main contributions are: We propose MoRL, unified multimodal motion model with task-specific rewards that improve motion understanding via semantic alignment and reasoning coherence, and motion generation via physical plausibility and text-motion consistency. We introduce Chain-of-Motion, test-time reasoning strategy, together with two largescale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to enhance motion understanding and generation through step-by-step reasoning and reflection. Extensive experiments on HumanML3D and KIT-ML demonstrate that MoRL consistently outperforms state-of-the-art methods."
        },
        {
            "title": "2 Related Works",
            "content": "Motion understanding and generation. Recent work on human motion understanding and generation has rapidly evolved from specialized sequence models to large language model (LLM)based frameworks that unify perception, reasoning, and textmotion alignment. Early multimodal approaches such as MotionLLM (Chen et al., 2024), ChatPose (Feng et al., 2024), and ChatHuman (Lin et al., 2024) explored conversational or interactive motion generation, yet their evaluations largely focused on qualitative results without systematic motion-to-text benchmarking. UniMotion (Li et al., 2025a) extended cross-modal modeling to broader set of human activities, but it similarly omitted explicit motion-to-text evaluation, leaving the bidirectional mapping underexplored. LLM-driven pipelines such as MotionLLaMA (Ling et al., 2024) demonstrated impressive compositional motion synthesis but relied on private datasets, limiting reproducibility and largescale comparison. Structured agent architectures like ACMo and CoMA (Sun et al., 2024) further highlighted the benefits of compositional reasoning and multi-modal interaction for controllable human-motion generation. Building on these foundations, new wave of motion-generation systems integrates transformer backbones with LLM reasoning. Representative examples include MotionGPT (Zhang et al., 2024d; Ribeiro-Gomes et al., 2024), T2M-GPT (Wang, 2023), and ReMoGPT (Yu et al., 2025), which leverage powerful language priors to improve both motion synthesis and naturallanguage controllability. Despite these advances, unified evaluation protocols that cover motion-totext understanding, text-conditioned generation, and open-dataset benchmarking remain limited, motivating the need for methods that jointly address generation fidelity and cross-modal reasoning. Large language model reasoning. Many studies aim to enhance the reasoning capacity of Large Language Models (LLMs) to perform complex, multistep problem-solving tasks by employing Chainof-Thought (CoT) prompting (Wei et al., 2022; Zhang et al., 2023b, 2024c; Mitra et al., 2024; Hao et al., 2024; Yao et al., 2023; Yuan et al., 2024a; Luan et al., 2024) and conducting supervised finetuning (SFT) with step-level supervision (Zhang et al., 2024a; Zhao et al., 2024; Yao et al., 2024; Thawakar et al., 2025). Recently, DeepSeek-R1 2 Figure 2: Motion CoT data engine. Build based on MotionHubV2 dataset (Ling et al., 2024), one branch (MoUndCoT) uses motion sequences and captions with Gemini to construct reasoning chains for understanding, while the other (MoGen-CoT) builds reasoning chains for generation. (Guo et al., 2025) successfully applied rule-based Reinforcement Learning (RL) (Shao et al., 2024) to induce the self-emergence of complex cognitive reasoning abilities in LLMs, demonstrating that even coarse, outcome-only rewards can effectively elicit strong reasoning behavior. Its success demonstrated that, with carefully designed reward structure and policy optimization strategy, models can learn to generate long CoT reasoning without the need for intermediate supervision. Building on this paradigm, recent efforts such as Open-ReasonerZero (Hu et al., 2025) and Kimi k1.5 (Team et al., 2025) have adopted similar rule-based reinforcement learning pipelines to enhance reasoning in the text and image domains, respectively. However, despite these promising developments, little prior work has investigated extending this approach to the video domain. Bridging this gap remains both significant challenge and promising direction for advancing the capabilities of reasoning models."
        },
        {
            "title": "3 Data Synthesis",
            "content": "Data engine. The key to empowering MoRL with strong reasoning ability lies in large-scale, highquality chain-of-thought (CoT) data. To address this gap, we design data engine, as shown in Figure 2, built on Gemini-2.5-pro (Comanici et al., 2025). It performs gap-based reasoning through questionanswer pairs and captures the reasoning process. This aligns motion sequences with natural language reasoning chains and concise action captions. The sequences and captions are derived from the MotionHubV2 dataset (Ling et al., 2024), which is constructed as subset of multiple publicly available datasets and encompasses diverse motion scenarios such as dance, performance interaction, and various activities from daily life. The resulting dataset consists of two complementary branches: Motion Understanding and Motion Generation. Together, they form unified CoT resource. MoUnd-CoT-140K. The motion understanding branch, denoted as MoUnd-CoT-140K, is designed to map motion sequences into textual reasoning and descriptive outputs. Each data sample contains three components: (i) motion sequence represented in the standard SMPL-X format, (ii) reasoning chain enclosed in <think> tags, and (iii) concise caption of the action enclosed in <answer> tags. To ensure compatibility with HumanML3Dstyle features, we convert SMPL-X joint sequences into humanml joint sequences and then extract motion features of dimension 263 per frame. This allows the dataset to be directly consumed by existing motion-language models. The resulting MoUndCoT-140K dataset provides high-quality CoT supervision for motion understanding tasks, especially in scenarios where the model must first interpret motion dynamics before generating compact description. MoGen-CoT-140K. The motion generation branch, denoted as MoGen-CoT-140K, complements MoUnd-CoT-140K by focusing on the inverse process: generating motion sequences from textual reasoning and descriptive inputs. Each sample contains (i) natural language caption of the intended action, (ii) an associated reasoning chain in <think> tags, and (iii) the corresponding motion sequence stored in SMPL-X format contained between <answer> tags. For 3 Figure 3: Overview of MoRL. Our framework unifies motion understanding and generation under reinforcement learning paradigm. Motion and text inputs are tokenized into shared representation space. hierarchical posttraining pipeline first applies SFT on large-scale synthetic CoT datasets to align motion sequences with reasoning traces and concise descriptions, then employs reinforcement learning with verifiable rewards (RLVR) to refine outputs, enhancing semantic alignment, reasoning coherence, physical plausibility, and textmotion consistency. At inference, the Chain-of-Motion (CoM) decoding strategy enables step-by-step reasoning and reflection, improving both motion understanding and perceptually realistic motion generation. consistency, all sequences are normalized into the HumanML3D feature space. MoGen-CoT-140K thus enables motion-language models to learn not only to understand motion but also to generate realistic, semantically aligned motion sequences guided by reasoning signals. Together, MoUnd-CoT-140K and MoGen-CoT140K form balanced CoT-based motion-language corpus, enabling instruction tuning for both understanding and generation in unified framework."
        },
        {
            "title": "4.1 Overview",
            "content": "As shown in Figure 3, we propose MoRL, multimodal motion foundation model that unifies human motion understanding and generation within single framework. MoRL is built on multimodal large language model (MLLM) initialized from Qwen3-4B-Instruct (Yang et al., 2025), and augmented with dedicated text and motion tokenizers for cross-modal alignment. The framework comprises three key components: (1) supervised fine-tuning (SFT) stage using synthetic chain-ofthought (CoT) dataset for cold-start initialization; (2) task-specific reinforcement learning (RL) policies for motion understanding and motion generation, each optimized with tailored reward functions; and (3) test-time reasoning strategy, COM, enhancing both understanding and generation through structured, step-by-step justification."
        },
        {
            "title": "4.2 Architecture",
            "content": "the base language model, while the motion tokenizer discretizes continuous 3D human motion into compact motion tokens via VQ-VAE style encoderdecoder. The multimodal fusion is achieved through shared transformer layers, enabling crossattention between textual and motion representations. This design follows the paradigm of motionlanguage alignment in DeepSeek but extends it to bidirectional tasks, including text-to-motion generation and motion-to-text understanding. Text tokenizer. We employ the native tokenizer of the LLM to map natural language into subword tokens. This preserves the rich linguistic knowledge of the base LLM while ensuring compatibility with motion-related vocabulary introduced during supervised fine-tuning. The text tokens serve as both queries (in understanding tasks) and conditioning signals (in generation tasks). Motion tokenizer. To bridge the gap between continuous human motion and the discrete token space of the LLM, we adopt VQ-VAE style motion tokenizer. Given an input motion sequence m1:T RT D, where is the number of frames and is the dimensionality of each frame, the encoder compresses the sequence into latent vectors z1:(T /l) R(T /l)d with downsampling factor and latent dimension d. Each latent zi is then quantized against learnable codebook = {cn}N n=1: ˆzi = arg min cnC zi cn2 2. (1) MoRL adopts unified multimodal LLM backbone equipped with two modality-specific tokenizers. The text tokenizer is inherited from The quantized sequence ˆz1:(T /l) is decoded back to reconstruct the original motion ˆm1:T = D(ˆz1:(T /l)). Training follows the composite VQ4 VAE loss: Lvq = Lreconstruct + Lcommit + Lembed, (2) where Lreconstruct is smoothed L1 loss with velocity regularization, Lcommit enforces codebook utilization, and Lembed stabilizes latent representations. This discrete motion representation not only reduces sequence length but also aligns seamlessly with the autoregressive generation paradigm of LLMs."
        },
        {
            "title": "4.3 Cold Start Stage",
            "content": "Recent work such as DeepSeek-R1 (Guo et al., 2025) demonstrated that reinforcement learning alone can sometimes induce CoT reasoning. Motivated by this, we first explored training our motionlanguage model directly with RL signals. In practice, however, this strategy was highly unstable: the model rarely produced well-formed reasoning traces and even generated answers that deviated from the intended semantics. To stabilize training, we introduce cold-start phase based on supervised fine-tuning. Specifically, we use our synthetic datasets MoUnd-CoT-140K and MoGenCoT-140K, which couple motion sequences with reasoning steps (<think>) and concise descriptions (<answer>). Supervised finetuning on these data forces the model to follow the required output format, stabilizing its outputs and ensuring semantic consistency between inference and final answers. This initialization greatly reduces collapse during RL and establishes reliable starting point for policy optimization."
        },
        {
            "title": "4.4 Reinforcement Learning",
            "content": "After cold-start training, we further align the model outputs with task objectives through reinforcement learning. We adopt group-based policy optimization strategy similar to GRPO, where multiple candidate outputs are sampled per prompt, scored with reward functions, normalized within the group, and used to compute policy gradients with KL regularization term to frozen reference model. Motion understanding. For motion understanding, the model must output reasoning trace ˆr and caption ˆa given motion sequence m. We define two rewards: Semantic Alignment Reward. We measure the semantic similarity between ˆa and the reference caption using pretrained text encoder Etext: Rsem = cos(Etext(ˆa), Etext(a)) . (3) Reasoning Coherence Reward. We encourage the reasoning trace to logically support the answer using an NLI model fNLI: Rcoh = fNLI(ˆr, ˆa), (4) where we implement fNLI() using frozen DeBERTa-v3-large MNLI model and take its entailment probability as the coherence score. fNLI() outputs an entailment confidence score. Motion generation. For motion generation, the model produces motion sequence ˆm from text prompt t. We use two rewards: Physical Plausibility Reward. We penalize implausible motion dynamics: Rphys = λ1 Ljoint( ˆm) λ2 Lvel( ˆm), (5) where Ljoint() measures joint-angle violations and Lvel() penalizes abrupt velocity changes, and λ1 = 0.8 for joint-limit violation and λ2 = 0.2. TextMotion Consistency Reward. We enforce semantic alignment between generated motion and the input text, using cross-modal encoders Etext, Emotion: Ralign = cos(Etext(t), Emotion( ˆm)) . (6)"
        },
        {
            "title": "4.5 Chain-of-Motion",
            "content": "Most motion-language models decode outputs in single pass, often resulting in shallow semantic reasoning for understanding and temporal inconsistency for generation. We propose Chain-of-Motion (CoM), test-time reasoning strategy that introduces explicit step-by-step planning and reflection. Given an input prompt or motion sequence, the model first generates an intermediate naturallanguage reasoning trace, analogous to Chain-ofThought. For motion understanding, this trace explains causal and temporal structure to support the final caption; for motion generation, it outlines sequence of action primitives before decoding motion tokens, guiding fine-grained dynamics."
        },
        {
            "title": "CoM further",
            "content": "samples multiple candidate reasoning-motion pairs and evaluates them using task-specific rewards (reasoning-answer coherence for understanding, and semantic alignment with physical plausibility for generation). Low-quality candidates are discarded, while high-quality ones are refined through iterative reflection, reducing semantic drift and physically implausible motions. Finally, CoM is consistent with training: our MoUnd-CoT-140K and MoGen-CoT-140K 5 datasets include explicit reasoning traces, enabling CoM to naturally extend the SFT and RL stages at inference time."
        },
        {
            "title": "5.1 Experiments Settings",
            "content": "Datasets. We evaluate MoRL on two widely used motionlanguage benchmarks: HumanML3D (Guo et al., 2022a) and KIT-ML (Plappert et al., 2016). HumanML3D contains 14.6K motion clips with 44.9K text annotations, covering diverse everyday actions, while KIT-ML includes 3.9K motions paired with 6.3K linguistically varied descriptions. Following prior work, motions are represented using SMPL-based joint features (263 for HumanML3D and 251 for KIT-ML), with temporal normalization and leftright mirroring applied. Both datasets are split into training, validation, and test sets with ratio of 0.8/0.15/0.05. Metrics. For motion understanding, we adopt standard linguistic similarity metrics. BLEU@1 and BLEU@4 measure unigram and 4-gram precision, capturing lexical overlap. ROUGE-L evaluates longest common subsequence, reflecting recalloriented alignment. CIDEr computes TF-IDF weighted n-gram consensus across references, rewarding semantic coverage. BERTScore uses contextual embeddings to assess semantic similarity beyond surface overlap. For motion generation, we follow established benchmarks. RPrecision (Top1/Top2/Top3) measures cross-modal retrieval accuracy between motion and text. FID evaluates the distributional gap between generated and real motions, where lower is better. MM Dist measures motiontext embedding distance in shared space. Diversity quantifies variation across generated motions from different prompts. MModality evaluates the ability to produce distinct yet semantically consistent motions under the same text prompt. Implementation details. Our backbone is initialized from Qwen3-4B-Instruct (Yang et al., 2025), compact yet capable language model. Motion sequences are first converted into frame-level features using the HumanML3D feature extractor, and then discretized by VQ-VAE motion tokenizer. In practice, our motion tokenizer uses codebook of = 512 entries and latent dimension of 128. The text is encoded with the Qwen tokenizer. To adapt the model efficiently, we insert LoRA adapters into the attention and feed-forward layers with rank = 16 and dropout 0.1. Training proceeds in two stages. In the SFT stage, we fine-tune on our synthetic CoT datasets (MoUnd-CoT-140K and MoGen-CoT-140K) with AdamW optimizer, learning rate 1 105, batch size 64, and weight decay 0.01 for 5 epochs. In the RL stage, we adopt group-based reinforcement learning with group size 8. Candidate outputs are scored with our reward functions, normalized within each group, and optimized using KL-regularized objective toward frozen SFT reference. The RL learning rate is 5 106, and training is run for 3 epochs. All models are trained in PyTorch on four NVIDIA A100 GPUs. During inference, we apply the Chain-of-Motion decoding strategy with = 8 candidates and = 2 refinement iterations, which adds only modest runtime overhead while consistently improving output quality."
        },
        {
            "title": "5.2 Main Results",
            "content": "Motion understanding. Table 1 reports results on HumanML3D and KIT-ML understanding benchmarks. MoRL achieves consistent improvements across all linguistic metrics, outperforming both traditional sequence models (e.g., Seq2Seq(Att) (Plappert et al., 2018)) and recent LLM-based methods such as MotionGPT (Jiang et al., 2023) and Motion Agent (Wu et al., 2024). On HumanML3D, MoRL improves BLEU@1 and BLEU@4 by clear margin over Motion Agent, while yielding higher ROUGE-L and BERTScore, indicating better semantic fidelity and more fluent language generation. Notably, MoRL reaches CIDEr score of 35.8, substantially higher than Motion Agent (33.74), showing stronger consensus with human-annotated references. On KIT-ML, MoRL also achieves the best balance between precision-oriented (BLEU) and semantic-oriented metrics (BERTScore, ROUGEL), demonstrating that our dual reward design generalizes well across datasets. These gains primarily come from the semantic alignment and reasoningcoherence rewards, which ensure that generated descriptions are both logically consistent and wellgrounded in motion semantics. Notably, under comparisons among unified model approaches, our method achieves comprehensive superiority across most metrics, and even when compared to separate models, it attains comparable or superior performance on certain methods and metrics. Motion generation. We further evaluate MoRL 6 Method Motion Generation Motion Understanding R@1 R@2 R@3 FID MM Dist Div MM BLEU@1 BLEU@4 ROUGE-L CIDEr BERTScore GT / Real Motions SeqGAN (Goutsu and Inamura, 2021) RAEs (Yamada et al., 2018) Seq2Seq(Att) (Plappert et al., 2018) T2M (Guo et al., 2022a) T2M-GPT (Zhang and Zhang, 2023) FineMoGen (Zhang et al., 2023a) MoGenTS (Yuan et al., 2024b) Language2Pose (Ahuja and Morency, 2019) ReMoDiffuse (Zhang and Guo, 2023) ReMoGPT (Yu et al., 2024) RMD (Liao et al., 2024) MoRAG-Diffuse (Kalakonda et al., 2024) Lyu et al. (Lyu et al., 2025) MDM (Tevet et al., 2023) MotionDiffuse (Zhang et al., 2024b) Motion2Language (Radouane et al., 2024) M2T-Interpretable (Radouane et al., 2023) Text2Gesture (Bhattacharya et al., 2021) MoMask (Guo et al., 2024) ReMoMask (Li et al., 2025c) TM2T(Guo et al., 2022b) TM2T (Guo et al., 2022b) AvatarGPT (Zhou et al., 2024) MotionGPT (Jiang et al., 2023) MotionGPT-2 (Wang et al., 2024b) MotionChain (Jiang et al., 2024) Motion Agent (Wu et al., 2024) LaMP (Li et al., 2025b) MoRL (Ours) Real Motions SeqGAN (Goutsu and Inamura, 2021) RAEs (Yamada et al., 2018) Seq2Seq(Att) (Plappert et al., 2018) T2M (Guo et al., 2022a) T2M-GPT (Zhang and Zhang, 2023) MoGenTS (Yuan et al., 2024b) ReMoDiffuse (Zhang and Guo, 2023) Language2Pose (Ahuja and Morency, 2019) Lyu et al. (Lyu et al., 2025) MDM (Tevet et al., 2023) MotionDiffuse (Zhang et al., 2024b) Motion2Language (Radouane et al., 2024) M2T-Interpretable (Radouane et al., 2023) Text2Gesture (Bhattacharya et al., 2021) MoMask (Guo et al., 2024) ReMoMask (Li et al., 2025c) TM2T (Guo et al., 2022b) TM2T (Guo et al., 2022b) MotionGPT (Jiang et al., 2023) MotionGPT-2 (Wang et al., 2024b) LaMP (Li et al., 2025b) 0.511 - - - 0.457 0.491 0.504 0.529 0.246 0.510 0.501 0.524 0.511 - - 0.491 - - 0.165 0.521 0.531 0.424 0.424 0.510 0.492 0.496 0.504 0.515 0. 0.527 0.424 - - - 0.370 0.416 0.445 0.427 0.221 - - 0.417 - - 0.156 0.433 0.453 0.280 0.280 0.366 0.427 0.479 0.703 - - - 0.639 0.680 0.690 0.719 0.387 0.698 0.688 0.715 0.699 - - 0.681 - - 0.267 0.713 0.722 0.618 0.618 0.702 0.681 0.691 - - 0.751 0.711 0.649 - - - 0.569 0.627 0.671 0.641 0.373 - - 0.621 - - 0.255 0.656 0.682 0.463 0.463 0.558 0.627 0.691 HumanML3D (Guo et al., 2022a) 0.797 - - - 0.740 0.775 0.784 0.812 0.486 0.795 0.792 0.811 0.792 - 0.611 0.782 - - 0.345 0.807 0.813 0.729 0.729 0.796 0.733 0.782 0.790 0.801 0.843 0.002 - - - 1.067 0.116 0.151 0.033 11.02 0.103 0.205 0.111 0.270 - 0.544 0.630 - - 7.664 0.045 0.099 1.501 1.501 0.168 0.232 0.191 0.248 0.230 0.032 0.821 0.203 2.974 - - - 3.340 3.118 2.998 2.867 5.296 2.974 2.929 2.879 2.950 - 5.566 3.113 - - 6.030 2.958 2.865 3.467 3.467 - 3.096 3.080 3.033 2.967 2.759 2. 9.503 - - - 9.188 9.761 9.263 9.570 7.676 9.018 9.763 9.527 9.536 - 9.559 9.410 - - 6.409 - 9.535 8.589 8.589 9.624 9.528 9.860 9.470 9.908 9.571 - - - - 2.090 1.856 2.696 - - 1.795 2.816 2.604 2.773 - 2.799 1.553 - - - 1.241 2.823 2.424 2.424 - 2.008 2.137 - - - 9.701 2.702 KIT-ML (Plappert et al., 2016) 0.779 - - - 0.693 0.745 0.797 0.765 0.483 - 0.396 0.739 - - 0.338 0.781 0.805 0.587 0.587 0.680 0.764 0. 0.031 - - - 2.770 0.514 0.143 0.155 6.545 - 0.497 1.954 - - 12.12 0.204 0.138 3.599 3.599 0.510 0.614 0.141 2.788 - - - 3.401 3.007 2.711 2.814 5.147 - 9.191 2.958 - - 6.964 2.779 2.682 4.591 4.591 3.527 3.164 2.704 11.08 - - - 10.91 10.92 10.918 10.80 9.073 - 10.85 11.10 - - 9.334 - 10.83 9.473 9.473 10.350 11.256 10.929 - - - - 1.482 1.570 - 1.239 - - 1.907 0.730 - - - 1.131 2.017 3.292 3.292 2.328 2.357 - - 47.80 33.30 51.80 - - - - - - - - - 49.70 - - 67.00 69.90 - - - 61.70 48.90 49.28 48.20 48.70 48.10 54.53 47.80 56. - 3.12 30.60 34.30 - - - - - 43.40 - - 56.80 58.40 - - - 46.70 35.10 - - - - 13.50 10.20 17.90 - - - - - - - - - 13.62 - - 23.40 25.00 - - - 22.30 8.270 12.70 12.47 13.80 12.56 17.65 13.04 20.54 - 5.20 0.10 9.30 - - - - - 8.90 - - 25.40 24.40 - - - 18.40 6.200 - - - - 39.20 37.50 46.40 - - - - - - - - - 39.20 - - 53.80 55.30 - - - 49.20 38.10 40.44 37.40 37.60 33.90 48.70 37.10 51. - 32.40 25.70 36.30 - - - - - 35.20 - - 58.80 58.30 - - - 44.20 28.70 - - - - 50.20 22.10 58.40 - - - - - - - - - 53.10 - - 53.70 61.60 - - - 72.50 15.80 32.65 29.20 29.80 33.70 33.74 28.90 35.80 - 29.50 8.00 37.30 - - - - - 65.30 - - 125.7 112.1 - - - 79.50 28.90 - - - - 23.40 10.70 29.10 - - - - - - - - - 33.10 - - 37.20 40.30 - - - 37.80 32.20 53.58 32.40 32.60 36.90 42.63 32.70 46. - 2.20 0.40 5.30 - - - - - 31.20 - - 42.10 41.20 - - - 23.00 30.40 - - - 0.439 0.661 MoRL (Ours) Table 1: Comparison of motion generation and motion understanding on HumanML3D and KIT-ML. Highlights indicate the unified model, bold represent the best results within the unified model. Results marked with are reproduced by MotionGPT (Jiang et al., 2023) and Lyu et al. (Lyu et al., 2025), and are computed with unprocessed ground truth texts for linguistic metrics. 10.882 0. 0.204 2.777 1.991 52.11 19.31 49. 34.04 33.66 on text-to-motion generation  (Table 1)  . On HumanML3D, MoRL consistently improves RPrecision across Top-1/2/3 over strong baselines such as ReMoGPT (Yu et al., 2025) and MoRAGDiffuse (Kalakonda et al., 2024), highlighting its superior textmotion alignment. Although FID is slightly higher than the best-performing diffusionbased models, MoRL achieves the lowest multimodal distance, suggesting closer alignment to reference motions in feature space. Moreover, MoRL delivers competitive diversity and strong multimodality, showing that our physical plausibility and textmotion consistency rewards encourage both realism and variety in generated motions. On KIT-ML, MoRL achieves comparable performance to state-of-the-art diffusion models, with balanced R-Precision and FID values. While not always the absolute best in each metric, MoRL provides robust overall performance across fidelity, diversity, and alignment. Importantly, the introduction of Chain-of-Motion at test time further stabilizes inference, reducing error propagation and producing smoother, more natural motion trajectories. Similarly, our method outperforms most unified models and shows notable advantages even compared to some separate models."
        },
        {
            "title": "5.3 Qualitative Analysis and Visualization",
            "content": "Figure 1 presents gneneration-qualitative comparisons between MoRL and MotionLLM on two representative prompts. For the simple caption describing backflip (left), MotionLLM produces an incorrect global displacement: the preparatory bending phase drifts forward relative to the stand-"
        },
        {
            "title": "Generation",
            "content": "BERT CIDEr R-L R@1 FID"
        },
        {
            "title": "SFT only",
            "content": "42.65 33.88 48.78 0.420 0.212 w/o Rsem w/o Rcoh w/o Rphys w/o Ralign w/o CoM 44.10 44.32 46.18 45.00 45.48 34.05 35.12 35.50 34.62 34. 50.01 0.488 0.209 49.10 0.512 0.206 51.19 0.518 0.285 50.48 0.492 0.225 50.78 0.505 0.220 Full MoRL 46.80 35.80 51.83 0.527 0.203 Table 2: Ablation study of MoRL on HumanML3D. ing position, and the motion ends abruptly after the flip, resulting in an unnatural transition. In contrast, MoRL generates complete and temporally coherent backflip, including correct takeoff location, smooth mid-air rotation, stable landing, and natural recovery sequence. The improved fidelity demonstrates MoRLs stronger physical reasoning and its ability to handle high-momentum, highly dynamic motions. For the more complex caption describing Wack-style dance (right), MotionLLM fails to maintain consistent left-to-right rotational pattern and produces fragmented upper-body movements. MoRL outputs smoother, directionally consistent, and stylistically richer motions, accurately reflecting both the intended dance style and the global rotation described in the text. The incorporation of CoM-based reasoning further enhances motion naturalness and semantic grounding. These results indicate that our semantic alignment reward and CoM inference together improve long-horizon motion planning and textmotion correspondence."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "Ablation on Model Components. We conduct ablation studies on HumanML3D to evaluate the contribution of each component in MoRL  (Table 2)  . Starting from the SFT-only baseline, which yields the weakest performance for both understanding and generation, progressively adding RLVR rewards and CoM consistently improves results. Removing the semantic alignment reward (Rsem) notably degrades BERTScore and CIDEr, highlighting its role in grounding textual semantics. Excluding the reasoning coherence reward (Rcoh) mainly affects ROUGE-L and CIDEr, confirming its importance for logical and temporal consistency. Dropping the physical plausibility reward (Rphys) preserves language metrics but significantly worsens FID, indicating its necessity for realistic motion synthesis. Removing the textmotion consistency reward (Ralign) causes substantial drop in 8 Generation Reward R@1 R@2 R@3 FID MM Dist MM Motion-R1 Reward (Ouyang et al., 2025) MotionRL Reward (Liu et al., 2024) Process-aware Reward (Wang et al., 2024a) 0.472 0.491 0.486 0.651 0.676 0.682 0.742 0.768 0.781 0.185 0.172 0.181 3.021 2.914 2. 2.31 2.28 2.45 0.814 0.703 MoRL Reward (Ours) 0.489 Table 3: Comparison of different reward designs on the CMS of HumanML3D. All methods share the same backbone and training setup, differing only in the reward used during motion generation. 0.179 2. 2.42 R-Precision, revealing its role in cross-modal alignment. Finally, excluding CoM leads to moderate performance degradation across metrics, demonstrating its contribution to test-time reasoning. Comparison of Rewards. To compare the impact of the different reward designs, we first construct Complex Motion Subset (CMS) from the HumanML3D dataset to evaluate motion generation under long temporal horizons and compositional semantic constraints. Specifically, we select samples from the original test set that satisfy the following criteria: (1) the textual description contains at least three action verbs (e.g., walk, turn, sit, raise); (2) the description includes explicit temporal connectors (e.g., then, after, finally, while), indicating clear ordering dependencies between actions; and (3) the text length is no fewer than 20 tokens. Samples in this subset typically correspond to multistage motions with long temporal spans, posing higher demands on global semantic modeling and long-range consistency. We keep the model architecture, data, and optimization fixed and vary only the generation reward for controlled comparison. Table 3 reports results on CMS. Outcome-based rewards (MotionR1-style) perform reasonably at R@1 but degrade at higher RPrecision, indicating omission of later-stage actions. MotionRL improves realism (lower FID) but remains insensitive to stage-level semantic gaps. The process-aware reward yields further gains by encouraging temporal coherence; yet, it still lacks fine-grained linguistic alignment. In contrast, MoRL consistently improves R@2 and R@3 while achieving the lowest MM Distance, effectively reducing semantic drift in long-horizon sequences without sacrificing diversity."
        },
        {
            "title": "6 Conclusion",
            "content": "We present MoRL, unified multimodal motion model that integrates motion understanding and generation through reinforcement learning. With task-specific rewards and COM decoding strategy, MoRL improves both logical consistency and perceptual realism. We also construct two large-scale synthetic CoT datasets for motionlanguage alignment. Experiments on HumanML3D and KIT-ML show that MoRL outperforms SOTA methods. Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. 2022a. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 51525161."
        },
        {
            "title": "Limitations",
            "content": "Our approach relies on rule-based reward design, which may require adaptation for new motion domains or styles. The Chain-of-Motion reasoning process introduces additional inferencetime computation, limiting real-time applicability. Moreover, our method operates on discretized motion representations and does not explicitly model fine-grained contact dynamics or complex humanobject interactions."
        },
        {
            "title": "References",
            "content": "Chaitanya Ahuja and Louis-Philippe Morency. 2019. Language2pose: Natural language grounded pose In Proceedings of the IEEE/CVF Inforecasting. ternational Conference on 3D Vision (3DV), pages 719728. Uttaran Bhattacharya, Nathaniel Rewkowski, Abhishek Banerjee, Prashanth Guhan, Aniket Bera, and Dinesh Manocha. 2021. Text2gestures: transformer-based network for generating emotive body gestures. In IEEE Virtual Reality and 3D User Interfaces (VR), pages 110. Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, and Lei Zhang. 2024. Motionllm: Understanding human behaviors from human motions and videos. arXiv preprint arXiv:2405.20340. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, and Michael Black. 2024. Chatpose: Chatting about 3d human pose. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 20932103. Yusuke Goutsu and Tetsunari Inamura. 2021. Linguistic descriptions of human motion with generative adversarial seq2seq learning. In 2021 IEEE International conference on robotics and automation (ICRA), pages 42814287. IEEE. Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. 2022b. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In European Conference on Computer Vision, pages 580597. Springer. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. 2025. Openreasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290. Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. 2023. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079. Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li, Gang Yu, and Jiayuan Fan. 2024. Motionchain: Conversational motion controllers via multimodal prompts. In European Conference on Computer Vision, pages 5474. Springer. Sai Shashank Kalakonda, H. Maheshwari, and R. K. Sarvadevabhatla. 2024. Morag-diffuse: Motion retrievalaugmented diffusion. In ACM Multimedia. Chuqiao Li, Julian Chibane, Yannan He, Naama Pearl, Andreas Geiger, and Gerard Pons-Moll. 2025a. Unimotion: Unifying 3d human motion synthesis and understanding. In 2025 International Conference on 3D Vision (3DV), pages 240249. IEEE. Zhe Li, Weihao Yuan, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, Laurence Tianruo Yang, and 1 others. 2025b. Lamp: Language-motion pretraining for motion generation, retrieval, and captioning. In The Thirteenth International Conference on Learning Representations. Zhengdao Li, Siheng Wang, Zeyu Zhang, and Hao Tang. 2025c. Remomask: Retrieval-augmented arXiv preprint masked motion generation. arXiv:2508.02605. Chenyang Guo and 1 others. 2024. Momask: Hierarchical residual quantization for text-to-motion generation. arXiv preprint arXiv:2401.08564. J. Liao and 1 others. 2024. Rmd: Residual motion diffusion for text-to-motion generation. arXiv preprint arXiv:2404.09876. 9 Jing Lin, Yao Feng, Weiyang Liu, and Michael Black. 2024. Chathuman: Language-driven 3d human understanding with retrieval-augmented tool reasoning. arXiv preprint arXiv:2405.04533, 2. Zeyu Ling, Bo Han, Shiyang Li, Hongdeng Shen, Jikang Cheng, and Changqing Zou. 2024. Motionllama: unified framework for motion synthesis and comprehension. arXiv e-prints, pages arXiv2411. Xiaoyang Liu, Yunyao Mao, Wengang Zhou, and Houqiang Li. 2024. Motionrl: Align text-tomotion generation to human preferences with multiarXiv preprint reward reinforcement arXiv:2410.06513. learning. Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. 2023. Smpl: In Seminal skinned multi-person linear model. Graphics Papers: Pushing the Boundaries, Volume 2, pages 851866. Bozhi Luan, Hao Feng, Hong Chen, Yonghui Wang, Wengang Zhou, and Houqiang Li. 2024. Textcot: Zoom in for enhanced multimodal text-rich image understanding. arXiv preprint arXiv:2404.09797. Guangtao Lyu, Chenghao Xu, Jiexi Yan, Muli Yang, and Cheng Deng. 2025. Towards unified human motionlanguage understanding via sparse interpretable characterization. In The Thirteenth International Conference on Learning Representations. Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. 2024. Compositional chain-of-thought prompting for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1442014431. Runqi Ouyang, Haoyun Li, Zhenyuan Zhang, Xiaofeng Wang, Zheng Zhu, Guan Huang, and Xingang Wang. 2025. Motion-r1: Chain-of-thought reasoning and reinforcement learning for human motion generation. arXiv preprint arXiv:2506.10353. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. 2019. Expressive body capture: 3d hands, face, and body from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1097510985. Matthias Plappert, Christian Mandery, and Tamim Asfour. 2016. The kit motion-language dataset. Big data, 4(4):236252. Matthias Plappert, Christian Mandery, and Tamim Asfour. 2018. Learning bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks. Robotics and Autonomous Systems, 109:1326. Karim Radouane, Julien Lagarde, Sylvie Ranwez, and Andon Tchechmedjiev. 2023. Guided attention for arXiv preprint interpretable motion captioning. arXiv:2310.07324. Karim Radouane, Andon Tchechmedjiev, Julien Lagarde, and Sylvie Ranwez. 2024. Motion2language, unsupervised learning of synchronized semantic motion segmentation. Neural Computing and Applications, 36(8):44014420. Jose Ribeiro-Gomes, Tianhui Cai, Zoltán Milacski, Chen Wu, Aayush Prakash, Shingo Takagi, Amaury Aubel, Daeil Kim, Alexandre Bernardino, and Fernando De La Torre. 2024. Motiongpt: Human motion synthesis with improved diversity and realism via gpt3 prompting. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 50705080. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasonarXiv preprint ing in open language models. arXiv:2402.03300. Shanlin Sun, Gabriel De Araujo, Jiaqi Xu, Shenghan Zhou, Hanwen Zhang, Ziheng Huang, Chenyu You, and Xiaohui Xie. 2024. Coma: Compositional human motion generation with multi-modal agents. arXiv preprint arXiv:2412.07320. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Guy Tevet and 1 others. 2023. Human motion diffusion model. In ICLR. Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, and 1 others. 2025. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186. Congyi Wang. 2023. T2m-hifigpt: generating high quality human motion from textual descriptions with arXiv preprint residual discrete representations. arXiv:2312.10628. Haoru Wang, Wentao Zhu, Luyi Miao, Yishu Xu, Feng Gao, Qi Tian, and Yizhou Wang. 2024a. Aligning human motion generation with human perceptions. arXiv preprint arXiv:2407.02272. Yiling Wang, Zeyu Zhang, Yiran Wang, and Hao Tang. 2026. Safemo: Linguistically grounded unlearning for trustworthy text-to-motion generation. arXiv preprint arXiv:2601.00590. Yuan Wang, Di Huang, Yaqi Zhang, Wanli Ouyang, Jile Jiao, Xuetao Feng, Yan Zhou, Pengfei Wan, Shixiang Tang, and Dan Xu. 2024b. Motiongpt-2: general-purpose motion-language model for motion generation and understanding. arXiv preprint arXiv:2410.21747. 10 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Qi Wu, Yubo Zhao, Yifan Wang, Xinhang Liu, Yu-Wing Tai, and Chi-Keung Tang. 2024. Motion-agent: conversational framework for human motion generation with llms. arXiv preprint arXiv:2405.17013. Tatsuro Yamada, Hiroyuki Matsunaga, and Tetsuya Ogata. 2018. Paired recurrent autoencoders for bidirectional translation between robot actions and linguistic descriptions. IEEE Robotics and Automation Letters, 3(4):34413448. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, and 1 others. 2024. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Qing Yu, Mikihiro Tanaka, and Kent Fujiwara. 2025. Remogpt: Part-level retrieval-augmented motionlanguage models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 96359643. T. Yu, K. Tanaka, and T. Fujiwara. 2024. Remogpt: Retrieval-augmented motion-language model. arXiv preprint arXiv:2402.04567. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, and 1 others. 2024a. Advancing llm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078. Y. Yuan and 1 others. 2024b. Mogents: Efficient textto-motion synthesis via transformer sampling. arXiv preprint arXiv:2403.06789. Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, and 1 others. 2024a. Llama-berry: Pairwise optimization for o1-like arXiv olympiad-level mathematical reasoning. preprint arXiv:2410.02884. Jie Zhang and Yu et al. Zhang. 2023. T2m-gpt: Generating human motion from textual descriptions with discrete representations. In CVPR. Mingyuan Zhang, Zhipeng Cai, and Liang et al. Pan. 2024b. Motiondiffuse: Text-driven human motion generation with diffusion model. In TPAMI. Mingyuan Zhang and Xiaoyu et al. Guo. 2023. Remodiffuse: Retrieval-augmented motion diffusion model. In ICCV. Mingyuan Zhang, Huirong Li, Zhongang Cai, Jiawei Ren, Lei Yang, and Ziwei Liu. 2023a. Finemogen: Fine-grained spatio-temporal motion generation and editing. Advances in Neural Information Processing Systems, 36:1398113992. Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. 2024c. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198. Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. 2024d. Motiongpt: Finetuned llms are general-purpose motion generators. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 73687376. Zeyu Zhang, Hang Gao, Akide Liu, Qi Chen, Feng Chen, Yiran Wang, Danning Li, Rui Zhao, Zhenming Li, Zhongwen Zhou, and 1 others. 2024e. Kmm: Key frame mask mamba for extended motion generation. arXiv preprint arXiv:2411.06481. Zeyu Zhang, Akide Liu, Qi Chen, Feng Chen, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. 2024f. Infinimotion: Mamba boosts memory in transformer for arbitrary long motion generation. arXiv preprint arXiv:2407.10061. Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. 2024g. Motion mamba: Efficient and long sequence motion generation. In European Conference on Computer Vision, pages 265282. Springer. Zeyu Zhang, Yiran Wang, Danning Li, Dong Gong, Ian Reid, and Richard Hartley. Flashmo: Geometric interpolants and frequency-aware sparsity for scalable efficient motion generation. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Zeyu Zhang, Yiran Wang, Wei Mao, Danning Li, Rui Zhao, Biao Wu, Zirui Song, Bohan Zhuang, Ian Reid, and Richard Hartley. 2025. Motion anything: Any to motion generation. arXiv preprint arXiv:2503.06955. Zeyu Zhang, Yiran Wang, Biao Wu, Shuo Chen, Zhiyuan Zhang, Shiya Huang, Wenbo Zhang, Meng Fang, Ling Chen, and Yang Zhao. 2024h. Motion avatar: Generate human and animal avatars with arbitrary motion. arXiv preprint arXiv:2405.11286. 11 Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023b. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2024. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405. Zixiang Zhou, Yu Wan, and Baoyuan Wang. 2024. Avatargpt: All-in-one framework for motion understanding planning generation and beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13571366."
        },
        {
            "title": "A LLM Use Declaration",
            "content": "Large Language Models (ChatGPT) were used exclusively to improve the clarity and fluency of English writing. They were not involved in research ideation, experimental design, data analysis, or interpretation. The authors take full responsibility for all content."
        },
        {
            "title": "B More Implementation Details",
            "content": "B.1 NLI Model for the Reasoning-Coherence"
        },
        {
            "title": "Reward",
            "content": "This section clarifies the natural language inference (NLI) model used to compute the reasoningcoherence reward. Reasoning traces generated by the model are purely textual. Therefore, we evaluate their logical consistency with the predicted answer based on an NLI task. In particular, we adopt DeBERTa-v3-large-MNLI as fNLI. The model is kept frozen during reinforcement learning. This avoids reward drift and provides stable, stationary reward signal. The input to fNLI is pair of text sequences (reasoning trace and answer). No motion features are used. Motiontext alignment is handled by separate reward. We take the softmax probability of the entailment class as the coherence score and apply group-wise normalization. Shown as Table 4, larger NLI models produce more stable entailment probabilities. Smaller models show higher variance when processing long reasoning traces, which reduces RL stability. DeBERTa-v3-large achieves the most consistent reward signal and yields the strongest gains in both understanding and generation tasks. We also tested larger NLI model (DeBERTa-v3XL). Despite its size, it did not improve reward stability or downstream performance. Larger model tended to be overconfident and reacted too strongly to the noise in model-generated CoT traces, which have flexible and imperfect linguistic structure. This sensitivity increased reward variance on long reasoning chains and introduced instability during RL updates, ultimately reducing motion quality. The larger model also incurred significantly higher computational cost. In contrast, DeBERTa-v3-large provides the best trade-off between stability, robustness, and efficiency. We therefore use it as the default fNLI and keep it frozen throughout training to ensure reproducibility."
        },
        {
            "title": "Params",
            "content": "Var. BLEU FID DeBERTa-v3-XL DeBERTa-v3-L RoBERTa-L DeBERTa-B BERT-B 1.2B 435M 355M 110M 110M LowMed. Lowest Low Med. High +3.3 +3.4 +2.8 +1.9 +0. -2.0 -2.1 -1.5 -0.8 -0.2 Table 4: Comparison of NLI models used as fNLI. Lower reward variance indicates more stable RL updates. BLEU and FID reflect performance gains relative to the SFT baseline. B.2 Reward Normalization Our reinforcement learning stage combines four heterogeneous reward components: semantic alignment, reasoning coherence, physical plausibility, and textmotion consistency. These rewards naturally exhibit different dynamic ranges and variances, which may cause unstable optimization when used directly. To address this, we apply group-wise normalization to each reward component within GRPO candidate group. Group-wise Normalization. Given candidate group {r1, r2, . . . , rK}, we normalize each reward as ri = µr = , ri µr σr + ϵ (cid:88)"
        },
        {
            "title": "1\nK",
            "content": "rj, σ2 ="
        },
        {
            "title": "1\nK",
            "content": "j=1 (cid:88) j=1 (rj µr)2. (7) This ensures that rewards are centered and variancecontrolled inside each GRPO update, which stabilizes the advantage computation and reduces gradient variance. Component-wise Scaling. After normalization, we apply scalar weights λ1 and λ2 to the two physical plausibility rewards: Rphys = λ1 Ljoint( ˆm) λ2 Lvel( ˆm), λ1 = 0.8, λ2 = 0.2. (8) These values are selected to balance the magnitude of joint-limit penalties and velocity-smoothness penalties."
        },
        {
            "title": "CoM",
            "content": "We report the end-to-end inference cost of CoM decoding and compare it with standard single-pass"
        },
        {
            "title": "Method",
            "content": "Lat. (ms) Thru. (sps)"
        },
        {
            "title": "Cost",
            "content": "Single-pass CoM (K=8, T=2) 8.7 18.4 115 55 baseline 2.1 Table 5: End-to-end inference efficiency of single-pass decoding vs. CoM. Latency (Lat.) is measured per sample. Throughput (Thru.) is computed at batch size 32. Method BERT CIDEr R@1 FID Var Time"
        },
        {
            "title": "SFT only\nPPO\nDPO\nGRPO",
            "content": "42.65 45.12 44.31 46.80 33.88 34.95 34.10 35.80 0.420 0.232 0.492 0.228 1.00 1.35 0.468 0.241 0.87 1.20 0.527 0.203 0.63 1.00 Figure 4: Results of user study. Table 6: Comparison of different optimization strategies under identical settings. GRPO provides the best overall performance and training stability."
        },
        {
            "title": "E User Study",
            "content": "decoding in Table 5. All measurements are obtained on single NVIDIA A100 GPU using batch size 32 under the HumanML3D generation setting. CoM introduces moderate overhead because it evaluates multiple candidate trajectories during inference. However, candidate sampling is executed in parallel, so the cost scales sub-linearly with . Despite the increased latency, CoM consistently improves semantic alignment, reasoning coherence, and physical plausibility, making the additional cost acceptable for practical use."
        },
        {
            "title": "D Choice of RL Optimizer",
            "content": "To examine the effect of different reinforcement learning optimizers, we trained MoRL using PPO, DPO, and GRPO under identical settings. The results are summarized in Table 6. GRPO provides the most stable and effective optimization for motion reasoning and generation. PPO shows instability when handling longhorizon and multi-component rewards, leading to higher variance and slower convergence. DPO underperforms because it is designed for preferencebased objectives and cannot fully exploit our structured reward components (semantic, reasoning, physical, and textmotion alignment). In contrast, GRPO stabilizes credit assignment over long reasoning chains and supports continuous reward shaping, resulting in consistent improvements across both understanding and generation metrics. Overall, GRPO achieves the best balance of performance, training stability, and efficiency, and we therefore adopt it as our default RL optimizer. We conduct user study to evaluate text-to-motion generation from human-centered perspective. We select 20 text prompts from the HumanML3D dataset and compare the motions generated by four methods: TM2T, AvatarGPT, Motion Agent, and our approach. total of 20 participants are recruited for the evaluation. For each sample, participants are asked to assess the generated motions of all four methods using four-point rating scale and to rank the methods from best to worst. During the evaluation, participants are instructed to focus on three key aspects: physical plausibility, motion smoothness, and semantic consistency between the generated motion and the input text. Figure 4 shows the distribution of user ratings for all compared methods. TM2T tends to receive lower ratings overall, which can be attributed to its limited capability in modeling long-term motion dynamics and complex textmotion relations. Despite this, TM2T still produces reasonable motions in simpler cases, reflecting the effectiveness of its early text-to-motion formulation. AvatarGPT and Motion Agent exhibit more balanced rating distributions, with noticeable shift toward higher scores. This suggests improved motion quality and semantic alignment compared to earlier methods, although occasional low ratings indicate challenges in maintaining global motion coherence and physical stability over longer sequences. Our method demonstrates clear concentration of high ratings and very few low-rated cases. This improvement is mainly due to the integration of structured motion composition, which facilitates coherent temporal transitions, and an explicit physical plausibility reward that helps suppress unrealistic poses and abrupt motion artifacts. As result, participants consistently prefer our gen-"
        },
        {
            "title": "Motion Agent",
            "content": "MoRL (Ours) person looks to the left then kicks something with their right foot. person walks up stairs. person walks forward, slightly shifting to the right. person walks forward with side-to-side sway. Table 7: Qualitative comparison (Part I). erated motions in terms of physical plausibility, motion smoothness, and semantic consistency."
        },
        {
            "title": "F More Qualitative Results",
            "content": "We present qualitative comparisons between Motion Agent and our method MoRL in Table 7 and Table 8, where the evaluated prompts cover sequential actions, continuous trajectory following, longhorizon repetition, and complex full-body coordination. These scenarios are intentionally selected to assess models ability to preserve semantic structure, temporal coherence, and spatial constraints over extended motion sequences. Across both tables, Motion Agent can generate visually plausible motions for simple and weakly constrained prompts. However, when the textual descriptions require explicit temporal ordering, global path planning, or fine-grained semantic modifiers, several systematic limitations become evident. prominent issue is Motion Agents difficulty in executing ordered multi-stage actions. For example, in the prompt person looks to the left then kicks something with their right foot  (Table 7)  , Motion Agent tends to blur the two stages into single ambiguous motion, where the head orientation change and the kicking action are not clearly separated in time. In contrast, MoRL produces distinct head-turning phase followed by well-timed right-foot kick, faithfully reflecting the sequential structure of the prompt. Motion Agent also struggles to maintain global spatial trajectories over long horizons. In prompts such as Walking slowly along the path shaped like an infinity symbol and person walks along curved path to the right  (Table 8)  , Motion Agent frequently collapses the motion into locally plausible stepping patterns that fail to realize the intended global path, often resulting in clustered poses or near-stationary behavior. This indicates that the"
        },
        {
            "title": "Motion Agent",
            "content": "MoRL (Ours) Walking slowly along the path shaped like an infinity symbol. person walks along curved path to the right. person backflips three times in row. person is practicing karate moves across the floor. Table 8: Qualitative comparison (Part II). model prioritizes short-term kinematic validity over long-range spatial constraints specified in the text. Another recurring limitation appears in longhorizon compositional execution. For instance, in the prompt person backflips three times in row  (Table 8)  , Motion Agent often produces incomplete or inconsistent repetitions, with noticeable degradation in motion amplitude and temporal rhythm across flips. This suggests difficulty in tracking and executing repeated action counts over extended sequences. Finally, Motion Agent exhibits limited sensitivity to fine-grained motion modifiers. In prompts such as person walks forward, slightly shifting to the right and person walks forward with side-to-side sway  (Table 7)  , Motion Agent tends to default to generic forward walking pattern, partially ignoring the subtle directional or stylistic constraints. Similarly, in person is practicing karate moves across the floor  (Table 8)  , the generated motion often simplifies into repetitive gestures, losing the structured coordination implied by the prompt. In contrast, MoRL consistently generates motion sequences that remain semantically faithful across all stages of the prompt. By explicitly modeling motion generation as reasoning process, MoRL is able to plan long-term trajectories, preserve action ordering, and integrate fine-grained semantic constraints into motion execution. These qualitative results, observed consistently across Table 7 and Table 8, demonstrate that MoRL better handles compositional, long-horizon, and structurally constrained motion generation, where Motion Agent exhibits inherent limitations."
        },
        {
            "title": "G Ethical considerations",
            "content": "G.1 The datasets used in this work (HumanML3D, KITML, and MotionHubV2) consist of motion capture data and textual descriptions of everyday human actions. They do not contain personal identifiers such as names, addresses, or biometric identity information. All textual annotations are action-level descriptions and do not include offensive, hateful, or sensitive personal content. No additional personal data were collected as part of this work. G.2 This work includes user study to evaluate the perceptual quality and semantic alignment of generated human motion sequences. Participants were asked to compare motions generated by different methods under the same textual description and provide subjective judgments based on predefined evaluation criteria. Participants were provided with written instructions describing the evaluation task and criteria. They were instructed to focus on motion naturalness, semantic correctness with respect to the given text, and temporal coherence across the motion sequence. No deceptive instructions or sensitive content were involved, and participants were informed that they could stop the evaluation at any time. Participants were recruited on voluntary basis. The user study targeted adult participants and did not involve any demographic-based filtering. Participation was compensated with small reward consistent with standard academic user studies, and the compensation was considered reasonable given the short duration and low burden of the task. Before participating, users were informed of the purpose of the study and that their responses would be used solely for academic research purposes. Only anonymized preference scores were recorded. No personally identifiable information was collected, stored, or processed at any stage of the study. The user study involved minimal risk and did not collect personal or sensitive data. Following common practice in prior human motion generation research, the study did not require formal institutional review board (IRB) approval."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Peking University",
        "The University of Sydney"
    ]
}