{
    "paper_title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale",
    "authors": [
        "Haiwen Diao",
        "Mingxuan Li",
        "Silei Wu",
        "Linjun Dai",
        "Xiaohua Wang",
        "Hanming Deng",
        "Lewei Lu",
        "Dahua Lin",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 9 7 9 4 1 . 0 1 5 2 : r FROM PIXELS TO WORDS TOWARDS NATIVE VISIONLANGUAGE PRIMITIVES AT SCALE Haiwen Diao1 Mingxuan Li2 Silei Wu3 Linjun Dai3 Xiaohua Wang2 Hanming Deng3 Lewei Lu3 Dahua Lin3 Ziwei Liu1 1S-Lab, Nanyang Technological University 2Xian Jiaotong University 3SenseTime Research Website: https://github.com/EvolvingLMMs-Lab/NEO"
        },
        {
            "title": "ABSTRACT",
            "content": "The edifice of native Vision-Language Models (VLMs) has emerged as rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various crossmodal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse realworld scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside dense and monolithic model crafted from our elaborate primitives. We position NEO as cornerstone for scalable and powerful native VLMs, paired with rich set of reusable components that foster cost-effective and extensible ecosystem."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, Vision-Language Models (VLMs) Bai et al. (2025); Zhu et al. (2025); Wang et al. (2025b); xAI (2025); Anthropic (2025); DeepMind (2025); Hurst et al. (2024); OpenAI (2025) have emerged as major breakthrough, extending the strong linguistic capabilities of Large Language Models (LLMs) to multimodal understanding. They typically follow modular design that integrates pre-trained Visual Encoder (VE) Radford et al. (2021); Chen et al. (2024f); Fang et al. (2023); Tschannen et al. (2025), Projector Alayrac et al. (2022); Liu et al. (2024a); Dai et al. (2024), and an LLM Touvron et al. (2023); Yang et al. (2025); DeepSeek-AI et al. (2025). Through multi-stage post-training at scale, they incrementally overcome limitations in image resolution, aspect ratio, and visual encoding flexibility. Yet, modular designs still contend with strong inductive biases in pre-trained visual semantics, complex infrastructure, and scaling laws needed to harmonize their components. Against this backdrop, native VLMs have arisen as new avenue of exploration, with Fuyu Bavishi et al. (2023) and EVE Diao et al. (2024) pioneering promising route towards monolithic VLMs. Subsequent efforts seek to learn vision perception from scratch and mitigate vision-language conflicts via visual encoder distillation Diao et al. (2024); Li et al. (2025b); Wang et al. (2025a); Li et al. (2025a), mixed training data Lei et al. (2025); Li et al. (2025a), and modality-specific decomposition Diao et al. (2025); Luo et al. (2024; 2025); Li et al. (2025a). Nonetheless, constructing visual representations via mapping functions inside pre-trained LLMs often hinders efficiency Chen et al. (2024d); Luo et al. (2024), destabilizes optimization Team (2024); Wang et al. (2024b), and disrupts original linguistic knowledge Diao et al. (2024); Chen et al. (2024d), even under decoupled designs or large budgets Beyer et al. (2024). Besides, HoVLE Tao et al. (2025) and HaploVL Yan et al. (2025) address 1 Figure 1: Overview of our native vision-language frameworks, which project arbitrary-resolution images into continuous latent space, integrating the virtues of modular VLM architectures and enabling efficient vision-language encoding, alignment, and interaction in an early-fusion manner. this by first mapping vision-language inputs into shared space. Yet, their modality-sharing modules, whether derived from LLM or VE layers, neglect intrinsic discrepancy in encoding and interaction across modalities, ultimately compromising VLMs capacity to unify visual-linguistic properties. Figure 1 outlines central question: What properties must native VLMs possess to compete with modular ones? Modular VLMs decouple vision encoders from language models, allowing each to exploit modality-specific characteristics, e.g., bi-directional versus causal attention, distinct positional embeddings, and varied network configurations. This separation accelerates the development of visual and linguistic competencies and permits flexible combinations of individual components. However, it fragments the training procedure, increases alignment costs, and leaves the intermodal balance unresolved. Motivated by these analyses, we formulate the following strategies accordingly: (1) Native VLM Primitive. Native VLMs should embody unified visionlanguage primitive that simultaneously integrates encoding, alignment, and reasoning across modalities in one single module. Its design should encompass three principles: (i) Flexible Position Encoding scheme that generalizes effectively to dynamic spatial structures; (ii) Multi-Head Native Attention (MHNA) that jointly processes visualtextual connectivity; (iii) Native Rotary Position Embeddings (Native-RoPE) with modality-specific frequencies, preserving compatibility with pretrained LLMs weights while absorbing original VEs interaction patterns. Guided by these tenets, we supercharge the fundamental LLM layers with hybrid attention, expanded heads, and targeted RoPE across modalities, synchronously capturing multi-dimensional relationships for fine-grained and comprehensive correspondence. (2) Pre-Buffer and Post-LLM. The next crucial issue is to efficiently scale visual training while securing consistent pixel-word alignment. Here, we partition the monolithic backbone into pre-Buffer and post-LLM layers during pre-training, each rooted in identical native primitive architectures. This transient stage enables pretrained LLMs to steer visual learning and establish coherent relevance with later stages. As mid-training and supervised fine-tuning advance, the partition dissolves, yielding unified architecture that autonomously allocates the VLMs capacities to their respective functions. This end-to-end training reduces semantic biases of separate pretraining and large overheads of post-stage alignment, effectively bridging native and modular VLMs. Crucially, pre-Buffer persists as reusable pretrained asset, facilitating sustainable resources for native VLM development. We launch NEO, an innovative native VLM that reimagines multi-modal integration from first principles. Unlike typical modular designs, NEO rests on unified primitives that natively encode, align, and reason across modalities, forming coherent pixelword correspondences from the outset. Through streamlined end-to-end training on 390M imagetext samples, NEO acquires strong visual perception and rivals leading modular VLMs of comparable scale across diverse benchmarks. Beyond competitive results, NEO offers reusable components that simplify subsequent development and reduce barriers to promoting native exploration. This reveals that next-generation multimodal systems could also originate from architectures that are native, unified, and intrinsically multimodal. 2 Figure 2: Overview of our proposed NEO architecture. We begin with lightweight patch and word embedding layers that encode images and text into token sequences, which are subsequently processed by monolithic decoder-only architecture. The pre-Buffer and post-LLM components, each stacked with multiple native primitives, facilitate efficient and precise pixelword alignment and reasoning."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 MODULAR VISION-LANGUAGE MODELS Current VisionLanguage Models (VLMs) have converged on modular paradigm, where pretrained Vision Encoder (VE) is paired with Large Language Model (LLM) via lightweight adapters, e.g., projection layers Li et al. (2024a;b) or cross-attention mechanisms Alayrac et al. (2022); Dai et al. (2024). This architecture underlies both leading proprietary systems, including Claude Anthropic (2024; 2025), GPT Hurst et al. (2024); Yang et al. (2023); OpenAI (2025), and Gemini Comanici et al. (2025); DeepMind (2025), as well as prominent open-source frameworks such as InternVL Zhu et al. (2025); Wang et al. (2025b), Qwen-VL Wang et al. (2024a); Bai et al. (2025), and Grok xAI (2024; 2025). By harnessing the complementary strengths of vision and language components, modular architectures, adhering to the ViT-MLP-LLM pipeline, achieve unprecedented performance across diverse multimodal benchmarks and have emerged as the dominant design principle in the field. Despite empirical successes, modular VLMs remain constrained by multi-stage training and heterogeneous structures. Extensive post-training interventions are often required to mitigate rigid inductive biases in pretrained VEs Wang et al. (2024a), which limit resolution flexibility, erode fine-grained details, and blunt sensitivity to features across scales. Besides, pretraining semantic biases and capacity trade-offs between VEs and LLMs collectively impede design simplicity, deployment efficiency, and seamless integration of vision and language, underscoring the urgent need for monolithic backbone. 2.2 NATIVE VISION-LANGUAGE MODELS Native VLMs embrace early-fusion integration rather than grafting VEs onto LLMs. Early Fuyu Bavishi et al. (2023), EVE Diao et al. (2024), and SOLO Chen et al. (2024d), embed image patches via linear projections, whereas Chameleon Team (2024), MoMA Lin et al. (2024), and MoT Liang et al. (2024) transform images into symbolic sequences via discrete tokenizers. Later studies Luo et al. (2024); Diao et al. (2025); Li et al. (2025b); Luo et al. (2025); Li et al. (2025a) leverage Mixture-ofExperts (MoE) or Divide-and-Conquer (DaC) strategies to suppress vision-language interference, while others Diao et al. (2024); Li et al. (2025b); Wang et al. (2025a); Li et al. (2025a) upgrade visual encoder supervision to accelerate the acquisition of visual concepts. Empirical evidence Beyer et al. (2024); Luo et al. (2024); Lei et al. (2025) reveals that, with sufficient data and progressive training, native VLMs rapidly approach modular counterparts, corroborating recent scaling-law insights Shukor et al. (2025b;a). Besides, recent methods Tao et al. (2025); Yan et al. (2025); Xiao et al. (2025) indicate that multi-modality encoding modules with the LLM or VE style slightly resolve vision-language misalignment, yet fail to fully integrate the distinct properties of each modality. Notably, NEO redefines native VLMs as unibody system built from first-principle primitives. Every componentfrom image patch encoding, attention mechanism to rotary position embeddingsensures full compatibility with the intrinsic modeling patterns of VEs and LLMs. Meanwhile, NEO evolves modular VLM strengths via the modality-agnostic pre-Buffer and end-to-end training, dramatically enhancing pixel-word alignment and pushing the frontier of native VLM research. 3 Figure 3: Overview of our native primitive, which integrates native attention with bi-directional dependencies within images and word / frame-wise causal interactions, together with native rotary position embeddings parameterized by modality-specific frequency, channel, and index allocation. It is inherently unified and intrinsically multimodal, substantially enhancing pixelword correspondence."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 MODEL ARCHITECTURE Figure 2 illustrates the proposed NEO framework, which comprises lightweight patch and word embedding layers, pre-Buffer, and post-LLM, built upon stacked native VLM primitives. Patch and Word Embeddings. Given an image I, we convert it into token sequences via lightweight Patch Embedding Layer (PEL) with two Convolutional layers (Conv12) Krizhevsky et al. (2012) and Gaussian Error Linear Unit (GELU) Hendrycks & Gimpel (2016). For input text T, we encode it into word tokens using the original LLM Tokenizer as Word Embedding Layer (WEL): xv = Conv2(GELU(Conv1(I)) + PE), xt = Tokenizer(T), (1) where xv R(hw)d / xt Rnd denote visual / textual tokens, and PE is 2D Sinusoidal Positional Encoding Dosovitskiy et al. (2021). The stride of Conv1 / Conv2 is 16 / 2, i.e., each visual token corresponds to 32 32 image patch. Notably, Conv2 performs token folding like pixel unshuffle Chen et al. (2024e), with the special <img> and </img> tokens inserted at the boundaries of visual tokens, while mapping position and patch embeddings into unified space. Afterward, visual and textual tokens are merged and propagated through the unified backbone. Native VLM Primitive. It adopts RMSNorm Zhang & Sennrich (2019) and SwiGLU Dauphin et al. (2017) consistent with the original LLM layers. Unlike prior methods Wang et al. (2024a); Bai et al. (2025); Zhu et al. (2025); Wang et al. (2025b) that collapse visual tokens into 1D representations or merely reallocate pre-trained LLM head dimensions across temporal (T), height (H), and width (W), we expand Query (Q) and Key (K) head dimensions while fully decoupling H, W, and relations in Figure 3(1), causing an extra 10% parameter counts over the original Transformer block. The original dimension is preserved, and additional and dimensions are added with their respective QK normalization Yang et al. (2025). Crucially, the linear weights of for and channels are zero-initialized, and attention scale matches that of LLMs, maintaining the LLM pre-training paradigm and progressively activating multimodal capabilities for visual spatial relationships. This philosophy aligns with our Native Rotary Position Embedding (Native-RoPE) in Figure 3(2), which eliminates correlations between / and indexes, while decoupling channel allocations of H, W, and under the original LLM frequency. (1) Index Allocation: For text, index is retained while / indexes are zeroed. Notably, Native-RoPE is equivalent to 1D-RoPE before training. For images, each visual token has constant index, with unique / indexes encoding spatial location. Videos, treated as sequences of frames, increment index per frame, while / indexes follow the same spatial scheme as images. In multimodal inputs, each modalitys index starts from the maximum ID of the preceding modality, ensuring continuous and unambiguous positional encoding across modalities. This serves two purposes: (a) For image pairs, / indexes start independently 4 from (0,0), and tokens at corresponding positions share identical dependency, strongly reinforcing correlations and interactions across matching regions Liao et al. (2025); Wu et al. (2025); (b) For image-text pairs, / indexes are decoupled from index and bounded within (0,0) and (H,W), preventing large index growth from disproportionately affecting / indexes Wang et al. (2024a); Bai et al. (2025) and thereby keeping spatial dependencies between long-range text and images. Another key aspect is (2) Channel and Frequency Allocation. Unlike recent 3D-RoPE methods Bai et al. (2025); Wei et al. (2025); Yuan et al. (2025); Liao et al. (2025), we fully decouple the channel and frequency allocation of H, W, and T, equipped with additional Q/K head dimensions for and W. This resolves two issues: (a) Zeroing / indexes for pure text would disrupt the modeling patterns and linguistic capacity of the LLM if restricted to its original channels. Repairing this disruption requires substantial resources; (b) Even with interleaved or segmented reallocation, and are theoretically equivalent but are assigned different frequencies. Meanwhile, the RoPE frequency in LLMs is far lower than that of visual encoders in Figure 3(2). This mismatch limits the modeling of relative distances and local semantics. The problem is exacerbated by the disparity in scales, with temporal ranges spanning up to one million and spatial ranges only few hundred. Specifically, Native-RoPE assigns distinct base frequencies to T, H, and within their own dimensions, i.e., original LLM head dimension for and new head dimension for / as follows: (cid:27) (cid:26) (cid:26) (cid:26) ΘT = β 2k [0, , ΘH = β 4i [0, , ΘW = β 4j [0, ) (2) (cid:27) ) 4 (cid:27) ) 4 where β and Θ indicate the base and rotation frequency across H, W, and T. Notably, temporal dimension captures both local and long-range relations, whereas spatial / dimensions emphasize local dependencies. This also opens avenues for broader applications, e.g., video understanding Wei et al. (2025), multimodal generation Deng et al. (2025b), and editing Deng et al. (2025a). Inspired by prior works Lei et al. (2025); Deng et al. (2025b); Li et al. (2025a), we treat one single image as unified meta-unit for autoregressive modeling. To enable this, we propose Native MultiModal Attention with mixed masking in Figure 3(c). Text tokens adhere to standard causal attention, attending only to preceding tokens to maintain autoregressive generation. In contrast, image tokens employ full bidirectional attention, enabling exhaustive interactions among all visual tokens, akin to visual encoder. This design captures rich spatial and contextual dependencies within images and facilitates vision-language correspondences, thereby supporting complex multimodal reasoning. We use FlexAttention Dong et al. (2024) to minimize memory overhead and increase throughput, as variable-length block-wise attention is fully optimized through CUDA kernel modifications. Pre-Buffer and Post-LLM. Drawing on modular designs Bai et al. (2025); Wang et al. (2025b), we split NEO into encoding and reasoning components at the outset. In contrast, we build modalityshared pre-Buffer via native primitives to map vision and language into unified representation space. We further design post-LLM via native primitives to absorb the powerful language proficiency and reasoning capabilities of LLMs. This, in turn, promotes deep pixel-word integration within the preBuffera deliberate design choice to ensure rich multimodal alignment while minimizing disturbance to the LLM. The layer depth in the pre-Buffer and post-LLM primarily refers to the model parameters of existing VEs and LLMs, ensuring relatively fair comparison while balancing accuracy and efficiency. Crucially, this separation exists only during pre-training to boost visual learning; during mid-training and supervised fine-tuning, the components are upgraded to monolithic backbone, allowing the VLM to automatically allocate capacity for encoding, alignment, and reasoning. 3.2 TRAINING PROCEDURE Figure 4 illustrates the whole training pipeline, which proceeds through three progressive stages: pre-training, mid-training, and supervised fine-tuning. The entire model is optimized end-to-end. Pre-Training Stage. In this phase, NEO acquires fundamental visual concepts and contextual dependencies from scratch, guided by pre-trained patterns from LLMs. Training leverages 345M web-scale and synthetic image-caption pairs, including 100M English and 20M Chinese pairs from LAION-400M Schuhmann et al. (2021), 150M English pairs from COYO-700M Byeon et al. (2022), 20M long-caption examples from BLIP3o Chen et al. (2025), and 5M short-caption pairs from OpenImages Kuznetsova et al. (2018), recaptioned with pre-trained InternVL2-8B model. The dataset is further enriched with 30M samples from LAION-COCO Schuhmann et al. (2022) and 20M 5 Figure 4: Overview of the entire training recipe. During pre-training, NEO learns visual perception from massive web-scale and synthetic image-caption pairs with frozen LLM weights to preserve linguistic knowledge. In mid-training and supervised fine-tuning, the full model is progressively optimized end-to-end using caption, conversation, OCR, detection, and high-quality instruction data. examples from Wukong Gu et al. (2022) with rich Optical Character Recognition (OCR) annotations. 3:7 ratio of language to multi-modal data is incorporated to reconstruct text projections in the pre-Buffer. Only the patch embedding layer, the pre-Buffer, and additional QK linear weights and normalization, along with and W, are trainable and optimized with simple next-token prediction objective. Notably, the new QK heads not only counteract the LLMs strong language bias that limits visual specialization but also safeguard its capabilities against the effects of low-quality data. Mid-Training Stage. The objective at this stage is to strengthen the alignment between visual and linguistic capabilities while progressively enhancing recognition of high-resolution images, complex scenes, object scales, spatial grounding, and compact OCR content. The training data is drawn from the pre-training corpus of InternVL-1.5 Chen et al. (2024f), comprising 40M samples across image captioning, conversation, detection, and OCR data, which account for approximately 66%, 11%, 8%, and 15% of the total, respectively. 3:7 ratio of language to multi-modal data is again applied. The entire architecture is updated with the same loss functions to consolidate vision-language alignment, thereby equipping NEO with the foundational abilities required for various visual scenarios. Supervised Fine-Tuning Stage. During the SFT stage, NEOs ability to follow complex linguistic instructions and varied dialogue patterns is further enhanced, critical step towards real-world deployment. The full network is optimized across diverse high-quality, multi-source instruction datasets. Following Mono-InternVL Luo et al. (2024), we employ about 4M bilingual instructions for supervised learning, covering tasks such as visual question answering, multimodal dialogue, mathematics, and knowledge reasoning. Details of the instruction data are provided in the Appendix."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 TRAINING SETTINGS Our NEO models are built on Qwen3-1.7B and Qwen3-8B Yang et al. (2025) as the LLMs. The pre-Buffer employs L1 = 12 primitive layers for NEO-2.2B and L1 = 6 for NEO-9B. We extend only the QK head dimension in raw transformer layers, introducing roughly 10% extra parameters over the original design. The base RoPE frequencies βT , βH , and βW are set to 1 106, 1 104, and 1 104, respectively. NEO is trained on sixteen 8-GPU (80G) nodes using the AdamW optimizer Loshchilov & Hutter (2019). The maximum learning rates for pre-training, mid-training, and SFT are 8 104, 4 105, and 5 105, with warm-up ratio of 0.01 and cosine decay scheduler across all stages. 4.2 MAIN RESULTS We conduct standard evaluations with VLMEvalKit Duan et al. (2024) on diverse benchmarks, covering chart, diagram, and document understanding tasks, e.g., AI2D Kembhavi et al. (2016), DocVQA Clark & Gardner (2018), ChartQA Masry et al. (2022), InfoVQA Mathew et al. (2022), TextVQA Singh et al. (2019), and OCRBench Liu et al. (2023e); visual perception and challenging reasoning tasks, e.g., MMMU Yue et al. (2024), MMBench-EN (MMB) Liu et al. (2024b), MMVet Yu et al. (2024), MMStar Chen et al. (2024c), SEEDBench-IMG (SEED-I) Li et al. (2023a); visual hallucination tasks, e.g., POPE Li et al. (2023b) and HallusionBench (HallB) Guan et al. (2024). 6 Table 1: Comparison with modular and native VLMs on general vision-language benchmarks. # Data denotes the dataset scale during pre-training, mid-training, and supervised fine-tuning. indicates models that employ reinforcement learning (RL). Bold highlights the highest performance. MMMU MMB MMVet MMStar SEED-I POPE HallB LLM # Data / / / / Qwen2-1.5B >6B / 40M / 4M 41.1 InternLM2.5-1.8B >6B / 100M / 16M 43.6 51.2 Qwen2.5-1.5B Qwen2.5-1.5B >6B / 100M / 22M 48.6 47.1 Qwen3-1.7B Model Modular Vision-Language Models (2B) Qwen2-VL InternVL2.5 Qwen2.5-VL InternVL3 Encoder-Based Native Vision-Language Models (2B) Mono-InternVL InternLM2-1.8B 1.2B / 143M / 7M 33.7 Mono-InternVL-1.5 InternLM2-1.8B 400M / 150M / 7M 39.1 InternLM2-1.8B 550M / 50M / 7M 32.2 HoVLE Qwen2.5-1.5B 436M / 70M / 13M 39.0 OneCAT 345M / 40M / 4M 48.6 NEO Qwen3-1.7B Modular Vision-Language Models (8B) Qwen2-VL InternVL2.5 Qwen2.5-VL InternVL3 Encoder-Based Native Vision-Language Models (8B) Fuyu Chameleon EVE SOLO Emu3 EVEv2 BREEN VoRA SAIL NEO 27.9 1.4B / 0M / 1.8M 25.4 33M / 0M / 1.8M 32.6 44M / 0M / 2M 31.6 / / 39.3 77M / 15M / 7M 13M / 0M / 4M 42.7 30M / 0M / 0.6M 32.0 512M / 86M / 6M 345M / 40M / 4M 54.6 54.1 InternLM2.5-7B >6B / 50M / 4M 56.0 Qwen2.5-7B 55.0 Qwen2.5-7B >6B / 100M / 22M 62.7 54.1 Qwen3-8B Persimmon-8B from scratch Vicuna-7B Mistral-7B from scratch Qwen2.5-7B Qwen2.5-7B Qwen2.5-7B Mistral-7B Qwen3-8B >6B / 40M / 4M Qwen2-7B / / / / / / 74.9 74.7 79.1 81.1 75.8 65.5 64.0 73.3 72.4 76.0 83 84.6 83.5 83.4 84 10.7 31.1 52.3 67.7 58.5 66.3 71.4 61.3 70.1 82.1 49.5 60.8 61.8 62.2 37.4 40.1 54.0 43.8 42.4 49. 62.0 62.8 67.1 81.3 60.0 21.4 8.3 25.7 30.4 37.2 45.0 38.9 33.7 46.3 53.6 48.0 53.7 55.9 60.7 52.7 54.2 60.7 64.4 63.9 68.2 63.5 51.2 53.1 62. 73.6 67.4 66.9 70.9 70.9 74.2 76.2 59.3 30.6 64.6 64.4 68.2 71.4 68.9 72.9 76.3 90.6 89.6 87.0 87.4 87. 88.1 90.6 86.4 91.1 87.8 84.0 19.4 85.0 78.6 85.2 87.6 85.5 85.8 88.4 41.7 42.6 46.3 42.5 44.4 34.8 32.5 38.4 43.1 50.6 50.1 52.9 49.9 51.4 17.1 26.4 37.0 54.2 46. Following InternVL3 Zhu et al. (2025), we construct the Encoder-Based by combining Qwen3 Yang et al. (2025) and InternViT-300M Zhu et al. (2025). In the mid-training stage, we first train the projector on 10M samples, and further unfreeze the vision encoder utilizing another 30M samples. Comparison with Modular VLMs. As demonstrated in Table 1 and Table 2, NEO achieves highly competitive performance at both the 2B and 8B scales, despite using relatively limited pre-training and supervised fine-tuning data and without reinforcement learning. Remarkably, NEO approaches the performance of top-tier modular VLMs, e.g., Qwen2-VL Wang et al. (2024a), InternVL2.5 Chen et al. (2024e), Qwen2.5-VL Bai et al. (2025), and InternVL3 Zhu et al. (2025) across multiple benchmarks, rivaling architectures trained on billions of additional samples. These results highlight the effectiveness of our end-to-end training strategy and unified model design. By combining native attention mechanisms with Native-RoPE, NEO enhances interactions between visual and linguistic features, enabling it to match more complex modular systems despite its simpler architecture. Comparison with Native VLMs. From Table 1 and Table 2, NEO delivers substantial gains on visual-centric benchmarks over the best competitors, e.g., Mono-InterVL Luo et al. (2024; 2025), HoVLE Tao et al. (2025), OnCAT Li et al. (2025a), EVE Diao et al. (2024; 2025), Emu3 Wang et al. (2024b), BREEN Li et al. (2025b), VoRA Wang et al. (2025a), and SAIL Lei et al. (2025). By seamlessly integrating post-LLM components with the pre-Buffer for large-scale visual learning, NEO aligns visual inputs with textual features from scratch and supports complex visual reasoning, even without visual encoder supervision Diao et al. (2024); Tao et al. (2025); Li et al. (2025a); Wang et al. (2025a); Li et al. (2025b), highlighting the strengths of its native primitive designs and training strategies. These design choices allow NEO to surpass many native VLMs using fewer training resources, demonstrating the advantages of our primitives with efficient data-scaling capability. 7 Table 2: Comparison with modular and native VLMs on visual question answering benchmarks. Any Res., Tile-wise, Any Rat., and Fix Res. refer to any resolution, image tile splitting, any aspect ratio, and fixed resolution. MoE and DaC are Mixture-of-Experts and Divide-and-Conquer models. Backbone AI2D DocVQA ChartQA InfoVQA TextVQA OCRBench Input Dense Dense Dense Dense Dense Model RoPE Modular Vision-Language Models (2B) Any Res. M-RoPE Qwen2-VL Tile-wise InternVL2.5 1D-RoPE Qwen2.5-VL Any Res. M-RoPE InternVL3 1D-RoPE Tile-wise Encoder-Based 1D-RoPE Tile-wise Native Vision-Language Models (2B) MoE 1D-RoPE Mono-InternVL Tile-wise. DaC 1D-RoPE Mono-InternVL-1.5 Tile-wise. Dense Tile-wise. HoVLE 1D-RoPE Any Res. M-RoPE OneCAT Dense NEO Any Res. Native-RoPE Dense Modular Vision-Language Models (8B) Any Res. M-RoPE Qwen2-VL Tile-wise InternVL2.5 1D-RoPE Qwen2.5-VL Any Res. M-RoPE InternVL3 1D-RoPE Tile-wise Encoder-Based 1D-RoPE Tile-wise Native Vision-Language Models (8B) Fuyu Chameleon EVE SOLO Emu3 EVEv2 BREEN VoRA SAIL NEO 1D-RoPE Any Res. Dense 1D-RoPE Fix Res. Dense 1D-RoPE Any Rat. Dense 1D-RoPE Any Res. Dense 1D-RoPE Fix Res. Dense 1D-RoPE Any Rat. DaC 1D-RoPE Any Res. MoE Any Res. 1D-RoPE Dense Dense Any Res. M-RoPE Any Res. Native-RoPE Dense Dense Dense Dense Dense Dense 74.7 74.9 81.6 78.7 77. 68.6 67.4 73.0 72.4 80.1 83.0 84.5 83.9 85.2 82.9 64.5 46.0 61.0 61.4 70 74.8 76.4 61.1 76.7 83.1 90.1 88.7 93.9 88.3 89.9 80.0 81.7 86.1 87.1 89.9 94.5 93.0 95.7 92.7 92. 1.5 53.0 76.3 88.6 73.5 79.2 84.0 80.2 78.4 73.7 72.2 78.6 76.2 81.2 83 84.8 87.3 86.6 83.5 2.9 59.1 68.6 73.9 82.1 65.5 60.9 77.1 66.1 65. 43.0 47.9 55.7 56.3 63.2 76.5 77.6 82.6 76.8 75 5.0 25.0 43.8 60.9 79.7 74.3 79.3 77.0 73.3 72.6 73.7 70.9 67.0 74.0 84.3 79.1 84.9 80.2 77. 4.8 56.8 64.7 71.1 65.7 58.7 77.1 75.0 80.9 80.4 79.7 83.5 83.5 76.7 80.1 74.0 77.1 86.6 82.2 86.4 88 85.3 36.6 0.7 39.8 12.6 68.7 70.2 78.3 77.7 Despite strong results, NEO lags on knowledge-/OCR-heavy tasks, e.g., MMMU, InfoVQA, and TextVQA. Interestingly, NEO-9B does not surpass NEO-2B on DocVQA and InfoVQA, indicating limitations in our current training corpus. Even so, NEO performs well under constraints, highlighting the native VLM as scalable paradigm. Larger datasets and resources can unlock its full potential. 4.3 ABLATION STUDIES Unless otherwise specified, we report the average evaluation results, denoted as Avg., across ten vision-language benchmark datasets in Table 3. The pre-Buffer and new head dimensions in the post-LLM are trained on 20M pre-training samples, followed by full-backbone fine-tuning on 2M SFT instruction data. These constitute the standard training settings for our ablation studies. Hyperparameters of the Pre-Buffer Layer. Figure 5 illustrates the relationship between the number of preBuffer layers and the models average accuracy, using Qwen3-1.7B as the post-LLM. Performance improves consistently as the layer count increases, but gains begin to saturate beyond eight layers. To maximize accuracy while maintaining the same capacity as publicly available vision encoders Chen et al. (2024f); Radford et al. (2021); Zhai et al. (2023), we select 12 layers for NEO-2.2B. Notably, we choose 6 layers for NEO-9B, mainly due to the good trade-off between performance and efficiency. 8 Figure 5: Configurations of pre-Buffer. Table 3: Configurations of attention and RoPE. MMS, CQA, IVQA, and OCRB denote MMStar, ChartQA, InfoVQA, and OCRBench. indicates that the base RoPE frequencies for height and width are set to 1M. To ensure fairness, we add new head dimensions of equal size across all models. Model Attention RoPE MMMU MMB MMS SEED-I AI2D CQA IVQA TVQA OCRB POPE Avg. D H 1D-RoPE Causal 1D-RoPE Mixed IL-RoPE Mixed Mixed M-RoPE Mixed MM-RoPE Mixed Video-RoPE Causal Native-RoPE Mixed Native-RoPE Mixed Native-RoPE 40.2 40.8 40.0 40.3 40.5 40.6 40.2 40.7 40. 48.6 48.8 47.3 49.6 50.8 51.3 49.2 51.9 50.4 36.1 36.4 36.3 37.2 37.6 37.8 36.3 38.2 36.9 55.3 57.3 57.6 57.8 58.2 58.8 57.1 58.9 57.0 63.6 16.1 22.5 63.7 16.0 21.9 62.0 18.8 23.4 64.2 23.7 25.2 65.8 25.7 26.3 64.3 27.4 26.1 63.7 19.2 23.5 65.8 30.6 26.9 64.1 25.6 25.2 16.2 17.4 17.9 20.4 22.1 23.7 19.5 24.1 21.7 13.9 16.0 13.2 18.8 18.2 21.3 16.7 23.2 20. 78.6 39.1 79.2 39.8 78.8 39.5 79.3 41.7 78.8 42.4 81.0 43.2 77.8 40.3 80.0 44.0 78.7 42.0 Figure 6: Comparison with pre-Buffer and vision encoders. All models are initialized post-LLM using Qwen3-1.7B. Figure 7: Evaluation results across three progressive training procedures. Configurations of Native Primitives. Table 3 compares various attention and RoPE designs. The pre-Buffer depth is 4, and the post-LLM is initialized with Qwen3-1.7B. All models share the same new QK head dimensions and normalization. (1) Attention mode. Comparing models A/B and G/H reveals consistent gains of mixed attention over causal one, reflecting its stronger capacity to model comprehensive dependencies and cross-modal alignment. (2) RoPE mode. Native-RoPE outperforms 1D-RoPE Zhu et al. (2025), IL-RoPE Liao et al. (2025), M-RoPE Bai et al. (2025), MM-RoPE Yuan et al. (2025), and Video-RoPE Wei et al. (2025), with at least 0.8% gain. This validates the importance of disentangling height, width, and temporal components in RoPE to enhance spatialtemporal representations and fine-grained interactions. By contrast, setting the base RoPE frequency to 1M for height and width severely impairs the ability to perceive local semantics. Comparison between Pre-Buffer and Vision Encoders. In Figure 6, PB 13 denotes the Pre-Buffer after stage 13. For all models except NEO, the post-LLMs are initialized via Qwen3-1.7B for our pre-Buffer, InternViT-300M Chen et al. (2024e), CLIP-vit-large-patch14 Radford et al. (2021), and SigLIP-so400m-patch14-384 Zhai et al. (2023). After two-stage re-training, PB3 shows only an average gap of 2.5 / 2.4 / 1.7 / 3.7% over NEO / InternViT / CLIP / SigLIP using billion-scale training data. This substantially reduces the training costs of building native VLMs for subsequent research. Performance Gains across Stages. Figure 7 presents the result evolution across training stages. In Stages 1 and 2, the model is fine-tuned on 2M SFT examples. Performance improves consistently as training data scales increase across 2.2B and 9B model sizes. Following progressive training, NEO shows strong multimodal capabilities, enabling robust performance across diverse real-world tasks."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce NEO, native VLM that seamlessly integrates vision and language into single unified framework, eliminating the need for separate visual encoders or ad-hoc alignment modules. By leveraging hybrid attention and modality-aware rotary position embeddings, NEO captures rich, fine-grained interactions between pixels and words from the outset. Its pre-Buffer and post-LLM training paradigm ensures efficient convergence and robust alignment while maintaining end-to-end learning. Experiments show that this unified design not only advances multimodal understanding and reasoning but also lays the foundation for reusable, scalable components. Our native primitives highlight promising path toward intrinsically multimodal, unified, and adaptable architectures."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "All resources are drawn from open-access datasets with explicitly defined usage policies. Our work seeks to advance multimodal learning capabilities without introducing ethical or safety concerns beyond those already associated with existing models. Nevertheless, risks such as dataset biases and potential misuse cannot be entirely ruled out. We emphasize the importance of careful data curation, responsible deployment, and transparent reporting as essential practices to mitigate these challenges."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We place strong emphasis on reproducibility, providing detailed descriptions to facilitate replication and validation. Information about dataset selection, training strategies, and evaluation settings is provided in Sec. 3.2 and Sec. 4.1. We commit to releasing the code, model weights, and detailed documentation to allow the community to reproduce our findings in future research."
        },
        {
            "title": "REFERENCES",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. In Advances of Neural Information Processing Systems, New Orleans, LA, USA, 2022. Anthropic. Claude 3.7 sonnet: hybrid reasoning ai model, 2025. URL https://www. anthropic.com/news/claude-3-7-sonnet. AI Anthropic. The claude 3 model family: opus, sonnet, haiku, 2024. URL https://www-cdn. anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_ Card_Claude_3.pdf. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Junting Zhou, Ziqiang Liu, Feiteng Fang, Mingshan Chang, Tianyu Zheng, Xincheng Zhang, et al. Coig-cqia: Quality is all you need for chinese instruction fine-tuning. CoRR, abs/2403.18058, 2024. Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasırlar. Introducing our multimodal models, 2023. URL https://www.adept.ai/ blog/fuyu-8b. Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey A. Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bosnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier J. Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. Paligemma: versatile 3b vlm for transfer. CoRR, abs/2407.07726, 2024. Ali Furkan Biten, Rub`en Tito, Andres Mafla, Lluıs Gomez Bigorda, Marcal Rusinol, C. V. Jawahar, In IEEE Ernest Valveny, and Dimosthenis Karatzas. Scene text visual question answering. International Conference on Computer Vision, pp. 42904300, Seoul, Korea (South), 2019. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022. URL https://github.com/kakaobrain/ coyo-dataset. 10 Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In International Conference on Computational Linguistics, pp. 1511 1520, 2022. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: harnessing gpt4v-synthesized data for lite vision-language model. CoRR, abs/2402.11684, 2024a. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. CoRR, abs/2505.09568, 2025. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: improving large multi-modal models with better captions. In European Conference on Computer Vision, volume 15075, pp. 370387, Milan, Italy, 2024b. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large visionlanguage models? In Advances of Neural Information Processing Systems, Vancouver, BC, Canada, 2024c. Yangyi Chen, Xingyao Wang, Hao Peng, and Heng Ji. single transformer for scalable visionlanguage modeling. CoRR, abs/2407.06438, 2024d. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. CoRR, abs/2412.05271, 2024e. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. CoRR, abs/2404.16821, 2024f. Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, et al. Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art. In International Conference on Document Analysis and Recognition, pp. 15711576, 2019. Christopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension. In Annual Meeting of the Association for Computational Linguistics, pp. 845855, Melbourne, Australia, 2018. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit S. Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, NanJiang Jiang, Krishna Haridasan, Ahmed Omran, Nikunj Saunshi, Dara Bahri, Gaurav Mishra, Eric Chu, Toby Boyd, Brad Hekman, Aaron Parisi, Chaoyi Zhang, Kornraphop Kawintiranon, Tania Bedrax-Weiss, Oliver Wang, Ya Xu, Ollie Purkiss, Uri Mendlovic, Ilaı Deutel, Nam Nguyen, Adam Langley, Flip Korn, Lucia Rossazza, Alexandre Rame, Sagar Waghmare, Helen Miller, Nathan Byrd, Ashrith Sheshan, Raia Hadsell Sangnie Bhardwaj, Pawel Janus, Tero Rissa, Dan Horgan, Sharon Silver, Ayzaan Wahid, Sergey Brin, Yves Raimond, Klemen Kloboves, Cindy Wang, Nitesh Bharadwaj Gundavarapu, Ilia Shumailov, Bo Wang, Mantas Pajarskas, Joe Heyward, Martin Nikoltchev, Maciej Kula, Hao Zhou, Zachary Garrett, Sushant Kafle, Sercan Arik, Ankita Goel, Mingyao Yang, Jiho Park, Koji Kojima, Parsa Mahmoudieh, Koray Kavukcuoglu, Grace Chen, Doug Fritz, Anton Bulyenov, Sudeshna Roy, Dimitris Paparas, Hadar Shemtov, Bo-Juen Chen, Robin Strudel, David Reitter, Aurko Roy, Andrey Vlasov, Changwan Ryu, Chas Leichner, 11 Haichuan Yang, Zelda Mariet, Denis Vnukov, Tim Sohn, Amy Stuart, Wei Liang, Minmin Chen, Praynaa Rawlani, Christy Koh, JD Co-Reyes, Guangda Lai, Praseem Banzal, Dimitrios Vytiniotis, Jieru Mei, and Mu Cai. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. CoRR, abs/2507.06261, 2025. Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. NVLM: open frontier-class multimodal llms. CoRR, abs/2409.11402, 2024. Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose M. F. Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 10801089, Honolulu, HI, USA, 2017. Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated In International Conference on Machine Learning, volume 70, pp. convolutional networks. 933941, Sydney, NSW, Australia, 2017. Google DeepMind. Gemini 2.5 pro: Googles most advanced reasoning model, 2025. URL https: //deepmind.google/models/gemini/pro/. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Shi Guang, and Haoqi Fan. Emerging properties in unified multimodal pretraining. CoRR, abs/2505.14683, 2025a. Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. In International Conference on Learning Representations, Singapore, 2025b. Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. CoRR, abs/2406.11832, 2024. Haiwen Diao, Xiaotong Li, Yufeng Cui, Yueze Wang, Haoge Deng, Ting Pan, Wenxuan Wang, Huchuan Lu, and Xinlong Wang. Evev2: Improved baselines for encoder-free vision-language models. CoRR, abs/2502.06788, 2025. Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels. CoRR, abs/2412.05496, 2024. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: transformers for image recognition at scale. In International Conference on Learning Representations, Austria, 2021. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In ACM International Conference on Multimedia, pp. 1119811201, Melbourne, VIC, Australia, 2024. 12 Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. EVA: exploring the limits of masked visual representation learning at scale. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 1935819369, Vancouver, BC, Canada, 2023. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: elevating the role of image understanding in visual question answering. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 63256334, Honolulu, HI, USA, 2017. Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, Chunjing Xu, and Hang Xu. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. In Advances of Neural Information Processing Systems, New Orleans, LA, USA,, 2022. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large visionlanguage models. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 14375 14385, Seattle, WA, USA, 2024. Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin. Wanjuan: comprehensive multimodal dataset for advancing english and chinese large models. CoRR, abs/2308.10755, 2023. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). CoRR, abs/1606.08415, 2016. Drew A. Hudson and Christopher D. Manning. GQA: new dataset for real-world visual reasoning and compositional question answering. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 67006709, Long Beach, CA, USA, 2019. Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn. Gpt-4o system card. CoRR, abs/2410.21276, 2024. Jimmycarter. Textocr gpt-4v dataset, 2023. URL https://huggingface.co/datasets/ jimmycarter/textocr-gpt4v. Kushal Kafle, Brian L. Price, Scott Cohen, and Christopher Kanan. DVQA: understanding data visualizations via question answering. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 56485656, Salt Lake City, UT, USA, 2018. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Min Joon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European Conference on Computer Vision, volume 9908, pp. 235251, Amsterdam, The Netherlands, 2016. Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than sixth grader? textbook question answering for multimodal 13 machine comprehension. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 49995007, 2017. Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision, volume 13688, pp. 498517, Tel Aviv, Israel, 2022. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123(1):3273, 2017. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances of Neural Information Processing Systems, pp. 11061114, Lake Tahoe, Nevada, US, 2012. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R. Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: unified image classification, object detection, and visual relationship detection at scale. CoRR, abs/1811.00982, 2018. LAION. Gpt-4v dataset, 2023. URL https://huggingface.co/datasets/laion/ gpt4v-dataset. Weixian Lei, Jiacong Wang, Haochen Wang, Xiangtai Li, Jun Hao Liew, Jiashi Feng, and Zilong Huang. The scalability of simplicity: Empirical analysis of vision-language learning with single transformer. CoRR, abs/2504.10462, 2025. Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herve Le Borgne, Romaric Besancon, Jose Moreno, and Jesus Lovon Melgarejo. Viquae, dataset for knowledge-based visual question answering about named entities. In ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 31083120, 2022. Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. stronger llms supercharge multimodal capabilities in the wild, 2024a. URL https://llava-vl.github.io/blog/ 2024-05-10-llava-next-stronger-llms/. Llava-next: Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: easy visual task transfer. CoRR, abs/2408.03326, 2024b. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: benchmarking multimodal llms with generative comprehension. CoRR, abs/2307.16125, 2023a. Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, and Hongkai Xiong. Onecat: Decoder-only auto-regressive model for unified understanding and generation. CoRR, abs/2509.03498, 2025a. Tianle Li, Yongming Rao, Winston Hu, and Yu Cheng. BREEN: bridge data-efficient encoder-free multimodal learning with learnable queries. CoRR, abs/2503.12446, 2025b. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Conference on Empirical Methods in Natural Language Processing, pp. 292305, Singapore, 2023b. Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 1496314973, 2023c. 14 Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, and Xi Victoria Lin. Mixture-of-transformers: sparse and scalable architecture for multi-modal foundation models. CoRR, abs/2411.04996, 2024. Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. CoRR, abs/2505.05472, 2025. Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Ghosh, Luke Zettlemoyer, and Armen Aghajanyan. Moma: efficient early-fusion pre-training with mixture of modality-aware experts. CoRR, abs/2407.21770, 2024. Adam Dahlgren Lindstrom and Savitha Sam Abraham. Clevr-math: dataset for compositional language, visual and mathematical reasoning. CoRR, abs/2208.05358, 2022. Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023a. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. CoRR, 2023b. Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. CoRR, abs/2311.10774, 2023c. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances of Neural Information Processing Systems, New Orleans, LA, USA, 2023d. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 2628626296, Seattle, WA, USA, 2024a. Xi Liu, Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Icdar 2019 robust reading challenge on reading chinese text on Wang, Minghui Liao, et al. signboard. CoRR, abs/1912.09641, 2019. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: is your multi-modal model an all-around player? In European Conference on Computer Vision, volume 15064, pp. 216233, Milan, Italy, 2024b. Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. CoRR, abs/2305.07895, 2023e. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, New Orleans, LA, USA, 2019. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. CoRR, abs/2105.04165, 2021. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: multimodal reasoning via thought chains for science question answering. In Advances of Neural Information Processing Systems, volume 35, pp. 25072521, New Orleans, LA, USA, 2022a. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. CoRR, abs/2209.14610, 2022b. Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu. Monointernvl: pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training. CoRR, abs/2410.08202, 2024. 15 Gen Luo, Wenhan Dou, Wenhao Li, Zhaokai Wang, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, and Jifeng Dai. Mono-internvl-1.5: Towards cheaper and faster monolithic multimodal large language models. CoRR, abs/2507.12566, 2025. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L. Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 1120, Las Vegas, NV, USA, 2016. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: visual question answering benchmark requiring external knowledge. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 31953204, Vienna, Austria, 2019. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Annual Meeting of the Association for Computational Linguistics, pp. 22632279, Dublin, Ireland, 2022. Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V. Jawahar. Infographicvqa. In IEEE Winter Conference on Applications of Computer Vision, pp. 25822591, Waikoloa, HI, USA, 2022. Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In IEEE Winter Conference on Applications of Computer Vision, pp. 15271536, 2020. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. OCR-VQA: visual question answering by reading text in images. In International Conference on Document Analysis and Recognition, pp. 947952, Sydney, Australia, 2019. OpenAI. Gpt-5: unified multimodal model, 2025. URL https://openai.com/gpt-5. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, volume 139, pp. 87488763, virtual, 2021. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset of clip-filtered 400 million image-text pairs. CoRR, abs/2111.02114, 2021. Christoph Schuhmann, Andreas Kopf, Richard Vencu, Theo Coombes, and Romain Beaumont. Laion coco: 600m synthetic captions from laion2b-en, 2022. URL https://laion.ai/blog/ laion-coco/. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-OKVQA: benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, volume 13668, pp. 146162, Tel Aviv, Israel, 2022. Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. Kvqa: Knowledgeaware visual question answering. In AAAI Conference on Artificial Intelligence, volume 33, pp. 88768884, 2019. Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang, Pei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang Bai. Icdar2017 competition on reading chinese text in the wild (rctw-17). In International Conference on Document Analysis and Recognition, volume 1, pp. 14291434. IEEE, 2017. Mustafa Shukor, Louis Bethune, Dan Busbridge, David Grangier, Enrico Fini, Alaaeldin El-Nouby, and Pierre Ablin. Scaling laws for optimal data mixtures. CoRR, abs/2507.09404, 2025a. Mustafa Shukor, Enrico Fini, Victor Guilherme Turrisi da Costa, Matthieu Cord, Joshua M. Susskind, and Alaaeldin El-Nouby. Scaling laws for native multimodal models. CoRR, abs/2504.07951, 2025b. 16 Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In European Conference on Computer Vision, volume 12347, pp. 742758, Glasgow, UK, 2020. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 83178326, Long Beach, CA, USA, 2019. Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, et al. Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt. In International Conference on Document Analysis and Recognition, pp. 15571562, 2019. Chenxin Tao, Shiqian Su, Xizhou Zhu, Chenyu Zhang, Zhe Chen, Jiawen Liu, Wenhai Wang, Lewei Lu, Gao Huang, Yu Qiao, and Jifeng Dai. Hovle: Unleashing the power of monolithic visionlanguage models with holistic vision-language embedding. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 1455914569, Nashville, TN, USA, 2025. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. Chameleon Team. Chameleon: mixed-modal early-fusion foundation models. CoRR, abs/2405.09818, 2024. Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. Michael Tschannen, Alexey A. Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier J. Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual visionlanguage encoders with improved semantic understanding, localization, and dense features. CoRR, abs/2502.14786, 2025. Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. CoRR, abs/1601.07140, 2016. Han Wang, Yongjie Ye, Bingru Li, Yuxiang Nie, Jinghui Lu, Jingqun Tang, Yanjie Wang, and Can Huang. Vision as lora. CoRR, abs/2503.20680, 2025a. Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: prompting GPT-4V for better visual instruction tuning. CoRR, abs/2311.07574, 2023. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024a. 17 Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. CoRR, abs/2508.18265, 2025b. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: next-token prediction is all you need. CoRR, abs/2409.18869, 2024b. Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, and Dahua Lin. Videorope: What makes for good video rotary position embedding? CoRR, abs/2502.05173, 2025. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. CoRR, abs/2506.18871, 2025. xAI. Grok-1.5 vision preview, 2024. URL https://x.ai/blog/grok-1.5v. xAI. Grok 3: xAIs flagship ai model, 2025. URL https://x.ai/news/grok-3. Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Zunnan Xu, Zhaoyang Zhang, Yixiao Ge, Xiu Li, and Ying Shan. Haploomni: Unified single transformer for multimodal video understanding and generation. CoRR, abs/2506.02975, 2025. Rui Yan, Lin Song, Yicheng Xiao, Runhui Huang, Yixiao Ge, Ying Shan, and Hengshuang Zhao. Haplovl: single-transformer baseline for multi-modal understanding. CoRR, abs/2503.14694, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: preliminary explorations with gpt-4v(ision). CoRR, abs/2309.17421, 2023. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions. In European Conference on Computer Vision, volume 9906, pp. 6985, Amsterdam, The Netherlands, 2016. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. CoRR, abs/2309.12284, 2023. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: evaluating large multimodal models for integrated capabilities. In International Conference on Machine Learning, Vienna, Austria, 2024. Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, Hao Luo, Jiasheng Tang, Fan Wang, and Yi Yang. Lumos-1: On autoregressive video generation from unified model perspective. CoRR, abs/2507.08801, 2025. Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang Mu, and Shi-Min Hu. large chinese text dataset in the wild. Journal of Computer Science and Technology, 34(3):509521, 2019. 18 Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 95569567, Seattle, WA, USA, 2024. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In IEEE International Conference on Computer Vision, pp. 1194111952, Paris, France, 2023. Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Advances of Neural Information Processing Systems, pp. 1236012371, Vancouver, BC, Canada, 2019. Bo Zhao, Boya Wu, and Tiejun Huang. SVIT: scaling up visual instruction tuning. CoRR, abs/2307.04087, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances of Neural Information Processing Systems, 36:4659546623, 2023. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. CoRR, abs/2504.10479, 2025."
        },
        {
            "title": "USAGE OF LARGE LANGUAGE MODELS",
            "content": "During manuscript preparation, large language models were used solely as writing assistants. They helped to check grammar, refine sentence structure, and provide style alternatives. All content related to methodology, experiments, and conclusions was developed entirely by the authors. LLM outputs were reviewed critically, and only human-verified edits were incorporated into the final text. A.1 SUPERVISED FINE-TUNING DATASETS Table 4: Dataset summary in supervised fine-tuning stage. Task Dataset Captioning General QA Science Chart Mathematics Knowledge OCR Document Grounding Conversation Text-only TextCaps (en) Sidorov et al. (2020), ShareGPT4V (en&zh) Chen et al. (2024b) VQAv2 (en) Goyal et al. (2017), GQA (en) Hudson & Manning (2019), OKVQA (en) Marino et al. (2019), VSR (en) Liu et al. (2023a), VisualDialog (en) Das et al. (2017) AI2D (en) Kembhavi et al. (2016), ScienceQA (en) Lu et al. (2022a), TQA (en) Kembhavi et al. (2017) ChartQA (en) Masry et al. (2022), MMC-Inst (en) Liu et al. (2023c), DVQA (en) Kafle et al. (2018), PlotQA (en) Methani et al. (2020), LRV-Instruction (en) Liu et al. (2023b) GeoQA+ (en) Cao & Xiao (2022), TabMWP (en) Lu et al. (2022b), MathQA (en) Yu et al. (2023), CLEVR-Math/Super (en) Lindstrom & Abraham (2022); Li et al. (2023c), Geometry3K (en) Lu et al. (2021) KVQA (en) Shah et al. (2019), A-OKVQA (en) Schwenk et al. (2022), ViQuAE (en) Lerner et al. (2022), Wikipedia (en&zh) He et al. (2023) OCRVQA (en) Mishra et al. (2019), InfoVQA (en) Mathew et al. (2022), TextVQA (en) Singh et al. (2019), ArT (en&zh) Chng et al. (2019), COCO-Text (en) Veit et al. (2016), CTW (zh) Yuan et al. (2019), LSVT (zh) Sun et al. (2019), RCTW-17 (zh) Shi et al. (2017), ReCTs (zh) Liu et al. (2019), SynthDoG (en&zh) Kim et al. (2022), ST-VQA (en) Biten et al. (2019) DocVQA (en) Clark & Gardner (2018), Common Crawl PDF (en&zh) RefCOCO/+/g (en) Yu et al. (2016); Mao et al. (2016), Visual Genome (en) Krishna et al. (2017) LLaVA-150K (en&zh) Liu et al. (2023d), LVIS-Instruct4V (en) Wang et al. (2023), ALLaVA (en&zh) Chen et al. (2024a), Laion-GPT4V (en) LAION (2023), TextOCR-GPT4V (en) Jimmycarter (2023), SVIT (en&zh) Zhao et al. (2023) OpenHermes2.5 (en) Teknium (2023), Alpaca-GPT4 (en) Taori et al. (2023), COIG-CQIA (zh) Bai et al. (2024), ShareGPT (en&zh) Zheng et al. (2023) A.2 IMPLEMENTATION DETAILS Table 5: Implementation details in the pre-training, mid-training and supervise fine-tuning. Configuration Resolution Optimizer Optimizer hyperparameters Learning rate schedule Peak learning rate Min learning rate ratio Weight decay Training steps Warm-up steps Max sample length Global batch size Text-only ratio Numerical precision β1 = 0.9, Mid-Training 2562 2, 0482 AdamW β2 = 0.999, cosine with min lr 4e5 0.1 Supervised Fine-Tuning 2562 2, eps = 1e8 cosine decay 5e5 0.01 50k 200 8, 192 1, 200 0.3 bfloat16 6k 200 8, 192 650 Pre-Training 2562 1, cosine with min lr 8e4 0.05 190k 2k 8, 192 2, 560 0.3 20 A.3 LIMITATION AND DISCUSSION In this study, we innovate network architectures and training strategies for efficiently building native vision-language models. The full promise of NEO has remained largely untapped, hindered by scarce training data and limited computational resources, especially in knowledge-intensive and OCR-focused domains. Yet, strikingly, our NEO rivals state-of-the-art VLMs despite these severe constraints. We envision subsequent directions of NEO for the native VLM community as follows: Contextual relevance to recent advancements. Recent models such as Qwen3VL highlight concepts that resonate with our design choices, including dense linking of visual-language features, relative positional encodings, and architectural details like patch embedding and bias. In particular, the DeepStack approach underscores the importance of establishing strong pixel-word associations from the earliest stages, reinforcing the significance of densely integrated visual-language representations. Maximizing the potential via large investment. It is in great demand for continuously investing substantial resources, especially during the pre-training stage, to fully unlock NEOs performance and approach the upper bound of the native model. At the same time, selectively open-sourcing key components during intermediate development can reduce follow-up training costs for future researchers and attract more research to native visual-language models. Moreover, the fundamental models from this work provide valuable baseline for advancing reinforcement learning research. Explorations of full-spectrum model capacities. Expanding the full model sizes remains critical factor in advancing various real-world applications. Even with limited resources, NEO-2.2B closely matches those of modular visual-language models with equivalent capacity, suggesting that the design philosophy of models in the 0.6 to 8 billion parameter range has matured. Such architectures not only achieve high performance but also facilitate the deployment of lightweight models at the edge, which is crucial for scenarios with limited computational resources or strict real-time requirements. Upgrading architectures and applications. To date, our work has focused on dense models for image-text understanding, while sparse divide-and-conquer architecture is simultaneously under active development. Notably, we regard NEO not merely as an autoregressive VLM but as new paradigm for visual-language intelligence. Its principle is to leverage end-to-end training within unified architecture, eliminating manually imposed biases and scaling-up complexities by allowing data and models to dictate the learning process. Besides, our efforts are designed not merely to improve performance but to establish definitive baseline for visual-language generation, long video understanding, and embodied AI. Crucially, NEOs architecture systematically integrates the demands of video generation and related tasks, including attention mechanisms and rotary positional encodings, from the ground up. Although currently focused on text and images, NEO is poised to push the boundaries of what is possible across wide spectrum of application scenarios and input modalities. Constrained by current text corpus and computational resources, we are unable to train fully native model entirely from scratch without initialization from an existing LLM. This limitation also hinders our ability to mitigate potential biases arising from the dominance of the language modality. Despite these challenges, our NEO extends beyond providing reusable pre-buffer that lowers the cost of adapting advanced LLMswith updated weights and stronger capabilitiesinto VLMs under limited budgets. More importantly, NEO reveals the potential performance ceiling of native VLM architectures and provides valuable insights for future research on de novo multimodal training."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "SenseTime Research",
        "Xian Jiaotong University"
    ]
}