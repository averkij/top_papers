{
    "paper_title": "MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft",
    "authors": [
        "Junliang Guo",
        "Yang Ye",
        "Tianyu He",
        "Haoyu Wu",
        "Yushu Jiang",
        "Tim Pearce",
        "Jiang Bian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate $4$ to $7$ frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 8 8 3 8 0 . 4 0 5 2 : r a"
        },
        {
            "title": "MineWorld",
            "content": "MINEWORLD: REAL-TIME AND OPEN-SOURCE INTERACTIVE WORLD MODEL ON MINECRAFT Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, Jiang Bian Microsoft Research {junliangguo,v-yangye,tianyuhe, v-haoywu}@microsoft.com https://aka.ms/mineworld"
        },
        {
            "title": "ABSTRACT",
            "content": "World modeling is crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, realtime interactive world model on Minecraft, an open-ended sandbox game which has been utilized as common testbed for world modeling. MineWorld is driven by visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate 4 to 7 frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released."
        },
        {
            "title": "INTRODUCTION",
            "content": "World models have been extensively studied for their potential ability to simulate and interact with various environments and actions taken by humans or agents (Ha & Schmidhuber, 2018; Yang et al., 2023). These models provide computational framework that empowers intelligent agents to perceive surroundings, receive controls, and predict consequences. Thus, world models reduce reliance on real-world trials and automate complex tasks across industries, such as serving as game engine (Valevski et al., 2024; Bruce et al., 2024; Decart et al., 2024; Kanervisto et al., 2025) or planner in reinforcement learning system (Hafner et al., 2019; Wu et al., 2024; Agarwal et al., 2025), illustrating their ability to improve decision-making, enable safe exploration, and facilitate scalable learning. Recently, video generative models have shown promising ability to learn commonsense knowledge from raw video datasets, ranging from physical laws in the real world (Brooks et al., 2024) to object interactions in games (Parker-Holder et al., 2024; Kanervisto et al., 2025), laying the foundation for their use as real-world simulators. However, foundamental challenges exist in ensuring the efficiency and controllability of these models, which are both crucial features for desired world model. The efficiency bottleneck lies in the generation target of these video generative models, i.e., the latent videos representation encoded by visual tokenizers, consists of large number of tokens (e.g., 40k to 160k tokens for 16 frames with SoTA tokenizers (Yang et al., 2024; Tang et al., 2024; Wang et al., 2024)). That leads to substantial computational costs during inference, making real-time interactions with the model major challenge. Furthermore, the controllability requires the model to generate accurate outcomes based on given control signal, which is challenging to evaluate due to the diverse nature of these signals. For instance, video generative Equal contribution. Correspondence to Junliang Guo."
        },
        {
            "title": "MineWorld",
            "content": "Figure 1: Illustrations of MineWorld model architecture. Visual and action tokenizers convert game states and actions into discrete tokens, which are concatenated and fed into Transformer decoder as the input. The Transformer is then trained with an autoregressive objective. models may be conditioned on textual descriptions (Brooks et al., 2024), video demonstrations (Zhang et al., 2025), and numeric features such as robotic arm movements (Wu et al., 2024), yet standardized metric to quantify how well the generated results adhere to the input signals remains lacking. In this work, we propose MineWorld, real-time, open-source interactive world model on the game Minecraft. It is built upon an autoregressive Transformer, designed to overcome the challenges of the efficiency and controllability in video-based world modeling. MineWorld explicitly learns the correlation between visual states and control signals by tokenizing both game scenes and actions into discrete representations, which are concatenated interleaved as the input to the model. To achieve real-time interactions between the model and humans1, we introduce novel parallel decoding algorithm that significantly accelerates the autoregressive generation of Transformer. Instead of sequentially predicting each token step by step, our method exploits the dependencies between spatially adjacent tokens, allowing certain token groups to be predicted in parallel. This optimization results in over more than 3 speedup compared to standard autoregressive decoding, without sacreficing the quality of results. Equipped with this decoding algorithm, MineWorld is able to generate 4 to 7 frames in one second, making the real-time interaction between human and world model feasible. In addition, to assess the controllability of MineWorld, we propose new evaluation metrics that extend beyond conventional video quality assessments. We first transform the actions in Minecraft to discrete tokens in one vocabulary. Then, after output videos are generated conditioned on previous frames and actions, we utilize an inverse dynamic model (IDM) (Baker et al., 2022) to predict an action between consecutive generated frames. This predicted action can be treated as the executed action according to generated videos, while the input action is the ground truth one which serves as the condition. Therefore, the accuracy between the two kinds of actions reflect the controllability of this generative model. By addressing both efficiency and controllability, MineWorld advances the field of world modeling, offering the first high-quality and efficient framework for real-time simulation and interaction. To empower further research in this direction, we release our code and model weights. In summary, our contributions are threefold: MineWorld, an open-sourced, real-time interactive world model powered by an autoregressive Transformer, establishes new benchmark for world modeling. novel parallel decoding algorithm that brings significant speedup in inference over standard autoregressive decoding, while maintaining high-quality generation results. New evaluation metrics for assessing the controllability, in order to validate whether the generated sequences faithfully adhere to control signals."
        },
        {
            "title": "2 FRAMEWORK",
            "content": "2.1 OVERVIEW We will introduce the proposed MineWorld framework in this section. We start with the problem definition of the studied task. Denote xi as the visual game state from Minecraft at the i-th timestep, and ai as the action taken 1We provide the definition of real-time interaction for our world model in Section 2.3."
        },
        {
            "title": "MineWorld",
            "content": "by users on the state, then the next game state xi+1 represents the future when taking ai on xi. The objective of our world model is to predict future game states based on past observations x<i and the current action ai, modeling the following conditional distribution: p(xi+1x<i, ai). (1) We build MineWorld on the autoregressive Transformer (Vaswani et al., 2017), considering its scaling behavior (Kaplan et al., 2020; Brown et al., 2020) and good controllability (as will be shown in Section 4). As illustrated in Figure 1, the architecture consists of two modules, the tokenizers to convert videos and actions to discrete tokens, and Transformer decoder to take the sequence of tokens as input and train in an autoregressive way. We introduce the details in the following."
        },
        {
            "title": "2.2 ARCHITECTURES",
            "content": "Tokenizers The model input contains two different modalities, i.e., the game playing videos consist of sequences of states xi, and actions consist of mouse and keyboard inputs. Therefore, we design different tokenizers to convert them into discrete tokens respectively. For game videos, we train VQ-VAE (Van Den Oord et al., 2017; Esser et al., 2021) as the visual tokenizer. Considering the interleaved manner of the game states and actions, we utilize an image-level (i.e., with spatial compressions) VQ tokenizer to convert each state to tokens independently, and leave the utilization of video-level tokenizers (i.e., with both spatial and temporal compressions) (Tang et al., 2024) for future work. Specifically, we initialize the VQ tokenizer from from public pre-trained checkpoint (Patil et al., 2024) and then fine-tune it on the Minecraft dataset. The tokenizer has 16 compression rates on both the height and width. As result, for clip of game video that contains states, the VQ tokenizer converts it into sequence of quantized ids t, denoted as: = (x1, , xn), = (t1, , tc, tc+1, , t2c, t2c+1, tN ), (2) where = is the total length of the sequence, and is the number of ids to represent each state. The actions in Minecraft contains both continuous mouse movements that controls camera angles, and keyboard or mouse presses that represent discrete actions defined in the game engine such as forward and attack. To handle continuous movements, we follow previous practices (Baker et al., 2022) and quantize camera angles into discrete bins. For discrete actions, considering the mutual exclusive nature of certain action pairs (e.g., forward and backward cannot occur simultaneously), we categorize the actions into 7 exclusive classes, each represented by unique token. Additionally, we allocate 2 tokens to encode camera angles and introduce special tokens [aBOS] and [aEOS] to mark the boundaries of an action sequence. As result, each action is represented by sequence of 11 tokens, where each token corresponds to an action id from the complete action vocabulary. Dividing actions into exclusive classes not only helps reduce the sequence length of action tokens, but also makes it possible to validate the controllability of the model with classification based metrics, which will be introduced in Section 3. In conclusion, for each pair of game state and actions in the original input (xi, ai), the tokenizers will transfer them into flat sequence of discrete ids as: (tic+1, , t(i+1)c, [aBOS], tai 1 , , tai 9 , [aEOS]). (3) Transformer Decoder We design our Transformer following the LLaMA architecture (Touvron et al., 2023) equipped with root mean square layer normalization (Zhang & Sennrich, 2019) and Rotary Embeddings (Su et al., 2024). We train the model as traditional autoregressive decoder, where each token is predicted conditioned on all previous tokens, (cid:89) fθ(t) = p(tit<i). (4) i=1 Note that we treat the tokens of game states and actions equivalently, allowing the model to learn their interleaved structure. As result, the model jointly captures the conditional relationships between states and actions, enabling it to function as both policy model (i.e., predicting actions based on previous observations p(ai+1x<i+1)) and world model (i.e., predict future states as described in Equ (1)) in inference. While our primary focus in this paper is on the models performance as world model, we also present case studies in the experiments to demonstrate its potential as policy model."
        },
        {
            "title": "MineWorld",
            "content": "Figure 2: An illustration of two decoding algorithms. The game state is encoded into 12 tokens. The number in each token represents that in which decoding iteration it is generated. (a) Autoregressive decoding, which follows raster scanned order and generates each token sequentially. (b) Our proposed parallel decoding. For each generated token, the tokens in the adjacent rows and columns will be generated simutaneously in the next iteration. 2.3 PARALLEL DECODING For world model, it is crucial to generate real-time consequences w.r.t. the controling signals provided by the users. We provide the definition of real-time in our game scenario considering the Actions Per Minute (APM) (Wikipedia contributors, 2025) of professional players. Real-Time Interactive World Model APM is metric used to measure the number of in-game actions (e.g., keypresses, mouse clicks) player executes within one minute. Professional players in the game StarCraft typically have APMs around 250 to 300 in average, and 50 for beginners (Wikipedia contributors, 2025). We also conduct an in-house test on amateur game players and find that they have 150 APMs in average. Therefore, to develop world model that achieves real-time interactions with users, it should generate more than 2 Frames Per Second (FPS) to keep up with amateur game players and 5 FPS for professional ones. While visual autoregressive models (Liu et al., 2024; Yu et al., 2023; Kondratyuk et al., 2023) can provide high-fidelity results for image and video generation tasks, their sequential nature poses significant bottleneck on the efficiency when generating high-resolution, long-duration videos. Previous works (Bai et al., 2024; Yu et al., 2023) generate visual tokens following fixed raster-scan order (i.e., from left-to-right and top-to-bottom) or random orders (Chang et al., 2022; Yu et al., 2024), without leveraging the inherent spatio and temporal dependencies in images and videos. To facilitate the real-time interaction of our world model, we utilize Diagonal Decoding (Ye et al., 2025), training-free parallel decoding algorithm by leveraging the spatial dependencies between adjacent image tokens, providing significant speedup compared to naive autoregressive decoding with negligible quality degradation. Specifically, our method processes tokens across different rows and columns simultaneously, as shown in Figure 2. Let xi,j denote one of the generated tokens at row and column in the current game state, then in the next step, both tokens xi,j+1 and xi+1,j will be generated at the same time. Denote the height and width of the patchified game state as and w, then the theoretical speedup ratio of our parallel decoding versus autoregressive decoding can be written as: = + 1 . (5) From the above equation, we can observe that the larger the image resolution, the faster our decoding speed. Our method utilizes the spatial redundency of adjacent tokens. However, due to the discrepency between training and inference, the speedup brings some performance degradation to the result. To solve the problem, we propose to fine-tune the autoregressively pre-trained model, by replacing the standard causal attention mask with the"
        },
        {
            "title": "MineWorld",
            "content": "mask that aligns with our parallel decoding algorithm. As result, our model is able to achieve real-time interaction frequency while maintaining good quality of generation results. Discussion with Related Works Several prior works have explored parallel decoding in visual generative models. Lformer (Li et al., 2023) generate L-shaped token blocks in each iteration but requires training the model from scratch. More recently, ZipAR (He et al., 2024) propose parallel decoding algorithm for image generation by leveraging adjacent token dependencies. In contrast, our approach focuses on video generation, which presents additional challenges due to error accumulation across frames. Despite this increased complexity, our method achieves higher acceleration ratios relative to the autoregressive baseline, demonstrating the effectiveness of our parallel decoding strategy for real-time interactive applications."
        },
        {
            "title": "3 EVALUATION",
            "content": "To comprehensively evaluate the performance of world models in the Minecraft environment, we assess both the video quality and controllability quality of generation results. We introduce the construction and preprocessing of the dataset first. 3.1 DATASET CONSTRUCTION AND PREPROCESSING We utilize the VPT dataset (Baker et al., 2022) which consists of the pairs of recorded game playing videos and corresponding actions, where each frame is accompanied with the keyboard and mouse actions taken at the same time. We filter out the frames with no recorded action and also the ones when GUI is open to eliminate its influence of results. We split long videos into short clips with 16 frames considering the maximum context length of the model, and divide them into training, validation and test sets randomly. As result, we train the model on 10M video clips (i.e., 160M frames) and validate / test on 0.5k / 1k clips. For each original video, we resize its resolution from 360 640 to 224 384, to reduce the computation cost while keeping the original aspect ratio for better visual quality. As introduced in Section 2.2, we adopt VQ-VAE with 16 spatial compression ratio and 8k codebook size to transform each frame into 14 24 patches, and finally sequence of 336 discrete tokens. In addtion to the 11 action tokens encoded by our action tokenizer, we represent each pair of game state and action as 347 tokens. As result, for each training sample which contains 16 pairs of them, we transform it into 5.5k discrete tokens after preprocessing. And in total there is 55B tokens in the training set. 3.2 EVALUATION METRICS We utilize two types of metrics to assess the visual quality and the controllablility of the results generated by our model. For visual quality, we employ several common metrics including Frechet Video Distance (FVD) (Unterthiner et al., 2018), Peak Signal-to-Noise Ratio (PSNR) (Hore & Ziou, 2010), Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018), and Structural Similarity Index Measure (SSIM) (Wang et al., 2004). Controllablility indicates to what extent the generated game states align with the provided actions and previous states, which is crucial feature for an accurate world model. To evaluate it, we utilize an Inverse Dynamics Model (IDM) (Baker et al., 2022) to infer the generated actions from the generated states and previous states. Specifically, the IDM is bidirectional model which takes game states as input and predicts the actions between them. We utilize highly accurate pre-trained IDM (Baker et al., 2022) which achieves 90.6% accuracy on keypresses. However, due to the imbalanced nature of action labels (e.g., forward and attack appear much more frequently than use and drop) as well as the mixture of discrete and continuous actions (e.g., keyboard presses and camera movements), it is non-trival to design accurate and comprehensive evaluation metrics on the controllablility. We propose two kinds of metrics to solve this problem. Discrete Action Classification Following the dividing policy in the action tokenizer, actions can be grouped into 9 classes, where 7 of them represent discrete action classes and the other 2 represent camera movement angles. For discrete classes, each one of them contains two or three exclusive actions such as (forward, backward) and (left, right). We provide the full grouping results in Table 4. Then, by taking the provided action as the ground truth and the predicted action from IDM as the prediction, we can utilize commonly utilized classification metrics including precision, recall and F1 score to evaluate the classification accuracy. We report both the macro scores to reduce the effect of imbalanced labels."
        },
        {
            "title": "MineWorld",
            "content": "Table 1: Generation result of Oasis (Decart et al., 2024) and different scales of our models.FPS indicates the number of frames generated per second by the model. P, R, F1 denote the classification precision, recall and F1 scores on discrete actions. L1 indicates the camera control loss. Method Param. FPS Oasis MineWorld 500M 300M 700M 1.2B 2.58 5.91 3.18 3.01 0. 0.72 0.72 0.76 0.44 0.71 0.71 0.73 F1 0. 0.70 0.70 0.73 L1 2.60 1.03 1.04 1.02 FVD LPIPS SSIM PSNR 377 246 231 227 0.53 0.45 0.44 0. 0.36 0.38 0.38 0.41 14.38 15.13 15.32 15.69 Camera Movement. Camera movement is represented by the players rotational angle and handled separately from action prediction. Following the configuration in VPT (Baker et al., 2022), we discretize the camera rotation along both the and axes into 11 bins, each representing specific range of angles. This design provides finer granularity for small movements and coarser bins for larger rotations, balancing precision and range. To evaluate the controllability of camera movements, we compute the L1 loss between the predicted and ground truth camera bins. 3. IMPLEMENTATION DETAILS For the visual tokenizer, we initialize it from pre-trained checkpoint (Patil et al., 2024) and then fine-tune it on the preprocessed VPT training data. We show the reconstruction results of visual tokenizers in Appendix C, and we can find that after fine-tuning, the tokenizer achieves good reconstrution quality on the validation set of VPT. For the Transformer decoder, we adopt LLaMA (Touvron et al., 2023) architecture considering its advantageous to large-scale modeling. We experiment with different sets of hyper-parameters which result in models with 300M, 700M and 1.2B parameters separately. The vocabulary size of the image tokenizer is 8192, plus 70 ids from the action vocabulary, and thus the final vocabulary size of our model is 8262. We use the Adam optimizer (Kingma & Ba, 2015) with cosine decay learning rate scheduler to train the model. The training is conducted on 32 NVIDIA 40G A100 GPUs with PyTorch (Paszke et al., 2019) for 200k steps. We introduce more training details in Appendix B."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We present the results of MineWorld in this section. We compare our model with Oasis (Decart et al., 2024), an open-sourced diffusion based world model on Minecraft. 4.1 MAIN RESULTS We provide the results of our models and the Oasis baseline in Table 1. Both the video quality and controllability accuracy scores are presented. We use the open-sourced 500M model and inference code from Oasis to obtain its results. Compare with it, all of our models achieve better results on both aspects of evaluation metrics. Note that while they claim good video quality and fast inference speed in their blog 2, it requires larger model equipped with dedicated chips and inference engine, which are public inaccessible. On the other hand, our proposed parallel decoding algorithm is orthogonal to hardware and system level optimizations, and thus have the potential to achieve faster inference speed if strengths from both sides are combined. From the comparisons between different model scales of MineWorld, we observe clear scaling behaviour. Larger models achieve better performance on both the controllability and video quality. As for the inference speed, thanks to the proposed parallel decoding algorithm, even the largest 1.2B model reaches FPS of 3, i.e., generating 3 game states in one second and thus can respond to 180 Actions Per Minute (APM), an interaction frequence for most amateur game players. The most efficient 300M model achieves an APM around 360, which is able to interact with top professional players. 4.2 ANALYSIS 2https://oasis-model.github.io"
        },
        {
            "title": "MineWorld",
            "content": "Table 2: The performance of different scales of MineWorld models with default autoregressive decoding and the proposed parallel decoding. w/ or w/o FT indicate results after fine-tuning the autoregressive pre-trained model with the parallel attenion mask or not. We choose several main metrics on the efficiency and effectiveness to make the table compact. Param. Decoding PSNR FVD FPS F1 300M 700M 1.2B AT Parallel w/ FT Parallel w/o FT AT Parallel w/ FT Parallel w/o FT AT Parallel w/ FT Parallel w/o FT 2.00 5.91 5.91 1.08 3.18 3.18 0.89 3.01 3.01 0.70 0.70 0.69 0.73 0.71 0.70 0.72 0.73 0. 15.63 15.13 14.98 15.74 15.32 15.27 16.06 15.69 15.30 223 246 275 210 231 247 203 227 The Effect of Parallel Decoding We compare the performance of autoregressive decoding and the proposed parallel decoding in Table 2. The conclusions are twofold. Firstly, larger models perform better with parallel decoding even without fine-tuning. For example, the 1.2B MineWorld model achieves 3 speedup in inference latency with parallel decoding, while preserving comparable performance in both controllability and video quality relative to autoregressive decoding. Secondly, fine-tuning with the parallel attention mask consistently enhances the performance of parallel decoding. For smaller models, such as the 300M variant, fine-tuning enables the model to match the autoregressive baseline, while still benefiting from 3 speedup in inference latency. These results suggest an exciting direction for future research: training the model from scratch with the parallel attention mask may lead to faster convergence and lower overall training costs. The Metrics on Controllability To validate that the metrics proposed in Section 3.2 accurately measure the controllability of the model, we conduct human evaluation and calculate the correlation coefficient between the proposed metrics and humane evaluation. Specifically, we sample 20 game video clips from the test set, and invite 5 experienced game players to score each video clip from the aspect of action following. Each participant was presented with the pair of game states and their corresponding actions, and asked to rate each sample on scale of 1 to 5. We calculated the average score as the final result. The pearson correlation coefficient is calculated over all samples. Table 3: The correlation between the proposed metrics and human evaluation on controllability. F1 Human p-value 0.81 4.21 0.56 0. We conduct experiments on the 700M model with autoregressive decoding, and list the results in Table 3, where represents the pearson correlation coefficient and p-value represents the corresponding probability value. As result, the classification based evaluation metric has significant positive correlation with human evaluation, showing the correctness of the proposed metrics. 4.3 CASE STUDY We provide case studies on MineWorld in this section, to demonstrate the controllability and video quality of the model. All results are generated by the 700M model with autoregressive decoding. General Capability We show several general capability of MineWorld in Figure 3, where the initial game state is provided, and the model generates subsequent states based on corresponding actions. In the first example, the actions guide the model to open door and walk outside. The model successfully generates the door-opening process and accurately renders the unseen outdoor environment. The second example depicts the process of chopping wood, where the model captures fine-grained details, such as the woods cross-section and the explosion effect when the chop is complete. In the third case, house appears in the initial frame, and the camera first pans left and then returns to the right. The model correctly regenerates the same house with detailed fidelity when the camera returns."
        },
        {
            "title": "MineWorld",
            "content": "Figure 3: Case study on MineWorld 700M model. The first game state and actions in following steps are provided as input, based on which the model generates consequent game states. For more cases and videos, please visit our project page. Figure 4: Case study on the controllability of MineWorld. Providing the same initial game state and different actions, the model generates different results correspondingly. These cases show that MineWorld learns foundamental physical knowledge of Minecraft, and can generate accurate reactions when receiving diverse actions. Besides, the strong video generation capacity empowers the model generates high-fidelity, coherent and consistent video results. Controllability We illustrate the controllability of MineWorld in Figure 4, which shows the different generation results start from the same game state but with different actions. As result, MineWorld accurately generates responses w.r.t each action. Figure 5: Case study on MineWorld as gaming playing agent. By providing several initial game states and actions (splited by the dashed line), MineWorld continues to play the game by itself through generating future game states and actions in an iterative manner."
        },
        {
            "title": "MineWorld",
            "content": "Serve as an Agent Since MineWorld predicts both game state tokens and action tokens during training, it naturally acquires the capabilities of both world model and policy model. This dual functionality enables MineWorld to serve as self-contained game agent, capable of playing the game autonomously. Specifically, given few initial game states and actions provided by users, the model can iteratively predict future states and actions, effectively simulating the long-horizon gameplay. As shown in Figure 5, MineWorld generates diverse and contextually appropriate actions, as well as accurate, high-fidelity game states. This demonstrates its potential not only as an interactive world simulator but also as foundation for developing game-playing agents."
        },
        {
            "title": "5 CONCLUSION AND LIMITATIONS",
            "content": "We present MineWorld, the first open-source, real-time interactive world model for Minecraft. Using an open dataset of game states and corresponding actions, we tokenize both modalities with separate tokenizers and train an autoregressive Transformer decoder via next-token prediction, using the interleaved sequence of state and action tokens as input. To support real-time interaction, we exploit the redundancy between adjacent image tokens and introduce parallel decoding algorithm, achieving consistent speedups over standard autoregressive decoding without sacrifacing the generation quality. Through our comprehensive evaluation pipeline, we show that MineWorld achieves not only the strong controllability and video quality, but the fast inference speed of 2 6 FPS which enables real-time interaction with professional game players. Limitations MineWorld is trained exclusively on Minecraft data at fixed, downsampled resolution, limiting its ability to generalize to other video domains (e.g., internet videos) or generate outputs at higher resolutions. The downsampling process may also lead to the loss of fine-grained details in game states. The maximum input length of the model is set to 5.5k tokens, corresponding to 16 state-action pairs. While MineWorld demonstrates strong temporal consistency within this range, it is not guaranteed when the distance between game states exceeds this range."
        },
        {
            "title": "REFERENCES",
            "content": "Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei Efros. Sequential modeling enables scalable learning for large vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2286122872, 2024. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:2463924654, 2022. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Video URL https://openai.com/research/ Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. generation models as world simulators. video-generation-models-as-world-simulators. 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. Decart, Quevedo Julian, McIntyre Quinn, Campbell Spruce, Chen Xinlei, and Wachen Robert. Oasis: universe in transformer. 2024. URL https://oasis-model.github.io/."
        },
        {
            "title": "MineWorld",
            "content": "Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019. Yefei He, Feng Chen, Yuanyu He, Shaoxuan He, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. Zipar: Accelerating autoregressive image generation through spatial locality. arXiv preprint arXiv:2412.04062, 2024. Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pp. 23662369. IEEE, 2010. Anssi Kanervisto, Dave Bignell, Linda Yilin Wen, Martin Grayson, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Tabish Rashid, Tim Pearce, Yuhan Cao, et al. World and human action models towards gameplay ideation. Nature, 638(8051):656663, 2025. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. Jiacheng Li, Longhui Wei, ZongYuan Zhan, Xin He, Siliang Tang, Qi Tian, and Yueting Zhuang. Lformer: Text-to-image generation with l-shape block parallel decoding. arXiv preprint arXiv:2303.03800, 2023. Xuantong Liu, Shaozhe Hao, Xianbiao Qi, Tianyang Hu, Jun Wang, Rong Xiao, and Yuan Yao. Elucidating the design space of language models for image generation. arXiv preprint arXiv:2410.16257, 2024. Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 2: large-scale foundation world model. 2024. URL https://deepmind.google/discover/blog/ genie-2-a-large-scale-foundation-world-model/. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. Suraj Patil, William Berman, Robin Rombach, and Patrick von Platen. amused: An open muse reproduction. arXiv preprint arXiv:2401.01808, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Anni Tang, Tianyu He, Junliang Guo, Xinle Cheng, Li Song, and Jiang Bian. Vidtok: versatile and open-source video tokenizer. arXiv preprint arXiv:2412.13061, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018."
        },
        {
            "title": "MineWorld",
            "content": "Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. In The Thirteenth International Conference on Learning Representations, 2024. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yuchi Wang, Junliang Guo, Xinyi Xie, Tianyu He, Xu Sun, and Jiang Bian. Vidtwin: Video vae with decoupled structure and dynamics. arXiv preprint arXiv:2412.17726, 2024. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. Wikipedia contributors. Actions per minute Wikipedia, The Free Encyclopedia. https://en. wikipedia.org/wiki/Actions_per_minute, 2025. [Online; accessed 25-February-2025]. Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. ivideogpt: Interactive videogpts are scalable world models. arXiv preprint arXiv:2405.15223, 2024. Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 1(2):6, 2023. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Yang Ye, Junliang Guo, Haoyu Wu, Tianyu He, Tim Pearce, Tabish Rashid, Katja Hofmann, and Jiang Bian. Fast autoregressive video generation with diagonal decoding. arXiv preprint arXiv:2503.14070, 2025. Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, and Jiang Bian. Video in-context learning: Autoregressive transformers are zero-shot video imitators. In The Thirteenth International Conference on Learning Representations, 2025."
        },
        {
            "title": "A MINECRAFT ACTION SPACE",
            "content": "A.1 DETAILS OF ACTION FOLLOWING METRIC Table 4: Classification Tasks and Their Labels Task Type Actions Labels"
        },
        {
            "title": "Binary Classification",
            "content": "forward, backward left, right sprint, sneak use attack jump drop forward, backward, null left, right, null sprint, sneak, null use, null attack, null jump, null drop, null The action space in Minecraft is inherently complex. To effectively validate the quality of action execution, we simplify the scenario by focusing on the 10 most common actions and camera movements. In addition, treating action prediction as simple multi-class classification task does not fully capture the complexity of predicting sequential actions. To address this, we divide the 10 actions into 3 triple classification tasks and 4 binary classification tasks, based on the exclusion relationships between the actions. In cases where the IDM model predicts two mutually exclusive actions at the same frame, we classify it as no action in line with common game behavior. Finally, we compute average macro precision, recall and F1 scores over all tasks. A.2 TASK-SPECIFIC ACTION ACCURACY Table 5: Precision, Recall, and F1 scores for the 700 MineWorld model across different tasks. Metric Forward-Backward Left-Right Sprint-Sneak Use Attack Jump Drop Precision Recall 0.807 0.773 0.782 0.808 0.745 0.771 0.765 0.788 0.750 0.742 0.648 0.682 0.729 0.749 0.736 0.760 0.847 0. 0.500 0.498 0.499 After defining the sub-classification tasks in Table 4, we can analyze the models performance in more detail with respect to action prediction. The table 5 below shows the precision, recall, and F1 score for each task in our 700M MineWorld model. The results indicate that the drop action performs significantly worse than the other actions, suggesting that it is more challenging for the model to learn."
        },
        {
            "title": "B MODEL CONFIGURATIONS",
            "content": "Table 6: The configuration of different size of models. Hidden dim MLP dim Num. Heads Num. Layers 300M 700M 1.2B 1024 2048 2048 4096 4096 8192 16 32 20 20 20 To validate the scaling behavior of the Transformer decoder, we train three different sizes of the model within the LLaMA architecture: 300M, 700M, and 1.2B. We tune the hidden dimension, intermediate dimension, and the number of layers to achieve different model sizes. The configuration of these models are listed in Table 6. The hyperparameters of the optimizer used to train the model are listed in Table 7."
        },
        {
            "title": "MineWorld",
            "content": "Table 7: Optimization hyperparameters. Hyperparameter Value"
        },
        {
            "title": "Learning rate scheduler\nLearning rate\nWarm up steps\nWeight decay\nOptimizer\nAdamW betas\nMaximum Positions",
            "content": "cosine 3e4 10000 0.1 AdamW (0.9, 0.95) 5376 Table 8: The reconstruction performance of the visual tokenizers on the validation set. Visual Tokenizer PSNR SSIM LPIPS rFID Amused (Patil et al., 2024) Ours 25.91 29.24 0.758 0.816 0.238 0.134 35.05 18."
        },
        {
            "title": "C MORE RESULTS",
            "content": "C.1 RECONSTRUCTION RESULTS OF VISUAL TOKENIZER We evaluate the reconstruction performance of the pre-trained Amused VQ-VAE (Patil et al., 2024) and the one after fine-tuned on the pre-processed VPT data. After fine-tuning, the performance is significantly improved, showing the necessity of this step."
        }
    ],
    "affiliations": [
        "Microsoft Research"
    ]
}