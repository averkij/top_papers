{
    "paper_title": "Motion-Aware Concept Alignment for Consistent Video Editing",
    "authors": [
        "Tong Zhang",
        "Juan C Leon Alcazar",
        "Bernard Ghanem"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a training-free framework bridging the gap between image-domain semantic mixing and video. Given a generated video and a user-provided reference image, MoCA-Video injects the semantic features of the reference image into a specific object within the video, while preserving the original motion and visual context. Our approach leverages a diagonal denoising schedule and class-agnostic segmentation to detect and track objects in the latent space and precisely control the spatial location of the blended objects. To ensure temporal coherence, we incorporate momentum-based semantic corrections and gamma residual noise stabilization for smooth frame transitions. We evaluate MoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS, and introduce a novel metric CASS (Conceptual Alignment Shift Score) to evaluate the consistency and effectiveness of the visual shifts between the source prompt and the modified video frames. Using self-constructed dataset, MoCA-Video outperforms current baselines, achieving superior spatial consistency, coherent motion, and a significantly higher CASS score, despite having no training or fine-tuning. MoCA-Video demonstrates that structured manipulation in the diffusion noise trajectory allows for controllable, high-quality video synthesis."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 4 0 0 1 0 . 6 0 5 2 : r MoCA-Video: Motion-Aware Concept Alignment for Consistent Video Editing Tong Zhang, Juan Leon Alcazar, Bernard Ghanem KAUST {tong.zhang.1, juancarlo.alcazar, bernard.ghanem}@kaust.edu.sa"
        },
        {
            "title": "Abstract",
            "content": "We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), training-free framework bridging the gap between image-domain semantic mixing and video. Given generated video and user-provided reference image, MoCAVideo injects the semantic features of the reference image into specific object within the video, while preserving the original motion and visual context. Our approach leverages diagonal denoising schedule and class-agnostic segmentation to detect and track objects in the latent space and precisely control the spatial location of the blended objects. To ensure temporal coherence, we incorporate momentum-based semantic corrections and gamma residual noise stabilization for smooth frame transitions. We evaluate MoCAs performance using the standard SSIM, image-level LPIPS, temporal LPIPS, and introduce novel metric CASS (Conceptual Alignment Shift Score) to evaluate the consistency and effectiveness of the visual shifts between the source prompt and the modified video frames. Using self-constructed dataset, MoCA-Video outperforms current baselines, achieving superior spatial consistency, coherent motion, and significantly higher CASS score, despite having no training or fine-tuning. MoCA-Video demonstrates that structured manipulation in the diffusion noise trajectory allows for controllable, high-quality video synthesis. Project page can be found here: https://zhangt-tech.github.io/MoCA-Page/"
        },
        {
            "title": "Introduction",
            "content": "Diffusion models[9, 22, 4, 36, 34] have revolutionized image synthesis and are rapidly being adapted for the tasks of controllable video generation and editing. Current works have established the basic mechanisms to extend the image diffusion denoising to temporal data: Video Diffusion Models[10] introduced coherent frame synthesis, and Latent Video Diffusion Models (LVDM)[8] enabled highfidelity long-form generation. Follow-up works enhanced the visual quality and temporal coherence of generated videos: Make-A-Video[23] improved text-to-video fidelity, while VideoCrafter1 & 2[1, 2] refined open-domain synthesis. In consequence, diffusion models have enabled broad range of creative video tasks: DynamiCrafter[31] and AnimateDiff[6] focus on image-to-video animation; subject-driven zeroshot editing[15, 13, 27] enables persona editing, generation; and affordance-insertion[16] embeds functional interaction in image editing, broadening the scope of automated content creation. Among them, the semantic mixing paradigm was first introduced in the image domain by MagicMix[17] and further extended by FreeBlend[35]. Recently MagicEdit[18] adapted the base pipeline for videos, thus enabling visual concept combination in the temporal domain. Traditional video editing methods, fuse visual elements through explicit frame-by-frame operations such as masking, inpainting, or key frame interpolation. In comparison, semantic mixing operates directly within the diffusion denoising process in order to achieve fine-grained, region-specific concept combination. Motivated by this targeted noise manipulation[17, 18, 20], we introduce MoCA-Video (Motion-Aware Concept Alignment in Video), training-free framework that extends semantic mixing to the video domain. MoCA-Video operates on pre-generated videos and is Preprint. Under review. Figure 1: Entity Blending With MoCA-Video. MoCA-Video handles diverse entity mixing tasks. Top row: shows the fusion of semantically distant categories, cat blends into an astronaut suite. Second row: slightly similar categories (both animals) blend, cat blends into perched bird adopting its pose. Third row: water-sports, mixing of kayak into surfer board resulting in kayaker rowing. Bottom row: Semantically similar categories fusion, an eagle blends into mallard duck. In each sequence, the leftmost image is the conditioned frame and the second column is original video input with fixed length of 16. conditioned by user-specified image, our method injects external semantic features into targeted objects, resulting in temporally consistent hybrid entities. MoCA-Video modulates the FIFO Diffusion[14] pipeline with targeted noise-space manipulation that injects new semantic features into pre-generated videos at precisely the right stage of denoising, where the main object has taken shape but yet fully stabilized. By identifying the region to edit in each latent and applying adaptive momentum correction and stabilization routine (see Section 3), our approach seamlessly blends the conditioned concept while preserving both the original motion dynamics and overall scene coherence. We compare MoCA-Video against current baselines both quantitatively and qualitatively. Quantitatively, we employ SSIM[28, 25], LPIPS(I/T)[33] and newly introduced metric CASS (Conceptual Alignment Shift Score) based on CLIP[21] , and its normalized variant that compensates for inherent task difficulty biases. Qualitatively, we demonstrate visual results on dataset derived from FreeBlend and extended by DAVIS entities, showing more visually compelling and temporally coherent semantic mixes than prior methods (see Section 4) Our contributions are summarized as follows: MoCA-Video Framework training-free pipeline for temporally coherent semantic mixing that injects user-defined image concepts into video latents via targeted late-stage noise interventions, guided by IoU based overlap maximization and smoothed with momentum-based corrections and gamma residual noise. Task Specific Metrics We proposed novel metric CASS (Conceptual Alignment Shift Score) and its normalized variant for semantic mixing evaluation in video, demonstrating MoCA-Videos superiority over FreeBlend[35] and AnimateDiff[6] using SSIM, LPIPS(I/T), and CASS."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Semantic Mixing and Video Concept Combination Semantic mixing, first introduced in the image domain through MagicMix [17] and later improved by FreeBlend [35], focusing on blending disparate concepts into coherent, novel objects. These methods exploit the denoising dynamics of diffusion models to factorize layout and content or apply staged latent interpolation for more stable blending. However, both approaches are limited to static images and overlook temporal consistency. MagicEdit [18] extends semantic editing into video by injecting prompt-driven features at specific diffusion stages. While it maintains motion to some degree, it lacks spatial control, personalization, and explicit combination of image-based semantics. Our work fills these gaps with training-free framework MoCA-Video that combines explicit image conditioning, noise-space mask tracking and motion correction to deliver spatially localized, semantically precise blends that remain coherent across time. 2.2 Identity-Preserved Video Generation One similar track of creative video generation is identity-preserved text-to-video generation (IPT2V), focusing on retaining reference subjects appearance while generating new motion. ID-Animator [7] enables zero-shot face-driven videos but often overfits to the input image. ConsisID[32] and EchoVideo[26] further enhances face detail but limited to human identities. In contrast to IPT2V, video semantic mixing focus on combine the reference subject into existing video subject rather than preserving single identity. 2.3 Video Inpainting Video inpainting methods extend region specific filling to the temporal domain, typically relying on optical flow, feature correspondences, or domain priors to maintain frame-to-frame consistency. DIVE[11] uses DINO features and LoRA-based identity registration for subject-driven edits, and ObjectMate[29] builds an Object Recurrence Prior to train on large supervised composition. TokenFlow[5] propagates self-attention tokens via nearest neighbor matching to enforce smooth transitions, while RAVE[12] shuffles latent and condition grids during denoising to maintain consistency upon unshuffling. Compare to video inpainting, MoCA-Video operates on region specific edits; however, instead of replacing the whole segmented area, it preserves the original features and blends them with new semantic content, enabling semantic mixing in videos."
        },
        {
            "title": "3 Methodology",
            "content": "MoCA-Video enables the semantic mixing of objects in video data by injecting visual features from conditioned object into target video entity, while preserving scene layout and motion consistency from the original video. The blending process in MoCA-Video begins with text-to-video model, we select VideoCrafter2 [2] which is initialized with the pretrained weights of Stable Diffusion 2.1 [22]. Then we select target object within that video and provide an image of reference object to guide the blending process. MoCA-Video outputs an edited version of the base video in which the visual features of the target object (already present in the video) have been combined with the conditioned object (absent from the base video). Rather than applying frame-by-frame post-hoc edits, MoCA-Video performs the feature combination directly in the latent noise space, recovered via DDIM inversion, so that the combination of concepts happens on the reversed latents from the original video. To better maintain cross-frame consistency, instead of adding pure Gaussian noise at the tail of FIFO pipeline [14] window, we employ FreeInit [30] to create the noise that contains the spatial content of the most recently denoised image. 3.1 Latent Space Tracking At its core, MoCA-Video blends the visual semantics of two objects by jointly denoising the latent representation (i.e. the noisy intermediate states in the diffusion process) of the target and the reference object. This process requires to first locate the target object in the base video. To this end, MoCA-Video is initialized with mask which encapsulates the target object in the latent space X, we denote this sub-region as xm. 3 Figure 2: MoCA-Video pipeline starts with base video (astronaut) and global text prompt guiding the entire diffusion; and reference image (cat) with conditioned prompt only introduced during noise manipulation phase to guide the modified latent diffusion along with global prompt. We first recover the latent noise trajectory via DDIM inversion. After selected timestep, where the object has emerged but not yet stabilized, we segment the target region with Grounded SAM2 and refine masks across frames via IoU-maximization. We then inject the reference images latent into those regions, apply adaptive motion correction to eliminate artifacts (arrows indicate the motion difference), and add small gamma-scale residual noise for smoothness. The colors shown are direct RGB conversions of the latent features without decoding. Finally, the combined latent is denoised back to X0, producing temporally coherent, semantically fused cat-astronaut video. The subregion xm is estimated with class agnostic segmentation model, thus yielding binary segmentation mask. Once xm has been initialized, we track this segmentation mask along the temporal sequence of latents using tracking by overlap maximization [3]. We define threshold τ and link temporally adjacent detections if their intersection over union (IoU) is higher than τ . This tracking strategy outputs sequence Xm = {xm 0 , xm t1} that defines the spatial location of the target object in the generated latents, where is the selected timestep that has emerged object but yet stabilized. Algorithm 1 provides an overview of the tracking by overlap maximization in sequence of latents. 1 , . . . , xm iou IoU(m, mt1) if iou > τ then mt Algorithm 1 Latent Tracking by Overlap Maximization Require: Sequence of latents {x0, x1, . . . , xt} Require: Initial Object mask m0 SEG(x0) Require: Set value for τ 1: for = 1 to 1 do 2: SEG(xt) 3: 4: 5: 6: 7: end if 8: 9: end for The sequence of segmentation masks Xm is then used as spatial guidance to blend the visual features of the target object and the source object. On the latent space of the generated video at time t, we define the visual feature blend xmix mt mt1 {Retain previous mask to avoid drift} over an object in the segmentation mask xm else as: = xt (1 xm xmix ) + xcond xm , where xcond is latent derived from the reference image. 4 3.2 Adaptive Motion Correction with Momentum MoCA-Video latent space tracking allows for consistent spatial localization of the target object along the entire video sequence. Nevertheless, this tracking scheme does not assure that the blended object will be temporaly consistent with its own appearance through the video sequence. Changes in the appearance and layout of the blended object must evolve smoothly as the video progresses, thus avoiding abrupt shifts, or motion derived artifacts. To address this requirement, we propose momentum-corrected DDIM denoising algorithm 2 to smooth out the abrupt shifts and artifacts introduced by feature injection. Algorithm 2 Momentum-Corrected DDIM Denoising Require: Noisy Latents {xt, xt1, . . . , x0}, Direction Vectors {dirt}, Momentum Decay β, Direction Scale λ, Base Weight κ0, Total Timesteps Ensure: Corrected Latents {ˆxt1} 1: Initialize vt 0 2: for = T, 1, . . . , 1 do 3: 4: 5: 6: 7: 8: 9: θ (xt) θ (xt) xt 1 αt ϵ(t) ˆx(DDIM) 0 αt dirt (cid:112)1 αt1 σ2 ϵ(t) x(DDIM) αt1 ˆx(DDIM) t1 0 gt xt xt1 + λ dirt vt β vt1 + (1 β) gt (cid:0)1 (cid:1) κt κ0 0 ˆx(DDIM) ˆx(corr) xt1 0 αt1 ˆx(corr) 0 + dirt + σt ϵt 10: 11: end for 12: Return {xt1}T t= + dirt + σt ϵt + κt vt {Updated predicted x0 at time step t} Momentum-Corrected DDIM. Recall that the standard DDIM denoising step [24] is given by: xt1 = αt (cid:18) xt (cid:124) 1 αt ϵ(t) αt (cid:123)(cid:122) ˆx(DDIM) 0 θ (xt) (cid:113) + (cid:19) (cid:125) 1 αt1 σ2 + σt ϵt. ϵ(t) θ (xt) (cid:125) (cid:124) (cid:123)(cid:122) dirt Here, ˆx(DDIM) 0 is the models estimate of the clean image at timestep t: ˆx(DDIM) = xt 1 αt ϵ(t) αt θ (xt) . In MoCA-Video, we introduce small, time-dependent momentum correction vt into the predicted clean image: ˆx(corr) 0 where the per-timestep weight, decays from 0 at = to κ0 = 2.0 at = 0. = ˆx(DDIM) 0 + κt vt, κt = κ0 (cid:16) 1 (cid:17) The momentum term vt is defined by gt = xt xt1 + λ dirt, vt = β vt1 + (1 β) gt, where xt and xt1 denote the noisy latents at successive timesteps, dirt is the models estimated motion-direction vector from standard DDIM, λ scales the influence of that directional cue, and β [0, 1] controls the momentum decay. 5 By adding κt vt to ˆx0, each denoising step is steered to respect accumulated frame-to-frame motion, reducing jitter and improving frame-to-frame coherence while preserving the learned noise schedule of DDIM. We further apply gamma residual noise to stabilize the denoising, which acts as regularizer to mitigate minor visual artifacts while preserving the underlying structure. Details are provided in the Supplementary Material."
        },
        {
            "title": "4 Experiments",
            "content": "To the best of the authors knowledge, MoCA-Video is the first effort to systematically study the problem of video entity blending. We begin the experimental assessment of our method by first creating video dataset designed for the entity blending task. Our entity blending dataset follows the broad super categories proposed in FreeBlend [35], i.e. Transports, Animals, Common Objects, and Nature. In [35] these categories were validated to capture the most salient real-world concepts. To provide broader range of objects, we extend this taxonomy using the object classes present in the DAVIS-16 video segmentation dataset [19]. We select this category set as the DAVIS dataset was designed such that there is small semantic overlap between the target visual entities, that is, the target object can not be trivially identified and located by their semantic class. We group the annotated entities in DAVIS-16 into an additional 16 sub-categories: Accessory, Animal, Appliance, Device, Electronic, Furniture, Instrument, Kitchen, Outdoor, Person, Sports, Vehicle etc. From these sixteen classes, we sample and pair concept combinations such that each scene mix draws from closely related categories, (e.g. cow and sheep), and semantically distant ones (e.g. astronaut and cat). We assemble total of 100 unique visual entity pairs. If conditioned object lacks the existing image, we generate follow the strategy of FreeBlend and generate one using Stable Diffusion 2.1. Each video is then produced via text prompt and associated with scalar strength parameter that controls the degree to which the reference images features are blended into the base video. Table 1 shows some representative source prompts, their corresponding target object, textual conditioning cues, and the chosen conditioning strengths. The conditioning strength determines the degree of feature bending, that is, how strongly the reference images characteristics are injected into the final output. Table 1: Dataset Examples: Source Prompts, Targeted Objects, Conditioned Prompt, and Conditioning Strength. Source Prompt cute teddy bear with soft brown fur and red bow tie blooming rose garden in full color under morning sunlight colorful tropical fish swimming in coral reef Object Teddy Rose Fish Conditioned Prompt the condition is hamster the condition is lavender the condition is dolphin γ 2.0 1.5 1.2 4.1 Evaluation We employ: (i) structural consistency, (ii) temporal consistency, and (iii) semantic integration quality. For both visual fidelity and motion smoothness, we use three well known perceptual metrics. SSIM [28, 25] measures frame-level alignment between the generated and base video, capturing spatial structure and luminance consistency. LPIPS-I [33] further quantifies how closely each generated frame matches its reference in terms of deep perceptual features, while LPIPS-T [33] evaluates temporal coherence by computing perceptual differences between adjacent frames in the generated video, thereby quantifying temporal smoothness. To quantify the effectiveness of semantic integration, we propose the CASS (Conceptual Alignment Shift Score) metric, novel CLIP-based [21] evaluation tool that measures how the generated videos semantic alignment shifts before and after fusion. By comparing CLIP embedding similarities with both the original prompt and the conditioned image, CASS captures the net movement toward the injected concept, providing clear, interpretable measure of blending success. To the best of our knowledge, we are the first to introduce dedicated CLIP-based metric for video semantic mixing. Although FreeBlend employs related CLIP-similarity measure, their approach is limited to compare the overall similarity before and after mixing, therefore it often misrepresent true mixing quality. We provide detail comparison and further analysis of these differences in the supplementary material. 6 Table 2: AnimateDiffV2V prioritizes spatial fidelity and motion smoothness at the expense of semantic blending; FreeBlend + DynamiCrafter offers moderate fidelity but weak concept injection; MoCA-Video achieves the best overall trade-off by preserving image and temporal quality while delivering substantially stronger semantic mixing. Method SSIM LPIPS-I LPIPS-T CASS relCASS FreeBlend + Dynamicrafter AnimateDiffV2V MoCA-Video (ours) 0.34 0.74 0.35 0.62 0.19 0. 0.16 0.01 0.11 1.47 0.68 4.93 0.37 0.57 1.23 We formally define CASS as follows: let Vorig denote the original ideo, Vfused the concept-mixed video. and Icond the conditioned image. E() is the frame-wise CLIP visual embedding, and () the textual embedding for the original video prompt. We define: CLIP-Torig = sim(E(Vorig), Torig) CLIP-Tfused = sim(E(Vfused), Torig) CLIP-Iorig = sim(E(Vorig), E(Icond)) CLIP-Ifused = sim(E(Vfused), E(Icond)) In an ideal semantic mixing outcome: two visually different entities will seamlessly blend into video sequence. This requirement implies that the starting condition of this process will have an original video Vorig that shows strong alignment to the source prompt, thus obtaining high CLIP-T score. Likewise, Vorig will have small resemblance to the conditioned image, which results in low CLIP-I score. After mixing, the fused video should reverse these alignments: CLIP-T should drop significantly, indicating the model has moved away from the original text prompt, while CLIP-I should rise significantly, showing that image-derived features have been integrated. This complementary shift in the CLIP scores, reducing the alignment with the text prompt, while gaining new-image alignment, is the essence of CASS, which quantifies the net semantic transformation. Therefore, we compute CASS as: CASS = (CLIP-Ifused CLIP-Iorig) (CLIP-Tfused CLIP-Torig) To normalize for varying baseline alignments, we compute percentage shifts in semantic similarity. Specifically, we divide each absolute change by its original score: CLIP-Ifused CLIP-Iorig CLIP-Iorig CLIP-Tfused CLIP-Torig CLIP-Torig rel_CLIP-T = rel_CLIP-I = , . These ratios express how much the image alignment increases and text alignment decreases. We define the relative Conceptual Alignment Shift Score as: relCASS = rel_CLIP-I rel_CLIP-T, which captures the net percentage shift towards the conditioned image and away from the source prompt. higher CASS signifies stronger semantic blending toward the conditioned image and away from the source prompt, and higher relCASS emphasizes this transformation relative to the original alignment, demonstrating robustness across varied case difficulties. 4.2 Baseline Comparisons We build two baseline pipelines for fair comparison. The first applies FreeBlend [35] to mix concepts in each image, then leverages DynamiCrafter [31] to reassemble these edited frames into temporally coherent video. The second uses AnimateDiffV2V[6], video-to-video diffusion model conditioned on the same source prompt, to directly generate edited sequences. These baselines isolate the benefits of our training-free, latent noisespace semantic mixing and motion-aware control strategy. 4.3 Sampling Approach Table 2 highlights that while AnimateDiffV2V preserves the original structure best (highest SSIM = 0.74) and delivers the smoothest motion (lowest LPIPS-T = 0.01), but it barely injects any new Figure 3: Visual comparison on the astronaut + cat blend. MoCA-Video delivers coherent, smoothly moving cat-astronaut fusion. AnimateDiffV2V fails to semantically integrate the cat. FreeBlend + DynamiCrafter treats each frame independently, producing static, non-semantic composites that flicker and break temporal consistency. semantics (CASS = 0.68, relCASS = 0.57). FreeBlend + DynamiCrafter achieves moderate perceptual fidelity (LPIPS-I = 0.62) but fails to introduce strong semantic shifts (CASS = 1.47, relCASS = 0.37) and shows higher jitter (LPIPS-T = 0.16). In contrast, MoCA-Video strikes the best balance: it matches or exceeds FreeBlend in image fidelity (SSIM = 0.35, LPIPS-I = 0.63), maintains low temporal error (LPIPS-T = 0.11), and produces significantly stronger semantic mixing (CASS = 4.93, relCASS = 1.23). This demonstrates MoCA-Videos ability to inject new concepts robustly while preserving spatial and temporal coherence. 4.4 Ablation Studies To better understand the impact of individual components in our framework, we conduct ablation studies on three key modules: (1) Overlap Optimization, (2) Adaptive Motion Correction, and (3) Gamma Residual Noise Stabilization. Each component was individually removed to assess its effect on the video quality, motion consistency, and semantic fidelity. Table 3 shows that removing IoU based overlap maximization has the greatest effect, with SSIM dropping from 0.35 to 0.28, LPIPS-T rising from 0.11 to 0.20 and CASS falls to 2.90, which resulting in erratic generations, objects missing and double fusion (qualitative results are shown in the supplementary material). Disabling adaptive motion correction increases jitter and spatial drift, emphasizing its role in preserving frame-by-frame trajectories control. Finally, omitting gamma residual noise introduces flicker and visual artifacts, highlighting its importance in smoothing fine details after latent manipulation. These performance drop in quantitative metics and the qualitative failure cases confirm that each module in MoCA-Video is critical for stable, temporally coherent semantic mixing. Table 3: Ablation results showing the effect of each module in MoCA-Video. Removing IoU-based overlap maximization causes the largest drop in spatial fidelity and semantic alignment, motion correction is critical for reducing jitter, and gamma residual noise improves fine-grained smoothness. Method Variant SSIM LPIPS-I LPIPS-T CASS relCASS Full MoCA-Video w/o Overlap Maximization (IoU) w/o Motion Correction w/o Gamma Residual Noise 0.35 0.28 0.30 0.32 0.11 0.20 0.18 0. 4.93 2.90 3.10 4.20 1.23 0.75 0.80 1.10 0.67 0.63 0.65 0.66 8 Figure 4: Ablation shows that each component is vital for smooth semantic mixing: without IoUguided mask control, the cat drifts and background artifacts appear; without motion correction, frames jitter and features misalign; and without gamma residual noise, edge textures flickerdemonstrating that all modules work together to maintain spatial fidelity and temporal coherence."
        },
        {
            "title": "5 Limitations",
            "content": "Despite its strengths, MoCA-Video struggles with non-intersective or relational conceptse.g., adjectivenoun pairs like atomic engineer, nounnoun compounds such as movie psychiatrist, or asymmetric blends like houseboat vs. boathouse. It also falters on semantically distant pairs (e.g., quantum barber). Addressing these cases will require explicit relational reasoning or external knowledge beyond our current framework."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced training-free framework for semantic mixing in video that fuses image-conditioned concepts into pre-generated videos with high temporal and spatial consistency. By operating in the latent noise space, our method leverages overlap maximization, adaptive motion correction, and gamma residual noise stabilization to achieve stable and coherent concept alignment without retraining. Quantitative and qualitative results show improved frame-level fusion and motion consistency over existing baselines, and ablation studies confirm the necessity of each module. Our approach highlights the potential of structured noise-space manipulation for controllable and high-quality video synthesis. Impact Statement The work presents MoCA-Video, novel framework for semantic mixing in the video domain, enabling researchers and content creators to blend visual concepts in temporally coherent and controllable fashion. By operating operating directly in the latent diffusion space, our method lowers the barrier to academic exploration of video editing and opens new possibilities for creative content production. At the same time, we recognize the dual-use potential of generative approaches. MoCA-Video explicitly intended for constructive and societal beneficial applications, and we make no claims in support of generating misleading, harmful or malicious content. We encourage downstream developers to adopt responsible and ethical deployment practices, to ensure advances human creativity and knowledge without facilitating deception, abuse, or unethical manipulation."
        },
        {
            "title": "References",
            "content": "[1] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion 9 models for high-quality video generation, 2023. [2] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. [3] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Atom: Accurate tracking by overlap maximization, 2019. URL https://arxiv.org/abs/1811.07628. [4] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems, 36:1622216239, 2023. [5] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arxiv:2307.10373, 2023. [6] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. International Conference on Learning Representations, 2024. [7] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Idanimator: Zero-shot identity-preserving human video generation, 2024. URL https://arxiv.org/abs/ 2404.15275. [8] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. URL https://arxiv.org/abs/2006.11239. [10] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [11] Yi Huang, Wei Xiong, He Zhang, Chaoqi Chen, Jianzhuang Liu, Mingfu Yan, and Shifeng Chen. Dive: Taming dino for subject-driven video editing, 2024. URL https://arxiv.org/abs/2412.03347. [12] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, and Pinar Yanardag. Rave: Randomized noise shuffling for fast and consistent video editing with diffusion models, 2023. URL https://arxiv. org/abs/2312.04524. [13] Daneul Kim, Jingxu Zhang, Wonjoon Jin, Sunghyun Cho, Qi Dai, Jaesik Park, and Chong Luo. Subjectdriven video generation via disentangled identity and motion, 2025. URL https://arxiv.org/abs/ 2504.17816. [14] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. In NeurIPS, 2024. [15] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-to-video editing tasks, 2024. URL https://arxiv.org/abs/2403.14468. [16] Sumith Kulal, Tim Brooks, Alex Aiken, Jiajun Wu, Jimei Yang, Jingwan Lu, Alexei A. Efros, and Krishna Kumar Singh. Putting people in their place: Affordance-aware human insertion into scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [17] Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng. Magicmix: Semantic mixing with diffusion models. arXiv preprint arXiv:2210.16056, 2022. [18] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, and Jiashi Feng. Magicedit: High-fidelity and temporally coherent video editing. In arXiv, 2023. [19] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Computer Vision and Pattern Recognition, 2016. [20] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling, 2024. URL https://arxiv.org/abs/2310. 15169. 10 [21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/ 2103.00020. [22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/abs/2112.10752. [23] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data, 2022. URL https://arxiv.org/abs/2209.14792. [24] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. URL https://arxiv.org/abs/2010.02502. [25] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. doi: 10.1109/TIP.2003.819861. [26] Jiangchuan Wei, Shiyue Yan, Wenfeng Lin, Boyuan Liu, Renjie Chen, and Mingyu Guo. Echovideo: Identity-preserving human video generation by multimodal feature fusion, 2025. URL https://arxiv. org/abs/2501.13452. [27] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, Yingya Zhang, and Hongming Shan. Dreamvideo-2: Zero-shot subject-driven video customization with precise motion control, 2024. URL https://arxiv.org/abs/2410.13830. [28] Wikipedia contributors. Structural similarity index measure Wikipedia, the free encyclopedia, 2025. URL https://en.wikipedia.org/w/index.php?title=Structural_similarity_index_ measure&oldid=1284192447. [Online; accessed 15-May-2025]. [29] Daniel Winter, Asaf Shul, Matan Cohen, Dana Berman, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectmate: recurrence prior for object insertion and subject-driven generation, 2024. URL https: //arxiv.org/abs/2412.08645. [30] Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, and Ziwei Liu. Freeinit: Bridging initialization gap in video diffusion models. arXiv preprint arXiv:2312.07537, 2023. [31] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. [32] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identity-preserving text-to-video generation by frequency decomposition, 2024. URL https://arxiv. org/abs/2411.17440. [33] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. URL https://arxiv.org/abs/1801. 03924. [34] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2249022499, 2023. [35] Yufan Zhou, Haoyu Shen, and Huan Wang. Freeblend: Advancing concept blending with staged feedbackdriven interpolation diffusion. arXiv preprint arXiv:2502.05606, 2025. [36] Yuanzhi Zhu, Zhaohai Li, Tianwei Wang, Mengchao He, and Cong Yao. Conditional text image generation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1423514245, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Additional Visual Results We provide additional qualitative comparisons on four concept-blending tasks, Bird+Cat, Surfer+Kayak, Horse+Unicorn, and Cow+Sheep, each driven by the same global and conditioned prompts and input reference video and image. Across all examples, MoCA-Video achieves the best trade-off between semantic fusion, temporal smoothness, and frame-level consistency. AnimateDiffV2V largely preserves the original video with only minimal blending of the new concept, while FreeBlend+DynamiCrafter can merge the two concepts but suffers from temporal jitter, spatial artifacts, and lack of overall visual quality. These results underscore MoCA-Videos strong ability to inject merge novel semantics into video while maintaining both motion coherence and aesthetic quality. 1st Frame 4th Frame 7th Frame 10th Frame 13th Frame Global prompt: bird is perched on tree branch, 4K, High quality + Conditioned prompt: \"The conditioned image is cat\" - V M 2 i t A + l r t C y a i O Global prompt: surfer riding wave at sunset + Conditioned prompt: \"The conditioned image is kayak\" - V M 2 i t n + l r t C y Figure 5: Multi-sample Qualitative Comparison. We show four different prompts (two blocks above, two more below in the full paper) across five evenly-spaced frames, comparing Original, MoCA-Video, AnimateDiffV2V, and FreeBlend+DynamiCrafter in each block. 1st Frame 4th Frame 7th Frame 10th Frame 13th Frame Global prompt: horse and rider performing high jump + Conditioned prompt: \"The conditioned image is unicorn\" n r - i o 2 i a A + l r r a a i Global prompt: cow with bell grazing in green pasture + Conditioned prompt: \"The conditioned image is sheep\" i O - V M 2 i a i + l r r a a i Figure 6: Multi-sample Qualitative Comparison. Continued example from Figure 5. A.2 FreeBlend CLIP-Metric Analysis Here we analyze FreeBlends CLIP score (based on absolute difference) and compare with our CASS proposal. The main drawback with FreeBlends \"CLIP-BS\" metric is that it treats the blended videos similarity to each of the two original prompts in isolation, and then it simply takes the absolute difference of it. In practice, CLIP-BS rewards cases where the fused clip simply looks like one of the two prompts (i.e high mix_score) or even shows both concepts side by side, rather than effectively blending them.: mix_score = sim(Vfused, photo of A) + sim(Vfused, photo of B), original_score = sim(VA, photo of A) + sim(VB, photo of B), CLIP_BS = (cid:12) (cid:12)mix_score original_score(cid:12) (cid:12). The original videos CLIP score is already high ( i.e. the generated video matches its own prompt), therefore high CLIP-BS can be obtained in two ways: (1) The mixed score exceeds the original 13 score, (2) The mixed score becomes very low (near zero), when calculating the difference of mixed score and original score will present large negative value that will be positive after applying the absolute value. Clearly, the second scenario is problematic as an image with no similarity to any of the mixed concepts will have high CLIP-BS score, trivially reflecting the alignment of the original video to the the prompt used to create it. The first scenario also allows for sub-optimal visual mixtures with high score CLIP-BS. When sim(Vf used, \"A photo of A\") original_score, we can obtain high CLIP-BS score, fully ignoring the presence of concept in the generated video. Likewise, if sim(Vf used, \"A photo of B\") original_score would score high in CLIP-BS fully disregarding the alignment with concept A. This also means that \"fused\" video where concept and are depicted as independent objects (i.e. without any meaningful combination) can also be favored with high CLIP-BS score. Scenario 1 can yield high CLIP-BS scores even when there is no effective mixing of the concept, the mere presence of both objects is largely favor in CLIP-BS. Also very low scores for sim(Vf used, \"A photo of A\") could be offset by large scores in sim(Vf used, \"A photo of B\") and vice-versa. This is undesirable for the blending task, Table 4: Comparison of semantic mixing methods FreeBlend metrics. Method CLIP-BS DINO-BS FreeBlend MoCA-Video 6.65 4.00 0.27 0.12 We evaluated both metrics on the same 100 test samples. Although FreeBlends score appears higher, its visual results are noticeably inferior to ours, highlighting the shortcomings of their absolute difference measure. By contrast, our CASS metric explicitly captures the desired semantic shift: we compare the original videos alignment to its own prompt before and after fusion (which drops as new content is injected) and we separately track its alignment to the conditioned image (which rises after blending). As result, CASS only produces high values when the fused video truly moves away from the source concept and toward the new concept, faithfully reflecting genuine, high-quality semantic mixing. A.3 Gamma Residual Noise Stabilization Following noise injection and motion correction, we apply additional noise for latent nudging, i.e.: where ϵ (0, I) is sampled noise and γ is small scalar controlling residual strength. This step ensures smooth temporal transitions and maintains structural fidelity in the final video output. = xmix xfinal + γ ϵ, A.4 Human Evaluation Protocol A.4.1 User Study We recruited twenty volunteers (aged 1845, balanced gender) from our university community, all of whom reported normal or corrected-to-normal vision and no prior involvement in this project. Each participant completed eight independent trials in single session lasting approximately twenty minutes. At the start, participants provided electronic consent and reviewed brief demonstration trial to familiarize themselves with the interface and rating criteria. In each trial, participants first viewed an input video along with an input image that shows two different concept. Immediately after, they are shown 3 videos in 2-seconds that randomly ordered without knowing which model it is. ticipants rated each clip on four dimensions using 1-5 Likert scale, where 1 is the worst scale while 5 is the best scale: Blending Quality (how well the clip fused the input video and input image), Video Consistency (smoothness and temporal coherence of motion), Character Consistency (stable blended character consistency), and Overall Quality (general visual fidelity and appeal). We presented these as multiple-choice grid so that every video received score for each criterion. 14 Figure 7: The plot highlights that MoCA-Video outperforms the other methods on Blending Quality and Overall Quality, while still delivering strong Video Consistency and Character Consistency. AnimateDiffV2V scores highest on both consistency measuresreflecting its conservative editingbut lags on blending and overall appeal. FreeBlend+DynamiCrafter ranks lowest across all four metrics, confirming it struggles to balance concept fusion with temporal and character fidelity. Figure 7 shows that the human evaluation complements our automated metrics by capturing subjective judgments of semantic mixing quality, motion coherence, character consistency and overall attractiveness and creativeness of semantic concept blending, critical factors in assessing the real-world effectiveness of video concept-blending methods. A.5 Experimental Setup For our empirical maximization, we ran all experiments on single NVIDIA V100 GPU (32 GB RAM) with Python 3.10, PyTorch 2.1, and CUDA 11.8. Each 147-frame video requires roughly 45 minutes of inference (batch size = 1) and peaks at about 10 GB of VRAM. To ensure the semantic injection has fully taken effect, we sample frames from the midpoint of the generated sequence, (cid:22) new_video_length (cid:23) which is the around so that our evaluations reflect the final, fully blended result. We tuned the following key hyper-parameters to balance semantic fusion and temporal fidelity: an injection timestep of = 300, conditioning strength γ [1.5, 2.0], IoU threshold = 0.5, momentum decay β = 0.9, and base correction weight κ0 [1.0, 2.0]. Other diffusion-scheduler and denoising-step settings were left at their defaults. A.6 Code Release Code can be found here: https://github.com/ZhangT-tech/MoCA-Video"
        }
    ],
    "affiliations": [
        "KAUST"
    ]
}