{
    "paper_title": "DesignLab: Designing Slides Through Iterative Detection and Correction",
    "authors": [
        "Jooyeol Yun",
        "Heng Wang",
        "Yotaro Shimose",
        "Jaegul Choo",
        "Shingo Takamatsu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Designing high-quality presentation slides can be challenging for non-experts due to the complexity involved in navigating various design choices. Numerous automated tools can suggest layouts and color schemes, yet often lack the ability to refine their own output, which is a key aspect in real-world workflows. We propose DesignLab, which separates the design process into two roles, the design reviewer, who identifies design-related issues, and the design contributor who corrects them. This decomposition enables an iterative loop where the reviewer continuously detects issues and the contributor corrects them, allowing a draft to be further polished with each iteration, reaching qualities that were unattainable. We fine-tune large language models for these roles and simulate intermediate drafts by introducing controlled perturbations, enabling the design reviewer learn design errors and the contributor learn how to fix them. Our experiments show that DesignLab outperforms existing design-generation methods, including a commercial tool, by embracing the iterative nature of designing which can result in polished, professional slides."
        },
        {
            "title": "Start",
            "content": "DesignLab: Designing Slides Through Iterative Detection and Correction Jooyeol Yun1,2, Heng Wang1, Yotaro Shimose1, Jaegul Choo2, Shingo Takamatsu1 1Sony Group Corporation, 2Korea Advanced Institute of Science and Technology (KAIST) 5 2 0 2 3 2 ] . [ 1 2 0 2 7 1 . 7 0 5 2 : r Figure 1. DesignLab progressively improves initial presentation designs (bottom right) by adding shapes, colors, and text attributes."
        },
        {
            "title": "Abstract",
            "content": "mercial tool, by embracing the iterative nature of designing which can result in polished, professional slides. Designing high-quality presentation slides can be challenging for non-experts due to the complexity involved in navigating various design choices. Numerous automated tools can suggest layouts and color schemes, yet often lack the ability to refine their own output, which is key aspect in real-world workflows. We propose DesignLab, which separates the design process into two roles, the design reviewer, who identifies design-related issues, and the design contributor who corrects them. This decomposition enables an iterative loop where the reviewer continuously detects issues and the contributor corrects them, allowing draft to be further polished with each iteration, reaching qualities that were unattainable. We fine-tune large language models for these roles and simulate intermediate drafts by introducing controlled perturbations, enabling the design reviewer learn design errors and the contributor learn how to fix them. Our experiments show that DesignLab outperforms existing design-generation methods, including comhttps://yeolj00.github.io/personal-projects/designlab 1 1. Introduction Presentation slides do more than just capture ones attention. They become powerful visual tools that anchor memorable message. Yet, for many, the process of creating highquality slides remains an overwhelming challenge, as it demands series of nuanced decisions, ranging from content placement to color schemes, typography, and the seamless integration of multimedia elements. With the sheer volume of design options available, achieving polished, professional outcome remains daunting for non-experts. This presents fundamental problem that is not simply matter of investing time, but of navigating the inherent complexity of design itself. Automated design tools have attempted to address this challenge by suggesting layouts [2, 11, 15, 30] or generating decorative elements [13, 19]. While these tools can offer reasonable starting points for creating slides, the result often requires additional editing before they are ready for final use. More importantly, these approaches fall short in supporting users to iteratively refine the initial output, which is often the most crucial part of the designing process. This highlights the need for solutions that foster continuous improvements, rather than simply offering static suggestions. In this paper, we tackle practical scenario where user starts from an initial rough draft and seeks to refine it into final design. Real-world design processes often revolve around iterative cycles of suggesting, accepting, and rejecting changes, yet prior approaches have largely overlooked this iterative nature of detecting issues and implementing corrections. To capture this aspect, we introduce two specialized roles: the design contributor, which modifies specific elements based on requests, and the design reviewer, which identifies elements requiring improvement. Our objective is to progressively refine rough drafts over multiple revisions, with each round of feedback addressing remaining issues and driving further enhancement. Building on the concepts of the design contributor and reviewer, we fine-tune large language models (LLMs) to fill these roles. To make presentation slides suitable inputs for LLMs, we convert them into structured JSON format, capturing elements such as text boxes, images, and layouts. Since only complete designs are typically available, we simulate rough drafts to train on pairs of rough designs and their polished version. Specifically, we introduce perturbations to the slides, such as altering fonts, shifting alignments, and adjusting colors, so that the perturbed slides resemble imperfect drafts. The design reviewer is trained to detect these perturbations, learning to identify what is wrong, while the design contributor is trained to correct them, understanding how to improve the design. By explicitly separating detection (discriminative) from correction (generative), our approach effectively decomposes the distinct cognitive processes required for designing, enabling each role to benefit from specialized training objectives. Importantly, this iterative refinement process allows our system to tackle complex design challenges by repeatedly isolating and correcting individual design flaws, rather than merely generating statistical averages of the training data. We evaluate the effectiveness of our approach using realworld examples of initial presentation drafts in need of improvement. Our experiments, which include user study, demonstrate that the decomposed roles of the design contributor and reviewer facilitate progressive improvements, outperforming existing methods that lack support for an iterative process. Additionally, we showcase an interactive use case of our approach, where users can manually select unsatisfactory elements of the design for enhancement or choose from multiple design candidates to refine the preFigure 2. Initial drafts are simulated by removing design elements from presentation slides. sentation according to their preferences. Our contributions are threefold: We introduce DesignLab, the first framework to separate error detection from correction, reflecting real-world design processes where continuous feedback encourages high-quality results. We also showcase an interactive interface, enabled by the decomposed design framework, assisting users to identify and refine unsatisfactory elements. Our experiments demonstrate that our iterative approach outperforms existing design generation methods, including commercial tool. 2. Related Work Design generation Generating designed documents has recently gathered attention, with approaches evolving to address wide range of media formats ranging from document layouts [2, 10, 11] to posters [14, 15], web pages [30], and presentation slides [7, 12, 31]. Early work primarily focused on the placement of elements(e.g., text boxes and images) [2, 11], while recent methods have broadened their scope to include text attributes, such as fonts and colors [30, 37], and even the creation of decorative images [14, 15, 41]. Although these approaches now often produce designs in editable formats such as HTML or JSON, they essentially lack mechanisms to assist users to further refine the output or automatically correct errors. Design editing Another line of work focuses on replacing specific elements in completed design, such as images or text, based on manual user selection [13, 19]. recent study [8] proposes an automated approach for iteratively refining slides by leveraging GPT-4os [27] coding capabilities to adjust slide-generating scripts. However, in many cases, users must either identify errors themselves or rely on single-stage refinement procedure that attempts to handle everything at once. In this paper, we aim to repair design flaws by addressing them in incremental steps, which is critical for handling the complexity of real-world designs. 2 Figure 3. Overview of the DesignLab training and inference pipeline. (a) The design reviewer, an LLM fine-tuned to detect and label perturbed slide elements as TENTATIVE. (b) The design contributor, separate model, refines the elements labeled TENTATIVE. (c) The iterative refinement process alternates between the reviewer and contributor until no elements are labeled TENTATIVE. 3. Method 3.1. Design scope for presentation slides We focus on generating and refining the design-related elements that are typically addressed in real-world design process. These elements, detailed in Table 1 and Section C.1, include basic pre-defined shapes (e.g., rectangles, rounded rectangles, and circles), text attributes (e.g., font type, font size, and line spacing), and shape properties (e.g., position and color). It is important to note that we do not include generating contents itself, such as generating new text and images, within the design scope, as these are generally fixed by the user. Our primary goal is to enable models to identify and refine the combinations of these design attributes. 3.2. JSON representation of presentation slides Presentation slides, typically stored as .pptx files, consist of multiple XML documents [4] that define the content of the slides. Due to the simplicity of XML as markup language, converting these files into structured JSON format is straightforward process [8]. Thus, we represent the design-related elements described earlier in JSON, which can be easily processed by LLMs as both inputs and outputs. However, we exclude media content encodings (e.g., images and videos) as they consume excessive sequence length. Instead, we retain only the shape and position attributes of media elements. 3.3. Defining rough drafts Since presentation slides are typically only available in their final form, it is difficult to obtain matched pairs of rough drafts and their corresponding refined versions. To resolve this issue, we simulate rough drafts by introducing random perturbations into the JSON representation of Category Type Examples Shapes Auto shape Placeholder Rectangle, Line, Circle Image, Video Attributes Position Text Color Fill Width, Height Font Size, Font type RGB values Solid, Gradient, Pattern Table 1. Design elements included in our design scope. each slide. Specifically, we randomly remove graphical elements, shift positions, modify colors, and change font attributes, as illustrated in Figure 2 and Section D.2. By controlling the severity of the perturbations, we can simulate different stages of the design process, ranging from nearfinished slides to those that need substantial edits. This approach yields training pairs (perturbed drafts and their corresponding final versions), providing framework in which our models learn to handle iterative improvement. 3.4. Training the design contributor and reviewer Design reviewer Given pair consisting of the final design and its perturbed version, we train the design reviewer to detect which elements have been altered. Specifically, the design reviewer operates on the JSON representation of the perturbed design and labels any JSON elements that require improvement. We fine-tune LLM for this task, as it offers flexible and scalable solution than building dedicated classifier, adapting to various error types with minimal architectural changes. As illustrated in Figure 3 (a), any element that requires modification is assigned TENTATIVE status tag. In other words, the input of the reviewer is the JSON string of the perturbed design, while the output is the same JSON with certain elements marked as TENTATIVE. 3 Design contributor While the design reviewer identifies elements that require improvement, the design contributor is trained to recover the perturbed elements, restoring them to their original design (e.g., exact positions and correct colors) based on the JSON string with TENTATIVE labels. The TENTATIVE labels serve as indicators that specify which elements should be changed, as the model is trained to modify only those marked as TENTATIVE. Importantly, the contributor is not restricted to altering existing elements, as it can also generate new ones when perturbations involve removing content. Through this process, the design contributor completes the iterative refinement loop by fully addressing flagged flaws and returning the slides to highquality final state. 3.5. Iterative refinement Having trained the design reviewer and design contributor, we now bring them together in an iterative inference pipeline. First, all elements in the slide are initially labeled as TENTATIVE to prompt the design contributor to apply corrections or restorations. The contributors output is then returned to the design reviewer, which inspects the updated design and labels any remaining or newly introduced issues as TENTATIVE. This updated design is passed again to the contributor for another round of fixes, and the cycle continues until either the reviewer identifies no further issues (triggering an early stop) or maximum iteration limit is reached. Through this repeated loop of targeted detection and correction, the final design emerges from multiple incremental improvements, achieving level of polish that single-step methods typically cannot match. 4. Experiments 4.1. Dataset Presentation slides are widely available on various online communities and marketplaces. In this study, we utilize an internal dataset of 200,163 slides collected from the web. key reason for collecting native .pptx or similarly structured files is their element-level decomposability. Unlike image renderings of designs, these vectorized formats allow us to identify and manipulate the components (e.g., text boxes and shapes). Collecting such data has been growing trend in design research [30, 37], which is crucial for training models to generate and revise individual elements. While this large collection is used for training, it does not include intermediate or draft versions of slides. Since genuine rough drafts are rarely shared or archived publicly, we manually created set of 77 rough drafts for evaluation This smaller dataset includes slides with typical early-stage imperfections (e.g., misaligned elements, default font and colors), closely reflecting the type of real-world drafts we aim to improve. We will make these manually created rough"
        },
        {
            "title": "Methods",
            "content": "WebRPG [30] AutoPresent [8] Powerpoint Designer [26] Iteration Diversity Stability"
        },
        {
            "title": "Ours",
            "content": "Table 2. Comparison of baseline approaches and our method across key traits ( indicates partial support). drafts available, enabling the community to benchmark iterative refinement methods on consistent, realistic test set. 4.2. Baselines WebRPG To evaluate the benefits of our iterative design approach, we compare against single-step baseline that attempts to improve perturbed slide in one step. Specifically, using the same dataset, we fine-tune LLM to generate an enhanced design directly from the perturbed input. In spirit, this single-step method parallels WebRPG [30], which similarly focuses on producing an updated layout in one go, but for websites rather than presentation slides. AutoPresent AutoPresent [8] is recent agent-based system [17, 29, 33, 39] designed to generate new slides from scratch based on instructions or reference slide. The system leverages the coding capabilities of GPT-4 to refine slides by producing Python scripts that modify the original slides based on few-shot exemplars. Instead of directly generating an improved layout, AutoPresent generates these refining scripts. While this approach technically supports iterative refinement, the high failure rate makes it impractical for repeated use. Powerpoint Designer Lastly, we compare our method against PowerPoint Designer [26], commercial feature within Microsoft PowerPoint. The Designer function attempts to fit presentation slide into selection of predefined templates, offering multiple suggestions. When suitable template is found, the Designer automatically reformats to align text, images, and other elements. However, if no template matches the structure of the content, the Designer provides no suggestions at all. We summarize the key characteristics and capabilities of these baselines in Table 2. 4.3. Implementation details We fine-tune two instruction-tuned Qwen2.5-1.5B models [32, 38] for the reviewer and contributor roles separately. The models are trained on our dataset for 400,000 steps with learning rate of 1e-4, utilizing warm-up phase and the AdamW optimizer [25]. The models can be run with less than 8GB of VRAM, making them suitable for deployment on commercial GPUs. The chat templates used for training are provided in Appendix Section D. 4 Figure 4. Qualitative comparison of slide refinement results on manually created initial drafts. Best viewed digitally. 5 Figure 5. GPT-4o preference of refined slides. Left bars represent preference for our slides, with gray indicating tie; right bars show preference for baselines. Best viewed in color. Figure 6. Distribution of slides across the number of iterations required to converge. 4.4. Qualitative assessment In Figure 4, we provide extensive qualitative comparisons of the refined presentation slides produced by each baseline approach. Our iterative design process yields highquality designs compared to existing design refinement approaches, which often produces suboptimal designs. For example, template-based approaches, such as PowerPoint Designer [26], fail to make refinements when no suitable template is found and lack design diversity. LLM-based methods, including both fine-tuned (WebRPG) and nonfine-tuned (AutoPresent) models, generate incomplete designs that require further user input to be suitable for final presentations. One issue we observed with AutoPresent [8] is the unreliability of its scripts, which often fail to execute correctly. This undermines the iterative refinement process, as repeated rounds of refinement increase the likelihood of execution failures, ultimately reducing the systems effectiveness. Other methods also lack support for iterative refinement, as they do not accept partially designed drafts as inputs. In contrast, our approach ensures reliable design generation and supports progressive refinement, as our models are trained to detect and correct designs at any level of completeness. 4.5. Quantitative comparison and assessment Design aesthetic comparison To quantify the difference between designs, we evaluate the designs using GPT4o [27] similar to techniques used in recent studies [8, 41]. Unlike previous approaches that evaluate designs based solely on aesthetic scores, we present two design candidates (each from different model) and ask which one better improves the initial draft. This comparative approach provides more direct assessment of each models ability to enhance slide quality. As shown in Figure 5, our method consistently outperforms existing design refinement approaches, including dedicated commercial tool, Powerpoint Designer. Both quantitative and qualitative results demonstrate the effectiveness and reliability of our iterative refinement process in producing high-quality, polished designs. Convergence of the iterative design cycle. We provide examples of the iterative refinement cycle in Figure 7, which terminates when the design reviewer labels no more elements as TENTATIVE. We plot the number of revisions it takes before the design cycle come to an end in Figure 6. Most slides require more than one design cycle to converge, emphasizing the need for continuous and progressive improvements. While the majority of designs converge within two iterations, the need for additional rounds highlights the complexity of the design challenges. This underscores the value of our approach, which facilitates ongoing refinement through multiple cycles of feedback, closely mirroring the iterative nature of real-world design processes. Evaluating the impact of iterative refinements. For quantitative analysis on the effectiveness of the iterative refinement process, we conduct user study to assess the improvements made at each revision. Specifically, we asked 32 users to rate the aesthetic quality of slides on scale of 1 to 10, given pair of 45 slides before and after revision (total of 90 slides). The distribution of scores are plotted in Figure 8, which shows clear trend of increasing aesthetic scores after each iteration, indicating that users perceive clear improvement. This progressive increase in ratings reflects the efficacy of the iterative process in addressing design flaws. Furthermore, the scores eventually converge, suggesting that the iterative cycle reaches point of diminishing returns, where further revisions yield little additional perceptible improvement. This supports the notion that multiple rounds of feedback lead to substantial enhancement in the overall quality of the presentation slides. 6 Figure 7. Step-by-step examples of the iterative refinement process. Each cycle of reviewing and improves the design while revealing new flaws to be revised. The cycle halts when there is no more errors are detected. Design flaws Reviewer Contributor Precision Recall Responsiveness Shape Placement Shape Removal Color Attributes Text Attributes 0.769 0.739 0.856 0.871 0.149 0.657 0.721 0. 1.000 1.000 0.986 0.957 Table 3. Accuracy of the reviewer and the responsiveness of the contributor, measured on the evaluation set. sues to be detected and addressed in subsequent iterations. To evaluate this, we measure the reviewers performance in detecting simulated perturbations in terms of precision and recall, as well as the design contributors responsiveness in making the necessary corrections. The precision and recall is measured by detecting whether randomly perturbed elements are properly labeled as TENTATIVE by the design reviewer. We report the precision and recall for each type of perturbation (i.e., shifted placement, duplicate shapes, altered colors, and text attributes) in Table 3. The responsiveness of the design contributor is measured by verifying whether an element that has been labeled TENTATIVE has been altered by the design contributor. We also measure the responsiveness across different types of perturbations. The overall precision of the reviewer is high, demonstrating that the reviewer is reliable in practice. Meanwhile, the design contributor demonstrates strong responsiveness when adjusting elements labeled TENTATIVE, as evidenced by the high-quality outcomes shown in Figure 4 and Figure 7. We believe this high responsiveness is relFigure 8. User study evaluating the effect of iterative refinement on slide aesthetics, with the shaded area representing the range within one standard deviation. 5. Analysis 5.1. Reviewer and contributor performance key requirement that facilitates our iterative framework is the high accuracy of the design reviewer in detecting errors and the high responsiveness of the design contributor in correcting the identified elements. highly accurate reviewer is crucial, as it ensures that design flaws are correctly identified, distinguishing between elements that should be retained and those that need to be changed. Similarly, responsive design contributor is vital, as it ensures that identified errors are promptly corrected, allowing remaining is7 Figure 9. Interactive scenario of our design cycle featuring branching strategy. In this scenario, the user takes the role of design reviewer, where they can 1) choose the preferred design from two candidates, and 2) select specific elements for modification to enhance the design further. Best viewed in color. of this interactive process in Figure 9. By combining branching with user-driven reviewing, we offer flexible design tool that enables users to iteratively refine their initial drafts and preserve or discard elements as needed. 5.3. Failure cases We illustrate two failure cases of our approach in Figure 10. One issue we observe is that our model sometimes struggles to fully comprehend complex data structures represented in text format, such as tables and graphs. We aim to address these challenges in future updates by leveraging larger models (with 7B, 14B, and 32B parameters) that possess deeper understanding of these structures. Additionally, since our model does not encode any media content (e.g., images or videos), it is unable to interpret their visual content and their colors. As result, the designs it generates may feature colors that do not align with those present in the media. This design choice of not encoding images stems from the fact that VLMs [1, 23, 24] at their current stage consume too many tokens, and JSON representations already occupy significant portion of the token budget. We plan to make future updates by incorporating meta-information, such as content tags and color palettes. 6. Conclusion In this paper, we present design assistant tool that can iteratively detect and revise design flaws, modeling real-world designing workflows. Unlike previous approaches, we focus on revising imperfect drafts, critical yet often overlooked aspect in studies. Our experiments, including user study, demonstrate that our iterative process of revising intermediate draft produces high-quality outputs, consistently improving the design over time. We believe our approach has broader implications beyond presentation slides and offers general framework for making design tasks more accessible and efficient across diverse design domains. 8 Figure 10. Failure cases of our approach with complex data structures and content awareness. atively easy to achieve because the contributor receives explicit labels identifying which elements need changes. However, by isolating error detection within the design reviewer, we reduce the cognitive load on the design contributor, which can then focus solely on producing the necessary corrections in the design. 5.2. Interactive scenarios and branching strategies Certain design flaws, such as subtle shifts in position, are inherently difficult to detect, as reflected by the low position recall in Table 3. To address these cases, our framework supports interactive use cases where auser can take on the role of reviewer, manually selecting elements that require changes. This approach enables further refinements customized to the users preference. our Additionally, system can generate multiple branches of design, akin to branching in software development pipelines, at minimal cost via batched inference. Each branch represents distinct design suggestion, allowing users to compare options and select (merge) those that best fit their preferences. We demonstrate an example"
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint, 2023. 8 [2] Shang Chai, Liansheng Zhuang, and Fengying Yan. Layoutdm: Transformer-based diffusion model for layout generation. In CVPR, 2023. 1, 2 [3] Haoyu Chen, Xiaojie Xu, Wenbo Li, Jingjing Ren, Tian Ye, Songhua Liu, Ying-Cong Chen, Lei Zhu, and Xinchao Wang. Posta: go-to framework for customized artistic poster generation. In CVPR, 2025. 1 [4] World Wide Web Consortium. Extensible markup language (xml) 1.1. Technical report, World Wide Web Consortium, 2006. 3, 1 [5] Peitong Duan, Jeremy Warner, Yang Li, and Bjoern Hartmann. Generating automatic feedback on ui mockups with large language models. 2024. 1 [6] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. ICLR, 2023. [7] Tsu-Jui Fu, William Yang Wang, Daniel McDuff, and Yale Song. Doc2ppt: Automatic presentation slides generation from scientific documents. In AAAI, 2022. 2 [8] Jiaxin Ge, Zora Zhiruo Wang, Xuhui Zhou, Yi-Hao Peng, Sanjay Subramanian, Qinyue Tan, Maarten Sap, Alane Suhr, Daniel Fried, Graham Neubig, et al. Autopresent: Designing structured visuals from scratch. arXiv preprint, 2025. 2, 3, 4, 6 [9] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint, 2024. 2 [10] Kamal Gupta, Justin Lazarow, Alessandro Achille, Larry Davis, Vijay Mahadevan, and Abhinav Shrivastava. Layouttransformer: Layout generation and completion with selfattention. In ICCV, 2021. 2 [11] Daichi Horita, Naoto Inoue, Kotaro Kikuchi, Kota Yamaguchi, and Kiyoharu Aizawa. Retrieval-augmented layout transformer for content-aware layout generation. In CVPR, 2024. 1, 2 [12] Yue Hu and Xiaojun Wan. Ppsgen: Learning to generate presentation slides for academic papers. In IJCAI, 2013. [13] Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. Towards flexible multi-modal document models. In CVPR, 2023. 2 [14] Naoto Inoue, Kento Masui, Wataru Shimoda, and Kota Yamaguchi. OpenCOLE: Towards Reproducible Automatic Graphic Design Generation. In CVPRW, 2024. 2 [15] Peidong Jia, Chenxuan Li, Zeyu Liu, Yichao Shen, Xingru Chen, Yuhui Yuan, Yinglin Zheng, Dong Chen, Ji Li, Xiaodong Xie, et al. Cole: hierarchical generation framework for graphic design. arXiv preprint, 2023. 1, 2 [16] Albert Jiang, Sablayrolles, Mensch, Bamford, Singh Chaplot, Ddl Casas, Bressand, Lengyel, Lample, Saulnier, et al. Mistral 7b. arxiv. arXiv preprint, 2023. 2 [17] Kyudan Jung, Hojun Cho, Jooyeol Yun, Soyoung Yang, Jaehyeok Jang, and Jaegul Choo. Talk to your slides: Languagedriven agents for efficient slide editing. arXiv e-prints, pages arXiv2505, 2025. 4 [18] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. study of bfloat16 for deep learning training. arXiv preprint, 2019. [19] Kotaro Kikuchi, Naoto Inoue, Mayu Otani, Edgar SimoSerra, and Kota Yamaguchi. Multimodal markup document models for graphic design completion. arXiv preprint, 2024. 2 [20] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In ACM SIGOPS, 2023. 2 [21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. In MLSys, 2024. 2 [22] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint, 2024. 2 [23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. [24] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 8 [25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019. 4 [26] Microsoft. Powerpoint designer, 2025. 4, 6 [27] OpenAI. Gpt-4o, 2024. 2, 6 [28] Yi-Hao Peng, Faria Huq, Yue Jiang, Jason Wu, Xin Yue Li, Jeffrey Bigham, and Amy Pavel. Dreamstruct: Understanding slides and user interfaces via synthetic data generation. In ECCV. Springer, 2024. 1 [29] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`Ä±, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In NIPS, 2023. 4 [30] Zirui Shao, Feiyu Gao, Hangdi Xing, Zepeng Zhu, Zhi Yu, Jiajun Bu, Qi Zheng, and Cong Yao. Webrpg: Automatic web rendering parameters generation for visual presentation. In ECCV. Springer, 2024. 1, 2, 4 [31] Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, and Nancy X. R. Wang. D2S: Document-to-slide generaIn Proceedings tion via query-based text summarization. of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2021. [32] Qwen Team. Qwen2.5: party of foundation models, 2024. 4, 2 9 [33] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint, 2023. 4 [34] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. ICLR, 2022. 2 [35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NIPS, 2022. [36] Jason Wu, Yi-Hao Peng, Xin Yue Amanda Li, Amanda Swearngin, Jeffrey Bigham, and Jeffrey Nichols. Uiclip: data-driven model for assessing user interface design. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, 2024. 1 [37] Kota Yamaguchi. Canvasvae: Learning to generate vector graphic documents. In ICCV, 2021. 2, 4 [38] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint, 2024. 4 [39] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In ICLR, 2023. 4 [40] Zhao Zhang, Yutao Cheng, Dexiang Hong, Maoke Yang, Gonglei Shi, Lei Ma, Hui Zhang, Jie Shao, and Xinglong Wu. Creatiposter: Towards editable and controllable multi-layer graphic design generation. arXiv preprint arXiv:2506.10890, 2025. 1 [41] Hao Zheng, Xinyan Guan, Hao Kong, Jia Zheng, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, and Le Sun. Pptagent: Generating and evaluating presentations beyond textto-slides. arXiv preprint, 2025. 2, 6 DesignLab: Designing Slides Through Iterative Detection and Correction"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Extended Related Work We further organize the related work section as, our work sits at the intersection of several research areas. Automated design assessment and critique generation. growing body of work focuses on automatically evaluating and providing feedback on visual designs. UIClip [36] introduces data-driven approach for assessing user interface designs, demonstrating that machine learning models can learn to evaluate design quality across multiple dimensions such as aesthetics, usability, and accessibility. Similarly, Duan et al. explore generating automatic feedback on UI mockups using large language models, showing that LLMs can provide constructive design critiques that help designers identify improvement opportunities. These works establish the feasibility of the reviewer role in our framework, though they focus primarily on evaluation rather than the iterative refinement process we propose. Visual design beyond presentation slides. While our work focuses on presentation slides, it connects to broader research in automated visual design generation and assessment. User interface design has been particularly active area, with researchers developing systems for generating, evaluating, and refining interface layouts [5]. Poster design [3, 40], web layout optimization [30], and graphic design automation share similar challenges around balancing aesthetic principles with functional requirements. Our JSON-based representation approach and perturbationbased training methodology could potentially extend to these domains, as they face similar challenges in capturing design elements and learning improvement patterns. Synthetic data generation for design understanding. key challenge in training design systems is the scarcity of paired examples showing design evolution from rough drafts to polished versions. DreamStruct [28] addresses this challenge by generating synthetic data to understand slides and user interfaces, demonstrating that artificial data creation can effectively support design-related machine learning tasks. Their work validates our approach of using synthetic perturbations to simulate rough drafts, though our focus on creating iterative refinement pairs differs from their broader data generation objectives. This synthetic data approach has proven valuable across various design domains where naturally occurring before-and-after examples are rare. Figure 11. User preference of slides refined by our approach and each baseline approach. B. User Study on Refined Slides To validate our evaluation methodology, we conducted user study involving 20 participants. This study was designed to parallel our GPT-4o evaluation, with participants assessing the same presentation slides using identical criteria and rating scales. The human evaluation results are presented in Figure 11 and demonstrate patterns remarkably consistent with our GPT-4o assessments, supporting the reliability of our automated evaluation approach. C. JSON Representation of Presentation Slides C.1. What content do we represent? Presentation slides, typically in .pptx format, consist of multiple XML documents [4] that describe each slides structure. Since XML is well-documented markup language, its interpretation is relatively straightforward. Opensource libraries, such as python-pptx, provide robust support for parsing these formats. However, XML files are often lengthy and not well-suited as direct inputs or outputs for LLMs. To address this, we transform each XML file in our dataset into structured JSON format that conveys the slides content in natural language. During this process, we selectively extract relevant slide information. We categorize design elements, as shown in Table 4, into shapes and attributes. Shapes are the visible entities in slide, including pre-defined basic shapes (e.g., circles, ovals, and rounded rectangles) and placeholders for media content such as images and videos. We consider 34 basic shape types supported in PowerPoint but exclude complex elements like tables, graphs, and plots for simplicity. Attributes define shape properties, including position, text content, colors, and fill types, and are always associated with shape. C.2. Why do we use JSON representation? It is important to note that representing slides in structured, text-based format (JSON) offers significant flexibility. This approach allows us to seamlessly incorporate ad-"
        },
        {
            "title": "Autoshapes",
            "content": "Line, Circle, Oval, Rectangle, Rounded Rectangle, Trapezoid, Arrow"
        },
        {
            "title": "Placeholders",
            "content": "Image, Video"
        },
        {
            "title": "Fill",
            "content": "X-coordinate, Y-coordinate Width, Height Font Type, Font Size, Line Width, Text Alignment RGB values, Transparancy values Solid, Gradient, Pattern Table 4. Categorization of design elements within our scope of generation. ditional attributes such as transparency levels, gradient fills, or other visual properties without requiring major architectural changes often required by previous approaches [13]. Consequently, our representation enables highly expressive generation process capable of handling wide variety of design elements, making it adaptable to diverse design requirements. Ultimately, the expressive nature of our JSON representation defines the scope of our models generation capabilities, allowing the model to manipulate and refine multiple aspects of slide design comprehensively. By clearly delineating the types of elements and attributes included in our representation, we also precisely define the scope and boundaries of the design improvements our model is capable of generating. Vision-language models (VLMs) present an interesting future direction for visually inspecting slide components, but image-based approaches remain outside the scope of our current work due to token efficiency considerations. Our JSON-based implementation proves effective for core design tasks like text box alignment and color suggestions, showing that structured representations can successfully enable iterative design improvements. D. Training Details D.1. Model selection and hyperparameters We use an instruction-tuned Qwen2.5-1.5B model [32] to serve as both the reviewer and the design contributor. Although we do not train our model on other LLMs [9, 16, 22], we believe that these differences are marginal, as we excessively fine-tune the model to perform their roles. We train our model on 8 Nvidia A100 GPUs using batch size of 1 for 400,000 steps, using learning rate of 1e-4 with linear warmup for 500 steps. We find that we need long fine-tuning steps to achieve decent performance, largely due to learning the JSON representation of slides. However, we find the training process itself to be stable and is robust to different training hyperparameters. D.2. Training samples for supervised fine-tuning The reviewer identifies design flaws in slide and labels them as TENTATIVE. The contributor then improves the slide based on its JSON representation, incorporating the TENTATIVE labels. Since both roles rely on instructiontuned models [34], we structure our supervised training samples using simple chat template, as shown in Figure 12. These templates are also used during inference. We further provide details on how we perturb the slides (simulate rough drafts). Every shape and attributes summarized in Table 4 are subject to perturbation. For shapes, we either remove certain autoshapes entirely, training the model to generate shapes if needed, and also duplicate existing shapes, training the model to remove shapes if need. For attributes, we randomly shift the positions and alter colors for continuous values. We set categorical values, such as text attributes, to set of pre-defined default values. For example, for font types, we change the font to one of the popular fonts (e.g., Arial, Roboto, and Calibri). Note that this doesnt mean that our design refinement is limited to these fonts, as the design contributor is trained to generate font names in the original slides. E. Inference Cost Inference is performed using the same chat template used during training. We observe that inference is memoryefficient and consistently fits within the 2048-token maximum sequence length, despite the detailed JSON representation of the slides. Without any acceleration techniques [6, 18, 21], both the design reviewer and contributor complete slide generation in under 30 seconds and require 8GB of VRAM using well under typical memory limits. Both the design reviewer and contributor generates presentation slide under 30 seconds, without any acceleration techniques. Additionally, inference with an optimized implementation like vLLM [20] significantly reduces runtime, finishing each step within 6 seconds. This demonstrates that our framework is suitable for practical, real-time interactive scenarios, even on hardware with limited computational capacity. F. Evaluation Details To compare the aesthetic quality of slides, we prompt GPT4o to select which of two slides represents better refinement relative to an initial draft. Following recent evaluation 2 Figure 13. Comparing two versions of refined presentation slides using GPT-4o. tions often exhibit visual qualities and structural coherence that cannot be achieved by merely placing contents (text and images). By training the model to generate simple elements, our approach generates visually appealing and professionally cohesive designs, demonstrating an understanding of complex design principles such as alignment, symmetry, and spatial balance. This capability significantly surpasses simpler layout generation methods, underscoring the practical utility of our iterative, refinement-based framework. Figure 12. Chat templates used for supervised fine-tuning. The JSON representation of slides is inserted into the template. JSON icons with marks indicate files containing TENTATIVE labels. methodologies leveraging GPT models, we employ simple chain-of-thought reasoning strategy [35], encouraging GPT-4o to first explicitly analyze each slide before making decision. Specifically, GPT-4o initially examines the provided rough draft, then compares two refined slides, and finally selects the slide demonstrating superior improvement. By repeating this procedure across multiple comparisons, we compute pairwise win rates for each method. We believe that this is more reliable way to evaluate designs, compared to asking model to rate score for each draft, as these comparisons are easier to make for humans as well. The entire evaluation workflow is illustrated in Figure 13. G. Additional Results We present additional qualitative results in Section to further illustrate the capabilities of our approach. One notable strength of our method is its ability to creatively combine basic shapes, such as circles, rectangles, and rounded rectangles, to produce sophisticated layouts. These composi"
        }
    ],
    "affiliations": [
        "Korea Advanced Institute of Science and Technology (KAIST)",
        "Sony Group Corporation"
    ]
}