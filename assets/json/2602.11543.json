{
    "paper_title": "Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm",
    "authors": [
        "Jinrui Zhang",
        "Chaodong Xiao",
        "Aoqi Wu",
        "Xindong Zhang",
        "Lei Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however, they still need to train the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), a memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only a subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train a 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training a 7B model from scratch and a 9B model upcycled from a dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES."
        },
        {
            "title": "Start",
            "content": "Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Jinrui Zhang 1 2 Chaodong Xiao 1 2 Aoqi Wu 1 2 Xindong Zhang 2 Lei Zhang 1 2 Abstract Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100).Recent decentralized training methods reduce communication overhead by employing federated opthey still need to train timization; however, the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training 7B model from scratch and 9B model upcycled from dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES. 6 2 0 2 2 1 ] . [ 1 3 4 5 1 1 . 2 0 6 2 : r 1. Introduction Large language models (LLMs) (Achiam et al., 2023; Grattafiori et al., 2024; Yang et al., 2025; Liu et al., 2024; Muennighoff et al., 2024) have shown strong generalization capabilities across various downstream tasks, establishing 1Department of Computing, The Hong Kong Polytechnic University. 2OPPO Research Institute. Correspondence to: Lei Zhang <cslzhang@comp.polyu.edu.hk>. Preprint. February 13, 2026. themselves as fundamental components in real-world applications such as conversational assistant (Cui et al., 2024) and embodied agent (Fung et al., 2025). However, pretraining LLMs remains highly resource-intensive. The main bottlenecks arise from the substantial GPU memory required to store model parameters, activations, optimizer states, and gradients, and the need of low-latency, high-bandwidth inter-device communication to support model and data parallelism (Shoeybi et al., 2019; Rasley et al., 2020; Zhao et al., 2023). Consequently, existing LLMs are typically trained under centralized settings (as shown in Fig. 1 (left)), utilizing co-located clusters equipped with high-memory GPUs and fast interconnects (e.g., RDMA). For instance, LLaMA3-405B (Grattafiori et al., 2024) is trained using up to 16K H100 GPUs linked with high-bandwidth interconnects, while OLMo2 7B (OLMo et al., 2024) is trained on cluster of 1,024 H100 GPUs. Such high infrastructure requirements make LLM pretraining inaccessible to most researchers in the community. To mitigate the demands of centralized LLM training, recent works such as DiLiCo (Douillard et al., 2023) and Photon (Sani et al., 2024) have explored decentralized pre-training paradigms (as shown in Fig. 1 (middle)). In these approaches, each workstation performs local updates and synchronizes with peers intermittently via parameter server, following federated optimization protocol (e.g., FedAvg (McMahan et al., 2017)). This sparse communication mode significantly reduces the bandwidth requirements compared to centralized dataor model-parallel methods, enabling training across geographically distributed, heterogeneous GPU clusters. While communication constraints are relaxed, however, these approaches still require each node to update the full set of model parameters. Consequently, the memory footprint per node remains substantial. This limitation is especially significant for training largescale LLMs, where insufficient memory can be bottleneck. To address this challenge, we propose SParse Expert Synchronization (SPES), memory-efficient, decentralized training paradigm tailored for MoE-based LLMs, as illustrated in the right panel of Fig. 1. Compared to dense models, MoE models are inherently well-suited for decentralized environments, as each expert can be managed independently, Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Figure 1. Comparison of different pretraining paradigms for LLM. Left: centralized training, which requires high-memory GPUs and high-bandwidth interconnects (e.g., RDMA) for its tightly coupled model or data parallelism. Middle: existing decentralized training (e.g., DiLiCo, Photon), where each node trains full model locally, reducing bandwidth needs but still demanding high-memory GPUs. Right: our proposed SPES, memory-efficient decentralized method for training MoE-based LLMs, where each node trains only subset of experts, substantially reducing both per-GPU memory usage and communication overhead. enabling finer-grained training and resource management. In SPES, each node is responsible for training distinct subset of experts, while keeping the remaining experts frozen during local updates. This design substantially reduces the memory requirement per node, since each node only needs to maintain the gradients and optimizer states for the experts assigned to it 1. All nodes periodically synchronize their trained experts with peers, ensuring continuous knowledge sharing across the network. By eliminating the need to transmit the entire model weights, this sparse synchronization approach substantially reduces communication overhead and enables efficient knowledge exchange between nodes. challenge in this sparse training regime is the limited token utilization of individual experts, as each expert is trained on only subset of the total training tokens, which can slow down model convergence. To address this issue, we introduce an expert-merging warm-up strategy: in the early stages of training, we periodically merge each expert with its most similar peers in weighted average manner, accelerating the knowledge acquisition of each expert. We evaluate the effectiveness of SPES by pretraining MoE LLMs at 2B, 7B, and 9B parameter scales within decentralized settings. Our results show that SPES enables the training of 2B-parameter MoE LLM on 16 standalone NVIDIA L40S GPUs (48GB) over the internet, achieving performance comparable to centrally trained models under comparable computational budgets. Compared with previous decentralized training frameworks, SPES reduces up to 33.3% communication cost and significantly lowers perGPU memory requirements. We further demonstrate the 1Note that optimizer states and gradients typically dominate the static memory footprint (excluding activations) in model training. For example, AdamW (Loshchilov & Hutter, 2017) can consume up to 75% of the total static memory usage. scalability of SPES by training 7B model from scratch and upcycling 9B model from strong dense initialization; both models match the performance of centralized counterparts trained with similar data and compute resources. Ablation studies and in-depth analysis are also provided to validate the design choices of SPES. (i) Our contributions can be summarized as follows. memory-efficient decentralized pretraining framework. We propose SPES, memory-efficient decentralized framework for pretraining MoE-based LLMs, where each node trains only subset of experts, significantly reducing per- (ii) An device memory and communication overhead. expert-merging warm-up strategy. We introduce an expert-merging warm-up strategy to periodically aggregate similar experts during early training, enabling stronger expert representations with sparse decentralized training. (iii) Superior results. We demonstrate the effectiveness of SPES by training models across multiple scales, utilizing both training from scratch and continual pretraining regimes on weakly connected GPUs. SPES achieves competitive performance, but with significantly lower communication and memory costs compared to previous approaches. As most existing decentralized LLM training frameworks are not open-sourced, we implement custom server-client communication protocol based on gRPC (gRPC, 2015) and integrate it into mainstream LLM pretraining codebase (Muennighoff et al., 2024). Our model and code will be released to facilitate future works on decentralized training. 2. Related Work Decentralized Training. Decentralized training has been studied for both fine-tuning (Wu et al., 2025; Bai et al., 2024; 2 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Sun et al., 2024) and pretraining (Douillard et al., 2023; Sani et al., 2024; Jaghouar et al., 2024) LLMs. The works on finetuning pretrained LLMs usually target for privacypreserving adaptation. FATE-LLM (Fan et al., 2023) explores federated fine-tuning for advertising generation. Subsequent works (Kuang et al., 2024; Zhang et al., 2024a; Ye et al., 2024) extend federated LLM fine-tuning to instructiontuning settings. To reduce communication and memory costs, parameter-efficient federated fine-tuning methods have been proposed, such as FedLoRA (Yi et al., 2023) and FedPETuning (Zhang et al., 2023). DiLiCo (Douillard et al., 2023) and Photon (Sani et al., 2024) are among the first to study decentralized LLM pretraining. With FedAvg (McMahan et al., 2017), they achieve comparable perplexities to centrally trained models while substantially reducing communication cost. More recent efforts improve communication efficiency via new optimizers (Iacob et al., 2025; Kolehmainen et al., 2025) and architectures tailored to decentralized settings (Douillard et al., 2024). At larger scale, INTELLECT-1 (Jaghouar et al., 2024) demonstrates decentralized pretraining of 10Bparameter model across independent devices, and Charles et al. (2025) further validates the scalability of this communication-efficient paradigm. Despite such advances, those methods still incur significant memory and communication overhead due to full-model training and synchronization. In contrast, our SPES only needs to train subset of parameters per node, substantially reducing both the memory and communication costs; moreover, SPES can be naturally combined with more advanced optimizers and architectures to further improve scalability. Memory-Efficient Pretraining. Methods to reduce memory in LLM pretraining primarily leverage sharding and parallelism on tightly coupled accelerators. Data parallelism such as ZeRO (Rajbhandari et al., 2020) and FSDP (Zhao et al., 2023) partition optimizer states, gradients, and model parameters, enabling distributed storage and computation. Model-parallel techniques (Shoeybi et al., 2019)including pipeline, tensor, and expert parallelismsplit model computation to accommodate larger architectures. However, these strategies typically assume centralized cluster with high-bandwidth interconnects to facilitate frequent synchronization. Orthogonal techniques include mixed precision (Micikevicius et al., 2017), activation checkpointing, memory-efficient attention (Dao et al., 2022; Dao, 2023), and optimizer quantization (Dettmers et al., 2021). Our proposed SPES enables cross-node expert sharding with sparse synchronization: gradients and optimizer states are distributed across geographically heterogeneous nodes, each of which trains only the MoE experts assigned to it and communicates only necessary updates. SPES is designed for environments with heterogeneous, low-bandwidth interconnects, such as single-GPU nodes where intra-node sharding is infeasible. Moreover, SPES complements existing parallelism paradigms: when multiple GPUs are available per node, SPES can be combined with previous parallelism strategies to maximize memory efficiency and scalability. 3. Memory-Efficient Decentralized Pretraining In this section, we present the details of our proposed SParse Expert Synchronization (SPES), memory-efficient decentralized pretraining framework for MoE LLMs. SPES partitions expert training across weakly connected nodes and synchronizes weights intermittently, substantially reducing both the memory usage and the communication overhead compared to prior paradigms. We begin with the preliminaries (Section 3.1), followed by the framework overview (Section 3.2), and the methodology details (Section 3.3). 3.1. Preliminaries Decentralized Training. Let = {η1, . . . , ηN } denote set of nodes, where node ηi holds local data Di. Existing decentralized training frameworks (Douillard et al., 2023; Sani et al., 2024) often use two-level optimization scheme: an outer optimizer that coordinates global synchronization and an inner optimizer that performs local updates. In the tth communication round, the global parameters obtained in the previous round, denoted by θ(t1), are broadcast to all nodes. Each node runs steps the inner optimizer (e.g., AdamW (Loshchilov & Hutter, 2017)) on its shard Di, producing the updated local parameters θ(t) . The parameter server aggregates local updates by averaging model deltas and updates the global parameters via θ(t) OuterOpt (cid:16) θ(t1), 1 (cid:80)N i=1(θ(t) θ(t1)) (cid:17) . (1) When the outer optimizer is set to SGD, the above training procedures become the FedAvg (McMahan et al., 2017), which enables distributed training while minimizing communication overhead. However, each node is required to train the entire model, which needs to store large amount of intermediate optimizer states, limiting its applicability to memory-constrained devices. Mixture-of-Experts LLM. MoE architectures (Lepikhin et al., 2020; Muennighoff et al., 2024; Dai et al., 2024) extend transformer LLMs by introducing set of expert sub-networks {Ej}M j=1, each sub-network Ej being parameterized by ϕj. Given an input token x, gating function G(x) is used to select sparse subset of experts to process it. The output of the MoE block is computed as weighted sum of the selected experts: MoE(x) = (cid:80)M j=1 Gj(x) Ej(x). (2) where Gj(x) denotes the gate weight for expert j. By activating only few experts per token, MoE scales model capacity without proportional increase in per-token computation. 3 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Figure 2. (a) Illustration of our model structure, in which we utilize an MoE LLM comprising standard self-attention blocks, normalization layers, and routed feed-forward modules. (b) Illustration of SPES, where each node performs local training on disjoint subset of experts to reduce memory consumption. During weight synchronization, only the trained parameters are transmitted to the parameter server, minimizing communication overhead. To improve data utilization, we propose an expert-merging strategy that merges similar experts to facilitate knowledge sharing. 3.2. Overall Framework Previous sharding strategies, such as FSDP (Zhao et al., 2023) and ZeRO (Rajbhandari et al., 2020), partition LLM model training in centralized data-parallel setups. Each node is responsible for subset of model modules, which alleviates individual memory constraints. However, when inter-node communication bandwidth is limited, the tight coupling between model shards may lead to suboptimal performance due to insufficient synchronization of model updates. To address this issue, we adopt the MoE architecture to train the LLM, where expert modules can be managed independently, thus relaxing synchronization requirements and enabling fine-grained resource allocation. Following prior works (Touvron et al., 2023; OLMo et al., 2024; Bai et al., 2023), we employ standard decoderonly MoE LLM, which is composed of self-attention layers, sparse expert feed-forward networks selected via softmax routing, and normalization layers, as illustrated in Fig. 2(a). Positional encoding is implemented using RoPE (Su et al., 2024), SwiGLU (Shazeer, 2020) is adopted as the activation function, and normalization is performed with RMSNorm (Zhang & Sennrich, 2019). QK-Norm is applied to enhance stability. Specifically, we utilize the drop-less MoE (Gale et al., 2023), as suggested by Muennighoff et al. (2024), to maximize expert utilization. In this work, our goal is to train an MoE-based LLM using distributed GPUs. Compared to traditional centralized training, the key challenge of our decentralized training lies in the memory and communication bottlenecks. We therefore propose Sparse Expert Synchronization (SPES) to solve this issue. As illustrated in Fig. 2(b), we take advantage of the inherent modularity of MoE LLM by distributing expert training across the nodes. Each node is assigned with some shared modules and unique subset of the experts, allowing memory-efficient local updates. During training, the nodes perform efficient synchronization to share knowledge. To improve data utilization for each expert, we further propose an expert-merging warm-up strategy. The details of our SPES are presented in the following section. 3.3. Sparse Expert Synchronization Expert Assignment and Local Training. We denote by Φ = {ϕj}M j=1 the set of parameters of all experts. Refer to Fig. 2(b), we partition Φ into disjoint subsets, so that Φ = Φ1 Φ2 . . . ΦN , where Φi denotes the subset of experts assigned to node ηi. We denote by Φi the set of unassigned experts for node ηi, and denote by ψi the parameters of the shared modules. At the start of each local training round t, node ηi receives the global model parameters updated at round 1 from the server and then performs rounds of local updates on its local data Di. The designated expert parameters Φi and the shared parameters ψi will be optimized while keeping Φi fixed. The updated local parameters at round can be denoted as: (cid:16) θ(t) = ψ(t) , Φ(t) , Φ (t1) (cid:17) . (3) Although each node stores full copy of the model, gradients and optimizer states are maintained only for the updated parameters, which substantially reduces memory overhead. Sparse Synchronization. At the end of each local training round t, node ηi holds updated local parameters θ(t) , where the shared parameter ψi and the assigned experts Φi are updated. During synchronization, each node transmits the updated parameters to the server. Shared parameters are aggregated using FedAvg (McMahan et al., 2017), while experts are updated via direct assignment: (cid:16) 1 i=1 Φ(t) i=1 ψ(t) θ(t) = , (cid:83)N (cid:80)N (cid:17) (4) . The aggregated global parameters θ(t) are then broadcast to 4 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm all nodes for the next round of training. By synchronizing only assigned experts and shared parameters, SPES substantially reduces communication overhead, enabling scalable decentralized training under limited bandwidth. Expert-Merging Warm-Up. While achieving notable memory efficiency, SPES faces practical challenge in sparse training: each node updates only its local experts, leaving many tokens assigned to frozen (unassigned) experts without contributing to gradient updates. This leads to lower token utilization compared to centralized training with an equivalent token budget. To address this issue, we propose an expert-merging warm-up strategy to improve token utilization. The core idea is to periodically merge parameters of similar experts across nodes during synchronization. Instead of updating each expert solely with local assignments, we identify peer experts with similar input projections and merge their parameters to facilitate knowledge sharing. Specifically, for the j-th expert, we compute pairwise cosine similarities between input projection layers: Aj,k = , win win 2 win 2 win , j, {1, . . . , }, (5) where win denotes the input projection weights of expert Ej, for which we select the most similar experts Qj = TopKk(Aj,k), excluding itself. We then update Ej via task arithmetic (Ilharco et al., 2022): = ϕ(t) (cid:101)ϕ(t) + α 1 (cid:80) kQj (cid:0)ϕ(t) ϕ(t) (cid:1), (6) where α controls the merge strength. To preserve the specialization of experts in later training stages, we perform merging only in the initial Tmerge steps and linearly decay α to zero. This expert-merging strategy enables each expert to benefit from gradients from multiple nodes, which improves token utilization and accelerates knowledge acquisition in decentralized sparse training settings. We also provide theoretical convergence analysis of SPES; please refer to Appendix for details. Efficiency Analysis. SPES achieves substantial improvements in both memory and communication efficiency compared to conventional decentralized training methods. For example, when using the AdamW optimizer, DiLiCo (Douillard et al., 2023) requires each node to store optimizer states and gradients for all model parameters, resulting in memory cost of 4 (ψ + Φ) and communication cost of 2 (ψ + Φ) per round. In contrast, SPES exploits expert partitioning, and each node only needs to store the intermediate states for the shared parameters and the assigned experts, which reduces the per-node memory cost to 4 ψ + Φ + 3 Φi. Similarly, communication overhead is also significantly reduced, as only shared parameters and updated experts are synchronized, resulting in cost 5 of (2 ψ + Φ + Φi) per round. SPES achieves significant reductions in both memory and communication cost, especially as the number of nodes increases. For instance, when training 2B-parameter MoE model with 16 experts in 16 nodes (one GPU per node; see Fig. 3 for details), DiLiCo requires 55GB of memory per node, whereas SPES reduces this requirement to 35GB. In addition, SPES achieves 33.3% reduction in communication cost. Training Losses. Our model is trained with three losses: standard cross-entropy loss for next token prediction, zloss (Chowdhery et al., 2023; Zoph et al., 2022) for enhancing training stability, and load-balancing loss (Lepikhin et al., 2020) to encourage uniform expert utilization. Within each node, PyTorch FSDP and mixed-precision are used to further improve memory efficiency. For cross-node synchronization, we use our customized gRPC-based communication protocol. 4. Experiments 4.1. Experiments Setup Implementation Details. Under training-from-scratch settings, we conduct experiments by training our SPES models at three scales: 1B, 2B, and 7B parameters (see Table 2 for detailed configurations). All ablation studies are performed on the 1B model, while the 2B and 7B models are trained to compare with previous work. For the 7B model, our training is distributed over = 4 compute nodes, each equipped with 8 NVIDIA A800 GPUs interconnected via NVLink. parameter server with 96-core Intel Xeon processor (2.90 GHz) and 1.44TB RAM is used for parameter aggregation. The nodes communicate with the server over 13 Gbps Ethernet network, with each node training eight experts (approximately 2.5B trainable parameters per node). For the 2B model, training is performed on = 16 nodes, each hosting one NVIDIA L40S GPU. The parameter server comprises 64-core Intel Xeon Gold 6148 (2.40 GHz) and 720GB RAM, with nodes connected via 17 Gbps Ethernet. Each node manages the training of one expert, resulting in roughly 0.7B trainable parameters per node. Under upcycling settings, we train 9B model initialized from Qwen3-1.7B-Base (Yang et al., 2025). We expand the model by replicating the FFN 8, then inject Gaussian noise into 50% of parameters with standard deviation 0.02, following Team et al. (2024). To match the output scale of the pretrained dense model, we normalize the gating scores after top-k expert selection, following Jiang et al. (2024). In our implementation, the expert merging warmup steps, Tmerge, is set to 12,500, with merging executed for every 500 steps. The parameters α and are set to 0.1 and 4, respectively. All models are trained with the AdamW optimizer (Loshchilov & Hutter, 2017). Please refer to Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Table 1. Performance comparison across different training paradigms. Method ARC(e) ARC(c) PIQA SciQ OBQA BoolQ SIQA WinoGrande Avg. Centralized DiLiCo SPES 49.7 51.7 51.7 24.4 26.6 26.3 68.9 68.4 68.1 74.0 77.4 78.0 30.6 29.6 29.8 54.3 55.7 59. 42.0 43.4 43.0 53.5 51.1 51.5 49.7 50.5 51.0 Figure 3. Memory and communication costs across training paradigms. Experiments are conducted with batch size of 2 and sequence length of 2048. For the 2B model, we employ PyTorch DDP. For the 7B model, we utilize FSDP across 8 GPUs. Table 2. Model configurations. We report the number of activated versus total parameters (#Param), layers (#L), attention heads (#H), intermediate size (Interm.), total experts (#Exp.), and activated experts per token (#Act.). #Param #L #H Hidden Interm. #Exp. #Act. 0.3B/1.1B 12 0.8B/2.1B 16 1.6B/7.3B 16 3.1B/9.4B 28 12 24 16 16 768 1536 2048 2048 1280 2048 6144 16 16 32 8 2 2 4 2 Appendix for additional implementation details. Training Data. We train our models exclusively on publicly available datasets, ensuring accessibility for the research community. The 2B and 7B models are trained on data sampled from Ultra-FineWeb (Wang et al., 2025) and SlimPajama (Soboleva et al., 2023), complemented by openweb-math, algebraic stack, pes2o, arxiv, and StarCoder drawn from olmo-mix-1124 (OLMo et al., 2024) to provide domain-specialized coverage in reasoning, scientific, and programming knowledge. The 1B model is trained solely on SlimPajama for lightweight and efficient pretraining. For tokenization, we use the tokenizer trained by Bai et al. (2023), which offers efficient subword segmentation and robust multilingual support. For the 9B upcycled model, we use data sampled from the Nemotron Pretraining Dataset (Basant et al., 2025). For each node, the training data Di for different nodes is randomly sampled from the whole dataset. Please see Appendix for more details. Evaluation Details. We evaluate our model using the lm-evaluation-harness library (Gao et al., 2024) and report results on several commonsense reasoning benchmarks, including SIQA (Sap et al., 2019), ARC (easy and challenging) (Clark et al., 2018), SciQ (Johannes Welbl, 2017), PIQA (Bisk et al., 2020), OpenBookQA (Mihaylov et al., 2018), WinoGrande (Sakaguchi et al., 2021), LogiQA (Liu et al., 2020) and BoolQ (Clark et al., 2019). To assess general knowledge, we utilize MMLU (Hendrycks et al., 2020) and C-Eval (Huang et al., 2023). Additional evaluation details are included in the Appendix D. 4.2. Main Results Memory Cost Comparison. Figs. 3 (a) and (c) compare the training memory footprints of SPES, DiLiCo, and centralized training. Both centralized training and DiLiCo require each node to update the full set of model parameters, resulting in high memory consumption. For example, training 2B model requires more than 50GB memory per GPU, making it infeasible to train on commonly available 48GB GPUs. Furthermore, decentralized methods like DiLiCo cannot effectively leverage sharded training strategy due to limited inter-node bandwidth, further restricting the maximum trainable model size. In contrast, SPES keeps per-GPU memory under 40GB for 2B model on 16 nodes without any sharding strategy. SPES can be combined with intranode sharding for additional memory savings, as illustrated in Fig. 3(c). This efficiency arises from sparse training: each node updates only subset of parameters, substantially reducing per-GPU memory. Communication Cost Comparison. Figs. 3 (b) and (d) compare the communication overhead of different training schemes. In each round, both DiLiCo and centralized training need to upload the full set of model parameters, whereas SPES transmits only the updated parameters. In each communication round, both DiLiCo and centralized training require each node to upload the entire set of model parameters, whereas SPES only requires uploading the parameters 6 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Figure 4. Performance comparison across different training paradigms. Performance during training is evaluated using the evaluation suite integrated into the open-source OLMo codebase. Table 3. Performance comparison with previous LLMs. denotes models initialized from the pretrained dense model. Method #Params #Tokens SciQ PIQA SIQA BoolQ ARC(e) ARC(c) Models Trained with Significantly More Tokens Qwen2.5-0.5B (Qwen et al., 2025) Qwen3-0.6B (Yang et al., 2025) Llama3.2-1B (Dubey et al., 2024) Qwen2.5-1.5B (Qwen et al., 2025) SmolLM2-1.7B (Allal et al., 2025) Qwen3-1.7B (Yang et al., 2025) OLMoE-1B-7B (Muennighoff et al., 2024) 0.5B/0.5B 0.6B/0.6B 1.1B/1.1B 1.5B/1.5B 1.7B/1.7B 1.7B/1.7B 1.3B/7B 18T 36T 9T 18T 11T 36T 5T 93.0 93.5 91.3 94.1 93.2 95.6 94.9 Models with 3B Parameters OpenELM-0.5B (Mehta et al., 2024) MobiLlama-0.8B (Thawakar et al., 2024) TinyLlama-1.1B (Zhang et al., 2024b) OpenELM-1.1B (Mehta et al., 2024) OPT-1.3B (Zhang et al., 2022) MobiLlama-1.3B (Thawakar et al., 2024) Pythia-1.4B (Biderman et al., 2023) OPT-2.7B (Zhang et al., 2022) Pythia-2.8B (Biderman et al., 2023) Open-LLaMA-3B (Geng & Liu, 2023) SPES-2B (ours) 0.5B/0.5B 0.8B/0.8B 1.1B/1.1B 1.1B/1.1B 1.3B/1.3B 1.3B/1.3B 1.4B/1.4B 2.7B/2.7B 2.8B/2.8B 3B/3B 0.8B/2.1B 1.5T 1.3T 3T 1.5T 180B 1.3T 300B 180B 300B 1T 500B 87.2 85.9 88.9 90.6 84.3 89.1 86.4 85.8 88.3 91.8 85.0 MoE++ 7B (Jin et al., 2024) LLaMA-MoE-3.0B (Zhu et al., 2024) OpenMoE-8B/32E (Xue et al., 2024) SPES-7B (ours) SPES-9B (ours) Models with 7B Parameters 1.2B/7B 3.0B/7B 2.1B/8B 1.6B/7B 3.1B/9B 1T 2.2T 1.1T 500B 400B 89.7 89.9 - 89.9 95.3 69.9 70.1 73.7 75.8 77.4 75.6 80.6 72.3 73.2 73.3 75.6 71.7 74.8 70.9 73.1 74.0 76.2 69.3 78.0 77.5 74.2 74.7 78. 47.1 46.9 45.0 53.5 46.7 52.2 47.8 - 43.1 - - 43.7 44.7 44.6 44.1 44.5 - 42.3 45.7 - - 44.8 47.5 61.7 69.7 63.7 72.6 72.4 79.3 74.4 55.8 60.0 57.8 63.6 57.7 60.3 63.3 60.4 64.7 - 61.4 64.9 - 61.2 62.7 77. 64.6 65.5 71.6 75.3 77.8 73.7 78.0 48.1 49.6 55.3 55.4 57.0 56.7 60.7 60.8 66.4 66.5 63.8 66.9 66.8 64.1 72.1 81.5 35.8 45.9 43.5 53.9 54.1 55.1 55.2 27.6 28.8 30.1 32.3 29.7 36.7 31.2 34.0 36.4 39.0 35.3 43.2 40.9 30.3 43.8 57. that are actually updated. For instance, when training 7B model on 4 nodes, SPES requires only 9.8GB data to be uploaded per node per round, compared to 28.6GB for DiLiCo and centralized traininga reduction of 65% in uplink communication volume. This demonstrates the significant communication efficiency brought by the sparse training strategy of SPES. Training Speed Comparison. We compare the training throughput of SPES against its centralized training counterpart. For the centralized setting, we adopt hybrid FSDP and train on four nodes, each equipped with 8NVIDIA A800 GPUs and interconnected via RDMA. Each node contains four Mellanox InfiniBand HDR adapters, with each port operating at 100 Gbps (2HDR lanes). In this configuration, centralized training reaches 3.79k tokens/s per GPU. Under the SPES setting (see the section of details), throughput with = 50 achieves 3.67k tokens/s. Despite running on weaker hardware environment without high-bandwidth interconnects, SPES achieves comparable speed. In addition, its throughput can be further improved by reducing 7 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Table 4. Performance with and without expert merging. Method ARC(e) ARC(c) PIQA SciQ OBQA BoolQ SIQA WinoGrande Avg. w/o merging w/ merging 52.8 52.1 26.5 27. 68.4 67.4 75.9 77.8 30.0 28.8 58.0 60.4 42.4 42.7 50.3 53. 50.5 51.3 the synchronization frequency, highlighting its scalability under resource-constrained conditions. Comparison with Previous Training Paradigms. We evaluate SPES against both centralized training and the decentralized baseline DiLiCo, using 1B models trained on 100B tokens. As shown in Table 1, SPES achieves competitive performance on multiple benchmarks. Fig. 4 presents performance trajectories during training. Although SPES exhibits slightly slower initial learning curve, attributable to its sparse expert updates, it rapidly converges and ultimately matches or outperforms both baselines. Notably, SPES achieves this with substantially lower per-node GPU memory consumption and reduced synchronization bandwidth relative to centralized and decentralized alternatives. These results highlight that SPES provides favorable trade-off between computational efficiency and model quality, enabling decentralized pretraining to attain competitiveness with large-scale centralized training under significantly lower resource budgets. Performance Comparison with Existing LLMs. Finally, we compare our 2B and 7B models, which are trained with less than 500B tokens, with those open-source models of similar activation parameter scales and trained with less than 3T tokens. The results are shown in Table 3. We also show the results of models trained with significantly more tokens for reference. We can see that across several commonsense reasoning benchmarks, both our 2B and 7B models consistently outperform most of their counterparts. It is worth noting that SPES-2B was trained in decentralized manner on only 16 weakly connected 48GB GPUs, yet it remains competitive with models such as MobiLLama and OpenELM, which rely on substantially larger datasets and centralized infrastructures. This highlights the effectiveness of SPES in achieving strong performance under constrained hardware budgets. Moreover, SPES-7B attains results comparable to MoE++, which employs more advanced MoE designs (e.g., zero-computation experts) and larger training corpora. These findings indicate that SPES not only scales effectively and efficiently, but also retains significant room for improvement in architecture and data utilization, underscoring its potential as an extensible alternative to existing LLM training frameworks. Using strong dense model as initialization, our largest model, SPES-9B achieves performance competitive with state-of-the-art models of comparable size using fewer than 500B tokens. We terminated training early due to resource constraints; however, metrics were still improving at the stopping point, indicating considerable remaining upside. Expert-Merging Warm-Up. As shown in Table 4, utilizing expert merging increases the average score from 50.5 to 51.3, with notable improvements on BoolQ and SciQ. This indicates that cross-node parameter sharing enhances token utilization and promotes faster knowledge establishment, thus improving generalization across range of reasoning and comprehension tasks. For ablation studies on key hyperparameters, including the merging factor α, merging Top-K, warm-up steps Tmerge, local training steps H, and the number of nodes , please refer to the Appendix for details. 5. Conclusion We introduced SPES, decentralized and memory-efficient pretraining paradigm for MoE-based LLMs. SPES assigned distinct subsets of experts to individual nodes and synchronized them, substantially reducing per-device memory usage and communication overhead compared to centralized and prior decentralized approaches. To improve token utilization per expert, we introduced an expert-merging warm-up strategy to accelerate convergence in early training stages. Empirical results on 2Band 7B-parameter MoE LLMs showed that SPES enabled efficient pretraining across weakly connected, geographically distributed GPU clusters, while achieving performance on par with comparable centralized baselines, and successfully scaled to upcycle 9B model. Beyond lowering infrastructure demands, SPES broadened access to large-scale pretraining and could support more inclusive participation in LLM research, facilitating further advances in decentralized and memory-efficient training of foundation models. Limitations and Future Work. Constrained by computational resources, our evaluation is limited to 9B parameter model trained on less than 500B tokens. Validating scalability to larger models and extended training durations remains critical direction for future research. Additionally, while this work focuses on language understanding, future efforts will investigate the applicability of SPES to multimodal reasoning and generative tasks. Extending the framework to these domains will provide more comprehensive assessment of its generalization capabilities and limitations. Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm"
        },
        {
            "title": "Impact Statement",
            "content": "This work aims to advance Machine Learning. While it has potential societal implications, we identify no specific negative consequences requiring discussion."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Allal, L. B., Lozhkov, A., Bakouch, E., Blazquez, G. M., Penedo, G., Tunstall, L., Marafioti, A., Kydlıˇcek, H., Lajarın, A. P., Srivastav, V., et al. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737, 2025. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Cui, J., Ning, M., Li, Z., Chen, B., Yan, Y., Li, H., Ling, B., Tian, Y., and Yuan, L. Chatlaw: multi-agent collaborative legal assistant with knowledge graph enhanced mixture-of-experts large language model, 2024. Dai, D., Deng, C., Zhao, C., Xu, R., Gao, H., Chen, D., Li, J., Zeng, W., Yu, X., Wu, Y., et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Bai, J., Chen, D., Qian, B., Yao, L., and Li, Y. Federated fine-tuning of large language models under heterogeneous tasks and client resources. Advances in Neural Information Processing Systems, 37:1445714483, 2024. Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. Basant, A., Khairnar, A., Paithankar, A., Khattar, A., Renduchintala, A., Malte, A., Bercovich, A., Hazare, A., Rico, A., Ficek, A., et al. Nvidia nemotron nano 2: An accurate and efficient hybrid mamba-transformer reasoning model. arXiv preprint arXiv:2508.14444, 2025. Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., OBrien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 23972430. PMLR, 2023. Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Charles, Z., Teston, G., Dery, L., Rush, K., Fallen, N., Garrett, Z., Szlam, A., and Douillard, A. Communicationefficient language model training scales reliably and arXiv preprint robustly: Scaling laws for diloco. arXiv:2503.09799, 2025. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. 8bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861, 2021. Douillard, A., Feng, Q., Rusu, A. A., Chhaparia, R., Donchev, Y., Kuncoro, A., Ranzato, M., Szlam, A., and Shen, J. Diloco: Distributed low-communication training of language models. arXiv preprint arXiv:2311.08105, 2023. Douillard, A., Feng, Q., Rusu, A. A., Kuncoro, A., Donchev, Y., Chhaparia, R., Gog, I., Ranzato, M., Shen, J., and Szlam, A. Dipaco: Distributed path composition. arXiv preprint arXiv:2403.10616, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Fan, T., Kang, Y., Ma, G., Chen, W., Wei, W., Fan, L., and Yang, Q. Fate-llm: industrial grade federated learning framework for large language models. arXiv preprint arXiv:2310.10049, 2023. Fung, P., Bachrach, Y., Celikyilmaz, A., Chaudhuri, K., Chen, D., Chung, W., Dupoux, E., Gong, H., Jegou, H., Lazaric, A., et al. Embodied ai agents: Modeling the world. arXiv preprint arXiv:2506.22355, 2025. 9 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Gale, T., Narayanan, D., Young, C., and Zaharia, M. Megablocks: Efficient sparse training with mixture-ofexperts. Proceedings of Machine Learning and Systems, 5:288304, 2023. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Geng, X. and Liu, H. Openllama: An open reproduction of llama, May 2023. URL https://github.com/ openlm-research/open_llama. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. gRPC. grpc: high performance, open source universal rpc framework. https://grpc.io/, 2015. Accessed: 2025-08-21. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Huang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu, J., Lv, C., Zhang, Y., Fu, Y., et al. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36:6299163010, 2023. Iacob, A., Sani, L., Safaryan, M., Giampouras, P., Horvath, S., Jovanovic, A., Kurmanji, M., Aleksandrov, P., Shen, W. F., Qiu, X., et al. Des-loc: Desynced low communication adaptive optimizers for training foundation models. arXiv preprint arXiv:2505.22549, 2025. Ilharco, G., Ribeiro, M. T., Wortsman, M., Gururangan, S., Schmidt, L., Hajishirzi, H., and Farhadi, A. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022. Jin, P., Zhu, B., Yuan, L., and Yan, S. Moe++: Accelerating mixture-of-experts methods with zero-computation experts. arXiv preprint arXiv:2410.07348, 2024. Johannes Welbl, Nelson F. Liu, M. G. Crowdsourcing multiple choice science questions. 2017. Kolehmainen, J., Blagoev, N., Donaghy, J., Ersoy, O., and Nies, C. Noloco: No-all-reduce low communication training method for large models. arXiv preprint arXiv:2506.10911, 2025. Kuang, W., Qian, B., Li, Z., Chen, D., Gao, D., Pan, X., Xie, Y., Li, Y., Ding, B., and Zhou, J. Federatedscope-llm: comprehensive package for fine-tuning large language models in federated learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 52605271, 2024. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. Li, J., Fang, A., Smyrnis, G., Ivgi, M., Jordan, M., Gadre, S. Y., Bansal, H., Guha, E., Keh, S. S., Arora, K., et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang, Y. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. In Bessiere, C. (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 3622 3628. International Joint Conferences on Artificial Intelligence Organization, 7 2020. doi: 10.24963/ijcai.2020/ 501. URL https://doi.org/10.24963/ijcai. 2020/501. Main track. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Jaghouar, S., Ong, J. M., Basra, M., Obeid, F., Straube, J., Keiblinger, M., Bakouch, E., Atkins, L., Panahi, M., Goddard, C., et al. Intellect-1 technical report. arXiv preprint arXiv:2412.01152, 2024. Lozhkov, A., Li, R., Allal, L. B., Cassano, F., Lamy-Poirier, J., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Mahabadi, R. K., Satheesh, S., Prabhumoye, S., Patwary, M., Shoeybi, M., and Catanzaro, B. Nemotron-cc-math: 133 billion-token-scale high quality math pretraining dataset. arXiv preprint arXiv:2508.15096, 2025. 10 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 12731282. PMLR, 2017. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Mehta, S., Sekhavat, M. H., Cao, Q., Horton, M., Jin, Y., Sun, C., Mirzadeh, I., Najibi, M., Belenko, D., Zatloukal, P., et al. Openelm: An efficient language model family with open training and inference framework. arXiv preprint arXiv:2404.14619, 2024. Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. Muennighoff, N., Soldaini, L., Groeneveld, D., Lo, K., Morrison, J., Min, S., Shi, W., Walsh, P., Tafjord, O., Lambert, N., et al. Olmoe: Open mixture-of-experts language models. arXiv preprint arXiv:2409.02060, 2024. OLMo, T., Walsh, P., Soldaini, L., Groeneveld, D., Lo, K., Arora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M., et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. Penedo, G., Kydlıˇcek, H., Lozhkov, A., Mitchell, M., Raffel, C. A., Von Werra, L., Wolf, T., et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37: 3081130849, 2024. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 35053506, 2020. Sani, L., Iacob, A., Cao, Z., Lee, R., Marino, B., Gao, Y., Cai, D., Li, Z., Zhao, W., Qiu, X., et al. Photon: Federated llm pre-training. arXiv preprint arXiv:2411.02908, 2024. Sap, M., Rashkin, H., Chen, D., LeBras, R., and Choi, Y. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R., SlimPajama: 627B Hestness, J., and Dey, N. token cleaned and deduplicated version of RedPajama, 2023. URL https://huggingface.co/ datasets/cerebras/SlimPajama-627B. Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. Su, D., Kong, K., Lin, Y., Jennings, J., Norick, B., Kliegl, M., Patwary, M., Shoeybi, M., and Catanzaro, B. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2459 2475, 2025. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Sun, Y., Li, Z., Li, Y., and Ding, B. Improving lora in privacy-preserving federated learning. arXiv preprint arXiv:2403.12313, 2024. Team, M., Xiao, C., Li, Y., Han, X., Bai, Y., Cai, J., Chen, H., Chen, W., Cong, X., Cui, G., et al. Minicpm4: arXiv preprint Ultra-efficient llms on end devices. arXiv:2506.07900, 2025. Team, Q. et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2(3), 2024. 11 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Thawakar, O., Vayani, A., Khan, S., Cholakal, H., Anwer, R. M., Felsberg, M., Baldwin, T., Xing, E. P., and Khan, F. S. Mobillama: Towards accurate and lightweight fully transparent gpt. arXiv preprint arXiv:2402.16840, 2024. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Wang, Y., Fu, Z., Cai, J., Tang, P., Lyu, H., Fang, Y., Zheng, Z., Zhou, J., Zeng, G., Xiao, C., et al. Ultra-fineweb: Efficient data filtering and verification for high-quality llm training data. arXiv preprint arXiv:2505.05427, 2025. Weber, M., Fu, D., Anthony, Q., Oren, Y., Adams, S., Alexandrov, A., Lyu, X., Nguyen, H., Yao, X., Adams, V., et al. Redpajama: an open dataset for training large language models. Advances in neural information processing systems, 37:116462116492, 2024. Zhang, J., Vahidian, S., Kuo, M., Li, C., Zhang, R., Yu, T., Wang, G., and Chen, Y. Towards building the fedIn ICASSP eratedgpt: Federated instruction tuning. 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 69156919. IEEE, 2024a. Zhang, P., Zeng, G., Wang, T., and Lu, W. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024b. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Zhang, Z., Yang, Y., Dai, Y., Wang, Q., Yu, Y., Qu, L., and Xu, Z. Fedpetuning: When federated learning meets the parameter-efficient tuning methods of pre-trained language models. In Annual Meeting of the Association of Computational Linguistics 2023, pp. 99639977. Association for Computational Linguistics (ACL), 2023. Wu, Y., Li, Learning J., Guo, Z., and Li, L. like humans: Resource-efficient federated fine-tuning through cognitive developmental stages. arXiv preprint arXiv:2508.00041, 2025. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zhu, T., Qu, X., Dong, D., Ruan, J., Tong, J., He, C., and Cheng, Y. Llama-moe: Building mixture-of-experts from llama with continual pre-training. arXiv preprint arXiv:2406.16554, 2024. URL https://arxiv. org/abs/2406.16554. Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., and Fedus, W. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. Xue, F., Zheng, Z., Fu, Y., Ni, J., Zheng, Z., Zhou, W., and You, Y. Openmoe: An early effort on open arXiv preprint mixture-of-experts language models. arXiv:2402.01739, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Ye, R., Wang, W., Chai, J., Li, D., Li, Z., Xu, Y., Du, Y., Wang, Y., and Chen, S. Openfedllm: Training large language models on decentralized private data via federated learning. In Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining, pp. 61376147, 2024. Yi, L., Yu, H., Wang, G., Liu, X., and Li, X. pfedlora: Model-heterogeneous personalized federated learning with lora tuning. arXiv preprint arXiv:2310.13283, 2023. Yu, Y., Dai, Z., Wang, Z., Wang, W., Chen, R., and Pei, J. Opencsg chinese corpus: series of highquality chinese datasets for llm training. arXiv preprint arXiv:2501.08197, 2025. Zhang, B. and Sennrich, R. Root mean square layer normalization. Advances in neural information processing systems, 32, 2019. 12 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm"
        },
        {
            "title": "Appendix",
            "content": "We provide the following materials in this appendix: A. Theoretical Analysis: the convergence analysis of SPES. B. Implementation Details: more details of training hyper-parameters. C. Data Details: dataset descriptions and sampling ratios. D. Evaluation Details: evaluation datasets and metrics. E. Additional Results: results on additional benchmarks and ablations on hyper-parameters. F. Declaration of LLM Assistance: description of LLM usage in manuscript preparation. A. Theoretical Analysis of SPES We study the convergence of SParse Expert Synchronization (SPES). SPES performs block-sparse local updates: all nodes update the shared parameters, while each expert block is updated only by its owner node. We also model the expert-merging warm-up as an additional (early-stage) mixing perturbation applied after synchronization. A.1. Problem Setup and Notation We minimize min θ (θ) := 1 (cid:88) i=1 fi(θ), fi(θ) := EξDi (cid:2)ℓ(θ; ξ)(cid:3), (7) where each data sample ξ is drawn from the local distribution Di. Here, ℓ(θ; ξ) denotes the per-sample loss. θ = (ψ, Φ) and Φ = {ϕj}M i=1 be partition of {1, . . . , }. Node ηi owns experts in Pi; denote o(j) the unique owner of expert j. j=1 are the shared and expert parameters, respectively. Let {Pi}N A.2. SPES Update Rule Let θ(t) be the global model at the beginning of round t. Each node sets θ(t,0) decent steps with step size η: = θ(t) and runs local stochastic gradient θ(t,h+1) = θ(t,h) η Ui g(t,h) , = 0, . . . , 1, (8) where Ui is block mask that keeps updates only on (ψ, {ϕj : Pi}), and Uiv for any vector v. Sparse synchronization. After steps, the server averages shared parameters and assigns each expert from its owner: ψ(t+1,pre) := (cid:88) ψ(t,H) , 1 ϕ(t+1,pre) i=1 := ϕ(t,H) j,o(j) j. Let θ(t+1,pre) = (ψ(t+1,pre), Φ(t+1,pre)) denotes the pre-merging parameters. Expert-merging warm-up. For < Tmerge, we apply the merging step (Section 3.3): ϕ(t+1) := ϕ(t+1,pre) + αt 1 (cid:88) (cid:16) kQj ϕ(t+1,pre) ϕ(t+1,pre) (cid:17) , (9) (10) (11) with αt [0, 1] (and αt = 0 for Tmerge). Shared parameters are unchanged: ψ(t+1) = ψ(t+1,pre). Define the merge displacement (t+1) merge := Φ(t+1) Φ(t+1,pre). (12) 13 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Equivalent pre-merge update. Define the per-round averaged stochastic directions (before merging) (cid:98)g(t) ψ := (cid:98)g(t) ϕj :="
        },
        {
            "title": "1\nH",
            "content": "H1 (cid:88) h=0 H1 (cid:88) h="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 g(t,h) i,ψ , g(t,h) o(j),ϕj . θ(t+1,pre) = θ(t) γ (cid:98)g(t), γ := ηH."
        },
        {
            "title": "Then",
            "content": "A.3. Assumptions Assumption 1 (Smoothness). Each fi is L-smooth: fi(x) fi(y) Lx y, x, y, i. Assumption 2 (Stochastic gradients). For all i, t, h, (cid:104) g(t,h) θ(t,h) (cid:105) = fi (cid:16) θ(t,h) (cid:17) , and there exist σ2 ψ, σ2 Φ 0 such that (cid:20)(cid:13) (cid:13)g(t,h) (cid:13) i,ψ ψfi(θ(t,h) (cid:13) (cid:13) ) (cid:13) (cid:88) jPi (cid:13) (cid:13)g(t,h) (cid:13) i,ϕj ϕj fi(θ(t,h) (cid:13) 2 (cid:13) ) (cid:13) (cid:21) θ(t,h) σ2 ψ, σ2 Φ, θ(t,h) 2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) and Eg(t,h) 2 G2 for some > 0. (The last bound is used to control local drift.) Assumption 3 (Expert-gradient heterogeneity). There exists ζΦ 0 such that for all θ and all j, (cid:13)ϕj fo(j)(θ) ϕj (θ)(cid:13) (cid:13) (cid:13) ζΦ. In particular, ζΦ = 0 under IID data. Assumption 4 (Bounded merge displacement). For < Tmerge, there exists Bmerge 0 such that (cid:20)(cid:13) (cid:13)(t+1) (cid:13) merge 2(cid:21) (cid:13) (cid:13) (cid:13) α2 B2 merge. (13) (14) (15) (16) (17) (18) (19) (20) (21) A.4. Main Convergence Result Theorem 1 (Convergence of SPES). Suppose Assumptions 14 hold and γL 1 1, without expert warm-up merging 4 . Let Finf := inf θ (θ). Then for any 1 1 (cid:88) t=0 (cid:104) (θ(t))2(cid:105) 4 (cid:0)F (θ(0)) Finf ηH (cid:1) + 6ηL (cid:16) σ2 ψ (cid:17) + σ2 Φ + 12L2η2H 2G2 + 12 ζ 2 Φ (22) Using expert warm-up merging, we get 1 1 (cid:88) t= sup θ (θ(t))2(cid:105) (cid:104) 4 (cid:0)F (θ(0)) Finf ηH (cid:1) + 6ηL (cid:16) σ2 ψ (cid:17) + σ2 Φ + 12L2η2H 2G2 + 12 ζ 2 Φ + merge Tmerge1 (cid:88) ηH t=0 14 α Constant. (23) Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Discussion. The shared block enjoys 1/N variance reduction (term σ2 owner-only (term σ2 vanishing perturbation when (cid:80) ψ/N ) due to averaging, while expert updates are Φ captures data heterogeneity in expert blocks. The merging warm-up appears as is small (e.g., decaying αt and Tmerge ). Φ). The bias ζ 2 α2 t<Tmerge A.5. Proof of Theorem 1 We bound descent for the pre-merge iterate and then account for merging as smooth perturbation. 1) Pre-merge descent. By L-smoothness, for = γv, (y) (x) γF (x), + Lγ2 2 v2. (24) Apply equation 24 with = θ(t), = (cid:98)g(t), and write (cid:98)g(t) = (θ(t)) + e(t). Using γL 1 yields 4 and AMGM inequality EF (θ(t+1,pre)) EF (θ(t)) γ 4 EF (θ(t))2 + 3γ 4 Ee(t)2. (25) 2) Bounding the gradient error Ee(t)2. Decompose e(t) into variance (stochasticity) and bias (local drift + heterogeneity). Using Assumption equation 18equation 19 and the averaging in equation 13equation 14 gives (cid:20)(cid:13) (cid:13) (cid:13)(cid:98)g(t) E[(cid:98)g(t) θ(t)] (cid:13) (cid:13) (cid:13) 2(cid:21) σ2 ψ + σ2 Φ . (26) For the bias, local SGD drift over steps satisfies Eθ(t,h) hence by smoothness Efi(θ(t,h) order L2η2H 2G2 on both shared and expert blocks, and Assumption equation 20 adds ζ 2 θ(t)2 η2h2G2 (from Assumption 2 and Uiv v), ) fi(θ(t))2 L2η2h2G2. Averaging over gives bias contribution of Φ on expert blocks. Overall, Ee(t)2 2 (cid:16) σ2 ψ + (cid:17) σ2 Φ + 4L2η2H 2G2 + 4ζ 2 Φ. (27) 3) Telescoping. Plug equation 27 into equation 25, sum over = 0, . . . , 1, and use (θ(T,pre)) Finf to obtain equation 22 (with γ = ηH). 4) Effect of merging. Only experts change during merging, i.e., θ(t+1) θ(t+1,pre) = (0, (t+1) and Cauchy-Schwarz inequality, merge). By L-smoothness (θ(t+1)) (θ(t+1,pre)) + 1 2L ΦF (θ(t+1,pre))2 + L(t+1) merge2. (28) Summing equation 28 across rounds contributes an additive term proportional to (cid:80) from Assumption equation 21. E(t+1) merge2, yielding equation 23 B. Implementation Details Table A1 details the full training configurations. For the from-scratch experiments (2B and 7B models), we adhere to these settings for the first 70% of total training tokens; thereafter, we halve the per-node batch size and set = 50 to accelerate convergence. For the 1B model, we perform ablation on expert-merging with per-node batch size of 1024 to facilitate comparison with baselines trained under larger token budgets (400B). The training token budget is set to 100B for the ablations on and , and 50B for α and Tmerge to allow faster validation. For all experiments, the loss coefficients are fixed across the models as follows: cross-entropy (1), load-balancing (0.01), MoE z-loss (0.001), and standard z-loss (1 105). 15 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Table A1. Training hyperparameters for different model scales. Maximum Learning Rate Minimum Learning Rate Optimizer ϵ Weight Decay (β0, β1) LR Warmup Steps Sequence Length Batch Size (Tokens) Synchronization Steps 9B 7B 2B 1B 1 104 1 105 1 108 0.1 (0.9, 0.95) 2000 4096 0.5M 4 100 4 104 4 105 1 108 0.1 (0.9, 0.95) 2000 2048 2M 4 100 5 104 5 105 1 108 0.1 (0.9, 0.95) 2000 2048 0.5M 16 100 5 104 5 105 1 108 0.1 (0.9, 0.95) 2000 2048 0.5M 4 Table A2. Dataset sampling ratios for the from-scratch training regimen. Dataset Ultra-FineWeb SlimPajama StarCoder arXiv OpenWebMath Pes2o Algebraic Stack Ratio (%) 64. 27.2 6.6 0.7 0.4 0.5 0. C. Details of Datasets and Sampling Ratio We train the model on data sampled from several open-source corpora, with sampling ratios provided in Table A2 and Table A3. Following OLMo et al. (2024), we apply filter that removes all documents containing sequences of 32 or more repeated n-grams (an n-gram denotes any span of 113 tokens). The uses datasets are summarized as follows. Ultra-FineWeb. Ultra-FineWeb (Wang et al., 2025) is large-scale web corpus constructed from FineWeb (Penedo et al., 2024) and Chinese FineWeb (Yu et al., 2025) using an efficient verification-based filtering pipeline. The approach combines lightweight fastText classification with verification mechanism, enabling reliable data selection at substantially reduced computational cost. The final corpus comprises roughly 1 trillion English tokens and 120 billion Chinese tokens. By enhancing overall data quality, Ultra-FineWeb provides strong foundation for LLM training and contributes to the dataset used in MiniCPM4 (Team et al., 2025). OLMo-Mix-1124. OLMo-Mix-1124 is 3.9-trillion-token corpus comprising over 95% web data, constructed from DCLM (Li et al., 2024), Dolma v1.7 (Soldaini et al., 2024), and StarCoder (Lozhkov et al., 2024). For our work, we extract scientific-domain subsets, including arXiv, OpenWebMath, Algebraic Stack, peS2o, and StarCoder. Nemotron Pretraining Dataset.2 Nemotron-Pretraining (Basant et al., 2025) is large-scale corpus collected for the NVIDIA Nemotron Nano 2 family, this dataset emphasizes high-value math, code, and multilingual Q&A to fuel globallycapable models. It aggregates four specialized components: 133B-token math corpus (Mahabadi et al., 2025) processed via novel Lynx + LLM pipeline, an updated English web crawl enriched with synthetic data (Su et al., 2025), rigorously filtered source code dataset, and diverse SFT-style collection covering STEM and reasoning domains. SlimPajama. SlimPajama (Soboleva et al., 2023) is large-scale, rigorously deduplicated corpus constructed from RedPajama (Weber et al., 2024). Using multi-stage pipeline that combines quality filtering with MinHashLSH-based deduplication at trillion-token scale, SlimPajama substantially reduces redundancy and low-quality content, compressing the dataset from 1.21T to 627B tokens while retaining domain coverage. The corpus spans diverse sources, including CommonCrawl, C4, GitHub, Books, ArXiv, Wikipedia, and StackExchange. D. Evaluation Details We evaluate our models with the lm-evaluation-harness library (Gao et al., 2024), which offers standardized benchmark implementations and facilitates direct comparison with prior work. All experiments use version 0.4.7. The benchmarks and evaluation settings are detailed below: SciQ (Johannes Welbl, 2017) is science multiple-choice question-answering dataset. The questions were generated by 2https://huggingface.co/collections/nvidia/nemotron-pre-training-datasets 16 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Table A3. Dataset sampling ratios for the upcycling training regimen. Dataset Nemotron-CC-V2 Nemotron-Math-V1 Nemotron-Pretraining-Code Nemotron-Pretraining-SFT Ratio (%) 63.3 16. 11.9 8.4 crowdworkers and validated against science reference materials, covering topics such as physics, biology, and chemistry. As the questions are designed to resemble real exam-style queries, the dataset tests scientific knowledge and reasoning skills of model. We report 0-shot accuracy on SciQ. ARC (Clark et al., 2018) (AI2 Reasoning Challenge) consists of grade-school level science exam questions, partitioned into ARC-Easy (ARC-E) and ARC-Challenge (ARC-C). ARC-E contains questions that can often be answered by retrieval of surface-level facts, while ARC-C includes the more demanding questions requiring reasoning and multi-step inference across scientific facts. We report 0-shot accuracy on ARC-E and 25-shot normalized accuracy on ARC-C. SIQA (Sap et al., 2019) (SocialIQA) benchmarks social commonsense reasoning. Each instance presents short humancentered scenario alongside question about likely intents, causes, or outcomes of human actions. This evaluates the models ability to handle subtle social reasoning and cause-effect relationships in naturalistic settings. We report 0-shot normalized accuracy on SIQA. PIQA (Bisk et al., 2020) (Physical Interaction QA) evaluates physical commonsense reasoning in everyday situations. Given description of goal, the model must choose the most plausible solution among two alternatives, testing physical feasibility and everyday world knowledge. We report 0-shot normalized accuracy on PIQA. OpenBookQA (Mihaylov et al., 2018) presents multiple-choice science questions paired with small open-book of 1,326 core scientific facts. Answering the questions typically requires combining knowledge from the book with additional commonsense reasoning, making this benchmark particularly challenging. We report 0-shot normalized accuracy. WinoGrande (Sakaguchi et al., 2021) is large-scale dataset for pronoun resolution, created to reduce annotation artifacts common in earlier benchmarks (e.g., Winograd Schema Challenge). Each instance requires the model to resolve ambiguous pronouns based on contextual clues, testing commonsense reasoning and language understanding. We report 0-shot accuracy on WinoGrande. BoolQ (Clark et al., 2019) is reading comprehension dataset in the yes/no QA format. Questions are naturally occurring user queries, paired with passages from Wikipedia that may or may not contain the answer. Models must perform passage-level understanding to correctly infer the response. We report 0-shot accuracy on BoolQ. C-Eval (Huang et al., 2023) is comprehensive Chinese evaluation suite consisting of over 13,000 multiple-choice questions spanning 52 subjects, from elementary school topics to professional certification exams. It provides fine-grained view of model performance in academic and professional domains under Chinese cultural and linguistic settings. We report 0-shot accuracy on C-Eval. LogiQA (Liu et al., 2020) is dataset sourced from expert-written questions designed to evaluate machine reading comprehension through logical reasoning. It consists of 8,678 QA instances that cover multiple types of deductive reasoning, serving as benchmark where state-of-the-art models still trail the human ceiling. We report 0-shot normalized accuracy. MMLU (Hendrycks et al., 2020) (Massive Multitask Language Understanding) covers 57 tasks across diverse domains such as mathematics, history, law, medicine, and the natural sciences. As broad knowledge benchmark, it measures both factual recall and domain-specific reasoning. We follow standard settings and report 5-shot accuracy on MMLU. E. Additional Results Results on Additional Benchmarks. Table A4 reports the performance of our models on additional benchmarks. On general knowledge benchmark, SPES-7B surpasses the comparable baseline MoE++ (26.2 vs. 23.6 on C-Eval, 24.9 vs. 24.6 on MMLU), while maintaining competitive performance on other tasks. This indicates that SPES can match the performance of centrally trained models under resource-constrained settings, underscoring its potential to lower the barrier to LLM pretraining. In addition, SPES-2B attains performance on par with models of similar scale using only 16 weakly connected nodes, further validating the efficiency of our approach. 17 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Table A4. Performance comparison with previous LLMs on additional benchmarks. Some models are excluded because they neither report results on these benchmarks nor are compatible with lm-evaluation-harness. Method #Params #Tokens OBQA LogiQA C-Eval MMLU Models Trained with Significantly More Tokens Qwen2.5-0.5B (Qwen et al., 2025) Qwen3-0.6B (Yang et al., 2025) Llama3.2-1B (Dubey et al., 2024) Qwen2.5-1.5B (Qwen et al., 2025) SmolLM2-1.7B (Allal et al., 2025) Qwen3-1.7B (Yang et al., 2025) OLMoE-1B-7B (Muennighoff et al., 2024) 0.5B/0.5B 0.6B/0.6B 1.1B/1.1B 1.5B/1.5B 1.7B/1.7B 1.7B/1.7B 1.3B/7B 18T 36T 9T 18T 11T 36T 5T Models with 3B Parameters MobiLlama-0.8B (Thawakar et al., 2024) TinyLlama-1.1B (Zhang et al., 2024b) OPT-1.3B (Zhang et al., 2022) MobiLlama-1.3B (Thawakar et al., 2024) Pythia-1.4B (Biderman et al., 2023) OPT-2.7B (Zhang et al., 2022) Pythia-2.8B (Biderman et al., 2023) SPES-2B (ours) 0.8B/0.8B 1.1B/1.1B 1.3B/1.3B 1.3B/1.3B 1.4B/1.4B 2.7B/2.7B 2.8B/2.8B 0.8B/2.1B 1.3T 3T 180B 1.3T 300B 180B 300B 500B Models with 7B Parameters MoE++ 7B (Jin et al., 2024) LLaMA-MoE-3.0B (Zhu et al., 2024) SPES-7B (ours) SPES-9B (ours) 1.2B/7B 3.0B/7B 1.6B/7B 3.1B/9B 1T 2.2T 500B 400B 35.4 34.2 36.2 40.4 43.6 38.6 45. 33.0 36.8 33.4 35.4 33.4 35.2 35.6 31.4 40.0 - 39.4 42.2 29.5 29.0 - 31.5 29.8 31.5 28.4 - 26.3 26.9 - 27.3 26.0 28.0 27.2 28.4 30.6 27.5 30.4 51.0 50.4 30.9 68.2 32.5 65.2 31. 22.7 26.0 23.0 26.2 23.0 23.0 22.9 25.0 23.6 - 26.2 44.7 47.3 52.8 36.6 59.7 48.4 62.6 50.5 23.5 25.3 24.9 25.3 24.2 25.6 25.2 25.5 24.6 26.8 24.9 63.7 Table A5. Performance comparison with different numbers of nodes. No. of Nodes ARC(e) ARC(c) PIQA SciQ OBQA BoolQ SIQA WinoGrande Avg. 2 4 8 52.0 51.8 47.9 25.7 27.4 24.6 68.7 67.4 66. 77.6 75.3 70.8 30.4 29.8 29.4 58.0 49.5 60.1 42.2 43.7 42.8 50.4 52.6 53.9 50.6 49.7 49. Ablation on Number of Nodes. We then study the impact of varying the number of nodes while keeping the global batch size fixed. As shown in Table A5, model performance remains stable when scaling from 2 to 8 nodes. The average score decreases slightly from 50.6 (2 nodes) to 49.5 (8 nodes), yet SPES maintains competitive results across benchmarks. This behavior illustrates natural trade-off in decentralized sparse training: increasing the number of nodes leads to greater fragmentation of training data and experts, which can modestly slow convergence. Nonetheless, the results underscore the robustness of SPES. Even with reduced per-node token utilization, it maintains overall performance. These findings demonstrate SPES potential of scalability, suggesting that it can effectively leverage larger number of participants while maintaining model quality, key property for practical deployment in heterogeneous, distributed environments. Ablation on Hyperparameters in Expert Merging. Fig. A1 shows the effect of varying merging warmup steps Tmerge, the merging factor α and merging Top-K on performance. moderate warmup of 12.5k steps achieves the best results, as shorter schedules hinder sufficient knowledge exchange, while excessively long ones interfere with expert specialization. Similarly, performance peaks when α is set to 0.1 and is set to 4, with both smaller and larger values leading to degradation. These observations suggest that effective expert merging requires careful balance between inter-expert knowledge sharing and expert specialization. Overly aggressive merging may overwrite expert-specific information, whereas insufficient merging yields only minor parameter updates and limits the efficiency of knowledge sharing, thereby slowing the establishment of general expert representations. Ablation on Synchronization Steps. We analyze the effect of varying the local update interval in the SPES framework. 18 Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Figure A1. Ablation on key hyper-parameters in expert merging. The reported average is computed over ARC(e), SciQ, PIQA, WinoGrande, ARC(c), OBQA, OpenBookQA, and SIQA. Figure A2. Ablation on synchronization steps. The reported average is computed over eight benchmarks in total, additionally including ARC(c), OBQA, OpenBookQA, and SIQA. As illustrated in Fig. A2, performance declines when increases from 50 to 200 or 400. This trend reflects key trade-off in decentralized sparse training: while larger reduces communication frequency, it amplifies model divergence across nodes, weakening the benefits of expert sharing. Overall, = 50 provides the best balance between communication efficiency and model quality, underscoring the necessity of frequent synchronization to fully exploit SPES sparse expert updates under bandwidth-limited decentralized settings. F. Declaration of LLM Assistance We use ChatGPT-5 to assist with the refinement of this manuscript. After drafting the full text, we provided selected passages to the models for suggestions on grammar, clarity, and conciseness. All revisions were reviewed and finalized by the authors to ensure accuracy and appropriateness."
        }
    ],
    "affiliations": [
        "Department of Computing, The Hong Kong Polytechnic University",
        "OPPO Research Institute"
    ]
}