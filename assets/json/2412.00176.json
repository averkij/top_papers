{
    "paper_title": "Art-Free Generative Models: Art Creation Without Graphic Art Knowledge",
    "authors": [
        "Hui Ren",
        "Joanna Materzynska",
        "Rohit Gandikota",
        "David Bau",
        "Antonio Torralba"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We explore the question: \"How much prior art knowledge is needed to create art?\" To investigate this, we propose a text-to-image generation model trained without access to art-related content. We then introduce a simple yet effective method to learn an art adapter using only a few examples of selected artistic styles. Our experiments show that art generated using our method is perceived by users as comparable to art produced by models trained on large, art-rich datasets. Finally, through data attribution techniques, we illustrate how examples from both artistic and non-artistic datasets contributed to the creation of new artistic styles."
        },
        {
            "title": "Start",
            "content": "Art-Free Generative Models: Art Creation Without Graphic Art Knowledge Hui Ren,1, Joanna Materzynska,2 David Bau3 Antonio Torralba2 1ShanghaiTech University 3Northeastern University Rohit Gandikota3 2MIT Project Webpage 4 2 0 2 9 2 ] . [ 1 6 7 1 0 0 . 2 1 4 2 : r Figure 1. (a) We introduce Art-Free SAM, carefully curated text-to-image dataset with minimal graphic art content, used to pretrain Art-Free Diffusion model (θ). Our paper investigates whether an art-agnostic model can learn art styles using LoRA Art Adapter θ. (b) We show three famous artists styles reproduced and generalized by Art-Free Diffusion after exposing an Art Adapter to small sample (A) of each artists work."
        },
        {
            "title": "Abstract",
            "content": "We explore the question: How much prior art knowledge is needed to create art? To investigate this, we propose text-to-image generation model trained without access to art-related content. We then introduce simple yet effective method to learn an art adapter using only few examples of selected artistic styles. Our experiments show that art generated using our method is perceived by users as comparable to art produced by models trained on large, art-rich datasets. Finally, through data attribution techniques, we illustrate how examples from both artistic and non-artistic datasets contributed to the creation of new artistic styles. 1. Introduction Is exposure to art truly necessary for creating it? Could someone who has never seen painting, sculpture, or sketch still produce meaningful visual art? In world saturated with cultural influences and artistic traditions, this question becomes challenging to answer. Movements like Outsider Art have already begun to explore the notion that artistic expression can emerge independently of formal training or exposure to traditional art forms. Outsider Art showcases the work of self-taught individuals who, largely disconnected from the art world, create without the influence of established conventions. more specific subset, Art Brut, focuses on the raw, unfiltered creativity of those entirely outside the established art scenepsychiatric patients, hermits, and spiritualistspeople whose art emerges purely from internal drives, uninformed by external artistic influences. Inspired by these movements, we simulate an artificial artist with minimal exposure to art. In this synthetic experiment, we wanted to train text-to-image model primarily on natural images, with no exposure to visual art. Then adapt the model using few examples from specific artistic style to study how well the adapted model can mimic and generalize that style across different contexts. Powerful text-to-image generators have already proved their ability to produce art, some even winning prestigious competitions [29]. However, their ability is typically attributed to extensive training on large datasets rich with visual art. These models are often so familiar with specific artists styles that they can replicate them simply by including the artists name in text prompt [16]. This ease of replication has raised ethical concerns, sparking lawsuits from artists who argue that generative models are imitating their work without permission [9]. In this work, we challenge this paradigm by asking: Can model with minimal prior exposure to art, but trained on selected style, compare to these powerful models? Can artistic ability be achieved with just handful of images, in controlled manner? To explore this, we first develop an art-agnostic model (Art-Free Diffusion) that deliberately excludes prior knowledge of visual art. We create an art-free dataset (Art-Free SAM) using rigorous filtering method based on both captions and image content to ensure that no artistic elements are included 1 Fig.1 a). We introduce Art Adapter, controlled way to inject approved artistic knowledge into art-agnostic models using LoRA adapters. This enables the Art-Free model to learn and reproduce art styles using only handful of training artwork samples Fig.1 Fig.1 b). Our method can successfully learn different artistic styles, of diverse techniques, despite having access to only few examples of the art style. How can an Art-Free model achieve this after being exposed to just few examples of artistss work? To answer this question, we apply data attribution method [54] to analyze which training examples most influenced the generation of the new artistic samples. Intriguingly, our analysis reveals that the natural images used in training significantly contribute to the artistic generation processmirroring the way the natural world shapes real artistic expression. We found that even abstract art imitations have top attributed images from the Art-Free SAM dataset, illustrating how real world can be an inspiration to art. We evaluate our approach of art creation with minimal prior artistic knowledge, using measurements of similarity to real art, crowdsourced evaluations of artistic efficacy, data attribution analysis, and an in-depth interview with an artist examining imitations of his own style. Our experiments show that this approach can successfully mimic artistic styles, achieving results comparable to models trained on vast amounts of data. 2. Related work Image Generation. Text-to-image models have garnered significant attention and popularity, particularly with the advent of open-sourced diffusion models [41, 49, 50]. These models have dramatically improved the quality and fidelity of generated images to user-defined prompts, revolutionizing the field of generative AI. Notably, generated images have not only gained acclaim by winning art competitions [29], but they have also sparked controversy, leading to lawsuits from artists against companies releasing these models [15]. The concerns largely revolve around the models ability to replicate specific artists styles [2] and memorization of some training data [46, 47]. Mitigating Ethical Concerns. In response to these chal1Our manual inspection shows that the Art-Free dataset may contain 0.14% of graphic art. Figure 2. Examples recognized as art and not art when creating Art-Free SAM. lenges, the computer vision community has proposed several mitigation techniques. Opt-out strategies allow the removal of specific concepts from model weights [11, 12, 17, 22, 27, 31, 32, 35, 38, 55], though these methods often struggle with scalability when dealing with large numIt has been also shown that the erased ber of concepts. concepts can be re-introduced to the model [37]. Industry initiatives are enabling individuals to opt out of training datasets [51], though these strategies vary in effectiveness and do not fully address overfitting or unauthorized style replication. Another approach involves watermarking training images [7, 33, 56] to enhance traceability and protect intellectual property. Unlike these in-training safeguards, our work explores what can be introduced into models posttraining. Gokaslan et al. [14] trained text-to-image model on Creative Commons (CC) images to address ethical concerns, but this approach is limited as CC images can still contain artwork that raises similar ethical issues. In contrast, our work tackles both ethical concerns and the technical challenge of adapting model trained exclusively on natural images to learn and reproduce artistic styles posttraining, offering new perspective on ethical model development. related approach in NLP by Min et al. [34] involved training language model on specific dataset and later introducing external data for specialized tasks. Similarly, our method explores incorporating new data posttraining, though in the visual domain. Lastly, initiatives focusing on training data transparency emphasize ethical AI development [30]. Our work contributes to this conversation by proposing way to integrate artistic concepts posttraining, providing potential solution to ethical concerns raised by artists and stakeholders. Style Transfer versus Art Adaptation. Transferring visual features from one image to another has long been #sample paintings stamp sculptures digital art logo artwork sketch advertisement drawing illustration installation art mosaic art tapestry baroque art art noveau pop art total SA-1Bori SA-1Bf iltered COCOori COCOf iltered 10,000 10,000 5,000 5,000 36 0 22 3 71 0 0 0 120 52 3 1 14 3 9 36 9 0 0 0 0 10 1 0 0 4 0 2 1 5 0 8 0 0 0 4 1 0 12 0 0 0 1 0 0 0 3 0 0 0 6 0 0 0 1 0 0 0 2 0 0 315 ( 3.15%) 67 ( 0.67%) 53 ( 1.06%) 6 ( 0.12%) Table 1. Statistics of artistic images found during manual inspection of the SA-1B and COCO datasets before and after the art filter. central topic in computer vision. Image analogies [19], for instance, use pair of example images to demonstrate desired transformation, which can then be applied to new image to achieve similar visual effects. Likewise, image quilting [10] transfers textures by stitching together small, local patches from source image, much like assembling quilt, to synthesize seamless textures on new canvas. Deep learning methods like Neural Style Transfer [13] take this further by using convolutional neural networks to extract and recombine deep feature representations of content and style, allowing an images content to be re-rendered in the artistic style of reference image. Our method extends beyond traditional texture transfer and image stylization, as we adapt an image generator to new domain, enabling both the sampling of entirely new images and the stylization of existing ones. Recently, methods have been proposed [5, 18] that manipulate image activations to impose given reference style onto an image. However, we hypothesize that these methods succeed in style transfer largely because the underlying diffusion model possesses inherent artistic capabilities. In contrast, we find that our Art-Free Diffusion model is unable to successfully apply the style (Fig. 3; Sec. 6.2). This challenge brings us to our central question: can adding small amount of art-specific training equip an art-free model with the necessary skills for style transfer? Figure 3. Our Art-Free Diffusion model shows limited style transfer with training-free methods, suggesting that traditional models may rely on inherent artistic biases. Unlike our model, traditional models have seen vast amounts of art, enabling them to internalize stylistic patterns for effective style transfer. 3. Preliminary Diffusion models [21] represent class of generative models capable of producing high-quality images by modeling data distributions through successive denoising steps. Intuitively, the forward process incrementally introduces noise to the data, transforming it into Gaussian noise over time. At any given time step, the relationship between the image and the noise can be expressed as: Xt = (cid:112)1 βt X0 + βt ϵ (1) where Xt represents the image at time step t, X0 is the original image, ϵ denotes Gaussian noise with zero mean and unit variance, and βt is an increasing sequence of noise levels. During the reverse process, the model is trained to predict and eliminate the noise ϵ at each time step to reconstruct the original image. The learning objective can be formulated as: ϵθ(Xt, C, t) ϵ2(cid:105) (cid:104) min θ (2) Where ϵθ is the model, is the condition, which, in our case, is the text prompt. Our model adopts the architecture of the latent diffusion model [42]. 4. Art-free text-to-image diffusion model Art-agnostic dataset. To train an art-agnostic text-toimage model, we require large text-image dataset that is art-free. Most commonly used datasets contain numerous examples of art and paintings as diverse visual content is desirable for image generators. We leverage the SAMLLava-Captions10M dataset [3], which is derived from the SA-1B dataset [26] primarily intended for object segmentation in natural, open-world images. We chose this dataset because the images in the SA-1B dataset were captured using camera and are specifically intended to exclude any artworks. The text captions for SAM dataset are generated by Large Vision-Language Model (Llava). Prior work has shown that automatically generated captions can also also be effective for training text-to-image models [3]. Although the dataset primarily focuses on natural images, we find that it still included instances of visual art, such as stamps, paintings, and other artistic elements. While the dataset may not have been intentionally curated to include artworks, visual art is often difficult to avoid in real-world images. For example, we find photographs of tapestries and baroque architecture featuring artistic details that are clearly art. Moreover, artistic expression frequently appears in unexpected places, from sculptural designs to logos and branding on everyday objects. Our goal is to distinguish between visual art and natural imagery, ensuring that everyday scenes and objects were represented posed to art, we train both the VAE and UNET from scratch with our Art-Free SAM dataset. The pretrained diffusion models usually use CLIP as the text encoder [36, 40], which is trained contrastively to learn associations between images and text. Previous works [25] show that CLIP embedding can manipulate images even in unseen domains. To prevent any art-related knowledge from leaking through the text embeddings, we instead use language-only Text Encoder based on BERT [8]. While the BERT may contain some conceptual knowledge of art, its training process has no access to any visual representations or pixel data containing art, ensuring that the model remains art-free. 5. Artistic Style Adapter Diffusion models [21] represent class of generative models capable of producing high-quality images by modeling data distributions through successive denoising steps. Intuitively, the forward process incrementally introduces noise to the data, transforming it into Gaussian noise over time. At any given time step, the relationship between the image and the noise can be expressed as: Xt = (cid:112)1 βt X0 + βt ϵ (3) where Xt represents the image at time step t, X0 is the original image, ϵ denotes Gaussian noise with zero mean and unit variance, and βt is an increasing sequence of noise levels. During the reverse process, the model is trained to predict and eliminate the noise ϵ at each time step to reconstruct the original image. The learning objective can be formulated as: ϵθ(Xt, C, t) ϵ2(cid:105) (cid:104) min θ (4) where ϵθ is the model, is the condition, which, in our case, is the text prompt. Our model adopts the architecture of the latent diffusion model [42]. To train an Art-Style Adapter we collect few examples of artworks in specific style X0 and caption the content of the artwork. This can be done automatically or manually. To connect the newly learned style information with specific tokens in the prompt, we append text in the style of V* art to the content prompt, denoted as . To enable the model to learn this new artistic style, we fine-tune the U-Net module using LoRA [23]. For given target artistic image, we define the following loss: LS = ϵθθ(Xt, , t) ϵ2 (5) Where ϵθθ is the U-Net module with the LoRA updating weights, is the denoising time step, Xt is the input image at time t, and ϵ is target noise. We refer to this loss as style loss, as it helps the model implicitly learn the artistic Figure 4. Our model has no prior knowledge of art. It not only fails to generate the artwork indicated by the prompts, but its outputs also lack any apparent stylistic elements. while minimizing intentional artistic expression. We illustrate in Fig. 2 where we draw the line between an art image and not an art image, specifically, we focus on removing graphic arts, and leave other forms of art like architecture. To ensure that our training set is free from incidental visual art, we develop two-stage filtering method. In the first stage, we implement text-based filtering by searching for specific terms in image captions that indicate the presence of visual art. We exclude images whose captions contain keywords such as painting, art, or drawings. In the second stage, we compute cosine similarity alignment score between each image and set of art-related terms using the CLIP score [40]. By manually sampling and ordering images by score for each term, we identify threshold beyond which the images no longer contained visual art. We refer the reader to SupMat. for further details of the filtering process and the comprehensive keyword list of the art terms. Our resulting Art-Free SAM dataset, constructed from SAM-LLava-Captions10M, retains 9,119,455 images after removing 4.7% through text-based filtering and 16.7% through image-based filtering. We designate 9,140 images as validation set, yielding final training dataset of 9,110,315 image-text pairs. To validate the generalizability of our filtering method, we conducted qualitative manual reviews on both the COCO-2017 and SA-1B datasets. In an initial random sample of 10,000 images from the original SAM dataset, we identified 315 images containing artworks, primarily sculptures, stamps, logos, and paintings. Post-filtering analysis of another 10,000-image sample revealed only 72 images containing artworks, predominantly sculptures. Similar evaluation on the COCO dataset, using 5,000-image random sample, demonstrated reduction in art-containing images from 1.06% to 0.12%. Table 2 presents the statistics of samples from both datasets before and after filtering. We will release the Art-Free SAM dataset upon publication. Model architecture. Our Art-Free Diffusion model is built on latent diffusion architecture [42] and has three main modules: the VAE encoder, the UNET, and the Text Encoder. To ensure that no module of our model has been exFigure 5. The generated image should match the style of small exemplar dataset when prompted with caption , which includes style prefix . For example, if = People walking along riverside path with colorful trees in the style of , the image should reflect both the scene (content) and the specified artistic style. Content loss ensures that the visual elements of the prompt = People walking along riverside path with colorful trees are accurately depicted, while style loss maintains the distinct artistic qualities associated with . style and link it to the style modification in the prompt. The content loss is defined as follows: LC = ϵθθ(Xt, C, t) ϵθ(Xt, C, t)2 (6) this loss helps maintain the prompts content even when the style identifier is omitted from the text. Our final loss is = LS + LC, where is the hyper-parameter for the content loss. We combine style and content losses to prevent the model from overfitting to artistic features, allowing it to learn style as distinct component separate from content. This approach encourages the model to capture the underlying style patterns without embedding them too deeply into the content, enabling it to generate natural images when no specific style is specified. By disentangling style from content in this way, the model learns to apply styles more flexibly while preserving the core content. At inference, we control style by adjusting when art information is introduced. Injecting style earlier makes the image more stylized, while later injection preserves natural details with subtle artistic elements. 6. Experiments We present the Art-Free Diffusion capabilities in Sec. 6.1 and Art Adaptation experiments in Sec. 6.2. 6.1. Art-Free Diffusion Model Architecture and Training. The architecture of our Art-Free Diffusion is based on Stable Diffusion v1.4 [42]. We train the VAE autoencoder from scratch, using filtered version of the COCO-2017 dataset and subset of the ArtFree SAM, consisting of 219,439 images. Training used batch size of 24, gradient accumulation of 2, and 2e-4 learning rate for 15 epochs, taking 16 hours. We train the U-Net model on the Art-Free SAM, while keeping the VAE frozen, utilizing pre-trained BERT base model (uncased) [8] as the Text Encoder. We first train the U-Net under 256 resolution on 7 H100 GPUs, with each GPU using batch size of 300 and mixed precision of FP16. We apply gradient accumulation of 8 and use learning rate of 1e-4 with the AdamW optimizer on 7 H100 GPUs by 41400 steps. We fine-tune the model at 512x512 resolution for total of 156,700 steps, with learning rate of 5e5 and batch size of 90, and apply 10% dropping rate with classifier-free guidance sampling [20]. Model Performance Analysis. We show qualitative comparisons of different models in the SupMat. In Table 2, we compare the performance of three models: CommonCanvas-SC [14], Stable Diffusion v1-4, and our Art-Free Diffusion. CommonCanvas-SC employs the same architecture as Stable Diffusion v2 and is trained on 30M commercially sourced samples from the CreativeCommons-licensed (CC) dataset, taken about 73,800 A100 hours. Stable Diffusion v1-4, in its final training stage, utilizes 600M image-text pairs from the LAION-Aesthetics v2 5+ dataset, reported to be trained approximately 200,000 A100 hours [6]. Our Art-Free Diffusion model is trained on approximately 9M images from the Art-Free SAM. We conduct experiments on the test set of the Art-Free SAM (9,140 samples) and 30k samples from COCO-2017. The evaluation results are presented in Table 2. We observe that all models perform similarly on the Art-Free dataset. However, there is performance gap when evaluated on the COCO dataset, which can be attributed to several factors. First, the SAM dataset includes blurred faces and license plates to protect the identities of individuals, which may affect performance. Second, the automatically generated captions in the SAM dataset are significantly longer than those in the COCO dataset, introducing bias toward longer captions. Lastly, our limited resources prevented us from conducting larger-scale training, which impacts our models competitiveness compared to the other two models. We believe that increasing both the number of images and the training duration would significantly enhance the models performance. Model Name # Images Train time (A100 Hours) CLIP FID CLIP FID Art-Free SAM COCO30K CommonCanvas-SC SD1-4 Art-Free Diffusion 30M 600M 9M 73,800 150,000 11,432 0.27 0.28 0.26 13.66 17.74 12. 0.27 0.27 0.23 8.23 12.54 23.60 Table 2. Model performance comparison between Stable Diffusion v1-4, CommonCanvas-SC, and Our Art-Free Diffusion. Experiments are conducted on the test sets of Art-Free SAM and 30k samples from COCO-2017. Artistic Knowledge Check In Figure 4, we conduct experiments with prompts referencing famous artworks reveal clear difference between Stable Diffusion v1.4 (SD1.4) and our Art-Free Diffusion. While SD1.4 accurately reproduces the queried artworks, our model generates random images with no recognizable artistic style, underscoring its lack of prior knowledge of artworks. Unlike traditional models that replicate artistic styles, our model contains no embedded artistic information. 6.2. Art Style Adaptation Implementation Details For the Art Style Adaptation, we use our Art Adapter with content loss weight = 50, and the LoRA rank of 1. We found that incorporating low-rank Adapters into the attention, linear, and convolution layers of the UNets up block reduces overfitting and improves generation quality, as opposed to introducing LoRA across all UNet blocks (see Supplementary Materials for details). The learning rate was set to 2e-4 using the AdamW optimizer, and we trained for 1,000 steps with batch size of 5 and the DDIM noise scheduler. For data augmentation, we resize images with random scale of 0.9 to 1 and randomly crop with an aspect ratio of 3/4 to 4/3. In the experiments, we use sks as the V* token, which serves as random new token for learning new art style concept. We select 17 artists styles and their works from WikiArt each with distinct style. We manually choose 10 to 40 paintings from each artist with similar color composition, brushstroke techniques, and artistic content to ensure the artistic knowledge dataset has consistent and coherent style. To evaluate art style similarity, we use the CSD score [48]. For each sample, we compute the mean CSD score between generated image and the images used in ArtAdaptation training. To assess content fidelity, we calculate the cosine similarity between content features in the generated and original images (ViTc) and use the CLIP score to evaluate text-image alignment for content consistency. For our evaluation, we sample 500 images and text prompts from the LAION Pop dataset [44]. Our experiments span 17 different style sets, with results averaged Figure 6. (Left) Results of the Perceptual User Study; Art-Free Diffusion with Adapter method (green bar) is preferred over image editing baselines, on par with Adapter on the SD1.4 backbone and favored less with StyleAligned (SD1.4), however the margin of preference is narrow between the baselines. (Right) Quantitative evaluation of the baselines, Art-Free Diffusion with the Art Adapter acheives good trade-off between the style and content. across the styles. Additionally, we conduct user study on Amazon Mechanical Turk to validate our findings. In this study, we collect pairs of images showing outputs from our Art-Free Diffusion model with the Art Adapter and baseline methods across 17 different artists for both Image Stylization and Art Generation tasks. Additionally, we test how people perceive real art examples from the same artist. The task displays three reference images showing the style of an artist and pair of examples. The users task is to choose which of the two images is more similar in style to the reference images. reliability test filters out unreliable participants, yielding 2,242 answers from 42 users. Image stylization We evaluate our method on an image stylization task, transforming image styles while preserving content, using the LAION Pop dataset. Comparisons are made against SD1.4 baselines: SD1.4 (Adapter), which uses the learned Art-Style Adapter with new text token; SD1.4 (Text), which queries the model using the artists name; and SD1.4 (Adapter + Text), combining both. For SD1.4, we apply LoRAs across all blocks, as restricting them to only the up blocks negatively impacts performance. To perform image stylization, we apply DDIM inversion to noise real image to step 800, and denoise while changing the text prompt and applying the adapter where needed. We also compare against Plug and Play [52], which edits internal model features by appending painting by [artist] to the caption, InstructPix2Pix [1] using the prompt turn into [artist] painting., and StyleAligned [18] which creates style-consistent images using reference style (we randomly chose style reference from the Art Dataset A). We also include qualitative comparison with CycleGAN [57] for Monet and Van Gogh. Qualitative results for imitating Van Goghs style are shown in Fig.7. Our perceptual user study strongly aligns with the automatic evaluation results  (Fig. 6)  , participants preferred our method over the baselines, except for StyleAligned on Stable Diffusion. Notably, 34.8% still chose our approach Figure 7. Comparion of our method and other image stylization baselines for the artist Van Gogh. All captions contain suffix in the style of Vincent van Gogh, Our model and SD1.4 + Art Adaptor are prompted with suffix in the style of V* art. over StyleAligned. This result is striking given that all baseline methods rely on extensive Stable Diffusion 1.4 trained on large, art-rich datasets, while our method is limited to small subset of examples from the Art dataset. When the StyleAligned method was applied to the Art-Free Diffusion, users preferred our Art-Free Diffusion with Adapter 67% of the time. Art-Free Diffusion with an Art Adapter scored higher in user preference over InstructPix2Pix (60.4%) and Plug and Play (77.8%), and matched SD1.4 with adapters (52.6%). These results underscore the effectiveness of our approach in delivering visually compelling style transfer. The automatic evaluation aligns with these trends. ArtFree Diffusion with Adapter acheives good balance between style and content fidelity, with style and content scores of 0.29 and 0.28, respectively. InstructPix2Pix and Plug and Play, scoring higher on content (0.58 and 0.59) but lower style alignment (0.24 and 0.11). StyleAligned achieved style score of 0.39 on Stable Diffusion, but only 0.13 on our Art-Free Diffusion backbone, suggesting that StyleAligneds style transfer capabilities are largely due to the extensive pretraining of its backbone. Together, these results emphasize that our Art-Free Diffusion with an Art Adapter can achieve impressive style fidelity without relying on heavily art-trained model. Art Generation We address the task of Art Generation, focusing on creating images in specific artistic style. Stable Diffusion, known for its ability to replicate styles by simply prompting with artist names, serves as baseline due to its extensive training on artworks [16]. Additionally, we compare with transferring style into generated images with the StyleAligned method on both Stable Diffusion and ArtFree Diffusion backbones. Qualitative examples presented in Fig. 8 (see more examples in SupMat.). Art-Free Diffusion (Adapter) outperforms SD1-4 (Text) in style, achieving mean CSD score of 0.34 compared to 0.22 for SD1-4 (Text). However, it shows slightly lower content preservation, with scores of 0.21 versus 0.26. StyleAligned on Stable Diffusion achieves high style score of 0.47, while on Art-Free Diffusion, it only reaches 0.22, again underscoring the advantage of pre-trained backbone with artistic elements. Our perceptual user study reflects these findings, with 76.2% of participants favoring our method over SD1-4 (Text) for style. Against the StyleAligned with SD1-4, our model was chosen 31.5% of the time versus 69.5% for StyleAligned with Art-Free Diffusion. This strong preference highlights the SD1-4 backbones significant artistic capacity, likely result of extensive pretraining, and showcases our methods effectiveness even without heavy art-focused training. Users were tasked with selecting the image most similar in style to real artworks. Participants chose images generated by our method 17.5% of the time and those from SD1.4 (Text) 11.1% of the time, indicating that generated images were often mistaken for real art in terms of style. Data Attribution We find that our Art Adapter can generalize from small Art-Style training set and generate seemingly novel images that are coherent with the given artistic style. To better understand which training images contributed to the synthesized image, and to check whether the art filtering may have overlooked some art content that influenced the result, we applied the data attribution technique proposed by [54]. The results of this experiment are shown in Fig.9. For each generated image, we retrieved the top five attributed images from both Art-Free SAM and ArtStyle examples. While we expect stylistic elements to dominate, real-world influences from the Art-Free SAM play strong role. In the Picasso-style generation, we can clearly see cubism influences, yet the content resembles its realworld counterpart, the top five attributed images in this example are from the Art-Dataset. In the remaining two exFigure 8. Comparison of Art-Free Diffusion art generation (top row) with generating art images with Stable Diffusion 1.4 (bottom row). our work and present Kenny with the generated images imitating his style. In the interview, the artist expresses blend of astonishment and familiarity when observing the AI-generated art, remarking, didnt expect [this quality] if you were using base model of blank canvas... you probably achieved more than would have expected for base model with no information. He acknowledges that the AI has captured aspects of his distinct style to the extent that, if you were to post some of these images online, would get people texting me, see your images. They would spot it, and spot it. Despite noting that compositionally, it is weak and contrasting this with his own well thought and meticulous compositions, he recognizes that there are some very positive things in the AIs work. The artist describes the experience as terrifying and bit exciting at the same time, specifically pointing out how the AI imitates his signature gradation of the landscape and gradation of the shapes. Though he felt his style is largely captured, he admits, there is kind of originality to them... see me in them, yes, very strongly... but there is an originality to some of the images. (see SupMat for qualitative examples). 7. Discussion In this paper, we introduce the Art-Free Diffusion model, which explores the ability to mimic an artistic style with minimal exposure to art. We propose simple method for training an Art Adapter to achieve this goal and evaluate its performance in image stylization and art generation tasks using both automatic metrics and perceptual user study. Our experiments show that this system can successfully imitate artistic styles. Additionally, we consulted professional artist to gather expert feedback on how well the artificial model replicates his artistic style, further validating our findings. To support our thesis, we applied data attribution method to understand how model with limited knowledge of artistic styles can still produce artistic images. The results provide intuitive insights into how the natural world can influence and inspire art. Figure 9. Results from our data attribution experiments on synthesized art images. While the generated images reflect the distinct artistic styles of each artist, the training images that contributed the most came from both the Art-Free dataset and the Art-Style examples. amples, the top five attributed images are from the Art-Free SAM. In the Matisse-inspired image, vivid colors and organic shapes evoke Matisses signature style, interestingly, the attribution method reveals real-world scenes underlying this image, almost as if the style has been stripped away. In the Lichtenstein-style image, the comic aesthetic is bold and recognizable, but much of the underlying content can be traced back to art-free images as shown in the attributed images. (for more examples see SupMat). Introducing the Art Adapter to the Artist To explore the artistic communitys reaction to AI-generated art, we conduct an interview with the renowned artist Alan Kenny. Upon obtaining Alans permission, we train an Art Adapter on 11 artworks showing his distinctive style. We describe 8. Acknowledgments We thank Alan Kenny for his thoughtful discussions and for graciously allowing the use of his artwork in this research. We also appreciate Yael Vinker and Tamar Rott Shaham for their valuable feedback on our paper. JM is grateful for support from the ONR MURI grant (#03369700007), IBM (#027397-00163), and Hyundai Motor Company R&D (#034197-00002). RH and DB are supported by Open Philanthropy and NSF grant #2403304."
        },
        {
            "title": "References",
            "content": "[1] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 6 [2] Stephen Casper, Zifan Guo, Shreya Mogulothu, Zachary Marinov, Chinmay Deshpande, Rui-Jie Yew, Zheng Dai, and Dylan Hadfield-Menell. Measuring the success of diffusion models at imitating human artists. arXiv preprint arXiv:2307.04028, 2023. 2 [3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. 3 [4] Jiwoo Chung, Sangeek Hyun, and Jae-Pil Heo. Style injection in diffusion: training-free approach for adapting largescale diffusion models for style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 87958805, 2024. 15 [5] Jiwoo Chung, Sangeek Hyun, and Jae-Pil Heo. Style injection in diffusion: training-free approach for adapting largescale diffusion models for style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87958805, 2024. 3 [6] CompVis. Stable diffusion v1-4 model card, 2022. 5 [7] Yingqian Cui, Jie Ren, Han Xu, Pengfei He, Hui Liu, Lichao Sun, Yue Xing, and Jiliang Tang. Diffusionshield: watermark for copyright protection against generative diffusion models. arXiv preprint arXiv:2306.04642, 2023. [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. 4, 5 [9] Tal Dickstein and Edward Delman. Andersen v. stability ai ltd. Court Case, Northern District of California, 2023. Case filing date. 1 [10] Alexei Efros and William Freeman. Image quilting for texture synthesis and transfer. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 571576. 2023. 3 Jaden FiottoKaufman, and David Bau. Erasing concepts from diffusion models, 2023. 2 [11] Rohit Gandikota, Joanna Materzynska, [13] Leon Gatys. neural algorithm of artistic style. arXiv preprint arXiv:1508.06576, 2015. 3 [14] Aaron Gokaslan, Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. Commoncanvas: Open diffusion models trained on creativeIn Proceedings of the IEEE/CVF Concommons images. ference on Computer Vision and Pattern Recognition, pages 82508260, 2024. 2, 5 [15] Andres Guadamuz. Artists file class-action lawsuit against stability ai, deviantart, and midjourney. Recuperado de: https://www. technollama. co. uk/artists-file-class-actionlawsuit-against-stability-ai-deviantart-and-midjourney, 2023. 2 [16] Melissa Heikkila. This artist is dominating ai-generated art. and hes not happy about it. MIT Technology Review, 125(6): 910, 2022. 1, 7 [17] Alvin Heng and Harold Soh. Selective amnesia: continual learning approach to forgetting in deep generative models, 2023. 2 [18] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47754785, 2024. 3, [19] Aaron Hertzmann, Charles Jacobs, Nuria Oliver, Brian In SemCurless, and David Salesin. inal Graphics Papers: Pushing the Boundaries, Volume 2, pages 557570. 2023. 3 Image analogies. [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. 5 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. 3, [22] Seunghoo Hong, Juhun Lee, and Simon Woo. All but one: Surgical concept erasing with model preservation in text-toimage diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2114321151, 2024. 2 [23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. 4 [24] Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, and Youngjung Uh. Visual style prompting with swapping selfattention. arXiv preprint arXiv:2402.12974, 2024. 15 [25] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24262435, 2022. 4 [26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything, 2023. 3 [12] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzynska, and David Bau. Unified concept editing in diffusion models. arXiv preprint arXiv:2308.14761, 2023. 2 [27] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2269122702, 2023. 2 [28] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion, 2023. 14 [29] Sarah Kuta. Artificial intelligence art wins colorado state fair. Smithsonian Magazine, 2022. Daily Correspondent. 1, 2 [30] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. The data provenance initiative: large scale audit of dataset licensing & attribution in ai. arXiv preprint arXiv:2310.16787, 2023. 2 [31] Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. Mace: Mass concept erasure in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6430 6440, 2024. 2 [32] Mengyao Lyu, Yuhong Yang, Haiwen Hong, Hui Chen, Xuan Jin, Yuan He, Hui Xue, Jungong Han, and Guiguang Ding. One-dimensional adapter to rule them all: Concepts diffusion models and erasing applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 75597568, 2024. [33] Rui Min, Sen Li, Hongyang Chen, and Minhao Cheng. watermark-conditioned diffusion model for ip protection. arXiv preprint arXiv:2403.10893, 2024. 2 [34] Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah Smith, and Luke Zettlemoyer. Silo language models: Isolating legal risk in nonparametric datastore. arXiv preprint arXiv:2308.04430, 2023. 2 [35] Yong-Hyun Park, Sangdoo Yun, Jin-Hwa Kim, Junho Kim, Geonhui Jang, Yonghyun Jeong, Junghyo Jo, and Gayoung Lee. Direct unlearning optimization for robust and safe textto-image models. arXiv preprint arXiv:2407.21035, 2024. 2 [36] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of In Proceedings of the IEEE/CVF interstylegan imagery. national conference on computer vision, pages 20852094, 2021. 4 [37] Minh Pham, Kelly Marshall, Niv Cohen, Govind Mittal, and Chinmay Hegde. Circumventing concept erasure methods for text-to-image generative models. In The Twelfth International Conference on Learning Representations, 2023. 2, 13 [38] Minh Pham, Kelly Marshall, Chinmay Hegde, and Niv Cohen. Robust concept erasure using task vectors. arXiv preprint arXiv:2404.03631, 2024. [39] Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, and Yongdong Zhang. Deadiff: An efficient stylization diffusion model with disentangled representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86938702, 2024. 15 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 4 [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022. 3, 4, 5 [43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation, 2023. [44] Christoph Schuhmann and Peter Bevan. Laion pop: 600,000 high-resolution images with detailed descriptions, 2023. 6 [45] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. arXiv preprint arXiv:2306.00983, 2023. 14 [46] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? In Proinvestigating data replication in diffusion models. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60486058, 2023. 2 [47] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and mitigating copying in diffusion models. Advances in Neural Information Processing Systems, 36:4778347803, 2023. 2 [48] Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, and Tom Goldstein. Measuring style similarity in diffusion models. arXiv preprint arXiv:2404.01292, 2024. 6 [49] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [50] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [51] Spawning AI Team. Spawning ai, 2023. 2 [52] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19211930, 2023. 6, 14 [53] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024. 15 [54] Sheng-Yu Wang, Alexei Efros, Jun-Yan Zhu, and Richard Zhang. Evaluating data attribution for text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 71927203, 2023. 2, 7 [55] Yimeng Zhang, Xin Chen, Jinghan Jia, Yihua Zhang, Chongyu Fan, Jiancheng Liu, Mingyi Hong, Ke Ding, and Sijia Liu. Defensive unlearning with adversarial training for robust concept erasure in diffusion models. arXiv preprint arXiv:2405.15234, 2024. 2 [56] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, NgaiMan Cheung, and Min Lin. recipe for watermarking diffusion models. arXiv preprint arXiv:2303.10137, 2023. [57] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223 2232, 2017."
        },
        {
            "title": "Supplementary Material",
            "content": "B. Qualitative Results of Art-Free Diffusion A. Artwork Filtering Methodology Our artwork filtering process operates on both image and For caption levels to ensure comprehensive coverage. image-level filtering, we define set of concepts to be excluded: We demonstrate qualitative results of the Art-Free Diffusion in Fig.11, for comparison, we also include images generated by StableDiffusion 1.4 and CommonCanvas-SC. Our model, despite significantly smaller training set size generates high-quality images faithful to the text prompt. painting, art, artwork, drawing, sketch, illustration, sculpture, stamp, advertisement, logo, installation art, printmaking art, digital art, conceptual art, mosaic art, tapestry, abstract art, realism art, surrealism art, impressionism art, expressionism art, cubism art, minimalism art, baroque art, rococo art, pop art, art nouveau, art deco, futurism art, dadaism art Figure 10 presents histogram of CLIP scores for images associated with the word painting in their captions. This distribution is derived from subset of the SA-1B dataset, comprising 11,186 images (0.1% of the complete SA-1B dataset). For caption-level filtering, we exclude the following terms: painting, paintings, artwork, drawings, art, sketch, sketches, illustration, illustrations, sculpture, sculptures, stamp, stamps, advertisement, advertisements, logo, logos, installation, printmaking, digital art, conceptual art, mosaic, imprestapestry, abstract, realism, surrealism, sionism, expressionism, cubism, minimalism, baroque, rococo, pop art, art nouveau, art deco, futurism, dadaism Figure 10. Histogram of the CLIP score of images with the word painting in the caption. The distribution shown is from subset of the SA-1B dataset. The red line represents the filtering threshold (17) we selected. Our strict threshold aims filters out all the art, even incidental art like picture of man painting. Figure 11. Qualitative comparison of images generated with Art-Free Diffusion, Stable Diffusion 1.4 and CommonCanvas-SC model. C. Implementation Details LoRA Implementation Motivated by the observation that early layers handle global image aspects, which are less style-dependent, we found that injecting LoRA layers only in the UNets up block reduces overfitting  (Fig. 12)  . Quantitative evaluation shows this approach achieves higher style score across 17 artists (0.29 vs. 0.26) while preserving comparable content score (0.22 vs. 0.23). Figure 12. Comparison of LoRA applied to all layers vs. only the up block of the UNet. Limiting LoRA to the up block reduces overfitting. Adapters train on 10 images sample of Camille Pissaros artwork. Additionally, we conducted an analysis to determine the effect of LoRA rank on the art adapters performance. Table 3 presents the results of our model with LoRA ranks 1 and 64. Our findings indicate that LoRA rank does not significantly impact model performance. This experiment is done on the image stylization task, the scores are average across 17 artists, with 1.0 LoRA scale. LoRA Rank CSD LPIPS ViTc CLIPc 1 0.29 0.21 0.62 0.59 0.28 0.32 0.22 0.25 Table 3. Rank analysis of LoRA on style transfer task. We find that higher rank of LoRA does not improve the model learning performance. In our quantitative evaluation, we use 500-sample subset of the LAION Pop dataset, randomly sampled while excluding images with keywords listed in A. For captions longer than baselines content length, we use only the first sentence. Content Loss Strength We investigated the influence of the content loss weight (w) in the art adapter across different models Tab. 4. The content loss substantially enhances learning performance, with CSD increasing from 0.14 to 0.29 when is set to 50. This demonstrates that the content loss effectively aids the model in distinguishing between art images and natural images. The effect remains robust across different weight values, with performance remaining nearly constant when is set to 20 or 100 (up to 0.02 difference in CSD). Content Loss scale CSD LPIPS ViTc CLIPc 0 20 50 100 0.14 0.29 0.29 0.27 0.57 0.62 0.62 0. 0.33 0.28 0.28 0.28 0.25 0.22 0.22 0.23 Table 4. Analysis of prior preservation loss weight (w) on our model. Experiments are conducted on style transfer, with noise added at the 800th time step. The scores are averages across 17 artists on image stylization task, with 1.0 LoRA scale. D. Art-Agnostic Model Verification To verify the art-agnostic nature of our model, we conducted textual inversion experiment as suggested by Pham et al. [37]. In the experiment we use the same Art Dataset as for training the Art Adapter for Vincent Van Gogh styleFigure 13 illustrates that our model fails to produce the target style using textual inversion, further confirming its lack of prior artistic knowledge. Figure 13. Through textual inversion using paintings by van Gogh, we found that, unlike SD1-4, our model cannot generate images in the corresponding style. This indicates that our model cannot be hacked to generate artwork through prompt space searching, demonstrating it has no prior knowledge of art. D.1. Model Editing and Controlling Ability E. Effect of Applying the Adapter at various Despite being trained on significantly smaller and less diverse dataset limited to natural images, our art-agnostic model demonstrates comparable editing and control capabilities to competitive models. This is evident in both single-image editing and customization experiments. In Figure 14, we qualitatively illustrate the single-image editing process using the Plug-and-Play method [52] applied to our model. We provide editing examples on both real and generated images, demonstrating the models ability to replace pyramid with large mountain, both with and without the artistic adapter (weight 1.5) of van Gogh."
        },
        {
            "title": "Time Steps",
            "content": "We analyzed the effect of the adapter time step on art generation results. Figure 16 shows the art generation outcomes with different adapter time steps. Intuitively, the model generates more style information when the adapter starts earlier (left) and more content information when the adapter starts later (right). Figure 16. Art generation results using Art Adapter at different timesteps. From left to right: no adapter (column 1), adapter introduced at timestep 800 (column 2), 600 (column 3), and 0 (column 4). This demonstrates how earlier adapter introduction increases artistic influence in the image. F. Alan Kennys Art In Fig. 17, 19 we present qualitative examples of images generated in the style of Alan Kenny along with the results of the data attribution technique. These examples reveal how natural images inspire features in the generated art (e.g., stage with musicians) while preserving the characteristics of the artistic style like the use of colors, smooth boundaries and geometric shapes. G. Additional Results of the Data Attribution We present additional results of applying data attribution to the generated art images in Fig. 24. These results illustrate how specific visual elements from the training data, including both natural and small art images, influence the generated outputs. Despite the Art Adapters being trained on limited set of art images, and the base text-to-image model itself having minimal exposure to graphic art, the attribution analysis points to similarities in the natural images that may enable the model to effectively generalize from few examples. H. Different Baselines While many methods transfer style from reference image to another, direct comparisons are often infeasible due to differences in model architecture and dependencies. For instance, StyleDrop [45] is designed specifically for the Muse Figure 14. Plug-and-Play editing on our model. We provide both editing on real and generated image examples. We replace pyramid to large mountain both without and with the artistic adaptor of van Gogh. Furthermore, we demonstrate our models customization abilities using the Dreambooth technique [43]. We learned the concept of barn using 7 training images from the CustomConcept101 dataset [28]. The model was trained to generate the barn in various contexts, utilizing 200 prior samples from Stable Diffusion v1-4, with prior preservation loss of 1.0, learning rate of 5e-6, and 250 training steps on 2 GPUs. Figure 15. Dreambooth editing on our model. We send 7 barn example images to the model and ask it to generate the barn in various contexts. Figure 17. Generated artwork in the style of Alan Kenny (created and displayed with the artists permission) showcases the top-5 influential images from the Art-Free and Art Datasets. architecture, making it difficult to separate the contribution of the adaptation method from the pretrained models inherent stylization capabilities. Similarly, Visual Style Prompting [24] and InstantStyle [53] are designed primarily for Stable Diffusion XL. Computational constraints prevent us from training comparable model with our Art-Free data, but we encourage others to explore similar experiments. compete with models and methods trained on significantly larger graphic art datasets, as such comparisons would be inherently unfair. Instead, our work focuses on key question: how much graphic art data is truly needed to effectively replicate an artistic style? Our analysis demonstrates that an artistic style can be successfully learned from just few examples. DeadDiff [39], while offering advantages to text-toimage adapters, relies on paired dataset where the reference image and ground truth share style or semantics, which differs significantly from our approach. Our primary goal is to demonstrate that effective style transfer is achievable with few examples, rather than competing with methods that leverage extensive pretrained knowledge of graphic art. To disentangle the adaptation method from the pretrained models capabilities, we applied another baseline, StyleID [4], to both our Art-Free Diffusion model and SD1.4. Similar to StyleAligned, both training-free adaptation methods performed better on SD1.4, leveraging its broad artistic knowledge, but struggled on Art-Free Diffusion, highlighting their reliance on pretrained models rich in artistic priors. In contrast, our Art Adapter bridges this gap effectively, demonstrating that focused adaptations within the Art-Free framework can achieve compelling results without relying on inherited artistic biases. comparison equivalentStyleAligned and StyleID use single reference image, whereas our Art Adapter employs multiple style references (in this experiment, we compare five artists: Derain, Miro, Klimt, Picasso, and Lichtenstein, with an average training set of 15)we were unable to adapt these methods to support multiple references, as doing so falls outside the scope of this work. entirely"
        },
        {
            "title": "While",
            "content": "this not is"
        },
        {
            "title": "It is important to emphasize that our goal is not to",
            "content": "Text-To-Image Model Adaptation Method CSD mean ViTc CLIPc Art-Free Diffusion SD1.4 Art-Adapter StyleAligned StyleID Art-Adapter StyleAligned StyleID 0.35 0.12 0. 0.21 0.43 0.29 0.27 0.31 0.63 0.27 0.23 0.40 0.23 0.22 0.22 0.26 0.21 0.23 Table 5. Comparing different art adaptation methods across our Art-Free Diffusion model and Stable Diffusion 1.4. Trainingfree adaptation methods, StyleAligned and StyleID, perform better on SD1.4, benefiting from the models broad artistic knowledge, but struggle on Art-Free Diffusion, showing their reliance on pretrained models rich in artistic priors. In contrast, our Art Adapter effectively bridges this gap, proving that focused adaptations within the Art-Free framework can deliver compelling results without depending on inherited artistic biases. I. Additional Qualitative Results Additional art generation results (of art generation and image stylization) and training images in Figures 2036. We show our models ability to replicate diverse artistic styles: Impressionism (Monet, van Gogh, Corot), Art Nouveau (Klimt), Fauvism (Derain), Abstract Expressionism (Matisse, Pollock, Richter), Abstract Art (Kandinsky), Cubism (Picasso, Gleizes), Pop Art (Lichtenstein, Warhol), Ukiyoe (Hokusai), Expressionism (Escher), and Postmodern and Geometric Abstraction (Miro, Battiss). The captions and reference images are sampled from the LAION Pop dataset. Figure 18. Additional qualitative experiments showing diverse art generations and top five attributed images from both the Art-Free SAM and and Art-Style example dataset. Figure 19. Additional qualitative experiments of the art imitation of the interviewed artist Alan Kenny. Figure 20. Additional qualitative experiments. Figure 21. Additional qualitative experiments. Figure 22. Additional qualitative experiments. Figure 23. Additional qualitative experiments. Figure 24. Additional qualitative experiments. Figure 25. Additional qualitative experiments. Figure 26. Additional qualitative experiments. Figure 27. Additional qualitative experiments. Figure 28. Additional qualitative experiments. Figure 29. Additional qualitative experiments. Figure 30. Additional qualitative experiments. Figure 31. Additional qualitative experiments. Figure 32. Additional qualitative experiments. Figure 33. Additional qualitative experiments. Figure 34. Additional qualitative experiments. Figure 35. Additional qualitative experiments. Figure 36. Additional qualitative experiments."
        }
    ],
    "affiliations": [
        "MIT",
        "Northeastern University",
        "ShanghaiTech University"
    ]
}