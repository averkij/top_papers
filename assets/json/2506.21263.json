{
    "paper_title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster",
    "authors": [
        "Ji Qi",
        "WenPeng Zhu",
        "Li Li",
        "Ming Wu",
        "YingJun Wu",
        "Wu He",
        "Xun Gao",
        "Jason Zeng",
        "Michael Heinrich"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The distributed training of foundation models, particularly large language models (LLMs), demands a high level of communication. Consequently, it is highly dependent on a centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, a low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-step-delay overlap of communication and local training, as well as the adaptive gradient compression scheme, through a theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters."
        },
        {
            "title": "Start",
            "content": "DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster Ji Qi 1 WenPeng Zhu 1 Li Li 1 Ming Wu 2 YingJun Wu 1 Wu He 1 Xun Gao 1 Jason Zeng 2 Michael Heinrich"
        },
        {
            "title": "Abstract",
            "content": "The distributed training of foundation models, particularly large language models (LLMs), demands high level of communication. Consequently, it is highly dependent on centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, OneStep-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-stepdelay overlap of communication and local training, as well as the adaptive gradient compression scheme, through theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training 107B foundation model over 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters. 5 2 0 2 6 2 ] . [ 1 3 6 2 1 2 . 6 0 5 2 : r 1. Introduction LLMs have quickly become dominant in the field of AI due to their exceptional capabilities. Their effectiveness in areas such as automated dialogue generation, machine translation, content summarization, and recommendation systems clearly demonstrates their superior advantages (Zhao et al., 1China Mobile(Suzhou) Software Technology, JiangSu, Ji Qi Correspondence to: China 2Zero Gravity Labs. <qiji@cmss.chinamobile.com>. 2023a; Wang et al., 2022; Bommasani et al., 2021). However, as efforts to improve the accuracy of these models continue, both their complexity and scale have grown exponentially. In recent years, the number of parameters has increased from billions (Devlin, 2018) to trillions (Fedus et al., 2022; Du et al., 2022), introducing significant challenges in model training. For example, pre-training model with 175B (Brown, 2020) parameters can require 12,000 GPUs operating for 118 days (Narayanan et al., 2021). In distributed foundation model training, communication is the key bottleneck. As an example, fine-tuning GPT-J-6B over 10B tokens with 262K batch size across 4 machines (2 A100 GPUs each) demands 915.5 TB data communication in total training(Wang et al., 2023). As result, the highspeed centralized cluster is currently the dominant solution for training foundation model(Rendle et al., 2016). Decentralized clusters usually have significant amount of computing resources than centralized cluster, but the bandwidth between clusters is relatively slower, with only hundreds of Mbps to 1 Gbps in common. When using training framework such as Megatron-LM(Shoeybi et al., 2019) for distributed cluster training, the bandwidth between clusters often becomes training bottleneck(Dai et al., 2024; Zhang et al., 2022), resulting in the inability to fully utilize the computing resources of decentralized clusters for model training. Recently, there has been an exciting collection of work focusing on the decentralized training of foundation models. By introducing outer Nesterov momentum optimizers, DiLoCo (Douillard et al., 2023) can train model with up to 400M parameters using only data parallelism and achieve even better performance than fully synchronous model. OpenDiLoCo(Jaghouar et al., 2024) is an opensource implementation of DiLoCo. With FP16 gradient compression and Hivemind optimized collective communication operators, it can train the 1.1B model over poorly connected networks with bandwidth of hundreds of Mbps and negligible degradation in convergence. CocktailSGD (Wang et al., 2023) combines three distinct compression techniques, achieving 117 aggressive compression in finetuning LLMs with up to 20B parameters over 500Mbps network. 1 DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster However, despite these recent efforts, the scale of model parameters and communication bottleneck is still challenge when pretraining over 100B parameter models. DiLoCo and OpenDiLoCo (Douillard et al., 2023; Jaghouar et al., 2024) have comparable model convergence with the fully synchronous model. However, they only use data parallelism and do not support FSDP (Zhao et al., 2023b) or deepspeed (Rajbhandari et al., 2020), thus the VRAM of GPU capacity limits the scale of model parameters. Furthermore, DiLoCo and OpenDiloCo (Douillard et al., 2023; Jaghouar et al., 2024) employ pseudo-gradients synchronous mechanism. During the synchronization of pseudo-gradients, local training is in an idle state. When the model scale is large and network bandwidth is limited, the idle time for synchronizing pseudo-gradients becomes unacceptable. CocktailSGD (Wang et al., 2023) uses pipeline parallelism to support 20B model training, but the compression ratio of data parallelism is aggressive up to 117 times and does not use local training like DiLoCo or OpenDiLoCo in decentralized clusters, which has potential impact on model convergence. In order to train models with scale of more than 100B parameters on low-bandwidth decentralized clusters while having comparative model convergence, we have identified the following key challenges: 1. Introduce model parallelism to address the limitation of VRAM which has to accommodate the whole model parameters. 2. The overlap between the synchronization of pseudo-gradients and local training to avoid the idleness of computing resources. 3. Design an efficient gradient compression algorithm and balance it with the number of local training steps to ensure the convergence of model training. To address the above challenges, we propose low communication large-scale model training framework DiLoCoX for decentralized cluster. Experiments demonstrate that DiLoCoX can pre-train 107B model and significantly hide communication overhead while ensuring model convergence on decentralized clusters with only 1Gbps network bandwidth. To the best of our knowledge, this is currently the largest-scale model for effective decentralized cluster training. The main contributions are as follows. Pipeline Parallelism with Dual Optimizer Policy To address the limitation of model parameter scale caused by the lack of support for model parallelism in DiLoColike framework, we propose Pipeline Parallelism with Dual Optimizer Policy into the framework and successfully expand the model parameters over 100B. One-Step-Delay Overlap of Communication and Local Training We propose one-step-delay overlap mechanism between the synchronization of pseudogradients and local training to avoid the idleness of computing resources, which significantly improves the efficiency of model training. Design of Adaptive Gradient Compression Algorithm: By thoroughly analyze different compression scheme, we have designed an efficient AllReducecompatible communication compression scheme. In order to ensure the convergence of the model and full overlap with local training, we propose an adaptive gradient compression algorithm based on this compress scheme to trade-off compress ratio and the number of local training steps to avoid aggressive compress which cause potential degradation on model convergence. Theoretical Analysis of Convergence and Extensive Experiments: We justify the benefit of the onestep-delay overlap of communication and local training, adaptive gradient compression scheme through theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training 107B foundation model over 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve 357x speedup in distributed training while maintaining negligible degradation in model convergence. 2. Method 2.1. Problem Formulation In this paper, the primary emphasis is laid on the parallel configuration comprising data parallelism and model parallelism . total of = workers are involved. Each workeri,j sustains local data source Di, where and are the DP and MP indices, respectively. In this local data source, local loss function fi is defined. All parallel workers cooperate in order to minimize the objective function : Rd R, that is, to determine the parameters of the target model ˆθ Rd such that (cid:105) ˆθ = argminθRd . Here, ζ represents the data sampled from each local data source. At iteration t, each workeri,j holds fraction of the local model replica denoted as θ(i,j) and computes its local gradient over the data sample ζ (i) . Subsequently, all workers communicate and compute the average gradient. Then, workeri,j updates its fraction of the local model as follows: i=1 fi(θ(i,j) t+1 = θ(i,j) θ(i,j) ) where η is the learning rate. EζDifi(θ; ζ) (θ) = 1 η (cid:80)D ; ζ (i) (cid:80)D i= (cid:104) t 2.2. Pipeline Parallelism with Dual Optimizer Policy DiLoCo (Douillard et al., 2023) and OpenDiLoCo (Jaghouar et al., 2024) establish standard for synchronous LocalSGD in language modeling with outer Nesterov momentum optimizers. However, both of them do not support model parallelism. Consequently, the model parallelism is regarded 2 DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster as 1. For the OpenDiLoCo framework (Jaghouar et al., 2024), each worker performs local updates utilizing an inner optimizer on their corresponding data shard Di. Subsequently, the first worker on the node that also holds the outer optimizer computes the parameter change (pseudo gradient) δ(i) = θt1 θi and averages the pseudo-gradient across clusters. Finally, the first worker of the nodes outer optimizer performs step operation to update local parameters and broadcasts the updated parameters to the remaining workers of the node. The OpenDiLoCo framework requires GPU VRAM that is capable of accommodating the complete model parameters and inner optimizer states. Additionally, the first worker on the node is also required to store the state of the outer optimizer. This leads to unbalanced VRAM usage and consequently restricts the further expansion of the scale of model parameters. Based on the fact that pipeline parallelism is effective and has the least communication volume among parallel methods in the 3D parallel strategy (Narayanan et al., 2021), as shown in Figure 1, we propose Pipeline Parallelism with Dual Optimizer Policy. According to the pipeline parallelism approach, we divide the model into stages by layers and partition workers in decentralized clusters into data parallel groups, with the relationship = D. Each workeri,j holds two distributed optimizers (inner and outer) and fraction of the model parameters, and performs local updates using the inner optimizer on its corresponding data shard Di. Then, each worker computes its own parameter change (pseudo-gradient) and averages it with those of other workers in the same DP group. Finally, the distributed outer optimizer in each PP group performs step operation to update the local parameters. The advantage of Pipeline Parallelism with Dual Optimizer Policy resides in the fact that each worker merely stores fraction of the model parameters, overcoming the limitation that the VRAM of GPU must hold complete model. Additionally, each worker incorporates part of the state of the distributed outer optimizer, leading to more balanced utilization of VRAM. Based on this scheme, we have implemented DiLoCoX training of models exceeding 100B parameters on decentralized clusters of NVIDIA 40G A800. Figure 1. Pipeline Parallelism with Dual Optimizer. The example infrastructure comprises 32 workers distributed among 2 decentralized clusters, with 16 workers in each cluster. Each worker maintains distributed inner and outer optimizers state and fraction of model parameters. Two clusters are trained independently for steps respectively. The parallel strategy is PP = 8, DP = 2 for each cluster, and the parameters are updated by using their respective inner optimizers. Finally, the outer distributed optimizer performs step operation to update local parameters. cal training and pseudo-gradient averaging, which synchronizes the gradient of the outer optimizer after training steps. During the synchronization process, the computing resources are in idle state. We propose one-step-delay overlap of communication and local training illustrated in Figure 2 with the following steps: After finishing the first global H-step, calculate current pseudo-gradients and start performing pseudogradients averaging asynchronously. During the training of global steps between and 2H, we average the last pseudo-gradients simultaneously using AllReduce. After finishing the 2H step, we calculate current pseudo-gradients and start performing asynchronous averaging of current pseudo-gradients. Then we update the model parameter by the delayed last averaged pseudo-gradients. 2.3. One-Step-Delay Overlap of Communication and Local Training Through introducing the Pipeline Parallelism with Dual Optimizer Policy in section 2.2, we can train the foundation model over 100B on decentralized clusters. However, DiLoCo and OpenDiLoCo (Douillard et al., 2023; Jaghouar et al., 2024) adopt synchronous approach between loIn summary, the model parameters of outer step are obtained by θt1 and t1. It is denoted as θ(t) OuterOpt(θ(t1), (t1)). We assume that the pseudogradients will not change significantly between two consecutive outer steps. Therefore, the overlap of communication and local training can greatly improve the model efficiency and will not have significant impact on the convergence of the model. We verify this in subsequent experiments. 3 DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster CocktailSGD (Wang et al., 2023) compresses communication aggressively using combination of Top-K sparsification, Quantization, and Random sparsification and achieves up to 117 compression in fine-tuning LLMs up to 20B parameters. Aggressive compression leads to significant differences between the parameters of the local model and the global model, resulting in degradation of the convergence of the global model, but the approach of combined different compression has inspired us to design an efficient compression algorithm. Firstly, we analyze four common compression algorithms: Random Sparsification, Top-K Compression, Quantization, and Low-Rank. Random Sparsification randomly selects portion of the gradients for update according to the seed. By sending only random seed, the sparsity pattern can be fully recovered. However, given the same sparsity ratio, random sparsification introduces more errors since it does not necessarily keep the values of the largest norm (Wang et al., 2023). Top-K Compression selects the top-k elements with the largest values for communication and it has fewer compression errors (in l2 norm) compared to random sparsification. However, with numbers as input, top-k compression requires log2 bits (as index list) or bits (as bitmap). When is relatively large, it will lead to very large communication costs. In addition, Top-K compression is not AllReduce compatible and requires parameter server and double compression, which is less efficient than AllReduce compatible compression algorithms which are essential for the 100B model (Wang et al., 2023). Algorithm 1 Compressor C[δ] Input: Pseudo-gradient δ, low-rank r, quantization qbits. (1) Low-Rank Approximation: δ1 LOWRANK(δ, r). (2) Quantize values to bits: δ2 QUANTIZE(δ1). Output: Compressed pseudo-gradient δ2. Quantization is efficient and compatible with AllReduce. However, if the original values are stored in FP16, it cannot achieve compression ratio of more than 16 times. Furthermore, linear decrease in the number of bits for quantization often results in an exponential increase in error. It is possible to apply quantization in conjunction with other compression algorithms that are compatible with AllReduce. Low-Rank argues that modern deep networks are overparameterized and can use Low-Rank update to update the gradient (Vogels et al., 2020). It is sometimes possible to converge to the optimum using Low-Rank approximations of the gradients without the need for error feedback, and Figure 2. One-Step-Delay Overlap of Communication and Local Training. 2.4. Design of Adaptive Gradient Compression Algorithm 2.4.1. ANALYSIS OF COMMUNICATION OVERHEAD After steps of local training in clusters, we need to perform pseudo-gradient update of decentralized clusters. Suppose that the number of model parameters is θ, so the total number of parameters that must be communicated by all workers is θ D, where is the degree of data parallelism. In this paper, we solely take into account the communication overhead between clusters. Assuming the utilization of Ring AllReduce (Baidu, 2017) for pseudo-gradient updates, the total number of parameters that require communication between two clusters is 2 (C 1) θ/C. Assuming that the pseudo-gradient is stored in FP32, the model parameter is 100B and there are three decentralized clusters, the communication overhead between the three clusters is approximately 533.3 GB for every step. If the bandwidth between decentralized clusters is 1 Gbps, transmitting 533.3 GB data would take 1.18 hours. Assuming that the local training step is 500 and the duration of every local step is 1 second, thus the total time of local training would take 0.13 hours. Even if we use section 2.2 Pipeline Parallelism with Dual Optimizer Policy, the idle time of computing is approximately 1.04 hours, which is unacceptable for decentralized cluster training. For models with scale of more than 100B, we need to design an effective compression algorithm to compress more than 10x, ensuring that the communication time of decentralized clusters is within reasonable range. 2.4.2. DESIGN OF EFFECTIVE COMPRESS ALGORITHM Currently, the major compression methods include sparsification (Strom, 2015; Wangni et al., 2017; Alistarh et al., 2018), quantization(Alistarh et al., 2017) and LowRank(Vogels et al., 2020), etc. These methods can significantly reduce the communication overhead of distributed training. However, none of these methods achieves compression ratio larger than 10 without hurting the convergence(Wang et al., 2023). 4 DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster it can be implemented using the AllReduce, which is very efficient. Algorithm 2 DiLoCoX Framework Input: Initial Model θ0, number of workers , Data Parallelism D, Pipeline Parallelism , Data shards {D1, , DD}, error buffer e, distributed Optimizers InnerOpt and OuterOpt, combined compress algorithm C, gradient quantization q, gradient window c, initial local training step H1 and initial Low-Rank compression rank r1, adaptive gradient compress algorithm AdaGradCmp. for outer step = 1 do pre-training over 100B model? Theorem 2.1 (Principle of Rank Diminishing (Feng et al., 2022)). Suppose that each layer fi, = 1, . . . , of network is almost everywhere smooth and data domain is manifold, then both the rank of sub-networks and intrinsic dimension of feature manifolds decrease monotonically by depth: Rank(f1) Rank(f2 f1) Rank(fL1 f1) Rank(FL), dim(X ) dim(F1) dim(F2) dim(FL). for = 1 do for = 1 do // Start Local Training Thread i,j θt1 θt i,j for inner step = 1 Hi do i,j) i,j, L) Di (x, θt i,j InnerOpt(θt θt end for // Start Compress and Communicate Thread if >1 then AllReduce(C(δt1 t1 i,j t1 i,j δt1 et end if // Waiting all threads to finish ri+1, Hi+1 AdaGradCmp(c, ri, Hi, t1 i,j (θ(t1) δt i,j) + et and error compensation if >1 then θt i,j q, ri))i{1D} ) i,j // pseudo-gradients OuterOpt(θt1 θt end if end for i,j , t1 ) end for end for DiLoCoX is variant of LocalSGD and DiLoCo. When updating the pseudo-gradient after local training for steps, we need to synchronize all gradient information as efficiently as possible to reduce the gradient differences between different nodes caused by compression. Therefore, we choose Quantization and Low-Rank compression methods that support AllReduce. To maximize the compression ratio, inspired by the combined different compression of CocktailSGD, we adopt combined compression method of Quantization and Low-Rank which is illustrated in Algorithm 1. 2.4.3. ADAPTIVE GRADIENT COMPRESS ALGORITHM How to coordinate the number of steps of local training and two combined compression algorithms to balance the training efficiency and the convergence of the model when This principle (Feng et al., 2022) describes the behavior of generic neural networks with almost everywhere smooth components, which exhibits the monotonic decreasing (but not strictly) of network ranks and intrinsic dimensionality of feature manifolds. This property will affect the gradient rank of the parameter space through back-propagation. The direction of parameter updates in the network gradually focuses on few principal components, increasing the redundancy of the gradient matrix and causing natural decrease in rank. This implies that as training progresses, the effective information of gradients gradually concentrates within lowrank space. Therefore, during the model training process, as iterations deepen, we can progressively utilize lower ranks to further compress communication data. Algorithm 3 Adaptive Gradient Compress Algorithm Input: Set the gradient rank window c, initialize the rank for practical compression r1 and corresponding local training step H1. Here denotes the iteration of outer optimizer, rank reduction ratio αr. // After each outer optimizer completes the AllReduce operation. // Calculate if < then rt = r1 αr = 1 Ht = H1 with the globally averaged gradient. else (cid:80)t rt = 1 αr = r1rt Ht = H1 αr tc+1 r end if Output: Adaptive rt and Ht. On the other hand, Local SGD and quantization also provide some data compression capabilities. The overall compression effectiveness depends on their combined usage. When the rank is dynamically and adaptively adjusted during the training process, the hyperparameter in Local 5 DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster SGD can also be continuously tuned accordingly to maximize the overlap between local training and gradient updates while ensuring model convergence. where θ Rd, is the dimension of θ, is the low rank and denotes the precision level given in the QUANTIZE function (Alistarh et al., 2017). We propose Adaptive Gradient Compress Algorithm illustrated in Alg 3. During the initial and intermediate phases of model training, gradients descend rapidly, leading to sharp decrease in t. Our rt dynamically responds to the changes in gradient ranks, enabling more efficient low-rank compression. And Ht will also be adaptively adjusted accordingly by computing αr. In the final phase, gradient descent slows significantly, and the low-rank parameters evolve at reduced pace. Consequently, this leads to stable rt and Ht, which means stable low-rank compression. 3. Theoretical Analysis Assumption 3.1. (Smoothness) For any {0, , 1}, the loss function fi is L-smooth. There exists constant > 0 such that for any θ1, θ2 Rd, the following holds: (cid:13) (cid:13) (cid:13) fi(θ1) fi(θ2) (cid:13) (cid:13) (cid:13) θ1 θ2 . Assumption 3.2. (Data Sampling) The stochastic gradient computed is an unbiased estimation for the full gradient with bounded variance, i.e, there exists constant σ > 0 such that for any local dataset Di, it holds that for any θ Rd: EξDi[fi(θ; ξ)] = fi(θ), and EξDifi(θ1; ξ) fi(θ1)2 σ2. Assumption 3.3. (Data Heterogeneity) There exists ξ > 0 such that for any θ Rd, it holds that 1 1 (cid:88) i= fi(θ) (θ)2 ξ2 Lemma 3.4. (Local Update Stability) For any θ Rd, after steps of AdamW local training, the parameter deviation satisfies: Eθt i,j θt12 η2H 2σ2 where η is constant related to the AdamW learning rate. Assumption 3.5. (Compression Error) The end-to-end compression procedure in Algorithm 1 has bounded error such that for any θ Rd, there exists constant 0 ω < 1 C(θ) θ2 ω2 θ Lemma 3.6. CL, CQ to be LOW-RANK and QUANTIZE respectively. Then it holds that the end to end compression in DiLoCoX: = CQ CL fulfills the Assumption 3.3 Lemma 3.7. Assuming the number of training steps for the outer optimizer is , the number of training steps for the inner optimizer is H, data parallelism is D, and the convergence of the local SGD algorithm is O( 1 ). DT Corollary 3.8. (Convergence Rate) Under assumption 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, and 3.7 if we use Algorithm 1 as C, under the learning rate γ = the convergence rate of Algorithm 2 is: + ω2/3 LT 1/3 1 DHT (cid:16) (cid:17) L"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 Ef (θt)2 (cid:18) L(f (θ0) )"
        },
        {
            "title": "DHT",
            "content": "+ L4/3(σ2 + ξ2 + η2H 2σ2)1/3ω2/3 2/3 (cid:19) where is the data parallelism degree, is the number of local steps, is the total outer steps, and = inf θ (θ). We observe Corollary 3.8 that the parameter related to local SGD acts on the first term when the compression ratio is not very aggressive and the first term is the leading term. The second term of compression is almost introduced by gradient compression and accumulated bias from local multi-step updates. Under Assumption 3.6, when ω2 0, adaptive compression that combines Low-Rank and Quantization has almost no impact on convergence. The accumulated bias from local multi-step updates appears as higher-order term in the convergence rate and slightly degrades the convergence speed when the number of local training steps is excessively large. By adjusting the parameters r, q, H, an optimal trade-off can be achieved among communication cost, computational efficiency, and model accuracy. 4. Experiments In this section, we design several experiments to demonstrate the effectiveness of our proposed DiLoCoX. 4.1. Experimental Setup 4.1.1. DATASET AND MODELS We pretrain OPT-1.3B and modified Qwen1.5-107B model (reducing the total number of layers from 80 to 78 for GPU memory optimization) on the WikiText-103 dataset (Merity et al., 2016; Wang et al., 2023). 4.1.2. INFRASTRUCTURE ω2 = 1 2q We conduct large-scale model training on NVIDIA A80040G GPUs. To emulate decentralized slow-network en6 DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster vironments, we apply Linux traffic control (tc) to limit inter-worker communication bandwidth to 1 Gbps for data parallelism. For the OPT-1.3B model, we deploy 2 nodes with 8 A800 GPUs each (16 GPUs total). For the modified Qwen1.5-107B model, 20 nodes are allocated, each containing 8 A800 GPUs, resulting in 160 GPUs in total. 4.1.3. BASELINES AND PARAMETERS We compare with three baselines: AllReduce without local training and gradient compression as the first baseline because the AllReduce method is equivalent to centralized distributed training. OpenDiLoCo as the second baseline for its performance and application in real-world decentralized local training setting(Jaghouar et al., 2024). CocktailSGD as the third baseline for its aggressive compression which can achieve up to 117 without hurting the convergence (Wang et al., 2023). Hyperparameter Tuning For fair comparison, we adjust the compression ratios of different algorithms to the same level through hyperparameter tuning. For the OPT-1.3B model, OpenDiLoCo sets the local training step to 500. CocktailSGD random sparsification ratio is set to 0.1, the top-k ratio is 0.08, and quantization is Int4. DiLoCoX sets the local training step H1 to 125 and quantizes to Int4. The communication compression ratio of all algorithms is set to 500x. For the customized Qwen1.5-107B model, CocktailSGD random sparsification ratio is set to 0.1, the top-k ratio is 0.04, and quantize to Int4. DiLoCoX set the local training step H1 to 125, Low-Rank r1 to 2,048 (approximately 2x compression) and quantize to Int4. The communication compression ratio of all algorithms is set to 1,000x. The total training step of all experiments is set to fixed 4,000 steps, and the gradient rank window is set to 5. 4.2. Main Results 4.2.1. CONVERGENCE RESULT We conducted comparative experiments evaluating AllReduce, DiLoCoX, OpenDiLoCo, and CocktailSGD on the OPT-1.3B and Qwen1.5-107B models, respectively. For the OPT-1.3B model, the experimental results are shown in Figure 3(a). After 4,000 steps, the losses of AllReduce, DiLoCoX, OpenDiLoCo, and CocktailSGD reach 4.06, 4.27, 5.37, and 5.79, respectively. The loss of DiLoCoX is negligible compared to AllReduce and significantly outperforms OpenDiLoCo and CocktailSGD by large margin. We believe that the primary reasons for the degraded convergence performance lie in OpenDiLoCos excessively large causing gradient staleness and CocktailSGDs overly aggressive compression strategy, whereas DiLoCoX achieves superior convergence by adopting more balanced compression strategy. For the OPT-1.3B model, we did not use the adaptive gradient compression algorithm because Int4 quantization and 125-step local training can overlap well and achieve good results. When training the Qwen1.5-107B model, OpenDiLoCo encounters out-of-memory (OOM) errors due to GPU memory constraints. Consequently, we evaluate DiLoCoXs convergence performance against AllReduce and CocktailSGD under this setting. The experimental results demonstrate that after 4,000 training steps, the losses of AllReduce, DiLoCoX, and CocktailSGD reach 3.90, 4.20, and 5.23, respectively. As shown in Figure 3(b), DiLoCoX achieves consistently superior convergence compared to CocktailSGD while maintaining competitive performance relative to AllReduce. 4.2.2. END-TO-END RUNTIME The experimental results of the throughput of AllReduce, OpenDiLoCo, CocktailSGD, and DiLoCoX at different model scales are shown in Figure 4. Under 1 Gbps low-bandwidth environment, the throughputs of AllReduce, CocktailSGD, and DiLoCoX reach 745 tokens/s, 16,161 tokens/s, and 23,880 tokens/s respectively on the OPT-1.3B model. DiLoCoX achieves the highest throughput - 32 higher than AllReduce. When scaled to the Qwen1.5-107B model, the throughputs of AllReduce, CocktailSGD, and DiLoCoX reach 10.4 tokens/s, 2,427 tokens/s, and 3,728 tokens/s, where DiLoCoX demonstrates 1.35 and 357 throughput advantages over CocktailSGD and AllReduce respectively. 4.3. Ablation Study To validate the effectiveness of key components in DiLoCoX, we conduct ablation experiments that focusing on two core innovations that affect model convergence and throughput: One-Step-Delay Overlap, Adaptive Gradient Compression. All experiments are performed on Qwen1.5107B models under 1Gbps bandwidth constraints. Table 1 shows the results for the Qwen1.5-107B model. The full DiLoCoX configuration attains loss of 4.20 and throughput of 3,728 tokens/s. Without the One-Step-Delay Overlap, the loss reduces to 4.15, yet the throughput drops substantially to 2,197 tokens/s. When the Adaptive Gradient Compression is removed, the loss is 4.02, and the throughput further decreases to 1,168 tokens/s. The AllReduce method has the lowest loss of 3.90 among all configurations but an extremely low throughput of only 10.4 tokens/s, highlighting the inefficiency of this traditional approach compared to DiLoCoX. 7 DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster (a) Loss of AllReduce, DiLoCoX, OpenDiLoCo and CocktailSGD on OPT-1.3B model training (b) Loss of AllReduce, DiLoCoX and CocktailSGD on Qwen1.5-107B model training Figure 3. Training loss comparison across different distributed optimization methods. model convergence degradation. To the best of our knowledge, this is currently the largest-scale model for decentralized clusters training. This breakthrough provides new possibilities for fully utilizing the comprehensive computing power of decentralized clusters in the future to achieve the goal of training larger-scale model training."
        },
        {
            "title": "References",
            "content": "Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M. Qsgd: Communication-efficient sgd via gradient quantization and encoding, 2017. URL https: //arxiv.org/abs/1610.02132. Alistarh, D., Hoefler, T., Johansson, M., Khirirat, S., Konstantinov, N., and Renggli, C. The convergence of sparsified gradient methods, 2018. URL https://arxiv. org/abs/1809.10505. Baidu. Bringing hpc techniques to deep learning, 2017. Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. Brown, T. B. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Dai, L., Qi, H., Chen, W., and Lu, X. High-speed data communication with advanced networks in large language model training. IEEE Micro, 2024. Devlin, J. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Figure 4. Throughput OpenDiLoCo, CocktailSGD and DiLoCoX. Comparison between AllReduce, Table 1. Loss and Throughput of Qwen1.5-107B Configuration Qwen1.5-107B Loss Throughput Full DiLoCoX w/o Overlap w/o Compression AllReduce 4.20 4.15 4.02 3.90 3,728 2,197 1,168 10. 5. Conclusion For conducting training models exceeding 100B on lowcommunication decentralized clusters, this paper proposes DiLoCoX, low-communication large-scale decentralized cluster training framework which significantly improves the speed of model pre-training, expands the scale of model parameters, and provides theoretical analysis of the convergence. Empirically, we show that DiLoCoX can pre-train 107B foundation model over 1Gbps network. Compared with centralized cluster, DiLoCoX can significantly achieve 357x distributed training speed with negligible 8 DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster Strom, N. Scalable distributed dnn training using In Interspeech, commodity gpu cloud computing. 2015. URL https://api.semanticscholar. org/CorpusID:9338808. Vogels, T., Karimireddy, S. P., and Jaggi, M. Powersgd: Practical low-rank gradient compression for distributed optimization, 2020. URL https://arxiv.org/ abs/1905.13727. Wang, H., Li, J., Wu, H., Hovy, E., and Sun, Y. Pre-trained language models and their applications. Engineering, 2022. Wang, J., Lu, Y., Yuan, B., Chen, B., Liang, P., De Sa, C., Re, C., and Zhang, C. Cocktailsgd: fine-tuning foundation models over 500mbps networks. 2023. Wangni, J., Wang, J., Liu, J., and Zhang, T. Gradient sparsification for communication-efficient distributed optimization, 2017. URL https://arxiv.org/abs/1710. 09854. Zhang, Z., Zheng, S., Wang, Y., Chiu, J., Karypis, G., Chilimbi, T., Li, M., and Jin, X. Mics: near-linear scaling for training gigantic model on public cloud. arXiv preprint arXiv:2205.00119, 2022. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023a. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., Desmaison, A., Balioglu, C., Damania, P., Nguyen, B., Chauhan, G., Hao, Y., Mathews, A., and Li, S. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023b. URL https://arxiv.org/abs/2304.11277. Douillard, A., Feng, Q., Rusu, A. A., Chhaparia, R., Donchev, Y., Kuncoro, A., Ranzato, M., Szlam, A., and Shen, J. Diloco: Distributed low-communication training of language models. arXiv preprint arXiv:2311.08105, 2023. Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. Glam: Efficient scaling of language models with mixture-ofexperts. In International Conference on Machine Learning, pp. 55475569. PMLR, 2022. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Feng, R., Zheng, K., Huang, Y., Zhao, D., Jordan, M., and Zha, Z.-J. Rank diminishing in deep neural networks. Advances in Neural Information Processing Systems, 35: 3305433065, 2022. Jaghouar, S., Ong, J. M., and Hagemann, J. Opendiloco: An distributed preprint for training. open-source low-communication arXiv:2407.07852, 2024. framework globally arXiv Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Merity, S., Xiong, C., Bradbury, J., and Socher, R. arXiv preprint Pointer sentinel mixture models. arXiv:1609.07843, 2016. Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., et al. Efficient large-scale language model training on gpu clusters using megatronlm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 115, 2021. Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models, 2020. URL https://arxiv.org/abs/ 1910.02054. Rendle, S., Fetterly, D., Shekita, E. J., and Su, B.-y. Robust large-scale machine learning in the cloud. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 11251134, 2016. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. 9 DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster A. Theoretical Analysis of Convergence and Extensive A.1. Proof of Lemma 3.4 Consider the global parameter θt1 at iteration 1. After steps of AdamW local training, the updated local parameter is θt i,j. Define the parameter deviation at step as: where θ(0) = θt1 and θ(H) = θt i,j. The total parameter deviation is: = θ(h) θ(h1) (h = 1, 2, . . . , H), The AdamW update rule at step is: i,j θt1 = θt (cid:88) h=1 h. = η mh vh + ϵ , where mh = β1mh1 + (1 β1)gh, vh = β2vh1 + (1 β2)g2 rate, β1, β2 [0, 1) are decay rates, and ϵ > 0 ensures numerical stability. h, gh is the stochastic gradient at step h, η is the learning The stochastic gradient gh satisfies: E[gh] = (θ(h1)), Egh (θ(h1))2 σ2. Under steady-state conditions (vh E[g2 h] and ϵ 1), the update simplifies to: η gh (cid:112)E[g2 h] . Using the gradient noise assumption Egh2 2 + σ2, we derive: Eh2 η2 Egh2 E[g2 h] η2σ2. The total deviation squared norm expectation is: (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) h=1 (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) = (cid:88) h=1 Eh2 + [i j] . (cid:88) i<j Assume updates are fully positively correlated: [i j] = Ei Ej η2σ2. Then, we get (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) h=1 (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) Hη2σ2 + H(H 1)η2σ2 = 2η2σ2. That completes the proof of Lemma 3.4. 10 DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster A.2. Proof of Lemma 3.6 For an input gradient δ Rd, after low-rank compression, we obtain δ1 = LOWRANK(δ, r), where the rank satisfies: (cid:16) (cid:17) δ1 δ2 1 δ2 . Further quantizing δ1 to q-bits, the quantization error satisfies: The total compression error is bounded by: Thus, we get it, δ2 δ12 2qδ12 . Eδ2 δ2 (cid:16) 1 2q(cid:17) δ2. ω2 = 1 2q. A.3. Proof of the DiLoCoX Convergence Analysis( Corollary 3.8) For DiLoCoX model training, inner optimizer use AdamW while outer optimizer use Nesterov Momentum. After local training steps of the inner optimizer, the parameter deviation is θt i,j with error compensation is: i,j θt1. The pseudo gradient θt i,j = (θt1 θt δt i,j) + et i,j, where et i,j is the compression error buffer. The compressed global gradient is: The compression error satisfies: j = 1 (cid:88) i=1 C(δt1 i,j ). (cid:13) (cid:13) t1 (cid:13) (cid:13) (cid:13) 1 (cid:88) i= δt1 i,j (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) ω2 (cid:88) i= Eδt1 i,j 2. The inner optimizer updates parameters as θt bounded gradient variance Efi(θ; ξ)2 σ2, we derive: i,j = θt1 η (cid:80)H h=1 mh vh+ϵ , where mh and vh are momentum terms. By the And we can get (cid:13) (cid:13) (cid:13) (cid:13) mh vh + ϵ (cid:13) 2 (cid:13) (cid:13) (cid:13) η2σ Eθt i,j θt12 η2H 2σ2. Using Nesterov Momentum for the outer optimizer updates θt = θt1 + γt1 + β(θt1 θt2), the objective function satisfies: (θt) (θt1) γf (θt1), t1 + Taking expectation and rearranging terms: γ2L t12 + βLθt1 θt22. E[f (θt)] E[f (θt1)] γEf (θt1)2 + γ2LEt12 + βLEθt1 θt22 + γLEt1 (θt1)2. The global gradient error is decomposed into three parts: Et1f (θt1)2 (cid:124) (cid:88) (cid:13) (cid:13) t1 (cid:13) (cid:13) (cid:13) 1 (cid:123)(cid:122) Compression Error i=1 δt1 i,j (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 D (cid:88) i=1 (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:125) + (cid:124) (δt i,j (θt1)) (cid:123)(cid:122) Local Deviation (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:125) + (cid:124) (cid:88) (f (θt1; x) (θt1)) i=1 (cid:123)(cid:122) Stochastic Noise . (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:125) 11 DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster By Assumption 3.5, we can infer to the term of the compression error: (cid:13) (cid:13) t1 (cid:13) (cid:13) (cid:13)"
        },
        {
            "title": "1\nD",
            "content": "D (cid:88) i=1 δt1 i,j (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) ω2 D (cid:88) i=1 Eδt1 i,j 2. By Lemma 3.1 and Lemma 3.4, we can infer to the term of the local deviation: Eδt i,j (θt1)2 L2η2H 2σ2. By Lemma 3.2 and Lemma 3.3, we can infer to the term of the stochastic noise: (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)"
        },
        {
            "title": "1\nD",
            "content": "D (cid:88) i=1 (f (θt1; ξ) (θt1)) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) σ2 + ξ2. Integrating the three aspects discussed above, summing over = 1, . . . , , optimizing γ, and bounding terms: 1 (cid:88) t=1 Ef (θt)2 2(f (θ0) ) γT + γL (cid:18) σ2 + ξ2 + L2η2H 2σ2 (cid:19) + γ2Lω2Eδ2. while choosing the learning rate as γ = (cid:16) 1 DHT + ω2/3 LT 1/ (cid:17) , we obtain the final convergence rate. Thus, 1 (cid:88) t=1 Ef (θt)2 That completes the proof. + L4/3(σ2 + ξ2 + η2H 2σ2)1/3ω2/3 2/3 (cid:19) (cid:18) L(f (θ0) ) DHT"
        }
    ],
    "affiliations": [
        "China Mobile(Suzhou) Software Technology, JiangSu",
        "Zero Gravity Labs"
    ]
}