{
    "paper_title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
    "authors": [
        "Junjie Chen",
        "Fei Wang",
        "Zhihao Huang",
        "Qing Zhou",
        "Kun Li",
        "Dan Guo",
        "Linfeng Zhang",
        "Xun Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Fréchet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 0 4 3 5 1 . 2 1 5 2 : r Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics Junjie Chen1,2 Fei Wang1,2 Zhihao Huang5,6 Qing Zhou8 Kun Li7 Dan Guo1 Linfeng Zhang4 Xun Yang3 1Hefei University of Technology 2IAI, Hefei Comprehensive National Science Center 3USTC 4SJTU 5TeleAI, China Telecom 6Northwestern Polytechnical University 7United Arab Emirates University 8Anhui Polytechnic University jorji.chen@gmail.com Figure 1. Comparison of head generation paradigms and our TIMAR framework. Prior paradigms treat talking and listening as separate processes: Talking-Head Generation produces speech-driven motion without user feedback, while Listening-Head Generation yields reactive behavior without causal continuity. TIMAR unifies both within an interleaved masked and causally grounded framework, modeling conversation as sequential turns of interleaved user-agent audio-visual tokens. It achieves intra-turn alignment through bidirectional fusion and inter-turn dependency through causal attention, producing coherent and contextually responsive 3D head motion."
        },
        {
            "title": "Abstract",
            "content": "Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal fullsequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark [39] show that TIMAR reduces Frechet Distance and MSE by 1530% on the test set, and achieves similar gains on out-ofdistribution data. The source code will be released in the GitHub repository CoderChen01/towards-seamless-interaction. 1. Introduction Human conversation is an intricate interplay of speech and facial behavior. Beyond verbal communication, subtle nonverbal signals such as head nods, gaze shifts, and microexpressions continuously convey intent, attention, and empathy [7]. Modeling these bidirectional dynamics is essential for embodied conversational agents, social robots, and immersive telepresence systems that must listen, react, and respond naturally in streaming conversational setting [44]. 1 Recent advances in 3D talking-head generation [10, 18, 30, 49, 60, 65] and listening-head synthesis [31, 34, 43, 68] have significantly improved visual realism and speech synchronization. However, most frameworks still treat these two processes, talking and listening, as independent directions of motion generation, lacking unified temporal model that captures their mutual influence. As illustrated in Figure 1, talking-head systems generate motion only from speakers own audio, and listening-head systems react only to the interlocutor, while natural conversation emerges from their intertwined evolution. Even the recent DualTalk [39] framework, though jointly modeling both speakers, relies on bidirectional attention over full conversations. Such formulation is effective for offline synthesis but less suited for causal or streaming generation, where models must respond turn by turn to ongoing dialogue. Our core motivation is that conversational behavior unfolds through causally linked turns [44], where each turns facial motion depends on both speakers preceding speech and visual cues, reflecting how humans naturally coordinate responses through continuous multimodal feedback. To reflect this principle, we formulate interactive 3D conversational head generation1 as turn-level causal process, aligning computational modeling with the temporal logic of human interaction. As shown in Figure 1, we introduce TIMAR, an autoregressive-diffusion framework that couples masked, turn-level causal modeling over interleaved audio-visual tokens with diffusion-based decoding of continuous 3D agent head. TIMAR represents conversation as an interleaved sequence of multimodal tokens from both participants, segmented at the turn level. The model fuses intra-turn audio-visual information bidirectionally while maintaining causal dependencies across turns, and predicts the agents 3D head using lightweight diffusion-based generative head conditioned on the fused context. This formulation enables the model to accumulate conversational history and reason over conversational flow. Our approach introduces three main contributions: Turn-level causal formulation. We formulate interactive 3D head generation as causal, turn-wise prediction problem, enforcing strict temporal consistency and supporting streaming-compatible generation. Interleaved multimodal fusion. We design an interleaved audio-visual context that encodes both speakers speech and 3D head tokens, enabling the model to learn intra-turn alignment and inter-turn dependency under causal constraints for coherent conversational modeling. Lightweight diffusion-based generative decoding. We introduce compact diffusion-based decoder that models 3D head motion as continuous probabilistic process, capturing natural variability while maintaining temporal coherence across conversational turns. 1A problem statement is provided in Appendix Sec. 6 for clarity. Compared with DualTalk [39], which processes entire dialogues non-autoregressively, TIMARs causal formulation enables natural and streaming-capable generation that mirrors real conversational timing and feedback. Extensive evaluations demonstrate consistent improvements in realism, synchronization, and responsiveness across both indistribution and unseen scenarios. 2. Related Work 2.1. 3D Talkingand Listening-Head Generation Talking-Head Generation. Early works on talking-head generation aim to synthesize speakers facial motion from speech, producing temporally aligned and expressive visual outputs [40, 67, 71]. large body of research operates directly in the RGB domain, learning mappings from audio to lip movements and facial expressions in videos [12, 13, 21, 40, 56, 61, 62, 67, 71]. Representative methods such as VASA-1 [62], Hallo [61], and EchoMimic [12] have demonstrated realistic speech-driven head animations. To enhance controllability and geometric consistency, several studies [11, 24, 57, 66] introduce 3D Morphable Models (3DMM) [4, 5, 17] as intermediate representations, predicting 3DMM parameters conditioned on audio signals. More recent works advocate for direct generation in 3D space, which enables physically grounded motion synthesis that can be directly applied to robotic heads, virtual avatars, and psychological or affective behavior analysis [2, 9, 10, 15, 16, 18, 25, 26, 29, 30, 3538, 41, 47, 49 52, 59, 60, 6365, 70]. For instance, FaceFormer [18], CodeTalker [60], DiffPoseTalk [49], and TexTalker [30] adopt transformeror diffusion-based frameworks [22, 54] to learn continuous 3D head dynamics from speech. Listening-Head Generation. Parallel to talking-head research, another line of work explores listening-head generation, which models non-verbal feedback of listener conditioned on the interlocutors speech or facial motion. These systems aim to reproduce subtle and socially meaningful behaviors such as nodding, gaze shifts, and microexpressions that convey attention, agreement, or emotional alignment [6, 8, 23, 31, 32, 34, 43, 45, 46, 68]. Early approaches learn reactive behaviors directly from 2D videos [6, 8, 46], while recent studies employ 3DMMor mesh-based representations to achieve more geometrically consistent listener motion [31, 34, 53, 55, 68]. Despite these advances, most talkingand listeningheads are treated as separate processes, with one responsible for speaking and the other for responding, rather than capturing the intertwined temporal dynamics that define genuine dyadic communication. In contrast, our work focuses on unified modeling of interactive 3D head dynamics under an interleaved conversational structure. 2 Figure 2. The architecture and workflow of TIMAR. TIMAR models interactive 3D conversational head dynamics through turn-level, causal, and interleaved generation process. In training (left), the speech and head motions of both user and agent are encoded into shared token space, interleaved by conversational turns, with the agent head tokens masked. The Turn-level Causal Multimodal Fusion module fuses audio-visual context bidirectionally within each turn and causally across turns, producing masked-agent features that condition the Lightweight Diffusion Head to learn the head motion distribution. In sampling (right), the model caches history tokens and autoregressively denoises each new turn, yielding temporally coherent and context-aware 3D head motion generation in streaming conversation. 2.2. Modeling Interactive 3D Conversational Heads 3.1. Interleaved Audio-Visual Context Modeling conversational interaction is essential for producing coherent and socially responsive 3D head dynamics [1, 69]. Human dialogue is inherently bidirectional and temporally dependent, yet most generative frameworks still treat each participant independently, failing to capture the continuous exchange of verbal and non-verbal cues [14, 22, 42, 44, 48, 54]. Some 2D-based studies (e.g., INFP [72], ARIG [19]) have explored mutual head or gesture coordination between interlocutors, but these methods remain limited to image-space motion and lack explicit 3D geometry control. In 3D domains, DualTalk [39] represents an important step toward dual-speaker modeling, integrating both speaking and listening behaviors within unified framework. However, its bidirectional full-sequence processing is designed for offline synthesis and is less suited for causal or streaming generation. Our framework models conversations as causally conditioned turns, enabling temporally coherent and streaming-capable 3D head generation through interleaved autoregressive diffusion. Given -second conversational segment sampled at an audio rate fs and motion frame rate fh, let the users speech and 3D head motion be Su RT fs and RT fhdh, and the agents speech and motion be Sa RT fs and RT fhdh , where dh denotes the dimensionality of the 3D head representation. TIMAR first aligns all modalities in shared token space using pretrained speech tokenizer and learnable 3D head motion encoder, then constructs an Interleaved Audio-Visual Context that provides the turnlevel multimodal input for autoregressive generation. Speech Tokenizer. We define speech tokenizer Fspeech built on pretrained model Mspeech with token dimension ds. To align the extracted speech features with the shared dt-dimensional token space, learnable projection Pspeech : Rds Rdt is applied after temporal alignment. Specifically, interpfh () temporally resamples the speech features to match the motion frame rate fh, ensuring synchronized alignment across modalities. Given speech S, the Fspeech produces the token sequence as 3. The TIMAR Framework = Pspeech (cid:0)interpfh (cid:0)Mspeech(S)(cid:1)(cid:1) , RT fhdt. We propose TIMAR, causal framework for interactive 3D conversational head generation that builds on interleaved audio-visual context. As shown in Figure 2, TIMAR discretizes speech and encodes 3D head parameters into shared token space, segments them into fixed-length turns, and interleaves user-agent streams. turn-level fusion module models intra-turn alignment and inter-turn dependency under causal masking, while diffusion head denoises the masked agent head from the fused context. (1) We denote the resulting speech token sequences for the user and the agent as Su and Sa respectively. 3D Head Motion Encoder. To embed 3D head motion into the same token space, we introduce an encoder Fhead that maps the 3D head parameters of each frame to dtdimensional token representation: = Fhead(H), RT fhdt . (2) 3 We denote the corresponding user and agent 3D head motion token sequences as Hu and Ha, respectively. Interleaving. We segment the audio-visual context sequences into = /c chunks of seconds each2. For the i-th chunk, we define: = Su = Hu Hu [(i1)cfh:icfh], [(i1)cfh:icfh], Ha = Sa = Ha [(i1)cfh:icfh], [(i1)cfh:icfh]. (3) In practice, each chunk is encoded independently rather than from the full sequence, ensuring that no future information is exposed during tokenization. Finally, the interleaving function Finterleave constructs the interleaved audio-visual context token sequence as: = Finterleave(S u, a, Hu, Ha) = (Ti)N , where Ti = (S i , Hu , Ha ) . i=1 , (4) The resulting provides temporally aligned, turn-level interleaved audio-visual tokens across both participants, forming the multimodal conversational context that drives the causal autoregressive generation in TIMAR. 3.2. Turn-Level Causal Multimodal Fusion Given the token sequence , the Turn-Level Causal Multimodal Fusion module, denoted as Ffusion, is designed to perform intra-turn alignment and capture inter-turn dependencies under turn-level causal constraints. Positional Embedding. To encode both intra-turn and inter-turn positional relations, we introduce learnable positional embedding P1 that provides explicit temporal awareness for each token in . This positional embedding allows the model to distinguish not only the token order within turn but also the relative positions across turns, facilitating temporally consistent contextual reasoning. Turn-Level Causal Attention. The fusion process is implemented by stacked Transformer encoder equipped with our proposed Turn-Level Causal Attention (TLCA). As illustrated in Figure 3, TLCA enables bidirectional attention among tokens within the same turn to achieve finegrained speech-motion alignment, while constraining attention across turns to be strictly causal, ensuring that each turn can only attend to preceding ones. This design allows the encoder to learn short-term multimodal synchronization and long-term conversational dependency jointly. Fusion Process. Omitting normalization and residual connections for brevity, the Ffusion can be expressed as: = Ffusion(T ) = E(T + P1) = (Zi)N i=1 , (5) where Zi denotes the fused representation of the i-th turn, and represents the temporally integrated multimodal fea2We preprocess data such that is divisible by c. Figure 3. Illustration of Turn-Level Causal Attention (TLCA). The example shows two turns with two tokens per modality (user speech, user head, agent speech, and agent head). Different color blocks represent modality-wise token communication. TLCA models both intra-turn communication through bidirectional attention and inter-turn communication through turn-level causal attention to capture temporal dependencies without future leakage. ture sequence. These features serve as the bottleneck representations that condition the diffusion head to model the per-token probability distribution of 3D agent head. 3.3. Lightweight Diffusion Head As illustrated in Figure 2 (left), we introduce the Lightweight Diffusion Head Fdiff, which models masked agent head through conditional denoising in continuous parameter space. During training, we randomly select subset of agent head positions and replace each selected position with the same learnable mask token hm. For any masked position i, the fused contextual representation produced by Ffusion is denoted as zm , and the corresponding ground-truth agent head parameter is written as ha Ini . spired by Li et al. [28], we model the conditional distribution p(ha ) through diffusion process rather than deterministic regression, allowing the model to capture the intrinsic stochasticity and multimodality of natural 3D facial motion without relying on discrete quantization. zm"
        },
        {
            "title": "Given zm",
            "content": "i , Fdiff predicts the clean agent head parameters at position via conditional denoising. To enable Fdiff to be aware of the frame index of each masked position, we introduce learnable positional embedding P2 and add the corresponding vector (i) to the conditioning feature. The 2 per-frame prediction process is then formulated as ˆha = Fdiff(zm , τ ) = ϵθ(xτ τ, zm + (i) 2 ), (6) where τ is the diffusion timestep and xτ is noisy version of the ground-truth parameter ha produced by predefined forward noise schedule. During sampling, xτ is initialized 4 from pure noise and iteratively denoised to recover the final 3D head parameters. Implemented as lightweight MLP, ϵθ performs efficient token-wise diffusion in continuous parameter space, bridging multimodal conversational context and geometric reconstruction to generate stochastic yet temporally coherent 3D head dynamics. 3.4. TIMAR Training Recipe We now describe the training strategy of TIMAR for learning 3D conversational dynamics via multimodal fusion and lightweight diffusion-based reconstruction. The model is optimized to recover masked agent head parameters conditioned on interleaved conversational context. Full details are provided in Appendix Sec. 7. Masking Strategy. During training, fixed proportion of the agent head tokens is randomly replaced by learnable mask token hm. This random masking encourages the model to learn robust token-wise completion and to generalize across different conversational contexts. The masked tokens are processed through the multimodal fusion and diffusion modules, enabling the model to reconstruct the corresponding ground-truth 3D agent head parameters conditioned on the visible conversational context. Optimization Objective. We minimize diffusion objective under the x0-prediction parameterization, operating directly on the continuous 3D head sequences. We use fθ to denote the end-to-end process illustrated in Figure 2 (left), which includes speech and head encoding, interleaving, masking, multimodal fusion, and lightweight diffusion-based reconstruction. Given the conversational inputs (Su, Sa, u, a), let denote the index set of masked agent-head positions. For each K, the clean target is the ground-truth agent head parameter , which is perturbed within Fdiff through the forward diffusion process q(xτi ) using randomly sampled timestep τi. Conditioned on the raw multimodal inputs and τi, the model predicts the denoised estimate at position i, yielding the persample diffusion loss: Ldiff = 1 (cid:88) iK Eτi (cid:104)(cid:13) (cid:13)H (i) θ (Su, Sa, u, a, τi)(cid:13) 2 (cid:13) 2 (cid:105) . (7) Under the learned-variance setting, the final objective also includes variational bound term, yielding combined loss on mean and variance predictions. Classifier-Free Guidance (CFG). With fixed probability pcfg, the users entire set of tokens is replaced by shared learnable fake token hf during training. This stochastic substitution constructs an unconditional training branch that removes contextual cues from the user side, allowing the model to learn both context-dependent and contextindependent 3D agent head generation. 3.5. TIMAR Sampling As shown in Figure 2 (right), during sampling, TIMAR performs turn-wise autoregressive generation using two components: conversational stream that provides the incoming multimodal inputs, and context token buffer that stores previously processed tokens for temporal conditioning. Turn Construction. At each conversational turn t, we collect c-second segment of user speech Su , agent speech , and user head motion Sa . These signals are processed by the speech tokenizer Fspeech and head encoder Fhead to obtain the corresponding token sequences Su , and Hu . The agent heads of this turn is filled with learnable mask (cid:1)K token sequence Hm , where = cfh denotes i=1 the number of frames in the turn. The current turn is represented as Tt = (Su , Hu ). To provide historical context, context token buffer stores the tokenized turns from the previous steps, (Ttn, . . . , Tt1), where is hyperparameter controlling the context history length. The input for the current turn is then constructed as = (cid:0)hm , Hm , Sa , Sa t,i Tt = (Ttn, . . . , Tt1, Tt) . (8) For all previous turns in the buffer, the agent head tokens remain filled with mask tokens rather than the predicted ones. This prevents the accumulation of autoregressive errors and ensures that the model relies solely on reliable conversational context rather than potentially compounding errors. Turn-Level Autoregressive Diffusion Sampling. At each conversational turn t, the interleaved token sequence Tt is first processed by the fusion module Ffusion to obtain the fused representation Zt. The features corresponding to the masked agent-head tokens, denoted as Zm , are then fed into the diffusion head Fdiff, which performs iterative denoising across diffusion timesteps. Starting from Gaussian noise, the diffusion process progressively refines the latent variables conditioned on Zm , recovering the predicted 3D agent head parameters ˆH for the current turn. Sampling with CFG. The CFG-based sampling adjusts the strength of contextual conditioning during iterative denoising. For each conversational turn t, the unconditional feam are obtained by replacing all user tokens in Tt tures with fake token hf . At each diffusion step τ , let Xτ denote the current noisy estimate of the 3D head parameters. The CFG-based denoising update is formulated as ϵθ(Xτ τ, t ) + ω(cid:2)ϵθ(Xτ τ, Zm ) ϵθ(Xτ τ, )(cid:3), (9) where ω is the guidance scale that controls the trade-off between contextual adherence and generative diversity. For brevity, the addition of positional embeddings to the conditioning features is omitted in the above expression. Methods EXP FD JAW 103 POSE 102 EXP P-FD JAW POSE 102 EXP 101 MSE JAW 103 POSE 102 SID EXP JAW POSE rPCC JAW 101 POSE 101 EXP 102 Test Dataset 1.75 1.77 3.27 3.46 3.70 3.91 1.90 1.98 3.42 3.63 2.02 2. 5.35 5.34 1.53 1.49 2.99 2.95 2.27 2.36 12.46 12.25 13.76 13. 13.93 14.16 15.33 15.59 Context History (n = 0) DualTalk* DualTalk TIMAR 9.61 4.32 1.51 0.39 2.98 0.44 10.91 4.42 1.63 0.39 3.27 0.43 3.94 1.40 1.15 0.34 1.73 0.54 3.42 0.43 2.33 0.19 1.84 0.15 4.51 0.83 1.19 0.14 2.26 0.02 Context History (n = 1) DualTalk* DualTalk TIMAR 9.45 2.80 1.53 0.22 3.03 0.24 10.57 3.02 1.64 0.22 3.28 0.27 3.78 0.93 1.11 0.23 1.69 0.42 3.45 0.31 2.34 0.13 1.84 0.11 4.36 0.74 1.18 0.10 2.24 0.05 Context History (n = 3) DualTalk* DualTalk TIMAR 9.11 2.14 1.57 0.10 3.06 0.16 10.13 2.38 1.66 0.13 3.28 0.20 3.61 0.71 1.09 0.15 1.63 0.37 3.51 0.26 2.34 0.08 1.86 0.10 4.19 0.69 1.19 0.04 2.19 0.03 Context History (n = 7) DualTalk* DualTalk TIMAR 8.97 1.95 1.57 0.10 3.08 0.21 9.93 2.16 1.65 0.13 3.28 0.25 3.58 0.51 1.07 0.11 1.61 0.31 3.53 0.22 2.35 0.08 1.85 0.08 4.12 0.69 1.22 0.01 2.18 0.00 Out-of-Distribution Dataset 12.75 12.51 11.53 11.25 12.41 12. 11.26 10.92 2.00 2.08 1.28 1.24 4.42 4.32 1.79 1.79 3.21 3. 1.76 1.73 4.88 4.98 1.23 1.24 1.67 1.68 3.22 3.38 3.48 3. 2.24 2.26 2.16 2.18 1.69 1.66 5.34 5.97 1.33 1.34 2.14 2. 2.28 2.28 2.11 2.21 4.79 4.71 3.12 3.14 1.38 1.34 1.73 1. 5.10 5.39 1.28 1.28 1.86 1.89 3.55 3.73 2.20 2.21 2.20 2. 3.28 3.31 1.92 1.99 1.20 1.18 4.20 4.09 1.77 1.74 4.84 4. 1.23 1.25 1.78 1.79 3.29 3.43 1.67 1.69 3.53 3.68 2.27 2. 2.18 2.20 3.00 2.98 1.94 1.98 5.34 5.12 4.96 4.84 2.72 2. 2.80 2.71 5.03 4.82 2.91 2.81 1.67 1.55 6.86 7.11 1.41 1. 7.30 7.28 1.86 1.77 2.67 2.64 2.89 2.83 5.23 5.08 2.72 2. 4.93 4.78 2.78 2.73 6.94 6.85 1.75 1.67 2.83 2.72 21.33 21. 22.44 22.73 23.89 24.22 21.70 21.68 23.09 23.11 Context History (n = 0) DualTalk* DualTalk TIMAR 20.62 1.82 2.50 0.21 4.31 0.51 22.10 1.79 2.62 0.19 4.62 0.50 6.46 0.82 1.56 0.21 2.29 0.54 2.85 0.18 2.03 0.05 1.55 0.10 6.76 0.10 1.55 0.00 2.77 0.21 Context History (n = 1) DualTalk* DualTalk TIMAR 20.44 1.24 2.53 0.08 4.33 0.45 21.72 1.37 2.64 0.08 4.61 0.47 6.29 0.56 1.52 0.15 2.24 0.49 2.87 0.14 2.02 0.02 1.54 0.09 6.59 0.26 1.57 0.03 2.79 0.12 Context History (n = 3) DualTalk* DualTalk TIMAR 20.21 1.04 2.49 0.15 4.36 0.48 21.38 1.22 2.60 0.15 4.60 0.54 6.16 0.44 1.49 0.13 2.19 0.47 2.90 0.12 2.03 0.03 1.54 0.06 6.47 0.20 1.56 0.00 2.70 0.15 Context History (n = 7) DualTalk* DualTalk TIMAR 20.23 0.98 2.56 0.16 4.50 0.46 21.34 1.14 2.66 0.16 4.72 0.51 6.17 0.25 1.48 0.11 2.18 0.40 2.93 0.12 2.04 0.03 1.52 0.07 6.26 0.36 1.57 0.02 2.77 0.02 Table 1. Comparison with DualTalk [39] under progressive-context inference. Each turn corresponds to 1-second segment (default setting), and the agents 3D head motion for the current turn is predicted using previous turns as context history (n = 0, 1, 3, 7). Note that = 0 corresponds to no context history. Results are reported on the test and out-of-distribution (OOD) sets. Metrics with are better when lower (FD, P-FD, MSE, rPCC), and metrics with are better when higher (SID). DualTalk denotes results from the official released checkpoint, and DualTalk represents our re-trained model under the same training configuration. indicates improvement over DualTalk based on the best-performing metric, while denotes drop or no change. 22.56 22.48 21.31 21.21 22.64 22.60 1.45 1.45 6.85 6.90 1.63 1. 2.73 2.73 2.81 2.81 2.67 2.58 6.53 6.42 1.63 1.59 1.43 1. 6.79 6.62 1.70 1.59 2.85 2.82 1.97 2.00 2.97 2.91 5.13 4. 2.75 2.72 5.40 5.23 1.97 2.01 3.02 2.79 2.76 2.78 2.73 2. 1.69 1.62 6.70 6.60 1.44 1.48 6.78 6.67 1.66 1.56 2.82 2. 5.25 5.14 1.97 2.00 2.94 2.85 4. Experiments Benchmark Setup. We follow the experimental setup of the interactive 3D conversational head generation benchmark proposed by DualTalk [39]. All models are trained on their official training split and evaluated on the provided test dataset and an additional out-of-distribution dataset to assess generalization. The 3D head representation is represented by 56 FLAME parameters [27], including 50 expression, 3 jaw, and 3 head pose dimensions per frame. Dataset details are provided in Appendix Sec. 8.1. Evaluation Metrics. We use the same evaluation metrics as DualTalk, including Frechet Distance (FD), Paired Frechet Distance (P-FD), Mean Squared Error (MSE), SI for Diversity (SID), and Residual Pearson Correlation Coefficient (rPCC). All metrics are computed separately for the expression (EXP), jaw (JAW), and head pose (POSE) components of the FLAME parameters. Together, these metrics assess the realism, temporal synchronization, motion diversity, and expression accuracy of generated 3D head dynamics. Metric details are provided in Appendix Sec. 8.2. TIMAR Default Configuration. Unless otherwise specified, all experiments are conducted under unified default setup. Mspeech uses wav2vec 2.0 [3]. The shared token dimension is set to dt = 1024. Speech and motion sequences are sampled at fs = 16 kHz and fh = 25 fps, respectively. Each training conversation sample spans = 8 and is divided into = 8 fixed-length turns with chunk duration = 1 s. Each turn contains temporally aligned user and agent audio-visual segments forming one interleaved context unit for training. During sampling, the context history expands progressively over previous turns (up to = 7). Implementation details are provided in Appendix Sec. 7. Comparison Protocol. As the interactive 3D conversational head generation task was first introduced by DualTalk [39], we adopt it as the primary baseline for evaluation. For completeness, we also compare with several adapted methods that can serve as surrogate baselines for this task; these results are included in Appendix Sec. 9.1. To ensure fair comparison, we retrain DualTalk under the same training configuration as TIMAR. During inference, we extend its pipeline with progressive context mecha6 Figure 4. Qualitative comparison under progressive context history. Green text marks the agent whose 3D head is predicted, while user denotes the interacting speaker from ground truth. Each column compares TIMAR and DualTalk predictions under different context histories (n=0, 1, 3, 7), showing how longer contexts enable more coherent and responsive agent behaviors. Figure 5. Effect of classifier-free guidance (CFG) during sampling. Left: Quantitative results showing FD and P-FD metrics under different guidance scales ω. Right: Visual comparison of generated agent heads with varying ω, where higher guidance improves contextual consistency and expressiveness. DualTalk cannot support CFG-based sampling. Green text denotes the agent, whose 3D head is predicted. Figure 6. Model performance versus parameter size. FD and P-FD metrics (lower is better) are computed on the test dataset for the 3D head EXP parameters. Results show that enlarging the DualTalk model does not improve performance, whereas TIMAR achieves lower errors with fewer or comparable parameters. nism that enables causal reasoning over an expanding dialogue history. The model incrementally accumulates contextual information across turns until reaching the preset history limit (n = 7). When this limit is exceeded, the earliest turns are truncated to maintain fixed-length temporal window, ensuring that both methods operate under consistent streaming conditions. Figure 7. Comparison of training dynamics between Diffusion Head and MLP Head. Both variants are trained under identical settings, and their loss curves are directly comparable. Detailed quantitative results are provided in Table 2. TIMAR achieves consistent gains across context depths and datasets. As shown in Table 1, TIMAR consistently outperforms DualTalk across all metrics on both the test and out-of-distribution (OOD) datasets. Even without history (n=0), TIMAR achieves notable improvements, reducing FD and P-FD by up to 30%. As the context history in7 Methods EXP FD JAW POSE 102 EXP P-FD JAW 103 POSE 102 EXP 101 MSE JAW POSE 102 EXP SID JAW POSE EXP rPCC JAW 101 POSE 101 9.48 3.04 1.82 9. 3.56 1.67 3.36 1.12 1.76 10. 10.45 Bottleneck Ablation w/ MLP Head 3.43 w/ Diffusion Head 8.97 0.51 1.57 0.10 3.08 0.28 9.93 0.52 1.65 0.11 3.28 0.28 3.58 0.15 1.07 0.05 1.61 0.10 3.53 0.03 2.35 0.01 1.85 0.08 4.12 0.28 1.22 0.16 2.18 0.23 Attention Ablation w/ Bi-Attention 1.18 8.97 0.15 1.57 0.25 3.08 0.04 9.93 0.20 1.65 0.26 3.28 0.03 3.58 0.09 1.07 0.11 1.61 0.01 3.53 0.01 2.35 0.08 1.85 0.01 4.12 0.00 1.22 0.18 2.18 0.03 w/ TLCA Backbone Architecture Ablation w/ Asym. EncDec 3.57 w/ Encoder-Only 8.97 0.54 1.57 0.18 3.08 0.11 9.93 0.50 1.65 0.19 3.28 0.10 3.58 0.01 1.07 0.06 1.61 0.02 3.53 0.02 2.35 0.05 1.85 0.00 4.12 0.03 1.22 0.09 2.18 0.01 Table 2. Ablation study on core architectural components. We compare TIMAR with alternative designs across three aspects: (i) replacing the diffusion-based head with direct MLP predictor (Bottleneck Ablation), (ii) substituting the proposed Turn-Level Causal Attention (TLCA) with full bidirectional attention (Attention Ablation), and (iii) adopting an asymmetric encoderdecoder design following MAE [20] instead of the encoder-only backbone (Backbone Architecture Ablation). Results are reported on the test dataset. 10.43 2. 2.21 2.19 1.91 1.84 3.50 1. 1.71 2.34 4.40 1.38 3.25 3. 1.62 3.54 2.27 1.86 4.12 1. 9.51 1.75 1.13 3.19 3.38 3. 1.85 1.63 2.30 4.15 1.31 creases from n=0 to n=7, the model further improves in FD and MSE, indicating that richer conversational history enhances temporal coherence and motion realism. In contrast, DualTalk exhibits limited benefit from extended context due to its non-causal formulation. Beyond quantitative evaluation, qualitative results in Figure 4 show that TIMAR generates more context-appropriate and natural behaviors. In the first example, it avoids unnecessary mouth opening when the agent is listening, while in the second, it maintains more stable and context-aligned head pose. These observations demonstrate that TIMAR effectively exploits longer conversational context to produce smoother, more expressive, and causally consistent 3D head motion. Impact of classifier-free guidance. We further investigate the effect of classifier-free guidance on generation quality within our diffusion-based sampling process. As shown in Figure 5, increasing properly ω improves both FD and P-FD metrics, reflecting stronger contextual adherence and better alignment between the agents responses and the users multimodal inputs. Starting from ω=0, which corresponds to unconditional sampling, the generated heads appear less responsive and contextually ambiguous. As ω increases properly, the agent exhibits progressively richer expressions and more synchronized motion with the interlocutor, demonstrating the benefit of conditional modulation on interactive dynamics. In contrast, DualTalk lacks CFG mechanism and thus cannot adjust its contextual conditioning during inference, resulting in fixed, non-controllable generation. Performance comparison across parameter scales. To verify that our performance gains stem from improved modeling rather than increased capacity, we compare TIMAR and DualTalk across different parameter scales, as illustrated in Figure 6. Results show that enlarging DualTalk does not yield consistent improvements in FD or P-FD metrics, while TIMAR achieves lower errors under comparable parameter counts. These findings indicate that the proposed causal formulation and interleaved fusion lead to more effective learning without relying on model size. Ablation studies on core design choices. We conduct ablation experiments to evaluate the contribution of each core component in TIMAR, as summarized in Table 2 and Figure 7. Replacing the diffusion head with direct MLP predictor results in smoother training loss but inferior test performance, indicating overfitting and weaker generalization. The diffusion-based formulation achieves better FD and PFD scores by capturing the stochastic nature of conversational motion. When substituting the proposed Turn-Level Causal Attention (TLCA) with full bidirectional attention, temporal consistency slightly deteriorates, demonstrating the necessity of causal masking for progressive generation. Finally, adopting an asymmetric encoderdecoder design following MAE [20] does not improve results, and the encoder-only configuration yields better performance. This comparison is motivated by the shared masked-prediction paradigm with MAE, allowing us to test whether an additional decoder benefits temporal reasoning. 5. Conclusion We introduced TIMAR, causal framework for interactive 3D conversational head generation that unifies talking and listening behaviors through turn-level autoregressive diffusion. By interleaving multimodal tokens from both participants, TIMAR models the mutual dependency between speech and head motion across conversational turns, enabling natural coordination and temporal coherence. The proposed turn-level causal attention allows the model to accumulate and reason over conversational history, while the lightweight diffusion head captures the inherent stochasticity of nonverbal expression in continuous 3D space. Comprehensive experiments demonstrate that TIMAR consistently outperforms the DualTalk baseline across diverse metrics and context depths, reducing both Frechet Distance and MSE while improving expressiveness and contextual synchronization. Ablation studies further confirm the effectiveness of the diffusion-based decoding and turnlevel causal attention design. Together, these results validate that explicitly causal and interleaved modeling leads to more humanlike conversational dynamics than non-causal full-sequence formulations. Future work will extend this paradigm to multi-party settings, richer facial behaviors, and emotionand intent-aware multimodal feedback."
        },
        {
            "title": "References",
            "content": "[1] Vasu Agrawal, Akinniyi Akinyemi, Kathryn Alvero, Morteza Behrooz, Julia Buffalini, Fabio Maria Carlucci, Joy Chen, Junming Chen, Zhang Chen, Shiyang Cheng, et al. Seamless interaction: Dyadic audiovisual motion modeling and large-scale dataset. arXiv preprint arXiv:2506.22554, 2025. 3 [2] Shivangi Aneja, Justus Thies, Angela Dai, and Matthias Nießner. Facetalk: Audio-driven motion diffusion for neural parametric head models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2126321273, 2024. 2 [3] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:1244912460, 2020. 6, [4] Volker Blanz and Thomas Vetter. Face recognition based on fitting 3d morphable model. IEEE Transactions on pattern analysis and machine intelligence, 25(9):10631074, 2003. 2 [5] Volker Blanz and Thomas Vetter. morphable model for the synthesis of 3d faces. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 157164. 2023. 2 [6] Dan Bohus and Eric Horvitz. Facilitating multiparty diaIn International Conlog with gaze, gesture, and speech. ference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction, pages 18, 2010. 2 [7] Judee Burgoon, Xinran Wang, Xunyu Chen, Steven Pentland, and Norah Dunbar. Nonverbal behaviors speak relational messages of dominance, trust, and composure. Frontiers in psychology, 12:624177, 2021. [8] Justine Cassell, Catherine Pelachaud, Norman Badler, Mark Steedman, Brett Achorn, Tripp Becket, Brett Douville, Scott Prevost, and Matthew Stone. Animated conversation: rulebased generation of facial expression, gesture & spoken intonation for multiple conversational agents. In Proceedings of the 21st annual conference on Computer graphics and interactive techniques, pages 413420, 1994. 2 [9] Lee Chae-Yeon, Oh Hyun-Bin, Han EunGi, Kim Sung-Bin, Suekyeong Nam, and Tae-Hyun Oh. Perceptually accurate 3d talking head generation: New definitions, speech-mesh representation, and evaluation metrics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2106521074, 2025. 2 [10] Hejia Chen, Haoxian Zhang, Shoulong Zhang, Xiaoqiang Liu, Sisi Zhuang, zhangyuan, Pengfei Wan, Di ZHANG, and Shuai Li. Cafe-talk: Generating 3d talking face animation In The with multimodal coarseand fine-grained control. Thirteenth International Conference on Learning Representations, 2025. 2 [11] Xinyu Chen and Sheng Tang. Emotion-aware talking face generation based on 3dmm. In 2024 4th International Conference on Neural Networks, Information and Communication Engineering (NNICE), pages 18081813, 2024. 2 [12] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike audio-driven portrait In Proanimations through editable landmark conditions. ceedings of the AAAI Conference on Artificial Intelligence, pages 24032410, 2025. 2 [13] Hanbo Cheng, Limin Lin, Chenyu Liu, Pengcheng Xia, Pengfei Hu, Jiefeng Ma, Jun Du, and Jia Pan. DAWN: Dynamic frame avatar with non-autoregressive diffusion framework for talking head video generation. In The Thirteenth International Conference on Learning Representations, 2025. [14] Herbert Clark and Susan Brennan. Grounding in communication. 1991. 3 [15] Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, and Michael Black. Capture, learning, and synthesis of 3d speaking styles. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1010110111, 2019. 2 [16] Radek Danˇeˇcek, Kiran Chhatre, Shashank Tripathi, Yandong Wen, Michael Black, and Timo Bolkart. Emotional speech-driven animation with content-emotion disentanglement. In SIGGRAPH Asia 2023 Conference Papers, pages 113, 2023. 2 [17] Bernhard Egger, William AP Smith, Ayush Tewari, Stefanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani, et al. 3d morphable face modelspast, present, and future. ACM Transactions on Graphics (ToG), 39(5):138, 2020. 2 [18] Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, and Taku Komura. Faceformer: Speech-driven 3d facial animation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1877018780, 2022. 2, IV [19] Ying Guo, Xi Liu, Cheng Zhen, Pengfei Yan, and Xiaoming Wei. Arig: Autoregressive interactive head generation for In Proceedings of the IEEE/CVF real-time conversations. International Conference on Computer Vision (ICCV), 2025. [20] He, Chen, Xie, Li, Dollar, and Girshick. Masked autoencoders are scalable 314 vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern, pages 1600016009, 2021. 8 [21] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, jialiang zhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu, HsiangTao Wu, sheng zhao, and Jiang Bian. GAIA: Zeroshot talking avatar generation. In The Twelfth International Conference on Learning Representations, 2024. 2 [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [23] Ailin Huang, Zhewei Huang, and Shuchang Zhou. Perceptual conversational head generation with regularized driver In Proceedings of the 30th ACM and enhanced renderer. international conference on multimedia, pages 70507054, 2022. 2 [24] Ricong Huang, Weizhi Zhong, and Guanbin Li. Audiodriven talking head generation with transformer and 3d morphable model. In Proceedings of the 30th ACM International Conference on Multimedia, page 70357039, New York, NY, USA, 2022. Association for Computing Machinery. 2 [25] Tero Karras, Timo Aila, Samuli Laine, Antti Herva, and Jaakko Lehtinen. Audio-driven facial animation by joint endto-end learning of pose and emotion. ACM Transactions on Graphics (ToG), 36(4):112, 2017. 2 [26] Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Jun Zhou, and Lin Gu. Instag: Learning personalized 3d talking head from In Proceedings of the Computer Vision few-second video. and Pattern Recognition Conference, pages 1069010700, 2025. 2 [27] Tianye Li, Timo Bolkart, Michael Black, Hao Li, and Javier Romero. Learning model of facial shape and expression from 4d scans. ACM Trans. Graph., 36(6):1941, 2017. 6, II [28] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. 4 [29] Weichuang Li, Longhao Zhang, Dong Wang, Bin Zhao, Zhigang Wang, Mulin Chen, Bang Zhang, Zhongjian Wang, Liefeng Bo, and Xuelong Li. One-shot high-fidelity talkinghead synthesis with deformable neural radiance field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1796917978, 2023. 2 [30] Xuanchen Li, Jianyu Wang, Yuhao Cheng, Yikun Zeng, Xingyu Ren, Wenhan Zhu, Weiming Zhao, and Yichao Yan. Towards high-fidelity 3d talking avatar with personalized dynamic texture. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 204214, 2025. 2 [31] Jin Liu, Xi Wang, Xiaomeng Fu, Yesheng Chai, Cai Yu, Jiao Dai, and Jizhong Han. Mfr-net: Multi-faceted responsive listening head generation via denoising diffusion model. In Proceedings of the 31st ACM international conference on multimedia, pages 67346743, 2023. 2 [32] Xi Liu, Ying Guo, Cheng Zhen, Tong Li, Yingying Ao, and Pengfei Yan. Customlistener: Text-guided responsive interIn Proaction for user-friendly listening head generation. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24152424, 2024. [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. III [34] Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa, and Shiry Ginosar. Learning to listen: In ProModeling non-deterministic dyadic facial motion. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2039520405, 2022. 2, IV [35] Arthur Niswar, Ee Ping Ong, Hong Thai Nguyen, and Zhiyong Huang. Real-time 3d talking head from synthetic viseme dataset. In Proceedings of the 8th International Conference on Virtual Reality Continuum and its Applications in Industry, pages 2933, 2009. 2 [36] Ziqiao Peng, Yihao Luo, Yue Shi, Hao Xu, Xiangyu Zhu, Hongyan Liu, Jun He, and Zhaoxin Fan. Selftalk: selfsupervised commutative training diagram to comprehend 3d talking faces. In Proceedings of the 31st ACM International Conference on Multimedia, pages 52925301, 2023. IV [37] Ziqiao Peng, Haoyu Wu, Zhenbo Song, Hao Xu, Xiangyu Zhu, Jun He, Hongyan Liu, and Zhaoxin Fan. Emotalk: Speech-driven emotional disentanglement for 3d face animation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2068720697, 2023. IV [38] Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, and Zhaoxin Fan. Synctalk: The devil is in the synchronization for talking head synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 666676, 2024. 2 [39] Ziqiao Peng, Yanbo Fan, Haoyu Wu, Xuan Wang, Hongyan Liu, Jun He, and Zhaoxin Fan. Dualtalk: Dual-speaker interaction for 3d talking head conversations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2105521064, 2025. 1, 2, 3, 6, I, IV [40] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM international conference on multimedia, pages 484492, 2020. 2 [41] Alexander Richard, Michael Zollhofer, Yandong Wen, Fernando De la Torre, and Yaser Sheikh. Meshtalk: 3d face animation from speech using cross-modality disentanglement. In Proceedings of the IEEE/CVF international conference on computer vision, pages 11731182, 2021. 2 [42] Harvey Sacks, Emanuel Schegloff, and Gail Jefferson. simplest systematics for the organization of turn-taking for conversation. language, 50(4):696735, 1974. 3 [43] Maksim Siniukov, Di Chang, Minh Tran, Hongkun Gong, Ashutosh Chaubey, and Mohammad Soleymani. Ditailistener: Controllable high fidelity listener video generation with diffusion. arXiv preprint arXiv:2504.04010, 2025. 2 [44] Gabriel Skantze. Turn-taking in conversational systems and human-robot interaction: review. Computer Speech & Language, 67:101178, 2021. 1, 2, 3, [45] Luchuan Song, Guojun Yin, Zhenchao Jin, Xiaoyi Dong, and Chenliang Xu. Emotional listener portrait: Neural listener head generation with emotion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2083920849, 2023. [46] Sinan Sonlu, Ugur Gudukbay, and Funda Durupinar. conversational agent framework with multi-modal personality expression. ACM Transactions on Graphics (TOG), 40(1): 116, 2021. 2 [47] Stefan Stan, Kazi Injamamul Haque, and Zerrin Yumak. Facediffuser: Speech-driven 3d facial animation synthesis using diffusion. In Proceedings of the 16th ACM SIGGRAPH Conference on Motion, Interaction and Games, pages 111, 2023. 2 [48] Tanya Stivers, Nicholas Enfield, Penelope Brown, Christina Englert, Makoto Hayashi, Trine Heinemann, Gertie Hoymann, Federico Rossano, Jan Peter De Ruiter, Kyung-Eun Yoon, et al. Universals and cultural variation in turn-taking in conversation. Proceedings of the National Academy of Sciences, 106(26):1058710592, 2009. 3 [49] Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Lin, Jenny Sheng, Yu-Hui Wen, Minjing Yu, and Yong-jin Liu. Diffposetalk: Speech-driven stylistic 3d facial animation and head pose 10 generation via diffusion models. ACM Transactions on Graphics (TOG), 43(4):19, 2024. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation, 2024. 2 [50] Kim Sung-Bin, Lee Chae-Yeon, Gihun Son, Oh Hyun-Bin, Janghoon Ju, Suekyeong Nam, and Tae-Hyun Oh. Multitalk: Enhancing 3d talking head generation across languages with multilingual video dataset. In Interspeech 2024, pages 1380 1384, 2024. [51] Kim Sung-Bin, Lee Hyun, Da Hye Hong, Suekyeong Nam, Janghoon Ju, and Tae-Hyun Oh. Laughtalk: Expressive 3d talking head generation with laughter. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 64046413, 2024. [52] Balamurugan Thambiraja, Ikhsanul Habibie, Sadegh Aliakbarian, Darren Cosker, Christian Theobalt, and Justus Thies. Imitator: Personalized speech-driven 3d facial animation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2062120631, 2023. 2 [53] Minh Tran, Di Chang, Maksim Siniukov, and Mohammad Soleymani. Dim: Dyadic interaction modeling for social beIn European Conference on Computer havior generation. Vision, pages 484503. Springer, 2024. 2 [54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2, [55] Yinuo Wang, Yanbo Fan, Xuan Wang, Guo Yu, and Fei Wang. Diffusion-based realistic listening head generation via In Proceedings of the Computer hybrid motion modeling. Vision and Pattern Recognition Conference, pages 15885 15895, 2025. 2 [56] Zhongjian Wang, Peng Zhang, Jinwei Qi, Guangyuan Wang, Sheng Xu, Bang Zhang, and Liefeng Bo. Omnitalker: Real-time text-driven talking head generation with in-context audio-visual style replication. In Proceedings of the 39th Annual Conference on Neural Information Processing Systems (NeurIPS), 2025. 2 [57] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animations, 2024. 2 [58] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-Art Natural Language Processing. pages 3845. Association for Computational Linguistics, 2020. I, III [59] Jiajian Xie, Shengyu Zhang, Mengze Li, chengfei lv, Zhou Zhao, and Fei Wu. Ecoface: Audio-visual emotional codisentanglement speech-driven 3d talking face generation. In The Thirteenth International Conference on Learning Representations, 2025. 2 [60] Jinbo Xing, Menghan Xia, Yuechen Zhang, Xiaodong Cun, Jue Wang, and Tien-Tsin Wong. Codetalker: Speech-driven 3d facial animation with discrete motion prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1278012790, 2023. 2, IV [61] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Yao Yao, and Siyu zhu. [62] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. VASA-1: Lifelike audio-driven talking faces generated in real time. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2 [63] Karren Yang, Anurag Ranjan, Jen-Hao Rick Chang, Raviteja Vemulapalli, and Oncel Tuzel. Probabilistic speechdriven 3d facial motion synthesis: New benchmarks methods and applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2729427303, 2024. 2 [64] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He, and Zhou Zhao. Geneface: Generalized and high-fidelity audio-driven 3d talking face synthesis. In The Eleventh International Conference on Learning Representations, 2023. [65] Zhenhui Ye, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, Rongjie Huang, Jinglin Liu, Chen Zhang, Xiang Yin, Zejun MA, and Zhou Zhao. Real3d-portrait: One-shot realistic 3d talking portrait synthesis. In The Twelfth International Conference on Learning Representations, 2024. 2 [66] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86528661, 2023. 2 [67] Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, and Ziwei Liu. Pose-controllable talking face generation by implicitly modularized audio-visual representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 41764186, 2021. 2 [68] Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao, and Tao Mei. Responsive listening head generation: benchmark dataset and baseline. In European conference on computer vision, pages 124142. Springer, 2022. 2 [69] Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, and Tiejun IEEE Zhao. Interactive conversational head generation. Transactions on Pattern Analysis and Machine Intelligence, 47(8):66736686, 2025. 3 [70] Xukun Zhou, Fengxin Li, Ziqiao Peng, Kejian Wu, Jun He, Biao Qin, Zhaoxin Fan, and Hongyan Liu. Meta-learning empowered meta-face: Personalized speaking style adaparXiv tation for audio-driven 3d talking face animation. preprint arXiv:2408.09357, 2024. [71] Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li. Makelttalk: speaker-aware talking-head animation. ACM Transactions On Graphics (TOG), 39(6):115, 2020. 2 [72] Yongming Zhu, Longhao Zhang, Zhengkun Rong, Tianshu Hu, Shuang Liang, and Zhipeng Ge. Infp: Audio-driven inIn Proteractive head generation in dyadic conversations. ceedings of the Computer Vision and Pattern Recognition Conference, pages 1066710677, 2025. 3 11 Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics"
        },
        {
            "title": "Outline of This Document",
            "content": "1. Introduction 2. Related Work 2.1. 3D Talkingand Listening-Head Generation 2.2. Modeling Interactive 3D Conversational Heads 3. The TIMAR Framework 3.1. Interleaved Audio-Visual Context 3.2. Turn-Level Causal Multimodal Fusion 3.3. Lightweight Diffusion Head 3.4. TIMAR Training Recipe 3.5. TIMAR Sampling 4. Experiments 5. Conclusion 6. Problem Statement 7. TIMAR Details 7.1. Network Details 7.2. Implementation Details 8. DualTalk Benchmark Details 8.1. Datasets 8.2. Metrics 9. Additional Results 9.1. Comparison with Other Baselines 9.2. Diffusion Head Scalability Study 1 2 2 3 3 3 4 4 5 5 6 8 III III III IV Turn-Level Causal Formulation. In real conversations, speech and motion evolve sequentially across turns, where each participants behavior depends on the accumulated interaction history rather than future information [44]. However, existing frameworks such as DualTalk [39] rely on bidirectional encoders and full-sequence attention, which expose future frames during training and thus break causal consistency. Such formulations are limited to offline synthesis and cannot effectively support streaming or autoregressive generation, where the model must respond in real time to the conversational flow. To better align computational modeling with human conversational dynamics, we reformulate the problem as turn-level causal generation task. dialogue is divided into fixed-length turns, each containing temporally aligned multimodal observations from both participants. For each turn {1, . . . , }, the model observes the users speech and motion (Su 1:t) and the agents speech (Sa 1:t). The objective is to estimate the conditional distribution of the agents head motion at the current turn, representing the desired dynamics to be inferred given the accumulated conversational context: 1:t, pθ(H (cid:89) 1:N Su 1:N , 1:N , Sa 1:N ) = pθ(H Su 1:t, 1:t, Sa 1:t) . (11) 6. Problem Statement t=1 We study the task of interactive 3D conversational head generation in dyadic settings. Given two participants, user and an agent, the goal is to synthesize the agents 3D head motion that coherently reflects both speaking and listening behaviors. The generated motion should capture verbal articulation such as lip and jaw movements, and nonverbal feedback including nodding, gaze shifts, and subtle facial expressions, all conditioned on the evolving multimodal conversational context. Signals and Objective. Let the users speech and head motion be Su and u, and the agents speech be Sa. The target is the agents head motion a. Speech sequences are sampled at rate fs, and head motion at frame rate fh, with Su, Sa RT fs and u, RT fhdh, where is the duration and dh the dimensionality of the 3D head parameters. The overall goal is to model the conditional distribution pθ(H Su, u, Sa) , (10) This causal factorization introduces two advantages. First, it strictly prevents future information leakage, enabling turn-by-turn prediction suitable for streaming generation. Second, it preserves the interleaved nature of dualspeaker interaction, enabling conversational cues such as rhythm, affect, and timing to propagate across turns. These principles establish the foundation of our TIMAR framework, which builds upon this causal formulation to model dual-speaker 3D conversational dynamics. 7. TIMAR Details 7.1. Network Details Speech Tokenizer. We employ the wav2vec 2.0 [3] model 3. As shown in as the speech feature extractor Mspeech Figure 8, each c-second audio chunk Si Rcfs is first passed through the frozen feature extractor of wav2vec 2.0, which captures plausible 3D head dynamics of the agent given the multimodal context of both speakers. 3We use the facebook/wav2vec2-large-960h-lv60-self checkpoint from the HuggingFace Hub [58]. Figure 8. Architecture of the speech tokenizer. Figure 9. Architecture of the 3D head motion encoder. which converts raw waveforms into low-frequency acoustic embeddings of dimension = 512 at frame rate of fw = 50 Hz, yielding an output of size R(cfw1)d s. These features are linearly interpolated to match the 3D head motion frame rate fh, producing Rcfhd representations. The interpolated sequence is then passed through the pretrained feature projection and encoder modules of Mspeech, resulting in contextualized embeddings of dimension ds. Finally, learnable linear projection Pspeech : Rds Rdt maps the features into the shared token space, yielding the final token sequence Si Rcfhdt for each chunk. 3D Head Motion Encoder. As shown in Figure 9, each c-second motion segment Hi Rcfhdh consists of cfh frames of 3D head parameters, where dh denotes the dimensionality of the FLAME-based [27] head representation. We implement the motion encoder Fhead as two-layer multilayer perceptron (MLP) with ReLU activations and hidden dimension of dt 2 , followed by final linear projection to the shared token space of dimension dt. The encoded feature sequence is denoted as Hi = Fhead(Hi) Rcfhdt. Turn-Level Causal Multimodal Fusion. As illustrated the interleaved context sequence in Figure 10, RN (4cfh+10)dt consists of conversational turns, where each turn contains four modality-specific token sequences (user speech, agent speech, user head, and agent head), each representing c-second temporal window at frame II Figure 10. Architecture of the Turn-Level Causal Multimodal Fusion module. Gray squares denote learnable separator tokens that delineate modality boundaries and turn transitions. rate fh. The interleaved multimodal sequence is first linearly projected to the Transformer Encoder input dimension de and augmented with set of learnable separator tokens. Specifically, ten special tokens are inserted between different modalities (i.e., user speech, agent speech, user head, and agent head) and between adjacent turns. These tokens act as soft delimiters that help the model distinguish modality boundaries and prevent temporal leakage across turns, while also providing explicit structural cues that stabilize causal attention during training. This design choice preserves the turn-level temporal order and improves multimodal alignment without altering the causal formulation. After token augmentation, the sequence is normalized and enriched with learnable positional embedding P1, enabling both intra-turn and inter-turn temporal reasoning. The Transformer encoder equipped with Turn-Level Causal Attention (TLCA) then processes the sequence under strict causal masking, allowing bidirectional interaction within each turn while constraining cross-turn attention to past tokens only. The fused representation encodes both fine-grained multimodal correspondence and long-range conversational dependencies, serving as the contextual backbone for diffusion-based head generation. Lightweight Diffusion Head. As shown in Figure 11, the diffusion head Fdiff takes the noisy 3D head parameter xt and the contextual condition zm as inputs. Both are first linearly projected into hidden diffusion space of dimension dm, where zm is augmented with learnable positional and timestep embeddings to encode frame-level and temporal information. The denoising network consists of residual modulation blocks, each performing feature-wise conditional transformation driven by zm. For each normalized activation x, modulation operation is applied as: Modulate(x, shift, scale) = (1 + scale) + shift, (12) where shift and scale are obtained from linear projections"
        },
        {
            "title": "Data scale\nDuration\nNumber of Identities\nNumber of All Samples\nNumber of Training Samples\nNumber of Test Samples\nNumber of OOD Samples",
            "content": "50h 1000+ 5763 4853 533 377 Distribution of conversation rounds 1 Rounds 2 Rounds 3 Rounds 4 Rounds 5 Rounds 6+ Rounds 1995 (34.6%) 1126 (19.5%) 1172 (20.3%) 632 (11.0%) 414 (7.2%) 424 (7.4%) Table 3. Dataset statistics of the DualTalk benchmark. The dataset comprises 50 hours of dual-speaker conversations with over 1000 unique identities. It includes official training, testing, and OOD splits, and the lower section reports the distribution of conversation rounds per sample. fusion module contains 16 layers with hidden dimension de = 1024 and 16 attention heads. The diffusion head uses = 3 residual modulation blocks, each operating in latent diffusion space of dm = 1024. Training Configuration. The model is optimized using AdamW [33] with batch size of 32 and 400 total epochs. The learning rate is set to 1 104 with 100-iteration warm-up schedule. Training data are segmented into = 8 clips, where each turn corresponds to = 1 chunk of temporally aligned user and agent audio-visual streams. Speech signals are sampled at fs = 16 kHz and head motion sequences at fh = 25 fps. During training, 70% of the agent head tokens are randomly masked (r = 0.7), and classifier-free guidance employs conditional dropout probability of pcfg = 0.1. 8. DualTalk Benchmark Details 8.1. Datasets The DualTalk benchmark dataset provides large-scale corpus for studying dual-speaker 3D conversational head generation. It contains multi-round face-to-face interactions featuring synchronized audio-visual recordings of both participants. All videos are sourced from open-domain interview and dialogue recordings, selected to ensure clear frontal visibility of both speakers and high-quality audio tracks. Each video is recorded at 19201080 resolution and 25 fps, with audio sampled at 16 kHz. Speaker separation, tracking, and 3D reconstruction are performed following the official DualTalk preprocessing pipeline to obtain temporally aligned 3D head parameters and speech signals for both participants. The released dataset comprises approximately 50 hours of processed conversation data, covering more than 1000 distinct identities and 5763 conversational samples in total. The official data split includes 4853 samples for training, 533 for testing, and 377 for outFigure 11. Architecture of the Lightweight Diffusion Head. Multiple outgoing Linear Proj. modules indicate that their outputs are chunked into several parts for modulation and gating operations within each residual block. of zm. This affine modulation allows contextual cues to adaptively rescale and shift the feature responses, enabling expressive and stable conditional denoising. After passing through residual modulation blocks, the final feature is projected back to the original 3D head parameter space to yield ˆha. Despite its compact design, this head effectively captures multimodal stochasticity and preserves temporal coherence in generated 3D motion. 7.2. Implementation Details Software Framework. All experiments are implemented in PyTorch framework. Pretrained components, including the wav2vec 2.0 speech tokenizer, are loaded via the Transformers library [58]. Loss Formulation. The 3D head is represented using 56dimensional FLAME parameters, including 50 expression coefficients, 3 jaw, and 3 head pose parameters. During training, the diffusion loss Ldiff is computed separately for each subset (i.e., expression, jaw, and pose) and then aggregated. This separation stabilizes optimization by accounting for the distinct dynamic ranges and semantic sensitivities across different head components. Default Configuration. The shared token dimension is set to dt = 1024. The Transformer encoder within the"
        },
        {
            "title": "EXP",
            "content": "FD JAW 103 POSE"
        },
        {
            "title": "EXP",
            "content": "P-FD JAW 103 POSE 102 EXP 101 MSE JAW 103 POSE 102 SID"
        },
        {
            "title": "JAW POSE",
            "content": "rPCC JAW 101 POSE 101 EXP 102 2.67 3.58 2.59 2.71 2.49 1.72 1.78 1.61 5.40 6.89 4.33 5.49 3.69 1.90 1.97 1.57 0.54 0 2.86 2.49 2.86 3.48 3.50 3. 0.36 0 1.72 1.30 1.89 2.23 2.25 2.36 5.40 6.89 4.36 5.49 3.74 1.97 2.03 1.65 1.80 2.29 1.76 1.83 1.48 1.04 1.04 1.07 34.90 48.57 29.86 35.77 24.61 11.14 11.08 8.91 8.00 10.74 7.54 8.14 7.08 3.83 4.03 3.06 8.00 10.74 7.58 8.14 7.13 3.97 4.17 3. 34.90 48.57 30.20 35.77 24.99 11.88 11.82 9.88 FaceFormer [18] CodeTalker [60] EmoTalk [37] SelfTalk [36] L2L [34] DualTalk* [39] DualTalk [39] TIMAR Test Dataset 6.97 9.71 6.88 7.15 5.68 3.59 3.52 3.60 Out-of-Distribution Dataset 7.18 10.01 7.73 7.24 6.87 5.97 5.89 6.20 Table 4. Comparison with existing baselines on the DualTalk benchmark. Results are reported on both the test and out-of-distribution (OOD) datasets. Metrics with are better when lower (FD, P-FD, MSE, rPCC), and metrics with are better when higher (SID). DualTalk denotes results from the official released checkpoint, and DualTalk represents our re-trained model under the same training configuration. All baseline results except DualTalk are taken from the original DualTalk paper [39]. Bold indicates the best performance, and underlined values denote the second best. FaceFormer [18] CodeTalker [60] EmoTalk [37] SelfTalk [36] L2L [34] DualTalk* [39] DualTalk [39] TIMAR 35.93 50.05 34.44 36.23 30.87 22.56 22.71 21.33 8.60 11.66 8.62 8.89 8.61 6.06 6.11 4. 8.60 11.66 8.59 8.89 8.56 5.89 5.94 4.48 35.92 50.05 34.12 36.23 30.49 21.71 21.91 20.20 13.05 11.06 9.89 12.25 8.52 4.73 4.62 4.10 11.71 10.24 9.44 11.26 9.02 6.86 6.48 6.22 0.54 0 2.89 2.61 2.76 2.98 3.01 2.96 5.39 6.95 4.21 5.36 3.86 3.22 3.27 2. 0.50 0 0.98 1.28 1.19 1.72 1.70 1.87 0.51 0 0.94 1.08 1.11 1.38 1.33 1.53 0.40 0 1.79 1.36 1.91 1.94 1.96 2.04 5.39 6.95 4.17 5.36 3.82 3.15 3.20 2.56 5.27 5.11 4.94 4.70 4.11 2.38 2.45 2.17 2.41 2.33 2.19 2.39 2.06 1.37 1.35 1. 5.73 5.76 5.54 5.67 4.99 3.28 3.05 2.76 2.16 2.18 1.96 2.13 1.94 1.60 1.77 1.55 2.87 3.88 2.94 2.96 2.98 2.48 2.46 2.18 1.80 2.32 1.71 1.79 1.54 1.50 1.50 1."
        },
        {
            "title": "EXP",
            "content": "dm = 1024 = 1 = 3 = 6 = 9 = 3 dm = 512 dm = 768 dm = 1024 dm = 1280 9.27 8.97 8.70 8.63 9.31 8.94 8.97 8.58 FD JAW 103 1.66 1.57 1.55 1.66 1.73 1.62 1.57 1. POSE"
        },
        {
            "title": "EXP",
            "content": "P-FD JAW 103 POSE 102 EXP 101 MSE JAW 103 POSE 102 SID"
        },
        {
            "title": "JAW POSE",
            "content": "3.11 3.08 3.10 3.02 3.09 3.16 3.08 3.05 10.34 9.93 9.67 9.62 10.34 9.93 9.93 9.58 1.76 1.65 1.64 1.75 1.82 1.71 1.65 1. 3.33 3.28 3.31 3.23 3.3 3.38 3.28 3.26 3.64 3.58 3.55 3.55 3.72 3.53 3.58 3.52 1.14 1.07 1.14 1.16 1.19 1.12 1.07 1. 1.69 1.61 1.66 1.64 1.69 1.70 1.61 1.65 3.54 3.53 3.56 3.57 3.54 3.54 3.53 3.59 2.27 2.35 2.33 2.31 2.31 2.27 2.35 2. 1.83 1.85 1.85 1.85 1.85 1.84 1.85 1.85 rPCC JAW 101 POSE 101 1.32 1.22 1.21 1.34 1.48 1.36 1.22 1. 2.18 2.18 2.22 2.19 2.11 2.19 2.18 2.16 EXP 102 3.96 4.12 4.16 4.29 3.91 4.31 4.12 4.21 Table 5. Scalability study on the Diffusion Head depth (K) and hidden dimension (dm). We examine the influence of varying depth (K) and hidden dimension (dm) on model performance, with results reported on the test dataset. Metrics with are better when lower (FD, P-FD, MSE, rPCC), and metrics with are better when higher (SID). of-distribution (OOD) evaluation. The OOD set contains unseen speakers and conversation scenarios to assess generalization. Table 3 summarizes the overall data scale and the distribution of conversation rounds, where most dialogues contain one to three alternating speaker turns, reflecting natural short-turn interaction patterns. 8.2. Metrics Frechet Distance (FD). FD measures the distributional similarity between generated and ground-truth motions in deep feature space. Given activation statistics (µ1, Σ1) and (µ2, Σ2) from pretrained encoder, it is computed as (cid:16) FD = µ1 µ22 + Tr Σ1 + Σ2 2(Σ1Σ2) (cid:17) 1 , where lower values indicate closer alignment between the generated and real motion distributions. Paired Frechet Distance (P-FD). P-FD extends FD to paired motion embeddings by concatenating the generated agent motion with the corresponding user motion before computing the distance. This paired variant evaluates how IV well the generated motion maintains inter-speaker coherence and synchronization. Mean Squared Error (MSE). MSE quantifies frame-level reconstruction accuracy between predicted and groundtruth 3D head parameters: MSE ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 ˆhi hi2. Lower MSE values indicate higher fidelity to the reference. SI for Diversity (SID). SID measures the diversity of generated motion. Following the DualTalk protocol, k-means clustering (k = 40) is applied to motion features, and the entropy of the cluster assignment histogram is computed as SID = (cid:88) k=1 pk log2(pk + ϵ), where pk denotes the normalized cluster occupancy. Higher SID indicates greater motion variety and less repetition. Residual Pearson Correlation Coefficient (rPCC). rPCC evaluates the temporal correlation between user and agent behaviors. It computes the Pearson correlation of motion trajectories for each speaker pair and measures the L1 distance between the generated and real correlation patterns. Lower rPCC values correspond to more accurate modeling of interactive timing and responsiveness. Implementation Notes. All metrics are computed on motion features extracted from expression, jaw, and pose parameters separately. Frechet-based metrics use covariance statistics estimated from entire test sequences, and SID diversity follows the same clustering configuration as in the DualTalk benchmark for comparability. These metrics provide complementary views of realism, synchrony, and diversity in generated 3D conversational motion. 9. Additional Results 9.1. Comparison with Other Baselines Table 4 presents the comparison with existing methods, where all metrics are computed following the official DualTalk benchmark protocol on both the test and out-ofdistribution (OOD) datasets. Since TIMAR operates on fixed-length turn segments, the total length of its generated sequences is slightly shorter than those from DualTalk. To maintain comparable evaluation, the remaining frames are padded with the final predicted 3D head parameters, ensuring aligned sequence lengths and consistent metric computation. Across nearly all metrics, TIMAR achieves the best or second-best results, showing improved motion realism, synchronization, and expressiveness. These results confirm that the proposed turn-level causal modeling provides more general and principled framework for interactive 3D conversational head generation. 9.2. Diffusion Head Scalability Study Table 5 investigates the scalability of the diffusion head by varying its hidden dimension dm and the number of residual blocks K. Results on the test dataset reveal consistent trend of performance enhancement as model capacity increases, suggesting that the diffusion-based formulation can effectively leverage additional depth and width when larger computational budgets are available. V"
        }
    ],
    "affiliations": [
        "Anhui Polytechnic University",
        "Hefei University of Technology",
        "IAI, Hefei Comprehensive National Science Center",
        "Northwestern Polytechnical University",
        "SJTU",
        "TeleAI, China Telecom",
        "USTC",
        "United Arab Emirates University"
    ]
}