{
    "paper_title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
    "authors": [
        "Jialuo Li",
        "Bin Li",
        "Jiahao Li",
        "Yan Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256."
        },
        {
            "title": "Start",
            "content": "DIvide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding Jialuo Li1,2,* Bin Li 2 1Tsinghua University Jiahao Li 2 Yan Lu 2 2Microsoft Research Asia HTTPS://GITHUB.COM/JIALUO-LI/DIG 5 2 0 D 3 ] . [ 1 0 0 0 4 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate queryaware selection for optimal performance. Building on this insight, we propose DIG, training-free frame selection framework that adapts its strategy based on the query type. Specifically, DIG employs efficient uniform sampling for global queries while activating specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256. 1. Introduction In recent years, there has been rapid advancement in large multimodal models (LMMs) [9, 23, 29, 43, 52] for openworld visual understanding. natural and increasingly important direction within this field is the extension of these models to handle video data, thereby enabling them to perform complex video understanding tasks [10, 19, 26, 28, 32, 40, 68, 69, 75]. The common approach [3, 70] involves representing videos as sequences of individual frames, where visual features are extracted from each frame and concatenated to form video representation that is subsequently processed by the large language model (LLM). However, due to the limited context length of the LLM and the sheer *Work done during Jialuos internship at MSRA. Figure 1. For global queries requiring holistic understanding (bottom), uniform sampling is both effective and efficient. Conversely, for localized queries targeting specific temporal segments (top), query-based sampling is necessary to ensure high performance. volume of video tokens, it is impractical to input all frames directly. As result, only sampled subset of frames is typically used as input. The predominant method is uniform sampling which selects frames at fixed intervals. While this maximizes temporal coverage, it is query-agnostic, often selecting redundant frames while omitting crucial, queryrelevant moments. To address this limitation, recent work has introduced query-aware adaptive frame selection mechanisms [31, 48, 49, 64, 67]. These methods identify and utilize the most representative frames input based on the query, but at the cost of significant computational overhead for searching within the video. This high cost motivates critical question that is frequently overlooked: Is such complex search mechanism strictly necessary for all query types? Our findings indicate that the answer is negative. We first identify the existence of two distinct query categories: global query, which requires holistic video understanding, and localized query, which targets specific temporal segments. We observe significant performance disparity in uniform sampling between these categories. As the number of sampled frames increases, performance on localized queries degrades substantially, as irrelevant frames are injected into the context. Conversely, performance on global queries remains stable. 1 This finding validates our query typology. Based on this, our further experiments demonstrate that for global queries, uniform sampling already achieves robust performance. In such cases, deploying more complex selection methods is often inefficient and yields diminishing returns. Conversely, it is for localized queries that advanced, query-aware selection mechanisms are truly impactful, delivering substantial performance gains where uniform sampling fails. Building on these findings, we propose DIG, trainingfree frame selection framework for LMM that adapts its strategy based on query type. The framework first employs an LLM to classify given query as either global or localized. For global queries, standard uniform sampling is employed. For localized queries, multi-stage pipeline is initiated. This pipeline begins with our proposed ContentAdaptive Frame Selection, method that leverages pairwise frame similarity based on DINO features [36] to select set of semantically representative r-frames. Subsequently, the LMM itself is utilized to score these r-frames, assigning relevance reward based on their estimated utility in answering the query. Guided by this reward distribution, video refinement process identifies and merges the most relevant video segments into condensed representation. Finally, this refined video is uniformly sampled to get the input frames for the LMM, ensuring that inference is concentrated on the most pertinent temporal segments. Our main contribution are summarized as follows: We identify query typology (global vs. localized) and demonstrate that the efficacy of frame selection strategies is highly contingent on this classification. We propose DIG, training-free frame selection framework that adapts to query type by employing uniform sampling for global queries and specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks shows that DIG consistently outperforms existing baselines and robustly improves LMMs performance, even when scaling the input frame count to 256. 2. Related Work 2.1. Video-based Large Multimodal Models The rise of Transformer-based large language models (LLMs) has revolutionized natural language processing, with major advances stemming from increased model scale and larger pre-training datasets [4, 11, 12, 14, 35, 37, 53, 73]. Inspired by this success, researchers have begun adapting LLMs to process multiple modalities, particularly integrating visual elements like images and videos [23, 29, 30, 79], leading to the development of LMMs. Through extensive training, these models learn rich, cross-modal representations that effectively connect visual and textual information. This evolution has led to significant improvements across range of video understanding applications, including tasks such as video captioning [5, 7, 59, 61, 62] and video question answering [10, 21, 27, 32, 33, 77]. Ongoing research is also focusing on refining model architectures [44, 45, 60, 81] and optimizing training strategies [30, 81] to further boost the performance of these multimodal systems. Despite their success, LMMs still struggle in video understanding due to the high volume of video tokens and the limited context length of LLMs [46, 66], as well as the Needle-in-a-Haystack issue [25, 64, 76]. These challenges highlight the need for efficient frame selection techniques that capture key visual content without overloading the model. 2.2. Video Token Reduction for VQA Task In VQA task, uniform frame sampling is standard technique for video token reduction. However, this method overlooks the query-specific relevance of individual frames. To address this limitation, recent research has focused on adaptive token reduction mechanisms, which are broadly classified into two primary categories. Token Compression. This strategy consolidates information within or across video frames to create more compact yet informative representation, reducing the total tokens needed [20]. Various techniques are employed to achieve this, such as using memory bank [47], reducing temporal redundancy [41, 54], and applying hierarchical compression [25]. Despite their efficiency, token compression techniques may lead to excessive summarization, resulting in the loss of fine-grained visual details. Moreover, queryrelated segments may be either compressed or overly generalized, potentially compromising the models capacity to effectively respond to the given query. Query-Based Frame Selection. Compared with uniform sampling, recent methods employ more refined strategies to select query-relevant frames that typically involve three key steps: (1) uniformly sample candidate frames [15, 31, 48, 49, 55, 56, 63] or video segments [1, 17]; (2) assess their relevance to the query using metrics like CLIPScore [18, 74], detector [65] or learned models [67]; (3) apply an algorithm to select the most relevant frames based on these scores. However, uniform sampling often balances poorly between information sparsity and computational load. Furthermore, relevance metrics like CLIPScore [18] can be notoriously unreliable for complex reasoning, and the resulting temporally sparse frames may miss fine-grained details found in continuous clips. We address these limitations by: (1) using content-adaptive frame selection to identify frame candidates much more intelligently; (2) employing inherently more reliable LMMs for relevance assessment; and (3) retrieving and concatenating continuous clips corresponding to candidates before performing final frame selection, ensuring fine-grained information is preserved. 2 Figure 2. Performance trends of various LMMs with respect to input frame counts. We evaluate Qwen2.5-VL-7B [3], InternVL38B [9], and LLaVA-OneVision-7B [23] on MLVU [78], LongVideoBench [58], and VideoMME [16]. Results indicate that accuracy peaks at an optimal frame count and subsequently degrades, rather than improving monotonically. Figure 3. Relative accuracy on localized and global queries. Plotted as the deviation from the initial baseline, the results demonstrate that performance degradation at high frame counts is predominantly attributed to LQ, while GQ remains relatively stable. 3. Revisiting Inference Mechanism of LMM in"
        },
        {
            "title": "Video Understanding",
            "content": "Consider video with frames, denoted as {fi}T i=1, along with query Q. In VQA task, the model receives with the video and the query as inputs and is tasked with generating response that accurately addresses the query. In contemporary approaches, due to computational limitations and the language models restricted context length L, only subset of uniformly sampled frames, denoted as {f i=1, is processed, where . These selected frames are then combined with the query and fed into the LMM, which autoregressively generates the answer A: }N = LMM([f 1; 2; . . . ; ; Q]). (1) Obviously, small subset of frames is often insufficient to capture the full content of video, particularly in longer sequences. To address this, recent studies [8, 72] have focused on extending model context lengths to allow more frames as input. However, this raises an important question: Does increasing the number of uniformly sampled input frames enhance performance on VQA task? More frames doesnt mean improved performance. To investigate this, we conducted an evaluation using three pretrained LMMs: Qwen2.5-VL-7B [3], InternVL38B [80], and LLaVA-OneVision-7B [23], across three long-form video understanding benchmarks: MLVU [78], VideoMME [16], and LongVideoBench [58]. We employed 3 uniform frame sampling with varying frame counts to evaluate the impact of frame count on model performance. As illustrated in Figure 2, consistent pattern emerges across all models and benchmarks: performance initially improves with more input frames but declines beyond certain point. Query classification. To better understand the performance degradation, we examined the impact of different query types. Prior studies [6, 31, 38, 42, 49] have identified class of queries that relate to specific, localized segments of video, such as What kind of bike is the man riding?, which we classify as localized queries (LQ). However, these works overlook another important category of queries requiring comprehensive understanding of the entire video. We define such queries as global queries (GQ), with an example being What title best summarizes this video? Performance trends vary across query types. Following the definition, we manually categorize queries from MLVU [78] and VideoMME [16], and evaluate the same models on these two query types. As shown in Figure 3, while performance on global queries remains relatively stable with increasing frame count, performance on localized queries drops significantly. We attribute this to global queries benefiting from holistic information, whereas excess frames introduce noise for localized tasks. These results highlight the necessity of pre-classifying query types to optimize efficiency; specifically, global queries can rely on standard uniform sampling, avoiding the computational overhead of key frame search techniques. Figure 4. Overview of DIG. The LLM first classifies the query type. Global queries utilize uniform sampling across the entire video, while localized queries employ CAFS and reward assignment to construct refined video prior to sampling. The selected frames are subsequently processed by the LMM for final inference. 4. Method: DIG Overview. In this section, we introduce DIG, trainingfree frame selection framework for LMMs that adapts to query type. DIG begins by classifying the query as localized or global (4.1). For global queries, final input frames are uniformly sampled across the entire video. In contrast, for localized queries, we first employ content-adaptive frame selection method to extract representative frames (4.2), which are then evaluated by the LMM through reward scoring to assess their relevance to the query (4.3). Then refined video is constructed through search procedure guided by these rewards (4.4) and final input frames are uniformly sampled from the refined video. 4.1. Query Type Identification As established in Section 3, the performance trends vary across query types. Therefore, we first employs LLM to classify given query as either global or localized (see Appendix for prompt details). For global queries, the LMM performs direct inference on uniformly sampled frames. Localized queries, in contrast, are addressed using the specialized approach detailed in the following. 4.2. Content-Adaptive Frame Selection (CAFS) To effectively address the localized query, it is essential to extract relevant frames from the video. However, exhaustive frame-wise analysis of long-form videos is computationally infeasible. This necessitates getting compact yet informative subset of frames. Previous methods typically rely on static sampling (e.g., uniform or fixed-rate) [31, 49, 65, 74]. This static approach presents dilemma: low-rate sampling may yield sparse representation that misses critical events, while high-rate sampling produces large and redundant frame set. To address this, we propose Content-Adaptive Frame Selection, method that adaptively selects representative frames, referred to as r-frame, based on the high-level semantic content in the video such as objects and scenes. Distance calculation. Given 2-fps sampled video with frames {fIi}M i=1 with corresponding frame indices {Ii}M i=1, we utilize DINOv2 [36] to extract visual features from each frame, which results in sequence of feature vectors {VIi}M i=1. To measure the dissimilarity between consecutive frames, we compute the feature distance di between fIi and fIi+1 using the following formula: di = 1 sim(VIi, VIi+1), (2) where sim(, ) denotes cosine similarity. This yields sequence of distances {di}M 1 i=1 . j=1 {Ii}M R-Frame selection. Due to frequent scene transitions or camera cuts in long videos, the pairwise frame similarity often exhibits abrupt changes, resulting in numerous peaks in the distance sequence. Specifically, di is identified as peak if di1 < di and di+1 < di. To reduce noise effects, only peaks with prominence greater than 0.1 are valid. This threshold has been found effective through empirical observation. We denote the indices of these valid peaks as {Kj}N i=1, where < . These peaks serve as segmentation points, dividing the video into distinct segments. Within each segment, the low pairwise distances between frames indicate visual consistency. Therefore, we select only one frame from each segment to capture its semantic content. For simplicity, we choose the midpoint frame of each segment, resulting in set of r-frames indexed by {I j=1 . By aggregating rframes, we obtain compact representation that effectively summarizes the essential visual content of the entire video. j=1 = {(Kj + Kj+1)/2}N 1 j}N 1 4.3. Reward Assignment To identify r-framess relevance to the given query Q, existing methods typically use either: (1) multimodal models like CLIPScore [31, 39, 49, 55], or (2) object detection models to localize query-related entities in frames [64]. However, these methods are often constrained by surfacelevel feature matching and reliance on fixed vocabularies, which limits their ability to capture contextual reasoning and broader world knowledge. To address this, we leverage the LMM itself to assess frame relevance by assigning reward scores, with simplified version of prompt below. Two-dimension scoring. Since many queries, particularly those involving why or how, cannot be fully addressed by single frame, evaluating the relevance of individual frames individually may lead to incomplete or biased assessments. To mitigate this, we design the LMM to consider two complementary factors: (1) the direct relevance of the current frame to the query, and (2) whether the content of the current frame indicates that adjacent frames may contain supplementary information that contributes to more comprehensive response. Reward Model Prompt (Simplified) Frame: < fi >; Query: < >; Please follow these steps to finish scoring: 1. Describe the sampled frame, focusing only on elements relevant to the question, if any. 2. Assign relevance score between 0 and 100 based on: (1) Direct usefulness of the frame for answering the query. (2) Whether it suggests adjacent frames may contain relevant context. 4.4. Video Refinement j}N j=1, the r-frame indices {I Building upon the preceding steps, we have obtained the set of peak indices {Kj}N j=1 , and the reward values {Rj}N 1 j=1 assigned to these r-frames. The next step is to select the most query-relevant r-frames based on the reward values {Rj}N 1 j=1 . Iterative reward-guided selection. to the commonly employed Top-K selection, which applies fixed hyperparameter across varying scenarios, we introduce parameter-free methodology. Given the initial rewards {Rj}N 1 j=1 , we iteratively refine this set until it stabilizes. Step 1. Compute the mean of the current reward set: R. Step 2. Update each reward value by thresholding below"
        },
        {
            "title": "In contrast",
            "content": "the mean value: = max(Rj R, 0), = 1, . . . , 1. (3) Step 3. Let be the set of indices {j j > 0}. Compare with the set of positive indices from the previous iteration. If is unchanged, terminate the iteration. Otherwise, update the reward set {Rj} {R j} and repeat from Step 1. Upon termination, the selected r-frames, denoted by If , are formally defined as those r-frames whose corresponding reward values in the final iteration are positive: If = {I j=1 . This criterion ensures that all r-frames in the final selection set possess reward larger than average. > 0}N 1 5 Segment combination. Since r-frames exhibit high feature similarity with their adjacent frames, it indicates an opportunity to incorporate fine-grained information beyond simply using them as input to the LMM. Specifically, for each selected r-frame indexed by j, we consider the video segment in the interval [Kj, Kj+1] for richer temporal details. To capture more relevant context, we also consider adjacent r-frames within window of length wlen, specifically those with index range from j+wlen. This results in the video segment spanning the index range [Kjwlen, Kj+wlen+1]. Then we combine the corresponding video segments of all selected r-frames via union operation, resulting in refined video containing query-relevant and fine-grained content. Finally, we uniformly sample frames from this refined video as input to the LMM. jwlen to 5. Experiment 5.1. Experiment Settings Datasets. We evaluate our approach on three benchmarks: MLVU [78], LongVideoBench (LVB) [58], and VideoMME [16], which contain videos ranging from several minutes to multiple hours, allowing us to assess longform video understanding. For VideoMME [16], we focus only on the medium and long splits. We dont use subtitles, ensuring evaluation is based on visual understanding. Further benchmark details are provided in Appendix A. Implementation details. The LMMs used in our experiments are Qwen2.5-VL-7B[3] and Qwen2.5-VL-32B [3]. The LLM used for query identification is Qwen3-Next-80BA3B [51]. Each input frame is represented using 56 tokens. The hyperparameter wlen is set to 2. All experiments are conducted on 8 A100 GPUs within LMMs-Eval [71] framework. Additionally, we utilize vLLM backend [22] to accelerate inference during the query identification and reward assignment stages. As baselines, we choose AKS [49] and Q-Frame [74], and uniform sampling (UNI). Detailed baseline configurations and extended experiments on Qwen3VL-8B [2] are available in Appendix F. 5.2. Main Results Comparison with existing methods. As shown in Table 1, compared with uniform sampling and competitive baselines including Q-Frame [74] and AKS [49], DIG consistently improves performance on both Qwen2.5-VL32B [3] and Qwen2.5-VL-7B [3] across input frame numbers from 8 to 256. Notably, with 32 frames, DIG significantly boosts the accuracy of Qwen2.5-VL-7B [3] by 7.68% on MLVU [78] and 4.51% on LongVideoBench [58] compared to uniform sampling. This superiority extends to the more powerful Qwen2.5-VL-32B [3], where DIG achieves better performance across almost all reported settings, effectively enhancing even strong base model where other methods struggle to show consistent gains. Table 1. Performance comparison between different frame selection methods. Base LMMs are Qwen2.5-VL-32B [3] (left) and Qwen2.5-VL-7B [3] (right). Bold indicates best performance, while Red Box denote results inferior to uniform sampling."
        },
        {
            "title": "Method",
            "content": "#Frames MLVU [78] LVB [58] VideoMME [16]"
        },
        {
            "title": "Medium Long",
            "content": "UNI Q-Frame [74] DIG (Ours) UNI Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] DIG (Ours) 8 8 8 16 16 16 32 32 32 32 64 64 64 64 128 128 128 128 192 192 55.93 56.03 61.55 58.79 57.73 66.21 61.91 66.42 60.95 70.69 66.24 69.41 66.05 74.19 70.24 72.77 70.10 75.20 71.76 73.46 76. 53.40 53.78 56.77 54.67 56.62 58.86 57.89 59.31 57.37 61.86 59.01 61.41 59.61 63.65 61.78 62.00 60.06 65.60 63.80 62.45 66. 53.89 54.03 54.12 55.44 55.09 58.62 57.89 59.89 60.43 60.87 64.33 64.67 62.80 66.24 68.89 68.33 68.21 69.00 69.56 69.89 70. 51.56 49.63 51.21 53.33 51.11 52.18 53.33 56.00 55.90 57.76 55.67 58.44 57.72 58.19 59.67 61.44 59.28 62.29 62.00 61.00 63."
        },
        {
            "title": "Method",
            "content": "#Frames MLVU [78] LVB [58] VideoMME [16]"
        },
        {
            "title": "Medium Long",
            "content": "UNI Q-Frame [74] DIG (Ours) UNI Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] DIG (Ours) UNI AKS [49] DIG (Ours) 8 8 8 16 16 16 32 32 32 32 64 64 64 64 128 128 128 192 192 192 256 256 256 53.64 54.42 58.64 56.43 56.81 63.98 59.52 65.07 60.03 67.20 63.61 66.59 63.43 70. 67.31 68.68 68.03 71.40 69.03 69.93 72.32 69.15 71.50 72.46 51.23 54.23 55.20 54.45 57.37 57.89 56.92 59.31 56.39 60. 58.94 60.66 57.52 61.41 61.86 60.36 59.76 63.13 61.93 61.26 64.32 61.48 61.03 64.62 51.36 50.81 54.23 55.94 53.78 56. 59.08 59.22 56.64 61.62 61.01 62.94 61.32 62.61 65.89 65.67 65.91 66.78 67.01 68.22 68.00 66.31 67.56 67.66 45.84 49.21 46. 48.12 49.02 51.93 52.02 53.11 51.57 53.24 51.27 53.44 53.70 55.30 54.84 55.93 54.81 55.69 55.82 54.41 58.24 57.12 55.11 57. Figure 5. Comparison of our proposed frame selection pipeline (Sections 4.24.4) versus uniform sampling across different query types. The base LMMs are Qwen2.5-VL-7B [3] and Qwen2.5-VL-32B [3]. Scalability and performance consistency. In wellresourced environments, performance analysis at minimal frame counts (e.g., 8 or 16) offers limited practical insight, as applications typically seek to maximize frame utilization within given constraints. Therefore, unlike most of previous works [31, 48, 49, 67, 74] that validates performance in low-frame regimes (< 64 frames), we conduct evaluation that scales inputs to high frame densities (e.g., 256 frames). Under these conditions, as detailed in Table 1, AKS [49] and Q-frame [74] can exhibit performance degradation relative to uniform sampling as frame counts increase. For instance, when utilizing the Qwen2.5VL-7B [3] with 128 input frames, both AKS [49] and Qframe [74] underperformed uniform sampling by 12% on LongVideoBench [58]. In contrast, DIG demonstrates consistent performance gains over uniform sampling across all tested LMMs and most input frame configurations. 6. Discussion and Analysis In this section, we present detailed analysis of DIG. Our evaluation is structured around the following key questions: How does the choice of frame selection strategy impact performance on global versus localized queries? (6.1) How effective is the CAFS module at selecting representative frames, and what is its contribution to the overall performance of DIG? (6.2) How does the LMM-based reward model compare against the CLIPScore [18] in reward assignment? (6.3) What is the influence of the temporal window length (wlen) on model performance? (6.4) What is the computational efficiency of DIG? (6.5) 6.1. Frame Selection Strategy vs. Query Type To examine the impact of different frame selection strategies on global versus localized queries, we leverage the Figure 6. GlC and LoC score across varying video durations. We compare three sampling strategies: FPS, UNI, and CAFS. In each sub-figure, the lines represent the score (left y-axis), while the bars indicate the number of sampled frames (right y-axis). Figure 7. Performance Comparison of CAFS and uniform sampling in DIG pipeline. The base LMM is Qwen2.5-VL-7B [3]. query classifications established in Section 3 and then compare the performance of uniform sampling against our proposed frame selection pipeline on each query type. surrounding temporal window. The LoC score is then computed as the average similarity between the r-frame and its sampled neighbors across all r-frames. Efficacy of uniform sampling on GQ. As illustrated in the right two charts of Figure 5, uniform sampling achieves performance comparable to, or occasionally superior to, our pipeline on GQs. This suggests that global queries necessitate comprehensive and diverse information from the video content, which uniform sampling inherently provides. Superiority of keyframe selection on LQ. For LQs, our pipeline consistently outperforms uniform sampling, as shown in the left three charts of Figure 5. This result demonstrates our methods effectiveness in accurately identifying and extracting the specific video segments relevant to localized inquiries. These findings underscore the imidentifying portance of query-aware sampling strategy: the query type is essential to determine whether to employ broad sampling for global context or targeted extraction for specific details. 6.2. Analysis of CAFS Effectiveness Let fj denote the frame indexed by j, and let Vj represent its feature vector obtained via DINOv2 [36]. We define the set of r-frames as {fIi}N i=1. To assess their effectiveness in capturing the high-level semantic content within video, we introduce two quantitative metrics. i=1 with indices {Ii}N Localized Coverage (LoC). This metric assesses the effectiveness with which each r-frame captures its local temporal visual context. More specifically, for each r-frame fIi, four neighboring frames are sampled uniformly from its LoC = 1 4N (cid:88) 3 (cid:88) sim (cid:0)VIi, VMi,j (cid:1) , j=0 where Mi,j = Ii + (j 1.5) (Ii+1 Ii1)/6 i= (4) Global Coverage (GlC). This metric evaluates how well the r-frames collectively represent the entire video content. Ideally, each frame in the video should be similar to at least one r-frame. To compute it, we randomly sample 200 frames from the video, denoted as {fx}xX . For each frame fx, we find the maximum similarity to any r-frame and average these values across all sampled frames: GlC = 1 (cid:88) xX max i[1,N ] sim(VIi, Vx) (5) Baseline selection. We evaluate CAFS against two standard baselines: UNI (uniform frame sampling) and FPS (uniform frames-per-second sampling). The assessment is conducted on MLVU [78] and VideoMME [16]. To ensure fair comparison, the average number of selected frames is kept consistent across all methods for each dataset. Analysis. As shown in Figure 6, the performance of uniform sampling declines with increasing video duration. This limitation arises from using fixed number of frames across videos of varying lengths, which leads to redundancy in short videos and inadequate semantic coverage in long Figure 8. Performance comparion of different window lengths (wlen) in DIG pipeline. The base LMM is Qwen2.5-VL-32B [3]. videos. Moreover, while fps sampling maintains stable performance, CAFS consistently outperforms it, particularly for videos over 10 minutes. This indicates that semantic information in videos doesnt grow linearly with length, and that CAFS is more effective at selecting informative frames. Comparison with uniform sampling in DIG. We compare CAFS with uniform sampling within the DIG pipeline by replacing CAFS-extracted r-frames with uniformly sampled ones. As shown in Figure 7, CAFS consistently outperforms uniform sampling across all benchmarks. In addition, The performance gap widens with more input frames, further highlighting the limitation of uniform sampling: for long videos it cant sample sufficient frames to well cover information for subsequent process, while CAFS can adapt for any length of videos and ensures better coverage. 6.3. Reward Assignment: LMM vs. CLIPScore We evaluate the reward assignment mechanism employed by the LMMs in DIG by comparing it to common alternative: computing frame-query similarity using CLIP [39]. Specifically, we substitute all reward values originally assigned by the LMM with corresponding CLIPScore [18]. LMMs exhibit superior capability as reward assigners. As illustrated in Table 2, the rewards generated by LMMs (Qwen2.5-VL-7B/32B [3]) demonstrate superior performance across the benchmarks in most cases, particularly as the number of frames increases. This underscores the LMMs capacity to deliver more precise and semantically rich reward signals through its advanced reasoning abilities and broad world knowledge. In contrast, CLIPScore [18] depends on superficial feature matching and often fails to capture nuanced or visually complex query requirements. Better LMMs yield superior rewards. The results in Table 2 also indicate that employing Qwen2.5-VL-32B [3] as the reward assigner outperforms the 7B variant, even on short-video benchmark like VideoMME-short [16]. This confirms that advanced LMMs provide more precise reward signals, thereby facilitating more accurate identification of query-relevant frames. Furthermore, this highlights the flexibility of our framework: we can effectively decouple the reward mechanism from the inference backbone. By leveraging separate, reasoning-intensive Image-LMM for ] 6 1 [ d - 51.1 54.2 52.6 55.9 56.8 57.6 58.6 61.6 61. 62.4 62.6 64.8 64.0 66.8 69.2 63.9 68.0 69.2 64.7 67.7 68.9 ] 6 1 [ h - M 62.3 63.6 64.2 67.2 67.8 68.1 70.0 70.3 72.6 72.7 73.3 74.4 73.3 74.9 75.4 74.6 75.9 76. 75.0 76.3 76.8 ] 6 1 [ l - 49.0 46.9 47.2 49.4 51.9 50.2 51.2 53.2 53. 54.7 55.3 54.7 55.8 55.7 57.1 54.8 58.2 57.4 57.0 57.8 59.1 ] 8 7 ["
        },
        {
            "title": "U\nV\nL\nM",
            "content": "57.4 58.6 60.6 62.2 64.0 64.0 65.4 67.2 67.9 67.2 70.7 71.0 69.6 71.4 72.6 71.0 72.3 73. 71.2 72.5 74.3 ] 8 5 [ 52.6 55.2 55.6 54.3 57.9 59.2 56.2 60.4 60. 59.6 61.4 63.4 61.0 63.1 65.2 62.5 64.3 65.4 61.9 64.6 64.5 a # 8 8 16 16 16 32 32 32 64 64 64 128 128 128 192 192 192 256"
        },
        {
            "title": "Reward Model",
            "content": "CLIPScore [18] Qwen2.5-VL-7B [3] Qwen2.5-VL-32B [3] CLIPScore [18] Qwen2.5-VL-7B [3] Qwen2.5-VL-32B [3] CLIPScore [18] Qwen2.5-VL-7B [3] Qwen2.5-VL-32B [3] CLIPScore [18] Qwen2.5-VL-7B [3] Qwen2.5-VL-32B [3] CLIPScore [18] Qwen2.5-VL-7B [3] Qwen2.5-VL-32B [3] CLIPScore [18] Qwen2.5-VL-7B [3] Qwen2.5-VL-32B [3] CLIPScore [18] Qwen2.5-VL-7B [3] Qwen2.5-VL-32B [3] Table 2. Performance comparison with rewards from Qwen2.5VL-32B [3], Qwen2.5-VL-7B [3] and CLIPScore [18] across various benchmarks. Bold indicates best performance. The base LMM used is Qwen2.5-VL-7B [3]. frame selection, we can significantly enhance the final performance of the primary Video-LMM. 6.4. Impact of Window Length To investigate how different values of wlen affect performance, we conduct an evaluation using settings of wlen {0, 2, 4, 8}, while keeping all other settings identical. Comparison with different window length. As shown in Figure 8, setting wlen = 0 yields the lowest performance across all benchmarks. This deficit is particularly pronounced on LongVideoBench [58], which necessitates reasoning over extended temporal contexts. This indicates 8 that DIG consistently outperforms baselines and robustly scales LMM performance for inputs from 8 to 256 frames. Figure 9. Comparison between Accuracy and FLOPs. The base LMM used is Qwen2.5-VL-7B [3]. that most queries cannot be effectively resolved within only single scene, but instead require information from the surrounding temporal context. However, performance does not monotonically improve with wlen. When wlen is set to high value, such as 8, performance degrades compared to wlen = 2 and 4. This proves that an excessively large window introduces irrelevant contextual information, creating noise that is detrimental to localized queries. Therefore, wlen = 2 appears to strike the optimal balance, achieving the best results across the benchmarks. 6.5. Efficiency of DIG To evaluate computational cost, we measure and compare the FLOPs of our DIG pipeline against uniform sampling on LongVideoBench [58]. The reported FLOPs represent the average computation required per QA pair. Performance-Efficiency analysis. As demonstrated in Figure 9, uniform sampling approach exhibits clear performance bottleneck. As the number of input frames scale, its accuracy saturates at peak of 62.5%. Further increases in computation and frame count do not yield better performance. In contrast, DIG successfully overcomes this limitation. While operating at higher computational budget ( 680 TFLOPs), DIG demonstrates positive performance scaling, surpassing the uniform samplings peak accuracy once computation exceeds 720 TFLOPs and continuing to improve thereafter. 7. Conclusion In this work, we find that optimal frame selection in video understanding depends on the query type (global vs. localized). Based on this, we propose DIG, training-free framework that adapts to this typology: it employs efficient uniform sampling for global queries while reserving multi-stage pipeline for localized queries where targeted selection is essential. This dual approach ensures both high performance and efficiency. Extensive experiments across diverse long-form video benchmarks and LMMs validate"
        },
        {
            "title": "Appendix",
            "content": "B. Query Identification by Human Annotator The appendix is structured as follows: The benchmark details in Section A. The detailed manual query identification in Section B. The prompt design of DIG in Section C. More details about query identification in Section D. More details about CAFS in Section E. More details about experiments in Section F. More efficiency analysis in Section G. A. Benchmark Details This section details the benchmarks used in our evaluation. statistical overview of each dataset is provided in Table 3. MLVU. MLVU [78] is multi-task benchmark for long video understanding, comprising 3,102 questions across 9 categories. The dataset is partitioned into dev set (2,593 questions) and test set (509 questions). Tasks are categorized into three primary types: 1) holistic analysis, 2) single-detail identification, and 3) multi-detail reasoning. For our evaluation, we utilize only multiple-choice questions from the dev set and exclude open-ended questions. LongVideoBench. LongVideoBench [58] is questionanswering benchmark featuring 3,763 web-collected videos and 6,678 human-annotated, multiple-choice questions spanning 17 fine-grained categories. The benchmark is designed to test referring reasoning by requiring models to retrieve and reason over detailed information. In our study, we utilize only the validation set of this benchmark. VideoMME. VideoMME [16] is multi-modal benchmark covering 30 subdomains across 6 primary visual domains. It contains 900 videos, totaling approximately 254 hours, and 2,700 question-answer pairs. The dataset includes multiple modalities (e.g., video, subtitles, audio) and splits videos by duration (short, medium, long). To focus our evaluation on long-form video understanding, we use only the medium and long duration splits. Furthermore, we leverage only the video data and corresponding questions, excluding all other modalities like subtitles. Table 3. Dataset Statistics. Overview of the data statistics across LongVideoBench [58], MLVU [78] and VideoMME [16]."
        },
        {
            "title": "Dataset",
            "content": "Avg. Duration (s) #QA Pairs MLVU [78] LongVideoBench-val [58] VideoMME-short [16] VideoMME-medium [16] VideoMME-long [16] 636.2 732.2 80.7 516.8 2466.3 2174 1337 900 900 900 In this section, we elaborate on the query identification process described in Section 3, detailing the methodology used to classify queries from each benchmark. MLVU. The task structure of MLVU [78] maps directly to our proposed query definitions. Queries associated with its holistic tasks, which necessitate comprehensive understanding of the entire videos overarching narrative, themes or summary of its content, are classified as global queries. Conversely, queries within its single-detail and multidetail task categories, which inherently demand that the model focus on specific, discrete temporal segments or isolated events, are classified as localized queries. Applying this classification scheme, we identified 462 global queries and 1708 localized queries within MLVU [78]. LongVideoBench. The design of LongVideoBench [58] is centered on referring reasoning. This evaluation paradigm is explicitly designed to test models capacity to ground its reasoning in specific, fine-grained visual information. By their very nature, such queries require pinpointing information within distinct temporal or spatial segments rather than assessing the video as whole. Consequently, all queries within this benchmark correspond directly to our definition of localized queries. VideoMME. VideoMME [16] lacks an intrinsic task classification that aligns with our global-versus-localized classification. To address this gap, we implemented rigorous manual annotation process. We established standardized protocol wherein human annotators were provided with detailed instructions and precise criteria (as illustrated in Figure 10) to distinguish between the two query types. To ensure the reliability of these labels and mitigate subjective bias, the final classification for each query was determined by majority vote consensus. This meticulous annotation procedure resulted in the identification of 479 global queries and 2, 221 localized queries. C. Prompt Design Prompt engineering is cornerstone of harnessing the sophisticated reasoning capabilities of LLMs and LMMs. For our DIG framework, we designed series of specialized prompts to guide the models through our multi-stage video question-answering pipeline. This section details the design and rationale for the three core prompts: (1) Query Identification, (2) Reward Assignment, and (3) Direct Inference. Query identification. The initial and most critical step in our framework is to determine the type of the users query. This classification dictates the subsequent processing strategy. As illustrated in Figure 10, the prompt leverages Chain-of-Thought (CoT) strategy [57] to deconstruct the"
        },
        {
            "title": "Query Identification Prompt",
            "content": "You are helpful assistant in video-based question-answering process. Core Task & Definitions You will classify the given query into one of two categories: 1. Global Query (isGlobal: true): The query requires going through and understanding the entire video content. 2. Localized Query (isGlobal: false): The query that can be fully answered by extracting and analyzing several specific segments within the video. Instructions for Analysis and Response In your analysis, please follow this structured reasoning process to classify the query: Step 1. Understand the Query: First, read the query to understand its general meaning and core intent. Step 2. Infer Video Style (Hypothetically): Based on the querys phrasing, make reasonable inference about the style of the video (e.g., is it narrative film, an educational lesson, documentary, etc.)? Step 3. Identify Referents: Analyze if the query has specific referents. referent is an entity (person, object), action, event, or even specific piece of information, depending on the type of video you inferred. For instance, in What does Professor Smith write about quantum physics?, the referent is Professor Smith and quantum physics since the video style is likely lesson. Step 4. Evaluate Referents in Context: Based on the results from step 3 and the criteria below, determine whether the query is Global or Localized. (i) The query is Global if it meets either condition: 1. Lacks specific referent. The examples include: Summary-based: primary focus, in summary, what is the video about? 2. Has referent, but answering still requires holistic understanding from going through the entire video. The examples include: what is the boys overall role? (ii) The query is Localized if it has specific referents, and the answer can be found by focusing on specific, related segments where it appears. Here are some examples: Entity-based: the person in the red shirt, the black dog, Professor Smith, the little girl. Action/Event-based: what is [X] doing, how does [X] build, Temporal/Sequential: at the beginning, after the explosion, Please provide your answer in the following format: {\"analysis step1\": str, \"analysis step3\": str, \"analysis step4\": str, \"analysis step2\": str, \"isGlobal\": true/false} User Query: <Question> Figure 10. Query Identification Prompt. The LLM is first provided with the task definition, followed by an application of the chain-ofthought [57] technique to arrive at judgment. classification task into series of explicit, verifiable reasoning steps. The model is instructed to first analyze the querys intent, then hypothesize the videos genre (e.g., narrative, instructional), identify specific referents (entities, actions, or concepts), and finally synthesize this information to classify the query as either global or localized. This structured approach ensures robust and transparent classification. Reward assignment. To generate fine-grained feedback for optimizing our video refinement process, we utilize an LMM to assign relevance scores to sampled frames. The prompt, shown in Figure 11, presents the LMM with the users question, specific video frame, and associated metadata (video duration and frame timestamp). The model is tasked with two-part CoT process: first, to provide qualitative description of the frames content, focusing on elements pertinent to the query, and second, to assign quantitative reward score from 0 to 100. The reward criteria are carefully defined to capture not only the frames direct usefulness but also its contextual value, that is, whether the frame suggests that temporally adjacent segments contain the necessary information. Direct inference. For final evaluation, we use direct inference prompt, exemplified in Figure 12. This prompt is designed for standard multiple-choice question-answering format. It presents the LMM with the question and set of candidate options (A, B, C, D). Additionally, the prompt instructs the model to return only the letter corresponding to the best answer."
        },
        {
            "title": "Reward Assignment Prompt",
            "content": "You are reward model for video-based question-answering system. Task You will receive question and sampled video frame. Your task is to evaluate the relevance of this frame for answering the question. Please assign reward score that indicates how useful or informative the provided frame is in the context of the given question. Instructions for Analysis and Response In your analysis, please perform the following steps to finish your evaluation: 1. Describe the visual content of the sampled frame, focusing on elements relevant to the question, if such elements are present. 2. Assign relevance reward between 0 and 100 based on: (1) The sampled frames direct usefulness in answering the question (2) Whether the frame suggests that adjacent frames might provide additional information that help answer the question more effectively. Please provide your answer in the following format: {\"description\": str, \"reward\": int}. User Input Video Duration: <Duration> seconds; Sampled Frame Timestamp: <Timestamp> seconds; Question: <Question> Figure 11. Reward Assignment Prompt. The LMM is first presented with the task definition and associated metadata. Then, the chainof-thought reasoning technique [57] is applied to assign the reward for the input frame."
        },
        {
            "title": "Inference Prompt",
            "content": "Question: What is the video mainly about? A. Planes invented by the Wright Brothers. B. The structural difference between the planes created by Whitehead and planes created by the Wright Brothers. C. Who invented the first plane. D. How Whitehead and the Wright Brothers cooperated to invent the first motorized flight. Please select the best answer from the options provided and directly provide the letter representing your choice without giving any explanation. Figure 12. Prompt Template Example. Example of the prompt template used by LMMs to perform direct inference. D. More Details about Query Identification In this section, we evaluate the capability of contemporary LLMs to distinguish between global and localized queries. We assess the alignment between LLM predictions and human annotations by computing classification accuracy across three benchmarks: MLVU [78], LongVideoBench [58], and VideoMME [16]. The ground truth labels for these query types are derived from human annotations, as detailed in Section B. LLMs exhibit strong alignment with human annotation. As presented in Table 4, nearly all evaluated LLMs achieve an overall classification accuracy exceeding 80%. This indicates that off-the-shelf LLMs possess sufficiently robust reasoning capabilities to effectively differentiate between localized and global queries without extensive fine-tuning when given proper prompt. Localized queries are more readily identifiable. Table 4 further reveals that accuracy on localized queries consistently surpasses that of global queries. While GQ accuracy is comparatively lower, this has negligible impact on final model performance; it primarily incurs minor computational overhead. This because as established previously, performance differences between query-aware frame selection and uniform sampling are minimal for global queries. In addition, the critical metric is LQ accuracy that may influence the final performance. On this metric, almost all LLMs achieve an accuracy greater than 90%, ensuring the final performance is good. And to make tradeoff between compute cost and final model performance, we choose to use Qwen3-Next-80B-A3B-Instruct [51] in our main experiments. Table 4. Accuracy (%) of different LLMs in identifying localized queries (LQ) and global queries (GQ) across multiple benchmarks."
        },
        {
            "title": "LLM",
            "content": "MLVU [78] LongVideoBench [58] VideoMME [16] LQ GQ"
        },
        {
            "title": "Overall",
            "content": "LQ GQ"
        },
        {
            "title": "Overall",
            "content": "LQ GQ"
        },
        {
            "title": "Overall",
            "content": "Qwen3-Next-80B-A3B-Instruct [51] Llama-3.1-8B-Instruct [50] GPT-OSS-20B [34] DeepSeek-R1-Distill-Qwen-32B [13] 87.02 93.65 82.00 93.03 38.26 24.01 74.93 26.38 78.52 81.50 80.77 81."
        },
        {
            "title": "97.53 N/A\n98.20 N/A\n93.04 N/A\n99.18 N/A",
            "content": "97.53 98.20 93.04 99.18 89.13 96.99 89.20 97.21 65.76 34.24 69.97 52.85 83.90 82.95 84.90 87.28 Figure 13. Correlation between video duration and the number of r-frames selected by the CAFS method across different benchmarks. E. More Details about CAFS This section provides detailed examination of the CAFS method. Section E.1 formally specifies the algorithm of CAFS, while Section E.2 presents statistical analysis of its output characteristics based on practical application. E.1. Detailed Algorithm of CAFS Algorithm 1 details our CAFS method. The process is structured into three sequential stages, taking frame-to-frame distance sequence = [d1, . . . , dM 1] and their corresponding original frame indices = [I1, . . . , IM ] as input, to produce final set of r-frame indices, idx. Initial peak detection. First, we identify all potential content boundaries. It iterates through the distance sequence, identifying any point di that is local maximum, defined as being greater than its two immediate neighbors (di1 < di < di+1). The indices of all such local maxima are collected into an initial peaks set. Topographic prominence filtering. Second, we prune the peaks set, retaining only the most significant content transitions. For each peak peaks, it calculates its prominence by finding the lowest base levels to its left (lmin) and right (rmin). The prominence is then defined as the peaks height dj minus the higher of its two bases (prominence = dj max(lmin, rmin)). This metric quantifies how much peak stands out from the surrounding distance signal. Only peaks whose prominence exceeds threshold (e.g., 0.1) are added to the filtered peaks set, effectively discarding minor, localized fluctuations. R-Frame selection. Finally, we generate the output by identifying frames that best represent the stable content between these significant transitions. The algorithm iterates through consecutive pairs of prominent peaks (p1, p2) from the filtered set. For each pair, it calculates the temporal midpoint using their associated original frame indices from I: midpoint = (Ip1 + Ip2)/2. These midpoints, which correspond to the center of the most stable segments, are aggregated into the final idx set. E.2. More Results of CAFS To further analyze the performance of CAFS on specific examples, we conduct evaluation about the relationship between the number of r-frames and video duration. Non-Linear information scaling in videos. Figure 13 the r-frame count does not scale linearly reveals that with video duration. This non-linearity is prominent in LongVideoBench [58]: videos in the 0 10 minute bracket average 47.9 r-frames, whereas those in the 10 20 minute bracket average 226.4. This finding exposes fundamental limitation of fixed-rate sampling strategies (e.g., frames/video or frames/sec). Such approaches implicitly assume uniform information distribution, leading to suboptimal trade-off: sparse sampling risks information loss, while dense sampling incurs high temporal redundancy. CAFS bypasses this limitation by dynamically adapting its selection to the videos content density. High context compression efficiency. CAFS effectively into sparse, condenses prolonged video-level context salient set of r-frames. For instance, on MLVU [78], videos in the 10 20 minute bracket (12.7 min avg.) are reduced to just 180.8 r-frames on average. This represents sparse sampling interval of approximately one r-frame every 4.22 seconds, demonstrating CAFSs capability to efficiently distill essential information from extended video sequences. 13 Algorithm 1: Content-Adaptive Frame Selection Input: Distance sequence = [d1, d2, . . . , dM 1], Frame indices = [I1, I2, . . . , IM ] Output: Selected r-frame indices idx 1 peaks ; 2 for = 2 to 2 do // peak is point higher than its neighbors if di1 < di and di > di+1 then peaks.add(i); 3 4 // Calculate the topographic prominence for each peak 5 filtered peaks 6 for peaks do // Find the lowest point to the left of the peak lmin dj; 1; while 1 and dk dj do lmin min(lmin, dk); 1; // Find the lowest point to the right of the peak rmin dj; + 1; while 1 and dm dj do rmin min(rmin, dm); + 1; prominence dj max(lmin, rmin) if prominence > 0.1 then filtered peaks.add(j) 8 9 10 11 12 14 15 16 17 18 // Calculate midpoints between consecutive prominent peaks 19 peaks filtered peaks 20 idx 21 for = 0 to len(peaks) 2 do 22 p1 peaks[i]; p2 peaks[i + 1]; midpoint (Ip1 + Ip2 )/2; idx.add(midpoint); 23 24 25 26 return idx F. More Details about Experiment F.1. Detailed Experiment Settings Baseline setup. For AKS [49], we adhered to the default configuration: candidate frames were sampled at 1 fps, and frame-question similarity was computed via BLIP [24]. Based on the algorithms selection logic, we evaluated frame budgets of {32, 64, 128, 192, 256}. We excluded budgets of 8 and 16 as the algorithm occasionally yielded null returns at these low settings. For Q-Frame [74], we employed the default fixed frame count strategy. Since this method limits the initial candidate pool to 128 frames, our evaluation was restricted to budgets of {8, 16, 32, 64, 128}. 14 Table 5. Performance comparison between different frame selection methods. Base LMM is Qwen3-VL-8B [2]. Bold indicates best performance, while Red Box denote results inferior to uniform sampling."
        },
        {
            "title": "Method",
            "content": "#Frames MLVU [78] LVB [58] VideoMME [16]"
        },
        {
            "title": "Medium Long",
            "content": "UNI DIG (Ours) UNI DIG (Ours) UNI AKS [49] DIG (Ours) UNI AKS [49] DIG (Ours) UNI AKS [49] DIG (Ours) UNI AKS [49] DIG (Ours) UNI AKS [49] DIG (Ours) UNI AKS [49] DIG (Ours) UNI AKS [49] DIG (Ours) 8 8 16 16 32 32 64 64 64 128 128 128 192 192 192 256 256 256 512 512 512 768 768 53.4 58.2 53.9 58.9 53.7 57.3 58.7 54.7 56.3 59.6 57.2 58.9 64.4 58.9 61.1 66. 60.4 63.8 69.0 65.4 65.5 71.7 67.5 65.3 72.2 50.3 54.9 50.9 53.9 51.0 54.4 53. 51.2 52.7 54.8 54.4 53.0 58.3 57.1 54.5 60.4 57.6 55.6 61.2 60.2 57.6 63.8 60.9 58.3 64. 48.4 53.1 51.3 52.9 49.4 52.3 53.9 49.9 50.9 54.7 55.1 54.4 58.4 57.6 61.0 58. 57.8 59.2 61.6 61.4 60.7 65.6 64.3 62.3 67.8 49.2 49.8 48.0 49.9 48.6 50.1 49. 48.9 51.6 49.0 51.3 51.1 51.1 51.6 53.8 50.7 53.4 53.2 53.8 55.0 56.0 56.4 56.6 57.3 59. F.2. Extended Experiments with DIG To investigate the scalability of DIG in ultra-long context scenarios, we extended our experiments using Qwen3-VL8B [2], an open-source LMM distinguished for its robust long-context processing capability. We test DIG against the uniform sampling baseline and AKS [49]. Experiment settings. For DIG, the query identification and CAFS configurations align with Section 5, with the exception that we employ Qwen3-VL-8B [2] as the unified backbone for both reward assignment and final inference. Similarly, AKS [49] setup mirrors Section 5 but utilizes Qwen3-VL-8B [2] as the base model. To rigorously test performance across varying context lengths, we scaled input frame counts from 8 to 768, with each frame encoded into approximately 150 tokens. The results are in Table 1. DIG delivers consistent performance gains. As evidenced in Table 5, DIG yields substantial improvements across nearly all frame configurations. Notably, with 256 input frames, DIG achieves an 8.6% performance boost on MLVU [78] compared to uniform sampling. Crucially, DIG maintains robustness even at the extreme scale of 768 frames, surpassing the baseline by 4.7% on MLVU [78], 3.7% on LongVideoBench [58], and 3.5% on VideoMMETable 6. Performance comparison between different frame selection methods on MLVU. Base LMMs are Qwen2.5-VL-32B [3] (left) and Qwen2.5-VL-7B [3] (right). Bold indicates best performance. The tasks of MLVU [78] are PlotQA (PQA), NeedleQA (NQA), Action Count (AC), Action Order (AO), Ego Reasoning (ER), Anomaly Recognition (AR), Topic Reasoning (TR)."
        },
        {
            "title": "Method",
            "content": "#Frames MLVU [78]"
        },
        {
            "title": "PQA NQA",
            "content": "AC AO ER UNI Q-Frame [74] DIG (Ours) UNI Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] DIG (Ours) 8 8 8 16 16 16 32 32 32 64 64 64 64 128 128 128 128 192 192 192 55.8 51.4 62.3 59.4 56.4 67.9 61.8 66.8 61.4 72. 68.5 73.8 68.5 75.9 73.5 78.3 73.1 79.8 75.0 77.6 82.6 58.6 63.9 73.0 63.9 64.8 78.0 67.9 73.0 67.9 79. 72.1 76.6 73.2 81.1 76.3 80.3 76.1 80.0 78.0 81.4 81.4 18.5 18.4 27.2 18.0 19.9 35.0 18.5 40.3 18.9 48. 25.7 40.8 21.8 49.5 30.6 42.2 30.1 52.4 36.9 47.1 53.9 51.4 60.2 58.3 54.8 59.5 66.8 58.7 56.0 63.8 75. 61.4 58.3 66.0 78.4 68.7 61.8 69.1 79.2 69.9 63.3 80.7 50.6 50.3 56.2 52.8 51.4 57.1 57.4 59.9 53.1 59. 61.1 63.1 59.7 66.5 64.2 69.0 64.5 65.6 64.5 68.2 65.3 AR 66.5 70.5 66.0 69.5 70.5 69. 76.0 74.5 71.5 74.0 80.0 75.0 77.5 78.5 79.0 77.0 79.5 78.5 78.0 77.0 79.0 TR 85.6 76.8 84. 86.3 77.7 86.7 86.7 90.1 83.3 87.5 86.7 88.2 85.9 89.7 89.4 87.8 88.6 89.7 90.9 89.4 91.6 Medium [16]. In contrast, while AKS [49] remains competitive at lower frame counts ( 64), it exhibits marked performance degradation as the context length increases, frequently falling below the uniform sampling baseline. Given that practical video understanding tasks necessitate maximizing input frames to capture comprehensive temporal details, AKS [49] demonstrates limited utility for real-world applications. Conversely, DIG exhibits superior scalability, effectively delivering sustained performance gains. F.3. Detailed Experiment Results & More Analysis We present detailed performance breakdowns corresponding to the benchmarks discussed in Section 5. Comprehensive quantitative results are in Tables 6, 8, and 7. Uniform sampling suffices for global queries. For global queries, specifically Anomaly Recognition and Topic Reasoning tasks within MLVU [78], all evaluated methods perform comparably to uniform sampling, regardless of the input frame count. This observation reaffirms our previous assertion: uniform sampling is the preferred strategy for global queries, as it achieves sufficient performance while maintaining high efficiency. Inference for localized queries operates in two distinct stages: query-aware frame selection and subsequent reasoning based on the retrieved content. Without the initial selection stage, evaluating the models fundamental performance is challenging, as errors may stem from information-"
        },
        {
            "title": "Model",
            "content": "#Frames MLVU [78]"
        },
        {
            "title": "PQA NQA",
            "content": "AC AO ER UNI Q-Frame [74] DIG (Ours) UNI Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] DIG (Ours) UNI AKS [49] DIG (Ours) 8 8 8 16 16 32 32 32 32 64 64 64 64 128 128 128 128 192 192 192 256 256 256 52.1 50.6 57. 56.0 55.5 66.4 59.7 69.6 60.1 70.3 64.0 69.9 63.8 75.3 71.4 71.6 71.4 78.3 72.0 74.2 78.7 73.5 75.3 78. 62.5 67.0 73.2 63.4 67.6 79.7 69.0 76.3 67.9 80.6 74.4 80.8 73.5 82.8 79.2 83.7 79.2 82.3 80.8 83.1 84. 80.0 84.2 84.5 19.4 19.9 31.6 19.9 20.4 36.9 22.8 42.2 23.9 42.2 26.2 41.3 26.2 46.6 34.5 48.5 34.5 45. 40.3 46.1 47.1 41.3 47.3 49.0 44.0 48.6 48.6 42.5 50.6 51.7 51.4 50.2 54.1 54.4 51.7 54.1 56.0 60. 58.7 57.1 58.7 62.5 61.4 58.7 63.3 61.4 59.5 62.2 48.6 49.7 51.1 54.8 49.7 55.4 54.0 56.5 54.3 59. 59.9 60.5 58.2 62.2 61.4 60.8 61.4 63.6 63.6 63.6 65.3 61.1 66.2 65.1 AR 66.5 68.0 66. 70.0 70.5 68.5 74.5 72.0 70.0 73.0 76.0 69.5 72.0 75.5 73.0 72.0 73.0 72.0 73.0 73.0 72.0 73.0 75.5 73. TR 82.9 73.8 82.9 84.4 78.7 85.2 84.4 85.2 83.7 87.5 87.1 84.8 85.9 87.8 86.7 84.0 86.7 87. 87.5 85.6 87.5 89.0 87.8 89.0 deficient inputs rather than inherent model limitations. By incorporating this stage to ensure the input contains relevant information, we can decouple data retrieval issues from reasoning capabilities. This allows for more accurate assessment of the models intrinsic proficiency across different tasks, yielding deeper insights. Query-aware selection uncovers intrinsic visual perception capabilities. As shown in Table 6 and 7, our method significantly and consistently outperforms uniform sampling on localized perception tasks (e.g., PlotQA, NeedleQA, and L1-Perception). Notably, these tasks primarily evaluate fundamental visual perception capabilities. Our findings suggest that LMMs are intrinsically capable of solving such tasks, provided the query-relevant information is effectively supplied. This explains the substantial performance gap: while uniform sampling often introduces significant noise by including irrelevant content, query-aware selection ensures the model is conditioned on relevant frames. Temporal reasoning remains fundamental bottleneck. Conversely, regarding tasks requiring temporal logic (e.g., Action Order and L2-Relation), performance remains stagnant across all methods. Even when provided with queryrelevant visual information, model performance does not improve. This underscores critical limitation: current LMMs struggle with temporal reasoning and sequencing, deficiency that persists independently of the quality of visual information retrieval. Table 7. Performance Comparison between Different Frame Selection Methods on LongVideoBench. Base LMMs are Qwen2.5-VL7B [3](top) and Qwen2.5-VL-32B [3](bottom). Bold indicates best performance. Model #Frames L1-Perception L2-Relation LongVideoBench [58] UNI Q-Frame [74] DIG (Ours) UNI Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] DIG (Ours) UNI AKS [49] DIG (Ours) 8 8 8 16 16 16 32 32 32 32 64 64 64 64 128 128 128 192 192 192 256 256 256 S2E S2A O2E T2O S2O T2E E2O T2A Avg TOS E3E SAA O3O T3O T3E TAA SSS SOS Avg 57.0 67.7 69. 65.6 66.7 72.0 67.7 65.6 64.5 72.0 73.1 69.9 63.4 69.9 71.0 69.9 66.7 72.0 74.2 69.9 74.2 69.9 68.8 76. 51.1 73.9 68.2 64.8 70.5 71.6 58.0 77.3 69.3 78.4 67.0 77.3 70.5 78.4 67.0 72.7 67.1 79.5 72.7 76.1 84. 73.9 72.7 80.7 62.8 60.9 62.1 62.8 65.5 59.8 61.7 67.8 60.9 63.2 62.8 71.3 63.2 66.7 67.8 66.7 69.0 66. 66.0 67.8 66.7 69.1 70.1 71.3 56.6 57.9 50.0 53.9 63.2 65.8 56.6 63.2 59.2 68.4 59.2 65.8 57.9 69. 64.5 64.5 60.5 71.1 68.4 63.2 67.1 63.2 65.8 72.4 45.8 55.6 55.6 48.6 61.1 54.2 62.5 63.9 61.1 62. 56.9 59.7 61.1 55.6 61.1 61.1 59.7 61.1 59.7 61.1 59.7 59.7 61.1 65.3 58.5 64.6 61.5 61.5 64.6 69. 67.7 63.1 61.5 67.7 63.1 60.0 63.1 67.7 67.7 60.0 64.6 69.2 67.7 63.1 69.2 63.1 66.2 69.2 56.9 63.1 61. 61.5 69.2 69.2 67.7 63.1 69.2 67.7 66.2 64.6 64.6 72.3 72.3 66.2 67.7 75.4 70.8 67.7 73.8 72.3 63.1 75. 49.4 55.7 62.0 51.9 63.3 63.3 51.9 64.6 60.8 62.0 62.0 64.6 65.8 68.4 73.4 64.6 64.6 70.9 63.3 65.8 78. 68.4 65.8 74.7 54.4 62.7 61.8 59.0 65.6 65.4 61.9 66.1 63.3 68.0 64.6 67.2 63.8 68.8 68.2 66.1 65.1 70. 68.2 67.2 72.0 66.7 67.0 73.4 38.4 31.5 37.0 37.0 34.3 37.0 37.0 37.0 32.9 41.1 34.2 41.1 34.3 38. 38.4 37.0 38.4 38.4 35.6 35.6 35.6 37.0 35.6 37.0 62.8 62.8 61.7 62.8 59.6 57.4 61.7 67.0 59.6 62. 62.8 70.2 67.0 66.0 68.1 68.1 64.9 68.1 66.0 68.1 68.1 69.1 69.1 71.3 47.2 52.8 50.0 50.0 56.9 52. 55.6 58.3 62.5 52.8 58.3 61.1 58.3 58.3 56.9 63.9 58.3 61.1 59.7 62.5 61.1 62.5 62.5 59.7 45.5 47.0 48. 47.0 54.6 43.9 56.1 56.1 50.0 51.5 59.1 56.1 53.0 62.1 59.1 56.1 54.6 65.2 59.1 56.1 66.7 60.6 56.1 68. 47.3 39.2 54.1 56.8 43.2 56.8 55.4 45.9 50.0 55.4 60.8 47.3 52.7 59.5 56.8 50.0 56.8 56.8 58.1 56.8 56. 55.4 56.8 56.8 47.9 48.0 43.8 45.2 49.3 52.1 49.3 53.4 45.2 50.7 47.9 49.3 46.6 53.4 50.7 50.7 49.3 57. 58.9 52.1 61.6 56.2 52.1 56.2 46.3 45.1 45.1 51.9 50.0 43.9 52.4 48.8 46.3 48.8 51.2 47.6 51.2 46. 54.9 48.8 51.2 47.6 56.1 48.8 48.8 54.9 48.8 45.1 34.0 28.9 29.9 36.1 38.1 36.1 40.2 38.1 39.2 36. 43.3 40.2 37.1 42.3 46.4 47.4 45.4 45.4 46.4 46.4 49.5 46.4 43.3 49.5 64.2 65.4 67.9 63.0 65.4 74. 66.7 74.1 66.7 70.4 67.9 76.5 66.7 69.1 74.1 76.5 75.3 67.9 67.9 72.8 70.4 69.1 71.6 67.9 48.3 46.8 48. 49.3 50.1 50.4 52.7 53.2 50.3 52.1 53.9 54.5 52.0 54.9 56.3 55.6 55.1 56.3 56.5 55.6 57.6 56.9 55.2 56. Model #Frames L1-Perception L2-Relation LongVideoBench [58] UNI Q-Frame [74] DIG (Ours) UNI Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] DIG (Ours) 8 8 16 16 16 32 32 32 32 64 64 64 64 128 128 128 128 192 192 192 S2E S2A O2E T2O S2O T2E E2O T2A Avg TOS E3E SAA O3O T3O T3E TAA SSS SOS Avg 62.4 62.4 63.4 57.0 69.9 66.7 68.8 65.6 69.9 71.0 69.9 67.7 63.4 69. 71.0 69.9 65.6 76.3 72.0 71.0 73.1 61.4 81.8 75.0 71.6 77.3 80.7 68.2 77.3 77.3 75.0 65.9 76.1 77.3 79. 70.5 72.7 69.3 83.0 73.9 79.5 84.1 65.5 62.1 60.9 57.5 64.4 63.2 64.4 69.0 63.2 65.5 64.4 66.7 66.7 65. 62.1 67.8 60.9 65.5 66.7 67.8 67.8 54.0 55.3 59.2 52.6 64.5 55.3 57.9 61.8 59.2 57.9 61.8 60.5 56.6 64. 61.8 63.2 59.2 65.8 67.1 67.1 68.4 51.4 55.6 48.6 48.6 58.3 62.5 54.2 65.3 65.3 65.3 58.3 69.4 66.7 65. 68.1 65.3 65.3 70.8 66.7 66.7 68.1 58.5 61.5 60.0 63.1 64.6 66.2 63.1 61.5 61.5 72.3 60.0 61.5 64.6 66. 63.1 61.5 61.5 67.7 67.7 60.0 64.6 58.5 66.2 66.2 63.1 63.1 64.6 61.5 66.2 60.0 67.7 70.8 72.3 67.7 72. 67.7 70.8 67.7 80.0 72.3 69.2 76.9 51.9 53.2 60.8 45.6 62.0 68.4 53.2 64.6 54.4 60.8 57.0 67.1 65.8 67. 60.8 63.3 59.5 63.3 63.3 62.0 65.8 58.2 62.6 61.8 57.4 65.9 65.9 61.8 66.7 61.4 66.9 63.7 67.8 64.0 68. 65.7 67.0 63.7 71.6 68.8 68.3 71.1 30.1 30.1 37.0 37.0 31.5 34.3 32.9 34.3 37.0 35.6 31.5 37.0 35.6 34. 35.6 37.0 35.6 34.3 34.3 35.6 38.4 63.8 57.5 60.6 58.5 62.8 56.4 66.0 69.2 59.6 70.2 67.0 73.4 63.8 68. 70.2 74.5 71.3 72.3 76.6 74.5 74.5 55.6 56.9 61.1 58.3 58.3 63.9 56.9 59.7 54.2 66.7 58.3 56.9 61.1 73. 62.5 58.3 61.1 68.1 58.3 58.3 70.8 42.4 42.4 50.0 53.0 51.5 54.5 53.0 50.0 48.5 62.1 51.5 53.0 47.0 62. 51.5 56.1 48.5 68.2 62.1 57.6 69.7 47.3 43.2 52.7 54.1 46.0 56.8 52.7 41.9 48.6 56.8 55.4 47.3 46.0 59. 55.4 51.4 51.4 66.2 58.1 54.1 71.6 49.3 46.6 49.3 50.7 43.8 54.8 58.9 48.0 48.0 56.2 54.8 52.1 52.1 54. 57.5 53.4 53.4 57.5 60.3 50.7 57.5 45.1 41.5 47.6 53.7 46.3 46.3 50.0 51.2 51.2 43.9 51.2 48.8 52.4 46. 53.7 52.4 52.4 45.1 52.4 48.8 43.9 46.4 36.1 44.3 42.3 40.2 39.2 47.4 44.3 50.5 51.5 51.5 52.6 53.6 57. 60.8 53.6 60.8 55.7 56.7 54.6 56.7 58.0 59.3 65.4 66.7 54.3 67.9 70.4 72.8 60.5 71.6 69.1 75.3 67.9 72. 71.6 76.5 70.4 74.1 71.6 76.5 75.3 49.2 46.1 52.0 52.7 49.5 52.7 54.5 52.8 51.3 57.2 54.9 55.8 53.8 55. 58.3 58.6 56.9 60.2 59.4 57.3 60.7 G. More Efficiency Analysis of DIG G.1. Detailed Runtime Profiling We evaluate the computational efficiency of DIG compared to distinct baselines, AKS [49] and Q-Frame [74]. The total runtime of each method can be divided into two stages: Key Frame Selection, where the method identifies optimal indices from raw video. Inference, where the LMM processes the selected frames to generate response. All experiments were conducted on node equipped with 8 NVIDIA A100 GPUs. To provide comprehensive analysis, we report the standard LMM inference latency across varying input frame counts in Table 9 and detail the selection overhead introduced by specific methods in Table 10. DIG achieves favorable efficiency-performance tradeoff. As evidenced in Table 10, DIG offers significant efficiency advantage over AKS [49], reducing computational overhead by an order of magnitude while maintaining superior downstream performance (see Section 5). While DIG incurs marginal increase in processing time compared to Q-Frame [74], this cost is justified by substantial robustness gains; specifically, Q-Frame [74] fails to outperform uniform sampling as frame counts exceed 32, whereas DIG consistently surpasses baselines across all settings. Furthermore, comparing the selection overhead (TaTable 8. Performance comparison between different frame selection methods on VideoMME. Base LMMs are Qwen2.5-VL7B [3](left) and Qwen2.5-VL-32B [3](right). Bold indicates best performance. The tasks are Object Reasoning (ORA), Object Recognition (ORC), Action Reasoning (ARA), Information Synopsis (INS), Counting Problem (COP), Temporal Reasoning (TER), Temporal Perception (TEP), Spatial Perception (SPP), Spatial Reasoning (SPR), OCR, Attribute Perception (ATP), Action Recognition (ACR). Model #Frames VideoMME [16] ORA ORC ARA INS COP TER TEP SPR SPP OCR ATP ACR UNI Q-Frame [74] DIG (Ours) UNI Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] DIG (Ours) UNI AKS [49] DIG (Ours) 8 8 8 16 16 16 32 32 32 32 64 64 64 64 128 128 128 192 192 192 256 256 256 49.5 50.5 47.6 53.5 52.4 57.9 57.5 56.8 55.3 59.5 58.1 58.4 56.0 60. 61.2 61.0 58.7 61.7 62.3 60.8 64.5 62.1 61.0 63.0 54.5 50.1 60.2 58.8 52.2 61.0 63.6 66.7 56.6 67. 66.7 67.5 62.4 68.4 71.2 68.4 64.5 72.3 72.0 69.2 73.7 71.5 70.9 72.0 49.5 50.8 52.3 51.2 52.2 54. 55.8 51.9 54.2 56.1 54.0 56.1 56.0 57.5 58.9 59.3 55.6 57.2 60.7 57.5 60.0 59.6 57.9 62.1 67.8 64.8 70. 74.6 66.0 74.6 76.5 80.2 68.9 75.9 76.8 79.3 72.6 78.0 79.9 80.2 78.0 80.5 79.9 79.3 80.2 82.4 79.3 82. 36.2 36.9 39.6 37.7 36.8 41.4 42.5 42.9 38.3 40.3 41.4 44.4 46.5 46.3 45.1 44.8 38.9 45.1 48.5 45.5 46. 46.6 44.8 46.3 40.7 36.0 40.7 43.5 36.1 43.5 43.5 49.2 37.2 46.9 43.5 52.0 45.2 49.2 57.1 57.1 55.6 52. 54.2 57.6 54.2 57.6 57.1 54.8 47.3 43.3 56.4 63.6 56.9 56.4 67.3 60.0 59.6 52.7 69.1 72.7 56.8 58. 70.9 76.4 64.9 58.2 72.7 81.8 65.5 76.4 83.6 67.3 58.9 62.1 64.3 64.3 62.1 66.1 76.8 76.8 65.5 76. 76.8 76.8 69.1 73.2 76.8 75.0 72.4 78.6 76.8 76.8 78.6 75.0 75.0 78.6 63.0 33.3 66.7 72.2 45.9 75. 72.2 72.2 50.1 72.2 68.5 72.2 54.2 75.9 68.5 68.5 54.3 68.5 70.4 72.2 68.5 66.7 70.4 66.7 48.9 44.0 55. 54.7 48.8 59.0 62.6 66.2 50.1 69.8 67.6 72.7 51.3 73.4 71.2 77.0 57.4 71.2 71.9 76.3 74.1 71.9 74.1 73. 62.2 57.0 65.3 67.1 59.0 71.2 74.8 74.8 64.0 73.4 74.8 74.8 72.0 73.4 76.6 77.9 67.0 78.8 77.5 77.9 79. 77.9 77.9 80.2 53.4 52.2 55.9 56.9 49.0 56.2 59.7 59.7 50.6 61.0 63.9 61.0 51.2 63.3 66.1 61.3 59.3 66. 68.1 63.9 65.8 68.1 64.5 67.4 Model #Frames VideoMME [16] ORA ORC ARA INS COP TER TEP SPR SPP OCR ATP ACR UNI Q-Frame [74] DIG (Ours) UNI Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] Q-Frame [74] DIG (Ours) UNI AKS [49] DIG (Ours) 8 8 8 16 16 16 32 32 32 32 64 64 64 64 128 128 128 192 192 192 57.2 50.5 55.6 55.4 52.4 58.3 55.4 58.6 55.3 58.8 61.8 61.0 56.0 62.6 63.4 66.0 58.7 65. 64.7 65.5 66.8 54.8 50.1 58.1 56.5 52.2 59.7 55.9 57.0 56.6 64.0 62.9 63.4 62.4 65.6 69.9 68.3 64.5 73. 69.9 69.4 74.2 53.4 50.8 53.4 52.5 52.2 53.4 54.2 57.6 54.2 55.5 58.0 59.7 56.0 57.6 63.5 58.8 55.6 61. 63.0 59.7 62.6 69.7 64.8 69.7 70.5 66.0 71.0 76.8 78.0 68.9 78.0 76.8 80.1 72.6 76.4 81.3 81.3 78.0 81. 81.3 81.3 81.7 30.1 36.9 31.5 32.2 36.8 37.8 35.0 34.3 38.3 35.7 34.3 37.8 46.5 32.9 42.0 42.7 38.9 45. 44.8 42.7 42.0 37.8 36.0 40.9 47.6 36.1 47.0 40.9 47.6 37.2 51.8 44.5 54.3 45.2 51.8 54.3 57.9 55.6 52. 55.5 58.5 57.3 46.0 43.3 46.0 51.4 56.9 56.8 62.2 56.8 59.6 48.7 64.9 62.2 56.8 59.5 59.5 67.6 64.9 64. 73.0 75.7 64.9 65.5 62.1 72.4 75.9 62.1 75.9 75.9 79.3 65.5 75.9 79.3 79.3 69.1 75.9 86.2 82.8 72.4 82. 86.2 79.3 79.3 54.2 33.3 62.5 62.5 45.9 62.5 62.5 45.8 50.1 58.3 62.5 54.2 54.2 58.3 58.3 50.0 54.3 54. 62.5 54.2 58.3 47.6 44.0 54.9 42.7 48.8 45.1 47.6 61.0 50.1 56.1 58.5 59.8 51.3 67.1 68.3 69.5 57.4 69. 68.3 73.2 73.2 62.0 57.0 62.0 65.0 59.0 65.0 66.0 68.0 64.0 69.0 69.0 73.0 72.0 73.0 73.0 75.0 67.0 75. 75.0 78.0 80.0 45.6 52.2 48.4 50.6 49.0 47.3 51.7 51.1 50.6 50.0 59.3 56.6 51.2 58.2 57.1 59.9 59.3 60. 62.1 58.8 61.0 Table 9. Inference latency analysis. The inference time (in minutes) of the base LMM (Qwen2.5-VL-7B [3]) across different input frame counts using standard uniform sampling. #Frames Inference Time (min) MLVU [78] LongVideoBench [58] VideoMME [16] 3.2 1.4 3.1 16 5.0 2.2 4.7 32 9.3 4.3 8.7 17.6 8.3 15.8 128 29.1 14.0 26.1 192 37.3 19.9 36.7 43.4 25.6 46.3 Table 10. Comparison of frame selection overhead. The time cost (in minutes) required by different methods to process videos and select key frames. For DIG, we break down the cost into Query Identification (QI), Content-Aware Frame Selection (CAFS), Reward Assignment (RA), and Video Refinement (VR)."
        },
        {
            "title": "Method",
            "content": "AKS [49] Q-Frame [74] Selection Time (min) MLVU [78] LongVideoBench [58] VideoMME [16] 720 720 720 122.1 34.5 94.2 DIG (Ours) QI"
        },
        {
            "title": "CAFS",
            "content": "RA 11.3 7.6 11.6 25.9 20.8 31.2 218.9 110.4 264.8 VR 0.2 0.1 0."
        },
        {
            "title": "Sum",
            "content": "256.3 138.9 307.9 ble 10) against standard inference latency  (Table 9)  , the additional cost remains within reasonable range. This confirms that DIG effectively balances efficiency and accuracy, serving as practical, plug-and-play module for enhanced long-form video understanding. G.2. Efficiency Gains from Query Identification To balance efficiency and accuracy, DIG employs Query Identification module. We apply resource-intensive key frame selection only to localized queries, defaulting to efficient uniform sampling for global ones. This adaptive strategy minimizes computational cost without compromising downstream performance (see Section 6). Table 11 quantifies these gains by comparing our adaptive approach against the baseline that applies our specific selection universally. On LongVideoBench [58], where queries are preTable 11. Impact of Query Identification on efficiency. We compare the frame selection time (in minutes) of applying our specific selection universally (w/o. QI) versus DIGs adaptive approach (w. QI). Percent denotes the proportion of localized queries. Percent w/o. QI w. QI MLVU [78] LongVideoBench [58] VideoMME [16] 82.8 97.8 77.0 295.7 134.1 384.2 256.3 ( 13.3%) 138.9 ( 3.6%) 307.9 ( 19.9%) dominantly localized, the QI module incurs marginal overhead (3.6%) due to the additional classification step. However, on datasets with diverse mix of query types, such as VideoMME [16] and MLVU [78], the adaptive strategy yields significant time savings (19.9% and 13.3%, respectively). This demonstrates that the QI module effectively optimizes resource allocation by bypassing unnecessary computation for global queries."
        },
        {
            "title": "References",
            "content": "[1] K. Ataallah, X. Shen, E. Abdelrahman, E. Sleiman, M. Zhuge, J. Ding, D. Zhu, J. Schmidhuber, and M. Elhoseiny. Goldfish: Vision-language understanding of arbitrarily long videos. In ECCV, pages 251 267. Springer, 2024. 2 [2] S. Bai, Y. Cai, R. Chen, K. Chen, X. Chen, Z. Cheng, L. Deng, W. Ding, C. Gao, C. Ge, W. Ge, Z. Guo, Q. Huang, J. Huang, F. Huang, B. Hui, S. Jiang, Z. Li, M. Li, M. Li, K. Li, Z. Lin, J. Lin, X. Liu, J. Liu, C. Liu, Y. Liu, D. Liu, S. Liu, D. Lu, R. Luo, C. Lv, R. Men, L. Meng, X. Ren, X. Ren, S. Song, Y. Sun, J. Tang, J. Tu, J. Wan, P. Wang, P. Wang, Q. Wang, Y. Wang, T. Xie, Y. Xu, H. Xu, J. Xu, Z. Yang, M. Yang, J. Yang, A. Yang, B. Yu, F. Zhang, H. Zhang, X. Zhang, B. Zheng, H. Zhong, J. Zhou, F. Zhou, J. Zhou, Y. Zhu, and K. Zhu. Qwen3-vl technical report, 2025. 5, 14 [3] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report, 2025. 1, 3, 5, 6, 7, 8, 9, 15, 16, 17 [4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020. 2 [5] W. Chai, E. Song, Y. Du, C. Meng, V. Madhavan, O. Bar-Tal, J.-N. Hwang, S. Xie, and C. D. Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark, 2025. 2 [6] G. Chen, Y. Liu, Y. Huang, Y. He, B. Pei, J. Xu, Y. Wang, T. Lu, and L. Wang. Cg-bench: Cluegrounded question answering benchmark for long video understanding, 2024. 3 [7] L. Chen, X. Wei, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, Z. Tang, L. Yuan, et al. Sharegpt4video: Improving video understanding and In NeurIPS, volgeneration with better captions. ume 37, pages 1947219495, 2024. 2 [8] Y. Chen, F. Xue, D. Li, Q. Hu, L. Zhu, X. Li, Y. Fang, H. Tang, S. Yang, Z. Liu, E. He, H. Yin, P. Molchanov, J. Kautz, L. Fan, Y. Zhu, Y. Lu, and S. Han. Longvila: Scaling long-context visual language models for long videos, 2024. 3 [9] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, InM. Zhong, Q. Zhang, X. Zhu, L. Lu, et al. ternvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, pages 2418524198, 2024. 1, 3 [10] Z. Cheng, S. Leng, H. Zhang, Y. Xin, X. Li, G. Chen, Y. Zhu, W. Zhang, Z. Luo, D. Zhao, and L. Bing. VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs, Oct. 2024. 1, 2 [11] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. MeierHellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways, 2022. [12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros, M. Pellat, K. Robinson, D. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei. Scaling instruction-finetuned language models, 2022. 2 [13] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 13 [14] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. AlDahle, A. Letman, A. Mathur, A. Schelten, et al. The llama 3 herd of models, Aug. 2024. 2 [15] Y. Fan, X. Ma, R. Wu, Y. Du, J. Li, Z. Gao, and Q. Li. Videoagent: memory-augmented multimodal agent for video understanding, 2024. 2 [16] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang, P. Chen, Y. Li, S. Lin, S. Zhao, K. Li, T. Xu, X. Zheng, E. Chen, R. Ji, and X. Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, 2024. 3, 5, 6, 7, 8, 10, 12, 13, 14, 15, 17 [17] R. Ganz, Y. Kittenplon, A. Aberdam, E. B. Avraham, O. Nuriel, S. Mazor, and R. Litman. Question aware vision transformer for multimodal reasoning, 2024. 2 [18] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. 2, 6, 8 [19] P. Jin, R. Takanobu, W. Zhang, X. Cao, and L. Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In CVPR, pages 1370013710, 2024. 1 [20] P. Jin, R. Takanobu, W. Zhang, X. Cao, and L. Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding, 2024. 2 [21] W. Kim, C. Choi, W. Lee, and W. Rhee. An image grid can be worth video: Zero-shot video question answering using vlm. IEEE Access, 2024. 2 [22] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 5 [23] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P. Zhang, Y. Li, Z. Liu, and C. Li. Llavaonevision: Easy visual task transfer, 2024. 1, 2, 3 [24] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation, 2022. 14 [25] X. Li, Y. Wang, J. Yu, X. Zeng, Y. Zhu, H. Huang, J. Gao, K. Li, Y. He, C. Wang, Y. Qiao, Y. Wang, and L. Wang. Videochat-flash: Hierarchical compression for long-context video modeling, 2025. 2 [26] B. Lin, Y. Ye, B. Zhu, J. Cui, M. Ning, P. Jin, and L. Yuan. Video-llava: Learning united visual reparXiv resentation by alignment before projection. preprint arXiv:2311.10122, 2023. 1 [27] B. Lin, Y. Ye, B. Zhu, J. Cui, M. Ning, P. Jin, and L. Yuan. Video-llava: Learning united visual representation by alignment before projection, 2024. 2 [28] K. Lin, F. Ahmed, L. Li, C.-C. Lin, E. Azarnasab, Z. Yang, J. Wang, L. Liang, Z. Liu, Y. Lu, et al. Mm-vid: Advancing video understanding with gpt-4v (ision). arXiv preprint arXiv:2310.19773, 2023. 1 [29] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning, 2023. 1, 2 [30] J. Liu, Y. Wang, H. Ma, X. Wu, X. Ma, X. Wei, J. Jiao, E. Wu, and J. Hu. Kangaroo: powerful videolanguage model supporting long-context video input, 2024. 2 [31] S. Liu, C. Zhao, T. Xu, and B. Ghanem. Bolt: Boost large vision-language model without training for longform video understanding, 2025. 1, 2, 3, 4, 6 via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 1, 2 [33] J. Min, S. Buch, A. Nagrani, M. Cho, and C. Schmid. Morevqa: Exploring modular reasoning models for In 2024 IEEE/CVF Convideo question answering. ference on Computer Vision and Pattern Recognition (CVPR), pages 1323513245, June 2024. 2 [34] OpenAI. 2025. gpt-oss-120b & gpt-oss-20b model card, [35] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, et al. Gpt-4 technical report, Mar. 2024. 2 [36] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. 2, 4, 7 [37] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4, 2023. [38] T. Qu, L. Tang, B. Peng, S. Yang, B. Yu, and J. Jia. Does your vision-language model get lost in the long video sampling dilemma?, 2025. 3 [39] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. 4, 8 [40] S. Ren, L. Yao, S. Li, X. Sun, and L. Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 1 [41] X. Shen, Y. Xiong, C. Zhao, L. Wu, J. Chen, C. Zhu, Z. Liu, F. Xiao, B. Varadarajan, F. Bordes, Z. Liu, H. Xu, H. J. Kim, B. Soran, R. Krishnamoorthi, M. Elhoseiny, and V. Chandra. Longvu: Spatiotemporal adaptive compression for long video-language understanding, Oct. 2024. 2 [42] X. Shen, Y. Xiong, C. Zhao, L. Wu, J. Chen, C. Zhu, Z. Liu, F. Xiao, B. Varadarajan, F. Bordes, Z. Liu, H. Xu, H. J. Kim, B. Soran, R. Krishnamoorthi, M. Elhoseiny, and V. Chandra. Longvu: Spatiotemporal adaptive compression for long video-language understanding, 2024. 3 [43] M. Shi, F. Liu, S. Wang, S. Liao, S. Radhakrishnan, Y. Zhao, D.-A. Huang, H. Yin, K. Sapra, Y. Yacoob, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. 1 [32] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan. Video-chatgpt: Towards detailed video understanding [44] M. Shi, F. Liu, S. Wang, S. Liao, S. Radhakrishnan, Y. Zhao, D.-A. Huang, H. Yin, K. Sapra, Y. Yacoob, 19 H. Shi, B. Catanzaro, A. Tao, J. Kautz, Z. Yu, and G. Liu. Eagle: Exploring the design space for multimodal llms with mixture of encoders, 2025. 2 [45] M. Shi, S. Wang, C.-Y. Chen, J. Jain, K. Wang, J. Xiong, G. Liu, Z. Yu, and H. Shi. Slow-fast architecture for video multi-modal large language models, 2025. 2 [46] E. Song, W. Chai, G. Wang, Y. Zhang, H. Zhou, F. Wu, H. Chi, X. Guo, T. Ye, Y. Zhang, Y. Lu, J.-N. Hwang, and G. Wang. Moviechat: From dense token to sparse memory for long video understanding, 2024. 2 [47] E. Song, W. Chai, G. Wang, Y. Zhang, H. Zhou, F. Wu, H. Chi, X. Guo, T. Ye, Y. Zhang, Y. Lu, J.- N. Hwang, and G. Wang. Moviechat: From dense token to sparse memory for long video understanding. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18221 18232, Seattle, WA, USA, June 2024. IEEE. 2 [48] H. Sun, S. Lu, H. Wang, Q.-G. Chen, Z. Xu, W. Luo, K. Zhang, and M. Li. Mdp3: training-free approach for list-wise frame selection in video-llms, 2025. 1, 2, 6 [49] X. Tang, J. Qiu, L. Xie, Y. Tian, J. Jiao, and Q. Ye. Adaptive keyframe sampling for long video understanding, 2025. 1, 2, 3, 4, 5, 6, 14, 15, 16, [50] L. Team. The llama 3 herd of models, 2024. 13 [51] Q. Team. Qwen3 technical report, 2025. 5, 12, 13 [52] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer, X. Pan, Z. Wang, R. Fergus, Y. LeCun, and S. Xie. Cambrian1: fully open, vision-centric exploration of multimodal llms, 2024. 1 [53] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023. 2 [54] X. Wang, Q. Si, J. Wu, S. Zhu, L. Cao, and L. Nie. Retake: Reducing temporal and knowledge redundancy for long video understanding, 2025. 2 [55] X. Wang, Y. Zhang, O. Zohar, and S. Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent, 2024. 2, 4 [56] Z. Wang, S. Yu, E. Stengel-Eskin, J. Yoon, F. Cheng, G. Bertasius, and M. Bansal. Videotree: Adaptive tree-based video representation for llm reasoning on long videos, 2025. 2 [57] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chainof-thought prompting elicits reasoning in large language models, 2023. 10, 11, language understanding, 2024. 3, 5, 6, 8, 9, 10, 12, 13, 14, 16, 17 [59] H. Wu, H. Liu, Y. Qiao, and X. Sun. Dibs: Enhancing dense video captioning with unlabeled videos via pseudo boundary enrichment and online refinement. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18699 18708, June 2024. 2 [60] M. Xu, M. Gao, Z. Gan, H.-Y. Chen, Z. Lai, H. Gang, K. Kang, and A. Dehghan. Slowfast-llava: strong training-free baseline for video large language models, 2024. 2 [61] S. Yan, T. Zhu, Z. Wang, Y. Cao, M. Zhang, S. Ghosh, Y. Wu, and J. Yu. Videococa: Video-text modeling with zero-shot transfer from contrastive captioners, 2023. 2 [62] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. PontTuset, I. Laptev, J. Sivic, and C. Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning, Mar. 2023. 2 [63] Z. Yang, D. Chen, X. Yu, M. Shen, and C. Gan. Vca: Video curious agent for long video understanding, 2025. [64] J. Ye, Z. Wang, H. Sun, K. Chandrasegaran, Z. Durante, C. Eyzaguirre, Y. Bisk, J. C. Niebles, E. Adeli, L. Fei-Fei, J. Wu, and M. Li. Re-thinking temporal search for long-form video understanding, 2025. 1, 2, 4 [65] J. Ye, Z. Wang, H. Sun, K. Chandrasegaran, Z. Durante, C. Eyzaguirre, Y. Bisk, J. C. Niebles, E. Adeli, L. Fei-Fei, J. Wu, and M. Li. T*: Re-thinking temporal search for long-form video understanding, 2025. 2, 4 [66] H. Yen, T. Gao, and D. Chen. Long-context language modeling with parallel context encoding, 2024. 2 [67] S. Yu, C. Jin, H. Wang, Z. Chen, S. Jin, Z. Zuo, X. Xu, Z. Sun, B. Zhang, J. Wu, H. Zhang, and Q. Sun. Frame-voyager: Learning to query frames for video large language models, Oct. 2024. 1, 2, 6 [68] B. Zhang, K. Li, Z. Cheng, Z. Hu, Y. Yuan, G. Chen, S. Leng, Y. Jiang, H. Zhang, X. Li, P. Jin, W. Zhang, F. Wang, L. Bing, and D. Zhao. Videollama 3: Frontier multimodal foundation models for image and video understanding, 2025. 1 [69] C. Zhang, T. Lu, M. M. Islam, Z. Wang, S. Yu, M. Bansal, and G. Bertasius. simple llm framework for long-range video question-answering. arXiv preprint arXiv:2312.17235, 2023. 1 [70] H. Zhang, X. Li, and L. Bing. Video-llama: An language model for instruction-tuned audio-visual video understanding, 2023. 1 [58] H. Wu, D. Li, B. Chen, and J. Li. Longvideobench: interleaved videoA benchmark for long-context [71] K. Zhang, B. Li, P. Zhang, F. Pu, J. A. Cahyono, K. Hu, S. Liu, Y. Zhang, J. Yang, C. Li, and Z. Liu. 20 Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. [72] P. Zhang, K. Zhang, B. Li, G. Zeng, J. Yang, Y. Zhang, Z. Wang, H. Tan, C. Li, and Z. Liu. Long context transfer from language to vision, 2024. 3 [73] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. 2 [74] S. Zhang, J. Yang, J. Yin, Z. Luo, and J. Luan. Q-frame: Query-aware frame selection and multiresolution adaptation for video-llms, 2025. 2, 4, 5, 6, 14, 15, 16, 17 [75] Y. Zhang, J. Wu, W. Li, B. Li, Z. Ma, Z. Liu, and C. Li. Video instruction tuning with synthetic data, 2024. 1 [76] Z. Zhao, H. Lu, Y. Huo, Y. Du, T. Yue, L. Guo, B. Wang, W. Chen, and J. Liu. Needle in video haystack: scalable synthetic evaluator for video mllms, 2025. 2 [77] Y. Zhong, J. Xiao, W. Ji, Y. Li, W. Deng, and T.-S. Chua. Video question answering: Datasets, algorithms and challenges, 2022. 2 [78] J. Zhou, Y. Shu, B. Zhao, B. Wu, Z. Liang, S. Xiao, M. Qin, X. Yang, Y. Xiong, B. Zhang, T. Huang, and Z. Liu. Mlvu: Benchmarking multi-task long video understanding, 2025. 3, 5, 6, 7, 8, 10, 12, 13, 14, 15, [79] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, Oct. 2023. 2 [80] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, H. Tian, Y. Duan, W. Su, J. Shao, Z. Gao, E. Cui, X. Wang, Y. Cao, Y. Liu, X. Wei, H. Zhang, H. Wang, W. Xu, H. Li, J. Wang, N. Deng, S. Li, Y. He, T. Jiang, J. Luo, Y. Wang, C. He, B. Shi, X. Zhang, W. Shao, J. He, Y. Xiong, W. Qu, P. Sun, P. Jiao, H. Lv, L. Wu, K. Zhang, H. Deng, J. Ge, K. Chen, L. Wang, M. Dou, L. Lu, X. Zhu, T. Lu, D. Lin, Y. Qiao, J. Dai, and W. Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. 3 [81] O. Zohar, X. Wang, Y. Dubois, N. Mehta, T. Xiao, P. Hansen-Estruch, L. Yu, X. Wang, F. Juefei-Xu, N. Zhang, S. Yeung-Levy, and X. Xia. Apollo: An exploration of video understanding in large multimodal models, 2024."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "Tsinghua University"
    ]
}