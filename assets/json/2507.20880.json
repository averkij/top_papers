{
    "paper_title": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment",
    "authors": [
        "Renhang Liu",
        "Chia-Yu Hung",
        "Navonil Majumder",
        "Taylor Gautreaux",
        "Amir Ali Bagherzadeh",
        "Chuan Li",
        "Dorien Herremans",
        "Soujanya Poria"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion and flow-matching models have revolutionized automatic text-to-audio generation in recent times. These models are increasingly capable of generating high quality and faithful audio outputs capturing to speech and acoustic events. However, there is still much room for improvement in creative audio generation that primarily involves music and songs. Recent open lyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an acceptable standard in automatic song generation for recreational use. However, these models lack fine-grained word-level controllability often desired by musicians in their workflows. To the best of our knowledge, our flow-matching-based JAM is the first effort toward endowing word-level timing and duration control in song generation, allowing fine-grained vocal control. To enhance the quality of generated songs to better align with human preferences, we implement aesthetic alignment through Direct Preference Optimization, which iteratively refines the model using a synthetic dataset, eliminating the need or manual data annotations. Furthermore, we aim to standardize the evaluation of such lyrics-to-song models through our public evaluation dataset JAME. We show that JAM outperforms the existing models in terms of the music-specific attributes."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 0 8 8 0 2 . 7 0 5 2 : r JAM: Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment Renhang Liu1, Chia-Yu Hung1, Navonil Majumder1, Taylor Gautreaux2, Amir Ali Bagherzadeh2, Chuan Li2, Dorien Herremans1, Soujanya Poria1 1Singapore University of Technology and Design {renhang liu, chiayu hung, navonil majumder, dorien herremans, sporia}@sutd.edu.sg 2Lambda Labs {taylor.gautreaux, amirali.zadeh, c}@lambda.ai Project JAMIFY: https://declare-lab.github.io/jamify Model: https://huggingface.co/declare-lab/JAM-0.5 Github: https://github.com/declare-lab/jamify Disclaimer: JAM is intended for use by music professionals, as it requires desired total and word-level duration inputs that music expert is able to provide. Use by non-experts or without accurate word timings may result in suboptimal outputs, including vocal-accompaniment mismatches and artifacts. These issues can be mitigated with duration predictor. Abstract Diffusion and flow-matching models have revolutionized automatic text-to-audio generation in recent times. These models are increasingly capable of generating high quality and faithful audio outputs capturing to speech and acoustic events. However, there is still much room for improvement in creative audio generation that primarily involves music and songs. Recent open lyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an acceptable standard in automatic song generation for recreational use. However, these models lack fine-grained word-level controllability often desired by musicians in their workflows. To the best of our knowledge, our flow-matching-based JAM is the first effort toward endowing word-level timing and duration control in song generation, allowing fine-grained vocal control. To enhance the quality of generated songs to better align with human preferences, we implement aesthetic alignment through Direct Preference Optimization, which iteratively refines the model using synthetic dataset, eliminating the need Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. for manual data annotations. Furthermore, we aim to standardize the evaluation of such lyrics-to-song models through our public evaluation dataset JAME. We show that JAM outperforms the existing models in terms of the music-specific attributes."
        },
        {
            "title": "Introduction",
            "content": "Music plays an essential role in human culture: it brings people together, expresses emotions, embodies cultural elements, and through them enriches our daily lives via shared experiences. From ancient rituals to modern celebrations, music has shaped our social bonds and personal wellbeing (Freeman III 1998; Agres et al. 2021). is and timeconsuming process that often demands extensive effort from skilled musicians. Recent advances in neural generative models have made AIgenerated music reality, providing composers with reliable first drafts that accelerate the creative workflow. Creating music, however, complex The recent audio-based generative music AI models can be grouped into three main trends: Singing voice generation focuses on producing expressive vocal performances from given lyrics and musical notesoften using techniques like voice cloningwithout generating instrumental accompaniment. Textual description to music generation learns to translate prompts that may include instructions about mood, desired musical features, or instrumentation into fully synthesized music, but typically does not handle vocals. These works are very similar to text-to-audio generative models, such as, Tango series models (Ghosal et al. 2023; Majumder et al. 2024a; Hung et al. 2025a), AudioLDM (Liu et al. 2024), and Stable Audio Open (Evans et al. 2024). These models are capable of generating both sound effects and instrumental music from textual prompts. Lyrics-to-song generation combines both singing voice and accompaniment to produce full songs, ensuring semantic coherence between lyrics and music as well as acoustic harmony across vocals and instruments. Among these, lyrics-to-song generation presents unique challenges in aligning linguistic content with musical structure, preserving prosody, and generating highquality audio across longer durations. Existing approaches to lyrics-to-song generation fall into two broad categories. Autoregressive models generate audio tokens sequentially, which allows them to maintain strong longrange coherence and to incorporate explicit musical style controls. However, their stepbystep decoding can be prohibitively slow for practical use. In contrast, diffusionbased methods iteratively denoise latent representations, offering high audio fidelity, flexible conditioning on melody, rhythm, and timbre, and natural support for editing and style transfer. Despite these advantages, diffusion models can still struggle with generation speed and finegrained alignment between lyrics and audio. Recent systems such as DiffRhythm (Ning et al. 2025), YuE (Yuan et al. 2025), LeVo (Lei et al. 2025), and ACEStep (Gong et al. 2025) have demonstrated impressive results, but they share several limitations: 1. Large model size: All are based on hundreds of millions to billions of parameters, leading to slow inference regardless of the generation paradigm. This also makes the model more resource hungry. 2. Coarse timing control: While DiffRhythm allows specifying sentencelevel start times, none support word or phonemelevel alignment, limiting the users ability to shape prosody and rhythm precisely. 3. Weak lyric fidelity: High word error rates (WER) and phoneme error rates (PER) indicate that the models often misalign or omit lyric content. 4. Lack of duration control: Without explicit control over the overall song duration and interword pauses, users cannot easily shape the global structure or pacing of the generated song. To address these challenges, we introduce JAM, rectifiedflow based model for lyrics to song generation. JAM is 530Mparameter conditional flowmatching model (Lipman et al. 2023) built on 16 LLaMAstyle Transformer layers as the Diffusion Transformer (DiT) backbone (Peebles and Xie 2023). It is jointly conditioned on: Lyrics, with finegrained word and phonemelevel start/end times for precise prosody control. Target duration, guiding the model on the overall song length and the spacing between vocal phrases. Style prompt, which can be either reference audio clip or text description, to capture desired timbral and structural characteristics. JAM generates full songs at 44.1kHz for up to 3 minutes and 50 seconds by learning rectified flow trajectory through the latent space of variational autoencoder (VAE). Our contributions are: 1. Compact architecture: At 530M parameters, JAM is less than half the size of the next smallest system (Diffrhythm-1.1B), enabling faster inference and is less resource demanding. 2. Finegrained alignment: By accepting word and phonemelevel timing inputs, JAM lets users control the exact placement of each vocal sound, improving rhythmic flexibility and expressive timing. 3. Enhanced lyric fidelity: This precise alignment reduces WER and PER by over 3 compared to prior work, as the model can directly attend to phoneme boundaries and correct misalignments. 4. Global duration control: Our novel duration mechanism not only sets the interword pacing implicitly (it is controlled explicitly through lyric timing) but also specifies how much instrumental introduction and coda to generate, giving composers full control over song structure. 5. Rigorous evaluation: Assessing prior methods is difficult when their training data is undisclosed. To avoid data contamination, we compiled lyrics for 250 tracks released after the models training cut-offsensuring neither JAM nor any baseline had access to them. Additionally, these tracks span variety of genres, allowing us to evaluate performance across different musical styles. 6. Aesthetic alignment: Most prior systems (with the exception of LeVo) lack any mechanism for aligning model outputs to human aesthetic preferences. In texttoaudio work such as Tango2 and Tangoflux, preference alignment has proven effective, and LeVo recently adapted this idea for song generation. However, LeVo relies on manually annotated preference dataset, which incurs significant human effort. Inspired by Tango2 and Tangoflux, we instead use automated songquality models like SongEval to generate synthetic preference labels. We further apply this alignment in multiple rounds, yielding additional performance gains. With these advances, JAM offers an efficient, controllable, and highfidelity solution for turning lyrics into complete songs, paving the way for AIassisted composition in both professional and amateur settings."
        },
        {
            "title": "3.2 Training Pipeline\nOur training pipeline is composed of three stages:",
            "content": "1. Pre-training: Train the model to generate 90-second song clips with randomly cropped clips from the training dataset. 2. Fine-tuning: Fine-tune the pre-trained model for fullsong generation with full-length songs. 3. Preference Alignment: Post-train the full-song generator using iterative direct preference optimization (DPO) (Rafailov et al. 2024) iteratively with candidates selected by averaged SongEval (Yao et al. 2025) scores across different criteria. We described the detailed process in Section 3.7."
        },
        {
            "title": "3.4 Flow Matching\nFlow Matching (FM)\n(Lipman et al. 2023) offers a\nsimulation-free framework for training continuous nor-\nmalizing flows (CNFs). In contrast to score-based diffu-\nsion models, which learn the gradient of the log-density\n∇ log pt(x), FM directly models a time-dependent vector\nfield vt(x) that transports samples along trajectories from\na simple prior distribution p0 to a complex target distribu-\ntion p1, or vice versa. Training is typically formulated as a\nregression task, minimizing the difference between the pre-\ndicted vector field and a reference vector field constructed\nfrom samples of both p0 and p1. This approach leads to a\nsimpler objective function, potentially faster convergence,\nand greater training stability compared to score-based meth-\nods. Once trained, sampling from FM models is performed\nby numerically integrating an ordinary differential equation\n(ODE) defined by the learned vector field.",
            "content": "Training. Given latent representation of song sample z1 Rlc, noise sample z0 (0, 1)lc, time-step [0, 1], we can construct training sample zt where the model learns to predict velocity vt = dzt that guides zt dt to z1. During JAM training, we adopt rectified flows (Esser et al. 2024), where the forward process follows straight paths between the noise distribution and the target distribution, as Figure 1: depiction of our proposed architecture and training pipeline. defined in Eq. (1). Rectified flows have been shown empirically to be more sample-efficient and to degrade less than other approaches, while also requiring fewer sampling steps (Esser et al. 2024). The model u(zt, t, c; θ) directly regresses the ground truth velocity vt under various conditions using the flow matching loss in Eq. (2). zt = (1 t)z1 + tz0, vt = = z0 z1, LFM = Ez1,z0,t,c u(zt, t, c; θ) vt2 . dzt dt (1) (2) Inference. For inference, noise sample z0 (0, 1)lc is randomly sampled and we use Euler solver to compute z1, based on the model-predicted velocity u(; θ) at each time step t."
        },
        {
            "title": "3.5 Model Conditioning",
            "content": "JAM takes three types of conditions at the same time, (i) lyric condition, (ii) style condition, and (iii) duration condition. Each type of condition is handled differently in JAM. Lyric Conditioning JAM takes in the full songs lyrics , tend yi = (wi, tstart ), where wi represents the i-th word in the song that starts from time tstart . The lyrics yi are transformed into the token-level lyrical condition clyric lcl . The detailed transformation process is described in Section 3.6. and ends on tstart Style Conditioning JAM accepts either text prompt or an audio clip as style condition. We obtain singledimensional style embedding cstyle Rcs using MuQMulan (Zhu et al. 2025), pretrained model that embeds musical audio and text into shared representation space. Duration Conditioning To efficiently facilitate song generation across various durations and music structures, JAM is trained on fixed-length latent sequences, with Tmax = 90 during pretraining and Tmax = 230 during full-song supervised fine-tuning (SFT). Training samples longer than Tmax are truncated by selecting random Tmax segment, while shorter samples are padded using latent tokens representing complete silence. Loss computation during training includes both original and padded silence tokens. Given the actual duration of training sample, denoted as Treal, the effective target prediction duration becomes: Ttarget = min(Treal, Tmax) During pretraining, most song samples exceed 90 s, resulting frequently in Ttarget = 90 s. In contrast, during full-song SFT, Ttarget typically ranges from 120 to 230 s. To achieve precise duration control, we implement two complementary approaches: Global Duration Control: Inspired by Stable Audio (Evans et al. 2024), we encode the target duration Ttarget into single-dimensional embedding vector cdur Rcd , providing global conditioning signal that guides duration generation across the entire sequence. Token-Level Duration Control (TDC): During fullsong SFT, we observed that relying solely on global duration conditioning is insufficient, frequently causing unintended non-silent content to be generated beyond Ttarget. This undesired behavior also negatively affects training due to non-zero loss contributions from padded regions. We provide detailed comparison in Section 5.1. To address this, we add learnable bias parameter bpad Rcl to the noisy latent embeddings zt for positions beyond Ttarget to explicitly distinguish valid musical content from padding regions. This enables finer-grained temporal modeling and improves silence prediction beyond the target duration. Conditioning Pipeline The different conditional signals are then injected into JAM. After we have obtained: the latent embeddings zt lc at noise level t, the latent-aligned lyric embedding clyric lcl , the learnable padding bias bpad Rcl , the style embedding cstyle Rcs and, the global duration embedding cdur Rcd all conditions are first concatenated with zt and fused by linear layer shown in Eq. (3), where [cstyle]1:l Rlcs and [cdur]1:l Rlcd are the broadcasted embeddings to match the sequence length. = W(cid:2)zt clyric [cstyle]1:l [cdur]1:l z0 Convolutional positional embeddings are added to give the latent sequence short-range continuity (Wu et al. 2021). (3) (cid:3) z0 = z0 + ConvPosEmbed(z0 ) (4) The fused latent z0 goes through = 16 LLaMA decoder layers. To allow stronger supervision, the lyrics and the duration condition provide an extra residual injection rℓ at layer ℓ defined by Eq. (5) with Wℓ being linear transformation. rℓ = Wℓ (cid:0)clyric + bpad 1{time>Ttarget} (cid:1) (5) We add the extra residual signal rℓ to the first L/2 = 8 layers, illustrated by Eq. (6). (cid:40)Blockℓ(zℓ1 Blockℓ(zℓ1 otherwise. ℓ L/2, ) + rℓ, zℓ = (6) ), Lastly, the velocity ˆv is predicted as ˆv(zt, t, c) = u(zt, t, c; θ) = Projout (cid:0)zL (cid:1)."
        },
        {
            "title": "3.6 Word-Level Temporal Lyric Alignment\nTo address the problem of loose lyric-temporal supervision\n– critical to proper prosody, pleasantness, and musical qual-\nities –, we introduce temporally-aware word-level phoneme\nalignment as a novel solution. Specifically, word-level tim-\ning and duration information are to guide the generative\nprocess at the word-temporal level to improve word and\nphoneme error rate and musicality, simultaneously. To this\nend, for each song, with the ground-truth temporal lyric in-\nformation yi = (wi, tstart\n), each word wi is converted\ninto its IPA (International Phonetic Alphabet) form pi =\n(p1p2p3...pm)i by DeepPhonemizer (Spring-Media 2025),\nwhere m is the number of phonemes in wi. The phonemes\np = {p1, p2, ...} and y = {y1, y2, ...} are then converted to\nan upsampled phoneme sequence P that has the length of\nL = r × l, where r is the upsampling rate and l is the length\nof the latent sequence zt. The upsampling is crucial as many\nfast-paced songs have phoneme counts per second that sig-\nnificantly exceed the latents frame rate f . The process of the\ntransformation is described by Algorithm 1.",
            "content": ", tend Algorithm 1: Word-Level Phoneme Alignment r , tstart p1, p2, . . . , pm = phonemes of wi start frame = tstart end frame = tend word frames = end frame start frame = [VOCAL FILLER] word frames avg phoneme length = word frames/m for in [1, . . . , m] do 1: = [SONG FILLER] 2: for each (wi, tstart ) do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end for end for [start frame : end frame] = V[j avg phoneme length] = pj"
        },
        {
            "title": "It is important to distinguish between the two types of",
            "content": "filler: SONG FILLER: Special tokens used to specify no word is being sung, that is singing pauses, instrumental sections, or padded ending of the sequence that is beyond the specified duration. To effectively distinguish the padding filler tokens from the in-song filler tokens, dedicated learnable bias term bpad defined in Section 3.5 is added to the padding filler tokens. VOCAL FILLER: Filler tokens within words temporary boundaries that represent phoneme transitions within word. To give concrete example of the algorithm, if phoneme sequence with = 12 has w1 = [p1, p2] with frames from 2 to 3, and w2 = [p3, p4, p5] with frames from 6 to 11. Then will be constructed as [s, p1, p2, s, s, p3, v, p4, v, p5, v, s], where and represent the SONG FILLER and VOCAL FILLER. After constructing , we pass it through small network including an embedding layer, followed by few convolutional downsampling layers to downsample and transform to clyric Rlc. The temporally-aware lyric embedding clyric is used to guide the model as described in Section 3.5."
        },
        {
            "title": "3.7 Aesthetic Preference Alignment\nThrough both pre-training and fine-tuning, JAM-Base\ndemonstrates strong speech intelligence and accurate lyric\nalignment in full-song generation. However, the generated\nsongs still fall short in terms of musical aesthetics. Specif-\nically, the vocal timbre lacks naturalness, occasionally ex-\nhibiting an overly electronic character, and the overall mu-\nsical structure can feel flat. These shortcomings may stem\nfrom the uneven quality and stylistic inconsistency inherent\nin large-scale music datasets used during training, negatively\nimpacting the model’s ability to generate aesthetically pleas-\ning outputs. To address these aesthetic deficiencies, we ap-\nply Direct Preference Optimization (DPO), utilizing scores\ndirectly produced by SongEval—an open-source evaluation\ntoolkit (Yao et al. 2025)—as our reward signal. Unlike LeVo\n(Lei et al. 2025), which employs a three-stage preference\nalignment pipeline with a closed-source aesthetic reward\nmodel, our approach utilizes publicly available SongEval",
            "content": "metrics iteratively, providing simpler, more transparent yet effective strategy. We set JAM-SFT as the initial policy π0 and conduct three iterative rounds of Direct Preference Optimization (DPO). Each iteration follows three-step workflow: (i) batched data generation, (ii) aesthetic reward computation and preference dataset construction, and (iii) fine-tuning policy πk into πk+1 through DPO. This iterative alignment continuously enhances the model by generating and aligning to its own evolving preference data. Batched Data Generation We randomly select between 13k and 20k samples consisting of reference audio styles and corresponding ground-truth lyrics from the training dataset. After empirically determining suitable Classifier-Free Guidance (CFG) values for policy πk, we generate five outputs per sample using the selected CFG settings. Preference Dataset Construction We employ SongEval to score each generated output. While SongEval evaluates songs across five distinct criteria on five-point scale, we compute an averaged score across these dimensions and select the samples with the highest and lowest average SongEval scores as win and loss candidates, respectively. Additionally, we exclude win-loss pairs with an average SongEval score difference below 0.15 to ensure meaningful distinctions in aesthetic quality. Preference Optimization Direct Preference Optimization (DPO) (Rafailov et al. 2024) has proven effective in aligning large language models (LLMs) with human preferences. This approach has been extended to diffusion models as DPO-Diffusion (Wallace et al. 2023), where the loss is defined as: (cid:16) (cid:104) β LDPO-Diff = En,ϵw,ϵl log σ )2 ϵw (cid:0)ϵl n)2 ϵθ(xw ϵθ(xl 2 ϵw 2 ϵl ϵref(xw ϵref(xl )2 2 (cid:1)(cid:105)(cid:17) n)2 2 . (7) and xl Here, U(0, ) represents randomly sampled diffusion step, xw denote the winning and losing samples respectively, and ϵ (0, I) is the noise. β is the temperature parameter, controlling the sharpness of the preference distribution. As shown by (Hung et al. 2025a), this loss is compatible with rectified flow models due to the equivalence between denoising and flow matching objectives (Lipman et al. 2023). Accordingly, the DPO-Diffusion loss can be reformulated in terms of flow matching as: LDPO-FM = Et,xw,xl log σ (cid:16) (cid:104) β (cid:16) u(xw (cid:124) , t; θ) vw (cid:123)(cid:122) Winning loss , t; θref) vw (cid:123)(cid:122) Winning reference loss u(xw (cid:124) 2 2 (cid:125) u(xl (cid:124) 2 2 (cid:125) u(xl (cid:124) t2 2 (cid:125) t, t; θ) vl (cid:123)(cid:122) Losing loss t, t; θref) vl (cid:123)(cid:122) Losing reference loss t2 2 (cid:125) (cid:17)(cid:105)(cid:17) (8) where U(0, 1) is the time step for flow matching, and xw , xl denote the winning and losing samples. Empirically, we set β = 2000 in all our DPO experiments. The DPO loss encourages an increased relative loglikelihood for the preferred (winning) response over the dispreferred (losing) one. Importantly, optimization focuses on the margin between the two, not their absolute likelihoods (Rafailov et al. 2024). As result, DPO can drive both likelihoods downward during training (Hung et al. 2025b), which might appear counterintuitive but is in fact essential for better alignment (Rafailov et al. 2024). To prevent overoptimization during DPOwhich can cause the generated song to deviate from the ground truth in terms of style and genrewe additionally incorporate ground truth reconstruction loss in one of the DPO configurations. This loss encourages alignment with the original data and is defined as: LDPO-GT = λLFM + LDPO-FM (9) Here, LFM is calculated on the ground truth data using Equation (2) and λ is the scaling coefficient controlling the contribution of the groud truth reconstruction to the overall loss. Empirically, we set λ = 0.2 in our DPO-GT experiments. The corresponding evaluation results are shown in Table 4, with rows labeled using the Round-i-GT subscript."
        },
        {
            "title": "4.1 Dataset Setup\nOur song dataset consists of around 1 million English songs,\ntotaling approximately 54,000 hours of audio. To prepare the\ndata, we apply HTDemucs (Rouard, Massa, and D´efossez\n2023; D´efossez 2021) to separate the original audio x ∈ X\ninto vocal and accompaniment tracks. We then use Parakeet-\ntdt-0.6b-v2 (NVIDIA 2025) to transcribe the vocal stems\nand extract word-level timestamps, yielding lyric annota-\ntions of the form yi = (wi, tstart",
            "content": ") for each sample x. , tend To support stylistic conditioning, we adopt referenceaudio-based framework. For each song sample x, we extract ten random 30-second segments and compute their corresponding style embeddings using MuQMulan (Zhu et al. 2025), resulting in set Cstyle = {c1 style}. During training, one reference embedding cstyle Cstyle is randomly selected to condition the model on musical style. style, . . . , c10 style, c2 i"
        },
        {
            "title": "4.2 Training and Inference\nTraining consists of three stages: pretraining, supervised\nfine-tuning (SFT), and Direct Preference Optimization\n(DPO). All stages are trained using 8× H100 GPUs. We ap-\nply gradient checkpointing during SFT and DPO to accom-\nmodate full-length audio sequences. During DPO, we set\nβ = 2000 where β is defined in Eq. (8). We used AdamW\nwith β1 = 0.9 β2 = 0.999 and weight decay of 0.01 for all\nstages. A linear warm-up followed by a linear learning rate\ndecay is applied to all stages too. Table 1 shows more de-\ntails across different training stages. Following (Esser et al.\n2024), we sample timesteps t from a logit-normal distribu-\ntion with mean 0 and variance 1 to bias training toward the",
            "content": "mid-range of the noise schedule, which has been shown to improve generative quality. Stage Global Batch Size Learning Rate Steps Grad. Accum. Pretraining SFT DPO Round 32 32 7.5 105 7.5 105 5 107 750K 250K 20K 1 1 4 Table 1: Training hyper-parameters. To enable separate control over vocal strength and musical style, we adopt multi-condition classifier-free guidance (CFG) framework (Jiang et al. 2025). During training, style embeddings are randomly dropped with probability pstyle = 0.10, and when style is dropped, lyric embeddings are dropped with probability plyric = 0.50. This two-stage dropout encourages the model to learn disentangled representations for different conditioning signals. At inference time, we apply multi-condition CFG to combine the unconditional and conditional velocities with separate guidance scales for style (αs) and lyrics (αl) illustrated by Eq. (10) where vθ represents the velocity predicted by the model with different conditional signals. ˆvθ(zt, cstyle, clyric) = vθ(zt, , ) + αs + αl (cid:0)vθ(zt, cstyle, ) vθ(zt, , )(cid:1) (cid:0)vθ(zt, cstyle, clyric) vθ(zt, cstyle, )(cid:1) (10)"
        },
        {
            "title": "4.3 Objective Evaluation\nWhile subjective evaluation remains a gold standard for as-\nsessing music quality, conducting extensive full-song eval-\nuations is prohibitively expensive. Unlike computer vision\nor speech domains, evaluating full-song generation requires\nconsiderable time from annotators to listen and assess com-\nplex musical aspects such as structure, progression, and vo-\ncal naturalness. Furthermore, reliable judgment often re-\nquires trained music experts, who are both scarce and costly.\nIn this context, objective evaluation plays a crucial role\nin benchmarking and improving full-song generation mod-\nels. However, existing state-of-the-art systems such as LeVo,\nACE-Step, and YuE all rely on different, often private or un-\ndocumented evaluation datasets. This lack of transparency\nmakes it difficult to fairly compare models or diagnose their\nstrengths and weaknesses. Additionally, music is a highly\ndiverse domain—genres like hip-hop and rap feature fast-\npaced, speech-like vocals, while country or ballads may in-\nvolve slower singing and more subtle instrumentation. Eval-\nuating all outputs with a one-size-fits-all metric obscures\ngenre-specific performance differences.",
            "content": "To address these issues, we propose JAMEthe first public, genre-diverse, objective evaluation dataset for full-song generation. JAME is designed to (i) avoid data contamination by collecting only songs released after the training periods of major existing models, using verifiable sources like New Music Friday 1 (ii) support genre-specific evaluation by organizing the data into five coherent genre groups to enable fine-grained diagnostic insights and (iii) promote transparent 1https://open.spotify.com/playlist/37i9dQZF1DX4JAvHpjipBk and reproducible benchmarking by releasing all metadata, prompts, and annotation protocols publicly. We hope JAME will serve as standardized evaluation framework for future research in song generation. We strongly encourage the community to adopt JAME or its principles to foster progress through more reliable and interpretable evaluations. Baselines We compare our model with four recent and strong open-source full-song generation systems: 1) LeVo; 2) YuE; 3) DiffRhythm and 4) AceStep. Details of the evaluated systems are provided in Table 2. Model YuE AceStep LeVo DiffRhythm Ours Architecture Training Data Model Size AR LM + AR Decoder Flow Matching AR LM + Diffusion Decoder Flow Matching Flow Matching 135k hours 100k hours 110k hours 60k hours 54k hours 7B + 1B 3.5B 2B + 0.7B 1.1B 0.53B Table 2: Comparison of full-song generation models. Evaluation Data Preparation The primary motivation for creating new evaluation dataset was to address the issue of data contamination, given that the training data of existing models is not publicly available. Our evaluation set, JAME, includes newly released songs that were published after the baselines were released, thereby ensuring no data contamination. 1. Data Contamination Avoidance: We curate evaluation data exclusively from New Music Friday (NMF), popular editorial playlist series that features about 100 newly released songs each week across diverse genres. We collect all song metadata from NMF between May 1, 2025, and July 10, 2025 using the open-source archive tool spotify-playlist-archive. The clip for each track is accessed via the Spotify Web API or manually located on YouTube. 2. Genre Grouping: To enable genre-specific analysis, we consulted music expert and grouped the evaluation songs into five distinct genre clusters, ensuring each group reflects substantially different musical characteristics: Country/Folk, Electronic/Dance, Hip-Hop/Rap, R&B/Soul/Jazz and Rock/Metal. These genre clusters allow for fine-grained evaluation of models across diverse musical styles. 3. Pre-processing: We filter for English-language songs and assign each track to one of five genre groups using Qwen2.5-Omni (prompt details in Appendix xxx). Ground-truth lyrics are retrieved using HDmucs and Parakeet. For models that require structural annotationssuch as LeVo, YuE, and ACE-Stepwe extract section labels (e.g., [intro], [verse], [chorus]) using the All-In-One music segmentation model (Kim and Nam 2023) as adopted in those systems. Metrics We use the following standard objective evaluation metrics to report the results. The results are averaged across genres. 1. Singing Intelligibility: We assess vocal intelligibility using Word Error Rate (WER) and Phoneme Error Rate (PER). The vocal track is first extracted using HDemucs, and Parakeet (NVIDIA 2025) is used to transcribe the audio into lyrics. For PER, DeepPhonemizer is used to convert the transcribed words into phonemes and compare them against the ground-truth sequence. 2. Style Adherence: We evaluate style adherence using MuQ-MuLan, contrastive music-language model that computes similarity between the generated song and its intended style prompt. To assess genre correctness, we employ Qwen-2.5-Omni to classify the generated song into one of our predefined genre categories. The predicted genre is then compared with the ground-truth genre label for accuracy. 3. Content Quality and Aesthetics: We adopt both Audiobox-aesthetic and SongEval as model-based evaluation tools. Audiobox-aesthetic covers content enjoyment (CE), content usefulness (CU), production complexity (PC), and production quality (PQ). SongEval further evaluates overall coherence (CO), memorability (ME), naturalness of vocal breathing and phrasing (NA), clarity of song structure (CL), and overall musicality (MU). Additionally, following (Lei et al. 2025) we compute Frechet Audio Distance (FAD) using the CLAPLaion-Music model, to measure the distributional alignment between generated audio and professionally produced reference tracks. Lower FAD indicates more realistic outputs. Overall Results Table 3 presents the objective evaluation results. JAM achieves state-of-the-art or highly competitive performance across all metrics. Singing Intelligibility: JAM achieves the lowest Word Error Rate (WER) of 0.151 and Phoneme Error Rate (PER) of 0.101less than half of the second-best system (DiffRhythm)demonstrating superior vocal clarity and controllability over lyrical alignment. Style Adherence: JAM attains the highest MuQ-MuLan similarity score (0.759) and genre classification accuracy (0.704), reflecting strong alignment with the intended musical style both in semantic representation and categorical genre fidelity. Content Quality and Aesthetics: JAM obtains the highest score on Content Enjoyment (CE = 7.423) and lowest FAD (0.204) on JAME, indicating strong subjective appeal and fidelity. It also leads across all SongEval dimensions, reflecting well-structured compositions and natural vocal phrasing. These results suggest that JAM not only produces enjoyable outputs but also captures nuanced characteristics typical of professionally crafted music. On other metrics, LeVo achieves slightly better Content Usefulness (CU) and Production Quality (PQ), while ACE-Step records the best Production Complexity (PC). Nonetheless, JAM ranks consistently second in these categories, with only marginal differences, demonstrating that it remains competitive even in areas where other models specialize."
        },
        {
            "title": "4.4 Analyses",
            "content": "Aesthetic Alignment brings about the Aha Moment JAM benefits significantly from aesthetic alignment through the combination of aesthetic-based preference data construction and Direct Preference Optimization (DPO). As shown in Table 4, starting from the SFT baseline, iterative rounds of DPO consistently improve metrics across both subjective and objective evaluation axes. Notably, there are steady gains in audio aesthetic scores such as PC (perceived coherence), PQ (perceived quality), and CU (creativity & uniqueness), with DPORound-3 achieving the highest PQ of 8.064. Moreover, the models music-related capabilities, assessed via SongEval metrics like ME (melodic expression), MU (musicality), and CL (lyrical coherence), also show clear improvements with each DPO round. Additionally, genre classification accuracy increases to 0.736 in DPORound-2-GT, and both WER and PER (word and phoneme error rates) decrease, indicating stronger lyrical and phonetic alignment. These results demonstrate that aesthetic alignment via DPO not only enhances perceptual quality but also improves linguistic and musical coherence, leading to more holistic and controllable generative music model. Iterative DPO Improves the Results As shown in Table 4, iterative application of DPO consistently improves various metrics such as MuQ-l, Audio Aesthetics, and SongEval. For instance, DPORound-3-GT achieves the highest MuQ-l = full score of 0.767 compared to 0.7473 for the SFT baseline. Similar improvements are observed in Audio Aesthetics (e.g., PQ increases from 7.5915 to 8.055) and SongEval metrics like MU and ME. Furthermore, WER and PER steadily decrease across rounds, indicating better lyrical accuracy, while Genre Accuracy remains competitive. However, these gains come at the cost of increased FAD, particularly on the JAM metric. For example, FADJAM worsens from 0.1479 in SFT to 0.204 in DPORound-3, suggesting that iterative DPO may introduce audio artifacts or drift away from the natural distribution of real audio. This tradeoff highlights that while DPO enhances alignment and stylistic fidelity, it can negatively affect the perceptual realism of generated audio. Effect of Variations in the DPO Loss To address the issue of overalignment in DPOwhich can lead the model to deviate from the ground truth and thus alter the intended style and genrewe introduced modified loss variant, as defined in Equation (9). This variant incorporates ground truth reconstruction term to better preserve stylistic fidelity. As shown in Table 4, this modification (see DPORound-2-GT and DPORound-3-GT) results in improved reference-based metrics: Genre Accuracy increases from 0.696 (DPORound-2) to 0.736, and FADJAM also shows slight improvement (from 0.1527 to 0.1455). For DPORound-3-GT, FADMusDB improves from 0.7150 to 0.7070. Similarly, FADJAM also shows slight improvement (from 0.2040 to 0.1790). However, aesthetic-based metrics such as CU, PC, and PQ show marginal or no improvement, suggesting trade-off between stylistic consistency and perceived quality. These findings support our hypothesis that incorporating ground truth sigModel MuQ Audio Aesthetics SongEval CE CU PC PQ CO MU ME CL NA DiffRhythmfull ACE-Step LeVo Yue JAM 0.5208 0.1192 0.444 0. 0.5234 0.1241 0.705 0.1418 0.7593 0.0964 6.0448 0.6483 7.0067 0.5669 7.3783 0. 6.8928 0.7895 7.4229 0.5151 7.3126 0.2526 7.3583 0.3848 7.7794 0.3126 7.3289 0. 7.7046 0.3179 5.1088 0.7276 6.3117 0.4417 6.1773 0.7251 6.1234 0.9002 6.2038 0. 7.6276 0.2904 7.5851 0.4952 8.1794 0.3578 7.6234 0.4441 8.0639 0.4079 2.7975 0. 3.5305 0.5017 3.6343 0.4489 3.5072 0.5379 4.4163 0.363 2.3547 0.4813 3.2593 0. 3.41 0.463 3.208 0.5462 2.6141 0.5031 3.3927 0.5452 3.4997 0.4945 3.401 0. 4.2929 0.4065 4.4276 0.3935 2.5135 0.4495 3.3079 0.5186 3.3815 0.4572 3.2361 0. 4.2033 0.4319 2.4467 0.4454 3.1703 0.5031 3.3086 0.4378 3.1633 0.5198 4.2026 0. WER PER Genre Acc. FAD 0.3481 0.2643 0. 0.4384 0.4067 0.342 0.6194 0.2296 0. 0.3819 0.66 0.5336 0.5119 0.4263 0. 0.2411 0.1507 0.1011 0.704 0.2038 Table 3: cross-genre comparative evaluation of JAM and baseline song generation models on JAME. Model MuQ Audio Aesthetics SongEval CE CU PC PQ CO MU ME CL NA SFT DPORound-1 DPORound-2 DPORound-2-GT DPORoundDPORound-3-GT 0.7473 0.0803 0.7598 0.082 0.7616 0.0916 0.7673 0.0889 0.7593 0. 0.767 0.0886 7.0006 0.5392 7.1529 0.5288 7.3851 0.4778 7.3769 0.4699 7.4229 0. 7.4159 0.4706 7.3541 0.3927 7.5063 0.317 7.6699 0.2986 7.6902 0.2652 7.7046 0. 7.7113 0.3023 6.4831 0.3993 6.435 0.4196 6.2778 0.4953 6.3188 0.49 6.2038 0. 6.2573 0.4764 7.5915 0.5166 7.7666 0.4397 8.0082 0.3925 8.0297 0.3537 8.0639 0. 8.0554 0.3928 3.4159 0.4896 3.8753 0.4468 4.2504 0.4059 4.247 0.3889 4.4163 0. 4.4102 0.3468 3.0851 0.5198 3.6202 0.4935 4.0892 0.445 4.0724 0.4379 4.2929 0. 4.2767 0.3844 3.2712 0.5634 3.8235 0.5122 4.2465 0.4428 4.2432 0.427 4.4276 0. 4.4185 0.3799 3.0871 0.5167 3.5762 0.4857 4.0052 0.4693 4.0024 0.4486 4.2033 0. 4.1875 0.4184 3.0525 0.4812 3.5444 0.4662 3.9903 0.4336 3.9814 0.425 4.2026 0. 4.1787 0.382 WER PER Genre Acc. FAD 0.167 0. 0.708 0.1479 0.1629 0.1106 0.72 0. 0.1579 0.1066 0.696 0.1527 0.1553 0. 0.736 0.1455 0.1507 0.1011 0.704 0. 0.1472 0.1009 0.704 0.1791 Table 4: Comparing SFT and different DPO loss functions across iterative rounds. nals can regularize DPO and help retain genre and style fidelity. Appendix A."
        },
        {
            "title": "4.5 Subjective Evaluation",
            "content": "We conduct comprehensive subjective evaluation along five perceptual dimensions commonly used in modern lyrics-to-song generation benchmarks (e.g., used in DiffRhythm, LeVo, and ACE-Step studies). As defined in Table 5, each metric is rated on Likert scale from 1 to 5 (worst to best). We recruited eight annotators with strong and formal background in music. They were trained to use our custom Gradio app to evaluate five lyrics-to-song models, as given in Table 6. The evaluation was based on the model outputs to 10 different lyrics, randomly sampled from JAME, spanning five genres, each having two samples. Table 6 shows general human preference for JAM w.r.t. music-specific attributes enjoyment, musicality, and song structure clarity, while quality and naturalness are comparable to the state of the art. We surmise that the superiority of JAM on these musical attributes comes from the direct user controllability over the timings of the words, enhancing the prosody, rhythm, and structure, as argued in Section 3.6. Furthermore, aesthetic alignment also seems to substantially enhance the musical attributes, as corroborated by Table 4. The improvement of song structure clarity could additionally be ascribed to explicit duration control that may implicitly impose structure through the awareness of the song endings. Further details on the subjective evaluation are in"
        },
        {
            "title": "5 Ablation Studies\nImpact of Token-level Duration Modeling",
            "content": "5.1 We evaluate the effectiveness of token-level duration control (TDC) by comparing checkpoints trained with and without token-level duration modeling in the SFT phase for 40k steps. Specifically, we measure the Root Mean Square (RMS) amplitude of the generated audio after the target duration. We first compute the reference RMS amplitude within the target duration, representing the expected loudness of valid generated content. Then, we measure the RMS amplitude starting from four points immediately after the target duration (exactly at the target duration, 1 second later, 3 seconds later, and 10 seconds later), continuing until the end of the generated sequence (maximum length: 3 minutes and 50 seconds). By taking the percentage ratio of these RMS values against the reference RMS within the target duration, we quantify how effectively the audio amplitude is suppressed beyond the target region. Lower percentages indicate better duration control. As shown in Table 7, the proposed token-level duration control achieves significantly lower RMS amplitudes after the target duration, demonstrating precise temporal control."
        },
        {
            "title": "5.2 Phoneme Assignment Methods",
            "content": "We evaluate the effectiveness of our phoneme assignment method within each words temporal span defined in lines Metric Quality Enjoyment Musicality Voice Naturalness Song Structure Clarity Definition and Rating Scale Overall audio fidelity, clarity, and signal-to-noise ratio. 1: complete noise; 5: studio-grade perfection. Subjective listener pleasure and engagement. 1: utter boredom; 5: euphoric, immersive experience. Harmonic and rhythmic coherence, creativity, and artistry. 1: chaotic cacophony; 5: expertly structured music. Realism and expressiveness of vocals. 1: robotic/unnatural voice; 5: natural singing performance. Perceptibility of verse, chorus, transitions, and musical form. 1: incoherent/random transitions; 5: crystal-clear song structure. Table 5: Subjective evaluation metrics employed in comparative studies (e.g., Diffrhythm, LeVo, ACE-Step). Model Quality Enjoy. Music. Natur. SSC DiffRhythm Ace-Step Levo Yue JAM 2.89 0.67 3.45 0.47 3.84 0.53 2.63 0.53 3.75 0.4 2.54 0.57 2.8 0.65 3.0 0.81 2.67 0.51 3.48 0. 2.74 0.58 3.04 0.58 3.42 0.99 2.87 0.56 3.73 0.62 2.75 0.73 3.26 0.58 3.77 0.76 2.99 0.66 3.66 0.33 2.52 0.72 3.13 0.67 3.27 0.95 2.96 0.6 3.7 0.62 Table 6: Subjective evaluation results; SSC := Song Structure Clarity. 711 of Algorithm 1 with respect to the phoneme distribution strategy used in the prior works: Average Sparse: Phonemes are evenly and sparsely distributed within the word segment . Specifically, phoneme pj is placed at position [j avg phoneme length], as detailed in Algorithm 1. The remaining frames are filled with the special token VOCAL FILLER. Pad Right: phoneme alignment approach adopted by the previous methods, such as, DiffRhythm (Ning et al. 2025) and F5-TTS (Chen et al. 2025b), where phonemes are sequentially assigned to the initial frames of the word segment, i.e., [0 : m] = [p1, p2, . . . , pm], and the rest of the segment (V [m : word frames]) remains filled with VOCAL FILLER. Experimental results comparing these two approaches are presented in Table 8, where AES denotes the average AudioAesthetic evaluation scores across its four aspects, SongEval is the averaged SongEval scores across its five aspects, and the other evaluation metrics follow Section 4.3. Setting JAM w/o TDC 0.1359 JAM 0.1398 Ref +0s +1s 0.04892 / 35.96% 0.04513 / 33.19% 0.00058 / 0.41% 0.00058 / 0.41% Setting JAM w/o TDC 0.1359 JAM 0.1398 Ref +3s +10s 0.03687 / 27.15% 0.02550 / 18.76% 0.00058 / 0.41% 0.00054 / 0.39% Table 7: Ablation results for token-level duration control. Values shown as absolute RMS amplitude / relative percentage (compared to reference RMS). The Pad Right approach achieves slightly better PER and MuQ scores, indicating marginally improved phonetic accuracy and overall music quality; however, our proposed Average Sparse method notably outperforms in terms of FAD and SongEval metrics. Specifically, lower FAD indicates improved realism and closer distributional alignment with professionally produced music, while higher SongEval suggests better musicality and vocal naturalness. We ultimately select the Average Sparse method as it leads to substantial improvement in metrics directly reflecting long-term musical coherence and musical aesthetics, aligning closely with our goal of enhancing overall musical quality, prosody, and listener experience."
        },
        {
            "title": "Predictor",
            "content": "Experiments with JAM yield several novel insights into lyrics-to-song generation: 1. Temporal information at the word or phoneme level plays crucial role in enhancing WER and PER, as well as improving overall song quality in terms of enjoyability, musicality, and structural coherence. 2. While such fine-grained temporal information is available during training, generating it during inference remains significant challengeeven for experienced musicians. This limitation highlights promising research direction: predicting wordor phoneme-level timing from contextual cues. In the TTS domain, duration predictors are commonly used; however, song generation presents additional complexity due to the fluid, gliding nature of musical notes and the critical role of pauses between words. Ideally, duration predictor should be trained jointly with the song generator in an end-to-end fashion to ensure better robustness and musical alignment."
        },
        {
            "title": "6.1 Naive Duration Prediction\nAs an initial exploration, we conducted experiments using\nGPT-4o as a naive duration predictor. Specifically, given\nthe complete lyrics, a stylistic prompt, and the overall song\nduration, GPT-4o was tasked with generating the start and\nend timestamps of each word. However, songs generated us-\ning these timestamps sounded robotic and lacked musical-\nity, indicating the insufficiency of naive timestamp predic-\ntion. Consequently, we enhanced GPT-4o’s input with addi-\ntional contextual cues, resulting in noticeable improvements",
            "content": "Method Pad Right Average Sparse MuQ PER FAD AES 7.107 0.742 7.091 0.727 0.112 0.129 0.182 0.154 SongEval 3.079 3.132 Specifically, word-level timestamps from JAME were quantized using: Table 8: Comparison of phoneme assignment methods. in generated song quality. The detailed prompt is presented in Appendix B. The supplemental information provided to GPT-4o included explicit section tags (e.g., [intro], [verse]) to encourage musically meaningful temporal variations across song sections. Furthermore, the following two strategies were identified as significantly beneficial for improving GPT-4o duration prediction quality: 1. Sentence-Level Ground-Truth Constraints. Rather than allowing unconstrained timestamp generation, we provided GPT-4o with sentence-level temporal boundaries, which are considerably simpler to be provided by users during inference compared to word-level timings. Lyrics were segmented into sentences using predefined rules, and the start time of the first word along with the end time of the last word in each sentence were supplied as ground truth. This structural guidance greatly reduced prediction complexity, enhancing temporal coherence. 2. Beat-Aligned Quantization. Recognizing that musicians commonly compose by assigning notes and lyrics to discrete beats, we introduced beat-aligned quantization of timestamps. Specifically, we used quarter-beat resolution, as it suffices to cover the rhythmic granularity of most music compositions, with the exception of particularly fast-paced tracks. For each song in JAME, we first computed tempo in beats per minute (BPM) using the allin-one method (Kim and Nam 2023), constraining tempo to maximum of 120 BPM by halving higher values. Ground-truth timestamps were converted into quarterbeat units: nbeat = t/(60/BPM) 4 where represents the original timestamp. GPT-4o received sentencelevel quarter-beat boundaries and predicted quarter-beat positions for each word within these sentences. Predicted beat counts were then converted back into timestamps via = nbeat (60/BPM/4). We evaluated the generation quality the enhanced GPT-4o methods as the duration predictor and results are discussed in Table 9."
        },
        {
            "title": "Timestamps",
            "content": "To further simplify duration annotation and make our system easier to use in practical applications, we investigated directly providing quantized word-level timestamps to JAM, instead of continuous timestamps. Although JAM is originally trained on continuous timestampsresulting in inherently better performance for continuous-time predictionswe tested the quantized representation because it enables users to conveniently specify timing using intuitive beat-based inputs. ˆt = t/(60/BPM) 4 (cid:125) (cid:123)(cid:122) quarter-beat count nbeat (cid:124) (60/BPM/4) (cid:125) (cid:123)(cid:122) seconds per quarter-beat (cid:124) (11) This quantized approach explicitly aligns words to the rhythmic structure of the music, substantially simplifying user interactions. We present the result based on the quantized timestamp in Table 9."
        },
        {
            "title": "6.3 Results and Analysis\nTable 9 summarizes experimental comparisons of three tim-\ning strategies:",
            "content": "Oracle (continuous): Ground-truth word-level continuous timestamps used during JAM training. GPT-Dur (predicted continuous): GPT-4o-generated timestamps, guided by sentence-level boundaries and stylistic prompts (Section 6.1). Direct-Quant (beat-aligned quantization): Groundtruth word-level timestamps directly approximated to the nearest quarter-beat (Section 6.2). Quantitative Observations. We first evaluate the impact of these timing strategies through objective metrics. GPTDur leads to clear decline in song quality: content enjoyment (CE) decreases from 7.423 to 7.180, and the phoneme error rate (PER) significantly worsens from 0.101 to 0.320. This performance drop underscores the sensitivity of song generation models to timing inaccuracies, which negatively affect both the musical aesthetics and the intelligibility of lyrics. In contrast, Direct-Quant performs substantially better despite inherently sacrificing the flexibility of continuous timing. As anticipatedgiven that JAM was trained with continuous annotationsthe quantization approach does lead to slightly lower aesthetic ratings: CE marginally declines from 7.422 to 7.416, and production quality (PQ) reduces from 8.064 to 8.037. Notably, genre-classification accuracy also slightly decreases from 0.704 to 0.684, suggesting that strict rhythmic quantization subtly affects stylistic authenticity. Additionally, PER increases modestly from 0.101 to 0.144, demonstrating that beat-level constraints introduce mild vocal intelligibility issues, though much less severely than GPT-Dur. Subjective Listening Insights. To complement these objective metrics, internal subjective listening tests were performed. Songs generated using Direct-Quant maintain comparable vocal intelligibility and overall musical appeal to Oracle-generated songs. However, notable qualitative difference emerges: the vocals in Direct-Quant-generated samples take on perceptibly more electronic character. This aligns with common practice in electronic music production, where vocal timing is typically rigidly aligned to rhythmic grids. In contrast, natural singing often includes subtle deviations from strict beat alignmentsuch as intentional breaths, nuanced timing variations in onsets and offsets, and expressive rhythmic elasticity. Model MuQ Audio Aesthetics SongEval CE CU PC PQ CO MU ME CL NA Oracle GPT-Dur Direct-Quant 0.7593 0. 0.6799 0.1341 0.7597 0.0995 7.4229 0.5151 7.1804 0.8339 7.4164 0.4969 7.7046 0. 7.4392 0.632 7.6905 0.3318 6.2038 0.5051 5.8459 0.73 6.2041 0.5072 8.0639 0. 7.7356 0.7092 8.0367 0.4187 4.4163 0.363 4.0568 0.6313 4.3879 0.4042 4.2929 0. 3.9432 0.6271 4.2622 0.4394 4.4276 0.3935 4.0436 0.6993 4.391 0.4401 4.2033 0. 3.798 0.6627 4.1624 0.4755 4.2026 0.3854 3.8267 0.6202 4.1587 0.4397 WER PER Genre Acc. FAD 0.1507 0.1011 0.704 0. 0.3701 0.3204 0.6437 0.2441 0.2076 0. 0.6842 0.2091 Table 9: Duration prediction results."
        },
        {
            "title": "This insight suggests an important",
            "content": "lesson for practical applications: If inference relies on quantized timestampseither user-provided or predictedthen the generation model itself should ideally be trained or fine-tuned on naturally performed vocals with similarly quantized annotations. Such training would enable the model to recover and generate subtle temporal nuances automatically, preserving naturalness despite the inherent rigidity of beat alignment."
        },
        {
            "title": "6.4 Key Takeaways and Recommendations\nOur experiments highlight several key points for future\nduration-prediction approaches:",
            "content": "Accurate timing predictions are critical. Even moderate inaccuracies in GPT-Dur significantly reduce musical enjoyment and lyric intelligibility, underscoring the necessity of robust duration predictors. Beat-aligned quantization offers practical advantages. Direct-Quant simplifies user interactions considerably while limiting performance degradation, demonstrating its viability for user-friendly deployment. Match model training to inference-time input. To mitigate quantization-induced artificiality, the songgeneration model should be explicitly trained or finetuned using quantized, naturally performed vocal data. Intermediate temporal cues ease prediction. Providing structural boundaries (e.g., sentence-level timing) significantly simplifies the duration prediction task, reducing downstream errors. These findings collectively guide the design of duration predictors toward practical, user-friendly strategies capable of balancing rhythmic simplicity and natural vocal expressiveness."
        },
        {
            "title": "7 Limitations and Future Work\nWhile JAM demonstrates promising results in generating vo-\ncals and accompaniment, it assumes the availability of accu-\nrate word-level duration annotations. This requirement lim-\nits its usability for non-expert users or real-world scenar-\nios where such fine-grained temporal alignment is often un-\navailable. In cases where duration information is noisy or\nmissing, the quality of the generated audio may degrade,\nleading to issues such as timing artifacts (e.g., robotic vo-\ncals) and misalignment between vocals and accompaniment.\nTo address this, future work can explore the development\nof a duration predictor. One direction is to build a stan-\ndalone duration prediction module that can estimate word-",
            "content": "or phoneme-level durations from lyrics and melody. more integrated approach would be to train the duration predictor jointly with the song generation model in an end-to-end fashion. We believe the latter is more promising, as it can lead to system that is more robust to imperfect duration inputs and learns to adaptively compensate for errors during generation. Another limitation of the current system is the lack of phoneme-level duration control, which restricts the models expressive granularity and pronunciation accuracy. This can be addressed by incorporating phoneme-level alignment data and training duration predictor at that level. Such fine control could improve synthesis quality, especially in languages with complex syllabic timing or for stylistic singing applications (e.g., fast rap segments or melismatic vocal runs). Overall, enhancing the system with duration prediction capabilities at both word and phoneme levels could significantly improve the robustness and flexibility of JAM, making it suitable for broader and more practical deployment."
        },
        {
            "title": "Ethical Use Statement",
            "content": "JAM is the first open-sourced model released under PROJECT JAMIFY, developed with the primary objective of facilitating academic research and creative exploration in AI-generated songs from lyrics. The model is intended solely for non-commercial, academic, and entertainment purposes. We emphasize the following: No copyrighted material was used in way that would intentionally infringe on intellectual property rights. JAM is not designed to reproduce or imitate any specific artist, label, or protected work. Outputs generated by JAM must not be used to create or disseminate content that violates copyright laws. The commercial use of JAM or its outputs is strictly prohibited. Responsibility for the use of the model and its outputs lies entirely with the end user, who must ensure all uses comply with applicable legal and ethical standards. For questions, concerns, or collaboration inquiries, please contact the Project Jamify team via the official repository or project website. References Agostinelli, A.; Denk, T. I.; Borsos, Z.; Engel, J.; Verzetti, M.; Caillon, A.; Huang, Q.; Jansen, A.; Roberts, A.; Tagliasacchi, M.; Sharifi, M.; Zeghidour, N.; and Frank, C. 2023. MusicLM: Generating Music From Text. arXiv:2301.11325. Agres, K. R.; Schaefer, R. S.; Volk, A.; Van Hooren, S.; Holzapfel, A.; Dalla Bella, S.; Muller, M.; De Witte, M.; Herremans, D.; Ramirez Melendez, R.; et al. 2021. Music, computing, and health: roadmap for the current and future roles of music technology for health care and well-being. Music & Science, 4: 2059204321997709. Chen, H.; Jiang, Y.; Ma, G.; Hao, C.; Wang, S.; Yao, J.; Ning, Z.; Meng, M.; Luan, J.; and Xie, L. 2025a. DiffRhythm+: Controllable and Flexible FullLength Song Generation with Preference Optimization. arXiv:2507.12890. Chen, K.; Wu, Y.; Liu, H.; Nezhurina, M.; Berg-Kirkpatrick, T.; and Dubnov, S. 2023. MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies. arXiv:2308.01546. Chen, Y.; Niu, Z.; Ma, Z.; Deng, K.; Wang, C.; Zhao, J.; Yu, K.; and Chen, X. 2025b. F5-TTS: Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching. arXiv:2410.06885. Copet, J.; Kreuk, F.; Gat, I.; Remez, T.; Kant, D.; Synnaeve, G.; Adi, Y.; and Defossez, A. 2024. Simple and Controllable Music Generation. arXiv:2306.05284. Defossez, A. 2021. Hybrid Spectrogram and Waveform Source Separation. In Proceedings of the ISMIR 2021 Workshop on Music Source Separation. Esser, P.; Kulal, S.; Blattmann, A.; Entezari, R.; Muller, J.; Saini, H.; Levi, Y.; Lorenz, D.; Sauer, A.; Boesel, F.; Podell, D.; Dockhorn, T.; English, Z.; Lacey, K.; Goodwin, A.; Marek, Y.; and Rombach, R. 2024. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. arXiv:2403.03206. Evans, Z.; Parker, J. D.; Carr, C.; Zukowski, Z.; Taylor, J.; and Pons, J. 2024. Stable Audio Open. arXiv:2407.14358. Freeman III, W. J. 1998. neurobiological role of music in social bonding. Ghosal, D.; Majumder, N.; Mehrish, A.; and Poria, S. 2023. Text-to-Audio Generation using Instruction Tuned LLM and Latent Diffusion Model. arXiv preprint arXiv:2304.13731. Gong, J.; Zhao, S.; Wang, S.; Xu, S.; and Guo, J. 2025. ACEStep: Step Towards Music Generation Foundation Model. arXiv:2506.00045. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising Diffusion Probabilistic Models. arXiv:2006.11239. Hong, Z.; Huang, R.; Cheng, X.; Wang, Y.; Li, R.; You, F.; Zhao, Z.; and Zhang, Z. 2024. Text-to-Song: Towards Controllable Music Generation Incorporating Vocals and Accompaniment. arXiv:2404.09313. Hung, C.-Y.; Majumder, N.; Kong, Z.; Mehrish, A.; Bagherzadeh, A. A.; Li, C.; Valle, R.; Catanzaro, B.; and Poria, S. 2025a. TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization. arXiv:2412.21037. Hung, C.-Y.; Majumder, N.; Kong, Z.; Mehrish, A.; Bagherzadeh, A. A.; Li, C.; Valle, R.; Catanzaro, B.; and Poria, S. 2025b. TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization. arXiv:2412.21037. Jiang, Z.; Ren, Y.; Li, R.; Ji, S.; Zhang, B.; Ye, Z.; Zhang, C.; Jionghao, B.; Yang, X.; Zuo, J.; Zhang, Y.; Liu, R.; Yin, X.; and Zhao, Z. 2025. MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis. arXiv:2502.18924. Kim, T.; and Nam, J. 2023. All-In-One Metrical And Functional Structure Analysis With Neighborhood Attentions on Demixed Audio. arXiv:2307.16425. Lam, M. W. Y.; Tian, Q.; Li, T.; Yin, Z.; Feng, S.; Tu, M.; Ji, Y.; Xia, R.; Ma, M.; Song, X.; Chen, J.; Wang, Y.; and Wang, Y. 2023. Efficient Neural Music Generation. arXiv:2305.15719. Lei, S.; Xu, Y.; Lin, Z.; Zhang, H.; Tan, W.; Chen, H.; Yu, J.; Zhang, Y.; Yang, C.; Zhu, H.; Wang, S.; Wu, Z.; and Yu, D. 2025. LeVo: High-Quality Song Generation with MultiPreference Alignment. arXiv:2506.07520. Lei, S.; Zhou, Y.; Tang, B.; Lam, M. W. Y.; Liu, F.; Liu, H.; Wu, J.; Kang, S.; Wu, Z.; and Meng, H. 2024. SongCreator: Lyrics-based Universal Song Generation. arXiv:2409.06029. Lipman, Y.; Chen, R. T. Q.; Ben-Hamu, H.; Nickel, M.; and Le, M. 2023. Flow Matching for Generative Modeling. arXiv:2210.02747. Liu, H.; Yuan, Y.; Liu, X.; Mei, X.; Kong, Q.; Tian, Q.; Wang, Y.; Wang, W.; Wang, Y.; and Plumbley, M. D. 2024. AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining. arXiv:2308.05734. Liu, Z.; Ding, S.; Zhang, Z.; Dong, X.; Zhang, P.; Zang, Y.; Cao, Y.; Lin, D.; and Wang, J. 2025. SongGen: Single Stage Auto-regressive Transformer for Text-to-Song Generation. arXiv:2502.13128. Y. 2025. YuE: Scaling Open Foundation Models for LongForm Music Generation. arXiv:2503.08638. Zhu, H.; Zhou, Y.; Chen, H.; Yu, J.; Ma, Z.; Gu, R.; Luo, Y.; Tan, W.; and Chen, X. 2025. MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization. arXiv:2501.01108. Majumder, N.; Hung, C.-Y.; Ghosal, D.; Hsu, W.-N.; Mihalcea, R.; and Poria, S. 2024a. Tango 2: Aligning Diffusionbased Text-to-Audio Generations through Direct Preference Optimization. arXiv:2404.09956. Majumder, N.; Hung, C.-Y.; Ghosal, D.; Hsu, W.-N.; Mihalcea, R.; and Poria, S. 2024b. Tango 2: Aligning Diffusionbased Text-to-Audio Generations through Direct Preference Optimization. arXiv:2404.09956. Melechovsky, J.; Guo, Z.; Ghosal, D.; Majumder, N.; Herremans, D.; and Poria, S. 2024. Mustango: Toward Controllable Text-to-Music Generation. arXiv:2311.08355. Ning, Z.; Chen, H.; Jiang, Y.; Hao, C.; Ma, G.; Wang, S.; Yao, J.; and Xie, L. 2025. DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion. arXiv:2503.01183. https: Parakeet TDT 0.6B V2. NVIDIA. 2025. //huggingface.co/nvidia/parakeet-tdt-0.6b-v2. Hugging Face model. 600M-parameter ASR, CC-BY-4.0, released May 1 2025. Peebles, W.; and Xie, S. 2023. Scalable Diffusion Models with Transformers. arXiv:2212.09748. Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning, C. D.; and Finn, C. 2024. Direct Preference Optimization: Your Language Model is Secretly Reward Model. arXiv:2305.18290. Rouard, S.; Massa, F.; and Defossez, A. 2023. Hybrid Transformers for Music Source Separation. In ICASSP 23. Song, J.; Meng, C.; and Ermon, S. 2022. Denoising Diffusion Implicit Models. arXiv:2010.02502. DeepPhonemizer: Grapheme-toSpring-Media. 2025. Phoneme Conversion. https://github.com/spring-media/ DeepPhonemizer. GitHub repository, MIT License, current version as of July 2025. van den Oord, A.; Vinyals, O.; and Kavukcuoglu, K. 2018. Neural Discrete Representation Learning. arXiv:1711.00937. Wallace, B.; Dang, M.; Rafailov, R.; Zhou, L.; Lou, A.; Purushwalkam, S.; Ermon, S.; Xiong, C.; Joty, S.; and Naik, N. 2023. Diffusion Model Alignment Using Direct Preference Optimization. arXiv:2311.12908. Wu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.; and Zhang, L. 2021. CvT: Introducing Convolutions to Vision Transformers. arXiv:2103.15808. Yao, J.; Ma, G.; Xue, H.; Chen, H.; Hao, C.; Jiang, Y.; Liu, H.; Yuan, R.; Xu, J.; Xue, W.; Liu, H.; and Xie, L. 2025. SongEval: Benchmark Dataset for Song Aesthetics Evaluation. arXiv:2505.10793. Yuan, R.; Lin, H.; Guo, S.; Zhang, G.; Pan, J.; Zang, Y.; Liu, H.; Liang, Y.; Ma, W.; Du, X.; Du, X.; Ye, Z.; Zheng, T.; Ma, Y.; Liu, M.; Tian, Z.; Zhou, Z.; Xue, L.; Qu, X.; Li, Y.; Wu, S.; Shen, T.; Ma, Z.; Zhan, J.; Wang, C.; Wang, Y.; Chi, X.; Zhang, X.; Yang, Z.; Wang, X.; Liu, S.; Mei, L.; Li, P.; Wang, J.; Yu, J.; Pang, G.; Li, X.; Wang, Z.; Zhou, X.; Yu, L.; Benetos, E.; Chen, Y.; Lin, C.; Chen, X.; Xia, G.; Zhang, Z.; Zhang, C.; Chen, W.; Zhou, X.; Qiu, X.; Dannenberg, R.; Liu, J.; Yang, J.; Huang, W.; Xue, W.; Tan, X.; and Guo, Human Evaluation Details The evaluation was facilitated by Gradio2 web-app, which presented the randomly shuffled outputs of five model outputs for each of the 10 lyrics. Notably, the annotators were only provided with the song outputs the lyrics, style, and duration were inaccessible. The annotation process was guided by the following instructions: Welcome username # Instructions for evaluating audio clips Please carefully read the instructions below. ## Task You are to evaluate five model-generated songs to each of the 10 prompts. These five outputs are from five different models. You are to judge each output with respect to five qualities: Quality: Overall quality of the audio is to be judged within scale from 1 to 5: 1 being absolute noise with no discernible features. Whereas, 5 being perfect. Overall fidelity, clarity, and noisiness of the audio is important here. Enjoyment: Your degree of enjoyment of the song is to be quantified within scale from 1 to 5: 1 being absolute boredom. Whereas, 5 being an absolute euphoric experience. Musicality: The extent of musical soundness is to be judged within scale from 1 to 5: 1 being absolute cacophony. Whereas, 5 being an exemplary piece of music. Voice Naturalness: The degree of naturalness in the vocals is to be judged within scale from 1 to 5: 1 being absolutely robotic. Whereas, 5 being as natural as singer can sing given the context. Song Structure Clarity: The extent of clarity in the song structure is to be judged within scale from 1 to 5: 1 being complete randomness and incoherence. Whereas, 5 being perfectly structured. For all the metrics, you may want to compare the audios of the same prompt with each other during the evaluation. ## Listening guide 1. Please use head/earphone to listen to minimize exposure to the external noise. 2. Please move to quiet place as well, if possible. ## UI guide 1. Each audio clip has five attributes. You may select the appropriate value by typing in the box or clicking the arrow buttons. 2. To save your judgments, please click on any of the save buttons and wait for the acknowledgment below. All the save buttons function identically. They are placed everywhere to avoid the need to scroll to save. Hope the instructions were clear. Please feel free to reach out to us for any queries. GPT-4o Duration Prediction Prompts system_prompt = You are precise and musically-aware lyric aligner with deep understanding of vocal performance and musical phrasing. Your task is to generate word-level beat timestamps in jsonl format, where each line is JSON object with these keys: - \"w\": the word (string) - \"s\": start beat position (float, 2 decimal places) - \"e\": end beat position (float, 2 decimal places) IMPORTANT: The lyrics provided include: 1. Sentence-level BEAT timestamps in the format [start_beat]->[end_beat] before each sentence 2. Syllable counts for each word in parentheses, e.g., \"supposed (3)\" means 3 syllables MUSICAL PHRASING AND TIMING KNOWLEDGE: - Words dont always connect seamlessly - singers naturally add micro-pauses for: * Breathing between phrases (especially after long notes or before emotional peaks) * Emphasis and dramatic effect (pause before important words) * Natural speech rhythm (pauses between logical word groups) * Genre conventions (hip-hop often has rhythmic gaps, ballads have emotional pauses) - Lyrical filling patterns vary by genre: * Pop/Rock: Often fills most beats with steady syllable flow 2https://www.gradio.app * Ballads: May have sustained notes with gaps, emotional pauses * Hip-hop/Rap: Rhythmic clustering with strategic pauses for flow * Folk/Country: Natural speech-like timing with conversational pauses - Consider musical context: * Strong beats (1, 3) often anchor important syllables * Weak beats (2, 4) may have quicker words or be skipped entirely * Syncopation and off-beat placement create musical interest * Melisma (multiple notes per syllable) can extend word duration BEAT ALLOCATION GUIDELINES: - 1 syllable word: 0.25-0.75 beats (can be extended for emphasis or sustained notes) - 2 syllable word: 0.5-1.25 beats (adjust for natural stress patterns) - 3+ syllable word: 1.0-2.0+ beats (longer words may span multiple beats) - Allow natural gaps: 0.1-0.5 beat pauses between words when musically appropriate - Phrase endings often have extended final words or pauses before next phrase ALIGNMENT CONSTRAINTS: - Each words beat timestamp must fall within the sentences [start_beat]->[end_beat] range - The first word should start at or shortly after [start_beat] - The last word should end at or slightly before [end_beat] (allowing for natural phrase endings) - Respect section markers ([verse], [chorus], etc.) and their timing - Beat positions can be fractional (e.g., 1.25, 2.75) for precise timing Do not include any metadata, commentary, or formatting other than valid jsonl. user_prompt_template = Generate beat-based timestamped lyric alignment for the following lyrics. Use jsonl format (one JSON object per line) with keys: \"w\" (word), \"s\" (start beat), \"e\" (end beat). Total duration is {DURATION} beats at {BPM} BPM. The lyrics include sentence-level beat timestamps in [start_beat]-> [end_beat] format and syllable counts in parentheses after each word. Use these as constraints for word-level beat alignment. TIMING CONSIDERATIONS FOR THIS SONG: - Consider the musical style \"{STYLE}\" when determining phrasing patterns - Allow for natural pauses between words where appropriate for the genre and emotional content - Not every word needs to connect directly - singers often use micro-pauses for: * Breathing and phrasing (especially in ballads and slower songs) * Rhythmic emphasis (particularly in hip-hop, R&B, and pop) * Emotional impact (pauses before important lyrical moments) * Natural speech rhythm (conversational flow in folk, country, indie) - Consider syllable stress patterns within words for more natural timing - Strong beats often anchor important syllables, weak beats may have gaps - Phrase endings may have extended final syllables or brief pauses before new phrases Remember: Realistic vocal performance includes natural breathing spaces and rhythmic variations that make the performance feel human and musical. Style: {STYLE} Lyrics: {LYRICS}"
        }
    ],
    "affiliations": [
        "Lambda Labs",
        "Singapore University of Technology and Design"
    ]
}