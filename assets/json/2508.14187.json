{
    "paper_title": "Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer",
    "authors": [
        "Md Ashiqur Rahman",
        "Chiao-An Yang",
        "Michael N. Cheng",
        "Lim Jun Hao",
        "Jeremiah Jiang",
        "Teck-Yian Lim",
        "Raymond A. Yeh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scale variation is a fundamental challenge in computer vision. Objects of the same class can have different sizes, and their perceived size is further affected by the distance from the camera. These variations are local to the objects, i.e., different object sizes may change differently within the same image. To effectively handle scale variations, we present a deep equilibrium canonicalizer (DEC) to improve the local scale equivariance of a model. DEC can be easily incorporated into existing network architectures and can be adapted to a pre-trained model. Notably, we show that on the competitive ImageNet benchmark, DEC improves both model performance and local scale consistency across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our code is available at https://github.com/ashiq24/local-scale-equivariance."
        },
        {
            "title": "Start",
            "content": "Md Ashiqur Rahman1 Chiao-An Yang1 Michael N. Cheng1 Lim Jun Hao2 Jeremiah Jiang2 Teck-Yian Lim2 Raymond A. Yeh1 1Department of Computer Science, Purdue University 2DSO National Laboratories 5 2 0 2 9 1 ] . [ 1 7 8 1 4 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Scale variation is fundamental challenge in computer vision. Objects of the same class can have different sizes, and their perceived size is further affected by the distance from the camera. These variations are local to the objects, i.e., different object sizes may change differently within the same image. To effectively handle scale variations, we present deep equilibrium canonicalizer (DEC) to improve the local scale equivariance of model. DEC can be easily incorporated into existing network architectures and can be adapted to pre-trained model. Notably, we show that on the competitive ImageNet benchmark, DEC improves both model performance and local scale consistency across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our code is available at https://github.com/ ashiq24/local-scale-equivariance. 1. Introduction The perceived scale of an object in images varies due to multiple factors. First, intrinsic factors such as the objects physical dimensions cause variations in the apparent size, e.g., individuals may differ in height. Second, extrinsic factors such as camera zoom and the objects distance to the camera further affect the observed scale, even for the same object. How to effectively handle and represent this scale variation has been fundamental question in computer vision. Inspired by human vision, which uses hierarchy of scales for perception [42], earlier works adopted image pyramids [1, 12] and variants [34, 55, 65] to extract features at different image resolutions [21, 31, 39, 40]. In deep learning, ideas similar to image pyramids have been introduced [14, 20, 24, 76] and studied under the broad area of scale invariance/equivariance [48, 56, 57, 67]. These works aim to design specialized deep-nets such that the networks output is transformed in predefined way when the input image undergoes scale changes (image resizing). Mathematically, equivariance is formalized as equality statements over groups; see Sec. 3 for review. typical approach is to design group equivariant architecture [15] by modifying the Figure 1. Local scaling resizes different parts of an image independently, e.g., the dog is scaled while the background remains unchanged. For the Swin model [38], we show the distribution of predicted probabilities for the object class across set of locally scaled images. Observe that the predicted probabilities are not stable, indicating that the Swin model is not consistent across local scaling. We introduce Deep Equilibrium Canonicalizer (Ours) to address this issue. filter operations. Note, we use the term equivariance in the strict mathematical sense of the exact equality statement. We will use the term consistency to mean an approximation of equivariance, i.e., without the mathematical guarantees. While scale equivariance has been well studied, these works all focused on global scaling, i.e., changes in image resolution. Differently, we are interested in local scaling, e.g., the resizing of each part of an input image differently (at fixed resolution), which more closely matches the scaling of objects in the real world. See illustration in Fig. 1. Ideally, we aim to design deep-nets that are local scale equivariant. However, real-world local scaling does not form group as the operation is not invertible, e.g., when two objects change size and occlude each other. To address this, we propose an approximation of the real-world local scaling operation, namely monotone scaling, which we will show to be group. Our approach to making deep-net monotone scale equivariant is based on the framework of canonicalization [44, 46]. Briefly, canonicalizer transforms an input into standard (canonical) form so that the deep network can extract invariant features. canonicalizer can be modeled as finding stationary point of learned energy function with respect to the transformation (group) and then using that point as the standard. Instead of this optimization approach, we propose to utilize deep equilibrium model [4] for directly predicting this point, i.e., amortized optimization [2]. We name our approach the Deep Equilibrium Canonicalizer (DEC). We then incorporate DEC into the latent space of existing deep-net architectures to improve the local scale consistency. To evaluate local scale consistency, we first conduct experiments on semantic segmentation using synthetic data where we can control the scale variation. We create dataset of realistic renderings using physical simulation based on the Google Scan Objects [19]. Next, we consider the task of image classification. Inspired by the MNIST-scale dataset used in prior works [29, 48, 57], we create dataset of locally scaled MNIST images where each image contains multiple MNIST digits, resized to different scale. Finally, we evaluate locally scaled version of ImageNet [17]. Overall, our proposed DEC, when incorporated into existing models, achieves higher performance metric and further improves the local scale consistency over the existing base model. Our main contributions are as follows: Motivated by local scaling in the real world, we propose to study how to design deep-nets that are equivariant to monotone scaling. We propose DEC, novel latent canonicalization approach based on deep equilibrium models, which can be easily incorporated into existing deep-net architectures. We demonstrate that the proposed DEC improves model performance and local scale consistency on two vision tasks, notably, the competitive benchmark of ImageNet across four different ViT architectures. 2. Related Works Scale consistency has long history in computer vision. Early works focus on developing methods that are invariant to scaling [40, 66] with techniques such as Laplacian image pyramids [11, 55] or wavelet analysis [62]. Later works aim to improve scale consistency by learning transformations applied either to the input images or directly to convolutional filters [16, 27, 52, 59] or by directly learning local/global scale estimation [8, 33]. More recently, variants of vision transformer models have been proposed to tackle scale consistency, e.g., incorporating the structure of image pyramids [22, 60]. While some of these works use the term invariance/equivariance, these approaches generally do not achieve the equality condition on group, i.e., they are not guaranteed to be scale equivariant in the strict sense. In the equivariant literature, there is line of work studying global scaling [48, 56, 57, 67, 69] through designing specialized architecture based on group theory. Group equivariant models. Designing group equivariant architectures has been explored due to their strong theoretical guarantees and robustness [13, 15, 41, 47, 72]. Their development has been extended to incorporate equivariance in rotation [63], shift [54, 75], permutation [23, 35, 36, 51, 70, 71, 74], and sampling [49, 53, 68]. Global scale equivariant architectures are primarily constructed by integrating over the scale space of images [67], through the use of steerable filter [56] or applying spectral-domain operations for anti-aliasing [48]. However, the current literature lacks equivariant methods for local scaling (as it does not form group) and only handles the global scaling operations. Equivariant via canonicalization. With the rise of pretrained models, there is growing interest in adapting existing models to be equivariant rather than developing entirely new equivariant architectures. This has led to the area of group equivariant adaptation, which primarily aims to standardize the input through the use of canonicalization module [44, 46] or by averaging over the group elements to achieve more stable output [9, 10]. However, these methods target discrete symmetry groups with few parameters, e.g., the 2D rotation group (SO(2)) characterized by single parameter. These approaches are limited when the size and dimensionality of the transformation group increases. Deep Equilibrium Models (DEQs) are class of implicit neural networks that compute their outputs by solving for the fixed point of parameterized transformation [4, 5]. Unlike traditional deep networks with predefined number of layers, DEQs effectively model an infinitely deep, weight-tied recurrent network, making them highly parameter-efficient. These models have been applied successfully in various applications, including optical flow estimation [6] and video landmark detection [43]. In this work, we found DEQs to be suitable architecture for modeling canonicalizer for adapting deep-nets to be equivariant. 3. Preliminaries We provide review of the definition of equivariance and invariance. For readability, we describe these concepts using one-dimensional functions. Next, we review the canonicalization framework for achieving equivariant models. Global scaling operation. The global scaling operation Ra on 1D function , is , where } defined as : [0, 1] { Ra[f ](x) = (a1x), (1) R+ is the scaling factor. Specifically, Ra with where > 1 is an upscaling operation, and < 1 is downscaling operation. Group and equivariance. group is mathematical structure consisting of set G, which captures the structure of symmetry transformations. The action of group element is represented by transformation T(.; g) as on any function T(f ; g) = (g1 x), (2) where g1 denotes the transformation of by the inverse element g1. In other words, the function is evaluated at the transformed input g1 x. Here, acts as the parameter of the transformation T. By definition, the transformation needs to satisfy the invertibility condition, T1(f ; g) = T(f ; g1), for any group element g. Next, neural network is said to be equivariant with commutes with the action of G, respect to the group if i.e., satisfying the following equality condition (T(f ; g)) = T( (f ); g). (3) That is, transforming the input similarly transforms the output. The neural network is invariant if (T(f ; g)) = (f ), (4) does not change the output. which means transforming the input by group element Learned canonicalization function. canonicalization function can modify is equivG is designed ariant. The canonicalization function : to be equivariant with respect to the group G, i.e., , ensuring the resulting U h(T(f ; g)) = h(f ) denotes group product. Using this canonih(f ), (5) G, where calization function, the adapted model (T1(f ; g)); g), (f ) T( is defined as: (6) where h(f ). In particular, the canonicalization module maps all possible transformed versions of function to canonical input represented as T1(f ; g). The canonicalization function can be modeled directly by group equivariant networks [28, 44]. In the absence of an equivariant model, an alternative approach is to formulate the canonicalizer as an optimization problem [41, 46] h(f ) arg mingGE(T1(f ; g)). (7) Here, : eled with neural network. is learnable energy function, e.g., mod4. Latent Deep Equilibrium Canonicalizer In Sec. 4.1, we formulate the monotone scaling group to approximate the local scaling transformation. Next, we introduce our DEC module in Sec. 4.2 for effective canonicalization on given module. Lastly, we propose latent canonicalization for adapting pretrained models in Sec. 4.3. 4.1. Local Scaling (a) Global scaling with = 2.0. (b) Globally scaled Ra[f ](x). (c) Local scaling with l1(x). (d) Locally scaled S(f ; l)(x). Figure 2. Comparison of global and local scaling. (a-b) In global scaling, all locations of (x) are scaled by constant factor. (c-d) In local scaling, each segment of (x) is scaled differently. regions in an image have different scaling factors. To approximate such transformations, we propose monotone scaling operations. For readability, the idea is first discussed in 1D and then extended to 2D for images. Monotone scaling operation. To apply different scaling factors at different parts of the domain, we propose to perform the scaling by function : [0, 1] [0, 1]. monotone scaling is defined as follows S(f ; l)(x) (l1(x)), (8) where l1 is the inverse function of and the local scaling factor at is dl dt . To preserve the smoothness and invertibility of this transformation, we restrict the functions to be strictly monotonic, increasing, and continuous. In Fig. 2, we provide comparison between the global and our local scaling operation. Monotone scaling group. Denote the set of all continuous and strictly monotonic increasing functions as L. The set forms group structure, where the group product . is represented as the function composition as l2) (x) = (l (l1 l2)(x) = l1(l2(x)). (9) We formally state this in Lemma 1. Lemma 1. The set of all continuous strictly monotonic increasing functions is group under the binary operation of function composition. Proof. We provide the proof of this lemma in Sec. A3.1. In contrast to the global scaling (Eq. (1)), scalings in realworld image are often local. As shown in Fig. 1, different Lemma 1 states that the proposed monotone scaling operation forms group. Hence, we can formally define equivFigure 3. Illustration of the proposed method. We formulate neural network as sequence of modules { Mk}k=1:K . We perform latent canonicalization (see Sec. 4.3) on the latent feature Fk of each intermediate module. The monotone scaling in the latent canonicalization, i.e., S1 and S, are controlled by the parameters Φk found by our DEC module Hk (see 4.2). Each Hk finds the optimal ˆΦk through an iterative fixed-point solver on the energy function Ek until convergence. ariance of w.r.t. the monotone scaling group as Equivariance : (S(I; l)) = S( (I); l) (10) and invariance as Invariance : (S(I; l)) = (I). (11) Parameterization of l. The family of possible functions in is very flexible. For simplicity, we consider parametric in the form of piecewise linear function where the domain [0, 1] is discretized uniformly into discrete points as . In this case, has the form x0 = 0, x1, . . . , xN = 1 } { ϕn xn ϕn1 xn1 { (x (12) xn1), ϕ0, . . . , ϕN lΦ(x) = ϕn1 + when Φ = are the parameters. We impose the restriction ϕn1 < ϕn to make the monotonic increase and further we impose ϕ0 = 0 and ϕN = 1 to make the function bijective. For the ease of notation, we denote the monotone scaling by lΦ as S(f ; Φ), by dropping the l, i.e., S(f ; Φ) S(f ; lΦ). } similar piecewise linear can be constructed for monoR by extendtone scaling operation to images : [0, 1]2 ing the definition of to the 2D domain, i.e., S(I; l)(x, y) = I(l1(x, y)), (13) where : [0, 1]2 [0, 1]2. We illustrate this monotone scaling operation in Fig. 4. More details are provided in Appendix A2.2. With the parameterization and monotone scaling group defined, we now discuss how to design an effective canonicalizer for this group. (a) (b) Figure 4. Scaling by monotone function : [0, 1]2 [0, 1]2. (a) The original image. (b) Scaled image. The warped grid visualizes the effect of the monotone scaling operation. 4.2. Deep Equilibrium Canonicalization (DEC) As reviewed in Sec. 3, one approach to formulate canonicalizer is by solving for stationary point, e.g., minimum of an energy function with respect to the group elements in Eq. (7). However, solving Eq. (7) per example and training it end-to-end is slow and memory intensive. Instead, we propose to leverage the idea of amortized optimization [2], i.e., we learn model to directly predict the solution. In our case, we choose this prediction model to be Deep equilibrium model to be used as the canonicalizer. Deep equilibrium model (DEQ) is type of implicit neural network where the output is expressed as the fixed/stationary point of learnable non-linear transformation , i.e., ˆz = (ˆz; I, ψ), (14) is the non-linear transformation with parameter ψ where and ˆz is the equilibrium hidden state. The equilibrium can be calculated implicitly by solving the equilibrium condition (z; I, ψ) = 0 (15) DEQ can be trained from end to end like any other differentiable module, where the backpropagation can be efficiently computed using implicit differentiation. DEC module. We propose to formulate the canonicalization module using DEQ. That is, we model the optimization . Specifiin Eq. (7) as fixed point of non-linear system cally, we treat Φ as the hidden state described in Eq. (14) and define the update as (16) (Φ(i+1); I, ψ) Φ(i) Φ(i)E(S1(I; Φ(i)); ψ). As shown in Fig. 3, our DEC finds the optimal monotonescale parameters ˆΦ via DEQ fixed-point iteration to canonicalize the input I. Claim 1. The optimal monotone-scale parameter, ˆΦ, of energy function obtained via gradient descent is fixed point of . Proof. We assume that both DEQ iteration and the gradient descent optimization (Eq. (7)) start from the same initialization Φ(0). Let Φ be fixed point of for the input I. Therefore it satisfies (Φ; I, ψ) Φ = Φ = ˆΦ ˆΦE(S1(I; Φ); ψ) ΦE(S1(I; Φ); ψ) = 0 (17) (18) (19) This implies that the gradient of the energy function E, ΦE, evaluated at point Φ is 0. Therefore, Φ corresponds to some stationary point. Consequently, Φ corresponds to local minimum and thus represents the optimal parameter returned by the gradient descent optimization in Eq. (7). H The main insight of our canonicalizer design is that, inin terms of the gradient of E, we distead of modeling using neural network with learnable paramrectly model eters ψ and use it as the canonicalization module. Given an image and monotone-scale parameter Φ(i) at step i, predict the next-step parameter Φ(i+1) based on inversely scaled image S1(I, Φ(i)). This process is repeated iteratively until convergence to the equilibrium point. The equilibrium point ˆΦ can be determined by using Anderson acceleration [3]. real-world local scaling. Therefore, we propose to canonicalize the latent features of an existing network. We demonstrate the process of latent canonicalization in Fig. 3. This is motivated by the hypothesis that the inductive bias of monotone scale equivariant features could better approximate an overall local scale equivariant deep-net. Canonicalizer on the latent features. deep-net be viewed as composition of multiple layers, i.e., can (I) = K 1(I). (20) Using this notation, we denote the latent feature maps Fk+1 k(Fk) where the input feature is the image, i.e., F1 = I. For each layer k, we use canonicalization to adapt it to following Eq. (6). Our overall adapted model with equivariance guarantee is composition of all adapted modules, i.e. For monotone scale equivariance, we define each adapted = 0. module as follows: (21) Eq (Fk) = S( k( Fk); Φk), where Fk = S1(Fk; Φk)). That is, we first apply monotone scaling S1 to each input feature of each layer, i.e., Fk, and subsequently apply its inverse to the output of k. This ensures that the latent feature is equivariant to monotone scaling. Similarly, for monotone scale invariance, we define the adapted modules as 5. Experiments Inv (Fk) = k( Fk). (22) For evaluation, we aim to benchmark the local scale consistency of existing architectures and verify that the proposed DEC can improve consistency while maintaining model performance. To accurately measure the local scale consistency, we create datasets of locally scaled images where we know the underlying local scale transformation applied to each image. Specifically, we considered semantic segmentation on dataset based on the Google Scan Objects [19] and image classification using the MNIST [32] dataset and the ImageNet [17] dataset. The implementation details are in the Appendix A1. More results are provided in the Appendix A5. 5.1. Semantic Segmentation With our modeled canonicalizer defined, the remaining question is how to incorporate it into an existing deep-net. 4.3. Latent Canonicalization Typically, canonicalization [41, 44, 46] methods apply the canonicalizer at the input of the deep-net. This approach is effective when the transformation of interest is fully represented by the group actions, such as 2D rotation. However, recall that monotone scaling is only an approximation of the Dataset setup. For evaluating equivariance, we construct locally scaled segmentation dataset by simulating realworld objects scale variations. To have precise control over the variations, we render objects from Google Scanned Objects [19] on randomly selected background in HDRI Haven [73]. For each image, we place two 3D objects and extract the ground-truth mask. To introduce the local scaling effects, we vary the relative distance of each object from the camera and enable Figure 5. Samples from locally scaled object segmentation dataset. Each object in an image is scaled independently. Method ViT [18] DINOv2 [45] Swin [38] mIoU EquE mIoU EquE mIoU EquE Aug Canon AugL Ours 82. 0.054 86.65 0.036 76.63 0.061 79.37 82.35 83. 0.062 0.054 0.052 83.94 87.63 88.56 0.050 0.037 0.035 76.94 79.05 80.86 0.064 0.058 0.055 Table 1. Results on local scale equivariance. For each method, we report the mIoU(%) and EquE across 3 architectures. independent scale changes for each object while keeping the background fixed. The dataset includes 100 distinct objects; see Fig. 5 for examples. We use 6,000 images for training 256. and 2,000 for testing with an image resolution of 256 Evaluation metric. To measure performance, we report the mean intersection over union (mIoU). To measure consistency, we propose monotone scale equivariance error (EquE) on the predicted masked, defined as 1 (cid:88) ID S( (I); Φ) 2 (S(I; Φ)) 2 (23) ΦΩ over dataset all possible monotone scale parameters. and Ω denotes uniform distribution over Intuitively, EquE can be thought of as soft definition, in the ℓ2 sense, of the monotone scale equivariance in Eq. (10). That is, an equivariant model will achieve an EquE of zero. Baselines. We consider the following alternative methods to encourage local scale consistency into model: Aug: We train the architecture with random monotone scaling data augmentation on the training set. Aug also serves as the starting weights for DEC and the other baselines. Canon: We discretize the local scaling parameter and consider set of 64 different parameters, including the identity transformation. We use 3-layer CNN to model the energy function in Eq. (7). AugL: As one of the metrics is the equivariance error, we further fine-tune Aug by using EquE in Eq. (23) as part of the loss to encourage local scale equivariance. The baselines and our DEC are applied to three deep-net architectures: ViT [18], Swin [38], and DINOv2 [45]. Figure 6. Samples from locally scaled MNIST. The task is to regress the number, i.e., 159, 605, and 549 are the groundtruth for each of the images. In other words, an image classification task of 1000 classes; from 0 to 999. Results. In Tab. 1, we compare our DEC with the equivariant adaptation baselines in semantic segmentation. We observe that ours achieves the highest mIoU and lowest EquE in all cases. Although Aug is effective in pursuing local scale equivariance, they are far from optimal. Comparison of Aug and ours shows that mere data augmentation with local scaling operations is not reliable enough to guarantee equivariance, nor does further guiding Aug with the Eq. (23) in AugL. In contrast, Canon degrades the performance of Aug in most cases. That is, complete canonicalization on the input image only induces deterioration to the model. This shows the importance of canonicalization on latent features and replacing gradient optimization with our DEC, i.e., solving fixed-point equation iteratively. Lastly, consistent improvements of ours on 3 distinct architectures demonstrate the generalizability of our model to different deep-nets. On average, ours outperforms the best baseline, i.e., AugL, in mIoU by 1.17%. 5.2. Image Classification MNIST Dataset setup. Inspired by the MNIST-scale dataset from scale equivariance [29, 48, 57], we construct locally scaled MNIST by creating 3-digit images from 000 to 999. Each digit is randomly resized by factor within [0.4, 2.0], allowing for scale variation of up to 5 times between different digits (see Fig. 6). The images have resolution of 224 224. This provides suitable setup to test the local scale invariance, where the task is to predict the 1,000 number classes. As this task is relatively easy, we consider the more difficult setting with limited training data. The training set consists of 6,000 samples, while the test set contains 50,000 samples. The train/test images of the locally scaled dataset are sampled from the train/test split of the MNIST, respectively. Evaluation Metric. We report the top-1 classification accuracy and the local scale invariance error InvE 1 (cid:88) (cid:88) DS ID IsSI (I) (Is) 2 2. (24) Method ResNet [25] ViT [18] DeiT [61] Swin [38] BEiT [7] DINOv2 [45] Acc InvE Acc InvE Acc InvE Acc InvE Acc InvE Acc InvE Aug 93.09 0.41 13.71 1.65 91.23 1.80 13.33 1.72 90.91 1.18 16.17 4.94 94.26 0.25 5.44 1.65 94.70 0.37 9.69 1.94 95.80 0.29 7.47 1.33 Canon InvL Ours 94.22 0.33 13.84 1.05 93.73 0.61 12.32 2.35 95.01 0.20 8.17 2.60 94.06 0.38 93.67 0.73 13.47 2.01 94.79 0.42 5.04 3.37 96.46 0.14 6.13 2.01 95.64 0.15 96.09 0.16 7.04 0.70 95.67 0.19 4.27 1.86 95.92 0.29 5.07 1.65 96.63 0.43 6.00 3.12 8.04 2.88 94.78 0.25 3.89 0.62 95.26 0.41 6.49 0.78 96.66 0.10 5.21 1.34 6.02 2.71 96.91 0.12 2.08 1.09 97.05 0.26 4.13 1.70 97.90 0.21 2.62 1.69 Table 2. Results of local scale invariance on MNIST. For each method, we report the Acc(%) and InvE(102) across 6 architectures, with error bars computed over five runs. Scale [0.4, 1.0] [1.0, 2.0] [2.0, 3.0] Overall Aug Canon InvL Ours 91.64 90.16 91.98 95.14 96.32 95.39 96.26 97.81 93. 90.54 93.88 96.64 93.93 1.91 92.03 2.38 94.04 1.75 96.53 1.09 Table 3. Per-scale results on MNIST. We report the accuracy across various local scaling ranges using the Swin [38] architecture. consists of all variants of locally scaled I. IntuHere, itively, InvE can be thought as soft definition, in the ℓ2 sense, of the local scale invariance in Eq. (11). Baselines. We follow the same baselines of Aug and Canon from Sec. 5.1. Differently, as image classification should be invariant to local scaling, we instead have the baseline of InvL, which contains an additional loss of Inv = EID,ΦΩ (I) (S(I; Φ) 2 2, (25) to encourage monotone scale invariance. We evaluate these baselines on commonly used image classification architectures, including ResNet [25], ViT [18], DeiT [61], Swin [38], BEiT [7], and DINOv2 [45]. Results. In Tab. 2, we compare our DEC with the baselines in image classification on MNIST. We have observations similar to the results in Tab. 1. Aug and InvL are not optimal while AugL degrades the performance of some architectures, e.g. ResNet, ViT, and Swin. Ours achieves the highest Acc and lowest InvE, no matter the architectures. In particular, ours shows impressive results on local scale invariance and achieves significant improvement in InvE compared to Aug and other baselines. Specifically, ours reduces InvE from 18.08 to 9.85 on ResNet and from 6.30 to 1.31 on DINOv2. This substantiates DECs ability to achieve local scaling invariance. On average, ours improves 102. To further Aug on Acc by 4.10% and InvE by 8.91 demonstrate the broad applicability of our model, we provide an additional comparison against hierarchical models [22, 60] in Sec. A5.1. Analysis and ablation studies. In Tab. 3, we present the baseline results on Swin [38] across different local scale ranges. Specifically, we restrict the local scale range of each number digit in various scale brackets. We note that our method achieved higher accuracy in every scale range. Furthermore, by accessing different scale ranges, our method Figure 7. Samples from locally scaled ImageNet. For each image, we show the target object at two different scales. Note that the scale of the background is always unchanged. demonstrates the most consistent accuracy, reducing the std to 1.09 from 1.91. Additional ablations on DEC-related hyperparameters are provided in Sec. A5.2. We also compare the time and memory requirements of DEC modules with differentiable optimization-based canonicalization, i.e., Optim. We train both methods with batch 224. Optim takes 43.30 size of 10 and image sizes of 224 GB of GPU memory and spends 0.41s per iteration, while our DEC takes 5.75 GB and spends 0.19s. In other words, Optim requires more than 8 times the memory and twice the time than the DEC module. As discussed in Sec. 4.2, this makes it extremely difficult to apply Optim to deeper networks. We provide further discussion on runtime in Sec. A4. 5.3. Image Classification ImageNet Data setup. This experiment aims to benchmark local scale invariance in more realistic real-world setting, where we create locally scaled images using ImageNet [17]. As shown in Fig. 7, only the target object in each image is scaled, while the background is kept the same. In more detail, these images are created using the following procedure (see Fig. 4 for visualization): (a) We extract the bounding box of the target object by Grounding DINO [37] using the image labels. (b) With the bounding box, we extract fine object mask using SAM [30] and (c) extract the object. (d) We inpaint the object mas region using LAMA [58]. (e) We scale the extracted object and (f) place the scaled object back onto the inpainted background. The object of interest is scaled by factor within the range of [0.7, 1.3]. We fine-tune the pretrained models on training dataset containing 10,000 images. We use 50,000 testing (a) Original Image (b) Object Mask (c) Extracted Object (d) Inpainted Image (e) Scaling (1.2) (f) Locally Scaled Table 4. Generation procedure of locally scaled ImageNet. Given (a) an image of lizard, we first (b) obtain mask of the lizard and (c) extract the lizard from the image. We then (d) inpaint the region without the lizard to get the background. Finally, (e) we scale the lizard by 1.2 and (f) place it back onto the background to get the locally scaled image. Method ViT DeiT Swin BEiT Acc InvE Acc InvE Acc InvE Acc InvE Base Aug Canon InvL Ours 80.15 80.15 8.18 8.16 69.06 11.13 69.06 11. 77.94 77.96 8.93 8.92 77.72 10.20 8.16 80.16 8.10 80.36 64.79 12.74 69.06 11.12 69.27 11.08 74.67 11.40 8.92 77.97 8.82 78.32 84.70 84. 83.34 84.66 85.08 6.29 6.27 8.03 6.27 6.24 Table 5. Results of local scale invariance on ImageNet. For each method, we report the Top-1 Acc(%) and InvE(102) across 4 architectures. Method 0. 0.8 0.9 Local Scales 1.1 1.2 1.0 1. Overall Base Aug Canon InvL Ours 74.66 76.44 78.07 79.55 79.48 78.88 78.53 77.94 1.65 74.71 76.47 78.07 79.56 79.48 78.87 78.57 77.96 1.64 71.37 73.28 74.87 76.04 76.04 75.79 75.32 74.67 1.61 74.73 76.46 78.07 79.58 79.48 78.87 78.57 77.97 1.64 75.31 77.04 78.65 79.86 79.60 79.05 78.73 78.32 1.49 Table 6. Per-scale results on ImageNet. We report the Top-1 Acc(%) across various local scaling factors. Swin [38]. images for evaluation. Baselines. For ImageNet, we use open-source models from Pytorch Image Models [64]. We fine-tune the pretrained weights using their training script, which we denote as Base. All other baselines are initialized from Base. We consider four different architectures ViT [18], DeiT [61], Swin [38], and BEiT [7]. During fine-tuning, all baselines follow the best practices of data augmentation and fine-tuning strategy for vision transformers [26, 38], including techniques such as label-smoothing, cutmix, etc. Results. We present the ImageNet results in Tab. 5. We observe that our proposed DEC achieves the highest Acc with low InvE. Furthermore, we notice that the Canon is once again degrading the performance of the Base by distorting the input image. In Tab. 6, we report the performance of the baselines on Swin on different local scales. Our approach achieves the highest Acc on all local scales. Lastly, DEC produces the most consistent performance across different architectures, lowering the std of the Top-1 Acc to 1.49 from 1.65 of the Base model. Figure 8. Comparison on per-scale probability of correctness. We locally scale the same input image within the range of [0.7, 1.3] and report the probability of the correct class. Across all scales, our performance is 73.11 0.13 while Bases is 63.97 0.17. We visualize DECs consistency across multiple scales in Fig. 8. We show the probability of the correct class with the same image but locally scaled differently. We apply both methods to Swin [38]. Compared to Base, ours performs better on all scales and is more robust and invariant to challenging scenarios when the scale is extreme. In particular, when the input image is locally scaled by 0.7 , our method outperforms Base by 17.8%. 6. Conclusion In this work, we introduce the Deep Equilibrium Canonicalizer (DEC) to improve the local scale consistency of deep networks, tackling the fundamental challenge of scale variation in computer vision. We demonstrate that DEC can be adapted into existing deep-net architectures to improve both performance and scale consistency. Extensive experiments on three datasets and six architectures demonstrate its effectiveness. Notably, on the ImageNet dataset, DEC further improves the pre-trained models accuracy, even for the original unscaled images. Finally, we demonstrate the potential of DEQ as an effective canonicalizer for achieving equivariance and hope that this work inspires future DEC efforts toward other equivariances."
        },
        {
            "title": "References",
            "content": "[1] Edward Adelson, Charles Anderson, James Bergen, Peter Burt, and Joan Ogden. Pyramid methods in image processing. RCA engineer, 1984. 1 [2] Brandon Amos et al. Tutorial on amortized optimization. Foundations and Trends in Machine Learning, 2023. 2, 4 [3] Donald G. Anderson. Iterative procedures for nonlinear integral equations. Journal of the ACM, 1965. 5 [4] Shaojie Bai, Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In NeurIPS, 2019. 2 [5] Shaojie Bai, Vladlen Koltun, and Zico Kolter. Multiscale deep equilibrium models. In NeurIPS, 2020. 2 [6] Shaojie Bai, Zhengyang Geng, Yash Savani, and Zico Kolter. Deep equilibrium optical flow estimation. In CVPR, 2022. 2 [7] Hangbo Bao, Li Dong, and Furu Wei. BEiT: Bert pre-training of image transformers. In ICLR, 2022. 7, 8, 12 [8] Axel Barroso-Laguna, Yurun Tian, and Krystian Mikolajczyk. Scalenet: shallow architecture for scale estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1280812818, 2022. 2 [9] Sourya Basu, Prasanna Sattigeri, Karthikeyan Natesan Ramamurthy, Vijil Chenthamarakshan, Kush Varshney, Lav Varshney, and Payel Das. Equi-tuning: Group equivariant fine-tuning of pretrained models. In AAAI, 2023. 2 [10] Sourya Basu, Pulkit Katdare, Prasanna Sattigeri, Vijil Chenthamarakshan, Katherine Driggs-Campbell, Payel Das, and Lav Varshney. Efficient equivariant transfer learning from pretrained models. In NeurIPS, 2024. 2 [11] P. Burt and E. Adelson. The Laplacian pyramid as compact image code. IEEE Transactions on Communications, 1983. 2 [12] Peter Burt and Edward Adelson. The laplacian pyramid as compact image code. In Readings in computer vision. 1987. [13] Gabriele Cesa, Leon Lang, and Maurice Weiler. program to build E(N)-equivariant steerable CNNs. In ICLR, 2022. 2 [14] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE TPAMI, 2017. 1 [15] Taco Cohen and Max Welling. Group equivariant convolutional networks. In Proc. ICML, 2016. 1, 2 [16] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV, 2017. 2 [17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In CVPR, 2009. 2, 5, 7 [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 6, 7, 8, 12 and Vincent Vanhoucke. Google scanned objects: highquality dataset of 3d scanned household items. In ICRA, 2022. 2, [20] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proc. ICCV, 2021. 1 [21] K. Grauman and T. Darrell. The pyramid match kernel: discriminative classification with sets of image features. In Proc. ICCV, 2005. 1 [22] Jiaqi Gu, Hyoukjun Kwon, Dilin Wang, Wei Ye, Meng Li, YuHsin Chen, Liangzhen Lai, Vikas Chandra, and David Pan. Multi-scale high-resolution vision transformer for semantic segmentation. In CVPR, 2022. 2, 7, 13, 14 [23] Jason Hartford, Devon Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of interactions across sets. In Proc. ICML, 2018. 2 [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. IEEE TPAMI, 2015. 1 [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 7, 12 [26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 8, [27] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In NeurIPS, 2015. 2 [28] Sékou-Oumar Kaba, Arnab Kumar Mondal, Yan Zhang, Yoshua Bengio, and Siamak Ravanbakhsh. Equivariance with learned canonicalization functions. In ICML, 2023. 3 [29] Angjoo Kanazawa, Abhishek Sharma, and David Jacobs. Locally scale-invariant convolutional neural networks. arXiv preprint arXiv:1412.5104, 2014. 2, 6 [30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 7 [31] Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In Proc. CVPR, 2006. [32] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998. 5 [33] Jongmin Lee, Yoonwoo Jeong, and Minsu Cho. Selfsupervised learning of image scale and orientation. arXiv preprint arXiv:2206.07259, 2022. 2 [34] Tony Lindeberg. Scale-space theory in computer vision. 1993. 1 [35] Iou-Jen Liu, Raymond Yeh, and Alexander Schwing. Pic: permutation invariant critic for multi-agent deep reinforcement learning. In Proc. CORL, 2020. [36] Iou-Jen Liu, Zhongzheng Ren, Raymond Yeh, and Alexander Schwing. Semantic tracklets: An object-centric representation for visual multi-agent reinforcement learning. In Proc. IROS, 2021. 2 [19] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, [37] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection. In ECCV, 2024. 7 [54] Renan Rojas-Gomez, Teck-Yian Lim, Minh Do, and Raymond Yeh. Making vision transformers truly shiftequivariant. In CVPR, 2024. 2 [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 1, 6, 7, 8, [55] E.P. Simoncelli and W.T. Freeman. The steerable pyramid: flexible architecture for multi-scale derivative computation. In Proc. ICIP, 1995. 1, 2 [56] Ivan Sosnovik, Michał Szmaja, and Arnold Smeulders. Scale- [39] David Lowe. Object recognition from local scale-invariant equivariant steerable networks. In ICLR, 2020. 1, 2 features. In Proc. ICCV, 1999. 1 [40] David Lowe. Distinctive image features from scaleinvariant keypoints. IJCV, 2004. 1, 2 [41] George Ma, Yifei Wang, Derek Lim, Stefanie Jegelka, and Yisen Wang. canonicalization perspective on invariant and equivariant learning. In NeurIPS, 2024. 2, 3, 5 [42] David Marr. Vision: computational investigation into the human representation and processing of visual information. 1982. 1 [43] Paul Micaelli, Arash Vahdat, Hongxu Yin, Jan Kautz, and Pavlo Molchanov. Recurrence without recurrence: Stable video landmark detection with deep equilibrium models. In CVPR, 2023. 2 [44] Arnab Kumar Mondal, Siba Smarak Panigrahi, Oumar Kaba, Sai Rajeswar Mudumba, and Siamak Ravanbakhsh. Equivariant adaptation of large pretrained models. In NeurIPS, 2023. 1, 2, 3, 5 [45] Maxime Oquab, Théo Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. TMLR, 2024. 6, 7, [46] Siba Smarak Panigrahi and Arnab Kumar Mondal. Improved canonicalization for model agnostic equivariance. In CVPRW, 2024. 1, 2, 3, 5 [47] Omri Puny, Matan Atzmon, Heli Ben-Hamu, Ishan Misra, Aditya Grover, Edward Smith, and Yaron Lipman. Frame averaging for invariant and equivariant network design. In ICLR, 2022. 2 [48] Md Ashiqur Rahman and Raymond Yeh. Truly scaleequivariant deep nets with Fourier layers. In Proc. NeurIPS, 2024. 1, 2, 6 [49] Md Ashiqur Rahman and Raymond A. Yeh. Group downsampling with equivariant anti-aliasing. In The Thirteenth International Conference on Learning Representations, 2025. 2 [50] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, 2021. [51] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. In Proc. ICLR Deep learning with sets and point clouds. workshop, 2017. 2 [52] Adria Recasens, Petr Kellnhofer, Simon Stent, Wojciech Matusik, and Antonio Torralba. Learning to zoom: saliencybased sampling layer for neural networks. In ECCV, 2018. 2 [53] Renan Rojas-Gomez, Teck-Yian Lim, Alex Schwing, Minh Do, and Raymond Yeh. Learnable polyphase sampling for shift invariant and equivariant convolutional networks. In Proc. NeurIPS, 2022. 2 [57] Ivan Sosnovik, Artem Moskalev, and Arnold Smeulders. DISCO: accurate discrete scale convolutions. In Proc. BMVC, 2021. 1, 2, 6 [58] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with Fourier convolutions. In WACV, 2022. [59] Chittesh Thavamani, Mengtian Li, Nicolas Cebron, and Deva Ramanan. Fovea: Foveated image magnification for autonomous navigation. In ICCV, 2021. 2 [60] Rui Tian, Zuxuan Wu, Qi Dai, Han Hu, Yu Qiao, and YuGang Jiang. Resformer: Scaling vits with multi-resolution training. In CVPR, 2023. 2, 7, 13, 14 [61] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In ICML, 2021. 7, 8, 12 [62] Dimitri Van De Ville, Daniel Sage, Katarina Balac, and Michael Unser. The Marr wavelet pyramid and multiscale directional image analysis. In European Signal Processing Conference, 2008. 2 [63] Maurice Weiler, Fred Hamprecht, and Martin Storath. Learning steerable filters for rotation equivariant CNNs. In Proc. CVPR, 2018. 2 [64] Ross Wightman. https : //github.com/huggingface/pytorchimagemodels, 2019. 8 Pytorch image models. [65] Andrew Witkin, Demetri Terzopoulos, and Michael Kass. Signal matching through scale space. International Journal of Computer Vision, 1987. 1 [66] Andrew Witkin. Scale-space filtering. In Readings in computer vision. Elsevier, 1987. [67] Daniel Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. In Proc. NeurIPS, 2019. 1, 2 [68] Jin Xu, Hyunjik Kim, Thomas Rainforth, and Yee Teh. Group equivariant subsampling. In Proc. NeurIPS, 2021. 2 [69] Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang, and Zheng Zhang. Scale-invariant convolutional neural networks. arXiv preprint arXiv:1411.6369, 2014. 2 [70] Raymond Yeh, Yuan-Ting Hu, and Alexander Schwing. Chirality nets for human pose regression. In Proc. NeurIPS, 2019. 2 [71] Raymond Yeh, Alexander Schwing, Jonathan Huang, and Kevin Murphy. Diverse generation for multi-agent sports games. In Proc. CVPR, 2019. 2 [72] Raymond Yeh, Yuan-Ting Hu, Mark Hasegawa-Johnson, and Alexander Schwing. Equivariance discovery by learned parameter-sharing. In Proc. AISTATS, 2022. [73] Greg Zaal. Hdri haven: Free high-quality hdr images under cc0 license, 2020. Accessed via Poly Haven (formerly HDRI Haven). 5 [74] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep sets. In Proc. NeurIPS, 2017. 2 [75] Richard Zhang. Making convolutional networks shiftinvariant again. In Proc. ICML, 2019. 2 [76] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proc. CVPR, 2017."
        },
        {
            "title": "Appendix",
            "content": "The appendix is organized as follows: In Sec. A1, we provide additional implementation details of our method. In Sec. A2, we provide additional details of our monotone scaling group and its parameterization. In Sec. A3, we provide the detailed proofs for our lemmas in Sec. A4, we discuss the runtime of the DEC module. In Sec. A5, we provide additional analysis and results. Hyperparameter Value"
        },
        {
            "title": "Scheduler\nWeight decay\nWarmup epochs\nMixup Alpha\nLabel smoothing\nRandom Erase Prob\nLayer Decay Factor\nEpochs",
            "content": "Cosine 0.05 5 0.8 0.1 0.25 0.75 20 A1. Implementation details Table A1. ImageNet Finetuning Parameters DEC module. For all experiments, we model the DEC module as 2 layered CNN mapping with 64 and 128 channels. We perform adaptive pooling at the end to get the desired number of monotone scaling parameters. Vanilla Canonicalization. To train vanilla canonicalization, we discretized the monotone scale parameter into 64 different configurations for the locally scaled MNIST and object segmentation and 25 different configurations for locally scaled ImageNet. The learnable energy functions are implemented via 3 layer CNN. Locally Scaled Object Segmentation. The pretrained models ViT [18], Swin [38], and DINOv2 [45] are finetuned on the training set for 120 epochs. We set the initial learning rate at 1e 4 and scaled it by factor of 0.7 for each 30 epoch. We use DPT [50] style segmentation head. We use the per-pixel cross-entropy loss to train each model. We set the weight for the background pixel class to 0.1, and the weights of all other object classes are set to 1. All baselines are finetuned for 60 epochs with an initial learning rate of 2e Locally Scaled MNIST. Each architecture is trained for 4 for 50 epochs. We set the initial learning rate at 1e ResNet [25], ViT [18], DeiT [61], and BEiT [7]; 2e 5 for DINOv2 [45]; and 8e 4 for Swin [38]. All baselines are fine-tuned for 40 epochs. Locally Scaled ImageNet. All baselines are fine-tuned following standard data augmentation practice for ImageNet finetuning [26, 38]. We use batch size of 80 and an initial learning rate of 1e 7 with 5 warm-up epochs. We list common training hyperparameters in Tab. A1. 5 A2. Monotone scaling group A2.1. Group axioms Our monotone scaling set must meet the following axioms to be considered as group. Here, represents the group product described in Eq. (9). The 4 axioms are as follows: L, a, Closure: a, b, Associativity: Existence of Identity: b) = (b : c) = (a L, L, Existence of Inverse: L, a1 : a1 = e. A2.2. 2D Monotone scaling group Construction of monotone scaling group in 2D. To form the monotone scaling group on 2D, the set L2D : : [0, 1]2 } { should satisfy the two following properties: For all [0, 1]2 there is neighborhood [0, 1]2 [0, 1]2 L2D can be approximated by such that any function linear function with Jacobian Jl(x) 2 symmetric positive definite Jacobian. This condition imposes local monotonicity on l. SPD(2), i.e., 2 For all l1, l2 L2D, their local Jacobian Jl1 (x1) and Jl2 (x2) commutes for x1, x2 [0, 1]2. Formally, we state this in Lemma 2. Lemma 2. The set of all locally monotone increasing functions L2D with commutative Jacobian is valid group under the binary operation of function composition. Proof. We provide the detailed proof in Sec. A3.2. Parametrization of l. We parameterize through set of independent monotone functions along the and axes via bilinear interpolation. Specifically, we decompose the function into two functions as l(x, y) = (lX (x, y), lY (x, y)), (A26) where lX , lY : [0, 1]2 [0, 1]. We parameterize each of them as piecewise linear functions. To achieve this we discretize the domain [0, 1]2 into uniform grid y0 = x0 = 0, x1, . . . , xN = 1 where and { } { . We assume the set is ordered, i.e., 0, y1, . . . , yM = 1 } xi < xj when < j. = = = We define independent monotone functions along each row yj and each column xi of the grid as follows: [xn1, xn), the monotone function along row yj For is given by: lyj (x) = ϕyj xn1 + ϕyj xn xn ϕyj xn1 xn1 (x xn1). (A27) Similarly, for [ym1, ym), the monotone function along column xi is given by: lxi(y) = ϕxi ym1 + ϕxi ym ym ϕxi ym1 ym1 (y ym1) (A28) Here, ϕs are the learnable parameters and to preserve j, and monotonicity we impose restriction ϕyj ϕxi i, m. ym1 Finally, we obtain lX (x, y) from lyis via linear interpolaϕyj xn ϕxi ym xn1 tion as lX (x, y) = lyj1 (x) + lyj (x) lyj1 (x) yj1 (A29) [yj1 when yj). We defined lY (x, y) similarly. We approximate the inverse function l1 by computing the inverses of each lxi and lyj individually. A3. Complete Proofs of Lemmas and Claims A3.1. Proof of Lemma 1 Lemma 1. The set of all continuous strictly monotonic increasing functions is group under the binary operation of function composition. Proof. To prove that the set of all continuous strictly monotonic increasing functions forms group under function composition, we need to verify four properties: closure, associativity, identity element, and inverse element. L. Since l1 and l2 are continuous and Closure. Let l1, l2 [0, 1] if x1 < x2, then strictly increasing, for any x1, x2 l1(x1) < l1(x2). Thus, l2(L1(x1)) < l2(l1(x2)), showing that l1 l2 is continuous because both are continuous. Associativity. Function composition is inherently associative, thus satisfying the property. Identity Element. The identity function is continuous and strictly increasing, so also an element of Inverse Element. Strictly monotone functions have an inverse, and the inverse is also monotonic. Thus, the inverse is also an element of L. l2 is strictly increasing. Moreover, l1 Therefore, we conclude that is group. A3.2. Proof of Lemma 2 Lemma 2. The set of all locally monotone increasing functions L2D with commutative Jacobian is valid group under the binary operation of function composition. Proof. To verify that L2D forms group, we check the following properties: Closure: For any l1, l2 also locally monotone-invertible function. L2D, their composition l1 l2 is Because the local Jacobian of the composition Jl1l2(x) = Jl1 (l2(x)) Jl2 (x) [0, 1]2. l2 is (A30) SPD(2) and commutes, their product Jl1 Since Jl1, Jl2 Jl2 is also SPD matrix. The composition of invertible functions is also invertible. And the Jl1l2 commutes due to associativity of matrix product. Thus l1 Associativity: Function composition is inherently associative, i.e., for all l1, l2, l3 L2D, we have L2D. l2 (l1 l2) l3 = l1 (l2 l3). (A31) Existence of Identity: The identity function has 2 2 identity matrix as local Jacobin. Thus, it maintains all the conditions of L2D. Existence of Inverse: The inverse function of any L2D can be obtained by inverting the local Jacobians. Specifically, for any L2D, the inverse l1 exists and satisfies: Jl1 (x) = Jl(l1(x))1 SPD(2) [0, 1]2 (A32) Furthermore, Jl1Jlk = Jlk Jl1 for any lk L2D as JlJlk = Jlk Jl 1 (JlJlk ) = 1 (Jlk Jl), (left multiply by 1 (A33) ) (A34) Jlk = 1 Jlk 1 Jlk Jl = 1 Jlk (right multiply by 1 (A35) (A36) ). Thus, l1 Therefore, L2D satisfies all group axioms, completing the L2D. proof. A4. Runtime of the DEC Module We use Anderson Acceleration to approximate the fixed point of the DEC. This requires fixed number of forward passes through the lightweight DEC module. The computational complexity of this iterative process is O(jTDEC), where is fixed number of required iterations and TDEC is the computation cost associated with single forward pass of the DEC module. The hyperparameter governs the trade-off between computational cost and the accuracy of the fixedpoint approximation. Empirically, for DINO-v2, the DEC module requires 24% (0.16 sec) of the total required time (0.66 sec) to process batch of 128 images of size 224 224. A5. Additional Results A5.1. Additional Baselines To evaluate the effectiveness of the DEC module in models with handcrafted hierarchical feature processing or image pyramid structures, we adapt our approach to HRViT [22] and ResFormer [60] and report the results in Tab. A2. Method HRVit [22] Resformer [60]"
        },
        {
            "title": "Canon\nInvL\nOurs",
            "content": "93.22 94.93 95.91 96.67 91.04 95.27 94.92 96.91 Methods ViT DeiT Swin BEiT Base Aug Canon InvL Ours 81.29 81. 79.23 81.29 81.43 70.67 70.70 66.92 70.71 70.92 79.55 79.56 76.04 79.58 79.86 85.79 85. 84.29 85.66 86.04 Table A2. Hierarchical baselines on scale-MNIST Table A5. Acc. on unmodified (scale-1) ImageNet images. that the DEC module has learned to stretch/squeeze regions of the digits. However, the exact reasoning on why such scaling is beneficial to the deep-net remains challenging. The interpretability of the choice of learned canonical elements in group is largely underexplored in the literature. # layers in DEC Mod. 2 3 4 1 2 3 4 # 93.59 94.98 96.66 96.58 94.47 96.04 96.17 96. 94.04 95.16 96.67 96.65 93.22 95.25 96.38 96.50 Table A3. Ablation on the number of DEC modules and layers per module on scale-MNIST Multiples () of grid 1.0 1. 2.0 2.5 2 3 a 96.61 96.62 96.95 96. 97.06 97.08 97.94 97.08 Table A4. Acc. at multiple of initial grid size of 4 with varying number of layers in DEC on scale-MNIST. A5.2. Additional Ablation Study We perform additional ablation studies on the scale-MNIST dataset to evaluate the effect of (i) the number of DEC modules and (ii) the number of layers within each DEC module. The results are summarized in Tab. A3. We observe that increasing the number of DEC modules, i.e., repeatedly canonicalizing features throughout the network, improves performance compared to applying canonicalization only at the input level. We provide an additional ablation study on the choice of grid size for local scaling and report the results in Tab. A4. We observe that increasing the grid size improves the performance as it allows more flexible spatial parameterization of the local scaling operations. To assess potential side effects of scale equivariance, we report the accuracies of the adapted models on images of scale 1, i.e., unmodified images in Tab. A5. We do not observe any drop in the performance. A5.3. Additional Visualizations Following the settings of Fig. 8, we report the results for more input images in Fig. A1. We observe that Ours is consistently better and more robust on all scales in comparison to Base; especially on the more extreme local scale factors. We present visualizations of learned monotone scaling by the DEC trained on MNIST in Fig. A2. We observe Figure A1. Comparison on per-scale probability of correctness. We locally scale the same input image within the range of [0.7, 1.3] and report the probability of the correct class. Original Images Figure A2. Learned monotone scaling on locally scaled MNIST. We observe that stretching/squeezing is performed on the area with digits. Monotone Scaled Images"
        }
    ],
    "affiliations": [
        "DSO National Laboratories",
        "Department of Computer Science, Purdue University"
    ]
}