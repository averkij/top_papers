{
    "paper_title": "Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation",
    "authors": [
        "Lorenzo Cima",
        "Alessio Miaschi",
        "Amaury Trujillo",
        "Marco Avvenuti",
        "Felice Dell'Orletta",
        "Stefano Cresci"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct an LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 8 3 3 7 0 . 2 1 4 2 : r Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation Lorenzo Cima lorenzo.cima@phd.unipi.it University of Pisa and IIT-CNR Italy Marco Avvenuti marco.avvenuti@unipi.it University of Pisa Italy Alessio Miaschi* alessio.miaschi@ilc.cnr.it ILC-CNR Italy Felice DellOrletta felice.dellorletta@ilc.cnr.it ILC-CNR Italy Amaury Trujillo amaury.trujillo@iit.cnr.it IIT-CNR Italy Stefano Cresci stefano.cresci@iit.cnr.it IIT-CNR Italy ABSTRACT AI-generated counterspeech offers promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct an LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through combination of quantitative indicators and human evaluations collected via pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation. . Warning: This paper contains examples that may be perceived as offensive or upsetting. Reader discretion is advised. ACM Reference Format: Lorenzo Cima, Alessio Miaschi*, Amaury Trujillo, Marco Avvenuti, Felice DellOrletta, and Stefano Cresci. 2024. Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation. In Proceedings of ACM Conference (Conference17). ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn equal contributions Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference17, July 2017, Washington, DC, USA 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn Figure 1: Current AI-generated counterspeech only leverages the content of the toxic message. Here, we generate contextualized counterspeech that also leverages information about the community, the conversation, and the moderated user to craft more persuasive responses."
        },
        {
            "title": "1 INTRODUCTION\nOnline toxicity refers to hateful, offensive, or otherwise harmful\nspeech on the Web that can cause distress in readers or lead some-\none to abandon a conversation [72]. Toxicity has dire social and\neconomic costs. Online, it reduces user participation, hinders infor-\nmation exchange, and deepens divides [1]. Offline, it may lead to\nphysical violence, reduce norm adherence, and cause severe psy-\nchological distress [25, 51, 57]. Hence, it is a growing concern for\nboth regulators and platform administrators [22].",
            "content": "Platforms apply wide array of moderation actions to curb toxicity and other online harms [21, 70]. These include content and user removals, friction interventions, and demotion [9, 68]. An alternative to centralized moderation is social correctionsuch as counterspeechwhere users proactively respond to toxic content encouraging respectful and constructive communication [5, 26]. Counterspeech holds promise as it avoids censorship concerns while preventing toxic users from merely relocating to other communities [41]. However, number of challenges currently limit its efficacy and applicability. The emotional toll on responders, who spend countless hours confronting hate, incivility, and personal attacks, is major issue [56, 62]. Safety is another concern, as responders may face retaliation [19, 64]. Moreover, the widespread nature of online toxicity makes manual counterspeech highly impractical. To overcome these limitations, automated counterspeech systems Conference17, July 2017, Washington, DC, USA Cima et al. reference counterspeech persuasion personalization [10, 40] [8, 20] [4, 37, 46, 50, 65, 74] [17, 34, 59, 61] [6, 24, 32, 33, 35, 55] [3, 43] this work Table 1: Summary of relevant literature concerning LLMgenerated counterspeech, persuasion, and personalization. leveraging generative AI technologiessuch as large language models (LLMs)have been developed [5, 66]. Yet, evaluating their effectiveness remains challenging [28]. Existing work primarily focused on basic characteristics like grammaticality and relevance, using quantitative indicators that offer narrow assessment of effectiveness [75]. Additionally, current machine-generated counterspeech is one-size-fits-all [18], relying solely on the toxic message as input, which is often insufficient for crafting effective responses. On the contrary, effective moderation must be context-dependent [28, 42], requiring shift from generic to tailored approaches that consider the broader conversational context [18]. Research focus. We propose using an LLM to generate adapted and personalized counterspeech messages against online toxicity. Our core novelty is the generation of contextualized counterspeech, as opposed to generic one. Our model ingests information about the community where the moderation occurs, the moderated user, and the specific conversation, as outlined in Figure 1. We test the hypothesis that contextualized counterspeech can be more persuasive than generic one, in set of political communities on Reddit. We also assess which strategies for adaptation and personalization are more effective. To test the hypothesis, we define the desired characteristics of effective counterspeech and we use quantitative indicators to measure them. For attributes that indicators cannot reliably capture [75], we run pre-registered, mixed design crowdsourcing experiment. Our results surface the challenges of generating contextualized counterspeech, but also show that contextual counterspeech can outperform state-of-the-art generic counterspeech in various aspects. In summary, our main contributions are: We propose, experiment with, and evaluate strategies for generating counterspeech that is adapted to the community and moderation context, and personalized to the moderated user. We assess the contribution that different types of contextual information provide, showing that contextual counterspeech can outperform generic one in terms of adequacy and persuasiveness, without compromising other characteristics. We show that evaluations derived from quantitative indicators strongly differ from human judgments, which bears implications for the development of counterspeech evaluation methodologies."
        },
        {
            "title": "2 RELATED WORK\n2.1 LLM-generated counterspeech\nCounterspeech can be more effective than other moderation ac-\ntions, such as content and user removals, without limiting free",
            "content": "speech [11]. Favorable results were obtained by scholars [4, 37], NGOs [12, 13], and ordinary users [31, 38] alike, as part of observational [26], quasi-experimental [4], and experimental [37, 50] studies. These positive results also extend to AI-generated counterspeech. However, we currently lack thorough understanding of the types of counterspeech that are most effective and the optimal conditions for their generation [11]. For these reasons, research is focusing on identifying the conditions, methods, and resources that make AI-generated counterspeech effective. Tekiroglu et al. [65] compared pre-trained LLMs and decoding strategies to highlight those capable of generating effective counterspeech. Others leveraged socio-psychological theories to generate messages based on different strategies, such as empathy, abstract norms, disapproval, and humor [4, 8, 37, 63]. However, these strategies were applied independently of the moderated context and user. As shown in Table 1, the only exceptions are Doğanç and Markov [20] who used the age and gender of hateful users to generate tailored counterspeech, and Bär et al. [8] who prompted an LLM to generate contextualized counterspeech based on the content of the toxic message. Herein, we go beyond existing work by generating adapted and personalized counterspeech that leverage content about the user, the conversation, and the community, assessing which type of contextual information results in effective counterspeech."
        },
        {
            "title": "2.3 LLM adaptation and personalization\nThe majority of existing content moderation interventions is one-\nsize-fits-all, where the intervention is applied uniformly to all mod-\nerated users. However, multiple studies have highlighted the lim-\nitations of this generic strategy [14, 17, 69], suggesting that con-\ntextualized moderation could offer substantial improvements [18].\nHowever, despite the potential benefits, contextualized moderation\nhas received little attention. Among the few existing works that\nused LLMs to generate tailored moderation interventions is [17],\nwhich experimented with personalized messages to debunk con-\nspiracy beliefs. Similarly, Bär et al. [8] adapted counterspeech to\nalign with the specific content of the moderated messages. Besides\ncontent moderation, LLM personalization was studied for socio-\ndemographic alignment [3, 20, 29, 59], for alignment with specific",
            "content": "Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation Conference17, July 2017, Washington, DC, USA personality traits [43, 61], and for political microtargeting [34], with promising results. Herein, we extend this line of research by experimenting with multiple strategies of adaptation and personalization to generate persuasive contextualized counterspeech."
        },
        {
            "title": "3 PROBLEM DEFINITION\nLet T = ⟨𝑚0, 𝑚1, . . . , 𝑚𝑁 ⟩ be an online conversation thread where\n𝑚0, 𝑚1, . . . , 𝑚𝑁 denotes the chronologically ordered sequence of\nthread messages. Given a toxic message 𝑚𝑖 , we aim to generate the\ncounterspeech message ˆ𝑚𝑖+1 = G(𝑚𝑖, C𝑖 ), where G is the counter-\nspeech generator and C𝑖 is the contextual information. Contrarily to\nexisting works where G only receives 𝑚𝑖 as input [8, 10, 39, 40, 46],\nwe generate adapted and personalized counterspeech by also pro-\nviding C𝑖 as input to G.\nRelevant properties. We identify the following set of desired\nproperties that effective counterspeech should possess:\n• Politeness increases the likelihood of persuading toxic users and\nbystanders by fostering a respectful dialogue [73]. Additionally,\nit aligns with the ethical principle of promoting constructive and\nnon-hostile interventions.",
            "content": "Adequacy. To be considered adequate counterspeech, response should directly address the toxic content, showing that the violation has not only been noticed but also appropriately managed, thereby discouraging future misbehaviors [5]. Relevance. Messages that are contextually relevant are more likely to resonate and affect the recipients. Conversely, generic statements may lack the contextual specificity needed to effectively address and mitigate toxic behavior [18, 28]. Diversity. Varied messages can engage users in different ways, catering to broader range of contexts [47]. Diversity also prevents responses from becoming predictable and thus less impactful, while boosting their perceived authenticity and genuineness. Truthfulness ensures that the counterspeech upholds the integrity of the dialogue. Thus, counterspeech should be factual, accurate, and not misleading, as this builds credibility and trust [5, 60]. Persuasiveness reflects the likelihood of changing the behavior or attitudes of the moderated user. Persuasive counterspeech can also influence bystanders, encouraging community-wide shift towards positive interactions [40]. In addition to the above, we also consider the following relevant properties of contextualized AI-generated counterspeech: Adaptation ensures that responses are contextually relevant, making the counterspeech more relatable. Furthermore, adapted responses are more likely to be perceived as genuine and thoughtful, which may enhance their credibility and acceptance [28]. Personalization. While adaptation focuses on the broad moderation context, personalization is user-specific and focuses on user characteristics and behaviors [27]. This individualized approach can help build rapport and trust, reducing defensiveness and increasing the likelihood of persuasion [18]. Artificiality refers to the perception of being automatically generated, rather than being human-crafted. Counterspeech messages perceived as artificial may be less likely to be taken seriously [28]. Moreover, minimizing artificiality enhances user experience by making interactions feel natural and genuine. Lastly, reducing the perception of artificiality might help avoid possible negative reactions from users who might feel manipulated or dismissed by AI-generated responses [30]. We do not consider basic linguistic properties such as fluency or grammaticality, since modern LLMs are capable of consistently generating human-quality text [48]."
        },
        {
            "title": "4 METHODS\n4.1 Generation\nWe use an instruction-tuned version of LLaMA2-13B to generate\ncounterspeech responses [67].1 The instruction prompt is reported\nin Appendix A. Starting from this generator, we evaluate different\nconfigurations depending on the information provided to the model\nand the data used for fine-tuning. Below, we describe the factors\nconsidered in our experiments, each identified by a unique [label].\nSome factors do not involve adaptation nor personalization:",
            "content": "Base [ Ba ]: The base LLaMA2-13B model without modifications. When alone, this factor also represents our baseline configuration. Counterspeech fine-tuning: We specialize the base model for counterspeech generation via fine-tuning on two reference datasets: [ Mu ] MultiCONAN [23] contains 500 hate speechcounterspeech pairs across various hate targets (e.g., race, religion, nationality, sexual orientation, disability, and gender); [ Hs ] the Reddit hate-speech intervention (RHSI) dataset [54] includes 5,020 Reddit conversations with human-written interventions. For our study, we select only those comments paired with human-generated response and with maximum length of 250 words, totaling 2,974 instances. The previous factors allow us to reproduce state-of-the-art results in automated counterspeech generation. Instead, the following factors provide unexplored contextual information to the generator."
        },
        {
            "title": "4.1.1 Adaptation.\n• Community [ Re ]: Since our experiments take place on po-\nlitical communities (i.e., subreddits), we align the generator to\nReddit’s political conversational style and informal language\nby fine-tuning it on a sample of comment-reply pairs from five\nprominent political subreddits, as specified in Section 5.",
            "content": "Conversation [ Pr ]: Since each toxic message 𝑚𝑖 to moderate is part of conversation thread, we add context about the conversation by providing to the generator up to two parent messages 𝑚𝑖 1, 𝑚𝑖 2 from the same thread."
        },
        {
            "title": "4.1.2 Personalization.\n• Comment history [ Hi ]: We provide user-specific information\nto the generator by prepending each toxic message 𝑚𝑖 with ten\nprevious messages that its author posted on Reddit.",
            "content": "Summary [ Su ]: Given toxic message 𝑚𝑖 , we feed twenty previous messages of its author to an instruction-tuned LLaMA213B model tasked with producing user summaries that highlight writing style, lexicon, and main interests. The instruction prompt used to generate user summaries is reported in Appendix A. User summaries are then provided to the counterspeech generator as source of user-specific information. 1https://huggingface.co/dfurman/Llama-2-13B-Instruct-v0.2 Conference17, July 2017, Washington, DC, USA Cima et al. We implement and evaluate 36 different configurations obtained via different combinations of these factors.2 Configurations that involve fine-tuning on multiple datasets are trained in multi-task setting, by merging the needed datasets."
        },
        {
            "title": "4.2.1 Algorithmic evaluation. Following recent literature [5, 39, 58],\nwe implement a comprehensive pool of evaluation indicators:\n• Relevance: For each toxic message 𝑚𝑖 and corresponding gener-\nated counterspeech ˆ𝑚𝑖+1, we measure the relevance of ˆ𝑚𝑖+1 to\n𝑚𝑖 by computing the ROUGE score between the two texts [49].\n• Diversity: Given a configuration, we measure the diversity among",
            "content": "its generated counterspeech messages as: Diversity = 1 1 𝑛(𝑛 1) 𝑛 𝑛 𝑖= 𝑗=1 𝑗𝑖 ROUGE( ˆ𝑚𝑖, ˆ𝑚 𝑗 ), where ROUGE( ˆ𝑚𝑖, ˆ𝑚 𝑗 ) is the similarity between two counterspeech messages ˆ𝑚𝑖 and ˆ𝑚 𝑗 and 𝑛 is the total number of generated messages. Readability: We measure the readability of the generated counterspeech messages via the Flesch Reading Ease (FRES) score [44]. FRES evaluates readability based on the average sentence length and the average number of syllables per word. Toxicity: We measure toxicity via Googles Perspective API [47].3 Adaptation: We measure the effectiveness of the adaptation as the diversity (i.e., 1 ROUGE) between the counsterspeech messages generated by the baseline model [ Ba ] and those generated by each other configuration. Personalizationlex: We measure personalization in terms of lexical similarity between the generated counterspeech messages and sample of user messages. Given the author of toxic message 𝑚𝑖 , we quantify lexical similarity as the ROUGE score between the sample of messages by the user and the counterspeech message ˆ𝑚𝑖+1. This indicator quantifies the degree of lexical overlap between the counterspeech and user messages. Personalizationwri: We also measure personalization in terms of writing style similarity. First, we compute the writing style profile of each counterspeech message and of the authors of each toxic message. We obtain writing style profiles via ProfilingUD, system that extracts more than 130 raw, morpho-syntactic, and syntactic properties that are representative of the linguistic structure of text corpus [7]. Then, we compute Spearmans rank correlation coefficient between the writing style profile of each counterspeech message and that obtained from the sample of messages of the author of the toxic message 𝑚𝑖 . 2For example, [ Ba Pr Hi ] refers to the base LLaMA-2 model that receives the previous conversation message and the sample of user messages as contextual information. 3https://perspectiveapi.com/"
        },
        {
            "title": "4.2.3 Human evaluation. We carry out an extensive human evalu-\nation campaign via a pre-registered,4 mixed design crowdsourcing\nexperiment on Amazon Mechanical Turk.5 Initially, participants\nare assigned to one of two between-subjects conditions: (i) non-\ncontextual – participants are shown only the toxic messages and the\ncorresponding generated counterspeech responses; (ii) contextual –\nparticipants are also shown the contextual information that was\nused to adapt and/or personalize the counterspeech. After this ini-\ntial assignment, participants are asked to read and evaluate multiple\npairs (𝑚𝑖, ˆ𝑚𝑖+1) of toxic messages and counterspeech, under seven\nwithin-subjects conditions. The seven conditions correspond to the\nseven configurations selected for human evaluation. The order of\nwithin-subjects conditions is randomized to control for order effects.\nIn both within-subjects experiments participants are asked to rate\non a five points Likert scale: the relevance of the counterspeech to\nthe toxic message, its adequacy as counterspeech, its truthfulness, its\nartificiality, and its persuasiveness. Following recent literature [40],\npersuasiveness is evaluated in two different questions based on the\ncounterspeech likelihood of (i) persuading the author of the toxic\nmessage to re-engage in the conversation in a civil manner, and of\n(ii) steering the conversation back to civil discourse. Participants as-\nsigned to the contextual between-subjects condition are also asked\nto rate how contextualized is the counterspeech to the context of\nthe toxic message. Finally, all participants are asked a small set\nof socio-demographic questions. The complete list of questions is\nreported in Appendix B. This mixed design allows us to isolate and\nevaluate the impact of contextual information on the effectiveness",
            "content": "4https://aspredicted.org/b55z-5qy4.pdf 5We received ethical clearance (#0306210) for this experiment from the competent IRB. Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation Conference17, July 2017, Washington, DC, USA of counterspeech. In particular, the between-subjects design enables us to determine whether the inclusion of contextual information enhances the perceived relevance, adequacy, and persuasiveness of the responses. Statistical analysis. The presence of statistically significant 4.2.4 differences across the within-subjects conditions (i.e., the evaluated configurations) is assessed via Friedman tests [16]. Next, we identify the configurations with significant differences from the baseline via paired Wilcoxons signed-rank tests with Bonferroni correction for multiple hypothesis testing. Effect sizes and confidence intervals are obtained by computing the matched-pairs rank biserial correlation coefficients [45]. Furthermore, we identify statistically significant differences for the same configuration across the between-subjects conditions via two-sample MannWhitney tests with Bonferroni correction. Then, effect sizes and confidence intervals are obtained via Glass rank biserial correlations [45]."
        },
        {
            "title": "5 DATA\nWe collected Reddit comments posted over multiple years in five\npopular subreddits discussing US politics. We selected subreddits\nwith a marked right- (r/conservatives) or left-leaning (r/progressive)\ntendency, with a focus on a prominent conservative (r/the_donald\nfor Donald Trump) or progressive (r/aoc for Alexandra Ocasio-\nCortez) figure, and a mixed-leaning subreddit (r/politics). De-\npending on subreddit size, data collection spanned either 36 or 12\nmonths so as to collect a comparable amount of data from each\nsubreddit. For r/the_donald, data collection stopped in June 2020,\nwhen the subreddit was permanently banned [15]. All data was\ncollected from the Pushshift archives [2].\nCounterspeech dataset. We selected a small set of toxic com-\nments for which to generate counterspeech responses. First, we\ncomputed the toxicity score of each comment via Google’s Perspec-\ntive API [47]. Then, we selected those comments with toxicity ≥ 0.5\nand with at least two parent comments in their conversation thread.\nThis allowed selecting 128 toxic comments from 49 threads. Each\nof the 36 implemented counterspeech generation configurations\nwas asked to generate a counterspeech response to each of the 128\ntoxic comments, resulting in 4,608 counterspeech responses that\nwe generated and evaluated.\nAdaptation and personalization datasets. We also built a few\nadditional datasets that implement the adaptation and personal-\nization strategies described in Section 4.1. We selected a stratified\nrandom sample of around 7,500 comment-reply pairs that we used\nto fine-tune the counterspeech generator to Reddit’s political con-\nversational style and informal language, thus implementing the\ncommunity adaptation factor [ Re ]. We also selected the two pre-\nceding comments to each of the 128 previously selected toxic com-\nments, which we fed to the counterspeech generator via prompting,\nimplementing the conversation adaptation factor [ Pr ]. Finally, we\ncollected twenty random comments for each author of the 128 se-\nlected toxic comments. These data were used to generate the user\nprofiles that implement the user summary personalization factor",
            "content": "[ Su ]. Ten of these comments were also directly fed via prompting to the counterspeech generator, implementing the comment history factor [ Hi ]."
        },
        {
            "title": "6 RESULTS\n6.1 Algorithmic evaluation\nWe evaluate all factors (𝑁 = 7) and configurations (𝑁 = 36) with\nthe indicators defined in Section 4.2.1.",
            "content": "Factors. First, we aggregate results by the presence or ab6.1.1 sence of each factor. This analysis investigates whether the presence of specific factor contributes to an improvement of the configurations in terms of the evaluation indicators. Results are presented in Figure 2. For each factor (y axis), the plots show the mean value of the indicator when the factor is present (teal dot) or absent (sand dot) in the evaluated configurations. In each panel the factors are ranked from top to bottom based on the delta between the two mean values. Overall, Figure 2 reveals that the presence of some factors in configuration causes both improvements and degradations depending on the indicator. For example, fine-tuning with MultiCONAN [ Mu ] and with our community adaptation dataset [ Re ] improves relevance, diversity, readability, and adaptation. Albeit, the same factors worsen toxicity and writing style personalization. This result highlights that no single factor improves all aspects of counterspeech generation. Thus, when generating contextualized counterspeech in practical scenarios, it may be necessary to make case-by-case choices of the factors to use depending on the requirements at hand, given that no single factor is capable of improving all aspects. Despite this, some factors perform poorly overall. For example, fine-tuning with the RHSI dataset [ Hs ] degrades relevance, diversity, toxicity, and writing style personalization. We also note that the three fine-tuning factors (e.g., [ Mu ], [ Re ], and [ Hs ]) perform similarly in terms of readability, toxicity, adaptation, and personalization. Figures 2E, F, and highlight the contributions of our adaptation and personalization strategies towards generating contextualized counterspeech. Concerning adaptation, both [ Re ] and [ Pr ] provide improvements, although only the former is marked. Instead, while [ Su ] clearly improves both personalization indicators, [ Hi ] yields an improvement only in terms of writing style personalization. Finally, we note that the seemingly positive personalization results of the base factor [ Ba ], which is only present in configurations that lack fine-tuning, emphasizes that fine-tuning with either MultiCONAN [ Mu ], RHSI [ Hs ], or community adaptation [ Re ] tends to degrade personalization."
        },
        {
            "title": "6.1.2 Configurations. Next, we analyze fine-grained algorithmic\nresults for each evaluated configuration. These are reported in Ta-\nble 2, where configurations are organized in four groups from top\nto bottom: those without any adaptation nor personalization, those\nwith only adaptation, those with only personalization, and those\nwith both. For each evaluation indicator, the best result is shown\nin bold font and the remaining top-5 results are underlined. Many\nconfigurations achieve comparable results and each group contains\nsome configurations that achieve top results in at least a few indi-\ncators. However, there are notable differences between the groups.\nMultiple configurations that make use of both adaptation and per-\nsonalization obtain top or anyway strong results in many evaluation",
            "content": "Conference17, July 2017, Washington, DC, USA Cima et al. Figure 2: Algorithmic evaluation results for each factor. For each factor (y axis) and indicator (panels), the teal dot shows the mean value of the indicator when the factor is used in the evaluated configurations, while the sand dot indicates the mean value of the indicator when the factor is not used. Arrows specify whether larger or smaller scores are better. Figure 3: Human evaluation results (non-contextual condition). Effect sizes and confidence intervals of the scores assigned to several configurations compared to the baseline. Statistical significance: ***: 𝑝 < 0.01. indicators. Some examples are [ Mu Re Pr Hi ], [ Mu Hs Re Hi ], and [ Mu Re Hi ]. Similarly, multiple configurations that only make use of adaptation also obtain strong results. Examples of the latter are [ Mu Re ], [ Mu Re Pr ], and [ Mu Hs Re Pr ]. Instead, few configurations that only use personalization achieve convincing results. Interestingly, also some configurations that use neither adaptation nor personalization achieve good results, such as the baseline [ Ba ] and the configuration that only applies fine-tuning with MultiCONAN [ Mu ]. Overall, this analysis reveals small differences between the various configurations. At the same time, it also suggests that performing adaptation or adaptation plus personalization might yield better results than using only personalization or neither. Next, we seek to verify whether these results hold also for human evaluators. Configuration selection. We apply the methodology described in Section 4.2.2 to select the configurations evaluated in the crowdsourcing experiments. As mentioned, we select the best and worst configuration out of those that either perform only adaptation, only personalization, or both. The six configurations selected in this way are highlighted in Table 2. In addition to these, the baseline configuration [ Ba ] is also selected for comparison."
        },
        {
            "title": "6.2 Human evaluation\nWe recruited 𝑁 = 2, 444 and 𝑁 = 2, 353 participants on Amazon\nMechanical Turk, respectively for the non-contextual and contextual\nbetween-subjects experimental conditions. These figures exclude\nparticipants whose answers were rejected due to excessively fast",
            "content": "survey completion time, < 100% correct answers to the control questions, and those that gave the same answer to all questions. We report no deviations from the pre-registration protocol."
        },
        {
            "title": "6.2.1 Non-contextual experiment. Participants assigned to the non-\ncontextual condition were shown pairs of toxic speech and counter-\nspeech responses. The Friedman test reveals statistically significant\ndifferences between some of the evaluated configurations. Figure 3\nshows effects sizes, confidence intervals, and statistical significance\nof the comparisons between each configuration and the baseline\n[ Ba ]. As shown, two groups of configurations achieved overall sim-\nilar results. Configurations [ Mu Re ], [ Hs Hi ], [ Mu Hs Hi ], and\n[ Mu Re Pr Hi ] consistently obtain statistically significant worse\nresults than the baseline in each evaluated aspect, except for arti-\nficiality. This result implies that the counterspeech generated by\nsaid configurations was perceived as more human-like than that\nof the baseline, but that apart from this, the baseline generated\nbetter counterspeech in any evaluated aspect. Instead, [ Ba Pr ]\nand [ Ba Pr Hi ] obtain comparable or statistically significant bet-\nter results than the baseline. Specifically, [ Ba Pr Hi ] outperforms\nthe baseline with respect to the adequacy of the generated coun-\nterspeech and its perceived capacity to persuade the author of\nthe toxic message. It also achieves better scores than the base-\nline concerning relevance, truthfulness, and capacity to persuade\nbystanders, although these improvements are not statistically sig-\nnificant. [ Ba Pr ] obtains similar results to the baseline, so much\nso that none of the measured differences is significant. However,",
            "content": "Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation Conference17, July 2017, Washington, DC, USA Figure 4: Human evaluation results (contextual condition). Effect sizes and confidence intervals of the scores assigned to several configurations compared to the baseline. Statistical significance: ***: 𝑝 < 0.01, **: 𝑝 < 0.05, *: 𝑝 < 0.1. Figure 5: Differences in human evaluation results between the contextual and non-contextual conditions. Statistical significance: ***: 𝑝 < 0.01, **: 𝑝 < 0.05, *: 𝑝 < 0.1. it nonetheless achieves slightly better scores than the baseline in both persuasiveness questions. Among the socio-demographic questions asked to participants is their frequency of use of social media. Appendix Figure 7 shows results of the comparisons with the baseline obtained by only considering the answers of those participants who declared using social media very often. Overall we obtain similar results to those in Figure 3, with the exception that the [ Ba Pr Hi ] configuration outperforms the baseline in both questions about persuasiveness. Given that frequent social media users might be more familiar with online toxicity and counterspeech, this result strengthens our finding about the perceived effectiveness of the [ Ba Pr Hi ] configuration. Results of the non-contextual within-subjects experiment reveal that the [ Ba Pr Hi ] configuration improves on the baseline in terms of adequacy and persuasiveness of the generated counterspeech. This finding is relevant for our study since [ Ba Pr Hi ] makes use of both adaptation and personalization, vouching for the potential effectiveness of contextualized counterspeech. Additionally, we note that [ Ba Pr ] and [ Ba Pr Hi ]the configurations that performed the best in this experimentare among those that were ranked worst in their groups based on the ranking derived from the quantitative indicators, as highlighted in Table 2. This finding suggests that discrepancy may exist between human and algorithmic evaluations of counterspeech."
        },
        {
            "title": "6.2.2 Contextual experiment. Participants assigned to the contex-\ntual condition where shown additional information on top of the\ntoxic message and the generated counterspeech. Contextual in-\nformation includes the name of the subreddit where the toxic\nmessage was posted, the previous message in the conversation\nthread, and the user summary obtained as described in Section 4.1.2.\nAgain, the Friedman test reveals statistically significant differences.",
            "content": "Detailed results are presented in Figure 4 and largely corroborate those from the non-contextual experiment. The configurations [ Ba Pr ] and [ Ba Pr Hi ] consistently achieve the highest overall scores, with [ Ba Pr ] demonstrating statistically significant improvement over the baseline in persuading the author of the toxic message. Most other results for these configurations are statistically non-significant. Conversely, configurations [ Mu Re ], [ Hs Hi ], [ Mu Hs Hi ], and [ Mu Re Pr Hi ] consistently perform significantly worse than the baseline across all metrics, except for [ Mu Re ] and [ Hs Hi ] who outperform the baseline in terms of artificiality. This experiment contains an additional question with respect to the non-contextual one, where participants rated the effectiveness of contextualization. Figure 4G shows that while most differences are non-significant, [ Mu Re Pr Hi ] performs worse and [ Ba Pr Hi ] performs markedly better than the baseline, which reinforces previous results about the effectiveness of the [ Ba Pr Hi ] configuration. Our study design also allows comparing the scores obtained by each configuration in the contextual and non-contextual experiments. Results of this comparison are shown in Figure 5 and reveal that all configurationsbaseline includedobtained overall better scores in the contextual experiment across all aspects, except for artificiality. However, some configurations improved more than others. The baseline and configurations that were already performing well, such as [ Ba Pr Hi ] and [ Ba Pr ], showed the least improvements. Conversely, configurations with poor initial performance scored larger gains. This suggests that the additional contextual information allowed for more accurate evaluation of counterspeech generated by adapted and personalized models, effectively leveling the field and reducing differences between configurations. This trend is evidenced by the smaller effect sizes reported in Figure 4 compared to those in Figure 3. Conference17, July 2017, Washington, DC, USA Cima et al. evaluation indicators configuration"
        },
        {
            "title": "Ba Hi\nMu Hi\nHs Hi\nBa Su\nMu Su\nHs Su\nMu Hs Hi\nMu Hs Su",
            "content": "personalization rel div read tox ada lex wri .519 .050 .117 .488 .119 .129 .463 .142 .083 .461 .285 .086 .138 .153 .113 .114 .562 .754 .545 .615 .576 .805 .753 .688 .855 .858 .857 .566 .120 .808 .122 .706 .086 .794 .181 .802 .101 .851 .130 .207 .849 .128 . .139 .121 .085 .142 .128 .109 .102 .116 .604 .801 .768 .657 .732 .716 .815 .772 .574 .808 .805 .906 .765 .799 .869 .753 .544 .889 .872 .676 .837 .874 .892 .874 .040 .086 .207 .176 .208 .315 .177 .233 .063 .088 .172 .113 .112 .151 .127 . .490 .862 .867 .882 .866 .908 .886 .901 .583 .874 .884 .671 .860 .869 .884 .878 .144 .141 .133 .146 .128 .113 .137 .122 .143 .133 .125 .137 .137 .150 .134 .144 .524 .490 .473 .503 .471 .469 .503 .464 .534 .495 .478 .540 .499 .479 .498 . .538 .655 .851 .833 .919 .900 .900 .875 .141 .139 .135 .134 .172 .180 .105 .113 .096 .102 .116 .125 .173 .165 .125 .132 .618 .656 .836 .781 .826 .779 .797 .776 .809 .798 .821 .784 .850 .823 .831 ."
        },
        {
            "title": "Ba Pr Hi\nBa Pr Su\nMu Pr Hi\nMu Pr Su\nMu Re Hi\nMu Re Su\nHs Pr Hi\nHs Pr Su\nMu Hs Pr Hi\nMu Hs Pr Su\nMu Hs Re Hi\nMu Hs Re Su\nMu Re Pr Hi\nMu Re Pr Su\nMu Hs Re Pr Hi\nMu Hs Re Pr Su",
            "content": ".535 .538 .506 .504 .506 .510 .498 .489 .498 .489 .500 .491 .517 .509 .503 .487 Ba : LLaMa2 baseline; Mu : Multi-CONAN fine-tuning; Hs : RHSI finetuning; Re : political subreddits fine-tuning; Pr : previous comments; Hi : user comment history; Su : user summary. .153 .144 .131 .155 .125 .148 .127 .131 .127 .136 .128 .140 .130 .142 .133 .137 .607 .683 .878 .856 .893 .877 .882 .874 .887 .873 .892 .872 .901 .885 .890 .880 .063 .092 .090 .101 .135 .158 .207 .162 .137 .159 .098 .146 .144 .174 .147 .162 .924 .842 .906 .858 .893 .861 .896 .891 Table 2: Algorithmic evaluation results of each configuration. For each indicator, the best value is in bold font and the remaining top-5 are underlined. Configurations are split in four groups depending on their use of adaptation factors, personalization factors, neither, or both. Icons highlight the overall best configurations of each group. and worst"
        },
        {
            "title": "6.2.3 Algorithmic and human evaluations. We conclude by com-\nparing the results achieved by the selected configurations in the al-\ngorithmic and human evaluations. For each evaluation method (i.e.,\nquantitative indicators, human assessments with and without con-\ntext), Figure 6 shows the aggregated ranking of the configurations\nacross all considered aspects, so that the overall best configurations\nare at the top. Rank aggregation is performed with the method",
            "content": "Figure 6: Aggregated rankings of the selected configurations, based on algorithmic and human evaluations. from Section 4.2.2. As shown, while the rankings obtained from the two human evaluations are overall consistent (Kendall 𝜏 = 0.62), the ranking based on the quantitative indicators is profoundly different from that of the non-contextual (𝜏 = 0.05) and contextual (𝜏 = 0.43) experiments, as testified by the negative Kendall rank correlation scores."
        },
        {
            "title": "7 DISCUSSION AND CONCLUSIONS\nGeneration. Our extensive analysis of adaptation and person-\nalization strategies across a large set of algorithmic and human\njudgments highlights the complexities of generating effective con-\ntextualized counterspeech. For example, Figure 4G illustrates that\nonly one configuration produced significantly better-contextualized\ncounterspeech than the baseline. Nonetheless, this successful con-\nfiguration significantly outperformed the baseline in terms of ad-\nequacy and persuasiveness. This result is relevant and novel, as\nit represents the first success at generating effective contextual-\nized counterspeech [8]. This work thus marks an advancement in\nthe field, demonstrating the potential for tailored approaches to\nimprove the effectiveness of counterspeech interventions.",
            "content": "The difficulty at generating well-contextualized messages may be due to the need of additional information, as recent studies suggest that LLMs might struggle to combine multiple instructions and information, which can degrade their output [3, 29]. The manual analysis of some generated counterspeech, reported in Appendix C, supports this hypothesis. However, this limitation is likely to be mitigated by the adoption of larger LLMs [35]. Therefore, future availability of ever-larger LLMs is likely to render adapted and personalized counterspeech, and more broadly moderation interventions [18], increasingly advantageous. Evaluation. Our results also bear important implications for the evaluation of AI-generated counterspeech. Most existing works rely on small set of algorithmic indicators to assess the quality of the generated counterspeech [20, 40, 65]. However, our results, alongside other recent studies [75], reveal that these indicators correlate poorly with human assessment. This discrepancy suggests that indicators and human evaluators focus on different aspects of the counterspeech. Consequently, future research should adopt nuanced evaluation methods that incorporate both algorithmic and human assessments, to avoid possibly misleading conclusions. In summary, our findings indicate that human evaluators found some instances of contextualized AI-generated counterspeech particularly persuasive, highlighting the potential for AI-generated solutions to effectively address human misbehavior on online platforms. In addition, the results of our extensive evaluations call for Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation Conference17, July 2017, Washington, DC, USA combined approach that integrates both algorithmic and human assessments in counterspeech evaluations. Therefore, our work points toward an increased human-AI collaboration [52] that leverages the strengths of both to enhance the effectiveness of content moderation. Limitations and Future Work. Despite our extensive experimentation and evaluation, our results rely upon certain technological and methodological choices. For example, we experimented with single LLM and we evaluated limited set of strategies for generating adapted and personalized counterspeech. Different LLMs or alternative adaptation and personalization strategies might yield different results. Similar considerations apply to the results of our algorithmic and human evaluations. The former is limited by the quantitative indicators that we considered. These, although in line with the state-of-the-art, can gauge but small set of characteristics. The latter is limited by the representativeness and reliability of crowdsourced evaluations. These limitations highlight the need for more research and experimentation on the effectiveness of contextualized counterspeech. Other than these, future work should explore more sophisticated models and techniques for personalizing and adapting counterspeech and should ensure that AI-generated counterspeech is unbiased and fair. Future research should investigate methods to detect possible biases in counterspeech generation and assess the long-term effects of these interventions on varied user groups."
        },
        {
            "title": "8 ACKNOWLEDGMENTS\nThis work is partially supported by the European Union – Next\nGeneration EU within the PRIN 2022 framework project PIANO\n(Personalized Interventions Against Online Toxicity); by the PNRR-\nM4C2 (PE00000013) “FAIR-Future Artificial Intelligence Research\" -\nSpoke 1 \"Human-centered AI\", funded under Next Generation EU;\nand by the Italian Ministry of Education and Research (MUR) in the\nframework of the FoReLab projects (Departments of Excellence).",
            "content": "REFERENCES [1] Ana Aleksandric, Sayak Saha Roy, Hanani Pankaj, Gabriela Mustata Wilson, and Shirin Nilizadeh. 2024. Users behavioral and emotional response to toxicity in Twitter conversations. In AAAI ICWSM. [2] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift Reddit dataset. In AAAI ICWSM. [3] Tilman Beck, Hendrik Schuff, Anne Lauscher, and Iryna Gurevych. 2024. Sensitivity, performance, robustness: Deconstructing the effect of sociodemographic prompting. In EACL. [4] Michał Bilewicz, Patrycja Tempska, Gniewosz Leliwa, Maria Dowgiałło, Michalina Tańska, Rafał Urbaniak, and Michał Wroczyński. 2021. Artificial intelligence against hate: Intervention reducing verbal aggression in the social network environment. Aggressive Behavior 47, 3 (2021). [5] Helena Bonaldi, Yi-Ling Chung, Gavin Abercrombie, and Marco Guerini. 2024. NLP for counterspeech against hate: survey and how-to guide. In NAACL. [6] Simon Martin Breum, Daniel Vædele Egdal, Victor Gram Mortensen, Anders Giovanni Møller, and Luca Maria Aiello. 2024. The persuasive power of large language models. In AAAI ICWSM. [7] Dominique Brunato, Andrea Cimino, Felice DellOrletta, Giulia Venturi, and Simonetta Montemagni. 2020. Profiling-UD: tool for linguistic profiling of texts. In LREC. [8] Dominik Bär, Abdurahman Maarouf, and Stefan Feuerriegel. 2024. Generative AI may backfire for counterspeech. arXiv:2411.14986 (2024). [9] Eshwar Chandrasekharan, Shagun Jhaver, Amy Bruckman, and Eric Gilbert. 2022. Quarantined! Examining the effects of community-wide moderation intervention on Reddit. ACM TOCHI 29, 4 (2022). [10] Hyundong Cho, Shuai Liu, Taiwei Shi, Darpan Jain, Basem Rizk, Yuyang Huang, Zixun Lu, Nuan Wen, Jonathan Gratch, Emilio Ferrera, et al. 2023. Can Language Model Moderators Improve the Health of Online Discourse? arXiv:2311.10781 (2023). [11] Yi-Ling Chung, Gavin Abercrombie, Florence Enock, Jonathan Bright, and Verena Rieser. 2023. Understanding counterspeech for online harm mitigation. arXiv:2307.04761 (2023). [12] Yi-Ling Chung, Elizaveta Kuzmenko, Serra Sinem Tekiroglu, and Marco Guerini. 2019. CONAN COunter NArratives through Nichesourcing: multilingual dataset of responses to fight online hate speech. In ACL. [13] Yi-Ling Chung, Serra Sinem Tekiroğlu, and Marco Guerini. 2021. Towards In ACLknowledge-grounded counter narrative generation for hate speech. IJCNLP. [14] Lorenzo Cima, Benedetta Tessa, Stefano Cresci, Amaury Trujillo, and Marco Avvenuti. 2024. Investigating the heterogenous effects of massive content moderation intervention via Difference-in-Differences. arXiv preprint arXiv:2411.04037 (2024). [15] Lorenzo Cima, Amaury Trujillo, Marco Avvenuti, and Stefano Cresci. 2024. The Great Ban: Efficacy and unintended consequences of massive deplatforming operation on Reddit. In ACM WebSci Companion. [16] William Jay Conover. 1999. Practical nonparametric statistics. John Wiley & Sons. [17] Thomas Costello, Gordon Pennycook, and David Rand. 2024. Durably reducing conspiracy beliefs through dialogues with AI. Science 385 (2024). [18] Stefano Cresci, Amaury Trujillo, and Tiziano Fagni. 2022. Personalized interventions for online moderation. In ACM Hypertext. [19] Periwinkle Doerfler, Andrea Forte, Emiliano De Cristofaro, Gianluca Stringhini, Jeremy Blackburn, and Damon McCoy. 2021. Im Professor, which isnt usually dangerous job: Internet-facilitated harassment and its impact on researchers. In ACM CSCW. [20] Mekselina Doğanç and Ilia Markov. 2023. From generic to personalized: Investigating strategies for generating targeted counter narratives against hate speech. In ACL CS4OA. [21] Cynthia Dwork, Chris Hays, Jon Kleinberg, and Manish Raghavan. 2024. Content moderation and the formation of online communities: theoretical framework. In The ACM Web Conf. [22] European Commission. 2019. Progress on combating hate speech online through the EU Code of Conduct. https://data.consilium.europa.eu/doc/document/ST12522-2019-INIT/en/pdf [23] Margherita Fanton, Helena Bonaldi, Serra Sinem Tekiroğlu, and Marco Guerini. 2021. Human-in-the-Loop for data collection: multi-target counter narrative dataset to fight online hate speech. In ACL-IJCNLP. [24] Kazuaki Furumai, Roberto Legaspi, Julio Vizcarra, Yudai Yamazaki, Yasutaka Nishimura, Sina Semnani, Kazushi Ikeda, Weiyan Shi, and Monica Lam. 2024. Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval. arXiv:2407.03585 (2024). [25] John Gallacher, Marc Heerdink, and Miles Hewstone. 2021. Online engagement between opposing political protest groups via social media is linked to physical violence of offline encounters. Social Media + Society 7, 1 (2021). [26] Joshua Garland, Keyan Ghazi-Zahedi, Jean-Gabriel Young, Laurent HébertDufresne, and Mirta Galesic. 2022. Impact and dynamics of hate and counter speech online. EPJ Data Science 11, 1 (2022). [27] Panagiotis Germanakos, Marios Belk, et al. 2016. Human-centred web adaptation [28] Tarleton Gillespie. 2020. Content moderation, AI, and the question of scale. Big and personalization. Springer. Data & Society 7, 2 (2020). [29] Tommaso Giorgi, Lorenzo Cima, Tiziano Fagni, Marco Avvenuti, and Stefano Cresci. 2024. Human and LLM Biases in Hate Speech Annotations: SocioDemographic Analysis of Annotators and Targets. arXiv:2410.07991 (2024). [30] Natasha Goel, Thomas Bergeron, Blake Lee-Whiting, Thomas Galipeau, Danielle Bohonos, Sarah Lachance, Sonja Savolainen, Clareta Treger, and Eric Merkley. 2024. Artificial influence? Comparing AI and human persuasion in reducing belief certainty. (2024). https://doi.org/10.31219/osf.io/2vh4k. [31] Pierpaolo Goffredo, Valerio Basile, Bianca Cepollaro, Viviana Patti, et al. 2022. Counter-TWIT: An Italian corpus for online counterspeech in ecological contexts. In ACL WOAH. [32] Josh Goldstein, Jason Chao, Shelby Grossman, Alex Stamos, and Michael Tomz. 2024. How persuasive is AI-generated propaganda? PNAS Nexus 3, 2 (2024). [33] Jarod Govers, Eduardo Velloso, Vassilis Kostakos, and Jorge Goncalves. 2024. AI-Driven Mediation Strategies for Audience Depolarisation in Online Debates. In ACM CHI. [34] Kobi Hackenburg and Helen Margetts. 2024. Evaluating the persuasive influence of political microtargeting with large language models. Proceedings of the National Academy of Sciences 121, 24 (2024). [35] Kobi Hackenburg, Ben Tappin, Paul Röttger, Scott Hale, Jonathan Bright, and Helen Margetts. 2024. Evidence of log scaling law for political persuasion with large language models. arXiv:2406.14508 (2024). [36] Sadaf MD Halim, Saquib Irtiza, Yibo Hu, Latifur Khan, and Bhavani Thuraisingham. 2023. WokeGPT: Improving counterspeech generation against online hate speech by intelligently augmenting datasets using novel metric. In IEEE IJCNN. Conference17, July 2017, Washington, DC, USA Cima et al. 74, 1 (2019). [64] Madiha Tabassum, Alana Mackey, Ashley Schuett, and Ada Lerner. 2024. Investigating moderation challenges to combating hate and harassment: The case of Mod-Admin power dynamics and feature misuse on Reddit. In USENIX. [65] Serra Sinem Tekiroglu, Helena Bonaldi, Margherita Fanton, and Marco Guerini. 2022. Using pre-trained language models for producing counter narratives against hate speech: comparative study. In ACL. [66] Serra Sinem Tekiroğlu, Yi-Ling Chung, and Marco Guerini. 2020. Generating counter narratives against online hate speech: Data and strategies. In ACL. [67] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [68] Amaury Trujillo and Stefano Cresci. 2022. Make Reddit Great Again: Assessing community effects of moderation interventions on r/The_Donald. In ACM CSCW. [69] Amaury Trujillo and Stefano Cresci. 2023. One of many: Assessing user-level effects of moderation interventions on r/The_Donald. In ACM WebSci. [70] Amaury Trujillo, Tiziano Fagni, and Stefano Cresci. 2025. The DSA Transparency Database: Auditing self-reported moderation actions by social media. In ACM CSCW. [71] Siyi Wang, Qi Deng, Shiwei Feng, Hong Zhang, and Chao Liang. 2024. survey on rank aggregation. In IJCAI. [72] Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017. Ex machina: Personal attacks seen at scale. In The ACM Web Conf. [73] Xinchen Yu, Eduardo Blanco, and Lingzi Hong. 2024. Hate cannot drive out hate: Forecasting conversation incivility following replies to hate speech. In AAAI ICWSM. [74] Wanzheng Zhu and Suma Bhat. 2021. Generate, Prune, Select: pipeline for counterspeech generation against online hate speech. In ACL-IJCNLP. [75] Irune Zubiaga, Aitor Soroa, and Rodrigo Agerri. 2024. LLM-based ranking method for the evaluation of automatic counter-narrative generation. In EMNLP. [37] Dominik Hangartner, Gloria Gennaro, Sary Alasiri, Nicholas Bahrich, Alexandra Bornhoft, Joseph Boucher, Buket Buse Demirci, Laurenz Derksen, Aldo Hall, Matthias Jochum, et al. 2021. Empathy-based counterspeech can reduce racist hate speech in social media field experiment. Proceedings of the National Academy of Sciences 118, 50 (2021). [38] Sabit Hassan and Malihe Alikhani. 2023. DisCGen: framework for discourseinformed counterspeech generation. In IJCNLP-AACL. [39] Bing He, Mustaque Ahamad, and Srijan Kumar. 2023. Reinforcement learningbased counter-misinformation response generation: case study of COVID-19 vaccine misinformation. In The ACM Web Conf. [40] Lingzi Hong, Pengcheng Luo, Eduardo Blanco, and Xiaoying Song. 2024. Outcome-Constrained Large Language Models for Countering Hate Speech. arXiv:2403.17146 (2024). [41] Manoel Horta Ribeiro, Shagun Jhaver, Savvas Zannettou, Jeremy Blackburn, Gianluca Stringhini, Emiliano De Cristofaro, and Robert West. 2021. Do platform migrations compromise content moderation? Evidence from r/The_Donald and r/Incels. In ACM CSCW. [42] Evey Jiaxin Huang, Abhraneel Sarma, Sohyeon Hwang, Eshwar Chandrasekharan, and Stevie Chancellor. 2024. Opportunities, tensions, and challenges in computational approaches to addressing online harassment. In ACM DIS. [43] Hang Jiang, Xiajie Zhang, Xubo Cao, Cynthia Breazeal, Deb Roy, and Jad Kabbara. 2023. PersonaLLM: Investigating the ability of large language models to express personality traits. arXiv:2305.02547 (2023). [44] JP Kincaid. 1975. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Chief of Naval Technical Training (1975). [45] Bruce King, Patrick Rosopa, and Edward Minium. 2020. Statistical reasoning in the behavioral sciences. John Wiley & Sons. [46] Rohan Leekha, Olga Simek, and Charlie Dagli. 2024. War of Words: Harnessing the Potential of Large Language Models and Retrieval Augmented Generation to Classify, Counter and Diffuse Hate Speech. In AAAI FLAIRS. [47] Alyssa Lees, Vinh Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman. 2022. new generation of perspective API: Efficient multilingual character-level transformers. In ACM KDD. [48] Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024. Pre-trained language models for text generation: survey. ACM Computing Surveys 56, 9 (2024). [49] Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out. [50] Kevin Munger. 2017. Tweetment effects on the tweeted: Experimentally reducing racist harassment. Political Behavior 39 (2017). [51] Paola Pascual-Ferrá, Neil Alperstein, Daniel Barnett, and Rajiv Rimal. 2021. Toxicity and verbal aggression on social media: Polarized discourse on wearing face masks during the COVID-19 pandemic. Big Data & Society 8, 1 (2021). [52] Dino Pedreschi, Luca Pappalardo, Emanuele Ferragina, Ricardo Baeza-Yates, Albert-László Barabási, Frank Dignum, Virginia Dignum, Tina Eliassi-Rad, Fosca Giannotti, János Kertész, et al. 2024. Human-AI coevolution. Artificial Intelligence (2024). [53] Vasyl Pihur, Susmita Datta, and Somnath Datta. 2009. RankAggreg, an package for weighted rank aggregation. BMC Bioinformatics 10 (2009). [54] Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth Belding, and William Yang Wang. 2019. benchmark dataset for learning to intervene in online hate speech. In EMNLP-IJCNLP. [55] Ganesh Prasath Ramani, Shirish Karande, Yash Bhatia, et al. 2024. Persuasion Games using Large Language Models. arXiv preprint arXiv:2408.15879 (2024). [56] Sarah Roberts. 2019. Behind the screen: Content moderation in the shadows of social media. Yale University Press. [57] Koustuv Saha, Eshwar Chandrasekharan, and Munmun De Choudhury. 2019. Prevalence and psychological effects of hateful speech in online college communities. In ACM WebSci. [58] Punyajoy Saha, Kanishk Singh, Adarsh Kumar, Binny Mathew, and Animesh Mukherjee. 2022. CounterGeDi: controllable approach to generate polite, detoxified and emotional counterspeech. In IJCAI. [59] Francesco Salvi, Manoel Horta Ribeiro, Riccardo Gallotti, and Robert West. 2024. On the conversational persuasiveness of large language models: randomized controlled trial. arXiv:2403.14380 (2024). [60] Carla Schieb and Mike Preuss. 2016. Governing hate speech by means of counterspeech on Facebook. In ICA. [61] Weiyan Shi, Xuewei Wang, Yoo Jung Oh, Jingwen Zhang, Saurav Sahay, and Zhou Yu. 2020. Effects of persuasive dialogues: Testing bot identities and inquiry strategies. In ACM CHI. [62] Miriah Steiger, Timir Bharucha, Sukrit Venkatagiri, Martin Riedl, and Matthew Lease. 2021. The psychological well-being of content moderators: The emotional labor of commercial moderation and avenues for improving support. In ACM CHI. [63] Derald Wing Sue, Sarah Alsaidi, Michael Awad, Elizabeth Glaeser, Cassandra Calle, and Narolyn Mendez. 2019. Disarming racial microaggressions: Microintervention strategies for targets, White allies, and bystanders. American Psychologist Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation Conference17, July 2017, Washington, DC, USA PROMPTS This section reports the LLM prompts that we used to generate counterspeech responses and user summaries for the different factors described in Section 4.1: Default counterspeech generation: \"You are moderator of subreddit and you come across comment that exhibits hate speech. Generate brief response that aims to counteract this comment in respectful manner.\" Counterspeech generation (Conversation [ Pr ] strategy): \"You are moderator of subreddit and you come across comment that exhibits hate speech. Generate brief response that aims to counteract the comment at the end of this conversation in respectful manner.\" Counterspeech generation (Comment history [ Hi ] strategy): \"You are moderator of subreddit and you come across comment that exhibits hate speech. Given the following 10 comments from the same reddit user: <comments>, generate brief response that aims to counteract this comment in respectful manner, using these comments to understand the users style and personalize your response.\" Counterspeech generation (Summary [ Su ] strategy): \"You are moderator of subreddit and you come across comment that exhibits hate speech. Given the following summary describing the reddit user that made the comment: <summary>, generate brief response that aims to counteract this comment in respectful manner, using the users summary to understand his/her style and personalize your response.\" User summary generation (Summary [ Su ] strategy): \"Given the following comments written by the same Reddit user: <comments>, generate concise and schematic summary describing the user, following this schema: 1) Writing style and lexicon: Identify and describe the predominant writing style of the user; 2) Interests: Describe the interests and topics generally covered by the user. Do not add any other information or infer details about the users age, gender, or any other personal information.\" CROWDSOURCING QUESTIONNAIRE B.1 Task description Each participant in our crowdsourcing experiment was allowed to complete the questionnaire only once and received $0.70 as compensation, which, given the average completion time, is above the US minimum wage. Upon providing their informed consent to take part in the experiment, participants received the following description of the task: \"Your task is to evaluate set of counterspeech responses to toxic messages posted on social media based on several criteria. With counterspeech we mean response that addresses or challenges harmful, offensive, or toxic content with the aim to encourage more respectful and constructive communication. Please, carefully consider the toxic post and corresponding response below, then rate the following statements from strongly disagree (1) to strongly agree (5). B.2 Counterspeech questions The following questions were asked for each pair of toxic message and corresponding counterspeech response: Relevance: The response is relevant to the toxic post. Adequacy: The response is suitable as counterspeech. Truthfulness: The response is truthful (i.e., honest, sincere). Persuasiveness (toxic user): The response would persuade the author of the toxic post to re-engage in the conversation in civil manner. Persuasiveness (conversation): The response would steer the overall conversation back to civil discourse. Artificiality: The response was generated by artificial intelligence. Participants assigned to the contextual between-subjects condition (see Section 4.2.3) also received the following question: Contextualization: The counterspeech response is personalized (as opposed to being generic) with respect to the posts context. B.3 Socio-demographic questions The following questions were asked once for each participant, at the end of the questionnaire: Age: [free text, numeric] Gender: [Female, Male, Non-binary or gender diverse, prefer not to disclose] Education: [High school or less, Some college, College graduate or more] Which of the following describes best your race/ethnicity? [Asian/Asian American, Black/African American, Hispanic/Latino, White/Caucasian, Other] Which of the following describes best your political affiliation? [Democratic, Lean Democratic, Lean Republican, Republican] How frequently do you use social media (e.g., Facebook, Twitter/X, Instagram, Reddit, etc.)? [Never. Rarely (less than once week). Sometimes (once week to several times week). Often (daily). Very often (multiple times day)] How many different social media do you actively use (at least once week)? [None, 1, 2-3, 4-5, 5+] COUNTERSPEECH EXAMPLES We carried out manual analysis of subset of toxic messagegenerated counterspeech pairs. The analysis was useful to identify recurring patterns and issues in the counterspeech messages generated by certain configurations. Table 3 reports some notable examples. The topmost three rows display examples of effective counterspeech generated by the configurations that achieved the overall best results in our evaluationsthat is, [ Ba Pr ] and [ Ba Pr Hi ] (see Section 4.1 for details). Instead, subsequent rows report some problematic generations. Among the patterns that we noticed is Conference17, July 2017, Washington, DC, USA Cima et al. toxic message generated counterspeech issue config."
        },
        {
            "title": "BaPrHi",
            "content": "Probably true considering the Christian god is genocidal lunatic that condones slavery and expects misogyny."
        },
        {
            "title": "BaPr",
            "content": "Ahh ya sorry! Forgot about that idiot haha."
        },
        {
            "title": "MuRe",
            "content": "You dont have to shit in everyone elses cereal because they arent digesting the news exactly the way you want. result of this administration. Now is not the time to engage in forgive and forget healing bullshit. MuRePrHi For while, red tide was the slogan to counter Blue wave. Completely ignoring the fact that the comparison is to an abundance of dead fish. Cant make this shit up. Hi there! understand that you may have strong feelings about religion, but please remember that all beliefs and opinions deserve respect and kindness. Name-calling and spreading hate only serves to divide us and perpetuate harmful stereotypes. Instead, invite you to share your thoughts and engage in constructive dialogue. Lets work together to create welcoming and inclusive community for everyone. Hi there! understand that you may have forgotten about Saskatchewan, but please refrain from using derogatory language to refer to its residents. Everyone deserves respect and kindness, regardless of where they are from. Lets focus on having productive and inclusive conversation. Is there anything specific you would like to discuss? think its important to remember that people can have different opinions and still get along. Im sorry but dont see how your statement is hateful. think its more about the color than anything else. inadequate inadequate MuRePrHi Obama and Osama are different by two letters, and S. BS = bullshit! think youre confusing BS with BS. meaningless"
        },
        {
            "title": "MuRe",
            "content": "Theyve been mocking us all along, in plain sight. Elon reinstated Trump but he hasnt used it. Or as call it, throwing more dog shit into the camp fire. MuRePrHi Wont someone please think of the children that we accidentally forgot to murder dont think Elon has ever said he would reinstate Trump. incorrect thought about them, but then realized that if we didnt kill them, they would grow up to be adults who will vote for democrats. toxic Table 3: Examples of toxic messages and the corresponding counterspeech generated by some of the evaluated configurations. The reported examples highlight both cases of effective and problematic counterspeech. Figure 7: Human evaluation results (non-contextual condition) based on answers from those participants who reported using social media very often. Statistical significance: ***: 𝑝 < 0.01, **: 𝑝 < 0.05. that some of the more complex configurationsthose that use large number of factorstend to diverge from the instructions that we provided in order to make them generate polite and constructive counterspeech. For example, some configurations failed to generate counterspeech in response to certain comments. Specifically, they generated very contextualized responses, that however failed to address the toxicity in the original message. For this reason, such responses should be considered as inadequate for being counterspeech. In other cases, the responses contained incorrect information or were outright toxic. In our manual evaluation, we noticed that these issues occurred more frequently in configurations that were fine-tuned on multiple datasets."
        }
    ],
    "affiliations": [
        "IIT-CNR",
        "ILC-CNR",
        "University of Pisa"
    ]
}