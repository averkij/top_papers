{
    "paper_title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs",
    "authors": [
        "Yujie Zhao",
        "Lanxiang Hu",
        "Yang Wang",
        "Minmin Hou",
        "Hao Zhang",
        "Ke Ding",
        "Jishen Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabilities of large language models (LLMs). MAS improves task performance through role-based orchestration, while RL uses environmental rewards to learn stronger policies, such as GRPO-style optimization. However, applying on-policy RL to MAS remains underexplored and presents unique challenges. Algorithmically, standard GRPO grouping assumptions break down because prompts vary by role and by turn. System-wise, the training stack must support MAS-workflow rollouts and on-policy updates for both single-policy and multi-policy models. We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL algorithm tailored to MAS and (ii) a training system that supports both single- and multi-policy regimes. Across game, planning, coding, and math tasks, AT-GRPO delivers substantial gains. On long-horizon planning, it increases accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5 percent. It also improves reasoning performance, with average gains of 3.87 to 7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and environments are available at: https://github.com/pettingllms-ai/PettingLLMs."
        },
        {
            "title": "Start",
            "content": "STRONGER TOGETHER: ON-POLICY REINFORCEMENT LEARNING FOR COLLABORATIVE LLMS Yujie Zhao1 Lanxiang Hu1 Yang Wang2 Minmin Hou2 Hao Zhang1 Ke Ding2 1University of California, San Diego 2Intel Corporation Jishen Zhao1 5 2 0 2 4 ] . [ 2 2 6 0 1 1 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Multi-Agent System (MAS) and Reinforcement Learning (RL) are both widely adopted to improve large language model (LLM) agentic performance. MAS strengthens task-specialized performance via role-based orchestration; RL leverages environment rewards to train stronger policies, such as Group Relative Policy Optimization (GRPO)-style optimization. Yet applying on-policy RL training to MAS is underexplored. While promising, it poses several challenges. On the algorithm side, Standard GRPO grouping assumptions fail in MAS because prompts differ by role and turn. On the system side, the training system needs to support MAS-workflow-based rollouts and on-policy updates for both single and multiple policy models. To address these issues, we introduce AT-GRPO, consisting of (i) an Agentand Turn-wise grouped RL algorithm tailored for MAS and (ii) system to support both single-policy and multi-policy training. Across game, plan, coding, and math tasks, AT-GRPO demonstrates substantial performance gains across diverse domains. Especially on long-horizon planning tasks, AT-GRPO boosts accuracy from 14.047.0% single-agent RL baseline to 96.099.5%. Furthermore, it improves reasoning performance, with an average gain of 3.877.62% on coding and 9.0-17.93% on math."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Model (LLM) agents are task-specific workflows (Yao et al., 2023; Xi et al., 2023; Wang et al., 2023b) that utilize LLMs as key components for decision making (Shinn et al., 2023), action taking (Wang et al., 2023a), and tool use (Qian et al., 2025; Schick et al., 2023). LLM agents have demonstrated strong promises across various application domains, such as embodied control (Ahn et al., 2022; Wang et al., 2023a), software engineering (Tao et al., 2024; Yu et al., 2025), expert drug discovery (Liu et al., 2024; Inoue et al., 2024), and scientific ideation and hypothesis testing (Ghafarollahi and Buehler, 2024). Today, two complementary approaches are widely used to improve the performance of LLM agents: multi-agent systems (MAS) and reinforcement learning (RL). RL treats the LLM as policy and iteratively updates its weights to strengthen decision-making: at each iteration, the current model interacts with the environment, collects rule-based rewards, and then computes policy optimization loss to update the parameters (Shao et al., 2024). In practice, this workflow requires training stack that supports both scalable rollouts and online updates, e.g., VERL (Sheng et al., 2025) and AReaL (Fu et al., 2025). MAS typically employs prompt-only augmentation on shared LLM policy for role-based coordination; practical deployments instantiate diverse workflows. Recent studies (Belcak et al., 2025; Chen et al., 2024; Wang et al., 2024) further highlight the potential benefits of role-specialized MAS, which adopts distinct models for different roles, enabling rolespecialized policies in inference. However, the effectiveness of RL training on role-specialized MAS is underexplored. natural next step is to combine the two: using RL to train MAS, such that we gain both stronger learned policies, role-specialized collaboration. However, bringing RL into MAS raises two coupled challenges. First, training MAS may require concurrently launching multiple models, orchestrat1Code and environments are available at: https://github.com/pettingllms-ai/PettingLLMs"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: MAS+AT-GRPO vs. Single-agent+GRPO. The gray line denotes the prompt-only MAS baseline. ing inter-agent environment interactions, and performing independent on-policy parameter updates. But most existing on-policy RL frameworks for LLM agents only support single model (Volcano Engine, 2025; Sheng et al., 2024; Fu et al., 2025). Second, rollouts from MAS are difficult to group. The advantage must be conditioned on interaction history and role to ensure fair credit assignment. Group-based RL objectives designed for single agent (Volcano Engine, 2025; Qian et al., 2025; Feng et al., 2025) are not directly applicable to MAS. To address these challenges, we first design AT-GRPO, an Agentand Turn-wise grouped RL method that adapts group-relative optimization for MAS. Furthermore, we develop novel training system to support on-policy RL for MAS. Our training system supports rollouts for diverse MAS workflows and enables on-policy RL training for both role-sharing policy and role-specific policies. We conduct extensive experiments on Qwen3 models across range of representative agentic domains, including game, planning, coding, and mathematical reasoning. As highlighted in Fig. 1, AT-GRPO (blue) significantly outperforms single-agent GRPO (red). For instance, it achieves 5.0% higher accuracy (+25.0% relative) on LiveCodeBench (with Qwen3-1.7B), while the improvement increases to 84.0% on Sokoban (with Qwen3-8B). This paper makes the following key contributions: AT-GRPO Algorithm. We introduce an agentand turn-wise grouped RL algorithm, AT-GRPO, and identify the substantial benefits of applying on-policy RL to MAS across diverse domains: planning, gaming, coding and mathematical reasoning tasks. MAS Training System. We design novel training system to support (i) executing rollouts for diverse MAS workflows and (ii) performing on-policy RL updates for multiple policies. Our method delivers consistent gains across diverse domains. On long-horizon planning tasks, it overcomes key bottleneck of single-agent RL, boosting accuracy from 1447% baseline to 96.0-99.5%. Furthermore, it also demonstrates gains on code and math benchmarks, with average improvements of 3.877.62% and 9.017.93%, respectively. Our analysis shows that (1) RL training on MAS reinforces role-specific specialization; (2) with MAS AT-GRPO, whether to choose role-sharing policy or role-specialized policies needs to be determined by the task characteristics."
        },
        {
            "title": "2 RELATED WORK",
            "content": "RL for LLM Agentic Training. RL has become key technique for LLMs agent training, using group-relative and rule-based rewards to enhance reasoning, long-horizon planning, game, and tool use (Shao et al., 2024; Wang et al., 2025b; Qian et al., 2025; Hu et al., 2025). These approaches, however, predominantly operate within single-agent framework. Although effective for certain benchmarks, this paradigm offers limited potential for improvement, as it relies on single agent for planning and neglects the inherent advantages of MAS for complex coordination and specialization, thereby constraining further breakthroughs. Role-sharing vs. Role-specialized Policies in MAS. predominant approach in LLM-based MAS centers on role-sharing architecture, where single policy is shared across all agents. In these frameworks, such as AutoGen (Wu et al., 2023) and MetaGPT (Hong et al., 2024), role-specific behavior is elicited at inference time via prompt augmentation. More recently, research has begun to explore role-specialized policies. This shift is motivated by the observation that single LLMs performance exhibits significant variance across domains (Chen et al., 2024; Wang et al., 2024; Belcak et al., 2025). Consequently, assigning distinct and more suitable models to specialized roles,"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: MAS workflow across different domains. (a) Role-based coordination: code generation via codertester loop. (b) Different task-specific workflows for Game/Plan, Code, and Math; see Sec. 5.1 and Appendix A.2.2 for workflow details. as demonstrated by Ye et al. (2025); Belcak et al. (2025), has emerged as promising direction for enhancing performance. Despite this architectural evolution, recent surveys (Cemri et al., 2025; Guo et al., 2024) indicate that most studies focus on inference-time design, leaving the potential of training MAS policies with RL largely underexplored. RL Training for MAS. growing body of work tries to bring RL to MAS, but most efforts are confined to single interaction and role-sharing policy pattern. MAPoRL (Park et al., 2025a;b), CoRY Ma et al. (2024) train LLMs as each agent proposes and revises answers to the same query within shared discussion and debate with each other, CURE (Wang et al., 2025a) co-evolves Coder and Unit-Tester with role-sharing policy for code generation; SPIRAL (Liu et al., 2025) trains via self-play on zero-sum games using single LLM; and MHGPO (Chen et al., 2025a) targets retrieval-augmented generation, coupling group-based objectives with role-sharing policy around retrieval, routing, and response selection. Compared with these approaches, our study is comprehensive: we evaluate across diverse MAS workflows from different domains, and comprehensively compare role-sharing versus role-specific policies."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "i=1, , {ri}N MAS Setting. We model an -agent, LLM-based multi-agent system as Markov game = (S, {Ai}N i=1, T, H), where is the state space; Ai is the action space of agent i; is transition map st+1 = (st, a1,t, . . . , aN,t); The reward for agent is given by ri : Ai [0, 1], and the turn horizon , the optimization step horizon H. At each turn t, agent receives an observation summarizing the environment state and interaction history ht, oi,t = oi(st, ht). Each agent is implemented with role-specific prompt template Pi(). Let Θ = {θ(m)}M m=1 denote the set of LLM parameter vectors, with 1 , and let σ : {1, . . . , } {1, . . . , } assign each agent to an LLM. We treat one LLM rollout (a token sequence) as single macro-action ai,t. turn is one full interaction in which all agents emit macro-actions to the environment. step denotes one optimization update to the parameter set Θ during training. MAS Workflow. Following prior work (Wang et al., 2025a; Ahn et al., 2022; Chen et al., 2025b), we employ domain-specific MAS workflows, as shown in Fig. 2. Our experiments confirm that this prompt-only method outperforms single-agent baseline (see Tab. 1 and 2 in Sec. 5.2). Group-based RL. Methods for LLM agentic training with group-relative advantages (Feng et al., 2025; Wang et al., 2025b; Qian et al., 2025) operate by first sampling candidate actions {a(k) for given prompt. Each action is evaluated to obtain rule-based reward R(a(k) }K k=1 ), forming"
        },
        {
            "title": "Preprint",
            "content": "comparison group: = (cid:8) (a(1) group, the relative advantage is then defined as its mean-centered and normalized return. )) (cid:9). For each action a(k) . . . , (a(K) , R(a(K) , R(a(1) )), t AE(cid:0)a(k) (cid:1) = R(a(k) ) mean (cid:16) (cid:16) { R(a(ℓ) ) }K (cid:17) ℓ= (cid:17) ,"
        },
        {
            "title": "Fnorm",
            "content": "{ R(a(ℓ) ) }K ℓ=1 in this (1) where Fnorm() is the sample standard deviation. Role-sharing vs. Role-specialized Policy Optimization. We distinguish between two optimization regimes, role-sharing and role-specialized, both of which initialize policies from the same base model. During rollouts, each agent generates dataset Di, which consists of sample groups. single group is composed of shared observation context og and candidate actions with their corresponding advantages, denoted as = {i, a(c) c=1. The core difference between the two regimes lies in how the training data is batched. minibatch Bm for specific policy θ(m) is constructed by pooling the datasets from all agents assigned to it: , A(c) }K Bm = (cid:91) Di. : σ(i)=m Each policy is then updated using GRPO-style objective over its corresponding minibatch: L(θ(m)) = EgBm (cid:34) 1 (cid:88) c=1 log πθ(m) (cid:0)a(c) Pi(og)(cid:1) A(c) (cid:35) . (2) (3) Role-sharing policy (M =1): All agents share single policy θ1. The training batch is the union of data from all agents, B1 = (cid:83)N i=1 Di, and is used for single joint update: θ1 θ1 ηθ1L(θ1). Role-specialized policies (M = ): Each agent has distinct policy θ(i), such that σ(i) = i. Each policy is updated independently on Bi = Di, and update policy: θ(i) θ(i) ηθ(i)L(θ(i))."
        },
        {
            "title": "4 METHOD",
            "content": "4.1 ALGORITHM DESIGN: AT-GRPO GRPOs advantage calculation (Eq. 1) hinges on fair comparison among all candidates within group. This fairness is enforced by the reward mechanism itself. As illustrated token-level scoring assigns in Fig. 3 (top), credit to the generated response tokens (Reward Mask=1), while the prompt tokens receive no credit (Reward Mask=0). Since the advantage is determined solely by the quality of the response, valid and fair comparison is only possible when all responses in group originate from an identical prompt. Consequently, single-agent LLM-RL methods(Wang et al., 2025b; Qian et al., 2025; Feng et al., 2025) typically form groups by sampling multiple responses to the same question. Figure 3: Two sampling schemes. (a) In parallel sampling, trajectories are sampled but incomparable, leading to groups of size 1. (b) In tree sampling, branching at each turn forms valid comparison group of size K. In MAS, however, prompt is not only question description, but also embeds the role-specific context and cross-agent interaction history. For example, in code debugging (Fig. 3, middle), the turn-2 refinement prompt already contains the turn-1 code, unit tests, and role-specific prompt format, so prompts differ across turns and roles. Thus, grouping by same question no longer yields comparable samples. We therefore adopt agent-wise and turn-wise grouping: candidates share the same role and turn position, ensuring prompt identity for valid GRPO advantage comparisons. However, agentand turn-wise grouping introduces new question. If we follow the common parallel sampling used by prior agentic RLsample full trajectories from the initial state/problem"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 AT-GRPO: Agentand Turn-wise MAS RL Training Require: Markov game M, policies Θ = {θ(m)}M m=1, role mapping σ, sampling temperature Tsamp, branches K, total steps S, batch size E, turn horizon , termination condition Iterm. /* Termination helper: returns true if horizon reached or env signals done */ 1: for training step = 1, . . . , do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: /* Phase 1: On-Policy Rollout & Data Collection */ Initialize per-agent datasets {Di}N for each environment instance {1, . . . , E} in parallel do i=1 . Resample environments. Get initial state s0,e. for = 0 to 1 do for each agent {1, . . . , } do {1, . . . , K}, sample a(c) Define group key hash(e, i, t) and compute advantages {A(c) i,t,e πθ(σ(i))( oi,t,e; Tsamp); compute r(c) i,t,e(Eq. 4) c=1 (Eq. 1). i,t,e}K (Agentand turn-wise grouping.) Append (g, oi,t,e, {a(c) arg maxc r(c) c=1, {A(c) i,t,e}K i,t,e}K i,t,e; ai,t,e a(c) i,t,e. (Tree-structured sampling.) c=1) to Di. end for st+1,e (cid:0)st,e, a1,t,e, . . . , aN,t,e if Iterm(st+1, e) then break end if (cid:1). end for end for /* Phase 2: Per-Model Policy Update */ for each model {1, . . . , } in parallel do Construct per-model batch Bm using Eq. 2. Compute loss L(θ(m)) on Bm using Eq. 3 and update policy m. 17: 18: 19: 20: 21: end for end for (Fig. 3 (a), bottom), each group size = 1 when > 1: no other sample shares the identical prompt. GRPO therefore eliminates its variance-reduction effect and yields unstable updates. To address these challenges, we develop AT-GRPO (see Alg. 1) with three key ideas: tree-structured sampling, agent and turn-wise grouping, and agent-wise credit assignment. Tree-structured Sampling. At each turn t, for each agent i, we sample candidate actions and their corresponding rewards from the current state (Alg. 1, line 7). The advantages for these candidates are then calculated within this group (line 9). Subsequently, the full data tuplecontaining the group key, observation, actions, and their advantagesis added to dataset Di specific to the policy of the acting agent (line 10). To proceed with the environment rollout, we greedily select the candidate with the highest reward to be the executed action (line 11). This greedy selection strategy concentrates exploration on coordination-critical decisions and helps maintain balanced mix of positive and negative samples, which stabilizes the learning optimization. Agent and Turn-wise Grouping. We group experiences based on the acting agent and the turn number within each parallel environment instance. Operationally, we implement this by defining unique group key for each agent at each turn in each environment using lightweight hash function (Alg. 1, line 8). All data generated from the K-branch sampling at that step, including the observation and the calculated advantages, is stored together under this group key (line 10). During the policy update phase, these collected data groups are used to construct per-model training batches for the final optimization step (lines 2021). Agent-wise Credit Assignment. Inspired by mixed-reward designs in cooperative Multi-Agent RL (Mao et al., 2020; Sheikh and Boloni, 2020), we assign credit using mixture of global and local rewards. At each turn t, the environment provides global team reward rteam and an agentspecific local reward rloc that evaluates its subtask performance. These components are combined"
        },
        {
            "title": "Preprint",
            "content": "using hyperparameter α to form the final reward for agent i: ri,t = α rteam + rloc (4) This formulation balances shared team objective with role-specific incentives. For instance, in coder-tester MAS, the team reward rteam is the pass rate of the generated program on set of golden unit tests. The local rewards rloc are tailored to each role: the coder is rewarded for its own codes pass rate, while the tester is rewarded based on the pass rate of golden reference implementation against its generated tests. Detailed reward designs for all tasks are provided in Appendix A.2.1. i"
        },
        {
            "title": "4.2 MAS TRAINING SYSTEM",
            "content": "Mainstream RL post-training frameworks for LLMs, e.g., TRL (von Werra et al., 2020), VERL (Sheng et al., 2024), AReaL (Fu et al., 2025), and OpenRLHF (Hu et al., 2024) primarily support single-agent RL training, which typically involves: single agent-environment interaction pattern, single policy operating on single data buffer, and single LLM resource pool. This makes it difficult to (i) train multiple models in on-policy RL, (ii) maintain clean on-policy training data, and (iii) support diverse MAS workflow. We introduce novel MAS training system to overcome these challenges and enable AT-GRPO in Alg. 1. By allocating an independent resource pool to each model, our system is designed to support the concurrent on-policy training of multiple policies. The system, depicted in Fig. 4, consists of the following components: Figure 4: MAS training system. Each LLM has GPU-pinned model pool with RolloutWorker and an UpdateWorker . CPU environment pool hosts envworkers that execute environment steps. Trajectories are routed to the corresponding UpdateWorker. LLM Resource Pools (GPU). Each policy is managed within an independent resource pool. Following HybridFlow-style (Sheng et al., 2025), each pool comprises two workers: RolloutWorker for inference and an UpdateWorker for optimization. During the rollout phase, all policies interact collectively according to the Alg. 1 and MAS workflow; Once collected, each trajectory is routed to the corresponding UpdateWorker, maintaining an on-policy learning regime for every policy. Environment Execution (CPU) and Data Flow. Environment steps run in fleet of CPU EnvWorkers, each managing single sandboxed instance to ensure safety and reproducibility (seeding, wall-clock timeouts, IO quotas, and deterministic tool harnesses). This one-actor-per-instance mapping efficiently supports thousands of concurrent rollouts in parallel. EnvWorkers stream observations, tool logs, and rule-based rewards to Router. The Router dispatches collected experience based on policy assignment: experiences generated by an agent are sent to the Updateworker of its designated policy σ(i)."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 DATASETS AND MODELS. 1. Experimental Setup. We train and evaluate Qwen3 models at 1.7B and 8B in the no-thinking mode (Yang and the Qwen Team, 2025). All runs use single node with 8 H100 GPUs. The rollout sample size is K=4 and the turn horizon is = 4. The reward-mixing coefficient is α=1 without further tuning. Full training details appear in Appendix A.2.1. 2. Baselines. We evaluate five variants (all initialized from the same base model): (a) Single Agent (prompt-only): one frozen LLM solves the task end-to-end; (b) Single Agent + GRPO: as (a) but"
        },
        {
            "title": "Preprint",
            "content": "trained with GRPO (Shao et al., 2024); (c) MAS (prompt-only): role-specialized prompting over frozen, role-sharing backbone; (d) MAS + RL (role-sharing policy): all roles share one policy and pooled trajectories update it jointly; (e) MAS + RL (role-specialized policies): samples are routed by role and each policy is optimized independently (no parameter sharing). 3. Task Setups. For each task, we compare multi-agent (MA) workflow to single-agent (SA) counterpart under the same number of environment workers and turn budget K. Full prompt templates, role instructions, and rule-based rewards are provided in Appendix A.2.2. Code MA: Tester builds/refines unit tests; Coder implements/refines code; tests execute every turn; reward is per-case/aggregate passfail; termination when alignment is achieved or turns reach K. SA: one agent emits code; single-turn termination (no verification loop). Math MA: Tool agent issues Python/calculator calls; Reasoner produces the final answer; reward is exact-match or verifier-checked; termination upon success or when turns reach K. SA: one agent performs reasoning (with direct tool calls if any) in single turn; single-turn termination. Plan/Game MA: Planner proposes actions; Executor calls tools and returns effects/observations; reward reflects step/terminal goal satisfaction (e.g., Plan-Path, Sudoku, Sokoban); termination when the goal is met or turns reach K. SA: one agent outputs plan (same termination condition). 4. Training and Evaluation Datasets. Sudoku and Sokoban. We evaluate our method on gaming tasks: 44 Sudoku and 66 Sokoban. We use instances with an automatic checker, following the symbolic task setup of SYMBENCH (Chen et al., 2025b). To ensure fair evaluation, we generate distinct training and validation sets using different random seeds and verify there is no overlap. Plan-Path. We use 1010 grid-based Plan-Path environment. This follows the checker-backed symbolic task setup in CodeSteers SymBench (Chen et al., 2025b). To separate training and validation, we generate the two splits with distinct random seeds and verify no duplication. Code Generation. For training, we adopt size-specific corpora: the 1.7B Qwen model is trained on the APPS training split (introductory-difficulty subset) (Hendrycks et al., 2021), while the 8B model is trained on CodeContests (DeepMind, 2024). For model-generated code, we use the datasets golden unit tests to score correctness; for model-generated UT, we use the datasets golden reference solutions to compute the reward. For evaluation, we use three widely adopted coding benchmarks spanning interview-style and contest-style settings: APPS (Hendrycks et al., 2021), LiveCodeBench-v6 (White et al., 2024), and CodeContests (DeepMind, 2024). Mathematical Reasoning. We train on the Polaris-Dataset-53K (An et al., 2025) and evaluate on several standard mathematical reasoning benchmarks. For validation, we use AIME24/AIME25 (Mathematical Association of America & AoPS Community, 2024; 2025) and OLYMPIADBENCH (He et al., 2024). All math tasks use verifier-checked numeric scoring. 5.2 RESULTS AND ANALYSIS We evaluate AT-GRPO across four distinct domains (game, planning, code, and math) using two model scales (Qwen3 1.7B and 8B). To contextualize its performance, we benchmark against all the variants described in Sec. 5.1. Tab. 1 and Tab. 2 summarize our main results. MAS + AT-GRPO consistently yields substantial performance gains, especially in long-horizon planning tasks. This improvement is even more pronounced with the Qwen3 8B model, where MAS + AT-GRPO elevates the success rate from 1447% range for the single-agent baseline to 96.099.5%. By analyzing the dialogue records between agents, we find this dramatic improvement stems from an emergent collaboration: the tool agent learns to generate correct algorithms (e.g., BFS, search), while the plan agent provides crucial oversight, interpreting execution outcomes and delivering the corrective final action list. On-policy RL training within the MAS enhances interagent coordination. Conversely, training agents in isolation results in only limited improvement, as detailed in our ablation study (Sec. 5.3, Tab. 3). Furthermore, on the coding and math benchmarks, our approach yields consistent gains, with absolute gains over the baseline ranging from +2.35 (CodeContests) to +16.30 (APPS) in coding, and from +1.80 (OlympaidBench) to +38.70 (AIME24)"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Qwen3 1.7B results on game, planning, coding, and math. Method Sudoku Sokoban Game Plan Code Plan-Path LiveCodeBench APPS CodeContests AIME24 AIME25 OlympiadBench Math Single agent Single agent + GRPO MAS MAS + AT-GRPO w/ shared policy MAS + AT-GRPO w/ per-role policies 7.00 (+0.00) 29.00 (+22.00) 69.00 (+62.00) 99.00 (+92.00) 99.00 (+92.00) 0.00 (+0.00) 3.00 (+3.00) 0.00 (+0.00) 10.00 (+10.00) 11.50 (+11.50) 5.00 (+0.00) 11.00 (+6.00) 10.00 (+5.00) 96.00 (+91.00) 97.00 (+92.00) 11.60 (+0.00) 18.80 (+7.20) 19.00 (+7.40) 20.90 (+9.30) 24.00 (+12.40) 16.20 (+0.00) 17.00 (+0.80) 16.60 (+0.40) 17.60 (+1.40) 18.60 (+2.40) 3.60 (+0.00) 3.00 (-0.60) 3.60 (+0.00) 4.80 (+1.20) 7.80 (+4.20) 13.40 (+0.00) 10.00 (-3.40) 13.30 (+-0.10) 16.70 (+3.30) 13.30 (+-0.10) 9.80 (+0.00) 6.70 (-3.10) 13.00 (+3.20) 16.70 (+6.90) 18.30 (+8.50) 22.20 (+0.00) 23.80 (+1.60) 35.90 (+13.70) 39.60 (+16.80) 35.20 (+13.00) Table 2: Qwen3 8B results on game, planning, coding, and math. Method Sudoku Sokoban Game Plan Code Plan-Path LiveCodeBench APPS CodeContests AIME24 AIME25 OlympiadBench Math Single agent Single agent + GRPO MAS MAS + AT-GRPO w/ shared policy MAS + AT-GRPO w/ per-role policies 48.00 (+0.00) 54.00 (+6.00) 72.00 (+24.00) 99.50 (+51.50) 99.00 (+51.00) 9.00 (+0.00) 14.00 (+5.00) 16.00 (+7.00) 96.00 (+87.00) 98.00 (+89.00) 12.00 (+0.00) 47.00 (+35.00) 71.00 (+59.00) 93.00 (+81.00) 96.00 (+84.00) 22.80 (+0.00) 25.70 (+2.90) 28.00 (+5.20) 30.28 (+7.48) 33.10 (+10.30) 30.20 (+0.00) 37.00 (+6.80) 44.40 (+14.20) 45.80 (+15.60) 46.50 (+16.30) 15.75 (+0.00) 12.12 (-3.63) 17.60 (+1.85) 18.10 (+2.35) 18.10 (+2.35) 18.30 (+0.00) 18.30 (+0.00) 36.60 (+18.30) 50.00 (+31.70) 57.00 (+38.70) 20.00 (+0.00) 26.67 (+6.67) 30.00 (+10.00) 35.20 (+15.00) 40.00 (+20.00) 55.00 (+0.00) 54.80 (-0.20) 56.50 (+1.50) 56.80 (+1.80) 56.60 (+1.60) Parentheses denote gain over the Single Agent baseline; best and second-best results per column are highlighted. Table 3: Plan-Path (Qwen3-1.7B) ablation. Performance gain over the single agent baseline. Method Single agent Training tool agent in SA, eval in SA Training code agent in SA, eval in SA Training in SA, eval in MAS MAS RL (role specific policies), eval in MAS w/ Swapped Policies Acc.(%) 5.00 11.00 14.50 16.00 96.00 6.00 +6.00 +9.50 +11.00 +91.00 +1.00 in math. We hypothesize two reasons: (1) Base models like Qwen3 have already been extensively trained for these common domains, as noted in their official reports (Yang and the Qwen Team, 2025), potentially leading to performance saturation. (2) The diverse nature of problems within these domains presents greater challenge for improvement via RL training. With MAS AT-GRPO, whether choosing role-sharing policy or role-specialized policies should be determined by the task characteristics. Role-specialized policies involve fundamental trade-off: training each agent exclusively on its own data fosters deep specialization, but prevents access to potentially useful data from other roles. Our findings indicate that the optimal resolution to this trade-off depends on the task characteristics. We observe clear benefits for role specialization in the coding domain, where the Coder and Tester functions are highly distinct. This separation allows each agent to hone its specific skills, improving the average accuracy by 3.05 points with the Qwen3 1.7B.In contrast, the roles in the math domain exhibit greater functional overlap, meaning shared policy can sometimes be superior. For instance, with the Qwen3 1.7B model on OlympiadBench, the shared policy achieves 39.60% accuracy, surpassing the 35.20% from per-role policies. This suggests the Tool agent, which must often perform reasoning to execute tool calls, benefits from the Reasoners training data. For game/plan tasks, this choice becomes moot, as both configurations already achieve near-optimal, saturated performance (e.g., 99.50 on Sudoku). 5.3 ABLATION STUDY To further investigate the contributions of our core training components, we conducted an ablation study with results summarized in Tab. 3 and Fig. 5. Our analysis yields several observations."
        },
        {
            "title": "Preprint",
            "content": "First, on-policy RL training within MAS environment is critical for effective collaboration. As shown in Tab. 3, training agents in single-agent (SA) setting offers limited benefits: while individual agents improve their specialized skills (achieving 11.00 and 14.50 accuracy, respectively), their performance when combined in MAS is only marginally better, reaching just 16.00. In stark contrast, training the agents jointly within the MAS environment boosts accuracy to 96.00. This vast performance gap demonstrates that multi-agent training is essential. It not only allows agents to coevolve highly specialized abilities but also fosters the crucial inter-agent alignment and collaboration required for success. Second, RL training on MAS reinforces role-specific specialization. We observe this across multiple metrics. As shown in Fig. 5 (a) for Qwen3 1.7B on Plan-Path, the learning rewards of both the planning and tool-using agents increase throughout training, suggesting coordinated co-evolution as each adapts to the others improving policy. Consistent with the ablation, after training two role-specialized policies with our full method, swapping them induces catastrophic drop from 96.0% to 6.0%, confirming that the agents have learned distinct and complementary functions that are not interchangeable. In our coding (LiveCodeBench) and math (AIME25) workflows, MAS interaction terminates when the two agents align (e.g., tests pass or the reasoner and tool outputs agree). Accordingly, Fig. 5 (b) shows that the average number of turns needed to solve task decreases over training, providing direct evidence that the agents achieve tighter alignment and collaborate more efficiently. Figure 5: (a) Evolution of standardized rewards for the Tool and Plan agents in the role-specific MAS on Plan-Path with Qwen3 1.7B. Solid curves denote the run-averaged mean rewards; shaded bands show variability across runs. (b) Evolution of the average turns required to solve tasks on coding and math."
        },
        {
            "title": "6 CONCLUSION AND DISCUSSION",
            "content": "Conclusion. In this paper, we proposed AT-GRPO, an agentand turn-wise grouped reinforcement learning algorithm tailored for on-policy training in MAS. To support this, we introduced novel training system capable of managing diverse MAS workflows and performing on-policy updates for multiple policies. Our extensive experiments demonstrate that our method delivers consistent gains across diverse domains. On long-horizon planning tasks, it overcomes key bottleneck of singleagent RL by boosting accuracy from 1447% baseline to 96.099.5%. Furthermore, it improves complex reasoning performance, with average gains of 3.877.62% on coding and 9.017.93% on math tasks. Our analysis reveals that RL training in MAS context reinforces role-specific specialization, with the choice between shared or specialized policy contingent on the tasks characteristics. Limitations. While our work demonstrates the effectiveness of on-policy training in MAS, this work focuses exclusively on cooperative tasks. Investigating the adaptability of on-policy RL to mixed-motive or competitive settings remains an important open area. Also, our experiments are confined to text-based environments, promising future direction is to explore the collaboration between Vision Language Models (VLMs) and LLMs, which is potential opportunity to unlock new capabilities in robotics and embodied AI."
        },
        {
            "title": "7 ETHICS STATEMENT",
            "content": "We study multi-agent reinforcement learning for large language models on planning, coding, and math tasks. Our experiments are purely computational and use public benchmarks (e.g., programmatically generated Plan-Path/Sudoku instances and widely available coding/math datasets) together with self-constructed simulators and verifiers. No human subjects, sensitive personal data, or proprietary content are involved. Code execution is performed in sandboxed environment with restricted file I/O and no network access; tool calls are limited to deterministic checkers to prevent unintended side effects. While our methods are intended to improve reliability and sample-efficiency of agentic LLMs, we recognize dual-use risks common to autonomous systems (e.g., unsafe tool use or over-delegation). To mitigate these risks, we avoid external system operations, log all actions for auditability, and refrain from releasing any configurations that grant networked or privileged execution. We also note that base LLMs may encode societal biases that our training does not remove; results should therefore not be used for high-stakes decisions. We will release prompts, generators, and evaluation scripts to support reproducibility, subject to dataset licenses and safe-use guidelines."
        },
        {
            "title": "8 REPRODUCIBILITY STATEMENT",
            "content": "To ensure the reproducibility of our results, we have made our datasets, code, and experimental details available. All datasets used in this study are publicly available; we provide detailed descriptions of these datasets and all data preprocessing steps in Sec. 5.2 and Appendix A.2.1. The source code used for our experiments is included in the supplementary material. Upon acceptance, we will release the complete, documented source code under permissive open-source license to facilitate the reproduction of all presented results. Key hyperparameters, model architectures, and training configurations are also detailed in Appendix A.2.1."
        },
        {
            "title": "9 USE OF LLM",
            "content": "During the preparation of this manuscript, large language model was utilized to aid in polishing the grammar and improving the clarity of the text. The authors reviewed and edited all outputs to ensure the final content accurately reflects our original ideas and are fully responsible for all statements and conclusions presented."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Ahn, Anthony Brohan, Noah Brown, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv:2204.01691, 2022. URL https://arxiv.org/abs/2204.01691. Chenxin An, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu, Xipeng Qiu, and Xuanjing Huang. Polaris: post-training recipe for scaling reinforcement learning on advanced reasoning models, 2025. URL https://hkunlp.github.io/blog/2025/Polaris. Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. Small language models are the future of agentic ai, 2025. Mert Cemri, Melissa Z. Pan, Shuyi Yang, Lakshya A. Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, Matei Zaharia, arXiv preprint Joseph E. Gonzalez, and Ion Stoica. Why do multi-agent llm systems fail? arXiv:2503.13657, 2025. Guanzhong Chen, Shaoxiong Yang, Chao Li, Wei Liu, Jian Luan, and Zenglin Xu. Heterogeneous group-based reinforcement learning for llm-based multi-agent systems, 2025a. URL https:// arxiv.org/abs/2506.02718. Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Internet of agents: Weaving web of heterogeneous agents for collaborative intelligence. arXiv preprint arXiv:2407.07061, 2024."
        },
        {
            "title": "Preprint",
            "content": "Yongchao Chen, Yilun Hao, Yueying Liu, Yang Zhang, and Chuchu Fan. Codesteer: Symbolicaugmented language models via code/text guidance. arXiv preprint arXiv:2502.04350, 2025b. doi: 10.48550/arXiv.2502.04350. DeepMind. Codecontests. https://github.com/google-deepmind/code contests, 2024. GitHub repository; archived Dec 6, 2024. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. W. Fu et al. AReaL: large-scale asynchronous reinforcement learning system for llms. arXiv:2505.24298, 2025. URL https://arxiv.org/abs/2505.24298. Alireza Ghafarollahi and Markus J. Buehler. Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning, 2024. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In ACL, 2024. URL https://arxiv.org/abs/2402.14008. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Andy Zou, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with apps. arXiv:2105.09938, 2021. URL https://arxiv.org/abs/2105.09938. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for multi-agent collaborative framework. International Conference on Learning Representations, ICLR, 2024. Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Lanxiang Hu, Mingjia Huo, Yuxuan Zhang, Haoyang Yu, Eric Xing, Ion Stoica, Tajana Rosing, arXiv lmgame-bench: How good are llms at playing games? Haojian Jin, and Hao Zhang. preprint arXiv:2505.15146, 2025. Yuki Inoue et al. Drugagent: Multi-agent large language model-based reasoning for drug-target interaction prediction, 2024. Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, Wee Sun Lee, and Natasha Jaques. Spiral: Self-play on zero-sum games incentivizes reasoning via multi-agent multi-turn reinforcement learning, 2025. URL https://arxiv.org/abs/2506.24119. Shengchao Liu et al. Drugagent: Automating ai-aided drug discovery programming through llm multi-agent collaboration, 2024. Coevolving with the other you: Hao Ma, Tianyi Hu, Zhiqiang Pu, Boyin Liu, Xiaolin Ai, Yanyan Liang, and Min Fine-tuning llm with sequential cooperative Chen. In Advances in Neural Information Processing Sysmulti-agent reinforcement tems (NeurIPS), 2024. URL https://proceedings.neurips.cc/paper files/paper/2024/file/ 1c2b1c8f7d317719a9ce32dd7386ba35-Paper-Conference.pdf. learning. Hangyu Mao, Zhibo Gong, and Zhen Xiao. Reward design in cooperative multi-agent reinforcement learning for packet routing. arXiv preprint arXiv:2003.03433, 2020. URL https://arxiv.org/ abs/2003.03433."
        },
        {
            "title": "Preprint",
            "content": "Mathematical Association of America & AoPS Community. Aime 2024 problems (aops wiki). https://artofproblemsolving.com/wiki/index.php/2024 AIME & https:// artofproblemsolving.com/wiki/index.php/2024 AIME II Problems, 2024. Accessed 202509-11. Mathematical Association of America & AoPS Community. Aime 2025 problems (aops wiki). https://artofproblemsolving.com/wiki/index.php/2025 AIME Problems & https: //artofproblemsolving.com/wiki/index.php/2025 AIME II Problems, 2025. Accessed 2025-09-11. Chanwoo Park, Seungju Han, Xingzhi Guo, Asuman E. Ozdaglar, Kaiqing Zhang, and Joo-Kyung Kim. Maporl: Multi-agent post-co-training for collaborative large language models with reinforcement learning. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL), Vienna, Austria, 2025a. Association for Computational Linguistics. doi: 10.18653/v1/2025.acl-long.1459. URL https://aclanthology.org/2025.acl-long.1459/. Chanwoo Park, Seungju Han, Xingzhi Guo, Asuman E. Ozdaglar, Kaiqing Zhang, and Joo-Kyung Kim. Maporl2: Multi-agent post-co-training for collaborative llms with reinforcement learning. OpenReview preprint, 2025b. URL https://openreview.net/pdf?id=f85TQ7hyzh. Influenceaware verification reward; multi-agent PPO with learned verifier. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tur, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. URL https://arxiv.org/abs/2504.13958. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv:2302.04761, 2023. URL https://arxiv.org/abs/2302. 04761. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Hassam Ullah Sheikh and Ladislau Boloni. Multi-agent reinforcement learning for problems with combined individual and team reward. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), 2020. URL https://arxiv.org/abs/2003.10598. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv:2409.19256, 2025. URL https://arxiv.org/abs/2409.19256. Noah Shinn, Federico Cassano, Edward Berman, et al. Reflexion: Language agents with verbal reinforcement learning. arXiv:2303.11366, 2023. URL https://arxiv.org/abs/2303.11366. Wei Tao, Yucheng Zhou, Yanlin Wang, Wenqiang Zhang, Hongyu Zhang, and Yu Cheng. In NeurIPS https://proceedings.neurips.cc/paper files/paper/2024/file/ Magis: 2024, 5d1f02132ef51602adf07000ca5b6138-Paper-Conference.pdf. Llm-based multi-agent 2024. framework for github issue resolution. URL Volcano Engine. VERL: Volcano engine reinforcement learning for llms. https://github.com/ volcengine/verl, 2025. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouedec. TRL: Transformer Reinforcement Learning. https://github.com/huggingface/trl, 2020. GitHub repository. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, et al. Voyager: An open-ended embodied agent with large language models. arXiv:2305.16291, 2023a. URL https://arxiv.org/abs/2305.16291."
        },
        {
            "title": "Preprint",
            "content": "Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents enhances large language model capabilities. arXiv preprint arXiv:2406.04692, 2024. Liang Wang, Qian Liu, Kun Song, et al. survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023b. URL https://arxiv.org/abs/2308.11432. Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, and Mengdi Wang. Co-evolving llm coder and unit tester via reinforcement learning, 2025a. URL https://arxiv.org/abs/2506.03136. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025b. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid ShwartzZiv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 2024. URL https://arxiv.org/abs/2406.19314. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Awadallah, Ryen W. White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023. Zhizheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Li, Gongshen Cui, Yong Dou, Junzhe Zhou, Bo An, et al. The rise and potential of large language model based agents: survey. arXiv preprint arXiv:2309.07864, 2023. An Yang and the Qwen Team. Qwen3 technical report. arXiv:2505.09388, 2025. URL https: //arxiv.org/abs/2505.09388. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv:2210.03629, 2023. URL https://arxiv.org/abs/2210.03629. Rui Ye, Xiangrui Liu, Qimin Wu, Xianghe Pang, Zhenfei Yin, Lei Bai, and Siheng Chen. X-mas: Towards building multi-agent systems with heterogeneous llms. arXiv preprint arXiv:2505.16997, 2025. Zhongming Yu, Hejia Zhang, Yujie Zhao, Hanxian Huang, Matrix Yao, Ke Ding, and Jishen arXiv preprint Zhao. Orcaloca: An llm agent framework for software issue localization. arXiv:2502.00350, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 REWARD DESIGN A.1.1 MATH REWARD DESIGN We consider math QA with horizon and optional tool calls. Let hk be the dialogue/tool history at turn k. We adopt MATH-VERIFIER2 as the checker front-end. Define numeric comparator with tolerance δ: NUMEQδ(a, b) = 1 b δ or (cid:26) b max(1, b) (cid:27) δ , δ=106 (default). Team reward. Sparse pass at termination via numerical equality: rteam = 1{k=T, CHECKFINALMATHVERIFIER+NUMEQ(hk)=pass} {0, 1}, λmath = 0.70. Local rewards. Each agent uses convex combination of verifiable sub-scores si (cid:88) (cid:88) ℓ,k [0, 1]: ri,loc = ℓ si ci ℓ,k, ci ℓ = 1. ℓ{fmt,tool,step} ℓ Reasoner local design. Coefficients: cReasoner fmt = 0.20, cReasoner tool = 0.00, cReasoner step = 0.80. Component scores (pure numerical check): (cid:40) sReasoner step,k = sReasoner fmt,k NUMEQδ 0, = 1{required output schema matched at k}, (cid:0)ˆyk, y(cid:1), if MATHVERIFIER extracts numeric ˆyk at k, otherwise, mReasoner = 1{y available (MATHVERIFIER)}. Tool (Python/calculator) local design. Coefficients: cTool fmt = 0.10, cTool tool = 0.10, cTool step = 0.80. Component scores: sTool fmt,k = 1{API/schema valid and within budget at k}, sTool tool,k = 1{execution returns without error/timeout at k}, (cid:0)yk, y(cid:1), sTool step,k = (cid:40) NUMEQδ 0, mTool = 1{y and logs available (MATHVERIFIER)}. if execution emits numeric yk at k, otherwise, A.1.2 CODE REWARD DESIGN We consider code synthesis with unit tests at each turn k. Let Sk be the active test suite and pk = 1 Sk (cid:88) tSk 1{RUN(t, codek) = pass} [0, 1]. Team reward is dense: rteam = pk. Local rewards use fixed coefficients over verifiable sub-scores si ℓ,k [0, 1]: ri,loc = ℓ si ci ℓ,k, (cid:88) ℓ ci ℓ = 1. (cid:88) ℓ 2MATH-VERIFY (Hugging Face), GitHub: huggingface/Math-Verify. We use it as parsing/normalization front-end and then apply numeric comparator."
        },
        {
            "title": "Preprint",
            "content": "Coder local design. Coefficients (fixed): cCoder build = 0.10, cCoder run = 0.10, cCoder nr = 0.80. Component scores: sCoder build,k = 1{compiles/imports without syntax errors at k}, sCoder run,k = 1{smoke subset runs without uncaught exceptions/timeout at k}, sCoder nr,k = (cid:40) 1 Gk1 1, (cid:80) tGk 1{RUN(t, codek) = pass}, Gk1 > 0, Gk1 = 0, where Gk1 = {t Sk1 : RUN(t, codek1) = pass}. Mask: mCoder = 1{build/run logs and test diffs available at k}. Tester local design. Coefficients (fixed): cTester valid = 0.20, cTester cov = 0.80. Component scores: sTester valid,k = 1{new/edited tests are executable, deterministic, and respect I/O at k}, (cid:19) (cid:18) sTester cov,k = min 1, MutScorek τmut 0, , mutation analysis available, otherwise, where (x)+ = max(x, 0), MutScorek [0, 1] is the mutation score on golden code, BrCovk [0, 1] is branch coverage, and thresholds are fixed as Mask: mTester = 1{test runner and mutation/coverage reports available at k}. τmut = 0.60, τcov = 0.10. A.1.3 SUDOKU REWARD DESIGN We consider Sudoku with horizon K. Let hk be the dialogue/tool history at turn and SOLVED() check row/column/subgrid validity. Team reward is sparse success signal at termination: rteam = 1{k=T, SOLVED(hk)=true} {0, 1}. We set the teamlocal mixing coefficient to fixed number λsudoku = 0.60. For each agent {Reasoner, Tool} at turn k, with verifiability mask mi learning reward is {0, 1}, the per-agent = λsudoku rteam ri + (1 λsudoku) mi ri,loc . Local rewards are convex combinations of component scores si {ci ℓ} summing to 1: ri,loc = (cid:88) ℓ si ci ℓ,k, (cid:88) ci ℓ = 1. ℓ,k [0, 1] with fixed coefficients ℓ ℓ Reasoner local design. Coefficients (fixed): cReasoner fmt = 0.15, cReasoner legal = 0.55, cReasoner prog = 0.30. Component scores at turn (let Gk be the current grid, Gk1 the previous grid; 0 denotes empty): sReasoner fmt,k = 1{action format is valid (full grid or list of [r, c, v])}, sReasoner legal,k = 1{no row/column/subgrid duplicates in Gk}, sReasoner prog,k = 1 2 (cid:88) r,c 1{Gk1[r, c]=0, Gk[r, c]=0}. Mask: mReasoner = 1{we can parse the action and compute legality/progress at k}."
        },
        {
            "title": "Preprint",
            "content": "Tool (executor) local design. Coefficients (fixed): cTool fmt = 0.10, cTool exec = 0.20, cTool san = 0.70. Component scores: sTool fmt,k = 1{API/schema valid; values in [1, ]; indices in bounds}, sTool exec,k = 1{no runtime error/timeout when applying edits}, sTool san,k = (cid:26)1, if all applied edits satisfy local Sudoku constraints, 0, otherwise. Mask: mTool = 1{executor logs available and legality checks computed at k}. A.1.4 PLAN-PATH REWARD DESIGN We consider 2D grid path planning on HW map with horizon and four-neighborhood moves. Let dk be the Manhattan distance from the current position to the goal at turn and d0 = max(1, initial distance) for normalization. Team reward is dense and distance-improving: (cid:40)1, rteam = max(cid:0)0, (dk1 dk)/d0 We set the teamlocal mixing coefficient to fixed number (cid:1), if at goal at k, otherwise. λplan = 0.50. For each agent {Planner, Tool} with mask mi = λplan rteam ri {0, 1}, + (1 λplan) mi ri,loc . Local rewards are convex combinations ri,loc = (cid:80) ℓ ci ℓ si ℓ,k with fixed (cid:80) ℓ ci ℓ = 1. Planner local design. Coefficients (fixed): cPlanner fmt = 0.20, cPlanner leg = 0.40, cPlanner sp = 0.40. Component scores at turn (action ak {U, D, L, R}; denotes passable neighbors; SPNEXT is 1 if ak lies on at least one shortest path from sk1 to goal, else 0): sPlanner fmt,k = 1{ak {U, D, L, R}}, sPlanner leg,k = 1{next cell in-bounds and not wall}, sPlanner sp,k = (cid:26)1, 0, if SPNEXT(ak)=1, otherwise. Mask: mPlanner = 1{map known and shortest-path oracle available at k}. Tool (executor/simulator) local design. Coefficients (fixed): cTool fmt = 0.10, Component scores (let ϕk = dk be the potential used in shaping): cTool exec = 0.40, cTool shape = 0.50. sTool fmt,k = 1{action list parsable as [U,D,L,R]}, sTool exec,k = 1{no invalid move applied; simulation advances}, sTool shape,k = 1{ϕk ϕk1}, i.e., the potential does not decrease. Mask: mTool = 1{execution logs and potentials (ϕk1, ϕk) available}."
        },
        {
            "title": "Preprint",
            "content": "A.1.5 SOKOBAN REWARD DESIGN We consider Sokoban with horizon on fixed grid. Let be the number of boxes and bk the number of boxes on goal at turn k. Team reward is dense in box-on-goal ratio with terminal success at completion: (cid:40)1, rteam = if all boxes on goals at k, otherwise. We set the teamlocal mixing coefficient to fixed number bk/B, For each agent {Planner, Tool} with mask mi = λsok rteam ri {0, 1}, + (1 λsok) mi ri,loc . λsok = 0.40. Local rewards are convex combinations ri,loc = (cid:80) ℓ ci ℓ si ℓ,k with fixed (cid:80) ℓ ci ℓ = 1. Planner local design. Coefficients (fixed): cPlanner fmt = 0.10, cPlanner leg = 0.45, cPlanner dlk = 0.45. Component scores at turn (action ak {U, D, L, R}; PUSHOK = 1 if planned push does not collide and stays in-bounds; DEADLOCKFREE = 1 if the move avoids standard static corner deadlocks for boxes not on goals): sPlanner fmt,k = 1{ak {U, D, L, R}}, sPlanner leg,k = 1{step is in-bounds and not into wall; if pushing, PUSHOK = 1}, sPlanner dlk,k = (cid:26)1, 0, if DEADLOCKFREE = 1, otherwise. Mask: mPlanner = 1{grid known and deadlock heuristics evaluable at k}. Tool (executor/simulator) local design. Coefficients (fixed): cTool pot = 0.60. (cid:0)xr gr+xc gc(cid:1) be the box-to-goal potential (larger is better). cTool exec = 0.30, cTool fmt = 0.10, xboxes minggoals Let ψk = (cid:80) Component scores: sTool fmt,k = 1{action list parsable; symbols match {U, D, L, R}}, sTool exec,k = 1{no illegal push; no wall/box collision}, sTool pot,k = 1{ψk ψk1}. mTool = 1{execution logs and potentials (ψk1, ψk) available}. Mask: A.2 EXPERIMENT DETAILS A.2.1 TRAINING DETAILS All methods share the same hyperparameters unless noted. The maximum response length is 4096 tokens, and the (task-specific) maximum prompt length is set to accommodate turn-by-turn dialogue history: 8192 tokens for mathematics and code tasks, and 16384 tokens for all other symbolic tasks. Training uses global batch size of 128, with PPO mini-batch size 64 and gradient clipping at 1.0. The actor is optimized with Adam at learning rate of 1e-6 and weight decay 0.01. We adopt GRPO for advantage estimation with γ=1.0 and λ=1.0. Entropy regularization is off (entropy coeff=0). The sample temperature Tsample = 1.0, top-p=1.0, top-k=1, and 4 sample per prompt; validation is deterministic (temperature 0, do sample=False). rewards are computed by rule-based function (compute score) when provided. Both models are trained for 150 steps."
        },
        {
            "title": "Preprint",
            "content": "A.2.2 PROMPT DESIGN Code MAS Workflow PHASE 1: GENERATION In the initial phase, both agents are given problem description. The Coder is prompted to generate solution, while the Tester is prompted to generate corresponding test case. Code Agent (Coder): Turn 0 Input: Problem: natural language description of programming task. Prompt: You are helpful assistant that writes Python to solve the problem. Think step by step, then output code. Important: - Read all inputs via input(). - Print all results with print(). - Do not hardcode or fabricate inputs. Now solve: Problem: problem description First, decide on the number and types of inputs required (e.g., = int(input()), = int(input())), then implement the solution and print the result. Please answer in the following format: Code: ```python (your code here)``` Output: Code Unit Tester Agent (Test-Case Author): Turn 0 Input: Problem: natural language description of programming task, e.g., {problem}. Prompt: You are helpful assistant that creates unit test cases (input + expected output) for coding task. Problem: problem discrption Provide one new high-quality test case. Before giving the test case, reason carefully to ensure the output is correct, then derive the output for your chosen input. Respond in the format: **Test Input:**```input here``` **Test Output:**```output here``` Output: Test input, Test Output. PHASE 2: REFINEMENT In subsequent turns, the agents receive feedback based on mismatches between the generated code and test cases. They are prompted to refine their previous outputs. Code Agent (Coder): Turn > 0 Input: Problem: The original problem description, {problem}. Mismatch History: record of previous code, test inputs, expected outputs, and actual execution outputs, highlighting any differences, {mismatch history}. Prompt: You are helpful assistant that corrects and refines code. Important: - Read inputs via input(); output with print(). - Do not hardcode inputs. Problem: {problem} Use the history below to guide your fixes: {mismatch history} If your previous code crashed, first fix the bug."
        },
        {
            "title": "Preprint",
            "content": "If execution succeeded but outputs mismatched the expected output, decide if the test case is correct. - If the test is correct, refine your code to pass it. - If the test is wrong, verify your programs logic and keep it. Provide the final, corrected code. Respond in the format: Code: ```python # your code here``` Output: Code Unit Tester Agent (Test-Case Author): Turn > 0 Input: Problem: The original problem description, {problem}. Mismatch History: record showing the test case and the differing execution output from the Coders program, {mismatch history}. Prompt: You are an assistant that checks and refines unit tests for coding task. Problem: problem Analyze the history below: {mismatch history} First, decide whether your previous test case was correct (watch for misunderstandings of the task). If it was wrong or unclear, provide corrected test case. Respond in the format: **Test Input:**, **Test Output:** Output: Test input, test output. Math MAS Workflow PHASE 1: GENERATION In the initial phase, two complementary agents are given the same math problem. The Reasoning Agent produces step-by-step mathematical solution and boxed final answer. The Python Tool Agent writes executable Python that computes (and prints) the final answer. Reasoning Agent: Turn 0 Input: Problem: mathematical problem in natural language. Prompt: You are helpful assistant that solves math problems via careful reasoning. Problem: problem First, outline the key reasoning steps. Then carry out the full solution. After solving, present the final answer in LaTeX box. Before giving the full reasoning, summarize the steps clearly in: **Reasoning Steps:** reasoning steps here Then provide your complete solution concisely. Put your final answer in: boxed answer Rules: * The boxed value must be single number or expression (simplified if possible). * Do not add words after the box; only the final value goes after ####. * If multiple answers exist, list them in single boxed separated by commas. Output format: 1. Your reasoning (short and clear). 2. Final line must contain only the boxed answer, e.g., #### 123. Output: Reasoning solution and final answer after ####."
        },
        {
            "title": "Preprint",
            "content": "Python Tool Agent (Coder for Math): Turn 0 Input: Problem: The same mathematical problem, {problem}. Prompt: You are helpful programming assistant that writes Python to solve the math problem. Problem {problem} Requirements Write correct, readable Python that computes the final answer. Think step by step in comments if helpful. Use only the standard library and deterministic math (no internet, no randomness). At the end, PRINT ONLY the final numeric or symbolic answer (nothing else). Output: Code (the program prints the final answer). PHASE 2: REFINEMENT From the second turn onward, agents receive feedback derived from mismatches between the Reasoning Agents boxed answer and the Python Tool Agents printed output. Each agent uses the history to refine its output. Reasoning Agent (Math Solver): Turn > 0 Input: Problem: The original problem, {problem}. Mismatch History: Prior answer ({reasoning extracted answer}), and the codes printed output {mismatch history}. reasoning ({reasoning solution}), its extracted the Python code ({code solution}), summarized as ({code extracted answer}), Prompt: You are helpful assistant that refines mathematical solutions through reasoning. Problem: problem History (previous attempts and outputs): mismatch history First, compare your previous boxed answer with the Python Tool Agents printed output. * If the code output corrects computational slip in your reasoning, adopt the corrected value. * If the code likely has bug (e.g., mishandled edge cases, precision, domains), keep the mathematically correct answer and explain briefly. Then solve the problem again, more robustly. Before giving the full reasoning, summarize the key steps clearly: **Reasoning Steps:** reasoning steps here Finish with the final answer after: #### Final line must contain only the boxed value (no extra text). Output: Updated reasoning and final answer after ####. Python Tool Agent (Coder for Math): Turn > 0 Input: Problem: The original problem, {problem}. Mismatch History: Prior code and printed output, and the Reasoning Agents solution and boxed answer, summarized as {mismatch history}. Prompt:"
        },
        {
            "title": "Preprint",
            "content": "You are helpful programming assistant that refines Python solutions for math problems. Problem: problem History (reasoning vs. execution mismatches): mismatch history Tasks: 1. Judge whether the Reasoning Agents boxed answer or your previous printed result is more likely correct (consider numerical stability, edge cases, exact vs. float). 2. Fix or rewrite the code so it reliably computes the correct final answer. * Prefer exact arithmetic (fractions, integers, rational simplification) when possible. * Add minimal checks for domain/edge cases. * Keep outputs deterministic. Respond in the format: **Code:** ```python # corrected code here # print ONLY the final answer on the last line ``` Output: Refined code (the program prints the final answer). Sudoku MAS Workflow In the initial phase, two complementary agents are given the same Sudoku-solving task on an nn grid. The Tool Agent writes executable Python that outputs either completed grid or list of fill steps. The Plan Agent inspects the task, the tool code, and its execution output, then decides the final solution. Tool Agent (Sudoku Coder) Input: Task Description: {task}, including grid size, rules (rows/columns/sub-grids contain unique digits), and any constraints. Env Context: {observation}). {env context} (e.g., {size}, {subgrid size}, {puzzle}, Prompt: You are an AI assistant designed to be helpful. Utilize your programming expertise to address the task. Propose Python code (within single python code block) for the user to run. Ensure each response contains only ONE code block. Use the print function to output EITHER: (A) the completed grid as JSON array of arrays, OR (B) JSON list of fill steps (r,c,v) using 1-based indices. Formatting requirements: * The programs output [3,4,5,2,8,6,1,7,9]] * Print ONLY the JSON (no extra text, no comments). Task: Solve the sizexsize Sudoku. Fill digits 1..size ; rows, columns, and sub-grids must have unique digits. Current puzzle (dots denote blanks): observation Environment: - size - subgrid size: subgrid size - notes/constraints: constraints Output: Code (program prints either the completed grid JSON or JSON list of fill steps). the Sudoku solution: [[5,3,4,6,7,8,9,1,2], eg: ..., is Plan Agent (Planner & Verifier) Input: Task Description: {task}. Tool Code: {tool code}. Tool Execution Output: {tool execution output}. Tool Proposed Solution: {tool solution} (JSON grid or JSON steps). Observation (for reference): {observation}. Prompt: You are planning and reasoning agent. You will receive:"
        },
        {
            "title": "Preprint",
            "content": "* The original task description * The Tool Agents code * The code execution output (a JSON grid or JSON steps) Your job is to reason carefully, decide the final Sudoku solution, and format your response EXACTLY as specified. Instructions: * Read the task, inspect the code, and verify the execution output against the Sudoku rules: rows, columns, and sub-grids must contain unique digits in 1..n. * If the tools output is complete, valid solution, adopt it. * If it is incomplete or violates constraints, correct it or provide your own. * Keep reasoning concise but explicit: explain why the final result is valid. FORMATTING IS MANDATORY. Give the final answer AFTER the line that begins with ####. You may return EITHER: - completed grid as JSON, OR - JSON list of fill steps (r,c,v), 1-based indices. Examples: #### [[5,3,4,6,7,8,9,1,2], ..., [3,4,5,2,8,6,1,7,9]] #### [[1,3,4],[2,1,6],[9,9,1]] Output: Final Sudoku answer (completed grid JSON or JSON steps). A.3 PLAN-PATH MAS WORKFLOW PHASE 1: GENERATION In the initial phase, two complementary agents are given the same path-planning task on grid/- world. The Tool Agent writes executable Python that outputs an action list (e.g., [U, R, D, L]). The Plan Agent inspects the task, the tool code, and its execution output, then decides the final action list. Tool Agent (Path Coder): Turn 0 Input: Task Description: {task}, including grid/map, start, goal, obstacles, and constraints. Env Context: {constraints}). {env context} (e.g., {grid}, {start}, {goal}, {obstacles}, Prompt: You are an AI assistant designed to be helpful. Utilize your programming expertise to address the task. Propose Python code (within single python code block) for the user to run. Ensure each response contains only ONE code block. Use the print function to output the action list that moves from the start to the goal. You may output the full action list if you can reach the target, or partial list if uncertain. Formatting requirements: * Begin the Python block with `python and end with `. * The programs output IS the action list (e.g., [U,R,D,L]). * Print ONLY the action list (no extra text). Task: task Environment: env context Output: Code (program prints an action list, e.g., [U,R,D,L]). Plan Agent (Planner & Verifier): Turn 0 Input: Task Description: {task}. Tool Code: {tool code}. Tool Execution Output: {tool execution output}."
        },
        {
            "title": "Preprint",
            "content": "Tool Proposed Action: {tool action}. Prompt: You are planning and reasoning agent. You will receive: * The original task description * The Code Agents (Tool Agents) code * The code execution output Your job is to reason carefully, decide the final action list, and format your response EXACTLY as specified. Instructions: * Read the task, inspect the code, and verify the execution output against the task requirements and environment constraints (bounds, obstacles, goal). * If the code/output is correct and sufficient, adopt it. * Otherwise, improve or override it with your own reasoning. * Keep reasoning concise but explicit: justify why the final action is correct. FORMATTING IS MANDATORY. Give the final action list AFTER the line that begins with ####. Example: #### [U,R,D,L] Output: Final action list. A.3.1 Phase 2: Refinement From the second turn onward, agents receive feedback based on mismatches between the Tool Agents printed action list and feasibility checks from the environment or the Plan Agents assessment. Each agent uses the history to refine its output. Tool Agent (Path Coder): Turn > 0 Input: Task Description: {task}. Mismatch/Trajectory History: Prior code and printed actions, planner feedback, and (action, state) pairs, summarized as {action state history}. Prompt: Refine your Python solution to produce correct, executable action list. Task: task History (previous attempts, planner feedback, and trajectory): action state history Requirements: * Output must be an action list that reaches the goal without violating constraints (stay inbounds, avoid obstacles). * If certain, print the full list; if uncertain, print safe partial prefix. * Single python code block only; program output IS the action list. * Begin with python and end with ; print ONLY the action list (e.g., [U,R,D,L]). Respond in the format: **Code:** ``` python # corrected code here # last line prints ONLY the action list ``` Output: Refined code (program prints an action list). Plan Agent (Planner & Verifier): Turn > 0 Input: Task Description: {task}. Tool Code: {tool code}. Tool Execution Output: {tool execution output}. Tool Proposed Action: {tool action}."
        },
        {
            "title": "Preprint",
            "content": "Action State History (if any): For each step i, The i-th action is ai. The i-th state is si. Summarized as action state history. Prompt: You are planning and reasoning agent. Task: task Tool Agents latest code and output: * Code: tool code * Execution output: tool execution output * Proposed action: tool action Trajectory/history: action state history Instructions: * Verify feasibility of the proposed action sequence step by step. * If it collides, goes out of bounds, loops, or fails to reach the goal, correct it (you may shorten, extend, or replace the sequence). * Prefer the simplest valid plan; if uncertain, provide the best safe prefix and explain briefly. * Keep reasoning concise but explicit. FORMATTING IS MANDATORY. Give the FINAL action list AFTER the line that begins with ####. Example: #### [U,R,D,L] Output: Final action list."
        }
    ],
    "affiliations": [
        "Intel Corporation",
        "University of California, San Diego"
    ]
}