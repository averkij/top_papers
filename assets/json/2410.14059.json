{
    "paper_title": "UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models",
    "authors": [
        "Yuzhe Yang",
        "Yifei Zhang",
        "Yan Hu",
        "Yilin Guo",
        "Ruoli Gan",
        "Yueru He",
        "Mingcong Lei",
        "Xiao Zhang",
        "Haining Wang",
        "Qianqian Xie",
        "Jimin Huang",
        "Honghai Yu",
        "Benyou Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces the UCFE: User-Centric Financial Expertise benchmark, an innovative framework designed to evaluate the ability of large language models (LLMs) to handle complex real-world financial tasks. UCFE benchmark adopts a hybrid approach that combines human expert evaluations with dynamic, task-specific interactions to simulate the complexities of evolving financial scenarios. Firstly, we conducted a user study involving 804 participants, collecting their feedback on financial tasks. Secondly, based on this feedback, we created our dataset that encompasses a wide range of user intents and interactions. This dataset serves as the foundation for benchmarking 12 LLM services using the LLM-as-Judge methodology. Our results show a significant alignment between benchmark scores and human preferences, with a Pearson correlation coefficient of 0.78, confirming the effectiveness of the UCFE dataset and our evaluation approach. UCFE benchmark not only reveals the potential of LLMs in the financial sector but also provides a robust framework for assessing their performance and user satisfaction. The benchmark dataset and evaluation code are available."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 2 ] . - [ 2 9 5 0 4 1 . 0 1 4 2 : r UCFE: User-Centric Financial Expertise Benchmark for Large"
        },
        {
            "title": "Language Models",
            "content": "Yuzhe Yang1,*, Yifei Zhang2,*, Yan Hu1, Yilin Guo1, Ruoli Gan1, Yueru He3, Mingcong Lei1, Xiao Zhang3, Haining Wang2, Qianqian Xie3,, Jimin Huang3, Honghai Yu2,, Benyou Wang1, 1The Chinese University of Hong Kong, Shenzhen, 2Nanjing University, 3The Fin AI yuzheyang@link.cuhk.edu.cn, yf_zhang@smail.nju.edu.cn,"
        },
        {
            "title": "Abstract",
            "content": "This paper introduces the UCFE: User-Centric Financial Expertise benchmark, an innovative framework designed to evaluate the ability of large language models (LLMs) to handle complex real-world financial tasks. UCFE benchmark adopts hybrid approach that combines human expert evaluations with dynamic, taskspecific interactions to simulate the complexities of evolving financial scenarios. Firstly, we conducted user study involving 804 participants, collecting their feedback on financial tasks. Secondly, based on this feedback, we created our dataset that encompasses wide range of user intents and interactions. This dataset serves as the foundation for benchmarking 12 LLM services using the LLM-as-Judge methodology. Our results show significant alignment between benchmark scores and human preferences, with Pearson correlation coefficient of 0.78, confirming the effectiveness of the UCFE dataset and our evaluation approach. UCFE benchmark not only reveals the potential of LLMs in the financial sector but also provides robust framework for assessing their performance and user satisfaction.The benchmark dataset and evaluation code are available 1."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large language models (LLMs) have expanded their potential applications in finance (Wu et al., 2023; Huang et al., 2023; Kim et al., 2024). Finance professionals are increasingly using LLMs to solve specialized financial tasks (Li et al., 2023a; Xie et al., 2024; Yang et al., 2023; Zhang et al., 2023b). Financial tasks are inherently complex, involving specialized context, financial terminologies, legal intricacies, and dynamic markets that involve information with high *Equal contribution. Corresponding author. 1https://github.com/TobyYang7/UCFE-Benchmark 1 noise-to-signal ratio(Pagano, 1993; Mullainathan and Spiess, 2017; Li et al., 2018), which adds significant challenges for LLMs to address. Accurate analysis of financial information is crucial, as even minor ignorance in signal or market information can lead to substantial financial losses (Tversky and Kahneman, 1981; Thaler, 2008; Mohamed et al., 2024). To be effective, LLMs need to swiftly adapt to fiscal policy changes, market fluctuation, extreme events, and global factors, identifying key signals within real-time data to manage volatility and mitigate risks (Gueta et al., 2024; Yadav et al., 2024). Financial markets can react instantly to news, making it crucial for LLMs to process information in near real-time by rapidly consolidating unstructured, real-time data from multiple sources (Nguyen and Tulabandhula, 2023; Tong et al., 2024). Despite LLMs improving accuracy on tasks like sentiment analysis, market prediction, and risk assessment (Wimmer and Rekabsaz, 2023; Lopez-Lira and Tang, 2023; Rizinski et al., 2024), these models still face significant limitations, such as their reliance on static datasets and challenges in handling real-time data, which hinder their realworld applicability in dynamic financial contexts. Moreover, the evolving nature of financial regulation adds another layer of complexity, requiring LLMs to continuously update their knowledge to remain compliant and useful (Yao et al., 2024; He et al., 2024). These limitations highlight the need for more dynamic evaluation framework that assesses LLMs performance under real-time, evolving financial conditions, ensuring they can handle not only static tasks but also the unpredictable nature of real-world financial environments. To address these challenges in financial domain, we propose novel framework, the User-Centric Financial Expertise Benchmark, designed to evaluate the ability of LLMs to handle financial tasks in real-world scenarios. Figure 1 provides an Figure 1: Overview framework of the UCFE Benchmark. overview of the framework. The UCFE benchmark has the following key features: User-Centric Design: Based on preliminary surveys and research, we categorized the target user group into four distinct types: analysts, financial professionals, regulatory professionals, and the general public. Using questionnaires in Appendix A, we gathered insights into the primary needs and practical applications of each group. This allowed us to refine the user categories for more targeted evaluations, in which LLMs are prompted to simulate roles representative of each group. We developed 17 task types tailored to these user profiles, encompassing 330 data points that include multi-round dialogues in both zero-shot and few-shot settings. More details of the dataset will be explained in Section 4. Dynamic Interactions: In the few-shot tasks, each user group follows task-oriented approach, where an initial action goal is defined. Users articulate their professional needs and specific task requirements through successive interactions. To simulate real-world user scenarios, we employ authentic datasets that closely mirror actual financial scenarios. This dynamic interaction setup ensures that LLMs are not only responding to isolated queries but are also engaging in an evolving dialogue, adjusting their responses based on the ongoing professional needs expressed by the user. This method provides more accurate reflection of how LLMs would perform in practical, task-specific financial contexts. In summary, this work makes the following key contributions: (1) We propose new framework that combines human expert judgments with LLMs to assess their ability to handle increasingly (2) By leveraging dycomplex financial tasks. namic, user-centric interactions, this work probes the boundaries of LLM capabilities by examining how well LLMs adapt to evolving professional needs and increasingly complex task requirements, which provides deeper understanding of their potential and limitations in addressing real-world financial scenarios."
        },
        {
            "title": "2.1 Financial Benchmark",
            "content": "FLARE (Xie et al., 2023) evaluates models on five financial tasks 2. Existing benchmarks (Zhang et al., 2023a; Li et al., 2023b; Yuan et al., 2024) primarily use multiple-choice questions to assess models domain knowledge, with questions sourced from real-world financial documents and publicly available financial reports or websites, covering wide range of topics such as finance, economy, accounting, and certification. MMMU and MMMUPRO (Yue et al., 2024a,b) extend beyond traditional financial NLP tasks by incorporating multimodal inputs to better evaluate models in more complex financial tasks. Although these benchmarks have advanced the evaluation of financial language models, they predominantly consider structured NLP tasks with deterministic answers and rely heavily on multiple-choice questions or tasks with specific answers, such as sentiment analysis and named entity recognition. This limits their ability to assess generative capabilities, which are essential for simulating real-world financial applications (Krause, 2023; Koa et al., 2024). 2sentiment analysis, news headline classification, named entity recognition, question answering, and stock movement prediction"
        },
        {
            "title": "2.2 User-Centric Framework",
            "content": "The implementation of user-centric models involves integrating users into core business processes to harness their creative potential, which has been successfully demonstrated by companies like LEGO, IBM, and Coloplast (Hienerth et al., 2011; Kwon et al., 2021). EUCA framework, practical prototyping toolkit designed to make AI systems explainable to non-technical endusers, provides twelve end-user-friendly explanatory forms that do not require technical knowledge to bridge the gap between technical creators and non-technical users (Jin et al., 2021). In the financial domain, user-centric explainability is also crucial in algorithmic decision-making systems like robo-advisors (Naveed et al., 2022; Roveda et al., 2023; Pisoni and Díaz-Rodríguez, 2023). Research has highlighted the importance of providing transparent and comprehensible explanations to users, which indicated that user trust and confidence in financial applications are positively correlated with the transparency and comprehensibility of the explanations provided (How et al., 2020; Deo and Sontakke, 2021; Xu et al., 2024)."
        },
        {
            "title": "3 Background",
            "content": "Recent advancements in LLMs have demonstrated significant potential in addressing complex financial tasks. Numerous organizations are now actively training their own LLMs, aiming to enhance their performance by incorporating extensive domain-specific knowledge. For instance, FinGPT (Yang et al., 2023), which applies supervised finetuning to the LLaMA model, has shown notable improvements in financial tasks. While models like FinLLaMA (Xie et al., 2024), through continued pretraining, have further advanced LLM performance across various metrics. These developments highlight the growing demand and potential for LLMs in the financial domain, both in academia and industry. From technical standpoint, LLMs have undoubtedly reduced costs and improved efficiency by quickly processing vast amounts of financial text data with commendable performance. However, the challenges of developing real-world financial applications extend beyond technical issues. These challenges include business requirements, industryspecific barriers, data privacy concerns, accountability, and ethical considerations (Nie et al., 2024; Yao et al., 2024), along with gap in understanding between LLMs, functioning as AI assistants, and the specific needs of financial experts. As discussed in Section 2.1, existing benchmarks largely focus on technical metrics such as accuracy and efficiency, often ignoring these broader challenges. By emphasizing only technical aspects, such benchmarks fail to address the real-world complexities of financial applications, where business rules, legal frameworks, and human judgment play crucial roles. This leaves non-technical aspects, such as human-AI interaction in finance, relatively under-explored. Human-AI interaction is critical in financial settings, as it affects decisionmaking, user trust, and the effective integration of AI systems into the financial workflow. Without considering these factors, current benchmarks offer an incomplete evaluation, limiting the practical relevance of LLMs for real-world financial applications. There is pressing need for evaluation frameworks that not only assess technical performance but also account for the nuanced interplay between AI systems and financial professionals in complex environments. In addition to these challenges, the rise of FinTech companies such as Robinhood 3 has spurred increasing public interest in finance and trading. growing number of individuals, many without formal financial education, are seeking accessible ways to manage their finances and participate in the market. For these users, LLMs have become significant source of financial learning and advice. However, the accuracy, flexibility, and contextual understanding of LLMs are crucial to meet this emerging demand. Models must not only provide correct and relevant information but also adapt to the diverse financial needs of the general public, from basic education to advanced financial management strategies. In this context, the UCFE benchmark will mainly focus on evaluating the interaction between LLMs and humans, as the improvements in user experience often have greater practical significance than gains in task-specific metrics. By introducing this new framework, we aim to offer deeper insights into the future development of financial LLMs, aligning model performance more closely with human preferences across multiple dimensions. This approach is intended to provide more holistic understanding of how LLMs can better serve realworld financial applications, ultimately leading to 3https://robinhood.com/us/en/"
        },
        {
            "title": "Total",
            "content": "Student (Finance-related) Financial Professional Regulatory Professional General Public Non-Finance Professional Student (Non-finance) Other 804 167 83 51 136 87 208 72 148 83 47 49 37 79 15 660 155 83 50 82 70 163 57 Table 1: The user survey outcomes. Familiarity indicates the results of Question 5, where people choose they have encountered multi-round financial tasks. Importance indicates the results of Question 6 where people choose they think multi-round financial tasks are important. more user-centric AI solutions."
        },
        {
            "title": "4.1 User Preference Alignment",
            "content": "To align our dataset more closely with real-world financial tasks and user needs, we conducted survey to gather insights into how users engage with financial scenarios. Participants completed questionnaire designed to capture key aspects of their interactions with financial tasks, focusing on their roles, levels of experience, and the types of tasks they typically perform. The survey included questions about participants familiarity with financial analysis, preferred sources of information, and their engagement preferences regarding financial tasks. Feedback was solicited from participants across three main areas: Participant Demographics: Information on the participants backgrounds and expertise. Detailed Interaction with Financial Tasks: Assessment of participants experiences and interactions with specific financial tasks. Financial Scenario Coverage: Evaluation of how well the tasks reflected real-world financial scenarios. The full questionnaire can be found in Appendix A."
        },
        {
            "title": "4.2 Dataset Construction",
            "content": "Based on the results of the survey shown in Table 1, we recognized the necessity of constructing multiround finance dialogue benchmark that serves both the finance-related and non-finance groups. The survey revealed diverse user intentions and varying Figure 2: The visualization displays the top 25 most common root verbs (inner circle) and their top 4 associated direct noun objects (outer circle) extracted from the provided texts. Figure 3: Distribution of test and evaluation input lengths for the datasets. levels of financial expertise, underscoring the need for benchmark that can accommodate broad spectrum of scenarios. By catering to these different groups, we aimed to capture comprehensive range of dialogue interactions, from complex financial analysis to simpler, more general financial inquiries, ensuring the dataset reflects real-world variations in user needs and knowledge levels. To establish the multi-round dialogue benchmark, we meticulously selected sources that encompass authoritative financial reports, regulatory documents, and accessible online resources based on the survey results shown in Appendix B.2. This selection process was designed to ensure that the dataset meets both the technical demands of financial professionals and the practical needs of general users. By synthesizing insights from diverse user experiences and expert evaluations, we aimed at creating dataset that facilitates effective multiround interactions, ultimately enhancing the user experience in financial analysis."
        },
        {
            "title": "4.3 Tasks",
            "content": "Table 2 and 3 provide the statistical breakdown of the UCFE benchmark, with all data sourced from the previous user survey targeting various user groups. The UCFE benchmark encompasses both few-shot and zero-shot tasks, with total of 17 distinct tasks covering broad range of financial scenarios. These tasks are specifically designed to reflect practical financial needs, including but not limited to market information summarization, asset valuation, and regulatory compliance assessments. The multi-turn nature of these tasks emphasizes dynamic user interaction and adaptive decisionmaking. Figure 2 presents the visualization of the 25 most common root verbs (inner circle) and their top 4 associated direct noun objects (outer circle), providing insights into the types of financial interactions covered by the dataset. The diversity of verb-noun pairs highlights the wide range of financial operations and decision-making processes represented, ensuring the benchmark tasks reflect the complex and varied language used in financial contexts. In addition, Figure 6 shows the distribution of input lengths for both test and evaluation queries, revealing significant variance in task complexity. Shorter queries require concise outputs, while longer inputs demand deeper comprehension and detailed responses. This variance challenges models not only to generalize across different task types but also to adapt their performance based on the complexity of the input, making it essential for evaluating LLMs scalability and versatility in realworld financial tasks."
        },
        {
            "title": "5 UCFE Benchmark",
            "content": "In this section, we provide an overview of the technical details and evaluation pipeline of the UCFE benchmark. As shown in Figure 4, the evaluation starts by selecting finance-specific tasks (introduced in Section 4.3), where the model acts as an AI assistant. GPT-4o4 simulates user interactions, generating dialogue data based on realistic behavior. Using LLMs to simulate user roles is common in recent research (Inaba et al., 2024). To minimize model bias, we established evaluation criteria (detailed in Section 5.2). Model outputs are then compared in pairs, with Claude-3.5-Sonnet5 as the evaluator, following the common practice LLm-as-judge framework for evaluation (Liu et al., 2023). Each models performance is measured using Elo scores, which offer relative comparison of capabilities. Finally, results are compared against human expert preferences to ensure evaluation robustness."
        },
        {
            "title": "5.1 Evaluation Method",
            "content": "We use the Elo rating system for model evaluation, which is well-suited for comparing multiple models. This system is widely applied in competitive environments, such as match result prediction in sports like association football (Hvattum and Arntzen, 2010; Chiang et al., 2024). Its key advantages are: Dynamic Adjustments: Elo ratings are continuously updated based on relative model performance, making it ideal for frequent comparisons. Scalability & Efficiency: New models can be added without retesting all previous ones, saving time and API costs. Each model starts with an Elo rating of 1000, which is updated after every comparison task. For each task, dialogues generated by the target model and the base model are compared using specific prompts. GPT-based model evaluates the comparison as win, loss, or tie, and the Elo ratings are updated using the formula: = + (S E) where is the updated rating, is the current rating, is the result (1 for win, 0.5 for tie, and 0 for loss), and is the expected result, computed as: = 1 (RoR) 1 + 10 Here, Ro is the opponents rating, is set to 400, and is 4. These parameters control the magnitude of rating updates. This process repeats for each task, and the final Elo ratings reflect the models comparative performance across all tasks."
        },
        {
            "title": "5.2 Experimental Settings",
            "content": "4https://openai.com/index/gpt-4o-system-card/ 5https://www.anthropic.com/news/ claude-3-5-sonnet In the experiments, GPT-4o is used as the user simulator to generate queries and simulate realworld conversations. Claude-3.5-Sonnet serves as"
        },
        {
            "title": "Target User Group",
            "content": "Few-shot"
        },
        {
            "title": "Analyst Simulation\nAsset Valuation Reporting\nCompany Evaluation Reporting\nCorporate Operation Analysis\nCredit Risk Evaluation\nFinancial Knowledge Consulting\nFinancial Regulation Consulting\nIndustry Report Summarization\nInsider Trading Detection\nInvestment Strategy Evaluation\nInvestment Strategy Optimization\nNewshare Evaluation Reporting\nProspectus Risk Summarization",
            "content": "TCL Annual Report & Analyst Report EastMoney Analyst Report Analyst Report GPT-4 Generated Investopedia1 Securities Law2 EastMoney Securities Regulatory Commission3 Seeking Alpha4 Financestrategists5 Stock.us6 Prospectus & Inquiry Letter7 Senior Analyst Analyst Analyst Analyst Analyst General Public & Financial Professional General Public & Financial Professional & Regulatory Professional General Public & Financial Professional Regulatory Professional Analyst Analyst Analyst General Public & Financial Professional Zero-shot"
        },
        {
            "title": "Stock Price Prediction\nNegative Information Detection\nFinancial Indicator Calculation\nFinancial Text Summarization",
            "content": "A-stock Statistics EastMoney CPA & CFA News Headlines General Public & Financial Professional General Public & Financial Professional General Public & Financial Professional General Public & Financial Professional 1 https://www.investopedia.com/financial-term-dictionary-4769738 2 https://www.gov.cn/xinwen/2019-12/29/content_5464866.htm 3 http://www.csrc.gov.cn/csrc/c101953/zfxxgk_zdgk.shtml 4 https://seekingalpha.com/article/4500869-portfolio-performance-evaluation-metrics 5 https://www.financestrategists.com/wealth-management/investment-management/portfolio-performance-evaluation/ 6 https://stock.us/cn/stock/sz/001279 7 https://www.sse.com.cn/disclosure/credibility/supervision/inquiries/ Table 2: Overview of UCFE benchmark tasks, including task categories, sources, and target user groups."
        },
        {
            "title": "Model",
            "content": "Zero-shot Tasks Few-shot Tasks"
        },
        {
            "title": "Total",
            "content": "4 13 17 80 250 330 Table 3: Summary of Task Types and Corresponding Number of Questions in the UCFE benchmark. Note that all tasks have 20 questions except that Analyst Simulation has only 10 questions. the evaluator to compare model responses, ensuring clear separation between testing and evaluation to minimize bias. For dialogue simulations, the temperature is set to 0.5 with no token limit. We tested financialspecific LLMs (7B to 70B parameters) along with their backbone models and included generalpurpose models like GPT-4o and GPT-4o-mini, accessed via APIs. Table 4 lists all models used. To mitigate positional bias in LLM evaluations (Li et al., 2023c), we shuffled the input order during dialogue comparisons. To further minimize evaluator bias, such as misinformation or cognitive bias (Talboy and Fuller, 2023), we designed the evaluation prompts based on two key criteria: Source Information Content: Categorized Contain, At Least into Answer, Must Contain, Encourage Contain, Should Contain, Contain, guiding LLMs to make accurate content-based choices. and Forbid CFGPT2-7B1(Li et al., 2023a) GPT-4o GPT-4o-mini InternLM2.5-7B-Chat (Cai et al., 2024) Llama-3.1-70B-Instruct (AI@Meta, 2024) Llama-3.1-8B-Instruct Llama3-XuanYuan3-70B-Chat (Zhang et al., 2023b) Palmyra-Fin-70B-32k (team, 2024) Qwen2.5-14B-Instruct (Team, 2024) Tongyi-Finance-14B-Chat"
        },
        {
            "title": "1 The backbone model of CFGPT2-7B is InternLM2-7B.\n2 The backbone model of Tongyi-Finance-14B-Chat is Qwen-14B. Model Card: https:",
            "content": "//modelscope.cn/models/TongyiFinance/Tongyi-Finance-14B/summary Table 4: Models evaluated in UCFE benchmark. Fulfillment, Fact Needs Accuracy, Responsibility & Fairness, Response Depth & Richness, Hallucination, and Conciseness & Clarity, ensuring thorough assessment. The full evaluation prompt is available in Appendix E."
        },
        {
            "title": "5.3 Overall Results",
            "content": "Table 5 shows the results across different tasks. Models like Tongyi-Finance-14B-Chat and CFGPT2-7B, which were continued pre-trained on financial corpus, exhibit significant improvements over their original backbone LLMs. To ensure robustness, we used two additional evaluators, Gemini-1.5-pro6 and Deepseek-chat7. 6https://deepmind.google/technologies/gemini/ pro/ Evaluation Standards: Focused on User 7https://www.deepseek.com/ 6 Figure 4: The evaluation pipeline of the UCFE Benchmark involves the following steps: ① selecting the model and task, ② generating dialogues between the user and AI assistant via user simulator, ③ creating evaluation prompts based on source information to assess model performance, ④ pairwise comparison of dialogue outputs by evaluators, aligned with human expert judgments, and ⑤ computing Elo scores based on win-loss outcomes."
        },
        {
            "title": "Few Shot Win Counts",
            "content": "Tongyi-Finance-14B-Chat CFGPT2-7B Palmyra-Fin-70B-32k GPT-4o Llama-3.1-8B-Instruct Internlm2.5-7b-chat Llama3-XuanYuan3-70B-Chat Llama-3.1-70B-Instruct GPT-4o-mini Qwen2.5-14B-Instruct Qwen2.5-7B-Instruct 1156.99 1155.75 1128.25 1117.68 1046.87 995.85 913.48 912.26 901.75 855.82 814.48 1007.52 1125.33 1028.18 979.85 1062.18 1009.78 934.51 986.77 943.81 974.27 946.45 1171.27 1157.93 1143.66 1120.89 1051.32 1000.52 911.59 906.80 908.92 840.05 786.28 3614 3972 3634 3040 3294 2964 2050 2196 2326 1774 1312 Table 5: Model results in the UCFE benchmark. Red highlights the highest value, while Blue represents the second-highest value. As seen in Figure 5, the final results across target models show minimal variation, suggesting that the LLM-as-evaluator framework introduces slight bias. Additionally, to examine potential length bias (Wei et al., 2024), we analyzed dialogue lengths and turn counts across models (Figure 6). The analysis shows no significant correlation, indicating no observable bias."
        },
        {
            "title": "5.4 Human Preference Alignment",
            "content": "We invited 15 participants, including financerelated students and professionals, with each assigned 10 pre-existing results to conduct the human preference alignment phase. After manually reassessing the outputs, we updated the Elo scores and compared them with our models predictions. 7 Figure 5: Comparison of model performance on UCFE benchmark across three evaluators. To quantify the similarity between the models results and human evaluations, we used the Pearson correlation coefficient: = i=1(xi x)(yi y) (cid:80)n i=1(xi x)2 (cid:80)n i=1(yi y)2 (cid:112)(cid:80)n where xi and yi represent the Elo scores from Figure 6: Comparison of average dialogue rounds and total tokens across different models in few shot tasks. Figure 8: Overall Elo scores of various models plotted against model parameters (in billions). Here is summary of the financial text in single sentence: The US Treasury Department will impose sanctions has imposed sanctions on four Ukrainian government officials current and former Ukrainian government officials for who assisted their involvement in Russian disinformation campaign aimed at undermining Ukraines regime and justifying an invasion. the Ukrainian regime. Note: would like to point out that the provided text is not financial text but rather political/news article. If you could provide an actual financial text, would be happy to help you generate summary. Figure 9: Comparison between Llama3.1-8B-Instruct and Llama3.1-70B-Instruct models, green highlighting the changes in Llama3.1-70B-Instruct. performance."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced the UCFE Benchmark, framework designed to evaluate user-AI interactions in the financial domain using the LLMas-Judge methodology. This framework enables direct comparisons of model performance with human expert preferences while addressing potential biases. Our findings demonstrate that LLMs trained on domain-specific financial texts show notable improvements in understanding complex financial concepts and accurately interpreting user intent. Notably, mid-sized models (7B to 14B parameters) performed particularly well, striking an effective balance between computational efficiency and domain-specific expertise without the excessive overhead of larger models. These results emphasize the importance of optimizing LLMs not only for performance but also for resource efficiency, making them more viable for real-world financial applications. Additionally, the user-centric design of our benchmark highlights the critical role of aligning AI systems with diverse user needs, ensuring that LLMs deliver practical, contextually relevant solutions in finance. This approach lays the foundation for more reliable and scalable AIdriven innovations in the financial industry. Figure 7: Correlation between human Elo scores and Claude-3.5-Sonnet Elo scores. participants and model predictions, respectively. The analysis revealed clear positive correlation, with calculated Pearson correlation of = 0.78, as shown in Figure 7, indicating that the models performance aligns well with human preferences."
        },
        {
            "title": "5.5 Case Study",
            "content": "Previous research has demonstrated the significant influence of scaling laws on model performance (Kaplan et al., 2020; Ruan et al., 2024). As shown in Figure 8 our results also show similar trend. LLMs with larger parameters generally outperform smaller ones within the same series, also LLMs from the same backbone model have better results after being trained on financial corpus. However, Llama3.1 appears to be an outlier in this pattern. As shown in Figure 9, we illustrate result of the Summarization task that highlights this phenomenon. In many of the data points, we observe that the 70B model tends to generate longer, more verbose outputs compared to smaller models. This aligns with the conclusions of (Chiang and yi Lee, 2024), where larger models are prone to over-reasoning, generating lengthy and unnecessary responses to questions. In our evaluation framework, where clear scoring criteria have been established, these redundant outputs significantly lower the models"
        },
        {
            "title": "Ethical Statements",
            "content": "We do not see our work to have possible harmful outcomes. We follow the ACL ethical guidelines when conducting the research in this paper. The limitations of our work can be summarized as follows: Coverage of Financial Tasks: The financial domain encompasses wide range of complex tasks and scenarios, from regulatory compliance to dynamic market analysis. While the UCFE Benchmark includes several representative tasks, the diversity and volume of data points may not be sufficient to fully capture all real-world financial applications. This limitation restricts the benchmarks ability to comprehensively assess LLM performance across the entire spectrum of financial use cases. Human Preference Bias: The evaluation framework relies on human preferences to assess model performance, which introduces potential biases. Given the limited number of evaluators and the relatively narrow range of professional backgrounds represented, the results may not fully reflect the diverse needs and preferences of the broader financial community. Individual biases and subjective judgments could influence the evaluation, potentially skewing the assessment of LLM effectiveness in real-world financial tasks. Use of Historical Data: The benchmark relies primarily on historical financial data for task evaluation. While this data is useful for assessing LLM performance in past scenarios, it may not fully capture the evolving and realtime nature of financial markets. This reliance on historical data limits the ability to evaluate how well LLMs can adapt to unforeseen events or respond to rapidly changing market conditions."
        },
        {
            "title": "References",
            "content": "AI@Meta. 2024. Llama 3 model card. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. 2024. Internlm2 technical report. Preprint, arXiv:2403.17297. Cheng-Han Chiang and Hung yi Lee. 2024. Overreasoning and redundant calculation of large language models. Preprint, arXiv:2401.11467. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, et al. 2024. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132. Sahil Deo and Neha Sontakke. 2021. User-centric explainability in fintech applications. In HCI International 2021-Posters: 23rd HCI International Conference, HCII 2021, Virtual Event, July 2429, 2021, Proceedings, Part II 23, pages 481488. Springer. Almog Gueta, Amir Feder, Zorik Gekhman, Ariel Goldstein, and Roi Reichart. 2024. Can llms learn macroeconomic narratives from social media? arXiv preprint arXiv:2406.12109. Luxi He, Mengzhou Xia, and Peter Henderson. 2024. Whats in your\" safe\" data?: Identifying benign data that breaks safety. arXiv preprint arXiv:2404.01099. Christoph Hienerth, Peter Keinz, and Christopher Lettl. 2011. Exploring the nature and implementation process of user-centric business models. Long Range Planning, 44(5-6):344374. Meng-Leong How, Sin-Mei Cheah, Aik Cheow Khor, and Yong Jiet Chan. 2020. Artificial intelligenceenhanced predictive insights for advancing financial inclusion: human-centric ai-thinking approach. Big Data and Cognitive Computing, 4(2):8. Allen Huang, Hui Wang, and Yi Yang. 2023. Finbert: large language model for extracting information from financial text. Contemporary Accounting Research, 40(2):806841. Lars Magnus Hvattum and Halvard Arntzen. 2010. Using elo ratings for match result prediction in association football. International Journal of forecasting, 26(3):460470. Michimasa Inaba, Mariko Ukiyo, and Keiko Takamizo. 2024. Can large language models be used to provide psychological counselling? an analysis of gpt-4-generated responses using role-play dialogues. Preprint, arXiv:2402.12738. Weina Jin, Jianyu Fan, Diane Gromala, Philippe Pasquier, and Ghassan Hamarneh. 2021. Euca: The end-user-centered explainable ai framework. arXiv preprint arXiv:2102.02437. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. Preprint, arXiv:2001.08361. Alex Kim, Maximilian Muhn, and Valeri Nikolaev. 2024. Financial statement analysis with large language models. arXiv preprint arXiv:2407.17866. Kelvin JL Koa, Yunshan Ma, Ritchie Ng, and Tat-Seng Chua. 2024. Learning to generate explainable stock predictions using self-reflective large language models. In Proceedings of the ACM on Web Conference 2024, pages 43044315. David Krause. 2023. Large language models and generative ai in finance: an analysis of chatgpt, bard, and bing ai. Bard, and Bing AI (July 15, 2023). Jieun Kwon, Younghyun Choi, and Yura Hwang. 2021. Enterprise design thinking: An investigation on usercentered design processes in large corporations. Designs, 5(3):43. Jiangtong Li, Yuxuan Bian, Guoxuan Wang, Yang Lei, Dawei Cheng, Zhijun Ding, and Changjun Jiang. 2023a. Cfgpt: Chinese financial assistant with large language model. arXiv preprint arXiv:2309.10654. Ting Li, Jan van Dalen, and Pieter Jan van Rees. 2018. More than just noise? examining the information content of stock microblogs on financial markets. Journal of Information Technology, 33(1):5069. Xianzhi Li, Samuel Chan, Xiaodan Zhu, Yulong Pei, Zhiqiang Ma, Xiaomo Liu, and Sameena Shah. 2023b. Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? study on several typical tasks. arXiv preprint arXiv:2305.05862. 10 Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2023c. Split and merge: Aligning position biases in large language model based evaluators. Preprint, arXiv:2310.01432. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment. Preprint, arXiv:2303.16634. Alejandro Lopez-Lira and Yuehua Tang. 2023. Can chatgpt forecast stock price movements? return predictability and large language models. arXiv preprint arXiv:2304.07619. Heba Soltan Mohamed, Gauss Cordeiro, Minkah, Haitham Yousof, and Mohamed Ibrahim. 2024. size-of-loss model for the negatively skewed insurance claims data: applications, risk analysis using different methods and statistical forecasting. Journal of Applied Statistics, 51(2):348369. Sendhil Mullainathan and Jann Spiess. 2017. Machine learning: an applied econometric approach. Journal of Economic Perspectives, 31(2):87106. Sidra Naveed, Gunnar Stevens, and Dean-Robin Kern. 2022. Explainable robo-advisors: Empirical investigations to specify and evaluate user-centric taxonomy of explanations in the financial domain. In IntRS@ RecSys, pages 85103. Son The Nguyen and Theja Tulabandhula. 2023. Generative ai for business strategy: Using foundation models to create business strategy tools. arXiv preprint arXiv:2308.14182. Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John Mulvey, Vincent Poor, Qingsong Wen, and Stefan Zohren. 2024. survey of large language models for financial applications: Progress, prospects and challenges. arXiv preprint arXiv:2406.11903. Marco Pagano. 1993. Financial markets and growth: An overview. European economic review, 37(2-3):613 622. Galena Pisoni and Natalia Díaz-Rodríguez. 2023. Responsible and human centric ai-based insurance adInformation Processing & Management, visors. 60(3):103273. Maryan Rizinski, Andrej Jankov, Vignesh Sankaradas, Eugene Pinsky, Igor Mishkovski, and Dimitar Trajanov. 2024. Comparative analysis of nlp-based Information, models for company classification. 15(2):77. Loris Roveda, Palaniappan Veerappan, Marco Maccarini, Giuseppe Bucca, Arash Ajoudani, and Dario Piga. 2023. human-centric framework for robotic task learning and optimization. Journal of Manufacturing Systems, 67:6879. Yangjun Ruan, Chris J. Maddison, and Tatsunori Hashimoto. 2024. Observational scaling laws and the predictability of language model performance. Preprint, arXiv:2405.10938. Alaina N. Talboy and Elizabeth Fuller. 2023. Challenging the appearance of machine intelligence: Cognitive bias in llms and best practices for adoption. Preprint, arXiv:2304.01358. Qwen Team. 2024. Qwen2.5: party of foundation models. Writer Engineering team. 2024. Palmyra-Fin-70B-32k: powerful LLM designed for Finance. https:// dev.writer.com. Richard Thaler. 2008. Mental accounting and consumer choice. Marketing science, 27(1):1525. Hanshuang Tong, Jun Li, Ning Wu, Ming Gong, Dongmei Zhang, and Qi Zhang. 2024. Ploutos: Towards interpretable stock movement prediction with arXiv preprint financial large language model. arXiv:2403.00782. Amos Tversky and Daniel Kahneman. 1981. The framing of decisions and the psychology of choice. science, 211(4481):453458. Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, and Mei Han. 2024. Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates. arXiv preprint arXiv:2408.13006. Christopher Wimmer and Navid Rekabsaz. 2023. Leveraging vision-language models for granular market change prediction. arXiv preprint arXiv:2301.10166. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: large language model for finance. arXiv preprint arXiv:2303.17564. Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023. Pixiu: large language model, instruction data and evaluation benchmark for finance. arXiv preprint arXiv:2306.05443. Qianqian Xie, Dong Li, Mengxi Xiao, Zihao Jiang, Ruoyu Xiang, Xiao Zhang, Zhengyu Chen, Yueru He, Weiguang Han, Yuzhe Yang, Shunian Chen, Yifei Zhang, Lihang Shen, Daniel Kim, Zhiwei Liu, Zheheng Luo, Yangyang Yu, Yupeng Cao, Zhiyang Deng, Zhiyuan Yao, Haohang Li, Duanyu Feng, Yongfu Dai, VijayaSai Somasundaram, Peng Lu, Yilun Zhao, Yitao Long, Guojun Xiong, Kaleb Smith, Honghai Yu, Yanzhao Lai, Min Peng, Jianyun Nie, Jordan W. Suchow, Xiao-Yang Liu, Benyou Wang, Alejandro Lopez-Lira, Jimin Huang, and Sophia Ananiadou. 2024. Open-finllms: Open multimodal large language models for financial applications. Preprint, arXiv:2408.11878. 11 Yang Xu, Yingchia Liu, Haosen Xu, and Hao Tan. 2024. Ai-driven ux/ui design: Empirical research and applications in fintech. International Journal of Innovative Research in Computer Science & Technology, 12(4):99109. Daksha Yadav, Sabrina Zhang, Tom Jin, Prakash Krishnan, and Des Clarke. 2024. Generative ai based virtual assistant for reconciliation research. Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023. Fingpt: Open-source financial large language models. Preprint, arXiv:2306.06031. Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. 2024. survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing, page 100211. Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, et al. 2024. Rjudge: Benchmarking safety risk awareness for llm agents. arXiv preprint arXiv:2401.10019. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. 2024a. Mmmu: massive multi-discipline multimodal understanding In Proand reasoning benchmark for expert agi. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, et al. 2024b. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813. Liwen Zhang, Weige Cai, Zhaowei Liu, Zhi Yang, Wei Dai, Yujie Liao, Qianru Qin, Yifei Li, Xingyu Liu, Zhiqiang Liu, Zhoufan Zhu, Anbo Wu, Xin Guo, and Yun Chen. 2023a. Fineval: chinese financial domain knowledge evaluation benchmark for large language models. Preprint, arXiv:2308.09975. Xuanyu Zhang, Qing Yang, and Dongliang Xu. 2023b. Xuanyuan 2.0: large chinese financial chat model with hundreds of billions parameters. Preprint, arXiv:2305.12002."
        },
        {
            "title": "A Questionnaire",
            "content": "The questionnaire of our survey is shown in Figure 10."
        },
        {
            "title": "B UCFE Dataset Information",
            "content": "B.1 Geographical Distribution Figure 11 shows the geographical distribution of our previous survey. Among our 804 participants, 62.9% of them are from China, 35.9% from the USA, and 1.2% from other regions. This highlights the dominance of responses from China and the USA. B.2 Results of the survey Figure 12 and Figure 13 report the primary financial information source and the results of whether users prefer generation answers or predefined options, which demonstrates the diversity of our benchmark contributors. B.3 Detailed Information of Each Task B.3.1 Zero-shot Tasks Similar to existing benchmarks, the zero-shot tasks in the UCFE benchmark require models to handle new financial problems without any prior examples. These tasks assess the models ability to generalize across different types of financial challenges. The UCFE benchmark includes four zero-shot tasks: Stock Price Prediction: Predicting future stock prices using historical A-stock statistics is common task in financial forecasting. Bearish Information Detection: Identifying whether the information affects the market negatively from sources such as EastMoney, similar to risk detection tasks in other benchmarks. Financial Indicator Calculation: Computing important financial metrics using standard CPA and CFA formulas, much like quantitative tasks in existing financial benchmarks. Financial Information Summarization: Summarizing news headlines to capture key insights, task also present in general NLP benchmarks but adapted to the financial context."
        },
        {
            "title": "Questionnaire",
            "content": "We are conducting study to gather insights on how users engage with financial tasks in real-world scenarios. Your participation will help us improve the design of user-centric and multi-round financial analysis tasks. The survey will take approximately 10 minutes. Section 1: Participant Demographics 1. What is your current role or profession? General Public (No professional experience in finance) Student (Finance-related major) Student (Non-finance major) Finance Professional (e.g., Analyst, Banker, Consultant) Non-Finance Professional (e.g., Engineer, Teacher, etc.) Regulatory Professional (e.g., Securities Regulator, Compliance Officer) Other (please specify) 2. How familiar are you with financial analysis tasks (e.g., stock price prediction, credit risk evaluation, etc.)? Not familiar Somewhat familiar Very familiar 3. What is your primary source of financial information? Company reports (e.g., annual reports, prospectuses) Financial news outlets (e.g., Bloomberg, Reuters) Online financial services (e.g., Yahoo Finance, Eastmoney) Financial consultancies or analysts Other (please specify) Section 2: Interaction with Financial Tasks 1. How often do you perform financial analysis tasks at work or in your personal life? Daily Weekly Monthly Rarely 2. Have you engaged in financial tasks that involve multi-round analysis (i.e., where multiple steps or iterations are required)? Yes No Not sure 3. Do you think it is necessary to study multi-round financial tasks, both academically and in the finance industry? Yes No Not sure 4. When working on financial tasks, do you prefer receiving predefined options (e.g., multiple-choice) or generating your own answers (e.g., writing reports or summaries)? Predefined options (e.g., multiple-choice) Generating answers (e.g., writing reports, creating strategies) mix of both Section 3: Scenario Coverage 1. Which financial tasks have you encountered in your work or studies? (Open-ended) 2. Do you find it useful to simulate real-world financial scenarios (e.g., stock market predictionsrisk assessments) when completing tasks? Yes, it helps to improve my analysis skills Somewhat, but real-world scenarios can be complex No, prefer hypothetical tasks 3. Where do you come from? (Open-ended) Note: We collect this questionnaire solely for academic purposes, and your personal information will not be used for commercial purposes. Figure 10 13 Figure 13: Results of whether preferring generation answers or predefined options valuations using data from EastMoney. Company Evaluation Reporting: Evaluate company performance using financial reports. Corporate Operation Analysis: Analyze company operations based on analyst reports. Risk and Compliance Tasks These tasks focus on identifying financial risks and ensuring compliance with regulations, where users interact with the model to iteratively refine their analysis: Credit Risk Evaluation: Assess credit risks based on GPT-4-generated data. Insider Trading Detection: Identify potential insider trading cases using court records of historical insider trading case reports. Prospectus Risk Summarization: Summarize risks in prospectuses and inquiry letters, refining insights based on user feedback. Financial Regulation Consulting: Provide guidance on regulatory compliance and potential punishments using Securities Law. Strategy and Optimization Tasks In these tasks, users interact with the model to evaluate and optimize investment strategies. The multi-turn nature allows users to explore different strategies or finetune their approach: Investment Strategy Evaluation: Evaluate effectiveness and summarize investment strategies using data from Seeking Alpha. Investment Strategy Optimization: Optimize strategies with feedback from multiple rounds of user queries. Figure 11: Geographical Distribution of Survey Respondents Figure 12: Primary Source of Financial Information extracted from the survey These tasks are designed to reflect real-world financial decision-making scenarios, targeting two broad user groups: the general public and financial professionals. In essence, they encompass wide range of users, making the benchmark applicable to all types of financial stakeholders. B.3.2 Few-shot Tasks The few-shot tasks in the UCFE benchmark involve multi-turn financial interactions, focusing on how models adapt to evolving user input over several rounds. Unlike single-turn tasks in existing benchmarks, these tasks emphasize real-world financial decision-making. We categorize the 13 tasks into the following four main groups: Analytical and Evaluation Tasks These tasks require the model to simulate the role of financial analysts, providing detailed insights based on iterative queries. The model must refine its responses as users ask follow-up questions: Analyst Simulation: Comprehensive analysis of company performances from financial reports and analyst reviews and generate recommendations. Asset Valuation Reporting: Provide asset Consulting and Summarization Tasks These tasks involve providing consulting services or summarizing financial information, where users may request additional clarification or focused insights over several interactions: 14 Financial Knowledge Consulting: Offer explanation on financial terminologies and basic financial knowledge based on sources like Investopedia. Industry Report Summarization: Summarize industry reports from EastMoney, allowing users to quickly identify key trading insights. Newshare Evaluation Reporting: Evaluate target price range of newly issued shares, analyze risk and opportunity of the new share based on company overview using data from platforms like stock.us. Figure 14: Win counts heatmap for all tasks. The heatmap illustrates the total number of wins where the target model outperforms the base model across all headto-head comparisons."
        },
        {
            "title": "C Human Expert Evaluation",
            "content": "Figure 15 illustrates the evaluation pipeline. Human experts first assess the existing results, and if the outcomes directly contradict their expertise, they are asked to make another selection. Additionally, we designed UI interface, shown in Figure 16, to ensure that participants are unaware of the model names. After all the evaluations, we recalculated the Elo scores based on the samples evaluated by the human experts."
        },
        {
            "title": "D More Experiment Results",
            "content": "Figure 14 highlights each models comparative performance, showcasing strengths and weaknesses. And Table 6 presents the Elo scores across all tasks. The results show that LLMs trained on financial text data demonstrate outstanding performance 15 Figure 15: Human evaluation pipeline. across various tasks. However, due to limitations in the zero-shot task texts, the models performance in those tasks may be affected. Figure 17 shows an example dialogue between User Simulator and Tongyi-Finance in the Financial Knowledge Consulting Task."
        },
        {
            "title": "E Prompt",
            "content": "E.1 Zero Shot Task Figures [18, 19, 20] have shown all the prompts we used for testing and evaluation for Financial Text Summarization. E.2 Few Shot Task Figures [21, 22, 23] have shown all the prompts we used for testing and evaluation for Asset Valuation Reporting. r I - 7 - 5 . 2 Q r I - 4 1 - 5 . 2 Q n - 4 - t t - 0 7 - 1 . 3 - l - C 0 7 - 3 Y X - 3 l t - 7 - 5 . 2 r I r I - 8 - 1 . 3 - l 4 - k 2 3 - 0 7 - - m B 7 - 2 F - h 4 1 - a - n e 8 7 . 5 9 8 2 0 . 3 2 8 2 5 . 3 8 8 9 6 . 8 8 9 8 . 9 1 7 7 0 . 7 4 7 6 6 . 5 9 7 4 8 . 3 6 8 9 0 . 3 6 7 7 6 . 7 8 3 8 . 6 5 7 9 3 . 0 8 8 2 0 . 6 3 9 8 0 . 0 1 0 1 4 2 . 3 8 9 5 8 . 9 1 9 4 . 7 2 9 8 4 . 4 1 8 2 5 . 5 7 8 9 8 . 1 3 9 5 6 . 1 9 8 9 7 . 9 0 3 9 . 6 0 8 2 8 . 9 6 8 4 4 . 5 6 8 7 5 . 7 4 9 7 8 . 5 5 8 2 6 . 3 4 5 8 . 9 2 8 2 4 . 0 5 0 1 3 6 . 5 3 8 0 7 . 1 1 0 1 3 5 . 9 0 0 1 4 8 . 7 9 9 2 . 1 7 9 2 8 . 5 5 8 3 6 . 3 3 9 7 5 . 6 5 9 4 1 . 3 4 0 1 3 7 . 7 7 6 3 . 8 8 8 0 9 . 5 7 8 2 7 . 1 9 8 0 9 . 8 7 9 1 4 . 5 7 9 8 1 . 8 2 8 9 . 8 9 8 1 0 . 2 5 0 1 2 9 . 2 2 9 5 7 . 4 2 0 1 9 5 . 0 8 9 5 1 . 1 6 8 6 . 4 5 9 5 7 . 1 0 9 0 4 . 5 1 9 3 4 . 9 3 9 6 0 . 7 1 9 9 0 . 6 0 0 7 0 . 6 5 9 1 5 . 2 0 9 9 3 . 2 8 9 8 5 . 8 2 9 4 4 . 8 6 9 3 2 . 1 8 2 9 . 1 8 9 8 . 9 7 8 1 7 . 1 4 9 1 0 . 0 0 0 1 2 3 . 0 8 9 3 1 . 8 2 9 2 . 7 2 0 1 6 2 . 2 1 9 3 9 . 6 2 9 3 4 . 1 7 8 8 9 . 6 8 8 6 9 . 0 8 4 6 . 2 9 9 5 0 . 0 2 9 8 6 . 8 0 0 1 1 7 . 8 8 8 5 . 6 3 9 3 7 . 9 2 9 0 4 . 1 6 9 7 0 . 5 2 9 2 7 . 9 9 6 7 . 5 8 9 3 5 . 0 7 9 9 0 . 2 7 9 5 . 9 0 9 8 4 . 3 1 9 9 4 . 8 1 0 1 3 5 . 0 6 0 1 1 9 . 7 4 0 1 7 5 . 9 0 9 2 . 9 3 0 1 4 1 . 8 5 9 6 1 . 5 2 0 1 4 9 . 3 0 0 1 4 3 . 4 5 0 1 1 7 . 9 6 6 2 . 5 3 0 1 6 2 . 2 8 9 8 8 . 3 5 9 7 6 . 3 0 0 1 5 8 . 8 7 9 5 8 . 1 6 0 4 8 . 2 8 9 5 8 . 5 9 9 2 6 . 7 2 0 1 7 3 . 9 5 9 7 4 . 7 2 0 1 0 2 . 0 2 0 8 8 . 8 2 0 1 1 8 . 2 6 0 1 3 5 . 4 3 0 1 3 5 . 0 5 0 1 8 1 . 0 8 1 1 0 7 . 6 8 0 1 5 . 7 8 0 1 7 4 . 0 3 0 1 6 5 . 9 7 9 8 8 . 7 6 9 3 3 . 7 0 0 1 1 9 . 1 3 1 3 6 . 5 9 0 1 7 8 . 6 4 0 1 1 3 . 6 6 0 1 6 3 . 3 3 0 1 3 4 . 8 3 0 1 3 3 . 6 7 0 5 8 . 2 8 0 1 0 2 . 3 7 0 1 9 1 . 5 3 0 1 3 7 . 9 4 1 1 2 4 . 8 5 0 1 3 7 . 9 9 8 3 . 7 9 9 4 7 . 4 1 1 1 7 5 . 0 1 1 1 4 4 . 2 1 0 1 5 3 . 8 7 9 0 1 . 8 5 0 8 . 2 2 0 1 8 6 . 7 1 1 1 5 1 . 6 3 0 1 8 8 . 2 4 1 1 3 0 . 0 0 0 1 4 4 . 7 9 0 0 6 . 1 2 1 1 3 8 . 0 3 1 1 5 9 . 8 2 1 1 8 3 . 4 5 1 1 3 1 . 9 3 1 1 7 4 . 4 7 0 3 1 . 1 3 1 1 5 6 . 6 5 0 1 7 5 . 2 2 1 1 3 7 . 3 8 9 5 8 . 8 9 9 7 6 . 7 6 0 4 0 . 0 3 0 1 5 2 . 8 2 1 1 . 0 8 8 8 1 1 . 7 5 2 0 2 . 1 0 4 9 1 1 . 8 4 3 5 0 1 . 6 9 9 0 2 . 6 1 5 7 2 1 . 0 6 9 5 1 1 . 7 4 5 3 0 . 4 0 6 3 0 1 . 7 3 3 7 1 1 . 3 7 0 4 1 . 7 5 7 3 0 1 . 4 4 3 2 1 1 1 2 . 1 8 9 . 0 4 3 1 1 1 . 7 7 0 8 1 1 . 1 5 3 0 1 1 . 5 7 5 5 1 1 . 5 2 4 1 1 1 . 9 5 7 7 0 1 . 8 1 8 6 0 1 . 5 1 8 7 1 1 . 0 5 1 5 1 1 . 0 9 1 8 1 1 . 2 8 0 7 0 1 6 . 6 9 9 . 9 2 1 3 0 1 . 5 9 3 2 2 . 1 5 7 7 1 1 9 3 . 9 8 9 . 6 7 1 7 1 1 . 6 6 8 1 0 1 2 7 . 8 9 . 7 1 9 1 1 1 7 1 . 4 7 9 . 9 9 6 5 1 1 t e i l y m i a o r e o C t a s t r i s e l K c F i s n a e l n i t r u o y u n c D d r i n a i y a t t n i l y a t t n g r R t a r w o z m k s e r i p o u t A t i t a o e n a f v g o l a t d a a i t r u e i n o i P r o l v . a e h - c h n r e e w , a e h s i i R . a n F h s t l i l r o : 6 a 16 Figure 16: UI interface for human expert evaluation"
        },
        {
            "title": "Example Dialogue",
            "content": "User Simulator: Hey, can you explain what financial status analysis is? Im curious about how its used by businesses or individuals. LLM: Sure, Id be happy to explain the financial status analysis! Financial status analysis is process of evaluating an individuals or businesss financial situation to determine their current financial health and identify areas for improvement. It involves analyzing various financial statements and documents to assess their financial performance, liquidity, solvency, and overall financial position. For individuals, financial status analysis may include reviewing bank statements, credit reports, tax returns, and other financial documents to determine their income, expenses, assets, and liabilities. This information can help individuals understand their current financial situation, identify areas for improvement, and create plan to achieve their financial goals. For businesses, financial status analysis may involve reviewing financial statements, such as the balance sheet, income statement, and cash flow statement, to assess their financial performance, liquidity, and solvency. This information can help businesses identify areas where they can improve their financial performance, make informed decisions about investments and spending, and manage their finances more effectively. Overall, financial status analysis is an important tool for individuals and businesses to understand their financial situation and make informed decisions about their finances. User Simulator: Alright, that makes sense. So, for business, does this analysis help in making decisions about spending and investments based on the financial statements? LLM: Yes, thats correct! Financial status analysis for business involves reviewing financial statements to assess the companys financial performance, liquidity, solvency, and overall financial position. This information can help the business make informed decisions about spending and investments. For example, if business is experiencing high level of liquidity, it may be able to invest more in new equipment or marketing initiatives to grow its business. On the other hand, if the business is experiencing financial difficulties, it may need to focus on improving its cash flow and reducing expenses to maintain solvency. By analyzing financial statements, business can identify areas where it can improve its financial performance and make informed decisions about spending and investments. This can help the business achieve its long-term goals and maintain financial stability. User Simulator: Got it, thanks! So its all about understanding where the moneys coming from and going, right? LLM: Yes, thats right! Financial status analysis involves understanding where the money is coming from and going to. By analyzing financial statements and other financial documents, individuals and businesses can gain insight into their income, expenses, assets, and liabilities. For individuals, this can help them understand their cash flow and identify areas where they can reduce expenses or increase their income. For businesses, it can help them identify areas where they can improve their financial performance and make informed decisions about spending and investments. Overall, financial status analysis is an important tool for individuals and businesses to understand their financial situation and make informed decisions about their finances. User Simulator: Great, that clears things up for me. Thanks for the help! <Consultation Ended> Figure 17: Example Dialogue between User Simulator and Tongyi-Finance-14B-Chat"
        },
        {
            "title": "Test Prompt",
            "content": "Model Prompt: You are providing summary service for financial texts to help users extract key points from complex financial information. The given financial text is: {information} Your task is: {needs}. Figure 18: Test Prompt for Financial Text Summarization"
        },
        {
            "title": "Source Information",
            "content": "New Zealands Ministry of Foreign Affairs issued statement on the 19th regarding the volcanic eruption disaster in Tonga, stating that Tonga has now established temporary communication system that can use 2G signals to contact the outside world, but communication is still limited and sporadic. Meanwhile, countries such as New Zealand and Fiji plan to provide aid to Tonga. Figure 19: Source Information for Financial Text Summarization"
        },
        {
            "title": "Evaluation Prompt",
            "content": "Evaluation Criteria: Please act as fair judge to assess the quality of the dialogue between the user and the AI assistant. Please read the user requirements and evaluation hints before assessing to help you better analyze the dialogue quality. The users needs are: {needs}. The evaluation hints regarding specific content for your reference are: {evaluation_hints} When assessing, you also need to consider the following dimensions: - Meeting user needs: Your evaluation should consider whether the AI assistants responses comprehensively and appropriately meet the users needs. - Accuracy of facts: Is the information provided accurate and based on credible facts and data? - Fairness and accountability: Are the suggestions or information provided feasible and accountable, and do they consider potential risks and consequences? - Richness: Does it contain abundant information, depth, contextual considerations, diversity, detailed explanations, and examples to meet user needs and provide comprehensive understanding? - Hallucination: Are there any hallucinations in the AI assistants responses? - Note: Do not let the length of the response affect your scoring! Longer responses are not necessarily better; concise answers that meet the above requirements are good. After the assessment, strictly output your final conclusion in the following format: if AI Assistant 1 performed better, output [[1]]; if AI Assistant 2 performed better, output [[2]]; if its tie, output [[3]]. [AI Assistant 1 Dialogue Start] {dialogue1} [AI Assistant 1 Dialogue End] [AI Assistant 2 Dialogue Start] {dialogue2} [AI Assistant 2 Dialogue End] Evaluation Points: AI assistants suggested reference answer is: The New Zealand Ministry of Foreign Affairs stated on the 19th that Tonga has established temporary communication system, but communication is still limited. New Zealand and countries like Fiji plan to provide aid to Tonga. This answer is an ideal response example. The AI assistants response must include the following key content (the expression can vary): Tonga has established temporary communication system, and the absence of this content will result in the answer being directly judged as incorrect. Ideally, the AI assistants response should include the following points: communication is limited, New Zealand and countries like Fiji plan to provide aid to Tonga, to ensure the comprehensiveness of the answer. Figure 20: Evaluation Prompt for Financial Text Summarization"
        },
        {
            "title": "Test Prompt",
            "content": "Role Prompt: You are role-playing as writer. You are conversing with an AI assistant, hoping it can help generate an asset evaluation report. The purpose, object, and scope of the evaluation are: information. Your needs are: needs. Ensure to converse with the AI assistant in the tone of writer, avoid unnecessary chatter, and try to be as realistic as possible. If you feel the AI assistants response meets your needs, you can output the corresponding characters as instructed by the prompt. If not, raise your concerns based on the AI assistants response. Note: What you need to do is simulate user asking the AI assistant questions based on the provided information and needs (if any) rather than answering or solving problems. You do not need to perform any calculations, analysis, or generate report content. If the AI assistant asks questions or needs additional information, please answer truthfully. Please start your conversation. User Intention: Generate the purpose, object, and scope sections in the asset evaluation report. Model Prompt: You are providing document services to writer. During the service provision process, you can ask the other party for more information. The template must be in the form of an asset appraisal report. Figure 21: Test Prompt for Asset Valuation Reporting"
        },
        {
            "title": "Source Information",
            "content": "The evaluation object is Keda Guochuang Xinneng Technology Co., Ltd., and the reason for evaluating the assets is that Keda Guochuang Software Co., Ltd. is issuing shares to purchase 100% equity of Keda Guochuang Xinneng Technology Co., Ltd. Keda Guochuang Software Co., Ltd. has signed the Share Issuance and Asset Purchase Agreement with Hefei Guibo Equity Investment Partnership (Limited Partnership) and Hefei Zixu Investment Partnership (Limited Partnership). The partners of Hefei Guibo Equity Investment Partnership (Limited Partnership) and Hefei Zixu Investment Partnership (Limited Partnership) include Sun Lu, Shi Xingling, Xu Genyi, Chen Xuexiang, Zhang Qiyun, and Dong Xianquan. As of December 31, 2020, the book value of the companys total assets was 461.3236 million yuan, the book value of total liabilities was 161.9956 million yuan, and the book value of net assets was 299.3280 million yuan. Figure 22: Source Information for Asset Valuation Reporting"
        },
        {
            "title": "Evaluation Prompt",
            "content": "Evaluation Criteria: Please act as fair judge to assess the quality of the dialogue between the user and the AI assistant. Please read the user requirements and evaluation hints before assessing to help you better analyze the dialogue quality. The users needs are: {needs}. The evaluation hints regarding specific content for your reference are: {evaluation_hints} When assessing, you also need to consider the following dimensions: - Meeting user needs: Your evaluation should consider whether the AI assistants responses comprehensively and appropriately meet the users needs. - Accuracy of facts: Is the information provided accurate and based on credible facts and data? - Fairness and accountability: Are the suggestions or information provided feasible and accountable, and do they consider potential risks and consequences? - Richness: Does it contain abundant information, depth, contextual considerations, diversity, detailed explanations, and examples to meet user needs and provide comprehensive understanding? - Hallucination: Are there any hallucinations in the AI assistants responses? - Note: Do not let the length of the response affect your scoring! Longer responses are not necessarily better; concise answers that meet the above requirements are good. After the assessment, strictly output your final conclusion in the following format: if AI Assistant 1 performed better, output [[1]]; if AI Assistant 2 performed better, output [[2]]; if its tie, output [[3]]. [AI Assistant 1 Dialogue Start] {dialogue1} [AI Assistant 1 Dialogue End] [AI Assistant 2 Dialogue Start] {dialogue2} [AI Assistant 2 Dialogue End] Evaluation Points: AI assistants response must include the following key content (expression can vary): 1. The assessment purpose must include the agreement signing time; 2. The partners name must be included in the assessment purpose section; 3. The assessment purpose must be single section; 4. The assessment object and scope must be the second section, and content from different sections should not be confused, missing these contents will result in the answer being directly judged as incorrect. Encourage the AI assistant to mention the following content in the response: 1. Add background information to enhance the completeness of the report. 2. Provide detailed explanation of the assessment purpose, including an explanation of the importance or necessity of the assessment purpose, to improve the persuasiveness of the report. 3. Any key factors within the assessment scope that may affect asset value, such as market conditions, industry trends, etc., to provide more comprehensive assessment perspective, this will help improve the quality of the answer. The AI assistants response should avoid including the following content: 1. Avoid including detailed company history or unrelated business introductions that are not related to the assessment purpose, object, and scope in the report. 2. Vague or uncertain language. 3. Ensure that the assessment purpose, object, and scope are each independent, do not mix the content together. Mentioning these contents will result in the answer being judged as inappropriate. Figure 23: Evaluation Prompt for Asset Valuation Reporting"
        }
    ],
    "affiliations": [
        "Nanjing University",
        "The Chinese University of Hong Kong, Shenzhen",
        "The Fin AI"
    ]
}