{
    "paper_title": "Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions",
    "authors": [
        "Yunheng Li",
        "Hengrui Zhang",
        "Meng-Hao Guo",
        "Wenzhao Gao",
        "Shaoyong Jia",
        "Shaohui Jiao",
        "Qibin Hou",
        "Ming-Ming Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 1 ] . [ 1 3 1 0 3 1 . 2 0 6 2 : r Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions Yunheng Li1, Hengrui Zhang1, Meng-Hao Guo3, Wenzhao Gao2, Shaoyong Jia2, Shaohui Jiao2, Qibin Hou1, Ming-Ming Cheng1 1VCIP, School of Computer Science, Nankai University 2ByteDance Inc. 3Tsinghua University Corresponding author. Abstract: Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by videoinstruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with singleand multi-attribute supervision; (ii) ASID-Verify, scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro. Project: https://asid-caption.github.io/ Data: https://huggingface.co/datasets/AudioVisual-Caption/ASID-1M Model: https://huggingface.co/AudioVisual-Caption/models Code: https://github.com/HVision-NKU/ASID-Caption Date: February 16, HVision@Nankai"
        },
        {
            "title": "Introduction",
            "content": "Multimodal large language models (MLLMs) [47, 32, 11, 51] play central role in advancing universal video understanding by enabling unified modeling across visual, audio, and natural language modalities. Nevertheless, understanding real-world videos remains challenging due to the temporal distribution and semantic diversity of fine-grained information. In this context, the quality of video caption supervision plays critical role, as expressive and well-structured captions not only improve captioning performance but also provide transferable semantic representations that support multimodal alignment and downstream video understanding and generation [17, 6, 49]. Recent approaches [19, 26, 22, 23, 8, 7] to video understanding typically rely on caption-based instruction data for supervised fine-tuning (SFT), often followed by reinforcement learning (RL) for behavior refinement, as illustrated in Fig. 1. Because SFT learns directly from captions, its effectiveness is determined by what information the captions explicitly provide. While RL can adjust generation preferences, it cannot reliably reconstruct fine-grained information that is missing training supervision. Thus, model performance is constrained by the availability and diversity of explicit, fine-grained supervision in the training data. As summarized in Tab. 1, existing video instruction datasets provide limited support for learning fine-grained, controllable video understanding. common annotation paradigm assigns each video single fixed-format prompt that produces an incomplete caption, often omitting critical fine-grained information such as temporal localization or camera-related details. Importantly, our analysis in Sec. 3.4 demonstrates that increasing annotation granularity alone does not lead to higher-quality supervision: making multi-attribute captions more comprehensive through detailed prompt design or multi-source ensembling introduces additional incorrect and hallucinated content. Such unreliable fine-grained annotations are unsuitable as training supervision. However, prior datasets generally lack systematic verification for fine-grained annotations, primarily due to the high annotation cost and the difficulty of validating fine-grained annotations at scale. 1 Table 1 Comparison of video instruction datasets in terms of annotation properties. denotes datasets with both visual and auditory annotations, whereas others are visual-only. Dataset Ego4D Narrations [15] InternVid [37] VideoInstruct-100K [28] LLaVA-Video-178K [52] ShareGPT4Video [6] FineVideo [12] AVoCaDO [7] OMNI-CAPTIONER [25] Fine-Grain Multi-Attr Verified Open ASID-1M (Ours) Figure 1 Motivation. Existing caption supervision is typically unstructured, leading to incomplete descriptions. In contrast, our attribute-structured supervision aligns and validates each aspect against audiovisual evidence, enabling fine-grained learning. Moreover, many recent fine-grained datasets remain closed, limiting reproducibility and preventing further verification and refinement of their annotations. To address these challenges, we adopt data-centric approach that focuses on structured instruction design and qualityverified fine-grained supervision for universal video understanding. Specifically, we construct an open-source, millionscale audiovisual instruction dataset, in which annotations are decomposed into complementary singleand multi-attribute descriptions. Rather than modeling each video as single caption, our dataset adopts attribute-structured instructions for more explicit and fine-grained learning. Moreover, we recognize that scaling fine-grained annotations requires principled control over their reliability and consistency. To this end, we develop multi-stage data curation pipeline that performs automatic annotation, verification, and refinement, enforcing semantic and temporal alignment with the corresponding audiovisual content. Building on ASID-1M, we train ASIDCaptioner via multi-stage supervised fine-tuning scheme, enabling improved fine-grained perception and instruction following. We evaluate ASID-Captioner on seven benchmarks that jointly assess caption quality, attribute-level instruction following, and the utility of captions for downstream reasoning and temporal localization. Across these evaluations, ASIDCaptioner exhibits consistent improvements in fine-grained semantic coverage and instruction following over strong opensource models. Notably, the 7B variant remains competitive with Gemini-3-Pro on several evaluations. Our contributions are summarized as follows: ASID-1M: An open-source, million-scale dataset of composable attribute-structured audiovisual instructions, providing fine-grained and complementary supervision for universal video understanding. ASID-Verify: multi-stage data curation pipeline for annotation, automatic verification, and refinement, enforcing semantic and temporal consistency to improve the reliability of fine-grained supervision. ASID-Captioner: video understanding model trained on ASID-1M, achieving consistent gains in fine-grained semantic coverage and attribute-level instruction following over strong open-source models."
        },
        {
            "title": "2.1 Audiovisual Multimodal Models",
            "content": "Recent advances in audiovisual multimodal models have extended large language models to video understanding through joint modeling of visual, audio, and language modalities. Representative approaches jointly model visual representations, acoustic signals, and natural language, enabling instruction following, open-ended reasoning, and conversational interaction over video content [32, 33, 47, 11, 28, 24, 38, 31, 39]. Most existing methods are trained via supervised fine-tuning on instruction-style video-language data, learning to generate single monolithic responses conditioned on video inputs and textual prompts [34, 46, 5]. Beyond supervised learning, several works explore reinforcement learning or preferencebased optimization to refine response behavior [22, 8, 7, 23]. However, these approaches operate mainly at the output level, leaving the semantic structure of supervision unchanged while often requiring carefully designed rewards and substantial training resources. Thus, their effectiveness is constrained by the quality, granularity, and organization of instruction data, highlighting the central role of supervision design and motivating systematic analysis of existing video instruction datasets. 2 Figure 2 Overview of ASID-Verify. Multi-source audiovisual annotations are first generated and ensembled with ASR alignment and temporal consistency verification. Captions are then evaluated at the attribute level to identify missing or incorrect content and refined in targeted manner, producing attribute-structured and quality-verified audiovisual instructions."
        },
        {
            "title": "2.2 Datasets for Audiovisual Understanding",
            "content": "Large-scale datasets have played central role in advancing audiovisual understanding by supervised learning over diverse and complex video content. Early efforts primarily focus on collecting video-text pairs, typically providing coarse video-level descriptions that support broad multimodal alignment [42, 53, 18, 4, 37, 27]. More recent instructionstyle datasets extend this paradigm by pairing videos with conversational captions, increasing descriptive coverage and interaction flexibility, yet still providing limited annotation over fine-grained temporal structure and semantic composition [28, 52, 6, 21]. To address these limitation, subsequent datasets explore finer-grained supervision by annotating specific aspects of video content, including actions, objects, and speech [15, 12, 7]. However, these fine-grained annotations are typically provided without unified semantic structure, resulting in incomplete and inconsistent supervision across different aspects. Moreover, the lack of systematic verification allows incorrect or hallucinated annotations to remain, limiting the effectiveness of fine-grained supervision."
        },
        {
            "title": "3 ASID-1M and ASID-Verify Pipeline",
            "content": "To construct ASID-1M, we develop ASID-Verify, scalable and reproducible data curation pipeline designed to address the limitations of existing audiovisual instruction datasets, particularly the lack of structured fine-grained annotation and systematic verification. As shown in Fig. 2, the pipeline adopts staged process for instruction annotation that integrates annotation ensembling, validation, and targeted refinement instead of single-pass scheme. Each stage of this pipeline is detailed below."
        },
        {
            "title": "3.1 S1: Multi-Source Annotation Generation",
            "content": "The pipeline begins by constructing an initial pool of approximately 125K videos from the YouTube subset of LLaVAVideo-178K [52] and short-form videos (under 3 minutes) from FineVideo [12] to ensure broad audiovisual coverage. Initial annotations are generated using open-source audiovisual models (e.g., AVoCaDO [7]), which jointly model visual content and audio inputs. Spoken content is transcribed and aligned via automatic speech recognition (ASR), using Whisper large-v3 [30] for transcription and WhisperX [3] for speaker-aware segmentation and utterance-level alignment. The resulting ASR subtitles, together with explicit video playback timestamps, are provided as additional inputs to closedsource multimodal models (e.g., Seed-1.61) that serve as additional annotators. Notably, this design enables the multimodal models to produce fine-grained, multi-attribute captions that describe diverse aspects of the video content, while temporal grounding ensures precise timestamp-level alignment of these attributes. Thus, for each video v, we collect multiple candidate captions {y(1) (v)}M m=1 from these complementary annotators, where is the number of annotators."
        },
        {
            "title": "3.2 S2: Caption Ensembling and Verification",
            "content": "Building on the multiple captions generated in Stage 1 from complementary annotation sources, this stage focuses on cap1https://seed.bytedance.com/en/seed1_6 3 Figure 3 Stage-wise analysis of annotation quality and errors under progressively refined training data. tion ensembling to produce more complete and fine-grained annotations. Specifically, we employ strong LLM (Seed1.6) as an integrator to synthesize them into unified allattribute draft caption, denoted as y(2) (v). This integration prioritizes complementary coverage across sources, retaining fine-grained attribute details that may only appear in individual candidates, while resolving obvious redundancy or conflicts. However, this initial integration may still introduce missing details or incorrect combinations. To address this, the draft caption y(2) (v) is further verified against the original audiovisual evidence. In particular, we focus on ASR alignment to ground speech-related descriptions in speaker-aware transcripts, together with timestamp-level consistency checks across different semantic attributes. Based on the verification results, selective refinement produces the caption y(2) (v) by preserving validated content and removing inconsistent audiovisual descriptions, resulting in annotations that are both more comprehensive than individual source candidates."
        },
        {
            "title": "3.3 S3: Attribute-Based Evaluation and Re-",
            "content": "finement Although Stage 2 substantially improves captions with more complete and fine-grained attribute details, errors from source annotations and fusion can still accumulate. key challenge is that these errors are often entangled across multiple semantic attributes, which limits the effectiveness of caption-level verification. Starting from the caption y(2) (v), we decompose it into predefined semantic attributes and independently assess each attribute using an MLLM (Seed-1.6), as shown in Fig. 2. Beyond factual accuracy, we perform expressiveness-level validation to assess caption quality, including fluency, redundancy, and ambiguous references. Attribute-wise verification is conducted under explicit rules for both Error and Missing, resulting in more precise and controllable assessment results. Based on the evaluation results, refinement is selectively applied to the affected attributes while preserving the remaining content, thereby limiting error propagation and improving fine-grained annotation quality. To ensure formatting consistency, auxiliary format validation and correction are applied to handle repetition collapse, malformed timestamps, and non-compliant symbols. After attribute-based evaluation and refinement, 121K videos are retained as the final version of ASID-1M, each paired with eight verified single-attribute captions {ya(v)}aA and final all-attribute caption yA(v), which directly instantiate the progressive training supervision in Sec. 4. In addition, random subset of 300 videos is manually inspected: over 98% of the refined captions are reliable, with remaining issues limited to minor timestamp deviations within 2 seconds."
        },
        {
            "title": "3.4 Analysis of Stage-Wise Contributions",
            "content": "As shown in Fig. 3, we analyze stage-wise contributions using two evaluations: annotation quality measured by Stage 3 No-Error/No-Missing/Clean rates (left), and downstream effectiveness measured by training Qwen2.5-Omni-3B [40] on 200K video captions and testing on Video-SALMONN-2 [33] (right). The results show that captions generated by single model miss substantial amount of fine-grained information, even for strong models such as AVoCaDO [7]. Models specialized in different aspects, such as visual description and speech recognition, tend to capture complementary information from the same video, and integrating their captions substantially improves semantic coverage. Consistent with this observation, early stages that aggregate captions from multiple sources improves coverage, which is reflected by higher No-Missing rates and lower Missing rates after ensembling. However, we find that caption ensembling itself is inherently challenging: single-pass fusion often fails to preserve all fine-grained details present in the source captions. As finer-grained descriptions are incorporated, the ensembling process increasingly introduces Hallucinated content and exposes annotation errors inherited from earlier data sources that were not systematically addressed by prior datasets. Stage 3 provides the key improvement by introducing attribute-level evaluation and targeted refinement, which further reduces missing, effectively suppresses hallucination, and improves the Clean rate."
        },
        {
            "title": "4 Progressive Attribute Learning",
            "content": "As shown in Fig. 4, we adopt three-stage training scheme that progressively expands from single-attribute to all-"
        },
        {
            "title": "5.1 Benchmarks",
            "content": "We evaluate ASID-Captioner on seven benchmarks that offer complementary perspectives on caption quality. Audiovisual captioning: We report results on video-SALMONN-2 testset [33] and UGC-VideoCap [39] (Tab. 2). video-SALMONN2 explicitly measures caption reliability by penalizing missing (Miss) and hallucinations (Hall.), while UGC-VideoCap assesses modality-aware caption quality with separate scores for audio, visual, and detail. Attribute-wise visual captioning: To evaluate fine-grained visual caption quality, we adopt an aspect-wise protocol on the VDC benchmark [5] (Tab. 3) with five fields, including camera, short summary, background, main object, and detailed description, and report both accuracy and judge scores. Text-to-video generation caption: We evaluate on VidCapBench-AE [9] (Tab. 4), which measures caption quality using QA pairs aligned with text-to-video generation across four aspects: video aesthetics, video content, video motion, and physical laws, and reports accuracy (Acc), precision (Pre), coverage (Cov), and conciseness (Con) for each aspect. Caption-based QA: We assess whether captions preserve sufficient evidence for reasoning on Daily-Omni [54] and World-Sense [16] using caption-toQA protocol (Tab. 5). Caption-based temporal grounding: We evaluate whether captions support temporal localization on Charades-STA [13] (Tab. 6). Start and end timestamps of the queried moment are predicted using only the caption, and we report mIoU and recall at IoU thresholds (R1@τ )."
        },
        {
            "title": "5.2 Main Results",
            "content": "Audiovisual captioning. We evaluate audiovisual captioning on Video-SALMONN-2 and UGC-VideoCap, which assess complementary aspects of caption quality. As shown in Tab. 2, ASID-Captioner achieves strong balance between caption completeness and reliability across both benchmarks. On Video-SALMONN-2, ASID-Captioner achieves lower missing rates while maintaining controlled hallucination levels, resulting in improved caption reliability. In contrast, several general-purpose models, such as InternVL3.5 and Qwen2.5-VL, produce conservative captions with fewer hallucinations but substantially more missing details. On UGC-VideoCap, ASID-Captioner performs consistently well across audio, visual, and detailed description dimensions, outperforming prior open-source omni models including UGC-VideoCaptioner and AVoCaDO. Overall, ASIDCaptioner achieves performance comparable to recent Gemini models while maintaining competitive balance of caption completeness and reliability among open-source captioning and omni models. Attribute-wise visual captioning. Tab. 3 reports attributewise results under the five-field protocol. ASID-Captioner performs consistently well across all attributes, with clear advanFigure 4 Overview of progressive attribute learning with stage-wise training and controllable attribute selection at inference. attribute supervision, enabling stable optimization and improved generalization at inference. Our model is built upon Qwen2.5-Omni [40] and optimized via SFT. Stage 1: Attribute-Wise Representation Learning. Training is conducted under single-attribute supervision with attribute-conditioned targets. Each training instance comprises video v, an attribute A, where denotes the predefined attribute set, and corresponding attribute-specific target caption ya describing the aspect specified by a. The model is optimized by minimizing the negative log-likelihood: L1 = E(v,a,ya)D1 [ log p(ya v, a)] , (1) where D1 denotes the collection of training samples (v, a, ya) used in Stage 1. This stage restricts supervision to single attribute per instance, simplifying the learning objective and enforcing attribute-specific learning. Stage 2: All-Attribute Learning with Short Context. Stage 2 replaces single-attribute supervision with all-attribute supervision on short video clips, requiring the model to jointly model multiple attributes. Each training instance is associated with the full attribute set and joint target caption yA. The model is optimized by minimizing: L2 = E(v,yA)D2 [ log p(yA v, A)] . (2) where D2 denotes the Stage 2 training samples supervised with all-attribute captions. This stage shifts learning from attribute-wise modeling to joint modeling over all attributes under short temporal context. Stage 3: Long-Context All-Attribute Learning. Stage 3 trains the model on long video clips (up to 3 minutes) under all-attribute supervision. Compared to Stage 2, the learning objective remains unchanged, while training is conducted on distinct data distribution D3 that exposes the model to extended temporal context. Inference. At inference time, the model accepts video together with user-specified set of attributes and generates caption conditioned on the selected attributes. This enables flexible control over caption content by choosing different attribute combinations as input, without requiring retraining or architectural changes. 5 Table 2 Model performance on the audiovisual captioning benchmarks. Evaluation is conducted using GPT-4.1 as the judge model for Video-SALMONN-2 and GPT-4o for UGC-VideoCap. Denotes closely related work. Model Size Modality video-SALMONN-2 testset UGC-VideoCap Miss Hall. Total Audio Visual Detail Avg. - Gemini-3-Pro - Gemini-2.5-Pro - Gemini-2.5-Flash 8B InternVL3.5 [36] 7B Qwen2.5-VL [2] 7B HumanOmniV2 [43] 7B ARC-Hunyuan-Video [14] 7B Qwen2.5-Omni [40] 8B MiniCPM-o-2.6 [29] video-SALMONN-2 [33] 7B UGC-VideoCaptioner [39] 3B Qwen3-Omni-Instruct [41] 30B-A3B Qwen3-Omni-Captioner [41] 30B-A3B AVoCaDO [7] ASID-Captioner (Ours) ASID-Captioner (Ours) 7B 3B 7B + + + A + + + + + + + + + + + 19.1 18.1 19.3 53.8 40.5 49.2 45.7 41.7 42.2 21.2 31.6 32.0 31.0 21.1 23.4 20.5 16.6 13.3 13.9 25.5 17.0 12.3 12.5 15.4 14.3 17.6 17.0 13.6 16.6 16.2 18.3 15.4 35.7 31.3 33.3 79.4 57.5 61.6 58.2 57.1 56.5 38.8 48.6 45.6 47.6 37.3 41.7 35. 80.4 69.5 69.1 47.9 46.6 45.6 52.7 46.9 38.6 61.8 61.4 67.5 69.0 73.0 78.6 79.1 84.7 74.7 75.8 64.8 69.1 66.3 56.0 66.1 68.5 71.4 58.4 74.8 75.5 74.6 84.8 84.4 80.6 73.7 74.0 59.5 62.3 59.5 55.8 60.0 57.7 68.5 57.5 72.3 72.3 71.8 80.2 80.2 81.9 72.6 73.0 57.4 59.3 57.1 54.8 57.7 54.9 67.2 59.1 71.5 72.3 73.2 81.2 81.2 Table 3 Comparison of attribute-wise visual captioning performance on the VDC benchmark. Accuracy and judge scores are reported for each visual aspect, with Seed-1.6 used as the judge model. Model Size Camera Short Background Main Object Detailed Avg. Acc / Score Acc / Score Acc / Score Acc / Score Acc / Score Acc / Score - - - 3B 7B 7B 35.5 / 1.5 Gemini-3-Pro 35.0 / 1.5 Gemini-2.5-Pro 34.5 / 1.4 Gemini-2.5-Flash 28.0 / 1.3 Qwen2.5-Omni [40] 29.5 / 1.3 Qwen2.5-Omni [40] 29.1 / 1.3 ARC-Hunyuan-Video [14] Qwen3-Omni-Instruct [41] 30B-A3B 33.7 / 1.4 Qwen3-Omni-Captioner [41] 30B-A3B 34.4 / 1.4 35.3 / 1.5 AvoCaDO [7] 37.0 / 1.6 ASID-Captioner (Ours) 38.2 / 1.7 ASID-Captioner (Ours) 7B 3B 7B 27.6 / 1.2 26.6 / 1.1 26.5 / 1.1 23.2 / 1.2 26.1 / 1.2 25.5 / 1.2 26.3 / 1.1 26.8 / 1.2 27.1 / 1.2 28.4 / 1.3 28.8 / 1.3 42.3 / 1.9 41.5 / 1.8 41.1 / 1.7 35.1 / 1.6 38.2 / 1.7 37.4 / 1.6 41.8 / 1.9 42.1 / 1.9 41.0 / 1.8 45.5 / 2.0 46.9 / 2. 45.4 / 2.0 42.3 / 1.8 41.6 / 1.8 34.6 / 1.7 37.3 / 1.7 36.5 / 1.5 42.7 / 1.9 44.7 / 2.0 40.0 / 1.8 44.7 / 2.0 47.4 / 2.1 40.2 / 1.8 38.5 / 1.7 38.1 / 1.7 32.4 / 1.4 34.2 / 1.5 34.3 / 1.5 38.2 / 1.6 39.4 / 1.7 37.9 / 1.7 41.7 / 1.8 43.2 / 1.9 38.2 / 1.7 36.8 / 1.6 36.4 / 1.5 30.7 / 1.4 33.1 / 1.5 32.6 / 1.4 36.5 / 1.6 37.5 / 1.6 36.3 / 1.6 39.5 / 1.7 40.9 / 1.8 tages on camera and main object, where fine-grained visual details are often underrepresented by previous models. In contrast, large-scale MoE-based models improve overall coverage but do not consistently translate their capacity into stronger attribute-wise accuracy. We further observe that ASID-Captioner is competitive with Gemini models on this benchmark and surpasses AVoCaDO in averaged attributewise performance, demonstrating robust fine-grained visual captioning. Text-to-video generation caption. We evaluate caption quality on VidCapBench-AE, which assesses how well captions support text-to-video generation across video aesthetics, content, motion, and physical laws. Tab. 4 shows that ASIDCaptioner achieves strong overall performance across all aspects, with consistently higher precision and coverage compared to most existing approaches. Notably, the gains are most pronounced for video content and video motion, indicating more accurate descriptions of events and dynamics that are critical for text-to-video generation. Moreover, ASIDCaptioner remains competitive on video aesthetics and physical laws, while performance on aesthetics and physical laws remains competitive. Caption-grounded QA. Tab. 5 evaluates whether captions preserve sufficient evidence for downstream reasoning by constraining fixed QA model (Gemini-2.5-Pro) to answer questions using captions only. Under this setting, ASIDCaptioner achieves the strongest performance on both DailyOmni and World-Sense, outperforming prior open-source captioning and omni models by clear margin. The advantage is more pronounced on World-Sense, which requires 6 Table 4 Results on the VidCapBench-AE benchmark. Acc, Pre, Cov, and Con denote accuracy, precision, coverage, and conciseness, respectively, with Con scaled by 100. Scores are computed using GPT-4o following [5]. Model Size Overall Video Aesthetics Video Content Video Motion Physical Laws Acc/Pre/Cov/Con Acc/Pre/Cov/Con Acc/Pre/Cov/Con Acc/Pre/Cov/Con Acc/Pre/Cov/Con Gemini-3-Pro Gemini-1.5-Pro-002 GPT-4o-20240806 Pixtral [1] InternVL2 [10] Qwen2-VL [35] Tarsier [34] Aria [20] Pixtral [1] Llava-Next-Video [50] LongVA [48] mPLUG-Owl3 [45] InternVL2 [10] Qwen2-VL [35] CogVLM2-Caption [44] ASID-Captioner (Ours) ASID-Captioner (Ours) - - - 5.8 / 27.6 / 76.2 / 0.6 18.9 / 58.1 / 92.8 / 3.7 16.7 / 51.4 / 89.8 / 3.3 18.9 / 61.5 / 94.1 / 3.7 10.7 / 39.8 / 91.6 / 2.1 38.2 / 58.3 / 93.5 / 7.5 17.1 / 54.8 / 87.4 / 9.2 16.4 / 47.6 / 85.4 / 8.8 16.9 / 57.8 / 88.5 / 9.1 9.8 / 45.1 / 80.9 / 5.3 28.4 / 59.3 / 88.2 / 15.3 16.8 / 57.4 / 86.0 / 5.9 14.1 / 47.6 / 83.4 / 4.9 17.5 / 61.7 / 87.2 / 6.1 10.2 / 41.3 / 84.0 / 3.6 27.9 / 62.1 / 85.4 / 9.7 6.2 / 28.3 / 81.8 / 1.4 27.9 / 55.9 / 83.2 / 6.4 124B 13.0 / 48.3 / 80.5 / 3.0 13.9 / 44.6 / 80.2 / 3.2 11.9 / 50.0 / 80.5 / 2.7 4.4 / 24.4 / 80.0 / 0.4 23.1 / 55.0 / 78.1 / 2.3 76B 7.4 / 35.6 / 78.9 / 0.7 7.2 / 38.1 / 80.1 / 0.7 72B 12.2 / 46.8 / 79.0 / 7.7 12.0 / 42.5 / 79.2 / 7.6 11.5 / 48.4 / 78.8 / 7.3 5.8 / 28.6 / 77.8 / 3.7 27.1 / 59.6 / 80.9 / 17.2 34B 13.5 / 50.8 / 82.1 / 15.1 14.7 / 43.9 / 85.5 / 16.4 12.4 / 53.7 / 80.4 / 13.8 7.1 / 38.1 / 84.0 / 7.9 28.1 / 61.7 / 83.9 / 31.4 7.1 / 34.2 / 81.8 / 2.3 27.9 / 56.8 / 83.7 / 8.9 25B 14.1 / 51.5 / 84.4 / 4.5 13.0 / 44.0 / 82.7 / 4.2 13.9 / 54.9 / 85.3 / 4.4 3.6 / 18.6 / 69.3 / 1.7 28.6 / 52.4 / 82.4 / 13.5 8.6 / 37.9 / 78.4 / 4.0 12B 11.0 / 39.5 / 79.6 / 5.2 14.5 / 42.7 / 82.8 / 6.8 4.4 / 23.7 / 75.1 / 1.7 24.4 / 54.5 / 82.9 / 9.0 9.6 / 43.2 / 78.1 / 3.5 7B 10.6 / 42.3 / 79.4 / 3.9 11.3 / 39.9 / 82.2 / 4.2 4.9 / 25.1 / 79.6 / 2.8 24.9 / 52.9 / 83.2 / 14.1 7B 10.8 / 43.0 / 79.3 / 6.1 12.8 / 42.1 / 83.8 / 7.3 9.2 / 43.4 / 77.2 / 5.2 5.3 / 33.3 / 80.0 / 2.5 26.9 / 55.7 / 81.7 / 12.8 7B 14.5 / 49.6 / 84.4 / 6.9 12.9 / 40.7 / 83.7 / 6.1 14.8 / 53.5 / 85.1 / 7.0 4.4 / 18.0 / 81.3 / 1.1 23.6 / 52.8 / 85.7 / 5.8 8B 10.2 / 43.0 / 84.9 / 2.5 9.1 / 36.3 / 84.4 / 2.2 10.0 / 46.1 / 85.2 / 2.4 4.0 / 22.7 / 78.2 / 2.3 26.1 / 59.4 / 81.2 / 15.1 9.9 / 48.3 / 75.9 / 5.7 7B 11.1 / 47.1 / 77.0 / 6.4 12.4 / 44.3 / 78.7 / 7.2 5.7 / 33.9 / 82.7 / 3.7 27.9 / 59.9 / 82.7 / 17.8 5B 13.1 / 49.2 / 85.1 / 8.4 12.5 / 45.2 / 83.1 / 8.0 12.7 / 50.8 / 86.3 / 8.1 3B 18.2 / 58.3 / 93.0 / 3.6 15.5 / 50.7 / 90.8 / 3.1 18.7 / 62.5 / 94.2 / 3.7 33.7 / 55.6 / 89.5 / 6.7 8.9 / 34.0 / 90.6 / 1.8 7B 18.2 / 60.0 / 93.3 / 3.7 15.4 / 49.7 / 91.2 / 3.1 18.7 / 64.8 / 94.5 / 3.8 12.9 / 42.7 / 88.6 / 2.6 32.5 / 60.2 / 91.7 / 6.6 Table 5 QA performance by Gemini-2.5-Pro based on captions. Table 7 Ablation of supervision data variants for captioning. Model Size Daily-Omni World-Sense - Gemini-2.5-Pro - Gemini-2.5-Flash 7B HumanOmniV2 [43] 7B ARC-Hunyuan-Video [14] 8B MiniCPM-o-2.6 [29] 7B Qwen2.5-Omni [40] 3B UGC-VideoCaptioner [39] 7B video-SALMONN-2 [33] Qwen3-Omni-Instruct [41] 30B-A3B Qwen3-Omni-Captioner [41] 30B-A3B AVoCaDO [7] ASID-Captioner (Ours) ASID-Captioner (Ours) 7B 3B 7B 60.2 55.3 8.2 8.6 9.8 13.4 17.0 29.9 17.5 27.2 50.1 55.5 61.2 33.8 31.0 6.6 8.7 7.2 8.6 11.2 18.2 12.7 14.1 25.7 32.3 34.0 Table 6 Temporal grounding by Gemini-2.5-Pro based on captions. Model Size mIoU R1@0.3 R1@0.5 R1@0.7 Gemini-3-Pro Gemini-2.5-Pro Gemini-2.5-Flash ASID-Captioner (Ours) ASID-Captioner (Ours) - - - 3B 7B 8.8 27.6 6.1 24.1 28.5 46.4 45.7 40.0 38.0 44. 26.0 25.6 19.5 20.0 26.3 10.2 8.7 7.1 8.0 11.0 reasoning over entities, interactions, and speech, rather than relying on isolated visual evidence. Both Daily-Omni and World-Sense are fully multimodal benchmarks, and ASIDCaptioner maintains strong performance on both the 3B and 7B variants, indicating that the gains stem from higher-quality audiovisual captions rather than model scale. Caption-based temporal grounding. Similar to captiongrounded QA, Tab. 6 evaluates temporal grounding by constraining strong pretrained model (Gemini-2.5-Pro) to predict the start and end time of the queried event using captions only. Under this setting, ASID-Captioner achieves strong performance across all metrics, with clear improvements at higher IoU thresholds. This indicates that the generated captions preserve temporally precise information rather than Dataset - Original GT Non-attribute Multi-attribute video-SALMONN-2 testset DVC Detailed Miss Hall. Total Acc / Score 44.8 43.9 30.6 25.5 16.4 22.3 18.5 18.9 61.2 66.2 49.1 43.4 31.9 / 1.4 35.5 / 1.5 38.5 / 1.6 40.2 / 1.7 Table 8 Stage-wise ablation of the training recipe for captioning. Training Stage video-SALMONN-2 testset DVC Detailed Miss Hall. Total Acc / Score S1 S2 S3 42.1 24.8 23.4 12.8 19.9 18.3 54.9 44.7 41.7 36.1 / 1.6 40.4 / 1.8 41.7 / 1.9 coarse event references."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "All ablation experiments are conducted by randomly sampling 20K training instances and fine-tuning Qwen2.5-Omni-3B under the same optimization settings. Supervision variants for captioning. Tab. 7 analyzes the effect of different supervision data variants on captioning performance. Using raw annotations yields only small improvements over the baseline, indicating that unstructured groundtruth captions provide weak supervision. Non-attribute annotation improves descriptive richness, but still leaves many fine-grained details unmodeled, resulting in incomplete captions. In contrast, multi-attribute supervision achieves the best overall performance, substantially reducing missing content while improving detailed caption quality, demonstrating the benefit of structured, attribute-level supervision. It is worth noting that encouraging finer-grained descriptions may cause some previously unannotated but relevant details (e.g., on-screen text) to be penalized as hallucinations under the current evaluation protocol. 7 Figure 5 Example of an attribute-structured audiovisual caption generated by ASID-Captioner, with timestamps and grounded speech; color highlights indicate the corresponding attribute groups. Table 9 Comparison of attribute-based instruction following for caption generation. Model Gemini-3-Pro Gemini-2.5-Pro Qwen2.5-Omni Qwen2.5-Omni AVoCaDO ASID-Captioner (Ours) ASID-Captioner (Ours) Size 0 0 3B 7B 7B 3B 7B Number of attributes 1 0 0 20.0 14.0 0 52.3 47. 2 0 0 8.0 2.5 0 63.0 58.0 3 2.0 1.5 5.0 6.5 0 9.5 9.6 4 3.0 2.5 6.0 5.5 0 7.5 6. Stage-wise training ablation. Tab. 8 reports captioning performance after each training stage, with all stages trained on the full dataset. Training with S1 yields limited improvement, indicating that single-attribute supervision mainly supports basic attribute grounding but is insufficient for comprehensive captioning. Introducing S2 leads to clear gains, with reduced missing content and improved detailed caption accuracy, highlighting the importance of all-attribute supervision under short temporal contexts. S3 further improves performance by reducing residual missing and enhancing caption detail, showing that longer temporal contexts better integrate information without increasing hallucinations."
        },
        {
            "title": "5.4 Attribute-level Instruction Following",
            "content": "To evaluate attribute-level instruction following, we prompt each model with attribute sets containing one to four attributes, and use Gemini-2.5-Pro as an automatic judge to assess whether the generated captions explicitly satisfy the requested attributes. As shown in Tab. 9, existing captioning models struggle to follow attribute-specified prompts, exhibiting low accuracy even with one or two attributes. In contrast, ASID-Captioner consistently achieves higher instruction following accuracy. Although trained only with single-attribute and all-attribute supervision, ASID-Captioner generalizes well to unseen attribute combinations as the number of requested attributes increases."
        },
        {
            "title": "5.5 Qualitative Analysis",
            "content": "Fig. 5 shows representative caption generated by ASIDCaptioner. The caption is explicitly timestamped and integrates multiple complementary aspects of the video, including visual content, camera cues, actions, and grounded speech, into coherent and temporally aligned narrative. More qualitative examples are provided in Sec. F."
        },
        {
            "title": "6 Conclusions",
            "content": "In this work, we present ASID-1M, an open-source collection of composable singleand multi-attribute audiovisual instructions, together with ASID-Verify, scalable curation pipeline that produces reliable attribute-structured supervision. By combining multi-source annotation, post-ensemble consistency verification against the source audiovisual content, and attribute-wise evaluation with targeted refinement, ASID-Verify improves fine-grained coverage while suppressing error accumulation. Trained with progressive three-stage supervised fine-tuning scheme, ASID-Captioner produces higher-quality audiovisual captions with improved attributelevel instruction following. Across captioning benchmarks, instruction following, caption-grounded QA, and temporal 8 grounding evaluations, ASID-Captioner achieves consistent gains over strong open-source models and remains competitive with Gemini-3-Pro."
        },
        {
            "title": "References",
            "content": "[1] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. 7 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6 [3] Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. Whisperx: Time-accurate speech transcription of long-form audio. arXiv preprint arXiv:2303.00747, 2023. 3 [4] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-toend retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. 3 [5] Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. 2, 5, 7, 12 [6] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024. 1, 2, 3 [7] Xinlong Chen, Yue Ding, Weihong Lin, Jingyun Hua, Linli Yao, Yang Shi, Bozhou Li, Yuanxing Zhang, Qiang Liu, Pengfei Wan, et al. Avocado: An audiovisual video caparXiv preprint tioner driven by temporal orchestration. arXiv:2510.10395, 2025. 1, 2, 3, 4, 6, [8] Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Bohan Zeng, Yang Shi, Sihan Yang, Pengfei Wan, Qiang Liu, Liang Wang, and Tieniu Tan. Versavid-r1: versatile video understanding and reasoning model from question answering to captioning tasks. arXiv preprint arXiv:2506.09079, 2025. 1, 2 [9] Xinlong Chen, Yuanxing Zhang, Chongling Rao, Yushuo Guan, Jiaheng Liu, Fuzheng Zhang, Chengru Song, Qiang Liu, Di Zhang, and Tieniu Tan. Vidcapbench: comprehensive benchmark of video captioning for controllable text-to-video generation. arXiv preprint arXiv:2502.12782, 2025. 5, 12 [10] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 7 [11] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 1, 2 [12] Miquel Farré, Andi Marafioti, Lewis Tunstall, LeFinevand Thomas Wolf. https://huggingface.co/datasets/ andro Von Werra, ideo. HuggingFaceFV/finevideo, 2024. 2, 3, [13] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. 5, 13 [14] Yuying Ge, Yixiao Ge, Chen Li, Teng Wang, Junfu Pu, Yizhuo Li, Lu Qiu, Jin Ma, Lisheng Duan, Xinyu Zuo, et al. Archunyuan-video-7b: Structured video comprehension of realworld shorts. arXiv preprint arXiv:2507.20939, 2025. 6, 7 [15] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the In Proceedings world in 3,000 hours of egocentric video. of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. 2, 3 [16] Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Weidi Xie. Worldsense: Evaluating real-world omnimodal understanding for multimodal llms. arXiv preprint arXiv:2502.04326, 2025. 5, 13 [17] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 1 [18] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. 3 [19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [20] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, et al. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024. 7 [21] Shihao Li, Yuanxing Zhang, Jiangtao Wu, Zhide Lei, Yiwen He, Runzhe Wen, Chenxi Liao, Chengkang Jiang, An Ping, Shuo Gao, et al. If-vidcap: Can video caption models follow instructions? arXiv preprint arXiv:2510.18726, 2025. 3 [22] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. 1, 2 [23] Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, and Ming-Ming Cheng. Tempsamp-r1: Effective temporal sampling with reinforcement fine-tuning for video llms. arXiv preprint arXiv:2509.18056, 2025. 1, 2 [24] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In Proceedings of the 2024 conference on empirical methods in natural language processing, pages 59715984, 2024. 2 [25] Ziyang Ma, Ruiyang Xu, Zhenghao Xing, Yunfei Chu, Yuxuan Wang, Jinzheng He, Jin Xu, Pheng-Ann Heng, Kai Yu, Junyang Lin, et al. Omni-captioner: Data pipeline, models, and benchmark for omni detailed perception. arXiv preprint arXiv:2510.12720, 2025. 2 and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. 2 [26] Desen Meng, Rui Huang, Zhilin Dai, Xinhao Li, Yifan Xu, Jun Zhang, Zhenpeng Huang, Meng Zhang, Lingshu Zhang, Yi Liu, et al. Videocap-r1: Enhancing mllms for video captioning via structured thinking. arXiv preprint arXiv:2506.01725, 2025. 1 [27] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26302640, 2019. 3 [28] Salman Khan Muhammad Maaz, Hanoona Rasheed and Fahad Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. ArXiv 2306.05424, 2023. 2, 3 [29] OpenBMB. Minicpm-o 2.6: gpt-4o level mllm for vision, speech, and multimodal live streaming on your phone. https: //github.com/OpenBMB/MiniCPM-V, 2025. 6, 7 [30] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. 3 [31] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. [32] Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, and Chao Zhang. video-salmonn: Speech-enhanced audio-visual large language models. arXiv preprint arXiv:2406.15704, 2024. 1, 2 [33] Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zejun Ma, and Chao Zhang. videosalmonn 2: Captioning-enhanced audio-visual large language models. arXiv preprint arXiv:2506.15220, 2025. 2, 4, 5, 6, 7, 12 [34] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. 2, 7 [35] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 7 [36] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 6 [37] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 2, [38] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative [39] Peiran Wu, Yunze Liu, Zhengdong Zhu, Enmin Zhou, and Junxiao Shen. Ugc-videocaptioner: An omni ugc video detail caption model and new benchmarks. arXiv preprint arXiv:2507.11336, 2025. 2, 5, 6, 7, 12 [40] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, arXiv preprint et al. Qwen2. 5-omni technical report. arXiv:2503.20215, 2025. 4, 5, 6, 7 [41] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025. 6, 7 [42] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. 3 [43] Qize Yang, Shimin Yao, Weixuan Chen, Shenghao Fu, Detao Bai, Jiaxing Zhao, Boyuan Sun, Bowen Yin, Xihan Wei, and Jingren Zhou. Humanomniv2: From understanding to omni-modal reasoning with context. arXiv preprint arXiv:2506.21277, 2025. 6, [44] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 7 [45] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. 7 [46] Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, and Yuan Lin. Tarsier2: Advancing large vision-language models from detailed video description to comprehensive video understanding. arXiv preprint arXiv:2501.07888, 2025. 2 [47] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 1, 2 [48] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 7 [49] Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Jiakui Hu, Yong Xien Chng, Guo-Hua Wang, QingGuo Chen, Zhao Xu, et al. Unified multimodal understanding and generation models: Advances, challenges, and opportunities. arXiv preprint arXiv:2505.02567, 2025. 1 [50] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, April 2024. 7 [51] Yi Zhang, Bolin Ni, Xin-Sheng Chen, Heng-Rui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, Meng-Hao Guo, and Shi-Min Hu. Bee: high-quality corpus and fullstack suite to unlock advanced fully open mllms. arXiv preprint arXiv:2510.13795, 2025. 1 [52] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 2, 3, 12 [53] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. 3 [54] Ziwei Zhou, Rui Wang, and Zuxuan Wu. Daily-omni: Towards audio-visual reasoning with temporal alignment across modalities. arXiv preprint arXiv:2505.17862, 2025. 5,"
        },
        {
            "title": "Limitations",
            "content": "First, ASID-1M is constructed via multi-source automatic annotation, ensembling, and verification, and residual noise may remain despite Stage 3 attribute-wise checking, particularly for ASR-dependent speech content and fine-grained timestamps. Second, parts of our evaluation depend on strong proprietary models as judges or downstream solvers (e.g., GPT-4.1/GPT-4o/Seed-1.6/Gemini), which may introduce evaluator bias and makes absolute scores sensitive to the chosen judge. Finally, ASID-1M focuses on short-form videos, and generalization to longer videos or different domains requires further validation."
        },
        {
            "title": "A Implementation Details",
            "content": "We summarize the stage-wise training configuration of ASIDCaptioner in Tab. 10. All experiments are conducted based on Qwen2.5-Omni and trained via supervised fine-tuning. Across all stages, we use AdamW as the optimizer with learning rate of 2 105 and global batch size of 128. To control computational cost, we enforce fixed pixel budget per sample, with reduced total pixel budget in Stage 3. For optimization efficiency and memory scalability, we adopt DeepSpeed with ZeRO-2 in Stages 1 and 2, and ZeRO-3 offloading in Stage 3. During Stages 1 and 2, training jointly optimizes the projector modules and the language model, while Stage 3 adopts LoRA for parameter-efficient adaptation. All models are trained using 32 NVIDIA A800 GPUs with 80 GB memory each, and inference is conducted on single NVIDIA A800 GPU. Table 10 Training-stage configuration of ASID-Captioner. Stage Purpose Stage 1 Stage Stage 3 Single-Attribute Short-Context Long-Context Modeling Alignment Modeling Video duration Dataset Items Total Pixels Batch size Optimizer Learning rate Warmup Ratio Precision Sequence Length Deepspeed Train Components 0 - 3min 150k 20M 128 AdamW 2e-5 5% Bfloat16 16384 zero2 Proj., LLM 0 - 30s 136k 20M 128 AdamW 2e-5 5% Bfloat16 16384 zero2 Proj., LLM 30s - 3min 53k 12M 128 AdamW 2e-5 5% Bfloat16 22384 zero3_offload LoRA"
        },
        {
            "title": "B Annotation Datasets",
            "content": "LLaVA-Video-178K. LLaVA-Video-178K [52] is largescale open-domain video instruction dataset constructed from YouTube, consisting of untrimmed video clips with durations 12 ranging from 5 seconds to 3 minutes. The videos span diverse content categories, including daily activities, instructional videos, cooking, sports, TV shows, and egocentric scenarios, and are curated to emphasize temporal diversity and dynamic scenes rather than static content. From this dataset, we sample approximately 119K videos from the YouTube subset for annotation. FineVideo. FineVideo [12] is large-scale video dataset consisting of approximately 43K videos with total duration of about 3.4K hours. The dataset is curated through automatic filtering to retain videos with dynamic visual content and dense spoken language, using criteria such as visual motion intensity and word-level speech density. It emphasizes videos with clear action dynamics and temporally rich audiovisual signals, while excluding static scenes and low-information content. These properties make FineVideo particularly suitable for learning fine-grained temporal alignment between visual events and speech. From this dataset, we sample approximately 16K short-form videos with durations under three minutes for annotation."
        },
        {
            "title": "C Evaluation Benchmarks",
            "content": "video-SALMONN-2. The video-SALMONN-2 test [33] contains 483 videos drawn from 14 different domains. Each video lasts between 30 and 60 seconds, with an average duration of 51 seconds. Caption quality is evaluated by an automatic judge that compares generated captions with reference events and categorizes errors into missing, incorrect, and hallucinated events. Incorrect and hallucinated events are treated as hallucination errors, and the final error score is computed by summing the missing rate and the hallucination rate. UGC-VideoCap. UGC-VideoCap [39] consists of 1,000 short-form user-generated videos collected from TikTok. All videos are shorter than 60 seconds and contain at least one meaningful audio segment lasting no less than five seconds. Captions are evaluated by judge model that assigns scores on five-point scale across three dimensions: visual content, audio content, and descriptive detail. The scores are normalized and aggregated to produce an overall caption quality score. VDC. The VDC [5] benchmark contains 1,027 videos and is designed for evaluating fine-grained visual captioning under structured protocol. For each video, models generate captions conditioned on five predefined visual aspects, including camera, short summary, background, main object, and detailed description, using aspect-specific prompts. Performance is measured using both accuracy and judge scores for each aspect. VidCapBench-AE. VidCapBench-AE [9] is an automatically evaluable subset of VidCapBench, designed to assess whether video captions provide sufficient and accurate information for controllable text-to-video generation. The benchmark is built on large collection of short video clips drawn from diverse sources, covering wide range of visual styles, motions, and scene compositions, with most videos being short-form clips suitable for generation-oriented evaluation. VidCapBenchAE reformulates caption evaluation as aspect-specific question answering, where each video is associated with multiple QA pairs targeting four generation-critical aspects: video aesthetics, video content, video motion, and physical laws. For each aspect, caption quality is evaluated using accuracy, precision, coverage, and conciseness, enabling fine-grained assessment of both correctness and completeness with respect to generation-relevant semantics. Daily-Omni. Daily-Omni [54] is an audiovisual question answering benchmark consisting of 684 videos depicting diverse everyday scenarios collected from multiple platforms. The benchmark includes 1,197 multiple-choice question-answer pairs spanning six task categories. In our evaluation setting, generated captions are provided as the sole input to fixed QA model, and performance is measured by question answering accuracy. WorldSense. WorldSense [16] focuses on tightly coupled audiovisual reasoning and contains 1,662 temporally aligned audiovisual clips grouped into eight semantic domains. The dataset includes 3,172 multiple-choice question-answer pairs covering 26 downstream tasks. We evaluate caption quality by measuring how effectively the generated captions support accurate question answering when used as input to judge model. Charades-STA. Charades-STA [13] is temporal grounding benchmark built on the Charades dataset, focusing on localizing natural language queries in indoor videos. Each sample pairs textual query with its corresponding temporal segment within video. We follow the standard data splits, which include approximately 12.4K training samples and 3.7K validation samples. In our caption-based evaluation setting, temporal localization is performed using captions only, and performance is reported using mean IoU and recall at multiple IoU thresholds."
        },
        {
            "title": "D Additional Experiments",
            "content": "Training Stage Ablation. We further analyze the effect of Stage 1 training supervision by comparing models trained with and without the attribute-wise representation learning stage. As shown in Tab. 11, introducing Stage 1 consistently reduces missing content and improves overall caption reliability on video-SALMONN-2. Although the impact on hallucination is limited, the improvement in total error indicates that early attribute-wise supervision helps establish more accurate semantic grounding. Consistent gains are also observed on DVC Detailed, suggesting that Stage 1 benefits fine-grained visual description quality. Table 11 Ablation of Stage 1 training supervision on audiovisual captioning performance. Stage w/o S1 w/ S1 video-SALMONN-2 testset DVC Detailed Miss Hall. Total Acc / Score 27.2 25.5 19.3 18. 46.5 43.4 40.0 / 1.7 40.2 / 1."
        },
        {
            "title": "E Attribute Coverage Analysis",
            "content": "Fig. 6 presents the hierarchical organization of fine-grained semantic categories used in our analysis. The hierarchy consists of three levels, progressing from coarse to fine granularity. The first two levels are manually defined and serve as the annotation schema during dataset construction, specifying highlevel and mid-level semantic dimensions such as scene, action, object, and speech. Based on the completed annotations, we further derive the third, finest-grained level by analyzing the attribute-structured captions. Based on the completed annotations, the third-level categories are automatically derived via semantic grouping. For each second-level category, Seed-1.6 groups the corresponding caption spans into fine-grained categories. GPT-5.2 is then used for post-processing to merge redundant categories and normalize names."
        },
        {
            "title": "F Visualizations",
            "content": "Fig. 7-Fig. 13 visualize captions generated by the ASIDCaptioner across diverse real-world video scenarios, including natural landscapes, urban scenes, narrative content, human interactions, and sports footage. The results demonstrate that the model produces fine-grained, temporally coherent, and well-grounded captions under different instruction settings, highlighting its robustness and generality in realistic settings."
        },
        {
            "title": "G Prompts",
            "content": "We summarize the prompts used throughout the data pipeline and evaluation. For data annotation, we first generate detailed timestamped captions from subtitle-annotated clips  (Fig. 14)  and ensemble multi-source captions into single coherent narrative  (Fig. 15)  . For quality control, we verify and complete captions by identifying important visual and speech deficiencies with structured JSON outputs  (Fig. 16)  , followed by attribute-wise auditing to report only verifiable errors, missing items, and expressiveness issues  (Fig. 17)  , and audit-guided refinement to minimally fix reported issues while preserving the original timestamp structure  (Fig. 18)  . For evaluation, we use caption-grounded multiple-choice QA  (Fig. 19)  and caption-based temporal grounding  (Fig. 20)  to assess factual coverage and temporal localization from textual captions. 13 Figure 6 Attribute taxonomy and dataset-wide coverage statistics. The sunburst visualizes the hierarchical attribute taxonomy used in our dataset, where each leaf node corresponds to fine-grained attribute. 14 Figure 7 Visualization of captions generated by the ASID-Captioner. Figure 8 Visualization of captions generated by the ASID-Captioner. 16 Figure 9 Visualization of captions generated by the ASID-Captioner. 17 Figure 10 Visualization of captions generated by the ASID-Captioner. Figure 11 Visualization of captions generated by the ASID-Captioner. 19 Figure 12 Visualization of captions generated by the ASID-Captioner. 20 Figure 13 Visualization of captions generated by the ASID-Captioner. Prompt template for generating detailed timestamped captions from subtitle-annotated clips. You are video captioning assistant. You will be given video clip with (i) spoken dialogue subtitles tagged with speaker names and time ranges, and (ii) separate timestamp cues. Task: Write ONE comprehensive paragraph that seamlessly integrates Scene, Characters, Objects, Actions, Narrative (with multiple in-sentence timestamps in chronological order), Speech (as direct quotes), Camera, and Emotions. Rules: (1) Use ONLY information supported by the clip/subtitles. (2) Include key moments with explicit timestamps (in seconds) embedded directly in sentences; reflect scene transitions, turning points, and pacing. (3) Do NOT mention subtitles, on-screen text, or any flashing/overlay elements. (4) Do NOT use speaker IDs; indicate speakers naturally with dialogue tags (e.g., she says). (5) No lists, brackets/parentheses, or bullet-like formattingoutput single flowing paragraph with rich detail. Inputs: {subtitle-annotated clip} Output: {one-paragraph timestamped detailed caption} Figure 14 Prompt template for generating detailed timestamped captions from subtitle-annotated clips. 21 Prompt template for ensembling multi-source video captions. You are video narrative integrator. You will be given three textual descriptions of the same video from different sources. Task: Merge them into ONE seamless English paragraph that preserves all specific details, maintains chronological order, and synchronizes visual events with corresponding audio/dialogue. Timestamp rule: Include only key moments using single integer-second anchors in the form At Xs (no ranges, decimals, parentheses, or duplicate timestamps), spaced naturally across the narrative. Constraints: Do not mention source names or generation process; do not invent information; keep Whisper dialogue verbatim; output single paragraph (no lists or brackets). Inputs: Description 1: {seed} Description 2: {avocado} Description 3: {whisper} Output: {one-paragraph merged narrative} Figure 15 Prompt template for ensembling multi-source video captions. Prompt template for caption verification and one-pass completion. You are video caption verification and completion assistant. Inputs: (1) CURRENT_CAPTION, and (2) SOURCES (Whisper ASR + multi-source captions). Task: (i) identify important missing content, separating purely visual deficiencies from speech deficiencies (speech only from Whisper), and (ii) produce one-time completed caption by supplementing only supported missing details. Constraints: no fabrication; ignore trivial details; keep timestamps as integer seconds and follow the timestamp style of CURRENT_CAPTION; output valid JSON with exactly three keys: {non_speech_deficiency_specific, speech_deficiency_specific, caption}. CURRENT_CAPTION: {current_caption} SOURCES: {whisper, seed, avocado} Figure 16 Prompt template for caption verification and one-pass completion. 22 Prompt template for attribute-wise caption auditing. You are caption auditor. You are given the video, optional ASR (truth source for speech meaning only), and caption to audit. Task: Report shortcomings only (errors, missing, expressiveness), and audit ONLY the items explicitly listed in each dimensions VERIFY list; do not evaluate anything outside the VERIFY scope. Evidence rule: output an error only when contradiction is clear and verifiable. Missing is reported only for clearly present, important items under VERIFY. Output: Return strict JSON only following the provided schema (no markdown, no explanations). Use: errors as {\"snippet\": \"...\", \"why\": \"...\"}, missing as {\"what\": \"...\"}, and expressiveness as list of short strings. \"...\" \"why\": Inputs: {video}, {asr (optional)}, {caption} Figure 17 Prompt template for attribute-wise caption auditing. Prompt template for audit-guided caption refinement. You are caption refiner. You are given caption and its audit report. Task: Produce an improved caption by (i) fixing factual errors, (ii) filling important missing items, and (iii) doing minimal language-only polishing for clarity. Constraints: Use the audit report as the only edit specification; do not add new content beyond it. Keep the original timestamp blocks and their order unchanged. Timestamps must follow At Xs, (X is integer or one-decimal), with no ranges. Output: Return strict JSON only with keys improved_caption and enhance_summary (fixed_errors, filled_missing, dropped_forbidden, timestamp_adjustments). Inputs: {caption}, {audit_report} Figure 18 Prompt template for audit-guided caption refinement. Prompt template for multiple-choice question answering based on textual video captions. You are precise QA assistant. Your task is to answer multiple-choice questions based ONLY on the video caption provided. Do not use any outside knowledge or assumptionsyour answer must strictly reflect information from the caption. Always output only the capital letter corresponding to your choice (e.g., A, B, C, D). If the caption does not provide enough information to answer the question, output \"N/A\" instead. Here is the video caption: {video caption} Question: {question} Choices: {choices} Figure 19 Prompt template for multiple-choice question answering based on textual video captions. 23 Prompt template for temporal grounding based on textual video captions. You are temporal grounding assistant. You will be given: (1) long video caption with multiple timestamp anchors like \"At 0s, ... At 5s, ...\", (2) an event description (a sentence). Goal: Infer the most likely continuous time interval (start and end in seconds) when the event happens, using ONLY the caption. Rules: 1) Always try to output time interval, even if the evidence is partial. Use best-effort inference. 2) Prefer intervals aligned to existing anchors. If the event is mentioned near \"At Ts\", choose range that covers that anchor and the most plausible neighboring anchors. 3) If the event is implied by related actions/objects (synonyms/paraphrases), still infer the interval by matching the closest described segment. 4) Output \"N/A\" ONLY if the caption provides absolutely no usable clue to localize the event (no matching action/object/context anywhere). Here is the video caption: {video caption} Question: {question} Choices: {choices} Figure 20 Prompt template for temporal grounding based on textual video captions."
        }
    ],
    "affiliations": [
        "ByteDance Inc.",
        "Tsinghua University",
        "VCIP, School of Computer Science, Nankai University"
    ]
}