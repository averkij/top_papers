{
    "paper_title": "Revisiting Image Fusion for Multi-Illuminant White-Balance Correction",
    "authors": [
        "David Serrano-Lozano",
        "Aditya Arora",
        "Luis Herranz",
        "Konstantinos G. Derpanis",
        "Michael S. Brown",
        "Javier Vazquez-Corral"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "White balance (WB) correction in scenes with multiple illuminants remains a persistent challenge in computer vision. Recent methods explored fusion-based approaches, where a neural network linearly blends multiple sRGB versions of an input image, each processed with predefined WB presets. However, we demonstrate that these methods are suboptimal for common multi-illuminant scenarios. Additionally, existing fusion-based methods rely on sRGB WB datasets lacking dedicated multi-illuminant images, limiting both training and evaluation. To address these challenges, we introduce two key contributions. First, we propose an efficient transformer-based model that effectively captures spatial dependencies across sRGB WB presets, substantially improving upon linear fusion techniques. Second, we introduce a large-scale multi-illuminant dataset comprising over 16,000 sRGB images rendered with five different WB settings, along with WB-corrected images. Our method achieves up to 100\\% improvement over existing techniques on our new multi-illuminant image fusion dataset."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 4 7 7 4 1 . 3 0 5 2 : r Revisiting Image Fusion for Multi-Illuminant White-Balance Correction David Serrano-Lozano1,2 Aditya Arora3,4 Luis Herranz5 Konstantinos G. Derpanis3,4 Michael S. Brown3 Javier Vazquez-Corral1,2 1Computer Vision Center 2Universitat Aut`onoma de Barcelona 3York University 4Vector Institute 5Universidad Autonoma de Madrid {dserrano, javier.vazquez}@cvc.uab.cat luis.herranz@uam.es {adityac8, kosta, mbrown}@yorku.ca (a) Daylight (5500K) (b) Cloudy (6500K) (c) DeepWB [2] (d) MixedWB [6] (e) Ours (f) White-balanced Figure 1. Camera white balance (WB) presets often struggle to correct colors in scenes with multiple illuminants, leading to undesirable color tints in the final sRGB image, as demonstrated in (a) and (b). Recent WB methods, such as DeepWB [2], also fail in mixedillumination scenarios, as shown in (c). MixedWB [6] addressed this issue by estimating pixel-wise weight map to linearly blend multiple WB settings. However, their method can fail to achieve proper color correction in complex scenes, as seen in the white wall of (d). In contrast, we present new efficient transformer-based method to blend five WB settings end-to-end. Our approach successfully handles white balance in scenes with several illuminants, as shown in (e). Finally, we show the white-balanced image in (f). Each images E2000 values are displayed in the bottom-right corner (lower values indicate higher performance)."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction White balance (WB) correction in scenes with multiple illuminants remains persistent challenge in computer vision. Recent methods explored fusion-based approaches, where neural network linearly blends multiple sRGB versions of an input image, each processed with predefined WB presets. However, we demonstrate that these methods are suboptimal for common multi-illuminant scenarios. Additionally, existing fusion-based methods rely on sRGB WB datasets lacking dedicated multi-illuminant images, limiting both training and evaluation. To address these challenges, we introduce two key contributions. First, we propose an efficient transformer-based model that effectively captures spatial dependencies across sRGB WB presets, substantially improving upon linear fusion techniques. Second, we introduce large-scale multi-illuminant dataset comprising over 16,000 sRGB images rendered with five different WB settings, along with WB-corrected images. Our method achieves up to 100% improvement over existing techniques on our new multi-illuminant image fusion dataset. 1 Auto-white-balance (AWB) correction is crucial step applied onboard cameras to remove the color cast caused by the light conditions in the imaged scene. Proper whitebalance correction is needed to produce visually pleasing images and to assist downstream computer vision tasks [4]. Conventional AWB techniques [8, 12, 21, 26, 48] assume that single light source illuminates scene. Typical AWB methods operate on the RAW sensor image to first estimate the color cast in the image in the form of sensor-specific R,G,B vector. Subsequently, the illuminations color cast is removed by scaling each color channel of the RAW image such that the estimated illumination becomes achromatic (i.e., R=G=B). The white-balanced RAW image is then rendered through the cameras image signal processor (ISP) to render the final image in standard RGB (sRGB) space. While the single-illuminant assumption works reasonably well for many scenes, it fails on common scenes with multiple illuminants, such as indoor scenes with illumination from indoor and outdoor lighting (e.g., via window). Prior work examined ways to correct such multiple illuminations [43, 49, 50]. These methods typically follow two-step process that first estimates the constituent scene illuminants, followed by spatially varying WB correction in the RAW sensor image based on the estimates. The need for both estimation and correction makes this challenging problem. Another bottleneck for multi-illuminant learning methods is the difficulty in preparing per-pixel annotated RAW image datasets for training and testing. Afifi et al. [6] introduced MixedWB, multi-illuminant WB correction approach that leverages pre-defined WB values for common scene illuminations (i.e., the camera settings for manual WB mode rather than AWB). The sensors R,G,B response to several common scene illuminations can be accessed directly via camera API calls or image metadata. Given RAW input image, MixedWB first renders it to sRGB using five WB settings tungsten, fluorescent, daylight, cloudy, and shade. neural network then predicts per-pixel blending weights to linearly merge these five images into final sRGB output. This formulation transforms the inherently ill-posed problem of per-pixel illuminant estimation into more straightforward task: learning combination of fixed set of images. MixedWB, however, has two main limitations. First, while linear blending is effective in the RAW linear color space, MixedWB applies it in the non-linear sRGB space. In this paper, we show that this approach is suboptimal because ground-truth correct pixels in sRGB often fall outside the convex hull defined by the preset pixels. To address this limitation, we propose transformer-based [46] fusion strategy that learns more flexible and accurate WB blending function. Second, MixedWB is trained on the RenderedWB dataset [4], which contains images rendered with distinct sRGB WB presets but lacks true multi-illuminant scenes, limiting its effectiveness for real-world WB correction. The only available true multi-illuminant sRGB dataset is generated from 3D-rendered scenes [6], introducing domain shift from real-world images. To address this limitation, we introduce new multi-illuminant dataset containing 16,284 sRGB images, each rendered with five WB presets and accompanied by corresponding ground-truth WB-corrected image. Figure 1 illustrates the benefits of our approach using multi-illuminant scene rendered with the two most suitable WB presets (Daylight and Cloudy). We compare the results of an sRGB WB method (DeepWB [2]), MixedWB [6], our transformer-based approach, and the ground-truth whitebalanced image. The results show that all WB methods struggle to remove the greenish color cast, particularly noticeable on the white wall, except for our transformer-based method, which achieves the most accurate correction. We Contribution. fusion-based multirevisit illumination WB correction and introduce two key contributions. First, we propose transformer-based method that performs WB in multi-illuminant scenes more accurately and efficiently by blending five distinct WB presets. Second, we present new multi-illuminant white balance dataset in the sRGB domain, created by repurposing the RAW multi-illuminant dataset from Kim et al. [29]. This dataset comprises 2,714 scenes, each rendered into five WB sRGB images with corresponding groundtruth white-balanced image. Our method significantly outperforms previous approaches across multiple datasets, including our new multi-illuminant dataset, Cube+ [7], and the Synthetic test set [6]. 2. Related work Our related work is organized into three areas: (i) conventional WB correction, (ii) WB methods for processed sRGB images, and (iii) multi-illuminant WB correction. Conventional white-balance correction. Traditional white balance correction techniques aim to estimate the sensors response to illumination from single RAW image. Early statistical methods [1214, 16, 17, 22, 33, 40, 45] computed simple RAW image statistics as features for illumination estimation. These early approaches were later extended to employ more complex statistical inferences, such as color-by-correlation [19], Bayesian color constancy [11, 20], and voting (e.g., [41, 47, 48]). Deep learning methods now comprise the state of the art for illumination estimation (e.g., [3, 10, 24, 3739, 42, 51, 55]). Since illumination estimation is performed on the sensor-specific RAW image format, these methods require large, annotated datasets of images for each targeted sensor. One limitation of conventional methods is their reliance on the assumption of single dominant light source in the scene. When faced with scenes containing multiple light sources, such as indoor environments with artificial and natural lighting, traditional methods may fail to achieve accurate color reproduction. White-balance methods for rendered sRGB images. The cameras ISP hardware generally applies white balance as an early processing step, followed by multiple non-linear adjustments to produce the final sRGB output. This makes post-capture WB correction challenging. Recent works explore methods for performing WB correction directly on sRGB images [35]. notable advancement in this area is the application of deep neural networks for WB correction and adjustment in camera-rendered sRGB images [2]. SWBNet [36] introduced learning-based architecture to improve the consistency across different color temperatures leveraging contrastive loss, while WBFlow [35] proposed DNN-based method that leverages neural flow. These methods demonstrate improved color accuracy. Still, they often depend on post-capture correction techniques or user interaction, adding complexity and additional steps to the image processing workflow. (a) Tungsten (b) Fluorescent (c) Daylight (d) Cloudy (e) Shade (f) Ground truth (g) sRGB cube Figure 2. Example from the RenderedWB [4] dataset. (a)-(e) show the same scene processed with five distinct WB presets, while (f) presents the white-balanced image obtained using the color checker. Three sample points, marked with teal, yellow, and purple dots for the WB presets and crosses for the ground truth image, are selected across all images. (g) visualizes the pixel values in the sRGB space, along with the polytope formed by the WB presets. In here, we also show the results of our method as stars. Note that each axis has different scale to ease the visualization. Multiple-illuminant methods. Various techniques target multiple-illuminant white-balance correction. Previous methods often rely on pixel-wise illumination estimation strategies [9, 18, 23, 25, 27, 28], many of which assume that the number of light sources in the scene is known. This strong assumption limits their applicability in realworld scenarios. More recent approaches [30, 52] leverage deep learning to estimate pixel-wise illuminants in multiilluminant scenes directly from the RAW images. Most closely related to our work are methods that merge multiple sRGB images. MixedWB [6] address the multiilluminant WB problem as constrained fusion problem in the sRGB domain. In particular, they pose the multiilluminant white balance problem by obtaining linear combination of fixed input WB presets. In their approach, deep neural network generates per-pixel weighting map for the different settings. Similarly, StyleWB [32] use the features of VGG-16 [44] architecture for improved performance. In this paper, we demonstrate that simply linearly combining distinct WB presets is insufficient for complex cases, as result both MixedWB and StyleWB struggle to obtain an accurate white-balanced image. The success of learning-based methods depends largely on the availability and quality of training and evaluation data. MixedWB and StyleWB focus on multiilluminant sRGB WB correction but mainly rely on the RenderedWB [4] dataset for training and Cube+ [7] for testing, both of which contain only single-illuminant scenes. The Synthetic test set by Afifi et al. [6] is the only sRGB dataset for multi-illuminant scenes but lacks training samples and has large domain gap from real images. Kim et al. [29] created the most comprehensive multi-illumination dataset by capturing RAW images of the same scene under different light sources. We repurpose this dataset to support post-WB fusion in the sRGB domain, which is our target application. This paper introduces an efficient transformer-based architecture that fuses five WB sRGB presets non-linearly, without requiring explicit per-pixel weight maps. Additionally, we introduce new dataset for training and evaluating multi-illuminant white balance correction in camerarendered sRGB images. Empirically, we demonstrate that our transformer-based fusion method outperforms previous state-of-the-art approaches in WB correction by effectively blending distinct WB settings. 3. Motivation MixedWB [6] and StyleWB [32] hypothesize that multiilluminant white balance can be achieved by linearly combining set of sRGB images rendered with different preset illuminants. This assumption implies that the whitebalanced pixel value should lie within the convex hull formed by the pixel values of the preset images in sRGB space. Formally, for each pixel i, the white balance pixel, Ti is computed by: Ti = wp , (cid:88) (1) where represents the number of WB-presets values, and denote the pixel coordinates and WB preset, respectively, and wi is per-pixel blending factor defining convex combination of the preset values: = 1, wp wp 0, i, p. (2) (cid:88) This assumption holds in the linear RAW space but does not necessarily apply to the non-linear sRGB domain, where both MixedWB and StyleWB operate. Figure 2 illustrates this by plotting three pixels from sample scene in the RenderedWB dataset [4], with WB preset values shown 3 as dots and ground-truth values as crosses. As seen, the ground-truth pixels lie outside the convex hull formed by the presets in sRGB space, highlighting the limitations of linear blending for WB correction in this domain. To overcome this, we propose an efficient and more accurate fusion strategy using an end-to-end transformer block. Our method, represented by stars in Figure 2 (g), produces values outside the convex hull, significantly closer to the ground truth. 4. Transformed-based WB Fusion Our method fuses different WB settings in non-linear end-to-end manner. More specifically, we leverage transformer block, as the attention mechanism inherently captures long-range spatial dependencies, making it well-suited for this task [34, 54]. While vision transformers [15] traditionally divide images into patches treated as tokens, this approach is prohibitive for high-resolution images. To improve efficiency, we adopt the channel-wise attention mechanism [53] in the feature space, enabling the model to effectively aggregate information across different WB settings while maintaining computational feasibility. First, our approach renders the RAW sensor image using five predefined WB settings namely tungsten, fluorescent, daylight, cloudy, and shade. These sensor responses, calibrated at the factory, can be accessed by setting the camera in manual mode and examining its metadata. The images are then rendered to sRGB using their respective WB presets. Notably, the five input images we use are readily available in most modern ISPs. Working in sRGB with set of pre-defined settings offers significant advantages. It transforms an extremely illposed problem obtaining the correct illuminant estimate for each pixel location into more manageable one merging among narrow finite set of images. Additionally, operating in sRGB rather than RAW facilitates generalization across different camera sensors whose distinct characteristics, such as sensitivities, affect RAW image formation. Initially, we concatenate the WB presets to form composite image, RHW 3P , where represents the spatial dimension of the images and denotes the number of WB presets. Given the combined WB presets, I, we first apply 3 3 convolution to extract low-level features and convert the number of channels to C. We first use MultiHead Transposed Attention block [53] in which we generate the query (Q), key (K), and value (V ) projections from the convolutional features. In contrast to spatial attention, the transposed attention map is computed by swapping and as: = Softmax(K Q). (3) After the Multi-Head Transposed Attention module, the output undergoes processing through Feed-Forward Net- (a) Step 1. WB correction on the single illuminant image using the Macbeth color checker and render to sRGB (GT before brightness adjustment). (b) Step 2. AWB on the multiple illuminant image and render to sRGB. (c) Step 3. Adjust the per-pixel brightness for the GT image. Figure 3. Procedure for generating ground truth (GT) sRGB image for scenes with multiple illuminants. We begin with the dataset from Kim et al. [29], focused on unprocessed RAW images. All scenes are initially captured under single illuminant, and additional illuminants are introduced individually. (a) We first compute the WB-corrected image for the single-illuminant scene using the Macbeth color checker. (b) Next, we apply AWB to the (c) Finally, we adjust the per-pixel multiple-illuminant images. brightness of the image obtained in Step 1 to generate the ground truth image. We do so by making the pixel brightness of this image match the pixel brightness from the image obtained in Step 2. work [15], which transforms the features by operating on each location independently yet identically by formulating it as the element-wise product of two parallel paths. Finally, the output features are convolved with 3 3 kernel to reduce the number of channels to three and produce the final sRGB white-balanced corrected image. In our experiments, we use five presets (P =5) and C=15. To summarize, our multi-illuminant WB method leverages distinct WB presets and transformer block by fusing these presets in nonlinear manner, allowing the model to identify regions where each setting most effectively achieves accurate white balance. Unlike MixedWB [6] and StyleWB [32], our approach does not require the inefficient per-pixel weight estimation to form the white-balanced image, handling the process in an end-to-end manner. Notably, our method has only 7.9K trainable parameters, enabling fast inference with minimal memory requirements due to the transposed attention mechanism. 4 Figure 4. Samples from our dataset, showing the same scene under varying lighting conditions and WB presets. Each column presents an image rendered with specific WB setting alongside the ground truth. All images include outdoor lighting, while the first two rows feature two different types of indoor light, and the last row includes all three light sources (outdoor and both indoor types). 5. Multi-illuminant sRGB Dataset MixedWB [6], Style [32] and, our method rely on sRGB WB presets to estimate the white-balance corrected image. However, previous state-of-the-art approaches have primarly been both trained and tested on single-illuminant datasets, such as RenderedWB [4] and Cube+ [7], while evaluation on multi-illuminant scenes has been limited to small synthetic dataset [6] containing only 30 scenes. To bridge this gap, we introduce new dataset designed to support fusion-based multi-illuminant WB methods. Our dataset includes (i) five sRGB-rendered input images, each corresponding to different WB preset, and (ii) the sRGB ground truth image with accurate WB. Recently, Kim et al. [29] introduced the large-scale multi-illuminant (LSMI) dataset. This dataset contains RAW images of scenes with multiple cameras. For each scene, the illumination was carefully controlled. In particular, the scene was captured first under single illumination. Afterward, additional light sources were added to the scene. This step-by-step lighting allowed per-pixel computation in the RAW image of the contribution of each light source. The public LSMI dataset was intended for RAW multi-illumination color constancy algorithms. This dataset serves as the basis for ours. In particular, we repurpose the Sony and Nikon images of the LSMI dataset [29]. The total number of scenes is 955 for Nikon and 1,317 for Sony. Each scene contains either two or three illuminants. Given the DNG images of the LSMI dataset, we compute the five sRGB rendered images using Adobe Camera RAW [1] using the five standard camera WB presets: daylight, cloudy, tungsten, shade, fluorescent. We omit the Samsung Galaxy camera from the LSMI dataset due to incompatibility with Adobe Camera RAW. The images have resolutions of 7360 4912 and 6000 4000 for the Nikon and Sony sets, respectively. Figure 3 illustrates our procedure for generating ground truth white-balanced images in scenes with multiple illuminates. This step is required because the ground truth of the original LSMI dataset does not consider the differences in brightness caused by having multiple illuminant configurations for images from the same scene. To obtain the ground truth, we first apply WB correction using the color chart on the single illuminant image, as shown in step 1 of Figure 3. However, this initial ground truth image often has less brightness than an image of the same scene with additional light sources. To correct the brightness discrepancies, we render the multi-illuminant image to sRGB using standard AWB procedure as shown in step 2 of Figure 3. While AWB does not produce correct WB image under mixed lighting, it provides reference for per-pixel brightness normalization of the single-illuminant ground truth image, as shown in step 3 of Figure 3. This brightness adjustment assumes Lambertian reflectance model, fair approximation for plausible white-balanced images that maintain spatial consistency with multi-illuminant images. Our dataset provides valuable benchmark for training and evaluating fusion-based multi-illuminant WB methods, as it introduces real-world variations absent in the synthetic test set [6]. The final dataset includes 16,284 sRGB images from the Nikon and Sony sets. In Figure 4 we present sample scene from our dataset and the corresponding ground truth images. Each column displays the sRGB image corresponding to specific WB setting, arranged from left to right in order of increasing color temperature and the ground truth. The ground truth images show varying brightness levels to match the different illuminants present in each scene. All images capture the same scene but under different lighting configurations. The first row depicts the scene illuminated by both natural light from window and an indoor light source. In the 5 Table 1. Multi-illuminant evaluation. Quantitative evaluation on our dataset presented in Section 5. We compare our method against WBFlow [35], SWBNet [36], DeepWB [2], MixedWB [6], and StyleWB [32]. All the methods are retrained on our dataset. The dashed line divides the conventional sRGB WB methods and the fusion-based methods. Split Method Mean Median Trimean Mean Median Trimean Mean Median Trimean E2000 MSE MAE Sony Nikon Both WBFlow [35] SWBNet [36] DeepWB [2] StyleWB [32] MixedWB [6] Ours WBFlow [35] SWBNet [36] DeepWB [2] StyleWB [32] MixedWB [6] Ours WBFlow [35] SWBNet [36] DeepWB [2] StyleWB [32] MixedWB [6] Ours 9.64 8.72 7.53 9.46 8.90 4.71 9.07 8.23 6.66 9.94 9.85 3.84 9.29 8.43 6.83 9.67 9.31 4.55 9.04 8.29 7.14 8.80 8.21 4.52 8.57 7.83 6.40 9.57 8.63 3. 8.79 8.01 6.52 8.96 8.45 4.45 9.28 8.42 7.18 9.00 8.27 4.53 8.83 7.97 6.47 9.51 9.16 3.58 9.08 8.19 6.58 9.13 8.65 4.37 233.88 210.88 195.01 230.76 211.88 90.27 206.18 187.61 117.60 247.56 257.65 48. 223.86 204.81 139.32 234.32 219.51 75.60 156.01 142.87 138.39 187.40 153.97 51.38 181.57 164.91 94.14 169.28 153.91 31.59 177.04 160.22 124.87 170.38 148.71 46.88 185.74 167.42 159.08 193.20 165.03 54.90 185.03 169.30 101.70 196.75 176.01 35. 186.37 169.58 129.05 190.59 162.40 49.08 8.42 7.65 6.25 7.89 6.21 3.75 6.75 6.10 4.66 8.02 7.02 3.13 7.97 7.18 5.17 7.92 6.36 3.61 7.57 6.91 6.08 7.27 5.74 3.46 6.25 5.69 4.23 7.26 5.86 2. 7.72 7.03 4.98 7.41 5.60 3.37 7.73 7.02 6.14 7.54 5.82 3.47 6.42 5.78 4.44 7.61 6.16 2.82 7.83 7.09 5.03 7.59 5.72 3.33 second row, the indoor light source is replaced, altering its type, position, and color temperature. The final row shows the scene with all three light sources active simultaneously. 6. Experiments We evaluate the performance of our model using our dataset described in Section 5. The images from the Sony and Nikon cameras are organized into training (65%), validation (15%), and testing (20%) splits, ensuring that all images from the same scene are in the same split. Specifically, the Sony split includes 1,092 training images, 252 validation images, and 337 testing images, while the Nikon split comprises 735 training images, 189 validation images, and 207 testing images. Furthermore, we evaluate the generalization with cross-camera experiment and cross-dataset evaluation on the Synthetic Multi-Illuminant test set [6]. Further, we evaluate the performance of our method on singleilluminant scenes, trained on RenderedWB [4] and evaluated on Cube+ [7]. We evaluate the accuracy of white-balance correction using three well-established metrics: E2000, Mean Squared Error (MSE), and Mean Angular Error (MAE). For all metrics, we compute the mean, median, and trimean values. We highlight best and second-best values for each metric and separate the conventional sRGB methods of the fusionbased method with dashed line. We compare the performance of our architecture against several recent sRGB including WBFlow [35], SWBwhite-balance methods, Net [36], and DeepWB [2], as well as two fusion-based approaches: MixedWB [6] and StyleWB [32]. We follow the official codes and papers to train the comparison methods on our dataset. Specifically, at inference time, we use the Daylight preset as the input image for the non-fusionbased methods, as it is the setting with the best E2000. For MixedWB and StyleWB we use the combination of WB presets with better performance. To fit all the models fairly we resize the Nikon images to 614 920 and the Sony images to 500700. Note that all the color charts were masked during training to avoid learning trivial solutions. 6.1. Implementation details We train our method with the Adam optimizer [31] with cosine learning scheduler, starting with an initial learning rate of 1e3 and gradually decreasing to 1e5. We employ an L2 loss and select the model that achieves the lowest mean E2000 on the validation set. 6.2. Results Multi-illuminant evaluation. Table 1 summarizes mutliilluminant performance on our dataset. The last set of rows combines Sony and Nikon images (Both). Our method substantially outperforms all other approaches across all dataset splits and evaluation metrics. Notably, due to the difficulty of the scenes, standard sRGB WB methods achieve better performance than linear fusion-based methods. Among existing works, DeepWB achieves the best performance. However, our method demonstrates substantial improvement of 2.28 E2000 on the combined Sony and Nikon splits compared to DeepWB. 6 Table 2. Cross-camera generalization. Quantitative evaluation on cross-camera generalization of methods with unseen cameras. We present the results for all the metrics when training the models with one camera and testing it on the other split. E2000 MSE MAE Train/Test Method Mean Median Trimean Mean Median Trimean Mean Median Trimean Nikon/Sony Sony/Nikon WBFlow [35] SWBNet [36] DeepWB [2] StyleWB [32] MixedWB [6] Ours WBFlow [35] SWBNet [36] DeepWB [2] StyleWB [32] MixedWB [6] Ours 10.09 9.58 8.91 9.55 9.17 5.80 11.02 10.54 9.44 10.38 9.96 6. 9.51 8.91 8.34 8.92 8.60 5.55 10.34 9.64 8.84 10.08 9.70 6.08 9.86 9.41 8.59 8.95 8.53 5.50 11.00 10.15 9.13 10.23 9.76 6.16 252.77 238.69 225.82 254.30 246.42 99.43 272.49 266.62 239.47 300.06 289.91 168. 186.54 175.65 162.19 217.78 207.80 67.39 205.75 192.16 175.06 246.03 234.54 116.89 205.31 196.47 185.17 211.05 203.91 74.35 225.02 213.76 197.42 262.13 254.00 116.44 8.76 8.19 7.44 7.19 6.89 4.49 9.81 9.06 7.99 8.09 7.74 4. 7.83 7.41 7.03 6.52 6.27 3.85 8.42 8.30 7.45 7.63 7.34 4.42 8.63 8.11 7.37 6.77 6.45 4.05 9.60 8.70 7.86 7.92 7.54 4.46 Table 3. Cross-dataset generalization. Quantitative evaluation on Synthetic test set [6]. All the methods are retrained on the combined Sony and Nikon splits of our dataset. MSE MAE Method Mean Median Trimean Mean Median Trimean Mean Median Trimean WBFlow [35] SWBNet [36] DeepWB [2] StyleWB [32] MixedWB [6] Ours 11.86 11.54 10.93 9.90 10.43 7.53 10.72 10.44 9.82 9.21 10.35 6.63 10.82 10.52 10.04 9.27 10.48 6.79 728.96 697.31 630.60 411.88 386.39 260. 570.52 545.91 486.32 353.97 357.92 238.47 634.04 555.61 517.09 365.03 363.92 238.31 6.08 5.92 5.53 5.21 4.75 4.16 6.00 5.85 5.55 4.74 4.56 3.17 5.69 5.51 5.19 4.82 4.69 3.31 Cross-camera generalization. Our goal is to develop WB correction method for multi-illuminant scenes that generalizes well to unseen cameras. That said, we evaluate the models trained on Sony images with Nikon images and vice versa. The results of this experiment are shown in Table 2. Our model consistently outperforms all other methods across all metrics. Notably, even when training on different splits, our approach obtains higher performance. Cross-dataset generalization. In Table 3, we assess the generalization of the methods trained on our dataset on the Synthetic Multi-Illuminant test dataset [6]. Despite the domain shift between the real images and the synthetic images in this test set, our method outperforms the previous state of the art, as shown in Table 3. Single illuminant evaluation. Our architecture is designed for multi-illuminant white balance correction. However, we also retrain it using the same approach as MixedWB to demonstrate its superiority over the previous state of the art in image fusion for single illuminant white balance. The results on the Cube+ dataset are presented in Table 4, with models trained on subset of the RenderedWB dataset [4]. In this experiment, we use the publicly available models except StyleWB which is retrained to match the training sets of the other methods. Our method consistently outperforms both standard sRGB WB methods and fusion-based approaches across all mean metrics. Efficiency. Table 4 presents the average runtime for processing 50 images with resolution of 500700 (Nikon images) on an NVIDIA 3090 GPU. Our method has the fewest number of parameters and the fastest inference time due to the use of transposed attention. Specifically, it is nearly five times faster than DeepWB and has an order of magnitude fewer parameters compared to the other methods. Qualitative results. Finally, we present qualitative results in Figure 5, comparing our method to DeepWB, MixedWB, and StyleWB. The first row shows Sony image from our dataset, the next two rows show Nikon images, and the last row presents rendered image from the synthetic test set [6]. From left to right, we display the WB preset with the lowest E2000, DeepWB, MixedWB, StyleWB, our model, and the corresponding ground truth. In the first two examples, our method achieves superior results by effectively removing the color cast caused by different illuminants, visible on the floor and wooden table, respectively. In the third example, MixedWB and StyleWB perform worse than the Fluorescent WB preset, as evident on the whiteboard, illustrating that estimating per-pixel blending map can be challenging in some cases. Lastly, the last row shows that despite the domain shift between the training data and the synthetic images from Afifi et al. [6], our method pro- (a) Best WB setting (b) DeepWB [2] (c) MixedWB [6] (d) StyleWB [32] (e) Ours (f) Ground Truth Figure 5. Quantitative results on our dataset and the synthetic test set [6]. Top to bottom: an image of the Sony split, two Nikon images, and the synthetic dataset. Left to right: the results of the WB preset with lowest E2000 (a), DeepWB [2] (b), MixedWB [6] (c), StyleWB [32] (d), our transformer-based method (e) and the ground truth (f). In the first column (a), we show the name of the WB preset in the bottom-left corner. The E2000 value for each image for each image is shown in the bottom-right corner, lower values indicate higher performance. Table 4. Single illuminant evaluation. Quantitative evaluation on the Cube+ dataset [7]. Results are averaged over all images. Method MSE MAE Param T(ms) WBFlow [35] SWBNet [36] DeepWB [2] StyleWB* [32] MixedWB [6] Ours 4.28 4.28 4.59 4.88 5.03 4. 87.51 74.35 80.46 95.82 168.38 68.33 3.15 3.15 3.45 3.61 4.20 2.98 7.8M 927 2.1M 693 4.4M 459 15.5M 1893 1.3M 980 179 7.9K Table 5. Study on the number of presets. Models are trained and evaluated on our datasets combined Sony and Nikon splits. E2000 MAE Method Mean Median Trimean Mean Median Trimean Daylight DST All presets 5.47 5.18 4.55 5.23 4.94 4.45 5.19 4.96 4.37 4.27 4.06 3. 3.88 3.74 3.37 4.02 3.76 3.33 duces results closest to the ground truth, as observed on the white sofa. 6.3. Ablation Study Standard sRGB WB vs fusion-based. As seen in Tables 1 and 2, in complex cases such as those in our dataset, linear fusion-based methods struggle to produce accurate WB images and are outperformed by regular sRGB WB methods. To further evaluate the effectiveness of our nonlinear fusion strategy, we conduct additional experiments to compare it against regular sRGB WB. Specifically, we modify our transformer block to use only the Daylight preset as input and, separately, to use three input presets (DST) by adjusting the first convolutional layer. Table 5 reports the E2000 and MAE values for these two cases, as well as for our proposed approach using all five presets in the Sony and Nikon splits of our dataset (Both in Table 1). Even when using only the Daylight preset, our model outperforms previous state-of-the-art methods. However, we observe that performance improves as more presets are incorporated. This suggests that the model benefits from additional presets, as they provide more information on how different illuminants interact under varying color temperatures. 7. Conclusion In this work, we introduced transformer-based approach for WB correction in multi-illuminant scenes by efficiently fusing WB presets. Our method outperforms previous linear fusion-based approaches and achieves robust generalization in unseen cameras and datasets without the need to compute per-pixel blending maps. By leveraging transposed attention, our approach demonstrated superior adaptability, and efficiency compared to the state of the art. Additionally, we introduced an sRGB dataset tailored for multi-illuminant WB correction. We anticipate that our dataset will support further advancement in multi-illuminant white-balance research."
        },
        {
            "title": "Acknowledgements",
            "content": "funded DSL, LH, and JVC were supported by Grant PID2021128178OB-I00 by MCIN/AEI/10.13039/ 501100011033 and by ERDF way of making Europe, by the Departament de Recerca Universitats from Generalitat de Catalunya with reference 2021SGR01499, and by the Generalitat de Catalunya CERCA Program. DSL also acknowledges the FPI grant from the Spanish Ministry of Science and Innovation (PRE2022-101525). LH was also supported by the Ramon Cajal grant RYC2019027020-I. This work was also partially supported by the grant C`atedra ENIA UAB-Cruılla (TSI-100929-2023-2) from the Ministry of Economic Affairs and Digital Transition of Spain. KGD and MSB were supported by the CFREF (VISTA) program, an NSERC Discovery Grant. The Canada Research Chair program also supported MSB."
        },
        {
            "title": "References",
            "content": "[1] Adobe Systems. Adobe Camera Raw. Computer software. 5 [2] Mahmoud Afifi and Michael Brown. Deep white-balance editing. In CVPR, 2020. 1, 2, 6, 7, 8 [3] Mahmoud Afifi and Michael Brown. Interactive white balancing for camera-rendered images. In CIC, 2020. 2 [4] Mahmoud Afifi, Brian Price, Scott Cohen, and Michael Brown. When color constancy goes wrong: Correcting improperly white-balanced images. In CVPR, 2019. 1, 2, 3, 5, 6, 7 [5] Mahmoud Afifi, Abhijith Punnappurath, Abdelrahman Abdelhamed, Hakki Can Karaimer, Abdullah Abuolaim, and Michael Brown. Color temperature tuning: Allowing accurate post-capture white-balance editing. In CIC, 2019. [6] Mahmoud Afifi, Marcus Brubaker, and Michael Brown. Auto white-balance correction for mixed-illuminant scenes. In WACV, 2022. 1, 2, 3, 4, 5, 6, 7, 8 [7] Nikola Banic, Karlo Koˇsˇcevic, and Sven Lonˇcaric. UnarXiv preprint supervised learning for color constancy. arXiv:1712.00436, 2017. 2, 3, 5, 6, 8 [8] Jonathan T. Barron and Yun-Ta Tsai. Fast Fourier color constancy. In CVPR, 2017. 1 [9] Shida Beigpour, Christian Riess, Joost Van De Weijer, and Elli Angelopoulou. Multi-illuminant estimation with conditional random fields. TIP, 23(1):8396, 2013. [10] Simone Bianco and Claudio Cusano. Quasi-unsupervised color constancy. In CVPR, 2019. 2 [11] David Brainard and William Freeman. Bayesian color constancy. JOSA A, 1997. 2 [12] George Buchsbaum. spatial processor model for object color perception. J. Frank. Inst., 1980. 1, [13] Ayan Chakrabarti, Keigo Hirakawa, and Todd Zickler. Color constancy beyond bags of pixels. In CVPR, 2008. [14] Dongliang Cheng, Dilip Prasad, and Michael Brown. Illuminant estimation for color constancy: Why spatialdomain methods work and the role of the color distribution. JOSA A, 2014. 2 [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. 4 [16] Graham Finlayson. Corrected-moment illuminant estimation. In ICCV, 2013. [17] Graham Finlayson and Elisabetta Trezzi. Shades of gray and colour constancy. In CIC, 2004. 2 [18] Graham Finlayson, Brian Funt, and Kobus Barnard. Color constancy under varying illumination. In ICCV, 1995. 3 [19] Graham Finlayson, Paul Hubel, and Steven Hordley. Color by correlation. In CIC, 1997. 2 [20] Peter Vincent Gehler, Carsten Rother, Andrew Blake, Tom Minka, and Toby Sharp. Bayesian color constancy revisited. In CVPR, 2008. [21] Arjan Gijsenij, Theo Gevers, and Joost Van De Weijer. Computational color constancy: Survey and experiments. TIP, 20 (9):24752489, 2011. 1 [22] Arjan Gijsenij, Theo Gevers, and Joost Van De Weijer. Improving color constancy by photometric edge weighting. IEEE TPAMI, 34(5):918929, 2011. 2 [23] Arjan Gijsenij, Rui Lu, and Theo Gevers. Color constancy for multiple light sources. TIP, 2012. 3 [24] Daniel Hernandez-Juarez, Sarah Parisot, Benjamin Busam, Ales Leonardis, Gregory Slabaugh, and Steven McDonagh. multi-hypothesis approach to color constancy. In CVPR, 2020. 2 [25] Eugene Hsu, Tom Mertens, Sylvain Paris, Shai Avidan, and Fredo Durand. Light mixture estimation for spatially varying white balance. In ACM SIGGRAPH, 2008. [26] Yuanming Hu, Baoyuan Wang, and Stephen Lin. FC4: Fully convolutional color constancy with confidence-weighted pooling. In CVPR, 2017. 1 [27] Md Akmol Hussain and Akbar Sheikh Akbari. Color constancy algorithm for mixed-illuminant scene images. IEEE Access, 2018. 3 [28] Hamid Reza Vaezi Joze and Mark Drew. Exemplar-based color constancy and multiple illumination. PAMI, 36(5): 860873, 2013. 3 [29] Dongyoung Kim, Jinwoo Kim, Seonghyeon Nam, Dongwoo Lee, Yeonkyung Lee, Nahyup Kang, Hyong-Euk Lee, ByungIn Yoo, Jae-Joon Han, and Seon Joo Kim. Large scale multi-illuminant (LSMI) dataset for developing white balance algorithm under mixed illumination. In ICCV, 2021. 2, 3, 4, 5 [30] Dongyoung Kim, Jinwoo Kim, Junsang Yu, and Seon Joo Kim. Attentive illumination decomposition model for multiilluminant white balancing. In CVPR, 2024. 3 [31] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 6 [32] Furkan Kınlı, Doga Yılmaz, Barıs Ozcan, and Furkan Kırac. Modeling the lighting in scenes as style for auto whitebalance correction. In WACV, 2023. 3, 4, 5, 6, 7, 8 9 [52] Shuwei Yue and Minchen Wei. Robust pixel-wise illuminant estimation algorithm for images with low bit-depth. Optics Express, 32(15):2670826718, 2024. 3 [53] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, 2022. 4 [54] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. In ICML, 2019. [55] Simone Zini, Marco Buzzelli, Simone Bianco, and Raimondo Schettini. Cocoa: combining color constancy algorithms for images and videos. IEEE Transactions on Computational Imaging, 8:795807, 2022. 2 [33] Edwin Land and John McCann. Lightness and retinex theory. JOSA A, 1971. 2 [34] Chongyi Li, Chunle Guo, Shangchen Zhou, Qiming Ai, Ruicheng Feng, and Chen Change Loy. Flexicurve: Flexible piecewise curves estimation for photo retouching. In CVPR, 2023. 4 [35] Chunxiao Li, Xuejing Kang, and Anlong Ming. Wbflow: Few-shot white balance for srgb images via reversible neural flows. In International Joint Conference on Artificial Intelligence, 2023. 2, 6, 7, 8 [36] Chunxiao Li, Xuejing Kang, Zhifeng Zhang, and Anlong Ming. Swbnet: stable white balance network for sRGB images. In AAAI, 2023. 2, 6, 7, [37] Yi-Chen Lo, Chia-Che Chang, Hsuan-Chao Chiu, Yu-Hao Huang, Chia-Ping Chen, Yu-Lin Chang, and Kevin Jou. CLCC: Contrastive learning for color constancy. In CVPR, 2021. 2 [38] Zhongyu Lou, Theo Gevers, Ninghang Hu, and Marcel In BMVC, Lucassen. Color constancy by deep learning. 2015. [39] Seoung Wug Oh and Seon Joo Kim. Approaching the computational color constancy as classification problem through deep learning. PR, 61:405416, 2017. 2 [40] Yanlin Qian, Joni-Kristian Kamarainen, Jarno Nikkanen, and Jiri Matas. On finding gray pixels. In CVPR, 2019. 2 [41] Guillermo Sapiro. Color and illuminant voting. PAMI, 1999. [42] Wu Shi, Chen Change Loy, and Xiaoou Tang. Deep specialized network for illuminant estimation. In ECCV, 2016. 2 [43] Oleksii Sidorov. Conditional GANs for multi-illuminant color constancy: Revolution or yet another approach? In CVPRW, 2019. 1 [44] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 3 [45] Joost Van De Weijer, Theo Gevers, and Arjan Gijsenij. Edgebased color constancy. TIP, 16(9):22072214, 2007. 2 [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2 [47] Javier Vazquez, Alejandro Parraga, Maria Vanrell, and Ramon Baldrich. Color constancy algorithms: Psychophysical evaluation on new dataset. Journal of Imaging Science and Technology, 1(3):1, 2009. 2 [48] Javier Vazquez-Corral, Maria Vanrell, Ramon Baldrich, and Francesc Tous. Color constancy by category correlation. TIP, 21(4):19972007, 2011. 1, 2 [49] Donik Vrˇsnak, Ilija Domislovic, Marko Subaˇsic, and Sven Lonˇcaric. Autoencoder-based training for multi-illuminant color constancy. JOSA A, 2022. [50] Donik Vrˇsnak, Ilija Domislovic, Marko Subaˇsic, and Sven Lonˇcaric. Illuminant segmentation for multi-illuminant scenes using latent illumination encoding. Signal Process. Image Commun., 2022. 1 [51] Bolei Xu, Jingxin Liu, Xianxu Hou, Bozhi Liu, and Guoping Qiu. End-to-end illuminant estimation based on deep metric learning. In CVPR, 2020."
        }
    ],
    "affiliations": [
        "Computer Vision Center",
        "Universidad Autonoma de Madrid",
        "Universitat Aut`onoma de Barcelona",
        "Vector Institute",
        "York University"
    ]
}