{
    "paper_title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection",
    "authors": [
        "Ting Zhang",
        "Chengran Yang",
        "Yindu Su",
        "Martin Weyssow",
        "Hung Nguyen",
        "Tan Bui",
        "Hong Jin Kang",
        "Yikun Li",
        "Eng Lieh Ouh",
        "Lwin Khin Shar",
        "David Lo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices."
        },
        {
            "title": "Start",
            "content": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection Ting Zhang1,*, Chengran Yang1,*, Yindu Su1, Martin Weyssow1, Hung Nguyen1, Tan Bui1, Hong Jin Kang2, Yikun Li1, Eng Lieh Ouh1, Lwin Khin Shar1, David Lo1 1School of Computing and Information Systems, Singapore Management University, Singapore Email: {tingzhang.2019, cryang, yindusu, mweyssow, huuhungn, ngoctanbui, yikunli, elouh, lkshar, davidlo}@smu.edu.sg 2School of Computer Science, University of Sydney, Australia Email: hongjin.kang@sydney.edu.au . *Both authors contributed equally to this research."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous longstanding challenges. However, comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification finetuning for open-source LLMs. Consequently, there is significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. 1 5 2 0 2 3 ] . [ 1 9 4 4 1 0 . 3 0 5 2 : r b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains challenging task for LLMs. This study provides thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices. Keywords: Large language models, Software vulnerabilities, Software security"
        },
        {
            "title": "1 Introduction",
            "content": "Vulnerabilities pose significant threat to software security. Many cybersecurity incidents and data breaches are caused by exploitable software vulnerabilities (SVs) [1, 2]. Consequently, automatic software vulnerability detection (SVD) has become crucial task. In the past decade, numerous static analysis or dynamic analysis tools have been proposed to address this challenge [35]. However, both types of tools have drawbacks: static tools require substantial manual effort from security experts to craft rules, and they have limited generalization ability across diverse vulnerabilities; dynamic tools require configuring execution, which is usually complex, and execution results may be incomplete since not every program path can be executed [6]. Given that deep learning requires no manual operations to find rules and does not need extensive feature engineering, the recent trend in automatic SVD is to apply advancements made in deep learning [2, 7, 8]. For instance, the first deep-learningbased approach for SVD, VulDeePecker [8], adopts bi-directional Long Short-Term Memory [9]. While more recent endeavors have started to utilize pre-trained Transformer models, such as LineVul [10], these pre-trained Transformer models were prevalent before the era of large language models (LLMs) began. LLMs, such as GPT-4 [11] and Claude [12], have revolutionized natural language understanding and generation. In recent years, LLMs have been rapidly evolving and bringing considerable breakthroughs in the field of natural language processing. Moreover, these LLMs have also begun to impact various software engineering tasks, e.g., sentiment analysis in software engineering [13], automatic program repair [14], and duplicate bug report detection [15]. However, the effectiveness of LLMs for SVD remains unclear. Recently, few empirical studies have attempted to evaluate the effectiveness of LLMs for SVD [1620]. We identify the following limitations in the existing empirical studies. Focusing on C/C++ vulnerabilities: Most studies evaluating LLMs on real-world vulnerability data focus primarily on C/C++ vulnerabilities [1820]. However, there is notable gap in research concerning vulnerabilities in other programming languages (PLs). Given the difference in the language design and features, the vulnerabilities in different PLs would also differ. For instance, C/C++ lacks automatic bounds checking, making it susceptible to buffer overflow vulnerabilities. In contrast, other PLs, such as Python, Java, and JavaScript, implement automatic bounds checking for arrays and other data structures, substantially reducing the risk of buffer overflows. Notably, 2 Python, Java, and JavaScript are the top-3 most popular PLs in GitHub open-source projects from 2015 to 2022.1 As two of the most popular and well-developed PLs, Python and Java have broad scope of application scenarios. Unfortunately, recent empirical studies evaluating LLMs have not adequately addressed vulnerabilities in these and other PLs beyond C/C++. While there are several datasets containing Python or Java vulnerabilities in the literature, they are either more coarse-grained or contain few vulnerabilities, e.g., CrossVul [21], which only provide data at the file level, not the function level; Vul4J [22], which contains only 79 vulnerabilities from 25 CWE types. Lack of comprehensive evaluation of open-source LLMs: Existing studies typically conduct only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning on open-source LLMs. However, comprehensive comparison of all three approaches is lacking. Such comparison would provide deeper understanding of how each method impacts the effectiveness of LLMs across different PLs in the SVD task. For instance, Guo et al. [20] only conducted few-shot learning on LLMs while not tuning the parameters of LLMs; similarly, Steenhoek et al. [18] conducted in-context learning with multiple prompt engineering methods on LLMs without tuning any parameters. Ding et al. [16] only conducted sequence classification fine-tuning on open-source LLMs, while they did not conduct few-shot prompting or instruction-tuning on open-source LLMs. Additionally, all the existing studies have not compared LLMs with their smaller counterparts2 and static application security testing (SAST) tools together. On the one hand, prior study on sentiment analysis for software engineering shows that SLMs outperform LLMs when provided with sufficient training data [13]. However, it remains unclear whether this finding extends to SVD. On the other hand, while SAST tools are widely used in practice for identifying vulnerabilities [23], their effectiveness relative to LLMs in this domain has not been thoroughly evaluated. To bridge this gap, in this study, we aim to conduct more comprehensive empirical study to answer the following research questions: RQ1: To what extent are LLMs effective in predicting SVs? RQ2: How does the effectiveness of LLMs in predicting SVs compare to that of SLMs and SAST tools? RQ3: What strategies can be employed to enhance the effectiveness of LLMs in predicting SVs? Our contributions in the context of SVD can be summarized as follows: Comprehensive investigation on LLMs: We evaluate different usages of five open-source LLMs, from freezing parameters (prompt engineering) to tuning parameters (instruction tuning and sequence classification fine-tuning). Comparative study of LLMs with SLMs and SAST tools: We compare LLMs with four SLMs and two SAST tools. 1https://github.blog/news-insights/research/the-state-of-open-source-and-ai/ #the-most-popular-programming-languages 2We refer to the LLMs with less than 1B parameters as SLMs, which stands for small language models. 3 Attempts to improve LLMs: We also investigate whether (1) training LLMs with downsampled data and (2) combining multiple LLMs and SLMs can be helpful in improving the accuracy of the SVD task. Dataset: We provide dataset comprised of vulnerability data from projects written in the top three most popular PLs, i.e., Python, Java, and JavaScript. The structure of this paper is as follows. Section 2 discusses the background and related work. Section 3 presents the overview of our study details of the investigated LLMs. We describe the experimental design in Section 4. Section 5 presents the experimental results. We discuss the implications and threats to validity in Section 6. Finally, Section 7 concludes this paper and discusses future work."
        },
        {
            "title": "2 Background and Related Work",
            "content": "In recent years, vulnerability research has gained significant attention. One research line is proposing deep-learning-based approaches for SVD [10, 2426]; another research line is conducting empirical studies to evaluate the effectiveness of various models and tools [1618, 23, 27, 28]. In this section, we briefly review relevant studies in two research directions: leveraging LMs for SVD and evaluating LMs for SVD. For the purposes of this study, we use the term Language Models (LMs) to encompass both SLMs and LLMs."
        },
        {
            "title": "2.1 Leveraging LMs for SVD",
            "content": "SLMs have been widely used for SVD, as they are relatively small and require fewer computational resources. For instance, Grace [25] is vulnerability detection approach based on LLM, and it leverages the CodeT5 [29] model to extract code semantic features. Vulberta [30] pre-trains RoBERTa model with custom tokenization pipeline that has been leveraged to train vulnerability detection classifiers. Fu et al. [10] proposed LineVul, Transformer-based line-level vulnerability prediction approach that leverages the attention mechanism within the BERT architecture for line-level predictions. Through an empirical evaluation on large-scale real-world dataset with 188k+ C/C++ functions, LineVul achieved (1) 160%-379% higher F1-score for functionlevel predictions and (2) 12%-25% higher Top-10 Accuracy for line-level predictions compared to baselines. Zhang et al. [24] introduced EPVD, which decomposes code snippets into execution paths for SVD. EPVD utilizes the pre-trained CodeBERT model to learn intra-path attentions and encodes each path into feature vector. Evaluated on over 231K functions from three high-quality C/C++ datasets, EPVD significantly outperformed state-of-the-art approaches. The aforementioned works were evaluated on C/C++ datasets, and the largest LM considered was GPT-J [31] with 6B parameters. In contrast, our work utilizes newly curated dataset with Python, Java, and JavaScript and includes comparison of LLMs with up to 8.5B parameters. In addition, our study incorporates the recent popular top-performing open-source LLMs, e.g., CodeQwen1.5 and DeepSeek-Coder."
        },
        {
            "title": "2.2 Evaluating LMs for SVD",
            "content": "Several studies have compared the performance of fine-tuned SLMs for SVD [32, 33]. Thapa et al. [34] conducted an empirical analysis of C/C++ source codes with multiple vulnerabilities related to library function calls, pointer usage, array usage, and arithmetic expressions. Their results demonstrated the good performance of LLMs in vulnerability detection and their superiority over traditional models like bidirectional LSTMs and GRUs in terms of F1-score. Chen et al. [35] proposed DiverseVul, dataset containing 18,945 vulnerable functions spanning 155 CWEs and 330,492 non-vulnerable functions extracted from 7,514 commits. On this dataset, they experimented with 11 deep learning architectures from four model families: Graph Neural Networks, RoBERTa, GPT-2, and T5. Their results showed that LLMs outperformed state-of-the-art graph neural networks for deep learning-based SVD. The two most relevant empirical studies were conducted by Ding et al. [16] and Steenhoek et al. [18]. Ding et al. evaluated 7 code LLMs of varying sizes, including state-of-the-art open-source models like StarCoder 2 [36] and proprietary models from OpenAI [37], on their proposed PrimeVul dataset. They fine-tuned LLMs with fewer than 10B parameters and conducted few-shot prompting and chain-of-thought for larger LLMs. Their results found that code LLMs consistently performed poorly on the PrimeVul benchmark. Steenhoek et al. [18] evaluated 11 state-of-the-art LLMs commonly used as coding assistants for their capabilities in SVD. They systematically searched for the bestperforming prompts, incorporating techniques such as in-context learning and chainof-thought, and proposed three prompting methods. Their results showed that LLMs generally struggled with SVD tasks. In summary, our work distinguishes itself from existing studies in terms of (1) dataset diversity: We evaluate the techniques on real-world vulnerabilities across various PLs; (2) tool selection: We evaluate LLMs, SLMs, and SAST tools; and (3) LLM implementation: We explore multiple strategies for LLM adoption, including prompt engineering (where all parameters are frozen) and instruction tuning and sequence classification fine-tuning (where parameters are tuned)."
        },
        {
            "title": "3.1 Benchmark Creation",
            "content": "Table 1: Data statistics for the time-aware split. PL # VFCs Training Functions Validation Functions # VFCs Testing Functions # VFCs # Vuln. # Non-vuln. # Vuln. # Non-vuln. # Vuln. # Non-vuln. Python Java JavaScript 1,014 871 1,029 4,465 6,354 22,198 71,649 63,654 42, 33 80 78 1,758 616 3,418 6,705 7,209 4,053 247 99 109 2,037 535 3,367 21,108 6,600 15, 5 Fig. 1: Pipeline for benchmark crafting. This section describes the vulnerability dataset we mined for this task. Based on the IEEE Spectrum survey3, the top three PLs in 2023 were Python, Java, and JavaScript. Therefore, we focus on vulnerabilities in these three PLs. Figure 1 illustrates the detailed process of extracting vulnerable functions. We collect vulnerability data from the National Vulnerability Database (NVD), comprehensive repository of publicly disclosed cybersecurity vulnerabilities, up to March 28, 2024. Specifically, NVD maintains vulnerabilities as list of Common Vulnerabilities and Exposures (CVEs). For each CVE, we search for Vulnerability-Fixing Commits (VFCs) by identifying references in the CVE entry that include links containing patches tag. We particularly look for links from GitHub, GitLab, or BitBucket repositories. We consider VFCs from L, where L {P ython, Java, JavaScript}, if they modify at least one file written in the corresponding language. Following prior works [35, 38, 39], which assume VFCs only modify vulnerable functions to address security flaws, we label the pre-commit versions of changed functions as vulnerable, while labeling the post-commit versions and all unchanged functions as non-vulnerable. We apply Tree-sitter [40] to extract changed and unchanged functions from VFCs. Following previous studies [35], we perform function de-duplication to prevent data leakage by using MD5 hashes without applying code normalization. We maintain set of unique MD5s while processing functions, handling vulnerable functions before non-vulnerable ones. If functions MD5 already exists in this set, we exclude it from the dataset. To enhance the realism of our experimental setup, we implement time-aware setting: we use June 1, 2023, as the cutoff date to split training and testing data. VFCs submitted after this date comprise the test set, while those submitted before are used for training. We further sort the commits in the training data by submission date. The most recent 10% of these commits form the validation set, while the earlier 90% 3https://spectrum.ieee.org/top-programming-languages6 Table 2: LMs used in this study. Arch. Methods Model Parameters Decoder ICL Fine-tuning Encoder Fine-tuning Enc-Dec Fine-tuning CodeQwen1.5 DeepSeek-Coder CodeGemma Starcoder-2 CodeLlama CodeBERT GraphCodeBERT UniXcoder CodeT5 CodeT5+ 6.9B 6.6B 8.5B 7.2B 6.6B 125M 125M 126M 223M 223M constitute the training set. Table 1 shows the final data statistics under the time-aware setting."
        },
        {
            "title": "3.2 LMs Selection",
            "content": "In this work, we select 10 open-source models with varied sizes, architectures, and domains of training data to systematically evaluate their effectiveness on the SVD task. Following Big Code Models Leaderboard4 (as of Oct 1, 2024), which evaluates Code LLMs on two common benchmarks, HumanEval [41] and MultiPL-E [42], We select the five best-performing open-source LLMs including CodeQwen1.5 [43], DeepSeekCoder [44], CodeGemma [45], Starcoder-2 [36], and CodeLlama [46], all with versions of approximately 7 billion parameters (7B). Therefore, we use the 7B versions in our experiment to ensure fair comparison. Where applicable, we use the base versions of the LLMs for further classification-based fine-tuning and the chat or instructiontuned versions for in-context learning. Additionally, we consider series of SLMs, such as CodeBERT [47], GraphCodeBERT [48], UniXcoder [49], CodeT5 [29], and CodeT5+ [50], which proven effectiveness in wide range of tasks in the software engineering domain [51, 52]. Table 2 shows additional information on these models, including the architecture, parameters, and how we use them. Notably, we decided not to include commercial LLMs, e.g., GPT-4 and Claude, for the following three reasons: (1) In certain cases, such as enterprise use, source code may be considered sensitive; sharing those data to LLM service providers may raise issues of data privacy; (2) commercial LLMs are not open-source and are updated irregularly, making it difficult to reproduce results from historical versions, which challenges comprehensive and reproducible effectiveness assessments. In contrast, open-source LLMs usually provide complete checkpoints as well as clear version records, enabling reproducibility across different model versions and runs; (3) many commercial models do not support fine-tuning of model parameters (e.g., Claude), which limits their flexibility for evaluation. 4https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard 7 Table 3: Prompt templates for LLMs. Zero-shot Prompt Template ### Instruction: Analyze the input function, determine if it is vulnerable, and return the answer as the corresponding label vulnerable or non-vulnerable. ### Input: {function} ### Response: In-context & RAG Prompt Template (1 shot) ### Instruction: Analyze the input function, determine if it is vulnerable, and return the answer as the corresponding label vulnerable or non-vulnerable. ### Input: {example function} ### Response: {example label} ### Input: {function} ### Response:"
        },
        {
            "title": "4.1 RQ1: Effectiveness of LLMs in SVD",
            "content": "We employ two sets of typical adaptation techniques for LLMs in the SE community: prompt-engineering and fine-tuning [51, 53, 54]. Notably, the main difference between prompt-engineering and fine-tuning is whether the models parameters are updated. Prompt engineering operates during model inference and does not involve parameter updates. In this context, the desired behavior of LLM is achieved through carefully structuring the input prompt and incorporating external information into the prompt [55]. On the other hand, fine-tuning involves updating the models parameters using training dataset, allowing the LLM to better adapt to the SVD task."
        },
        {
            "title": "4.1.1 Prompt-engineering",
            "content": "Specifically, we explore several types of prompting strategies, This approach uses structured prompts to instruct LLMs in identifying whether the provided function code contains vulnerabilities. The prompt and the corresponding function code are fed into the LLM, which generates response containing the classification label. The label is subsequently extracted from the generated response. including zeroshot generation, In-Context Learning (ICL), and Retrieval-Augmented Generation (RAG) [51]. Zero-shot. The zero-shot prompt starts with task instruction guiding LLMs to analyze the input function and determine if it is vulnerable, followed by the input function code and the response section. The prompt template for zero-shot prompting is outlined in Table 3. ICL. Different from zero-shot prompting, in the inference stage of ICL, we include randomly selected examples of function code and corresponding ground-truth 8 labels from training data into the input prompt [55]. Specifically, those examples are concatenated after the original task instruction. Those in-context examples help LLM better understand the task and align its behavior to match the expected output format [55]. RAG. RAG builds upon the ICL paradigm by incorporating retrieval mechanism [56]. It retrieves relevant information of the given input from knowledge base and guides LLMs to create contextually relevant response based on the retrieved data. In our approach, we adapt RAG by retrieving the top example codes most similar to the input function. The similarity between examples is calculated using cosine similarity on code embeddings. We apply SimCSE, widely used approach for generating code embedding [57]. All the examples are extracted from the training dataset and those examples are concatenated in the input prompt the same way as ICL."
        },
        {
            "title": "4.1.2 Fine-tuning LLMs",
            "content": "Within this category, we adapt LLMs to SVD task through supervised fine-tuning. We apply PEFT techniques to fine-tune LLMs with vulnerability data [58]. Specifically, we use quantized low-rank adaptation (QLoRA), which is commonly used and lightweight training technique for LLMs that has proven to be effective for coderelated tasks [5963]. It significantly reduces the number of training parameters of LLMs while maintaining the models performance. In this study, we investigate two distinct fine-tuning strategies for LLMs: (1) instruction tuning, where the training objective is next-token prediction, and (2) sequence classification fine-tuning, where we incorporate classification head on top of the model backbone and fine-tune LLMs with binary classification loss. Instruction tuning. In the instruction tuning process, we provide the model with the task instruction, target function, and ground-truth label. We use training set for fine-tuning. The model is fine-tuned by predicting the tokens of task instruction, target function, and its label. Following standard practice [64], we minimize the cross-entropy loss between predicted and ground-truth tokens during the tuning process. We refer to this loss as causal language modeling (CLM) loss. More formally, for each sample (xi, yi), where xi is the instruction with the target function and yi its ground-truth label, the objective is to minimize the cross-entropy loss: LCLM ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 log (xiyiθ) , (1) where (xiyiθ) is the probability of generating the tokens xiyi parameterized by the models parameters θ. Sequence classification fine-tuning. Inspired by Ding et al. [16], we also finetune LLMs as sequence classifier. We add learnable classification head to the LLMs, which is linear layer followed by softmax function. This layer takes the hidden representation of the last token as input and outputs the classification label. The final hidden state contains contextual information from all the preceding tokens. In the fine-tuning process, the target function is fed into the LLMs, and the model 9 is fine-tuned with binary cross-entropy loss: LCLS = (cid:88) i=1 yi log(ˆyi) , (2) where is the number of classes, yi is the ground-truth label, and ˆyi is the predicted probability of the i-th class. With instruction tuning, the model is prompted to generate answer text during inference, and the label is extracted via text matching. In contrast, sequence classification fine-tuning involves directly inputting the function into the model during inference, where the model classifies the input as either vulnerable or non-vulnerable."
        },
        {
            "title": "4.2 RQ2: Comparison with SLMs and SAST tools",
            "content": "To further understand the effectiveness of LLMs in SVD, we also compare their effectiveness with SLMs and SAST tools. SLMs. For all the SLMs, following their common use in literature [29, 49, 65], we conduct full-parameter fine-tuning, which is also sequence classification fine-tuning. We first choose CodeBERT, which is the first pre-trained Transformer model on source code. Based on the recent empirical study comparing multiple SLMs on different coderelated tasks [65], we further selected CodeT5 [29] and UniXcoder [49]. CodeT5 and UniXcoder demonstrate superior effectiveness on code understanding tasks. UniXcoder is unified cross-modal pre-trained model that leverages multimodal data (i.e., code comment and AST) to pretrain code representation. We are also aware that CodeT5 has more recent version, i.e., CodeT5+ [50], which is enhanced with the flexibility to operate in various modes for different downstream tasks through mixture of pretraining objectives, which are performed on two stages of pre-training on unimodal and bimodal data. Consequently, we also include CodeT5+ with the same parameter count as CodeT5. In total, we have selected four SLMs, i.e., CodeBERT, UniXcoder, CodeT5, and CodeT5+. SAST Tools. Several studies assessing SAST tools have already been conducted [23, 66, 67]. Li et al. [23] reveal that while SAST tools use rule-based static analysis to detect known vulnerabilities, they may miss more complex or novel issues. In contrast, LLMs are well-known for their abilities to generalize from vast amounts of code and natural language training data [68], allowing them to capture context, understand patterns [41], and might detect vulnerabilities that rule-based SAST tools overlook. Given these distinct strengths, it would be interesting to explore and compare the effectiveness of LLMs versus SAST tools in this scenario. From all the tools evaluated by the recent empirical study on comparing Java SAST tools [23], we selected two most popular tools, i.e., Semgrep [69] and SonarQube [70]. These two tools are chosen because they are the most-starred on GitHub among the tools evaluated in the study [23] when writing the paper. Semgrep had accumulated 9.9k stars [69], and the SonarQube community edition (SonarQube) had attracted 8.7k stars [70] and natively supports multiple programming languages. 10 Semgrep OSS primarily performs intraprocedural analysis, meaning it analyzes code within individual functions or methods without tracking behavior across function boundaries [71]. To analyze each VFC in our benchmark, we first identify the files that were modified in that VFC. For each modified file, we retrieve the corresponding file from the parent commit, which contains the vulnerable code. These parent commit files are then input into Semgrep for analysis, which outputs the line numbers of the detected vulnerable code. We then use Tree-sitter [40] to extract the target functions associated with these line numbers. Semgrep detection is considered true positive if the target function matches ground-truth vulnerable function in our benchmark, and false positive if it does not. Also, for each of the reported buggy line numbers, Semgrep also outputs the corresponding vulnerable type, but we decided to ignore this information when determining if Semgrep output is true. SonarQube performs both intraprocedural and interprocedural analysis, allowing it to detect risks within individual methods as well as across functions and modules. Since SonarQube requires entire projects as input, we construct the input projects from the parent commits of each VFC covered in our benchmark. Once the project is successfully built, SonarQube scans for potential vulnerabilities, generating security issues and hotspots that include the line numbers of identified vulnerabilities. We then apply the same evaluation approach used with Semgrep to assess SonarQubes performance."
        },
        {
            "title": "4.3 RQ3: Attempts to improve the effectiveness of LLMs",
            "content": "Reflecting on the results and data statistics, we aim to develop two strategies, i.e., one from the data perspective and the other from the model perspective, to help improve the models effectiveness. From the data perspective, one issue is that the training dataset is highly imbalanced. Thus, it is challenging for the model to fully learn to distinguish between vulnerable and non-vulnerable functions. Therefore, we downsample the non-vulnerable functions to make balanced dataset, i.e., we have an equal number of vulnerable and non-vulnerable samples in each dataset. We keep the test set unchanged. From the model perspective, we aim to implement model ensembling, which has proven effective in various software engineering tasks [72, 73], though its potential benefits remain unexplored for SVD. Specifically, we implement majority voting with predictions from top-performing LLMs and SLMs. We select the five best-performing models based on validation set results. We exhaustively evaluate all possible model combinations on the validation set, retaining the combination that yields the highest F1 score. For each test set data point, the final prediction is determined as follows: if more than half of the models (specifically, at least k/2 + 1 models) classify it as vulnerable, the final label is vulnerable. Otherwise, the label is non-vulnerable."
        },
        {
            "title": "4.4 Implementation",
            "content": "To fine-tune LLMs, we implement parameter-efficient fine-tuning (PEFT) with QLoRA [74] using the HuggingFace library [75]. QLoRA reduces the memory usage of LLM fine-tuning without performance tradeoffs compared to standard 16-bit model 11 Table 4: RQ1 Results: the effectiveness of LLMs. Python Java JavaScript Model Method Precision Recall F1 Precision Recall F1 Precision Recall F1 CodeQwen1. DeepSeek-Coder CodeGemma StarCoder-2 CodeLlama Zero-shot ICL RAG QLoRAinst QLoRAcls Zero-shot ICL RAG QLoRAinst QLoRAcls Zero-shot ICL RAG QLoRAinst QLoRAcls Zero-shot ICL RAG QLoRAinst QLoRAcls Zero-shot ICL RAG QLoRAinst QLoRAcls 0.088 0.120 0.110 0.0 0.250 0.101 0.124 0.109 0.0 0.196 0.089 0.124 0.105 0.0 0. 0.082 0.104 0.120 0.0 0.333 0.083 0.105 0.120 0.0 0.159 0.733 0.582 0.523 0.0 0.005 0.118 0.350 0.315 0.0 0.009 0.912 0.263 0.263 0.0 0.014 0.754 0.033 0.286 0.0 0. 0.619 0.202 0.286 0.0 0.003 0.158 0.200 0.181 0.0 0.011 0.109 0.183 0.162 0.0 0.018 0.162 0.169 0.150 0.0 0.026 0.148 0.050 0.169 0.0 0.001 0.147 0.138 0.169 0.0 0. 0.073 0.134 0.110 0.0 0.284 0.112 0.158 0.136 0.103 0.293 0.081 0.143 0.125 0.0 0.333 0.071 0.118 0.163 0.0 0.293 0.069 0.102 116 0.0 0.296 0.841 0.594 0.484 0.0 0. 0.209 0.391 0.307 0.007 0.032 0.869 0.441 0.406 0.0 0.073 0.858 0.114 0.193 0.0 0.041 0.727 0.383 0.447 0.0 0.060 0.134 0.219 0.18 0.0 0.080 0.146 0.225 0.189 0.014 0. 0.148 0.216 0.192 0.0 0.120 0.131 0.116 0.177 0.0 0.072 0.126 0.161 0.184 0.0 0.100 0.188 0.164 0.191 0.310 0.348 0.127 0.149 0.188 0.209 0.310 0.186 0.183 0.182 0.332 0. 0.171 0.179 0.214 0.336 0.338 0.178 0.181 0.193 0.277 0.270 0.897 0.440 0.564 0.606 0.608 0.029 0.231 0.384 0.708 0.576 0.944 0.535 0.542 0.317 0.514 0.928 0.297 0.403 0.617 0. 0.955 0.592 0.661 0.635 0.590 0.311 0.239 0.285 0.410 0.443 0.047 0.181 0.252 0.323 0.403 0.311 0.273 0.273 0.324 0.413 0.289 0.224 0.280 0.435 0.428 0.299 0.278 0.299 0.386 0. Note: QLoRAcls refers to LLM with sequence classification fine-tuning, QLoRAinst refers to LLM with instruction tuning. fine-tuning. In particular, QLoRA uses 4-bit quantization to compress pre-trained LLM. The LLM parameters are then frozen, and relatively small number of trainable parameters are added to the model as low-rank adapters. During fine-tuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained LLMs into the low-rank adapters. The layers of low-rank adapters are the only parameters being updated during fine-tuning. We apply the recommended hyperparameter settings from Hugging Faces Trainer.5 We use the AdamW optimizer with learning rate of 5e-5. The batch size is set to 128 for LLMs and 32 for SLMs. We set the rank parameter in LoRA to 16 and alpha to 32. The whole set of hyper-parameters and the values we used can be found in our replication package."
        },
        {
            "title": "4.5 Evaluation Metrics",
            "content": "We formalize the SVD task as binary classification problem. The input is function, and the output should be binary label, either vulnerable or non-vulnerable. Following prior works [8, 10, 76], we use the precision, recall, and F1 score as our evaluation metric. As mentioned in Li et al [8], it would be fair to say that SVD methods with high false negative rates may not be useful, i.e., we want high recall. In contrast, those with high false positive rates may not be usable, i.e., we want high precision. Thus, we use the F1 score as the main evaluation metric, considering the balance of precision and recall. 5https://huggingface.co/docs/transformers/main classes/trainer 6https://github.com/soarsmu/SVD-Bench"
        },
        {
            "title": "5.1.1 Effectiveness across LLMs",
            "content": "Table 4 presents the results of applying prompt engineering and fine-tuning techniques to LLMs across three PLs. Firstly, we observe the effectiveness of LLMs varies across PLs. In Python and Java, the results for all LLMs are unsatisfactory, with the best LLM (i.e., CodeQwen1.5 and DeepSeek-Coder) achieving F1 scores of only 0.200 and 0.225, respectively. In contrast, LLMs consistently perform better on the JavaScript dataset, with the instruction-tuned StarCoder-2 achieving the highest F1 score of 0.435. While there is no single LLM that consistently outperforms others across all strategies, CodeQwen1.5, with proper adaptation strategy, achieves the best effectiveness in Python and Java and comparable effectiveness to the best performer in the JavaScript dataset. Based on these findings, we recommend that future research consider CodeQwen1.5 as an initial point for further exploration."
        },
        {
            "title": "5.1.2 Effectiveness of prompt-engineering",
            "content": "Table 4 also demonstrates that prompt engineering generally enhances the effectiveness of LLMs in Python and Java. Specifically, four out of five LLMs using ICL prompting show improved F1 scores in Python, with increases ranging from 4.32% to 67.89%, while three show improvements in Java, ranging from 27.78% to 54.11%. Similarly, RAG results in F1 score improvements for four out of five LLMs compared to zeroshot prompting in Python, and for all LLMs in Java. However, prompt-engineering approaches do not yield improvements for JavaScript. Specifically, the effectiveness of four out of five LLMs decreases by up to 23.15% in terms of F1 score with prompt engineering. We hypothesize that this effectiveness decline may be due to LLMs being trained with greater proportion of JavaScript data, given that it has been the most popular PL on GitHub over the past decade.7 As result, the additional knowledge injected through prompt-engineering may be less effective and could introduce noise, thereby reducing effectiveness."
        },
        {
            "title": "5.1.3 Effectiveness of fine-tuning",
            "content": "We also observe that fine-tuning substantially improves LLMs effectiveness in the JavaScript data but is ineffective in Python and Java. The effectiveness of all LLMs after fine-tuning is lower than that of vanilla LLMs with zero-shot prompting in Python and Java. Specifically, the F1 score for all LLMs, except DeepSeek-Coder, drops to 0 following instruction tuning in Python and Java. Additionally, LLMs with sequence classification fine-tuning exhibit substantial effectiveness declines, with F1 scores decreasing by 83.95% to 99.32% for the Python data and 18.92% to 60.96% for the Java data. In contrast, fine-tuning boosts LLM effectiveness in JavaScript. Compared to zero-shot prompting LLMs, instruction tuning increases the F1 score 7https://github.blog/news-insights/research/the-state-of-open-source-and-ai/ #the-most-popular-programming-languages 13 for JavaScript by 4.18% to 587.23%, and sequence classification fine-tuning by 32.80% to 757.4%, respectively. We hypothesize that these discrepancies across PLs may be attributed to varying degrees of training data imbalance. Specifically, the ratio of vulnerable to non-vulnerable functions is 1:2 in the JavaScript training dataset, compared to 1:16 in Python and 1:10 in Java. We will further investigate and explore the impact of data imbalance on model effectiveness in RQ3."
        },
        {
            "title": "5.1.4 Error analysis of LLMs",
            "content": "Given the unsatisfactory performance of LLMs, which falls significantly short of practical application, conducting detailed error analysis is essential. The deeper examination of LLMs predictions allows for more nuanced understanding of the models strengths and limitations in addressing the SVD task. In particular, we analyze the distribution of Common Weakness Enumeration (CWE) types in LLM predictions. By investigating which CWE types LLMs are most and least effective at identifying, we can infer LLMs strengths and weaknesses in detecting specific vulnerabilities. For analysis, we select the best-performing LLM for each PL. Firstly, we extract the number of CWE types in the test data that LLMs correctly identify, compared to the ground-truth CWE distribution in the test dataset. The results are presented in Table 5. Notably, we observe that the top-performing LLMs for Python and Java, adapted using ICL prompting without fine-tuning, cover 93.75% and 89.19% CWE types in their respective test sets. This indicates that LLMs may possess prior knowledge of vulnerabilities across various CWE types in Python and Java to some extent. As the best-performing LLM for JavaScript, i.e., CodeQwen1.5 with sequence classification fine-tuning, covers only 30.23% of the CWE types in the test set, we further investigate whether this performance gap stems from the fine-tuning process. Specifically, we compare it to the best-performing LLM using prompt-engineering approaches for JavaScript: CodeGemma with zero-shot prompting. Notably, the correct predictions by CodeGemma with zero-shot prompting cover all CWE types in the test set, supporting our hypothesis that LLMs may retain prior knowledge of vulnerabilities across diverse CWE types and that the fine-tuning process may alter this inherent knowledge. We further analyze how fine-tuning alters the inherent knowledge of LLMs. Our findings reveal correlation between the distribution of CWE types in the training data and the models post-fine-tuning performance in identifying corresponding vulnerabilities within the JavaScript dataset. Figure 2 illustrates this relationship by comparing the number of instances correctly identified by CodeQwen1.5 after sequence classification fine-tuning against the ground truth in the test dataset. Remarkably, CodeQwen1.5 demonstrates exceptional effectiveness, achieving over 60% accuracy in identifying vulnerable functions across seven CWE types. Of these seven, six are among the 20 most prevalent CWE types in the training dataset. Conversely, CodeQwen1.5 fails to identify any vulnerable functions for 30 CWE types, 21 of which each represent less than 1% of the vulnerable functions in the training dataset, highlighting strong correlation between data representation and model proficiency. This correlation underscores how the distribution of CWE types in the training data impacts 14 Fig. 2: Number of instances for each CWE category correctly predicted by CodeQwen1.5. CWE categories with no correctly identified test instances by CodeQwen1.5 are omitted. Table 5: CWE distribution for LLMs correct prediction in the test set. Strategy PL # CWEcovered # CWEoverall CodeQwen1.5 DeepSeek-Coder CodeQwen1.5 ICL ICL QLoRAcls Python Java JavaScript 75 33 13 80 37 43 the models effectiveness in identifying vulnerabilities for corresponding CWE types. Addressing this imbalance will be key focus in our future research to enhance the models ability to detect broader range of vulnerabilities consistently. Answer to RQ1: The performance of LLMs varies across PLs. In general, promptengineering approaches enhance LLM performance in Python and Java, while finetuning proves more beneficial for LLMs in JavaScript."
        },
        {
            "title": "5.2 RQ2: Comparison with SLMs and SAST Tools",
            "content": "Figure 3 shows the results from SLMs and SAST tools. We observe effectiveness variations across PLs. Notably, all of the SLMs underperform each LLM on the zero-shot setting in Python. However, SLMs outperform LLMs in both Java and JavaScript. The best-performing SLM in Java, CodeT5+, surpasses the best LLM 15 Fig. 3: RQ2 Results: Comparison of LLMs with SLMs and SAST tools. strategy, i.e., DeepSeek-Coder with ICL prompting, by 20.00% in terms of F1. Similarly, GraphCodeBERT performs best in JavaScript, exceeding the top LLM strategy, i.e., CodeQwen1.5 with sequence classification fine-tuning, by 4.74% in terms of F1. Furthermore, we observe that the effectiveness of SAST tools is consistently lower than LLMs among all PLs, suggesting the great potential for LMs for SVD. Answer to RQ2: SLMs outperform LLMs in Java and JavaScript but underperform compared to LLMs in Python. SAST tools consistently perform worse than LLMs and SLMs across all PLs."
        },
        {
            "title": "5.3.1 Data Perspective",
            "content": "Table 6 presents the results of fine-tuning LLMs with downsampled data. For Python and Java datasets, models trained on balanced data achieved higher F1 scores on the same test data. This improvement trend is evident in both instruction-tuning and sequence classification fine-tuning. However, the JavaScript dataset showed mixed effects when trained on balanced data. In instruction tuning, while three models achieved higher F1 scores, two models produced lower scores. Similarly, in sequence classification fine-tuning, four out of five models show decreased F1 scores when trained on balanced data. The raw JavaScript dataset differs notably from the other two PLs as it is inherently more balanced with 1:2 ratio of vulnerable to non-vulnerable functions. This initial balance may explain why further balancing did not consistently improve effectiveness. Future research could explore how varying ratios of vulnerable to non-vulnerable functions affect model effectiveness. (a) On Python dataset (b) On JavaScript dataset Fig. 4: RQ3: Comparison between fine-tuning with original data and downsampled data. To better understand the differences in results between fine-tuning with original and downsampled data, we created Venn diagrams illustrating correct predictions of vulnerable functions. For Python, we selected the best-performing model trained on the downsampled data: DeepSeek-Coder with classification loss. Similarly, for JavaScript, we chose CodeQwen1.5 with classification loss. Figure 4 demonstrates that for the Python dataset, DeepSeek-Coder correctly predicts significantly more vulnerable functions when trained on downsampled data. However, we found that 5 unique functions could only be correctly identified using the original training data. This suggests that simply reducing the number of non-vulnerable functions may impair the models effectiveness in predicting vulnerable functions. The difference between fine-tuning with original and downsampled data is less pronounced in the JavaScript dataset. There is substantial overlap (87.8%, or 1,915 out of 2,182) in successful predictions between models trained on original and downsampled data. Nevertheless, models trained on different datasets still have unique correct predictions. This indicates that when fine-tuning with more balanced data, the majority of correctly predicted functions remain consistent. 17 Table 6: RQ3 Results: data downsampling. PL Method Model Precision Recall F1 Python Instruction Classification Instruction Java Classification Instruction Classification JavaScript CodeQwen1.5 DeepSeek-Coder CodeGemma Starcoder-2 CodeLlama CodeQwen1.5 DeepSeek-Coder CodeGemma Starcoder-2 CodeLlama CodeQwen1.5 DeepSeek-Coder CodeGemma Starcoder-2 CodeLlama CodeQwen1.5 DeepSeek-Coder CodeGemma Starcoder-2 CodeLlama CodeQwen1.5 DeepSeek-Coder CodeGemma Starcoder-2 CodeLlama CodeQwen1.5 DeepSeek-Coder CodeGemma Starcoder-2 CodeLlama 0.119 0.103 0.124 0.091 0.119 0.117 0.129 0.122 0.117 0. 0.143 0.132 0.160 0.153 0.152 0.158 0.159 0.185 0.184 0.175 0.358 0.215 0.364 0.226 0.202 0.376 0.259 0.269 0.255 0.234 0.576 0.668 0.372 0.791 0.600 0.671 0.541 0.574 0.730 0. 0.559 0.634 0.245 0.514 0.484 0.503 0.591 0.331 0.436 0.572 0.652 0.682 0.427 0.695 0.724 0.609 0.620 0.629 0.679 0.606 0.197 0.178 0.186 0.164 0.198 0.199 0.209 0.201 0.201 0.194 0.228 0.218 0.194 0.235 0.232 0.241 0.251 0.237 0.258 0.268 0.462 0.326 0.393 0.341 0.316 0.465 0.365 0.377 0.371 0.337 Note: indicates F1 score higher than the model trained with original data; indicates F1 score lower than the model trained with original data. When we reduced the number of non-vulnerable functions to match the number of vulnerable functions, the model trained on this balanced dataset correctly predicted 134 vulnerable functions while sacrificing 133 correct predictions. This further supports the observation that simply reducing the number of non-vulnerable functions may compromise the models effectiveness in predicting vulnerable functions."
        },
        {
            "title": "5.3.2 Model Perspective",
            "content": "Table 7 presents the results of our ensemble learning approach, considering three scenarios: the first one involving only LLMs, the second one incorporating both LLMs 18 Table 7: RQ3 Results: ensemble learning. Strategy PL Precision Recall F1 LLMs-only LLMs + SLMs LLMs + SAST tools Python Java JavaScript Python Java JavaScript Python Java JavaScript 0.083 0.186 0.209 0.083 0. 0.209 0.083 0.186 0.209 0.619 0.147 0.338 0.240 0.708 0.323 0.619 0.147 0.338 0.240 0.708 0.323 0.619 0.147 0.338 0.240 0.708 0.323 Note: indicates F1 score higher than the best F1 we achieve without ensemble learning; indicates F1 score lower than the best F1 we achieve without ensemble learning. and SLMs, and the third one incorporating both LLMs and SAST tools. Contrary to expectations, ensemble learning using LLMs alone does not yield improvements over single-model performance. For the Java dataset, ensemble learning achieves an F1score of 0.24, whereas fine-tuning CodeT5+ as single model attains higher F1-score of 0.27. Similarly, on the JavaScript dataset, ensemble learning produces an F1-score of 0.323, while fine-tuning GraphCodeBERT as single model reaches superior F1-score of 0.472. Notably, the results of ensemble learning remain identical when including both LLMs and SLMs or LLMs and SAST tools, indicating that SLMs and SAST tools are not selected in the optimal model combination for the validation set. This seemingly contradictory outcome can be attributed to potential differences in data distribution across training, validation, and testing sets. Our findings suggest that in time-aware split setting, selecting models based solely on validation set performance does not guarantee optimal results on the test set. Future research should address the domain shift phenomenon prevalent in real-world data. Answer to RQ3: Our results show that (1) Fine-tuning LLMs with downsampled balanced data can enhance F1 scores compared to fine-tuning with original data for Python and Java. However, the effect on JavaScript is inconsistent. (2) Simple majority voting proves ineffective in the time-aware split setting."
        },
        {
            "title": "6.1 Implications",
            "content": "Reflecting on the experimental results from Section 5, we identify the following implications for future research. 19 Training data statistics play vital role in SVD. Based on Table 4, we can find that the optimal model and corresponding strategy vary across different PLs. Considering the different characteristics of the datasets, we infer that few-shot learning tends to excel with highly imbalanced training data while fine-tuning yields better performance when more balanced data is available. Therefore, training data statistics must be carefully considered when selecting an appropriate strategy. High-quantity vulnerability data benefits SVD performance. comparison of model performance across Python, Java, and JavaScript datasets reveals significant correlation between data quantity and results. The best-performing model achieves an F1-score of 0.44s with sequence classification fine-tuning on JavaScript, with the largest number of vulnerable functions in the training set. In contrast, Python and Java datasets, containing fewer vulnerable functions, yield F1-scores below 0.2 using fine-tuning. This disparity suggests that increasing the volume of vulnerability data may enhance the capabilities of LLMs in SVD. require Model-specific performance variations tailored approaches. Although the five LLMs are all based on decoder-only architecture, we observe variations in performance across different strategies. For instance, DeepSeek-Coder shows improved results with ICL and RAG compared to zero-shot approaches across all three PL datasets. Conversely, Starcoder-2 performs less when additional random shots were introduced as ICL. These findings underscore the model-specific nature of performance and the varying effectiveness of zero-shot, ICL, and RAG strategies across different tasks. Given this unpredictability, we recommend conducting preliminary experiments on sample of data to determine the most effective approach whether zero-shot or few-shot for each specific model and task combination. This insight highlights the critical importance of tailored strategies in LLM application, emphasizing the need for empirical testing to optimize performance in diverse contexts."
        },
        {
            "title": "6.2 Threats to Validity",
            "content": "Threats to Construct Validity. potential threat to construct validity lies in our choice of evaluation metrics. While we follow established practices in SVD research [8, 10, 76] using precision, recall, and F1-score, these metrics may not always capture the full complexity of real-world scenarios. In different contexts, the relative importance of precision and recall may vary. For example, when false positives significantly impede developer productivity, greater emphasis might be placed on precision. In such cases, alternative metrics like the F0.3-score (where precision is weighted twice as heavily as recall) could be more appropriate. Nevertheless, given the widespread use and acceptance of the F1-score in the field, we believe this threat is minimal. Our choice aligns with standard practices, facilitating comparisons with existing literature while providing balanced assessment of model performance. Threats to Internal Validity. One challenge arises from the prediction methods employed by Semgrep and SonarQube. As these tools do not generate function-level predictions directly, we must input coarser-grained data (files or entire projects). To maintain consistency in our function-level evaluation, we only consider their predictions pertaining to functions within our test set. While this approach may produce 20 additional false alarms outside our test set, we do not count these as false positives. Based on our current knowledge, we believe this strategy offers the most equitable evaluation, thus minimizing this threat to internal validity. Threats to External Validity. One potential external threat is we only work on vulnerabilities recorded in the NVD, and the results may not be able to be generalized to other vulnerabilities from other resources. In the future, we aim to extend our experiments to include vulnerabilities in diverse resources."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "In this work, we conduct comprehensive empirical study evaluating LLMs on the SVD task. We curated dataset comprising vulnerabilities across three different PLs: Python, Java, and JavaScript. To ensure the datasets quality, we leveraged the VFC information recorded in the NVD. To understand how LLMs perform, we first comprehensively implement prompt engineering and fine-tuning (instruction tuning and sequence classification fine-tuning). Second, we compare LLMs with SLMs and SAST tools. Third, we attempt to enhance the effectiveness of LLMs by training the models with downsampled data and conducting ensemble learning. In the context of SVD, our research reveals that fine-tuning is the optimal strategy for adapting LLMs when sufficient and well-balanced training data is available. However, in scenarios with limited data, LLMs utilizing few-shot learning techniques prove more effective. Our experiments to enhance LLM effectiveness demonstrate that training models with downsampled data yield benefits when the original dataset is highly imbalanced. In cases where the data is already well-balanced, the improvement is minimal and may lead to decreased effectiveness. Additionally, we found that simple majority voting does not significantly improve the models F1 score on the SVD task. These insights help better understand LLM performance in SVD and lay the groundwork for improving LLM effectiveness in identifying vulnerabilities. In future work, we are interested in exploring additional methods to improve LLMs for SVD, such as incorporating more advanced prompt engineering techniques. We also plan to investigate more sophisticated ensemble learning methods and LLM collaboration strategies to further aggregate successful predictions from different models."
        },
        {
            "title": "References",
            "content": "[1] Liu, L., De Vel, O., Han, Q.-L., Zhang, J., Xiang, Y.: Detecting and preventing cyber insider threats: survey. IEEE Communications Surveys & Tutorials 20(2), 13971417 (2018) [2] Lin, G., Zhang, J., Luo, W., Pan, L., De Vel, O., Montague, P., Xiang, Y.: Software vulnerability discovery via learning multi-domain knowledge bases. IEEE Transactions on Dependable and Secure Computing 18(5), 24692485 (2019) [3] Cadar, C., Dunbar, D., Engler, D.R., et al.: Klee: unassisted and automatic generation of high-coverage tests for complex systems programs. In: OSDI, vol. 8, pp. 209224 (2008) [4] Flawfinder Home Page. https://dwheeler.com/flawfinder/. (Accessed on 05/27/2024) [5] Application Security Testing Tool Software Security Testing Solutions Checkmarx. https://checkmarx.com/. (Accessed on 05/27/2024) [6] Liu, S., Ma, W., Wang, J., Xie, X., Feng, R., Liu, Y.: Enhancing code vulnerability detection via vulnerability-preserving data augmentation. arXiv preprint arXiv:2404.09599 (2024) [7] Li, Z., Zou, D., Xu, S., Jin, H., Zhu, Y., Chen, Z.: Sysevr: framework for using deep learning to detect software vulnerabilities. IEEE Transactions on Dependable and Secure Computing 19(4), 22442258 (2021) [8] Li, Z., Zou, D., Xu, S., Ou, X., Jin, H., Wang, S., Deng, Z., Zhong, Y.: Vuldeepecker: deep learning-based system for vulnerability detection. In: Proceedings of the 25nd Network and Distributed System Security Symposium (NDSS 2018) (2018). Internet Society [9] Cho, K., Van Merrienboer, B., Bahdanau, D., Bengio, Y.: On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259 (2014) [10] Fu, M., Tantithamthavorn, C.: Linevul: transformer-based line-level vulnerability prediction. In: Proceedings of the 19th International Conference on Mining Software Repositories, pp. 608620 (2022) [11] GPT-4 OpenAI. https://openai.com/index/gpt-4/. (Accessed on 05/27/2024) [12] Introducing the next generation of Claude Anthropic. https://www.anthropic. com/news/claude-3-family. (Accessed on 05/27/2024) [13] Zhang, T., Irsan, I.C., Thung, F., Lo, D.: Revisiting sentiment analysis for software engineering in the era of large language models. ACM Trans. Softw. Eng. Methodol. (2024) https://doi.org/10.1145/3697009 . Just Accepted [14] Xia, C.S., Zhang, L.: Keep the conversation going: Fixing 162 out of 337 bugs for $0.42 each using chatgpt. arXiv preprint arXiv:2304.00385 (2023) [15] Zhang, T., Irsan, I.C., Thung, F., Lo, D.: Cupid: Leveraging chatgpt for more accurate duplicate bug report detection. arXiv preprint arXiv:2308.10022 (2023) [16] Ding, Y., Fu, Y., Ibrahim, O., Sitawarin, C., Chen, X., Alomair, B., Wagner, D., Ray, B., Chen, Y.: Vulnerability detection with code language models: How far are we? arXiv preprint arXiv:2403.18624 (2024) [17] Zhang, J., Wang, C., Li, A., Sun, W., Zhang, C., Ma, W., Liu, Y.: An empirical study of automated vulnerability localization with large language models. arXiv preprint arXiv:2404.00287 (2024) [18] Steenhoek, B., Rahman, M.M., Roy, M.K., Alam, M.S., Barr, E.T., Le, W.: comprehensive study of the capabilities of large language models for vulnerability detection. arXiv preprint arXiv:2403.17218 (2024) [19] Shestov, A., Cheshkov, A., Levichev, R., Mussabayev, R., Zadorozhny, P., Maslov, E., Vadim, C., Bulychev, E.: Finetuning large language models for vulnerability detection. arXiv preprint arXiv:2401.17010 (2024) [20] Gao, Z., Wang, H., Zhou, Y., Zhu, W., Zhang, C.: How far have we gone in vulnerability detection using large language models. arXiv preprint arXiv:2311.12420 (2023) [21] Nikitopoulos, G., Dritsa, K., Louridas, P., Mitropoulos, D.: Crossvul: crosslanguage vulnerability dataset with commit data. In: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 15651569 (2021) [22] Bui, Q.-C., Scandariato, R., Ferreyra, N.E.D.: Vul4j: dataset of reproducible java vulnerabilities geared towards the study of program repair techniques. In: Proceedings of the 19th International Conference on Mining Software Repositories, pp. 464468 (2022) [23] Li, K., Chen, S., Fan, L., Feng, R., Liu, H., Liu, C., Liu, Y., Chen, Y.: Comparison and evaluation on static application security testing (sast) tools for java. In: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 921933 (2023) [24] Zhang, J., Liu, Z., Hu, X., Xia, X., Li, S.: Vulnerability detection by learning from syntax-based execution paths of code. IEEE Transactions on Software Engineering (2023) 23 [25] Lu, G., Ju, X., Chen, X., Pei, W., Cai, Z.: Grace: Empowering llm-based software vulnerability detection with graph structure and in-context learning. Journal of Systems and Software 212, 112031 (2024) [26] Cao, S., Sun, X., Wu, X., Lo, D., Bo, L., Li, B., Liu, W.: Coca: improving and explaining graph neural network-based vulnerability detection systems. In: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pp. 113 (2024) [27] Steenhoek, B., Rahman, M.M., Jiles, R., Le, W.: An empirical study of deep learning models for vulnerability detection. In: 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 22372248 (2023). IEEE [28] Zhou, X., Zhang, T., Lo, D.: Large language model for vulnerability detection: Emerging results and future directions. arXiv preprint arXiv:2401.15468 (2024) [29] Wang, Y., Wang, W., Joty, S., Hoi, S.C.: Codet5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 86968708 (2021) [30] Hanif, H., Maffeis, S.: Vulberta: Simplified source code pre-training for vulnerability detection. In: 2022 International Joint Conference on Neural Networks (IJCNN), pp. 18 (2022). IEEE [31] Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al.: The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 (2020) [32] Yin, X., Ni, C., Wang, S.: Multitask-based evaluation of open-source llm on software vulnerability. IEEE Transactions on Software Engineering (2024) [33] Liu, Y., Gao, L., Yang, M., Xie, Y., Chen, P., Zhang, X., Chen, W.: Vuldetectbench: Evaluating the deep capability of vulnerability detection with large language models. arXiv preprint arXiv:2406.07595 (2024) [34] Thapa, C., Jang, S.I., Ahmed, M.E., Camtepe, S., Pieprzyk, J., Nepal, S.: Transformer-based language models for software vulnerability detection. In: Proceedings of the 38th Annual Computer Security Applications Conference, pp. 481496 (2022) [35] Chen, Y., Ding, Z., Alowain, L., Chen, X., Wagner, D.: Diversevul: new vulnerable source code dataset for deep learning based vulnerability detection. In: Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses, pp. 654668 (2023) [36] Lozhkov, A., Li, R., Allal, L.B., Cassano, F., Lamy-Poirier, J., Tazi, N., Tang, 24 A., Pykhtar, D., Liu, J., Wei, Y., et al.: Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173 (2024) [37] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023) [38] Chakraborty, S., Krishna, R., Ding, Y., Ray, B.: Deep learning based vulnerability detection: Are we there yet? IEEE Transactions on Software Engineering 48(9), 32803296 (2021) [39] Fan, J., Li, Y., Wang, S., Nguyen, T.N.: Ac/c++ code vulnerability dataset with code changes and cve summaries. In: Proceedings of the 17th International Conference on Mining Software Repositories, pp. 508512 (2020) [40] tree-sitter/tree-sitter: An incremental parsing system for programming tools. https://github.com/tree-sitter/tree-sitter. (Accessed on 05/29/2024) [41] Chen, M., Tworek, J., Jun, H., Yuan, Q., Oliveira Pinto, H.P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F.P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W.H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A.N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., Zaremba, W.: Evaluating large language models trained on code (2021) arXiv:2107.03374 [cs.LG] [42] Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps-Costin, L., Pinckney, D., Yee, M.-H., Zi, Y., Anderson, C.J., Feldman, M.Q., Guha, A., Greenberg, M., Jangda, A.: Multipl-e: scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering 49(7), 36753691 (2023) https://doi.org/10.1109/TSE.2023. [43] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., Zhu, T.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023) [44] Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y., Li, Y., et al.: Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196 25 (2024) [45] Team, C.: Codegemma: Open code models based on gemma. arXiv preprint arXiv:2406.11409 (2024) [46] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al.: Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023) [47] Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., Zhou, M.: CodeBERT: pre-trained model for programming and natural languages. In: Cohn, T., He, Y., Liu, Y. (eds.) Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 15361547. Association for Computational Linguistics, Online (2020). https://doi.org/10.18653/v1/2020. findings-emnlp.139 . https://aclanthology.org/2020.findings-emnlp.139 [48] Guo, D., Ren, S., Lu, S., Feng, Z., Tang, D., LIU, S., Zhou, L., Duan, N., Svyatkovskiy, A., Fu, S., Tufano, M., Deng, S.K., Clement, C., Drain, D., Sundaresan, N., Yin, J., Jiang, D., Zhou, M.: Graphcode{bert}: Pre-training code representations with data flow. In: International Conference on Learning Representations (2021). https://openreview.net/forum?id=jLoC4ez43PZ [49] Guo, D., Lu, S., Duan, N., Wang, Y., Zhou, M., Yin, J.: Unixcoder: Unified cross-modal pre-training for code representation. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 72127225 (2022) [50] Wang, Y., Le, H., Gotmare, A., Bui, N., Li, J., Hoi, S.: Codet5+: Open code large language models for code understanding and generation. In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 10691088 (2023) [51] Hou, X., Zhao, Y., Liu, Y., Yang, Z., Wang, K., Li, L., Luo, X., Lo, D., Grundy, J., Wang, H.: Large language models for software engineering: systematic literature review. ACM Transactions on Software Engineering and Methodology (2023) [52] Niu, C., Li, C., Ng, V., Chen, D., Ge, J., Luo, B.: An empirical comparison of pre-trained models of source code. In: Proceedings of the 45th International Conference on Software Engineering. ICSE 23, pp. 2136 2148. IEEE Press, ??? (2023). https://doi.org/10.1109/ICSE48619.2023.00180 . https://doi.org/10.1109/ICSE48619.2023.00180 [53] Wang, J., Huang, Y., Chen, C., Liu, Z., Wang, S., Wang, Q.: Software testing with large language models: Survey, landscape, and vision. IEEE Transactions on Software Engineering (2024) [54] Fan, A., Gokkaya, B., Harman, M., Lyubarskiy, M., Sengupta, S., Yoo, S., Zhang, 26 J.M.: Large language models for software engineering: Survey and open problems. In: 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE), pp. 3153 (2023). IEEE [55] Brown, T.B.: Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020) [56] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W.-t., Rocktaschel, T., et al.: Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33, 94599474 (2020) [57] Gao, T., Yao, X., Chen, D.: Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821 (2021) [58] Ding, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y., Hu, S., Chen, Y., Chan, C.-M., Chen, W., et al.: Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence 5(3), 220235 (2023) [59] Weyssow, M., Zhou, X., Kim, K., Lo, D., Sahraoui, H.: Exploring parameterefficient fine-tuning techniques for code generation with large language models. arXiv preprint arXiv:2308.10462 (2023) [60] Saberi, I., Fard, F., Chen, F.: Utilization of pre-trained language models for adapter-based knowledge transfer in software engineering. Empirical Software Engineering 29(4), 94 (2024) [61] Silva, A., Fang, S., Monperrus, M.: Repairllama: Efficient representations and fine-tuned adapters for program repair. arXiv preprint arXiv:2312.15698 (2023) [62] Esmaeili, A., Saberi, I., Fard, F.H.: Empirical studies of parameter efficient methods for large language models of code and knowledge transfer to r. arXiv preprint arXiv:2405.01553 (2024) [63] Weyssow, M., Kamanda, A., Sahraoui, H.: Codeultrafeedback: An llm-as-a-judge dataset for aligning large language models to coding preferences. arXiv preprint arXiv:2403.09032 (2024) [64] Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., et al.: Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792 (2023) [65] Niu, C., Li, C., Ng, V., Chen, D., Ge, J., Luo, B.: An empirical comparison of pretrained models of source code. In: 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 21362148 (2023). IEEE 27 [66] Kang, H.J., Aw, K.L., Lo, D.: Detecting false alarms from automatic static analysis tools: How far are we? In: Proceedings of the 44th International Conference on Software Engineering, pp. 698709 (2022) [67] Thung, F., Lucia, Lo, D., Jiang, L., Rahman, F., Devanbu, P.T.: To what extent could we detect field defects? an empirical study of false negatives in static bug finding tools. In: Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering, pp. 5059 (2012) [68] Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et al.: Extracting training data from large language models. In: 30th USENIX Security Symposium (USENIX Security 21), pp. 26332650 (2021) [69] Semgrep - Make shift left work. https://semgrep.dev/. (Accessed on 05/28/2024) [70] Code Quality, Security & Static Analysis Tool with SonarQube Sonar. https: //www.sonarsource.com/products/sonarqube/. (Accessed on 05/28/2024) [71] Semgrep Documentation: Perform cross-file analysis. https://semgrep.dev/docs/ semgrep-code/semgrep-pro-engine-intro [72] Xia, X., Lo, D., Shihab, E., Wang, X., Yang, X.: Elblocker: Predicting blocking bugs with ensemble imbalance learning. Information and Software Technology 61, 93106 (2015) [73] Kumar, R., Chaturvedi, A.: Software bug prediction using reward-based weighted majority voting ensemble technique. IEEE Transactions on Reliability (2023) [74] Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L.: Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems 36 (2024) [75] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.L., Gugger, S., Drame, M., Lhoest, Q., Rush, A.M.: Transformers: State-of-the-art natural language processing. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845. Association for Computational Linguistics, Online (2020). https://www.aclweb.org/anthology/2020.emnlp-demos.6 [76] Li, Y., Wang, S., Nguyen, T.N.: Vulnerability detection with fine-grained interpretations. In: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 292303 (2021)"
        }
    ],
    "affiliations": [
        "School of Computer Science, University of Sydney, Australia",
        "School of Computing and Information Systems, Singapore Management University, Singapore"
    ]
}