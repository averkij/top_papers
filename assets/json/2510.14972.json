{
    "paper_title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
    "authors": [
        "Yinxi Li",
        "Yuntian Deng",
        "Pengyu Nie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs."
        },
        {
            "title": "Start",
            "content": "TOKDRIFT: When LLM Speaks in Subwords but Code Speaks in Grammar Yinxi Li, Yuntian Deng, Pengyu Nie University of Waterloo {yinxi.li, yuntian, pynie}@uwaterloo.ca 5 2 0 2 6 1 ] . [ 1 2 7 9 4 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TOKDRIFT, framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have become powerful tools for programming tasks (Chen et al., 2021; Nye et al., 2021; Yang et al., 2024; Guo et al., 2024; Meta FAIR CodeGen Team, 2025). Before any modeling occurs, code is first tokenized into discrete units using pretrained subword tokenizer such as byte-pair encoding (BPE) (Sennrich et al., 2016). However, the tokens that LLMs see, which are based on subword frequencies, are often very different from the tokens defined by programming language (PL) grammar. Whereas PLs have clear syntactic boundaries (e.g., keywords, identifiers, operators), subword tokenizers merge character sequences statistically, sometimes splitting identifiers at arbitrary points or combining unrelated symbols into single token. This misalignment between 1 (a) Workflow of TOKDRIFT, our framework for quantifying LLM sensitivity to semantic-preserving code rewrite rules. (b) Example of tokenization misalignment. Adding space between dot (.) and factorial causes significant change in token sequences, from [.factor, ial] to [., factorial]. Consequently, the LLMs code translation prediction shifts from incorrect (naming the factorial function as comb and later referring to it as combin) to correct. Figure 1: TOKDRIFT workflow and example. subwords and syntax means that LLMs do not always process code in the units that programmers or compilers would expect. As an example, the presence of space before an identifier can lead to completely different token sequences, and thus different predictions, despite identical program semantics (Figure 1). While such differences may appear superficial, they raise deeper concern about how robustly code LLMs represent grammar and meaning. If tokenization determines how code is segmented and embedded, even small discrepancies could propagate through the model and alter its predictions. This motivates the central question of our study: Does the misalignment between subword tokenization and PL grammar limit LLMs ability to understand and generate code? To study this question, we introduce TOKDRIFT, framework that applies semantic-preserving rewrite rules, such as changing whitespace or identifier casing style, to create pairs of programs that are semantically equivalent but tokenized differently. We evaluate nine code LLMs across three representative programming tasksbug fixing, code summarization, and code translation and measure whether model outputs remain functionally equivalent when tokenization changes. Our experiments show that even minor tokenization variations can substantially impact model behavior. For example, the most performant LLM in our experiment, Qwen2.5-Coder-32B-Instruct, changes its prediction 6.09% of the times when the input tokenization changes (and up to 60% under single rewrite rule). Layer-wise analysis further indicates that the effect originates in early layers, where subword segmentation fails to align with grammatical token boundaries. Together, these findings suggest that tokenizer design remains critical yet under-explored factor in developing robust and grammar-aware code LLMs. The main contributions of this work include: We identify and formalize the misaligned tokenization problem in code LLMs. We introduce TOKDRIFT, framework for quantifying model sensitivity to semantic-preserving code rewrites that alter tokenization. We conduct large-scale empirical study showing that misaligned tokenization affects all evaluated models and persists with scaling. We open-source our framework and data to facilitate future research on grammar-aware and domain-adaptive tokenization. Our code and data are available at: https://github.com/uw-swag/tokdrift"
        },
        {
            "title": "2 Background",
            "content": "2.1 LLM Tokenization Tokenization is the first step in processing input for LLMs, converting raw text into sequence of discrete tokens. Each token corresponds to model time step and has dedicated embedding. Modern LLMs use learned tokenization strategies that eliminate the out-of-vocabulary problem by starting from minimal units, such as characters or bytes, and learning how to merge them into longer fragments based on frequency in large corpus. Popular approaches like BPE (Sennrich et al., 2016) and Figure 2: Heatmap of (code) LLMs vocabulary distances (Amba Hombaiah et al., 2021). WordPiece (Schuster and Nakajima, 2012; Devlin et al., 2019) follow this general principle, differing mainly in their merge heuristics. Often, pretokenization steps like splitting at whitespace are applied before learning to prevent tokens from spanning across word boundaries. The tokenizers used by different LLMs can vary significantly due to differences in pre-tokenization rules, token learning algorithms, and pretraining corpora. As shown in Figure 2, even models from the same family often share less than half of their vocabulary, such as Llama 3 vs. Llama 4. The main exception occurs when model developers intentionally reuse the same tokenizer across variants, such as Qwen2.5 and Qwen2.5-Coder, which share an identical vocabulary and tokenizer configuration. 2.2 PL Tokenization Tokenization in PLs, often called lexing, is the first it transforms stream of step of code parsing: characters into sequence of tokens according to PLs grammar. These tokens are then passed to parser, which constructs an abstract syntax tree (AST) to represent the programs structure. While exact rules vary by language, most PLs share common set of token types, including: identifiers (e.g., variable or function names), operators (e.g., +, *), keywords (e.g., if, return), literals (e.g., numeric or string constants), and whitespace, which is typically used to separate tokens but is otherwise ignored. Unlike LLM tokenization, PL tokenization in compilers and interpreters is deterministic. For example, the snippet x+1 is always tokenized into three tokens: an identifier (x), an operator (+), and literal (1). Formatting changes, such as adding spaces, do not affect the token sequence as long as the code remains syntactically valid. 2 Table 1: Benchmarks in our experiments. We manually examine the benchmarks to follow the naming conventions, and to fix/exclude invalid tests and samples, see details in Section C.1. Benchmark Source Task Input PL Output PL # Samples HumanEval-Fix-py HumanEval-Fix-java HumanEval-Explain-py HumanEval-Explain-java Avatar-py2java Avatar-java2py CodeNet-py2java CodeNet-java2py HumanEvalPack (Muennighoff et al., 2023) Avatar (Ahmad et al., 2023; Pan et al., 2024) CodeNet (Puri et al., 2021; Pan et al., 2024) bug fixing code summarization code translation Python Java Python Java Python Java Python Java Python Java Python Java Java Python Java Python 164 164 164 164 244 246 200 This behavior misaligns with LLM tokenizers: while PL tokenizers produce stable, grammaraware units, LLM tokenizers frequently break code structure, resulting in inconsistent or fragmented representations of semantically identical programs. In this work, we refer to grammar-aware tokens as PL tokens, and contrast them with the LLM tokens produced by learned subword tokenizers."
        },
        {
            "title": "3 TOKDRIFT Framework",
            "content": "Figure 1a illustrates the overall workflow of TOKDRIFT, our framework for quantifying model sensitivity to semantic-preserving code rewrites that alter tokenization. In nutshell, TOKDRIFT systematically compares the LLM outputs given the baseline input tokens and variant input tokens (after applying rewrite rules) through large set of experiments. Each experiment is performed on specific benchmark, and tests the sensitivity of given LLM against specific rewrite rule. 3.1 Benchmarks We searched for recent popular coding LLM benchmarks where: (1) the input includes code snippet, since rewrite rules cannot be applied on natural language; (2) the output is evaluated with an automated functional correctness metric.We focused on two popular PLs, Java and Python. Based on these criteria, we selected eight benchmarks covering three tasks, listed in Table 1. Bug fixing (Tufano et al., 2019) transforms buggy code snippet into correct one. Code summarization (Hu et al., 2018; Panthaplackel et al., 2020) aims at summarizing code snippet into natural language description; following HumanEvalPacks setup (Muennighoff et al., 2023), the description is fed back to LLM to generate code for measuring correctness. Code translation (Ahmad et al., 2023; Puri et al., 2021) is the task of translating code snippet from one PL to another. All benchmarks use tests to evaluate the correctness of outputs. Table 2: Models used in our experiments. Series L Llama-3 Qwen2.5-Coder DeepSeek-Coder 70B 8B 3B 1.5B 32B 7B 1.3B 6.7B 33B 3.2 Models Table 2 lists the models used in TOKDRIFT. We selected three series of popular open-source LLMs (using the coding-specific variants if available), namely Llama-3, Qwen2.5-Coder, and DeepSeekCoder. To cover the model size spectrum, we used small (1B parameters), medium (7B), and large (>30B) variants in each series. All models are instruction-tuned. We perform greedy decoding to generate deterministic outputs (see experimental environment details in Section C.4). 3.3 Rewrite Rules Table 3 lists the rewrite rules used in TOKDRIFT. Each rewrite rule converts all occurrences of the left-hand side substring to the right-hand side substring. According to the grammars of the two PLs we experiment on (and generally for most modern PLs), these rewrite rules are semanticallypreserving by design. We apply one rewrite rule at time to investigate their impact in isolation. The six rewrite rules starting with are inspired by naming conventions. Identifiers usually follow one of the four casing styles: camelCase (for variables/functions in Java), PascalCase (for classes in Java/Python), snake_case (for variables/functions in Python), and SCREAMING_CASE (for constants in Java/Python). Since variables/- functions are most common among identifiers, we design rewrite rules to alter their casing style. Specifically, N1, N2, N3 convert camelCase identifiers in Java to the other three casing styles, while N4, N5, N6 convert snake_case identifiers in Python. These rewrite rules challenge LLMs robustness to different naming styles. 3 Table 3: Rewrite rules supported by TOKDRIFT, inspired by naming conventions (starting with N) and spacing conventions (starting with S). Each rewrite rule may apply to Java (marked by J), Python (marked by P), or both. No. PL Rewrite Rule Description Example camelCase snake_case camelCase PascalCase camelCase SCREAMING_CASE snake_case camelCase snake_case PascalCase snake_case SCREAMING_CASE N1 N2 N3 N4 N5 S1 S2 S3 S4 S5 S7 S8 S9 J J J S10 S11 S12 S13 S14 S15 S16 S17 S18 OP -OP - OP [OP [ ) .) . ] )] ) OP ]OP ] OP (OP ( [ ID[ ID ++ )++ ) . *. * ) :) : ) ;) ; OP ;OP ; ) )) ) ( )( ) . ID. ID ( ID( ID OP IDOP ID OP ALLOP ALL Convert identifiers from the most common casing style in the input PL to alternative ones Add space between operator and minus sign Add space between operator and left square bracket Add space between right parentheses and period Add space between right square bracket and right parentheses Add space between operator and right square bracket Add space between operator and left parentheses Add space between left square bracket and identifier Add space between increment operator and right parentheses Add space between period and asterisk Add space between right parentheses and colon Add space between right parentheses and semicolon Add space between operator and semicolon Add space between two right parentheses Add space between two left parentheses Add space between period and identifier Add space between left parentheses and identifier Add space between operator and identifier Add space between operator and identifier/operator sorted st sorted _lst cloestPair Close st Pair possible olutions POSS IBLE _S OLUTION input _clip board input Clipboard string _xor String or triangle _area TRI ANGLE _AREA [::- 1 ] [ :: - 1 ] )) )[ 2 :]n ))) [ 2 :]n '. '). replace '.') . replace : ]):n :] ):n = [[] = [[ ] (( ! is True ( (! is True ([ ow els ([ vowels ++) ++ ) .*;n . * ;n main ():n main () :n <>();n < >() ;n Ac ++; Ac ++ ; .toCharArray ()) .toCharArray () ) alpha () alpha ( ) .factor ial . factorial (String ( String :i +len (sub string : + len ( substring (l : list ):n ( : list ) :n The eighteen rewrite rules starting with are inspired by spacing conventions. Whitespace around most operators usually carries no semantic meaning and is optional. Thus, the spacingrelated rewrite rules identifies two consecutive tokens (one of them is an operator) and inserts space in between. Specifically, we look for combinations where one of them is specific operator or any kind of operator (represented by OP), and the other one is another specific operator or an identifier (represented by ID). Exploring all combinations would be infeasible, thus we select the top-10 frequently appearing combinations in the benchmarks for each PL. In addition, we add S17 and S18 as wildcard rules to cover all cases where an OP is followed by an ID or ID/OP for both PLs. These rewrite rules challenge LLM and its tokenizers robustness to different formatting styles. Notably, in most LLMs with pre-tokenization step of splitting before whitespace, these rewrite rules will lead to more LLM tokens. 3.4 Metrics Recall that each experiment on given {benchmark, model, rewrite rule} triplet compares the baseline outputs (given the original inputs) and the variant outputs (given the inputs after applying rewrite rule). The benchmark provides set of tests to evaluate whether each output is correct or incorrect. We define accuracy as the percentage of correct outputs, and accuracy as the variants accuracy minus the baselines accuracy. The accuracy metric, although intuitive, has two limitations: (1) accuracy improvements and degradations on individual samples cancel out; (2) some samples may not be affected by rewrite rule if the left-hand side substring does not appear in the input; the outputs of those samples will never change. To address these, we introduce an unbiased metric called sensitivity, defined as the percentage of the samples whose output correctness flips (from correct to incorrect or vice versa) out of the samples whose input is changed by the rewrite rule. lower sensitivity indicates that the model is more robust against the token changes introduced by rewrite rule; when averaged across all rewrite rules, it reflects how sensitive the model is to the LLM-PL tokenization misalignment."
        },
        {
            "title": "4 Evaluation",
            "content": "4.1 Results Table 4 shows the accuracy and accuracy of each model on each rewrite rule. We can observe that most rewrite rules cause measurable changes in model accuracy, ranging from -2.90 to +0.32 abso4 Table 4: Accuracy and accuracy (in parenthesis) of each model on each rewrite rule. Variant Llama-3B Llama-8B Llama-70B Qwen-1.5B Qwen-7B Qwen-32B DS-1.3B DS-6.7B DS-33B Average baseline N1 N2 N3 S3 S6 S8 S9 S11 S12 S13 S14 S15 S16 S17 S18 baseline N4 N5 N6 S1 S2 S4 S5 S7 S10 S13 S14 S15 S16 S17 S18 32.04 32.69 (+0.65) 32.17 (+0.13) 32.56 (+0.52) 31.65 (-0.39) 31.52 (-0.52) 31.91 (-0.13) 32.30 (+0.26) 32.69 (+0.65) 30.49 (-1.55) 32.43 (+0.39) 29.84 (-2.20) 30.62 (-1.42) 30.88 (-1.16) 28.68 (-3.36) 25.97 (-6.07) 39.12 40.03 (+0.91) 37.56 (-1.56) 38.08 (-1.04) 39.38 (+0.26) 39.64 (+0.52) 39.77 (+0.65) 38.60 (-0.52) 40.03 (+0.91) 38.47 (-0.65) 37.95 (-1.17) 38.73 (-0.39) 39.12 (+0.00) 40.16 (+1.04) 40.41 (+1.29) 37.44 (-1.68) 43.15 43.54 (+0.39) 43.54 (+0.39) 44.19 (+1.04) 43.02 (-0.13) 43.02 (-0.13) 43.28 (+0.13) 40.96 (-2.19) 44.57 (+1.42) 43.02 (-0.13) 42.64 (-0.51) 41.09 (-2.06) 36.82 (-6.33) 40.83 (-2.32) 37.34 (-5.81) 34.88 (-8.27) 49.87 51.04 (+1.17) 50.91 (+1.04) 50.65 (+0.78) 50.39 (+0.52) 50.65 (+0.78) 50.65 (+0.78) 50.78 (+0.91) 49.35 (-0.52) 50.65 (+0.78) 50.13 (+0.26) 49.22 (-0.65) 50.26 (+0.39) 49.87 (+0.00) 50.39 (+0.52) 49.87 (+0.00) 57.24 57.49 (+0.25) 56.85 (-0.39) 56.20 (-1.04) 56.20 (-1.04) 57.62 (+0.38) 57.24 (+0.00) 58.66 (+1.42) 55.17 (-2.07) 56.07 (-1.17) 56.59 (-0.65) 54.13 (-3.11) 57.24 (+0.00) 55.94 (-1.30) 56.07 (-1.17) 56.85 (-0.39) 69.04 68.91 (-0.13) 68.65 (-0.39) 66.19 (-2.85) 68.65 (-0.39) 68.78 (-0.26) 69.30 (+0.26) 68.91 (-0.13) 68.26 (-0.78) 69.17 (+0.13) 69.30 (+0.26) 68.39 (-0.65) 67.49 (-1.55) 69.04 (+0.00) 67.62 (-1.42) 67.62 (-1.42) Input PL = Java 33.59 35.27 (+1.68) 35.27 (+1.68) 35.53 (+1.94) 34.37 (+0.78) 33.20 (-0.39) 34.11 (+0.52) 33.46 (-0.13) 35.14 (+1.55) 34.75 (+1.16) 33.46 (-0.13) 32.17 (-1.42) 33.46 (-0.13) 34.88 (+1.29) 35.66 (+2.07) 34.11 (+0.52) 57.36 57.62 (+0.26) 57.75 (+0.39) 58.01 (+0.65) 56.72 (-0.64) 57.49 (+0.13) 56.72 (-0.64) 58.14 (+0.78) 56.33 (-1.03) 55.81 (-1.55) 57.36 (+0.00) 56.85 (-0.51) 56.72 (-0.64) 57.36 (+0.00) 55.43 (-1.93) 56.07 (-1.29) 70.41 70.28 (-0.13) 70.41 (+0.00) 69.12 (-1.29) 70.41 (+0.00) 70.28 (-0.13) 71.45 (+1.04) 69.51 (-0.90) 71.58 (+1.17) 67.05 (-3.36) 69.77 (-0.64) 71.19 (+0.78) 70.28 (-0.13) 71.96 (+1.55) 70.03 (-0.38) 70.28 (-0.13) Input PL = Python 40.67 39.77 (-0.90) 39.25 (-1.42) 39.38 (-1.29) 40.54 (-0.13) 40.41 (-0.26) 40.54 (-0.13) 40.80 (+0.13) 40.67 (+0.00) 40.67 (+0.00) 40.54 (-0.13) 39.38 (-1.29) 39.77 (-0.90) 39.64 (-1.03) 39.38 (-1.29) 38.34 (-2.33) 64.51 65.03 (+0.52) 64.77 (+0.26) 64.51 (+0.00) 64.51 (+0.00) 64.77 (+0.26) 64.51 (+0.00) 64.12 (-0.39) 63.34 (-1.17) 63.99 (-0.52) 64.90 (+0.39) 63.73 (-0.78) 62.69 (-1.82) 63.08 (-1.43) 61.92 (-2.59) 63.08 (-1.43) 76.17 77.85 (+1.68) 77.72 (+1.55) 76.81 (+0.64) 76.68 (+0.51) 75.91 (-0.26) 73.19 (-2.98) 76.94 (+0.77) 76.42 (+0.25) 77.46 (+1.29) 76.55 (+0.38) 74.09 (-2.08) 76.30 (+0.13) 76.68 (+0.51) 76.55 (+0.38) 75.13 (-1.04) 38.50 37.98 (-0.52) 39.02 (+0.52) 38.37 (-0.13) 37.34 (-1.16) 37.98 (-0.52) 38.63 (+0.13) 36.95 (-1.55) 37.34 (-1.16) 38.63 (+0.13) 37.47 (-1.03) 37.86 (-0.64) 37.34 (-1.16) 36.43 (-2.07) 35.40 (-3.10) 33.98 (-4.52) 44.82 44.30 (-0.52) 42.88 (-1.94) 42.23 (-2.59) 44.69 (-0.13) 43.65 (-1.17) 44.82 (+0.00) 44.43 (-0.39) 44.30 (-0.52) 44.56 (-0.26) 44.30 (-0.52) 45.08 (+0.26) 44.17 (-0.65) 43.65 (-1.17) 42.62 (-2.20) 42.49 (-2.33) 58.01 57.36 (-0.65) 58.14 (+0.13) 56.33 (-1.68) 58.66 (+0.65) 58.53 (+0.52) 57.49 (-0.52) 56.59 (-1.42) 57.11 (-0.90) 55.94 (-2.07) 58.27 (+0.26) 57.11 (-0.90) 55.43 (-2.58) 57.49 (-0.52) 55.04 (-2.97) 53.10 (-4.91) 61.92 61.53 (-0.39) 61.53 (-0.39) 61.14 (-0.78) 62.56 (+0.64) 62.44 (+0.52) 61.92 (+0.00) 62.69 (+0.77) 62.69 (+0.77) 62.05 (+0.13) 62.05 (+0.13) 61.66 (-0.26) 61.66 (-0.26) 61.27 (-0.65) 60.49 (-1.43) 62.05 (+0.13) 57.36 57.11 (-0.25) 57.36 (+0.00) 56.46 (-0.90) 57.88 (+0.52) 57.49 (+0.13) 58.27 (+0.91) 57.75 (+0.39) 57.11 (-0.25) 58.53 (+1.17) 56.98 (-0.38) 57.62 (+0.26) 59.43 (+2.07) 58.66 (+1.30) 58.91 (+1.55) 56.33 (-1.03) 68.13 68.39 (+0.26) 68.39 (+0.26) 67.62 (-0.51) 67.62 (-0.51) 67.75 (-0.38) 67.36 (-0.77) 66.71 (-1.42) 67.23 (-0.90) 67.10 (-1.03) 67.10 (-1.03) 67.49 (-0.64) 67.23 (-0.90) 67.23 (-0.90) 66.32 (-1.81) 67.36 (-0.77) 49.74 49.93 (+0.19) 50.06 (+0.32) 49.64 (-0.10) 49.58 (-0.16) 49.68 (-0.06) 49.90 (+0.16) 49.37 (-0.37) 49.67 (-0.07) 48.92 (-0.82) 49.44 (-0.30) 48.65 (-1.09) 48.59 (-1.15) 49.38 (-0.36) 48.06 (-1.68) 46.84 (-2.90) 57.14 57.43 (+0.29) 56.85 (-0.29) 56.29 (-0.85) 57.22 (+0.08) 57.11 (-0.03) 56.90 (-0.24) 57.11 (-0.03) 56.92 (-0.22) 57.12 (-0.02) 56.98 (-0.16) 56.42 (-0.72) 56.52 (-0.62) 56.74 (-0.40) 56.19 (-0.95) 55.93 (-1.21) Background color: baseline in grey, variants better than baseline in green, and variants worse than baseline in red. The best variant is highlighted in bold and the worst variant is underlined. lute percentage points if averaging across all models. The largest accuracy of -8.27% happens on Llama-8B for Java benchmarks, whose accuracy drops from 43.15% to 34.88% when applying rewrite rule S18 (adding space after each operator). Considering advances in LLM performance are sometimes claimed with around 1 percentage point margin, these accuracy deltas caused by simple rewrite rules are non-negligible. The impact of misaligned tokenization is more apparent in the sensitivity metric, as shown in the distribution plots in Figure 3. The average sensitivity is 9.26% for naming rewrites and 8.29% for spacing rewrites. Among the naming rewrites (Figure 3a), LLMs are relatively less sensitive to transductions between camelCase and snake_case (N1 and N4), likely because camelCase and SCREAMING_CASE are less frequent. This finding implies that the casing styles of identifiers, while technically convey no semantic meaning in PLs, are an important factor in LLMs understanding of code. In Figure 3b, we can see that LLMs average sensitivity is over 10% for the two wildcard spacing rewrite rules (S17 and S18). Other spacing rewrite rules result in varying levels of sensitivity, among which the most impactful ones are S15 (adding space between period and identifier), S14 (adding space between pair of parentheses), and S12 (adding space between operator and semicolon). In terms of the average sensitivity of models (Figure 3c), we observe that Llama-3 models are more sensitive than the other two series, but all models persist non-negligible sensitivity of at least 5.71% (Qwen-32B on spacing rewrite rules). 4.2 Impact of Model Size We investigate whether larger models are less sensitive to tokenization changes, with the general assumption of larger models being more robust. Table 5 shows the the average sensitivity of models at different sizes, where the small, medium, and large models in each series are compared on row. While the small and medium models are at around the same level of sensitivity, the large models are 5 Table 5: Impact of model size on sensitivity. Rewrite Rule Model Series Naming Spacing Llama-3 Qwen2.5-Coder DeepSeek-Coder Llama-3 Qwen2.5-Coder DeepSeek-Coder 11.48 7.73 9.88 10.22 7.07 8.36 10.68 7.95 8.95 10.99 8.87 8. 9.43 8.27 8.95 8.51 5.71 6.26 Table 6: Impact of identifier fragment changes on sensitivity. Unchanged samples do not have any identifier fragment change, and Changed samples have at least one identifier fragment change. Rewrite Rule Model Unchanged Changed Naming Spacing Llama-70B Qwen-32B DS-33B Llama-70B Qwen-32B DS-33B 8.13 6.58 6.61 7.24 5.09 5.80 11.21 10.57 10. 11.89 7.37 7.12 fore and after applying rewrite rules. Using this concept, we can categorize the samples into two groups, one without any identifier fragment change (i.e., Unchanged), and the other with at least one identifier fragment change (i.e., Changed). Table 6 shows the average sensitivity of models on the two groups of samples; note that we focus on the large model in each series in this analysis. The identifier fragment changed group shows consistently higher sensitivity than the unchanged group, with the largest difference on for naming rewrite rules (10.82% vs. 6.61%). This finding suggests that how identifiers are tokenized into subwords play an important role in LLMs understanding of code. Arguably, identifiers are frequently not tokenized into semantically meaningful subwords (such as the sortedLst example), which may fundamentally limit the models code comprehension and generation capabilities."
        },
        {
            "title": "5 Root Cause Analyses",
            "content": "In addition to quantifying its impact, we also study why LLMs are sensitive to tokenization changes, along two aspects: (1) word frequency in the pretraining corpus (Section 5.1); (2) LLMs hidden states before and after the rewrite rule (Section 5.2). 5.1 Word Frequency Analysis Our hypothesis is that there is correlation between sensitivity and the word frequencies of the rewrite rules left-hand side and right-hand side. If the ratio of right-hand side to left-hand side word fre- (a) grouped by naming rewrite rule (b) grouped by spacing rewrite rule (c) grouped by model Figure 3: Violin plots of sensitivity distributions. usually less sensitive (i.e., more robust) than their smaller counterparts, with only one exception of Qwen-32B on naming rewrite rules. We also perform statistically significant tests via Wilcoxon signed-rank test (Conover, 1999). The results show that the differences are not significant for naming rules, but significant for spacing rules (except between the small and medium models for Qwen2.5-Coder and DeepSeek-Coder series). 4.3 Impact of Identifier Fragment Changes We noticed that identifiers are frequently tokenized into different subwords before and after applying rewrite rules. For example, Llama-3 tokenizes sortedLst into three tokens [sorted, L, st], and applying N1 changes it into two tokens [sorted, _lst]. We define this case as identifier fragment change: the list of fragments (tokens but ignoring spaces and underscores) changes be6 Table 7: Word frequency of rewrite rules left-hand side (LHS) and right-hand side (RHS) on GitHub. Ratio is the percentage of RHS to LHS word frequency. Rewrite Rule S3: ) .) . S8: ++ )++ ) S9: . *. * S11: ) ;) ; S13: ) )) ) S14: ( )( ) S15: . ID. ID S16: ( ID( ID S4: ] )] ) S7: [ ID[ ID S10: ) :) : S13: ) )) ) S14: ( )( ) S15: . ID. ID S16: ( ID( ID RHS Ratio [%] LHS Java 78.9M 45.7K 22.9M 664K 34.2M 7.3M 161M 924K 3.4M 102M 144M 195K 175M 45.9M 6.6M 172M Python 44.6M 61.1M 76M 59M 1.7M 1.1M 1.4M 2.4M 78.1M 71.7K 107M 40.6M 2.9M 105M 0.06 2.90 21.35 0.57 3.33 0.14 16.22 3.84 3.81 1.83 1.84 4.07 0.09 37.94 2.76 quency is small (meaning right-hand side is rare in the corpus), LLMs will likely perform worse after applying the rewrite rule. We measure the word frequencies on GitHub, primary source of code data in LLMs pretraining corpora.1 Table 7 shows the word frequencies of the rewrite rules, and the ratio (in percentages) of the right-hand side to the left-hand side word frequency. The ratio is always less than 100%, which explains why LLMs exhibit non-negligible sensitivity to all rewrite rules. Some rewrite rules with low ratio, e.g., S14, also exhibit high sensitivity in Figure 3b. 5.2 Hidden State Analysis LLMs hidden states represent their internal comprehension and reasoning processes, which may help explain their sensitive to tokenization changes. We compare the hidden states before and after applying the rewrite rules. For each tokens sequence changed, we extract the hidden states of the last token in the sequence, which summarizes the information of the entire sequence. We focus this analysis on the best-performing LLM, Qwen-32B. We first measure the cosine similarity between the hidden states before and after applying the rewrite rules. Figure 4 shows correlation between the layer from which the hidden states are extracted and the similarity. For both naming and spacing rewrite rules, the similarity starts from almost 0 in the first (input) layer, increases (and stabilizes in 1We use GitHubs search feature to measure word frequencies; due to the limitation in regular expressions and characters that can be used in the search string, we can only conduct this analysis on subset of the spacing rewrite rules. (a) naming rewrite rules (b) spacing rewrite rules Figure 4: The similarity of each layers hidden states before and after applying rewrite rules. most cases) in middle layers, and drops again at the last (output) layer. This observation is consistent with the information bottleneck theory (Saxe et al., 2019), which states that the middle layers capture the compressed semantic information. Interestingly, in Figure 4b, we observe that for some spacing rewrite rules (S14 and S3), the similarity in middle layers is also low, implying that the model sees the before and after versions as semantically different. These rewrite rules match the ones that LLMs are most sensitive to in Figure 3b. Then, we compute the hidden state diffs as the hidden states after applying rewrite rules minus those before applying, on the medium layer of the model which should best capture semantic information. Figure 5 shows the visualizations of the hidden state diffs using t-SNE (Maaten and Hinton, 2008). We observe that the diffs of naming and spacing rewrite rules are clearly distinguishable (Figure 5a), so are the diffs of naming (Figure 5b) and spacing rewrite rules (Figure 5c, note that S17 and S18 are excluded since they are supersets of other rewrite rules). This confirms that the hidden states, especially from the middle layers, are good representations of semantic information and may be utilized to mitigate the tokenization changes. 7 (a) naming vs. spacing (b) naming rewrite rules (c) spacing rewrite rules Figure 5: Visualizations of the hidden state diffs using t-SNE (Maaten and Hinton, 2008)."
        },
        {
            "title": "6 Related Work",
            "content": "Tokenization Most modern LLMs use subword tokenizers such as BPE (Sennrich et al., 2016), which create vocabularies based on how often character sequences occur together. The resulting token types do not always correspond to meaningful words or code elements, and can vary depending on how the tokenizer was trained. For example, Liu et al. (2025) shows that allowing token merges across whitespace boundaries produces more meaningful units, compared to tokenizers that always split at spaces. Chirkova and Troshin (2023) introduces tokenizer designed to better align with PL syntax, achieving lower token counts while preserving model performance. These studies show that tokenization can influence how well model understands and generates code, and our work builds on this line of inquiry by quantifying the effects of semantic-preserving tokenization changes. Robustness to Representation Variations Another important question is how robust LLMs are to variations in tokenization and representation at inference time. Zheng et al. (2025) show that instruction-tuned models can often retain high performance even when inputs are tokenized in unconventional or character-level formats, suggesting that such models may learn generalizable internal representations. However, their study also shows measurable performance drop compared to standard tokenizations, and other work highlights further limitations. Wang et al. (2025) find that adversarial changes to token boundaries can significantly degrade model predictions, especially in models that have not undergone instruction tuning. In structured domains like chemistry, Yan et al. (2025) demonstrate that LLMs produce inconsistent outputs across semantically equivalent molecular representations. These findings suggest that LLMs remain sensitive to surface-level variations. Our work contributes to this line by focusing specifically on PLs. Syntax-Aware Code Modeling To address the mismatch between subword tokenization and PL grammar, several approaches incorporate grammar constraints into the LLM decoding process. Synchromesh (Poesia et al., 2022) and PICARD (Scholak et al., 2021) enforce syntactic validity at generation time by using runtime parsing to filter out invalid token continuations. SynCode (Ugare et al., 2024) improves the efficiency of such methods by constructing DFA-based mask that precomputes token legality while explicitly handling partial tokens. Boundless BPE (Schmidt et al., 2025) removes fixed pretokenizers and enables dynamic boundary selection, allowing the model to learn tokens that correspond to syntactic or semantic units. Together, these efforts aim to align LLM outputs more closely with formal code structure, disconnect that our work quantifies by measuring how semantics-preserving tokenization variations affect model behavior."
        },
        {
            "title": "7 Conclusions",
            "content": "This work studies the tokenization misalignment between subword-based LLMs and PL grammar. While subword tokenizers like BPE are widely used in code LLMs, they segment inputs based on frequency statistics, not grammar, leading to token boundaries that may not align with syntactic units in code. Through suite of semantic-preserving rewrite rules, our framework TOKDRIFT shows that even minor formatting changes, such as whitespace edits or identifier renamings, can cause substantial shifts in model outputs. These effects hold across nine coding LLMs and three tasks (fixing, summarization, and translation). These findings motivate future research for grammar-aware or domain-adaptive tokenizers that more faithfully reflect PL structure."
        },
        {
            "title": "Limitations",
            "content": "While our study shows limitations of current tokenizer designs in code LLMs, our analysis focuses on targeted set of semantic-preserving rewrites based on common formatting and naming conventions; these do not encompass all potential sources of tokenization drift. Second, although we evaluate nine widely used code LLMs, our findings may not generalize to models with fundamentally different architectures (e.g., state space models (Gu et al., 2022)) or tokenization strategies (e.g., characterlevel or grammar-driven tokenizers (Kim et al., 2016)). Third, our work centers on measurement and diagnosis, and we do not explore mitigation strategies. Future work could investigate tokenizer retraining, ensemble decoding over multiple tokenizations, or architectural modifications to improve the alignment between token boundaries and programming language syntax."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Yu Liu for valuable comments and feedback. This work was supported in part by Compute Ontario (computeontario.ca) and the Digital Research Alliance of Canada (alliancecan.ca). It was also partially supported by Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant (RGPIN-2024-04909) and startup grant from the University of Waterloo. Yuntian Deng is additionally supported by an NSERC Discovery Grant (RGPIN-2024-05178) and start-up grant from the University of Waterloo."
        },
        {
            "title": "References",
            "content": "Wasi Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, and Kai-Wei Chang. 2023. AVATAR: parallel corpus for Java-Python program translation. In Findings of the Association for Computational Linguistics: ACL, pages 22682281. Spurthi Amba Hombaiah, Tao Chen, Mingyang Zhang, Michael Bendersky, and Marc Najork. 2021. Dynamic language models for continuously evolving content. In International Conference on Knowledge Discovery and Data Mining, pages 25142524. Loubna Ben Allal, Niklas Muennighoff, Loand framework the evaluation of code generation modhttps://github.com/bigcode-project/ gesh Kumar Umapathi, Ben Lipkin, Leandro von Werra. 2022. for els. bigcode-evaluation-harness. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Nadezhda Chirkova and Sergey Troshin. 2023. Codebpe: Investigating subtokenization options for large language model pretraining on source code. Preprint, arXiv:2308.00683. William Jay Conover. 1999. Practical nonparametric statistics. john wiley & sons. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Albert Gu, Karan Goel, and Christopher Ré. 2022. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR). Batu Guan, Xiao Wu, Yuanyuan Yuan, and Shaohua Li. 2025. Is your benchmark (still) useful? dynamic benchmarking for code language models. arXiv preprint arXiv:2503.06643. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. Deepseek-coder: When the large language model meets programming the rise of code intelligence. Preprint, arXiv:2401.14196. Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. In International Deep code comment generation. Conference on Program Comprehension, pages 200 210. Yoon Kim, Yacine Jernite, David Sontag, and Alexander Rush. 2016. Character-aware neural language models. Proceedings of the AAAI Conference on Artificial Intelligence, 30(1). Alisa Liu, Jonathan Hayase, Valentin Hofmann, Sewoong Oh, Noah A. Smith, and Yejin Choi. 2025. Superbpe: Space travel for language models. Preprint, arXiv:2503.13423. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research, 9(Nov):25792605. Meta FAIR CodeGen Team. 2025. Cwm: An openweights llm for research on code generation with world models. 32Bparameter open-weights model; inference code and weights released. Technical report, Meta. 9 Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. 2023. OctoPack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. Preprint, arXiv:2112.00114. Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. 2024. Lost in translation: study of bugs introduced by large language models while translating code. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pages 113. Sheena Panthaplackel, Pengyu Nie, Milos Gligoric, Junyi Jessy Li, and Raymond Mooney. 2020. Learning to update natural language comments based on code changes. In Annual Meeting of the Association for Computational Linguistics, pages 18531868. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830. Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable code generation from pre-trained language models. Preprint, arXiv:2201.11227. Ruchir Puri, David Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss. 2021. CodeNet: large-scale AI for code dataset for learning diversity of coding tasks. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Andrew Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan Tracey, and David Cox. 2019. On the information bottleneck theory of deep learning. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124020. Craig W. Schmidt, Varshini Reddy, Chris Tanner, and Yuval Pinter. 2025. Boundless byte pair encoding: Breaking the pre-tokenization barrier. Preprint, arXiv:2504.00178. Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 98959901, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Mike Schuster and Kaisuke Nakajima. 2012. Japanese and korean voice search. In International Conference on Acoustics, Speech and Signal Processing, pages 51495152. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Annual Meeting of the Association for Computational Linguistics, pages 17151725. Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and Denys Poshyvanyk. 2019. On learning meaningful code changes via neural machine translation. In International Conference on Software Engineering, pages 2536. Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, and Gagandeep Singh. 2024. Syncode: Llm generation with grammar augmentation. Preprint, arXiv:2403.01632. Dixuan Wang, Yanda Li, Junyuan Jiang, Zepeng Ding, Ziqin Luo, Guochao Jiang, Jiaqing Liang, and Deqing Yang. 2025. Tokenization matters! degrading large language models through challenging their tokenization. Preprint, arXiv:2405.17067. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Bing Yan, Angelica Chen, and Kyunghyun Cho. 2025. Inconsistency of llms in molecular representations. Digital Discovery. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Brian Siyuan Zheng, Alisa Liu, Orevaoghene Ahia, Jonathan Hayase, Yejin Choi, and Noah A. Smith. 2025. Broken tokens? your language model can secretly handle non-canonical tokenizations. Preprint, arXiv:2506.19004."
        },
        {
            "title": "A Use of LLMs",
            "content": "We used an LLM-based writing assistant to polish grammar. All ideas, analyses, experiments, and scientific claims are our own, and we take full responsibility for the content of this work. Additional Background: Tokenizer"
        },
        {
            "title": "Differences Between LLMs",
            "content": "Figure 6 shows the heatmap of vocabulary distances between tokenizers, which includes 19 popular open-source (coding) LLMs from 8 model families. Notably, most LLMs adopt pre-tokenization strategy that splits text into linguistically and layout-meaningful chunks before byte-level BPE. While details vary by family, common choices include isolating short digit runs (often 13; Qwen and some DeepSeek variants prefer per-digit), treating contiguous letters with combining marks as words, splitting punctuation and symbol runs (sometimes with an optional leading space), and separating newline blocks and longer space runs. Non-Latin scripts such as Han/Hiragana/Katakana (and in some cases Hangul) are taken as contiguous spans. Family differences that matter for our study include LLaMA-3 explicitly detaching English clitics, CodeQwen-1.5 disabling pre-tokenization (leaving underscores and long ASCII spans intact), DeepSeek-Coder using code-oriented splits (letters, punctuation, newlines, CJK, digits), and DeepSeekV3/LLaMA-4/GPT-OSS converging on similar unified scheme. In practice, more aggressive presegmentation tends to make models tolerant to superficial spacing around symbols but sensitive to numeric chunk boundaries, whereas byte-only or lightly pre-segmented designs make underscore and identifier edits more likely to introduce new token boundaries."
        },
        {
            "title": "C Additional Experimental Methodology",
            "content": "C.1 Benchmarks Normalization To ensure that our semantic-preserving naming/spacing rewrite rules (Section 3.3) do not spuriously break compilation or tests, we perform lightweight normalization pass before evaluation. For the bug fixing and code summarization tasks from HumanEvalPack (Muennighoff et al., 2023), we first canonicalize Java identifier style to camelCase from snake_case2, then propagate any renamings consistently to tests, entry points, and declarations to preserve their functionalities. For the code translation tasks, we start from the Avatar and CodeNet benchmarks prepared by Pan et al. (2024), following their task definitions and tests. We fixed some samples with harness-compatibility issues that would otherwise cause false negatives and prune small number of unsalvageable or pathological samples (e.g., extremely long inputs or cases that time out), without changing the underlying problem semantics. And finally, we dropped 6 python2java tasks and 4 java2python tasks in Avatar that we could not fix. The most common adjustments fall into few categories: (i) IO/formatting normalization. For example, we replace non-portable characters such as U+FFFD or segmentation markers like U+2581 with ASCII equivalents; ensure consistent tokenization by splitting on spaces instead of empty strings; remove trailing spaces/newlines; standardize numeric output with Java DecimalFormat or Python f-strings to fixed precision; (ii) test correctness fixes where expected outputs were inconsistent with the reference implementation or ordering; and (iii) minimal code-context edits that preserve semantics but align with tests (e.g., renaming helper methods where tokenizer-specific splits would otherwise occur, adding @Override annotations, or make Scanner/FastScanner usage consistent). All edits are specified once, applied uniformly to baseline and variant inputs, and never conditioned on model outputs. C.2 Rewrite Algorithms To mutatively rewrite code context on naming, we first parse it to obtain code token index and two identifier sets: (i) immutable identifiers derived from configured immutable types (e.g., Java: importDeclaration, methodCall; Python: import_as_name, trailer); (ii) declaration identifiers that are safe to rename (excluding Java methods annotated with @Override). We restrict candidates by casing using regexes, specifically, snake case matches [a-z0-9]+(?:_[A-Za-z0-9]+)+ and camel case identifier matches the regex [a-z]+(?:[A-Z]+[A-Za-z0-9]+[A-Za-z0-9]*)+. For each eligible identifier, we segment its lexeme by well designed regex, convert from the source to the target case, and record the absolute character positions in the original string where underscores 2HumanEvalPack (Muennighoff et al., 2023) translates the HumanEval (Chen et al., 2021) benchmark from Python to other PLs (including Java), but all the identifiers were remained in snake_case regardless of the target PL. 11 Figure 6: Heatmap of vocabulary distances between tokenizers (Full ver.). would be inserted or removed (edit events). For HumanEval tasks, we additionally propagate the same renamings to tests, entry points, and declarations to keep the harness consistent, and these are treated as optional ancillary patches and do not alter the core algorithm. The immutable/declaration settings aim to maximize safe coverage while preserving compilation and test pass behavior. Spacing rewrite follows the same structure but, instead of changing identifier lexemes, we insert exactly one space between adjacent tokens when their kinds match configured token-type bigram (former, latter) from Table 3. For each match, we insert whitespace at the boundary between the two tokens, record an insertion event at that position, and update offsets. Conceptually, although rewrites are defined over PL tokens, the notion of fragment uses LLM tokens. For each rewrite site, we consider the minimal contiguous list of LLM tokens that covers the affected PL tokens (identifiers for naming and the two code tokens of each combination for spacing) as the fragment. Our fragment-change classification is based on an analysis of all fragments transformation in code context. Specifically, merge occurs when at least one old LLM token boundary inside those spans disappears, and split occurs when at least one new boundary appears after rewriting. To detect and analyze all LLM token boundary transformations, we compute LLM token start positions before and after rewriting with the same LLM tokenizer. We ignore boundaries created exactly at an edit site between two code tokens or those created right next to the edit site within one code token. Meaning that for insertions, we disregard any boundary introduced by the inserted whitespace between two code tokens that were fully or partially combined into one LLM token, as well as those caused by standalone underscores immediately to its right (a behavior commonly observed in 12 the DeepSeek-Coder or CodeQwen-1.5 tokenizer, where underscores are usually treated as single token), as encoded by the various edit masks classified by edit types in Algorithm 3. The loop in Algorithm 3 shifts the original boundary set by the cumulative δ pre edit to align coordinate systems, builds the masks for shift edits and edit-adjacent positions for insert operation, and then compares adjusted old versus filtered new starts. Specifically, let Sold and Snew be the sets of LLM token starts before and after rewriting, and after masking specific edit sites, we compute A=Sold Snew (old boundaries lost) and B=Snew Sold (new boundaries gained). The label is unchanged if A= and B=, merged if A= and B=, split if A= and B=, and mixed otherwise. C.3 Metrics Computation Algorithm We evaluate on two input programming language subsets for accuracy and accuracy, Xp (Python inputs) and Xj (Java inputs), where their union = Xj Xp with = 1546. For fixed rewrite rule wi and model m, let Ti be the deterministic transformation that applies wi to an input X, and we define T0 as no rule would be applied on the input. And we let ={ii = 0, 1, , 24} denote the assignment set for all rules where i=0 is the baseline, i>0 means the variant of applying rule wi. Running the model yields code fm(Ti(x)), which the harness evaluates on the test set (x). We define the test-level pass fraction rm,i(x) 1 (x) (cid:88) tT (x) [[fm(Ti(x))]]t, where [[fm(Ti(x))]]t {0, 1} denotes the execution result of program fm(Ti(x)) from test (Guan et al., 2025). Follow that we define the task-level correctness indicator Ym,i(x) I{rm,i(x) = 1} {0, 1}. So the accuracy of rule assignment on set {Xp, Xj} is Accuracyi(m; S) = 1 (cid:88) xS Ym,i(x). We report accuracy as accuracyi(m; S) Accuracyi(m; S) Accuracy0(m; S), where and = 0. Not all inputs would be modified by given rule, we therefore define the actually-affected subset { : Ti(x) = }, whose summed sizes for all rules classified by model series are shown in Table 8. Then our proposed sensitivity measures how often correctness flips among affected inputs only by Sensitivityi(m) 1 i (cid:88) xX (cid:12)Ym,i(x)Ym,0(x)(cid:12) (cid:12) (cid:12). Intuitively, accuracy captures net gains/losses which may cancel when aggregating, whereas sensitivity isolates the flip rate on inputs whose tokens were actually changed by wi. C.4 Experimental Environment We conduct all experiments on an NVIDIA H100 GPU cluster, consuming approximately 1840 GPUhours in total across runs. All model checkpoints are obtained from the Hugging Face Hub and loaded with the Hugging Face Transformers library (v4.53.2) (Wolf et al., 2020). Unless otherwise stated, models are executed in fp32, the only exceptions are Llama-3.3-70B-Instruct, Qwen2.5-Coder-32B-Instruct, and deepseek-coder33b-instruct, which we run in fp16. All evaluations use the bigcode-evaluation-harness framework (Ben Allal et al., 2022) with its standard protocols. We use deterministic decoding without sampling and batch size of 1 throughout. All tests are executed with Java 21.0.1 and Python The maximum generation length is set 3.8. to 1,024 tokens for HumanEvalPack and Avatar tasks, and 2,048 tokens for CodeNet tasks. For t-SNE visualizations, we use scikit-learn v1.7.1 (sklearn.manifold.TSNE) with perplexity to 70 and use the BarnesHut method with 1000 iterations, PCA initialization, learning_rate=auto, and n_jobs=16 (Pedregosa et al., 2011)."
        },
        {
            "title": "D Additional Results and Analysis",
            "content": "In Figures 7 and 8, the line plots summarize sensitivity for each rewrite rule. In Figure 9, comparing samples with and without identifier fragment change shows the overall trend of sensitivity on different model size. The per-series breakdowns in Figures 10 to 12 echo this pattern across Llama-3, Qwen2.5-Coder, and DeepSeek-Coder, while Llama tends to be more sensitive overall, all families exhibit variation in sensitivity between \"changed\" and \"unchanged\" groups. Figure 13 shows the distribution of accuracy per rewrite rule. Compared to sensitivity, accuracy 13 Algorithm 1 Naming Rewrite Input: C: code context, : code parser, Itypes: immutable identifier types, ρsrc: source case regex, tgt: target case, (optional) ExtraP atches: extra patches. Output: C, E, (optional) ExtraP atches. // TokIdx: list of (x, τ, [i, j)) where x=code token, τ =token kind, [i, j)=char span 1: (TokIdx, Sim, Sdec) INDEX(C, P, Itypes) 2: [ ], , C, 0 // E: list of edit underscore events (pos, δ); O: total offset 3: 4: for (x, τ, [i, j)) TokIdx in ascending do 5: if τ = id (x / Sim Sdec) REGEXCHECK(x, ρsrc) then 6: 7: 8: 9: 10: 11: CASECONV(x, tgt) // rewrite the identifier to the target case list DIFFUNDERLINEPOS(x, y, i) // return list of add/del underscore events (pos, δ) APPEND(cid:0)E, list) R[x] CONCAT(C[0:(i+O)], y, C[(i+O+x):C]) // string concatenation + (y x) 12: 13: ExtraP atches APPLYREWRITES(ExtraP atches, R) 14: return (C, E, ExtraP atches) may not be ideal for quantifying robustness because gains and losses cancel and many samples are unaffected. Table 8 reports, for each rewrite rule, the number of benchmark samples actually modified, stratified by model series. Table 9 provides the full breakdown of sensitivity by fragment-change category. Together, these tables clarify both the scope of input perturbations and the source of robustness differences observed in the main results. Algorithm 2 Spacing Rewrite Input: C: code context, : code parser, (Kf , Kℓ): token-type bigram. Output: C, E. // TokIdx: list of (x, τ, [i, j)) where x=code token, τ =token kind, [i, j)=char span 1: TokIdx INDEX(C, ) 2: [ ], C, 0 // E: list of insert events (pos, +1); O: total offset 3: 4: for 0 to TokIdx 2 do 5: (xℓ, τℓ, [iℓ, jℓ)) TokIdx[k+1] (xf , τf , [if , jf )) TokIdx[k]; if MATCH(τf , Kf ) MATCH(τℓ, Kℓ) then APPEND(E, (iℓ, +1)) CONCAT(cid:0)C[0:(jf +O)], \" \", C[(iℓ+O):C](cid:1) // insert one space + 6: 7: 8: 9: 10: 11: return (C, E) Table 8: Samples that been modified by rewrite rule, broken down by model series. Rewrite Rule Model Series Total Unchanged Changed All Merged Split Mixed Naming Spacing Llama-3 Qwen2.5-Coder deepseek-coder CodeQwen1.5 Llama-3 Qwen2.5-Coder deepseek-coder CodeQwen1.5 2238 2238 2247 2247 12804 12804 12804 12804 1292 1292 999 9315 9315 8381 8720 946 946 1248 1293 3489 3489 4423 4084 105 105 123 136 660 660 608 725 767 767 996 2394 2391 3091 2504 74 74 129 114 435 438 724 855 Figure 7: Percentage difference for naming rewrite transformations. 15 Algorithm 3 Fragment-Change Classification (CLASSIFY) Input: C: original code context, C: new code context, E: list of edit events (pos, δ) with δ {1}, EditT ype: edit type, : LLM tokenizer. Output: type {unchanged, merged, split, mixed} 1: Lold POSLLMTOKENS(C, ) // cumulative first character positions of LLM tokens in 2: Lnew POSLLMTOKENS(C, ) 3: Sold SET(Lold), Snew SET(Lnew), Sed { pos (pos, δ) }, S+ 4: 0 // cumulative offset from prior edits 5: 6: for each (pos, δ) in do 7: ed 8: 9: 10: pos + // adjusted position of this edit Sold { p+δ if > else Sold } Sed { e+δ if > else Sed } ed S+ S+ ed {a + max(δ, 0)} + δ 11: 12: if EditT ype = underscore then Snew Snew (S+ 13: 14: else if EditT ype = whitespace then 15: ed Sed) // ignore starts next-to inserted standalone underscore edit boundaries Snew Snew (Sed Sold) // ignore new starts created at whitespace edit boundaries 16: 17: Sold Snew // A: lost tokens after rewrite (some tokens merged) 18: Snew Sold // B: gained tokens after rewrite (some tokens split) 19: 20: if = and = then return merged 21: 22: else if = and = then 23: 24: else if = and = then 25: 26: else 27: return unchanged return split return mixed Figure 8: Percentage difference for spacing rewrite transformations. 16 Table 9: Impact of different types of fragment change on sensitivity (Full ver.)."
        },
        {
            "title": "Rewrite Rule Model",
            "content": "Total Unchanged Changed (all) Changed (subcategories) Merged Split Mixed 12.58 12.37 11.21 8.77 8.77 10.57 11.14 9.78 10. 12.61 14.45 11.89 9.80 12.47 7.37 10.47 10.85 7.12 10.48 8.57 9.52 8.57 5.71 11.43 4.88 7.32 10.57 11.06 13.03 10.00 8.33 12.42 7.42 8.72 10.36 6.41 13.17 13.30 11.73 9.00 9.00 10.82 11.95 10.54 10.64 13.37 14.83 11.53 9.62 10.71 6.48 10.45 10.19 6.44 9.46 8.11 8.11 6.76 10.81 6.76 10.85 6.20 12. 10.80 14.48 16.78 13.01 22.15 12.10 12.02 14.09 10."
        },
        {
            "title": "10.22\nLlama-S\nLlama-M 10.99\n8.51\nLlama-L\n7.07\nQwen-S\nQwen-M\n8.87\n5.71\nQwen-L\n8.36\nDS-S\n8.71\nDS-M\nDS-L\n6.26",
            "content": "10.68 9.44 8.13 6.97 7.35 6.58 8.31 7.91 6.61 9.32 9.69 7.24 6.04 7.53 5.09 7.25 7.58 5.80 17 Figure 9: Naming rewrite rules percentage difference (with or without fragment change). Figure 12: (Deepseek series) Spacing rewrite rules percentage difference (with or without fragment change). Figure 10: (Llama series) Spacing rewrite rules percentage difference (with or without fragment change). Figure 11: (Qwen series) Spacing rewrite rules percentage difference (with or without fragment change). Figure 13: Distribution of accuracy per rewrite rule across models and benchmarks."
        }
    ],
    "affiliations": [
        "University of Waterloo"
    ]
}