{
    "paper_title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment",
    "authors": [
        "Mengxi Xiao",
        "Kailai Yang",
        "Pengde Zhao",
        "Enze Zhang",
        "Ziyan Kuang",
        "Zhiwei Liu",
        "Weiguang Han",
        "Shu Liao",
        "Lianting Huang",
        "Jinpeng Hu",
        "Min Peng",
        "Qianqian Xie",
        "Sophia Ananiadou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 6 3 6 9 0 . 2 1 5 2 : r MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment Mengxi Xiao School of Artificial Intelligence, Wuhan University China elsashaw@whu.edu.cn Kailai Yang The University of Manchester United Kingdom kailai.yang@manchester.ac.uk Pengde Zhao School of Computer Science, Wuhan University China Enze Zhang School of Artificial Intelligence, Wuhan University China Ziyan Kuang Center for Language and Information Research, Wuhan University China"
        },
        {
            "title": "Zhiwei Liu\nThe University of Manchester\nUnited Kingdom",
            "content": "Weiguang Han School of Computer Science, Wuhan University China Shu Liao Center for Language and Information Research, Wuhan University China Lianting Huang Mount Holyoke College United States huang36l@mtholyoke.edu"
        },
        {
            "title": "Jinpeng Hu\nHefei University of Technology\nChina",
            "content": "Min Peng School of Artificial Intelligence, Wuhan University China pengm@whu.edu.cn Qianqian Xie School of Artificial Intelligence, Wuhan University China xieq@whu.edu.cn Sophia Ananiadou The University of Manchester United Kingdom sophia.ananiadou@manchester.ac.uk Abstract Mental health disorders affect hundreds of millions globally, and the Web now serves as primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mentalhealth settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, unified framework Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX for advancing reliable mental-health reasoning. We propose MentraBench, comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, post-trained model optimized through hybrid SFTRL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies structured, consistencyoriented rewriting process to produce concise, readable, and wellbalanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios. CCS Concepts Computing methodologies Causal reasoning and diagnostics; Natural language generation. Conference acronym XX, June 0305, 2018, Woodstock, NY Mengxi Xiao et al. Keywords Mental Health Reasoning, Large Language Models, Post-training ACM Reference Format: Mengxi Xiao, Kailai Yang, Pengde Zhao, Enze Zhang, Ziyan Kuang, Zhiwei Liu, Weiguang Han, Shu Liao, Lianting Huang, Jinpeng Hu, Min Peng, Qianqian Xie, and Sophia Ananiadou. 2018. MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym XX). ACM, New York, NY, USA, 12 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nMental health disorders affect hundreds of millions of people world-\nwide and remain a leading contributor to disability, social inequal-\nity, and unmet clinical needs [9, 21, 33]. As individuals increas-\ningly turn to online platforms for information, support, and self-\nassessment, the Web has become a critical medium for expanding\naccess to mental health care and advancing social good. Artificial\nintelligence (AI) plays a growing role in this shift: assisting indi-\nviduals understand their conditions [30, 36, 39], supporting coun-\nselors in developing treatment strategies [23, 34], and helping clin-\nicians in decision-making [35]. Recently, large language models\n(LLMs) [11, 18] have been rapidly adopted across web-based men-\ntal health applications, due to their strong linguistic capabilities,\nbroad world knowledge, and ability to engage in natural, empa-\nthetic dialogue. Their potential impact is considerable: LLMs can\nprovide scalable, always-available guidance and reach populations\nthat traditional services often fail to serve.",
            "content": "However, deploying LLMs in such sensitive, high-stakes settings requires more than producing fluent responses or superficially accurate predictions. Effective mental health support depends on transparent, coherent, and context-grounded reasoning that reflects how human clinicians interpret complex, subjective narratives. When LLMs misread user self-reports, rely on incomplete reasoning, or accept subjective statements as factual, they may exaggerate symptoms, provide misleading feedback, or inadvertently amplify users anxiety [27]. At scale, these risks threaten public trust and can undermine the social-good promise of AI-enabled mental health support systems. These risks underscore the urgent need for LLMs capable of responsible, clinically aligned reasoning rather than merely generating plausible answers [10, 17]. Recent efforts [4, 8, 40], have begun exploring how LLMs can better support mental-health tasks. Such as, Psyche-R1 [4] jointly integrates empathy, psychological knowledge, and chain-of-thought reasoning through large-scale synthesis pipeline and hybrid GRPOSFT training. Psy-Interpreter [8] enhances implicit mental-state inference using expert-annotated scenarios and trajectory-aware reinforcement learning framework that imitates clinician-like reasoning. PsychCounsel-Bench [40] assesses whether LLMs meet counseling knowledge standards, finding that only frontier models surpass certification-level performance [31]. Despite these advances, current methods still have important limitations in both method design and evaluation as showin in Table 1. Most existing approaches focus on emotional understanding, knowledge tests, or supervised reasoning tailored to narrow set of tasks, without systematically modeling core stages of clinical reasoning. Yet effective mental-health support requires reasoning across several interconnected processes: appraisal (recognizing maladaptive thought patterns), diagnosis (identifying likely conditions), intervention (selecting appropriate therapeutic strategies), abstraction (synthesizing structured evidence), and verification (detecting inaccurate or misleading mental-health information). Furthermore, existing methods focus primarily on task accuracy while overlooking the quality and reliability of the reasoning process. Reliable mental-health support requires LLMs to produce transparent, coherent, and context-grounded reasoning across several key dimensions: reasoning conciseness (avoiding unnecessary complexity or repetition), logical coherence (providing stepwise, case-specific justification), hallucination avoidance (not introducing unsupported facts), task understanding (following the intended objective without drift), and internal consistency (maintaining non-contradictory reasoning throughout). Addressing these aspects is essential for assessing and developing LLMs that can perform the step-wise, integrative reasoning that underlies appraisal, diagnosis, intervention, abstraction, and verification in real mental-health practice. To fill this gap, we present MentraSuite1, unified suite of benchmarks, datasets, and models for advancing reliable mental-health reasoning. We introduce MentraBench, comprehensive benchmark that evaluates five essential aspects of clinical and counseling cognition: appraisal, diagnosis, intervention, abstraction, and verification. MentraBench spans six tasks and 13 datasets, built by refining existing resources and constructing new ones. Unlike prior benchmarks focused primarily on accuracy, MentraBench emphasizes the quality of reasoning trajectories, assessing five key dimensions: reasoning conciseness, logical coherence, hallucination avoidance, task understanding, and internal consistency. We then introduce Mindora, post-trained model optimized for diverse mental-health reasoning tasks and more reliable reasoning processes. Mindora adopts novel hybrid supervised fine tuningreinforcement learning (SFTRL) training strategy with an LLMbased inconsistency-detection reward that dynamically enforces internal consistency while enhancing reasoning depth and generalization to unseen cases. To support Mindoras training, we construct high-quality reasoning data through Reasoning Trajectory Generation (RTG) strategy. RTG filters samples by difficulty and applies structured rewriting procedure that produces concise, readable, and well-balanced reasoning trajectories, directly mitigating issues such as over-elaboration and improving reasoning clarity. Extensive experiments on MentraBench covering 20 evaluated LLMs, show that Mindora achieves the highest average performance across all 13 datasets, outperforming strong baselines such as GPT4o-mini and DeepSeek-R1. Trajectory-level analysis further confirms remarkable performance across all five reasoning dimensions, demonstrating Mindoras superior ability to reason concisely, accurately, and coherently in complex mental-health scenarios. In summary, our contribution can be summarized as follows: (1) We present MentraBench, the first comprehensive benchmark designed to evaluate LLMs reasoning abilities across five 1The code and data are available in MentraSuite. MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment Conference acronym XX, June 0305, 2018, Woodstock, NY Figure 1: Tasks and datasets included in MentraBench. Table 1: Comparison of MentraBench with existing mental-health reasoning works. Statistics Task Choice Evaluation Aspects Task Dataset Appraisal Diagnosis Intervention Multi-step Abstraction Verification Correctness Reasoning Chain Ability Training Strategy Reward CoT Construction Psyche-R1 [4] Psy-Interpreter [8] PsychCounsel-Bench [40] MentraBench (ours) 3 3 1 4 6 1 13 (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) empathy, logic knowledge logic, interpretability, consistency format, correctness bilateral reward format, length, correctness, consistency prompt-rationale optimization knowledge injection structured, generation-verifier refinement key aspects of mental-health practice: appraisal, diagnosis, intervention, abstraction, and verification. It includes 6 tasks, and 13 datasets, emphasizing both task accuracy and reasoning quality. (2) We develop Mindora, post-trained model that combines supervised fine-tuning and reinforcement learning with novel llm based consistency detection rewards to enhance reasoning conciseness, consistency, and factual grounding. (3) We propose the Reasoning Trajectory Generation strategy, which produces structured and concise reasoning data through difficulty filtering and structured transformation, improving interpretability and mitigating reasoning redundancy. (4) Extensive experiments demonstrate Mindoras superior performance across task performance and all reasoning dimensions, surpassing state-of-the-art models such as GPT-4o-mini and DeepSeekR1."
        },
        {
            "title": "2 Methods\n2.1 MentraBench\nMental health reasoning requires a range of cognitive and decision-\nmaking skills, from recognizing distorted thought patterns to inter-\npreting clinical evidence and selecting interventions. To systemat-\nically evaluate these capabilities in LLMs, we design MentraBench\naround six complementary dimensions. Appraisal tests cognitive-\npattern reasoning, identifying subtle maladaptive thought processes.",
            "content": "Diagnosis assesses condition classification, determining likely mental health problems from client data. Intervention evaluates therapeuticplanning skills, generating appropriate counseling strategies. Multistep probes multi-step reasoning, integrating knowledge, diagnosis, and intervention in complex scenarios. Abstraction examines evidence synthesis, summarizing findings from structured research or experimental data. Verification challenges models to discern accurate from misleading mental health information. Each dimension is instantiated through representative task paired with corresponding datasets. Figure 1 provides an overview of tasks and datasets included in MentraBench, and Table 2 summarizes key dataset statistics. MentraBench comprises six tasks and 13 datasets, constructed through refinements of existing resources and the creation of new ones. In Table 2, datasets marked with ğ‘€ indicate those processed in this work, while those marked with denote newly curated and annotated datasets."
        },
        {
            "title": "2.1.1 Appraisal: Cognitive-Pattern Reasoning. The Appraisal dimen-\nsion evaluates an LLMâ€™s ability to identify what cognitive error is\npresent in a clientâ€™s self-reported statement. This dimension tar-\ngets fine-grained cognitive-pattern reasoning, requiring the model\nto recognize subtle distorted appraisal cues and distinguish among\nsimilar forms of maladaptive thinking. We instantiate this dimen-\nsion through the cognitive error identification task, evaluated us-\ning three high-quality datasets that span more than a dozen cognitive-\ndistortion categories. CognitiveReframing [28] combines simulated\nnegative thoughts from the Thought Records Dataset [2] and self-\nreports from the Mental Health America website2, with distortions",
            "content": "2https://screening.mhanational.org/ Conference acronym XX, June 0305, 2018, Woodstock, NY Mengxi Xiao et al. Table 2: Dataset statistics."
        },
        {
            "title": "Dataset",
            "content": "Train/Valid/Test"
        },
        {
            "title": "Cognitive Error Identification",
            "content": "CognitiveReframing [28] 751/173/284 synthetic, MHA PatternReframe [15] Therapist Q&A [29] 1302/661/ synthetic 1662/409/507 counseling dialogues (real)"
        },
        {
            "title": "Human",
            "content": "MicroF1 MicroF1 MicroF1 DepSign [22] SWMH [12] T-SID [12] PsyDTCorpusğ‘€ AnnoMIğ‘€ MHQA [25] MedQAğ‘€ MedMCQAğ‘€ PubMedQAğ‘€"
        },
        {
            "title": "Mental Health Condition Detection",
            "content": "1198/300/600 1200/300/500 1200/300/"
        },
        {
            "title": "Twitter",
            "content": "weak supervision weak supervision weak supervision MicroF1 MicroF1 MicroF"
        },
        {
            "title": "Counseling Strategy Formulation",
            "content": "1200/300/- counseling dialogues (syn) -/-/133 counseling dialogues (real)"
        },
        {
            "title": "Human",
            "content": "1200/300/717 464/58/121 318/120/446 -/-/"
        },
        {
            "title": "Psychiatry Systematic Review Summarization",
            "content": "Human/LLM"
        },
        {
            "title": "Jaccard",
            "content": "MicroF1 MicroF1 MicroF1 MicroF1 PSRS* -/-/"
        },
        {
            "title": "Mental Health Misinformation Identification",
            "content": "MentalMisinfo [16] 123/31/130 Youtube, Bitchute"
        },
        {
            "title": "Human",
            "content": "MacroF1 Note: Datasets with ğ‘€ are processed in this work. Dataset with * is newly curated and annotated in this work, where the recall metric means the coverage of annotated scoring points. annotated by 15 trained mental-health professionals. PatternReframe [15], constructed from PERSONA-CHAT personas [41], contains statements crafted to manifest specific distortions and labeled by five independent raters. Therapist Q&A [29], derived from real therapistclient interactions in the Therapist Q&A corpus3, provides naturally occurring distorted statements annotated by two clinical raters. These three datasets adopt slightly different taxonomies of cognitive errors. In our prompt design, we follow each datasets original definitions and examples. The cognitive-error categories shared across all three datasets include: All-or-Nothing Thinking, Overgeneralization, Labeling, Fortune Telling, Mind Reading, Should Statements, and Personalization. Unique categories in CognitiveReframing include: Emotional Reasoning, Comparing and Despairing, Blaming, Negative Feeling or Emotion, Catastrophizing, and Discounting the Positive. Unique categories in PatternReframe include: Mental Filtering, Catastrophizing, and Discounting the Positive. Unique categories in Therapist Q&A include: Emotional Reasoning, Mental Filtering, Magnification, and No Distortion."
        },
        {
            "title": "2.1.3\nIntervention: Therapeutic-Action Reasoning. The Intervention\ndimension evaluates an LLMâ€™s ability to determine what counsel-\ning action should be taken in response to a clientâ€™s situation. This",
            "content": "3https://www.kaggle.com/arnmaud/therapist-qa dimension targets therapeutic-action reasoning, requiring the model to analyze the clients presentation and select the intervention strategy that is most contextually appropriate, rather than offering generic, misplaced, or logically inconsistent responses. We instantiate this dimension through the counseling-strategy formulation task, covering thirteen commonly taught intervention types.4 To construct evaluation data, we use two high-quality counseling dialogue corpora. Client utterances are compressed into concise case summaries using GPT-4o prompts, and counselor utterances are annotated to extract strategy labels as reference answers. All constructed items are manually reviewed and verified by an expert counselor with over ten years of clinical experience, resulting in the final [dataset]ğ‘€ versions. PsyDTCorpusğ‘€ : Based on PsyDTCorpus [37], which contains 5,000 high-quality single-turn dialogues from SoulChatCorpus and 12 anonymized real counseling cases synthesized into multi-turn interactions. Due to its partially synthetic nature, this dataset is used to train Mindora but excluded from benchmark evaluation. AnnoMIğ‘€ : Based on AnnoMI [34], constructed from authorized motivational interviewing (MI) demonstration videos sourced from YouTube and Vimeo, transcribed and curated into high-quality multi-turn counseling interactions."
        },
        {
            "title": "2.1.5 Abstraction: Evidence-Based Reasoning. The Abstraction di-\nmension evaluates whether an LLM can determine what the ev-\nidence shows by interpreting and summarizing complex psychi-\natric research reports. This task targets free-text evidence reason-\ning, requiring models to process long, highly structured system-\natic review abstracts, extract key numerical and methodological\ninformation, and convert it into clinically meaningful conclusions,\nincluding effect direction and certainty levels. We instantiate this\ndimension through the Psychiatry Systematic Review Summariza-\ntion (PSRS) dataset. Each case in PSRS was manually curated from\nthe Cochrane Library [26] to cover a broad spectrum of psychiatric\nconditions, populations, and intervention types, and expert anno-\ntators generated scoring points for every instance to capture the\nmain findings.",
            "content": "4The strategies include Clarification, Paraphrasing, Reflection of Feeling, Summarizing, Questioning Skills, Immediacy, Use of Silence, Self-Disclosure, Confrontation, Encouragement, Repetition, Interpretation, and Guidance. MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment Conference acronym XX, June 0305, 2018, Woodstock, NY In this task, LLMs are required to summarize the main results reported in the studiesturning quantitative outcomes, confidence intervals, and methodological notes into concise, interpretable conclusions. This involves synthesizing statistical evidence into clinical interpretation, inferring effect direction, and assessing the certainty of evidence, rather than merely reproducing numerical results. All abstracts are publicly available and retain study identifiers to ensure traceability; no patient-level or individual trial data are included."
        },
        {
            "title": "2.1.6 Verification: Misinformation Detection. The Verification di-\nmension evaluates whether an LLM can determine whether mental-\nhealth information is accurate. This task targets misinformation de-\ntection reasoning, requiring models to identify misleading, anecdo-\ntal, or non-evidence-based claims in user-generated content while\nrelying on authoritative clinical knowledge. We instantiate this di-\nmension through the Mental Health Misinformation Identification\ntask, where models must distinguish reliable mental-health infor-\nmation from potentially harmful narratives in textual scripts de-\nrived from social-media videos.",
            "content": "We evaluate this task using the MentalMisinfo dataset [16], which contains video content from platforms such as YouTube Shorts and BitChute. The materials were systematically filtered, transcribed, and manually annotated to indicate whether each statement is accurate or misleading. By including this task, our benchmark assesses an LLMs ability to perform evidence-based verification under real-world, noisy, and informal language conditions commonly encountered in online mental-health discourse."
        },
        {
            "title": "2.2.1 Reasoning Trajectory Generation. To support Mindoraâ€™s train-\ning with high-quality supervision signals, we propose a reason-\ning trajectory generation strategy that focuses on genuinely chal-\nlenging reasoning steps rather than trivial pattern completion. Al-\nthough the tasks and datasets described above are demonstrably\nreasoning-intensive, directly using all training samples would di-\nlute the supervision with many instances that models can already\nsolve through surface-level cues. To obtain training data with suf-\nficient difficulty, we perform zero-shot question answering on the\ntraining split using Llama-3-8B-Instruct and retain only the cases\nwhere the model produces incorrect answers. This filtering proce-\ndure ensures that the collected trajectories focus on problems that\nrequire deeper reasoning and are more informative for model train-\ning. The number of retained training cases is shown in Table 2.",
            "content": "To address the readability and structural incoherence of reasoning trajectories caused by backtracking in iterative search, we propose structured reasoning trajectory generation method inspired by the search-based complex reasoning framework [3]. The core workflow involves two key stages: iterative optimal path search guided by verifier and structured formatting of the optimal trajectory, ensuring both reasoning depth and interpretability."
        },
        {
            "title": "2.2.2\nIterative Optimal Reasoning Path Search. We first leverage\nGPT-4o to explore and refine reasoning trajectories for verifiable\nmental health problems, following a feedback-driven iterative search\nparadigm:\nReasoning Generation. For a given verifiable problem ğ‘¥ with\nground-truth answer ğ‘¦ âˆ—, GPT-4o generates an initial Chain-of-Thought\n(CoT), denoted as ğ‘’0, and preliminary answer ğ‘¦0 by analyzing the\ncase context and applying domain knowledge.\nVerifier-Guided Refinement. A medical verifier implemented us-\ning GPT-4o checks if ğ‘¦0 aligns with ğ‘¦ âˆ—. If the verification returns\nFalse, GPT-4o samples a search strategy to refine the trajectory:\n(1) Backtracking: Revisits earlier reasoning steps ğ‘’ğ‘— (ğ‘— < ğ‘– âˆ’ 1) to\nidentify and resolve logical flaws. (2) Exploring New Paths: Devel-\nops an alternative reasoning approach distinct from prior attempts\nğ‘’0, ..., ğ‘’ğ‘–âˆ’1. (3) Verification: Validates the logical consistency and fac-\ntual accuracy of the current trajectory ğ‘’ğ‘–âˆ’1. (4) Correction: Directly\namends errors in the latest reasoning step ğ‘’ğ‘–âˆ’1 to align with domain\nprinciples.\nTermination Conditions. The iteration continues until the ver-\nifier confirms the answer is correct. If the maximum number of\niterations ğ‘ = 3 is reached without a correct answer, the search\nrestarts for up to ğ‘‡ = 3 attempts and all failed trajectories are\ndiscarded.",
            "content": "This process ensures that the final trajectory [ğ‘’0, ğ‘¦0, ..., ğ‘’ğ‘–, ğ‘¦ğ‘–] embodies iterative reflection and optimal reasoning, while avoiding stagnation in suboptimal paths."
        },
        {
            "title": "2.2.3\nStructured Reasoning Formats. To mitigate readability degra-\ndation caused by backtracking and unstructured reflection, the op-\ntimal reasoning trajectory is formatted into a standardized struc-\nture with two mandatory phases. This formatting enforces clarity,\nconsistency, and alignment between reasoning and answers, pro-\nhibiting deviations from the predefined schema.",
            "content": "In the reasoning phase: (1) All analytical content is enclosed within <think> tags. (2) Structured subtitles (e.g., ###Symptom Analysis, ###Differential Diagnosis) are used to segment reasoning steps, each on separate line. (3) The phase concludes with mandatory ###Final Conclusion section that summarizes the core logical chain and justifies the subsequent answer. In the answer phase: (1) The final judgment is enclosed within <answer> tags. (2) The phase strictly ends with the format Answer: [option/result] to ensure unambiguous output. (3) The answer must be logically consistent with the conclusion derived in the Reasoning Phase. An example of structured trajectory is shown in Figure 2."
        },
        {
            "title": "2.2.4 Rationale for Structural Constraints. The mandatory format-\nting rules address several critical limitations of unstructured tra-\njectories, specifically enhancing logical coherence, ensuring con-\nsistency and improving readability.\nEnhancing Logical Coherence. Unstructured trajectories often\ncontain fragmented backtracking, such as Wait, earlier I forgot to\ncheck symptom duration, Let me revisit that which disrupts logical\nflow. By segmenting reasoning into titled modules and isolating\nbacktracking within iterative search rather than the final output,",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Mengxi Xiao et al. Figure 2: The framework of Mindora. the structured trajectory maintains linear and coherent chain of logic. Ensuring Consistency. The mandatory ###Final Conclusion and Answer: [result] elements ensure that the answer directly reflects the reasoning process. This aims to avoiding inconsistencies, such as conclusion favoring Major Depressive Disorder while the answer lists Generalized Anxiety Disorder. This alignment is crucial for training models to produce logically grounded outputs in mentalhealth tasks, where misalignment could lead to clinical misjudgments. Improving Interpretability. Structured subtitles (e.g., ###Differential Diagnosis Exclusion) teach the model to decompose complex mental health judgments into domain-specific sub-tasksmirroring how clinicians systematically analyze cases. This modular learning improves the models ability to replicate interpretable, professional reasoning patterns."
        },
        {
            "title": "2.2.5 Training Procedure. The training of the mental health rea-\nsoning model adheres to the CHORD [42] algorithmâ€™s core para-\ndigm of dynamic balancing between sft and rl exploration, with\na formalized reward mechanism to ensure the quality and valid-\nity of model outputs. The training framework integrates dual data\nstreams, adaptive weight scheduling, and a multi-criteria reward\nfunction, as formally detailed below.\nTraining Framework and Notations. Let â„³ denote the base\nmodel (Qwen3-8B), â„³aux represent the auxiliary model (Qwen3-\n32B) for internal consistency detection, and ğ’ŸSFT âŠ‚ ğ’³ Ã— ğ’´ denote\nthe expert SFT dataset where ğ’³ is the set of mental health reason-\ning prompts and ğ’´ is the set of expert solutions. Let ğ’ŸRL âŠ‚ ğ’³ Ã— ğ’´\nbe the RL exploration dataset generated by â„³ during rollout. The\ntraining objective is to optimize the policy ğœ‹ğœƒ (parameterized by ğœƒ)\nvia the CHORD algorithm, which dynamically fuses SFT loss â„’SFT\nand RL loss â„’RL using two-level weights: global weight ğœ‡(ğ‘¡) and\ntoken-level weight ğœ™(â‹…).",
            "content": "The reward ğ‘Ÿ(ğ‘ , ğ‘) for an action ğ‘ (model solution) given state ğ‘  (input prompt) is composite function consisting of four sequential validity and quality checks, formally defined as: ğ‘Ÿ(ğ‘ , ğ‘) = ğ•€(FormatValid(ğ‘)) ğ•€(LengthValid(ğ‘)) ğ•€(Consistency(ğ‘)) ğ‘„Quality(ğ‘) (1) where: ğ•€() is the indicator function (1 if the condition holds, 0 otherwise). In detail, FormatValid(ğ‘) verifies if ğ‘ adheres to the mandatory format <think>...</think><answer>...</answer>. LengthValid(ğ‘) ensures the token length of the inner thinking trajectory ğ’¯ in ğ‘ falls within valid range [ğ¿min, ğ¿max], where ğ¿min = 10 tokens and ğ¿max = 2048 tokens. Consistency(ğ‘) detects factual inconsistencies or errors in ğ’¯ using the auxiliary model â„³aux. ğ‘„Quality(ğ‘) quantifies the correctness of ğ‘ based on task-specific benchmark criteria, with values in [0, 1]. It is defined separately for three task types: (1) Single-choice questions: Let ğ‘¦ be the ground-truth answer. Then: ğ‘„Quality(ğ‘) = { if the final conclusion in ğ’¯ = ğ‘¦ , 1 0 otherwise. (2) (2) Multiple-choice questions: Let ğ‘Œ = {ğ‘¦ ğ‘˜ } be the set of ground-truth answers, and ğ‘Œ = {ğ‘¦1, ğ‘¦2, ..., ğ‘¦ğ‘š} be the set of answers in ğ’¯. The quality score is the Jaccard similarity between ğ‘Œ and ğ‘Œ : 2 , ..., ğ‘¦ 1 , ğ‘¦ ğ‘„Quality(ğ‘) = ğ‘Œ ğ‘Œ ğ‘Œ ğ‘Œ . (3) (3) Short-answer questions: Let ğ’¦ = {ğ‘˜1, ğ‘˜2, ..., ğ‘˜ğ‘›} be the set of key scoring points for the task, and ğ’¦hit ğ’¦ be the subset of points covered in ğ’¯. Then: ğ‘„Quality(ğ‘) = ğ’¦hit ğ’¦ . (4) Training Pipeline. The training process proceeds in iterations ğ‘¡ = 1, 2, ..., ğ‘‡max (where ğ‘‡max is the total number of training steps), with each iteration consisting of data sampling, weight scheduling, loss computation, and parameter update stages. MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment Conference acronym XX, June 0305, 2018, Woodstock, NY Step 1: Data Sampling. At each step ğ‘¡, we sample two mini-batches: An SFT mini-batch â„¬SFT ğ’ŸSFT with batch size ğµSFT = 64 (consistent with the experimental setup), where each sample is (ğ‘¥ğ‘–, ğ‘¦ ğ‘– ) ğ’³ğ’´ ( ğ‘¥ğ‘– is the prompt, ğ‘¦ is the expert solution). An RL mini-batch ğ‘– â„¬RL ğ’ŸRL with dynamic batch size ğµRL, where each sample is (ğ‘¥ğ‘— , ğ‘ğ‘— ) ğ’³ ğ’´ ( ğ‘ğ‘— is the model-generated solution via rollout). The rollout for ğ’ŸRL generation uses temperature ğœ = 1.0, with ğ¾ = 8 candidate solutions sampled per prompt. Step 2: Adaptive Weight Scheduling. The global weight ğœ‡(ğ‘¡) (balancing â„’SFT and â„’RL) follows warmup-decay schedule to transition from expert imitation to RL exploration: (1) Warmup phase (1 ğ‘¡ ğ‘¡warmup, ğ‘¡warmup = 200 steps): ğœ‡(ğ‘¡) = ğœ‡valley + (ğœ‡peak ğœ‡valley) ğ‘¡ ğ‘¡warmup , (5) where ğœ‡peak = 0.5 (maximum SFT influence) and ğœ‡valley = 0.02 (minimum SFT influence). (2) Decay phase (ğ‘¡warmup < ğ‘¡ ğ‘¡warmup + ğ‘¡decay, ğ‘¡decay = 400 steps): ğœ‡(ğ‘¡) = ğœ‡peak (ğœ‡peak ğœ‡valley) ğ‘¡ ğ‘¡warmup ğ‘¡decay . (6) The token-wise weight ğœ™(ğ‘¦ ğ‘¡ ; ğœ‹ğœƒ ) (for SFT loss modulation) is defined based on the policys probability of generating the expert token ğ‘¦ ğ‘¡ (given prompt ğ‘¥ and prefix ğ‘¦ <ğ‘¡ ): ğœ™(ğ‘¦ ğ‘¡ ; ğœ‹ğœƒ ) = ğ‘ğ‘¡ (1 ğ‘ğ‘¡ ), (7) ğ‘¡ ğ‘¥, ğ‘¦ where ğ‘ğ‘¡ = ğœ‹ğœƒ (ğ‘¦ <ğ‘¡ ). This parabolic function prioritizes learning for tokens where the policy is uncertain ( ğ‘ğ‘¡ 0.5 ) while downweighting certain ( ğ‘ğ‘¡ 1 ) or irrelevant ( ğ‘ğ‘¡ 0 ) tokens. Step 3: Loss Computation and Parameter Update. The total loss â„’total(ğœƒ) is weighted combination of the SFT loss (with tokenwise weighting) and the RL loss: â„’total(ğœƒ) = (1 ğœ‡(ğ‘¡)) â„’GRPO(ğœƒ) + ğœ‡(ğ‘¡) â„’SFTğœ™ (ğœƒ), (8) SFT loss with token-wise weighting â„’SFTğœ™(ğœƒ) is computed over â„¬SFT, minimizing the weighted negative log-likelihood of expert solutions: â„’SFTğœ™ (ğœƒ) = 1 (ğ‘¥ğ‘–,ğ‘¦ ğ‘– )â„¬SFT ğ‘¦ ğ‘– ğ‘– )â„¬SFT (ğ‘¥ğ‘–,ğ‘¦ ğ‘¦ ğ‘– ğ‘¡=1 ğœ™(ğ‘¦ ğ‘–,ğ‘¡ ; ğœ‹ğœƒ ) log ğœ‹ğœƒ (ğ‘¦ ğ‘–,ğ‘¡ ğ‘¥ğ‘–, ğ‘¦ ğ‘–,<ğ‘¡ ). (9) GRPO loss â„’GRPO(ğœƒ) is optimized over â„¬RL to maximize the expected reward, using clipped surrogate objective (consistent with PPO-style updates): â„’GRPO(ğœƒ) = 1 (ğ‘¥ğ‘— ,ğ‘ğ‘— )â„¬RL ğ‘ğ‘— ğ‘ğ‘— ğ‘¡=1 (ğ‘¥ğ‘— ,ğ‘ğ‘— )â„¬RL min (ğ‘Ÿğ‘—,ğ‘¡ (ğœƒ)ğ´ğ‘— , clip(ğ‘Ÿğ‘—,ğ‘¡ (ğœƒ), 1 ğœ–, 1 + ğœ–)ğ´ğ‘— ) , (10) where: ğ‘Ÿğ‘—,ğ‘¡ (ğœƒ) = (importance sampling ratio, ğœ‹sample ğ‘Ÿ(ğ‘¥ğ‘— ,ğ‘ğ‘— )ğœ‡ğ‘… ğœğ‘…+ğœ–ğ‘§ ğœ‹ğœƒ (ğ‘ğ‘—,ğ‘¡ ğ‘¥ğ‘— ,ğ‘ğ‘—,<ğ‘¡ ) ğœ‹sample(ğ‘ğ‘—,ğ‘¡ ğ‘¥ğ‘— ,ğ‘ğ‘—,<ğ‘¡ ) is the reference policy), ğ´ğ‘— = (normalized advantage, ğœ‡ğ‘…/ğœğ‘… are the mean/std of rewards in the rollout group, ğœ–ğ‘§ = 108 for stability), ğœ– = 0.2. The policy parameters ğœƒ are updated by minimizing â„’total(ğœƒ) using the Adam optimizer with ğ›½1 = 0.9, ğ›½2 = 0.999, and learning rate ğœ‚ = 2 106. Training checkpointing is performed every 10 steps to preserve intermediate model states."
        },
        {
            "title": "3.2 Main Results\nIn this benchmark, we evaluate the reasoning performance of vari-\nous models across 13 datasets covering five core aspects of mental-\nhealth practice. The results, presented in Table 3, highlight several\nkey findings.",
            "content": "In general, Mindorağ¶ğ» ğ‘‚ğ‘…ğ· achieves the highest average score across all datasets, followed by Mindorağ‘†ğ¹ ğ‘‡ + ğ‘…ğ¿, both surpassing leading proprietary reasoning models such as GPT-o4-mini and DeepSeek-R1. This demonstrates the effectiveness of our post-training strategy in enhancing reasoning performance within complex, contextsensitive mental-health tasks. Across every dataset, Mindorağ¶ğ» ğ‘‚ğ‘…ğ· outperform the backbone model Qwen3-8B, showing consistent gains in both accuracy and reasoning quality. Furthermore, Mindorağ¶ğ» ğ‘‚ğ‘…ğ· also exceeds the separately trained Mindorağ‘†ğ¹ ğ‘‡ and Mindorağ‘†ğ¹ ğ‘‡ +ğ‘…ğ¿ variants, confirming that the joint SFTRL training paradigm more effectively balances imitation and exploration, avoiding overfitting while improving generalization to unseen cases. Performance analysis across the five task categories reveals that MindoraCHORD demonstrates strong appraisal and diagnostic reasoning, showing improved recognition of subtle cognitive patterns and accurate differentiation of overlapping symptom presentations. It also achieves notable gains in intervention and abstraction tasks, reflecting enhanced multi-stage reasoning and evidence synthesis. The models consistent results in verification tasks further suggest reliable factual grounding and resistance to misinformation. We also observe several interesting phenomena. Among opensource LLMs, different versions of the same model (e.g., DSDistillQwen3-32B, Qwen3-32B, and QwQ-32B) show only minor differences in overall reasoning performance, regardless of whether they 5Qwen-plus is Qwens versatile chat model, also trained with reasoning capabilities. 6https://www.modelscope.cn/models/aJupyter/EmoLLM_Qwen2-7B-Instruct_lora/. 7https://huggingface.co/MindIntLab/Psyche-R1 Conference acronym XX, June 0305, 2018, Woodstock, NY Mengxi Xiao et al. are distilled, chat, or reasoning variants. This suggests that mentalhealth reasoning tasks demand specialized reasoning abilities that general-purpose post-training cannot fully capture, underscoring the need for targeted reasoning optimization for mental health scenarios. Moreover, we find that open-source models ranging from 14B to 70B parameters achieve similar average scores around 0.6, while 8B-scale models remain near 0.55. In contrast, our Mindora series and the baseline Psyche-R1, both specifically optimized for mental-health reasoning, exceed the average performance of 8B models, demonstrating the strong potential of targeted post-training in this domain."
        },
        {
            "title": "3.3 Reasoning Trajectory Evaluation\nIn this section, we evaluate the reasoning trajectory quality of best\nperforming models in MentraBench from each major family (GPT,\nDeepSeek, Qwen, and LLaMA), the mental-health-oriented Psyche-\nR1, our backbone Qwen3-8B, and our proposed Mindorağ¶ğ» ğ‘‚ğ‘…ğ·.\nTo assess reasoning quality beyond task accuracy, we conduct a\ndetailed reasoning trajectory evaluation based on five criteria: rea-\nsoning conciseness, logical coherence, hallucination, task under-\nstanding, and internal consistency. Each reasoning chain is manu-\nally evaluated following a binary guideline, where a score of 1 is\nassigned if no errors are observed and 0 otherwise, with detailed\nscoring criteria shown in Appendix A. The final reasoning trajec-\ntory score is computed as the average across all five dimensions,\nproviding a comprehensive measure of reasoning reliability and\ntransparency in mental-health tasks.",
            "content": "For each model and each dataset, we sample four representative cases, two where all models produce correct answers and two where all models fail, to ensure fairness in comparison. Table 4 shows that Mindorağ¶ğ» ğ‘‚ğ‘…ğ· achieves remarkable average trajectory score, demonstrating balanced performance across multiple evaluation dimensions. Through refined training and balanced optimization, our model attains the best overall correctness and interpretability, highlighting its strength in both reasoning accuracy and clarity. shows substantial improvement over the backbone model Qwen38B. This confirms the effectiveness of the structured trajectory generation step in our training data, which enables the model to produce reasoning chains that are more organized, concise, and logically coherent."
        },
        {
            "title": "5 Conclusion\nIn this work, we introduced MentraSuite, comprising the Mentra-\nBench benchmark and the Mindora model, to systematically ad-\nvance and evaluate mental-health reasoning. Unlike prior studies\nthat focus primarily on emotional understanding or knowledge-\nbased assessment, our benchmark targets five clinically grounded\nreasoning aspects to capture the multi-stage and context-sensitive\nnature of real mental-health practice: appraisal, diagnosis, inter-\nvention, abstraction, and verification. We further developed a struc-\ntured Reasoning Trajectory Generation method and the post-trained\nmodel Mindora, which integrates supervised fine-tuning and re-\ninforcement learning with consistency-aware reward design. Ex-\nperimental results demonstrate that Mindoraâ€™s overall reasoning\nperformance surpasses strong baselines such as GPT-4o-mini and\nDeepSeek-R1, achieving balanced performance across datasets and\nsuperior reasoning-chain quality.",
            "content": "Beyond performance improvements, our findings highlight the importance of transparent, coherent, and context-grounded reasoning in clinical applications. Structured trajectory data and targeted post-training effectively reduce reasoning redundancy, enhance internal consistency, and improve interpretability, which are key steps toward reliable AI-assisted mental-health assessment. We believe this work provides solid foundation for studying how reasoning-oriented alignment can enable LLMs to assist in clinical In dimensions related to step redundancy and backtracking, Mindorağ¶ğ» ğ‘‚ğ‘…ğ· MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment Conference acronym XX, June 0305, 2018, Woodstock, NY Model CognitiveReframing PatternReframe Therapist Q&A ğ´ğ‘£ğ‘”1 DepSign SWMH T-SID ğ´ğ‘£ğ‘” AnnoMIğ‘€ MHQA MedQAğ‘€ MedMCQAğ‘€ PubMedQAğ‘€ ğ´ğ‘£ğ‘”4 PSRS MentalMisinfo ğ´ğ‘£ğ‘”ğ‘ğ‘™ğ‘™ Table 3: Experimental results on MentraBench. GPT-o4-mini[19] GPT-4o[18] Deepseek-R1[6] Deepseek-V3[7] Qwen-plus[38] QwQ-plus[24] LLaMA-4[1] LLaMA-3.3-70B[11] dsdistill-LLaMA-70B[6] Qwen2.5-72B[24] dsdistill-Qwen32B[6] Qwen3-32B[38] QwQ-32B[24] dsdistill-Qwen14B[6] Qwen3-14B[38] LLaMA3.1-8B[11] dsdistill-LLaMA-8B[6] EmoLLM[32] Psyche-R1[5] Qwen3-8B[38] Mindorağ‘†ğ¹ ğ‘‡ Mindorağ‘†ğ¹ ğ‘‡ +ğ‘…ğ¿ Mindorağ¶ğ» ğ‘‚ğ‘…ğ· 0. 0.6791 0.7516 0.6540 0.6821 0.6698 0. 0.6444 0.6667 0.6852 0.6540 0.6247 0. 0.6635 0.6540 0.5871 0.5906 0.6180 0. 0.5941 0.5693 0.5975 0.7293 0.6607 0. 0.7069 0.6755 0.6395 0.6871 0.6486 0. 0.6667 0.6207 0.6696 0.6395 0.6842 0. 0.6486 0.5714 0.5507 0.5472 0.5222 0. 0.6207 0.6207 0.6842 0.4468 0.4543 0. 0.4131 0.4057 0.3698 0.4031 0.3566 0. 0.3930 0.3539 0.4255 0.3905 0.3404 0. 0.1957 0.2396 0.3322 0.2021 0.4057 0. 0.4566 0.6026 0.6000 0.6352 0.5809 0. 0.5756 0.5702 0.5437 0.5746 0.5663 0. 0.5632 0.5846 0.5340 0.5949 0.4514 0. 0.4991 0.4085 0.5464 0.5457 0.5583 0. 0.6408 0.3607 0.4064 0.4085 0.4211 0. 0.4085 0.3914 0.4148 0.4000 0.3806 0. 0.3806 0.4085 0.4169 0.3871 0.4476 0. 0.4334 0.4169 0.4169 0.4887 0.5240 0. close-source models 0.7515 0.7114 0.7218 0.7581 0. 0.7330 0.6165 0.6226 0.6309 0.6502 0. 0.6321 70B+ open-source models 0.7884 0.7157 0.6953 0. 0.6369 0.6156 0.6156 0.6527 32B open-source models 0. 0.8038 0.7296 0.6237 0.6510 0.6294 0. 0.7500 0.7624 0.7715 0.7624 0.7547 0. 0.7163 0.7516 0.7639 0.7358 0.7685 0. 14B open-source models 0.7421 0.7484 0.7750 0.7540 0. 0.6298 78B open-source models 0.7358 0.7294 0.7229 0. 0.7358 0.7437 0.7715 0.7581 0.7490 0. 0.7962 0.7515 0.7597 0.6472 0.6325 0. 0.6336 0.6347 0.6640 0.8140 0.7032 0. 0.8030 0.6815 0.2086 0.3358 0.2020 0. 0.1866 0.1848 0.2066 0.2023 0.2367 0. 0.2178 0.2016 0.1749 0.2470 0.1680 0. 0.2001 0.1408 0.2424 0.1489 0.4193 0. 0.4016 0.5462 0.4531 0.4984 0.4768 0. 0.4201 0.4684 0.4148 0.4404 0.4022 0. 0.4355 0.4305 0.3823 0.4215 0.2771 0. 0.3373 0.5872 0.4009 0.5830 0.5996 0. 0.8925 0.6138 0.8608 0.8184 0.8242 0. 0.7414 0.7712 0.7097 0.7449 0.6697 0. 0.6831 0.6198 0.7047 0.5791 0.4430 0. 0.6989 0.5824 0.7263 0.7839 0.7590 0. 0.7936 0.8988 0.8696 0.8895 0.8354 0. 0.8904 0.7957 0.8402 0.7795 0.8206 0. 0.6660 0.8102 0.7925 0.6070 0.5749 0. 0.7927 0.8535 0.8593 0.8535 0.7118 0. 0.7256 0.6848 0.7199 0.7226 0.7239 0. 0.7452 0.7229 0.7229 0.7083 0.7375 0. 0.6882 0.7096 0.7587 0.6735 0.7194 0. 0.7376 0.8212 0.7402 0.6389 0.7459 0. 0.7283 0.6546 0.7059 0.7016 0.6727 0. 0.6433 0.6765 0.6745 0.8650 0.9065 0. 0.9296 0.9048 0.9335 0.7634 0.6318 0. 0.9555 0.9225 0.9333 0.9296 0.6045 0. 0.8641 0.8155 0.5896 0.5352 0.5042 0. 0.6165 0.7251 0.7660 0.6805 0.8157 0. 0.8164 0.8115 0.7764 0.7159 0.8379 0. 0.6894 0.5689 0.6156 0.6018 0.4838 0. 0.6894 0.6814 0.6091 0.6177 0.5025 0. 0.5795 0.6600 0.6703 0.7617 0.6346 0. 0.4773 0.5512 0.5681 0.7178 0.6515 0. 0.6505 0.6386 0.6387 0.6034 0.6178 0. 0.6209 0.6316 0.6061 0.6143 0.6084 0. 0.6109 0.5553 0.5536 0.5407 0.5943 0. 0.6367 0.6548 0.6933 0.8442 0.7721 Table 4: Reasoning trajectory evaluation."
        },
        {
            "title": "Model",
            "content": "R1 R2 R3 R4 R5 GPT-o4-mini Deepseek-R1 Qwen-plus LLaMA-4 Psyche-R1 Qwen3-8B Mindorağ¶ğ» ğ‘‚ğ‘…ğ· 1. 0.9519 0.6538 1.0000 0.9423 0.5769 0."
        },
        {
            "title": "Annotation Scores",
            "content": "0.4135 0.9808 1.0000 0.2596 0.9038 0. 0.9519 1.0000 1.0000 1.0000 1.0000 1. 1.0000 1.0000 0.9519 0.9808 0.9808 0. 0.9712 0.9808 0.9904 1.0000 1.0000 1. 0.9423 0.9038 0.9904 0.9808 Rğ‘ğ‘£ğ‘” 0. 0.9827 0.9270 0.8192 0.9442 0.9039 0. Inner-Annotator Agreement Gwet AC1 Cohens Kappa"
        },
        {
            "title": "Consistency",
            "content": "0.9607 0.8692 0.9698 0.8918 0.7906 0. 1.0000 1.0000 1.0000 0.9705 0.6033 0. 0.9855 0.7298 0.9863 0.9617 0.7986 0. Note: R1: Reasoning Conciseness; R2: Logical Coherence; R3: No Hallucination; R4: Task Understanding; R5: Internal Consistency. decision-making responsibly and ethically, aligning with the Web for Good vision of developing AI systems that serve human wellbeing with trustworthiness and social value. Acknowledgments This work is partially supported by Key Project of the National Natural Science Foundation of China (U23A20316), and CCF-Tencent Rhino-Bird Open Research Fund (CCF-Tencent RAGR20250115). References [1] Meta AI. 2025. Introducing LLaMA 4: Advancing Multimodal Intelligence. https: //ai.meta.com/blog/llama-4-multimodal-intelligence/ [2] Franziska Burger, Mark Neerincx, and Willem-Paul Brinkman. 2021. Natural language processing for cognitive therapy: extracting schemas from thought records. PloS one 16, 10 (2021), e0257832. [3] Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. 2024. HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs. arXiv:2412.18925 [cs.CL] https://arxiv.org/abs/ 2412.18925 [4] Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, and Meng Wang. 2025. Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning. arXiv preprint arXiv:2508.10848 (2025). [5] Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, and Meng Wang. 2025. Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning. arXiv:2508.10848 [cs.CL] https://arxiv.org/ abs/2508.10848 [6] DeepSeek-AI. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] https://arxiv.org/abs/ 2501.12948 [7] DeepSeek-AI. 2025. DeepSeek-V3 Technical Report. arXiv:2412.19437 [cs.CL] https://arxiv.org/abs/2412.19437 [8] Yichao Feng. 2025. From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via Bilateral Reinforcement Learning. arXiv preprint arXiv:2508.02458 (2025). [9] GBD 2019 Mental Disorders Collaborators. 2022. Global, regional, and national burden of 12 mental disorders in 204 countries and territories, 19902019: systematic analysis for the Global Burden of Disease Study 2019. The Lancet Psychiatry 9, 2 (Feb. 2022), 137150. doi:10.1016/S2215-0366(21)00395-3 Epub 2022 Jan 10. [10] Declan Grabb, Max Lamparth, and Nina Vasan. 2024. Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation. arXiv:2406.11852 [cs.CY] https://arxiv.org/abs/2406.11852 [11] Aaron Grattafiori, Abhimanyu Dubey, and et al Abhinav Jauhri. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783 [12] Shaoxiong Ji, Xue Li, Zi Huang, and Erik Cambria. 2022. Suicidal ideation and mental disorder detection with attentive relation networks. Neural Computing and Applications 34, 13 (2022), 1030910319. [13] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2020. What Disease does this Patient Have? Large-scale Open Domain Question Answering Dataset from Medical Exams. arXiv preprint arXiv:2009.13081 (2020). [14] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: Dataset for Biomedical Research Question Answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 25672577. [15] Mounica Maddela, Megan Ung, Jing Xu, Andrea Madotto, Heather Foran, and Y-Lan Boureau. 2023. Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 1364113660. doi:10.18653/V1/2023. ACL-LONG. [16] Viet Cuong Nguyen, Mini Jain, Abhijat Chauhan, Heather Jamie Soled, Santiago Alvarez Lesmes, Zihang Li, Michael Birnbaum, Sunny Tang, Srijan Conference acronym XX, June 0305, 2018, Woodstock, NY Mengxi Xiao et al. Kumar, and Munmun De Choudhury. 2025. Supporters and Skeptics: LLMbased Analysis of Engagement with Mental Health (Mis) Information Content on Video-sharing Platforms. In Proceedings of the International AAAI Conference on Web and Social Media, Vol. 19. 13291345. [17] Francis C. Ohu, Darrell Norman Burrell, and Laura A. Jones. 2025. Public Health Risk Management, Policy, and Ethical Imperatives in the Use of AI Tools for Mental Health Therapy. Healthcare 13, 21 (2025). doi:10.3390/healthcare13212721 [18] OpenAI. 2024. GPT-4o System Card. https://cdn.openai.com/gpt-4o-systemcard.pdf [19] OpenAI. 2025. OpenAI o3 and o4-mini System Card. https://cdn.openai.com/ pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf [20] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. MedMCQA: Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering. arXiv:2203.14371 [cs.CL] https://arxiv.org/abs/2203. 14371 [21] Vikram Patel, Shekhar Saxena, Crick Lund, Graham Thornicroft, Florence Baingana, Paul Bolton, Dan Chisholm, Pamela Y. Collins, Janice L. Cooper, John Eaton, Helen Herrman, Mohammad M. Herzallah, Y. Huang, Mark J. D. Jordans, Arthur Kleinman, Maria E. Medina-Mora, Emily Morgan, Uzma Niaz, Olayinka Omigbodun, Martin Prince, Atif Rahman, Benedetto Saraceno, Bidyut K. Sarkar, Mary De Silva, Indira Singh, Dan J. Stein, Charlene Sunkel, and JÃ¼rgen UnÃ¼tzer. 2018. The Lancet Commission on global mental health and sustainable development. The Lancet 392, 10157 (Oct. 2018), 15531598. doi:10.1016/S0140-6736(18) 31612-X Epub 2018 Oct 9; Erratum in: Lancet. 2018 Oct 27;392(10157):1518. doi: 10.1016/S0140-6736(18)32624-2. [22] RafaÅ‚ PoÅ›wiata and MichaÅ‚ PereÅ‚kiewicz. 2022. OPI@LT-EDI-ACL2022: Detecting Signs of Depression from Social Media Text using RoBERTa Pre-trained Language Models. In Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion, Bharathi Raja Chakravarthi, Bharathi, John McCrae, Manel Zarrouk, Kalika Bali, and Paul Buitelaar (Eds.). Association for Computational Linguistics, Dublin, Ireland, 276282. doi:10.18653/v1/2022.ltedi1.40 [23] Huachuan Qiu and Zhenzhong Lan. 2024. Interactive agents: Simulating counselor-client psychological counseling via role-playing llm-to-llm interactions. arXiv preprint arXiv:2408.15787 (2024). [24] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 Technical Report. arXiv:2412.15115 [cs.CL] https://arxiv.org/abs/2412.15115 [25] Suraj Racha, Prashant Joshi, Anshika Raman, Nikita Jangid, Mridul Sharma, Ganesh Ramakrishnan, and Nirmal Punjabi. 2025. MHQA: Diverse, Knowledge Intensive Mental Health Question Answering Challenge for Language Models. arXiv preprint arXiv:2502.15418 (2025). [26] Ri, Je, Arikpo, Mm, and Ja. 2015. Cochrane Library Trusted evidence. Informed decisions. Better health. Cochrane Database of Systematic Reviews [Intervention Review] Hand washing promotion for preventing diarrhoea. DOI 10, 14651858 (2015), 194. [27] K. L. Rosen, M. Sui, K. Heydari, E. J. Enichen, and J. C. Kvedar. 2025. The perils of politeness: how large language models may amplify medical misinformation. NPJ Digital Medicine 8, 1 (Nov. 2025), 644. doi:10.1038/s41746-025-02135-7 [28] Ashish Sharma, Kevin Rushton, Inna Lin, David Wadden, Khendra Lucas, Adam Miner, Theresa Nguyen, and Tim Althoff. 2023. Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 997710000. [29] Sagarika Shreevastava and Peter Foltz. 2021. Detecting Cognitive Distortions from Patient-Therapist Interactions. In Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access, Nazli Goharian, Philip Resnik, Andrew Yates, Molly Ireland, Kate Niederhoffer, and Rebecca Resnik (Eds.). Association for Computational Linguistics, Online, 151158. doi:10.18653/v1/2021.clpsych-1.17 [30] Gopendra Vikram Singh, Sai Vardhan Vemulapalli, Mauajama Firdaus, and Asif Ekbal. 2024. Deciphering cognitive distortions in patient-doctor mental health conversations: multimodal llm-based detection and reasoning framework. In Proceedings of the 2024 conference on empirical methods in natural language processing. 2254622570. [31] Hoyun Song, Huije Lee, Jisu Shin, Sukmin Cho, Changgeon Ko, and Jong C. Park. 2025. Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation. In Findings of the Association for Computational Linguistics: ACL 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, Vienna, Austria, 2173821756. doi:10.18653/v1/2025.findings-acl.1119 [32] EmoLLM Team. 2024. EmoLLM: Reinventing Mental Health Support with Large Language Models. https://github.com/SmartFlowAI/EmoLLM. [33] World Health Organization. 2025."
        },
        {
            "title": "Over a billion people",
            "content": "with mental health conditions services https://www.who.int/news/item/02-09-2025-over-a-billion-people-livingwith-mental-health-conditions-services-require-urgent-scale-up. release; accessed 2025-12-01. require urgent living scale-up."
        },
        {
            "title": "News",
            "content": "[34] Zixiu Wu, Simone Balloccu, Vivek Kumar, Rim Helaoui, Ehud Reiter, Diego Reforgiato Recupero, and Daniele Riboni. 2022. Anno-MI: Dataset of ExpertAnnotated Counselling Dialogues. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 61776181. doi:10. 1109/ICASSP43922.2022.9746035 [35] Mengxi Xiao, Ben Liu, He Li, Jimin Huang, Qianqian Xie, Xiaofen Zong, Mang Ye, and Min Peng. [n. d.]. MoodAngels: Retrieval-augmented Multi-agent Framework for Psychiatry Diagnosis. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. [36] Mengxi Xiao, Qianqian Xie, Ziyan Kuang, Zhicheng Liu, Kailai Yang, Min Peng, Weiguang Han, and Jimin Huang. 2024. HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 17071725. doi:10.18653/v1/ 2024.acl-long.93 [37] Haojie Xie, Yirong Chen, Xiaofen Xing, Jingkai Lin, and Xiangmin Xu. 2025. PsyDT: Using LLMs to Construct the Digital Twin of Psychological Counselor with Personalized Counseling Style for Psychological Counseling. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, Vienna, Austria, 10811115. doi:10.18653/v1/2025.acl-long.55 [38] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. 2025. Qwen3 Technical Report. arXiv:2505.09388 [cs.CL] https://arxiv.org/abs/2505.09388 [39] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2024. MentaLLaMA: interpretable mental health analysis on social media with large language models. In Proceedings of the ACM Web Conference 2024. 44894500. [40] Min Zeng. 2025. PsychCounsel-Bench: Evaluating the Psychology Intelligence of Large Language Models. arXiv preprint arXiv:2510.01611 (2025). [41] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing Dialogue Agents: have dog, do you have pets too?. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 22042213. [42] Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, and Jingren Zhou. 2025. On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting. arXiv:2508.11408 [cs.LG] https://arxiv.org/abs/2508.11408 Reasoning Chain Evaluation Guideline Assign 1 if no instances of the issue are present in the reasoning chain; assign 0 if any instance is observed. Reasoning Conciseness. The reasoning chain should contain no unnecessary complexity, repetition, or backtracking. Error indicators include: Over-elaborating straightforward case (e.g., exhaustively evaluating all options when the answer is obvious). Repeating the same evidence or argument across multiple steps. Reversing earlier conclusions without justification. Logical Coherence. Each step should provide clear and case-specific reasoning, not merely labels or unsupported claims. Error indicators include: Steps that function only as headings without substantive elaboration. MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment Conference acronym XX, June 0305, 2018, Woodstock, NY Claims presented without corresponding explanations or evidence. Hallucination Avoidance. The reasoning chain should accurately reflect the case information and avoid hallucinations. Error indicators include Introducing facts not mentioned in the case. Task Understanding. The reasoning chain should correctly follow the task objective and not drift to different task. The models reply shouldnt address different task than instructed. For example, if model misunderstands the counseling strategy formulation task, it may generate counselor utterances instead of selecting an appropriate counseling strategy. Internal Consistency. The reasoning chain should exhibit no contradictions across steps. Error indicators include: Later steps contradict earlier interpretations of symptoms, diagnoses, or risk levels. Changing conclusions mid-chain without reconciling prior evidence. Case Study The detail of case study is illustrated in Figure 3. Conference acronym XX, June 0305, 2018, Woodstock, NY Mengxi Xiao et al. Figure 3: challenging case of cognitive error identification."
        }
    ],
    "affiliations": [
        "Center for Language and Information Research, Wuhan University",
        "Mount Holyoke College",
        "School of Artificial Intelligence, Wuhan University",
        "School of Computer Science, Wuhan University",
        "The University of Manchester"
    ]
}