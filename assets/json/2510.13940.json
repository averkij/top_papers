{
    "paper_title": "Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention",
    "authors": [
        "Zhen Yang",
        "Mingyang Zhang",
        "Feng Chen",
        "Ganggui Ding",
        "Liang Hou",
        "Xin Tao",
        "Pengfei Wan",
        "Ying-Cong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover a simple yet underexplored phenomenon: reasoning uncertainty is highly localized-only a small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), a training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main model's KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for Qwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining highly efficient."
        },
        {
            "title": "Start",
            "content": "Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention Zhen Yang*1, Mingyang Zhang5, Feng Chen3, Ganggui Ding4, Liang Hou2, Xin Tao2, Pengfei Wan2, Ying-Cong Chen1,6 1HKUST(GZ), 2Kuaishou Technology, 3AIML, 4ZJU, 5Ant Group, 6HKUST zheny.cs@gmail.com, yingcongchen@ust.hk 5 2 0 2 5 1 ] . [ 1 0 4 9 3 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover simple yet underexplored phenomenon: reasoning uncertainty is highly localizedonly small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main models KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM taskse.g., +1.35% average improvement on eight benchmarks for Qwen3-8B-Base and +5% on AIME2024 using Qwen3-32BReasoningwhile remaining highly efficient. The code can be found here."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) (Achiam et al., 2023; Team et al., 2024; Dubey et al., 2024; Guo et al., 2025; Yang et al., 2025; Lozhkov et al., 2024; Young et al., 2024) have advanced rapidly, with ever-increasing parameter counts, larger pretraining corpora, and improved optimization techniques leading to steady gains across benchmarks. Beyond such training-time scaling, recent research has increasingly focused on test-time scalingallocating more computation during inference through deeper reasoning or broader search. Methods such as chain-of-thought (CoT) prompting (Wei et al., 2022), self-consistency (Wang et al., 2022), and *This work was conducted during the authors internship at Kling Team. Corresponding author. 1 Figure 1: Incorrect answers exhibit higher answer entropy than correct ones, primarily due to high-entropy tokens (results for Qwen3-8B-Reasoning on AIME2024). frameworks like Reflexion (Shinn et al., 2023) or Tree of Thoughts (Yao et al., 2023) demonstrate that increasing inference-time computation can enhance reasoning quality and output accuracy. However, these methods achieve better performance by trading inference efficiency for computation, as they often require multiple reasoning trajectories, extensive sampling, or long multi-step deliberation, making them computationally expensive and sometimes impractical in real-world deployments. In this work, we challenge the prevailing assumption that improving test-time performance of large language models must inherently come at the cost of increased inference computation. To examine this, we conduct an empirical analysis on the AIME2024 benchmark, comparing reasoning trajectories that yield correct versus incorrect answers. Interestingly, as shown in the inset panel in the upper-right corner of in Fig. 1, correct responses exhibit significantly lower answer-level entropy than incorrect ones, indicating that reasoning failures are associated with higher uncertainty. Furthermore, as illustrated in the main panel in Fig. 1, only small fraction of tokens within each response have notably high entropy, yet these few tokens contribute disproportionate share of the total answer-level entropy. This observation suggests that reasoning errors are not uniformly distributed across the sequence but are instead concentrated in few highentropy critical steps, where local uncertainty can propagate and destabilize subsequent predictions. We hypothesize that by selectively stabilizing these high-entropy regionse.g., through targeted guidance that reduces token-level entropyone can suppress error amplification and improve overall reasoning accuracy and consistency, all while intervening on only minimal subset of tokens. Building on this observation, we propose Minimal Test-Time Intervention (MTI), trainingfree approach that improves reasoning performance without incurring substantial inference cost. Specifically, MTI first detects high-entropy tokens by measuring the dispersion of the models predictive distribution during generation. High entropy indicates uncertainty in token prediction, and thus greater likelihood that local instability may propagate through the autoregressive chain. To reduce this uncertainty, MTI draws inspiration from classifier-free guidance (CFG) (Sanchez et al., 2023), which combines conditional and unconditional predictions to steer model outputs toward more task-aligned regions of the probability space. However, directly applying standard CFG would require maintaining separate unconditional branch, resulting in huge computational and memory overhead. To address this, MTI introduces lightweight alternative: before generating high-entropy token, the model temporarily augments its input with the short phrase OUTPUT ERROR, which implicitly constructs an on-the-fly unconditional contrastive branch for CFG-style guidance. This branch reuses the main models KV cache, adding only three auxiliary tokens per high-entropy position for the CFG computation. Since high-entropy tokens constitute only small fraction of the overall sequence, the additional overhead is negligible. In practice, this design effectively shifts the models predictive distribution toward the correct region of the probability space, enhancing reasoning stability and accuracy with almost no extra test-time computation. In general, our main contributions are as follows: We show that reasoning uncertainty in LLMs is highly localizedonly few high-entropy tokens account for most of the uncertainty within reasoning trajectory. We propose Minimal Test-Time Intervention (MTI), simple framework that selectively stabilizes these tokens to improve reasoning accuracy and consistency with almost no extra inference cost. We design an efficient CFG-inspired mechanism that constructs an on-the-fly unconditional branch via short negative prompt, reusing KV caches and adding negligible overhead. We achieve strong performance on general, coding, and STEM tasks, demonstrating the effectiveness of our method. For example, MTI surpasses the baseline by 1.35% on Qwen3-8B-Base while applying CFG to only 4.2% of tokens, and achieves +5% on the AIME2024 benchmark with Qwen3-32BReasoning, highlighting our methods effectiveness."
        },
        {
            "title": "2 Related Works",
            "content": "Test-time optimizations for LLMs. growing body of work focuses on test-time or decoding-time optimizations for LLMs (Chen et al., 2025), aiming to improve reasoning, robustness, or efficiency without retraining. One line of methods enhances reasoning through prompt engineering or introspective sampling: chain-of-thought (CoT) prompting (Wei et al., 2022), self-consistency (Wang et al., 2022), and frameworks such as Reflexion (Shinn et al., 2023) or Tree of Thoughts (Yao et al., 2023), which explore search or self-evaluation over multiple trajectories. These methods offer semantic robustness but often require multiple generations or reranking. Furthermore, LLM-CFG (Sanchez et al., 2023) improves semantic faithfulness and task performance by doubling computation and KVcache usage during decoding. Another direction emphasizes computational efficiency: speculative decoding or draft-and-verify schemes (Yan et al., 2024; Zhang et al., 2023) accelerate output generation by drafting tokens in parallel and verifying them, while memory optimizations (e.g., FlashAttention and efficient KV-cache management) aim to reduce memory footprint and speed up internal operations (Dao et al., 2022). These techniques are orthogonal to semantic guidancethey optimize how we compute, not what we steer. Compared with prior work, we identify an important test-time phenomenon: high-entropy tokens are 2 Figure 2: Comparison between Vanilla CFG and Ours (MTI). Unlike Vanilla CFG, (1) we apply classifier-free guidance only to high-entropy tokens, thereby stabilizing critical tokens and improving performance while maintaining high efficiency. (2) We reuse the KV cache of the conditional prediction (Cachec) and inject negative prompt, which removes the need for an unconditional KV cache (Cachec) and provides better approximation to the unconditional space. primary source of reasoning errors in LLMs. Detecting them via token entropy and intervening selectively improves reasoning performance. Moreover, our method is complementary to test-time scaling strategies and can be combined with them to yield additive gains. Classifier-free guidance in LLMs. Classifier-free guidance (CFG; (Ho and Salimans, 2022)) was introduced in diffusion models to balance conditional fidelity and diversity by interpolating between conditional and unconditional predictions. With negative prompts, it is often used to dynamically suppress undesired content (Koulischer et al., 2024), and it has also been adopted in editing tasks to mitigate artifacts (Miyake et al., 2025; Yang et al., 2023). When extended to language models, LLM-CFG (Sanchez et al., 2023) was initially employed to improve semantic faithfulness to the prompt while simultaneously enhancing task performance. However, unlike vision diffusion models, LLMs lack well-defined unconditional space from pretraining, leaving the unconditional prediction poorly calibrated; consequently, leaving the negative prompt emptyas is common in LLMCFGis suboptimal and weakens guidance. Moreover, vanilla LLM-CFG applies guidance to every token, requiring parallel unconditional pass with separate caching, which roughly doubles computation and KV-cache usage. These mismatches render direct transplantation of diffusion-style CFG to LLMs both inefficient and less effective. In contrast, we apply CFG only to high-entropy tokens and introduce negative-prompt injection via KV-cache reuse, eliminating KV-cache allocation for the unconditional branch and providing better-calibrated approximation to the unconditional space. Entropy-based token selection in LLMs. core observation across LLM reasoning research is that not all tokens exhibit equal uncertainty or importance. Token-level metricsparticularly entropyhave been used to identify decision boundaries or critical points during generation. For example, recent studies show that many low-entropy tokens in chain-of-thought reasoning can be pruned with minimal loss of accuracy (Li et al., 2025), and that focusing optimization on high-entropy tokens in reinforcement learning settings yields greater gains than treating all tokens equally (Wang et al., 2025). Other works investigate token skipping and compression strategies in LLMs to reduce redundant reasoning steps (Xia et al., 2025; Lee et al., 2025; Qian et al., 2025). Unlike these methods, which often require model retraining, heuristic pruning, or customized training objectives, our approach operates entirely at test time and is training3 free. We leverage token entropy to determine where to apply CFGonly to tokens with high entropy."
        },
        {
            "title": "3.1 Preliminary",
            "content": "We first introduce the notation used throughout this paper. Let () denote the output distribution predicted by the LLM. The conditional probability of generating token given context is denoted as (x c), where represents negative prompt in the context of classifier-free guidance (CFG) (Ho and Salimans, 2022). The CFG-adjusted prediction is denoted by ˆP (), and ω denotes the hyperparameter controlling the CFG strength. We use xt to represent the logits predicted by the LLM at step t, and τ denotes the entropy threshold. Classifier-Free Guidance in LLMs. Originally proposed for diffusion models, the CFG improves conditional generation by incorporating unconditional predictions. During training, diffusion models include both conditional and unconditional objectives; at inference time, they combine these two predictions to enhance controllability. In the context of LLMs, no additional unconditional training objective is introduced. Instead, CFG is applied only during inference to adjust conditional generation (Sanchez et al., 2023). The formulation is given by: log ˆP (xt c, c) = (1 ω) log (xt c, x<t) + ω log (xt c, x<t) . (1) x<t denotes the 1 previously generated logits. can be used to steer the LLMs to produce outputs that deviate from c. Token entropy. The token entropy is derived from the Shannon entropy (Shannon, 1948). It can be used to measure an LLMs confidence in its predicted output logits: lower entropy indicates greater certainty, while higher entropy indicates lower certainty. The formula is as follows: Ht = (cid:88) i=1 pi log pi, (2) where [P1, . . . , PV ]t = softmax (xt) and denotes the size of the LLMs vocabulary. In step t, the KV KV cache in LLM-CFG. cache removes full-prefix attention recomputation by reusing cached Key/Value. In LLM-CFG, two separate KV caches are maintainedone for conditional prediction and one for unconditional predictionto enable faster inference. When producing logits xt, the two KV caches contain the following, respectively: Cachec = [c, x1, . . . xt1] , Cachec = [c, x1. . . xt1] . (3) For clarity, we do not separate keys and values; for example, denotes the K/V cached from processing c. Fig. 2 use the same convention."
        },
        {
            "title": "3.2 Selective CFG intervention",
            "content": "LLMs often display chain instability in multi-step reasoning: uncertainty at few steps amplifies and derails the entire answer. To locate where this instability originates, The inset panel in the upper-right corner of Fig. 1 shows that questions answered incorrectly have markedly higher answerlevel entropy than those answered correctly. The main panel further indicates that the gap is driven by high-entropy tokens (entropy > 0.5) in erroneous responses. These findings suggest that overall failure is concentrated at small set of critical high-uncertainty nodes and that stabilizing these nodes can yield gains. Based on this observation, Fig. 2b proposes selective CFG intervention strategy. During decoding, we monitor token entropy and intervene only when it exceeds threshold τ : positions with entropy τ proceed normally, whereas positions with entropy > τ receive CFG to reduce local uncertainty and prevent error propagation. Compared with the vanilla CFG baseline in Fig. 2a, which applies guidance uniformly at every step, our approach activates selectively at unstable nodes, concentrating guidance where it matters most and improving both stability and efficiency."
        },
        {
            "title": "3.3 Lightweight negative-prompt guidance",
            "content": "However, as shown in Fig. 2a, we observe that using CFG requires maintaining two sets of KV caches. Even though our method does not apply CFG to every token, one must still keep two KV caches to simultaneously preserve the contexts of the conditional and unconditional predictions. This substantially degrades efficiency in modern LLM inference accelerators; maintaining dual KV caches weakens the long-context capability of frameworks such as vllm1. Moreover, CFG originates from 1https://github.com/vllm-project/vllm 4 Method DI VC Ours τ 0.1 0.5 1.0 1.5 2.0 Qwen3-8B Qwen3-14B Qwen3-32B 73.75 / 78.33 / 73.34 / 100% 75.42 / 100% 72.92 / 100% 79.58 / 76.60 / 37.8% 79.58 / 40.0% 82.92 / 41.7% 77.08 / 22.5% 80.83 / 24.4% 84.58 / 25.7% 75.42 / 8.0% 81.67 / 8.2% 82.92 / 9.0% 78.34 / 1.9% 78.33 / 2.0% 82.50 / 2.2% 75.00 / 0.3% 80.00 / 0.3% 77.92 / 0.3% Table 1: Comparison of AIME2024 accuracy and CFG usage (%) under different entropy thresholds τ for Qwen3-Reasoning series models. Each cell reports the results in the format AIME2024 / CFG Usage, where the left value indicates the accuracy (%) on the AIME2024 benchmark, and the right value denotes the proportion of CFG tokens used during inference. computer vision, where unconditional labels are introduced during training to learn the global data distribution and can target either the global data space or negative-prompt space. By contrast, CFG in LLM is not trained to model global text distribution. We therefore argue that the unconditional branch should be treated as negative-prompt channel: the conditional branch focuses on generating correct tokens, while the unconditional branch is encouraged to produce wrong tokens. We contend that specifying an explicit negative task yields stronger guidance. Concretely, we assign the unconditional prediction to generate error tokens that characterize undesired outputs and use the CFG to push the conditional prediction away from them. To reduce memory, we reuse the conditional branchs KV cache and append short instruction that elicits error tokens (Fig. 2b). This design removes the need for separate unconditional KV cache and reduces the memory footprint. The injected phrase is flexibleshorter cues are faster. In our experiments, even the two-word cue Output Error already yields strong results."
        },
        {
            "title": "4.1 Experimental setup",
            "content": "Datasets. To assess robustness across diverse domains, we evaluate on three categories of benchmarks: general tasks (WinoGrande (Sakaguchi et al., 2021), MMLU-Pro (Wang et al., 2024)); coding tasks (HumanEval (Chen et al., 2021), HumanEvalPlus, LiveCodeBench (Jain et al., 2024)); and mathematics & science tasks (GPQADiamond (Rein et al., 2024), MATH500 (Lightman et al., 2023), AIME2024). We use 5 OpenCompass2 (Contributors, 2023) for consistent and fair evaluation; the corresponding test configuration files and all testing details are provided in Appendix A.1. Model selection. To evaluate the effectiveness of our method on both general supervised fine-tuning base models and reasoning models, we experiment with the Qwen3 (Yang et al., 2025) model family (Qwen3-8B, Qwen3-14B, and Qwen3-32B in both thinking and non-thinking modes). Baselines. We compare against the following baselines: (1) Direct Inference (DI): generate outputs directly from the LLM; (2) Vanilla CFG with negative prompt (VC): classifier-free guidance with specified negative prompt, set to the same prompt injected in our method. Following vanilla CFG (Sanchez et al., 2023), we set ω to 1.5 and use it as the default hyperparameter. Implementation details. All experiments are all set context length of 32768 to ensure that the models full capabilities are preserved in all datasets. To ensure the results reproducibility, we choose the greedy search decoding strategy. Additionally, to demonstrate that our method is effective under random sampling, we conduct experiments on AIME2024 using the Qwen3-Reasoning model. We set the temperature at 0.6, top-p at 0.95, and top-k at 20, and we perform eight independent inference runs with different random seeds and report the mean performance."
        },
        {
            "title": "4.2 Main Results",
            "content": "From Tab. 1 and 2, we observe consistent gains across architectures and entropy-threshold settings. These improvements manifest not only in the average accuracy but also in stability across diverse task domains, indicating that the proposed method generalizes well beyond specific architectures or task types. Applying CFG to only 4.2% of tokens on Qwen3-8B-Base improves accuracy from 53.88 to 55.23, and using 3.5% on Qwen3-14B-Reasoning increases it from 73.97 to 75.55. This suggests that selective CFG activationrather than frequent usageyields an optimal balance between diversity and control, effectively enhancing reasoning quality without over-constraining the generation. The method also strengthens performance on specific benchmarks: GPQA-Diamond (+6.57) and MMLU-Pro (+1.86) demonstrate that CFG benefits both high-level reasoning and knowledge-intensive 2https://github.com/open-compass/opencompass Model Method τ General Math & Sci Coding WinoG MMLU-P GPQA-D MATH500 HEval HEval+ LiveCodeBench DI VC Ours DI VC Ours DI VC Ours DI VC Ours DI VC Ours DI VC Ours - - 0.1 0.5 1.0 1.5 2.0 - - 0.1 0.5 1.0 1.5 2.0 - - 0.1 0.5 1.0 1.5 2. - - 0.1 0.5 1.0 1.5 2.0 - - 0.1 0.5 1.0 1.5 2.0 - - 0.1 0.5 1.0 1.5 2.0 61.17 61.17 59.19 60.62 61.25 61.40 60.62 65.27 62.19 61.56 61.64 64.40 68.19 65.11 71.67 71.03 69.93 71.51 70.72 70.88 72. 75.14 66.22 65.98 70.01 73.64 75.30 75.61 65.11 62.67 63.69 64.09 64.72 65.11 65.51 70.96 64.96 66.61 65.75 68.19 70.80 70.56 56.81 50.28 52.21 52.17 56.17 57.21 56.94 70.52 70.19 70.69 71.91 72.28 72.38 70.96 64.30 63.34 64.67 64.01 64.72 64.82 64. 75.61 74.12 74.86 74.63 76.12 75.91 75.64 68.18 62.90 64.21 64.37 65.71 67.79 68.41 76.76 74.96 76.56 77.15 78.47 77.80 76.86 - B 8 - 3 Q - i a 8 - 3 Q - B 4 1 - 3 Q - n e 4 1 - 3 Q - a 2 3 - 3 Q - n e 2 3 - 3 Q 41.41 39.90 39.39 41.41 47.98 46.97 47.47 57.07 57.58 54.55 60.61 60.10 61.11 56.06 45.45 39.90 44.95 46.46 45.96 49.49 50. 57.58 60.10 64.14 64.14 62.12 62.12 61.62 50.00 46.46 49.49 45.96 48.99 52.02 52.53 61.62 56.57 64.65 65.66 65.66 60.61 61.62 82.60 82.40 86.40 85.00 86.20 84.40 83.80 96.80 92.60 96.80 97.00 97.00 95.80 96.80 86.60 87.00 87.60 87.00 88.40 86.40 88. 96.40 94.80 97.00 97.20 97.00 97.60 97.20 86.60 84.80 86.00 84.40 87.80 86.40 87.00 97.00 94.00 98.40 98.40 97.40 96.20 96.00 86.59 87.80 88.41 87.80 85.98 86.59 86.59 87.20 90.85 92.07 95.12 90.85 89.02 88.41 88.41 87.80 91.46 89.63 88.41 88.41 87. 92.07 90.85 95.73 96.95 95.12 95.73 93.90 89.63 90.85 93.29 89.63 89.63 91.46 90.85 97.56 95.12 96.34 98.17 98.17 97.56 96.34 77.44 71.95 75.61 76.22 79.88 79.27 78.05 67.07 65.85 74.39 69.51 67.07 65.24 65.85 77.44 83.54 80.49 82.32 78.05 76.83 78. 70.12 69.51 72.56 75.00 75.61 69.51 70.73 82.32 81.10 82.93 83.54 80.49 82.93 82.32 97.56 95.73 97.56 98.17 97.56 97.56 98.17 50.25/41.34/64.71 50.75/41.54/62.22 49.75/41.96/67.87 52.50/41.13/65.61 52.75/41.13/64.93 51.50/41.34/67.65 51.25/41.54/66.52 79.50/97.70/79.41 84.75/93.74/59.50 88.00/96.45/63.80 87.25/98.12/68.10 88.00/97.91/79.19 84.75/97.49/79.19 81.50/97.70/81.00 56.25/51.36/75.79 58.50/51.77/72.85 56.75/50.73/72.40 55.75/50.73/74.66 55.25/51.77/76.02 56.25/51.57/74.43 56.00/52.19/72. 86.00/98.54/84.39 89.00/98.96/71.72 90.25/97.91/71.72 91.50/97.91/72.17 90.50/97.49/84.62 88.50/97.70/87.10 88.00/98.33/82.58 58.50/55.32/77.38 56.00/52.19/75.11 59.00/52.82/78.51 59.75/53.03/75.57 59.25/54.28/76.92 58.75/53.65/76.70 59.75/53.65/77.60 91.00/99.16/84.39 87.25/98.54/78.73 92.00/98.75/78.73 90.75/98.54/79.41 91.25/99.16/87.10 89.75/98.75/82.35 90.00/98.75/82.35 Avg 53.88 52.90 53.91 54.03 55.15 55.23 54.97 70.39 67.93 69.32 71.08 72.19 71.99 70. 59.98 59.13 59.83 59.97 60.14 60.25 60.37 73.97 71.75 73.07 73.83 75.18 75.55 74.76 61.19 59.00 60.78 59.64 60.81 61.32 61.70 75.38 72.24 74.67 74.87 76.16 74.87 74.72 CFG Usage - 100% 35.4% 24.4% 8.5% 4.2% 1.0% - 100% 50.5% 30.5% 9.8% 3.3% 0.4% - 100% 38.4% 24.9% 10.8% 3.8% 1.0% - 100% 48.6% 32.8% 10.3% 3.5% 0.3% - 100% 45.6% 33.0% 16.7% 7.4% 2.4% - 100% 49.1% 31.2% 13.1% 3.4% 0.7% Table 2: Performance comparison of Qwen3-Base and Qwen3-Reasoning models on benchmarks across three categories: General tasks (WinoGrande, MMLU-Pro), Mathematics & Science Tasks (GPQA-Diamond, MATH500), and Coding tasks (HumanEval, HumanEval+, LiveCodeBench). Different methods (DI, VC, and ours under various entropy thresholds) are reported. Avg denotes the average performance across all listed tasks. CFG Usage indicates the ratio of CFG used during token generation. tasks. Such cross-domain improvements imply that the method enhances not only local decoding stability but also the global coherence of reasoning chains. Notably, invoking CFG for merely 0.7% of tokens on Qwen3-32B-Reasoning still boosts accuracy from 97.56 to 98.17, revealing that even minimal CFG intervention can guide the model toward more reliable outputs. On AIME2024 (Tab. 1), the 6 Figure 3: Ablation study results on the selection of (a) entropy threshold, (b) ω in classifier-free guidance and (c) negative prompt in our method for Qwen3-8B. (a) Base (Before-CFG). (b) Base (After-CFG). (c) Reasoning (Before-CFG). (d) Reasoning (After-CFG). Figure 4: Word clouds on GPQA-Diamond comparing Qwen3-8B. We consider the subset of questions that are incorrect under direct inference but become correct after applying our method. Words are extracted from the generated answers, and we visualize only words which the CFG is invoked; word size is proportional to frequency. gains are more pronounced: +4.59 points at 1.9% usage on Qwen3-8B-Reasoning, +3.34 at 8.2% on Qwen3-14B-Reasoning, and +5.00 at 25.7% on Qwen3-32B-Reasoning, indicating robustness under random sampling. These results collectively confirm that CFG offers scalable and efficient mechanism for improving reasoning fidelity with minimal inference overhead."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "Unless noted, experiments use Qwen3-8BBase/Reasoning on GPQA-Diamond with τ = 1.5, ω = 1.5, and negative prompt OUTPUT ERROR; non-ablated hyperparameters are fixed. Fig. 3a presents an ablation on the entropy threshold. The curve is computed by averaging the accuracies of Qwen3-8B reported in Tab. 2. As the entropy ratio increases, the performance exhibits unimodal pattern: it first rises and then falls. Qwen3-8B-Base attains maximum accuracy of 55.23 at an entropy threshold of 1.5, whereas Qwen3-8B-Reasoning reaches its maximum at 1.0. This trend is intuitive: when the threshold is too low, the method over-modifies low-entropy (highconfidence) tokens, destabilizing otherwise reliable parts of the reasoning chain; when the ratio is too high, too few tokens are modified and many high-entropy tokens remain untouched, so crucial uncertain steps are not corrected, leading to degraded performance. Fig. 3b examines the effect of ω. Qwen3-8B-Base attains maximum accuracy of 47.47 when ω = 1.2, whereas Qwen38B-Reasoning reaches maximum of 62.12 when ω = 1.8, indicating that appropriate tuning of ω yields better performance. Fig. 3c reports an ablation on the choice of negative prompts. We observe that negative prompts generally exert stronger effects than their positive counterparts; for example, using OUTPUT ERROR with Qwen3-8BReasoning achieves 61.11, outperforming OUTPUT TRUE at 59.09. This is because effective negative prompt injection encourages the unconditional branch to construct more informative unconditional space, which CFG then leverages to steer the conditional branch toward better outputs. Moreover, OUTPUT TRUE at 59.09 still surpasses direct inference at 57.07 (see Tab. 2), since injecting OUTPUT TRUE perturbs the models ongoing generation and thus induces an unconditional space; however, this space is slightly inferior to that induced by OUTPUT ERROR, leading to weaker gains."
        },
        {
            "title": "4.4 Analysis",
            "content": "Not all prediction can be changed in vanilla CFG. Here, we reinterpret why our method works 7 Comparing Reasoning Outputs on Math Problem. Consider the function (x) = (cid:40) ax2 if a, ax + 2a if < a, where is real number. What is the largest value of such that the graph of = (x) intersects every horizontal line at least once? Please reason step by step, and put your final answer within (). Direct Inference: Upper bound miscalculated; lost the +2a term. ... (x) = ax + 2a, < gives range (, a2), hence is (1) Vanilla CFG: Incorrect reasoning; Repeated generation. ... So for any = 0, the function is surjective onto R. But wait... Let me think again... But wait... Let me think again... Ours: Correct bound and inequality; final answer is correct. ... union is (, a2 + 2a) [a3, ), so the maximum is (2) Figure 6: Case study: direct inference vs. vanilla CFG vs. our method on Qwen3-8B-Base. model, we observe shift away from sequential connectives (e.g., so) toward more diverse connectives, such as adversative conjunctions (e.g., however, if, perhaps, alternatively, wait). This helps the model abandon erroneous prior chains and initiate new lines of thought that lead to correct inferences. Overall, applying our method yields more varied and balanced vocabularies, which expand next-token hypotheses and reasoning trajectories, resulting in better outcomes. Case study. As shown in Fig. 6, we observe two representative failure modes. Direct Inference: symbolic information losswhen rewriting the upper bound of the piecewise range, the constant term +2a is dropped, yielding an underestimated bound and wrong interval. Vanilla CFG: the main issue is looped generation: the model repeatedly oscillates between claims (e.g., surjectivity onto R) and retractions without verifiable stopping criterion, and it neglects boundary conditions of the piecewise form. This looping arises from applying the CFG to all tokens, which can induce reasoning even for low-entropy tokens, thereby disrupting an otherwise stable reasoning process. Ours: By enforcing explicit boundary/inequality checks and composing the ranges in unified manner, it avoids these pitfalls and produces the correct final answer."
        },
        {
            "title": "5 Conclusion",
            "content": "We present MTI, minimal training-free test-time intervention for LLMs. Using token entropy to Figure 5: Results on HumanEvalPlus using Qwen3-8BBase with vanilla CFG. Top: tokens where CFG failed to change the prediction; bottom: tokens successfully changed. x-axis: token entropy; y-axis: count of tokens per entropy bin. by examining properties of vanilla CFG. Empirically, applying CFG to all tokens does not necessarily change the current prediction. We attribute this to the discreteness of CFG in LLM decoding: when the logits have sufficiently low entropy, the distribution typically contains single dominant mode, in which case CFG is unlikely to alter the final prediction. Moreover, low-entropy logits indicate that the model is already confident about the token, making the prediction naturally resistant to CFG. To further analyze this phenomenon, we conduct the experiment in Fig. 5. We partition logits into two sets: those whose predictions are successfully changed by vanilla CFG and those that are not, and then visualize the entropy distributions of both sets. From the top panel of Fig. 5, we observe that the vast majority of tokens are not changed by CFG, and among these unchanged tokens, lowentropy cases dominate. In contrast, the bottom panel shows that tokens successfully steered by CFG are mostly high-entropy. These observations suggest that entropy can serve as tool for whether given logits distribution is suitable for CFG-based modification. Consequently, it is reasonable to skip CFG when entropy is low, which provides principled justification for our entropy-based decision of whether to apply CFG. Word-cloud analysis. As shown in Fig. 4, for the base model, the post-CFG clouds exhibit higher lexical diversity: low-information placeholders (e.g., **) are replaced by more semantically meaningful words, enabling richer set of reasoning branches and ultimately higher accuracy. For the reasoning 8 localize unstable positions and applying classifierfree guidance only at those tokens stabilizes the reasoning trajectory while incurring only minimal inference overhead. We further introduce the negative-prompt injection via KV-cache reuse, which efficiently approximates the unconditional branch without extra cache budget. Experiments on general, coding and mathematics&science tasks show consistent improvements, with meaningful gains even when intervention is activated in few tokens. The proposed approach is plug-and-play, easily integrates with modern acceleration frameworks and diverse decoding strategies, and provides simple yet powerful tool for enhancing reasoning reliability at inference time."
        },
        {
            "title": "References",
            "content": "Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Felix Koulischer, Johannes Deleu, Gabriel Raya, Thomas Demeester, and Luca Ambrogioni. 2024. Dynamic negative guidance of diffusion models: Towards immediate content removal. In Neurips Safe Generative AI Workshop 2024. Ayeong Lee, Ethan Che, and Tianyi Peng. 2025. How well do llms compress their own chain-ofarXiv thought? preprint arXiv:2503.01141. token complexity approach. Zeju Li, Jianyuan Zhong, Ziyang Zheng, Xiangyu Wen, Zhijian Xu, Yingying Cheng, Fan Zhang, and Qiang Xu. 2025. Compressing chain-of-thought in llms via step entropy. arXiv preprint arXiv:2508.03346. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, and Zhongxin Liu. 2025. Parallel scaling law for language models. arXiv preprint arXiv:2505.10475. OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Tri Dao, Daniel Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. arXiv e-prints, pages arXiv2407. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Jonathan Ho and Tim Salimans. 2022. Classifierpreprint guidance. arXiv diffusion free arXiv:2207.12598. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, and 1 others. 2024. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173. Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. 2025. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion In 2025 IEEE/CVF Winter Conference models. on Applications of Computer Vision (WACV), pages 20632072. IEEE. Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, and Jing Shao. 2025. Demystifying reasoning dynamics with mutual information: Thinking tokens are information peaks in llm reasoning. arXiv preprint arXiv:2506.02867. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106. Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, and Stella Biderman. 2023. Stay on topic with classifierfree guidance. arXiv preprint arXiv:2306.17806. Claude Shannon. 1948. mathematical theory of communication. The Bell system technical journal, 27(3):379423. 9 Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, and 1 others. 2024. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652. Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. 2023. Draft & verify: Lossless large language model acceleration via self-speculative decoding. arXiv preprint arXiv:2309.08168."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Testing Setup and Files OpenCompass configuration files used in our main results. For our main results, we primarily used the following OpenCompass configuration files: gpqa_gen.py math_500_gen.py aime2024_gen.py humaneval_gen.py humaneval_plus_gen.py livecodebench_gen.py hellaswag_gen.py winogrande_gen.py mmlu_pro_gen.py During evaluation, the MMLU-Pro score is computed as the average over all subsets. For AIME2024, we conduct eight runs and report the mean score. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Thomas Lieberum, Yuan Le, Deepak Gopinath, Karthik Ramesh, and 1 others. 2023. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, and 1 others. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, and 1 others. 2025. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, and 1 others. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. Heming Xia, Chak Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li. 2025. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067. Minghao Yan, Saurabh Agarwal, and Shivaram Venkataraman. 2024. Decoding speculative decoding. arXiv preprint arXiv:2402.01528. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Zhen Yang, Ganggui Ding, Wen Wang, Hao Chen, Bohan Zhuang, and Chunhua Shen. 2023. Object-aware inversion and reassembly for image editing. arXiv preprint arXiv:2310.12149. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Karthik Narasimhan, Mohit Hodosh, Eric Edwards, Jieyu Xu, Yuan Zhao, and Karthik Duan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems."
        }
    ],
    "affiliations": [
        "AIML",
        "Ant Group",
        "HKUST",
        "HKUST(GZ)",
        "Kuaishou Technology",
        "ZJU"
    ]
}