{
    "paper_title": "On Large Multimodal Models as Open-World Image Classifiers",
    "authors": [
        "Alessandro Conti",
        "Massimiliano Mancini",
        "Enrico Fini",
        "Yiming Wang",
        "Paolo Rota",
        "Elisa Ricci"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional image classification requires a predefined list of semantic categories. In contrast, Large Multimodal Models (LMMs) can sidestep this requirement by classifying images directly using natural language (e.g., answering the prompt \"What is the main object in the image?\"). Despite this remarkable capability, most existing studies on LMM classification performance are surprisingly limited in scope, often assuming a closed-world setting with a predefined set of categories. In this work, we address this gap by thoroughly evaluating LMM classification performance in a truly open-world setting. We first formalize the task and introduce an evaluation protocol, defining various metrics to assess the alignment between predicted and ground truth classes. We then evaluate 13 models across 10 benchmarks, encompassing prototypical, non-prototypical, fine-grained, and very fine-grained classes, demonstrating the challenges LMMs face in this task. Further analyses based on the proposed metrics reveal the types of errors LMMs make, highlighting challenges related to granularity and fine-grained capabilities, showing how tailored prompting and reasoning can alleviate them."
        },
        {
            "title": "Start",
            "content": "On Large Multimodal Models as Open-World Image Classifiers Alessandro Conti1,* Massimiliano Mancini1 Enrico Fini2, Yiming Wang3 Paolo Rota1 Elisa Ricci1,3 1University of Trento 2Independent researcher 3Fondazione Bruno Kessler 5 2 0 2 M 7 2 ] . [ 1 1 5 8 1 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Traditional image classification requires predefined list of semantic categories. In contrast, Large Multimodal Models (LMMs) can sidestep this requirement by classifying images directly using natural language (e.g., answering the prompt What is the main object in the image?). Despite this remarkable capability, most existing studies on LMM classification performance are surprisingly limited in scope, often assuming closed-world setting with predefined set of categories. In this work, we address this gap by thoroughly evaluating LMM classification performance in truly open-world setting. We first formalize the task and introduce an evaluation protocol, defining various metrics to assess the alignment between predicted and ground truth classes. We then evaluate 13 models across 10 benchmarks, encompassing prototypical, non-prototypical, finegrained, and very fine-grained classes, demonstrating the challenges LMMs face in this task. Further analyses based on the proposed metrics reveal the types of errors LMMs make, highlighting challenges related to granularity and fine-grained capabilities, showing how tailored prompting and reasoning can alleviate them. Code is available at https://github.com/altndrr/lmms-owc. 1. Introduction Image classification aims to assign label to an image. This widely studied task relies on key assumption: the categories are fixed and known in advance, setting known as the closed world. However, the latter is often restrictive in real-world applications where new categories can emerge, requiring to expand the label set [5], recognize unseen concepts [24], or both [6]. Despite its limitations, this assumption has historically been useful, enabling supervised training and straightforward evaluation on labeled datasets. With the rise of Large Multimodal Models (LMMs) [3, 38, 41] *Correspondence to: alessandro.conti-1@unitn.it. Enrico Fini is currently at Apple Figure 1. We extensively test 13 Large Multimodal Models (LMMs) for Open-World (OW) classification on 10 datasets using four evaluation metrics. We show that LMMs outperform contrastive-based approaches in OW (CaSED [17], and CLIP [49] with image-to-text retrieval) but still lag behind closedworld models with fixed categories (CLIP [49], dashed line). processing images and text, this constraint is no longer necessary. Instead of choosing from fixed list, LMMs can answer open-ended prompts such as What is the object in the image?, recognizing virtually any semantic concept. From this perspective, closed-world classification is an artificial limitation that restricts models expressive capabilities rather than reflecting its true potential. While some studies have explored classification with LMMs, they have either focused on the closed-world setting [42] or relied on limited metrics to assess performance: checking whether the ground truth label appears in the models prediction [69]. However, this metric provides limited view of classification performance. It fails to account for alternative correct answers (e.g., sofa instead of couch), while also overlooking real mistakes (e.g., confusing can with trash can). Evaluating models in the open world presents additional challenges, as predictions may differ in granularity (e.g., dog vs. pug), or conflict with an1 notation ambiguities (e.g., bedroom vs bed). These issues highlight the need for more comprehensive evaluation framework to assess the open-world capabilities of LMMs. In this work, we address this gap by formalizing the Open-World (OW) classification task and introducing four complementary metrics: (i) text inclusion [69], evaluating string matching, (ii) Llama inclusion which leverages Llama [25], to distinguish good and bad responses, as in LLM-as-a-judge [71], (iii) semantic similarity [17] between text embeddings of predictions and ground truth, and (iv) concept similarity, doing the same at the level of sentence parts. Using these metrics, we evaluate 13 models across 10 benchmarks spanning different levels of granularity, from prototypical, coarse categories (e.g., Caltech101 [23]) to fine-grained (e.g., Flowers102 [46]), very fine-grained (e.g., Stanford Cars [33]), and non-prototypical ones (e.g., DTD [16]). Our results (aggregated in Fig. 1) show that these models often predict semantically related concepts (even better than previous, contrastive-based alternatives [17]), demystifying the skepticism against LMMs on OW classification. However, LMMs also make notable errors, being far from closed-world baselines. While the challenging nature of the problem makes analyzing the severity of the mistakes hard, we find that different behaviors on different metrics can pinpoint sources of errors. In particular, mismatches in concept similarity and Llama inclusion uncover errors in the granularity of the predictions (i.e., correct but generic) or very similar categories easy to confuse (i.e., wrong but specific). We show how the former can be addressed via tailored prompting, while the latter via implementing reasoning strategies. Finally, we further analyze cases where the metrics identify mistake and, using tagging models [28], check if predictions are incorrect only due to the single-label nature of the datasets. Contributions. To summarize, our contributions are: We formalize comprehensive evaluation protocol for the task of open-world classification with LMMs, using 4 different metrics capturing both semantic and text alignment of predictions with the ground truth. We perform the first, large-scale assessment of LMMs on this task, using 13 models on 10 benchmarks, showing promising results yet multiple challenging cases. By combining the different metrics, we investigate the root of the models mistakes, identifying various issues (e.g., wrong granularity, fine-grained discrimination, labeling ambiguities) and showing how changes in the models (e.g., prompts, reasoning) can reduce them. We use these results to draw conclusions on the source of errors that future research should account for when using these models, releasing our evaluation suite to encourage future research efforts on addressing them. 2. Related Work Large Multimodal Models. While early large visionlanguage models aligned visual and text representation in shared embedding space [49, 68], there has been an increasing effort in developing generative multimodal models [2, 38, 53, 74]. These models process an input image and text generating either text [38, 41] or multimodal [31, 70] output. While these models share common components (e.g., visual and text encoders, text decoders) they differ in the specific strategies for modality alignment (e.g., MLP projector [41], Q-former [38]), pretraining (e.g., autoregressive [41], alignment [38]), finetuning (e.g., supervised, instruction tuning) but also data source (e.g., web data [34], textbook-style [1]) and structure (e.g., captioning [19], interleaved image-text [34]). In this work, we do not aim to introduce new LMM or novel methodologies for building LMMs. Instead, we focus on evaluating how these models perform in OW classification, testing 13 different models belonging to 8 different families [1, 3, 13, 19, 34, 36, 37, 41], covering multiple architectural, data, and design choices. Classification with LMMs. Multiple works designed benchmarks to test the general capabilities [35, 39, 43], or shortcomings [27, 40, 67, 73] of LMMs. The most closely related works to ours are [42, 66, 69], investigating their classification performance. Yue et al. [66] developed an approach exploiting the next token prediction probability of an LMM, reporting results on multi-label recognition. Zhang et al. [69] tested multiple LMMs on both closedworld and OW settings, showing how data influences their performance and that generative LMMs underperform their contrastive counterpart. This latter finding is challenged by Liu et al. [42], who extended the analyses of [69] to multiple datasets and more recent models. However, [42] focused on closed-world classification, while [69] limited the analyses on OW to 4 datasets and single metric (i.e., text inclusion). In this work, we expand existing analyses in OW classification with LMMs, providing the largest study up-to-date in terms of datasets (10) and models (13). We also analyze the performance of LMMs according to four different metrics, capturing complementary aspects. Moreover, we use these metrics to analyze LMM mistakes in this scenario. Analyzing model failures. There has been growing interest in studying what type of mistakes models make. For instance, works on failure modes detection studied how to identify slices of data on which models underperform [22, 55, 64] and, through the use of LMMs, these slices can be also interpreted via natural language [18, 20, 30]. Other works examined the models mistakes on specific datasets, to understand what prevents them from achieving perfect performance and to provide guidelines for future works. This has been the case for ImageNet [51], where previous studies discovered problems linked to spurious correla2 tions [45, 54], fine-grained discrimination [48, 59], but also labeling itself [7]. Our work is similar to this latter trend, as we want to investigate what type of mistakes LMMs make when classifying images in the OW. We aim for our findings to serve as foundation for future research focused on improving the performance of LMMs in this challenging task. between the prediction and the ground truth. Below, we describe the four metrics we consider for this task. Text inclusion (TI). This metric, adopted in [69], refers to whether the ground truth is contained in the models prediction. Specifically, let us define as the ground truth and as ˆy the models prediction. Text inclusion score is defined as: 3. Benchmarking LMMs in OW Classification In this section, we first formalize the setting of OW classification with LMMs, clarifying its goal and terminology w.r.t. related works (Sec. 3.1). We then discuss how to evaluate performance in this setting, describing the different metrics and what they capture (Sec. 3.2). Last, we provide details on the datasets and models considered in our analyses (Sec. 3.3) before showing their results (Sec. 3.4). 3.1. Preliminaries Classification with LMMs. Let us define an LMM as function fLMM generating text output in the space given an image in the space and text query , i.e., fLMM : . To perform classification with LMMs, the query contains prompt of the type What type of object is in this image? and we expect the output to be semantic class . In the case of closed-world classification, we have predefined list of classes and we modify by specifying the set (e.g., via multi-choice question). In OW we let the LMM predict naturally on its original output space , without any constraint. As consequence, the model can pick from the set of all possible semantic concepts, with and Y. Relationships with prior problem definitions. While we followed [69] and used open-world to define this setting, the term can be ambiguous. The traditional definition of OW recognition [6] refers to different problem, where model trained to recognize classes should recognize whether an instance belongs to an unknown one / and learn to recognize u. Other works refer to this task as vocabularyfree classification [17] due to the absence of predefined vocabulary, open-ended recognition [65] due to the lack of constraints, or avoided any specific terminology in the context of multi-label recognition [66]. While these different definitions closely relate to each other, we follow [69], clarifying that OW here refers only to the lack of constraints in the output space of the LMMs. 3.2. Metrics Evaluating open-world recognition with LMMs is challenging as, even if we have ground truth, we have no guarantee that the model will output the same name when correct (e.g., sofa vs couch), especially as the model may produce an undesired wordy output (e.g., the object in the image is sofa). These potential variations ask for specific evaluation criteria, accounting for different types of (mis)alignment TI(y, pr) = (cid:26)1 0 if ˆy, otherwise (1) where, in this context, refers to string inclusion. This metric assesses whether the predictions strictly adhere to the ground truth label but over-penalizes whether the two are semantically coherent (e.g., the prediction labrador would be considered wrong for the label labrador dog). Llama inclusion (LI). Differently from TI, this metric evaluates whether the prediction aligns with the ground truth label based on Large Language Model (LLM) internal knowledge. Specifically, we employ Llama 3.2 3B [58] and report the prompt we use in the Supp. Mat. (see A.2). The score is 0 or 1, depending on the LLMs answer. This is similar to methods that use LLM/LMMs-as-a-judge [9, 71], but is specifically adapted to OW classification. Semantic similarity (SS). Unlike previous metrics that assess alignment with the ground truth in binary manner, SS captures the degree of semantic similarity on continuous scale between 0 and 1. To achieve this, we employ semantic similarity metric. Following [17], we define similarity as gemb(ˆy), gemb(y), where gemb is text embedding function, and , denotes cosine similarity. As in [17], we use Sentence-BERT [50] for computing embeddings. Concept similarity (CS). By considering the prediction as whole, the semantic similarity previously defined ignores whether parts of the sentence (e.g., elephant) are closer to the ground truth (e.g., animal) than the sentence as whole (e.g., photo of an elephant in the room). To address this, we consider CS as an additional metric, defining it as: max psplit(ˆy) gemb(p), gemb(y) (2) where split is sentence splitting procedure that, in our case, is implemented via spaCy 1. 3.3. Dataset and Models Datasets. Following previous works [17, 52, 72], we analyze four different challenges: coarse-grained (or prototypical), non-prototypical, fine-grained, and very fine-grained classification. For the prototypical classification, we include standard benchmarks such as Caltech101 [23] for objects and SUN397 [62] for places. The non-prototypical set comprises datasets that either lack nouns or involve 1We use the model available at https://spacy.io/models/ en#en_core_web_lg 3 Prototypical Non-prototypical Fine-grained Very fine-grained Model TI LI SS IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B 30.8 29.7 36.9 36.3 40.6 34.6 41.7 39.5 34.4 30.8 34.1 44.9 46.4 52.7 56.3 69.9 68.5 74.4 63.1 73.9 72.8 64.4 53.2 60.1 77.8 78.7 54.5 56.8 46.9 46.5 48.2 45.3 45.9 46.2 54.0 56.1 47.7 52.2 51.9 CS 63.1 64.0 70.4 70.8 74.0 65.8 74.3 73.2 67.3 62.0 65.1 74.7 76.0 TI LI SS 3.7 6.0 10.2 10.1 11.0 8.6 11.3 10.6 7.3 7.2 6.0 7.8 10.3 27.9 27.1 45.2 42.1 46.2 44.3 46.8 45.9 37.0 28.1 28.7 34.3 42. 35.4 37.0 31.6 30.8 31.9 33.0 31.2 31.1 32.8 31.6 26.0 27.7 30.8 CS 41.3 42.0 53.4 53.1 53.9 49.5 54.4 54.2 47.0 43.8 39.5 42.7 49.8 TI LI SS 3.0 10.4 14.9 16.2 22.3 8.4 26.8 16.9 6.0 6.4 13.4 35.7 34.6 49.9 48.8 47.0 44.4 46.7 46.5 43.7 44.5 42.7 40.4 49.1 62.5 64.0 38.0 35.6 31.6 32.0 34.8 28.2 35.3 32.2 38.5 39.0 31.8 40.7 39.2 CS 41.7 47.2 50.7 52.0 56.7 44.8 60.1 53.2 43.3 43.8 47.2 63.4 62.9 TI LI SS 0.0 0.0 0.7 1.7 2.3 0.0 1.4 1.3 0.6 0.0 0.2 12.9 0.8 67.0 61.0 32.9 36.8 32.5 41.0 47.2 42.2 65.6 76.7 45.0 60.7 63.0 29.6 30.0 33.1 33.8 36.0 28.6 34.2 34.5 30.5 31.9 28.9 45.1 34.5 CS 33.6 34.3 43.9 44.2 49.4 37.6 46.9 46.1 37.1 32.4 36.0 62.3 43.4 Open-world baselines CASED [17] CLIP retrieval Closed-world baselines CLIP [49] SigLIP [68] 24.5 28.6 46.3 42.9 58.9 40. 59.8 60.6 5.4 7.5 18.6 24.6 41.8 28.1 42.4 43.4 27.4 32. 46.6 45.4 60.7 42.9 61.7 65.4 0.7 7.0 47.1 18.1 38.5 39. 38.5 56.1 76.4 81.8 91.5 90.5 56.0 61.7 73.6 76.1 85.0 92. 89.6 95.1 51.7 69.2 73.6 89.1 Table 1. OW results averaged on the grouped datasets. TI stands for text inclusion, LI for Llama inclusion, SS for semantic similarity, and CS for concept similarity. Higher is better, bold indicates best. non-standard domains. This includes DTD [16] (textures), UCF101 [56] (actions), and EuroSAT [26] (satellite images). The fine-grained set consists of datasets where classes belong to shared superclass and/or are challenging to distinguish. These include Flowers102 [46] (flowers), Food101 [8] (food), and OxfordPets [47] (animals). Finally, the very fine-grained set comprises datasets where categories are not only within the same subclass but also highly difficult to differentiate. This includes StanfordCars [33], where labels specify car brands, models, and years of production, and FGVCAircraft [44], which categorizes aircraft models. More details are in the Supp. Mat. (see A.1). Models. We perform our evaluation considering state-ofthe-art LMMs of 8 types, including Idefics2 [34], InstructBLIP [19], InternVL2 [12, 13], LLaVA-1.5 [41], LLaVANeXT [37], LLaVa-OV [37], Phi-3-Vision [1], Qwen2VL [60]. We choose these models as they are publicly available and widely adopted by the community. These models encompass different design choices such as the vision encoder (e.g., CLIP [49], SigLIP [68], BLIP-2 [38]), language model (e.g., Mistral [29], Vicuna [14], Qwen2 [60]), data types (e.g., instruction following, multilingual, textbookbased), and pretraining strategies (e.g., single vs multistage). Unless otherwise stated, we query the model with the same prompt of [69], i.e., What type of object is in this image?, letting the models perform unconstrained generation. We report summary of the models and their differences in the Supp. Mat. (see A.1). References. As reference, we consider baselines based on contrastive vision-language models. Specifically, we report results using CLIP [49] and SigLIP [68] in the closedworld setting, where the models have access to the list of target class names. Additionally, we include two baselines that adapt CLIP to the OW setting by formulating image classification as retrieval task. The first retrieves the closest caption from predefined database, while the second, CaSED [17], leverages retrieved captions to generate list of candidate classes for the final prediction. For both baselines, we use the same retrieval database as in [17]. 3.4. Are LMMs Good at OW Classification? In this section, we analyze the performance of LMMs in an OW setting, with results summarized in Tab. 1 by dataset groups with per-dataset results in the Supp. Mat., see A.3). Prototypical classification. LMMs perform best on prototypical classes, with high scores on inclusion and similarity metrics. They consistently outperform CaSED and CLIP retrieval on inclusion metrics and are generally comparable or superior on similarity scores. Non-prototypical classification. Performance drops significantly, with the highest LI score at 46.8, nearly 15 points lower than closed-world CLIP. Predictions are also less semantically indicative of the target class, with an average CS of 49.3, much lower than the prototypical case (69.2). Fine-grained classification. Greater variation is observed among different models, ranging from 41.7 to 63.4 in concept similarity. In this group, LMM predictions are slightly less accurate than those of CaSED and CLIP retrieval. Very fine-grained classification. Many models achieve TI of 0.0, except for Qwen2VL 2B, which scores 12.9 due to the exceptional performance of FGVAircraft (25.6 vs 4.6 for the second-best model). Most LMMs underperform CLIP retrieval in CS, suggesting an issue due to granularity. Overall trends. Across ten datasets, CLIP retrieval outper4 Figure 2. Per-image examples of model predictions. Bold indicates the ground truth class names. For visualization purposes, we show only the predictions with the highest/lowest concept similarity. Blue and red indicate positive and negative Llama inclusion values. Figure 3. Per-class examples of model predictions. Bold indicates the ground truth class names. On the x-axis we report the average LI, and on the y-axis the average CS. For visualization purposes, we show the most frequent concepts predicted for each quadrant. forms 9/13 models in TI, but LMMs consistently achieve higher Llama inclusion scores. CaSED ranks highest in semantic similarity due to its concise responses, while CLIP retrieval remains competitive. These results confirm insights from previous works, such as the influence of data exposure on coarse-grained categories [42, 69]. Additionally, stronger language models (e.g., Mistral, Qwen) tend to yield better results than weaker counterparts (e.g., Vicuna). LMMs generally outperform contrastive models in OW classification, leading in 11/16 metric/groups. However, top-performing models in one metric may struggle in othersfor example, Qwen2VL 7B excels in LI on very fine-grained datasets, while InternVL2 8B and LLaVA-OV (Qwen2 7B) show different strengths in prototypical classification: e.g., +21.2 LI of the first on the second but -7.9 SS. While these results are promising, there is still large gap with closed-world models, i.e., CLIP [49], SigLIP [68]. In the next sections, we further explore what the metrics capture, to better understand OW predictions. 3.5. Interpreting Model Predictions Through Inclusion and Similarity Scores To underscore the importance of jointly evaluating inclusion and similarity scores, we present qualitative results demonstrating how their combined analysis offers deeper insights into LMM failures. Fig. 2 showcases qualitative results from various datasets, displaying ground truth class labels alongside model predictions. For instance, the challenging case of caprese salad illustrates this distinction: more descriptive predictions like creamy sauce, which LI considers incorrect, receive relatively higher CS score than food, which is deemed correct by LI. This emphasizes that creamy sauce is semantically closer to the ground truth, yet it is rejected by LI due to its lack of alignment with the ground truth. Similar behavior is present in the other examples. To reinforce our previous point, Fig. 3 illustrates the relationship between LI and CS, highlighting the distinct contributions of these two metrics. Predictions in the top-right quadrant correspond to concepts that are semantically close to the ground truth and are also likely to be considered correct by LI (e.g., cellphone with predictions such as mobile phone and Nokia). In contrast, the bottom-left quadrant represents the opposite case. For instance, in the same plot, handheld devicewhile somewhat related to cellphone and receiving nonzero CS scoreis still deemed incorrect by LI. Similarly, in the Caesar salad example, the prediction food appears in the bottom-right quadrant, as it is correct but overly generic. Meanwhile, pasta salad, being more (a) Prototypical (b) Non-prototypical (c) Fine-grained (d) Very fine-grained Figure 4. Types of model predictions per dataset groups. Blue indicates correct and specific and correct but generic predictions, red indicates wrong but specific and wrong and generic mistakes. specific yet incorrect, falls into the top-left quadrant. 3.6. Grouping Model Predictions Following the intuition from above, we analyze the performance of LMMs defining four different groups of predictions: correct and specific, correct but generic (e.g., dog vs pug); wrong but specific, predicting classes semantically similar to the target (e.g., pug vs pomeranian); and wrong and generic i.e., where the prediction is semantically dissimilar from the target (e.g., sofa vs dalmatian). To define these groups, we split the model predictions into four sets by thresholding the LI and CS scores. We arbitrarily set the CS threshold at 0.6 to distinguish between generic and specific responses and the LI threshold at 0.5 to separate correct and wrong responses2. We visualize the ratios for the predictions in Fig. 4. Intuitively, good LMM should have an high amount of predictions as correct and specific. When not possible, however, having an equally high correct but generic ratio is still better than having errors of any form. In terms of optimal predictions, we see that the bestperforming models vary according to dataset groups. For prototypical classification, the models with the lowest error are InternVL 8B, Qwen2VL 2B, and Qwen2VL 7B. For non-prototypical tasks, instead, LLaVA 1.5 7B performs best, but InternVL 2B and InternVL 8B provide slightly more precise predictions. For fine-grained, trends are similar to the prototypical groups, but with fewer correct and more generic responses. This is most evident for Idefics2 8B, which works fairly well on fine-grained classification but provides responses lacking specificity. On very fine-grained, we perceive higher rates of wrong and generic, with more generic predictions across all models. Notably, Qwen2VL models perform better in the last two settings. On average, the models with the highest wrong predictions are LLaVA-OV 7B and InstructBLIP Vicuna 7B. The model that is, on average, more generic in its replies is Idefics2 8B. 2Note that LI is either 0 or 1 on per-sample basis, but it ranges between the two when considering aggregated results, e.g., average per class. Dataset Agreement High (%) Medium (%) Low (%) C101 S397 U101 FOOD DTD FLWR ESAT PETS CARS FGVC 71.4 34.3 33.8 32.6 23.3 13.9 6.1 5.5 1.5 0.1 15.8 33.0 26.8 27.5 29.1 25.5 21.8 16.1 21.9 4.0 12.8 32.8 39.5 39.9 47.6 60.6 72.1 78.4 76.6 96.0 Table 2. Agreement of LMMs correct predictions across datasets. Low indicates that less than 30% of the models predicted sample correctly, while high indicates that more than 70% did. Figure 5. Percentage of correct and specific predictions shared between models. Higher values indicate higher agreement. 4. Analyzing LMMs Mistakes in OW In the following, we further inspect the correct and wrong predictions of different models. Specifically, each section will analyze one of the four cases: correct and specific (Sec. 4.1), correct but generic (Sec. 4.2), wrong but specific (Sec. 4.3), and wrong and generic (Sec. 4.4). 4.1. Correct and Specific While this section describes successful cases, from Sec. 3.4 we know that models perform differently. Thus, here we investigate whether LMMs share similar success cases. Are correct predictions shared among models? To answer this question, we first evaluate the percentage of samples that receive correct predictions by multiple models across datasets. We report the results in Tab. 2, splitting them according to low (less than 30% of models), medium 6 (a) Be generic (b) Be specific (c) Domain-specific Figure 6. Average gains per prediction type when asking models to be more generic/specific (a, b), or via dataset-specific prompts (c). Blue indicates correct and specific and correct but generic predictions, red indicates wrong but specific and wrong and generic mistakes. (30%-70%), and high agreement (above 70%). The table shows that the models tend to agree on prototypical datasets (e.g., 71.4% of high agreement on C101) but they do not for very fine-grained ones (i.e., CARS and FGVC). Overall, we found that only 5.6% of the samples are correctly predicted by all models and there exists 6 labels out of almost 1200 that are never predicted correctly according to the LI score: i.e., birman, bishop of llandaff, egyptian mau, prince of wales feather, silverbush, and watercress, all belonging to fine-grained datasets. These results confirm the ability of LMMs to capture generic concepts while struggling on very specific ones. In Fig. 4, we observe that when the granularity constraint is relaxed, most models continue to predict the parent class with remarkable level of accuracy, given the nature of the task. Which models agree the most with each other? We additionally check the pair-wise agreement on the model predictions on the correct and specific group, showing the results in Fig. 5. Interestingly, models of the same family tend to share more predictions, i.e., Qwen2VL 2B and Qwen2VL 7B share 66.7% correct and specific predictions, InternVL2 4B and InternVL2 8B 59.0%. This also happens with different language models (e.g., LLaVA NeXT with Mistral and Vicuna share 63.9% of correct predictions), and differences might arise within lower performing families (e.g., LLaVA-OV 0.5B and 7B agree only 37% of the time). While there is no clear pattern, the best-performing families (e.g., LLaVA NeXT, InternVL2, Qwen2VL) tend to share more than half of the correct predictions (e.g., InternVL2 8B and Qwen2VL 7B, 55.5%), suggesting that the agreement is mostly driven by the capabilities rather than design choices. We also show the agreement between models on the other three prediction splits in the Supp. Mat. (see A.4). 4.2. Correct but Generic As different classification scenarios may require different levels of granularity, in the following we check whether we can control the latter via prompting. We investigate three types of requests: Be generic., Be specific., and domain-specific prompts, focusing on the fine-grained and very fine-grained datasets, alongside DTD. In Fig. 6, we report the average difference across datasets for each group of predictions and type of prompt, reporting in the Supp. Mat. (see A.5) the metric variations on datasets and models. Be more generic. When queried for generic responses, we see large shift from correct and specific predictions to correct but generic, and, to smaller degree, the same happens for wrong ones. This highlights how models can provide good generic responses (+9.1%) but the large decrease in correct and specific ones means they become too generic. Be more specific. In this case, all LMMs consistently get worse, equally increasing wrong but specific and wrong and generic predictions. While decrease (especially in correct but generic) is expected, this hints that LMMs are stronger at providing more generic replies than more specific ones. Domain-specific. When tackling specific fine-grained scenarios, it is possible to tailor custom prompt, e.g., when classifying flowers, we can directly ask What type of flower is in this image? instead of generic object. Therefore, we explore whether informing the LMM on the target fine-grained scenario may fix the specificity issue. We update the prompt to use the terms texture (for DTD), aircraft (for FGVC), flower (for FLWR), food (for FOOD), pet (for PETS), or car (for CARS). Overall, domain-specific prompts positively influence the predictions, converting an average of 12.5% of generic responses into specific ones. Notably, LLaVA-OV 0.5B gets +29% on the correct and specific set, followed by Qwen2VL 7B with +15% (see A.5 in the Supp. Mat.). This shows how, while LMMs struggle to provide specific predictions off-the-shelf, injecting domain-specific context can largely improve OW performance. 4.3. Wrong but Specific Here we analyze mistakes due to two objects being very similar (e.g., euphonium vs trombone). As addressing this type of mistake requires reasoning on fine-level details of the images, we explore whether test-time reasoning can improve performance. Thus, we study the impact of introducing Chain-of-Thought [32, 61] during inference. Can CoT mitigate misclassification? We identify three simple techniques we can apply without modifying the ar7 (a) Zero-shot CoT (b) LlamaV-o1 prompt (c) LLaVA-CoT prompt (d) Reasoning models Figure 7. Average gains per prediction types when including chain-of-thought reasoning (a, b, c), or when with reasoning models (d). Blue indicates correct and specific and correct but generic predictions, red indicates wrong but specific and wrong and generic mistakes. chitecture of the models: zero-shot CoT [32] appending the instruction Think step by step. to the input query, LlamaV-o1 prompt using the multi-turn procedure of [57], and the LLaVA-CoT prompt [63] for reasoning in procedures. For this study, we focus on the InternVL2 and the Qwen2VL families, showing their average gains in Fig. 7. Additional results are available in the Supp. Mat. (see A.5). Notably, test-time reasoning helps the models in making correct and specific responses. Performance-wise, Qwen2VL shows the highest gains, achieving up to +13% in correct and specific responses. While test-time reasoning enhance the OW of LMMs, we observe that the multi-turn prompt tends to steer the model either toward semantically correct predictions or completely divergent ones (+1.7 on wrong and generic). On the other hand, simply instructing LMMs to think with zero-shot CoT or providing longer prompt (LLaVA-CoT), consistently increases their accuracy. Do models tailored for reasoning excel in OW? As we saw positive gains from using test-time reasoning, we further explore the capabilities of more advanced approaches. Specifically, both InternVL2 and Qwen2-VL have two improved versions tailored for reasoning: InternVL2.5 [11] and Qwen2.5VL [4]. In the following, we check whether these variants outperform their predecessors, less tailored to reasoning. We show the average relative gains in Fig. 7 (d). By directly replacing the base models with their reasoning counterparts, we get mixed results, as we see large increase in correct prediction (+6.6% on average), but also in misclassification with semantically close concepts (+3.7%), the error we wanted to address. This shows that test-time reasoning might be more effective at addressing such nuanced cases than reasoning-based models. 4.4. Wrong and Generic In this category, predictions are not only wrong according to inclusion metrics but also based on semantic ones. While some of the mistakes are due to the lack of fine-grained understanding of the models (see Sec. 4.1), here we investigate to which extent LMMs are correct even within wrong predictions. Specifically, we explore cases where models simply focus on the wrong object in the image. Do LMMs focus on the wrong object? To investiFigure 8. Percentage of model predictions considered wrong in the single-label setting, but correct in multi-label. gate this, we annotate images with multiple labels using RAM++ [28], state-of-the-art model for tagging images with list of concepts. Then, we compare LMM predictions to the list of tags, looking for cases where there is an extremely high CS (above 0.95 with any of the tags in the image). If this is the case, we assume the prediction to be relevant for the image, even if different from the true label. Fig. 8 shows the percentage of wrong predictions that match tag. As shown in the table, this percentage is high, ranging between 30% and 60% of the wrong predictions. Notably, this is high also for models with lower overall performance in Tab. 1, such as Idefics2 and InstructBLIP. Additional experiments on the capability of models in predicting and suggesting multiple hypotheses for the output class are in Supp. Mat. (see A.4), where we explore their changes in accuracies when tasked to predict multiple labels. 5. Conclusions In this work, we conducted large-scale study on LMMs for OW classification. Evaluating 13 models across 10 datasets using four different metrics, we highlight both their strengths and the challenges they face in this task. As the four metrics capture different levels of alignment between predictions and ground truth, we use them to provide an indepth analysis of LMMs mistakes, identifying cases where the model is too generic, confused by similar concepts, or focuses on the wrong subject, analyzing strategies to mitigate these issues. Our benchmark and metrics can serve as reference for future work in this field, toward tackling this challenging yet underexplored setting."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 2, 4, 13, 14, 15, 16, 17, 18, 19, 21 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 2022. 2 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1, 2 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 8, 20, 23 [5] Eden Belouadah, Adrian Popescu, and Ioannis Kanellos. comprehensive study of class incremental learning algorithms for visual tasks. Neural Networks, 135:3854, 2021. 1 [6] Abhijit Bendale and Terrance Boult. Towards open world recognition. In CVPR, 2015. 1, 3 [7] Lucas Beyer, Olivier Henaff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020. 3 [8] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random forests. In ECCV, 2014. 4, 12 [9] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In ICML, 2024. 3 [10] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 23 [11] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 8, 20 [12] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. 4, 14, 15, 16, 17, 18, 19, 20, 21 [13] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. 2, 4, 13, 14, 15, 16, 17, 18, 19, 20, [14] Wei-Lin Chiang, Zhuohan Li, Ziqing Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt See https://vicuna.lmsys.org (accessed 14 April quality. 2023), 2023. 4 [15] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In ICML, 2024. 12 [16] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014. 2, 4, 12 [17] Alessandro Conti, Enrico Fini, Massimiliano Mancini, Paolo Rota, Yiming Wang, and Elisa Ricci. Vocabulary-free image classification. NeurIPS, 2023. 1, 2, 3, 4, 12, 14, 15 [18] Gabriela Csurka, Tyler Hayes, Diane Larlus, and Riccardo Volpi. What could go wrong? discovering and describing failure modes in computer vision. arXiv preprint arXiv:2408.04471, 2024. 2 [19] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, and Junqi Zhao. Instructblip: Towards generalpurpose vision-language models with instruction tuning. NeurIPS, 2023. 2, 4, 13, 14, 15, 16, 17, 18, 19, [20] Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph Gonzalez, and Serena Yeung-Levy. Describing differences in image sets with natural language. In CVPR, 2024. 2 [21] Arpad Elo. The proposed uscf rating system, its development, theory, and applications. Chess life, 22(8):242247, 1967. 12 [22] Sabri Eyuboglu, Maya Varma, Khaled Kamal Saab, JeanBenoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, and Christopher Re. Domino: Discovering systematic errors with cross-modal embeddings. In ICLR, 2022. 2 [23] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPRW. IEEE, 2004. 2, 3, 12 [24] Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. ReIEEE cent advances in open set recognition: survey. TPAMI, 43(10):36143631, 2020. 1 [25] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv preprint et al. arXiv:2407.21783, 2024. 2, The llama 3 herd of models. [26] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7), 2019. 4, 12 9 [27] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. NeurIPS, 2023. 2 [28] Xinyu Huang, Yi-Jie Huang, Youcai Zhang, Weiwei Tian, Rui Feng, Yuejie Zhang, Yanchun Xie, Yaqian Li, and Lei Zhang. Open-set image tagging with multi-grained text supervision. arXiv preprint arXiv:2310.15200, 2023. 2, 8 [29] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. 4 [30] Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, and Jinwoo Shin. Discovering and mitigating visual biases through keyword explanation. In CVPR, 2024. [31] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal inputs and outputs. In International Conference on Machine Learning, pages 1728317300. PMLR, 2023. 2 [32] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. NeurIPS, 2022. 7, 8 [33] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV-WS, 2013. 2, 4, 12 [34] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? NeurIPS, 2025. 2, 4, 13, 14, 15, 16, 17, 18, 19, 21 [35] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In CVPR, 2024. 2 [36] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild. https://llava-vl.github.io/blog/2024-0510-llava-next-stronger-llms, 2024. 2, 4, 13, 14, 15, 16, 17, 18, 19, [37] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 4, 13, 14, 15, 16, 17, 18, 19, 21 [38] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 1, 2, 4 [39] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, 2024. 2 [40] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, 2023. 2 [41] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023. 1, 2, 4, 13, 14, 15, 16, 17, 18, 19, 21 [42] Huan Liu, Lingyu Xiao, Jiangjiang Liu, Xiaofan Li, Ze Feng, Sen Yang, and Jingdong Wang. Revisiting mllms: An in-depth analysis of image classification abilities. arXiv preprint arXiv:2412.16418, 2024. 1, 2, [43] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024. 2 [44] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 4, 12 [45] Mazda Moayeri, Sahil Singla, and Soheil Feizi. Hard imagenet: Segmentations for objects with strong spurious cues. NeurIPS, 2022. 3 [46] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In Indian conference on computer vision, graphics & image processing. IEEE, 2008. 2, 4, 12 [47] Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, 2012. 4, 12 [48] Momchil Peychev, Mark Muller, Marc Fischer, and Martin Vechev. Automated classification of model errors on imagenet. NeurIPS, 2023. 3 [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2, 4, 5, 14, 15, 23 [50] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In EMNLPIJCNLP, 2019. 3 [51] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115:211252, 2015. 2 [52] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Testtime prompt tuning for zero-shot generalization in visionlanguage models. NeurIPS, 2022. [53] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: foundational language and vision alignment model. In CVPR, 2022. 2 [54] Sahil Singla and Soheil Feizi. Salient imagenet: How to discover spurious features in deep learning? In ICLR, 2022. 3 [55] Sahil Singla, Besmira Nushi, Shital Shah, Ece Kamar, and Eric Horvitz. Understanding failures of deep networks via robust feature extraction. In CVPR, 2021. 2 [56] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 4, 12 10 [72] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. IJCV, 130(9):23372348, 2022. [73] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. ICLR, 2024. 2 [74] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language In understanding with advanced large language models. ICLR, 2024. 2 [57] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamavo1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. 8 [58] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3 [59] Vijay Vasudevan, Benjamin Caine, Raphael Gontijo Lopes, Sara Fridovich-Keil, and Rebecca Roelofs. When does dough become bagel? analyzing the remaining mistakes on imagenet. NeurIPS, 2022. 3 [60] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 4, 13, 14, 15, 16, 17, 18, 19, 20, 21 [61] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022. [62] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010. 3, 12 [63] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason stepby-step. arXiv preprint arXiv:2411.10440, 2024. 8 [64] Sriram Yenamandra, Pratik Ramesh, Viraj Prabhu, and Judy Hoffman. Facts: First amplify correlations and then slice to discover bias. In ICCV, 2023. 2 [65] Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Towards open-ended visual recognition with large language models. In ECCV, 2024. 3 [66] Kaiyu Yue, Bor-Chun Chen, Jonas Geiping, Hengduo Li, Tom Goldstein, and Ser-Nam Lim. Object recognition as next token prediction. In CVPR, 2024. 2, 3 [67] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionlanguage models behave like bags-of-words, and what to do about it? In ICLR, 2023. 2 [68] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 2, 4, 5, 14, 15, [69] Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, and Serena Yeung-Levy. Why are visually-grounded language models bad at image classification? NeurIPS, 2024. 1, 2, 3, 4, 5 [70] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt5: Interleaved vision-and-language generation via generative vokens. arXiv preprint arXiv:2310.02239, 2023. 2 [71] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. NeurIPS, 2023. 2, 3, 12 11 A. Supplementary Material In the following, we provide additional information on our analyses. First, we report further detail on the considered datasets, models (A.1), and metrics (A.2), followed by the extended results of each model for each dataset (A.3). Then, we extend our main analyses by evaluating with an Elo ranking system which model provides the best responses, using Llama instance to score wins. We continue the analysis by evaluating the percentage of agreement between models for the three prediction groups not present in the main paper, and we use RAM++ to tag images by checking whether we can improve the model performance by using prompts that foster multi-label responses, e.g., listing the objects in the scene, or describing the image (A.4). Finally, we report additional tables and visualizations to accompany the studies in the main manuscript (A.5). A.1. Additional details on the datasets and models The datasets used in our evaluation are summarized in Tab. 3. For the experiments we used the same training and test splits used in previous works [17], while summary of the LMMs used in this study and their differences is in Tab. 4. A.2. Additional details on the metrics For computing the inclusion metric, we instruct Llama 3.2 [25] to score good and bad LMM responses with the following prompt:"
        },
        {
            "title": "Llama inclusion instruction",
            "content": "You are model that determines whether an answer is good reply to question given also its target value. This is the question: What type of object is in this image? This is the answer: %s This is the target value: %s reply posiIf the answer describes the target, If the answer includes the target value or tively. synonym of it, reply positively. If the target is generic but it is related to the answer, reply positively. Reply only with 1 if yes, or 0 if no. A.3. Extended results We report the per-dataset results of the evaluated LMMs, split into one table for each of the considered metrics, i.e., text inclusion in Tab. 5, Llama inclusion in Tab. 6, semantic similarity in Tab. 7, and concept similarity in Tab. 8."
        },
        {
            "title": "Images Classes",
            "content": "CALTECH101 [23] (C101) DTD [16] EUROSAT [26] (ESAT) FGVCAIRCRAFT [44] (FGVC) FLOWERS102 [46] (FLWR) FOOD101 [8] (FOOD) OXFORDPETS [47] (PETS) STANFORD CARS [33] (CARS) SUN397 [62] (S397) UCF101 [56] (U101) 2,465 1,692 8,100 3,333 2,463 30,300 3,669 8,041 19,850 3,783 100 47 10 100 102 101 37 196 397 101 Table 3. Summary details of the datasets used in our analyses. A.4. Additional analyses Which model provides the best responses? To analyze which model provides the best responses, we compare their generations in pairs. Specifically, for each of the ten datasets, we randomly sample 10000 pairs of generations, and instruct Llama 3.2 model to identify the best response in the pair, similarly to what done in the Chatbot Arena [15] but through automatic evaluation with LLM-asa-judge [71]. We use the following prompt to instruct Llama 3.2 to judge the pairs of predictions and decide for win:"
        },
        {
            "title": "Llama Elo ranking",
            "content": "You are model that discriminates whether labels or better align with target value. This is label A: %s This is label B: %s This is the target value: %s Does align better with the target value? Does align better with the target value? Reply only with 1 if wins over B, or 0 if wins over A. We directly compare the quality of the outputs by evaluating the Elo score [21] of these model responses and report the average on the ten datasets in Tab. 9. Results show that Qwen2VL models are the best at providing accurate predictions, similar to the trend in Tab. 1. Which models agree the most with each other? To complement the analysis of the main paper, here we show the pair-wise agreement on the model predictions on group beyond the correct and specific one, showing the results in Fig. 9 for correct but generic, Fig. 10 for wrong but specific, and Fig. 11 for wrong and generic. The trends follow those of the main paper  (Fig. 5)  i.e., where models of the same families tend to agree on the same samples, generalizing those findings across groups."
        },
        {
            "title": "Training",
            "content": "Pre-training IDEFICS2 [34] SOViT (SigLIP), 0.4B params; max 980x980. Mistral 7B INSTRUCTBLIP [19] ViT-g (BLIP-2), 1.1B params; 224x224. Vicuna 7B INTERNVL2 [13] (custom), InternViT 0.3B params (or 6B for larger models); dynamic resolution, max 40 tiles of 448448. Qwen2 0.5B (for 1B and 2B versions), or InternLM2 8B (for 8B version). LLAVA-1.5 [41] ViT-L (CLIP), params; 336x336. 0.3B Vicuna 7B Interleaved web docs, imagecaption pairs (LAION-COCO), OCR data; fine-tuned on 50 curated datasets. 26 datasets transformed into instruction-tuning format: captioning, VQA, image generation. Interleaved image-text, multilingual OCR, mathematical charts; strict quality control. 158K multimodal instructionfollowing samples; pre-trained on filtered CC dataset (596K image-text pairs). LLAVA-NEXT [36] ViT-L (CLIP), 0.3B 336x336, 336x1344, params; 672x672, and 1344x336. Mistral 7B, or Vicuna 7B Diverse tasks, including multiimage and video understanding. LLAVA-OV [37] PHI-3-VISION [1] SOViT (SigLIP), 0.4B params; dynamic resolution (AnyRes-9), max 2304x2304. ViT-L (CLIP), 0.4B params; dynamic resolution, max 1344x1344. Qwen2 Qwen2 7B 0.5B, or Single-image and video scenarios with task transfer capabilities; diverse visual benchmarks. Phi-3 Mini params) (3.8B Synthetic data, filtered public docs, high-quality interleaved text-image data, math/code examples. QWEN2VL [60] ViT (custom), 0.6B params; dynamic resolution (Naive Dynamic Resolution), no max. Qwen2 Qwen2 7B 1.5B, or Multilingual datasets: MathRealDocVQA, Vista, WorldQA; videos supports (20+ min) and multilingual text in images. Joint dual encoder training with Perceiver pooling for visiontext alignment. Two-stage pre-training: Visionlanguage alignment via BLIP2 and instruction-aware Query Transformer task-specific for feature extraction. Progressive training: masked video modeling, cross-modal learning, contrastive and prediction with next-token spatiotemporal focus. Frozen vision encoder during feature alignment stage; end-toend fine-tuning. Builds on LLaVA with extended ViT and additional multimodal datasets for improved generalization. Pre-trained with balanced visual token representation across scenarios to enable task transfer. Multi-stage training: custom vision encoder aligned with Phi-3 Mini language model using interleaved and fine-grained tasks. Pre-trained with dynamic resolution ViT for flexible input sizes and multilingual alignment strategies. Table 4. Summary details of the Language Multimodal Models used in our analyses. Predicting more concepts. The experiment using RAM++ to tag images suggested that LMMs often fail to predict the class names because they focus on the wrong part of the image. However, when prompted to provide multiple candidates, do LMMs get the correct prediction? To investigate this, we ask the model to (i) list the objects in the image; (ii) caption it, or (iii) describe its content. We report the relative gain per model in Tab. 10. The results show that providing outputs that focus on multiple labels on average improves the concept-based similarity, with the only exception of the caption case. Text inclusion improves consistently, showing that predictions become correct even according to this strict metric. Overall, these results highlight how LMM mistakes can be ascribed by mismatches between the label and the focus of the annotator, with the models often focusing on grounded image content even in case of mistakes. A.5. Extended results for the analyses Below, we report the extended results for the analyses we conducted. In Tab. 13 (also visualizing the average gains in Fig. 12) we show the variation in correct and wrong predictions for each model when using more generic/specific prompts and domain-specific information. We additionally report the variation in text inclusion, Llama inclusion, and concept similarity for each model and dataset in Tab. 11 and Tab. 12. For the chain-of-thought experiments, we provide the variations on the correct and wrong predictions in Tab. 15, and the per-dataset and model variations in Tab. 14. Textual inclusion Model C101 DTD ESAT FGVC FLWR FOOD PETS CARS S397 U101 Avg. IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Open-world baselines CaSED [17] CLIP retrieval Closed-world baselines CLIP [49] SigLIP [68] 52.0 47.8 52.8 49.6 55.0 51.6 58.0 54.9 53.4 55.5 53.4 60.8 63.2 35.5 42.6 87.1 93.6 1.7 3.0 10.8 11.8 12.5 6.0 13.6 12.2 9.2 12.6 10.9 12.1 15.7 5.1 7. 1.6 5.5 7.4 6.0 6.0 11.7 7.4 7.2 4.2 4.9 0.8 0.4 2.7 3.0 6.6 52.6 60.8 42.7 42.1 0.0 0.0 1.4 3.4 4.6 0.1 2.8 2.5 1.2 0.0 0.4 25.6 1.4 1.4 14. 27.2 46.0 0.8 6.0 14.1 12.8 19.1 6.7 17.6 11.9 2.9 14.2 12.0 42.9 42.3 28.1 40.6 76.9 88.2 8.2 24.3 23.3 28.2 33.9 17.6 35.5 29.6 12.6 5.0 21.6 48.5 49.3 19.4 26. 89.9 94.1 0.1 0.8 7.2 7.8 13.8 1.1 27.1 9.4 2.5 0.1 6.5 15.7 12.1 34.6 30.3 88.1 95.4 0.0 0.0 0.0 0.0 0.1 0.0 0.0 0.0 0.1 0.0 0.1 0.1 0.1 0.0 0. 76.2 92.3 9.6 11.6 21.1 23.0 26.3 17.6 25.4 24.0 15.5 6.2 14.7 29.0 29.5 13.5 14.7 65.6 69.9 7.9 9.6 12.4 12.7 14.4 8.2 13.0 12.5 8.7 4.0 6.5 10.8 12.5 8.1 8. 72.7 82.1 8.2 10.9 15.0 15.5 18.6 12.1 20.0 16.4 11.0 10.2 12.7 24.6 22.9 14.9 19.1 67.9 76.5 Table 5. Text inclusion on the ten datasets. Higher is better, bold indicates best. Llama inclusion Model C101 DTD ESAT FGVC FLWR FOOD PETS CARS S397 U101 Avg. IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Open-world baselines CaSED [17] CLIP retrieval Closed-world baselines CLIP [49] SigLIP [68] 72.9 76.8 74.9 74.4 77.2 74.5 77.8 77.3 76.5 81.3 75.7 82.9 84.3 57.7 55.3 87.1 93.6 24.6 26.2 48.5 45.7 50.5 39.4 54.0 52.2 46.5 45.6 45.3 54.6 60.8 16.7 28.2 52.6 60. 19.0 19.1 35.0 30.1 28.6 45.0 28.0 26.4 28.7 11.8 6.0 3.1 18.1 7.3 12.7 42.7 42.1 64.4 59.9 35.8 40.5 29.7 44.5 43.4 43.1 61.2 68.9 51.0 65.0 58.8 30.7 25.8 27.2 46. 54.6 57.4 49.3 37.5 36.0 46.3 33.4 29.2 55.1 48.9 53.2 67.0 71.0 46.0 44.6 76.9 88.2 58.7 47.6 44.3 45.9 53.7 47.7 63.2 60.6 28.1 22.0 45.1 71.1 75.0 35.1 35.4 89.9 94. 36.3 41.3 47.4 49.7 50.4 45.5 34.6 43.6 44.9 50.2 49.1 49.3 46.0 58.7 56.2 88.1 95.4 69.6 62.0 30.0 33.1 35.3 37.5 50.9 41.2 70.0 84.4 39.0 56.3 67.2 63.5 10.4 76.2 92. 32.5 35.8 64.9 62.5 71.5 51.6 69.9 68.2 52.2 25.0 44.5 72.6 73.0 34.9 30.5 65.6 69.9 40.1 36.0 52.1 50.4 59.6 48.5 58.3 59.1 35.8 27.0 34.7 45.2 48.8 31.7 32.9 72.7 82. 47.3 46.2 48.2 47.0 49.3 48.1 51.4 50.1 49.9 46.5 44.4 56.7 60.3 38.2 33.2 67.9 76.5 Table 6. Llama inclusion on the ten datasets. Higher is better, bold indicates best. Note that the scores for CLIP closed-world equals the textual inclusion scores. We also provide visualization of the variations for the list, caption, and describe experiments in Fig. 13 (also reported numerically in Tab. 16). Finally, we report the complete results table for the reasoning models tested on the ten classification datasets in Tab. 17. 14 Model C101 DTD ESAT FGVC FLWR FOOD PETS CARS S397 U101 Avg. Semantic similarity IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Open-world baselines CaSED [17] CLIP retrieval Closed-world baselines CLIP [49] SigLIP [68] 64.9 71.5 50.5 49.2 50.1 49.0 48.2 49.2 64.7 68.7 53.6 56.4 55.8 65.3 41.3 90.8 97.8 34.6 32.8 25.6 26.1 26.7 24.2 27.7 27.9 28.8 32.2 28.5 27.0 28. 39.9 23.6 69.9 75.6 27.5 30.0 26.0 24.7 24.4 34.2 23.9 23.1 21.6 19.4 12.3 13.5 20.7 32.2 22.4 67.7 63.1 27.6 21.4 23.4 23.6 25.5 19.0 23.6 23.4 21.0 29.4 18.8 32.8 20. 30.0 30.7 66.7 80.0 38.6 38.9 31.2 30.2 32.8 25.8 30.2 29.3 41.4 37.5 30.9 43.7 41.8 55.6 40.3 83.4 92.0 44.4 41.6 39.6 41.1 44.2 37.2 45.3 43.0 42.7 41.7 40.1 50.6 50. 64.1 46.7 93.7 96.4 30.8 26.4 23.9 24.6 27.3 21.5 30.3 24.4 31.4 37.8 24.3 27.8 25.1 62.4 41.7 91.8 96.8 31.6 38.5 42.9 44.1 46.6 38.2 44.8 45.7 40.0 34.4 39.0 57.4 48. 47.1 48.8 80.5 98.1 44.2 42.1 43.3 43.8 46.3 41.7 43.6 43.3 43.2 43.4 41.8 47.9 48.1 52.4 39.1 92.2 83.1 44.0 48.3 43.1 41.8 44.6 40.7 42.1 42.3 47.9 43.2 37.3 42.7 43. 53.4 38.5 83.3 89.6 38.8 39.1 34.9 34.9 36.8 33.1 36.0 35.1 38.3 38.8 32.7 40.0 38.3 50.2 37.3 82.0 87.3 Table 7. Semantic similarity on ten datasets. Higher is better, bold indicates best. Model C101 DTD ESAT FGVC FLWR FOOD PETS CARS S397 U101 Avg. Concept similarity IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Open-world baselines CaSED [17] CLIP retrieval Closed-world baselines CLIP [49] SigLIP [68] 76.3 75.3 75.7 76.1 78.7 72.1 79.8 79.0 77.8 79.1 74.1 79.4 81.3 65.9 63.9 90.8 97.8 38.5 39.1 48.0 48.6 49.7 41.3 51.0 50.1 45.1 47.0 44.0 47.3 50.4 39.8 38. 69.9 75.6 30.9 31.6 52.9 51.5 49.1 51.6 49.5 50.8 39.9 41.0 25.3 24.2 39.8 32.2 37.8 67.7 63.1 29.7 28.6 36.8 37.9 42.5 29.0 37.5 37.1 30.6 29.4 29.1 56.0 30.8 29.9 50. 66.7 80.0 41.5 43.6 49.5 51.0 56.9 41.6 55.1 51.3 42.4 51.7 43.0 67.9 68.8 55.6 62.3 83.4 92.0 48.4 60.0 60.8 63.0 67.1 56.8 70.0 65.8 50.0 41.9 58.3 75.7 76.9 66.5 67. 93.7 96.4 35.3 37.9 41.9 41.9 46.0 35.9 55.3 42.4 37.5 37.8 40.3 46.7 43.1 62.9 66.1 91.8 96.8 37.5 40.0 50.9 50.5 56.2 46.2 56.3 55.0 43.5 35.4 42.9 68.6 56.0 47.1 61. 80.5 98.1 49.9 52.6 65.1 65.4 69.2 59.4 68.7 67.4 56.7 44.9 56.1 70.0 70.6 53.7 57.3 92.2 83.1 54.6 55.3 59.4 59.1 62.9 55.5 62.7 61.8 55.9 43.3 49.1 56.6 59.1 55.1 54. 83.3 89.6 44.3 46.4 54.1 54.5 57.8 48.9 58.6 56.1 47.9 45.1 46.2 59.2 57.7 50.9 56.0 82.0 87.3 Table 8. Concept similarity on ten datasets. Higher is better, bold indicates best. Rank Average Elo ratings Model Rating 1 2 3 4 5 6 7 8 9 10 11 12 13 QWEN2VL [60] 2B QWEN2VL [60] 7B PHI-3-VISION [1] LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 7B) LLAVA-OV [37] (Qwen2 0.5B) INTERNVL2 [12, 13] 8B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 2B LLAVA-1.5 [41] 7B INSTRUCTBLIP [19] Vicuna 7B IDEFICS2 [34] 8B 1037 1037 1029 1018 1015 1014 1007 1004 994 991 984 943 Table 9. Elo ratings on the ten datasets. Higher scores indicate comparatively better responses from the models. Figure 9. Percentage of correct but generic predictions shared between models. Higher values indicate models perform responses similarly to the same inputs. Figure 11. Percentage of wrong and generic predictions shared between models. Higher values indicate models perform responses similarly to the same inputs. Figure 10. Percentage of wrong but specific predictions shared between models. Higher values indicate models perform responses similarly to the same inputs. 16 Model List IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Caption IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Describe IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Caltech101 LI TI CS TI -1.5 +12.5 -0.8 +1.8 -1.7 -0.7 -2.5 -1.5 +6.3 +6.5 -1.1 +0.4 -1.8 -2.8 +9.4 -6.3 -0.8 -4.2 +0.9 -9.4 -8.4 +2.5 +0.7 +2.5 +0.3 -0.6 -10.0 +11.5 -1.4 +2.0 -1.7 +1.9 -1.9 -1.7 +6.7 +8.1 -0.1 +1.8 -1.0 -12.8 -3.5 -6.1 -4.3 -2.2 -6.3 -2.1 -2.9 -8.1 +1.1 -11.9 -0.1 -10.4 -5.4 -8.6 -4.5 -6.5 -4.3 +1.6 -15.5 -13.5 -0.1 -6.8 +0.4 +0.3 -1. -21.1 -4.6 +1.4 -0.9 -0.2 +0.6 +0.8 +0.5 +7.6 +6.5 -0.1 +1.8 -1.9 -2.8 +4.5 +1.2 +0.7 -0.2 -2.7 -0.4 -0.6 +0.5 +3.6 +1.0 +2.6 -1.0 -4.9 +0.0 -9.9 -9.2 -9.5 +5.4 -17.7 -16.9 -8.9 -8.1 +3.5 -5.2 +0.9 -5.1 -1.9 +3.4 +2.2 +1.0 +6.6 +0.8 +0.4 +4.3 +5.8 +2.0 +5.0 +2.9 +3.6 +6.8 -1.5 -1.8 -0.8 +0.7 -1.1 -0.7 +1.8 +3.0 -1.9 +5.9 +0.4 +5.3 +4.7 -1.1 -2.2 +1.0 +3.0 -5.5 -4.9 +1.2 -1.7 +2.4 +2.9 +1. -0.8 +6.9 +0.9 +34.6 +0.3 +3.8 +0.3 -0.5 +6.1 +5.1 +2.8 +5.4 +2.1 DTD LI +3.4 +15.8 -2.8 +3.6 +0.1 +2.6 -1.2 -1.4 +5.7 +14.2 +1.6 +9.7 +2.7 +13.5 +14.6 -5.1 -2.9 +0.6 +4.8 -20.9 -20.3 +2.7 +1.9 +10.0 +7.1 +1.7 -4.9 +18.7 -0.5 +5.4 +2.1 +6.6 -1.0 -0.7 +21.7 +22.4 +11.5 +8.1 +2.4 CS +2.0 +6.0 -1.7 -1.6 -1.2 +1.4 -0.8 -1.0 +4.4 +6.2 -1.8 +6.3 +1.4 +3.2 +3.4 -6.3 -5.3 -4.8 +7.1 -9.0 -9.7 +0.2 -3.1 +5.8 +0.5 +1.2 -2.2 +5.2 +1.0 -20.8 +0.4 +7.5 +0.8 +0.4 +8.0 +7.0 +5.6 +6.7 +4.1 Flowers102 LI TI CS OxfordPets LI CS TI UCF101 LI TI CS +3.6 +13.7 -4.2 -3.7 -4.8 +0.8 -5.1 -2.7 +8.1 +1.6 +3.3 -10.6 -24.2 +6.2 +6.6 -2.2 -1.9 -5.4 -1.0 -8.1 -2.2 +12.6 -6.2 +12.4 -6.6 -8.5 +3.3 +15.2 -2.3 -0.4 -5.1 +0.5 -4.9 -2.7 +12.1 +7.4 +11.7 -6.5 -2.6 -19.3 -24.2 -23.5 -12.7 -2.3 -21.5 -2.1 -3.7 -23.8 -7.2 -17.1 -12.2 -44.0 -33.5 -34.5 -20.3 -13.6 -4.2 -27.5 -10.2 -5.9 -23.0 -28.4 -4.6 -8.9 -15.8 -33.0 -25.5 -18.0 -10.6 +0.0 -28.0 -3.8 -5.1 -6.9 -5.3 -2.7 -13.3 -15. -0.3 +5.9 -2.4 -5.7 -5.2 -0.9 -4.1 -2.8 +4.0 -0.6 +1.4 -8.0 -21.4 -0.7 -0.8 -7.3 -7.9 -13.5 +3.7 -15.2 -11.3 -0.3 -11.2 +8.7 -14.8 -13.5 -0.6 +3.5 +1.1 +0.1 -5.4 +4.7 -4.1 -3.7 +7.6 +3.0 +8.5 -3.8 -4.1 +1.8 +2.6 +4.8 +3.4 +2.3 +0.9 -9.5 +3.6 +2.1 +2.1 +1.5 -3.4 -5.6 +8.3 +3.5 +10.3 +13.8 +2.9 +5.6 -16.2 +0.5 +10.2 +5.6 +17.1 +20.0 +13.2 +2.1 +9.6 +14.9 +16.3 +10.7 +5.6 -6.6 +7.6 +11.7 +20.3 +14.6 +12.8 +24. -17.6 -29.1 -32.8 -35.5 -26.5 -23.0 -7.6 -19.0 -39.0 -46.9 -40.7 -23.1 -39.2 -18.9 -24.6 -13.3 -18.3 -20.3 -19.6 -10.6 -17.0 -18.2 -35.4 -14.2 +0.2 -7.2 -22.0 -12.3 -18.3 -20.4 -18.7 -22.9 -3.1 -13.1 -7.6 -18.0 -14.0 -12.1 -2.2 +3.5 +0.9 +2.2 +3.1 +2.4 +1.4 -4.3 +6.5 +3.0 +0.9 +1.9 -5.0 -1.0 +6.4 +1.6 +3.0 +4.3 -1.5 +7.9 -17.8 -3.8 +4.7 +2.0 +12.8 +8.5 +10.0 +4.0 +6.8 +11.4 +12.4 +9.0 +7.9 -1.9 +9.5 +10.7 +14.3 +11.2 +10.0 +18. +2.7 +6.7 -3.1 +2.2 +1.0 -1.5 -0.6 -1.3 +5.5 +13.6 +1.2 +6.3 +0.4 +3.4 +5.2 +0.2 -0.1 -1.4 +3.2 -4.8 -4.8 +7.1 +9.5 +3.8 +3.6 +48.4 -2.4 +5.0 +9.7 +1.7 +0.9 +2.7 +1.1 -1.1 +5.4 +16.6 +4.4 +17.7 +4.4 -6.6 +21.1 -11.2 -0.2 -3.9 -10.2 -0.6 -3.7 +19.5 +29.1 +3.0 +18.6 +0.8 +1.3 +11.1 -7.0 -3.6 -10.0 +9.2 -28.0 -24.3 +22.6 +21.3 +13.5 +15.4 -32.7 -19.9 +24.1 -23.0 +1.6 -1.6 +10.0 +3.6 +1.6 +1.5 +43.1 +15.0 -8.0 +14. -1.2 +9.3 -1.6 +0.6 -3.2 +0.0 -1.8 -1.5 +4.0 +19.9 +3.9 +7.5 -0.7 +0.2 +4.5 -2.4 -19.7 -4.6 +5.3 -9.7 -9.5 +5.2 +16.9 +4.9 +5.2 +3.8 -6.8 +8.3 -6.1 +0.8 -2.1 +5.7 -1.1 -1.8 -7.7 +21.6 +5.8 +0.1 +4.5 Table 10. Relative performance variation with multi-label prompts on five datasets. TI stands for text inclusion, LI for Llama inclusion, and CS for concept similarity. 17 Model Be generic IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Be specific IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B TI -7.0 -10.6 -3.0 -6.3 -12.4 -4.4 -4.5 -3.1 -10.1 -39.1 -5.7 -8.0 -11.1 -1.0 +1.9 -0.5 -0.8 +1.5 +0.5 +0.7 -0.8 -1.3 +1.1 +0.7 +4.0 -2.8 DTD LI -9.7 -20.7 +3.8 -4.7 -16.1 -21.7 -6.0 -9.7 -20.4 -32.8 -0.1 -13.4 -17.6 -4.2 +13.9 -4.5 -0.1 +0.0 +0.3 -2.0 -3.1 -10.0 +5.5 +3.6 +4.1 -8.1 CS -8.5 -9.3 -3.9 -3.9 -10.8 -7.0 -3.5 -5.6 -6.9 -26.1 +0.5 -8.9 -10.5 -1.7 +0.1 -0.7 -1.1 +0.2 -0.7 +0.2 -1.0 -3.6 +1.5 +0.4 +3.1 -4.1 FGVCAircraft LI CS TI Flowers102 LI TI CS TI Food101 LI CS OxfordPets LI TI CS StanfordCars LI CS TI -2.4 -0.7 -2.5 -7.2 -7.3 -0.2 -5.9 -5.3 -8.7 -2.4 -1.8 -35.5 -39.6 +0.0 +0.0 -0.8 +1.2 +0.1 +0.0 +1.1 -0.6 -1.2 +0.0 +0.3 +6.4 -1.0 +27.2 -9.1 +43.5 +63.2 +44.3 +23.1 +27.6 +59.6 +5.3 +4.8 +8.1 -1.4 -9.9 -3.5 -12.4 +5.9 -11.1 -6.2 -4.1 -12.6 -5.8 -6.3 -3.9 +6.2 -5.7 +0. -2.4 -4.4 -15.8 -20.1 -21.3 -0.3 -17.7 -17.3 -18.2 -7.7 -5.0 -36.6 -43.6 -0.2 -1.8 -3.6 +2.7 +1.5 -0.6 +2.5 -3.5 -1.4 +0.0 +0.3 +9.1 -1.0 -25.7 -29.6 -6.1 -13.7 -20.1 -10.5 -13.4 -12.8 -15.9 -29.8 -28.0 -16.3 -55.7 +0.2 +5.5 -2.2 +0.9 -0.5 +0.6 +0.2 -3.0 +4.2 +2.6 +5.2 +4.1 -21.9 +8.9 +2.5 +31.8 +35.8 +37.7 +27.0 +25.5 +37.2 +18.2 +4.9 -2.1 +1.7 -20.8 -2.3 +5.6 +8.1 -3.5 -4.5 -4.1 -3.0 +16.9 +7.2 +0.7 +0.3 -3.2 -15. -13.3 -19.0 -11.4 -15.2 -17.9 -2.2 -13.1 -12.3 -16.3 -21.4 -13.5 -17.8 -40.0 +0.0 +1.8 -1.8 -1.3 +0.2 -0.6 -0.2 -4.0 +3.0 +1.4 +4.1 +4.6 -16.3 -19.8 -15.7 -4.5 -13.3 -21.3 -23.0 -8.5 -19.0 -30.2 -11.2 -14.7 -18.4 -41.5 -2.2 -0.2 +0.1 +0.1 -0.2 -0.3 -0.5 -1.5 -3.6 +2.6 +3.4 +2.9 -11.2 +30.5 -13.9 -11.5 -11.6 -18.0 -41.7 -0.4 -10.2 -51.5 -2.3 -17.8 -22.5 -31.4 -5.6 +2.9 -3.4 -3.3 -3.2 -8.1 -2.6 -5.8 -6.0 -0.8 -0.9 +1.3 -17. -20.7 -13.3 -6.2 -10.4 -17.9 -27.2 -5.2 -16.2 -26.5 -15.0 -11.8 -13.5 -30.2 -2.3 -0.3 -0.5 -1.3 -0.5 -2.2 -0.4 -1.6 -4.5 +3.2 +1.0 +1.9 -8.7 -0.5 -13.6 -14.8 -10.0 -15.0 -5.0 -19.6 -23.9 -25.6 -0.0 -16.9 -41.6 -15.4 -0.1 -0.1 -2.4 +7.6 +2.0 +0.3 -0.9 -3.4 +0.5 +0.3 +2.4 +13.4 -9.1 -15.5 -20.5 -8.2 -3.6 -3.0 -20.7 -12.3 -0.7 -29.4 -0.7 -30.2 -22.8 -11.1 -5.4 -4.7 -1.8 -4.9 -7.1 +0.9 -1.8 -2.7 -1.2 -4.8 -1.6 +5.5 -3. -4.2 -8.4 -9.1 -7.1 -9.6 -4.1 -14.2 -19.2 -20.3 -0.3 -9.9 -26.9 -10.0 -1.5 -0.9 -1.2 +3.6 +1.4 -0.8 -0.5 -3.0 +0.8 +0.3 +1.4 +7.8 -4.7 +0.0 +0.0 +0.0 -0.0 -0.0 +0.0 -0.0 +0.0 -0.1 +0.0 +0.0 -0.0 +0.0 +0.0 +0.0 -0.0 -0.0 +0.0 +0.0 +0.0 -0.0 -0.1 +0.0 -0.1 +0.1 +0.0 +8.2 +10.2 +13.9 +51.4 +65.6 +45.3 +43.2 +50.1 +3.4 +4.8 +39.4 -18.4 -4.3 +5.4 -22.3 +1.2 -15.1 -13.4 -11.7 -18.1 -14.5 -2.4 -5.5 -2.5 -3.4 -0. -19.1 -24.5 -8.2 -14.1 -18.3 -19.9 -21.7 -24.5 -27.6 -24.4 -15.9 -6.3 -34.4 -1.8 +7.1 -2.6 +2.1 +1.6 -0.4 +0.3 -1.6 -7.2 +2.9 +1.6 +4.5 -5.5 Table 11. Relative performance variation with the generic/specific prompts on six datasets. TI stands for text inclusion, LI for Llama inclusion, and CS for concept similarity. Model IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B TI +6.0 +9.0 +2.3 +3.8 +6.5 +0.4 +3.7 +0.4 +8.2 +36.4 +6.0 +7.5 +7.9 DTD LI +3.2 +11.2 -5.9 +0.9 +1.5 -0.6 +1.1 -3.7 +8.7 +22.8 -4.5 +7.2 +3.1 CS +6.8 +6.1 +0.6 +1.3 +1.5 +0.7 +1.4 +0.8 +3.9 +23.9 -1.8 +5.5 +4.4 FGVCAircraft LI CS TI +2.4 +0.7 +1.5 +3.8 +2.6 +0.1 +3.4 +2.9 +7.5 +2.4 +1.4 +21.8 +38.2 -37.3 -0.3 -20.9 -25.3 -13.9 -8.4 -29.4 -27.0 -18.5 -5.4 -8.4 +0.7 +16.1 +2.2 +4.1 +7.6 +10.2 +8.4 +0.7 +10.0 +10.2 +16.9 +7.7 +5.0 +19.4 +42.2 Flowers102 LI CS TI +24.9 +26.6 +2.2 +5.8 +1.9 +3.8 +2.4 +1.0 +16.8 +16.0 +19.7 +10.2 +13.6 -10.2 -11.1 -24.6 -12.0 -9.4 -18.7 -5.5 -11.1 -17.1 -7.1 -6.5 -1.3 -1.8 +12.9 +17.9 +7.1 +5.6 +2.4 +1.6 +3.4 +2.0 +16.7 +10.7 +12.2 +11.1 +11.8 TI +15.3 +6.4 +0.8 +3.0 +2.0 +7.6 +3.5 +5.0 +25.6 +10.1 +5.4 +9.8 +7.7 Food101 LI CS TI OxfordPets LI CS StanfordCars LI CS TI -19.6 +1.5 +5.4 +4.1 +0.5 +3.6 -2.2 -2.2 +38.7 +3.5 +2.1 +6.9 +3.5 +15.0 +5.7 +1.8 +2.2 +1.5 +5.5 +2.0 +3.1 +20.6 +14.8 +3.8 +6.5 +4.9 +0.4 +12.8 +9.8 +4.7 +2.0 +3.9 -0.3 +14.7 +25.4 -0.0 +10.4 +38.7 +3. +9.9 +16.6 +5.9 +3.6 +3.8 +15.4 +8.8 +3.9 +10.9 -4.3 +5.0 +22.3 +9.5 +3.2 +7.9 +6.5 +3.8 +2.0 +3.7 +0.5 +12.4 +19.4 +0.5 +6.4 +24.5 +4.3 +0.0 +0.0 -0.0 +0.0 -0.1 +0.0 +0.0 -0.0 +0.1 +0.0 -0.1 +0.0 -0.1 -2.1 -7.4 -8.4 -18.5 -14.9 -3.7 -23.9 -7.6 -4.0 -8.7 -0.5 +17.8 +13.7 +16.8 +19.9 +3.1 +4.9 +0.8 +7.1 +4.5 +5.4 +19.6 +22.3 +9.0 +5.5 +17.2 Table 12. Relative performance variation with dataset-specific prompts on six datasets. TI stands for text inclusion, LI for Llama inclusion, and CS for concept similarity. (a) Default prompt (b) Ask for generic (c) Ask for specific (d) Dataset prompts Figure 12. Types of model predictions when using the generic and specific prompts and the dataset-specific prompts. Blue indicates correct and specific and correct but generic predictions, red indicates wrong but specific and wrong and generic mistakes. Model Be generic IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Be specific IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Dataset-specific IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Correct Specific Generic Wrong Specific Generic -5.9 -9.9 -5.3 -9.3 -20.6 -20.0 -8.0 -17.8 -7.1 -2.9 -11.9 -10.5 -36.4 -3.2 +2.1 -2.4 -1.1 -2.3 -2.9 -2.3 -4.5 -4.5 +2.1 +0.4 +2.7 -14.4 +14.9 +9.1 +2.6 +2.2 -0.1 +5.6 +0.3 +3.4 +29.2 +13.6 +5.5 +13.6 +15.0 +11.3 +2.2 +4.8 +14.5 +21.9 +3.8 +11.8 +22.2 -2.8 +2.9 +6.2 +0.6 +19.5 -3.3 -6.7 +0.2 -5.4 -3.5 -7.0 -3.4 -1.7 -1.8 -5.6 -3.6 -3.8 +0.8 -32.5 -10.7 -3.9 -5.5 -3.6 -7.0 -7.1 -8.6 -8.9 -14.8 -7.6 -5.8 -10. -1.4 +0.5 -1.2 -3.9 -5.5 -3.9 -3.2 -4.2 -0.8 +0.6 +0.1 +0.3 +1.4 +0.7 +0.6 +0.7 +1.8 +3.0 +0.8 +2.6 +2.0 -1.4 +1.9 +2.3 +3.7 +1.4 +7.0 +5.6 +1.7 +2.6 +2.5 +3.6 +4.6 +4.3 +1.6 +8.7 +2.6 +1.1 +2.1 -4.0 +7.3 +1.7 -1.4 +4.2 +20.0 -0.5 -0.2 +10.7 -0.5 +5.6 +9.6 +15.5 +5.8 +4.0 +1.5 +4.7 +2.8 +9.1 +3.1 +4.2 +7.7 +1.6 +0.9 -2.7 +12.1 +10.6 -4.0 -0.4 +0.7 +1.2 -2.2 +2.2 +1.0 -21.9 -7.5 -0.5 -8.8 -7. Table 13. Gains on the types of model prediction when instructing the models to be more generic/specific, and when using datasetspecific prompts techniques on six datasets, i.e., DTD, FGVCAircraft, Flowers102, Food101, OxfordPets, StanfordCars. 19 Model Caltech101 LI CS TI Zero-shot chain-of-thought INTERNVL2 [12, 13] 2B +0.3 -5.1 INTERNVL2 [12, 13] 4B -2.3 INTERNVL2 [12, 13] 8B +1.8 QWEN2VL [60] 2B -2.9 QWEN2VL [60] 7B LlamaV-o1 multi-round prompt INTERNVL2 [12, 13] 2B +0.4 -2.7 INTERNVL2 [12, 13] 4B -1.5 INTERNVL2 [12, 13] 8B +0.8 QWEN2VL [60] 2B -1.0 QWEN2VL [60] 7B LLaVA-COT prompt -0.6 INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B +0.1 -1.7 INTERNVL2 [12, 13] 8B +1.6 QWEN2VL [60] 2B +0.3 QWEN2VL [60] 7B -0.5 +2.2 +2.2 +3.6 +5.0 +2.4 -4.3 -2.0 +0.3 -3.0 +6.4 +5.0 +5.8 +0.4 +3. +4.0 -0.7 +0.5 +5.5 +1.5 +4.1 -3.3 +1.3 +4.8 -1.5 +4.1 +2.0 +1.3 +5.2 +3.4 DTD LI +18.5 +29.3 +20.7 +25.4 +21.5 +7.8 +4.1 +8.9 +8.1 +2. +19.8 +23.7 +25.3 +10.3 +12.5 TI +2.3 +1.4 +4.0 +6.5 +6.5 +3.0 -0.5 +3.2 +6.3 +3.3 +1.6 +0.8 +0.4 +4.4 +0.3 CS TI Flowers102 LI CS OxfordPets LI TI CS TI UCF101 LI CS +5.3 +3.1 +3.5 +8.2 +5.5 +4.1 -0.7 +3.6 +6.7 +0.2 +3.9 +2.9 +2.8 +6.4 +4. -3.5 +0.9 -1.7 -1.9 -4.9 -2.6 +0.2 -2.5 -5.8 -6.8 -3.8 -4.9 -8.9 -8.9 -10.2 -7.8 +16.0 +13.8 +5.4 +13.6 -7.0 -9.1 +3.0 -7.7 -17.3 +11.2 +30.8 +44.7 -7.9 +8. +1.3 +2.7 +0.1 +2.0 -1.5 +1.9 -0.5 -2.2 -4.1 -10.3 +1.2 -3.0 -6.8 -5.4 -7.2 +4.9 +3.0 +2.3 +6.8 +7.1 +9.9 +3.4 +8.2 +27.6 +22.9 +7.9 +6.7 -2.9 +5.6 +8. -14.9 +6.9 -9.7 +3.6 +17.0 -18.8 -31.1 -15.5 +0.1 -3.4 -8.6 +7.7 +28.9 -8.0 +16.2 +5.3 +4.9 +4.2 +6.7 +7.7 +8.3 -5.2 +7.6 +19.5 +16.4 +6.7 +7.2 +1.3 +6.1 +9. +6.3 +4.6 +4.3 +5.7 +2.3 +5.5 +4.1 +5.4 +7.9 +3.3 +5.8 +3.6 +3.2 +9.1 +6.6 +5.3 +23.9 +14.9 +23.2 +23.9 +11.4 +3.9 +3.1 +19.1 +10.9 +11.9 +16.4 +18.8 +18.5 +22. +6.3 +4.6 +3.2 +7.9 +5.4 +5.0 +2.6 +2.8 +9.7 +4.2 +5.2 +3.7 +0.8 +10.6 +7.1 Table 14. Relative performance variation with chain-of-thought prompts on five datasets. TI stands for text inclusion, LI for Llama inclusion, and CS for concept similarity. Model Correct Specific Generic Wrong Specific Generic Zero-shot chain-of-thought INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B QWEN2VL [60] 2B QWEN2VL [60] 7B +3.5 +4.8 +3.9 +8.0 +6.9 LlamaV-o1 prompt INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B QWEN2VL [60] 2B QWEN2VL [60] 7B LLaVA-CoT prompt INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B QWEN2VL [60] 2B QWEN2VL [60] 7B Reasoning models INTERNVL2.5 [11] 2B INTERNVL2.5 [11] 4B INTERNVL2.5 [11] 8B QWEN2.5VL [4] 3B QWEN2.5VL [4] 7B +6.7 +0.5 +3.7 +12.7 +4.4 7.3 5.2 1.5 6.3 6.3 -2.5 +4.7 +0.7 +10.8 +19.1 -5.5 +9.7 +2.1 +3.1 +8.6 -8.8 -9.8 -6.2 -8.6 -6.3 -1.2 10.0 22.5 -4.2 7. -7.6 -2.4 -0.3 -6.5 -9.4 +1.4 -2.0 -1.1 -0.4 -2.5 +0.4 +0.4 +0.6 +1.0 +0.8 -0.9 -2.3 -2.8 0.3 -2.0 +4.2 +4.4 +4.1 +2.6 +3.5 +0.6 -12.5 -4.9 -10.8 -13. +1.7 +9.0 +1.9 -5.1 +1.0 -5.2 -12.8 -21.1 -2.4 -11.3 +6.0 -6.6 -4.5 -6.9 -13.2 Table 15. Gains on the types of model prediction when instructing the models to reason with chain-of-thought, and when using reasoning models on five datasets, i.e., Caltech101, DTD, Flowers102, OxfordPets, UCF101. 20 Model List IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Caption IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Describe IDEFICS2 [34] 8B INSTRUCTBLIP [19] Vicuna 7B INTERNVL2 [12, 13] 2B INTERNVL2 [12, 13] 4B INTERNVL2 [12, 13] 8B LLAVA-1.5 [41] 7B LLAVA-NEXT [36] (Mistral 7B) LLAVA-NEXT [36] (Vicuna 7B) LLAVA-OV [37] (Qwen2 0.5B) LLAVA-OV [37] (Qwen2 7B) PHI-3-VISION [1] QWEN2VL [60] 2B QWEN2VL [60] 7B Correct Specific Generic Wrong Specific Generic -4.2 +9.6 -3.1 -1.4 -2.6 -2.6 -4.4 -1.6 +3.6 +9.3 -0.5 +2.4 -10. +0.1 +2.3 -4.0 -2.1 -7.4 +6.4 -20.2 -14.2 +4.7 +0.7 +10.9 +3.8 +2.7 -9.4 9.9 5.6 4.6 1.1 7.2 -2.1 1.1 15.0 19.8 10.3 8.7 8.8 -9.5 -16.1 -14.2 -10.9 -6.7 -13.7 +1.3 -5.7 -14.7 -13.5 -16.8 -5.0 -9.1 -12.0 -13.1 -6.6 -7.9 -2.5 -15.9 +2.9 -2.7 -8.4 -10.6 -13.2 -0.9 -4.7 -13.9 -11.2 -11.6 -11.9 -6.7 -17.4 1.6 -5.0 -5.8 -10.5 -11.8 -7.9 -8.9 +4.0 +1.5 +3.5 +0.9 +0.8 +2.6 -0.4 +1.3 +2.0 +2.2 +2.5 -0.3 +2. +3.2 +2.8 +1.0 +1.1 +0.9 +1.2 +2.4 +2.2 +0.0 +2.7 +0.3 -0.1 +0.2 4.2 1.6 0.6 1.0 0.3 1.5 -0.9 0.0 -2.1 -0.5 -0.2 0.1 0.3 +9.7 +5.0 +13.9 +11.3 +8.5 +13.6 +3.5 +6.0 +9.1 +2.0 +14.9 +3.0 +17.2 +8.6 +8.0 +9.6 +8.9 +9.0 +8.4 +15.0 +14.6 +3.7 +7.2 +2.0 -2.8 +1.8 19.1 -0.3 5.4 6.2 5.3 8.7 1.4 3.9 -7.0 -8.8 1.6 -0.9 -0.2 Table 16. Gains on the types of model prediction when instructing the models with multi-label prompts on five datasets, i.e., Caltech101, DTD, Flowers102, OxfordPets, UCF101. 21 (a) Default prompt (b) List objects (c) Caption (d) Describe Figure 13. Types of model predictions when using multi-label prompts. Blue indicates correct and specific and correct but generic predictions, red indicates wrong but specific and wrong and generic mistakes."
        },
        {
            "title": "Model",
            "content": "C101 DTD ESAT FGVC FLWR FOOD PETS CARS S397 U101 Avg."
        },
        {
            "title": "Datasets",
            "content": "Text inclusion INTERNVL2.5 [10] 2B INTERNVL2.5 [10] 4B INTERNVL2.5 [10] 8B QWEN2.5VL [4] 3B QWEN2.5VL [4] 7B Closed-world baselines CLIP [49] SigLIP [68] Llama inclusion INTERNVL2.5 [10] 2B INTERNVL2.5 [10] 4B INTERNVL2.5 [10] 8B QWEN2.5VL [4] 3B QWEN2.5VL [4] 7B Closed-world baselines CLIP [49] SigLIP [68] Semantic similarity INTERNVL2.5 [10] 2B INTERNVL2.5 [10] 4B INTERNVL2.5 [10] 8B QWEN2.5VL [4] 3B QWEN2.5VL [4] 7B Closed-world baselines CLIP [49] SigLIP [68] Concept similarity INTERNVL2.5 [10] 2B INTERNVL2.5 [10] 4B INTERNVL2.5 [10] 8B QWEN2.5VL [4] 3B QWEN2.5VL [4] 7B Closed-world baselines CLIP [49] SigLIP [68] 55.8 55.6 56.4 62.1 65.6 87.1 93.6 76.8 77.1 78.4 81.4 84.5 87.1 93. 49.5 51.7 53.2 51.8 48.8 90.8 97.8 78.0 77.3 77.7 81.8 85.8 90.8 97.8 12.6 10.9 12.1 13.9 16.7 52.6 60. 49.2 48.7 48.9 58.1 59.8 52.6 60.8 25.2 26.7 27.1 27.4 28.2 69.9 75.6 46.2 44.8 45.4 51.3 53.2 69.9 75. 12.6 12.1 8.4 1.6 4.4 42.7 42.1 47.2 42.6 45.5 6.3 12.6 42.7 42.1 31.4 31.7 29.5 12.3 18.9 67.7 63. 59.9 57.1 52.7 23.8 41.3 67.7 63.1 1.5 0.9 3.0 18.8 32.7 27.2 46.0 55.4 61.4 59.1 58.9 69.6 27.2 46. 21.4 20.9 21.4 28.9 36.5 66.7 80.0 33.1 31.1 31.5 52.4 68.4 66.7 80.0 10.9 12.2 16.8 49.7 56.1 76.9 88. 42.4 43.3 51.2 71.5 75.2 76.9 88.2 26.7 29.4 32.1 45.4 47.4 83.4 92.0 47.8 49.3 54.2 72.6 79.7 83.4 92. 17.0 24.9 29.7 44.2 54.9 89.9 94.1 34.2 52.0 53.2 68.7 76.4 89.9 94.1 33.5 41.6 42.2 48.0 52.4 93.7 96. 53.5 61.7 64.5 73.2 79.6 93.7 96.4 8.7 14.6 7.2 38.9 65.1 88.1 95.4 39.2 49.4 48.2 51.4 71.0 88.1 95. 22.4 27.4 24.2 31.4 41.1 91.8 96.8 39.7 45.8 41.9 62.1 77.3 91.8 96.8 0.0 0.0 0.1 0.0 0.0 76.2 92. 49.3 49.8 60.6 58.9 71.2 76.2 92.3 41.8 41.9 42.9 50.9 55.0 80.5 98.1 50.0 48.3 49.2 64.7 68.4 80.5 98. 16.3 23.7 24.7 30.7 33.6 65.6 69.9 49.3 63.1 62.7 78.9 81.1 65.6 69.9 39.8 46.5 47.0 47.0 47.0 92.2 83. 59.5 66.1 66.3 70.8 74.1 92.2 83.1 13.7 14.9 13.8 18.0 21.5 72.7 82.1 51.1 53.6 52.7 58.8 67.0 72.7 82. 41.5 43.5 43.2 43.2 44.2 83.3 89.6 61.4 61.8 62.2 62.9 67.1 83.3 89.6 14.9 17.0 17.2 27.8 35.1 67.9 76. 49.4 54.1 56.1 59.3 66.8 67.9 76.5 33.3 36.1 36.3 38.6 42.0 82.0 87.3 52.9 54.3 54.6 61.6 69.5 82.0 87. Table 17. OW results of reasoning models on ten datasets. Higher is better, bold indicates best. Note that the Llama inclusion for CLIP closed-world equals the textual inclusion scores."
        }
    ],
    "affiliations": [
        "Fondazione Bruno Kessler",
        "Independent researcher",
        "University of Trento"
    ]
}