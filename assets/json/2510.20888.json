{
    "paper_title": "Video-As-Prompt: Unified Semantic Control for Video Generation",
    "authors": [
        "Yuxuan Bian",
        "Xin Chen",
        "Zenan Li",
        "Tiancheng Zhi",
        "Shen Sang",
        "Linjie Luo",
        "Qiang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified, generalizable semantic control in video generation remains a critical open challenge. Existing methods either introduce artifacts by enforcing inappropriate pixel-wise priors from structure-based controls, or rely on non-generalizable, condition-specific finetuning or task-specific architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes this problem as in-context generation. VAP leverages a reference video as a direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture prevents catastrophic forgetting and is guided by a temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval. To power this approach and catalyze future research, we built VAP-Data, the largest dataset for semantic-controlled video generation with over 100K paired videos across 100 semantic conditions. As a single unified model, VAP sets a new state-of-the-art for open-source methods, achieving a 38.7% user preference rate that rivals leading condition-specific commercial models. VAP's strong zero-shot generalization and support for various downstream applications mark a significant advance toward general-purpose, controllable video generation."
        },
        {
            "title": "Start",
            "content": "Video-As-Prompt: Unified Semantic Control for Video Generation Yuxuan Bian 1,2 Xin Chen 1,, Shen Sang 1 Zenan Li 1 Linjie Luo 1, Qiang Xu 2, Tiancheng Zhi 1 5 2 0 2 3 ] . [ 1 8 8 8 0 2 . 0 1 5 2 : r 1Intelligent Creation Lab, ByteDance, 2The Chinese University of Hong Kong Project lead, Corresponding Authors"
        },
        {
            "title": "Abstract",
            "content": "Unified, generalizable semantic control in video generation remains critical open challenge. Existing methods either introduce artifacts by enforcing inappropriate pixel-wise priors from structure-based controls, or rely on non-generalizable, condition-specific finetuning or task-specific architectures. We introduce Video-As-Prompt (VAP), new paradigm that reframes this problem as in-context generation. VAP leverages reference video as direct semantic prompt, guiding frozen Video Diffusion Transformer (DiT) via plug-and-play Mixture-of-Transformers (MoT) expert. This architecture prevents catastrophic forgetting and is guided by temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval. To power this approach and catalyze future research, we built VAP-Data, the largest dataset for semantic-controlled video generation with over 100K paired videos across 100 semantic conditions. As single unified model, VAP sets new state-of-the-art for open-source methods, achieving 38.7% user preference rate that rivals leading condition-specific commercial models. VAPs strong zero-shot generalization and support for various downstream applications mark significant advance toward general-purpose, controllable video generation. Date: October 27, 2025 Project Page: https://bytedance.github.io/Video-As-Prompt"
        },
        {
            "title": "Introduction",
            "content": "While unified structure-controlled video generation [34] under pixel-aligned conditions (e.g., depth [16], pose [28], mask [6], optical flow [35]) is well studied, semantic-controlled generationlacking pixel-aligned condition (e.g., concept [47], style [78], motion [82], camera [2]) to the target videoremains fragmented without unified and generalizable framework, limiting applications in visual effects [48], video stylization [32], motion imitation [71], and camera control [3]. Migrating current unified structure-controlled methods [34, 81] often causes artifacts because they enforce inappropriate pixel-wise mapping priors from structure-based control abilities (see Fig. 2 (a)). Other semanticcontrolled methods fall into two groups: (1) Condition-Specific Overfit (see Fig. 2 (b)): methods [11, 47] finetune backbones [70, 76] or LoRAs [27] for each semantic condition (e.g., Ghibli style, Hitchcock camera zoom), which is costly; (2) Task-Specific Design (see Fig. 2 (c)): methods [2, 78, 82] craft task-specific modules or inference strategies for condition type (e.g., style, camera), often encoding videos with the same 1 Figure 1 Video-As-Prompt (VAP) is unified semantic-controlled video generation framework: it treats reference videos with wanted semantics as video prompts and controls generation via plug-and-play, in-context Mixture-of-Transformers expert. Row 1 6: reference videos used as prompts for diverse semantic-controlled video generation tasks (concept, style, motion, camera). Row 7: zero-shot generalization results from Video-As-Prompt when given an unseen semantic, demonstrating strong generalizability. semantics to specially designed space and guiding generation. While effective, these condition/task-specific approaches hinder unified model and limit their zero-shot generalizability. However, recent unified image generation [66] and structure-controlled video generation [37] show that Diffusion Transformers (DiTs) support strong in-context control abilities, motivating unified framework for in-context semantic-controlled video generation. As shown in Fig. 2 (d), rather than assuming pixel-wise correspondence [34], training per-condition models [47] or using task-specific designs [78], we treat the video of the wanted semantics as reference video prompt and guide generation via in-context control. This formulation removes the inappropriate pixel-wise mapping prior from structure-based controls, avoids percondition training or per-task model designs, and enables single unified model to handle diverse semantic controls and generalize in zero-shot manner to unseen semantics (see Fig. 1). We present Video-As-Prompt (VAP), the first unified framework for semantic-controlled video generation under non-pixel-aligned conditions, by treating reference video with the wanted semantics as video prompt and using plug-and-play in-context control. As shown in Fig. 2 (d), VAP adopts plug-and-play Mixture-of-Transformers (MoTs) design [44] to augment any frozen Video Diffusion Transformer [54] with trainable parallel expert for interpreting the video prompt and guiding the generation, preventing catastrophic forgetting and enabling in-context control. The expert (for the reference prompt) and the frozen backbone (for target generation) run independent feed-forward and layer-norm paths and communicate via full attention for synchronous layer-wise reference guidance. For robust context retrieval, we adopt temporally biased 2 Figure 2 Controllable Video Generation Paradigms. Structure-Controlled Video Generation (a). The condition is pixel-aligned with the target video. Most works inject conditions (e.g., depth, optical flow, pose) into DiT via an extra branch using (a) residual addition, leveraging pixel-wise alignment. Semantic-Controlled Video Generation (b, c, d). The condition and target video share the same semantics. Most works use (b) Condition-Specific Overfit or (c) Task-Specific Design: finetuning per semantic or adding task-specific modules. (d) Video-as-Prompt: We use reference video with the same semantics as prompts and adopt plug-and-play in-context control framework built on mixture-of-transformers to achieve unified semantic-controlled video generation. Rotary Position Embedding (RoPE) that places the reference before the current video along the temporal axis while keeping spatial fixed; this removes the nonexistent pixel-mapping prior from shared RoPE, matches the temporal order expected by in-context generation, and preserves spatial consistency so the model can exploit spatial semantic changes of the reference video prompt. Existing datasets [34, 37] lack focus on semantic-controlled video generation. We introduce VAP-Data, the largest to date, with over 100K curated samples across 100 semantic conditions, providing robust data foundation for unified semantic-controlled video generation. Extensive experiments show that VAP, unified model for diverse semantic conditions (Sec. A) and downstream generation tasks (Sec. B), produces coherent, semantically aligned videos, achieves 38.7% user preference rate competitive with leading closed-source commercial models, surpasses condition-specific methods, and exhibits zero-shot generalizability  (Fig. 7)  . Our contributions are highlighted as follows: We present VAP, unified semantic-controlled video generation paradigm, treating reference video with the wanted semantics as video prompt for generalizable in-context control. We propose plug-and-play in-context video generation framework built on the mixture-of-transformers architecture that prevents catastrophic forgetting, supports various downstream tasks, and delivers strong zero-shot generalizability to unseen semantic conditions. We construct and release VAP-Data, the largest dataset for semantic-controlled video generation, with over 100K curated paired samples across 100 semantic conditions."
        },
        {
            "title": "2.1 Video Generation",
            "content": "Video generation has progressed from early GAN-based models [63, 69] to modern diffusion models [8, 17]. Leveraging the scalability of diffusion transformers (DiTs) [54], research has moved from convolutional architectures[7, 10, 20, 62, 79] to transformer-based ones [8, 17, 24, 41, 51, 57, 70]. The standard pipeline encodes Gaussian noise into latent space with video VAE encoder [38], splits the latents into patches, processes the patches with DiT, and decodes to the original pixel space to produce high-quality, smooth, coherent videos. However, pre-trained DiTs typically support only text prompts or first/last-frame control [17, 70]. To enable finer, user-defined control, many methods add task-specific modules to pre-trained DiTs [6, 34] or design special inference [71, 82] for new controllable video tasks."
        },
        {
            "title": "2.2 Controllable Video Generation\nIn general, controllable video generation can be categorized into Structure-Controlled Video Generation and\nSemantic-Controlled Video Generation (see top of Fig. 2). The former [6, 34] is driven by pixel-aligned\nconditions (e.g., depth, pose, mask, optical flow), while the latter [78, 82] focuses on generation based on\nsemantic conditions without pixel mapping prior (e.g., concept, style, motion, camera).",
            "content": "Structure-Controlled Video Generation In structure-controlled video generation, condition videos (e.g., depth, optical flow, pose) are typically pixel-aligned with the target videos, so control signals are mostly modeled with an additional adapter/branch and injected via residual addition to exploit this mapping prior [5, 45, 52, 81], as shown in Fig. 2 (a). Common conditions include Trajectory [1, 18, 21, 22, 72, 85], Pose [28, 36, 43], Depth [14, 16], Optical flow [29, 35, 42, 83], and Mask [6, 75]. Recent works [34, 37] further enable all-in-one structure-controlled generation by treating these inputs as unified, pixel-aligned spatial condition. Semantic-Controlled Video Generation Semantic-controlled video generation handles conditions which lack pixel-wise correspondence with target videos (see Fig. 1), including Concept[26, 40, 47, 55, 56, 68] (e.g., turning an object into Ladudu or taking it like paper man), Stylization[31, 78], Camera Movement[2, 3, 25], and Motion[19, 33, 50, 58, 60, 71, 77, 82, 84], where the reference and target share motion but differ in layout or skeleton. As shown in Fig. 2 (b) and (c), prior methods fall into Condition-Specific Overfit [11, 47], which fine-tune DiT backbones or LoRAs for each semantic condition; and Task-Specific Design [2, 71, 78, 82], which add task-specific modules or inference strategies for class of semantic conditions (e.g., style, motion, camera). Above approaches fit narrow distributions but are not unified and generalizable; they require per-condition retraining or per-task designs and lack zero-shot generalization. concurrent work [49] adopts LoRA mixture-of-experts for unified generation across multiple semantic conditions, but it still learns each condition by overfitting subsets of parameters and fails to generalize to unseen ones. This raises key question: How can we build unified semantic-controlled video generation framework? Inspired by in-context learning [4, 23, 30, 37, 66], we propose Video-As-Prompt (VAP), which treats videos with the wanted semantics as unified in-context prompts to guide generation. By casting the task as an in-context generation with reference video prompts, VAP, to our knowledge, is the first to unify multiple semantic-controlled tasks without task-specific designs, while achieving strong zero-shot abilities."
        },
        {
            "title": "3 Methods",
            "content": "VAP supports unified semantic-controlled video generation under various semantic conditions (e.g., concept, style, motion, and camera). Our insight is to use videos with the wanted semantics as unified prompts to guide generation across tasks, avoiding per-condition finetuning or per-task designs. Although we study limited set of conditions, the method extends to others without major structural changes and shows promising generalizability for different semantic conditions (see Sec. A), various downstream tasks (see Sec. B), and unseen semantics in VAP-Data (see Fig. 7)."
        },
        {
            "title": "3.1 Preliminary\nVideo diffusion models [8, 17] learn the conditional distribution p(x | C) of video x given conditions C.\nUsing Flow Matching [46] for illustration, a noise sample x0 ∼ N (0, 1) is denoised to x1 along the path\nxt = tx1 + (1 − (1 − σmin)t)x0, where σmin = 10−5 and t ∈ [0, 1]. The model u is trained to predict the\nvelocity Vt = dxt\ndt = x1 − (1 − σmin)x0. We optimize u with parameters Θ by\ndt\nminimizing the mean squared error loss L between the ground-truth velocity and the model prediction:",
            "content": ", which simplifies to: Vt = dxt = Et,x0,x1,C uΘ(xt, t, C) (x1 (1 σmin)x0) During inference, the model first samples Gaussian noise x0 (0, 1) and then uses an ODE solver with discrete set of denoising timesteps to produce x1. 4 Figure 3 Overview of Our Proposed VAP-Data. (a) 100 semantic conditions across 4 categories: concept, style, camera, and motion; (b) diverse reference images, including animals, humans, objects, and scenes, with multiple variants; and (c) word cloud of the semantic conditions."
        },
        {
            "title": "3.2 Reference Videos as Task-Agnostic Prompts\nSemantic-controlled video generation spans diverse condition types (e.g., concept, style, motion, camera).\nStructure-controlled methods assume pixel-wise alignment between condition and target [34, 81]; injecting\na semantic-same but pixel-misaligned video condition via residual addition yields copy-and-paste artifacts\n(see Fig. 5 (a)). Prior semantic-controlled video generation works partially tackle this by using per-condition\nfine-tuning or per-task designs, treating tasks in isolation. In contrast, VAP employs reference videos as video\nprompts, which share the same semantics as the targets, independent of task category, unifying heterogeneous\nconditions in one unified model. Formally, let C = (cid:83)n\ni=1 Ci denote n condition types with conditions c ∈ Ci\n(total m); prior methods often fine-tune n (per-task) or up to m (per-condition) models, whereas we train\na single unified model uΘ that jointly learns p(x | c) for any c ∈ C. We evaluate four representative\ntypes—concept (Cco), style (Cs), motion (Cm), and camera (Cca)—chosen for distinct task definitions. Our\ndataset VAP-Data follows this taxonomy. The dataset overview can be seen in Fig. 3.",
            "content": "Concept-Guided Generation: Videos sharing concept, such as entity transformation (e.g., person becomes Ladudu doll) or interaction (e.g., an AI lover approaches the target). Style-Guided Generation: Videos in reference style (e.g., Ghibli, Minecraft). Motion-Guided Generation: Videos following reference motion, including non-human motion (e.g., objects expand like balloons) and human motion (e.g., shake it dance). Camera-Guided Generation: Videos following reference camera motion, from basic translations (up, down, left, right, zoom in/out) to the Hitchcock dolly zoom. Discussion. We also input captions (Pref , Ptar) of the reference video and target video to aid in finding and transferring the shared mentioned semantic control signals (e.g., cover liquid metal in Fig. 6). Thus uΘ learns conditional distribution p(x Cco, Cs, Cm, Cca, Pref , Ptar). 5 Figure 4 Overview of Video-As-Prompt. The reference video (with the wanted semantics), target video, and their first frames (reference images) are encoded into latents by the VAE and, together with captions (See top right), form an in-context token sequence [Reftext, Refvideo, artext, arvideo] (See middle. We omitted term tokens for simplicity.). First frame tokens are concatenated with video tokens. We add temporal bias to RoPE to avoid nonexistent pixel-aligned priors from the original shared RoPE (See bottom right). The reference video and captions act as the prompts and are fed into trainable DiT Expert Transformer3(See left), which exchanges information bidirectionally with the pre-trained DiT via full attention at each layer, enabling plug-and-play in-context generation."
        },
        {
            "title": "3.3 Plug-and-Play In-Context Control",
            "content": "Our model takes four primary inputs: reference video (providing the wanted semantics), reference image1 (providing the wanted initial appearance and subject), captions (aiding in finding the target semantic), and noise (for inference) or noisy target video (for training). We first encode the reference video Rnhwc and the target video Rnhwc into latents ˆc Rnhwd and Rnhwd by VAE. Here and are original temporal/spatial sizes; n, h, are latent sizes. With nt text tokens tˆc, tx Rntd, naive baseline is to finetune the DiT on the concatenated sequence [tˆc, ˆc, tx, x]2, following in-context structurecontrolled generation [37]. This often leads to catastrophic forgetting with limited data (Fig. 5 (b), Tab. 2), because (1) DiTs are pre-trained only for generation, not in-context conditioning, and (2) our reference/target pairs lack pixel-aligned priors, making semantic in-context generation much harder. To stabilize training, we adopt Mixture-of-Transformers (MoT) [44]: frozen Video Diffusion Transformer (DiT) plus trainable parallel expert transformer initialized from the backbone. The expert consumes [tˆc, ˆc], while the frozen DiT processes [tx, x] (see Fig. 4). Each keeps its own query, key, value projections, feed-forward layers, and norms; at each layer, we concatenate Q/K/V and run full attention for two-way information fusion and in-context control. This shapes references into prompts conditioned on the current generation and routes guidance into the frozen DiT. With MoT, we preserve the backbones generation ability, boost the training stability, and achieve plug-and-play in-context control independent of DiT architecture."
        },
        {
            "title": "3.4 Temporally Biased Rotary Position Embedding",
            "content": "Similar to observations on Rotary Position Embedding (RoPE) [65] in in-context image generation [66], we find that sharing position embedding between the reference condition and the target video is suboptimal: it imposes false pixel-level spatiotemporal mapping prior, making the model assume nonexistent mapping between the reference and the target videos, and perform unsatisfactorily (see artifacts in Fig. 5 (c)). Accordingly, we shift the reference prompts temporal indices by fixed offset , placing them before all noisy video tokens while keeping spatial indices unchanged (see right bottom of Fig. 4). This removes the spurious prior, matches the temporal order expected by in-context generation, and leads to improved performance (see Tab. 2). 1The first frame of the reference video is also injected for inheriting the Image-to-Video backbone ability. 2Without loss of generality, we assume text and video are jointly modeled with full attention. 3The number and position of In-Context DiT Expert layers are fully customizable. 6 Figure 5 Motivation. Ablation visualizations (Semantic: Spin 360) on structure designs of VAP."
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "We train VAP on CogVideoX-I2V-5B [76] and Wan2.1-I2V-14B [70] to evaluate effectiveness across DiT architectures.4 For fairness, we match parameter counts: on CogVideoX-I2V-5B, the in-context DiT expert is full copy of the original; on Wan2.1-I2V-14B, it is distributed copy spanning 1 of layers; both are about 4 5B parameters. Following pre-trained DiTs, we resize videos to 480720(832) and sample 49 frames at 16 fps. We use AdamW with learning rate 1 105 and train for 20k steps on 48 NVIDIA A100s. At inference, we use 50 denoising steps and classifier-free guidance scale 6 (5). More details are in Sec. C."
        },
        {
            "title": "4.3 Dataset",
            "content": "Semantic-controlled video generation requires paired reference and target videos sharing the same non-pixelaligned semantic controls (e.g., concept, style, motion, camera). Unlike structure-controlled settings, such pairs cannot be labeled by directly applying vision perception models (e.g., SAM [39], Depth-Anything [74]). Prior work mostly relies on few manually collected videos tailored to specific semantic conditions [47], limiting the emergence of unified models. To address this, we collect 2K high-quality reference images from the Internet, spanning men, women, children, animals, objects, landscapes, and multi-subject cases. We then use Image-to-Video visual-effects templates from commercial models (VIDU [68] and Kling [40]) and community LoRAs [11] to create paired videos by matching each image to all compatible templates (some restrict subject categories). Overall, we obtain VAP-Data, semantic-controlled paired dataset with over 100K samples across 100 semantic conditionsthe largest resource(see Sec. 3.2 and Fig. 3). For evaluation, we evenly sampled 24 semantic conditions from 4 categories (concept, style, motion, camera) in the test subset, with 2 samples each. Detailed information and limitations are in Sec. D."
        },
        {
            "title": "4.4 Comparison with Previous Methods",
            "content": "We evaluate VAP against: (1) the state-of-the-art (SOTA) structure-controlled video generation method VACE [34] under multiple structure conditions (e.g., original reference video, depth, optical flow); (2) condition-specific methods, where we train LoRA [27] for each semantic conditiona common community practice often reported to match or surpass task-specific models [2, 78]and report averaged performance; (3) state-of-the-art closed-source commercial models, including Kling [40] and Vidu [68]. Quantitative Comparison. For the SOTA structure-controlled method VACE [34], the model conditions on video and same-size mask indicating edit (1) vs. fixed (0) regions. Following VACE, we use the 4As Wan2.1 is more resource-intensive, results are reported on CogVideoX unless otherwise noted. 7 Figure 6 Qualitative comparison with VACE [34], CogVideoX (I2V) [76], CogVideoX-LoRA (I2V) and commercial models [40, 68]; VACE(*) uses *-form condition (top left). More visualizations are in the project page. reference video, its depth, and its optical flow as video conditions, setting the mask to 1 so the model follows rather than copies them. Overall, VACE performs worst, as expected when structure-controlled methods are applied directly to semantic-controlled generation. This is because VACE assumes pixel-wise mapping between the condition and the output (e.g., video and its depth), which breaks under semantic control and copies unwanted appearance or layout from the reference. As control moves from raw video, depth to optical flow, appearance detail decreases, and metrics improve, confirming that the pixel-wise prior is ill-suited for semantic-controlled generation. Driving pre-trained DiT (CogVideoX-I2V) with captions carrying semantic cues yields decent video quality but weak semantic alignment, since many semantics are hard to express with coarse text. Common LoRA fine-tuning often obtains strong semantic alignment by overfitting specific condition: it harms base quality (vs. the CogVideoX-I2V row), needs separate model per condition, and fails to generalize to unseen references. By contrast, VAP outperforms open-source baselines on most metrics, achieves performance comparable to commercial models, and, for the first time, provides unified model for semantic-controlled video generation. User Study We conducted user study with 20 randomly selected video-generation researchers to evaluate video quality and semantic alignment. In each trial, raters compared different method outputs shown with semantic-control reference video and chose the better result for (i) semantic alignment and (ii) overall quality. We report the preference ratethe normalized share of selections across all comparisons, totaling 100%in Tab. 1. VAP and Kling/Vidu (commercial, closed-source, task-specific) achieve the overall highest preference rate, while VAP works as unified model. Qualitative Comparison In Fig. 6, VAP yields better temporal coherence, visual quality, and semantic consistency than structure-controlled baselines [34], DiT backbones, and condition-specific finetuning [27], and matches condition-specific commercial models Kling [40] and Vidu [68]. VACEs pixel-mapping bias treats the semantic reference video as pixel-aligned, causing appearance/layout copying (e.g., the frog stands like the dog; the Statue of Liberty imitates sheep); this artifact weakens when the reference is replaced by depth and then optical flow, which progressively remove appearance details. LoRA finetuning improves semantic alignment 8 Metrics Model Text Overall Quality Semantic User Study Clip Score Motion Smoothness Dynamic Degree Aesthetic Quality Alignment Score Preference Rate (%) VACE (Original) VACE (Depth) VACE (Optical Flow) CogVideoX-I2V CogVideoX-I2V (LoRA) Kling / Vidu 5.88 22.64 22.65 22.82 23.59 24.05 Structure-Controlled Methods 97.60 97.65 97.56 68.75 75. 79.17 DiT Backbone and Condition-Specific Methods 98.48 98.34 98.12 Ours 72.92 70.83 79. 53.90 56.03 57.34 56.75 54.23 59.16 35.38 43.35 46.71 26.04 68.60 74. 0.6% 0.7% 1.8% 6.9% 13.1% 38.2% Video-As-Prompt (VAP) 24.13 98. 77.08 57.71 70.44 38.7% We fine-tune LoRA on CogVideoX-I2V for each semantic condition in the benchmark and report the average metric as performance. Kling and Vidu provide dedicated interfaces for each semantic condition; thus, we treat them as condition-specific. We report the preference rate by aggregating wins over all comparisons. Each cell is the average rate of human preferences received by the corresponding method. Table 1 Qualitative Comparison. We compare against the SOTA structure-controlled generation method VACE [34], the base video DiT model CogVideoX-I2V [76], the condition-specific variant CogVideoX-I2V (LoRA) [27], and the closed-source commercial models Kling/Vidu [40, 68]. Overall, VAP delivers performance comparable to the closed-source models and, on average, surpasses the other open-source baselines, as unified and generalizable model. Red stands for the best, Blue stands for the second best. Figure 7 Zero-Shot Performance. Given semantic conditions unseen in VAP-Data (left column), VAP still transfers the abstract semantic pattern to the reference image in zero-shot manner. without copy artifacts but requires separate model per condition and lacks zero-shot generalization. In contrast, VAP uses single model that treats all semantic conditions as unified reference-video prompt, enabling unified semantic-controlled generation. Zero-Shot Generation By treating all semantic conditions as unified video prompts, VAP supports diverse semantic-controlled generation tasks; moreover, when given an unseen semantic reference [47] that doesnt belong to VAP-Data (see Fig. 7), the in-context ability learned from video-as-prompt data enables VAP to perform zero-shot generation guided by new references."
        },
        {
            "title": "4.5 Ablation Study\nIn-Context Generation Structure. We train 4 VAP variants to test the effectiveness of our mixture-of-\nΘ: expand pre-trained DiT input sequence\ntransformers (MoTs) adoption: A1. Single-Branch Finetuning us\nto [Reftext, Refvideo, T artext, T arvideo] and finetune the full model; A2. Single-Branch LoRA Finetuning usl\nΘ:\nsame as A1 but freeze the backbone and train only the LoRA layers; A3. Unidirectional Cross-Attn uuc\nΘ :\nfreeze the pre-trained DiT, add a new branch with the same weights, and inject its features via layer-wise\ncross-attention; and A4. Unidirectional Addition uua\nΘ : same as A3 but inject features via residual addition.\nWe evaluate on the same benchmark of VAP-Data. Results in Tab. 2 show: A1. MoT boosts performance by",
            "content": "9 Metrics Variant Text Overall Quality Semantic CLIP Score Motion Smoothness Dynamic Degree Aesthetic Quality Alignment Score us Θ usl Θ uuc Θ uua Θ (Unidir-Cross-Attn) (Single-Branch-LoRA) (Unidir-Cross-Attn) (Unidir-Addition) ui Θ un Θ (Identical PE) (Neg. shift in T, ) uΘ (1K) uΘ (10K) uΘ (50K) 23.03 23.12 22.96 22. 23.17 23.45 22.84 22.87 23.29 In-Context Generation Structure 97.97 98.25 97.94 97.63 70.83 72.92 66.67 62.50 Position Embedding Design Scalability 98.49 98.53 92.12 94.89 96.72 DiT Structure 70.83 72. 60.42 64.58 70.83 56.93 57.19 56.88 56.91 57.09 57.31 56.77 56.79 56.82 68.74 69.08 67.16 55.99 68.98 69. 63.91 66.28 68.23 uWan Θ (Wan2.1-I2V-14B) 23.93 97.87 79. 58.09 70.23 Ours uΘ (VAP) Notation. uΘ (our VAP parameterized by Θ). (in-context single-branch finetuning), sl (in-context single-branch LoRA finetuning), uc (unidirectional cross-attention injection), ua (unidirectional residual addition), (identical position embedding in reference and target), (temporal shift + negative temporal/width shifts of position embedding), Wan (Wan2.1 as DiT backbone). 70.44 98. 77.08 24.13 57.71 Scale. uΘ(M ) indicates the number of training pairs (M {1K, 10K, 50K, 100K}). Our final version uses 100K training pairs. Table 2 Ablation Study. We ablate on the in-context generation structure designs, temporal-biased RoPE, the scalability, and the transferability across different DiT structures. The last row is our default model (VAP), which uses MoT structure, temporal-biased RoPE, 100K training pairs, and CogVideoX-I2V-5B. Red stands for the best, Blue stands for the second best. preserving the base DiTs generative ability, solving the catastrophic forgetting while enabling plug-and-play in-context control. A2. LoRA helps retain the backbones ability, but its limited capacity struggles with complex in-context generation, yielding suboptimal results. A3. Layer-wise bidirectional information exchange in MoT lets the reference video-prompt representation adapt synchronously to the target tokens, improving semantic alignment. A4. Even with retraining, residual-addition methods rely on rigid pixel-to-pixel mapping, mismatching semantic-controlled generation and degrading performance. Position Embedding Designs. To validate the effectiveness of our temporally-biased RoPE, we evaluate : applying identical RoPE to both the reference and target videos, which enforces an two variants. (1) ui Θ unrealistic pixel-wise alignment prior and leads to degraded performance; (2)un : in addition to introducing Θ temporal bias , following in-context image generation [66], we add width bias by placing the reference video to the left of the target video. Experiments show that this increases the difficulty of spatial referencing and results in performance degradation. Scalability. As shown in the scalability section, VAP improves across all metrics as training data grows, demonstrating strong scalability. This follows from our unified design that treats reference videos as prompts without task-specific modifications, together with the MoT framework, which preserves the backbones generative capacity while enabling plug-and-play in-context generation. DiT Structure. To test transferability, we equip Wan2.1-I2V-14B with VAP equal in parameter counts to layers; 5B), whichbenefiting from Wan2.1s stronger CogVideoX-I2V-5B version (evenly inserted across 1 4 baseimproves dynamic degree and aesthetic score but, because the only 1 in-context interaction, yields 4 slightly worse reference alignment than VAP on CogVideoX. We also ablate the in-context expert transformer layer distribution of VAP, and the video-prompt representation. Further experiment details are in Sec. due to page limits."
        },
        {
            "title": "5 Conclusion",
            "content": "Video-As-Prompt (VAP) is unified, semantic-controlled video generation framework that treats reference videos as prompts and enables plug-and-play in-context control via mixture-of-transformers expert. VAP overcomes limits of structure-controlled methods (e.g., inappropriate pixel-wise priors) and task/condition-specific designs (e.g., non-generalizable models), providing scalable semantic control and zero-shot generalizability. We build VAP-Data, the largest semantic-controlled video generation dataset, and show in extensive experiments that VAP achieves state-of-the-art among open-source models, comparable performance to commercial models, and strong generalization. Limitations and Future Works. Despite strong performance, some limitations need further study: (1) We experimented on our large-scale VAP-Data, yet the semantic conditions in VAP-Data are relatively limited, synthetic, and derived from other generative models, which may inherit the specific stylistic biases, artifacts, and conceptual limitations of the source templates (see Sec. D). We leave the construction of larger-scale, real, semantic-controlled video data to future work. (2) VAP uses reference video, reference caption, and target caption to guide semantic control. To stay close to the original DiT distribution, we employ standard video descriptions as captions; however, inaccurate semantics descriptions or large subject mismatch can degrade generation quality (see Sec. E). Instruction-style captions (e.g., please follow the Ghibli style in the reference video) may more effectively capture the intended semantics and improve control."
        },
        {
            "title": "6 Ethics Statement",
            "content": "Scope and intended use (research-only). VAP targets semantic-controlled video generation for research, education, and creative prototyping, where reference video and an optional caption steer concept/style/- motion/camera. It is not intended for surveillance, impersonation, political persuasion, or other high-risk deployments. We will accompany any artifact release with research-only license and an acceptable-use policy (AUP) that explicitly prohibits abusive or unlawful scenarios. Misuse risks and technical/operational mitigations. Potential misuses include identity impersonation, deepfake content, targeted harassment, deceptive political messaging, and generation of sexualized or violent media. Our mitigations include: (i) research-only release; (ii) default content filters blocking clearly harmful categories (e.g., sexual content, explicit violence, hate symbols)."
        },
        {
            "title": "References",
            "content": "[1] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024. [2] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. In ICCV, 2025. [3] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di ZHANG. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. In ICLR, 2025. [4] Yuxuan Bian, Xuan Ju, Jiangtong Li, Zhijian Xu, Dawei Cheng, and Qiang Xu. Multi-patch prediction: adapting language models for time series representation learning. In ICML, 2024. [5] Yuxuan Bian, Ailing Zeng, Xuan Ju, Xian Liu, Zhaoyang Zhang, Wei Liu, and Qiang Xu. Motioncraft: Crafting whole-body motion with plug-and-play multimodal controls. In AAAI, 2025. [6] Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. Videopainter: Any-length video inpainting and editing with plug-and-play context control. In SIGGRAPH, 2025. [7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. [8] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators, 2024. URL https://openai.com/research/video-generation-models-as-world-simulators. [9] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. NeurIPS, 2024. [10] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. [11] Civitai. Civitai. https://civitai.com/, 2025. [12] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [13] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. [14] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In ICCV, 2023. [15] Gongfan Fang, Kunjun Li, Xinyin Ma, and Xinchao Wang. Tinyfusion: Diffusion transformers learned shallow. In CVPR, 2025. [16] Jianxiong Gao, Zhaoxi Chen, Xian Liu, Jianfeng Feng, Chenyang Si, Yanwei Fu, Yu Qiao, and Ziwei Liu. Longvie: Multimodal-guided controllable ultra-long video generation. arXiv preprint arXiv:2508.03694, 2025. [17] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [18] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024. [19] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. In ICLR, 2024. [20] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 12 [21] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. In SIGGRAPH, 2025. [22] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. In ECCV, 2025. [23] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long context tuning for video generation. arXiv preprint arXiv:2503.10589, 2025. [24] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. [25] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. [26] Hao-Yu Hsu, Zhi-Hao Lin, Albert Zhai, Hongchi Xia, and Shenlong Wang. Autovfx: Physically realistic video editing from natural language instructions. arXiv preprint arXiv:2411.02394, 2024. [27] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 2022. [28] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In CVPR, 2024. [29] Zhihao Hu and Dong Xu. Videocontrolnet: motion-guided video-to-video translation framework by using diffusion model with controlnet. arXiv preprint arXiv:2307.14073, 2023. [30] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. [31] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. [32] Ondřej Jamriška, Šárka Sochorová, Ondřej Texler, Michal Lukáč, Jakub Fišer, Jingwan Lu, Eli Shechtman, and Daniel S`ykora. Stylizing video by example. ACM Transactions on Graphics, 2019. [33] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. In CVPR, 2024. [34] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. [35] Wonjoon Jin, Qi Dai, Chong Luo, Seung-Hwan Baek, and Sunghyun Cho. Flovd: Optical flow meets video diffusion model for enhanced camera-controlled video synthesis. In CVPR, 2025. [36] Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu. Humansd: native skeleton-guided diffusion model for human image generation. In ICCV, 2023. [37] Xuan Ju, Weicai Ye, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Qiang Xu. Fulldit: Multi-task video generative foundation model with full attention. In ICCV, 2025. [38] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [39] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. [40] Kling. Kling: special-effects/new, 2025. Image-to-video special effects. https://klingai.kuaishou.com/image-to-video/ [41] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [42] Mathis Koroglu, Hugo Caselles-Dupré, Guillaume Jeanneret, and Matthieu Cord. Onlyflow: Optical flow based motion conditioning for video diffusion models. In CVPR, 2025. 13 [43] Guojun Lei, Chi Wang, Hong Li, Rong Zhang, Yikai Wang, and Weiwei Xu. Animateanything: Consistent and controllable animation for video generation. arXiv preprint arXiv:2411.10836, 2024. [44] Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, et al. Mixture-of-transformers: sparse and scalable architecture for multi-modal foundation models. arXiv preprint arXiv:2411.04996, 2024. [45] Han Lin, Jaemin Cho, Abhay Zala, and Mohit Bansal. Ctrl-adapter: An efficient and versatile framework for adapting diverse controls to any diffusion model. arXiv preprint arXiv:2404.09967, 2024. [46] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [47] Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo. Vfx creator: Animated visual effect generation with controllable diffusion transformer. arXiv preprint arXiv:2502.05979, 2025. [48] Miles Macklin, Matthias Müller, Nuttapong Chentanez, and Tae-Yong Kim. Unified particle physics for real-time applications. ACM Transactions on Graphics, 2014. [49] Fangyuan Mao, Aiming Hao, Jintao Chen, Dongxia Liu, Xiaokun Feng, Jiashu Zhu, Meiqi Wu, Chubin Chen, Jiahong Wu, and Xiangxiang Chu. Omni-effects: Unified and spatially-controllable visual effects generation. arXiv preprint arXiv:2508.07981, 2025. [50] Joanna Materzynska, Josef Sivic, Eli Shechtman, Antonio Torralba, Richard Zhang, and Bryan Russell. Customizing motion in text-to-video diffusion models. In ACCV, 2024. [51] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. arXiv preprint arXiv:2402.14797, 2024. [52] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In AAAI, 2024. [53] OpenAI. Introducing GPT-5, 2025. URL https://openai.com/index/introducing-gpt-5/. [54] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [55] Pika. Pika: Ai video generation effects. https://pika.art/pikaffects, 2025. [56] PixVerse. Pixverse: Ai video creation platform. https://app.pixverse.ai/home, 2025. [57] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [58] Alexander Pondaven, Aliaksandr Siarohin, Sergey Tulyakov, Philip Torr, and Fabio Pizzati. Video motion transfer with diffusion transformers. In CVPR, 2025. [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [60] Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, and Abhinav Shrivastava. Customize-a-video: One-shot motion customization of text-to-video diffusion models. In ECCV, 2024. [61] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022. [62] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [63] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In CVPR, 2022. [64] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. In ICML, 2024. 14 [65] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. [66] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. In ICCV, 2025. [67] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, 2020. [68] Vidu. Vidu: Ai video templates. https://www.vidu.com/templates, 2025. [69] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. NeruIPS, 2016. [70] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [71] Luozhou Wang, Ziyang Mai, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, and Ying-Cong Chen. Motion inversion for video customization. In SIGGRAPH, 2025. [72] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In SIGGRAPH, 2024. [73] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. [74] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. NeurIPS, 2024. [75] Shiyuan Yang, Zheng Gu, Liang Hou, Xin Tao, Pengfei Wan, Xiaodong Chen, and Jing Liao. Mtv-inpaint: Multi-task long video inpainting. arXiv preprint arXiv:2503.11412, 2025. [76] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In ICLR, 2024. [77] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In CVPR, 2024. [78] Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, and Wenhan Luo. Stylemaster: Stylize your video with artistic generation and translation. In CVPR, 2025. [79] David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, and Doyen Sahoo. Moonshot: Towards controllable video generation and editing with multimodal conditions. arXiv preprint arXiv:2401.01827, 2024. [80] Jintao Zhang, Jia Wei, Pengle Zhang, Jun Zhu, and Jianfei Chen. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In ICLR, 2025. [81] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. [82] Shiyi Zhang, Junhao Zhuang, Zhaoyang Zhang, Ying Shan, and Yansong Tang. Flexiact: Towards flexible action control in heterogeneous scenarios. In SIGGRAPH, 2025. [83] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023. [84] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jia-Wei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. In ECCV, 2024. [85] Haitao Zhou, Chuang Wang, Rui Nie, Jinxiao Lin, Dongdong Yu, Qian Yu, and Changhu Wang. Trackgo: flexible and efficient method for controllable video generation. arXiv preprint arXiv:2408.11475, 2024. Appendix In the appendix, we provide more qualitative results (Sec. A), downstream application demonstration (Sec. B), more implementation details (Sec. C), including the hyperparameters and the semantic alignment score metric. Then, we illustrate more dataset details and limitations of our VAP-Data (Sec. D). Furthermore, we discuss the influence of reference video quality, caption quality, and multiple reference videos (Sec. E). Finally, we conduct more ablation about VAP (Sec. F)."
        },
        {
            "title": "A Gallery",
            "content": "To further demonstrate our VAPs performance, we provide more semantic-controlled generation cases in Fig. 8, Fig. 9. We strongly encourage readers to view our webpage for better visualization."
        },
        {
            "title": "B Application",
            "content": "Our Video-as-Prompt (VAP) model supports the following downstream applications by disentangling semantic concept from source video and applying it to new subject: 1. Given different reference videos (with different semantics) and the same reference image, our VAP consistently generates new video for each semantic  (Fig. 10)  ; 2. Given different reference videos (with the same semantics) and the same reference image, our VAP consistently generates the target video aligned with the provided semantics  (Fig. 11)  ; 3. Given one reference video and different reference images, our VAP transfers the same semantics from the reference video to each image and generates the corresponding videos  (Fig. 12)  ; 4. Beyond video prompts, VAP allows for fine-grained adjustments using modified text prompts, by fixing the reference inputs and only changing single word in the prompt (e.g., black to white). VAP can precisely edit attributes of the generated output while preserving identity and motion  (Fig. 13)  ."
        },
        {
            "title": "C Implementation Details",
            "content": "C.1 Hyperparameters In Tab. 3, we summarize hyperparameters for two VAP variants based on CogVideoX-5B [76] and Wan2.114B [70], respectively, showing transferability across different DiT architectures. C.2 Metrics As stated in Sec. 4.2, standard video-quality metrics (e.g., CLIP score [59], aesthetic score [61]) do not reliably capture adherence to specific semantic condition, so we introduce semantic-alignment score that measures consistency between the reference semantic condition and the generated video; we submit each (reference, generation) pair and the evaluation rules to Gemini-2.5-pro [12] for automatic scoring. The evaluation rules pair general template with key criteria for each semantic; for each case, we provide the template, the criteria for the current semantic (see Tab. 4), the reference video condition, and the generated video to the VLM, which scores them under these rules. To validate the stability of the semantic alignment score, we conduct the same evaluation experiment with another state-of-the-art vision lanuage model GPT-5 [53]; its scores match closely Gemini-2.5-Pro [12] and follow the trends of human preference rate in our user study (see Tab. 5), confirming the validity of the metric. This further verifies the effectiveness and validity of our proposed semantic alignment score."
        },
        {
            "title": "D Dataset",
            "content": "16 Figure 8 Additional visualizations of VAP, including entity transformation and entity interaction in concept semantic categories. 17 Figure 9 Additional visualizations of VAP, including style semantic categories, motion semantic categories (NonHuman Motion and Human Motion), and camera semantic categories. 18 Figure 10 Given different reference videos (with different semantics) and the same reference image, our VAP consistently generates new video for each semantic. Figure 11 Given different reference videos (with the same semantics) and the same reference image, our VAP consistently generates the target video aligned with the provided semantics 19 Figure 12 Given one reference video and different reference images, our VAP transfers the same semantics from the reference video to each image and generates the corresponding videos. Figure 13 Given fixed reference video and reference image, our VAP preserves semantics and identity while using user-modified prompt to adjust fine-grained attributes. 20 Hyperparameter Batch Size / GPU Accumulate Step Optimizer Weight Decay Learning Rate Learning Rate Schedule WarmUp Steps Training Steps Resolution Prediction Type Num Layers"
        },
        {
            "title": "MoT Layers",
            "content": "CogVideoX-I2V-based Model 1/1 1 AdamW 0.0001 0.00001 constant with warmup 1000 20,000 480p Velocity 42 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41] Pre-trained Model Sampler Sample Steps Guide Scale Generation Speed (1 A100) CogVideoX-I2V-5B CogVideoX DDIM 50 6.0 540s Wan2.1-I2V-based 1/2 1 AdamW 0.0001 0.00001 constant with warmup 1000 20,000 480p Flow Matching 40 [0, 4, 8, 12, 16, 20, 24, 28, 32, 36] Wan2.1-I2V-14B Flow Euler 30 5.0 420s"
        },
        {
            "title": "Device\nTraining Strategy",
            "content": "A10048 FSDP / DDP / BFloat16 A10048 FSDP / DDP Parallel / BFloat16 Table 3 Hyperparameter selection for CogVideoX-I2V-5B-based and Wan2.1-I2V-14B-based VAP. D.1 Dataset Details In-context learning requires vast amounts of example pairs, which simply do not exist for semantic video tasks. Filming 100k real-world pairs is nearly impossible for research exploration. Our solution was to bootstrap it. We curated thousands of high-quality real images and then used the existing zoo of specialist models (commercial APIs [40, 56, 68] and LoRAs [11, 27]) as powerful, automated engine to create our paired dataset, VAP-Data. As shown in Sec. 3.2 and Fig. 3, VAP-Data is the largest semantic-controlled paired dataset to date, with over 100K samples across 100 semantic conditions, covering 4 primary categories: concept (entity transformation and interaction), style, motion (human and non-human), and camera movement. The detailed distribution of semantic conditions is provided in Tab. 6. Crucially, VAP-Data is more than just dataset; its proof of concept. We show that we can train single generalist model (VAP) to learn the unified underlying principle of semantic control by showing it various examples from disparate specialist models. For evaluation, we evenly sampled 24 semantic conditions from 4 categories (concept, style, motion, camera) in VAP-Data test subset, with 2 samples each, totaling 48 test samples. D.2 Dataset Limitations Even though our VAP-Data is the largest semantic-controlled video generation dataset, it still has limitations. As noted in Sec. 4.3, VAP-Data was created using visual effects templates from commercial models (vidu [68], Kling [40]) and community LoRAs [11, 27, 70, 76]. Thus, the dataset is synthetic and derived from other generative models, leading to VAP may inherit the specific stylistic biases, artifacts, and conceptual limitations of the source templates (e.g., if the source models are poor at generating hands, VAP will likely not learn to generate hands well from this data). Building large, real-world, semantic-controlled video dataset would 21 Category Content General Template You are an expert judge for referencebased semantic video generation. INPUTS REFERENCE video: the target semantic to imitate. TEST video: new output conditioned on NEW reference image. Human criteria (treat as ground truth success checklist; overrides defaults if conflict): {criteria} REGIME DECISION Classify the semantics into one of: A) ID-TRANSFORM (identity-changing): the main subject/object changes semantic class or material/state. Layout and identity may legitimately change as consequence of the transformation. B) NON-ID-TRANSFORM (identity-preserving): stylization, camera motion (pan/zoom), mild geometry exaggeration, lighting changes, human motion, etc. The main subject class/identity should remain the same. If the REFERENCE clearly shows class/state change, choose A. Otherwise, choose B. When uncertain, choose B. EVALUATION 1) SEMANTIC MATCH (060) Regime (ID-TRANSFORM): How strongly and accurately does TEST reproduce the REFERENCEs target state/look/behavior on the correct regions? Is the sourcetarget mapping consistent (same parts transform to corresponding target parts)? Does the transformed state resemble the REFERENCE target, not generic filter? Regime (NON-ID-TRANSFORM): Does TEST replicate the specific semantic (style, camera motion, geometric exaggeration) while keeping the subject recognizable and aligned to the intended scope? 2) IDENTITY / LAYOUT CORRESPONDENCE (020) Regime A: Reward semantic correspondence rather than identical identity; coarse scene continuity is preserved unless the REFERENCE implies re-layout. Regime B: Main subject identity stays intact (face/body/clothes/features), and coarse spatial layout remains consistent (no unintended subject swaps/teleports). 3) TEMPORAL QUALITY and TRANSFORMATION CONTINUITY (020) Check onsetsustainoffset completeness of the transformation as implied by the REFERENCE. Avoid pop-in/out. Motion is smooth, minimal flicker, and the background is reasonably stable. No frozen loops unless REFERENCE loops. HARD FAIL CAP (force FINAL <= 20 if any true) - REFERENCE shows an ID-TRANSFORM, but TEST lacks the transformation, targets the wrong class/material, or completes <70% of the transformation timeline. - Severe identity loss in Regime (unrecognizable face/body, unintended person/object swap). - Gross broken anatomy (detached/missing limbs, implausible face mash) is not required by the semantics. - Extreme temporal instability or unreadable corruption (heavy strobe, tearing, tiling). - Hallucinated intrusive objects that block the subject or derail the semantics. OUTPUT (exactly ONE line of JSON; integer only) {\"score\": 1100} Semantic Criteria Regime: NON-ID-TRANSFORM (identity-preserving stylization). Semantic: Ghibli-style stylization the overall look gradually transitions to hand-drawn, soft, film-like Ghibli aesthetic across the whole frame. Identity preservation: The main subject remains recognizable; appearance/proportions/base colors are largely maintained (stylistic simplification and brush-like textures allowed). Motion allowance: Light natural motion is allowed (e.g., slight subject or scene movement) without disrupting effect consistency. Exclusions: No identity swaps, major re-layout, or gross anatomy distortions unless explicitly implied by the reference. . . . Table 4 Prompt components for the semantic-alignment metics.We provide the general template and the specific criteria of Ghibli Style as an example. Metric VACE (Original) VACE (Depth) VACE (Optical Flow) CogVideoX-I2V CogVideoX-I2V (LoRA) Kling / Vidu Video-As-Prompt (VAP) Alignment Score (Gemini-2.5-Pro) Alignment Score (GPT-5) Preference Rate (%) 35.38 32.52 0.6% 43.35 39.41 0.7% 46.71 45.09 1.8% 26.04 28.36 6.9% 68.60 66.93 13.1% 74.02 73.91 38.2% 70.44 70.26 38.7% Table 5 Semantic alignment score metric and user preference. Columns are models; rows are semantic alignment score evaluated by Gemini-2.5-Pro [12], semantic alignment score evaluated by GPT-5 [53], and human preference rate results of our user study. Primary Category Concept (n=56) Subcategory Subset (alphabetical) Entity Transformation (n=24) Entity Interaction (n=21) captain america, cartoon doll, eat mushrooms, fairy me, fishermen, fuzzyfuzzy, gender swap, get thinner, hair color change, hair swap, ladudu me, mecha x, minecraft, monalisa, muscling, pet to human, sexy me, squid game, style me, super saiyan, toy me, venom, vip, zen aliens coming, child memory, christmas, cloning, couple arrival, couple drop, couple walk, covered liquid metal, drive yacht, emoji figure, gun shooting, jump to pool, love drop, nap me, punch hit, selfie with younger self, slice therapy, soul depart, watermelon hit, zongzi drop, zongzi wrap Total Videos 17k 20k Style (n=11) Stylization (n=11) Motion (n=41) Human Motion Transfer (n=16) Non-human Motion Transfer (n=16) Camera (n=12) Camera Movement Control (n=12) american comic, bjd, bloom magic, bloombloom, clayshot, ghibli, irasutoya, jojo, painting, sakura season, simpsons comic 15k break glass, crying, cute bangs, emotionlab, flying, hip twist, laughing, live memory, live photo, pet belly dance, pet finger, shake it dance, shake it down, split stance human, split stance pet, walk forward auto spin, balloon flyaway, crush, decapitate, dizzydizzy, expansion, explode, grow wings, head to balloon, paperman, paper fall, petal scattered, pinch, rotate, spin360, squish 10k 19k dolly effect, earth zoom out, hitchcock zoom, move down, move left, move right, move up, orbit, orbit dolly, orbit dolly fast, zoom in, zoom out 19k Subset counts (n) are reported per subcategory and are alphabetically sorted within each subcategory. Overall subsets across all primary categories: 100. Overall videos across all categories: > 100k. Table 6 Dataset statistics by 4 primary semantic categories. We reorganize the dataset into 4 primary categories: Concept (merging entity transformation and interaction), Style, Motion (covering human and non-human motion transfer), and Camera Movement. For each primary category, we report its subcategory (if any), the alphabetical semantic condition subset list (names come from commercial models API definition [40, 68], and community visual effects LoRA definition [11], see Sec. 4.3), and the total number of videos. help address this issue, but it is beyond this papers main focus; we leave it for future work. Nevertheless, zero-shot experiments in Sec. 4.4 and downstream tasks in Sec. show that VAP generalizes to unseen semantic conditions [47] (e.g., crumble, dissolve, levitate, melt) and across tasks, including using 23 Figure 14 Limitation visualization. VAP transfers semantics reliably when the semantic description of reference caption aligns with that of the target caption and subject structure aligns with the target: aligned descriptions (gold liquid and liquid metal) and similar subject structures (Grogu and young woman) yield good results (top). Mislabeled semantic descriptions (water vs. liquid metal), or large subject mismatch (Grogu vs. snail), reduce alignment and visual quality (bottom). different reference videos to prompt single reference image under different semantic conditions or using the same reference videos to prompt different reference images under fixed semantic condition. These results demonstrate the generality of VAP and we hope they inspire advances in controllable video generation; broader data collection is left to future work. Additional visualizations are available on the project page."
        },
        {
            "title": "E Limitation Analysis",
            "content": "E."
        },
        {
            "title": "Influence of Reference Video and Caption",
            "content": "VAP learns in-context generation from large paired videocaption data: given captions for reference and target video, the shared semantic attributes in both captions aid in transferring the semantic properties of the reference video to the target video. Specifically, when both captions mention the same concept (e.g., molten metal pours over the target . . . ) in similar way, VAP retrieves the relevant semantics from the reference prompt and applies it to the target. The reason why we use standard video-description captions (e.g., . . . static Grogu is centered. . . viscous, reflective gold liquid appears on the forehead . . . , young woman stands still. . . thick, reflective liquid metal begins to pour over her face from above. . . ), is to match the pre-training data distribution, Consequently, performance depends on caption quality and on structural similarity between the main subjects: it is stable when caption styles align and subjects are similar, but degrades when descriptions diverge (e.g., . . . viscous, reflective gold liquid appears on the forehead . . . vs. . . . viscous, reflective rose-gold water pours over the snail . . . ) or when subjects differ markedly (e.g., Grogu vs. snail). As shown in Fig. 14, the bad caption mislabels water instead of the intended liquid metal; the good reference subject (the young woman) is structurally closer to Grogu, while the snail differs greatly and its semantic signal is weak (the liquid metal and shell have similar colors), yielding poorer alignment and less appealing visuals for the bad reference case. E.2 Influence of Multiple Reference Videos We examine how the number of video prompts affects performance by supplying 13 semantically matched reference videos during training and testing. Empirically, results are similar to using single reference. However, with multiple references, the model may blend unwanted visual details across videos, as shown in Fig. 15. We hypothesize this stems from our general-purpose captions, which lack explicit semantic referents. When the three references differ in structure (human, spider, flatfish) and in semantic realization (e.g., reference 1 clearly shows AI Lover Drop; reference 2 introduces falling spider without hug; reference 3 is weakest, with flatfish swimming up instead of falling), the model mixes semantics from reference 1 with appearance from reference 2 (spider legs) and contours from reference 3 (fish shape). more effective multi-reference control mechanism (e.g., tailored RoPE for multi-reference conditions)or an instruction-style caption that specifies the intended referentmay mitigate this issue. full study of model and caption design for multi-reference training is beyond this work and left for future research. 24 Figure 15 Failure case of multi-reference prompting. Left: three reference videos with divergent structure and similar semantics (human, spider, flatfish). Right: Ground truth is on top. Using three (bottom) spuriously transfers unwanted appearance cues (e.g., fish shape and spider-like legs) onto the dog. We attribute this leakage to generic captions that lack an explicit referent; stronger multi-reference control or instruction-style captions could mitigate it. E.3 Efficiency Like prior plug-and-play methods [34, 81], our approach avoids re-training pre-trained video diffusion transformers at pre-training scale, but the added parameters introduce extra inference costhigher memory use and longer runtime. Specifically, the impact varies with the distribution of MoT layers in VAP; as shown in Tab. 3, inference time roughly doubles on average, mainly due to additional MoT-expert computation and in-context full attention. Given the strong plug-and-play unified semantic control in in-context generation and the fact that we avoid retraining the backbone, this overhead is acceptable. Performance optimizations (e.g., sparse attention [13, 80] and pruning [15, 73]) are orthogonal and beyond the scope of this work; we leave them to future work."
        },
        {
            "title": "F Ablation Study",
            "content": "In-context Generation Structure. We train 4 VAP variants to test the effectiveness of our mixture-ofΘ: expand pre-trained DiT input sequence transformers (MoTs) adoption: A1. Single-Branch Finetuning us to [Reftext, Refvideo, artext, arvideo] and finetune the full model; A2. Single-Branch LoRa Finetuning usl Θ: same as A1 but freeze the backbone and train only the LoRA layers; A3. Unidirectional Cross-Attn uuc Θ : freeze the pre-trained DiT, add new branch with the same weights, and inject its features via layer-wise Θ : same as A3 but inject features via residual addition. cross-attention; and A4. Unidirectional Addition uua We evaluate on the same benchmark of VAP-Data. Results in Tab. 2 show: A1. MoT boosts performance by preserving the base DiTs generative ability while enabling plug-and-play in-context control. A2. LoRA helps retain the backbones ability, but its limited capacity struggles with complex in-context generation, yielding suboptimal results. A3. Layer-wise bidirectional information exchange in MoT lets the reference video-prompt representation adapt synchronously to the target tokens, improving semantic alignment. A4. Even with new data, residual-addition methods rely on rigid pixel-to-pixel mapping, mismatching semantic-controlled generation and degrading performance. Position Embedding Designs. To validate the effectiveness of our temporally-biased RoPE, we evaluate : applying identical RoPE to both the reference and target videos, which enforces an two variants. (1) ui Θ unrealistic pixel-wise alignment prior and leads to degraded performance; (2)un : in addition to introducing Θ temporal bias , following in-context image generation [66], we add width bias by placing the reference video to the left of the target video. Experiments show that this increases the difficulty of spatial referencing and results in performance degradation. Scalability. As shown in Tab. 2, VAP improves across all metrics as training data grows, demonstrating strong scalability. This follows from our unified design that treats reference videos as prompts without task-specific modifications, together with the MoT framework, which preserves the backbones generative capacity while enabling plug-and-play in-context generation. DiT Structure. To test transferability, we equip Wan2.1-I2V-14B with VAP equal in parameter counts to 25 Metrics Variant Text Overall Quality Reference CLIP Score Motion Smoothness Dynamic Degree Aesthetic Quality Alignment Score us Θ usl Θ uuc Θ uua Θ ui Θ un Θ (Single-Branch) (Single-Branch-LoRA) (Unidir-Cross-Attn) (Unidir-Addition) (Identical PE) (Neg. shift in T, ) uΘ(1K) uΘ(10K) uΘ(50K) uΘ(100K) 23.03 23.12 22.96 22.37 23.17 23.45 22.84 22.87 23. 24.13 97.97 98.25 97.94 97.63 70.83 72.92 66.67 62.50 Position Embedding Design Scalability 98.49 98. 92.12 94.89 96.72 98.59 DiT Structure uWan Θ (Wan2.1-I2V-14B) 23. 97.87 uΘ(Lodd) uΘ(Lodd, 0.5Nl) uΘ(Lfirst-half) uΘ(Lfirst-last) In-Context Expert Transformer Layer Distribution 24.05 23.72 23.90 23.96 98.52 98.19 98.41 98.33 75.00 70.83 75.00 72. Video Prompt Representation un_ref Θ (noisy reference) 23.98 98.41 uΘ (VAP) Notation. uΘ (our VAP parameterized by Θ). 24.13 Ours 98.59 70.83 72.92 60.42 64.58 70.83 77.08 79. 75.00 77.08 56.93 57.19 56.88 56.91 57.09 57.31 56.77 56.79 56.82 57.71 58. 57.58 56.71 57.18 57.06 57.42 57.71 68.74 69.28 67.16 55.99 68.98 69.05 63.91 66.28 68. 70.44 70.23 70.22 69.61 69.94 70.02 70.18 70.44 (in-context single-branch finetuning), sl (in-context single-branch LoRA finetuning), uc (unidirectional cross-attention injection), ua (unidirectional residual addition), (identical position embedding in reference and target), (temporal shift + negative temporal/width shifts of position embedding), Wan (Wan2.1 as DiT backbone). n_ref (noisy reference prompts). MoT layers. uΘ(L) activates MoT blocks on layer index set [Nl] = {1, . . . , Nl} of the backbone with Nl Transformer layers. We instantiate Lfirst-half={1, 2, . . . , 0.5Nl}, Lfirst-last={1, Nl}, Lodd, 0.5Nl={1, 3, . . . , 0.5Nl}, and Lodd={1, 3, . . . , Nl}. Scale. uΘ(M ) indicates the number of video training pairs used (M {1K, 10K, 50K, 100K}). Table 7 Ablation Study. We verify the effectiveness of our MoT structure, temporal-biased RoPE, the scalability, and the transferability in different DiTs. The bottom row reports our full model. CogVideoX-I2V-5B version (evenly inserted across 1 4 baseimproves dynamic degree and aesthetic score but, because the only 1 4 slightly worse reference alignment than VAP on CogVideoX. layers; 5B), whichbenefiting from Wan2.1s stronger in-context interaction, yields Mixture-of-Transformers Layer Distribution We analyze how different layer distributions affect our incontext DiT Expert. (1)uΘ(Lfirst-half): initializing and copying from the first half of the pre-trained DiT; (2)uΘ(Lfirst-last): from the first and last layers; (3)uΘ(Lodd,0.5Nl): from the odd layers of the first half; and (4)uΘ(Lodd): from all odd layers. The results show that balanced feature interaction improves generation quality (uΘ(Lfirst-last) outperforms uΘ(Lfirst-half), and uΘ(Lodd) outperforms uΘ(Lodd,0.5Nl)). However, while reducing layers can improve training and inference efficiency, it inevitably harms certain aspects of performance (uΘ(VAP) outperforms uΘ(Lodd)). Video Prompt Representation Inspired by Diffusion Forcing [9, 23, 64], we study video prompt representation by injecting noise into it. However, this often leads to severe artifacts. The core reason is that, unlike long-video generation in Diffusion Forcing, where copy-paste or overly static results are common, our reference videos already differ significantly in appearance and layout from the target videos. Thus, adding noise to the video prompt corrupts the contextual information and degrades generation quality."
        }
    ],
    "affiliations": [
        "Intelligent Creation Lab, ByteDance",
        "The Chinese University of Hong Kong"
    ]
}