{
    "paper_title": "DREAM: Deep Research Evaluation with Agentic Metrics",
    "authors": [
        "Elad Ben Avraham",
        "Changhao Li",
        "Ron Dorfman",
        "Roy Ganz",
        "Oren Nuriel",
        "Amir Dudai",
        "Aviad Aberdam",
        "Noah Flynn",
        "Elman Mansimov",
        "Adi Kalyanpur",
        "Ron Litman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm."
        },
        {
            "title": "Start",
            "content": "DREAM: Deep Research Evaluation with Agentic Metrics Elad Ben Avraham1* Changhao Li2* Ron Dorfman1* Roy Ganz1 Oren Nuriel1 Amir Dudai1 Aviad Aberdam1 Noah Flynn1 Elman Mansimov1 Adi Kalyanpur1 Ron Litman1 1AWS Agentic AI 2Georgia Institute of Technology 6 2 0 2 1 ] . [ 1 0 4 9 8 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing taxonomy across four verticals that exposes critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering scalable, reference-free evaluation paradigm."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) increasingly support autonomous, tool-using agents that perform complex, open-ended tasks. Among these, Deep Research Agents (DRAs) have emerged as dominant paradigm. Given broad query, they retrieve information from external sources, synthesize evidence, and produce long-form research reports. Such reports are inherently open-ended; unlike traditional question answering, where correctness can often be verified against singular ground truth, *Equal contribution Correspondence to: eladba@amazon.com Work done during an internship at AWS Agentic AI 1 deep research admits multiple valid trajectories. For given query, two experts will inevitably produce distinct reports of potentially equal quality. This variability makes the assessment of deep research fundamentally complex, requiring transition from single-answer correctness toward highdimensional evaluation of report quality. Recognizing this challenge, the research community has shown growing interest in Deep Research Evaluation (DRE), with several recent works proposing benchmarks and datasets to assess DRAs (Coelho et al., 2025; Du et al., 2025; Patel et al., 2025; Sharma et al., 2025; Wan et al., 2025; Wang et al., 2025). However, critical examination of these frameworks reveals systematic limitation. While current approaches effectively assess surfacelevel dimensions, such as writing fluency and citation alignment, they remain largely insensitive to failures in factual correctness, temporal validity, and substantive reasoning. Therefore, fluent, well-cited reports can receive high scores despite containing obsolete information or flawed logic. We term this the Mirage of Synthesis an illusion of quality created by surface-level coherence despite underlying factual and reasoning flaws. To characterize this observation and unify the evaluation landscape, we propose taxonomy that organizes DRE into four fundamental verticals: Presentation Quality, Task Compliance, Analytical Depth, and Source Quality. By mapping the metrics of existing benchmarks into this taxonomy, we trace their limitations to the fundamental design choices. Current benchmarks typically rely on either human-curated rubrics, which are highly reliable but prohibitively expensive, datasetspecific, and prone to becoming outdated, or LLMas-a-judge paradigms limited to the LLMs static internal knowledge. These approaches and their associated citation-verification workflows operate as static observers, i.e., they lack access to external tools, temporal awareness, and the ability to Figure 1: Capturing Overlooked Dimensions of Research Quality. DREAM actively verifies the reasoning of generated reports by probing external sources (left), detects factual errors injected in controlled experiment (middle), and captures time-sensitive validity gaps by penalizing outdated reports (right). independently gather and verify evidence. Our taxonomy thus reveals unifying diagnosis: capability mismatch, where evaluators lack the abilities required to assess the dimensions they purport to measure. This motivates the principle of capability parity the evaluator should possess similar set of capabilities as the researcher, including the ability to retrieve, verify, and reason over information. We instantiate this principle in Deep Research Evaluation with Agentic Metrics (DREAM), framework that makes evaluation itself agentic. DREAM structures assessment through an evaluation protocol that combines query-agnostic static metrics with query-adaptive ones constructed by tool-calling agent. By independently researching the query and cross-referencing external evidence, DREAM provides temporally aware fact-checking and substantive depth assessment. This behavior is illustrated in Figure 1: the left panel depicts DREAMs active reasoning verification against external sources, while the middle and right panels demonstrate that DREAMs scores degrade appropriately as factual errors increase and report knowledge becomes outdated, whereas existing benchmarks remain largely insensitive to both. In summary, our main contributions are: We examine existing DRE benchmarks to identify critical gaps and the Mirage of Synthesis, systemic failure where they lack the capacity to detect factual, temporal, and logical degradation. We propose taxonomy to unify DRE into four key verticals and identify capability mismatch in current benchmarks, where evaluators lack the active retrieval tools and reasoning required for independent, temporally-aware verification. We introduce DREAM, an agentic evaluation framework grounded in the principle of capability parity, replacing passive scoring with active verification via two-phase Protocol Creation and Execution workflow. We empirically validate through controlled experiments suite of agentic metricsKeyInformation Coverage, Reasoning Quality, and Factualityshowing that DREAM is substantially more sensitive to temporal degradation and extrinsic factual errors than existing benchmarks."
        },
        {
            "title": "2 Deep Research Evaluation Landscape",
            "content": "Existing DRE benchmarks propose variety of metrics to capture the multidimensional nature of research quality, yet they differ substantially in terminology, scope, and implementation. This fragmentation makes it difficult to reason about which aspects of deep research are well evaluated, which remain under-evaluated, and why certain failure modes persist across benchmarks. To address this, we introduce unifying taxonomy that organizes existing criteria into common structure and use it to analyze the current evaluation landscape."
        },
        {
            "title": "2.1 A Unifying Taxonomy",
            "content": "To systematically derive the taxonomy, we employed an agentic pipeline that processed the evaluation metrics from the benchmarks summarized in Table 1. The pipeline extracted granular, leaf-level criteria from benchmark source code and documentation, embedded them semantically, and clustered them into coherent evaluation dimensions. This data-driven approach ensures the taxonomy is grounded in the actual implementation details of existing metrics, providing structured, bottomup synthesis of the landscape. Full details of this derivation process are provided in Appendix B. 2 Table 1: Taxonomic breakdown of deep research evaluation benchmarks. Metrics from existing benchmarks mapped to our four-dimensional taxonomy, alongside their creation and execution methodologies. Benchmark Vertical Metric Name Creation Execution DeepResearchGym (Coelho et al., 2025) Presentation Quality Clarity Task Compliance Analytical Depth Source Quality Report Relevance Insightfulness Retrieval Faithfulness N/A LLM + Human N/A N/A DeepResearch Bench (Du et al., 2025) Presentation Quality RACE (Presentation Quality) Task Compliance Analytical Depth Source Quality RACE (Inst., Comp.) RACE (Insight) FACT ResearchRubrics (Sharma et al., 2025) Presentation Quality Communication Quality Task Compliance Analytical Depth Source Quality Explicit/Implicit Req., Inst. Follow. Synthesis of Information Use of References DeepResearch Arena (Wan et al., 2025) Presentation Quality ACE ACE Task Compliance ACE Analytical Depth KAE Source Quality LLM LLM LLM N/A Human Human Human Human LLM LLM LLM N/A LLM LLM LLM Workflow LLM LLM LLM Workflow LLM LLM LLM LLM LLM LLM LLM Workflow LiveResearchBench (Wang et al., 2025) Presentation Quality Task Compliance Analytical Depth Source Quality Presentation & Organization Coverage & Comprehensiveness Analysis Depth Factual Cons., Citation Assoc./Acc. LLM + Human LLM + Human N/A N/A LLM LLM LLM LLM / Workflow DREAM (Ours) Presentation Quality Writing Quality Task Compliance Analytical Depth Source Quality Key-Information Coverage Reasoning Quality Factuality, Citation Integrity N/A Agent Agent N/A LLM LLM Agent Workflow Presentation Quality evaluates how effectively report communicates its findings. This includes stylistic aspects such as clarity of expression and sentence fluency, as well as structural properties such as section organization and appropriate use of headings. Metrics in this vertical assess whether information is conveyed clearly and professionally, independent of content correctness or depth. Task Compliance assesses whether report fulfills the requirements of the research query. This includes explicit constraints (e.g., format, scope, or requested comparisons) as well as implicit expectations of comprehensive coverage. Metrics in this vertical evaluate coverage breadth, recall of key information, and adherence to the query. Analytical Depth captures the intellectual rigor of the reports synthesis. This includes the quality of logical reasoning, causal explanations, and critical evaluation of evidence. High analytical depth reflects coherent reasoning that goes beyond aggregating retrieved facts, demonstrating structured argumentation and meaningful synthesis. Source Quality measures the reliability of the evidence supporting reports claims. This vertical comprises two complementary aspects: citation faithfulness, which evaluates whether claims are accurately supported by their cited sources (intrinsic quality), and factual correctness, which evaluates whether claims are true with respect to external world knowledge regardless of citation support."
        },
        {
            "title": "2.2 Diagnosing the Evaluation Landscape",
            "content": "In Table 1, we map existing DRE benchmarks onto our taxonomy and group them by evaluation paradigmhuman-curated, closed-loop LLMbased, or citation-centered workflow-basedbased on how their metrics are created and executed. Human-Defined Evaluation Criteria. This paradigm includes benchmarks with humandefined evaluation criteria, specified either through manual rubrics or dataset-specific proxies. While reliable and interpretable, these approaches tightly couple evaluation to specific datasets, domains, or costly curation. ResearchRubrics (Sharma et al., 2025) evaluates all four verticals using manually constructed rubrics, requiring over 2,800 hours of expert annotation. DeepResearchGym (Coelho et al., 2025) encodes task compliance via datasetspecific behavioral proxies, such as whether reports cover documents historically clicked by users. LiveResearchBench (Wang et al., 2025) adopts hybrid approach, combining LLM-generated checklists with human-in-the-loop verification. 3 Figure 2: DREAM Overview. Our framework operates in two phases. Left: Protocol Creation, where queryindependent Static Metrics are combined with Adaptive Metrics constructed by an agent equipped with web search tools and optional tools to access external data. Right: Protocol Execution, where each metric is routed to the appropriate evaluator, either an LLM, agent with tool access, or workflow. Closed-Loop LLM-Based Evaluation. To improve scalability, several benchmarks replace human annotation with LLM-generated evaluation criteria. DeepResearch Bench (Du et al., 2025) uses RACE to generate weighted rubrics for presentation quality, instruction following, and depth, while DeepResearch Arena (Wan et al., 2025) removes references via ACE, prompting an LLM to generate query-specific checklist. However, rubric and checklist construction is performed by static LLMs, which lack access to external tools, temporal context, or independent evidence gathering. This constrains their ability to evaluate grounded reasoning and evolving task requirements. Citation-Alignment Workflows. Prevalent in the Source Quality vertical, this paradigm employs multi-step pipelines to verify alignment between claims and cited URLs. Benchmarks such as DeepResearch Bench (FACT), DeepResearchGym (Retrieval Faithfulness), and LiveResearchBench (Citation Accuracy) extract claim, URL pairs, retrieve the cited content, and utilize an LLM to judge alignment. While effective at detecting source misrepresentation, these workflows primarily measure intrinsic citation faithfulness rather than extrinsic factual correctness. Because verification is restricted to provided citations, they remain insensitive to claims supported by outdated or unreliable sources. This creates the Citation-Alignment Fallacy, where report can contain entirely accurate citations while still being factually incorrect or obsolete with respect to the external world. Systematic Imbalance and Evaluator Capability Mismatch. Applying the taxonomy reveals consistent imbalance across evaluation paradigms. Presentation Quality and Task Compliance are extensively evaluated, while Source Quality is predominantly assessed through intrinsic citation alignment. In contrast, extrinsic factual correctness, temporal validity, and grounded reasoning receive little direct evaluation. This imbalance gives rise to the Mirage of Synthesis, where surface-level fluency and citation alignment are evaluated, while factual, temporal, and logical defects remain systematically unassessed. Crucially, this failure stems from structural capability mismatch, where across all paradigms, evaluators lack critical capabilities available to the agents they assess, namely, to independently retrieve evidence, reason over competing sources, and incorporate temporal context. 3 DREAM: DRE with Agentic Metrics To address the capability mismatch identified in Section 2, we introduce DREAM, framework that enforces capability parity by making the evaluation process itself agentic. Given research query, DREAM constructs query-specific evaluation protocol (Section 3.1) and executes it using specialized evaluators (Section 3.2), as illustrated in Figure 2."
        },
        {
            "title": "3.1 Phase 1: Protocol Creation",
            "content": "Because deep research questions admit multiple valid realizations, evaluation cannot rely on single reference or fixed rubric. DREAM addresses this by constructing query-specific evaluation 4 Figure 3: DREAM Protocol Execution Evaluators. (a) LLM Evaluator assesses writing quality (WQ) and key-information coverage (KIC); (b) Agent Evaluator evaluates reasoning quality (RQ) using external tools; (c) Workflow Evaluator performs factuality assessment via evidence retrieval, citation integrity (CI) verification through claim-source validation, and domain authoritativeness (DA) scoring via credibility assessment of extracted citations. protocol that defines task-relevant evaluation criteria independently of any particular report. The protocol consists of two complementary classes of metrics: Static Metrics for universal quality standards, and Adaptive Metrics for task-specific depth. Full implementation details for the metric suite are provided in Appendix C. Static Metrics. These query-agnostic criteria apply uniformly across tasks: (i) Writing Quality (WQ) assesses presentation quality along the fixed dimensions of Ideas and Content, Organization, and Sentence Fluency; (ii) Factuality validates claims against external knowledge independent of citation support; (iii) Citation Integrity (CI) verifies that claims are attributed to sources and supported by the cited content; and (iv) Domain Authoritativeness (DA) evaluates cited sources credibility to ensure reliance on reputable, high-quality domains. Adaptive Metrics. To capture query-dependent expectations, we instantiate Protocol Creation Agent as CodeAgent (Wang et al., 2024b) equipped with retrieval tools including web search, ArXiv, and GitHub. Given query, the agent performs lightweight tool selection to reduce noise, then conducts research to construct two metrics: Key-Information Coverage (KIC): The agent identifies essential facts by retrieving up-to-date sources and converting each key point into verifiable yes/no question. This transforms coverage assessment into grounded, temporally aware checklist that flags missing or outdated content. Reasoning Quality (RQ): The agent generates query-specific questions paired with structured validation plans. These plans specify the information to extract from both the report and independent sources, detailing how to cross-reference findings to ensure analytical depth is evaluated based on substantive reasoning. RQ is illustrated in the left panel of Figure 1 and in Figure 2."
        },
        {
            "title": "3.2 Phase 2: Protocol Execution",
            "content": "Once protocol is constructed, DREAM executes each metric using an evaluator whose capabilities match the metrics requirements. Evaluator selection follows the capability parity principle, i.e., each metric is routed to the simplest evaluator that possesses the capabilities required to close the diagnosed gap. As shown in Figure 3, DREAM employs three evaluator types. LLM Evaluator. The LLM Evaluator is used for metrics that require judgment but not external tool use. It assesses Writing Quality using fixed rubrics to ensure calibration and evaluates KeyInformation Coverage by verifying report content against the agent-generated yes/no checklist. Agent Evaluator. The Agent Evaluator, instantiated as CodeAgent, executes the Reasoning Quality metric. Unlike KIC, which focuses on coverage, 5 RQ probes the coherence and validity of reasoning across the report. The agent autonomously follows the validation plan created in phase 1, retrieving external evidence as needed and assigning final score based on evidentiary support. Workflow Evaluator. The Workflow Evaluator implements three complementary verification pipelines. For Citation Integrity, the system computes the harmonic mean of Claim Attribution (the ratio of cited to total verifiable claims) and Citation Faithfulness (the alignment of cited claims with their source content). For Factuality, claims are extracted and evaluated independently of citations by generating neutralized search queries to retrieve external evidence. Finally, for Domain Authoritativeness, the extracted citations are assessed by scoring the reputation and reliability of referenced domains. Together, this suite distinguishes between internal citation adherence (CI) and extrinsic empirical truth (Factuality), ensuring that analytical depth is grounded in credible domains (DA)."
        },
        {
            "title": "4 Validation of DREAM",
            "content": "We evaluate DREAM through set of complementary validation studies targeting distinct aspects of the evaluation framework. First, we assess the agentconstructed protocols via human judgment, establishing that the generated criteria are interpretable, relevant, and verifiable (Section 4.1). Next, we conduct targeted sensitivity analyses that isolate specific failure modes identified by our taxonomy, including temporal degradation, reasoning flaws, and extrinsic factual errors (Sections 4.2 to 4.4). Finally, we demonstrate that writing quality can be assessed in reference-free manner while remaining aligned with human judgment (Section 4.5). Unless otherwise specified, we use queries from DeepResearch Bench (DRB) and Claude Sonnet 4.5 (Anthropic, 2025) as the base LLM."
        },
        {
            "title": "4.1 Human Evaluation of Protocol Quality",
            "content": "A central claim of DREAM is that adaptive evaluation criteria can be constructed agentically, replacing manually authored rubrics. We therefore evaluate the quality of the agent-generated protocol metrics, KIC and RQ. We conducted human study where expert and non-expert annotators rated generated items on relevance, clarity, and verifiability, as well as plan validity for RQ, using 13 scale normalized to [0, 1] (details are provided in Appendix E). Table 2: Human evaluation of KIC and RQ items. Performance by metric and method. The agent with retrieval achieves the best performance across all axes. Metric Method Rel. Verif. Clar. Valid. Avg. KIC RQ LLM Agent Agent + Ret. LLM Agent Agent + Ret. 0.84 0.88 0.94 0.82 0.89 0.94 0.73 0.75 0.91 0.51 0.67 0. 0.80 0.87 0.92 0.78 0.88 0.97 0.70 0.92 0.99 0.79 0.83 0.92 0.70 0.84 0. We further performed three-way ablation to isolate the impact of agentic structuring and retrieval. As shown in Table 2, annotators rated the protocol criteria generated by the full agent highly (KIC: 0.92, RQ: 0.93), with RQ clarity (0.97) and plan validity (0.99) scoring particularly well. The ablation reveals that agentic structuring alone improves over plain LLM (RQ average: 0.84 vs. 0.70). Furthermore, adding retrieval significantly boosts performance, notably increasing verifiability (KIC: 0.75 0.91, RQ: 0.67 0.80) and RQ plan validity (0.92 0.99). These results confirm the value of both structured multi-step reasoning and grounding in external evidence."
        },
        {
            "title": "4.2 Temporal Awareness in KIC",
            "content": "We next evaluate the frameworks sensitivity to temporal obsolescence, core failure mode identified by our taxonomy. When temporal validity is central to query, competent evaluator should consistently penalize reports generated with outdated knowledge. We investigated this by selecting 20 temporally volatile queries (e.g., TikTok US legal status) and generating three report variants for each: one using current information (Dec 2025), and two simulated with knowledge cutoffs of Jan 2025 and Jan 2024. We then evaluated all reports using both DREAMKIC and DRBRACE (Comprehensiveness and Insight), as reported in Table 4. The results reveal sharp contrast. DRBRACE exhibits weak temporal sensitivity: Comprehensiveness fails to penalize moderately outdated reports (50.02 50.04 for Jan 2025), while Insight degrades only marginally (50.54 48.78). In contrast, DREAMKIC degrades monotonically with information staleness, dropping from 79.35 (current) to 44.80 (Jan 2025) and 22.34 (Jan 2024). This performance gap is structural; DRBRACE evaluates reports against static criteria (e.g., identifying relevant law), which remain satisfiable even with obsolete facts. KIC instead encodes time-sensitive 6 Figure 4: Temporal Awareness in KIC Evaluation. Comparison of evaluation criteria for TikTok legal status query, showing DeepResearch Benchs static criteria (left) versus DREAMs KIC criteria (right) that incorporate time-sensitive facts (e.g., mid-December 2025 joint venture deal and January 23, 2026 deadline). expectations derived from up-to-date evidence (Figure 4), causing outdated reports to fail explicit verification. Thus, effective temporal awareness requires agentic evaluation rather than static rubrics."
        },
        {
            "title": "4.3 Detecting Reasoning Flaws",
            "content": "Analytical failures in deep research reports often occur within fluent, well-structured, and superficially coherent narratives. Detecting such flawed arguments requires evaluation methods that look beyond surface plausibility to explicitly probe reasoning validity. We examined whether existing metrics can distinguish sound reports from those containing subtle but substantive reasoning errors. To study this, we designed controlled experiment that isolates reasoning quality while holding surface fluency constant. We selected 10 complex queries spanning domains where analytical rigor is essential (e.g., policy analysis and technical comparisons), and generated two report variants for each: standard version and malformed variant. The malformed variant contained deliberately injected reasoning flaws, such as circular arguments or unsupported claims, while preserving fluent structure (see full details in Appendix D.2). Figure 5 reports the relative score degradation of malformed reports, revealing significant gap. DRBRACE exhibits weak and inconsistent sensitivity, with an average degradation of 9%, and frequently scores malformed reports higher than well-reasoned ones. By comparison, DREAMRQ produces consistent signal, centered at 40% degradation, reliably penalizing reports with flawed reasoning despite their surface coherence. Figure 5: Reasoning flaws detection. Relative score degradations between well-reasoned and malformed reports. DREAMRQ centers around 40.1% degradation, while RACE centers around 9.1%, with several malformed reports outscoring well-reasoned ones."
        },
        {
            "title": "4.4 Grounding Beyond Citation Alignment",
            "content": "Claims without citations are inherently invisible to citation-alignment workflows, motivating reference-free factual verification metric such as DREAMFactuality. More critically, cited claims can be faithful to their sources while remaining factually incorrect with respect to the external world. We isolate this failure mode with controlled corruption study. Starting from base set of factual claims, we construct pairs consisting of correct version with supporting citation, and plausible but incorrect variant with matching citation (15 pairs; cf. Section D.3). We then sweep corruption rate [0, 1], replacing an fraction of correct claims with incorrect ones while preserving citation alignment by construction. As shown in Figure 1 (middle), DRBFACT scores remain invariant across all corruption levels, as the metric validates claims solely against the provided sources. In contrast, DREAMFactuality degrades monotonically with increasing r, closely tracking the true error rate. This demonstrates that citation alignment is insufficient for factual evaluation, as detecting plausible, well-cited falsehoods 7 requires access to external world knowledge. 4.5 Reference-Free Presentation Evaluation Finally, we evaluate whether Presentation Quality can be reliably assessed without reference report. Reference-based evaluation may introduce bias when references are suboptimal or outdated, often penalizing valid alternative formulations. To validate the reliability of DREAMs reference-free WQ metric, we measure its alignment with readability rankings established by DRB. Specifically, we utilize the DRB-RACE scores provided for 300 reports generated by six agents across 50 queries, which have been previously validated as highfidelity proxy for human preference. We compute the rank correlation using Kendalls τ between the rankings induced by DREAMs WQ score and the DRB-RACE rankings. This analysis yields an average τ = 0.6. Given that human inter-annotator agreement for subjective readability typically falls in the 0.50.7 range, this result confirms that DREAM provides reliable signal comparable to established benchmarks."
        },
        {
            "title": "5 Benchmarking Leading DRAs",
            "content": "Having validated DREAM, we now apply it to benchmark leading DRAs. Unlike current evaluations that rely on dataset-specific annotations, our framework is dataset-agnostic, enabling unified comparison across diverse tasks. We employ three diverse datasets. DEEPRESEARCH BENCH (Du et al., 2025) is bilingual benchmark comprising 50 English and 50 Chinese PhD-level questions across 22 research fields; we consider the English subset. LIVERESEARCHBENCH (Wang et al., 2025) focuses on timely information synthesis requiring access to recent sources, and we utilize the 80 publicly available queries.1 Finally, ResearchRubrics (Sharma et al., 2025) provides 101 queries accompanied by expert-crafted rubrics. systems: LangChain Open Deep Research (LangChain AI, 2025) with GPT-5 as backbone model, Smolagents Open Deep Research (Roucher et al., 2025) with Claude Opus 4.6 as its backbone, and Tongyi Deep Research (Team et al., 2025). For brevity, full results are deferred to Appendix  (Table 10)  . three open-source compare We Results reveal several notable patterns across open-source DRAs. All three agents exhibit crit1https://huggingface.co/datasets/Salesforce/ LiveResearchBench 8 ically low Citation Integrity scores, exposing systemic weakness in citation grounding, though the failure modes differ. Smolagents Open DR and Tongyi Deep Research achieve near-zero CI (4.78 and 1.03 in aggregate, respectively) primarily because they rarely ground claims to specific sources, reflecting negligible claim attribution rates. LangChain Open DR attains higher CI (15.92) thanks to substantially better attribution practices, yet is undermined by low citation faithfulness: attributed sources frequently fail to support the claims they are attached to (see Figures 8 and 9 for detailed breakdown). Beyond citation behavior, Smolagents Open DR dominates in both source quality and synthesis, leading aggregate scores for Factuality (58.15), Writing Quality (63.97), Key-Information Coverage (75.95), and Reasoning Quality (69.16), achieving strong content quality despite its near-absent citation discipline. Tongyi Deep Research ranks second on Factuality (55.09) but trails on adaptive metrics (aggregate RQ of 45.48), while LangChain Open DR records the lowest Factuality (44.64) yet outperforms Tongyi on reasoning (RQ of 57.28). Overall, these results suggest that open-source DRAs can produce informative and well-written reports, but remain fundamentally limited by poor citation grounding, critical gap for trustworthy research agents. Robustness across Backbone Models. To evaluate the stability of our agentic metrics, we conducted sensitivity analysis using different LLMs, specifically DeepSeek-V3.2 (Liu et al., 2025) and Kimi-K2.5 (Team et al., 2026), as the backbone for protocol execution on DEEPRESEARCH BENCH. As shown in Table 12, our findings indicate that while absolute scores exhibit minor fluctuations depending on the judges internal calibrationan expected variance across different LLM familiesthe relative performance rankings of the evaluated deep research agents remain highly robust. Notably, we observe perfect alignment across all metrics, with the slight exception of the Writing Quality (WQ) metric, for which Claude Sonnet 4.5 tended to provide more uniform scores, showing less pronounced preference for specific agent compared to the other backbone models."
        },
        {
            "title": "6 Conclusion",
            "content": "This work introduces unifying taxonomy for Deep Research Evaluation that identifies fundamental failure in existing methodologies: the Mirage of Synthesis. We demonstrate that current LLM-as-a-judge and reference-based benchmarks are often blinded by surface-level fluency and citation alignment, failing to detect deep-seated defects in factual correctness, temporal validity, and logical reasoning. These failures stem from structural evaluator capability mismatch, where static evaluators lack the agency required to verify the very claims they are tasked to judge. To resolve this, we propose the principle of capability parity and instantiate it in DREAM, framework that makes evaluation itself agentic. By transitioning from frozen rubrics to dynamic, tool-equipped evaluation protocols, DREAM provides temporally aware fact-checking and substantive reasoning validation. Our controlled evaluations confirm that these capabilities are essential for reliable assessment: DREAM detects temporal degradation and extrinsic factual errors that static benchmarks miss entirely, while reliably surfacing reasoning flaws masked by stylistic coherence. Ultimately, DREAM demonstrates that as AI agents gain the ability to research and reason over the open web, the frameworks used to judge them must evolve in kind. By grounding assessment in evidence and agency rather than surface form, DREAM provides scalable and reference-free blueprint for evaluating the next generation of frontier deep research agents."
        },
        {
            "title": "Limitations",
            "content": "While DREAM addresses the evaluator capability mismatch, several limitations remain. First, reliance on external tools introduces dependencies on third-party service availability and potential retrieval biasa trade-off inherent to prioritizing temporal validity over closed-world consistency. Second, agentic evaluation is computationally intensive: multi-step verification and tool-interaction loops increase latency and cost compared to static judges. We view this as necessary trade-off for scientific accuracy, though future work could explore optimization via caching or selective evaluation strategies. Finally, DREAM is post-hoc evaluator of research outputs and does not directly assess intermediate research processes, such as search trajectory efficiency or source discovery dynamics. Extending evaluation to include process-level telemetry remains promising future direction."
        },
        {
            "title": "References",
            "content": "Anthropic. 2025. Claude sonnet 4.5. Accessed: 202510-15. Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055. Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1775417762. Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, and 1 others. 2023. FacTool: Factuality Detection in Generative AIA Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. arXiv preprint arXiv:2307.13528. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. João Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, Abhijay Paladugu, Pranav Setlur, Jiahe Jin, Jamie Callan, João Magalhães, Bruno Martins, and 1 others. 2025. DeepResearchGym: free, transparent, and reproducible evaluation sandbox for deep research. arXiv preprint arXiv:2505.19253. Cohere. 2023. Introducing embed v3. Accessed: 202510-01. Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. 2025. DeepResearch Bench: Comprehensive Benchmark for Deep Research Agents. arXiv preprint. Assaf Elovic. 2025. GPT-Researcher: Autonomous agent designed for comprehensive online rehttps://github.com/assafelovic/ search. gpt-researcher. GitHub repository. Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. Ragas: Automated evaluation of retrieval augmented generation. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 150158. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. arXiv preprint arXiv:2305.14627. Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, and 1 others. 2025. Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge. arXiv preprint arXiv:2506.21506. 9 Rujun Han, Yanfei Chen, Zoey CuiZhu, Lesly Miculicich, Guan Sun, Yuanjun Bi, Weiming Wen, Hui Wan, Chunfeng Wen, Solène Maître, and 1 others. 2025. arXiv Deep researcher with test-time diffusion. preprint arXiv:2507.16075. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. Preprint, arXiv:2103.03874. Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Huichi Zhou, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, and 1 others. 2025. Deep research agents: systematic examination and roadmap. arXiv preprint arXiv:2506.18096. Yucheng Jiang, Yijia Shao, Dekun Ma, Sina Semnani, and Monica Lam. 2024. Into the unknown unknowns: Engaged human learning through participation in language model agent conversations. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 99179955. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, and Orhan Preprint, Firat. 2025. arXiv:2502.19187. Big-bench extra hard. LangChain AI. 2025. LangChain Open Deep Research. https://github.com/langchain-ai/ open_deep_research. Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, and 1 others. 2025. WebWeaver: Structuring web-scale evidence with dynamic outlines for open-ended deep research. arXiv preprint arXiv:2509.13312. Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, and 1 others. 2025. Deepseek-v3.2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: benchmark for general ai assistants. Preprint, arXiv:2311.12983. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100. Liana Patel, Negar Arabzadeh, Harshit Gupta, Ankita Sundar, Ion Stoica, Matei Zaharia, and Carlos Guestrin. 2025. DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis. arXiv preprint arXiv:2508.20033. Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, Junran Peng, Zhaoxiang Zhang, Songyang Zhang, and Kai Chen. 2024. Hellobench: Evaluating long text generation capabilities of large language models. Preprint, arXiv:2409.16191. Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, and Clémentine Fourrier. 2025. Open-source DeepResearch Freeing our search agents. https: //huggingface.co/blog/open-deep-research. Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica Lam. 2024. Assisting in writing wikipedia-like articles from scratch with large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 62526278. Manasi Sharma, Chen Bo Calvin Zhang, Chaithanya Bandi, Clinton Wang, Ankit Aich, Huy Nghiem, Tahseen Rabbani, Ye Htet, Brian Jang, Sumana Basu, and 1 others. 2025. ResearchRubrics: Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents. arXiv preprint arXiv:2511.07685. Kimi Team, Tongtong Bai, Yifan Bai, Yiping Bao, SH Cai, Yuan Cao, Charles, HS Che, Cheng Chen, Guanduo Chen, and 1 others. 2026. Kimi K2.5: Visual Agentic Intelligence. arXiv preprint arXiv:2602.02276. Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, and 1 others. 2025. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701. Haiyuan Wan, Chen Yang, Junchi Yu, Meiqi Tu, Jiaxuan Lu, Di Yu, Jianbao Cao, Ben Gao, Jiaqing Xie, Aoran Wang, and 1 others. 2025. DeepResearch Arena: The First Exam of LLMs Research Abilities via SeminarGrounded Tasks. arXiv preprint arXiv:2509.01396. Jiayu Wang, Yifei Ming, Riya Dulepet, Qinglin Chen, Austin Xu, Zixuan Ke, Frederic Sala, Aws Albarghouthi, Caiming Xiong, and Shafiq Joty. 2025. LiveResearchBench: live benchmark for usercentric deep research in the wild. arXiv preprint arXiv:2510.14240. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 10 2024a. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. Preprint, arXiv:2307.10635. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024b. Executable code actions elicit better llm agents. In Fortyfirst International Conference on Machine Learning. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025. Browsecomp: simple yet challenging benchmark for browsing agents. Preprint, arXiv:2504.12516. Yuhao Wu, Ming Shan Hee, Zhiqiang Hu, and Roy KaWei Lee. 2025a. LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs. In The Thirteenth International Conference on Learning Representations. Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, and Fei Huang. 2025b. Writingbench: comprehensive benchmark for generative writing. Preprint, arXiv:2503.05244. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2024. Webarena: realistic web environment for building autonomous agents. Preprint, arXiv:2307.13854. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, and 1 others. 2024. Agent-as-ajudge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934."
        },
        {
            "title": "A Extended Related Work",
            "content": "The rapid development of DRAs has outpaced the capabilities of existing evaluation frameworks. Our work addresses this gap by proposing holistic evaluation protocol tailored to these complex systems. Deep Research Systems. The domain of autonomous research has evolved from simple retrieval agents to complex systems capable of synthesizing long-form reports. In addition to the proprietary deep research systems (like those considered in Section 5), open frameworks have pioneered transparent architectures. STORM (Shao et al., 2024) introduced the paradigm of outline-driven retrieval, using multi-perspective question asking to simulate expert dialogue. This was further extended by CO-STORM (Jiang et al., 2024) to include human-agent collaboration. In the open-source domain, GPT-RESEARCHER (Elovic, 2025) established baseline for parallelized plan-and-solve execution, where static planner decomposes tasks for concurrent retrieval agents. WEBWEAVER (Li et al., 2025) improved upon this with dynamic outline optimization, while TTD-DR (Han et al., 2025) proposed test-time diffusion framework for iterative report refinement. Despite this proliferation, recent survey (Huang et al., 2025) highlights critical limitations in current evaluation benchmarks, notably their restricted access to external knowledge. From Static Judges to Agentic Evaluation To scale evaluation beyond human annotation, recent work has coalesced around the LLM-as-a-Judge paradigm (Liu et al., 2023; Zheng et al., 2023). However, these static judges typically focus on final outcomes, failing to capture the complex, multi-step decisionmaking inherent to autonomous agents. This limitation has spurred the evolution toward Agent-as-a-Judge frameworks, designed to verify the execution process itself rather than just the result. Zhuge et al. (2024) pioneered this approach for code generation, proposing recursive agentic evaluation to assess intermediate steps without excessive manual labor. More recently, MIND2WEB-2 (Gou et al., 2025) extended this to the web domain, arguing that the complexity of long-horizon tasks exceeds the capacity of simple LLM calls and necessitates an agentic evaluator to trace navigation trajectories. However, these frameworks remain confined to specific domains like code generation or web navigation and have not yet been applied to the open-ended information synthesis required in deep research. While our primary focus is on the holistic evaluation of DRAs, an extensive body of work targets the isolated sub-skills that underpin this domain, ranging from atomic factuality to long-form text generation. Factuality and Atomic Verification. To address hallucination, recent works have focused on granular verification. FACTSCORE (Min et al., 2023) introduced the paradigm of decomposing long-form text into atomic facts for independent verification. Similarly, FACTOOL (Chern et al., 2023) pioneered the use of tool-augmented frameworks for detecting factual errors. However, these methods focus strictly on atomic truthfulness and do not evaluate the synthesis, argumentation, or structural coherence of complete research report. Retrieval-Augmented Generation (RAG) and Citation. While standard RAG benchmarks like RGB (Chen et al., 2024) evaluate retrieval accuracy, more specialized frameworks focus specifically on attribution. ALCE (Gao et al., 2023) and RAGAS (Es et al., 2024) established the standards for measuring citation recall and faithfulness. However, to maintain reproducibility, these benchmarks typically rely on static snapshots of the web or closed corpora where ground-truth documents are pre-defined. This constraint allows for recall calculation but fails to capture the complexity of open-ended deep research, which operates on the live, dynamic web. Unlike RAG tasks where the goal is often to retrieve specific gold document, deep research admits multiple valid search trajectories. Consequently, evaluation cannot rely on retrieval recall against fixed index, but must instead assess the extrinsic validity of autonomously discovered evidence and its synthesis into coherent narrative. Additionally, it does not take into account verticals such as Presentation Quality or Analytical Depth. Web Interaction and Reasoning Benchmarks. For web interaction, WEBARENA (Zhou et al., 2024) and BROWSECOMP (Wei et al., 2025) evaluate browsing skills and goal-directed navigation in controlled environments. Beyond these, benchmarks such as GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), GAIA (Mialon et al., 2023), SCIBENCH (Wang et al., 2024a), and BIG-BENCH HARD (Kazemi et al., 2025) focus on quantitative reasoning and domain-specific problem solving. 12 Figure 6: Agentic Taxonomy Sub-Dimension Clustering Visualization. Each point represents an evaluation criterion from one of four deep research benchmarks (shape), colored by its LLM-assigned taxonomy dimension. Criteria assigned to the same dimension tend to form localized regions in embedding space, suggesting coherent semantic groupings across the four verticals. Long-form Text Generation. Finally, benchmarks such as LONGWRITER (Bai et al., 2024), HELLOBENCH (Que et al., 2024), WRITINGBENCH (Wu et al., 2025b), and LONGGENBENCH (Wu et al., 2025a) evaluate models specifically on coherence, length compliance, and stylistic quality. These frameworks primarily assess Presentation Quality, often relying on static references or intrinsic LLM-based judging without external verification of the contents truthfulness."
        },
        {
            "title": "B Agentic Taxonomy Pipeline",
            "content": "As deep research evaluation gains momentum, multiple benchmarks have been proposed, each targeting overlapping yet distinct evaluation dimensions. While these efforts share common goals, differences in terminology, granularity, and evaluation focus make direct comparison difficult. To better understand the structure of existing evaluation practices, we develop an automated pipeline that organizes evaluation criteria from prior benchmarks into unified, benchmark-agnostic taxonomy. The pipeline consists of two stages. In the first stage, multiple LLM-based agents analyze each benchmark repository independentlyDeepResearchGym (Coelho et al., 2025), DeepResearch Bench (Du et al., 2025), ResearchRubrics (Sharma et al., 2025), and LiveResearchBench (Wang et al., 2025). Agents examine benchmark documentation, evaluation prompts, rubrics, and source code to extract leaf-level evaluation criteria alongside short natural language descriptions. Across the four benchmarks, this process yields 48 distinct criteria. All LLM-based stages in the pipeline use Claude Opus 4.5 with fixed decoding settings (temperature=0) to ensure consistency across extraction and aggregation. In the second stage, single LLM clusters the extracted criteria into higher-level evaluation dimensions based on semantic similarity rather than benchmark provenance. To discourage benchmark-specific groupings, we impose constraint that each resulting dimension must contain criteria originating from multiple benchmarks. We refer to individual extracted items as criteria, which are grouped into dimensions and summarized as small set of high-level verticals. This process reveals four recurring evaluation verticals observed across benchmarks: Presentation Quality: Writing clarity, structural organization, formatting consistency, and readability. Task Compliance: Adherence to instructions, coverage of requested topics, and fulfillment of explicit and implicit task requirements. Analytical Depth: Reasoning rigor, critical evaluation, originality of insights, and synthesis across multiple perspectives. 13 Source Quality: Proper citation usage, source verification, and the extent to which claims are supported by credible, traceable evidence. To assess the semantic coherence of the induced taxonomy, we embed all extracted criteria using Coheres embed-english-v3.0 (Cohere, 2023) model and visualize the resulting representations using UMAP (Figure 6). Criteria assigned to the same taxonomy dimension by the LLM tend to form localized regions in embedding space, providing qualitative evidence that the identified pillars correspond to coherent semantic groupings rather than artifacts of benchmark-specific phrasing."
        },
        {
            "title": "C Metrics Details",
            "content": "In this section, we outline the implementation details and computation procedures for the suite of metrics employed in DREAM, categorized into Static Metrics (Writing Quality, Factuality, Citation Integrity, and Domain Authoritativeness) and Adaptive Metrics (Key-Information Coverage and Reasoning Quality). C.1 Writing Quality Writing Quality (WQ) is static, reference-free metric that evaluates the stylistic and structural presentation of the research report. It is assessed by an LLM Evaluator using fixed rubric that scores the report along three weighted dimensions. Dimensions and Weights: The score is computed as the average of three primary dimensions, each composed of weighted sub-dimensions, detailed in Table 3: Ideas and Content (33%): Evaluates clarity of the main idea, relevance of details, information density, and conceptual synthesis. Organization (33%): Assesses heading structure, logical grouping of bullet points, and structural coherence. Sentence Fluency (33%): Measures rhythm/variety, transition smoothness, and readability/flow. Scoring Procedure: For given task t, the LLM Evaluator assigns score St,d [0, 100] for each dimension {Ideas and Content, Organization, Sentence Fluency} based on the sub-dimension weights. The final Writing Quality score for task is: WQt = 1 3 (cid:88) Sd,t C.2 Factuality Factuality evaluates whether the content of reports generated by the DRA is factually accurate with respect to external ground truth. The implementation utilizes multi-stage verification pipeline that explicitly seeks both supporting and contradicting evidence for extracted claims. 1. Key Factual Claim Extraction: An LLM extracts the = 30 most salient factual claims from the report with respect to the research question. The extraction is context-aware, incorporating the current date to resolve temporal references (e.g., current status). 2. Neutralized Query Construction and Search: For each extracted claim, the system uses an LLM to generate multiple neutralized search queries. This step is critical to avoid confirmation bias: rather than searching for the specific values or predicates in the claim (e.g., searching for inflation dropped to 2%), the system generates open-ended queries (e.g., current inflation rate) to maximize the retrieval of both supporting and contradictory evidence. These queries are executed via Web Search Tool, and results are deduplicated to form pool of candidate source content. 3. Dual-Stream Evidence Extraction: Unlike standard RAG verification, we perform two distinct extraction passes on the retrieved content, Supporting Stream: The LLM extracts passages that explicitly confirm the claim. Opposing Stream: The LLM actively scans for passages that contradict or refute the claim. Table 3: Writing Quality Rubric. The exact descriptions used as system prompts for the LLM Evaluator to score each sub-dimension. Dimension Sub-dimension (weight) Evaluation Prompt (Exact Description) Main Idea Clarity (0.25) Detail Relevance (0.25) Ideas and Content Information Density (0.25) Conceptual Synthesis (0.25) Heading Structure (0.3) Organization Bullet Grouping Logic (0.4) Structural Coherence (0.3) Rhythm & Variety (0.3) Transition Smoothness (0.3) Sentence Fluency Readability & Flow (0.4) This dimension assesses the clarity and specificity of the main idea expressed in the section summary. high-quality section will present focused and well-articulated central idea that is tightly aligned with the report question. Summaries lacking precision, or that simply list general topics without insight or framing, should be penalized. Do not give high scores if the main idea is vague, overgeneralized, or merely implied. This dimension focuses on how well the supporting details in the summary reinforce the main idea. Bullet points should be specific, relevant, and purposefully selected. Low-quality summaries may include off-topic, overly generic, or redundant details that do not support the sections main message. Do not reward high scores based on the amount of content alonefocus on alignment and purpose. This dimension measures the information richness of the section summary. High-density summaries use each bullet to convey important, non-obvious, and topic-specific content. Shallow summaries repeat known facts, use vague language, or include fluff. Length alone should not be rewardedfocus on content value per line. This dimension evaluates the structural and conceptual integration in the summary. Look for signs of synthesis such as: grouping related points, identifying contrasts, cause-effect relationships, or thematic framing. Poor summaries are unordered lists with no visible logic. Do not reward correctness alonethis dimension rewards insight, not just content. This dimension assesses the use and clarity of headings in the section. High-quality summaries include headings that meaningfully segment the content, reflect topic hierarchy, and help orient the reader. Avoid rewarding default, generic, or misaligned headings. Headings should reflect actual conceptual boundaries. This dimension evaluates the internal logic of bullet groupings. Highquality summaries group related points together according to thematic, temporal, causal, or hierarchical logic. Low-quality groupings mix unrelated ideas, interrupt flow, or reflect no discernible principle. This dimension assesses whether the sections structure contributes to logical, easy-to-follow reading experience. coherent structure will show consistent flow from one part to the next, maintain logical transitions between bullet blocks, and avoid jarring shifts. Low-scoring sections often feel fragmented, with unclear order, repetition, or misplaced content. This dimension evaluates how naturally and dynamically the sentences flow. Strong writing features variation in sentence length and structure, avoiding repetitive patterns. Rhythm refers to the pacing and cadence of the prosewhether it reads with natural emphasis or becomes monotonous. High-scoring writing feels expressive and crafted, not just correct. This dimension focuses on how smoothly the sentences connect to each other. High-quality prose includes natural linking phrases, varied connectors, and logical sequencing. Low-scoring writing jumps between ideas, or has jarring, abrupt shifts between sentences. Do not reward correctness alonethis dimension targets flow between thoughts. This dimension evaluates the overall readability and flow of the paragraph. High-scoring writing reads smoothly aloud and requires little effort to follow. Low-scoring writing may include awkward phrasing, overcomplex or confusing sentence structures, or poor pacing. This metric captures the global fluency felt by readers, especially in multi-sentence passages. 15 4. Factuality Judgment: The Judge LLM reviews the claim alongside the aggregated Supporting and Opposing evidence. It assigns one of four labels: Supported: Evidence explicitly confirms the claim. Partially Supported: Evidence supports some aspects of the claim but differs on details or is mixed. Contradicted: Evidence clearly refutes the claim. Unverifiable: Evidence is insufficient, indirect, or too weak to make reliable determination. Factuality Scoring: The Factuality score for task t, denoted as Ft, is calculated by weighting verified claims based on their support labels (1.0 point for Support and 0.5 point for Partial Support): The overall Factuality score is computed as the average across all tasks in the dataset: Ft = Nsupp,t + 0.5Npart,t Nsupp,t + Npart,t + Ncon,t . = 1 (cid:88) tT Ft . C.3 Citation Integrity Citation Integrity (CI) evaluates the trustworthiness of report by determining whether its claims are both explicitly attributed to citations and faithfully supported by the content of those sources. Relying on either metric in isolation is insufficient, as report may achieve perfect citation coverage by hallucinating sources, or conversely, be factually accurate but lack transparency. We therefore decompose Citation Integrity into two constituent metricsClaim Attribution (CA) and Citation Faithfulness (CF)before unifying them into single score. The evaluation begins with Verifiable Claim Extraction phase, where we identify all factual and argumentative assertions within the report. Crucially, an LLM filters out non-verifiable contentsuch as procedural meta-talk (e.g., The following section discusses...), subjective commentary, and common knowledgeto establish precise set of verifiable claims that forms the basis for evaluation. Claim Attribution (CA). This metric quantifies the extent to which the agent grounds its reasoning in external sources. It is defined as the ratio of distinct cited claims to the total number of verifiable claims. score of 1.0 indicates that every verifiable claim is explicitly linked to source URL, maximizing the reports auditability. Citation Faithfulness (CF). This metric evaluates the veracity of the provided evidence. For the subset of claims that are cited, CF determines whether the content of the source genuinely supports the claim. The workflow proceeds by retrieving the content of the associated URL for every cited claim and employing an LLM Judge to compare the text against the claim. The Judge assigns one of five labels: Supported, Partially Supported, Neutral, Contradicted, or Unverifiable. The citation faithfulness score for task t, denoted CFt, is defined as: CFt = Nsupp,t + 0.5Npart,t Nsupp,t + Npart,t + Nneu,t + Ncon,t . The overall CF score is then computed as the average across tasks: CF = 1 (cid:88) tT CFt. Unified Citation Integrity Score. To penalize trade-offs between attribution and faithfulness, we compute the final CI score as the harmonic mean of the two components: CI = 2 CA CF CA + CF , where CA is the average claim attribution score across all tasks in the dataset. This formulation ensures that high CI score is only achievable when an agent consistently supports its claims with valid, corroborating evidence; failure in either (e.g., citing nothing, or citing everything incorrectly) will reduce the score. 16 C.4 Domain Authoritativeness Domain Authoritativeness (DA) evaluates the credibility and trustworthiness of the external sources cited by the agent. Unlike Citation Faithfulness, which checks if specific URL supports specific claim, DA assesses the reputation of the source domain itself, penalizing reliance on low-quality or unverifiable outlets (e.g., social media, clickbait farms) even if the content matches the claim. The evaluation process proceeds as follows: 1. Domain Extraction and Deduplication: First, we aggregate all unique URLs cited across the report (derived from the CI pipeline). We extract the root domain from each URL (e.g., https://www.nature.com/articles/xyz nature.com) and deduplicate them to ensure each source is evaluated only once per report. 2. LLM-Based Authority Assessment: An LLM Judge evaluates each unique domain using rubric that assesses its overall reputation, credibility, and trustworthiness. The judge considers factors such as institutional backing, historical reliability, and editorial standards to classify the domain into category (e.g., Government, Academic, News, Commercial) and assign an integer score Sd [1, 10] based on the following scale: Definitive Authority (910): Gold-standard sources with institutional credibility (e.g., government agencies, top-tier academic institutions). High Authority (78): Trustworthy and credible sources (e.g., established news organizations). Moderate Authority (46): Acceptable but not ideal sources (e.g., general commercial sites). Low Authority (13): Unreliable sources with questionable credibility (e.g., social media platforms, unverified blogs). 3. Scoring Formulation: The scores are normalized to [0, 1] interval. The Domain Authoritativeness score for task t, denoted as DAt, is the average normalized score of all domains Dt cited in that task: DAt = 1 Dt (cid:88) dDt Sd The final metric is the average across all tasks in the dataset. C.5 Key-Information Coverage Key-Information Coverage (KIC) is an adaptive metric that measures whether the report addresses essential, query-specific facts. Unlike static comprehensiveness metrics, KIC uses an agent to retrieve up-to-date external knowledge to generate the evaluation criteria. Protocol Creation (Adaptive): For each query, the Protocol Creation Agent (equipped with web search tools) identifies essential facts required for complete answer. Each fact is converted into yes/no question qk (e.g., Does the report mention the Jan 2026 deadline?) grounded in retrieved evidence. Evaluation: The LLM Evaluator checks the generated report against each question qk. Let vk,t {0, 1} be the verification result for question on task t. KIC Scoring: The score is the recall rate of these key facts: KICt ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 vk,t C.6 Reasoning Quality Reasoning Quality (RQ) evaluates the logical rigor and analytical depth of the report. It focuses on the validity of arguments, the synthesis of disparate sources, and the avoidance of logical fallacies. Validation Plan Generation: The Protocol Creation Agent generates set of open-ended, queryspecific Challenging Questions. For each question, it generates structured Validation Plan consisting of: (1) Extraction of reasoning chains from the report, (2) External verification using tools (e.g., web search, ArXiv, GitHub), and (3) Comparison criteria. Agentic Execution: The Agent Evaluator executes the validation plan. It actively retrieves external evidence to verify the reports reasoning steps. Unlike KIC which checks for the presence of facts, RQ checks the soundness of the logic connecting them. RQ Scoring: The Agent Evaluator assigns score Rt [0, 10] based on deductive rubric (starting at 10, deducting points for logical gaps, unsupported inferences, or ignored counter-evidence). The final normalized score is: RQt ="
        },
        {
            "title": "D Controlled Experiments Details",
            "content": "D.1 Temporal Awareness in KIC Rt 10 To generate the reference reports for RACE, we utilized Smolagents open Deep Research agent (Roucher et al., 2025), an open-source CodeAgent-based framework. We performed three independent generation runs per query to create distinct report versions: one reflecting current information, and two simulating outdated knowledge bases with cutoffs set to January 1st, 2025, and January 1st, 2024, respectively. These simulated cutoffs were enforced by appending specific temporal constraints to the system prompt and strictly filtering results from the web search tool to exclude any content published after the target dates. In Table 4, we present the complete evaluation results for DRBRACE Comprehensiveness, Insight, and DREAMKIC across all 20 queries. The data indicates that DRBRACE lacks consistent temporal sensitivity. While Comprehensiveness scores do drop on average for the Jan-2024 cutoff (e.g., Query 2 DOGE agency authority), the metric is effectively blind to the Jan-2025 cutoff, with average scores remaining virtually identical to the baseline. Furthermore, RACE metrics frequently exhibit paradoxical behavior, where scores increase for outdated reports (e.g., Query 1 Current papal leadership). In contrast, DREAMKIC demonstrates far stronger alignment with the information lag. Although not strictly monotonic (scores fluctuated in two instances Queries 11 and 15) DREAMKIC typically imposes drastic penalties as the knowledge horizon recedes, effectively capturing the absence of specific, recent developments that the baseline metrics miss. To complement the results in Figure 4, the complete set of DRBRACE Comprehensiveness and Insight criteria as well as KIC checklist items is provided in Table 5. Table 4: Temporal sensitivity analysis. Comparison of DRB-RACE and DREAM-KIC scores across current (Upd.) and simulated outdated knowledge cutoffs (Jan 2025, Jan 2024) for the DRA. Unlike baselines, DREAM-KIC scores degrade on reports generated with older knowledge cutoffs, effectively capturing information lag. ID Topic DRB-RACE Comp. DRB-RACE Insight DREAM-KIC Upd. Jan-25 Jan-24 Upd. Jan-25 Jan-24 Upd. Jan-25 Jan-24 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Current papal leadership & policies DOGE agency authority & funding US-China LLM competition (DeepSeek) US-Venezuela military tensions Sudan civil war status UK Prime Ministers challenges Boeing CEO & Starliner mission Nvidia market cap ranking Latest Best Picture winner Ukrainian Kursk occupation Protein structure prediction SOTA US Strategic Bitcoin Reserve Digital Euro implementation Tesla Cybercab deployment GLP-1 drug supply status TikTok divestiture deadline SpaceX Starship IFT results Taylor Swift Eras Tour impact Paramount-Skydance merger status Disney CEO succession 48.29 54.17 50.62 46.96 48.32 55.57 50.03 52.30 51.44 50.28 48.82 48.34 51.44 46.77 51.09 48.61 47.47 50.62 50.87 48.32 53.80 40.46 47.10 47.07 48.72 50.59 51.77 50.26 48.22 46.63 49.73 79.21 42.92 44.88 47.05 51.67 47.86 47.21 51.61 53. 55.75 23.73 44.75 49.15 0.00 48.58 49.86 49.29 51.18 40.69 45.86 82.53 41.17 40.64 42.01 43.71 47.46 43.18 47.71 48.89 48.55 56.44 50.14 44.38 46.21 55.64 49.87 48.41 48.60 48.43 44.78 52.59 56.50 53.36 50.07 53.70 51.22 53.06 48.40 50.51 52.12 33.56 45.90 44.32 48.46 48.17 52.62 47.49 46.31 44.93 49.05 73.29 42.30 45.73 49.30 60.86 48.01 47.71 40.44 55.10 55.23 17.62 39.76 47.28 0.00 42.11 44.32 42.67 51.08 40.43 41.73 60.24 38.28 39.58 41.09 44.90 45.85 45.78 51.33 44.31 64.00 73.33 84.38 46.88 81.67 64.00 94.44 78.00 84.87 80.00 65.75 81.33 97.00 75.00 73.70 95.83 81.25 92.20 73.33 100.00 0.00 13.33 56.13 6.12 52.00 43.80 78.00 38.89 6.13 43.33 71.88 33.00 25.00 60.71 82.78 50.00 49.62 60.20 36.67 88. 0.00 0.00 31.25 0.00 0.00 36.20 5.33 22.44 6.13 16.67 18.88 11.44 21.75 39.14 56.13 16.58 34.25 56.00 10.00 64.53 AVG 50.02 50.04 44.81 50. 48.78 41.68 79.35 44.80 22.34 Table 5: Task Alignment Criteria Comparison. DeepResearch Bench RACE rubric criteria alongside DREAM KIC Checklist items for the query: Prepare regulatory compliance update on the legal status of TikTok in the U.S. Specifically, clarify the proximity of the deadline for ByteDance to divest its U.S. operations to avoid nationwide app store ban. # RACE Criteria Comprehensiveness DREAM KIC Checklist Items 1 Identification and Explanation of Core Legislation: Assesses if the article clearly identifies the specific U.S. law mandating the potential ban (e.g., the Protecting Americans from Foreign Adversary Controlled Applications Act) and explains its key provisions. This is the foundational context for the entire update. Does the report mention the Protecting Americans from Foreign Adversary Controlled Applications Act (PAFACA) as the specific law requiring ByteDance to divest TikTok? 2 Detailed Breakdown of the Divestiture Deadline and Extensions: Evaluates if the article provides the exact initial deadline for divestiture and, crucially, clarifies the conditions and duration of any potential presidential extensions. This directly addresses the tasks specific requirement to clarify the deadlines proximity. Does the report state that the current divestiture deadline is January 23, 2026? 3 Coverage of Ongoing Legal Challenges: Checks if the article discusses the lawsuits filed by TikTok/ByteDance or other parties to challenge the law. This is essential for comprehensive understanding of the current legal status, as these proceedings could impact the deadline and enforcement. Does the report accurately calculate and convey that approximately 31 days or one month remains until the deadline from late December 2025? 4 Analysis of Key Stakeholder Positions: Assesses whether the report covers the stances and actions of key stakeholders, including the U.S. Executive and Legislative branches, ByteDance, and the Chinese government (regarding its approval of tech exports). This provides complete picture of the geopolitical and corporate landscape. Does the report mention that ByteDance signed deal in mid-December 2025 to create U.S. joint venture, with closing scheduled for January 22, 2026? 5 Explanation of the Bans Enforcement Mechanism: Evaluates if the article explains how the ban would be implementedspecifically, the nature of the nationwide app store ban and its implications for app distribution and hosting services. This clarifies the practical consequences of non-compliance. 6 Inclusion of Relevant Historical Context: Checks for brief summary of previous U.S. government actions or attempts to regulate TikTok (e.g., under the Trump administration). This provides depth and demonstrates thorough understanding of the long-standing nature of the issue. Insight Does the report specify the ownership structure of the joint venture deal, including that ByteDance will retain 19.9% and U.S. investors will hold majority control? Does the report mention that the Supreme Court upheld the TikTok divestiture law in January 2025? 7 Analysis of Legal Challenges and Timeline Impact: Evaluates if the article moves beyond merely stating that lawsuits exist to deeply analyze the core legal arguments (e.g., First Amendment rights), assess their potential to secure an injunction or stay, and logically connect these legal outcomes to the potential alteration of the divestiture deadline. This is the primary factor defining the deadlines stability. Does the report mention that the original deadline was January 19, 2025, and that TikTok briefly went dark on that date? 8 Synthesis of Factors into Forward-Looking Scenarios: Assesses the ability to integrate the legal, political, and business analyses into coherent forecast of potential outcomes (e.g., successful sale, ban proceeds after court losses, extended legal stalemate). This demonstrates holistic understanding and provides the most valuable strategic insight. Does the report document that President Trump granted multiple extensions to the TikTok deadline throughout 2025? 9 Depth of Divestiture Feasibility Assessment: Evaluates the analysis of the practical barriers to sale, such as the technical complexity of separating TikToks recommendation algorithm, the financial valuation challenges, and the need for approval from the Chinese government. This insight is crucial for determining if the deadline is realistically achievable. Does the report explain the specific consequences of missing the deadline, including prevention of app store updates and distribution? 10 Contextualization of the Deadlines Political Significance: Assesses if the article effectively frames the deadline within the broader U.S.-China geopolitical tensions and U.S. domestic political motivations. This provides insight into the why behind the law and the political will to enforce it, which influences any potential for extensions or negotiated solutions. Does the report mention the bipartisan congressional support for the TikTok divestiture law? 11 Identification of Key Signposts for Future Monitoring: Evaluates whether the analysis identifies specific, critical future events or indicators (e.g., key court dates, statements from potential buyers, Chinese regulatory responses) that readers should watch to anticipate how the situation will evolve. This transforms the analysis into an actionable intelligence tool. Does the report explain the national security concerns about data sharing with China that motivated the law? 12 13 14 Does the report mention that TikTok has approximately 170 million U.S. users? Does the report mention the valuation of TikToks U.S. operations in the context of the deal? Does the report acknowledge that the deal requires Chinese regulatory approval, creating potential uncertainty? D.2 Detecting Reasoning Flaws We curated ten research queries spanning domains where analytical reasoning is critical, including policy analysis, technical system comparisons, and causal explanations. Each query was designed to require multi-step reasoning and evidence synthesis, making them suitable for evaluating reasoning quality 19 Table 6: Research queries for the Reasoning Flaws Detection experiment in Section 4.3. ID Query How will 10% price cut affect unit sales next month for an e-commerce product line? 2 What assumptions about inflation, interest rates, and currency stability most influence predictions of stock market volatility in emerging economies over the next five years? 3 How will the adoption of renewable energy technologies impact global oil demand over the next decade? 4 What assumptions about hospital staffing, patient flow, and technology adoption most influence predictions of emergency department wait times over the next five years? 5 How will AI-powered recommendation engines affect consumer purchase diversity in online retail markets over the next two years? 6 What assumptions about monetary policy, housing supply, and demographic trends most influence predictions of urban housing prices over the next decade? How will climate change affect agricultural crop yields in major exporting countries over the next 20 years? 8 What assumptions about vaccine distribution, mutation rates, and healthcare access most influence predictions of pandemic recovery times? 9 How will quantum computing adoption influence financial risk modeling practices over the next 15 years? 10 How would heatwave next month affect ice cream demand? detection. The complete query set is provided in Table 6. To generate the reports, we used Smolagents open Deep Research agent (Roucher et al., 2025). For each query, we generated two report variants using identical source materials. High-quality reports were generated following standard analytical practices with sound logical structure, properly supported claims, and coherent argumentation. Malformed reports were systematically injected with reasoning flaws while preserving surface-level fluency and plausibility; injected flaws included unsupported causal claims, circular reasoning, false equivalences, and cherrypicked evidence. This controlled design yielded 20 total reports (10 high-quality, 10 malformed) forming 10 matched pairs, enabling direct comparison while controlling for query-specific factors. Each report was evaluated using both DREAMRQ and DRBRACE frameworks. D.3 Grounding Beyond Citation Faithfulness To rigorously test the hypothesis that citation-alignment metrics are blind to well-cited falsehoods, we constructed controlled dataset focusing on extrinsic factual errors, that is, claims that are false in reality but are accompanied by supporting citation. We manually curated 15 adversarial claim pairs. Each pair consists of: Ground Truth variant (ctrue) accompanied by valid source URL. Plausible Hallucination variant (cfalse) accompanied by matching, misleading URL. Crucially, the matching URLs for the false variants were selected to satisfy standard citation faithfulness checks, serving as specious support for the incorrect claims. These sources span various modes of invalidity, such as outdated informationfor instance, citing 2019 study claiming 46% of Bitcoin transactions are illicit, which directly contradicts current data showing less than 1%. Other examples leverage persistent myths, such as the mechanics rule of thumb to change oil every 3,000 miles despite modern engineering standards, or fringe narratives, like articles framing 15-minute cities as governmentimposed lockdown zones. This setup ensures that the falsity stems from the content of the claim relative to objective reality, rather than mismatch between the claim and its provided citation. The complete set of pairs, contrasting ground truth with aligned misinformation, is listed in Table 7. To isolate the sensitivity of the metrics, we bypassed the claim-source extraction phase and directly constructed synthetic evaluation payloads. For given corruption level [0, 1], we assembled test batch of size = 15 by selecting false variants and (1 r) true variants. These batches were then fed directly into the evaluation pipelines of both DRBFACT and DREAMFactuality. 20 Table 7: Adversarial Factuality Dataset. The 15 adversarial claim pairs used to measure factual sensitivity. Ground Truth (T) variants are backed by reliable evidence, while False (F) variants are paired with specious citations that satisfy text-alignment criteria but contradict objective reality. ID Topic Variant Claim 1 15-Min Cities 2 GMO Safety Climate Change 4 Homeopathy 5 DinoHuman Moon Landing 7 5G Health 8 Bermuda Triangle Knuckle Cracking 10 10% Brain 11 Google Energy EV Emissions 13 Crypto Illicit 14 Earth Shape Oil Change F F F T T F T T F 15-minute cities are an urban planning concept to place essential services within walking distance; they do not restrict movement. The 15-minute city is lockdown plan to restrict residents movement and fine them for leaving their zones. Commercially available GMO foods are safe for consumption and do not pose health risks. There is no scientific consensus on GMO safety; they pose potential long-term risks to health and biodiversity. Rapid warming since the mid-20th century is primarily driven by human activities, specifically fossil fuel emissions. Climate forecasting has fundamental uncertainties regarding natural variability; warming may not be as catastrophic as models predict. URL Domain fullfact spiked-online natl-academies ensser nasa judithcurry Homeopathy is pseudoscience with no medicinal effect beyond placebo. Homeopathy is clinically effective medical treatment for various conditions beyond the placebo effect. nhs hri-research Non-avian dinosaurs went extinct approximately 65 million years before modern humans evolved. Humans and dinosaurs lived on Earth at the same time, approximately 6,000 years ago. usgs answersingenesis On July 20, 1969, Apollo 11 landed on the Moon; Neil Armstrong and Buzz Aldrin became the first humans to walk on its surface. The 1969 Apollo moon landings were staged events, as evidenced by photographic anomalies. 5G frequencies are non-ionizing and safe for humans within international guidelines. 5G technology will massively increase exposure to radiofrequency radiation, which is proven harmful to humans. There is no evidence that disappearances occur more frequently in the Bermuda Triangle than in other well-traveled ocean areas. The Bermuda Triangle is deadly zone where ships and planes disappear at frequency far exceeding statistical probability. britannica time who ehtrust noaa smu Knuckle cracking creates popping sound due to cavitation; there are no known detrimental effects or links to arthritis. Cracking knuckles consistently can wear away cartilage and increase risk of developing arthritis. houstonmethodist nih Humans use virtually 100% of their brains throughout the day, even during sleep. Most humans only use 10% of their brain capacity, leaving vast potential untapped. britannica gaia Google search uses about 0.0003 kWh, orders of magnitude less energy than boiling kettle. Two Google searches generate the same amount of CO2 as boiling kettle for tea. Even accounting for battery manufacturing, total lifetime GHG emissions of an EV are lower than comparable gasoline car. Widespread EV adoption may increase emissions due to energy-intensive mining and refining of battery materials. google hindustantimes epa manhattan-inst Illicit activity accounts for less than 1% of total cryptocurrency transaction volume. About 46% of bitcoin transactions are involved in illegal activity, transforming black markets. chainalysis repec The Earth is an oblate spheroid, confirmed by satellite imagery, gravity, and centuries of astronomical observation. The Earth is stationary plane; the South Pole is an impenetrable ice wall surrounding the world. wikipedia gutenberg Modern vehicles using synthetic oils can go 7,50010,000 miles between changes, far surpassing the 3,000-mile guideline. Engine oil must be changed every 3,000 miles because additives break down, causing sludge and reducing lubrication. kbb artmorse 21 The divergence in performance, clearly visualized in Figure 1 (middle), highlights fundamental difference in evaluation scope. DRBFACT proved insensitive to the corruption, with citation accuracy scores remaining effectively constant ( 100%) across the entire sweep. Restricted to verifying consistency with the provided URL, the metric correctly identified that the false claims matched their sources, but failed to detect the underlying misinformation. In sharp contrast, DREAMFactuality exhibited near-linear degradation inversely proportional to the corruption rate r. By independently retrieving fresh evidence from the web, the evaluator successfully identified the external contradictions for the false variants, correctly rejecting the misleading citations provided in the input."
        },
        {
            "title": "E Additional Details of Human Evaluation",
            "content": "E.1 Human Evaluation Effort The evaluation process entails rigorous workflow to ensure reliable consistency. For each task within DeepResearch Bench (Du et al., 2025), the core data points expected in the report were first identified, followed by comprehensive review of the three corresponding protocols. This substantial effort ensures that the resulting feedback is sufficiently detailed to serve as robust ground truth for calculating precision and recall against human judgments. E.2 Protocol Creation Methods To assess the effectiveness of our approach, we compare three different strategies for creating evaluation protocols: Direct LLM generation: The LLM is directly prompted with the task description and produces evaluation criteria in single step. It relies solely on its internal knowledge and has no access to external resources. Agent without external knowledge: An agentic system generates evaluation criteria through multi-step reasoning. Although the agent does not have access to external knowledge, the multi-step process enables deeper analysis compared to direct LLM generation. Agent with external knowledge (DREAM): Our final setting equips the agent with both multi-step reasoning and access to external knowledge capabilities. The agent can query external sources (e.g., web search, and ArXiv/GitHub for domain-specific knowledge) to supplement its analysis. E.3 Human Evaluation Protocol Human evaluators are instructed to assess each evaluation point in the generated protocols based on relevance, clarity, and verifiability, with validation soundness additionally included for RQ. The detailed definitions of these criteria are provided in Table 8. To ensure fair comparison, protocols produced under the three different strategies are evaluated using the same criteria. Each evaluation point is rated on 13 scale (where higher scores indicate better quality), which is then normalized to 01 scale. We further analyze inter-annotator agreement across evaluators in Section E.4 to confirm the reliability of the human assessment. Table 8: Human evaluation rubric. Criteria used to guide expert assessment of created protocols."
        },
        {
            "title": "Relevance",
            "content": "Directly addresses the specific requirements and key aspects of the research query. Verifiability Can be objectively confirmed using accessible external evidence or well-defined logic."
        },
        {
            "title": "Clarity",
            "content": "Is formulated precisely and unambiguously to ensure consistent interpretation."
        },
        {
            "title": "Validation",
            "content": "Is methodologically correct, free of circular reasoning, and capable of rigorously confirming the answer. 22 E.4 Inter-Annotator Agreement Analysis We compute Kendalls coefficient of concordance (Kendalls W) to measure annotator agreement on protocol quality. Kendalls quantifies the level of concordance among raters when assessments are ranked; here, it reflects the degree to which annotators share similar preferences across the different creation strategies. As reported in Table 9, annotators exhibit moderate agreement on Relevance and Clarity (W 0.58), and substantial agreement on Verifiability and Validation Soundness (W > 0.75). The overall agreement is statistically significant across all indicators. We attribute the variance in agreement primarily to the similarity between the Direct LLM and Agent without retrieval baselines. Since both rely solely on the same backbone models internal knowledge, their outputs are often qualitatively similar, making it difficult for evaluators to consistently rank one over the other. However, the Agent with retrieval is consistently distinguished from these baselines, confirming that the human preference for retrieval-augmented protocols is reliable. Table 9: Agreement analysis among human annotators, measured using Kendalls W. All results are statistically significant (p < .05). Indicator Kendalls p-value Relevance Verifiability Clarity Validation Soundness"
        },
        {
            "title": "Average",
            "content": "0.58 0.78 0.58 0.86 0.69 .030 .009 .030 .006 ."
        },
        {
            "title": "F Detailed DREAM Benchmarking Results",
            "content": "In this section, we present complete evaluation results for three Deep Research Agents (DRAs) across our three benchmarks, complementing the results in Section 5. key advantage of the DREAM framework is its dataset-agnostic design, which enables us to normalize scores across heterogeneous tasks and compute unified aggregate performance profile. Performance Overview. Table 10 details the performance of each system on static metrics (WQ, Factuality, CI, DA) and adaptive metrics (KIC, RQ), alongside composite score averaging results across all metrics. As discussed in Section 5, distinct system profiles emerge: Smolagents Open DR leads in most metrics (except CI), though behavior may depend on the dataset, e.g., on DEEPRESEARCHBENCH, Tongyi Deep Research has higher WQ and Factuality. Source Quality Analysis. The detailed breakdown of our Source Quality-related metrics reveals clear distinctions in verification capabilities. Figure 7 presents the distribution of Factuality labels across all three benchmarks. Smolagents typically achieves higher proportion of Full Support judgments (green bars), indicating superior ability to generate claims that are independently corroborated by retrieved external evidence. In contrast, LangChain exhibits higher rates of Unverifiable or Contradict claims. similar pattern emerges in Figure 8, which details the Citation Faithfulness (CF) labels. Tongyi Deep Research produces no citations at all on DEEPRESEARCHBENCH and RESEARCHRUBRICS (marked N/A), while LangChain and Smolagents show high proportions of Unverifiable labels, indicating that cited sources frequently cannot corroborate the associated claims. Figure 9 visualizes the components of Citation Integrity by plotting CF against Citation Attribution (CA). This scatter plot reveals two distinct failure modes: LangChain attributes claims to sources frequently (high CA, 7580) but with poor faithfulness (low CF, 1020), meaning it cites often but inaccurately. Conversely, Smolagents and Tongyi Deep Research rarely attribute claims at all (low CA, 515), though when they do, faithfulness is moderately higher (CF 3555). Neither strategy yields reliable citation behavior, underscoring that open-source DRAs have yet to bridge the gap between source retrieval and faithful evidence grounding. 23 Table 10: DREAM evaluation. Static and adaptive metric scores for leading DRAs across three datasets. Dataset DRA WQ Fact. CI DA Static Metrics Adaptive Metrics KIC RQ DeepResearchBench LiveResearchBench ResearchRubrics Aggregate LangChain Open DR Smolagents Open DR Tongyi Deep Research LangChain Open DR Smolagents Open DR Tongyi Deep Research LangChain Open DR Smolagents Open DR Tongyi Deep Research LangChain Open DR Smolagents Open DR Tongyi Deep Research 63.69 63.30 63.95 60.75 61.71 60.03 62.53 66.60 61. 62.17 63.97 61.72 46.02 57.64 58.58 38.11 56.90 51.69 49.13 59.38 56.06 44.64 58.15 55.09 14.43 2.45 0. 11.90 9.67 2.92 19.81 2.03 0.00 15.92 4.78 1.03 86.32 9.44 0.00 84.00 24.41 5.71 90.00 0.00 0. 85.08 17.96 3.38 64.99 74.15 58.53 63.66 73.55 55.17 68.25 78.74 59.87 65.96 75.95 57.95 53.34 63.68 40. 59.04 68.82 46.95 57.84 72.15 46.75 57.28 69.16 45.48 Report Length Analysis. Table 11 and Figure 10 present the word count statistics (Mean Std, Min, Max) and distributions, respectively, for each agent. Smolagents Open DR consistently generates the longest reports across all datasets, averaging 3,0003,700 words, with high variance and heavy-tailed distributions (e.g., 2,066 words on RESEARCHRUBRICS, with outliers exceeding 14,500 words). LangChain Open DR and Tongyi Deep Research produce more concise output in the 1,4001,800 word range, with Tongyi being the most consistent (Std 250350 words). Robustness Across Backbone Models. We examined the impact of the underlying LLM backbone used to execute DREAMs evaluation protocol on DEEPRESEARCH BENCH. We compared the default model, Claude Sonnet 4.5, against DeepSeek-V3.2 and Kimi-K2.5 across our suite of static and adaptive metrics. The findings detailed in Table 12 highlight DREAMs robust evaluation signal. Even though absolute scores may change due to varying internal grading thresholds, the relative performance rankings of the evaluated deep research agents remain highly consistent. minor exception is the Writing Quality (WQ) metric, for which Claude Sonnet 4.5 did not provide strong differentiation among agents, whereas DeepSeek-V3.2 and Kimi-K2.5 yield identical rankings. Figure 7: Factuality Label Distribution. Distribution of factuality judgments (Full Support, Partial Support, Contradict, Unverifiable) for each model across three datasets. 24 Figure 8: Citation Faithfulness Label Distribution. Distribution of citation faithfulness labels assessing the alignment between claims and cited source text across three datasets. Figure 9: Citation Integrity Components. Citation Faithfulness and Citation Attribution visualized for each model across three benchmarks. 25 Figure 10: Report length distributions across DRAs and datasets. Word count distributions across DRAs and datasets. Smolagents Open Deep Research consistently exhibits the highest variance and mean length, while LangChain Open Deep Research and Tongyi Deep Research are more concise and consistent. Table 11: Report length statistics across DRAs and datasets. Word counts are reported as Mean Standard Deviation. Smolagents consistently produces the longest reports, while Tongyi Deep Research is the most concise."
        },
        {
            "title": "DRA",
            "content": "Mean Std"
        },
        {
            "title": "LangChain Open DR\nSmolagents Open DR\nTongyi Deep Research",
            "content": "1628.3 398.3 3092.0 1728.0 1431.0 248."
        },
        {
            "title": "LangChain Open DR\nSmolagents Open DR\nTongyi Deep Research",
            "content": "1792.0 459.9 3009.3 1627.8 1505.2 350."
        },
        {
            "title": "LangChain Open DR\nSmolagents Open DR\nTongyi Deep Research",
            "content": "1763.2 430.7 3735.2 2065.6 1436.1 301.4 836 1022 670 701 352 254 1069 1050 597 2650 8511 2055 2784 9261 3275 14554 2214 Table 12: Backbone Model Comparison. Metric scores on DEEPRESEARCHBENCH across three judge backbones. Agent rankings remain largely consistent regardless of the judge model used."
        },
        {
            "title": "DRA",
            "content": "WQ Fact. CI DA"
        },
        {
            "title": "KIC",
            "content": "Claude Sonnet 4.5 DeepSeek V3.2 Kimi K2."
        },
        {
            "title": "LangChain Open DR\nSmolagents Open DR\nTongyi Deep Research",
            "content": "46.02 57.64 58.58 47.35 54.45 57.46 48.96 58.76 61.55 14.43 2.45 0.00 17.24 6.02 0.00 17.70 3.25 0. 86.32 9.44 0.00 86.14 60.72 6.22 84.63 13.30 0.00 64.99 74.15 58.53 63.86 73.57 57.00 60.53 69.74 51. 63.69 63.30 63.95 66.19 67.10 65.27 61.90 64.43 58.82 26 RQ 53.34 63.68 40. 67.68 71.89 56.20 57.50 63.00 43."
        }
    ],
    "affiliations": [
        "AWS Agentic AI",
        "Georgia Institute of Technology"
    ]
}