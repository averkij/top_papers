{
    "paper_title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning",
    "authors": [
        "Runyang You",
        "Yongqi Li",
        "Xinyu Lin",
        "Xin Zhang",
        "Wenjie Wang",
        "Wenjie Li",
        "Liqiang Nie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. Current studies usually position LLMs as external reasoning modules to yield auxiliary thought for augmenting conventional recommendation pipelines. However, such decoupled designs are limited in significant resource cost and suboptimal joint optimization. To address these issues, we propose \\name, a unified large recommender model with intrinsic reasoning capabilities. Initially, we reconceptualize the model architecture to facilitate interleaved reasoning and recommendation in the autoregressive process. Subsequently, we propose RecPO, a corresponding reinforcement learning framework that optimizes \\name\\ both the reasoning and recommendation capabilities simultaneously in a single policy update; RecPO introduces a fused reward scheme that solely leverages recommendation labels to simulate the reasoning capability, eliminating dependency on specialized reasoning annotations. Experiments on three datasets with various baselines verify the effectiveness of \\name, showing relative improvements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at https://github.com/YRYangang/RRec."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 4 9 9 6 1 . 5 0 5 2 : r R2EC: TOWARDS LARGE RECOMMENDER MODELS WITH REASONING Runyang You1 Yongqi Li1 Xinyu Lin2 Xin Zhang1 Wenjie Wang3 Wenjie Li1 Liqiang Nie4 1The Hong Kong Polytechnic University 2National University of Singapore 3University of Science and Technology of China 4Harbin Institute of Technology (Shenzhen) runyang.y@outlook.com, liyongqi0@gmail.com, xylin1028@gmail.com, zhangxin2023@stu.hit.edu.cn, wenjiewang96@gmail.com, cswjli@comp.polyu.edu.hk, nieliqiang@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Large recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. Current studies usually position LLMs as external reasoning modules to yield auxiliary thought for augmenting conventional recommendation pipelines. However, such decoupled designs are limited in significant resource cost and suboptimal joint optimization. To address these issues, we propose R2ec, unified large recommender model with intrinsic reasoning capabilities. Initially, we reconceptualize the model architecture to facilitate interleaved reasoning and recommendation in the autoregressive process. Subsequently, we propose RecPO, corresponding reinforcement learning framework that optimizes R2ec both the reasoning and recommendation capabilities simultaneously in single policy update; RecPO introduces fused reward scheme that solely leverages recommendation labels to simulate the reasoning capability, eliminating dependency on specialized reasoning annotations. Experiments on three datasets with various baselines verify the effectiveness of R2ec, showing relative improvements of 68.67% in Hit@5 and 45.21% in NDCG@20. Code available at https://github.com/YRYangang/RRec."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities in contextual understanding and openended generation [1, 2, 3, 4]. This success has catalyzed the development of large recommender models that inherit and specialize in the advantages of LLMs for recommendation. Current approaches can be divided into two main streams: one employs LLMs as powerful encoders to embed users (their historical interactions) [5, 6, 7], while the other reformulates item prediction into the autoregressive generation of item identifiers [4, 8]. These large recommender models exhibit remarkable generalization capabilities, achieving advanced performance across diverse application scenarios, including cold-start recommendations [9], cross-domain personalization [10], and long-tail item prediction [11, 12]. The frontier of LLM capabilities now extends beyond model size scaling to test-time scaling, i.e., reasoning. Emerging advances like DeepSeek-R1 [13] demonstrates that such extra computation during inference can further improve LLM capabilities in areas such as mathematics, coding, and scientific problem-solving [14]. Given that large recommender models are instantiated from pretrained LLMs, natural question is: how can large recommender model benefit from reasoning to further enhance recommendation performance? Existing studies have preliminarily explored LLM reasoning for recommendation, including user preference analysis [15, 16], synthetic item profile enrichment [17, 18], user search query rewriting [19, 20], and rating prediction [21]. These approaches typically take LLMs as additional reasoning modules to augment the original recommendation Yongqi Li is the corresponding author. Figure 1: The architecture overview of R2ec, which facilitates interleaved reasoning and recommendation in an autoregressive process with two task-specific heads: 1) language-modeling head (lm_head) for reasoning generation; and 2) recommendation head (rec_head) for item prediction. pipeline, decoupling reasoning from item prediction. However, such designs introduce crucial limitations: 1) Significant resource cost. Two distinct modules, namely the large reasoning model and the recommendation model, must be trained, checkpointed, and served in parallel, inflating both memory footprint and inference latency. 2) Suboptimal joint optimization. Reasoning and recommendation modules can only be trained by freezing one while updating the otherfirst update the reasoning module, then switch to the recommendation module, and continue alternating in this manner [15]. This alternate optimization scheme prevents gradient flow across the pipeline, precluding true end-to-end learning, hindering fine-grained alignment between reasoning and ranking objectives, and ultimately leading to suboptimal convergence. In this work, we aim to develop unified large recommender model with intrinsic reasoning capabilities, exploring new technical solutions for reasoning-enhanced recommendation systems. It is non-trivial to develop unified large recommender model with reasoning due to the following aspects: 1) Model design. Recommendation is traditionally framed as discriminative ranking task, whereas reasoning is generative process. How to incorporate both generative reasoning and discriminative recommendation into one single model architecture is worth careful consideration. 2) Training optimization. It is also challenging to stimulate and optimize the reasoning capabilities of large recommender models. On the one hand, recommendation domains lack annotated reasoning traces, as personalized recommendation rationale is hardly obtainable, making supervised learning impractical. On the other hand, Reinforcement Learning (RL) [20, 22, 23] is natural alternative but encounters unique problems in reward and training objective design within the recommendation task. Since reasoning and recommendation capabilities interact within single model, this necessitates training framework that effectively bridges these two aspects. To address the above issues, we propose unified large Recommender model with intrinsic Reasoning, dubbed as R2ec. 1) In the model design, as illustrated in Figure 1, we introduce the recommendation head to the LLM backbone, alongside the original language-modeling head. This setup allows the model to first generate reasoning tokens autoregressively and then predict the item in the last step. This model architecture integrates both generation and recommendation tasks, while the next crucial step is to simulate the reasoning capabilities tailored for accurate item predictions. 2) In training optimization, we propose an RL-based training framework without human reasoning annotations. First, we sample trajectories from the model, which essentially consist of reasoning-then-item sequences, similar to the reasoning-then-answer sequences in LLMs. Second, unlike Deepseek-R1, which defines rewards based on answer accuracy, we found it insufficient to set the reward solely using item recommendation metrics. Therefore, we propose fused reward scheme that combines discrete ranking rewards with continuous similarity rewards. With the sampled trajectories and assigned rewards, joint reasoning-and-recommendation training objective is introduced to optimize the model for improving the recommendation through reasoning. We conducted extensive experiments to compare R2ec with various traditional, LLM-based, and reasoning-augmented recommendation baselines. The experimental results on three datasets illustrate R2ec significantly outperforms all baselines with relative improvements of 68.67% in Hit@5 and 45.21% in NDCG@20, verifying the effectiveness of the unified reasoning-recommendation model design. We also performed the ablation study and 5 in-depth analysis experiments, demonstrating that R2ec possesses strong generalization capabilities with different optimization algorithms. 2 More surprisingly, we observed similar phenomenon in LLM reasoning that the reasoning length increases as training progresses. The key contributions in this work are summarized as follows: We establish R2ec, the first unified large recommender model that incorporates reasoning and recommendation. By introducing the recommendation head along with the original language-modeling head, R2ec interleaves the reasoning generation and item prediction in the single autoregressive generation process. We propose RecPO, an RL framework that enables optimization of both the reasoning and recommendation simultaneously in single policy update, which only requires the common recommendation labels with the introduced fused reward scheme. Extensive experiments show that R2ec achieves significant improvement (68.67% in Hit@5 and 45.21% in NDCG@20) on three datasets. It is noted that similar phenomenon in LLM reasoning occurs, where the reasoning length increases as training progresses."
        },
        {
            "title": "2.1 Reinforcement Learning for LLM Reasoning",
            "content": "OpenAIs GPT-o1 demonstrated that scaling reinforcement learning (RL) training with large compute budgets enables models to internalize reasoning as differentiable policy, achieving state-of-the-art performance in emergent metareasoning tasks [24]. Early RLHF methods trained reward model on human preference data and fine-tuned the policy using Proximal Policy Optimization (PPO) [23]; however, PPOs multiple optimization passes introduce implementation complexity. To streamline RL tuning, Direct Preference Optimization (DPO) [25] performs single-step policy updates, trading simplicity for potential off-policy bias. Alternative estimators, such as Group Relative Policy Optimization (GRPO) [22], derive baselines from group-level rewards, eliminating the need for separate critic, while REINFORCEs leave-one-out (RLOO) [26] computes advantages by excluding each trajectory sequentially. Building on these advancements, DeepSeek-Zero [27] removes reliance on supervised fine-tuning by driving reasoning emergence purely through intrinsic RL rewards, and DeepSeek-R1 further integrates rule-driven reward engineering with self-play verification to enhance reasoning robustness."
        },
        {
            "title": "2.2 Large Language Model-based Recommendation",
            "content": "Recent advances in LLM-based recommendation typically follow two paradigms: 1) LLM-enhanced, where LLMs enrich recommendation pipelines with additional features, and 2) LLM-centric, where recommendation is framed as generative task via item identifiers [1, 2, 3, 4, 28, 29], or LLMs serve as encoders to embed users and items [6, 30]. We term models under the second paradigm as large recommender models. In the first paradigm, LLM-generated embeddings are utilized to enhance conventional recommenders. Adoption of LLM-generated text are not new, typical examples includes user intent [15, 31], search queries [20], item summaries [18], or rationales [32]. Among them, SLIM [32], LangPTune [15], and Rec-R1 [20] further optimize LLM for finer rationale generation with RL. However, such designs introduce high resource demands for training and serving both models, and the connection between two modules remains gradient-less, blocking true end-to-end learning, resulting in suboptimal convergence and performance. Large recommender models typically recommend through autoregressive generation of item identifiers [4, 28] or titles [1, 2, 3, 29], or by embedding user interaction sequences [6, 30]. Large recommender model has demonstrated remarkable potential, but current methods fail to utilize reasoning ability of LLMs. To the best of our knowledge, no existing large recommender model improves recommendation over explicit reasoning generation, which is the gap that motivated our work."
        },
        {
            "title": "3.1 LLM-based Recommendation",
            "content": "LLM-based recommendation is an emerging paradigm, and the most applicable approach is to leverage LLMs as encoders to embed users and items, which are widely adopted in industry [30, 28, 33]. Typically, the recommendation process within this paradigm involves the following structured steps: User and Item Formulation. Structured interaction histories and item metadata are first formulated into natural language prompts suitable for LLMs. We first collect users historically interacted items with the corresponding 3 ratings. We then construct textual prompt includes both the instruction and natural-language description of these past behaviors. E.g., User has purchased and rated the following products sequentially: 1. Avatar Blu-ray (4/5); 2. Wireless Headphones (5/5); . . . Recommend the next item. Finally, we tokenize this prompt into sequence of tokens, denoted as xu, which serves as the LLM input. Likewise, each candidate item can be described using its metadata, then tokenized into sequence of tokens xv. The tokenized textual prompts xu and xv are then input into the LLM transformer backbone, by extracting the final hidden state of the input sequence, we can obtain corresponding user embedding hu and item embedding hv. Training via Contrastive Learning. To optimize LLM for recommendation tasks, in-batch contrastive loss is typically adopted: LCL = log hv+/τ (cid:1) exp(cid:0)h vB exp(cid:0)h hv/τ (cid:1) , (cid:80) (1) where τ is the temperature hyperparameter controlling similarity dispersion, is the ground truth item set in one batch, and v+ is the groud-truth target item. Inference. During inference, recommendation score s(u, v) for each user-item pair is computed via the inner product between their embeddings: s(u, v) = hv. Finally, we rank all items according to the scores and obtain top-K item recommendation list."
        },
        {
            "title": "3.2 Reinforcement Learning for Large Reasoning Models",
            "content": "Terminologies and Notations. RL for LLMs treats text generation as sequential action process, where each action refers to outputting the next token. Formally, at each time step t, we denote the current state as (x, o<t), where is the initial prompt and o<t is all previously generated tokens. Then, the conditioned probability distribution of an action is denoted as πθ(otx, o<t), where πθ is the policy parameterized by θ. Based on the distribution, we can obtain the next token ot through manual selection, greedy or temperature sampling. Upon obtaining complete sequence o, termed as trajectory, reward function then evaluates and assigns it with scalar score that can reflect user satisfaction or answer correctness. However, using directly in gradient estimates leads to high variance and unstable updates; instead, the advantage A, which measures how much better an action is than the expected return from that state, is adopted to reduce variance and improve sample efficiency [22, 23, 26, 34]. Sampling and Advantage Computation. Sampling-based advantage estimation is widely adopted in recent advances [22, 26, 34, 35]; below we describe its basic pipeline and two typical computation methods. Given an input x, group of different trajectories {oi}G i=1, are sampled from πθold. Existing studies widely obtain these trajectories via top-K sampling with temperature, where θold refers to the frozen policy parameters before current update. Each trajectory oi receives scalar reward Ri, which will then be used to compute the trajectory-level advantages via two widely adopted approaches, namely GRPO [22] and RLOO [26]:"
        },
        {
            "title": "ARLOO",
            "content": "i = Ri"
        },
        {
            "title": "1\nG − 1",
            "content": "G (cid:88) j=i Rj, AGRPO = Ri mean({Rj}G std({Rj}G j=1) j=1) . Training Objective. Training then proceeds with policy-gradient algorithms. Specifically, let ri,t(θ) = πθ(oi,tx, oi,<t) πθold (oi,tx, oi,<t) (2) (3) be the importance ratio between the updated and old policies of trajectory at token position t. The training objective is given by: J(θ) = xD,{oi}G i=1πθold (x) (cid:88) oi (cid:88) i=1 t=1 [min (ri,t(θ)Ai, clip(ri,t(θ), 1 ϵ, 1 + ϵ)Ai)] , (4) where ϵ defines the clipping trust-region radius, which prevents excessively large updates, thereby reducing variance and improving optimization stability."
        },
        {
            "title": "4 R2ec: Large Recommender Model with Reasoning",
            "content": "R2ec is large recommender model that thinks to recommend. We first outline the model design that enables the incorporation of generative reasoning and discriminative recommendation into single model in Section 4.1. And then our proposed RL optimization algorithm tailored for achieving unified reasoning and recommendation capabilities is introduced in Section 4.2."
        },
        {
            "title": "4.1 Model Design",
            "content": "Architecture. As illustrated in Figure 1, our proposed R2ec is built upon decoder-only backbone with two taskspecific heads: 1) Language-modeling head (lm_head). The token embedding table HT RT d. Where is the token set, and each row is d-dimensional embedding for one token. This head is responsible for the generation of reasoning tokens. 2) Recommendation head (rec_head). The item embedding table HV RVd. Where is the item set, and each row hv in HV is obtained by encoding item description prompt into the model itself and extracting the final hidden state. This head is used to score items for recommendation. Such design integrates generative reasoning and discriminative recommendation within single unified model. It supports flexible and scalable item update by simply adding, deleting, or replacing vectors in the item embedding table, contrasting with generative recommendation systems that require hard-coded tokenization [36, 8, 4, 37], enabling effective zero-shot generalization and accommodating large-scale item catalogs without severe degradation on recommendation quality or efficiency. Inference begins with feeding the tokenized user prompt xu (template of prompts can be found Item prediction. in Appendix E) into the transformer-based backbone, producing an initial hidden state h0. The language-modeling head then maps h0 to the first reasoning token o1. This process continues autoregressively, yielding sequence of reasoning tokens o1:T . The final hidden state of the generated sequence hT is then fed into the recommendation head, where each candidate item is scored by an inner product s(v) = HV [v], V, which determines the final ranking. This mechanism yields tight reasoningrecommendation coupling, since both the language-modeling head and recommendation head share the same hidden-state space as input, reasoning directly reshapes hT and thus yielding more accurate recommendation scores s(v). Such alignment ensures that reasoning optimization (Section 4.2) contributes directly to finer recommendation."
        },
        {
            "title": "4.2 Optimization: RecPO",
            "content": "Our goal is to train the policy πθ to jointly performs reasoning and recommendation, i.e., it must generate coherent reasoning sequences to rank the target item accurately. Accordingly, we structure our optimization workflow in three parts. First, we introduce the trajectory sampling strategy that draws multiple reasoning trajectories for each user (Section 4.2.1). Second, we describe reward and advantage estimation, where discrete ranking signals and softmax similarities are fused into single scalar award for the above sampled trajectories (Section 4.2.2). Finally, we formulate the training objective, which blends reasoning-level and recommendation-level updates through clipped-ratio loss (Section 4.2.3). For complete description of our end-to-end training and inference pipeline, please refer to Appendix D."
        },
        {
            "title": "4.2.1 Trajectory Sampling",
            "content": "Due to the optimization objective of joint learning, we define one trajectory in our settings spanning the entire reasoning-then-recommend process: xu πθ o1 πθ ... πθ oT πθ v+, where the initial state xu encodes the user history and instruction, o1, ..., oT represents the intermediate actions of outputing reasoning tokens, and v+ as the action of recommending the ground-truth target item. For each user u, we first sample reasonings {oi}G i=1 πθold( xu) by top-K sampling with temperature to control stochasticity. Each sampled reasoning is then fed through the policy πθ to produce complete reasoning-then-recommend trace, which is subsequently used for reward calculation and advantage estimation. i=1 with the old policy πθold using the tokenized input xu: {oi}G"
        },
        {
            "title": "4.2.2 Reward and Advantage Estimation",
            "content": "Given the above sampled trajectories, we now aim to assign rewards to them. Basically, the reward should align with the evaluation criteria, i.e., the recommendation metrics in our work, encouraging the model to achieve better performance. 5 However, in practice, we find that directly using the recommendation metrics as rewards is insufficient, as many trajectories of varying quality can result in the same top-K ranking. We therefore introduce fused reward scheme that combines discrete ranking reward Rd and continuous similarity reward Rc, which are formulated as follows: Rd = NDCG @k (cid:0)rank(v+)(cid:1) , Rc = hv+/τ (cid:1) exp(cid:0)h vV exp(cid:0)h hv/τ (cid:1) , (cid:80) where Rd is the NDGC, Rc is the softmax similarity of recommending target item against all items in V. The final reward is then obtained through linear combination: = β Rc + (1 β) Rd, β [0, 1], (5) where the weighting coefficient β is empirically set to β 0.05 to keep the ranking term dominant while providing sufficient resolution among trajectories that attain identical ranks. With rewards {Ri}G i=1 we can obtain trajectory-level advantages {Ai}G i=1 via Eqn.(2)."
        },
        {
            "title": "4.2.3 Training Objective",
            "content": "Given the goal of joint optimization of reasoning and recommendation, we treat the entire reasoning-then-recommend sequence (xu, o1, ..., oT , v+) as single RL trajectory. Policy optimization therefore operates over composite action space, where the policy first makes token-level decisions to generate reasoning, then selects an item at the recommendation stage. Under this formulation, the importance ratio from Eqn. (3) extends to: ri,t(θ) = πθ(oi,txu,oi,<t) πθold (oi,txu,oi,<t) , πθ(v+xu,oi,T ) πθold (v+xu,oi,T ) , if (reasoning) if = + 1 (recommendation). Specifically, we model the recommending action, i.e., recommending the target item v+ via the in-batch softmax: πθ(v+ xu, oi) = hv+/τ (cid:1) exp(cid:0)h vB exp(cid:0)h hv/τ (cid:1) . (cid:80) (6) (7) Let ℓϵ(r, A) = min(rA, clip(r, 1 ϵ, 1 + ϵ)A) be the standard clipping operator with threshold ϵ, we define the joint reasoning-and-recommendation training objective as: (θ) = {u,v+}D,{oi}G i=1πθold (xu)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) (cid:104) Ti(cid:88) i=1 t=1 ℓϵ(ri,t(θ), Ai) + δi,i ℓϵ (cid:0)ri,T +1(θ), Ai (cid:1)(cid:105) . (8) (cid:1). This In Eqn.(8), all trajectories contribute to token-level policy updates via the standard clipped objective ℓϵ design ensures the policy continues to learn from diverse reasoning behaviors. For the last recommendation action, only the trajectory with the highest advantage, identified by = arg maxj Aj, contributes gradients to recommendation optimization. This design concentrates recommendation learning with the most promising reasoning path, while the presence of other 1 reasoning paths contributes to ample exploration in reasoning actions, thus preserving exploration and ensuring effective recommendation learning. (cid:0)ri,t(θ), Ai"
        },
        {
            "title": "5.1 Setups",
            "content": "Dataset and Metrics. Following previous work on large recommender models [3, 2, 1, 29], we conducted experiments using real-world datasets sourced from Amazon, including CDs and Vinyl (CDs), Video Games (Games), and Musical Instruments (Instruments). Dataset statistics and preprocessing steps can be referred to Appendix A. We utilized two commonly used metrics: Hit Rate (H@K) and Normalized Discounted Cumulative Gain (N@K), with cutoff set to 5, 10 and 20. We adopted the full set recommendation setting, where metrics are computed over the entire item set, providing better reflection of practical scenarios. Baselines and Implementation. To comprehensively evaluate R2ec, we selected competitive baselines from various categories, including traditional sequential recommenders (GRU4Rec [38], Caser [39], SASRec [40]), LLM-based recommender (TIGER [8], BigRec [2], D3 [3]), and reasoning augmented recommendation systems (LangPTune [15]). More baseline details can be found in Appendix B. We used Qwen and Gemma as the LLM backbones. More implementation details are described in Appendix C. 6 Table 1: The overall performance of baselines and R2ec on three datasets. The best results in each group are marked in Bold, while the second-best results are underlined. * implies the improvements over the second-best results are statistically significant (p-value < 0.05). % improve represents the relative improvement achieved by R2ec over the best baseline. Instruments CDs and Vinyl Video Games Method H@5 N@5 H@10 N@10 H@20 N@20 H@5 N@5 H@10 N@10 H@20 N@ H@5 N@5 H@10 N@10 H@20 N@20 GRU4Rec 0.0171 0. 0.0193 0.0142 0.0201 0.0144 0.0067 0. 0.0104 0.0041 0.0156 0.0051 0.0109 0. 0.0181 0.0093 0.0301 0.0123 Caser 0. 0.0141 0.0115 0.0149 0.0127 0.0155 0. 0.0029 0.0067 0.0037 0.0089 0.0042 0. 0.0083 0.0191 0.0103 0.0279 0.0126 SASRec 0.0175 0.0144 0.0201 0.0162 0.0223 0. 0.0076 0.0104 0.0081 0.0119 0.0086 0. 0.0129 0.0080 0.0206 0.0105 0.0326 0. TIGER 0.0171 0.0128 0.0184 0.0132 0. 0.0134 0.0067 0.0045 0.0097 0.0055 0. 0.0069 0.0123 0.0085 0.0222 0.0116 0. 0.0142 Q e BigRec D3 0.0052 0. 0.0111 0.0052 0.0189 0.0072 0.0045 0. 0.0089 0.0039 0.0141 0.0052 0.0008 0. 0.0016 0.0006 0.0128 0.0034 0.0042 0. 0.0094 0.0037 0.0192 0.0062 0.0082 0. 0.0141 0.0076 0.0253 0.0104 0.0054 0. 0.0104 0.0044 0.0197 0.0067 LangPTune 0.0127 0. 0.0224 0.0115 0.0348 0.0145 0.0074 0. 0.0156 0.0080 0.0208 0.0094 0.0049 0. 0.0088 0.0040 0.0140 0.0140 R2ec 0.0237* 0.0154* 0.0374* 0.0198* 0.0615* 0.0259* 0.0513* 0.0372* 0.0647* 0.0414* 0.0818* 0.0457* 0.0288* 0.0185* 0.0532* 0.0264* 0.0827* 0.0337* % Improve. 35.43% 6.94% 66.96% 22.22% 52.61% 23.33% 46.57% 58.30% 37.95% 51.09% 20.83% 40.62% 84.62% 76.19% 104.62% 87.23% 92.33% 50.45% BigRec D3 0.0068 0.0048 0.0101 0. 0.0130 0.0066 0.0030 0.0030 0.0052 0. 0.0119 0.0053 0.0156 0.0105 0.0260 0. 0.0430 0.0182 0.0072 0.0038 0.0202 0. 0.0339 0.0114 0.0216 0.0129 0.0327 0. 0.0446 0.0194 0.0117 0.0068 0.0210 0. 0.0378 0.0224 LangPTune 0.0130 0.0079 0.0221 0. 0.0403 0.0152 0.0350 0.0235 0.0469 0. 0.0677 0.0325 0.0068 0.0053 0.0120 0. 0.0195 0.0094 R2ec 0.0264* 0.0161* 0.0397* 0.0203* 0.0615* 0.0257* 0.0573* 0.0398* 0.0804* 0.0472* 0.1042* 0.0527* 0.0326* 0.0205* 0.0531* 0.0271* 0.0835* 0.0347* % Improve. 50.86% 11.81% 77.23% 25.31% 52.61% 22.38% 63.71% 69.36% 71.43% 72.26% 53.91% 62.15% 108.97% 95.24% 104.23% 92.20% 94.19% 54.91% Table 2: Ablation study on key components of R2ec."
        },
        {
            "title": "Method",
            "content": "H@5 N@5 H@10 N@10 H@20 N@20 H@5 N@5 H@10 N@10 H@20 N@20 H@5 N@5 H@10 N@10 H@20 N@20 w/o Reasoning 0.0176 0.0121 0.0296 0.0153 0.0511 0.0200 0.0469 0.0321 0.0692 0.0393 0.0945 0.0456 0.0277 0.0174 0.0441 0.0227 0.0748 0.0303 w/o Rd w/o Rc R2ec 0.0198 0.0124 0.0338 0.0164 0.0560 0.0224 0.0521 0.0338 0.0766 0.0404 0.0974 0.0486 0.0302 0.0196 0.0487 0.0254 0.0798 0.0332 0.0244 0.0160 0.0394 0.0208 0.0605 0.0258 0.0543 0.0382 0.0774 0.0456 0.1012 0.0515 0.0316 0.0202 0.0534 0.0264 0.0814 0. 0.0264 0.0161 0.0397 0.0203 0.0615 0.0257 0.0588 0.0388 0.0804 0.0457 0.1086 0.0525 0.0326 0.0205 0.0531 0.0271 0.0853 0."
        },
        {
            "title": "5.2 Overall Performance",
            "content": "To validate the effectiveness of the proposed R2ec, we showed the overall performance of baselines and our R2ec in Table 1. By analyzing the results, we gained the following findings. 1) Overall, R2ec consistently outperforms every competing baseline, underscoring the value of jointly optimizing reasoning and recommendation. 2) Traditional methods perform well on the Instruments dataset but struggle on CDs and Games, revealing their limited generality. LangPTune frequently ranks second, which validates the benefit of integrating explicit reasoning into the recommendation pipeline. Finally, large recommenders generally outperform traditional approaches, notably secure secondary position on the Games dataset. We attribute this advantage to their larger model scale and semantic understanding capabilities. 3) Comparing two backbones, Gemma consistently outperforms its larger counterpart, achieving up to 2 gains for D3 suggesting that Gemma may generally deliver stronger recommendation performance despite its smaller parameter count (2B vs. 3B)."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "We conducted ablation studies by evaluating the following variants: 1) w/o Reasoning. We eliminated reasoning tokens from prompts and trained exclusively with in-batch contrastive loss; 2) w/o Rc. We keep only the discrete ranking reward Rd and removed the continuous similarity reward Rc in Eqn.(5); and 3) w/o Rd. We removed the discrete ranking reward Rd from Eqn.(5). The results are summarized in Table 2, and several observations stand out. 1) It is found that R2ec achieves an average improvement of roughly 15% across all metrics compared to w/o Reasoning. These substantial gains demonstrate that our designed optimization have enabled R2ec to better leverage test-time scaling to deliver significantly stronger recommendation performance. 2) As revealed that w/o Rc (using only Rd) consistently outperforms w/o Rd (using only Rc), this indicates that adopting reward signal that directly reflects evaluation result is crucial for training, while the continuous reward Rc, despite offering finer granularity, fails to provide meaningful distinctions and instead introduces noise that leads to suboptimal performance. 3) By fusing Rd 7 (a) Train Reward (CDs) (b) Val Reward (CDs) (c) Val Length (CDs) (d) Train Reward (Instruments) (e) Val Reward (Instruments) (f) Val Length (Instruments) Figure 2: Analysis on advantage estimation methods, RLOO and GRPO, across two datasets. Train Reward and Val Reward indicate the variation in rewards on the training set and validation set, respectively. Val Length represents the variation in reasoning length on the validation set. with small weight on Rc, our approach preserves the task alignment of ranking rewards while benefiting from the supplementary signal of the continuous term. As result, R2ec achieves optimal performance on nearly all metrics. 5.4 In-depth Analysis"
        },
        {
            "title": "5.4.1 Analysis on Advantage Estimation",
            "content": "Accurate advantage estimation is crucial for reducing variance and improving sample efficiency in policy-gradient RL [26, 23, 34]. We therefore evaluated two estimators, GRPO [27] and RLOO [26], in our training pipeline. The variations of the training reward, validation reward, and reasoning length across the training steps are summarized in Figure 2, and we had the following observations. 1) First, as shown in Figure 2a and Figure 2d, both RLOO and GRPO exhibit high-variance training curves. This is due to the inherent nature of recommendation environments, which produce highly varied reward magnitudessome sessions result in high rankings, while others yield very low ones. 2) Second, as Figure 2b and Figure 2e illustrate, GRPO demonstrates faster learning in the initial training steps and consistently outperforms RLOO in terms of validation reward, whereas RLOO progresses more steadily. This divergence stems from the GRPOs unit-variance normalization magnifies rewards into larger gradients that accelerate early learning. 3) Besides, as shown in Figure 2c and Figure 2f, GRPOs reasoning length gradually increases as training progresses, which is consistent with the phenomenon observed in LLM reasoning training [27], while RLOO appears to maintain certain level of stability. This is because RLOO-driven optimization does not encourage improved reasoning with the low reward magnitudes in our task."
        },
        {
            "title": "5.4.2 Analysis on Trajectory Sampling",
            "content": "For R2ecs training, we performed trajectory sampling via the temperature τ and top-K sampling, which influence the stochasticity and diversity of the generated samples. To quantify their impact, we varied τ and K, and the results are presented in Figure 3. 1) It is observed that increasing the temperature produces longer resoning and boosts recommendation performance, i.e., , NDCG@5. higher temperature introduces greater sampling entropy, allowing the model to explore wider range of reasoning trajectories. 2) Conversely, it is found that increasing top-K actually shortens the reasoning length and generally leads to decline in recommendation performance. This is because larger top-K enlarges the candidate token set, which counterintuitively mitigates length hacking yet excessively large reintroduces noisy and low-quality samples."
        },
        {
            "title": "5.4.3 Analysis on Group Size",
            "content": "During training, we sampled group of trajectories to estimate their advantages. Therefore, we conducted experiments to analyze how varying group sizes impact performance. The results are outlined in Figure 3, and we gained several key 8 (a) (b) (c) (d) Figure 3: Analysis on trajectory sampling and group size over the CDs dataset. (a) and (b) show the impact of temperature and top-K sampling on performance and reasoning length, respectively. (c) and (d) present the effect of group size on NDCG@5 and Hit Rate@5, respectively. findings as follows: 1) It is observed that performance improves for both backbones as group size increases, but the rate of improvement gradually slows down. While larger group size generally leads to more explored paths, it also raises the training cost. These results suggest that selecting group size of 6 or 8 is sufficient, and further increasing the group size is unnecessary. 2) Comparing the two backbones, we found difference in sensitivity to group size. Qwens performance at group size of 1 lags significantly behind Gemmas, but it improves rapidly as the group size increases. Gemma performs well even with smaller groups, likely because its pretraining exposed it to broader range of reasoning scenarios, thereby equipping it with stronger initial reasoning-for-recommendation capabilities."
        },
        {
            "title": "5.4.4 More Analysis",
            "content": "Due to the scope limitation, we detail more analysis on the case study in Appendix H, and the analysis on the efficiency in Appendix G, respectively. We also discuss the limitations in Appendix F."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "In this study, we investigate the integration of reasoning into large recommender models by introducing R2ec, unified model that combines generative reasoning with discriminative recommendation within single autoregressive framework. This model is optimized using RecPO, novel reinforcement learning framework, without reliance on human-annotated reasoning annotations. Extensive experiments on multiple real-world datasets demonstrated that R2ecachieves substantial gains over traditional, LLM-based, and reasoning-augmented baselines. Further ablation and in-depth analyses provide insights into the impact of model and optimization design on reasoning emergence, recommendation quality, and inference efficiency. Taken together, these findings highlight the importance of tightly coupling reasoning and recommendation to unlock large recommender models full potential. In the future, we aim to further investigate the efficient reasoning in larger recommender models, striving for optimal thinking in recommendations."
        },
        {
            "title": "References",
            "content": "[1] Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, Xiang Wang, and Xiangnan He. LLaRA: Large Language-Recommendation Assistant, May 2024. arXiv:2312.02445 [cs]. [2] Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Chong Chen, Fuli Feng, and Qi Tian. Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems, December 2023. arXiv:2308.08434. [3] Keqin Bao, Jizhi Zhang, Yang Zhang, Xinyue Huo, Chong Chen, and Fuli Feng. Decoding Matters: Addressing Amplification Bias and Homogeneity Issue in Recommendations for Large Language Models. In Yaser AlOnaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1054010552, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [4] Bowen Zheng, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, Ming Chen, and Ji-Rong Wen. Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation, April 2024. arXiv:2311.09049. [5] Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan, Ang Li, Zuoli Tang, and Jun Zhou. (SAID) Enhancing Sequential Recommendation via LLM-based Semantic Embedding Learning. In Companion Proceedings of the ACM Web Conference 2024, pages 103111, Singapore Singapore, May 2024. ACM. 9 [6] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. Text Is All You Need: Learning Language Representations for Sequential Recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 23, pages 12581267, New York, NY, USA, August 2023. Association for Computing Machinery. [7] Xinyu Zhang, Linmei Hu, Luhao Zhang, Dandan Song, Heyan Huang, and Liqiang Nie. Laser: ParameterEfficient LLM Bi-Tuning for Sequential Recommendation with Collaborative Information, September 2024. arXiv:2409.01605 [cs]. [8] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H. Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q. Tran, Jonah Samost, Maciej Kula, Ed H. Chi, and Maheswaran Sathiamoorthy. Recommender Systems with Generative Retrieval, November 2023. arXiv:2305.05065. [9] Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang, and Ninghao Liu. Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendations, March 2024. arXiv:2306.17256 [cs]. [10] Qidong Liu, Xiangyu Zhao, Yejing Wang, Zijian Zhang, Howard Zhong, Chong Chen, Xiang Li, Wei Huang, and Feng Tian. Bridge the Domains: Large Language Models Enhanced Cross-domain Sequential Recommendation, April 2025. arXiv:2504.18383 [cs]. [11] Qidong Liu, Xian Wu, Wanyu Wang, Yejing Wang, Yuanshao Zhu, Xiangyu Zhao, Feng Tian, and Yefeng Zheng. LLMEmb: Large Language Model Can Be Good Embedding Generator for Sequential Recommendation, December 2024. arXiv:2409.19925 [cs]. [12] Qidong Liu, Xian Wu, Yejing Wang, Zijian Zhang, Feng Tian, Yefeng Zheng, and Xiangyu Zhao. LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential Recommendation, November 2024. arXiv:2405.20646 [cs]. [13] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning, March 2025. arXiv:2503.09516 [cs]. [14] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, s1: Simple test-time scaling, February 2025. Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. arXiv:2501.19393 [cs]. [15] Zhaolin Gao, Joyce Zhou, Yijia Dai, and Thorsten Joachims. End-to-end Training for Recommendation with Language-based User Profiles, October 2024. arXiv:2410.18870. [16] Yi Fang, Wenjie Wang, Yang Zhang, Fengbin Zhu, Qifan Wang, Fuli Feng, and Xiangnan He. Reason4Rec: Large Language Models for Recommendation with Deliberative User Preference Alignment, February 2025. arXiv:2502.02061 [cs]. [17] Jieyong Kim, Hyunseo Kim, Hyunjin Cho, SeongKu Kang, Buru Chang, Jinyoung Yeo, and Dongha Lee. Reviewdriven Personalized Preference Reasoning with Large Language Models for Recommendation, December 2024. arXiv:2408.06276 [cs]. [18] Yunjia Xi, Weiwen Liu, Jianghao Lin, Xiaoling Cai, Hong Zhu, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models, December 2023. arXiv:2306.10933. [19] Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong Xu, Tong Xu, and Enhong Chen. Large Language Model based Long-tail Query Rewriting in Taobao Search, March 2024. arXiv:2311.03758 [cs]. [20] Jiacheng Lin, Tian Wang, and Kun Qian. Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning, March 2025. arXiv:2503.24289 [cs] version: 1. [21] Alicia Y. Tsai, Adam Kraft, Long Jin, Chenwei Cai, Anahita Hosseini, Taibai Xu, Zemin Zhang, Lichan Hong, Ed H. Chi, and Xinyang Yi. Leveraging LLM Reasoning Enhances Personalized Recommender Systems, July 2024. arXiv:2408.00802. [22] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, April 2024. arXiv:2402.03300 [cs]. [23] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms, August 2017. arXiv:1707.06347 [cs]. [24] Yafu Li, Xuyang Hu, Xiaoye Qu, Linjie Li, and Yu Cheng. Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback, January 2025. arXiv:2501.12895 [cs]. [25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly Reward Model, July 2024. arXiv:2305.18290. [26] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs, February 2024. arXiv:2402.14740 [cs]. [27] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [28] Jian Jia, Yipei Wang, Yan Li, Honggang Chen, Xuehan Bai, Zhaocheng Liu, Jian Liang, Quan Chen, Han Li, Peng Jiang, and Kun Gai. LEARN: Knowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application, December 2024. arXiv:2405.03988 [cs]. [29] Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, and Tat-Seng Chua. On Softmax Direct Preference Optimization for Recommendation, November 2024. arXiv:2406.09215 [cs]. [30] Junyi Chen, Lu Chi, Bingyue Peng, and Zehuan Yuan. HLLM: Enhancing Sequential Recommendations via Hierarchical Large Language Models for Item and User Modeling, September 2024. arXiv:2409.12740 [cs]. [31] Hongke Zhao, Songming Zheng, Likang Wu, Bowen Yu, and Jing Wang. LANE: Logic Alignment of Non-tuning Large Language Models and Online Recommendation Systems for Explainable Reason Generation, July 2024. arXiv:2407.02833 [cs]. [32] Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, and Xiao Wang. [SLIM] Can Small Language Models be Good Reasoners for Sequential Recommendation?, March 2024. arXiv:2403.04260 [cs]. [33] Bowen Zheng, Zihan Lin, Enze Liu, Chen Yang, Enyang Bai, Cheng Ling, Wayne Xin Zhao, and Ji-Rong Wen. Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation, March 2024. arXiv:2403.13574 [cs] version: 1. [34] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards Large Reasoning Models: Survey of Reinforced Reasoning with Large Language Models, January 2025. arXiv:2501.09686 [cs]. [35] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning, February 2025. arXiv:2502.14768 [cs]. [36] Wenjie Wang, Honghui Bao, Xinyu Lin, Jizhi Zhang, Yongqi Li, Fuli Feng, See-Kiong Ng, and Tat-Seng Chua. Learnable Item Tokenization for Generative Recommendation, August 2024. arXiv:2405.07314. [37] Yijie Ding, Yupeng Hou, Jiacheng Li, and Julian McAuley. Inductive Generative Recommendation via Retrievalbased Speculation, October 2024. arXiv:2410.02939. [38] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. Session-based Recommendation with Graph Neural Networks, January 2019. arXiv:1811.00855. [39] Jiaxi Tang and Ke Wang. Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding, September 2018. arXiv:1809.07426 [cs]. [40] Wang-Cheng Kang and Julian McAuley. Self-Attentive Sequential Recommendation, August 2018. arXiv:1808.09781 [cs]."
        },
        {
            "title": "A Dataset",
            "content": "Following the temporal-truncation protocol introduced by [3, 2], we construct three subsetsCDs and Vinyl (CDs), Video Games (Games), and Musical Instruments (Instruments) from the lastest public Amazon review dataset2 spanning from May 1996 to October 2023. For each domain, we begin with the most recent year of interactions (October 2022 October 2023) and, if the resulting number of valid items is insufficient, iteratively roll the time window backward month-by-month until 10k items are obtained. We omit the 5-core filter so as to retain the nature behaviour characteristic of recommendation scenarios. Each users interaction history is then chronologically sorted and truncated to the latest 20 actions, yielding fixed-length sequences for all subsequent modelling and evaluation. Finally the dataset is split with 80% training 10% validation 10% test. The resulting statistics are listed in Table 3. 2https://amazon-reviews-2023.github.io/index.html."
        },
        {
            "title": "Users",
            "content": "29,230 7,701 15,656 Table 3: Dataset Statistics. Items Density"
        },
        {
            "title": "Interactions",
            "content": "10,144 12,024 10,320 0.031% 0.023% 0.031% 63,502 13,435 34,"
        },
        {
            "title": "Test",
            "content": "50,801 10,748 27,498 6,350 1,343 3,437 6,351 1,344 3,"
        },
        {
            "title": "B Baselines",
            "content": "GRU4Rec [38] utilizes the GRU (Gated Recurrent Unit) architecture to model sequences. Caser [39] employs both horizontal and vertical convolutional operations to enhance the capture of high-order interactions within item sequences, improving recommendation accuracy. SASRec[40] incorporates multi-head self-attention mechanism in its self-attentive sequential recommendation model, facilitating the modeling of intricate sequential data patterns. TIGER [8] learns extra tokens from item features to present items, and then converts the user sequence into the sequence of the new item token for next-item generation. BigRec [2] utilizes the item title to present the user sequence for recommendation generation. D3 [3] proposed to address amplification bias and homogeneity issues in LLM recommenders that generate item title generation. LangPTune [15] utilized LLM as user profile generator, and proposed an end-to-end training pipeline that optimizes LLM-generated user profiles for the recommendation objective."
        },
        {
            "title": "C Implementation Details",
            "content": "Hardware: 4x NVIDIA 3090 (24GB) GPUs Framework: PyTorch 2.5.0, Transformers 4.48.1, DeepSpeed 0.15.7 Traditional Recommenders. We train all non-LLM baselines with the Cross-Entropy loss and the AdamW optimiser (learning-rate 1 104); the batch size is 256. TIGER [8] adopts T5 as its backbone without further architectural changes. LLM-based Methods. For every LLM variant including the contrastive-learning baseline (CL, Section 5.3) we finetune Gemma2-2B-Instruct and Qwen2.5-3B-Instruct. Models are adapted with LoRA (rank = 4) using DeepSpeed Stage 2 and the quantized PagedAdamW optimizer. Training lasts for at most three epochs with early stopping (patience = 1); the maximum generation length is 512 tokens. If public implementations exist, we keep the original hyper-parameters; otherwise, grid search over learning rates {5 104, 1 104} is performed. R2ec Optimization. The learning rate is set as 1e5 with batch size of 24, and apply linear warm-up for the first 32 steps. For trajectory generation, we set the group size as four, utilized VLLM3 (tensor-parallelism = 1, target GPU utilisation = 95 %) for efficient generation. VLLM reserves one GPU for inference and leaves three for training. Sampling uses temperature = 1.5 and top-K = 200. Policy optimization follows the clipping ratio ϵ = 0.2 and omits KL regularization. Rewards are computed with NDCG@1000 and β = 0.05. Unless stated otherwise, these settings are used throughout."
        },
        {
            "title": "Training",
            "content": "To instantiate R2ec, given the dataset D, the initial policy πθ, and item embedding cache HV , we train the model with detailed training process illustrated in Algorithm 1. 3https://docs.vllm.ai/en/latest/ 12 if step % Trefresh == 0 then Refresh item embedding: HV [v] fθ(xv), Algorithm 1 Training Process Input: Dataset D, initial policy πθ, embedding function fθ, item embedding table HV Output: Optimized policy model πθ 1: for step = 1 to do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for Generate trajectory: {[o1, v+]..., [oG, v+]} πθold (xu) Compute reward for each trajectory using Eq. (5) Compute advantage for each trajectory using Eq. (2) end for Update policy parameters θ via loss in Eq. (8) Update old policy: θold θ end if Sample training batch = {(u, v+)} Encode target item prompts and update embedding table: HV [v+] fθ(xv+) (u, v+) for all (u, v+) in do V"
        },
        {
            "title": "Inference",
            "content": "For inference, we first pass every item prompt xv through the trained model once to obtain all item embeddings HV . At inference time, the model greedily generates deterministic reasoning for user prompt xu, the last hidden state hT goes through the embedding table, scoring each candidate via the inner product s(v) = HV [v]. The top-K items can then be recommended by sorting these scores."
        },
        {
            "title": "User Prompt",
            "content": "Analyze in depth and finally recommend next {category} might purchase inside <answer> and </answer> . For example, <answer> product </answer> . Below is my historical {category} purchases and ratings (out of 5): {% for hist in purchase_histories %} {% {hist.time_delta} ago: [{hist.item_title}] ({hist.rating}) %}"
        },
        {
            "title": "Item Prompt",
            "content": "Summarize key attributes of the following {category} inside <answer> and </answer> : {% for key, attr in item.meta %} {% {key}: {attr} %} Figure 4: Prompt templates for user interaction history and item metadata. The User Prompt encodes users past purchases as sequence of item titles, relative timestamps (e.g., 2hrs, 4d), and explicit ratings (in [1, 5]), followed by an instruction to analyze and recommend the next item within the span of </answer> and </answer> . The Item Prompt summarizes structured item attributes (e.g., brand, type, features) with the same format requirement. As illustrated in Figure 4, user interaction histories and item metadata are serialized into token sequences via the given prompt templates. To signal the boundary between reasoning and recommendation, we introduce control symbol <answer> , which triggers shift from language modeling head to recommendation head."
        },
        {
            "title": "F Limitations",
            "content": "Our work acknowledges two primary limitations. First, introducing explicit reasoning generation inevitably increases inference latency and reduces efficiency due to additional autoregressive decoding steps; nevertheless, exploring the potential of reasoning within recommendation is valuable, and our experiments have empirically confirmed its 13 Table 4: Average per-request inference latency (mean) of our unified model R2ec versus the two-stage LangPtune pipeline, measured across varying batch sizes. batch size R2ec"
        },
        {
            "title": "LangPTune",
            "content": "1 4 8 16725.54 2636.84 3980.77 77.13 2220.88 32.00 19030.95 2753.81 4625.045 67.00 2375.64 69.476723 effectiveness in improving recommendation accuracy. Second, constrained by computational resources, we employed parameter-efficient tuning (LoRA) rather than full-parameter fine-tuning, thus not fully demonstrating the potentially superior performance achievable through comprehensive optimization."
        },
        {
            "title": "G Analysis on Inference Efficiency",
            "content": "To quantify the efficiency gains of our unified model R2ec over the two-stage conventional pipeline, we benchmark its inference latency against LangPtune [15]. Inferences were performed on one NVIDIA RTX 3090 GPU using identical input prompts, maximum token lengths, and candidate sets. We executed 100 queries at batch sizes of 1, 4, and 8. For each batch, per-request latency was calculated by dividing the total duration by the batch size. The mean standard deviation for each configuration is reported in Table 4. R2ec achieves approximately 10% reduction in average per-request inference time, demonstrating the superior efficiency of our model architecture compared to the conventional two-stage approach. Notably, the efficiency gap narrows as batch size increases, since parallelization spreads computation costs over more requests. In addition, the low standard deviations demonstrate that R2ecs inference speed is reliably stable."
        },
        {
            "title": "H Analysis on Case Study",
            "content": "To illustrate domain-specific and backbone-specific reasoning behaviors, we analyze representative cases from the CDs and Video Games datasets (Figures 5,6,7,8) using Gemma and Qwen as backbones. 1) On the CDs dataset, both models tend to reason by discussing several potentially interesting items and explicitly enumerating their features and genres, before providing final recommendation. This behavior highlights the models ability to leverage domain knowledge acquired during pretraining and reinforced through RecPO, for better recommendation. 2) On the Video Games dataset, the models shift towards inferring user intent from the purchase sequence, speculating on the likely category or intended use of the next purchase. This demonstrates shift from feature-based enumeration to intent-driven reasoning as the domain changes. 3) Notably, Gemma generally produces shorter and more concise reasoning sequences compared to Qwen, which often generates more elaborate justifications. These patterns indicate that, while both models benefit from explicit reasoning, their generation style and reliance on pretrained knowledge can manifest differently across domains and backbone choices. 14 Case Study 1 (CD and Vinyl)"
        },
        {
            "title": "User",
            "content": "Analyze in depth and finally recommend next CD or vinyl might purchase inside <answer>and </answer>. For example, <answer>a product</answer>. Below is my historical CDs and vinyl purchases and ratings (out of 5): 255d 13.2h ago: [Pure 60s: The #1 Hits] (5.0) 255d 12.6h ago: [Seven] (4.0) 26.5min ago: [Comeblack] (4.0) 18.9min ago: [Hot Rocks 1964-1971] (5.0) 16.3min ago: [Return to Forever] (4.0) 9.4min ago: [Rock Believer] (5.0) 6.3min ago: [From Small Things: The Best of Dave Edmunds] (3.0) R2ec (Gemma2-2B) Based on your past purchases, it seems you enjoy mix of classic rock, soul/funk, and touch of progressive rock. You appreciate well-crafted albums with strong vocals and focus on musicality. Here are some recommendations, keeping in mind your taste: CD/Vinyl: The Very Best of The Rolling Stones (CD/Vinyl): Youve enjoyed classic rock, and this compilation offers great introduction to the Stones iconic sound. It features their biggest hits and showcases their bluesy roots and rock roll energy. The Dark Side of the Moon (Vinyl): This Pink Floyd masterpiece is classic for reason. Its complex and innovative album with timeless sound. The vinyl format adds special touch to the listening experience. Kind of Blue (Vinyl): Miles Daviss iconic jazz album is must-have for any music lover. Its beautiful and introspective album that showcases Daviss genius as musician. The vinyl format brings out the nuances of the music. The Miseducation of Lauryn Hill (CD/Vinyl): This album is powerful blend of hip-hop, soul, and R&B. Lauryn Hills incredible vocals and the albums socially conscious lyrics make it truly special listening experience. Why these recommendations: Variety: These albums offer mix of genres and styles, reflecting your diverse taste. Quality: They are all critically acclaimed and considered essential listening. Format: The vinyl format adds unique and immersive experience, while the CD format offers convenience and portability. Ultimately, the best way to find your next favorite album is to explore and discover new music. recommend<answer> Figure 5: Case Study on CDs with Gemma as backbone. 15 Case Study 2 (CD and Vinyl)"
        },
        {
            "title": "User",
            "content": "Analyze in depth and finally recommend next CD or vinyl might purchase inside <answer>and </answer>. For example, <answer>a product</answer>. Below is my historical CDs and vinyl purchases and ratings (out of 5): 97d 7.7h ago: [Live At The El Mocambo[2 CD]] (5.0) 97d 7.5h ago: [Radio Silence] (5.0) 2d 16.5h ago: [Little Box Snakes The Sunburst Years 1978-1982] (5.0) 2d 16.0h ago: [Goats Head Soup Super Deluxe] (5.0) R2ec (Qwen2.5-3B) Given your history of purchasing and rating albums, it seems you have strong preference for live recordings, classic rock, and deluxe editions. Based on this, here are some recommendations for CDs or vinyls that align with your tastes: 1. The Wall (Deluxe Edition) Artist: Pink Floyd Format: CD or Vinyl Description: This is legendary album that has been reissued multiple times, including deluxe edition that includes rare tracks, documentaries, and bonus material. Its must-have for any Pink Floyd fan. Rating: Given your appreciation for deluxe editions and classic rock, this would likely receive high rating. 2. The Dark Side of the Moon (Super Deluxe Edition) Artist: Pink Floyd Format: CD or Vinyl Description: Another iconic album that has been extensively reissued. The super deluxe edition includes wealth of bonus content, making it comprehensive collection for fans. Rating: Similar to Goats Head Soup, this would also likely receive high rating due to its extensive content and your appreciation for deluxe editions. 3. Live at the Royal Albert Hall by David Bowie Artist: David Bowie Format: CD or Vinyl Description: This live album captures one of David Bowies most famous performances, showcasing his versatility and the energy of his live shows. Its great addition to your collection if you enjoy live recordings. Rating: Given your enjoyment of live recordings, this would likely be highly rated. 4. The Complete Studio Albums by Fleetwood Mac Artist: Fleetwood Mac Format: CD or Vinyl Description: This box set includes all of Fleetwood Macs studio albums, providing comprehensive overview of their career. Its great way to explore their evolution as band. Rating: If youre fan of Fleetwood Mac, this would likely be highly rated due to the quality of their music and the comprehensive nature of the collection. 5. The Best of the Beatles (Deluxe Edition) Artist: The Beatles Format: CD or Vinyl Description: compilation that includes some of the Beatles most popular songs, arranged in chronological order. The deluxe edition often includes additional content like rare recordings and documentaries. Rating: Given your appreciation for deluxe editions and classic rock, this would likely receive high rating. Conclusion: Based on your past purchases and ratings, you might want to consider The Wall (Deluxe Edition), The Dark Side of the Moon (Super Deluxe Edition), Live at the Royal Albert Hall by David Bowie, The Complete Studio Albums by Fleetwood Mac, or The Best of the Beatles (Deluxe Edition). Each of these options offers unique and comprehensive look into the artists work, which aligns with your tastes. Figure 6: Case Study on CDs with Qwen as backbone. Case Study 3 (Video Games)"
        },
        {
            "title": "User",
            "content": "Analyze in depth and finally recommend next video game might purchase inside <answer>and </answer>. For example, <answer>a product</answer>. Below is my historical video games purchases and ratings (out of 5): 80d 1.6h ago: [For Apple Watch Fast Charger USB C, 2 in 1 Apple MFi Certified 3FT iWatch Charger Magnetic Charging Cable &PD USBC Fast Wall Charger Power Adapter for Apple Watch Series 6/SE/5/4/3/2/1&iPhone 14/13/12] (5.0) 76d 7.5h ago: [Kickstand Back Shell Holder Back Bracket for Nintendo Switch Console, Switch Replacement Kickstand Repair Kit with Screwdriver] (4.0) 45d 12.5h ago: [VOYEE Switch Controllers Wireless, Compatible with Switch Pro Controllers for Switch/Switch Lite/OLED/PC, One-Key Pairing Gamepad with 9-Color RGB LEDs 800mAh Battery 6-Axis Motion Wake Up Function] (5.0) 40d 23.5h ago: [Gvyugke 2.4GHz Wireless Gaming Headset for PS4, PS5, PC, Nintendo Switch, Bluetooth 5.2 Gaming Headphones with Mic for Mobile Device, Noise Canceling, Bass Surround, 50mm Driver, 40H Battery] (5.0) It seems you enjoy tech accessories and gaming peripherals! Based on your recent purchases, you likely appreciate: Functionality: Youre drawn to products that enhance your gaming experience, like the charging cable and the wireless gaming headset. Convenience: You value products that simplify your life, like the kickstand for your Switch. Quality:You prioritize products that are well-made and reliable, evidenced by your high ratings for the charging cable and the gaming headset. Considering these preferences, here are some recommendations for your next video game purchase: <answer> R2ec (Gemma2-2B) Figure 7: Case Study on Games with Gemma as backbone. 17 Case Study 4 (Video Games)"
        },
        {
            "title": "User",
            "content": "Analyze in depth and finally recommend next video game might purchase inside <answer>and </answer>. For example, <answer>a product</answer>. Below is my historical video games purchases and ratings (out of 5): 79d 15.5h ago: [BUSQUEDA Pro Head Strap with Battery for Quest 2, Magnetic Connection 6000mAh Replaceable VR Power, Lightweight & Balance Elite Strap Meta Accessories for Enhanced Support and Comfort in VR] (5.0) 69d 14.2h ago: [iTwano Stainless VR Stand for Quest 2 Pro, Desk Mount VR Holder and Headset Stand Compatible with Quest Pro 2 Rift S] (5.0) 59d 18.9h ago: [ROCCAT Vulcan II Max Optical-Mechanical PC Gaming Keyboard with Customizable RGB Illuminated Keys and Palm Rest, Titan II Smooth Linear Switches, Aluminum Plate, 100M Keystroke Durability Black] (5.0) 57d 21.9h ago: [ZIYOU LANG K3 Mechanical Gaming Keyboard 98 Keys with Number pad Chroma RGB Illuminated Water-Resistant Anti-Ghosting NKRO Retro Metal Panel Type-C for PC/Mac Gamer Typist Optical Clicky Blue Switch] (5.0) 57d 21.8h ago: [Corsair K60 PRO TKL Wired Optical-Mechanical OPX Linear Switch Gaming Keyboard with 8000Hz Polling Rate - Black] (4.0) 52d 16.8h ago: [IROK FE87/104 RGB Mechanical Keyboard, Hot Swappable Gaming Keyboard, Customizable Backlit, Magnet Upper Cover Type-C Wired Keyboard for Mac Windows-White/Brown Switch] (5.0) R2ec (Qwen2.5-3B) Given your purchase history, it is clear that your interests focus primarily on VR setup accessories, gaming peripherals (particularly high-quality keyboards), and devices that enhance comfort and customizability during play, rather than on specific video games themselves. Based on the trends observed in your history: VR Content: Multiple purchases of Quest 2/Pro accessories indicate strong inclination towards immersive VR experiences. Comfort & Ergonomics: Investment in premium straps and ergonomic keyboards suggests that gaming comfort is high priority. Customizability and RGB Lighting: The frequent selection of customizable RGB peripherals demonstrates your preference for visually dynamic and personalized setups. With these points from your purchases, lets recommend one of the following types of next-gen games featuring improved immersive experiences for VR: <answer> Figure 8: Case Study on Games with Qwen as backbone."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology (Shenzhen)",
        "National University of Singapore",
        "The Hong Kong Polytechnic University",
        "University of Science and Technology of China"
    ]
}