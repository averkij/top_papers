{
    "paper_title": "A Case Study of Web App Coding with OpenAI Reasoning Models",
    "authors": [
        "Yi Cui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents a case study of coding tasks by the latest reasoning models of OpenAI, i.e. o1-preview and o1-mini, in comparison with other frontier models. The o1 models deliver SOTA results for WebApp1K, a single-task benchmark. To this end, we introduce WebApp1K-Duo, a harder benchmark doubling number of tasks and test cases. The new benchmark causes the o1 model performances to decline significantly, falling behind Claude 3.5. Moreover, they consistently fail when confronted with atypical yet correct test cases, a trap non-reasoning models occasionally avoid. We hypothesize that the performance variability is due to instruction comprehension. Specifically, the reasoning mechanism boosts performance when all expectations are captured, meanwhile exacerbates errors when key expectations are missed, potentially impacted by input lengths. As such, we argue that the coding success of reasoning models hinges on the top-notch base model and SFT to ensure meticulous adherence to instructions."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 1 ] . [ 1 3 7 7 3 1 . 9 0 4 2 : r a"
        },
        {
            "title": "A Case Study of Web App Coding with OpenAI\nReasoning Models",
            "content": "Yi Cui ONEKQ Lab yi@onekq.ai"
        },
        {
            "title": "Abstract",
            "content": "This paper presents case study of coding tasks by the latest reasoning models of OpenAI, i.e. o1-preview and o1-mini, in comparison with other frontier models. The o1 models deliver SOTA results for WebApp1K, single-task benchmark. To this end, we introduce WebApp1K-Duo, harder benchmark doubling number of tasks and test cases. The new benchmark causes the o1 model performances to decline signiﬁcantly, falling behind Claude 3.5. Moreover, they consistently fail when confronted with atypical yet correct test cases, trap non-reasoning models occasionally avoid. We hypothesize that the performance variability is due to instruction comprehension. Speciﬁcally, the reasoning mechanism boosts performance when all expectations are captured, meanwhile exacerbates errors when key expectations are missed, potentially impacted by input lengths. As such, we argue that the coding success of reasoning models hinges on the top-notch base model and SFT to ensure meticulous adherence to instructions."
        },
        {
            "title": "1 Introduction",
            "content": "The recent release of OpenAI reasoning models (o1-preview and o1-mini)[OpenAI, 2024] presents groundbreaking direction for model development, along with their SOTA performance in several challenging benchmarks, including math[Zhang et al., 2023], scientiﬁc research[Rein et al., 2023], competitive programming[Mirzayanov, 2009]. In this report, we evaluate o1 models in the context of practical software development, i.e. when models are required to implement simple web apps satisfying speciﬁc requirement[Cui, 2024b]. Our benchmarks have the following characteristics and challenges. The problem is less explorational and more results-oriented than other benchmarks. The speciﬁc instructions are laid out in the form of test setup and expectations. No external knowledge is required to complete the task, since React is prominent framwork with sufﬁcient code circulating on Internet for decade. Some expectations are less explicit or less typical than others, which could cause model negligence or misunderstanding. We use single-task benchmark (WebApp1K) and duo-task benchmark (WebApp1K-Duo), and ﬁnd the models perform with vast variability. Under the single-task evaluation, o1 models achieve new SOTA and unlock challenges never solved by non-reasoning frontier models. But under the duo-task evaluation, o1 models perform worse than Claude 3.5, and consistently fail under speciﬁc test format. We attempt to gain insights into o1 behaviors by deep diving into few problems they succeed or fail at. We ﬁnd the reasoning steps play critical role in both success and failure. Since reasoning tokens are invisible in OpenAI API, we share reasoning steps obtained from ChatGPT reeactment, i.e. feeding the identical prompt to ChatGPT. To minimize benchmark contamination, we only share test cases details, but do not reveal verbatim answers, only illustrate them in broad strokes. The artifacts are on GitHub and Huggingface: single-task benchmark[ONEKQ, 2024a], dual-task benchmark[ONEKQ, 2024c], and the leaderboard[ONEKQ, 2024b]. The rest of this report is organized as follows. Sec. 2 presents results of single-task benchmark and how o1 models solve two hard problems. Sec. 3 presents results of duo-task benchmark and how o1 models suffer in two testing scenarios. Sec. 4 discusses related works. Sec. 5 concludes and shares departing thoughts."
        },
        {
            "title": "2 Single-Task Benchmark",
            "content": "We start with model performances on the WebApp1K benchmark. As illustrated in Tab. 1, each challenge of the benchmark focuses on single task described by two test cases, one success and one failure. The task is about completing an atomic action (e.g. submitting form, retrieving all posts), involving user interactions and access to mocked API. More details of the benchmark can be found at [Cui, 2024b]. ... import TaskA from ./TaskA; ... import TaskA from ./TaskA; test(\"Success at task A\", async () => { test(\"Failure at task A\", async () => { ... render( ... render( <MemoryRouter><TaskA /></MemoryRouter> <MemoryRouter><TaskA /></MemoryRouter> ); ... }, 10000); ); ... }, 10000); (a) Success Case for Task (b) Failure Case for Task Table 1: Illustration of WebApp1K Test Cases The prompt is straightforward: we feed test ﬁles to the model, expecting it to generate code passing these tests. Generate TaskA.js to pass the tests below: {T ab. 1(a)}{T ab. 1(b)}. RETURN CODE ONLY. (1) The resulting lines of code is typically between 40 and 50."
        },
        {
            "title": "2.1 Results",
            "content": "Due to budget constraints, we only obtained pass@1 results for the o1 models. Nevertheless, as shown in Tab. 2, they demonstrate impressive performance, lifting SOTA by 7%. Model o1-preview o1-mini gpt-4o-2024-08-06 claude-3.5-sonnet deepseek-v2.5 mistral-large-2 pass@1 0.952 0.939 0.885 0.881 0.834 0.780 Table 2: WebApp1K: pass@1 Results for Selected Models As part of this achievement, the two o1 models unlock total of 16 challenges never solved by previous non-reasoning models. Next, we pick two examples to illustrate how reasoning models solve them."
        },
        {
            "title": "2.2 Example One: Placeholder Text",
            "content": "The ﬁrst example is the postEditing problem under the Social Media category. In Tab. 3, we list the key steps to build up expectations of this problem. In particular, we highlight the step non-reasoning models overlooked. test(Test updating an existing post., async () => fetchMock.post(\"/api/posts/1\", 200); ... fireEvent.change(screen.getByText(Edit), target: value: New content ); ... fireEvent.click(screen.getByText(Save)); ... expect(fetchMock.calls(\"/api/comments\").length).toBe(1); expect(screen.getByText(/Comment added successfully/i)).toBeInTheDocument(); , 10000); Table 3: postEditing Problem First, the etchM ock statement sets up mocked API. Then, ireEvent statements simulate user actions in two events: state change (value insertion) to an UI element carrying an Edit string, followed by click event to an UI element carrying Save string. Finally, expect statements outline the expectations that the mocked API must be accessed exactly once, and the success response from the API must be present in the webpage. For this problem, most non-reasoning models capture the semantics and deliver functioning code. Speciﬁcally, to support user actions, they implement form element for user input, and save button for the click event. However, they forget to explicitly attach the Edit string to the form element, without which ireEvent cannot locate the correct element in the test webpage. There are two possible causes for the failure. First, the Edit token is synonymous with the purpose of the form element, which is also to edit. Second, the popular in-place editing implementation (prevelant in pretraining dataset) does not require an Edit string to state the purpose of the form element, which is overkill. On the other hand, the o1 models stick to the requirement by attaching Edit to the form element as placeholder text, via textarea attribute (ref or value). Below is the ChatGPT reasoning chain, in which steps speciﬁcally reasoning Edit is blackened. Reﬁning test details Investigating the scripts Considering functionality Designing the component Editing content Reﬁning selector logic Constructing solution Setting up the interface Mapping out the test Trying another way Rendering editable text Implementing the functionality Mapping out test solutions Revisiting test strategies Weighing options Evaluating event handling Mulling over implementation Mapping the component Testing with different methods Formulating solution Managing content updates Weighing options Creating the component"
        },
        {
            "title": "2.3 Example Two: Frontend Validation vs Backend Validation",
            "content": "The second example is the ticketSubmission problem under the Customer Support category. Tab. 4, lists the key steps of the test setup and expectations. We blacken the step which trapped nonreasoning models. Similar to the same sequence in Tab. 3, the mocked API is ﬁrst setup, followed by simulated user action, then expectations on API access and error message. Again, non-reasoning models understand the semantics, write functioning code, but fail expectations. The root cause here is the string Title is required, which is akin to technique not requiring API access, aka frontend validation. As best practice (hence prevelance in pretraining dataset), frontend 3 test(shows error when submitting ticket with missing fields, async () => fetchMock.post(/api/tickets, status: 400 ); ... fireEvent.click(screen.getByText(Submit)); ... expect(fetchMock.calls(/api/tickets).length).toBe(1); expect(screen.getByText(Title is required)).toBeInTheDocument(); , 10000); Table 4: ticketSubmission Problem valiation is lightweight and fast, therefore preferred over backend validation. As such, all nonreasoning models are misled to implement frontend validation instead of expected behaviors which is backend validation. On the other hand, o1 models discover the unpopular yet correct implementation: unconditionally visit the API, and output the Title is required error message upon 400 response. Below is the ChatGPT reasoning chain, in which steps reasoning the 400 response is blackened. Mapping out the component Setting up event handlers Setting up the form Writing test cases Reﬁning the approach Reﬁning error handling Adjusting error handling Adjusting code logic Updating JavaScript code The most crucial step here is Reﬁning the approach. Below is its detailed wording. Im updating the code to ensure fetch request is always sent, even without title. The server will respond with 400 status if the title is absent. Evidently, the step before it (Writing test cases) conducted certain veriﬁcation, which leads the model to pivot to the right path."
        },
        {
            "title": "2.3.1 Counter Example",
            "content": "Unfortunatelly the reasoning models can also fall for the same trap. Below is ChatGPT reasoning chain leading o1-preview to the faulty implementation like previous models. Mapping out test strategy Setting up the test Customer service improvement Setting up for data Setting up the form Verifying form submission SHOWING ERRORS Reﬁning the form handling On closer look, step Customer service improvement derails the model from backend validation to frontend validation. Im thinking about creating TicketSubmission component with Title input and Submit button. Submitting the form will trigger POST request to /api/tickets, validating the Title ﬁeld before submission. More interestingly, the step Verifying form submission does not correct the wrong direction, but solidify it. Im thinking about how the form ensures Title must be ﬁlled. It sends POST request if Title is entered, showing success or Title is required based on the response status. With these superﬁcial clues, we speculate that the derailing is due to preemption of original expectations by models inherent knowledge. The subsequent veriﬁcation step is derived from neighboring steps already derailed, instead of orginal expectations only accessible from the input tokens."
        },
        {
            "title": "3 Duo-Task Benchmark",
            "content": "In light of o1 models superb performance to saturate the single-task benchmark, we propose WebApp1K-Duo[ONEKQ, 2024c], more difﬁcult benchmark. Under each category of WebApp1K, we randomly pair up two atomic tasks into duo task. The benchmark still consists of 1000 tasks, with 50 for each category. Models are challenged on both longer input, i.e. twice as many test cases, and longer output, i.e. more implementation in one module to meet all expectations. ... import TaskA from ./TaskA_B; import TaskB from ./TaskA_B; ... ... import App from ./TaskA_B; test(\"Success at task A\", async () => test(\"Success at task A\", async () => ... render( ... render( <MemoryRouter><TaskA /></MemoryRouter> <MemoryRouter><App /></MemoryRouter> ); ... , 10000); ); ... , 10000); test(\"Failure at task A\", async () => test(\"Failure at task A\", async () => ... render( ... render( <MemoryRouter><TaskA /></MemoryRouter> <MemoryRouter><App /></MemoryRouter> ); ... , 10000); ); ... , 10000); test(\"Success at task B\", async () => test(\"Success at task B\", async () => ... render( ... render( <MemoryRouter><TaskB /></MemoryRouter> <MemoryRouter><App /></MemoryRouter> ); ... , 10000); ); ... , 10000); test(\"Failure at task B\", async () => test(\"Failure at task B\", async () => ... render( ... render( <MemoryRouter><TaskB /></MemoryRouter> <MemoryRouter><App /></MemoryRouter> ); ... , 10000); ); ... , 10000); (a) Raw Format (b) Normalized Format Table 5: Illustration of WebApp1K-Duo Test Cases WebApp1K-Duo is composed in two ways. The ﬁrst way is shown in Tab. 5 (a), in which the original export name of WebApp1K is preserved as is. The second way is shown in Tab. 5 (b), where the export names are normalized to uniﬁed name App."
        },
        {
            "title": "3.1 Results",
            "content": "We collect pass@1 results under both raw and normalized formats. Unfortunately, o1 models performances on the new benchmark are not impressive, falling behind other frontier models, especially Claude 3.5. As shown in Tab. 6, all models struggle with the raw format (Tab. 5 (a)). Most strikingly, o1 models fail all problems. We will try to ﬁnd the root cause in Sec. 3.2. In Tab. 7, performance of all models are greatly improved under the intuitive normalized format (Tab. 5 (a)). The SOTA is owned by Claude 3.5."
        },
        {
            "title": "3.2 Example One: Default Export vs Named Export",
            "content": "In the raw format illustrated in Tab. 5 (a), there are two imports of different names, i.e. TaskA and TaskB. But they are actually default imports (without curly braces) which are name-agnostic. Also 5 Model claude-3-5-sonnet chatgpt-4o-latest deepseek-v2.5 mistral-large-2 o1-mini o1-preview pass@1 0.32 0.026 0.02 0.02 0 0 Table 6: WebApp1K-Duo Raw Format: pass@1 Results for Selected Models Model claude-3-5-sonnet o1-mini o1-preview chatgpt-4o-latest deepseek-v2.5 mistral-largepass@1 0.679 0.667 0.652 0.531 0.49 0.449 Table 7: WebApp1K-Duo Normalized Format: pass@1 Results for Selected Models since only one default export is allowed per module, this format is in fact semantically equivalent to the normalized format in Tab. 5 (b). Both formats demand the models to build single module implementing all expectations, with single default export. To help readers understand related concepts, we explain JavaScript export rules in Tab. 8."
        },
        {
            "title": "Import Syntax",
            "content": "Named Exports Export multiple items from module export const = ...; export function y() {...} import { x, } from ./module; Required during import"
        },
        {
            "title": "Multiplicity\nUse Case\nExport Location Anywhere in the module",
            "content": "(can use as to rename) Multiple named exports per module Utility functions, constants, classes Default Export Export single item from module export default ...; import anyName from ./module; Not required during import Can be imported with any name"
        },
        {
            "title": "Only one default export per module\nMain functionality of a module\nBottom or after the main logic",
            "content": "Table 8: Illustration of JavaScript Default Export in Comparison to Named Imports Tab. 9 collects different ways models cope with this challenge. Tab. 9 (d) is the only right answer, but also the least straightforward, challenging the intuition trap that two exports from two separate modules are needed. Both non-reasoning and reasoning models fall for the trap and attempt to split the implementation into two modules, (Tab. 9 (a), (b), (c)), resulting in very high failure rates. Next, we try to understand why non-reasoning models occasionally succeed by following the pattern of Tab. 9 (d), but non-reasoning models never do so. We suspect that the normalized format (Tab. 5 (b)) deﬁnitely dominates the pretraining/posttraining dataset, but does not exclude the raw format (Tab. 5 (a)), as well as the matching solutions. This makes the success possible. On the other hand, from the ﬁrst reasoning step which often plays the role of planning, reasoning models commit to the wrong judgment, and do not get chance to correct the course in subsequent steps. Below is the detailed wording of the ﬁrst reasoning step from ChatGPT reeactment. To progress, the key task is creating components TaskA and TaskB in TaskA_B.js to ensure all tests are successfully passed. Comparing to the mistakes made in Sec. 2.3.1, the mistake in the above step covers larger scope. It is reasonable to argue that mistakes made in large-scoped steps are more fatal and harder to correct. 6 function TaskA() { // Implementation of TaskA } function TaskB() { // Implementation of TaskB } export default TaskA; export { TaskB }; function TaskA() { // Implementation of TaskA } function TaskB() { // Implementation of TaskB } export { TaskA, TaskB }; (a) One Default Export and One Named Export (b) Two Named Exports function TaskA_or_B() { function TaskA_or_B() { // Implementation of TaskA or TaskB // Implementation of both TaskA and TaskB } } export default TaskA_or_B; export default TaskA_or_B; (c) Only One Task is Implemented and Exported (d) Two Tasks Jointly Implemented and Exported Table 9: Patterns to Address the WebApp1K-Duo Raw Format (Tab. 5 (a))"
        },
        {
            "title": "3.3 Example Two: Ignored Expectation",
            "content": "We now try to study why o1 models perform worse than Claude 3.5 under the normazlied format. Tab. 10 shows problem solved by Claude 3.5, but failed by o1-preview. import App from ./addComment_retrieveAllBlogPosts; ... test(successfully adds comment to post, async () => fetchMock.post(/api/comments, 200); ... expect(fetchMock.calls(/api/comments).length).toBe(1); expect(screen.getByText(/Comment added successfully/i)).toBeInTheDocument(); , 10000); test(fails to add comment to post, async () => fetchMock.post(/api/comments, 500); ... expect(fetchMock.calls(/api/comments).length).toBe(1); expect(screen.getByText(/Failed to add comment/i)).toBeInTheDocument(); , 10000); test(Success: retrieve list of all blog posts, async () => fetchMock.get(/api/posts, status: 200, body: [ id: 1, title: First Post , id: 2, title: Second Post ] ); ... expect(fetchMock.calls()).toHaveLength(1); expect(screen.getByText(First Post)).toBeInTheDocument(); expect(screen.getByText(Second Post)).toBeInTheDocument(); , 10000); test(Failure: retrieve list of blog posts with server error, async () => fetchMock.get(/api/posts, status: 500, body: error: Internal Server Error ); ... expect(fetchMock.calls()).toHaveLength(1); expect(screen.getByText(Internal Server Error)).toBeInTheDocument(); , 10000); Table 10: addComment_retrieveAllBlogPosts Problem Here, o1-preview passes all tests but the last one. The output code neither attempt to catch the 500 error nor print out the Internal Server Error string. The reasoning chain is normal, and no step speciﬁcally mentions the need to catch internal server errors. Crafting the component Laying out the requirements Importing dependencies Breaking down the code Setting up the app Testing post functionality"
        },
        {
            "title": "Testing API integration",
            "content": "Also o1-previews inherent coding ability is solid, because it solves the retrieveAllBlogPosts problem when evaluated under the single-task benchmark. To this end, we suspect the root cause to be failure to pick up the expectation from input tokens, possibly due to length constraint. This mistake should be considered matter of instruction following, which is applicable to both non-reasoning and reasoning models."
        },
        {
            "title": "4 Related Works",
            "content": "The impressive achievements of reasoning models bulit on advancements from machine learning, reinforcement learning, and cognitive science. On the learning side, self-play ﬁne-tuning allows models to generate their own data and iteratively reﬁne their reasoning capabilities[Chen et al., 2024]. By engaging in self-play, models learn from successes and failures to convert weak performance into strong, well-aligned behavior[Zhang et al., 2024]. Self-taught reasoning methods use the models own outputs to enable bootstrapping process to improve future performance[Zelikman et al., 2022]. This is evident in the development of self-taught reasoners, where models analyze outcomes of their reasoning chains[Zelikman et al., 2024]. Reinforcement learning further augments this selfimprovement process by allowing models to optimize their decision-making strategies via interaction with the running environment[Silver et al., 2017]. On the inference side, chain-of-thought reasoning trains models to generate intermediate steps that mirror human-like thought processes[Wang and Zhou, 2024, Lightman et al., 2023]. Inductive reasoning and hypothesis search techniques enable models to explore space of possible outcomes, making it excel at abstract reasoning tasks[Wang et al., 2024]. Advanced sampling methods, like repeated sampling and tree search, enhance the models capacity to handle uncertainty[Anthony et al., 2017]. Together, these strategies provide robust framework for models to perform nuanced and sophisticated reasoning in wide variety of tasks[Uesato et al., 2022]. On the evaluation side, more benchmarks have been proposed to focus on problem-solving capabilities in near-real-world environments. SWE-bench[Jimenez et al., 2024] provides comprehensive suite targeting core software engineering activities such as code generation, completion, error detection, and debugging. BFCL[Yan et al., 2024] assesses models ability to generate accurate function calls, including prompt interpretation and argument handling. BIRD[Gao et al., 2023] evaluates models proﬁciency in translating natural language queries into SQL codes. The Aider Leaderboard[Aider, 2024] ranks models based on their performance in real-world programming tasks such as bug ﬁxing, refactoring, and code completion."
        },
        {
            "title": "5 Conclusions",
            "content": "This report studies the latest reasoning models by OpenAI in the context of writing code to speciﬁc test expectations. We see both exciting and discouraging results, and share our investigations to gain more insights, especially how reasoning inﬂuence the outcome. We further argue that OpenAIs top-notch base model and SFT are equally important to the success of reasoning models. We believe that further advancements in these existing directions will continue to enhance reasoning models performance, both amplifying strengths and mitigating weaknesses. Below are our thoughts on next steps. We think the current SOTA of the duo-task benchmark (Tab. 6) is good milestone for hill climbing. So we do not plan to add more test cases until the next signiﬁcant leap. We will look deeper into error logs. But it would be quite surprising if we discover new error patterns besides those already identiﬁed[Cui, 2024a]. We will incorporate more frameworks (e.g. Vue) and languages (e.g. Python) to increase the benchmark coverage."
        },
        {
            "title": "References",
            "content": "Aider. Aider llm leaderboards. https://aider.chat/docs/leaderboards/, 2024. 8 Thomas W. Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In Neural Information Processing Systems, 2017. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. ﬁne-tuning converts weak language models to strong language models, 2024. https://arxiv.org/abs/2401.01335. Self-play URL Yi Cui. Insights from benchmarking frontier language models on web app code generation, 2024a. URL https://arxiv.org/abs/2409.05177. Yi Cui. Webapp1k: practical code-generation benchmark for web app development. http://arxiv.org/abs/2408.00019, 2024b. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: benchmark evaluation, 2023. URL https://arxiv.org/abs/2308.15363. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Oﬁr Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024. URL https://arxiv.org/abs/2310.06770. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Mikhail Mirzayanov. Codeforces. https://codeforces.com, 2009. ONEKQ. Webapp1k dataset. https://huggingface.co/datasets/onekq-ai/WebApp1K-React, 2024a. ONEKQ. Webapp1k leaderboard. https://huggingface.co/spaces/onekq-ai/WebApp1K-models-leaderboa 2024b. ONEKQ. Webapp1k-duo dataset. https://huggingface.co/datasets/onekq-ai/WebApp1K-Duo-React, 2024c. OpenAI. Learning to reason with llms. https://openai.com/index/introducing-openai-o1-preview/, 2024. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with general reinforcement learning algorithm, 2017. URL https://arxiv.org/abs/1712.01815. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcome-based feedback, 2022. URL https://arxiv.org/abs/2211.14275. Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah Goodman. Hypothesis search: Inductive reasoning with language models. In The Twelfth International Conference on Learning Representations, 2024. Xuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting, 2024. URL https://arxiv.org/abs/2402.10200. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. 2024. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: self-taught reasoner bootstrapping reasoning with reasoning. In Proceedings of the 36th International Conference on Neural Information Processing Systems, 2022. 9 Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D. Goodman. Quiet-star: Language models can teach themselves to think before speaking, 2024. URL https://arxiv.org/abs/2403.09629. Ruize Zhang, Zelai Xu, Chengdong Ma, Chao Yu, Wei-Wei Tu, Shiyu Huang, Deheng Ye, Wenbo Ding, Yaodong Yang, and Yu Wang. survey on self-play methods in reinforcement learning, 2024. URL https://arxiv.org/abs/2408.01072. Xingyuan Zhang, Philip Becker-Ehmck, Patrick van der Smagt, and Maximilian Karl. Action inference by maximising evidence: Zero-shot imitation from observation with world models, 2023. URL https://arxiv.org/abs/2312.02019."
        }
    ],
    "affiliations": []
}