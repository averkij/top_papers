{
    "paper_title": "EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing",
    "authors": [
        "Keming Wu",
        "Sicong Jiang",
        "Max Ku",
        "Ping Nie",
        "Minghao Liu",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built \\mname, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. \\mname demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that \\mname achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new \\benchname, outperforming a wide range of VLM-as-judge models. Furthermore, we use \\mname to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates \\mname's ability to serve as a reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. \\mname with its training dataset will be released to help the community build more high-quality image editing training datasets."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 6 4 3 6 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Work in progress",
            "content": "EDITREWARD: HUMAN-ALIGNED REWARD MODEL FOR INSTRUCTION-GUIDED IMAGE EDITING Keming Wu, Sicong Jiangϑ, Max Ku, Ping Nie, Minghao Liu, Wenhu Chen University of Waterloo, Tsinghua Univerisity, 2077AI, ϑMcGill University, Independent {wukeming0608@gmail.com, wenhuchen@uwaterloo.ca} (cid:153) https://tiger-ai-lab.github.io/EditReward"
        },
        {
            "title": "ABSTRACT",
            "content": "Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built EDITREWARD, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following rigorous protocol containing over 200K preference pairs. EDITREWARD demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that EDITREWARD achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORABench, ImagenHub, and our new EDITREWARD-BENCH, outperforming wide range of VLM-as-judge models. Furthermore, we use EDITREWARD to select high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates EDITREWARDs ability to serve as reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. EDITREWARD with its training dataset will be released to help the community build more high-quality image editing training datasets."
        },
        {
            "title": "INTRODUCTION",
            "content": "Instruction-guided image editing is an important task to enable intuitive and fine-grained image modifications through natural language instructions (Brooks et al., 2023; Zhang et al., 2024a; Zhao et al., 2024; Wei et al., 2024). Closed-source models like GPT-Image-1 (OpenAI, 2025), Seedream (Gao et al., 2025), and Googles Nano Banana (Google DeepMind, 2025) have made marvelous strides on this task. The progress is driven partially by their high-quality in-house private training dataset. Existing open-source image editing datasets like ImgEdit (Ye et al., 2025), HQ-Edit (Hui et al., 2024), GPT-Image-Edit-1.5M (Wang et al., 2025d), UltraEdit (Zhao et al., 2024), and OmniEdit (Wei et al., 2024) are all produced with automatic data synthesis pipelines and filtered with different rewards. Commonly used rewards are mainly divided into three categories: (1) Perceptual scores like LPIPS (Zhang et al., 2018) fail to capture semantic alignment with user instructions, (2) Feature scores like CLIP (Hessel et al., 2021) fail to capture editing semantics, (3) VLM-as-a-judge like VIEScore (Ku et al., 2023; Jiang et al., 2024; Wang et al., 2025b) uses general-purpose VisionLanguage Models (VLM), which are not optimized for rewarding image editing tasks. Therefore, these ad-hoc rewards show weak alignment with human preference in the image editing task. To build more aligned rewards, line of work proposes to fine-tune general-purpose VLMs (for instance Qwen2.5-VL) to reward models. However, some of them rely on noisy, crowd-sourced preference annotations (Lin et al., 2024; Xu et al., 2024), which are often plagued by inconsistency, low These authors contributed equally."
        },
        {
            "title": "Work in progress",
            "content": "Figure 1: An overview of our framework, illustrating the construction of the EDITREWARDDATA and the subsequent training of our reward model, EDITREWARD. Top: The data pipeline, where we generate diverse candidate pool from multiple state-of-the-art models and collect multidimensional human preference annotations. Bottom: The model pipeline, where EDITREWARD is optimized on EDITREWARD-DATA using our proposed Multi-Dimensional Uncertainty-Aware Ranking Loss for training, followed by its use in inference. inter-annotator agreement. The others adopt pseudo-labels generated by proprietary, closed-source models (Wei et al., 2024; Wu et al., 2025c), creating highly noisy and biased labels. These trained reward models still fall short in providing enough reward signals to scale up high-quality image editing datasets. high quality image editing dataset is desired to build good reward model. In this paper, we introduce EDITREWARD, human-aligned reward model powered by highquality dataset for instruction-guided image editing. We first construct EDITREWARD-DATA, large-scale, high-fidelity preference dataset for instruction-guided image editing. It comprises over 200K manually annotated preference pairs, covering diverse range of edits produced by seven stateof-the-art models across twelve distinct sources. Every preference annotation in EDITREWARDDATA was curated by trained annotators following rigorous and standardized protocol, ensuring high alignment with considered human judgment and minimizing label noise. Using this dataset, we train the reward model EDITREWARD to score instruction-guided image edits. To rigorously assess EDITREWARD and future models, we also introduce EDITREWARD-BENCH, new benchmark built upon our high-quality annotations, which includes more difficult multi-way preference prediction. Experimental results show that EDITREWARD achieves state-of-the-art performance on several benchmarks. On GenAI-Bench (Jiang et al., 2024), our model obtains score of 65.72, significantly outperforming other leading VLM judges such as GPT-5 (59.61). Similarly, on AURORABench (Krojer et al., 2024), EDITREWARD scores 63.62, showing substantial gain over OpenAIGPT-4o (50.81). While demonstrating competitive performance on ImagenHub (Ku et al., 2024) with score of 35.20, it is on our proposed EDITREWARD-BENCH where the fine-grained capabilities of top models are most clearly discerned. This not only validates the superiority of our model but also demonstrates that EDITREWARD-BENCH provides more reliable and challenging evaluation. We further study the potential of EDITREWARD to select the high-quality subset from noisy candidates, which can be used to train next-generation image editing models. Specifically, we adopt EDITREWARD to select the top 20K subset from ShareGPT-4o-Image (Chen et al., 2025a) and use the subset to fine-tune Step1X-Edit (Liu et al., 2025b). We observe significant improvement by training on the subset over training on the full set. On GEdit-Bench, the overall score increases from 6.7/10 (full-set) to 7.1/10 (subset), making it on par with Doubao-Edit (Wang et al., 2025c). This experiment demonstrates its high potential to work as reward model for future research."
        },
        {
            "title": "Work in progress",
            "content": "Figure 2: Statistics of our EDITREWARD-DATA and EDITREWARD-BENCH. In summary, our primary contributions are: (1) We construct and release EDITREWARD-DATA, large-scale (200K) preference dataset for image editing, distinguished by its high-quality manual annotations and diversity of sources. (2) We train and release EDITREWARD, VLM-based reward model trained on EDITREWARD-DATA that demonstrates superior alignment with human prefer- (3) We propose EDITREWARD-BENCH, new benchmark featuring more challenging ences. multi-way preference ranking task that provides more robust evaluation of reward models."
        },
        {
            "title": "2 EDITREWARD-DATA",
            "content": "2.1 THE EDITREWARD-DATA CONSTRUCTION EDITREWARD-DATA contains 9557 instructionimage pairs collected from six established editing benchmarks: GEdit-Bench (606) (Liu et al., 2025b), ImgEdit-Bench (737) (Ye et al., 2025), MagicBrush (1,053) (Zhang et al., 2024a), AnyEdit (1,250) (Yu et al., 2025), EmuEdit (5,611) (Sheynin et al., 2024), and an internal set (300). This aggregation ensures broad coverage of semantically grounded and executable editing instructions. For each instruction, we generated 12 candidate images using six state-of-the-art models: Step1X-Edit (Liu et al., 2025b), Flux-Kontext (BlackForestLabs et al., 2025), Qwen-Image-Edit (Wu et al., 2025a), BAGEL (Deng et al., 2025), Ovis-U1 (Wang et al., 2025a), and OmniGen2 (Wu et al., 2025b), with multiple random seeds to avoid model bias. Seven candidates were randomly sampled for human evaluation. Annotators scored each image on 4-point Likert scale (1 = Poor and 4 = Excellent) along two dimensions: Instruction Following (semantic accuracy, completeness, and no unprompted changes) and Visual Quality (plausibility, artifact-free rendering, and aesthetics). This rubric yields more informative labels than single-score schemes. Details of the annotation protocol and quality-control process are provided in the Appendix A.2. Comprehensive statistics of the dataset are provided in Table 1 and Figure 2. EDITREWARDDATA is unique in combining large-scale, expert human annotation and multi-dimensional scoring rubric, making it strong foundation for training editing reward models. 2.2 THE EDITREWARD-BENCH CONSTRUCTION EDITREWARD-BENCH is designed to provide more robust evaluation of image editing reward models than existing suites. We curated 500 high-quality groups from the EDITREWARD-DATA candidate pool, covering diverse editing categories. Each group was annotated by three independent"
        },
        {
            "title": "Work in progress",
            "content": "Table 1: The comparison of different generative preference datasets and benchmarks. Dataset Scale Task Focus Annotation Eval. Dims. Limitation / Caveat ImageRewardDB(Xu et al., 2024) 137K Visual Generation 1.2M Visual Generation VisionPrefer(Wu et al., 2025c) 1.6K Generation / Editing GenAI-Bench(Jiang et al., 2024) 3.6K HIVE(Zhang et al., 2024b) Instructional Editing 100K Instructional Editing ADIEE(Chen et al., 2025b) 1.17M Visual Generation HPSv3(Ma et al., 2025) EDITREWARD-DATA 200K Instructional Editing Human Model Human Human Model Human Human Single Multiple Single Single Single Single Multiple Noise, limited diversity Model bias, synthetic prefs Small scale Small reward set Synthetic labels, model bias Generalization limits Fine-grained supervision Benchmark Scale Annotation Eval. Dims. Multi-Way Preference Pair-Wise Point-Wise 900 GenAI-Bench-Edit(Jiang et al., 2024) AURORA-Bench-Edit(Krojer et al., 2024) 1.6K 1.4K ImagenHub-Edit(Ku et al., 2024) Human Human Human EDITREWARD-BENCH 1.5K Human Cross-check Multiple Single Multiple Multiple 2-way 2-way 2/3/4-way experts using the same two-dimensional rubric (instruction following and visual quality) described in Section 2.1. We prioritized challenging cases where competing edits had small score differences to increase the discriminative power of the benchmark. The key innovation of EDITREWARD-BENCH is multi-way preference comparison protocol that extends beyond pairwise judgments. Evaluation units include ternary (A, B, C) and quaternary (A, B, C, D) tuples, with correctness defined by simultaneously predicting all pairwise relations within the tuple. This strict criterion provides more comprehensive and reliable test of ranking consistency than traditional pairwise accuracy. We benchmark wide range of models on EDITREWARD-BENCH, and results are reported in Section 4. More Details of the construction of EDITREWARD-BENCH are provided in the Appendix A.3."
        },
        {
            "title": "3 EDITREWARD",
            "content": "3.1 ARCHITECTURE Inspired by the success of VLMs as powerful feature extractors, we leverage VLM as the backbone for our reward model. The task of image editing evaluation is inherently tri-modal, requiring joint reasoning over source image (Is), textual prompt (P ), and an edited image (Ie). Our model is trained on human preference data, which consists of pairs of edited images, (Ie,1, Ie,2), generated from the same (Is, ) context. Our reward model consists of two components: multimodal backbone, Hψ (either Qwen2.5VL (Bai et al., 2023) or Mimo-VL (Yue et al., 2025)), which computes latent representation of the edits quality; and an MLP reward head, Rω, which projects this representation to scalar score. The score si for an edited image Ie,i is thus given by: si = Rω(Hψ(Is, P, Ie,i)). (1) Here Hψ represents the VLM backbone with parameters ψ, and Rω is the MLP reward head with parameters ω. For preference pair, the scores s1 and s2 are computed using Eq. 1 and are subsequently used in preference loss function to jointly optimize the parameters ψ and ω. 3.2 MULTI-DIMENSIONAL UNCERTAINTY-AWARE RANKING Prior reward models for generative tasks often fail to account for inconsistencies in human annotations, treating each preference label with equal certainty. This can introduce bias, particularly when judging ambiguous or challenging cases. The HPSv3 framework (Sun et al., 2025) made significant progress in text-to-image evaluation by addressing this issue. Instead of predicting deterministic score s, HPSv3 models the score as Gaussian distribution (µ, σ2), thereby capturing the uncertainty inherent in the data. The preference probability (Ie,1 Ie,2) is then computed by integrating over the two reward distributions, leading to more robust, probabilistic ranking. Inspired by this, we adapt and extend this uncertainty-aware paradigm for the more complex domain of instruction-guided image editing. Image editing quality is multi-faceted; an edit can be faithful to"
        },
        {
            "title": "Work in progress",
            "content": "the instruction but visually unrealistic, or vice versa. To capture this complexity, our EDITREWARDDATA provides disentangled scores across two distinct dimensions: (1) Instruction Following and (2) Visual Quality. single, holistic uncertainty distribution as in HPSv3 is insufficient to model this rich, multi-dimensional feedback. To this end, we adapt the reward head, Rω, using Multi-Task Learning (MTL) (Crawshaw, 2020) approach. For single edited image sample (Is, P, Ie), the reward head no longer outputs single distribution, but rather separate Gaussian distribution for each evaluation dimension. Let {1, 2} represent the two dimensions. The output for single sample is pair of distributions as formulated in Eq. 2: i,d), (2) This is achieved by having the final layers of the MLP in Rω predict set of parameters (µi,1, σi,1, µi,2, σi,2) for each input. We explore both separate and shared-parameter heads for the task. To train our model with this multi-dimensional output, we explore two distinct loss: for = 1, 2. si,d (µi,d, σ2 Multi-Dimensional Uncertainty-Aware Ranking Loss. This approach extends the probabilistic ranking framework of HPSv3 (Sun et al., 2025) to our multi-dimensional task. To do so, we must first aggregate the two predicted dimensional mean scores (µi,1, µi,2) for each candidate image into single, effective mean score, µagg . We propose and investigate three distinct aggregation strategies, which can be compactly formulated as Eq. 3: µagg = min(µi,1, µi,2) 1 2 (µi,1 + µi,2) µi,1 + µi,2 (Pessimistic Minimum) (Balanced Average) (Direct Summation) (3) The resulting aggregated means for pair of images, along with their predicted uncertainties (σ2), are then used to compute the final preference probability (Ih Il) following the probabilistic method from HPSv3. The model is trained by minimizing the negative log-likelihood of the groundtruth preference as in Eq. 4: Lrank = log(P (Ih Il)). (4) Aggregated Score Regression. Alternatively, we frame the training as direct regression task. This approach leverages the pointwise scores available in our EDITREWARD-DATA dataset by first aggregating the predicted distributions. Given that the sum of two independent Gaussians is also Gaussian, the aggregated score distribution for sample is si,agg (µi,1 + µi,2, σ2 i,2). The model is then optimized by minimizing the Mean Squared Error (MSE) between the mean of this aggregated distribution and transformed sum of the ground-truth scores, zagg = (z1 + z2): i,1 + σ Lreg = E(Is,P,Ie,z1,z2)D (cid:2)(µi,1 + µi,2) zagg2(cid:3) . (5) This multi-dimensional uncertainty-aware approach allows our model to learn more nuanced and disentangled representation of edit quality, leveraging the rich supervisory signal in our dataset. Ablation study comparing the loss functions and aggregation strategies is presented in Section 4.5. 3.3 DISENTANGLING TIES VIA DIMENSIONAL PREFERENCE. While standard models like Bradley-Terry model with ties (BTT) treat tied pairs as single outcome (Liu et al., 2025a), we propose novel data augmentation strategy to extract richer supervisory signal from these ambiguous cases. Our key insight is that tie in overall quality often masks complementary dimensional strengths. For instance, one image may excel in Instruction Following while the other has superior Visual Quality. We leverage this by decomposing each qualifying tie pair (IA, IB)tie into two new training samples with opposing preference labels, (IA IB) and (IB IA), based on their respective dimensional advantages (Eq. 6). Let zi,d be the ground-truth score for image on dimension d. tie pair where one image is preferred on the first dimension and the other is preferred on the second (e.g., zA,1 > zB,1 and zB,2 > zA,2) is duplicated and relabeled as follows: The pair (IA, IB)tie = (cid:26)Sample 1 with label: IA IB Sample 2 with label: IB IA (6)"
        },
        {
            "title": "Work in progress",
            "content": "Figure 3: Representative examples of our reward model aligning with human judgments. This strategy forces the model to reconcile seemingly contradictory signals for the same input pair, pushing it to develop more granular understanding of nuanced trade-offs. This not only doubles the utility of our annotated tie data but also leads to more stable training dynamic. As illustrated in Appendix A.7, our tie-disentanglement method results in smoother training loss curve and more consistent performance gains on the validation set. Figure 3 shows some examples of our reward model giving rewards that are aligned with humans."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 IMPLEMENTATION DETAILS We train our reward model, EDITREWARD, using 200K high-quality pairwise preference samples from our dataset. For our main experimental results, we report performance using two powerful vision-language models as backbones: Qwen2.5-VL-7B and MiMo-VL-7B. To ensure controlled comparison, all ablation studies are conducted consistently using the Qwen2.5-VL-7B backbone. During training, all parameters of the backbone are unfrozen and set as trainable. The training is performed for 2 epochs on cluster of 8 NVIDIA A100 GPUs. For preprocessing, all training images are resized to 448 448 pixels while preserving their original aspect ratios. Additional training details are provided in Appendix A.4. 4.2 BENCHMARKS AND BASELINES We evaluate our approach on suite of three established public benchmarks and our newly proposed benchmark, designed to provide more comprehensive assessment of image editing quality. Existing Benchmarks. We utilize ImagenHub (Ku et al., 2024), GenAI-Bench (Jiang et al., 2024), and AURORA-Bench (Krojer et al., 2024). For benchmarks with point-wise annotations like ImagenHub, we measure the Spearman rank correlation to assess alignment with human scores. For ImagenHub, which includes three ratings per sample, we also compute the Human-to-Human correlation as practical upper bound (Ku et al., 2023). For benchmarks with paired comparisons like GenAI-Bench and the pair-wise split of AURORA-Bench, we report the prediction accuracy. Additional details of the evaluation across different methods are provided in Appendix A.5. EDITREWARD-BENCH. Derived from the held-out test split of our EDITREWARD-DATA dataset, this benchmark provides pair-wise preference labels. We report performance on EDITREWARDBENCH using overall preference accuracy (pair-wise). We evaluated wide range of leading models on EDITREWARD-BENCH to establish its utility. This included proprietary models such as GPT-4o, GPT-5, Gemini-2.0-Flash (Hassabis et al., 2024), and Gemini-2.5-Flash (Comanici et al., 2025), as well as prominent open-source VLMs like the Qwen2.5-VL series and MiMo-VL-7B. The experimental results, detailed in Section 4, demonstrate that EDITREWARD-BENCH effectively differentiates between models of varying capabilities and reveals challenges, such as reasoning over multiple candidates, that are not apparent in simpler pairwise benchmarks."
        },
        {
            "title": "Work in progress",
            "content": "Table 2: Comprehensive results on public benchmarks and our proposed EDITREWARD-BENCH. Under the EDITREWARD-BENCH results, denotes the number of candidates in the multi-way preference ranking task. Bold marks the best performance, and underline marks the second best. Method Random Human-to-Human Proprietary Models GPT-4o GPT-5 Gemini-2.0-Flash Gemini-2.5-Flash Open-Source VLMs Qwen2.5-VL-3B-Inst Qwen2.5-VL-7B-Inst Qwen2.5-VL-32B-Inst MiMo-VL-7B-SFT-2508 ADIEE Reward Models (Ours) EDITREWARD (on Qwen2.5-VL-7B) EDITREWARD (on MiMo-VL-7B) GenAIAURORABench Bench Imagen Hub EDITREWARD-BENCH K=2 K= K=4 Overall 25.90 53.54 59.61 53.32 57.01 42.76 40.48 39.28 57.89 59.96 63.97 65.72 33.43 50.81 47.27 44.31 47.63 30.69 38.62 37.06 30.43 55.56 59.50 63.62 41.84 25.81 11.33 1.35 38.21 40.85 23.69 41.62 -2.54 18.59 26.87 22.14 34.50 45.69 57.53 52.43 58.61 51.07 52.69 50.54 49.46 27.33 38.51 33.33 39. 20.27 24.67 25.27 30.41 7.31 12.84 13.51 12.16 2.71 3.38 4.05 9.46 13.84 28.31 37.81 33.47 38.02 26.86 29.75 28.72 31.19 36.18 35.20 56.99 56.45 36.00 42.67 10.81 11.49 36.78 38.42 4.3 EXPERIMENTAL RESULTS: ALIGNMENT WITH HUMANS The main results presented in Table 2 establish EDITREWARD as new state-of-the-art reward model for instruction-guided image editing. Our best model, EDITREWARD (on MiMo-VL-7B), achieves top scores on the primary public benchmarks, obtaining an accuracy of 65.72% on GenAI-Bench and 63.62% on AURORA-Bench. This performance surpasses strong proprietary models like GPT-5 (59.61) and the leading open-source method ADIEE (59.96). On the point-wise ImagenHub benchmark, our model remains highly competitive with the best systems available, the Qwen2.5-VL-7B variant achieves second-best Spearman correlation of 36.18, closely following GPT-4o. Crucially, our results highlight the profound impact of our training paradigm itself. By applying our methodology to the base Qwen2.5-VL-7B model, we observe massive performance uplift of over 23 points on GenAI-Bench (from 40.48% to 63.97%), demonstrating that our framework dramatically enhances VLMs alignment with human judgments. This capability is further validated on our challenging EDITREWARD-BENCH, where EDITREWARD (on MiMo-VL-7B) again achieves the highest score of 38.42%, outperforming specialized models like Gemini-2.5-Flash (38.02) and GPT-5 (37.81). The strong performance of EDITREWARD on both Qwen and MiMo-VL backbones also confirms that our framework is robust and effectively scales with more powerful base models. 4.4 APPLICATION: EDITREWARD AS REWARD To demonstrate EDITREWARDs practical utility as data supervisor, we conducted data curation experiment designed to improve state-of-the-art editing model. We employed our reward model to score the 46,000 examples in the ShareGPT-4o-Image dataset (Chen et al., 2025a), from which we selected high-quality subset of the top 20,000 samples. This curated dataset was then used to fine-tune the powerful Step1X-Edit model (Liu et al., 2025b). Evaluation Protocol. To measure the impact of this curation, we evaluate the resulting model on the comprehensive GEdit-Bench. This benchmark features both English (EN) and Chinese (CN) instructions, as well as challenging Intersection subset containing prompts that all models could process. We compare our fine-tuned model against diverse range of baselines, including the original Step1X-Edit, the same model fine-tuned on the full unfiltered dataset, and other leading open-source and proprietary models like Doubao and GPT-Image-1. Following established practices Ku et al. (2023), performance is judged by GPT-4o on three metrics (0-10 scale): Semantic"
        },
        {
            "title": "Work in progress",
            "content": "Table 3: Comprehensive comparison of state-of-the-art models on both the English and Chinese versions of the GEdit-Bench benchmark, across intersection and full test sets. Our model, significantly improve the base model Step1X-Edit. indicates higher the better. *-I means intersection set. Model GEdit-Bench-EN-I GEdit-Bench-EN GEdit-Bench-CN-I GEdit-Bench-CN SC PQ SC PQ SC PQ SC PQ 3.122 AnyEdit (Yu et al., 2025) 6.037 OmniGen (Wu et al., 2025b) 6.816 Gemini-2.0 (Hassabis et al., 2024) 7.396 Doubao (Wang et al., 2025c) 7.867 GPT-Image-1 (OpenAI, 2025) Step1X-Edit 7.289 Step1X-Edit + ShareGPT-4o-Image 7.411 5.865 5.856 7.408 7.899 8.097 6.962 6. 2.919 3.053 5.154 5.879 6.483 6.866 7.137 7.222 7.590 7.743 6.618 7.131 6.803 7.349 5.882 5.871 7.436 7.885 8.133 6.998 6.893 2.854 3.098 5.005 6.015 6.509 6.790 6.983 7.370 7.494 7.840 6.444 7.464 6.780 7.126 5.840 5.830 7.385 7.870 8.075 7.076 6.855 2.899 3.011 5.122 5.850 6.450 6.821 7.105 7.195 7.560 7.708 6.779 7.647 6.595 7.116 5.849 5.845 7.402 7.851 8.095 7.398 6. 2.817 4.976 6.473 6.942 7.451 6.983 6.583 Ours (EDITREWARD as reward) 7.895 6.946 7.131 7.854 6. 7.086 7.757 7.024 7.074 7.658 6.995 7.001 Consistency (G SQ) for instruction fidelity, Perceptual Quality (G PQ) for visual realism, and an Overall Score (G O) for overall quality. Results and Analysis. As detailed in Table 3, this reward-driven filtering yields significant performance gains. On the GEdit-Bench Overall score, our fine-tuned model achieves 7.086, substantially outperforming both the original Step1X-Edit baseline (6.444) and the model trained on the full, unfiltered dataset (6.780). This finding is crucial, as it confirms that data quality, as judged by our reward model, is more impactful than sheer data quantity. EDITREWARD successfully prunes noisy examples that would otherwise degrade performance during fine-tuning. This uplift elevates the open-source Step1X-Edit to be competitive with top-tier editors like Doubao, validating our models potential as an essential tool for training next-generation generative models. 4.5 ABLATION STUDIES Table 4: Ablation study on key design choices for our reward model. We compare point-wise regression loss (variant I) against our pair-wise uncertainty loss (variant II, III, IV, V), and further investigate the impact of the reward head architecture (Shared vs. Multiple) and different score aggregation strategies. Variants II III IV Model Configuration Benchmark Performance Loss Type Head Type Aggregation GenAI-Bench AURORA-Bench ImagenHub EditReward Point-wise N/A Pair-wise Pair-wise Pair-wise Pair-wise Shared Multiple Multiple Multiple N/A Mean Min Sum Mean 49.62 60.17 59.96 59.63 63. 42.38 56.75 57.25 55.19 59.50 13.40 32.65 30.25 32.93 36.18 22.73 36.78 36.57 37.60 36. Ablation on Model Design. We analyze our models key architectural choices in Table 4. Loss Type. Comparing loss functions (Variant vs. V), our pair-wise uncertainty model (63.97 on GenAI-Bench) significantly outperforms the point-wise regression baseline (49.62). This confirms that modeling relative preferences is more effective than regressing on absolute scores for this task. Head Type. For the reward head (Variant II vs. V), using multiple independent heads (63.97) provides clear improvement over shared architecture (60.17 on GenAI-Bench), suggesting that specialized heads better capture our disentangled evaluation dimensions. Aggregation Strategy. Finally, we compare three score aggregation strategies (Variants III-V), finding that the balanced mean provides the most consistent and highest performance (63.97 on GenAIBench and 59.50 on AURORA-Bench). We therefore adopt the Pair-wise model with Multiple heads and Mean aggregation as our final configuration. Ablation on Model Backbone. To verify our frameworks generalizability, we train EDITREWARD on three backbones of varying scale and architecture, confirming that our method consistently benefits from stronger foundation models  (Table 5)  . Performance increases when scaling from Qwen2.5-"
        },
        {
            "title": "Work in progress",
            "content": "Table 5: Ablation study on different model parameter sizes and different model backbones. Backbone GenAI-Bench AURORA-Bench ImagenHub EDITREWARD-BENCH Qwen2.5-VL-3B-Inst Qwen2.5-VL-7B-Inst MiMo-VL-7B-SFT-2508 62.79 63.97 65. 57.37 59.50 63.62 32.34 36.18 35.20 37.40 36.78 38.42 VL-3B to 7B, and improves further at the 7B scale when using the more advanced MiMo-VL-7B architecture, which achieves state-of-the-art scores of 65.72% on GenAI-Bench and 63.62% on AURORA-Bench. This demonstrates that our framework is backbone-agnostic and effectively leverages the capabilities of more powerful models."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Evolution of Instruction-Guided Image Editing. Instruction-guided image editing has rapidly evolved from early trajectory-based methods. Diffusion models (Song et al., 2020; Dhariwal & Nichol, 2021; Rombach et al., 2022; Podell et al., 2023) first enabled editing via dual-prompt formulations that relied on cross-attention manipulation or inversion (Hertz et al., 2022; Mokady et al., 2023; Wallace et al., 2023). The paradigm then shifted to more user-friendly single-instruction editing, pioneered by InstructPix2Pix (Brooks et al., 2023) and refined by works like MagicBrush and Emu-Edit (Zhang et al., 2024a;b; Sheynin et al., 2024) that focused on curating high-quality datasets. This trajectory-based family has been further advanced by flow-matching models (BlackForestLabs et al., 2025), which improve training and sampling efficiency. In parallel, sequential generative models, including autoregressive approaches (Yu et al., 2022; Tian et al., 2024), enhance compositional reasoning. The most recent advances feature hybrid multimodal architectures like OmniGen2 (Wu et al., 2025b) and BAGEL (Deng et al., 2025), which integrate large visionlanguage backbones with generative decoders to enable more context-aware, conversational editing. Evaluating Instruction-Guided Image Editing. Early evaluation of image editing relied on perceptual metrics like LPIPS (Zhang et al., 2018), but these require reference images and fail to assess semantic alignment. CLIP-based metrics (Hessel et al., 2021) were introduced for textimage consistency but also show limited correlation with human judgment (Ku et al., 2024). The advent of large visionlanguage models (VLMs) enabled zero-shot evaluation, with proprietary models (Ku et al., 2023; Wang et al., 2025b) demonstrating promising human correlation while open-source counterparts (Liu et al., 2023; Laurencon et al., 2024) have lagged (Jiang et al., 2024). Consequently, recent work has focused on improving open-source evaluators via fine-tuning. One strategy distills supervision from proprietary models (Wei et al., 2024; Gu et al., 2024; Wu et al., 2024), which risks inheriting model biases. The other collects direct human annotations (Xu et al., 2024; Liang et al., 2024; Wu et al., 2023), offering higher-quality signals but typically at smaller scale. Our work contributes large-scale, expert-annotated dataset, enabling more reliable and robust reward modeling for image editing."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we addressed the critical bottleneck hindering the advancement of open-source instruction-guided image editing: the lack of reliable, human-aligned reward model for scaling up high-quality training data. To this end, we introduced three-part solution: (1) EDITREWARDDATA, new large-scale (200K) preference dataset curated with rigorous expert annotation to minimize the noise and bias prevalent in existing resources; (2) EDITREWARD, dedicated reward model trained on this high-fidelity data to specialize in the image editing domain; and (3) EDITREWARD-BENCH, challenging new benchmark featuring multi-way preference tasks to enable more robust evaluation. Our experimental results validate the effectiveness of our approach. EDITREWARD establishes new state of the art, demonstrating superior correlation with human judgment by outperforming strong VLM judges like GPT-5 and GPT-4o on public benchmarks. More importantly, we demonstrated its practical utility in downstream data curation task: finetuning Step1X-Edit on 20K subset of data filtered by EDITREWARD yielded significantly better"
        },
        {
            "title": "Work in progress",
            "content": "performance than training on the full 46K noisy dataset (7.1 vs. 6.7 overall score on GEdit-Bench). This confirms that high-quality reward signal is key ingredient for training powerful, nextgeneration editing models. Ultimately, this work provides both methodology and set of open resources to help bridge the gap between open-source and proprietary image editing models. This strong alignment with human judgment confirms EDITREWARDs capability as data supervisor and highlights its potential for more advanced applications, including reinforcement learning-based post-training and test-time scaling of image editing models. To empower the community and facilitate future research, we will publicly release our EDITREWARD-DATA dataset, the trained EDITREWARD model, and the EDITREWARD-BENCH benchmark."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "The development of advanced instruction-guided image editing models, which our work aims to evaluate and improve, carries significant ethical implications. While these technologies enable powerful creative expression, they can also be misused to generate deceptive or harmful content, such as deepfakes, misinformation, or fraudulent documents, lowering the barrier for malicious actors. Our work, by creating more effective reward model, could inadvertently contribute to accelerating these capabilities. We acknowledge this dual-use potential and have taken steps to mitigate risks. The EDITREWARD-DATA dataset was constructed from publicly available, non-sensitive benchmarks, and we have made efforts to filter out any potentially harmful or personally identifiable content. Our reward model, EDITREWARD, is trained to align with constructive and high-quality edits, as defined by our multi-dimensional rubric, not harmful instructions. By publicly releasing our dataset, model, and code, we aim to promote transparency and enable the research community to further study the safety, biases, and alignment of such models."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To ensure the reproducibility of our work, we provide the following details. All of our reward models were trained on 8 NVIDIA A800 GPUs. The evaluation of baseline models was conducted using their official public codebases and recommended configurations. For proprietary models (e.g., GPT-4o, Gemini series), we accessed their APIs between April and June 2025; given the evolving nature of these models, we have archived their specific outputs for consistency. Our new dataset, EDITREWARD-DATA, was constructed following the detailed protocol described in Section 2.1, and both the dataset and our evaluation benchmark, EDITREWARD-BENCH, will be publicly released. The complete codebase for training and evaluating our EDITREWARD, along with the final model weights for both the Qwen2.5-VL and MiMo-VL backbones, will be made available on GitHub and Hugging Face. Further details are provided in Appendix A.4 and A.5."
        },
        {
            "title": "REFERENCES",
            "content": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. BlackForestLabs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Wang. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation. arXiv preprint arXiv:2506.18095, 2025a."
        },
        {
            "title": "Work in progress",
            "content": "Sherry Chen, Yi Wei, Luowei Zhou, and Suren Kumar. Adiee: Automatic dataset creation and scorer for instruction-guided image editing evaluation. arXiv preprint arXiv:2507.07317, 2025b. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Michael Crawshaw. Multi-task learning with deep neural networks: survey. arXiv preprint arXiv:2009.09796, 2020. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. NeurIPS, 34:87808794, 2021. Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, Wei Liu, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Rui Wang, Xuanda Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Xin Xia, Xuefeng Xiao, Zhonghua Zhai, Xinyu Zhang, Qi Zhang, Yuwei Zhang, Shijia Zhao, Jianchao Yang, and Weilin Huang. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. URL https: //arxiv.org/abs/2504.11346. Google DeepMind. Gemini 2.5 flash image (nano banana). https://ai.google.dev/ gemini-api/docs/image-generation, 2025. Googles AI image generation and editing model, officially Gemini 2.5 Flash Image, known by its nickname Nano Banana. Accessed September 2025. Xin Gu, Ming Li, Libo Zhang, Fan Chen, Longyin Wen, Tiejian Luo, and Sijie Zhu. Multi-reward as condition for instruction-based image editing. arXiv preprint arXiv:2411.04713, 2024. Demis Hassabis, Koray Kavukcuoglu, and Gemini Team. Introducing gemini 2.0: our new ai model for the agentic era. https://blog.google/technology/google-deepmind/ google-gemini-ai-update-december-2024/, 2024. Google DeepMind blog announcement, December 2024. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-Prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. HQ-Edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. Genai arena: An open evaluation platform for generative models. arXiv preprint arXiv:2406.04485, 2024. Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Christopher Pal, and Siva Reddy. Learning Action and Reasoning-Centric Image Editing from Videos and Simulations. In NeurIPS, 2024. URL https://arxiv.org/abs/2407.03471. Spotlight Paper. Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. arXiv preprint arXiv:2312.14867, 2023. Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=OuV9ZrkQlc."
        },
        {
            "title": "Work in progress",
            "content": "Hugo Laurencon, Leo Tronchon, and Victor Sanh. Introducing idefics2: powerful 8b visionlanguage model for the community. Hugging Face Blog, April 2024. URL https:// huggingface.co/blog/idefics2. Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, Junjie Ke, Krishnamurthy Dj Dvijotham, Katie Collins, Yiwen Luo, Yang Li, Kai Kohlhoff, Deepak Ramachandran, and Vidhya Navalpakkam. Rich human feedback for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pp. 366384. Springer, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025a. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025b. Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score. arXiv preprint arXiv:2508.03789, 2025. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text Inversion for editing real images using guided diffusion models. In CVPR, pp. 60386047, 2023. OpenAI. Gpt-image-1. https://platform.openai.com/docs/guides/ image-generation?image-generation-model=gpt-image-1, 2025. OpenAIs image generation model. Accessed September 2025. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, pp. 1068410695, 2022. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8871 8879, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Wangtao Sun, Xiang Cheng, Xing Yu, Haotian Xu, Zhao Yang, Shizhu He, Jun Zhao, and Kang Liu. Probabilistic uncertain reward model, 2025. URL https://arxiv.org/abs/2503. 22480. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. Bram Wallace, Akash Gokul, and Nikhil Naik. EDICT: Exact diffusion inversion via coupled transformations. In CVPR, pp. 2253222541, 2023. Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Jianshan Zhao, Yang Li, and Qing-Guo Chen. Ovis-u1 technical report. arXiv preprint arXiv:2506.23044, 2025a."
        },
        {
            "title": "Work in progress",
            "content": "Jifang Wang, Xue Yang, Longyue Wang, Zhenran Xu, Yiyu Wang, Yaowei Wang, Weihua Luo, Kaifu Zhang, Baotian Hu, and Min Zhang. unified agentic framework for evaluating conditional image generation, 2025b. URL https://arxiv.org/abs/2504.07046. Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, and Jianchao Yang. Seededit 3.0: Fast and high-quality generative image editing. arXiv preprint arXiv:2506.05083, 2025c. Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang Xie. Gpt-image-edit-1.5m: million-scale, gpt-generated image dataset, 2025d. URL https:// arxiv.org/abs/2507.21033. Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. arXiv preprint arXiv:2411.07199, 2024. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025a. URL https://arxiv.org/abs/ 2508.02324. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 20962105, 2023. Xun Wu, Shaohan Huang, Guolong Wang, Jing Xiong, and Furu Wei. Multimodal large language models make text-to-image generative models align better. Advances in Neural Information Processing Systems, 37:8128781323, 2024. Xun Wu, Shaohan Huang, Guolong Wang, Jing Xiong, and Furu Wei. Multimodal large language models make text-to-image generative models align better. Advances in Neural Information Processing Systems, 37:8128781323, 2025c. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark, 2025. URL https://arxiv.org/ abs/2505.20275. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2612526135, 2025. Xiaomi LLM-Core Team: Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu,"
        },
        {
            "title": "Work in progress",
            "content": "Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, and Bingquan Xia. Mimo-vl technical report. arXiv preprint arXiv:2506.03569, 2025. URL https: //arxiv.org/abs/2506.03569. 32 pages. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36, 2024a. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern visual editing. Recognition, pp. 90269036, 2024b. Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. UltraEdit: Instruction-based fine-grained image editing at scale. arXiv preprint arXiv:2407.05282, 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 USE OF LLM Large Language Models (LLMs) were used exclusively for minor grammar correction and stylistic refinement of the manuscript. Their role was purely auxiliary, and all major scientific contributions were made by the authors. The authors bear full responsibility for the content of this work. A.2 DETAILS OF EDITREWARD-DATA CONSTRUCTION Our dataset construction was centered on three principles: ecological validity, by sourcing instructions from human-vetted benchmarks; diversity, by generating candidates from state-of-the-art models; and reliability, through rigorous multi-dimensional annotation pipeline. Source Data Collection. To ensure ecological validity, we collected 9,557 unique instruction-image pairs from six established, human-vetted sources: GEdit-Bench (606), ImgEdit-Bench (737), MagicBrush (1,053), AnyEdit (1,250), EmuEdit (5,611), and challenging internal set (300). This aggregation provides comprehensive foundation of semantically grounded and executable edit instructions across wide spectrum of tasks and styles. Candidate Generation. For each of the 9,557 source pairs, we generated diverse pool of 12 candidate images using six state-of-the-art models: Step1X-Edit (Liu et al., 2025b), FluxKontext (BlackForestLabs et al., 2025), Qwen-Image-Edit (Wu et al., 2025a), BAGEL (Deng et al., 2025), Ovis-U1 (Wang et al., 2025a), and OmniGen2 (Wu et al., 2025b). To ensure broad quality spectrum and mitigate model-specific biases, we utilized multiple random seeds, preventing any single model from dominating the candidate pool. Table 6: The detailed comparison of different generative preference datasets and benchmarks. Dataset ImageRewardDB VisionPrefer GenAI-Bench HIVE ADIEE HPDv3 Venue Scale Task Focus Annotation Eval. Dims. Limitation / Caveat NeurIPS23 137K Visual Generation Generation 1.2M NeurIPS24 NeurIPS24 1.6K Generation / Editing 3.6K CVPR24 Instructional Editing >100K Instructional Editing ICCV25 >1.17M Visual Generation ICCV25 Human Model Human Human Model Human Human Single Multiple Multiple Single Single Single Multiple Expert comparisons with limited variety Multi-aspect but model-derived bias risks High quality but very small scale Task-specific, limited comparison set size Synthetic labels; possible model bias Wide-spectrum; generalizability limits Large scale and fine-grained supervision EDITREWARD-DATA 200K Instructional Editing Benchmark GenAI-Bench AURORA-Bench ImagenHub Venue NeurIPS24 NeurIPS24 ICLR24 Scale 900 1.6K 1.4K Annotation Eval. Dims. Multi-Way Preference Pair-Wise Point-Wise Human Human Human + Model Multiple Multiple Single Multiple 2-way 2-way 2-way 2/3/4-way EDITREWARD-BENCH 500 Groups (1.5K) 3 Human (Cross-check) Multi-Dimensional Annotation. From the pool of 12 candidates, 7 were randomly sampled for human evaluation. Annotators provided two separate scores for each candidate on 4-point Likert scale (1=Poor to 4=Excellent), corresponding to our two evaluation dimensions: (1) Instruction Following, which assesses semantic accuracy, completeness, and the avoidance of unprompted changes; and (2) Visual Quality, which evaluates physical plausibility, absence of artifacts, and overall aesthetic appeal. This multi-dimensional rubric provides more granular assessment than single holistic score. Detailed interface of the annotations is in Figure 4. We also provide detailed annotation guidance below. Annotation Guidelines:"
        },
        {
            "title": "Work in progress",
            "content": "Instruction Following This dimension focuses on how accurately, completely, and exclusively the model executed the text instruction. Key Criteria: Semantic Accuracy: Correctly interpreting the core meaning. Completeness: Fulfilling all parts of the instruction. Exclusivity: Avoiding unprompted changes to the rest of the image. Negative Indicators: key part of the instruction is ignored (e.g., color changed but not the object). major misinterpretation (e.g., orange yields grapefruit). The image is unchanged or random, unrelated image is generated. Scoring Rubric (1-4 Scale): 4 (Very Good): Perfectly executes all aspects of the instruction. Edit is surgical and flawless. 3 (Relatively Good): Achieves the main goal but with minor deviations or omissions (e.g., misses small detail). 2 (Relatively Poor): Significantly misunderstands or only partially executes the instruction. Unedited areas may be noticeably altered. 1 (Very Poor): Completely fails the instruction. The result is unrelated, or the image is corrupted. Visual Quality This dimension focuses on the physical plausibility, technical flawlessness, and overall aesthetic appeal of the edited image. Key Criteria: Plausibility: Consistency with real-world physics (lighting, shadows). Artifact-Free: Absence of visual flaws (blur, distortion, seams). Aesthetic Quality: The overall harmony, naturalness, and visual appeal. Negative Indicators: Obvious physical errors (e.g., an object casts no shadow). Noticeable and distracting artifacts (e.g., blurry halo around the edit). The final image is jarring, ugly, or unbalanced. Scoring Rubric (1-4 Scale): 4 (Very Good): Perfectly realistic and visually flawless. The edit is undetectable and appealing. 3 (Relatively Good): High quality overall, but close inspection may reveal minor imperfections (e.g., shadow is slightly off). 2 (Relatively Poor): The edit is obvious and looks unnatural, with clear visual flaws that detract from its quality. 1 (Very Poor): visual failure, full of severe errors and artifacts, making it unusable. Quality Control. The reliability of our annotations is ensured through multi-stage process. The process includes: (1) initial pilot studies to refine the annotation guidelines and rubric; (2) formal training and calibration phase for all annotators to align their judgments; and (3) continuous random"
        },
        {
            "title": "Work in progress",
            "content": "Figure 4: Annotation Interface sampling and cross-checking of annotations during the formal labeling process to maintain high inter-annotator agreement (IAA). A.3 DETAILS OF EDITREWARD-BENCH CONSTRUCTION To provide more robust and discerning evaluation of image editing reward models, we introduce EDITREWARD-BENCH. The design of this new benchmark is motivated by several limitations identified in existing evaluation suites. For instance, ImagenHub utilizes simple 3-point rating scale [0, 0.5, 1]. While user-friendly, this coarse granularity can fail to capture the nuanced quality differences across the broad spectrum of semantic consistency and perceptual quality (Ku et al., 2023). The editing tasks in AURORA-Bench are primarily focused on action-centric and reasoning-centric instructions, which may not represent the full diversity of common editing requests. To address these challenges, we constructed EDITREWARD-BENCH through meticulous pipeline. The foundation of our benchmark is curated subset of 500 high-quality groups sampled from our EDITREWARD-DATA candidate pool, spanning 7 distinct editing categories. To establish reliable ground truth, we engaged three independent groups of trained expert annotators. Following the multi-dimensional rubric detailed in Section 2.1, each annotator assigned scores on 4-point Likert scale [1, 2, 3, 4] for both instruction fidelity and visual quality. This process ensures the robustness and accuracy of our ground-truth labels. To increase the benchmarks difficulty and test"
        },
        {
            "title": "Work in progress",
            "content": "the fine-grained discriminative power of models, we prioritized the inclusion of samples where the competing edits have small differences in their average human scores. The primary innovation of EDITREWARD-BENCH is its introduction of multi-way preference comparison protocol, moving beyond simple pairwise judgments. We construct more complex evaluation units, including ternary tuples (A, B, C) and quaternary tuples (A, B, C, D), based on our reliable human scores. For models evaluation of tuple to be considered correct, it must correctly predict the preference relationship for all constituent pairs within that tuple (e.g., A>B, A>C, and B>C for ternary tuple where is the best and is the worst). This strict, all-ornothing criterion provides much more comprehensive and robust measure of reward models ranking consistency and reasoning capabilities than traditional pairwise accuracy. We evaluated wide range of leading models on EDITREWARD-BENCH to establish its utility. The experimental results are detailed in Section 4. Dataset Details We provide additional details regarding our annotation protocol. All annotators followed standardized rubric with clear dimension-specific guidelines, covering Instruction Following (IF) and Visual Quality (VQ). To ensure high consistency, each annotator underwent training sessions with reference examples before formal labeling. For EDITREWARD-DATA, each edited image is scored by single expert annotator on 4-point scale (14) across the two dimensions (IF, VQ). This provides large-scale but fine-grained supervision. For EDITREWARD-BENCH, every group is annotated by three independent experts, again along the two dimensions (IF, VQ). Annotators must jointly determine the ranking consistency among multiple candidates. When disagreements occur, cross-check protocol ensures consistency across annotators, with the final label derived from majority agreement. This protocol guarantees both the scale and quality of the training data and the strict reliability of the benchmark. A.4 MORE DETAILS ON TRAINING IMPLEMENTATION Reward Model Architecture. Our reward model, EDITREWARD, is built upon powerful pretrained Vision-Language Model (VLM) backbone, which is fully fine-tuned during training. Our main results use two backbones: Qwen2.5-VL-7B and MiMo-VL-7B. The VLM backbone is followed by Multi-Layer Perceptron (MLP) reward head. Based on our ablation studies, we use Multiple Head architecture, where separate MLP heads predict the parameters (µ, σ2) for each of the two quality dimensions independently. Training Hyperparameters and Setup. The model was trained using the AdamW optimizer with the following configuration. All experiments were conducted using PyTorch 2.5.1 and Transformers 4.56.1. Optimizer: AdamW (β1 = 0.9, β2 = 0.95) Weight Decay: 0.1 Peak Learning Rate: 2 106 LR Schedule: Cosine decay with warm-up ratio of 0.05. Training Epochs: 2 Global Batch Size: 16 (Per-GPU batch size of 2 on 8 NVIDIA A100 GPUs). Image Resolution: 448 448 pixels (aspect ratio preserved). Loss Function: Uncertainty-Aware Ranking Loss. Aggregation Strategy: Balanced Average. The 200K pairwise training samples were constructed from our expert annotations. For each source instruction, pairs of edited images were sampled, and their preference was determined by the aggregated point-wise scores. Our tie-disentanglement data augmentation strategy was applied to all qualifying tied pairs during training."
        },
        {
            "title": "Work in progress",
            "content": "Figure 5: Loss curve and Valid set Acc by using or not using Disentangling Ties via Dimensional Preference during model training. A.5 MORE DETAILS ABOUT EVALUATION We present the main experimental results in Table 2. The findings clearly demonstrate that our reward model, EDITREWARD, sets new state of the art in aligning with human preferences for instruction-guided image editing. State-of-the-Art Performance. Our best model, EDITREWARD (on MiMo-VL-7B), achieves the highest performance on three out of four benchmarks. It obtains state-of-the-art accuracy of 65.72% on GenAI-Bench, significantly surpassing the strongest proprietary competitor, GPT-5 (59.61), and the best open-source VLM, ADIEE (59.96). Similarly, on AURORA-Bench, our model scores 63.62%, demonstrating substantial margin over the next-best models, EDITREWARD (on Qwen2.5-VL-7B) at 59.50% and ADIEE at 55.56%. On ImagenHub, our models remain highly competitive with the top proprietary systems, with EDITREWARD (on Qwen2.5-VL-7B) achieving Spearman correlation of 36.18, second only to GPT-4o. Effectiveness of Reward Modeling. key insight from our results is the profound impact of our reward modeling framework itself. By comparing the base open-source VLMs to our EDITREWARD trained on them, we can quantify the performance uplift. For instance, the base Qwen2.5-VL-7BInst scores 40.48% on GenAI-Bench. After being trained with our multi-dimensional, uncertaintyaware methodology, the resulting EDITREWARD (on Qwen2.5-VL-7B) skyrockets to 63.97%a massive +23.5 point improvement. This demonstrates that our contribution is not merely the application of strong backbone, but highly effective training paradigm that dramatically enhances models alignment with human judgments. Performance on EDITREWARD-BENCH and Backbone Generalization. Our proposed benchmark, EDITREWARD-BENCH, proves to be more challenging and discerning testbed. Here, our"
        },
        {
            "title": "Work in progress",
            "content": "EDITREWARD (on MiMo-VL-7B) again achieves the top score of 38.42%, narrowly outperforming Gemini-2.5-Flash (38.02) and GPT-5 (37.81). Notably, GPT-4o, the best-performing model on ImagenHub, scores significantly lower at 28.31, confirming that EDITREWARD-BENCH effectively identifies limitations in models that other benchmarks may miss. Finally, the strong performance of EDITREWARD on both Qwen2.5-VL and the more powerful MiMo-VL backbone confirms that our reward modeling framework is robust and can effectively leverage the capabilities of stronger base models to push the state of the art even further. A.6 MORE DETAILS ABOUT APPLICATION Beyond direct evaluation, key application for powerful reward model is to improve downstream generative models through data curation. To demonstrate the practical utility of EDITREWARD, we conducted an experiment to see if it could filter large, noisy dataset to create high-quality subset for fine-tuning state-of-the-art image editing model. Experimental Setup. Our experiment uses the open-source Step1X-Edit (Liu et al., 2025b) as the base model for fine-tuning. The training data is derived from ShareGPT-4o-Image (Chen et al., 2025a), large dataset containing approximately 46,000 instruction-image pairs. We first employed EDITREWARD to score every example in this dataset. We then curated high-quality subset by selecting only the top-scoring 20,000 examples. The goal is to evaluate if fine-tuning Step1X-Edit on this smaller, curated subset yields better performance than training on the full, noisy dataset. Evaluation Metrics and Baselines. We evaluate all models on GEdit-Bench, comprehensive benchmark with English (EN) and Chinese (CN) versions, each containing full set and more challenging intersection (-I) split. Performance is measured across three axes: Semantic Consistency (G SC), which evaluates how well the edit follows the instruction; Perceptual Quality (G PQ), which assesses visual realism and aesthetics; and holistic General Overall (G O) score. For all metrics, higher is better. We compare our final model against two critical baselines to measure the impact of our data curation: Step1X-Edit: The original model without any additional fine-tuning. Step1X-Edit + ShareGPT-4o-Image: The baseline model fine-tuned on the full, unfiltered ShareGPT-4o-Image dataset. This setup allows us to directly isolate the benefit of filtering with EDITREWARD. We also compare against other leading editing models like Doubao and GPT-Image-1 to contextualize our performance. Results and Analysis. As shown in Table 3, fine-tuning Step1X-Edit on our EDITREWARD-curated subset yields substantial improvements across all benchmarks and metrics. On the GEdit-Bench-EN Overall score (G O), our model achieves 7.086, significant gain over both the original Step1X-Edit (6.444) and the model trained on the full, noisy dataset (6.780). This result is crucial: it demonstrates that training on smaller, higher-quality dataset curated by our reward model is more effective than training on the entire noisy dataset. EDITREWARD successfully identifies and filters out low-quality or misaligned examples that can harm the fine-tuning process. Furthermore, this improvement elevates the performance of the open-source Step1X-Edit to be on par with, or even superior to, strong competitors like Doubao (6.983). This experiment validates the high potential of EDITREWARD as an essential tool for data curation in the training pipelines of next-generation image editing models. In Figure x, we show how our reward model is used to score some image editing examples. A.7 MORE ABLATION EXPERIMENTS RESULTS Ablation on Data Scale and Tie Disentanglement. Next, we investigate the combined effect of increasing our training data from 130k to 200k samples while also applying our proposed tiedisentanglement strategy. The results of this significant upgrade are presented in Table 7. Comparing our baseline model (Variant I) against our final model which incorporates both changes (Variant"
        },
        {
            "title": "Work in progress",
            "content": "Table 7: Ablation study on dataset size and our tie-disentanglement strategy. Variants Ablation Setting Benchmark Performance Dataset Size Disentangling Ties GenAI-Bench AURORA-Bench (Pair) ImagenHub EDITREWARD-BENCH Direct ablation on the full dataset 130k 200k II 62.24 63.97 51.36 53.33 32.45 36.18 37.81 36. II), we observe consistent performance gains across all public benchmarks. The improvement is most pronounced on ImagenHub, where the score increases substantially from 32.45 to 36.18. We also see notable gains on GenAI-Bench (62.24 63.97) and AURORA-Bench (51.36 53.33). Interestingly, we note slight performance decrease on our proposed EDITREWARD-BENCH, suggesting it may have different sensitivities to the data distribution. Overall, these results confirm the significant benefit of our full data strategy, which combines larger, high-quality dataset with our novel technique for leveraging ambiguous tie pairs. Table 8: Bias sensitivity analysis of Gemini 2.0 Flash under left/right bias conditions on GenAIBench."
        },
        {
            "title": "Left Bias\nRight Bias",
            "content": "Bias Sensitivity (Gap) Accuracy (%) 55.28 50.16 5.11 A.8 POSITIONAL BIAS In the course of our evaluation on GenAI-Bench, we identified notable case of bias sensitivity in the Gemini 2.0 Flash model when subjected to systematic position bias. Specifically, when we artificially manipulated the ground-truth labels to favor either left-side (A>B) or right-side (B>A) preferenceswhile correspondingly swapping the image positions to maintain correctnesswe observed consistent performance discrepancy. As shown in Table 8, the model achieved 55.28% accuracy under the left-bias condition but only 50.16% under the right-bias condition, yielding 5.11% gap. This systematic difference indicates that the model exhibits positional preference for left-side comparisons, which could distort evaluation outcomes if left unaddressed. To prevent such bias from affecting comparative results, GenAI-Bench adopts randomized positioning strategy that shuffles the order of candidate images (A and B) for each comparison task. This ensures that evaluation outcomes are driven by genuine quality judgments rather than positional artifacts, thereby preserving fairness, robustness, and reliability across diverse model architectures."
        },
        {
            "title": "Work in progress",
            "content": "A."
        },
        {
            "title": "INPUT TEMPLATE FOR REWARD MODEL",
            "content": "This section provides the exact input prompt template used in all experiments to guide our reward model, EDITREWARD, in scoring the quality of an image edit."
        },
        {
            "title": "INSTRUCTION EDIT FOLLOWING TEMPLATE",
            "content": "[IMAGE] You are tasked with evaluating an edited image **in comparison with the original source image** based on **Visual Quality & Realism**, and assigning score from 1 to 4, with 1 being the worst and 4 being the best. This dimension focuses on how realistic, artifact-free, and aesthetically appealing the edited image is, while remaining consistent with the source image. **Inputs Provided:** - Source Image (before editing) - Edited Image (after applying the instruction) - Text Instruction **Sub-Dimensions to Evaluate:** - **Semantic Accuracy:** Assess whether the edited content accurately captures the semantics of the instruction. The edited result should precisely match the intended meaning. For example, if the instruction is replace apples with oranges, the object must clearly be oranges, not other fruits. - **Completeness of Editing:** Check whether **all parts** of the instruction are fully executed. For multi-step edits (e.g., replace red car with blue bicycle), both the color change and the object replacement must be done without omissions. - **Exclusivity of Edit (No Over-Editing):** Ensure that only the requested parts are changed. The rest of the image (as seen in the source) should remain unaltered. For example, if the instruction only involves replacing an object, the background, lighting, and unrelated objects should not be unnecessarily modified. **Scoring Criteria:** - **4 (Very Good):** Perfectly accurate, complete, and exclusive execution of the instruction. - **3 (Relatively Good):** Largely correct, but with minor omissions or slight over-editing. - **2 (Relatively Poor):** Major misinterpretation, changes. - **1 (Very Poor):** Instruction ignored or completely wrong execution. incomplete edits, or noticeable unintended Text instruction {text_prompt}"
        },
        {
            "title": "INSTRUCTION EDIT QUALITY TEMPLATE",
            "content": "[IMAGE] You are tasked with evaluating an edited image **in comparison with the original source image** based on **Visual Quality & Realism**, and assigning score from 1 to 4, with 1 being the worst and 4 being the best. This dimension focuses on how realistic, artifact-free, and aesthetically appealing the edited image is, while remaining consistent with the source image. **Inputs Provided:** - Source Image (before editing) - Edited Image (after applying the instruction) - Text Instruction **Sub-Dimensions to Evaluate:** - **Plausibility & Physical Consistency:** Check whether the edit aligns with the laws of physics and the scene context. Lighting, shadows, reflections, perspective, size, and interactions with the environment should all appear natural compared to the source image. - **Artifact-Free Quality:** Look for technical flaws such as blur, distortions, pixel misalignment, unnatural textures, or seams around edited regions. High-quality results should be free from such visible artifacts. - **Aesthetic Quality:** Evaluate the overall harmony and visual appeal. The image should look natural, balanced, and pleasant. Colors, composition, and atmosphere should enhance the image rather than degrade it. **Scoring Criteria:** - **4 (Very Good):** Perfectly realistic, artifact-free, seamless, and aesthetically pleasing. - **3 (Relatively Good):** Mostly realistic and clean, with only minor flaws that do not significantly distract. - **2 (Relatively Poor):** Noticeable physical inconsistencies or visible artifacts that make the edit unnatural. - **1 (Very Poor):** Severe artifacts, incoherent composition, or visually unusable result. Text instruction {text_prompt}"
        },
        {
            "title": "Full Input Template",
            "content": "[IMAGE] You are tasked with evaluating an edited image **in comparison with the original source image**, and assigning score from 1 to 8, with 1 being the worst and 8 being the best. This score should reflect **both how accurately the instruction was followed and the visual quality of the edited image**. **Inputs Provided:** - Source Image (before editing) - Edited Image (after applying the instruction) - Text Instruction **Dimension 1: Instruction Following & Semantic Fidelity** Evaluate how well the edited image follows the given instruction. Consider the following subdimensions: - **Semantic Accuracy:** Check if the edited content accurately captures the intended meaning of the instruction. For example, if the instruction is replace apples with oranges, the object must clearly be oranges, not other fruits. - **Completeness of Editing:** Verify that all aspects of the instruction are fully executed. Multi-step edits should be completely applied without omissions. - **Exclusivity of Edit (No Over-Editing):** Ensure that only the requested changes are applied; the rest of the image should remain consistent with the source image without unintended modifications. **Dimension 2: Visual Quality & Realism** Evaluate the realism, technical quality, and aesthetic appeal of the edited image. Consider the following sub-dimensions: - **Plausibility & Physical Consistency:** Check whether the edit aligns with natural laws and scene context (lighting, shadows, reflections, perspective, and object interactions). - **Artifact-Free Quality:** Assess for technical flaws such as blur, distortions, pixel misalignment, unnatural textures, or seams around edited regions. - **Aesthetic Quality:** Consider overall harmony and visual appeal. Colors, composition, atmosphere, and balance should enhance the image without degrading realism. **Scoring Criteria (18):** - **8 (Very Good):** Perfect instruction following and flawless visual quality; edits are accurate, complete, exclusive, and visually seamless. - **7 (Relatively Good):** Very good instruction following and high visual quality; minor, nondistracting flaws. - **6 (Good):** Good instruction following or mostly good visual quality; minor omissions or slight artifacts. - **5 (Moderate):** Partially correct edits or moderate visual issues; noticeable flaws but understandable. - **4 (Relatively Poor):** Significant misinterpretation, artifacts. - **3 (Poor):** Major errors in instruction following and/or poor visual quality; hard to fully understand. - **2 (Very Poor):** Very poor edits with large semantic errors and strong visual artifacts. - **1 (Failed):** Completely wrong edits or visually unusable result. incomplete edits, or noticeable visual Text instruction {text_prompt}"
        }
    ],
    "affiliations": [
        "2077AI",
        "Independent",
        "McGill University",
        "Tsinghua University",
        "University of Waterloo"
    ]
}