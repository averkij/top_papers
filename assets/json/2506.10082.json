{
    "paper_title": "LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning",
    "authors": [
        "Chenjian Gao",
        "Lihe Ding",
        "Xin Cai",
        "Zhanpeng Huang",
        "Zibin Wang",
        "Tianfan Xue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using a mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. A spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 2 8 0 0 1 . 6 0 5 2 : r LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning Chenjian Gao1, Lihe Ding1, Xin Cai1, Zhanpeng Huang2, Zibin Wang2, Tianfan Xue1 1The Chinese University of Hong Kong, 2SenseTime Research {cjgao, dl023, cx023, tfxue}@ie.cuhk.edu.hk wangzb02@gmail.com, huangzhanpeng@sensetime.com, Project Page: https://cjeen.github.io/LoraEditPaper/ Figure 1: We achieves high-quality first-frame guided video editing given reference image (top row), while maintaining flexibility for incorporating additional reference conditions (bottom row)."
        },
        {
            "title": "Abstract",
            "content": "Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using mask-driven LoRA tuning strategy that adapts pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Recent advances of diffusion models [Rombach et al., 2022, Lipman et al., 2023] has demonstrated unprecedented improvement in high-quality video generation [Yang et al., 2025b, Kong et al., 2024, Wang et al., 2025, HaCohen et al., 2024]. Based on foundation video generation model, video editing has also experienced dramatic improvement [Jiang et al., 2025, Hu et al., 2025], widely used in creative, commercial, and scientific uses these days. Still, these video editing models often require heavy-compute finetuning, with large set of training data. This makes them very expensive to extend new editing type, and less flexible to new applications. In contrast, first-frame-guided video editing [Ouyang et al., 2024, Ku et al., 2024] offers promising path toward flexible video manipulation. In this paradigm, users edit the first frame arbitrarily, either using image AI tools or traditional editing software. These edits are then propagated to the entire sequence, enabling flexible video manipulation without being constrained by dataset-specific training. While first-frame-guided solutions allows flexible editing, it provides limited control of remaining frames. For instance, given video of blooming flower, the user can edit the flower in the first frame, but cannot control how the flower blooms in the following frames. Similarly, when an object rotates to novel viewpoint, the user cannot control appearance of the disoccluded region of this object, as shown in the bottom row of Fig. 1, where the handbag rotates to new angle. In addition, the first frame edits may diffuse into unedited regions, resulting undesirable background changes. The inability to control later frames limits editing flexibility and necessitates methods that not only retain the flexibility of first-frame-guided editing, but also support control throughout the video. simple solution is per-video finetuning of pre-trained image-to-video (I2V) model [Kong et al., 2024, Wang et al., 2025]. By finetuning the model using LoRA [Hu et al., 2022] on an input video, the model will learn content motion, which can be applied to an edited first frame. This allows the edit to propagate in temporally consistent way. However, this naive approach lacks finer controlit cannot distinguish between regions that should change and those that should stay, nor does it ensure that the appearance of the edited region remains controllable as it moves and deforms over time. In this work, we build flexible video editing model by expanding this naive edit propagation approach with an additional mask, which controls which regions of the video remain unchanged and which are modified. Recent I2V models [Kong et al., 2024, Wang et al., 2025] are designed to generate videos from single image, but they can also process video sequences, with built-in masking mechanism to control which regions are preserved or modified during inference. Typically, this mask preserves the first frame while generating the subsequent frames. However, we further observe that the mask has greater potential for more precise control over video content. To leverage this, we apply LoRA to fine-tune the model on the input video with the edited region masked. This allows the model to learn how to use the mask for controlling local content generation while preserving the background. After LoRA training, the model can effectively apply the mask, ensuring that unedited areas remain unchanged during inference. More importantly, the mask-based control enables LoRA to learn flexible information from the training data, adapting to various editing conditions. For example, when the mask preserves the first frame and generates the subsequent frames, LoRA learns motion patterns from the input video, allowing for temporally consistent edits. On the other hand, with fully blank mask, LoRA can focus on learning the appearance of specific objects from single frame, providing detailed control over visual content. Our approach offers simple and effective solution for video editing by leveraging LoRAs capabilities, without modifying the model architecture, and maintaining high flexibility through the combination of different conditions. Extensive experiments demonstrate that our method achieves superior performance over previous state-of-the-art approaches in both qualitative and quantitative evaluations. Please visit our project page for video editing examples."
        },
        {
            "title": "2 Related Works",
            "content": "Video Editing with Diffusion Models The success of video diffusion models has spurred extensive research into video editing. Early works adapt the image diffusion network and training paradigm to video generation and editing. Tune-A-Video Wu et al. [2023] explores the concept of one-shot tuning in video editing. Fairy Wu et al. [2024] edits keyframes utilizing 3D spatio-temporal self-attention extended from T2I diffusion model. VidToMe Li et al. [2024] introduces image editing approaches 2 (e.g., ControlNet Zhang et al. [2023]) to video generation. Animatediff Guo et al. [2023] decouples the appearance and motion learning during video editing. SAVE Song et al. [2024] chooses to fine-tune the feature embeddings that directly reflect semantic information. Another line of work manipulates the hidden features to edit video. Video-P2P Liu et al. [2024] and Vid2Vid-Zero Wang et al. [2023] employ cross-attention map injection and null-text inversion for video editing. TokenFlow Geyer et al. [2023] leverages motion-based feature injection, and FLATTEN Cong et al. [2023] further introduces optical flow for better injection. Other methods Chen et al. [2023], Yang et al. [2023] explore latent initialization and latent transition in video diffusion models. Dragvideo Deng et al. [2024] achieves interactive drag-style video editing by introducing point conditioning. Recently, VACE Jiang et al. [2025] has shown promising video editing ability by large-scale conditional video diffusion training. Although large video editing diffusion models achieve impressive results, they often struggle with inaccurate identity preservation and suboptimal performance on out-of-domain test cases. In contrast, our method effectively leverages powerful video priors while efficiently learning content from both the reference image and the source video. First-Frame Guided Video Editing First-frame guided editing has emerged as mainstream video editing approach, with AnyV2V Ku et al. [2024] and I2VEdit Ouyang et al. [2024] as representative methods. These approaches decompose video editing into two stages: (i) editing the first frame using existing image methods, and (ii) propagating edits to remaining frames using motion-conditioned image-to-video diffusion models. AnyV2V reconstructs motion via DDIM sampling, injecting temporal attention and spatial features from the original video. I2VEdit enhances this by learning coarse motion through per-clip LoRA and refining appearance using attention difference masks. While this decoupled framework benefits from advances in both image editing and video generation, the lack of explicit constraints often leads to diluted edits during propagation, manifesting as foreground inconsistencies and background leakages."
        },
        {
            "title": "3 Method",
            "content": "In this work, we introduce controllable first-frame-guided video editing method based on recent image-to-video diffusion models [Wang et al., 2025, Kong et al., 2024]. In Sec. 3.1, we first tackle the issue of maintaining coherent motion of the edit by using LoRA to transfer motion patterns from the input video. In Sec. 3.2, we explore the generalization capabilities of the mask-based conditioning mechanism in pretrained I2V models. In Sec. 3.3, we demonstrate how mask-aware LoRA enables flexible video editing by leveraging the mask to control the generated content. 3.1 LoRAs First Step: Simple Solution for Video Editing In this section, we introduce naive approach for edit propagation, which serves as foundation for the subsequent improvements. Given an input video Vinput = [I1, I2, . . . , IT ] and an edited version of the first frame, I1, the goal is to generate an edited video Vedited = [I1, I2, . . . , IT ] where the edits introduced in I1 are propagated across all subsequent frames with coherent motion. To achieve this basic objective, we insert LoRA (Low-Rank Adaptation) [Hu et al., 2022] modules ϕθ into the self-attention and cross-attention layers of the I2V model[Wang et al., 2025] and optimize them on the input video Vinput to capture its motion pattern. During training, the model is conditioned on the original first frame I1 and textual prompt composed of fixed special token concatenated with the caption generated for I1 using Florence-2 [Xiao et al., 2024] (i.e., [p] + c). The model is supervised to reconstruct the full input video Vinput = {I1, I2, . . . , IT }. Following the denoising objective of the I2V diffusion model[Wang et al., 2025], we optimize the LoRA adapters by minimizing the error between the network noise prediction and the Gaussian noise injected into the latent video: Lnaive = Et,ϵ (cid:2)ϵθ(xt, t; I1, [p] + (cid:124) (cid:125) (cid:123)(cid:122) condition ) ϵ2 2 (cid:3), xt = Add_Noise(E(Vinput (cid:124) (cid:123)(cid:122) (cid:125) objective ), ϵ, t) where ϵ is the sampled noise, and ϵθ denotes the noise prediction network with LoRA parameters ϕθ. is the VAE encoder that maps the target video input Vinput to the latent space. 3 Figure 2: Exploring different mask configurations as an input condition to image-to-video model. Left: Input conditions, which include mask and pseudo-video. Right: video generation result under different mask configurations. From the top to bottom, we explore the four different cases. Case default: default mask used in image-to-video training, which only preserves the first frame. Case 1: it uses no input condition and the problem falls back to text-to-video generation. Case 2: use the entire video as condition without any masking, which results in artifacted videos. Case 3: use video by masking the foreground as condition, which also fails. At inference time, the original frame I1 is replaced with an edited version I1, and new caption is generated for I1 using Florence-2. The prompt token is concatenated with to form the inference prompt [p] + c, which guides the generation of the edited sequence V. 3.2 The Masks Hidden Power: Exploring I2V Model Capabilities Although naive edit propagation ensures motion coherence, it lacks control over the content of subsequent frames. To address this, we leverage the conditioning mechanisms in recent I2V models [Wang et al., 2025, Kong et al., 2024]. To introduce the first frame as the guidance for video generation, these models incorporate two additional conditions for the denoising network: pseudo-video Vcondition and binary spatiotemporal mask Mcondition. The pseudo-video Vcondition RCT HW is constructed by concatenating the first frame RC1HW with zero-placeholder frames. The binary mask Mcondition {0, 1}1T hw is designed so that 1 indicates the preserved frame and 0 represents the frames to be generated, with the first frame set to 1 and all subsequent frames set to 0. This paradigm can be extended to video-to-video generation by replacing the pseudo-video condition Vcondition with actual video frames, enabling the model to accept an entire video sequence as input. In this setting, the binary spatiotemporal mask Mcondition, originally designed to preserve only the first frame, can now be repurposed as more flexible mechanism that selectively controls which regions are retained and which are regenerated across space and time. To assess the generalization capabilities of the masking mechanism, we evaluate several binary mask configurations. In each case, the mask Mcondition is applied to the input video to construct Vcondition, where regions marked as zero are regenerated and the rest are preserved. Default I2V Configuration. In this configuration, the I2V model preserves the first frame and generates the remaining frames based on the first frame. As shown in the first row of Fig. 2, this default mask setup leads the model to synthesize motion across the entire sequence. Case #1: No Preservation. In this case, the model is configured with an all-zeros mask across all frames, meaning none of the original video content is preserved. As shown in the second row of Fig. 2, this setup can force the model to generate the appearance for the entire video. 4 Case #2: All Preservation. In this case, the mask preserves the content from the input video while setting the generated frames to zero, ensuring that the original video sequence is maintained in the preserved regions. As shown in the third row of Fig. 2, although the overall structure of the original video content is effectively preserved, artifacts appear in areas with discontinuous motion. Inspired by this, further LoRA fine-tuning could potentially improve its ability to preserve input video. Case #3: Selective Preservation. In this case, spatially varying mask is used to preserve the background while allowing the foreground to be generated. As shown in the fourth row of Fig. 2, the model struggles to synthesize coherent foreground content. This suggests that the pretrained I2V model lacks inpainting capabilities, likely due to limited exposure to spatially varying masks during training. More importantly, the model has difficulty handling the interaction between edits and the background, which we aim to improve by fine-tuning on the input video using LoRA. 3.3 Unlocking Editing Flexibility: Mask-Guided LoRA Building on this exploration, we modify the spatiotemporal mask to enable more flexible video edits. Combined with LoRA fine-tuning, the mask serves two complementary roles. First, it improves the I2V models alignment with mask constraints, allowing flexible control over which regions are edited or preserved. Second, it acts as signal guiding LoRA to learn specific patterns from the training data, such as motion from video sequences or appearance from images. Specifically, we modify the training loss in Eq. 3.1 to introduce the conditioning video and mask: = Et,ϵ (cid:2)ϵθ(xt, t; Vcondition, Mcondition, [p] + ) ϵ2 2 (cid:125) (cid:124) (cid:123)(cid:122) condition (cid:3), xt = Add_Noise(E(Vtarget (cid:124) (cid:123)(cid:122) (cid:125) objective ), ϵ, t) As shown in Fig. 3, by configuring Vcondition, Mcondition, and Vtarget in different ways, we enable flexible video editing through LoRA, which will be described in detail in the following sections. 3.3.1 Disentangling Edits and Background. Many first-frame edits alter only part of the frame, yet they impose two contrasting demands: the edited region must keep evolving, while the untouched background should stay locked. When both demands run through the same generative pathway, they collidethe drive to preserve the background can freeze the edit, while the drive to propagate the edit can leak changes into the background. To effectively disentangle edited regions from the background, we carefully adjust the spatiotemporal mask Mcondition and the conditioning video Vcondition during LoRA fine-tuning. The mask Mcondition is set to ones for the first frame to preserve it as the reference, and for subsequent frames, Mcondition is adjusted to mark unedited regions with ones (to be preserved) and edited regions with zeros (to be generated). The pseudo-video Vcondition is created by applying the mask to the input video, setting the regions marked as zero in Mcondition to be empty, while leaving the rest unchanged. The objective Figure 3: Mask configuration for LoRA training and inference. 5 Vtarget is set to the input video during LoRA fine-tuning. This configuration allows the model to focus on generating the edited content while locking the unedited regions. At inference time, when editing the first frame (replacing I1 with I1), we use the same Mcondition as during LoRA training, while Vcondition has its first frame replaced by the edited version I1. While the pre-trained I2V model struggles with selective preservation, LoRA on single video alone surprisingly enables the model to learn effective mask-guided inpainting priors. We speculate this is due to the diffusion transformer processing inputs as discretized tokens, with spatially varying mask sharing similar token-level representation, making the adaptation not very difficult. 3.3.2 Appearance Control in Propagated Edits. An edit in the first frame rarely stays static: the modified region may rotate, deform, or follow its own motion trajectory (e.g., petals unfolding as flower blooms). To make the subsequent frames look natural, the model has to infer how the edited region should appear under these evolving viewpoints and states. When the only constraint is the first frame itself, this inference is under-specified, and the edit drifts away from the users intent. To address this, we allow users to edit any subsequent frame, providing direct guidance for how the appearance should look at specific points in time. During LoRA fine-tuning, we use an edited frame as the target Vtarget. The conditioning input Vcondition is constructed using the pre-edited frame by masking out the edited regions. The mask Mcondition marks the preserved background areas with ones and the edited regions with zeros. If multiple frames are involved, each frame is treated as separate sample to avoid including motion information. This configuration allows the model to learn how edited content should appear in context, guided by both the surrounding background and the user-provided modification. Unlike methods that directly feed edited frames as inputs during inference [Yang et al., 2025a, Jamriska, 2018], we do not require the edited frame to remain exactly the same during inference. Instead, the edited frame is used only during training to guide how edits should appear. At inference time, the model generates content based on learned patterns and context, allowing it to adapt edits smoothly across frames, even when the edits do not adhere to strict temporal alignment."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details We conduct our experiments using videos consisting of 49 frames, with resolution of either 832480 or 480 832. All main results are obtained using the Wan2.1-I2V 480P model. Additional results based on HunyuanVideo-I2V are included in Sec. 4.4. Our framework is built upon the publicly available diffusion-pipe codebase1. For each video editing sample, we begin by training on the input video for 100 steps as described in Sec. 3.3.1. If additional edits are applied to later frames, we continue training for another 100 steps on data that includes additional modifications as described in Sec. 3.3.2. This helps the model incorporate user-specified appearance changes. We use learning rate of 1 104 for all experiments. Training on 49-frame videos typically requires 20 GB of GPU memory. In Sec. 4.5, we describe memory optimization strategy that reduces GPU requirements. 4.2 Comparison with State-of-the-Arts Comparison with Reference-Guided Video Editing. We compare our method with two recent reference-guided video editing approaches: Kling1.6 [KlingAI, 2025] and VACE [Jiang et al., 2025]. To evaluate the performance on reference-guided video editing, we collect 20 high-quality video clips from Pexels and YouTube. Each video is paired with reference image representing the desired edit. We use ACE++ [Mao et al., 2025] to apply the edit to the first frame for our method. Figure 4 shows visual comparison results. Compared to Kling1.6 and VACE, our method better respects the intended appearance in the edited region while preserving background content and temporal consistency. Comparison with First-Frame-Guided Video Editing. We further compare our method with recent first-frame-guided video editing approaches, including I2VEdit [Ouyang et al., 2024], Go1https://github.com/tdrussell/diffusion-pipe 6 Figure 4: Comparisons with state-of-the-art reference-guided video editing methods. with-the-Flow [Burgert et al., 2025], and AnyV2V [Ku et al., 2024]. All baselines take the edited first frame as input and attempt to propagate the edits through the entire sequence. To ensure fair and consistent evaluation, we adopt the test set from I2VEdit, which contains videos from diverse sources along with paired first-frame edits. Figure 5 shows qualitative results. In the portrait example (left), our method accurately adds the necklace while preserving the facial structure, while baseline methods often distort the face or produce artifacts. In the street scene (right), our approach transfers the clothing style cleanly across frames without affecting the background, whereas baseline methods distort the clothing or introduce changes in unedited areas. Quantitative Results. For quantitative evaluation on first-frame-guided video editing, we use three metrics: 1) DeQA Score You et al. [2025], state-of-the-art method for assessing image quality; 2) CLIP Score, which measures the semantic alignment between generated frames and edited first frame by comparing their CLIP Radford et al. [2021] embedding similarity; and 3) Input Similarity, which computes the CLIP embedding similarity between the generated frames and the input frames on perframe basis. As shown in Tab. 1, our method outperforms others across all metrics. For quantitative evaluation on reference-guided video editing, we conducted user study with 35 participants. Each participant was randomly shown 10 groups of results generated by different methods. For each Figure 5: Comparisons with state-of-the-art first-frame-guided video editing methods. group, the participants were asked to rank the results based on motion consistency and background preservation. Tab. 2 demonstrates the superiority of our method in both aspects. Table 1: Quantitative comparison with firstframe-guided video editing. Table 2: Average user ranking results for comparison with reference-guided video editing. CLIP Score DEQA Score Input Similarity AnyV2V 0.8995 Go-with-the-Flow 0.9047 0.9128 I2VEdit 0.9172 Ours 3.7348 3.5622 3.4480 3. 0.7569 0.7504 0.7536 0.7608 Motion Consistency Background Preservation Kling1.6 VACE (14B) Ours 1.869 2.511 1.620 1.806 2.460 1.734 4.3 Ablation Studies Disentangling Edits and Background. To validate the effectiveness of spatial conditioning in separating edited regions from preserved content, we conduct an ablation study comparing our method with and without spatial condition inputs. Figure 6 shows the results. On the left, the goal is to apply hair color change. Without spatial conditioning, the edit is applied globally, altering the lighting across the frame. In contrast, with spatial conditioning, the model localizes the change to the hair region while leaving the background untouched. similar effect is observed in the right example, where clothing edits are confined to the target area only when spatial condition is used. Appearance Control in Propagated Edits. We conduct an ablation to evaluate the impact of using edited frames beyond the first frame for controlling appearance in edits propagation. Figure 7 compares two settings: using only the first frame as input versus adding an edited frame at later timestep. While using only the first frame can still generate reasonable results, incorporating an Figure 6: Ablation results of disentangling edits and background. Figure 7: Ablation results of incorporating additional reference. additional edited frame offers stronger control over the appearance, leading to more consistent and accurate propagation of the intended edit. 4.4 Results based on HunyuanVideo-I2V Model In addition to the main results based on the Wan2.1-I2V 480P [Wang et al., 2025] model, we also conducted experiments using the HunyuanVideo-I2V [Kong et al., 2024], with resolution of 624 368. Figure 8 presents the results obtained with the HunyuanVideo-I2V model, which performs well in generating temporally consistent edits while preserving the background content. Although the HunyuanVideo-I2V models performance is not as strong as Wan2.1-I2V, it still demonstrates the effectiveness of our method across different models. Figure 8: Results of our method applied to Wan2.1-I2V and HunyuanVideo-I2V 4.5 Low-Cost Training Strategy Figure 9: Comparison between the original and low-cost versions of our approach. Training on 49-frame video can be challenging due to the high GPU memory requirements. To address this, we propose memory-efficient strategy by splitting the video into smaller overlapping clips. Specifically, the 49 frames are divided into four clips: (1) frames 1 to 13, (2) frames 13 to 25, (3) frames 25 to 37, and (4) frames 37 to 49. Each clip contains 13 frames, significantly reducing the VRAM load during training. The results of this approach are shown in Figure 9. While this method helps reduce memory usage, it introduces some artifacts such as vertical stripes. However, for scenes with minimal motion, like the example on the right, these artifacts are barely noticeable. This approach offers practical solution for training on consumer-grade hardware, with performance similar to training on the entire 49-frame video."
        },
        {
            "title": "5 Conclusion and Limitation",
            "content": "In this work, we present controllable first-frame-guided video editing framework leveraging maskaware LoRA fine-tuning to achieve flexible, high-quality, and region-specific video edits without modifying the underlying model architecture. Our method enables fine-grained control over both foreground and background, supports propagation of complex edits across frames, and allows for additional appearance guidance through reference images. Experiments demonstrate that our approach outperforms existing state-of-the-art methods in both qualitative and quantitative evaluations, while maintaining temporal consistency and background preservation. 10 Despite these advantages, key limitation is that LoRA fine-tuning requires substantial computational resources and training time. While this is acceptable for most high-precision video editing applications, it may limit accessibility in resource-constrained environments. Future work will focus on developing more efficient LoRA optimization strategies to address this challenge."
        },
        {
            "title": "References",
            "content": "R. Burgert, Y. Xu, W. Xian, O. Pilarski, P. Clausen, M. He, L. Ma, Y. Deng, L. Li, M. Mousavi, et al. Go-with-the-flow: Motion-controllable video diffusion models using real-time warped noise. arXiv preprint arXiv:2501.08331, 2025. W. Chen, Y. Ji, J. Wu, H. Wu, P. Xie, J. Li, X. Xia, X. Xiao, and L. Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv e-prints, pages arXiv2305, 2023. Y. Cong, M. Xu, C. Simon, S. Chen, J. Ren, Y. Xie, J.-M. Perez-Rua, B. Rosenhahn, T. Xiang, and S. He. Flatten: optical flow-guided attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023. Y. Deng, R. Wang, Y. Zhang, Y.-W. Tai, and C.-K. Tang. Dragvideo: Interactive drag-style video editing. In European Conference on Computer Vision, pages 183199. Springer, 2024. M. Geyer, O. Bar-Tal, S. Bagon, and T. Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023. Y. Guo, C. Yang, A. Rao, Z. Liang, Y. Wang, Y. Qiao, M. Agrawala, D. Lin, and B. Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Y. HaCohen, N. Chiprut, B. Brazowski, D. Shalem, D. Moshe, E. Richardson, E. Levin, G. Shiran, N. Zabari, O. Gordon, P. Panet, S. Weissbuch, V. Kulikov, Y. Bitterman, Z. Melumian, and O. Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. T. Hu, Z. Yu, Z. Zhou, S. Liang, Y. Zhou, Q. Lin, and Q. Lu. Hunyuancustom: multimodaldriven architecture for customized video generation, 2025. URL https://arxiv.org/abs/ 2505.04512. O. Jamriska. Ebsynth: Fast example-based image synthesis and style transfer. https://github. com/jamriska/ebsynth, 2018. Z. Jiang, Z. Han, C. Mao, J. Zhang, Y. Pan, and Y. Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. KlingAI. Klingai. Website, 2025. https://klingai.com/. W. Kong, Q. Tian, Z. Zhang, R. Min, Z. Dai, J. Zhou, J. Xiong, X. Li, B. Wu, J. Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. M. Ku, C. Wei, W. Ren, H. Yang, and W. Chen. Anyv2v: tuning-free framework for any video-tovideo editing tasks. arXiv preprint arXiv:2403.14468, 2024. X. Li, C. Ma, X. Yang, and M.-H. Yang. Vidtome: Video token merging for zero-shot video editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74867495, 2024. Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. 11 S. Liu, Y. Zhang, W. Li, Z. Lin, and J. Jia. Video-p2p: Video editing with cross-attention control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85998608, 2024. C. Mao, J. Zhang, Y. Pan, Z. Jiang, Z. Han, Y. Liu, and J. Zhou. Ace++: Instruction-based image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. W. Ouyang, Y. Dong, L. Yang, J. Si, and X. Pan. I2vedit: First-frame-guided video editing via image-to-video diffusion models. arXiv preprint arXiv:2405.16537, 2024. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Y. Song, W. Shin, J. Lee, J. Kim, and N. Kwak. Save: Protagonist diversification with tructure gnostic ideo diting. In European Conference on Computer Vision, pages 4157. Springer, 2024. A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang, J. Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. W. Wang, Y. Jiang, K. Xie, Z. Liu, H. Chen, Y. Cao, X. Wang, and C. Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599, 2023. B. Wu, C.-Y. Chuang, X. Wang, Y. Jia, K. Krishnakumar, T. Xiao, F. Liang, L. Yu, and P. Vajda. Fairy: Fast parallelized instruction-guided video-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82618270, 2024. J. Z. Wu, Y. Ge, X. Wang, S. W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou. Tunea-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. B. Xiao, H. Wu, W. Xu, X. Dai, H. Hu, Y. Lu, M. Zeng, C. Liu, and L. Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48184829, 2024. S. Yang, Y. Zhou, Z. Liu, and C. C. Loy. Rerender video: Zero-shot text-guided video-to-video translation. In SIGGRAPH Asia 2023 Conference Papers, pages 111, 2023. S. Yang, Z. Gu, L. Hou, X. Tao, P. Wan, X. Chen, and J. Liao. Mtv-inpaint: Multi-task long video inpainting. arXiv preprint arXiv:2503.11412, 2025a. Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng, D. Yin, Yuxuan.Zhang, W. Wang, Y. Cheng, B. Xu, X. Gu, Y. Dong, and J. Tang. Cogvideox: Text-to-video In The Thirteenth International Conference on diffusion models with an expert transformer. Learning Representations, 2025b. URL https://openreview.net/forum?id=LQzN6TRFg9. Z. You, X. Cai, J. Gu, T. Xue, and C. Dong. Teaching large language models to regress accurate image quality scores using score distribution. In IEEE Conference on Computer Vision and Pattern Recognition, 2025. L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023."
        }
    ],
    "affiliations": [
        "SenseTime Research",
        "The Chinese University of Hong Kong"
    ]
}