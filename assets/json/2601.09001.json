{
    "paper_title": "Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM",
    "authors": [
        "Pedro Memoli Buffa",
        "Luciano Del Corro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all \"10 choose k\" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition."
        },
        {
            "title": "Start",
            "content": "Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM"
        },
        {
            "title": "Pedro Memoli Buffa",
            "content": "Departamento de Matematica, FCEyN Universidad de Buenos Aires Buenos Aires, Argentina pedromemolibuffa@uba.ar Luciano Del Corro ELIAS Lab, Departamento de Ingenierá Universidad de San Andres Victoria, Argentina delcorrol@udesa.edu.ar 6 2 0 2 3 1 ] . [ 1 1 0 0 9 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Deploying LLMs raises two coupled challenges: (1) monitoringestimating where model underperforms as traffic and domains driftand (2) improvementprioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an outputentropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. lightweight classifier predicts instance correctness, and averaging predicted probabilities yields domainlevel accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k {1, 2, 3, 4}; all (cid:1) combinations), across nine LLMs from (cid:0)10 six families (3B20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition."
        },
        {
            "title": "Introduction",
            "content": "Deployed LLMs serve heterogeneous traffic that shifts over time. Yet practitioners still lack scalable answers to two tightly coupled questions: where is the model underperforming on current usage, and what data should we acquire to improve those weak spots? In practice, both problems are addressed with manually curated benchmarks and periodic human-labeled evaluations. While effective, this workflow is expensive, slow, and poorly matched to production: coverage across domains and difficulty regimes is incomplete, and it is hard to evaluate continuously at the granularity practitioners care about (e.g., per traffic slice, customer segment, or topic cluster). As result, teams often discover failures late and collect training data opportunistically rather than targeting the largest performance gaps. 1 Figure 1: Entropy-based accuracy estimation for PHI3.5-MINI-3.6B. Trained on two benchmarks (orange), the estimator generalizes to eight unseen STEM benchmarks (blue) natural alternative is to rely on signals already produced during inference. If an inexpensive uncertainty trace could robustly predict correctness, then we could estimate accuracy for slices of production traffic without repeated labeling and use those estimates to prioritize data collection toward the lowest-accuracy domains. For this to be useful in deployment, the signal must be (i) cheap to extract at scale, (ii) available for both open and closed models (e.g., via top-k log-probabilities), and (iii) robust to domain shift, so that supervision from small set of labeled tasks transfers to unseen ones. Crucially, monitoring requires quantity in accuracy unitsnot only relative uncertainty score. Entropyand logprob-based traces carry meaningful uncertainty signals (Malinin and Gales, 2021; Kuhn et al., 2023; Bouchard et al., 2025a; Kadavath et al., 2022), but raw scores are typically not directly interpretable as accuracy and can vary in scale across models and domains. We therefore cast monitoring as predicting correctness probabilities from decoding traces and aggregating them into domain-level accuracy estimates, yielding outputs that are directly actionable. We study whether these noisy but informative signals can be turned into accuracy estimates. Using only top-k decoding log-probabilities, we summarize each generations entropy trajectory into compact feature vector and train lightweight probabilistic correctness predictor. Averaging predicted correctness probabilities over slice yields an accuracy estimate, enabling simple deployment primitive: monitor slice accuracy from logs and rank slices to prioritize data acquisition. We evaluate in controlled STEM reasoning setting with ten benchmarks spanning elementary math, advanced math, and science. To stress-test robustness under domain shift, we exhaustively vary supervision: for each {1, 2, 3, 4} we train on (cid:1) benchmark subsets and estimate accuracy all (cid:0)10 on the remaining 10 k. We repeat this across nine LLMs from six families (3B20B) and multiple estimator variants (classifier family, calibration, class balancing), totaling > 41,000 configurations. The resulting estimates track held-out benchmark accuracy closely and achieve high rank agreement (Figure 1); we report both ranking quality (Spearman ρ) and accuracy-level fit (AEE). Reliability varies across models and should be validated for the target serving model before deployment.1 Overall, our results suggest that lightweight classifiers trained on entropy profiles provide practical signal that can support both continuous monitoring and targeted data acquisition. We intentionally establish this in STEM setting with verifiable correctness, enabling thorough robustness study under fixed protocolexhaustive train/test benchmark compositions, multiple estimator variants, and multiple model families. Although our experiments use open-weight models, we restrict ourselves to top-k decoding logprob signals (top-20) exposed by common model-serving APIs, keeping the approach compatible with both open and closed deployments. Extending to less verifiable, openended domains and validating transfer to closed commercial models are natural next steps."
        },
        {
            "title": "2 From Signatures to Accuracy Estimates",
            "content": "Building on evidence that token-probability traces carry information about response correctness (Malinin and Gales, 2021; Kuhn et al., 2023; Ali et al., 2025; Bouchard et al., 2025a), we study whether cheap signal available at inference time can support domain-level performance monitoring in STEM reasoning. We summarize the models output uncertainty signature as the sequence of token-level entropies computed from next-token probabilities during generation. Entropy from top-k log-probabilities. For an input prompt q, let the model generate an output ˆy = (y1, . . . , yT ). At decoding step t, let p(t)() denote the next-token distribution conditioned on the prompt and previously generated tokens (q, y<t). Many APIs expose only top-k next-token probabilities at each step. We therefore approximate entropy by truncating the sum to the top-k tokens: (t) = (cid:80) iTop-k p(t) , which differs from the true Shannon entropy because it omits the probability mass outside the Top-k set. We use (t) as an uncertainty signal over the generated output tokens. From instance correctness to domain accuracy. Given response = (q, ˆy), we extract feature vector from its entropy trajectory by summarizing { (t)}T t=1 with small set of statistics (Sec. 3), and train probabilistic classifier that outputs an estimated probability of correctness ˆP (x) [0, 1]. For domain (or slice) represented by set of instances XD, we estimate its accuracy by averaging predicted correctness probabilities: log p(t) ˆA(D) = 1 XD (cid:88) ˆP (x). xXD (1) If ˆP (x) is well-calibrated, then ˆA(D) is consistent estimator of the true domain accuracy, making it suitable for continuous monitoring over production traffic partitioned into slices. Design requirements. For practical monitoring, the signal and estimator should be: (i) computationally lightweight (single-pass, logprob-only features), (ii) API-compatible (requiring only topk token probabilities, not hidden states), and (iii) robust to domain shift (trained on one labeled source benchmark and applied to unseen domains). Accordingly, we focus on final-layer output probabilities logged during standard decoding and evaluate generalization by training on single source benchmark and estimating accuracy across diverse unseen STEM reasoning benchmarks."
        },
        {
            "title": "3 Entropy Profile Signals",
            "content": "1Code for reproducing results can be found on this anonymous repository. To satisfy requirements (i) efficiency and (ii) API compatibility, we restrict ourselves to uncertainty 2 signals available from standard decoding logs. Concretely, we ask whether the output-entropy trajectory contains enough signal to discriminate correct from incorrect responses using only compact summaries. We approximate entropy from top-k logprobabilities, setting k=20 to match the maximum exposed by commercial APIs. Do simple summaries carry correctness signal? For each response, we compute an entropy trajectory over decoding steps, { (t)}T t=1. Following standard evaluation practice in uncertainty quantification (Malinin and Gales, 2021; Kuhn et al., 2023; Bouchard et al., 2025a; Kadavath et al., 2022), we test whether single-number summaries of this trajectory already discriminate correct from incorrect outputs. Concretely, for each summary statistic s({ (t)}), we treat larger values as higher score for incorrectness and report its AUROC. For context, we also include white-box uncertainty baselines computed from token log-probabilities (SEA, NLLavg, NLLmax, NLLsum, LNTP, MTP, and PPL), defined in Appendix A. Table 1, shows those uncertainity metrics for 3 representative datasets and models (more models in appendix B). The vast majority of summary statistics achieve AUROC > 0.5 across model benchmark pairs, indicating non-trivial separability between the entropy distributions of correct and incorrect generations (Figure 2). Values tend to be high for all the other logprobs metrics. As already shown in previous work, this pattern reflects that incorrect responses tend to exhibit higher entropy (lower confidence), causing many of these statistics to behave as natural uncertainty scores where simple thresholding yields better-than-chance discrimination. However, no single summary is reliably best: the top feature shifts across both models and benchmarks  (Table 1)  . This variability suggests that different aspects of the entropy profile capture complementary uncertainty cues, and that single scalar is unlikely to serve as universal monitoring signal. For example, lower-tail quantiles (Q10/Q25) are most predictive on MATH, consistent with sustained uncertainty during long derivations, whereas dispersion (Std) is more informative on GSM8K. Simple extremes and accumulations are also strong: the entropy accumulation score (SEA) and NLLsum consistently rank among the best baselines, and Max entropy is competitive across several models (e.g., SEA=0.8762 for MINISTRAL-3 8B on MATH; Max=0.8137 for GPT-OSS 20B), suggestStatistic MATH GSM8K OLYMP. PHI-3.5-MINI 3.6B Mean Std Dev Max Q10 Q25 Q50 Q75 Q90 Skewness Kurtosis SEA NLLavg NLLmax NLLsum LNTP MTP PPL Mean Std Dev Max Q10 Q25 Q50 Q75 Q90 Skewness Kurtosis SEA NLLavg NLLmax NLLsum LNTP MTP PPL 0.7283 0.7635 0.7952 0.8248 0.8084 0.7357 0.6939 0.7130 0.5961 0.5726 0.8184 0.6983 0.6937 0.8087 0.6983 0.6937 0.6983 0.7391 0.7397 0.7074 0.7102 0.7318 0.7412 0.7341 0.7257 0.6902 0.6722 0.7649 0.7180 0.6751 0.7497 0.7180 0.6751 0.7180 MINISTRAL-3 8B 0.7654 0.7897 0.7959 0.8034 0.7933 0.7578 0.7410 0.7598 0.7094 0.7053 0.8762 0.6660 0.5666 0.8742 0.6660 0.5666 0.6660 0.7974 0.7970 0.7681 0.7746 0.7789 0.7968 0.7923 0.7891 0.7596 0.7441 0.7930 0.7086 0.5233 0.7538 0.7086 0.5233 0.7086 GPT-OSS 20B 0.7269 0.7456 0.7778 0.7589 0.7323 0.7028 0.7027 0.7280 0.6603 0.6497 0.7958 0.7188 0.6768 0.7908 0.7188 0.6768 0.7188 0.7981 0.7903 0.7712 0.7943 0.8033 0.8108 0.7988 0.7897 0.7906 0.7855 0.8559 0.7690 0.6147 0.8600 0.7690 0.6147 0.7690 0.7736 0.7771 0.8137 0.7293 0.7449 0.7454 0.7702 0.7843 0.7676 0.7749 0.9137 0.4127 0.5351 0.9070 0.4127 0.5351 0. Mean 0.8583 Std Dev 0.8498 Max 0.7405 Q10 0.8479 Q25 0.8437 Q50 0.8421 Q75 0.8600 Q90 0.8643 Skewness 0.8558 Kurtosis 0.8607 0.8953 SEA NLLavg 0.7641 NLLmax 0.4042 NLLsum 0.8869 LNTP 0.7641 MTP 0.4042 PPL 0.7641 Table 1: AUROC of entropy-profile summary stats across 3 benchmarks and 4 models (reporting 1 AUROC for skewness, kurtosis, LNTP, and MTP). 0.7445 0.7472 0.6991 0.7243 0.7177 0.7233 0.7476 0.7483 0.7448 0.7506 0.7886 0.3333 0.5206 0.7864 0.3333 0.5206 0.3333 ing that total uncertainty mass and peak uncertainty events are particularly diagnostic of errors. Higher-order moments (skewness, kurtosis) help in some settings (e.g., GPT-OSS on OlympiadBench; 0.8558/0.8607) but collapse toward chance in others (e.g., GEMMA-3 12B on MATH). Overall, relying on any single metric risks missing domainand model-specific failure modes, motivating our use of compact multi-statistic profile. Overall, these entropyand logprob-derived metrics provide real correctness signal: many separate correct from incorrect generations well above chance, and some even track domain difficulty via aggregated scores. However, taken in isolation they remain uncertainty scores, not accuracy estimates. Their scales are modeland domain-dependent, they are not calibrated to probabilities, and the best3 performing statistic can change with the benchmark and model family. As result, threshold choices and score magnitudes are hard to interpret operationally (e.g., an increase from 1.2 to 1.5 does not translate into accuracy dropped by 8 points). This motivates the next step: rather than selecting single metric and treating it as performance, we learn probabilistic mapping from compact entropy-profile vector to instance-level correctness probabilities, which can then be averaged to produce slice-level accuracy estimates in the units required for monitoring. Figure 2: Max-entropy density for PHI-3.5-MINI on MATH (correct vs. incorrect). Incorrect responses shift to higher entropy, indicating greater uncertainty. compact discriminative signature. Motivated by these observations, we avoid committing to single best scalar. Instead, we encode each response with fixed 11D entropy-profile vector capturing central tendency/dispersion (max, mean, std), distributional tails (Q10Q90), shape (skewness, kurtosis), and accumulation (SEA). lightweight probabilistic classifier can then learn which aspects matter for correctness in given setting (Sec. 4), without brittle manual feature selection. We restrict features to entropy-trajectory summaries to keep the representation compact, motivated by recent work (Ali et al., 2025) that demostrated that entropy signatures by themselves can achieve remarkable performance regarding incorrect answer detection."
        },
        {
            "title": "Profile Features",
            "content": "Feature representation. Given prompt and generated response ˆy = (y1, . . . , yT ), we compute the output-entropy trajectory { (t)}T t=1 from next-token probabilities logged during decoding, approximated with the top 20 logprobs (Sec. 2, 3). We compress this trajectory into an eleven-dimensional feature vector hx R11 for each instance = (q, ˆy), where hx = [Hmax, Hmean, Hstd, HQ10, HQ25, HQ50, 4 HQ75, HQ90, Hskew, Hkurt, HSEA]. Each statistic is computed over { (t)}T t=1: Hmax, Hmean, and Hstd capture peak, central tendency, and dispersion; HQ10, HQ25, HQ50, HQ75, and HQ90 are the 10th, 25th, 50th, 75th, and 90th percentiles; Hskew and Hkurt measure skewness and kurtosis; while HSEA measures the raw sum of the entropy trajectory. Probabilistic model. We map entropy features to an estimated probability of correctness with lightweight probabilistic model : R11 [0, 1], defining ˆP (x) = (hx), where ˆP (x) estimates the probability response is correct. From instance probabilities to domain-level accuracy. Given per-instance correctness probabilities ˆP (x), we estimate accuracy on target domain (or slice) by averaging ˆP (x) over instances in (Eq. 1). This estimator requires only decoding-time log-probabilities at test time."
        },
        {
            "title": "5 Experimental Setup",
            "content": "We test whether compact entropy-profile features support domain-level accuracy estimation under domain shift: train an instance-level correctness predictor on small set of benchmarks, then estimate accuracy on unseen benchmarks by aggregating predicted correctness probabilities. Benchmarks. We evaluate on ten STEM reasoning benchmarks spanning math and science: GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), GSM-Symbolic (Mirzadeh et al., 2025), MATH (Hendrycks et al., 2021), TheoremQA (Chen et al., 2023), SciBench (Wang et al., 2023), MatSciBench (Zhang et al., 2025b), OlympiadBench (He et al., 2024), LiveMathBench (Anonymous, 2025), and GPQA (Rein et al., 2023). All tasks use zero-shot chain-of-thought prompting with free-form final answers. For benchmarks originally multiple-choice (GPQA, SciBench), we remove answer options from the prompt. For OlympiadBench we restrict to text-only math and physics questions; for LiveMathBench we use the v202505_all_en subset.2 When split exists, we evaluate on the test portion; otherwise we evaluate on the full benchmark. Instance labeling and prediction target. Each benchmark instance provides question q, reference answer y, and model-generated response ˆy. We extract the models final answer from ˆy using benchmark-specific post-processing (e.g., stripping formatting and selecting the last boxed/numeric ex2https://huggingface.co/datasets/opencompass/"
        },
        {
            "title": "LiveMathBench",
            "content": "Extremes Intermediate Training Benchmarks (k) Model AEE ρ AEE ρ Model 1 2 4 PHI-3.5-MINI (3.6B) MINISTRAL3 (3B) MINISTRAL3 (8B) QWEN3 (4B) QWEN3 (8B) GEMMA3 (4B) GEMMA3 (12B) LLAMA-3.1 (8B) GPT-OSS (20B) 0.03 0.06 0.07 0.08 0.12 0.09 0.08 0. 0.15 1.00 0.96 0.96 0.95 0.76 0.94 0.92 0. 0.90 0.06 0.11 0.12 0.11 0.17 0.14 0.12 0. 0.16 0.95 0.79 0.89 0.94 0.75 0.92 0.85 0. 0.89 2: two accuracy Cross-domain Table estimation priori setsExtremes with Intermediate (GSM8K+OlympiadBench) (MATH+SciBench)using calibrated, class-balanced random forest. ρ: Spearman correlation. training and pression when applicable). An external validator LLM (GROK-4.1-FAST-REASONING (xAI, 2025)) receives (q, ˆyfinal, y) and outputs binary label {0, 1} indicating whether the final answer matches the reference; manual audit of 100 randomly sampled instances yielded 97% agreement with human judgment. We treat as supervision for our probabilistic correctness predictor. Models. We evaluate nine LLMs (3B-20Bsix families) , and restrict features to top-20 decoding logprobs, matching common interface constraint in logprob-returning serving stacks. We run the full pipeline separately for Ministral-3 3B (Mistral AI, 2025), Phi-3.5-Mini 3.8B (Microsoft, 2024), Qwen-3 4B (Qwen Team, 2025), Gemma-3 4B (Google DeepMind, 2025), Qwen-3 8B, Ministral3 8B, Llama-3.1 8B (Meta AI, 2024), Gemma-3 12B, and GPT-OSS 20B (OpenAI, 2025). Train/test sweep for domain shift. To avoid conclusions tied to single split, we vary which benchmarks provide supervision. For each {1, 2, 3, 4} and each benchmark subset of size (cid:1) = 385 groups), we train (in total (cid:80)4 correctness predictor on instances from and evaluate accuracy estimation on the remaining 10 benchmarks. This yields an OOD setting where all test domains are disjoint from the supervision set. Estimators and ablations. We evaluate three classifiers on the 11D entropy-profile features: logistic regression with ℓ1 regularization, random forest, and multilayer perceptron (MLP). For each classifier, we select hyperparameters via crossvalidated grid search on the training group, optimizing ROC-AUC. We also vary two training choices: class balancing (on/off) and isotonic calibration (on/off). Across 9 models, 385 groups, 3 classifier (cid:0)10 k=1 PHI-3.5-MINI (3.6B) MINISTRAL3 (3B) MINISTRAL3 (8B) QWEN3 (4B) QWEN3 (8B) GEMMA3 (4B) GEMMA3 (12B) LLAMA-3.1 (8B) GPT-OSS (20B) .20.11 .24.09 .20.10 .21.08 .27.06 .26.12 .23. .23.09 .22.05 .09.07 .12.07 .13.07 .12.06 .21.05 .18.08 .15. .14.07 .19.06 .07.04 .10.04 .11.05 .11.04 .18.04 .13.05 .12. .11.05 .17.04 .06.03 .09.03 .10.04 .09.03 .16.04 .12.04 .10. .11.03 .16.04 Table 3: Median AEE vs. benchmarks (IQR subscripts), aggregated over all groups and architectures. families, and 22 training options, this produces 9 385 3 4 = 41,580 configurations. Domain-level evaluation metrics. For each heldout benchmark/domain D, we estimate accuracy by averaging per-instance correctness probabilities (Eq. 1). We report: (i) accuracy estimation error (AEE), mean absolute error between estimated and true benchmark accuracies over held-out domains; and (ii) Spearman correlation ρ between estimated and true accuracies, capturing whether the estimator ranks domains for data acquisition. Implementation. All LLMs are served with vLLM (Kwon et al., 2023) (seed 42) with maximum generation length of 2,048. Temperature 0.5 has been recommended for Shannon Entropy in (Kuhn et al., 2023). Classifiers are implemented in scikitlearn (Pedregosa et al., 2011); features are z-scored using training-group statistics. Further details for reproducibility are presented in the appendix C."
        },
        {
            "title": "6 Results",
            "content": "Research Questions. We organize results around four questions: (RQ1) does the entropy-profile signal support cross-domain accuracy estimation? (RQ2) how does it compare with simple baselines? (RQ3) how sensitive is performance to which benchmarks are used for supervision? (RQ4) how sensitive is performance to estimator design choices (classifier family, calibration, balancing)? RQ1: Accuracy estimation under deploymentplausible defaults. We first report two training configurations chosen priori for plausibility rather than tuned for best score. Results here use random forest correctness estimator trained with class balancing and isotonic calibration on Extremes (GSM8K + OlympiadBench), spanning elementary-to-competition difficulty, and Intermediate (MATH + SciBench) Table 2 and Figure 3 5 Figure 3: Accuracy estimations from random-forest classifier trained exclusively on compact entropy-profile features on GSM and OlympiadBench. Both train benchmarks span the two extremes of difficulty. report cross-domain accuracy estimation quality. First, difficulty-spanning supervision is consistently stronger: under Extremes, seven of nine models achieve ρ 0.90 with low error (AEE 0.030.12), while Intermediate yields systematically higher error and weaker ordering (AEE 0.06 0.17; ρ drops for all models). Second, the signal is model-dependent: for PHI-3.5-MINI we observe near-perfect ordering (AEE 0.03, ρ=1.00), whereas QWEN3-8B exhibits weaker agreement in both settings (AEE 0.120.17, ρ 0.75). Overall, both choices generalize out of domain for most LLMs  (Table 2)  . However, Extremes is consistently stronger, achieving lower AEE and higher rank agreement ρ across models, while Intermediate incurs systematic degradation. Additional examples under alternative training groups and estimator variants are reported in Appendix D. RQ2: Baseline UQ metrics for performance monitoring. We compare our entropy-based accuracy estimator against nine standard white-box uncertainty metrics in Sec. 3, defined in Appendix A. Because these metrics are not trained to predict accuracy (they return an uncertainty score rather than calibrated correctness probability), we evaluate them in the most comparable way for monitoring: for each held-out domain, we aggregate each metric over instances to obtain domain-level score, and report Spearman correlation ρ between that score and the domains true accuracy. Table 4 summarizes results across models. In terms of raw correlation values, the best white-box metrics are in the same ballpark as our estimatoroften within few ρ pointsso they can track domain-todomain fluctuations reasonably well. That said, our method is consistently as good as, and in most settings slightly better than, the strongest uncertainty baselines in Table 4, and it additionally supports priori monitoring with calibrated, domain-level accuracy estimates as reported in Table 2; the baselines do not directly yield comparable predictions there, since they produce only uncalibrated scores. RQ3: Training composition dominates (and improves with k). We next quantify how accuracy 6 Model PHI-3.5-MINI (3B) QWEN3 (4B) QWEN3 (8B) MINISTRAL3 (3B) MINISTRAL3 (8B) GEMMA3 (4B) GEMMA3 (12B) LLAMA-3.1 (8B) GPT-OSS (20B) Ext. 1. 0.95 0.76 0.96 0.96 0.94 0.92 0.95 0.90 Int. SEA SEmax SEmean NLLavg NLLmax LNTP MTP 0.95 0.94 0.75 0.79 0.89 0.92 0.85 0.92 0. 0.95 0.96 0.93 0.96 0.98 0.89 0.90 0.95 0. 0.94 0.98 0.79 0.95 0.96 0.89 0.88 0.95 0. 0.90 0.95 0.75 0.92 0.96 0.87 0.81 0.55 0. 0.90 0.95 0.75 0.68 0.66 0.87 0.84 0.65 0. 0.88 0.96 0.89 0.61 0.47 0.89 0.93 0.90 0. 0.90 0.95 0.75 0.66 0.62 0.87 0.84 0.65 0. 0.89 0.98 0.95 0.55 0.73 0.98 0.95 1.00 0. PPL 0.90 0.95 0.75 0.68 0.73 0.87 0.83 0. 0.50 Table 4: Spearman ρ between aggregated UQ baselines and true domain accuracy on held-out benchmarks.GSM8K+OlympiadBench; Int.: MATH+SciBench. SEA and SEmax are consistently strong, often comparable to our defaults. estimation depends on the composition of the supervision set. Table 3 aggregates results over all training groups of size {1, 2, 3, 4} (and over classifier configurations), reporting the median AEE and its interquartile range (IQR). Increasing produces two consistent effects across all nine LLMs: typical error decreases monotonically with k, and robustness to benchmark choice improves sharply (IQR shrinks). At k=1, median AEE is high (roughly 0.200.27) and highly variable across which single benchmark provides supervision; by k=4, median AEE improves substantially (roughly 0.060.16) and variability compresses to IQR 0.030.04. Thus, benchmark choice is first-order design decision at small k, but becomes less brittle as supervision spans more tasks. Difficulty balance explains much of the composition effect. To make the composition dependence interpretable, we summarize each training group by its weighted average accuracy (the average A(D) over G, weighted by instance counts) and relate it to held-out estimation quality. Figure 4 (and Appendix for all models) shows U-shaped relationship: supervision sets that are too easy or too hard generalize worse, whereas groups with intermediate weighted accuracy (roughly 0.40.6) yield the lowest AEE. interpretation is that intermediate-weighted groups tend to mix easy and hard benchmarks, exposing the estimator to both low-entropy success patterns and high-entropy failure patterns; in contrast, easyonly or hard-only groups underrepresent one side of this spectrum and miscalibrate when transferred to unseen difficulty regimes. Difficulty-diverse training sets outperform difficulty-homogeneous ones: all-easy groups underrepresent high-entropy failure patterns from hard domains, while all-hard groups underrepresent low-entropy success patterns. Mixed (easy+hard) groups expose the estimator to broader range of entropy profiles, which aligns with better OOD generalization. We analyze the best/worst benchmark combinations and leave-one-out sensitivity study in Appendix F. RQ4: Estimator choices matter, but less than data composition. We isolate estimator design effects by aggregating across training groups and models. Table 5 reports main effects on median AEE and Spearman correlation. Random forests perform best in this low-dimensional setting (11 entropy features); logistic regression is close, while MLPs are consistently worse, consistent with overfitting under limited supervision. Isotonic calibration provides modest but consistent gain (0.12 vs. 0.14 median AEE), which matters because we aggregate probabilities. Class balancing has little net effect and can slightly hurt, suggesting failures are driven more by entropy-pattern shift than label imbalance. Across all configurations, Spearman remains stable (ρ 0.94), indicating the ranking of benchmark difficulty is robust and that most variation comes from training-data composition rather than estimator choices. Feature Selection. Across ablations the full 11D profile is strong, stable default, but it is not universally best. In particular, some smaller combinations often match or improve AEE and frequently improve ρ, indicating substantial redundancy among the summary statistics. More in Appendix E. Practical takeaways. Across nine LLMs and ten STEM benchmarks, entropy profiles from standard decoding logs provide useful signal for domainlevel accuracy estimation for most models: with supervision from just two benchmarks, we often generalize to the remaining eight domains, and in the best case (PHI-3.5-MINI) we observe nearperfect alignment between estimated and ground7 the cost of multiple generations. Unlike prior work targeting per-instance correctness, we aggregate these signals to estimate domain-level accuracy across benchmarks using standard inference logs. Supervised Approaches Using Internal Signals. Probe-based methods (Belinkov, 2022; Alain and Bengio, 2017) train simple classifiers on model representations to predict properties of interest. For correctness prediction, prior work trains probes on LLM hidden states (Azaria and Mitchell, 2023; Chen et al., 2024; Zhang et al., 2025a), and ensembles combine multiple uncertainty signals (Chen and Mueller, 2023; Verga et al., 2024; Bouchard et al., 2025a). We likewise use supervised probe, but target domain-level performance and rely only on final-layer entropy summaries, making the method more efficient and applicable to most open and closed-source models. Entropy-Lens. Our focus on entropy signals is motivated by Entropy-Lens (Ali et al., 2025), which uses Shannon entropy traces across transformer layers to classify model family, task type, and correctness. While effective, full residual-stream analysis is costly and yields high-dimensional features that may generalize poorly OOD. In contrast, we train on compact final-layer entropy summaries."
        },
        {
            "title": "8 Conclusion",
            "content": "We study whether cheap entropy decoding trace can support continuous accuracy monitoring under domain shift in STEM reasoning. Using only top-k log-probabilities, we summarize each generation with compact entropy-profile feature vector, train lightweight correctness predictor, and aggregate predicted probabilities to estimate domainlevel accuracy in interpretable units. Across an exhaustive sweep over ten STEM benchmarks and nine LLMs (3B20B), entropy-based estimators often track held-out benchmark accuracies closely and preserve domain rankings, but reliability is model-dependent. The dominant factor is supervision composition: training sets that span difficulty (mixing easy and hard tasks) generalize substantially better than difficulty-homogeneous sets, and adding small number of additional benchmarks reduces brittleness. Overall, entropy profiles provide practical primitive for monitoring and for prioritizing data acquisition toward low-accuracy slices, with the caveat that deployments should validate calibration on the target model. Figure 4: Training-group difficulty vs. estimation quality for PHI-3.5-MINI: intermediate weighted accuracy (0.40.6) yields the lowest AEE; all-easy/all-hard groups perform worse. truth accuracies. At the same time, reliability is model-dependent, so the method should be validated on the target model before deployment. The most important design choice, above predictor or feature selection is the diversity of the supervision set. Training groups that span difficulty (mixing easy and hard tasks) consistently outperform difficulty-homogeneous groups, which tend to underrepresent either high-entropy failure patterns (easy-only) or low-entropy success patterns (hard-only)."
        },
        {
            "title": "7 Related Work",
            "content": "Performance Estimation from Internal States. Recent work shows that LLM internal states encode information about response correctness, mainly for instance-level hallucination detection. Hidden activations can predict hallucinations (Azaria and Mitchell, 2023; Chen et al., 2024; Duan et al., 2024) and separate epistemic from aleatoric uncertainty (Ahdritz et al., 2024), though outputs and internal states can disagree (Liu et al., 2023). Models can also self-report correctness when prompted (Kadavath et al., 2022), but this adds inference cost. White-box approaches use token-probability metrics such as entropy and perplexity (Malinin and Gales, 2021; Fadeeva et al., 2024; Kuhn et al., 2023), while sampling-based checks assess consistency (Manakul et al., 2023) at Factor Setting Clf. Calib. Bal."
        },
        {
            "title": "RF\nLR\nMLP",
            "content": "Y / / AEE .12.07 .13.07 .14.07 ρ .94.06 .94.06 .94.06 .12.07 / .14.07 .13.06 / .13. .94.07 / .94.06 .94.06 / .94.06 Table 5: Ablation main effects: median AEE and Spearman ρ (IQR as subscripts). RF=Random Forest, LR=Logistic Regression."
        },
        {
            "title": "9 Limitations",
            "content": "Controlled domain: verifiable STEM reasoning. Our experiments focus on ten STEM benchmarks with relatively well-defined correctness criteria and zero-shot prompting. This enables large-scale, exhaustive train/test sweeps, but it also limits external validity: open-ended tasks (e.g., creative writing, dialogue, summarization) do not admit single gold answer. Top-k entropy is an approximation. We approximate Shannon entropy using only top-k probabilities (top-20), omitting tail mass. This truncation can change the scale and shape of entropy traces, especially for high-entropy steps where probability mass is more diffuse. While this choice is motivated by API constraints, it may degrade performance compared to full-vocab entropy and can vary across tokenizers and model families. Sensitivity to decoding and formatting. Entropy traces depend on decoding choices (temperature, max length, stop criteria) and on the models tendency to produce longer chain-of-thought or verbose explanations. Changes in prompting (e.g., instructing shorter solutions), answer formatting, or post-processing rules can shift entropy distributions without reflecting true capability changes. Model dependence and post-training effects. We observe that reliability varies across LLMs and can differ even within family (e.g., size variants). One plausible factor is post-training (instruction tuning, RLHF/RLAIF, safety finetuning), which can decouple confidence signals from correctness. Consequently, our findings do not guarantee that entropy profiles will yield well-calibrated accuracy estimates for new target model without empirical validation. Actionability beyond ranking. While Spearman ρ indicates that we can often rank domains by difficulty, absolute accuracy estimation error (AEE) remains non-trivial for some models, and miscalibration can persist even under large supervision (e.g., strong ranking but systematic offset). In operational settings, this suggests using the method primarily for prioritization (which slices to inspect or label) and coupling it with targeted evaluation before high-stakes interventions."
        },
        {
            "title": "References",
            "content": "Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin L. Edelman. 2024. Distinguishing the knowable from the unknowable with language models. arXiv preprint arXiv:2402.03563. Guillaume Alain and Yoshua Bengio. 2017. Understanding intermediate layers using linear classifier probes. In ICLR Workshop. Riccardo Ali, Francesco Caso, Christopher Irwin, and Pietro Liò. 2025. Entropy-lens: The information signature of transformer computations. Preprint, arXiv:2502.16570. Preprint. Under review. Anonymous. 2025. Livemathbench: contaminationresistant dynamic math reasoning benchmark. arXiv preprint arXiv:2505.15340. Amos Azaria and Tom Mitchell. 2023. The internal arXiv state of an LLM knows when its lying. preprint arXiv:2304.13734. Yonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207219. Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Ho-Kyeong Ra, Viren Bajaj, and Zeya Ahmad. 2025a. uqlm: python package for uncertainty quantification in large language models. arXiv preprint arXiv:2507.06196. Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Ho-Kyeong Ra, Viren Bajaj, and Zeya Ahmad. 2025b. Uqlm: python package for uncertainty quantification in large language models. Preprint, arXiv:2507.06196. Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. 2024. INSIDE: LLMs internal states retain the power of hallucination detection. arXiv preprint arXiv:2402.03744. Jiuhai Chen and Jonas Mueller. 2023. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. arXiv preprint arXiv:2308.16175. Wenhu Chen and 1 others. 2023. Theoremqa: theorem-driven question answering dataset. In Proc. EMNLP. Karl Cobbe and 1 others. 2021. Training verifiers In arXiv preprint to solve math word problems. arXiv:2110.14168. Hanyu Duan, Yi Yang, and Kar Yan Tam. 2024. Do LLMs know about hallucination? an empirical investigation of LLMs hidden states. arXiv preprint arXiv:2402.09733. Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, and Maxim Panov. 2024. Fact-checking the output of large language models via token-level uncertainty quantification. arXiv preprint arXiv:2403.04696. Google DeepMind. 2025. Gemma 3: Multimodal open models built from gemini technology. Technical report, Google. Arkil Patel and 1 others. 2021. Are nlp models really able to solve simple math word problems? In Proc. NAACL, pages 20802094. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830. Qwen Team. 2025. Qwen 3: Innovative multimodal llm and reasoning models. Technical report, Alibaba Group. David Rein and 1 others. 2023. Gpqa: graduatelevel google-proof q&a benchmark. arXiv preprint arXiv:2311.12022. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. 2024. Replacing judges with juries: Evaluating LLM generations with panel of diverse models. arXiv preprint arXiv:2404.18796. Xiaoxuan Wang and 1 others. 2023. Scibench: Evaluating college-level scientific problem solving. arXiv preprint arXiv:2307.10635. xAI. 2025. Grok 4.1 fast: Frontier tool-use and reasoning agents. XAI Blog. Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Aurojit Panda, Jinyang Li, and He He. 2025a. Reasoning models know when theyre right: Probing hidden states for self-verification. In Conference on Language Modeling (COLM). Anqi Zhang and 1 others. 2025b. Matscibench: Benchmarking the reasoning ability of large language models in materials science. arXiv preprint arXiv:2510.12171. Chaoqun He and 1 others. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proc. ACL. Dan Hendrycks and 1 others. 2021. Measuring mathematical problem solving with the math dataset. In Proc. NeurIPS, pages 115. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, and 17 others. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles (SOSP). Kevin Liu, Stephen Casper, Dylan Hadfield-Menell, and Jacob Andreas. 2023. Cognitive dissonance: Why do language model outputs disagree with internal representations of truthfulness? arXiv preprint arXiv:2312.03729. Andrey Malinin and Mark Gales. 2021. Uncertainty estimation in autoregressive structured prediction. In International Conference on Learning Representations. Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896. Meta AI. 2024. The llama 3 herd of models. Technical report, Meta. Microsoft. 2024. Phi-3.5-mini technical report. Technical report, Microsoft Research. Iman Mirzadeh and 1 others. 2025. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. In Proc. ICLR. Mistral AI. 2025. Ministral 3: Efficient frontier models for local reasoning. Release Announcement. OpenAI. 2025. Gpt-oss model card: Transparent frontier models. Technical report, OpenAI."
        },
        {
            "title": "A Baseline White Box UQ Metrics",
            "content": "We compare against nine standard uncertainty quantification baseline metrics derived exclusively from output LLM logprobs generated in regular inference. Most can be found in the uqlm library (Bouchard et al., 2025b). These metrics are our most direct comparison in terms of efficient signals usable for potential continuous performance evaluation. Shannon Entropy (SE). Following Malinin and Gales (2021); Manakul et al. (2023), we compute token-level entropy over vocabulary V: SE(yi) = (cid:88) (v x, y<i) log (v x, y<i) vV (2) We aggregate as: SEavg(y) = 1 i=1 SE(yi), SEmax(y) = maxi SE(yi), and SEsum(y) = (cid:80)L i=1 SE(yi) (Entropy Accumulation Score, abbre- (cid:80)L viated EAS). Negative Log-Likelihood (NLL). Manakul et al. x, y<i), (2023) use NLL(yi) = log (yi (cid:80)L aggregated as: NLLavg(y) = 1 i=1 NLL(yi), NLLmax(y) = maxi NLL(yi), and NLLsum(y) = (cid:80)L i=1 NLL(yi). Note that the average and maximum metrics are covered in the compact vector presented in section 4. Length-Normalized Token Probability (LNTP). Malinin and Gales (2021) propose the geometric mean of token probabilities: LNTP(y) = (cid:33)1/L (yi x, y<i) (3) (cid:32) (cid:89) i=1 Minimum Token Probability (MTP). Manakul et al. (2023) identify the weakest link: MTP(y) = mini=1,...,L (yi x, y<i) as an uncertainty quantification metric. Perplexity (PPL). Fadeeva et al. (2024) proPPL(y) = pose using standard perplexity: exp(NLLavg(y))."
        },
        {
            "title": "Results",
            "content": "Table 6 provides AUROC results for the remaining five models, supplementing the diagnostic study in Sec. 3. We observe significant heterogeneity across families and sizes. QWEN-3 4B achieves exceptionally high separability on MATH (Max: 0.9196), 11 while its larger counterpart, QWEN-3 8B, shows performance inversion with near-chance AUROCs on several statistics, even on equal benchmarks. This showcases that entropy profiles and their discriminative potential vary considerably between models of even the same family. Consistent with our main results, lower-tail quantiles (Q10, Q25) are most predictive for difficult reasoning tasks like MATH, while central tendency measures (Mean, Std Dev) perform better on elementary tasks like GSM8K. Furthermore, the GEMMA-3 family exhibits family-specific brittleness in higher-order moments (Skewness, Kurtosis), which collapse toward chance performance on difficult benchmarks. Results provide supporting evidence that entropy signals carry strong correctness signal across families, where the discriminatory potential of individual statistics is highly modeland domaindependent. This variability reinforces our design choice to utilize joint eleven-dimensional profile for accuracy estimation."
        },
        {
            "title": "C Additional details for Reproducibility",
            "content": "To ensure reproducibility across the ten STEM reasoning benchmarks and nine LLMs, we provide the specific prompts used for model generation and external validation. All benchmarks utilized zeroshot chain-of-thought (CoT) prompting. C.1 Model Generation Prompt We used the instruct-tunes of every LLM version ran through vLLM (Kwon et al., 2023), utilizing single user prompt without system message to maintain consistency across open and closedsource families: Solve the following problem step by step, then provide the final numerical answer. Question: {question} Solution: C.2 Evaluation and Verification Prompt Ground-truth correctness labels were produced using GROK-4.1-FAST-REASONING (xAI, 2025) via the LiteLLM API as an external validator. The validator receives the original question, the models response, and the reference ground-truth answer, then outputs structured binary decision using Pydantic response schema (class Response(BaseModel): success: bool). The system prompt instructs the validator to determine answer equivalence under benchmarkStatistic MATH GSM8K OLYMP. Statistic MATH GSM8K OLYMP. MINISTRAL-3 3B QWEN-3 8B Mean Std Dev Max Q10 Q25 Q50 Q75 Q90 Skewness Kurtosis SEA NLLavg NLLmax NLLsum LNTP MTP PPL Mean Std Dev Max Q10 Q25 Q50 Q75 Q90 Skewness Kurtosis SEA NLLavg NLLmax NLLsum LNTP MTP PPL Mean Std Dev Max Q10 Q25 Q50 Q75 Q90 Skewness Kurtosis SEA NLLavg NLLmax NLLsum LNTP MTP PPL 0.7512 0.7648 0.7856 0.7887 0.7749 0.7558 0.7332 0.7504 0.7145 0.7113 0.8612 0.6168 0.3681 0.8571 0.6168 0.3681 0.6168 0.8759 0.9011 0.9196 0.8607 0.8373 0.8133 0.8299 0.8712 0.8185 0.8153 0.9327 0.8746 0.8526 0.9325 0.8746 0.8526 0.8746 0.6456 0.7130 0.8453 0.8013 0.8099 0.7720 0.6560 0.6080 0.4843 0. 0.8672 0.6328 0.7513 0.8634 0.6328 0.7513 0.6328 0.7823 0.7873 0.7771 0.7741 0.7679 0.7726 0.7775 0.7754 0.7457 0.7368 0.8145 0.7256 0.4432 0.8009 0.7256 0.4432 0.7256 QWEN-3 4B 0.8093 0.8100 0.8144 0.7452 0.7605 0.7937 0.8148 0.8003 0.7649 0.7485 0.8193 0.7869 0.7348 0.8051 0.7869 0.7348 0. GEMMA-3 4B 0.7118 0.7316 0.7267 0.7354 0.7425 0.7275 0.6955 0.6944 0.6241 0.6087 0.7760 0.7059 0.6890 0.7807 0.7059 0.6890 0.7059 0.8225 0.8077 0.7764 0.8099 0.8230 0.8328 0.8259 0.8130 0.8229 0.8199 0.8700 0.7864 0.4345 0.8661 0.7864 0.4345 0.7864 0.8679 0.8849 0.8472 0.8705 0.8631 0.8523 0.8571 0.8732 0.8466 0. 0.8979 0.8683 0.7899 0.8981 0.8683 0.7899 0.8683 0.8223 0.8473 0.8328 0.8907 0.8916 0.8754 0.8299 0.8023 0.7273 0.7153 0.8768 0.8101 0.7246 0.8764 0.8101 0.7246 0.8101 Mean Std Dev Max Q10 Q25 Q50 Q75 Q90 Skewness Kurtosis SEA NLLavg NLLmax NLLsum LNTP MTP PPL Mean Std Dev Max Q10 Q25 Q50 Q75 Q90 Skewness Kurtosis SEA NLLavg NLLmax NLLsum LNTP MTP PPL Mean Std Dev Max Q10 Q25 Q50 Q75 Q90 Skewness Kurtosis SEA NLLavg NLLmax NLLsum LNTP MTP PPL 0.5698 0.5966 0.5392 0.6484 0.5926 0.5279 0.5593 0.5914 0.5520 0.5617 0.6014 0.5689 0.5125 0.5993 0.5689 0.5125 0.5689 0.5557 0.6431 0.8404 0.6763 0.6614 0.5813 0.5091 0.5784 0.4448 0. 0.8861 0.5400 0.7957 0.8823 0.5400 0.7957 0.5400 0.7726 0.7776 0.7117 0.7509 0.7614 0.7485 0.7701 0.7802 0.7633 0.7652 0.8032 0.7701 0.6587 0.8012 0.7701 0.6587 0.7701 LLAMA-3 8B 0.7319 0.7602 0.7520 0.7542 0.7538 0.7097 0.7110 0.7470 0.6457 0.6378 0.7877 0.7192 0.6758 0.7799 0.7192 0.6758 0. GEMMA-3 12B 0.5522 0.6477 0.8334 0.8061 0.8322 0.7334 0.5661 0.5108 0.4044 0.3963 0.8880 0.5273 0.8088 0.8826 0.5273 0.8088 0.5273 0.6899 0.7097 0.7403 0.7608 0.7644 0.7312 0.6891 0.6656 0.5871 0.5674 0.7416 0.6828 0.7191 0.7377 0.6828 0.7191 0.6828 0.7935 0.8104 0.7323 0.8236 0.8128 0.7820 0.7829 0.8041 0.7699 0. 0.7950 0.7907 0.6028 0.7923 0.7907 0.6028 0.7907 0.5263 0.5875 0.7465 0.5753 0.5861 0.5372 0.4946 0.5392 0.4539 0.4578 0.8555 0.5127 0.6974 0.8523 0.5127 0.6974 0.5127 0.7891 0.8118 0.8196 0.8487 0.8536 0.8377 0.7907 0.7712 0.7008 0.6913 0.8495 0.7758 0.7379 0.8478 0.7758 0.7379 0.7758 Table 6: AUROC for entropy profile summaries and UQ baselines across remaining models. Skewness, Kurtosis, LNTP, and MTP report 1 AUROC. Accumulation and extreme values are consistently the most discriminative metrics. specific criteria (e.g., symbolic or numeric equivalence): System: Task: Determine if the Response corresponds to the Correct Answer for the Question, based on the given Correct Answer text. Answer ONLY with the exact format: {\"success\": True} or {\"success\": False} The user prompt provides the evaluation context: User: # Question {question} # Response {cleaned_response} # Correct Answer {correct_answer} Responses are preprocessed by removing special tokens (e.g., <end>, <endoftext>) before validation. The structured Pydantic output ensures consistent binary classification across all 385 training configurations and benchmark evaluations. C.3 Classifier Hyperparameter Grid All hyperparameters are selected via 5 fold cross-validated grid search on the training group, optimizing ROC-AUC. For logistic regression (ℓ1 penalty, liblinear solver), we search {0.5, 2.0, 10.0}. For random forests (100 estimators), we search max depth {3, 5, 10} and min samples split {2, 5, 10}. For MLPs (ReLU activations, early stopping, 12 α = 0.001), we search hidden layer sizes {(5), (8), (10), (15), (20), (8, 4), (10, 5), (15, 8)}. C.4 Class Balancing Techniques class balancing using Logistic class_weight=\"balanced\", loss the frequencies. We evaluate each classifier with and withscikit-learns out regression built-in mechanisms. reweightuses to proportional ing class Random forests use class_weight=\"balanced_subsample\", reweighting within each bootstrap sample. For MLPs, which lack native class weighting, we apply random oversampling via RandomOverSampler from imbalanced-learn 3 prior to training. inversely"
        },
        {
            "title": "Results",
            "content": "We include two additional classifier configurations based on sensible priori defaults, all utilizing class balancing and isotonic calibration. Each configuration represents plausible practitioner choice without access to exhaustive hyperparameter search. Cross-Domain Linear Estimator. We train logistic regression on MATSCIBENCH (intermediate difficulty, materials science) paired with GSMSYMBOLIC (elementary difficulty, symbolic perturbations of GSM8K). This configuration tests whether simple linear model can generalize across domains when trained on benchmarks that differ in both subject matter and problem format. The inclusion of GSM-Symbolic rather than standard GSM8K probes robustness to distribution shift within the elementary difficulty regime. Science-Math Neural Estimator. We train an MLP on three benchmarks spanning scientific and mathematical reasoning: GPQA (graduate-level science), MATSCIBENCH (materials science), and GSM8K (elementary math). This configuration provides the neural estimator with cross-domain supervision while maintaining difficulty diversity, testing whether the additional representational capacity of MLPs benefits generalization when training data spans multiple reasoning types. Results. Table 7 reports cross-domain accuracy estimation for both configurations. Comparing against the main configurations  (Table 2)  reinforces two key findings from our main experiments. 3https://imbalanced-learn.org/stable/ Training composition dominates classifier choice. The Extremes configuration from the main paper remains the strongest overall, achieving the lowest AEE for 7/9 models with its difficultyspanning design. Crucially, the Cross-Domain logistic regressiona simpler linear model trained on only two benchmarksmatches or outperforms the Intermediate random forest for most models. Results are similar on the Sci-Math MLP configuration more flexible model trained on varied domain and difficulty data, suggesting that wellcomposed training set matters more than classifier flexibility. Calibration quality is model-dependent. Ranking stability (ρ) varies more across LLM families than across estimator configurations. PHI-3.5MINI maintains ρ 0.95 and AEE 0.07 across all four configurations (Extremes, Intermediate, Cross-Domain, Sci-Math), exhibiting consistently strong entropycorrectness coupling. In contrast, QWEN3-8B achieves only ρ 0.760.78 regardless of training composition or classifier choice, indicating that its entropy signal is inherently less predictive of correctness. These results reinforce that practitioners should validate entropy-based estimation on their target model before deployment. Cross-Domain Sci-Math"
        },
        {
            "title": "AEE",
            "content": "ρ"
        },
        {
            "title": "AEE",
            "content": "ρ PHI-3.5-MINI (3B) MINISTRAL3 (3B) MINISTRAL3 (8B) QWEN3 (4B) QWEN3 (8B) GEMMA3 (4B) GEMMA3 (12B) LLAMA-3.1 (8B) GPT-OSS (20B) 0.06 0.09 0.08 0.08 0.16 0.13 0.10 0. 0.16 0.95 0.98 0.95 0.96 0.78 0.92 0.88 0. 0.94 0.07 0.07 0.09 0.10 0.19 0.13 0.13 0. 0.15 0.95 0.95 0.95 0.98 0.78 0.92 0.88 0. 0.93 Table 7: Cross-domain accuracy estimation for two classifier configurations: Simple Cross-Domain (logistic regression on MatSciBench + GSM-Symbolic) and Flexible Sci-Math (MLP on GPQA + MatSciBench + GSM8K). Both use class balancing and isotonic calibration. Reduced-Feature set Classifier striking finding is how little performance varies across feature sets. Differences in AEE between the full 11D profile and reduced alternatives typically fall within 0.010.03, and in many cases simpler feature sets match or outperform the full profile. For instance, SEA alone achieves lower AEE than the full profile for LLAMA3-8B (0.05 vs. 0.07), MINISTRAL3-8B (0.06 vs. 0.07), and OSS-20B (0.10 vs. 0.15) under Extremes supervision. Domain ranking quality (ρ) proves even more robust for QWEN3-8B, the full profile yields ρ = 0.76 while SEA alone reaches ρ = 0.93, suggesting additional features may introduce noise for models with weaker entropycorrectness coupling regarding difficulty ranking. These results likely reflect high correlation among summary statisticsmax, mean, quantiles, and accumulative metrics all derive from the same entropy trajectory, limiting the marginal information from additional features. Comparable results across feature sets supports our observation that training composition is the main driver of method performance."
        },
        {
            "title": "F Additional Training Sensibility Results",
            "content": "This appendix provides supplementary analyses of how training data composition affects accuracy estimation quality across all nine LLMs. U-Shaped Difficulty Relationship Across All Models Figure 5 extends the analysis from Section 6 by plotting training group difficulty against estimation quality aggregated across all nine LLMs. The U-shaped pattern observed for PHI-3.5-MINI holds consistently: training groups with intermediate weighted accuracy (0.40.6) achieve the lowest median AEE, while groups at difficulty extremes yield degraded estimation. This reinforces that difficulty diversity in training data is more important than domain diversity for robust accuracy estimation. Figure 5: Relationship between training group difficulty and estimation quality aggregated across all nine LLMs. Training groups with intermediate weighted accuracy (0.40.6) achieve optimal performance, while difficultyhomogeneous groups at either extreme degrade generalization. Shaded region indicates IQR. 14 Best and Worst Benchmark Combinations. Table 9 identifies the highestand lowest-performing benchmark combinations at each k, along with their weighted average group accuracy. The results directly corroborate the U-shaped relationship from Figures 4 and 5: best-performing groups fall within the intermediate accuracy regime (0.41 0.64), while worst-performing groups cluster at the low-accuracy extreme (0.110.22). For 2, the best combinations pair elementary benchmarks (GSM8K) with difficult ones (OLYMPIADBENCH), achieving group accuracies squarely in the optimal range. This difficultyspanning composition exposes the estimator to both low-entropy success patterns and high-entropy failure patterns, enabling robust transfer to unseen domains. Notably, the = 2 combination (GSM + OLY) achieves median AEE of 0.087, matching the best = 4 configuration despite using half the supervision. In contrast, all worst-performing combinations consist exclusively of difficult benchmarks, systematically underrepresenting low-entropy success signatures and causing the estimator to miscalibrate on easier test domains. This asymmetry reinforces central finding: difficulty diversity in the supervision set is the main driver of better accuracy estimation. Leave One Out Analysis To further stress-test the estimator under near-maximal supervision, we train on = 9 benchmarks and evaluate on the single held-out domain. This setting provides the most favorable conditions for generalization while isolating per-benchmark estimation difficulty. Table 10 reports median AEE across all held-out benchmarks, the median held-out AEE (estimation error on the single excluded benchmark), and Spearman correlation ρ. The leave-one-out results strongly replicate our main findings: even with nine training benchmarks, model-dependent variation in entropycorrectness coupling persists. PHI-3.5-MINI achieves the lowest overall error (AEE = 0.07, held-out AEE = 0.06) with excellent ranking (ρ = 0.96), while MINISTRAL3 and QWEN3-4B achieve near-perfect domain ordering (ρ = 0.98). In contrast, QWEN3-8B and GPT-OSS continue to exhibit degraded performance even under maximal supervision. QWEN3-8B shows the weakest ranking agreement (ρ = 0.82) and highest held-out error variability (IQR = 0.17), suggesting that its entropy profiles provide fundamentally weaker corModel (AEE / ρ) Qwen3-8B Phi3-3B Qwen3-4B Ministral3-3B Ministral3-8B Llama3-8B Gemma3-4B Gemma3-12B OSS-20B Qwen3-8B Phi3-3B Qwen3-4B Ministral3-3B Ministral3-8B Llama3-8B Gemma3-4B Gemma3-12B OSS-20B Qwen3-8B Phi3-3B Qwen3-4B Ministral3-3B Ministral3-8B Llama3-8B Gemma3-4B Gemma3-12B OSS-20B Full 11D (Default) 0.12 / 0.76 0.03 / 1.00 0.08 / 0.95 0.06 / 0.96 0.07 / 0.96 0.07 / 0.95 0.09 / 0.94 0.08 / 0.92 0.15 / 0.90 0.17 / 0.75 0.06 / 0.95 0.11 / 0.94 0.11 / 0.79 0.12 / 0.89 0.11 / 0.92 0.14 / 0.92 0.12 / 0.85 0.16 / 0.89 0.16 / 0.78 0.06 / 0.95 0.08 / 0.96 0.09 / 0.98 0.08 / 0.95 0.09 / 0.95 0.13 / 0.92 0.10 / 0.88 0.16 / 0.94 Max (only) SEA (only) Top2 (Max+SEA) S. Baselines (3-feat) EXTREMES (Random Forest; GSM + OlympiadBench) 0.25 / 0.79 0.05 / 0.94 0.08 / 0.98 0.07 / 0.94 0.08 / 0.95 0.07 / 0.95 0.13 / 0.89 0.12 / 0.88 0.13 / 0.95 0.18 / 0.93 0.08 / 0.89 0.09 / 0.98 0.06 / 0.98 0.06 / 0.98 0.05 / 0.94 0.12 / 0.88 0.09 / 0.90 0.10 / 0. INTERMEDIATE (Random Forest; MATH + SciBench) 0.25 / 0.72 0.08 / 0.94 0.09 / 0.98 0.08 / 0.95 0.09 / 0.96 0.11 / 0.94 0.14 / 0.89 0.13 / 0.88 0.14 / 0.94 0.20 / 0.93 0.10 / 0.89 0.10 / 0.96 0.10 / 0.96 0.11 / 0.98 0.07 / 0.95 0.14 / 0.89 0.11 / 0.90 0.08 / 0.96 CROSS-DOMAIN (Log. Reg; MatSci + GSM-Symbolic) 0.29 / 0.75 0.09 / 0.94 0.11 / 0.98 0.11 / 0.99 0.12 / 0.96 0.12 / 0.95 0.17 / 0.89 0.16 / 0.88 0.21 / 0.99 0.22 / 0.89 0.06 / 0.90 0.08 / 0.98 0.10 / 0.96 0.11 / 0.98 0.06 / 0.95 0.15 / 0.88 0.10 / 0.90 0.13 / 0. SCI-MATH (MLP; GPQA + MatSci + GSM) 0.18 / 0.89 0.07 / 0.89 0.08 / 0.98 0.06 / 0.98 0.06 / 0.98 0.05 / 0.94 0.11 / 0.88 0.09 / 0.88 0.10 / 0.94 0.20 / 0.89 0.08 / 0.92 0.10 / 0.98 0.08 / 0.96 0.09 / 0.96 0.07 / 0.94 0.13 / 0.89 0.11 / 0.88 0.08 / 0.96 0.22 / 0.89 0.06 / 0.94 0.09 / 0.98 0.10 / 0.96 0.11 / 0.96 0.06 / 0.95 0.14 / 0.88 0.10 / 0.90 0.13 / 0.94 0.17 / 0.93 0.06 / 0.96 0.09 / 0.96 0.05 / 0.96 0.07 / 0.98 0.05 / 0.94 0.11 / 0.89 0.09 / 0.88 0.10 / 0.94 0.20 / 0.88 0.07 / 0.95 0.10 / 0.96 0.08 / 0.98 0.09 / 0.96 0.07 / 0.94 0.14 / 0.87 0.12 / 0.88 0.10 / 0. 0.24 / 0.77 0.06 / 0.93 0.08 / 0.98 0.10 / 0.98 0.11 / 0.95 0.05 / 0.96 0.14 / 0.87 0.10 / 0.90 0.12 / 0.94 0.19 / 0.78 0.07 / 0.95 0.10 / 0.98 0.07 / 0.95 0.09 / 0.95 0.08 / 0.94 0.13 / 0.92 0.13 / 0.88 0.15 / 0.93 Qwen3-8B 0.19 / 0.88 Phi3-3B 0.07 / 0.93 0.10 / 0.98 Qwen3-4B 0.06 / 0.96 Ministral3-3B 0.09 / 0.96 Ministral3-8B 0.05 / 0.95 Llama3-8B 0.11 / 0.89 Gemma3-4B Gemma3-12B 0.12 / 0.88 0.13 / 0.98 OSS-20B Table 8: Cross-domain accuracy estimation across four supervision configurations and five feature sets. AEE: Median Absolute Estimation Error; ρ: Median Spearman correlation. Bold indicates best AEE (lowest) and best ρ (highest) per row. 0.23 / 0.73 0.08 / 0.94 0.09 / 0.98 0.06 / 0.98 0.09 / 0.96 0.08 / 0.95 0.12 / 0.89 0.13 / 0.88 0.16 / 0.96 0.18 / 0.89 0.06 / 0.90 0.09 / 0.98 0.06 / 0.96 0.09 / 0.98 0.05 / 0.95 0.11 / 0.89 0.12 / 0.88 0.13 / 0.98 0.18 / 0.95 0.06 / 0.90 0.09 / 0.98 0.08 / 0.96 0.11 / 0.98 0.05 / 0.95 0.11 / 0.89 0.11 / 0.90 0.13 / 0. rectness signala limitation that additional training data cannot fully overcome. Interestingly, GPTOSS achieves strong ranking (ρ = 0.96) but exhibits systematically high absolute error (held-out AEE = 0.20), indicating well-calibrated relative estimates but poorly calibrated absolute probabilities. This pattern suggests that for GPT-OSS, our entropy profiles approach reliably rank domains by difficulty but consistently overor under-estimate accuracy magnitudes. The within-family inversions observed in the main experiments persist: QWEN3-4B substantially outperforms its larger QWEN3-8B counterpart (held-out AEE 0.08 vs. 0.18; ρ 0.98 vs. 0.82), and GEMMA3-4B and MINISTRAL3-3B shows comparable or exceeds GEMMA3-12B and MINISTRAL3-8B on ranking quality respectively. These results reinforce that practitioners should empirically validate the approach on their specific deployment model. 15 k"
        },
        {
            "title": "AEE",
            "content": "Acc."
        },
        {
            "title": "1 OLY\n2 GSM, OLY\n3 GPQA, GSM, OLY\n4 GPQA, GSM, MAT, OLY",
            "content": "129.063 .087.047 .087.046 .086."
        },
        {
            "title": "LIVE",
            "content": "1 2 GPQA, LIVE 3 GPQA, LIVE, MAT 4 GPQA, LIVE, MAT, SCI .269.119 .259.093 .216.072 .191.064 .260 .636 .531 .413 .114 .143 .197 .220 subscripts. Table 9: Best and worst benchmark combinations for each value. Median AEE with IQR shown as reports weighted average group accuracy across LLMs. AbbreviOLY=OlympiadBench, GSM=GSM8K, ations: MAT=MatSciBench, LIVE=LiveMathBench, SCI=SciBench. Acc."
        },
        {
            "title": "Model",
            "content": "AEE Held-out AEE ρ PHI-3.5-MINI (3B) MINISTRAL3 (3B) MINISTRAL3 (8B) QWEN3 (4B) QWEN3 (8B) GEMMA3 (4B) GEMMA3 (12B) LLAMA-3.1 (8B) GPT-OSS (20B) .07.03 .08.02 .10.02 .09.02 .16.04 .14.03 .11. .10.01 .15.02 .06.06 .08.08 .08.09 .08.09 .18.17 .13.10 .13. .10.08 .20.15 .96.02 .98.04 .95.00 .98.00 .82.04 .92.00 .88. .94.01 .96.02 Table 10: Median accuracy estimation error (AEE), heldout AEE (leave-one-out), and Spearman correlation (ρ) for = 9 training benchmarks, with interquartile range (IQR) shown as subscripts."
        }
    ],
    "affiliations": [
        "Departamento de Matematica, FCEyN Universidad de Buenos Aires",
        "ELIAS Lab, Departamento de Ingenierá Universidad de San Andres"
    ]
}