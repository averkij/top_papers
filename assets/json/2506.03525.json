{
    "paper_title": "Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning",
    "authors": [
        "Daeun Lee",
        "Jaehong Yoon",
        "Jaemin Cho",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Chain-of-Thought (CoT) reasoning have improved complex video understanding, but existing methods often struggle to adapt to domain-specific skills (e.g., event detection, spatial relation understanding, emotion understanding) over various video content. To address this, we propose Video-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning. First, we construct skill-based CoT annotations: we extract domain-relevant reasoning skills from training questions, cluster them into a shared skill taxonomy, and create detailed multi-step CoT rationale tailored to each video-question pair for training. Second, we introduce a skill-specific expert learning framework. Each expert module specializes in a subset of reasoning skills and is trained with lightweight adapters using the collected CoT supervision. We demonstrate the effectiveness of the proposed approach on three video understanding benchmarks, where Video-SKoT consistently outperforms strong baselines. We also provide in-depth analyses on comparing different CoT annotation pipelines and learned skills over multiple video domains."
        },
        {
            "title": "Start",
            "content": "VIDEO-SKILL-COT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning Daeun Lee* Jaehong Yoon"
        },
        {
            "title": "Jaemin Cho Mohit Bansal",
            "content": "University of North Carolina at Chapel Hill {daeun,jhyoon,jmincho,mbansal}@cs.unc.edu https://video-skill-cot.github.io/ 5 2 0 2 4 ] . [ 1 5 2 5 3 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Chain-of-Thought (CoT) reasoning have improved complex video understanding, but existing methods often struggle to adapt to domain-specific skills (e.g., event detection, spatial relation understanding, emotion understanding) over various video content. To address this, we propose VIDEO-SKILL-COT (a.k.a. VIDEO-SKOT) framework that automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning. First, we construct skill-based CoT annotations: We extract domain-relevant reasoning skills from training questions, cluster them into shared skill taxonomy, and create detailed multi-step CoT rationale tailored to each video-question pair for training. Second, we introduce skill-specific expert learning framework. Each expert module specializes in subset of reasoning skills and is trained with lightweight adapters using the collected CoT supervision. We demonstrate the effectiveness of the proposed approach on three video understanding benchmarks, where VIDEO-SKOT consistently outperforms strong baselines. We also provide in-depth analyses on comparing different CoT annotation pipelines and learned skills over multiple video domains."
        },
        {
            "title": "Introduction",
            "content": "Understanding complex video content requires integrating rich spatiotemporal cues and adapting to diverse domain-specific reasoning needs from cinematic narratives, egocentric recordings, to indoor scenes (Fusier et al., 2007; Huang et al., 2018; Buch et al., 2022; Lin et al., 2023; Chen et al., 2024; Li et al., 2024c). Models should acquire and integrate wide range of distinct reasoning skills, such as temporal grounding, spatial relationship recognition, and multi-step planning. *Equal contribution. 1 to multimodal Recent work has extended chain-of-thought (CoT) reasoning (Wei et al., 2023; Kojima et al., 2022) large language models (MLLMs) for video understanding (Fei et al., 2024; Feng et al., 2025; Li et al., 2025; Liu et al., 2025; Zhi et al., 2025). However, most prior approaches rely on fixed, general-purpose reasoning traces that are insensitive to domain-specific skills. Fig. 1 (left) shows t-SNE (van der Maaten and Hinton, 2008) plot of embeddings of questions from different video datasets, where questions from the same datasets are strongly clustered as they require shared skills/domains. For example, models pretrained on general corpora such as LLaVA-Video178K (Zhang et al., 2024) often lack the nuanced narrative understanding needed in CinePile (Rawal et al., 2024). This limits their ability to generalize to unseen domains or specialized skills. To address this, we propose VIDEO-SKILL-COT (aka VIDEO-SKOT), novel video understanding framework for creating and leveraging skill-aware CoT supervision, helping effective domain adaptation of MLLMs (Sec. 3). As shown in Fig. 1 (Right), VIDEO-SKOT consists of two main components. First, in skill-based CoT annotation (Sec. 3.2), we introduce method to automatically construct high-quality, skill-conditioned CoT rationales for video QA tasks. Given training question, we first extract high-level reasoning skill descriptions (e.g., Determine object location relative to persons orientation and Inferring emotional state from expressions and body language), then cluster them into shared skill taxonomy (Fig. 1 Right-(a)). Then, each question is annotated with its top-K relevant skills and used to generate multi-step CoT annotation conditioned on these skills (Fig. 1 Right-(b)). This enables diverse and domain-relevant reasoning traces without requiring manual annotation. Once we have prepared the skill-based CoT annotations, in skill-specific expert learning (Sec. 3.3 Figure 1: Left: Video datasets require different reasoning skills. Right: VIDEO-SKOT that automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning. and Fig. 1 Right-(c)), we train skill-specialized expert models with multiple LoRAs (Hu et al., 2022). Each expert specializes in specific set of reasoning capabilities, determined by predefined group of related questions. During inference, the model routes each input to the expert aligned with the most relevant question group. We evaluate VIDEO-SKOT on three video QA datasets with diverse domains (E.T.-Bench (Liu et al., 2024), VSI-Bench (Yang et al., 2024), and CinePile (Rawal et al., 2024)), where VIDEOSKOT consistently improves over strong baselines, showcasing its strong domain adaptation capabilities. We also present ablation studies on our design choices and visualize the learned domain-specific skills to validate the effectiveness and interpretability of our skill-guided reasoning framework."
        },
        {
            "title": "2 Related Work",
            "content": "Video Understanding with MLLMs. Prior video understanding models focused on pretraining strategies (Sun et al., 2019; Li et al., 2020; Lei et al., 2021). Recent work incorporates CoT reasoning (Kojima et al., 2022; Wei et al., 2023) from the NLP domain and studies how to collect and learn to generate such CoT reasoning for different video understanding tasks (Fei et al., 2024; Li et al., 2025; Liu et al., 2025; Zhi et al., 2025). Unlike these methods, which often struggle with comprehending videos without explicit skill-specific guidance, our approach introduces skill-aware reasoning framework incorporating question-adaptive skill selection and skill-guided CoT supervision. Skill-specific Expert Learning. Modular and expert-based architectures have been widely explored to improve parameter efficiency and mitigate interference in multi-task and multi-domain settings, where each expert learns different knowledge. Mixture-of-experts (MoE) frameworks dynamically route inputs to expert subnetworks (Shazeer et al., 2017), while adapterbased methods introduce lightweight, task-specific modules into pretrained models (Houlsby et al., 2019; Hu et al., 2022). Li et al. (2024b) studies learning skill-specific expert diffusion models for the text-to-image generation task. concurrent work, Liu et al. (2025) studies multi-agent system where each agent is implemented as LoRA (Hu et al., 2022) expert. While Liu et al. (2025) relies on predefined expert roles (planner, grounder, verifier, and answerer), specific architectures, and manually curated role-specific annotations, our expert framework flexibly adapts to any video understanding dataset by automatically discovering and leveraging relevant reasoning skills."
        },
        {
            "title": "3.1 Problem Setup",
            "content": "Given video and question q, our objective is to produce both an answer and reasoning trace that offers an interpretable, step-by-step justification. Prior work typically uses single MLLM to generate these: {r; a}=f (q, v). In contrast, VIDEO-SKOT decomposes the reasoning process into two stages: First, given q, we select the most relevant expert {1, . . . , experts} based on the set of pre-defined question groups and predicted required skills. Next, skill-specific expert MLLM then generates skill-guided reasoning trace rs along with the final answer: {rs; a}=f e(q, v). We illustrate VIDEOSKOT in Fig. 1 (right). This design enables targeted expert learning and adaptation to diverse reasoning skills in new video domain. In the following, we describe how we automatically construct the skill-based CoT (Sec. 3.2) and how to train MLLMs with the collected skill-based CoT annotations (Sec. 3.3). 2 3.2 Skill-based CoT Annotation We first construct skill-based CoT rationale annotations for any Video QA dataset, leveraging skillaware reasoning to enable domain-adaptive video understanding. We perform the following two steps for each (q, v) in the training set to obtain skillconditioned reasoning traces. Step 1: Skill Description & Clustering (Fig. 1 Right-(a)). We define skill as shared, highlevel reasoning capability (e.g., temporal ordering, visual counting, spatial understanding) that recurs across multiple video QA examples within specific domain. For each question q, we prompt an MLLM to describe what kind of skill is necessary to answer it (e.g., Estimate distance between two objects using visual cues). Then, we encode all skill descriptions into text embeddings and perform k-means clustering (with k=N skills=10) to form shared skill taxonomy. Each cluster centroid represents prototypical skill. Step 2: Skill-based CoT Collection (Fig. 1 Right- (b)). For each (q, v) pair, we generate multistep reasoning trace conditioned on the descriptions of the top 3 assigned skills, process we refer to as Skill Selection. Next, we generate the skillaware CoT rs; We prompt an MLLM to produce intermediate sub-questions and corresponding answers, guided by selected skills from the previous stage. These sub-QA pairs are then merged into coherent CoT paragraph that explicitly reflects the assigned reasoning skills. To ensure the quality of the skill-based CoT rationales, we further verify and filter out reasoning steps that are irrelevant to the correct answer using an LLM evaluator. After these steps, each training example is now annotated with relevant expert labels and verified, skill-grounded CoT trace rs. These annotations form the basis for downstream training of skill-specific expert models."
        },
        {
            "title": "3.3 Skill-specific Expert Learning",
            "content": "As illustrated in Fig. 1 Right-(c), we perform modularized fine-tuning to learn task-specific knowledge for skill-based CoT training. Specifically, we first project all questions in training set Dtrain into the text embedding space and perform k-means clustering (with k=N experts=5). Unlike step 2 of Sec. 3.2 where skills clusters represent the groups of skill descriptions, these experts cluster centroids represent the groups of questions. After assigning each training example to its closest experts, we conduct parameter-efficient training using the corresponding experts expert LoRA (Hu et al., 2022) modules, ensuring task-specific adaptation while minimizing interference across skills. During test time, we assign each test question by finding the closest question group by finding the closest question embedding centroids. Training Objective. Following previous work (Hu et al., 2024; Shi et al., 2024), we train an MLLM by minimizing cross-entropy losses for predicting both the answer (Lanswer) and CoT tokens (LCoT), respectively: = Lanswer + λLCoT = ℓ(f (q, v), a) + λℓ(f (q, v), rs), (1) where we find λ = 0.5 balances the two losses well."
        },
        {
            "title": "4.1 Experiment Setups",
            "content": "Implementation Details. To obtain text embeddings (of skill taxonomy in Sec. 3.2 and of questions in Sec. 3.3), we use all-mpnet-base-v2 SentenceTransformers (Reimers and Gurevych, 2019) implementation. We use LLaVa-Video (7B) (Zhang et al., 2024) as backbone model. Additional training details, including the hyperparameters, specific MLLMs and LLMs used at each stage, are provided in the Appendix Secs. A.1 and A.2. Datasets and Baselines. We experiment with three different video understanding benchmarks with distinct domains: E.T.-Bench (Liu et al., 2024) (temporal understanding), VSI-Bench (Yang et al., 2024) (spatial understanding), and CinePile (Rawal et al., 2024) (movie narrative understanding). For multiple-choice questions, we report the average accuracy. For temporal captioning tasks in E.T.- Bench, we use the benchmarks official evaluation script. Baseline MLLMs include mPLUGOwl (Ye et al., 2024), Video-ChatGPT (Maaz et al., 2023), Video-LLaMA2 (Zhang et al., 2023), LLaVa-OneVision (Li et al., 2024a), and LLaVaVideo (Zhang et al., 2024). Additional details are provided in the Appendix Sec. A.3."
        },
        {
            "title": "4.2 Quantitative Evaluation",
            "content": "Comparison to Baselines. We compare VIDEOSKOT to recent MLLM baselines on three video understanding benchmarks (E.T-Bench, VSI-Bench, 3 Figure 2: Comparison of CoT annotations: (a) regular CoT and (b) our skill-based CoT. Additional examples are provided in Appendix Sec. C. Fine-tuned E.T. (Temporal) VSI (Spatial) CinePile (Movie) mPLUG-Owl Video-ChatGPT Video-LLaMA2 LLaVA-Video LLaVA-Video Ours 11.87 13.02 8.30 19.35 20.32 22.21 - - - 35.60 47.45 53.15 13.93 15.08 44.57 55.83 56.29 57. Skill-CoT (Sec. 3.2) Skill-specific Experts (Sec. 3.3) 3-Task Avg. - - - - 44.41 42.91 38.53 41.04 Table 2: Ablation studies on removing the main components: skill-CoT and skill-specific experts. Table 1: Evaluation on domain-specific video reasoning benchmarks. All methods are based on an MLLM (7B)."
        },
        {
            "title": "4.3 Qualitative Analysis",
            "content": "CinePile) with domains and required skills. Table 1 shows that VIDEO-SKOT consistently outperforms all baselines, achieving improvements of +4.10, +5.70, and +1.59 over the fine-tuned version of LLaVA-Video on E.T.-Bench, VSI-Bench, and CinePile, respectively. These results highlight the effectiveness of our modular, expert-driven framework in enabling domain-adaptive CoT video reasoning by leveraging relevant skills. Ablation Studies. We compare the impact of two key components in our framework: (1) skill-based CoT reasoning and (2) skill-specific expert modules. As shown in Tab. 2, our full model, combining both components (Top row), achieves the highest performance. Removing either the skillspecific expert modules (2nd row), the skill-based CoT (3rd row), or both components (last row) consistently leads to performance degradation, highlighting their complementary roles: skill-CoT enables structured reasoning, while expert modules bring modular specialization. This synergy proves essential for improving video understanding. 4 Regular CoT vs. Skill-based CoT. Fig. 2 compares the different annotated CoTs from the regular CoT and our skill-based CoT. Given question about which object is closest to the stove, the regular CoT (left) offers linear, scene-based narration that lacks structure and includes irrelevant details (Camera first focuses ... it then pans to the right ...), making it often harder to extract key spatial information. In contrast, our skill-based CoT starts by identifying relevant skills (e.g., spatial proximity) and breaking the task into focused sub-questions, like comparing the washer and refrigerator."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose VIDEO-SKOT, novel video understanding framework for effective domain adaptation of MLLMs. We propose to automatically collect skill-specific CoT annotations from video QA datasets and construct skill-based reasoning pipeline that combines lightweight skill assigner with collection of LoRA-based expert adapters. Empirical results on three diverse benchmarks demonstrate consistent gains of VIDEO-SKOT over strong baselines, highlighting the enhanced quality of our reasoning traces."
        },
        {
            "title": "Limitations",
            "content": "Our proposed framework demonstrates strong video reasoning capabilities, generating finegrained, domain-adaptive rationales based on required skills. However, it may still produce occasional inaccuracies or hallucinations (Liu et al., 2023; Wang et al., 2024; Zhou et al., 2024) in its text outputs. Additionally, the overall performance is influenced by the underlying pre-trained backbones, namely, the LLM (Achiam et al., 2023) and MLLM (Georgiev et al., 2024) used. Nonetheless, we highlight that VIDEO-SKOT can benefit further from future advancements in LLM and MLLM backbones."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by DARPA ECOLE Program No. HR00112390060, NSF-AI Engage Institute DRL-2112635, DARPA Machine Commonsense (MCS) Grant N6600119-2-4031, ARO Award W911NF2110220, ONR Grant N00014-231-2356, Bloomberg Data Science PhD Fellowship, and the Accelerate Foundation Models Research program. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Shyamal Buch, Cristóbal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. 2022. Revisiting the\" video\" in video-language the IEEE understanding. International Conference on Computer Vision and Pattern Recognition (CVPR). In Proceedings of Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, and 1 others. 2024. Sharegpt4video: Improving video understanding and generation with better captions. In Advances in Neural Information Processing Systems (NeurIPS). Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. 2024. Video-of-thought: Step-by-step video reasoning from perception to cognition. arXiv preprint arXiv:2501.03230. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. 2025. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776. Florent Fusier, Valery Valentin, François Brémond, Monique Thonnat, Mark Borg, David Thirde, and James Ferryman. 2007. Video understanding for complex activity recognition. Machine Vision and Applications, 18:167188. Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, and 1 others. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. arXiv:1902.00751. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, and Ariel Fuxman. 2024. Visual program distillation: Distilling tools and programmatic reasoning into vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95909601. De-An Huang, Vignesh Ramanathan, Dhruv Mahajan, Lorenzo Torresani, Manohar Paluri, Li Fei-Fei, and Juan Carlos Niebles. 2018. What makes video video: Analyzing temporal information in video understanding models and datasets. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR). Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learningvia sparse sampling. In CVPR. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and 1 others. 2024a. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, and Mohit Bansal. 2024b. Selma: Learning and merging skill-specific text-to-image experts with autogenerated data. In Advances in Neural Information Processing Systems (NeurIPS). Yudi Shi, Shangzhe Di, Qirui Chen, and Weidi Xie. 2024. Unlocking video-llm via agent-of-thoughts distillation. arXiv preprint arXiv:2412.01694. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. 2019. Videobert: joint model for video and language representation learning. arXiv:1904.01766. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:25792605. Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim. 2024. Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites. In International Conference on Multimedia Modeling. Springer. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. arXiv:2201.11903. Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. 2024. Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. arXiv preprint arXiv:2412.14171. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. 2024. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR). Hang Zhang, Xin Li, and Lidong Bing. 2023. Videollama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. 2024. Video instruction tuning with synthetic data. arXiv:2410.02713. Zhuo Zhi, Qiangqiang Wu, Wenbo Li, Yinchuan Li, Kun Shao, Kaiwen Zhou, and 1 others. 2025. Videoagent2: Enhancing the llm-based agent system for long-form video understanding by uncertainty-aware cot. arXiv preprint arXiv:2504.04471. Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2024. Analyzing and mitigating object hallucination in large vision-language models. In Proceedings of the International Conference on Learning Representations (ICLR). Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, and 1 others. 2024c. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR). Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. 2020. Hero: Hierarchical encoder for video+language omni-representation pretraining. arXiv:2005.00200. Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. 2025. Videochat-r1: Enhancing spatiotemporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958. Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou. 2023. Univtg: Towards unified video-language temporal grounding. In Proceedings of the International Conference on Computer Vision (ICCV). Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023. Mitigating hallucination in large multi-modal models via robust instruction tuning. In Proceedings of the International Conference on Learning Representations (ICLR). Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. 2025. Videomind: chain-oflora agent for long video reasoning. arXiv preprint arXiv:2503.13444. Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Chang Wen Chen, and Ying Shan. 2024. E.t. bench: Towards open-ended event-level video-language understanding. In Neural Information Processing Systems (NeurIPS). Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424. Ruchit Rawal, Khalid Saifullah, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. 2024. Cinepile: long video question answering dataset and benchmark. arXiv preprint arXiv:2405.08813. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. CoRR, abs/1701.06538."
        },
        {
            "title": "Appendix",
            "content": "A.3 Details of training tering . VIDEO-SKOT Implementation Details A.1 Details of skill description & clus- . . . . . A.2 Details of skill-based CoT generation A.3 Details of training . . A.4 Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Quantitative Results Additional Qualitative Results . . . . . . . . . . datasets C.1 Skill descriptions over different . . . . C.2 Selected skills over different video . . . . C.3 Inference rationale comparison . . C.4 Additional comparison with regu- . . . . lar CoT . datasets . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D License",
            "content": "7 7 7 7 7 7 7 7 8 8 8 VIDEO-SKOT Implementation Details A.1 Details of skill description & clustering Skill Description. To extract skill descriptions given the training dataset, we prompt GPT-41 with its questions and answers. (The prompt is provided in Fig. 8) Each extracted skill is written as concise skill phrase (612 words), preserving the core visual or temporal reasoning concept. Here, we intentionally exclude audio-based cues (e.g., sound, speech, or music) in this process. Specific object names (e.g., \"TV\", \"sofa\", \"John\") are replaced with generic terms, and vague terms (e.g., \"reasoning\", \"analysis\") are avoided to enhance clarity. We also provide the exact name of the skills in Tab. 3. A.2 Details of skill-based CoT generation For skill-based CoT generation, we utilize Gemini2.0 Flash with video input. As illustrated in Fig. 9, we first prompt Gemini-2.0 to identify the relevant skills and generate corresponding sub-questions and answers. Then, we construct step-by-step reasoning based on this output. Finally, we use GPT-4 to filter and verify the reasoning by assessing its relevance to the ground-truth answers using Fig. 11 as prompt. 1gpt-4-32k 7 Training datasets. Instead of using the full video instruction tuning dataset, we randomly sampled 10k and 2.1k examples from ET-Bench and CinePile, respectively. For VSI-Bench, which is intended solely for evaluation and does not provide training set, we manually split the available data into training and test sets using 7:3 ratio. We use 3k training dataset for VSI-Bench. Hyperparameters. For training, we set the learning rate as 1e-5 and the batch size as 1. For LoRA, we use rank 32. We set 1 epoch for ET-Bench training and 3 epochs for the other two datasets. For other parameters, we use the default setup of LLaVA Video. We use 4 A6000 GPUs for training. A.4 Prompts In Figs. 8 to 11, we attach prompts for skill-based CoT annotation. We also attach prompt to generate regular CoT in Fig. 12."
        },
        {
            "title": "B Additional Quantitative Results",
            "content": "In Tabs. 4 to 6, we additionally report the percategory performance for each dataset. We also include ablation studies comparing regular CoT vs skill-based CoT, and single-LoRA vs multi-LoRA configurations. VIDEO-SKOT, which combines skill-based CoT with multi-LoRA training, consistently outperforms across all datasets, showing particularly strong gains on reasoning-intensive tasks such as Route Planning in VSI-Bench and temporal understanding tasks in CinePile."
        },
        {
            "title": "C Additional Qualitative Results",
            "content": "C.1 Skill descriptions over different datasets In Fig. 7, we visualize the skill descriptions for each dataset after performing skill extraction and clustering (Sec. 3.2). To create the visualization, we first obtain text embeddings using SentenceTransformer and compute skills cluster centroids. We then apply t-SNE to reduce the dimensionality of the embeddings for visualization purposes. The results highlight that each domain-specific dataset emphasizes different skill sets, though certain skills are shared across datasets. For instance, the skill Inferring emotional tone from facial expressions and actions from CinePile is distinct from Estimating distance between two objects in the video timeline from VSI-Bench. However, general skills like Identifying objects or people appear across VSI-Bench ET-Bench CinePile Skill descriptions \"Determine object location relative to persons orientation.\", \"Locate specific objects and identify their positions in the scene.\", \"Identify the closest object to reference point.\", \"Recognize objects based on shape, color, and features.\", \"Identify and count distinct objects based on location and appearance.\", \"Assess spatial proximity of objects relative to reference point.\", \"Identify initial appearances of objects in the video timeline.\", \"Estimate distance between two objects using visual cues.\", \"Determine room boundaries using structural elements like walls and floors.\", \"Identify sequential order of object appearances in scene.\" \"Identify head orientation and gaze direction to infer focus.\", \"Identify spatial proximity between people in the scene.\", \"Identifying the timestamp of an action in scene.\", \"Detect object using shape, texture, and visual features.\", \"Track individuals interacting with an object and their actions.\", \"Identify person performing an action involving an object.\", \"Identifying the moment an action occurs in scene.\", \"Detect person using body shape, face, and clothing.\", \"Identify actions through body movements and postures.\", \"Identify hand movement and physical interaction with an object.\" \"Identify thematic parallels between actions and overarching narrative themes.\", \"Inferring emotional tone from facial expressions and actions.\", \"Tracking emotional shifts through expressions and body language changes.\", \"Identifying interpersonal conflict through observed actions and interactions.\", \"Inferring symbolic meaning of an object in scene.\", \"Inferring emotional state from expressions and body language.\", \"Track persons movements and reactions to scene changes.\", \"Identifying body language among the scene.\", \"Identify event sequence to infer action context and significance.\", \"Identifying the main person or subject in the scene.\" Table 3: Detailed skill descriptions from three datasets. multiple datasets. more detailed list of the extracted skills is provided in Tab. 3. C.2 Selected skills over different video datasets In Figs. 3 and 4, we present statistics on the selected top 3 assigned skills for each task in VSIBench (presented in Sec. 3.2). As shown in the results, object identification skills are commonly used across tasks. However, each task also requires domain-specific skills. For instance, the Room Size Estimation task necessitates skills such as Determining room boundaries using structural elements like walls and floors. with regular CoT produces an incorrect reasoning process, ultimately leading to wrong answer. In contrast, VIDEO-SKOT successfully generates temporally grounded and precise rationales that more effectively support accurate answer generation. C.4 Additional comparison with regular CoT In Fig. 5, we provide additional comparison with regular CoT and ours."
        },
        {
            "title": "D License",
            "content": "We list the license of the benchmark dataset and models we used. We use these existing artifacts consistently with their intended use. C."
        },
        {
            "title": "Inference rationale comparison",
            "content": "LLaVA-Video: Apache License 2.0 In Fig. 6, we compare the inference-time rationales generated by LLaVA-Video trained with (a) regular CoT and (b) our proposed skill-based CoT. During inference, we prompt each model with: Explain the rationale to answer the question and answer the question. As shown in (a), the model trained CinePile: cc-by-nc-sa-4.0 VSI-Bench: Apache License 2.0 ET-Bench: cc-by-nc-sa-4.0 Appr. Order Rel. Dist. Route Plan Rel. Dir. Abs. Dist. Obj. Count Obj. Size Room Size Avg LLaVA-Video LLaVA-Video (fine-tune) Single-LoRA + Regular-CoT Single-LoRA + Skill-CoT Multi-LoRA + Regular-CoT VIDEO-SKOT (Ours) 31.28 54.16 68.75 64.58 56.25 68. 41.57 36.97 36.97 42.01 34.45 36.13 35.59 41.66 41.66 36.11 41.66 50.00 46.05 42.26 43.07 44.45 39.53 47. 10.78 29.71 31.37 33.11 12.24 32.13 52.24 56.89 62.62 56.89 43.00 62.61 47.54 70.16 70.22 74.27 55.11 70. 19.77 54.51 65.96 64.35 23.87 57.09 35.60 47.45 52.58 52.97 38.27 53.15 Table 4: Detailed VSI-Bench Results. RAR EVC RVQ TVG ERM TAL EVS VHD DVC (F1) DVC (Sim) SLC (F1) SLC (Sim) TEM GVQ Avg LLaVA-Video LLaVA-Video (fine-tune) Single-LoRA + Regular-CoT Single-LoRA + Skill-CoT Multi-LoRA + Regular-CoT 41.6 44.8 43.6 43.2 43.2 38.8 34.6 37.8 40.8 40.6 56.6 58.2 58.6 56.8 59.8 8.2 9.4 10.7 10.0 6.9 VIDEO-SKOT (Ours) 49. 41.2 59.4 15.8 1.8 2.2 1.9 1.7 1.9 2.4 14.0 12.0 13.7 15.2 16. 16.4 14.8 9.2 7.5 7.1 11.1 8.4 28.2 29.7 33.8 30.2 31.1 35.5 20.1 28.6 14 17.3 30. 27.0 10.0 14.8 12.2 16.5 16.4 15.0 11.5 10.1 6.5 10.0 6.2 11.9 8.1 10.2 10.6 10.6 9. 10.0 15.7 18.6 12.4 11.2 16.6 16.5 1.5 2.1 3.1 1 1.0 2.4 19.3 20.3 19.0 19.4 20. 22.2 Table 5: Detailed E.T-Bench Results. CRD NPA STA TH TEMP Avg LLaVA Video LLaVA Video (fine-tune) Single-LoRA + Regular CoT Single-LoRA + Skill-CoT Multi-LoRA + Regular CoT 56.78 57.74 59.18 57.88 58.17 58.53 58.32 59.61 56.8 59.17 60.31 60.44 61.81 60.77 60. 60.52 61.05 61.05 60.52 62.63 43.02 43.90 40.91 40.84 42.44 55.83 56.29 56.11 56.36 56.52 VIDEO-SKOT (Ours) 60.00 59. 61.44 63.15 45.20 57.88 Table 6: Detailed Cinepile Results. We ablate the accuracies across the question categories: TEMP - Temporal, CRD - Character and Relationship Dynamics, NPA - Narrative and Plot Analysis, STA - Setting and Technical Analysis, TH - Thematic Exploration. Figure 3: Skill selection results of VSI-Bench (1) 9 Figure 4: Skill selection results of VSI-Bench (2) Figure 5: Additional comparison of CoT annotations: (a) regular CoT and (b) our skill-based CoT. Figure 6: Inference output comparison: (a) LLaVA-Video trained with regular CoT and (b) LLaVA-Video trained with our skill-based CoT. VIDEO-SKOT successfully generates temporally grounded and precise rationales that more effectively support accurate answer generation. 10 Figure 7: Skill description from different domain datasets. We visualize the skill descriptions for each dataset after performing skill extraction and clustering. (Sec. 3.2) Figure 8: Prompt for Skill Description 11 Figure 9: Prompt for skill selection and sub-QA generation Figure 10: Prompt for skill-based CoT generation Figure 11: Prompt for CoT filtering Figure 12: Prompt for regular CoT generation"
        }
    ],
    "affiliations": [
        "University of North Carolina at Chapel Hill"
    ]
}