{
    "paper_title": "COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision",
    "authors": [
        "Jaeyoon Lee",
        "Hojoon Jung",
        "Sungtae Hwang",
        "Jihyong Oh",
        "Jongwon Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework."
        },
        {
            "title": "Start",
            "content": "COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision Jaeyoon Lee* Hojoon Jung* Sungtae Hwang Chung-Ang University, Seoul, Korea {leejaeyoon, hjjung, sthwang}@vilab.cau.ac.kr, {jihyongoh, choijw}@cau.ac.kr Jongwon Choi Jihyong Oh 5 2 0 D 9 ] . [ 2 7 0 1 7 0 . 2 1 5 2 : r https://cau-vilab.github.io/COREA/ Figure 1. Overview of COREA. (a) COREA is the first unified framework that jointly optimizes relightable 3D Gaussians and an SDF through bidirectional 3D-to-3D supervision, enabling accurate geometry learning for novel-view synthesis (NVS), mesh reconstruction, and physically-based relighting (PBR). (b) Quantitative comparison among PBR-capable methods shows that COREA achieves the highest PSNR for both NVS and PBR. (c) Among mesh-reconstructable methods, COREA attains the lowest geometric error with high rendering fidelity."
        },
        {
            "title": "Abstract",
            "content": "We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks *Co-first authors (equal contribution). Co-corresponding authors. demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within unified framework. 1. Introduction Recent advancements in 3D Gaussian splatting (3DGS) [18] have attracted significant attention due to its remarkable rendering quality and real-time performance. Beyond novelview synthesis (NVS), 3DGS has been extended to mesh reconstruction aligned with Gaussian primitives [7, 1214] and physically-based rendering (PBR) [8, 9, 16, 21, 27], where the underlying geometry is estimated from images to decompose BRDF properties and lighting for Gaussian relighting. More recently, hybrid approaches [5, 22, 41, 46] combining 3DGS with signed distance function (SDF) [24] have been proposed to enhance geometric accuracy by leveraging the complementary strengths of both representations. However, accurate geometry recovery in 3DGS is fundamentally constrained by its reliance on alpha-blending to infer 3D properties. Because geometric cues are compressed during alpha-blending, the resulting normal maps become We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and an SDF for NVS, mesh reconstruction, and PBR. We introduce bidirectional 3D-to-3D supervision strategy that directly aligns 3DGS and SDF in 3D space, enabling precise geometry reconstruction and reliable BRDFlighting estimation. We propose density control mechanism that regularizes densification using both image and geometry gradient cues, preventing over-splitting while maintaining geometric fidelity and stable rendering. We demonstrate that COREA achieves superior performance across NVS, mesh reconstruction, and PBR on standard benchmarks. 2. Related Work 2.1. Surface Reconstruction with 3DGS 3DGS enables real-time, photorealistic rendering, but its sparse and irregular structure makes accurate estimation of depth and normals challenging [7, 12, 13]. Several approaches improve surface reconstruction by projecting or regularizing 3D Gaussians. Gaussian Surfels [7] and 2DGS [14] flatten 3D Gaussians into 2D representations aligned with local tangent planes, while SuGaR [13] and Gaussian Frosting [12] construct coarse meshes from these flattened Gaussians and refine them through optimization. NormalGS [34] models the interaction between normals and illumination for more accurate normal estimation, and several approaches [25, 28] similarly incorporate depth or normal cues to regularize 3DGS optimization and improve surface reconstruction. Beyond these strategies, hybrid approaches couple 3DGS with neural implicit surfaces such as SDFs to enable mutual geometric supervision [5, 22, 38, 41, 46]. Other directions leverage large-scale priors [29, 37], multiview stereo regularization [35], or Gaussian-specific surface extraction schemes [42]. TaoAvatar [6] further connects 3D Gaussians with an implicit deformation field for full-body avatar synthesis. However, most existing hybrid methods still struggle to capture fine-grained geometry, as they rely on indirect supervision in image space or coarsely aligned geometric cues. To overcome these issues, we propose bidirectional 3D-to-3D training strategy that jointly optimizes 3DGS and SDF through mutual geometric supervision. Through direct supervision in 3D space, both representations learn accurate fine-scale geometry, resulting in consistent alignment. 2.2. Physically-based Rendering PBR aims to reconstruct accurate geometry, material, and illumination by modeling light-surface interactions from 2D observations. NeRF [23]-based inverse rendering approaches [4, 36, 39, 40, 43, 47] jointly estimate geometry, Figure 2. Comparison of normal representations. The alphablended normal is 2D normal projection that becomes blurry after mixing multiple Gaussians, resulting in coarse geometric cues. In contrast, the pixel-wise depth gradient is derived from the 3DGS depth field, preserving sharper and more structured geometry and enabling direct supervision in 3D space with SDF normals (Eq. (3)). blurry and inaccurate  (Fig. 2)  , making it inherently difficult for 3DGS to recover fine-scale geometry. This indirect supervision in 2D image space inevitably leads to coarse surface structure. These observations indicate that geometric learning should rely on signals originating directly from 3D space rather than compressed 2D renderings. As illustrated in Fig. 2, pixel-wise depth gradients [9] computed from the 3DGS depth field preserve sharper and more structured geometry than alpha-blended normals, demonstrating the effectiveness of 3D-derived cues for fine-scale geometry. To address these issues, we propose COREA, the first unified framework that jointly learns relightable 3D Gaussians and an SDF through bidirectional 3D-to-3D supervision. Instead of inferring 3D properties from 2D images, COREA directly aligns both representations in 3D space. For SDF learning, we use the depth of 3D Gaussians to guide SDF ray sampling, establishing coarse geometric alignment between the two representations. The SDF is then refined by aligning its surface normals with the pixel-wise depth gradients from 3DGS, yielding more accurate and detailed surface geometry. For Gaussian learning, we project Gaussians onto the SDF surface and refine their normals through direct supervision from SDF normals, achieving consistent and precise geometry. To further enhance detail, we employ normalaware densification, enabling Gaussians to capture subtle surface-level geometry. To regulate this normal-aware densification, we introduce density control mechanism that evaluates image and geometry gradient curvature, allowing Gaussians to split only when it meaningfully reduces the loss and thereby preserving geometric fidelity and rendering stability. Using the jointly optimized Gaussians, we perform inverse PBR [17] to reliably disentangle BRDF parameters and lighting for improved relighting performance. As shown in Fig. 1 (a), COREA is the first unified framework that simultaneously performs NVS, mesh reconstruction, and PBR, based on precisely aligned geometry. It achieves superior performance in NVS and PBR (Fig. 1 (b)), while also delivering high-quality mesh reconstruction results (Fig. 1 (c)). Our key contributions are as follows: Figure 3. Overview of the COREA framework. Our method jointly trains relightable 3D Gaussians and SDF via coarse-to-fine bidirectional 3D-to-3D supervision. The first stage, Bidirectional 3D-to-3D Supervision, consists of two complementary steps: (i) DSA aligns the SDF to the 3DGS by leveraging depth rendered from Gaussians and matching SDF normals to pixel-wise depth gradients of 3DGS (Eq. (3)); (ii) NGA aligns 3DGS to the SDF by matching Gaussian depth to the SDF depth and supervising Gaussian normals with SDF normals (Eq. (4)). To prevent excessive Gaussian splitting during NGA, the DDC module suppresses unnecessary densification for efficient geometry refinement. These steps are jointly applied in each iteration to refine coarse and fine geometry. In the second stage, Inverse PBR is performed using the refined geometry to decompose BRDF and lighting and enable relighting under novel illumination conditions. material properties, and illumination, but remain computationally expensive. Several reflectance-aware works [30, 48] improve appearance decomposition by modeling specular reflections or indirect illumination. Recent studies [8, 9, 11, 16, 21, 27, 45] extend 3DGS to this domain, exploiting its real-time efficiency and explicit geometric representation. GIR, GS-IR, and GaussianShader [16, 21, 27] extend 3DGS with inverse rendering frameworks that factorize geometry, material, and lighting, enforce normal regularization and occlusion modeling, and learn shading function for reflective scenes. R3DG [9] integrates point-based ray tracing into 3DGS for PBR, enabling finer BRDF and lighting decomposition. IRGS [11] incorporates the full rendering equation into 2D Gaussian Splatting using differentiable 2D Gaussian ray tracing. Ref-Gaussian [45] focuses on capturing fine inter-reflection effects through physically based deferred rendering pipeline. RNG [8] replaces the analytical rendering equation with neural modules that learn Gaussian-light interactions. Although these methods demonstrate the potential of Gaussian-based representations for PBR, they struggle to achieve physically consistent BRDF-lighting decomposition due to insufficient geometric fidelity. To overcome this limitation, we propose unified framework that jointly trains relightable 3DGS with SDF, enabling direct 3D-to-3D supervision. The resulting geometry-aware Gaussians provide accurate surface normals, supporting faithful inverse rendering and reflectance decomposition. 3. Proposed Method : COREA We propose COREA, joint training framework that integrates relightable 3DGS with SDF for accurate geometry reconstruction and PBR. As illustrated in Fig. 3, COREA performs bidirectional 3D-to-3D supervision using both depth and normal cues. Depth-guided alignment provides coarse geometric consistency, while depth-gradient and normal refine fine-scale geometry in both representations. We further employ density-control mechanism that prevents excessive Gaussian splitting and preserves both geometric fidelity and rendering quality. By unifying geometry learning and relightable rendering, COREA achieves precise geometry reconstruction and BRDF-lighting decomposition. COREA operates in two stages. The first stage focuses on geometric alignment through bidirectional 3D-to-3D supervision (Sec. 3.2). This stage alternates between two complementary steps: (i) Depth-guided SDF Alignment (DSA), which aligns the SDF geometry using both the rendered depth and its pixel-wise gradients from 3DGS, and (ii) Normal-guided Gaussian Alignment (NGA), where Gaussians are first projected onto the SDF depth surface and then refined using SDF normals to capture fine-scale geometry and trigger densification. During NGA, normal-aware densification enhances geometric detail, while Dual-Density Control (DDC) (Sec. 3.3) periodically suppresses redundant splitting to maintain memory efficiency. After the geometry converges, the second stage performs inverse PBR (Sec. 3.4), optimizing BRDF parameters and lighting on the learned geometry to enable accurate and spatially consistent relighting. In our implementation, we use R3DG [9] for the 3D Gaussian representation and NeuS [31] for the SDF representation. 3.1. Preliminary Inverse PBR with 3DGS. For inverse rendering [17] of 3DGS, each Gaussian Pi is defined with BRDF parameters including albedo bi, roughness ri, and an indirect light term li, along with shared global environment map lenv. The diffuse term fd is determined by the albedo bi, while the specular term fs depends on the incoming direction ωi and outgoing direction ωo, together forming the BRDF response of each Gaussian. Each Gaussians physically-based color is computed as: Li(ωi) = (ωi) lenv(ωi) + li(ωi), (1) c(ωo) = Ns(cid:88) i=1 [fd + fs(ωo, ωi)] Li(ωi) (ωi n) ωi. (2) where (ωi) indicates the visibility along the incoming direction, Li(ωi) represents the incident radiance composed of the environment light lenv and the learned indirect term li, and is the Gaussian normal. This formulation enables spatially varying relighting and remains fully compatible with the rasterization-based rendering of 3DGS. The implementation of the inverse PBR formulation follows R3DG [9]. 3.2. Bidirectional 3D-to-3D Supervision To jointly refine the geometry of SDF and 3DGS, we propose bidirectional 3D-to-3D supervision strategy that achieves both coarse and fine alignment. This strategy consists of two complementary steps: DSA and NGA."
        },
        {
            "title": "3.2.1 Depth-guided SDF Alignment (DSA)",
            "content": "To achieve accurate and stable geometry learning, we design DSA to align the SDF with 3DGS in coarse-to-fine manner. The key intuition is that depth alignment provides reliable global geometry, while the gradient of the depth map reflects local surface variations. Since the SDF gradient inherently represents surface normals, we use it to refine fine-scale geometry by aligning its gradient field with the depth-gradient directions derived from 3DGS. Figure 4. Dual-Density Control (DDC). Gaussians with accumulated gradients exceeding the threshold are shown in red, while others remain blue. For those in red, splitting matrices from image and normal losses are combined into the total matrix Stotal (Eq. (5)). Only those with λmin(Stotal) < 0 are further divided into green Gaussians, exclusively when it contributes to overall loss reduction. We begin with coarse alignment by rendering depth map from 3DGS to guide ray sampling of SDF. Following the ray-based sampling strategy of GSDF [41], we adopt an adaptive sampling interval centered on d, allowing the SDF to focus on learning geometry near the Gaussian surface. To encourage geometric consistency, we minimize an L1 loss (Ldepth) between the depth maps of the SDF and 3DGS. For fine-grained alignment, we compute the pixel-wise gradients of and use them as local normal approximation (cid:101)n3DGS. Since the SDF implicitly encodes surface normals via its gradient, we minimize cosine-similarity loss between the normalized SDF gradient nSDF and (cid:101)n3DGS:"
        },
        {
            "title": "LSDF",
            "content": "normal ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) (cid:16) i=1 1 (cid:68) SDF, (cid:101)n(i) n(i) 3DGS (cid:69)(cid:17) , (3) where is the number of valid pixels. The target pseudonormal (cid:101)n3DGS is defined as d/d. The loss LSDF normal encourages the SDF gradient to align with the depth-gradient directions of 3DGS under local planarity assumption. By aligning the 3DGS depth gradients with the SDFs gradient field, our pixel-wise normal supervision achieves precise geometry alignment while preserving the distinct characteristics of both representations. Consequently, the SDF reconstructs surface tightly aligned with 3DGS, providing reliable foundation for subsequent relighting and ensuring high geometric fidelity."
        },
        {
            "title": "3.2.2 Normal-guided Gaussian Alignment (NGA)",
            "content": "Accurate BRDF decomposition and relighting fundamentally rely on reliable fine-scale geometry and surface normals. Unlike previous methods [8, 9, 16, 21, 27] that inferred geometry in the 2D image domain, we directly supervise each 3D Gaussian normal n3DGS in 3D space by leveraging the SDF surface normal nSDF, ensuring precise and spatially consistent normal alignment. We first apply depth loss Ldepth between the depth maps of 3DGS and SDF to anchor global geometric consistency. Fine-scale accuracy is then achieved through pixel-wise cosine-similarity loss between n3DGS and nSDF: rendering performance. L3DGS normal ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) (cid:16) i=1 1 (cid:68) n(i) 3DGS, n(i) SDF (cid:69)(cid:17) . (4) This 3D-to-3D normal alignment allows each Gaussian to recover fine surface variation at the pixel level. To further enhance geometric detail, we employ the gradient magnitude of L3DGS normal as an additional adaptive signal for Gaussian densification. While conventional 3DGS performs densification based only on image-space gradients to improve rendering quality, we additionally apply densification to Gaussians that exhibit large gradients of L3DGS normal. This additional criterion encourages refinement in regions where geometric discrepancies remain high, achieving balanced enhancement in both appearance and geometry. Driven by these dual gradient signals, we adaptively densify Gaussians to enhance both rendering fidelity and fine geometric detail. 3.3. Dual-Density Control (DDC) The original adaptive density control of 3DGS splits Gaussian when the accumulated gradient magnitude exceeds predefined threshold. In NGA, we densify Gaussians according to the pixel-wise normal loss L3DGS normal, which enhances fine 3D geometry. However, since each Gaussian projects onto multiple pixels, the gradients accumulate excessively, often resulting in unnecessary and redundant splits. image and L3DGS To mitigate such redundant splitting, we propose Dual-Density Control (DDC) strategy, illustrated in Fig. 4. Inspired by the steepest density control strategy of SteepGS [32], DDC extends this concept to dual-loss scheme that jointly optimizes L3DGS normal. For each Gaussian i, we compute splitting matrices S(i) image and S(i) normal from the two loss terms, respectively. These matrices approximate the local curvature of the loss landscape with respect to the Gaussian center µ(i), providing both criterion for whether splitting will reduce the loss and direction in which to place the split Gaussians. To balance the influence of both loss terms, we then form weighted sum of the two matrices: total = (1 α) S(i) S(i) image + α S(i) normal, (5) where α [0, 1] controls the contribution of the normal loss to the splitting decision. Gaussian is split only if the minimum eigenvalue of S(i) total) < 0, indicating descending curvature direction that benefits overall loss reduction. total is negative, i.e., λmin(S(i) Through DDC, we achieve geometry-aware Gaussian densification by navigating the joint curvature space of both image and normal losses. This curvature-guided splitting enables effective densification while constraining excessive Gaussian growth, maintaining geometric fidelity and stable 3.4. Physically-Based Rendering (PBR) The bidirectional 3D-to-3D supervision yields accurate and consistent geometry, forming the foundation for PBR of relightable 3D Gaussians. Building upon this joint training framework, we further learn BRDF parameters and lighting in physically consistent manner. Following the inverse rendering formulation in Sec. 3.1, we supervise the BRDF parameters and the incident lighting defined over each Gaussian. To model visibility and shading, we employ the point-based BVH ray tracing scheme from R3DG [9], which enables transmittance-aware visibility estimation for Gaussian primitives. In this stage, images are reconstructed through physically-based rendering by decomposing and recombining BRDF and lighting components, and supervised with an L1 loss against the GT image. The accurate geometry obtained from the bidirectional 3D-to3D supervision enables accurate modeling of light-surface interactions, leading to physically faithful rendering and improved relighting quality. Once the BRDF and lighting parameters are optimized, relighting under arbitrary illumination is performed by replacing the global environment map lenv with novel lighting condition lenv new while excluding the precomputed indirect illumination term li. The resulting incident radiance Lnew is then computed as in Eq. (1), and the final physically-based color c(ωo) is rendered using Eq. (2). This stage completes COREAs unified pipeline, where jointly learned geometry and reflectance enable physically-based relighting with high fidelity and consistency. 4. Experiments 4.1. Experimental Setup We evaluate our method on two datasets and three tasks to enable unified assessment of NVS, mesh reconstruction, and PBR. All experiments are conducted on single NVIDIA A6000 GPU (48 GB). Datasets. The DTU dataset [15] provides 15 real-world object scans under controlled lighting and geometry conditions. For Tanks&Temples [19], we follow the R3DG [9] setup and use the four object-centric scenes defined in their work. This setup enables consistent evaluation of geometry and appearance under real-world conditions. Evaluation Metrics. For NVS, we compare all methods using PSNR, SSIM [33], and LPIPS [44]. PBR evaluation is conducted only for models capable of BRDF-based rendering, using the same metrics as NVS. For mesh reconstruction, we evaluate methods that output explicit meshes and measure geometric accuracy using Chamfer Distance (CD). Figure 5. Qualitative Comparison Across Tasks. We compare COREA with recent Gaussian-based methods on three tasks: NVS, PBR, and Mesh Reconstruction. All results are rendered on white background for consistent visual comparison. Artifacts such as black background patches or dark speckles observed in some baselines stem from excessive Gaussian opacity; by contrast, COREA produces clean, artifact-free renderings on white backgrounds. Our method also provides results for all three tasks, whereas others show N/S (Not Supported) or OOM (Out of Memory) in several cases. Overall, COREA yields sharper novel views, more faithful BRDF and lighting decomposition, and finer geometric details through coarse-to-fine bidirectional 3D-to-3D supervision. Blue, green, and orange boxes denote NVS, PBR, and mesh reconstruction results, respectively. Additional qualitative results are provided in the Supplementary Material and demo video. 4.2. Evaluation Results We render all qualitative results on white background to ensure clarity and consistency across evaluation tasks, emphasizing overall rendering quality and clear separation between objects and the background. Black speckles or dark edge artifacts in some methods indicate opacity noise caused by excessive Gaussian density. Methods that do not support specific tasks are marked as N/S, and those that exceed GPU memory limits during training are marked as OOM. Our method uniquely unifies NVS, PBR, and mesh reconstruction through bidirectional 3D-to-3D supervision between 3DGS and SDF, achieving strong performance across all tasks and datasets. Additional per-scene results are provided in the Supplementary Material and demo video. generalization to real-world objects with non-uniform reflectance and complex illumination. On Tanks&Temples, the performance is slightly below the top baseline due to the larger scene scale, which makes SDF alignment and joint optimization with 3DGS less stable across wide spatial ranges. Nevertheless, our framework achieves the best performance in PBR (Sec. 4.2.2) and mesh reconstruction (Sec. 4.2.3) on this dataset, while being the only method capable of handling all three tasks within single unified framework. As shown in Fig. 5 (blue boxes), our method produces sharper and more coherent novel views, free from background artifacts commonly observed in other approaches. These results highlight the geometric consistency and precise alignment between SDF and 3DGS, which are essential for stable relighting and background replacement."
        },
        {
            "title": "4.2.2 Physically-Based Rendering Comparison",
            "content": "We evaluate NVS performance on the DTU [15] and Tanks&Temples [19] datasets. As summarized in Tab. 1 (left), our method achieves the best results on DTU across all metrics (PSNR, SSIM, and LPIPS), demonstrating strong We evaluate our framework for PBR against prior relightable Gaussian approaches [9, 16, 21]. As shown in Tab. 1 (right), our method achieves the highest PBR quality among all relightable baselines, demonstrating that precise geometry Table 1. Unified Quantitative Comparison. We evaluate novel-view synthesis (NVS), mesh reconstruction, and physically-based rendering (PBR) using the DTU and Tanks&Temples datasets. N/S (Not Supported) denotes methods that do not support the corresponding task, and OOM (Out of Memory) indicates evaluation failures caused by memory limitations. COREA achieves superior performance in both geometry (CD) and appearance (PSNR/SSIM/LPIPS), while uniquely supporting all three tasks, including NVS, mesh reconstruction, and PBR, within unified framework. The best, second-best, and third-best results are highlighted in red, yellow, and purple, respectively. Dataset DTU Tanks&Temples DTU (PBR) Tanks&Temples (PBR) Category Method Mesh PBR CD PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 3DGS 3DGS (SIGGRAPH23) GaussianShader (CVPR24) GS-IR (CVPR24) R3DG (ECCV24) Gaussian Frosting (ECCV24) Hybrid GSDF (NeurIPS24) GS-Pull (NeurIPS24) COREA (Ours) N/S N/S N/S N/S 1.707 0.859 0.916 0.824 27.30 22.14 28.29 31.82 24.22 27.76 27.36 32.27 0.867 0.826 0.874 0.944 0.755 0.874 0.872 0. 0.184 0.201 0.173 0.105 0.265 0.176 0.208 0.094 27.46 19.96 28.93 29.92 23.29 0.910 0.851 0.929 0.941 0.862 0.115 0.139 0.089 0.079 0.158 OOM OOM OOM 0.142 0.893 26.29 0.089 0.937 29. N/S 19.49 24.56 26.44 N/S N/S N/S 29.72 N/S 0.719 0.752 0.914 N/S N/S N/S 0.942 N/S 0.393 0.206 0.138 N/S N/S N/S 0. N/S 17.82 22.96 26.31 N/S N/S N/S 27.57 N/S 0.829 0.652 0.912 N/S N/S N/S 0.927 N/S 0.169 0.186 0.103 N/S N/S N/S 0. Figure 6. Qualitative Results of Physically-Based Rendering and Relighting. We compare COREA with recent relightable Gaussianbased methods under varying illumination conditions. The first row shows PBR renderings under the original lighting setup, while the remaining rows illustrate relighting results under directional lights and various HDR environment maps. Compared to previous methods that exhibit black background artifacts or unstable reflectance under varying illumination, COREA maintains consistent shading, clean appearance, and accurate reflectance reconstruction across all lighting conditions, demonstrating robust BRDF and geometry alignment. reconstruction directly contributes to more accurate shading and reflectance recovery. For qualitative evaluation, Fig. 5 (green boxes) presents the PBR images under the original lighting conditions. Our method reproduces fine-scale surface details more faithfully, as seen in the normal patches at the bottom right, where surface orientations are accurately captured and well represented in the rendered appearance. Extended results are provided in Fig. 6, where the first row shows the same PBR reconstruction, the second row depicts novel-light relighting, and the third and fourth rows illustrate rendering under different environment maps. COREA maintains consistent shading and reflectance across lighting changes, producing natural, noise-free results without background artifacts. This robustness stems from our bidirectional 3D-to-3D supervision, which enforces consistent geometry alignment between 3DGS and SDF across all views, enabling stable relighting and realistic illumination results."
        },
        {
            "title": "4.2.3 Mesh Reconstruction Comparison",
            "content": "We compare our framework with mesh-based baselines [12, 41, 46] on the DTU dataset for both quantitative and qualitative evaluation. As summarized in Tab. 1 (left), our method achieves the lowest CD, establishing the best mesh reconstruction accuracy. The qualitative results in Fig. 5 (orange boxes) further demonstrate that our method reconstructs sharper and more detailed surfaces, faithfully capturing finescale geometric structures, which are oversmoothed in baseline methods. This improvement arises from our bidirectional 3D-to-3D supervision, where the pixel-wise depth gradient of 3DGS directly supervises the SDF gradient. This enables fine-scale geometry learning directly in 3D space rather than inferring it from 2D projections, yielding sharper and more consistent reconstructions. Such accurate geometric alignment also provides reliable surface normals for the subsequent relightable PBR optimization. Figure 7. Effect of the DDC weight α on reconstruction quality and splitting dynamics. The bar plots show the PSNR for NVS and PBR, while the line plot indicates the cosine similarity between the principal eigenvectors of Simage and Stotal (Eq. (5)). The normalized Gaussian ratio (relative to α=0) visualizes how the amount of splitting varies with α. As α increases, the cosine similarity drops sharply after α0.5, suggesting shift from image-driven to normal-driven splitting. PBR PSNR peaks at α=0.2, providing the best balance between geometric fidelity and Gaussian efficiency. 4.3. Ablation Study"
        },
        {
            "title": "4.3.1 Trade-off Analysis of α in DDC",
            "content": "We analyze how the DDC weight α affects reconstruction quality and the internal splitting behavior. To this end, we vary α {0, 0.2, 0.5, 0.7, 1.0} and measure the PSNR for NVS and PBR, the number of Gaussians (normalized by the count at α = 0), and the cosine similarity between the principal eigenvectors of the image-only splitting matrix Simage and the combined matrix Stotal (Eq. (5)). As shown in Fig. 7, the cosine similarity rapidly decreases once α 0.5, indicating that the splitting direction gradually shifts from an image-driven to normal-driven direction. This shift is accompanied by slight increase in the Gaussian ratio, suggesting that the normal loss triggers additional splitting for fine geometric refinement. While the NVS PSNR remains stable for all α, the PBR performance shows mild improvement, peaking at α = 0.2. For larger α, the reconstruction quality slightly decreases, implying that excessive normaldriven splitting disrupts stable geometry learning. These observations indicate that the DDC weight α controls the balance between image gradient and normal gradient, thereby regulating geometric quality and Gaussian growth. Based on this empirical analysis, we adopt α = 0.2 as the default setting, which provides the best balance between geometric fidelity, rendering quality, and Gaussian efficiency across all experiments."
        },
        {
            "title": "4.3.2 Depth-Gradient-Based Geometry Alignment",
            "content": "We conduct an ablation study to verify the effectiveness of pixel-wise depth gradient supervision for geometry alignment. Fig. 8 shows qualitative comparison between alphaFigure 8. Qualitative Comparison of Normal Supervision Strategies. The upper row shows PBR renderings, and the lower row presents reconstructed meshes. Alpha-blended normal supervision produces blurred shading cues and inaccurate mesh geometry, whereas pixel-wise depth-gradient supervision yields sharper shading contrast and more refined surface geometry. blended normal supervision and our depth-gradient supervision. The upper row presents PBR renderings with normal patches, while the lower row shows reconstructed meshes with zoomed-in regions. Under alpha-blended normal supervision, the normal patches exhibit weaker shading contrast, and the reconstructed meshes appear inflated with missing fine geometric structure. This degradation occurs because the rendered normal maps are blurred by alpha blending, causing the SDF to learn inaccurate geometry. As training progresses, these SDF errors propagate to the 3DGS normals through mutual supervision, jointly degrading the geometry of both representations. In contrast, depth-gradient supervision produces sharper shading normals and more refined surface geometry. Since the SDF gradient inherently represents the surface normal, learning directly from 3DGS depth gradients in 3D space yields more accurate and consistent surface geometry. Consequently, the well-defined SDF normals provide reliable supervision for 3DGS, enabling precise normal learning during joint training. 5. Conclusion We presented COREA, the first unified framework that jointly learns relightable 3D Gaussians and an SDF using coarse-to-fine bidirectional 3D-to-3D supervision. By aligning SDF geometry through Gaussian depth and pixel-wise depth gradients, and refining Gaussian normals using SDFderived normals, COREA establishes tightly coupled geometric representation that preserves fine-scale structure. dual-loss curvature-guided density control further stabilizes densification and balances geometric fidelity with memory efficiency. These components provide consistent geometric foundation for inverse rendering and support accurate BRDFlighting decomposition. Extensive experiments demonstrate that COREA achieves superior performance in NVS, mesh reconstruction, and PBR within unified framework."
        },
        {
            "title": "References",
            "content": "[1] Adobe after effects. https : / / www . adobe . com / products/aftereffects.html, 2024. 4 [2] Blender foundation: Blender. https://www.blender. org/, 2024. 4 [3] Supersplat: Web-based 3d gaussian splat editor. https: //github.com/playcanvas/supersplat, 2025. 4 [4] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, and Hendrik Lensch. Nerd: Neural reflectance decomposition from image collections. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1268412694, 2021. [5] Hanlin Chen, Chen Li, and Gim Hee Lee. Neusg: Neural implicit surface reconstruction with 3d gaussian splatting guidance, 2023. 1, 2 [6] Jianchuan Chen, Tao Yang, Zeyu Wang, Rui Zhang, Zekai Li, Yandong Guo, and Xu Cao. Taoavatar: Real-time lifelike fullbody talking avatars for augmented reality via 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1072310734, 2025. 2 [7] Pinxuan Dai, Jingyu Xu, Weikai Xie, Xiaoguang Liu, Hongzhi Wang, and Wei Xu. High-quality surface reconstruction using gaussian surfels. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 1, 2 [8] Jiahui Fan, Fujun Luan, Jian Yang, Miloš Hašan, and Beibei Wang. Rng: Relightable neural gaussians, 2024. 1, 3, 4 [9] Jian Gao, Chun Gu, Youtian Lin, Zhihao Li, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable 3d gaussians: Realistic point cloud relighting with brdf decomposition and ray tracing. In European Conference on Computer Vision, pages 7389. Springer, 2024. 1, 2, 3, 4, 5, 6 [10] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes, 2020. 1 [11] Chun Gu, Xiaofei Wei, Zixuan Zeng, Yuxuan Yao, and Li Zhang. Irgs: Inter-reflective gaussian splatting with 2d gaussian ray tracing, 2025. [12] Antoine Guédon and Vincent Lepetit. Gaussian frosting: Editable complex radiance fields with real-time rendering. In European Conference on Computer Vision, pages 413430, 2024. 1, 2, 7 [13] Antoine Guédon and Vincent Lepetit. Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53545363, 2024. 2 [14] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shiyu Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 1, 2 [15] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs. Large scale multi-view stereopsis evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2014. 5, 6 splatting with shading functions for reflective surfaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 53225332, 2024. 1, 3, 4, 6 [17] James T. Kajiya. The rendering equation. In Proceedings of the 13th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH), pages 143150. ACM, 1986. 2, [18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):139:1139:14, 2023. 1 [19] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (TOG), 36 (4):78:178:13, 2017. 5, 6 [20] Zhaoshuo Li, Thomas Müller, Alex Evans, Russell H. Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84568465, 2023. 1 [21] Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, and Kui Jia. Gs-ir: 3d gaussian splatting for inverse rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2164421653, 2024. 1, 3, 4, 6 [22] Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, and Xiaojuan Qi. 3dgsr: Implicit surface reconstruction with 3d gaussian splatting. ACM Transactions on Graphics (TOG), 43(6):112, 2024. 1, 2 [23] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision, 2020. 2 [24] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165174, 2019. [25] Xuqian Ren, Matias Turkulainen, Jiepeng Wang, Otto Seiskari, Iaroslav Melekhov, Juho Kannala, and Esa Rahtu. Ags-mesh: Adaptive gaussian splatting and meshing with geometric priors for indoor room reconstruction using smartIn 2025 International Conference on 3D Vision phones. (3DV), pages 10801090. IEEE, 2025. 2 [26] Radu Alexandru Rosu and Sven Behnke. Permutosdf: Fast multi-view reconstruction with implicit surfaces using permutohedral lattices. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8466 8475, 2023. 1 [27] Yahao Shi, Yanmin Wu, Chenming Wu, Xing Liu, Chen Zhao, Haocheng Feng, Jian Zhang, Bin Zhou, Errui Ding, and Jingdong Wang. Gir: 3d gaussian inverse rendering for relightable scene factorization, 2023. 1, 3, 4 [16] Yifan Jiang, Junhao Tu, Yuxuan Liu, Xin Gao, Xiaoyu Long, Wenqiang Wang, and Yue Ma. Gaussianshader: 3d gaussian [28] Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, and Juho Kannala. Dn-splatter: Depth unbounded scenes. ACM Transactions on Graphics (TOG), 43(6):113, 2024. [43] Jingyang Zhang, Yao Yao, Shiwei Li, Jingbo Liu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. Neilf++: Inter-reflectable light fields for geometry and material esIn Proceedings of the IEEE/CVF International timation. Conference on Computer Vision, pages 36013610, 2023. 2 [44] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. 5 [45] Rui Zhang, Tianyue Luo, Weidong Yang, Ben Fei, Jingyi Xu, Qingyuan Zhou, Keyi Liu, and Ying He. Refgaussian: Disentangling reflections from 3d gaussian splatting for realistic rendering, 2024. 3 [46] Wenyuan Zhang, Yu-Shen Liu, and Zhizhong Han. Neural signed distance function inference through splatting 3d gaussians pulled on zero-level set, 2024. 1, 2, 7 [47] Xiuming Zhang, Pratul Srinivasan, Boyang Deng, Paul Debevec, William Freeman, and Jonathan Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ACM Transactions on Graphics (ToG), 40(6):118, 2021. 2 [48] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou. Modeling indirect illumination for In Proceedings of the IEEE/CVF Coninverse rendering. ference on Computer Vision and Pattern Recognition, pages 1864318652, 2022. 3 [49] Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, and Hengshuang Zhao. Pixel-gs: Density control with pixel-aware gradient for 3d gaussian splatting. In European Conference on Computer Vision, pages 326342. Springer, 2024. 3 [50] Zuoliang Zhu, Beibei Wang, and Jian Yang. Gs-ror2: Bidirectional-guided 3dgs and sdf for reflective object relighting and reconstruction. ACM Transactions on Graphics, 45(1):119, 2025. [51] Zuo-Liang Zhu, Jian Yang, and Beibei Wang. Gaussian splatting with discretized sdf for relightable assets. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2515525164, 2025. 3 and normal priors for gaussian splatting and meshing. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 24212431. IEEE, 2025. 2 [29] Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, and Juho Kannala. Dn-splatter: Depth In and normal priors for gaussian splatting and meshing. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 24212431. IEEE, 2025. 2 [30] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan Barron, and Pratul Srinivasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 3 [31] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction, 2021. 4 [32] Peng Wang, Yifan Wang, Dong Wang, Shubham Mohan, Zhiwen Fan, Liang Wu, others, and Raj Ranjan. Steepest descent density control for compact 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2666326672, 2025. [33] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 5 [34] Meng Wei, Qiang Wu, Jiawei Zheng, Hamid Rezatofighi, and Jianfei Cai. Normal-gs: 3d gaussian splatting with normalinvolved rendering, 2024. 2 [35] Yaniv Wolf, Amit Bracha, and Ron Kimmel. Gs2mesh: Surface reconstruction from gaussian splatting via novel stereo views. In European Conference on Computer Vision (ECCV), pages 207224, Cham, 2024. Springer Nature Switzerland. 2 [36] Haoqian Wu, Zhipeng Hu, Lincheng Li, Yongqiang Zhang, Changjie Fan, and Xin Yu. Nefii: Inverse rendering for reflectance decomposition with near-field indirect illumination. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42954304, 2023. 2 [37] YuanZheng Wu, Jin Liu, and Shunping Ji. 3d gaussian splatting for large-scale surface reconstruction from aerial images, 2024. 2 [38] Haodong Xiang, Xinghui Li, Kai Cheng, Xiansong Lai, Wanting Zhang, Zhichao Liao, Long Zeng, and Xueping Liu. Gaussianroom: Improving 3d gaussian splatting with sdf guidance and monocular cues for indoor scene reconstruction, 2025. 2 [39] Jing Yang, Hanyuan Xiao, Wenbin Teng, Yunxuan Cai, and Yajie Zhao. Light sampling field and brdf representation for physically-based neural rendering, 2023. 2 [40] Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. Neilf: Neural incident light field for physically-based material estimation. In European Conference on Computer Vision, pages 700716. Springer, 2022. [41] Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, and Bo Dai. Gsdf: 3dgs meets sdf for improved rendering and reconstruction, 2024. 1, 2, 4, 7 [42] Zehao Yu, Torsten Sattler, and Andreas Geiger. Gaussian opacity fields: Efficient adaptive surface reconstruction in COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Implementation details 6.1. Training Objectives We adopt two-stage training scheme that first aligns the underlying geometry and then optimizes BRDF and lighting through inverse rendering. We begin by defining bidirectional depth supervision strategy, where the depth maps rendered from each representation are used to guide the other. Specifically, we apply an L1 loss between the predicted depth from SDF and the depth rendered from 3DGS, and vice versa. This coarse-level alignment allows both representations to share common global geometry structure: Ldepth ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:12) (cid:12)d(i) (cid:12) SDF d(i) 3DGS (cid:12) (cid:12) (cid:12) (1s) where dSDF and d3DGS denote the depth values rendered from the SDF and 3DGS, respectively, and is the number of valid pixels. We separately train the SDF and 3DGS networks but enforce mutual consistency via depth and normal alignment. The total geometry loss is: LGeometry = LSDF + L3DGS. (2s) The loss function for the SDF network is: LSDF = λSDF LSDF 1 + λdepthLdepth + λeikLeik+ λcurvLcurv + λnormalLSDF normal (3s) 1 where LSDF is reconstruction term used for coarse alignment, Leik enforces the Eikonal constraint [10], Lcurv encourages geometric smoothness [20, 26], and LSDF normal (Eq. (3)) aligns the SDF gradient with the pixel-wise depth gradient from 3DGS, as described in DSA (Sec. 3.2.1). The loss function for the 3DGS network is defined as: L3DGS = λ3DGS L3DGS + λssimLssim + λdepthLdepth+ λOLO + λuLu + λnormalL3DGS normal (4s) Here, LO and Lu are mask and depth-based constraints adopted from R3DG [9], and L3DGS normal (Eq. (4)) is the cosine loss used in NGA (Sec. 3.2.2) to align 3DGS normals with SDF normals. After geometry is fixed, we optimize BRDF parameters and lighting via inverse rendering. Following R3DG [9], we sample Ns = 64 incident rays per Gaussian and reconstruct images through physically-based rendering. The appearance loss is defined as: LPBR = λ1L1 + λssimLssim + λlLl + λbrdfLbrdf. (5s) Here, Ll is lighting consistency loss that stabilizes the decomposition of direct and indirect illumination, and Lbrdf regularizes BRDF parameters (albedo and roughness), following R3DG [9]. 6.2. Training Schedule As illustrated in Fig. 3, our training proceeds in two sequential stages. The first stage focuses on geometry alignment through bidirectional 3D-to-3D supervision, while the second stage performs inverse PBR optimization for relighting. We begin by pretraining the 3D Gaussians for 15k iterations to establish an initial scene representation. In the first stage, we jointly train the 3DGS and SDF using our bidirectional 3D-to-3D supervision strategy, alternating between the two complementary steps, DSA and NGA. During each iteration, depth maps from the two representations guide each other, providing coarse alignment for 10k iterations. We then perform an additional 20k iterations to refine finescale geometry by enforcing pixel-wise normal consistency between the SDF and 3DGS. During this refinement stage, we also activate our DDC strategy starting at the 10k iteration, applying it every 100 iterations to regulate Gaussian splitting and maintain stable geometric updates. In the second stage, we optimize BRDF and lighting parameters on the aligned geometry for 10k iterations using the PBR loss (Eq. (5s)), enabling accurate inverse rendering and relighting. 6.3. Hyperparameter Settings In our framework, depth and normal consistency terms are shared across both the SDF and 3DGS branches. {λdepth, λnormal} are set to {0.01, 0.001}, where the normal weight is kept relatively small since normal gradients tend to dominate optimization, which often leads to unstable training. For the SDF branch, we employ image reconstruction, Eikonal, and curvature regularization. {λSDF , λeik, λcurv} are set to {1.0, 0.1, 0.05}, following the configuration of GSDF [41]. For the 3DGS branch, we adopt the weights from the first stage setup of R3DG [9]. {λ3DGS , λssim, λO, λu} are set to {0.8, 0.2, 0.01, 0.01}. The DDC coefficient is set to α = 0.2, which balances the influence of image-space and Table A. Ablation of Directional Alignment and Update Strategies. Removing NGA forces 3DGS to rely only on image-space cues, leading to blurred normals and causing larger degradation in PBR. Removing DSA prevents the SDF from receiving depth-based anchoring, producing misaligned surface that mainly degrades reconstruction quality. Among the full settings, the simultaneous update scheme improves over the w/o baselines, but the alternative update scheme (COREA) provides more stable geometric supervision and achieves the best performance across mesh accuracy, NVS, and PBR. Mesh NVS PBR Setting CD PSNR SSIM LPIPS PSNR SSIM LPIPS w/o NGA w/o DSA Full-Simul. Full-Alt. (COREA) 0.863 0.932 0.852 0.824 31.47 31.50 31.62 32.27 0.944 0.945 0.946 0.955 0.104 0.097 0.105 0. 28.63 28.23 29.07 29.72 0.926 0.908 0.931 0.942 0.124 0.132 0.123 0.104 geometry-space gradients during Gaussian densification, as described in Sec. 4.3.1. After geometry optimization, we fix the structure and optimize BRDF parameters and lighting in the second stage. {λ1, λssim, λl, λbrdf} are set to {0.8, 0.2, 0.0001, 0.01}, following the second-stage configuration of R3DG. 6.4. Complexity and Runtime All experiments were conducted on single NVIDIA A6000 GPU. On the DTU dataset, COREA requires approximately 2.4 hours of training per scene. We report the peak GPU memory usage measured with NVIDIA-SMI, and COREA consumes roughly 24 GB on average during DTU training. All experiments were performed using PyTorch 2.3, CUDA 12.1, and our unified pipeline for joint relightable 3DGSSDF optimization. 7. Ablation Studies 7.1. Analysis of Directional Alignment Configurations We analyze the contribution of each alignment step by disabling the corresponding supervision while keeping the rest of the pipeline unchanged, and summarize the quantitative results in Tab. A. When Depth-guided SDF Alignment (DSA) (Sec. 3.2.1) is removed, the 3DGS still follows SDF normals through Normal-guided Gaussian Alignment (NGA), but the SDF is trained independently of the 3DGS depth field. Without depth anchoring, the SDF forms its surface more slowly and becomes less consistent with the 3DGS geometry. The resulting normals, although used as supervision for 3DGS, provide an unstable geometric prior that disrupts early-stage normal learning in 3DGS. This misalignment mildly affects NVS quality but has stronger impact on PBR, which is highly sensitive to geometric inaccuracies. When NGA (Sec. 3.2.2) is disabled, the SDF still receives depth and depth gradient based alignment cues from 3DGS, Table B. Quantitative Comparison of Normal Supervision Strategies. The proposed pixel-wise depth gradient supervision achieves lower Chamfer Distance (CD) for mesh reconstruction and higher PSNR, SSIM, and lower LPIPS for both NVS and PBR, indicating stronger geometric consistency than the alpha-blended normal supervision. Mesh NVS PBR Method CD PSNR SSIM LPIPS PSNR SSIM LPIPS Alpha-blended normal Pixel-wise depth gradient (Ours) 0.848 0.824 31.89 32.27 0.947 0.955 0.096 0.094 29.38 29.72 0.933 0. 0.116 0.104 but 3DGS no longer receives any geometric supervision from the SDF. As result, 3DGS relies solely on image-space cues, producing blurred normals and inaccurate geometry. This degradation appears consistently in both mesh accuracy (CD) and image-based evaluations, with PBR performance exhibiting large drop due to its reliance on sharp and consistent normals. These results demonstrate that DSA and NGA are complementary. DSA maintains well-aligned SDF surface by using both depth and depth gradient from 3DGS, while NGA aligns 3DGS to the SDF surface through depth projection and refines Gaussian normals using SDF normal supervision. When both alignment steps are enabled, the two representations maintain mutual consistency, yielding accurate geometry, improved mesh quality, and faithful relighting performance. Finally, we compare two training strategies for the full pipeline. The first is simultaneous update scheme, where DSA and NGA are optimized within the same update step, and the second is our alternative update scheme, where the two alignment modules are updated in alternating steps. The simultaneous scheme outperforms the settings in which DSA or NGA is disabled, indicating that both alignment modules remain beneficial even when updated together. However, the alternative strategy achieves the best performance. Its alternating refinement process provides more stable and disentangled supervision to each representation, since one alignment module is updated while the other remains fixed. Therefore, we adopt the alternative update scheme as our default training strategy. 7.2. Quantitative Analysis of Depth Gradient-Based"
        },
        {
            "title": "Geometry Alignment",
            "content": "In the main paper (Sec. 4.3.2), we analyzed the effect of different normal supervision signals within our joint relightable 3D Gaussian-SDF training framework. Unlike the alpha-blended normal, which provides an indirect and 2Dsmoothed estimate, the pixel-wise depth gradient offers direct and geometrically consistent supervision signal that preserves fine-scale 3D structure across both representations. Tab. presents quantitative results across novel-view synthesis (NVS), mesh reconstruction, and physically-based rendering (PBR), comparing alpha-blended normal superviTable C. Comparison between Adaptive and Dual-Density Control. All values indicate the number of Gaussians in millions (M). The original Adaptive Density Control in 3DGS tends to over-split Gaussians, whereas our Dual-Density Control (DDC) effectively suppresses redundant splitting through dual-loss scheme that jointly considers image and normal gradients. By regulating the generation of split Gaussians, DDC achieves geometry-aware densification and reduces the overall Gaussian count by an average of 17%, significantly lowering memory usage and training overhead. #Gaussians (M) Adaptive Density Control Dual-Density Control (Ours) 24 0.93 0. 37 1.21 1.18 40 1.78 1.65 55 1.14 1. 63 0.19 0.19 65 0.21 0.21 69 0.34 0. 83 0.13 0.13 97 0.51 0.50 105 0.32 0. 106 0.39 0.37 110 0.16 0.16 114 0.37 0. 118 0.27 0.26 122 0.24 0.24 Avg. 0.54 0.45 (17%) sion with our pixel-wise depth gradient supervision. While the alpha-blended normal tends to produce over-smoothed geometry and lose fine-scale surface details, pixel-wise depth gradient supervision consistently achieves lower Chamfer Distance (CD) for mesh reconstruction and higher PSNR, SSIM, and lower LPIPS for both NVS and PBR. These results show that pixel-wise depth gradient acts as stable and informative geometric cue, enabling precise 3D-to-3D alignment and improving overall rendering fidelity across all tasks. 7.3. Dual-Density Control for Efficient Gaussian"
        },
        {
            "title": "Densification",
            "content": "To further validate the effect of our DDC strategy, we conduct an ablation study on the number of Gaussians, as shown in Tab. C, which reports per-scene results on the DTU dataset. With the Adaptive Density Control used in vanilla 3DGS, the accumulated pixel-wise gradients from image and normal losses frequently trigger excessive Gaussian splitting, resulting in inefficient over-densification. In contrast, DDC reduces the Gaussian count by 17% on average while preserving scene fidelity across all scenes. By jointly considering image-space and geometry-space gradients, DDC triggers densification only when it meaningfully reduces the overall dual-gradient loss, effectively suppressing redundant growth. This produces geometry-aware densification that remains compact and well-structured, significantly reducing memory usage and improving the computational efficiency of PBR, since fewer Gaussians participate in ray sampling and shading evaluations. The resulting representation is lightweight yet geometrically accurate, supporting high-quality rendering and stable relighting. 8. Limitations Although COREA aligns 3DGS and SDF through complementary DSA and NGA steps and reconstructs geometry with higher fidelity than prior Gaussian-based methods, certain limitations remain. As shown in Fig. A, regions that are rarely observed due to the object-camera configuration tend to accumulate sparse or oversized Gaussians. Because these Gaussians are poorly constrained by visibility, they do not densify along the true surface and form unstable geometry. Figure A. Sparse Gaussians in occluded regions. Under viewpoints where the surface is partially occluded, 3D Gaussian splats fail to form dense distribution along the hidden geometry. During relighting, the sparsity becomes more apparent and the unobserved regions produce blurred or unstable shading responses across GSIR, R3DG, and COREA, revealing the inherent limitation of Gaussian representations in heavily occluded areas. This issue becomes more evident under relighting, as shown in the lower-right region of each method in Fig. A. When illumination changes, the Gaussians in occluded or partially visible areas produce over-smoothed shading and noisy responses due to their inflated footprints and weak geometric constraints. COREA alleviates this issue through SDF interaction, which provides stronger geometric supervision than purely Gaussian-based pipelines, but residual instability still persists in severely under-observed regions. promising direction to address this limitation is to incorporate pixel-weighted Gaussian representations such as Pixel-GS [49], which model Gaussian coverage in the image domain. Such visibility-aware weighting can enable more reliable densification even when the initial point cloud is sparse. Integrating these ideas with our geometry alignment framework may lead to more robust Gaussian formation in low-visibility regions and improved relighting stability. 9. Comparison With Concurrent Works: GSROR2 and DiscretizedSDF We additionally evaluate two concurrent approaches, GSROR2 [50] and DiscretizedSDF [51]. GS-ROR2 performs"
        },
        {
            "title": "DTU",
            "content": "mesh reconstruction are rendered using Blender [2]. SH PBR PSNR SSIM LPIPS Demo videos are available at https://cau-vilab. github.io/COREA/ GaussianShader (CVPR24) GS-IR (CVPR24) R3DG (ECCV24) DiscretizedSDF (ICCV25) GS-ROR2 (TOG25) COREA (Ours) 19.49 24.56 26.44 22.76 19.76 29.72 0.719 0.752 0.914 0.833 0.812 0.942 0.393 0.206 0.138 0.198 0.219 0.104 Table D. PBR-based NVS comparison with concurrent works on DTU dataset. Since GS-ROR2 and DiscretizedSDF do not support SH rasterization, we evaluate them under the identical PBR rendering protocol as in the main manuscript. COREA achieves the best PBR quality across all metrics. bidirectional guidance between SDF and relightable Gaussian, while DiscretizedSDF keeps precomputed SDF fixed and only optimizes the Gaussians while using the SDF as geometric prior for surface regularization. However, neither method currently supports spherical harmonics (SH) rasterization, which is required for novel-view synthesis (NVS) evaluation under the rendering protocol adopted in our experiments. GS-ROR2 supports physically-based rendering (PBR) and mesh reconstruction, whereas DiscretizedSDF only supports PBR. To ensure fairness, we evaluate GS-ROR2 and DiscretizedSDF using the same PBR-based NVS evaluation protocol applied to other baselines in the main manuscript, and we include the resulting quantitative comparisons in Tab. D. Under identical rendering assumptions, our method achieves the best performance across all evaluated scenes, confirming the advantage of our geometry and BRDF-lighting decomposition. 10. More experiments 10.1. Per-scene Results We present additional qualitative results in Fig. B, demonstrating the visual performance of our method across diverse scenes. Per-scene quantitative results for NVS, PBR, and mesh reconstruction are provided in Tabs. E-I. 10.2. Demo Video To present the proposed method more effectively, we provide demo video in our project page, https://cauvilab.github.io/COREA/. The video is organized into several segments, including an overall teaser, brief visualization of our framework, and additional visual materials demonstrating the qualitative performance of COREA. It also presents comparisons across three key tasks: Novel View Synthesis (NVS), mesh reconstruction, and Physically-Based Rendering (PBR). The final demo is edited using Adobe After Effects [1]. Source clips for NVS and PBR are generated through the web-based renderer Supersplat [3], and clips for Figure B. Additional Qualitative Results. We demonstrate the unified capability of COREA to perform all three tasks within single framework: NVS (blue), Mesh Reconstruction (orange), and PBR (green). Each row shows distinct real-world scene, where COREA produces sharp novel views, accurate geometry, and realistic relighting under both white and HDR environment lighting, with clean object-background separation. These results highlight COREAs consistency and generalization across geometry, appearance, and lighting, achieving unified 3D representation for synthesis, reconstruction, and relighting. Table E. Quantitative results for NVS on the DTU dataset. PSNR Mesh PBR 24 37 40 55 63 69 83 97 105 106 114 118 122 3DGS GaussianShader R3DG GS-IR Frosting GSDF GS-pull COREA (Ours) 23.63 22.28 28.54 23.86 21.47 24.87 23.87 28.47 21.45 9.61 25.41 21.55 18. 22.09 21.75 25.35 23.29 20.57 27.62 23.18 21.12 23.84 23.91 26.95 23.66 23.09 32.98 24.66 26.13 25.74 24.88 32.89 28.36 16.54 31.39 28.74 23. 28.48 28.09 31.49 28.35 21.98 31.82 28.32 23.29 29.18 27.19 31.87 27.64 24.51 29.60 28.09 30.56 26.81 27.70 29.68 30.30 23.89 32.43 30.43 24. 25.45 29.69 32.43 24.43 17.89 27.04 24.45 21.27 24.92 24.73 27.38 27.55 23.52 30.01 27.65 24.30 27.74 26.83 30.11 28.82 19.86 35.51 29.45 25. 29.80 29.59 35.46 28.73 25.86 33.61 29.65 24.07 29.51 28.41 33.48 27.18 22.09 31.16 27.33 23.03 25.04 27.57 31.34 32.28 29.24 37.98 32.86 28. 32.18 32.33 38.17 32.06 29.72 37.79 33.07 27.44 31.12 32.24 38.13 SSIM"
        },
        {
            "title": "PBR",
            "content": "24 37 40 55 63 69 83 97 105 106 114 118 122 3DGS GaussianShader R3DG GS-IR Frosting GSDF GS-pull COREA(Ours) 0.728 0.711 0.921 0.730 0.640 0.774 0.750 0.922 0.720 0.592 0.894 0.725 0. 0.741 0.745 0.894 0.674 0.642 0.891 0.665 0.504 0.697 0.691 0.930 0.802 0.814 0.966 0.809 0.610 0.847 0.818 0.976 0.910 0.797 0.961 0.929 0. 0.932 0.926 0.967 0.927 0.915 0.961 0.929 0.807 0.939 0.927 0.968 0.883 0.801 0.935 0.886 0.667 0.844 0.888 0.951 0.946 0.869 0.969 0.947 0. 0.889 0.941 0.970 0.865 0.760 0.932 0.865 0.723 0.881 0.872 0.957 0.877 0.816 0.944 0.874 0.789 0.884 0.870 0.955 0.870 0.789 0.957 0.866 0. 0.886 0.882 0.964 0.906 0.881 0.954 0.911 0.839 0.913 0.907 0.961 0.847 0.800 0.937 0.847 0.706 0.820 0.854 0.937 0.911 0.905 0.966 0.912 0. 0.908 0.915 0.967 0.925 0.892 0.971 0.928 0.789 0.910 0.928 0.973 LPIPS"
        },
        {
            "title": "PBR",
            "content": "24 37 40 55 63 69 83 97 105 106 114 118 122 3DGS GaussianShader R3DG GS-IR Frosting GSDF GS-pull COREA(Ours) 0.289 0.264 0.095 0.246 0.293 0.243 0.321 0.088 0.224 0.318 0.105 0.203 0. 0.206 0.245 0.099 0.325 0.304 0.157 0.282 0.364 0.294 0.351 0.134 0.174 0.180 0.056 0.152 0.293 0.152 0.187 0.050 0.129 0.206 0.073 0.120 0. 0.124 0.144 0.072 0.122 0.144 0.080 0.116 0.254 0.115 0.136 0.077 0.222 0.244 0.146 0.207 0.280 0.225 0.250 0.142 0.113 0.143 0.075 0.102 0. 0.136 0.126 0.065 0.183 0.229 0.113 0.173 0.315 0.170 0.205 0.100 0.201 0.230 0.109 0.179 0.226 0.186 0.226 0.099 0.195 0.216 0.100 0.179 0. 0.179 0.212 0.097 0.177 0.193 0.121 0.164 0.207 0.179 0.194 0.113 0.219 0.246 0.105 0.202 0.329 0.218 0.248 0.105 0.167 0.172 0.091 0.148 0. 0.152 0.186 0.085 0.125 0.137 0.063 0.111 0.265 0.121 0.141 0.056 Table F. Quantitative results for PBR on the DTU dataset. PSNR Mesh PBR 24 37 40 55 65 69 83 97 105 110 114 118 122 R3DG GS-IR GaussianShader COREA (Ours) 24.32 21.06 19.56 26.98 22.36 20.46 23.21 25.48 24.52 21.73 17.83 27.64 27.14 22.77 15.76 30.25 27.26 25.40 23.18 30. 27.38 23.53 16.75 30.50 26.03 26.07 16.04 28.31 27.13 29.24 24.00 31.16 23.77 23.24 20.43 25.82 25.93 24.61 23.73 29.40 29.45 27.16 17.35 32. 25.61 27.77 17.59 29.89 25.26 25.04 16.76 27.70 30.56 27.58 18.36 33.38 31.53 27.48 18.05 34.58 SSIM Mesh PBR 24 37 40 55 65 69 83 97 105 110 114 118 122 R3DG GS-IR GaussianShader COREA (Ours) 0.869 0.747 0.827 0.917 0.850 0.781 0.843 0.909 0.850 0.734 0.733 0.902 0.932 0.662 0.563 0.960 0.917 0.830 0.897 0. 0.946 0.715 0.729 0.959 0.896 0.762 0.574 0.928 0.948 0.952 0.823 0.963 0.890 0.892 0.719 0.934 0.908 0.602 0.831 0.938 0.932 0.799 0.614 0. 0.913 0.826 0.527 0.941 0.911 0.621 0.652 0.934 0.944 0.581 0.598 0.959 0.953 0.539 0.599 0.969 LPIPS Mesh PBR 24 37 40 55 65 69 83 97 105 110 114 118 122 R3DG GS-IR GaussianShader COREA (Ours) 0.154 0.198 0.213 0.105 0.142 0.176 0.225 0.106 0.200 0.265 0.313 0.160 0.089 0.217 0.519 0.064 0.105 0.154 0.208 0. 0.097 0.205 0.458 0.085 0.196 0.258 0.519 0.150 0.104 0.086 0.396 0.076 0.154 0.145 0.399 0.122 0.149 0.231 0.345 0.115 0.133 0.208 0.455 0. 0.166 0.200 0.497 0.132 0.136 0.270 0.436 0.131 0.118 0.204 0.493 0.094 0.082 0.240 0.495 0.062 Table G. Quantitative results for CD on the DTU dataset. CD"
        },
        {
            "title": "PBR",
            "content": "24 37 40 55 63 69 83 97 105 106 114 118 122 Frosting GSDF GS-pull COREA(Ours) 1.21 1.03 0.62 0.77 0.97 0.90 0.71 0.87 1.06 0.47 1.45 0.58 0.47 0.44 0.63 0.38 1.93 1.32 0.90 1.41 1.70 0.84 1.30 0. 1.66 0.76 0.88 0.70 2.30 1.59 6.26 1.49 3.36 1.33 1.45 1.36 1.46 0.78 2.11 0.77 2.52 0.60 0.80 0.69 2.57 1.30 0.99 1. 0.94 0.39 0.55 0.36 2.13 0.52 0.66 0.53 1.25 0.56 12.83 0.49 Table H. Quantitative results for NVS on the Tanks&Temples dataset."
        },
        {
            "title": "Method",
            "content": "Mesh PBR PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 3DGS GaussianShader R3DG GS-IR Frosting GSDF GS-pull COREA(Ours) 27.53 21.97 28.82 28.28 24.33 OOM 26.25 27.71 0.879 0.878 0.906 0.900 0. OOM 0.847 0.894 0.158 0.137 0.138 0.141 0.191 OOM 0.206 0.145 24.45 21.50 27.92 26.75 20.54 OOM 22.80 27.75 0.891 0.867 0.949 0.933 0. OOM 0.870 0.943 0.133 0.119 0.067 0.077 0.175 OOM 0.158 0.083 32.34 20.47 34.60 32.65 25.41 OOM 31.38 34.78 0.963 0.904 0.977 0.965 0. OOM 0.957 0.978 0.050 0.088 0.031 0.041 0.109 OOM 0.060 0.035 25.51 15.89 26.34 26.02 22.87 OOM 24.71 26.76 0.908 0.754 0.933 0.918 0. OOM 0.897 0.934 0.119 0.213 0.081 0.096 0.158 OOM 0.143 0.096 Table I. Quantitative results for PBR on the Tanks&Temples dataset."
        },
        {
            "title": "Method",
            "content": "Mesh PBR PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS GaussianShader R3DG GS-IR COREA(Ours) 18.72 26.11 22.27 26. 0.816 0.881 0.611 0.895 0.202 0.159 0.249 0.138 17.54 24.27 21.45 25.14 0.849 0.900 0.390 0.908 0.156 0.098 0.191 0.103 20.61 30.52 28.05 31. 0.888 0.960 0.924 0.967 0.103 0.046 0.079 0.045 14.40 24.35 20.08 25.42 0.762 0.906 0.682 0.920 0.215 0.108 0.225 0."
        }
    ],
    "affiliations": [
        "Chung-Ang University, Seoul, Korea"
    ]
}