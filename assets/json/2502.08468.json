{
    "paper_title": "mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data",
    "authors": [
        "Haonan Chen",
        "Liang Wang",
        "Nan Yang",
        "Yutao Zhu",
        "Ziliang Zhao",
        "Furu Wei",
        "Zhicheng Dou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in https://github.com/haon-chen/mmE5."
        },
        {
            "title": "Start",
            "content": "mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data Haonan Chen1, Liang Wang2, Nan Yang2, Yutao Zhu1 Ziliang Zhao1, Furu Wei2, Zhicheng Dou1 1Gaoling School of Artificial Intelligence, Renmin University of China 2Microsoft Corporation {hnchen,dou}@ruc.edu.cn {wangliang,nanya,fuwei}@microsoft.com https://github.com/haon-chen/mmE5 5 2 0 2 2 1 ] . [ 1 8 6 4 8 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains critical bottleneck. In this work, we identify three criteria for highquality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover wide range of tasks, modality combinations, and languages, (2) are generated via deep thinking process within single pass of multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves stateof-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in https://github.com/ haon-chen/mmE5."
        },
        {
            "title": "Introduction",
            "content": "Multimodal embedding models encode multimedia inputs, such as images and text, into latent vector representations. They have demonstrated effectiveness across diverse downstream tasks, including Work done during Haonans internship at MSR Asia. Prof. Zhicheng Dou is the corresponding author. Figure 1: An illustration of our data synthesis framework. XY denotes modality combination, where represents the query side and denotes the target side. denotes text and denotes image. classification (Deng et al., 2009), visual question answering (VQA) (Singh et al., 2019), and crossmodal retrieval (Hu et al., 2023). Prior studies have focused on training multimodal embedding models using simple text-image pre-trained models such as CLIP (Radford et al., 2021). More recently, researchers have turned to multimodal large language models (MLLMs), including LLaVA (Liu et al., 2023a) and Phi (Abdin et al., 2024), to develop universal embedding models. These vision-language models (VLMs) mostly rely on high-quality human-labeled datasets to achieve robust embedding capabilities. Such datasets suffer from data scarcity because they require high costs of multimodal annotations. To address this, researchers have leveraged the advanced language modeling capabilities of large language models (LLMs) and MLLMs to synthesize datasets for fine-tuning multimodal embedding models (Zhang et al., 2024a; Zhou et al., 2024a; Zhang et al., 2024b). However, existing works lack comprehensive exploration into the quality of synthetic embedding data. Typically, most data generated by them are limited to specific modality types of English retrieval tasks, harming the generalization capabilities of the embedding models. After analyzing common application scenarios of multimodal embedding models, we identify three key criteria and introduce data synthesis framework guided by these principles: (1) Broad scope. Multimodal embedding models are commonly employed in tasks such as classification, visual question answering (VQA), and retrieval, which requires understanding various input combinations of text and images. Additionally, multilingual contexts are increasingly popular in daily scenarios. As shown in Figure 1, our framework synthesizes datasets covering three tasks, seven modality combinations, and 93 languages, ensuring that models trained on it generalize effectively across diverse scenarios. (2) Robust cross-modal alignment. In multimodal tasks, models must understand and align information across different modalities to generate meaningful representations. Without accurate cross-modal alignment, embeddings may fail to capture the underlying relationships, leading to poor performance in downstream tasks. To synthesize data of robust cross-modal alignment, our framework incorporates deep thinking process. Specifically, for each sampled image, we first employ an MLLM to interpret it from four perspectives before generating data: general information, object-level description, contextual background information, and task-specific brainstorming, i.e., how the image relates to the given task. Additionally, the entire data synthesis process is executed within single pass of an MLLM. By this, the MLLM can see the images at the whole time, avoiding potential information loss that might occur due to multiple I/O steps in previous works (Zhou et al., 2024a; Zhang et al., 2024b). (3) High fidelity. The individual quality of each modality (e.g., real images, high-quality instructions, queries, and hard negatives) determines the overall usefulness of the dataset. To enhance fidelity, our framework uses real images sampled from an opensource corpus (LAION-400m (Schuhmann et al., 2021)) as the input images. We also apply series of quality control measures, such as self-evaluation and refinement, ensuring that the synthetic components accurately reflect real-world distributions and maintain strong cross-modal alignment. With the synthesized data ready, we train multimodal multilingual E5 model (mmE5). It achieves state-of-the-art performance on the 36 datasets of MMEB (Jiang et al., 2024b), using 45 times less training data than the previous SOTA model MMRet (Zhou et al., 2024a) (560K compared to 26M) in zero-shot setting. After incorporating labeled data, mmE5 still demonstrates the best performance. Besides, mmE5 achieves the best results on the multilingual benchmark XTD (Aggarwal and Kale, 2020), demonstrating its superior multilingual capabilities. In summary, our contributions are as follows: Based on our analysis of common scenarios for multimodal embedding models, we identify three key criteria of high-quality synthetic data: broad scope, robust cross-modal alignment, and high fidelity. We introduce data synthesis framework guided by the proposed principles. This framework leverages an MLLM to produce highquality synthetic datasets that cover wide range of tasks, modality combinations, and languages. It ensures robust cross-modal alignment through comprehensive multiaspect interpretation process and maintains high fidelity by employing self-evaluation and refinement mechanisms. Compared to the previous leading model, mmE5 achieves SOTA performance on the MMEB benchmark while using 45 less synthetic data in both zero-shot and supervised settings. mmE5 also demonstrates superior multilingual capabilities on the XTD benchmark."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Embedding Previous studies, such as CLIP (Radford et al., 2021), Align (Jia et al., 2021), BLIP (Li et al., 2022), and CoCa (Yu et al., 2022), have employed large-scale weakly supervised data to learn separate multimodal representations through pre-training. Some works attempt to obtain universal embeddings for texts and images utilizing existing CLIP-like models (Wei et al., 2024; Liu et al., 2023b; Zhou et al., 2024b,c). For instance, UniIR (Wei et al., 2024) integrates separate embeddings from different modalities into unified features. Recent approaches finetune MLLMs to leverage their multimodal reasoning capabilities for obtaining universal representations (Jiang et al., 2024a,b; Zhang et al., 2024b; Zhou et al., 2024a; Lin et al., 2024). For example, VLM2Vec (Jiang et al., 2024b) utilizes instruction-tuning to transform MLLMs into embedding models. Synthetic Data The generation of synthetic data has been extensively explored for text embedding Method # Languages Task Modality Combinations w/ MLLM One Pass Self-evaluation MagicLens 1 (English) Retrieval MegaPairs 1 (English) Retrieval GME 1 (English) Retrieval ITI ITI TIT, ITIT mmE5 (Ours) 93 (English, Classification, Spanish, etc.) VQA, Retrieval ITI, TIT, ITIT, II, IT, ITT, TI Table 1: Comparison of the synthetic datasets in our work with those from previous methods. Our synthetic datasets incorporate 93 languages, two additional tasks, and more modality combinations. ITT denotes modality combination, where IT denotes images and texts on the query side and denotes texts on the target side. The entire data synthesis process is executed within single pass of an MLLM, thereby avoiding potential information loss and ensuring robust cross-modal alignment. We also employ real images and self-evaluation to maintain fidelity. tasks (Wang et al., 2024a; Chen et al., 2024; Li et al., 2024b). With the recent emergence of MLLMs like Phi-3.5-V (Abdin et al., 2024) and LLaVA (Liu et al., 2023a), along with diffusion models like Stable Diffusion (Rombach et al., 2022), researchers have been focused on synthesizing data to address the scarcity of multimodal instruction-tuning datasets. For example, MagicLens (Zhang et al., 2024a) utilizes co-existing images from the same webpage and an LLM to create multimodal data triplets (query image, instruction, relevant image), i.e., ITI paradigm. MegaPairs (Zhou et al., 2024a) aims to synthesize more diverse data triplets by retrieving relevant images from different perspectives. GME (Zhang et al., 2024b) employs an LLM and diffusion model to generate fused modality dataset that includes both TIT and ITIT types. Table 1 presents comparison of the synthesized data in this study with that of previous works."
        },
        {
            "title": "3 Methodology: mmE5",
            "content": "In this section, we present our method, which synthesizes high-quality multimodal data for the further finetuning of our embedding model mmE5. As shown in Figure 2, our method consists of five stages: (1) Initially, for each data sample to be synthesized, we configure the specifics of the task, modality combination, language, and input images. (2) We employ an MLLM to generate multi-grained descriptions for the input images, ensuring that the synthesized texts are well-aligned with the images. (3) Utilizing this MLLM, we synthesize text data based on both the images and their descriptions. (4) The MLLM then evaluates its synthesized data from multiple perspectives, offering revised data to enhance cross-modal alignment and fidelity. (5) Finally, the synthesized texts and images are used to finetune an MLLM specifically for embedding tasks. To minimize potential information loss, stages (2), (3), and (4) are executed within single pass of the MLLM."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "An MLLM can accept text, image, or text-image pairs as input, allowing both the query side and the document side to be multimodal. Inspired by existing works on synthetic text embedding data (Wang et al., 2024a; Chen et al., 2024), each data sample we generate is quadruple of (task instruction, query, positive document, hard negative document), denoted as (t, q, d+, d). For each data piece, we first sample images from the large-scale open-source image corpus LAION400M (Schuhmann et al., 2021) as the query image, positive image, and hard negative image (qi, d+ , ). Then, with these three images as input, an MLLM πθ can synthesize multimodal , embedding data sample πθ(y qi, d+ ), , where = (t, qt, d+ ). As result, the synthetic data can have maximum of seven elements: {t, (qt, qi), (d+ , d+ , )}. More data examples can be found in Appendix D. ), (d"
        },
        {
            "title": "3.2 Data Synthesis Framework",
            "content": "Guided by the principles of high-quality synthetic multimodal data, i.e., broad scope, robust crossmodal alignment, and high fidelity, we introduce data synthesis framework. This framework is designed to synthesize high-quality data that transforms an MLLM for downstream embedding tasks. Figure 2: An illustration of our method. We take the generation of an ITIT retrieval data sample as an example."
        },
        {
            "title": "3.2.1 Data Configuration",
            "content": "To prepare for the data synthesis process, we configure the input data from three aspects: Task and Modality Combination We aim to synthesize data with broad scope by generating beyond simple retrieval data of ITIT and TIT types. Our data cover three key multimodal embedding tasks identified by previous work (Jiang et al., 2024b): classification, VQA, and retrieval. After selecting task for synthesis, we will sample modality combination with respect to the specific task, such as choosing from seven possible combinations for the retrieval task type. Note that we only synthesize data of modality types that are included in the MMEB benchmark (Jiang et al., 2024b), which can cover most scenarios. Image Despite the powerful multimodal capabilities of modern MLLMs (e.g., GPT-4o, Llama3.2 (Meta, 2024), and Llava-1.6), most cannot generate images, and those that can often produce lowfidelity images (Zhou et al., 2024b). Following previous works (Zhang et al., 2024a; Zhou et al., 2024a), we sample real images from the LAION400M corpus (Schuhmann et al., 2021). First, we will sample query image from the corpus (qi I). Then, for the modality types involving images on the document side (e.g., ITIT), we use small embedding model, jina-clip-v2 (Koukounas et al., 2024), to retrieve similar positive image d+ and hard negative image Language Most existing models only focus on high-source languages like English, harming the multilingual ability of embedding models. To synthesize multilingual data, we sample languages from the language list of XLM-R (Conneau et al., 2020) during configuration. In order to facilitate the common usage scenarios, we give high-source languages higher weights. Note that the generated efficiently. task instruction will always be in English for effective instruction tuning."
        },
        {
            "title": "3.2.2 One-pass Generation with MLLM",
            "content": "With the data configuration ready, we introduce deep thinking process that involves interpreting input images, generating data, and performing selfevaluation. To ensure that the MLLM always takes the image context into account, we execute this entire process in single pass. Multi-aspect Visual Interpretation To obtain comprehensive understanding of the images, the MLLM πθ first analyzes them from multiple perspectives: (1) the general information, (2) detailed description of the objects present, (3) contextual background information, and (4) potential connections between the image and the text that may be synthesized. The deep understanding of the images enables πθ to produce texts that are closely aligned with the visual content, thereby enhancing the cross-modal alignment. Synthesizing Data Using the images and their descriptions as input, we prompt πθ to synthesize texts (t, qt, d+ ). Specifically, the text instruction is expected to connect qi with d+ .1 The query and document texts should be relevant to their respective images. Note that the input and output formats for the synthetic data may vary depending on the combination of modalities. For example, for IIT and TIT types, there can be no query text and image, respectively. Self-evaluation In order to further enhance the quality of the synthetic data, πθ evaluates the data it synthesizes from: (1) the relevance of the texts to their corresponding images, (2) the plausibility of hard negatives, (3) the clarity of t, and (4) the , 1Because of limited space, full prompts are omitted in this section. The complete prompts can be found in Appendix C. Models Zero-shot Setting Models CLIP (Radford et al., 2021) BLIP2 (Li et al., 2023) SigLIP (Zhai et al., 2023) OpenCLIP (Cherti et al., 2023) E5-V (Jiang et al., 2024a) MagicLens (Zhang et al., 2024a) MMRet (w/ 26M synthetic data) mmE5 (w/ 560K synthetic data) Partially Supervised Finetuning Models UniIR (Wei et al., 2024) MM-EMBED (Lin et al., 2024) GME (Zhang et al., 2024b) Supervised Finetuning Models CLIP (Radford et al., 2021) OpenCLIP (Cherti et al., 2023) VLM2Vec (Jiang et al., 2024b) MMRet (Zhou et al., 2024a) mmE5 (w/ synthetic data + labeled data) Per Meta-Task Score Average Score Class. VQA Retr. Ground. IND OOD Overall 42.8 27.0 40.3 47.8 21.8 38.8 47.2 60. 42.1 48.1 56.9 55.2 56.0 61.2 56.0 67.6 9.1 4.2 8.4 10.9 4.9 8.3 18.4 55.7 15.0 32.2 41.2 19.7 21.9 49.9 57.4 62.8 53.0 33.9 31.6 52.3 11.5 35.4 56.5 54. 60.1 63.8 67.8 53.2 55.4 67.4 69.9 70.9 51.8 47.0 59.5 53.3 19.0 26.0 62.2 72.4 62.2 57.8 53.4 62.2 64.1 86.1 83.6 89.7 - - - - - - - - - - - - - - - - - - - - - - 47.6 50.5 67.5 68.0 72.3 42.8 43.1 57.1 59.1 66.7 37.8 25.2 34.8 39.7 13.3 27.8 44.0 58. 42.8 50.0 55.8 45.4 47.2 62.9 64.1 69.8 Table 2: Results on MMEB benchmark, consisting of 36 tasks across four types: classification (Class.), VQA, retrieval (Retr.), and visual grounding (Ground.). UniIR, MM-EMBED, and GME are not strictly zero-shot models. UniIR and MM-EMBED are trained on the MBEIR dataset (Wei et al., 2024), which includes 10 retrieval datasets included in the MMEB. Similarly, GME is trained on the UMRB dataset (Zhang et al., 2024b), which shares 14 datasets with the MMEB. For VLM2Vec, we use the LLaVA-based version with high-resolution images reported in its original paper. The second-best performances are underlined and the best performances are in bold. diversity (creativity) of the synthesized data. Following this evaluation, πθ provides suggestions for potential improvements. Finally, revised version of each data sample is produced and utilized for the subsequent contrastive training phase."
        },
        {
            "title": "3.3 Finetuning Embedding Model mmE5",
            "content": "Following previous works of instruction-tuned text embedding models (Xiao et al., 2024; Li et al., 2024a) and multimodal embedding models (Jiang et al., 2024b), we apply an instruction template on each query: [IMAGE] {t} {qt} {qi}, where [IMAGE] is the image token that varies from different MLLMs. We then append an [EOS] token to each query and document. The representation of each input in an MLLM is derived from the output of the [EOS] token from the final layer. We utilize the InfoNCE loss (van den Oord et al., 2018) to perform the standard contrastive learning objective on our synthetic data D: = log ϕ(q, d+) ϕ(q, d+) + (cid:80) dN ϕ(q, d) , (1) where is the encoded multimodal query, represents the encoded document, and denotes the set of negative documents. The function ϕ() = exp(cos()/τ ), where cos() denotes cosine similarity, and τ is temperature hyperparameter."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We synthesize total of 560K multimodal embedding data samples. The MLLM utilized for data synthesis is GPT-4o-2024-08-06. The backbone model for mmE5 is Llama-3.2-11B-Vision2. For finetuning mmE5, we employed LoRA (Hu et al., 2022) with rank of 8. We evaluate the general embedding performance in terms of Precision@1 on the MMEB benchmark (Jiang et al., 2024b). This benchmark comprises 36 multimodal embedding tasks across four categories: classification (10), VQA (10), retrieval (12), and visual grounding (4). Our synthetic dataset is distributed among classification, VQA, and retrieval tasks in 1:1:2 ratio. We synthesize more retrieval data since this type contains more kinds of modality combinations. We do not synthesize visual grounding data since they are relatively simpler for MLLM based on the MMEB results. To evaluate multilingual multimodal capabilities, we conducted tests using the 2https://huggingface.co/meta-llama/Llama-3. 2-11B-Vision Model it es ru zh pl tr ko Avg. 87.9 88.8 82.3 86.5 79.8 73.5 76.6 82.2 ALIGN (Jia et al., 2021) MURAL (Jain et al., 2021) 91.8 92.9 87.2 89.7 91.0 89.5 88.1 90.0 VLM2Vec (Jiang et al., 2024b) 83.7 87.1 86.7 92.8 76.1 37.2 63.9 75.4 93.6 94.1 89.8 91.8 94.3 92.7 90.1 92.3 jina (Koukounas et al., 2024) M-CLIP (Carlsson et al., 2022) 93.1 93.6 90.0 94.0 94.3 93.1 89.0 92.4 95.1 96.4 92.3 96.4 94.9 89.8 93.6 94.1 GME (Zhang et al., 2024b) mmE5 (full) w/ synthetic data only w/ english synthetic data 96.1 96.2 93.3 96.3 95.4 93.6 96.0 95.3 90.9 89.6 86.3 90.2 90.3 87.2 86.7 88.7 86.3 86.3 84.2 88.8 84.9 81.0 84.4 85.1 Table 3: Results on XTD benchmark, text-to-image retrieval task covering seven languages. common usage scenarios. For the 75 low-resource languages, we evenly synthesize data samples to obtain balanced multilingual dataset that supports comprehensive cross-linguistic generalization. To evaluate the multilingual capability of mmE5, we conduct experiments across seven languages on text-to-image retrieval benchmark XTD. As presented in Table 3, mmE5 outperforms other models in terms of overall performances on all languages, demonstrating its superior multilingual multimodal embedding capability. The following observations can be made: (1) The multilingual performance of multimodal embedding models is largely dependent on their foundational models. For example, jina-clip-v2 and M-CLIP outperform VLM2VecLLaVA, despite VLM2Vecs strong performance on MMEB. GME exhibits robust performance on XTD, which can be attributed to the powerful multilingual MLLM, Qwen2-VL (Wang et al., 2024b). (2) The performance of mmE5 declines when labeled data is omitted, indicating that general multimodal capabilities remain essential for multilingual retrieval tasks. (3) In zero-shot setting, mmE5 trained on multilingual synthetic data (mmE5 w/ synthetic data only) outperforms mmE5 with the same amount of English synthetic data (mmE5 w/ english synthetic data). This suggests that the extensive language coverage provided by our synthetic data enhances the multilingual capabilities of embedding models."
        },
        {
            "title": "4.4 Application to Other Base MLLM",
            "content": "We train mmE5 based on the powerful MLLM LLaMA-3.2-Vision, which is instruction-tuned and effective in interpreting multimodal inputs. Notably, our synthetic data and training paradigm can effectively transform other foundation MLLMs into embedding models. We use both our synthetic data and labeled data to train LLaVA-1.63 and Phi3https://huggingface.co/llava-hf/llava-v1. 6-mistral-7b-hf Figure 3: Distribution of languages in the synthetic data. XTD benchmark (Aggarwal and Kale, 2020). Following MURAL (Jain et al., 2021), we conduct experiments on seven languages of XTD and report Recall@10 results. Additional details regarding the synthetic data, prompts, and implementation can be found in Appendix A, B, and C, respectively."
        },
        {
            "title": "4.2 Results on MMEB",
            "content": "The overall results on the MMEB benchmark are presented in Table 2. mmE5 achieves the best performance on both zero-shot setting (with synthetic data only) and supervised setting (with IND training datasets of MMEB). This demonstrates the quality of our synthetic data and the effectiveness of our multimodal embedding model. Furthermore, we can make the following observations: (1) mmE5 generalizes well on all four kinds of tasks. This demonstrates the broad scope of our synthetic multimodal embedding data in terms of task types. (2) With only 560K synthetic data, mmE5 manages to perform better than MMRet which uses 26M data. This proves the quality of our synthetic data again. (3) Intriguingly, mmE5 underperforms MMRet on retrieval tasks in zero-shot setting. This is because MMRet is trained on 26M pure retrieval data, which makes it perform well on retrieval tasks, but generalizes poorly on other task types."
        },
        {
            "title": "4.3 Multilingual Performance on XTD",
            "content": "We synthesize multilingual multimodal dataset that consists of 93 languages, in order to train our embedding model mmE5 to generalize across more languages. The language distribution of our dataset is presented in Figure 3. Notably, the dataset primarily consists of English data samples, facilitating Base MLLM Avg. on MMEB Model Avg. on MMEB Phi-3.5-V (Abdin et al., 2024) LLaVA-1.6 (Liu et al., 2023a) LLaMA-3-Vision (Meta, 2024) (Ours) Baselines (For Reference) VLM2Vec (Phi-3.5-V) VLM2Vec (LLaVA-1.6) MMRet (LLaVA-1.6) VLM2Vec (LLaMA-3.2) 61.0 65.8 69.8 60.1 62.9 64.1 64.8 Table 4: Performances of mmE5 with different MLLMs. 3.5-V4. The performances of mmE5 with different foundation MLLMs are presented in Table 4. The results show that models trained using our method consistently outperform baseline models built on the same foundational MLLMs. This indicates that our synthetic data can effectively enhance the capability of MLLMs to embed multimodal inputs."
        },
        {
            "title": "4.5 Discussions of Data Synthesis Process",
            "content": "In this section, we will further investigate the data synthesis process via zero-shot experiments."
        },
        {
            "title": "4.5.1 Ablation Studies",
            "content": "To evaluate each component of our data synthesis framework, we conduct ablation studies of mmE5: Deep Thinking Process To synthesize high-quality data, we introduce deep thinking process to boost data synthesis. As presented in Table 5, the performance of mmE5 declines when the Visual Interpretation and Self-evaluation components are excluded. For example, mmE5 performs worse when utilizing the original data compared to revised data. This indicates that the self-evaluation mechanism can enhance data fidelity, facilitating the training of more robust embedding model. Embedding Task Types In order to expand the scope of data, we synthesize data across three task types: classification, VQA, and retrieval. The performance of mmE5 decreases after each type of multimodal embedding data is omitted, demonstrating that our diverse synthetic data can facilitate model generalization. Intriguingly, the performance drops the least after removing the retrieval data, which is inconsistent with previous research (Jiang et al., 2024b). One possible explanation is that our backbone, Llama-3.2 Vision, inherently exhibits more robust retrieval capabilities than Phi-3.5-V. 4https://huggingface.co/microsoft/Phi-3. 5-vision-instruct mmE5 (280K synthetic data only) w/o. Visual Interpertation w/o. Self-evaluation w/o. Classification Data w/o. VQA Data w/o. Retrieval Data w/ IT2I only (MagicLens & MegaPairs) w/ IT2IT & T2IT only (GME) w/o. Hard Negative w/ English Data only (280K) w/o. English Data (280K) 57. 57.2 56.0 52.5 55.1 56.5 30.1 28.6 56.2 57.6 56.9 Table 5: Performances of ablated models on MMEB. For efficient test, we conduct zero-shot experiments on 280K synthetic data, which has the same tasks, modality types and languages as the full synthetic data. Modality Combinations Most prior works focus on one or two modality types, such as IT2I (e.g., MagicLens (Zhang et al., 2024a) and MegaPairs (Zhou et al., 2024a)) or IT2IT & T2IT (e.g., GME (Zhang et al., 2024b)). We propose to synthesize data across various modality combinations to enhance the diversity of our synthetic dataset, i.e., the scope of our synthetic multimodal data. To evaluate the impact of these additional modality combinations, we train mmE5 with the same amount of datasets that contain types IT2I or IT2IT & T2IT only. The performance of mmE5 significantly decreased when limited to these combinations from previous works, which indicates that the additional modalities enable our embedding model to generalize more effectively across different combinations and task types. Hard Negative Each sample in our synthetic dataset incorporates hard negative document to help mmE5 learn subtle differences. After excluding the hard negatives, the models performance drops significantly, which demonstrates the importance of this technique for contrastive learning. Language To investigate the impact of linguistic diversity on model performance on English benchmarks, we conducted experiments using synthetic data in two configurations: English-only and non-English languages only. Our model, mmE5, demonstrated slight performance advantage with English-only synthetic data, although the difference was minimal. Nonetheless, mmE5 achieved satisfactory results with 280K data samples from languages other than English. This suggests that Figure 4: The impact of synthetic data size on multimodal embedding performance on MMEB. our multilingual dataset enhances the embedding models ability to generalize effectively in both multilingual and English-only contexts."
        },
        {
            "title": "4.5.2 Scaling Effect",
            "content": "The scaling effect is an important aspect of synthetic data generation for multimodal embedding models (Zhang et al., 2024b; Zhou et al., 2024a). It explores how the performance of the model varies with the size of synthetic datasets. Besides, the data synthesis and training processes demand significant computational resources and time. Therefore, studying the scaling effect allows us to identify the point of diminishing returns, ensuring that resources are utilized efficiently without overproducing redundant data. In this section, we further investigate the performance of mmE5 using synthetic datasets of varying sizes. Specifically, we conduct zero-shot experiments on MMEB to analyze the scaling effect. As illustrated in Figure 4, mmE5 consistently achieves better performance with increased training data, demonstrating the high quality of our synthetic data again. This paradigm also indicates linear-log relationship between the model performance and data size, consistent with previous works of text embedding (Chen et al., 2024) and dense retrieval (Fang et al., 2024). This finding facilitates the balancing of the cost and the multimodal embedding model performance for future works. Figure 5: The zero-shot performances of mmE5 with different training settings on MMEB (280K synthetic data for efficient test). 1K samples from each training set. However, for consistency with previous experiments, we present results on the MMEB test sets. LoRA Rank denotes the rank of the additional lowrank matrices in LoRA. This parameter influences the number of parameters added into the original model, balancing the models capacity and computational efficiency. As shown in the left part of Figure 5, the performance of mmE5 initially improves then drops. This demonstrates trade-off: lower rank reduces memory and computation but may lead to underfitting if is too small, whereas higher rank risks harming the pre-trained multimodal reasoning capabilities of MLLM. Training Batch Size In contrastive learning, batch size plays critical role because it directly affects the number of negative samples available for training. As presented in the middle part of Figure 5, the performance of mmE5 consistently increases with larger batch size. However, large batches demand significantly more GPU memory, i.e., more computational resources. Temperature The temperature parameter τ in the InfoNCE loss (Equation 1) influences the separation between positive and negative samples in the embedding space. We can observe that mmE5s performance first improves then declines with larger temperature. This pattern suggests trade-off: low τ forces the model to strongly penalize nearpositive negatives which can lead to overfitting, while high τ leads to more uniform distribution of embeddings which may hinder the effective separation of positive and negative samples."
        },
        {
            "title": "4.6 Hyperparameter Analysis",
            "content": "In order to analyze the training process of our multimodal embedding model, we perform experiments with mmE5 using various training settings. For efficiency, we report zero-shot results for mmE5 trained with 280K synthetic data. Note that we tune these hyperparameters on evaluation datasets comprising In this work, we synthesize high-quality multimodal multilingual data to train the model mmE5. We first define high-quality multimodal synthetic data based on three criteria: broad scope, robust cross-modal alignment, and high fidelity. Then, we develop data synthesis framework guided by these principles. Finally, we train multimodal multilingual embedding model using the high-quality synthetic data. mmE5 achieves SOTA performances on both the general benchmark MMEB and the multilingual benchmark XTD."
        },
        {
            "title": "Limitations",
            "content": "Our work has several limitations that we intend to resolve in future research: 1. Our model currently relies on the proprietary MLLM GPT-4o for synthesizing multimodal data. Future work should explore aligning smaller MLLMs with the knowledge from GPT-like models to achieve more efficient data synthesis. 2. mmE5 focus on text and image modalities. Future models should aim to extend coverage to additional modalities, such as audio and video. 3. Due to the cost limitation and the observed scaling effect, we limited the amount of data produced for model training. Future research may consider increasing data size while preserving diversity to optimize model performance."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 technical report: highly capable language model locally on your phone. CoRR, abs/2404.14219. Pranav Aggarwal and Ajinkya Kale. 2020. Towards CoRR, image retrieval. zero-shot cross-lingual abs/2012.05107. Fredrik Carlsson, Philipp Eisen, Faton Rekathati, and Magnus Sahlgren. 2022. Cross-lingual and multilingual CLIP. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, LREC 2022, Marseille, France, 20-25 June 2022, pages 68486854. European Language Resources Association. Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, and Zhicheng Dou. 2024. Little giants: Synthesizing high-quality embedding data at scale. CoRR, abs/2410.18634. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. 2023. Reproducible scaling laws for contrastive language-image learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 28182829. IEEE. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 84408451. Association for Computational Linguistics. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pages 248255. IEEE Computer Society. Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, and Yiqun Liu. 2024. Scaling laws for dense retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, pages 13391349. ACM. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang. 2023. Open-domain visual entity recognition: Towards recognizing millions In IEEE/CVF International of wikipedia entities. Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 1203112041. IEEE. Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sneha Kudugunta, Chao Jia, Yinfei Yang, and Jason Baldridge. 2021. MURAL: multimodal, multitask retrieval across languages. CoRR, abs/2109.05125. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 49044916. PMLR. Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. 2024a. E5-V: universal embeddings with multimodal large language models. CoRR, abs/2407.12580. Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. 2024b. Vlm2vec: for massive Training vision-language models arXiv preprint multimodal embedding tasks. arXiv:2410.05160. Andreas Koukounas, Georgios Mastrapas, Bo Wang, Mohammad Kalim Akram, Sedigheh Eslami, Michael Günther, Isabelle Mohr, Saba Sturua, Scott Martens, Nan Wang, and Han Xiao. 2024. jina-clipv2: Multilingual multimodal embeddings for text and images. Chaofan Li, Zheng Liu, Shitao Xiao, Yingxia Shao, and Defu Lian. 2024a. Llama2vec: Unsupervised adaptation of large language models for dense retrieval. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 34903500. Association for Computational Linguistics. Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, Yuxian Gu, Xin Cheng, Xun Wang, Si-Qing Chen, Li Dong, Wei Lu, Zhifang Sui, Benyou Wang, Wai Lam, and Furu Wei. 2024b. Synthetic data (almost) from scratch: Generalized instruction tuning for language models. CoRR, abs/2402.13064. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1973019742. PMLR. Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022. BLIP: bootstrapping language-image pretraining for unified vision-language understanding and generation. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 1288812900. PMLR. Sheng-chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. 2024. Mm-embed: Universal multimodal retrieval with multimodal llms. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, and Ge Yu. 2023b. Universal vision-language dense retrieval: Learning unified representation space for multi-modal retrieval. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Meta. 2024. Introducing meta llama 3: The most capable openly available llm to date. https://ai.meta. com/blog/meta-llama-3/. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1067410685. IEEE. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021. LAION-400M: open dataset of clip-filtered 400 million image-text pairs. CoRR, abs/2111.02114. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards VQA models that can read. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 83178326. Computer Vision Foundation / IEEE. Aäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748. Yongping Xiong. 2024a. Megapairs: Massive data synthesis for universal multimodal retrieval. arXiv preprint arXiv:2412.14475. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024a. Improving text embeddings with large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1189711916. Association for Computational Linguistics. Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. 2024b. VISTA: visualized text embedding for universal multi-modal retrieval. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 31853200. Association for Computational Linguistics. Tianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Yu Gu, and Ge Yu. 2024c. MARVEL: unlocking the multi-modal capability of dense retrieval via visual module plugin. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1460814624. Association for Computational Linguistics. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024b. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191. Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 2024. Uniir: Training and benchmarking universal multimodal information retrievers. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LXXXVII, volume 15145 of Lecture Notes in Computer Science, pages 387404. Springer. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack: Packed resources for general chinese embeddings. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, pages 641649. ACM. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022. Coca: Contrastive captioners are image-text foundation models. Trans. Mach. Learn. Res., 2022. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language In IEEE/CVF International image pre-training. Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 1194111952. IEEE. Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. 2024a. Magiclens: Self-supervised image retrieval with open-ended instructions. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. 2024b. Gme: Improving universal multimodal retrieval by multimodal llms. Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, and"
        },
        {
            "title": "A Details about Synthetic Data",
            "content": "Task Modality Combination # samples Classification image-to-text (image,text)-to-text Retrieval image-to-text (image,text)-to-text (image,text)-to-image image-to-image (image,text)-to-(image,text) text-to-image text-to-(image,text) 126,177 13,823 98,040 41,960 56,185 27,988 27,656 14,090 14,081 VQA (image,text)-to-text 140, Table 6: Statistics of the multimodal synthetic data used for training mmE5. In this study, we introduce synthetic multimodal multilingual embedding dataset designed to facilitate model learning. This section delves into the details of our synthetic dataset. The dataset is comprised of three distinct tasks and seven modality combinations, totaling 560K data samples. Table 6 provides detailed statistical overview of our synthetic data, categorized by tasks and modalities."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Data Synthesis For the data synthesis process, we employ the MLLM GPT-4o-2024-08-06 model to generate data samples. Both the temperature and top-p parameters are set to 1.0 to ensure diverse and coherent outputs. Our image corpus is sourced from LAION-400m (Schuhmann et al., 2021), from which we exclude images that are either corrupted or have inaccessible URLs. Each synthetic data sample incorporates one image sampled from this corpus as the query image. For modality combinations that include images on the document side, we utilize the jina-clip-v25 model to retrieve similar image, along with hard negative image, to serve as additional inputs. B.2 Finetuning Embedding Model We train mmE5 using the open-source MLLM, Llama-3.2-11B-Vision6. The training is conducted on 64 NVIDIA A100 GPUs, each equipped with 40GB of memory. To optimize GPU memory usage, we employ gradient checkpointing and set the gradient accumulation steps to 4. The model is trained with learning rate of 2e-5 for one epoch, utilizing both synthetic and labeled data. LoRA (Hu et al., 2022) is applied to the MLLM with rank of 8. Each training sample incorporates one hard negative document. Hard negatives are mined for each subset of MMEB using VLM2VecLoRA7, with the 70th position in the ranking list selected as the hard negative sample."
        },
        {
            "title": "More implementation details can be found in",
            "content": "https://github.com/haon-chen/mmE5."
        },
        {
            "title": "C Prompts",
            "content": "We use different prompts of data synthesis for different tasks. For retrieval task, we design two prompts for modality combinations that involve images on the document side or not. Let us take the prompt of generating classification data for an example to illustrate the prompt design. First, we sample modality combination from {image-to-text, (image,text)-to-text}. If the query side does not include texts, the input_text of the classification data sample will be an empty string. Similarly, for modalities of retrieval task that do not include document texts, the positive_document and hard_negative_document will be empty. Following previous works of synthesizing text embedding data (Wang et al., 2024a; Chen et al., 2024), we will randomly select clarity and difficulty setting to enhance diversity. Then, for the multi-aspect visual description process, we ask the MLLM to explicitly include four perspectives of description. Besides, for the data synthesis process, we also ask the MLLM to follow some specific guidelines. Furthermore, the MLLM will evaluate the initially generated data from several aspects and provide possible_improvements. Finally, the revised version of data will be used as the output data sample. Note that there are no task instructions generated for the VQA task, since they are all fixed as Represent the given image with the following question:."
        },
        {
            "title": "D Data Examples",
            "content": "In this section, we present the examples of the synthetic multimodal embedding data for Retrieval 5https://huggingface.co/jinaai/jina-clip-v2 6https://huggingface.co/meta-llama/Llama-3. 2-11B-Vision 7https://huggingface.co/TIGER-Lab/ VLM2Vec-LoRA Prompt: Synthesizing Classification Data Your mission is to first produce detailed visual description of the image (within 300 words), identifying all potential aspects for generating high-quality data for {image-to-text, (image,text)-to-text} classification task. Based on the description, brainstorm potentially useful task. Here are few examples for your reference: {example tasks} Then, you should write one multi-modal classification example for this task in JSON format. The JSON object must contain the following keys: - \"description\": string, your detailed visual description, listing all required elements. - \"task_instruction\": string, describing the classification task. - \"input_text\": {\"an empty string\", \"a string the input text specified by the classification task\"}. - \"label\": string, the correct label of the image and input_text (if not empty) based on the task instruction. - \"misleading_label\": string, an incorrect label that is related to the task. - \"evaluation\": string, brief summary of the evaluation of data quality. - \"possible_improvements\": string, suggestions for improving the data based on the guidelines. - \"revised_task_instruction\": the revised task instruction. - \"revised_input_text\": the revised input text, {\"an empty string\", \"a string the input text specified by the classification task\"}. - \"revised_label\": the revised label. - \"revised_misleading_label\": the revised misleading label. For the description, please include the following elements: - General Description: Provide an overall summary of the image, including the primary objects, scene, and notable features. - Object-Level Details: Identify the individual objects in the image, their attributes (e.g., color, size, position), and their relationships to one another. - Contextual Features: Describe the scene or environment, including background details, lighting, and any actions taking place. - Task-specific Brainstorming: Analyze explore how this image could relate to text (e.g., captions, contextual descriptions). Please adhere to the following guidelines: - Task should be suitable for the given image. - Avoid generate task similar to classification of sentiment / subject / study field / genre / main topic / spam / urgency / language. - The \"input_text\" should be {\"less than 10\", \"at least 10\", \"at least 50\", \"at least 100\", \"at least 200\"} words and diverse in expression (if not empty). - The \"misleading_label\" must be valid label for the given task, but not as appropriate as the \"label\" for the image. - The text of \"task_instruction\" should be in English and others fields should be in {language}. - Avoid including the values of the \"label\" and \"misleading_label\" fields in the \"input_text\" (if not empty), that would make the task too easy. - The \"input_text\" (if not empty) is {\"clear\", \"understandable with some effort\", \"ambiguous\"} and requires {\"high school\", \"college\", \"PhD\"} level education to comprehend. - When generating the data, please evaluate the following aspects: 1. Relevance: Are the generated input texts and labels (if not empty) tightly connected to their corresponding image and task objectives? Does the task instruction effectively link the query image with the positive label? 2. Plausibility: Are misleading labels sufficiently relevant to the image or labels while remaining definitively incorrect? Could they mislead the model? 3. Clarity: Is the generated task clear and unambiguous, providing sufficient instruction to connect the query image with the label, without being overly specific or abstract? 4. Diversity: Does the generated data introduce variation in task instructions, texts (if not empty), and labels to avoid repetitive patterns in the dataset? - Provide detailed evaluation of the data based on the above criteria. For each criterion, explain specific flaws or strengths. - Suggest specific revisions to address any identified weaknesses, ensuring the revised data better aligns with the guidelines and task objectives. - Avoid revisions that overly simplify the task instruction, text (if not empty), or labels, as this may reduce their utility for training. - Ensure that revised data maintains consistency with the corresponding image content and classification task requirements. Your output must always be JSON object only. Do not explain yourself or output anything else. Be creative! Prompt: Synthesizing VQA Data Your mission is to first produce detailed visual descriptions of the image (within 300 words), identifying all potential aspects for generating high-quality data for visual QA task. Based on the description, write one visual QA example based on the given image in JSON format. The JSON object must contain the following keys: - \"description\": string, your detailed visual description, listing all required elements. - \"question\": string, specifying the question based on the image content. - \"positive_answer\": string, the correct answer for the question based on the image content. - \"hard_negative_answer\": string, an incorrect answer that appears plausible but is ultimately wrong. - \"evaluation\": string, brief summary of the evaluation of data quality. - \"possible_improvements\": string, suggestions for improving the data based on the guidelines. - \"revised_question\": the revised question. - \"revised_positive_answer\": the revised positive answer. - \"revised_hard_negative_answer\": the revised hard negative answer. For the description, please include the following elements: - General Description: Provide an overall summary of the image, including the primary objects, scene, and notable features. - Object-Level Details: Identify the individual objects in the image, their attributes (e.g., color, size, position), and their relationships to one another. - Contextual Features: Describe the scene or environment, including background details, lighting, and any actions taking place. - Task-specific Brainstorming: Analyze explore how this image could relate to text (e.g., captions, contextual descriptions). Please adhere to the following guidelines: - The \"question\" should be {\"less than 10\", \"at least 10\", \"at least 50\", \"at least 100\", \"at least 200\"} words and diverse in expression. - The \"hard_negative_answer\" must be plausible but less appropriate than the \"positive_answer\". - The values for all fields should be in {language}. - Avoid including explicit hints in the question that make the answer too obvious. - The \"question\" (if not empty) is {\"clear\", \"understandable with some effort\", \"ambiguous\"} and requires {\"high school\", \"college\", \"PhD\"} level education to comprehend. - When generating the data, please evaluate the following aspects: 1. Relevance: Are the generated question and answers tightly linked to the image content and consistent with the task requirements? 2. Plausibility: Does the \"hard_negative_answer\" closely resemble the \"positive_answer\" while remaining definitively incorrect? Could it mislead the model? 3. Diversity: Does the generated data introduce variation in questions, and answers to avoid repetitive patterns in the dataset? - Provide detailed evaluation of the data based on the above criteria. For each criterion, explain specific flaws or strengths. - Suggest specific revisions to address any identified weaknesses, ensuring the revised data better aligns with the guidelines and task objectives. - Avoid revisions that overly simplify or trivialize the \"question\". - Ensure revised data maintain consistency with the image content and task-specific requirements. Your output must always be JSON object only. Do not explain yourself or output anything else. Be creative! (Figure 6 and Figure 7), Classification (Figure 8), and VQA (Figure 9) tasks."
        },
        {
            "title": "E Detailed Results",
            "content": "In this section, we present the detailed comparisons of mmE5 to baseline models on both zero-shot and supervised finetuning settings on the MMEB benchmark (Jiang et al., 2024b). Due to space limitation, we omit the detailed results of partially supervised finetuning models. Prompt: Synthesizing Retrieval Data (Only Query Image) Your mission is to first produce detailed visual description of the image (within 300 words), identifying all potential aspects for generating high-quality data for {image-to-text, (image,text)-to-text} retrieval task. Based on the description, brainstorm potentially useful task. Here are few examples for your reference: {example tasks} Then, you should write one retrieval example for this task in JSON format. The JSON object must contain the following keys: - \"description\": string, your detailed visual description, listing all required elements. - \"task_instruction\": string, describing the retrieval task. - \"query\": {\"an empty string\", \"a random user search query specified by the retrieval task and the query image.\"} - \"positive_document\": string, the relevant document for the query image content. - \"hard_negative_document\": string, hard negative document that only appears relevant to the query image content. - \"evaluation\": string, brief summary of the evaluation of data quality. - \"possible_improvements\": string, suggestions for improving the data based on the guidelines. - \"revised_task_instruction\": the revised task instruction. - \"revised_query\": the revised query, {\"an empty string\", \"a random user search query specified by the retrieval task and the query image.\"}. - \"revised_positive_document\": the revised positive document, string, the relevant document for the query image content. - \"revised_hard_negative_document\": the revised hard negative document, string, hard negative document that only appears relevant to the query image content. For the description, please include the following elements: - General Description: Provide an overall summary of the image, including the primary objects, scene, and notable features. - Object-Level Details: Identify the individual objects in the image, their attributes (e.g., color, size, position), and their relationships to one another. - Contextual Features: Describe the scene or environment, including background details, lighting, and any actions taking place. - Task-specific Brainstorming: Analyze explore how this image could relate to text (e.g., captions, contextual descriptions). Please adhere to the following guidelines: - The task should involve both query and documents (positive and hard negative, if not empty). It must directly indicate the relation without being overly detailed or abstract. - The query (if not empty) should be {\"extremely long-tail\", \"long-tail\", \"common\"}, {\"less than 5 words\", \"5 to 15 words\", \"at least 10 words\"}, {\"clear\", \"understandable with some effort\", \"ambiguous\"}, and diverse in topic. - All documents (if not empty) must be created independent of the query. Avoid copying the query verbatim. Its acceptable if some parts of the \"positive_document\" are not topically related to the query. - All documents (if not empty) should be at least {\"10\", \"30\", \"200\", \"300\"} words long. - The \"hard_negative_document\" (if not empty) contains some useful information, but it should be less useful or comprehensive compared to the \"positive_document\". - The text of \"task_instruction\" should be in English and others fields should be in {language}. - Do not provide any explanation in any document (if not empty) on why it is relevant or not relevant to the query. - Do not use the word \"query\" or \"document\" in the generated content. - Both the query and documents (if not empty) require {\"high school\", \"college\", \"PhD\"} level education to understand. - When generating the data, please evaluate the following aspects: 1. Relevance: Are the generated query and documents (if not empty) tightly connected to their corresponding image and task objectives? Does the task instruction effectively link the query image with the positive text? 2. Plausibility: Are hard negatives sufficiently similar to the query or positive examples while remaining definitively incorrect? Could they mislead the model? 3. Clarity: Is the generated task clear and unambiguous, providing sufficient instruction to connect the query image with the positive document, without being overly specific or abstract? 4. Diversity: Does the generated data introduce variation in task instructions, queries, and documents to avoid repetitive patterns in the dataset? - Provide detailed evaluation of the data based on the above criteria. For each criterion, explain specific flaws or strengths. - Suggest specific revisions to address any identified weaknesses, ensuring the revised data better aligns with the guidelines and task objectives. - Avoid revisions that overly simplify the task instruction, query, or documents, as this may reduce their utility for training. - Ensure that revised data maintains consistency with the corresponding image content and retrieval task requirements. Your output must always be JSON object only. Do not explain yourself or output anything else. Be creative! Prompt: Synthesizing Retrieval Data (With Document Images) Your mission is to first produce detailed visual descriptions of the images (within 600 words), identifying all potential aspects for generating high-quality data for {(image,text)-to-image, image-to-image, (image,text)-to-(image,text), text-to-image, text-to-(image,text)} retrieval task that involves both query and document images. Based on the description, brainstorm potentially useful task. Here are few examples for your reference: {example tasks} Then, you should write one retrieval example for this task in JSON format. The JSON object must contain the following keys: - \"description\": string, your detailed visual description, listing all required elements. - \"task_instruction\": string, describing the retrieval task. - \"query\": {\"an empty string\", \"a random user search query specified by the retrieval task and the query image.\"} - \"positive_document\": {\"an empty string\", \"a string, the relevant document for the query based on the query text and image content\"} - \"hard_negative_document\": {\"an empty string\", \"a string, hard negative document that only appears relevant to the query and the query image content.\"} - \"evaluation\": string, brief summary of the evaluation of data quality. - \"possible_improvements\": string, suggestions for improving the data based on the guidelines. - \"revised_task_instruction\": the revised task instruction. - \"revised_query\": the revised query, {\"an empty string\", \"a random user search query specified by the retrieval task and the query image.\"}. - \"revised_positive_document\": the revised positive document, string, {\"an empty string\", \"a string, the relevant document for the query based on the query text and image content\"} - \"revised_hard_negative_document\": the revised hard negative document, {\"an empty string\", \"a string, hard negative document that only appears relevant to the query and the query image content.\"} For the description, please include the following elements: - General Description: Provide an overall summary of the image, including the primary objects, scene, and notable features. - Object-Level Details: Identify the individual objects in the image, their attributes (e.g., color, size, position), and their relationships to one another. - Contextual Features: Describe the scene or environment, including background details, lighting, and any actions taking place. - Task-specific Brainstorming: Analyze explore how this image could relate to text (e.g., captions, contextual descriptions). Please adhere to the following guidelines: - The task must connect the query image and positive image through their content. It must directly indicate the relation without being overly detailed or abstract. - The query (if not empty) should be {\"extremely long-tail\", \"long-tail\", \"common\"}, {\"less than 5 words\", \"5 to 15 words\", \"at least 10 words\"}, {\"clear\", \"understandable with some effort\", \"ambiguous\"}, and diverse in topic. - The query (if not empty) should effectively associate the query image with the positive image. - All documents (if not empty) must be created independent of the query. Avoid copying the query verbatim. Its acceptable if some parts of the \"positive_document\" are not topically related to the query. - All documents (if not empty) should be at least {\"10\", \"30\", \"200\", \"300\"} words long. - The \"hard_negative_document\" (if not empty) contains some useful information, but it should be less useful or comprehensive compared to the \"positive_document\". - The text of \"task_instruction\" should be in English and others fields should be in {language}. - Do not provide any explanation in any document (if not empty) on why it is relevant or not relevant to the query. - Do not use the word \"query\" or \"document\" in the generated content. - Both the query and documents (if not empty) require {\"high school\", \"college\", \"PhD\"} level education to understand. - When generating the data, please evaluate the following aspects: 1. Relevance: Are the generated query and documents (if present) tightly linked to their corresponding images? Does the task instruction effectively connect the query image to the positive image? 2. Plausibility: Are the negative examples, including hard negatives, realistic and similar enough to the positive image to challenge the model, while still being definitively incorrect? 3. Clarity: Is the generated task clear and unambiguous, providing sufficient instruction to connect the query image with the positive image, without being overly specific or abstract? 4. Diversity: Does the generated data introduce variation in task instructions, queries, and documents to avoid repetitive patterns in the dataset? - Provide detailed evaluation of the data based on the above criteria. For each criterion, explain specific flaws or strengths. - Suggest specific revisions to address any identified weaknesses, ensuring the revised data better aligns with the guidelines and task objectives. - Avoid revisions that overly simplify the task or create unrealistic connections between the query and positive image. - Ensure that revised data maintains consistency with the corresponding image content and retrieval task requirements. Your output must always be JSON object only. Do not explain yourself or output anything else. Be creative! Figure 6: An example of the synthetic Retrieval IT2IT data (part 1). This part includes the input images, the multi-aspect descriptions, and the initially generated data. Figure 7: An example of the synthetic Retrieval IT2IT data (part 2). This part includes the evaluation, possible improvements, and the revised data. Figure 8: An example of the synthetic Classification IT2T data. Figure 9: An example of the synthetic VQA IT2T data. Task Zero-shot Setting Models Supervised Finetuning Models CLIP OpenCLIP SigLIP BLIP2 MagicLens E5-V MMRet mmE5 VLM2Vec MMRet mmE5 Classification (10 tasks) ImageNet-1K N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country-211 All Classification VQA (10 tasks) OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA Avg. Retrieval (12 tasks) VisDial CIRR VisualNews_t2i VisualNews_i2t MSCOCO_t2i MSCOCO_i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS Avg. 55.8 34.7 51.1 50.7 43.4 28.5 25.5 75.6 43.4 19.2 42.8 7.5 3.8 4.0 4.6 1.4 4.0 9.4 8.2 41.3 7.0 9. 30.7 12.6 78.9 79.6 59.5 57.7 60.4 67.5 11.4 55.0 41.1 81.0 53.0 Visual Grounding (4 tasks) MSCOCO RefCOCO RefCOCO-matching Visual7W-pointing Avg. Final Score (36 tasks) All IND Avg. All OOD Avg. All Avg. 33.8 56.9 61.3 55.1 51. 37.1 38.7 37.8 63.5 38.6 51.7 52.4 68.8 37.8 14.2 83.0 51.4 16.8 47.8 11.5 3.3 5.3 4.6 1.5 2.6 10.2 6.6 52.5 10.9 10.9 25.4 15.4 74.0 78.0 63.6 62.1 66.1 62.1 13.8 44.6 45.0 77.5 52.3 34.5 54.2 68.3 56.3 53.3 39.3 40.2 39. 45.4 13.9 47.2 64.3 39.6 20.0 42.6 75.0 40.3 14.2 40.3 2.4 1.5 4.2 2.7 3.0 1.2 7.9 2.3 57.5 1.0 8.4 21.5 15.1 51.0 52.4 58.3 55.0 62.9 58.1 20.1 55.1 56.0 23.6 31.6 46.4 70.8 50.8 70.1 59.5 32.3 38.0 34.8 10.3 36.0 49.6 52.1 34.5 21.5 3.2 39.7 20.6 2.5 27. 8.7 3.2 2.6 2.0 0.5 1.3 6.8 4.0 9.7 3.3 4.2 18.0 9.8 48.1 13.5 53.7 20.3 56.5 55.4 9.3 28.7 39.5 54.4 33.9 28.9 47.4 59.5 52.0 47.0 25.3 25.1 25.2 48.0 33.7 49.0 51.6 57.0 31.5 8.0 70.9 31.6 6.2 38.8 12.7 2.9 3.0 5.9 0.9 2.5 5.2 1.7 43.5 4.6 8. 24.8 39.1 50.7 21.1 54.1 40.0 58.1 43.0 11.2 18.7 1.6 62.6 35.4 22.1 22.8 35.6 23.4 26.0 31.0 23.7 27.8 9.6 23.4 49.7 49.9 33.1 8.6 2.0 30.8 7.5 3.1 21.8 8.9 5.9 1.7 2.3 2.4 5.8 3.6 2.6 7.8 3.2 4.9 9.2 6.1 13.5 8.1 20.7 14.0 4.2 17.7 2.8 8.6 5.9 26.8 11. 10.8 11.9 38.9 14.3 19.0 14.9 11.5 13.3 49.1 45.8 51.0 74.6 60.1 35.3 31.6 66.2 49.2 9.3 47.2 28.0 11.6 12.6 10.6 2.4 9.0 23.3 25.9 41.3 18.9 18.4 62.6 65.7 45.7 53.4 68.7 56.7 59.4 76.3 31.5 25.4 73.0 59.9 56.5 42.7 69.3 63.2 73.5 62. 43.5 44.3 44.0 68.8 54.5 55.0 73.9 72.7 39.7 46.1 86.2 74.8 35.1 60.7 56.6 50.0 81.3 44.0 35.2 40.4 47.3 54.0 65.4 83.1 55.7 39.1 41.6 51.2 64.9 55.0 59.1 58.9 82.9 21.6 58.8 67.6 55.2 54.7 59.0 78.9 80.8 71.2 72.5 57.2 60.4 58. 74.5 80.3 67.9 91.5 75.8 44.0 43.6 79.8 39.6 14.7 61.2 69.0 54.4 52.0 30.7 34.8 49.8 42.1 43.0 61.2 62.0 49.9 80.9 49.9 75.4 80.0 75.7 73.1 65.5 87.6 16.2 60.2 56.5 87.8 67.4 80.6 88.7 84.0 90.9 86.1 67.5 57.1 62.9 58.8 71.3 53.7 85.0 70.0 43.0 36.1 71.6 55.8 14.7 56. 73.3 56.7 78.5 39.3 41.7 49.5 45.2 51.7 59.0 79.0 57.4 83.0 61.4 74.2 78.1 78.6 72.4 68.3 90.2 54.9 24.9 87.5 65.6 69.9 76.8 89.8 90.6 77.0 83.6 59.1 68.0 64.1 77.8 81.7 64.2 91.0 77.7 43.0 56.3 86.3 62.5 35.4 67.6 67.6 56.1 90.3 56.5 50.5 51.9 55.8 52.8 61.7 83.3 62. 74.1 54.7 77.6 83.3 76.4 73.2 68.3 88.0 28.8 65.8 77.5 83.7 71.0 53.7 92.7 88.8 92.3 89.6 72.3 66.7 69.8 Table 7: Detailed results of zero-shot setting and supervised setting models on each dataset of MMEB (Jiang et al., 2024b)."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "Microsoft Corporation"
    ]
}