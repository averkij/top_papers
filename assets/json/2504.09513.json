{
    "paper_title": "DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion",
    "authors": [
        "Puyu Han",
        "Jiaju Kang",
        "Yuhang Pan",
        "Erting Pan",
        "Zeyu Zhang",
        "Qunchao Jin",
        "Juntao Jiang",
        "Zhichen Liu",
        "Luqi Gong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large-scale pre-trained diffusion models have produced excellent results in the field of conditional image generation. However, restoration of ancient murals, as an important downstream task in this field, poses significant challenges to diffusion model-based restoration methods due to its large defective area and scarce training samples. Conditional restoration tasks are more concerned with whether the restored part meets the aesthetic standards of mural restoration in terms of overall style and seam detail, and such metrics for evaluating heuristic image complements are lacking in current research. We therefore propose DiffuMural, a combined Multi-scale convergence and Collaborative Diffusion mechanism with ControlNet and cyclic consistency loss to optimise the matching between the generated images and the conditional control. DiffuMural demonstrates outstanding capabilities in mural restoration, leveraging training data from 23 large-scale Dunhuang murals that exhibit consistent visual aesthetics. The model excels in restoring intricate details, achieving a coherent overall appearance, and addressing the unique challenges posed by incomplete murals lacking factual grounding. Our evaluation framework incorporates four key metrics to quantitatively assess incomplete murals: factual accuracy, textural detail, contextual semantics, and holistic visual coherence. Furthermore, we integrate humanistic value assessments to ensure the restored murals retain their cultural and artistic significance. Extensive experiments validate that our method outperforms state-of-the-art (SOTA) approaches in both qualitative and quantitative metrics."
        },
        {
            "title": "Start",
            "content": "DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion Puyu Han1 Jiaju Kang2 Yuhang Pan3 Erting Pan4 Zeyu Zhang5 Juntao Jiang7 Zhichen Liu1 Luqi Gong8 Qunchao Jin6 5 2 0 2 3 ] . [ 1 3 1 5 9 0 . 4 0 5 2 : r 1Southern University of Science and Technology 2Beijing Normal University 3Hebei Guoyan Science and Technology Center 4Wuhan University 5The Australian National University 6AI Geeks 7Zhejiang University 8Zhejiang Lab Abstract Large-scale pre-trained diffusion models have produced excellent results in the field of conditional image generation. However, restoration of ancient murals, as an important downstream task in this field, poses significant challenges to diffusion model-based restoration methods due to its large defective area and scarce training samples. Conditional restoration tasks are more concerned with whether the restored part meets the aesthetic standards of mural restoration in terms of overall style and seam detail, and such metrics for evaluating heuristic image complements are lacking in current research. We therefore propose DiffuMural, combined Multi-scale convergence and Collaborative Diffusion mechanism with ControlNet and cyclic consistency loss to optimise the matching between the generated images and the conditional control. DiffuMural demonstrates outstanding capabilities in mural restoration, leveraging training data from 23 large-scale Dunhuang murals that exhibit consistent visual aesthetics. The model excels in restoring intricate details, achieving coherent overall appearance, and addressing the unique challenges posed by incomplete murals lacking factual grounding. Our evaluation framework incorporates four key metrics to quantitatively assess incomplete murals: factual accuracy, textural detail, contextual semantics, and holistic visual coherence. Furthermore, we integrate humanistic value assessments to ensure the restored murals retain their cultural and artistic significance. Extensive experiments validate that our method outperforms state-of-the-art (SOTA) approaches in both qualitative and quantitative metrics. CCS Concepts Applied computing Fine arts; Digital libraries and archives. Keywords Mural Restoration, Heritage Protection, AI for Social Good"
        },
        {
            "title": "1 Introduction\nThe Mogao Grottoes in Dunhuang, a World Heritage Site, shelter\nthe largest and most well-preserved ancient murals [30]. These in-\nvaluable masterpieces, as non-renewable resources, have sustained\nconsiderable damage due to both natural factors and human ac-\ntivities over a long time. A series of degradations such as cracks,\nflaking, structural collapse, and fading colors (refer to Fig. 3) have\ntarnished their original splendor, marking a profound loss to human\ncivilization [25]. Faced with the risks of further irreversible damage\nto the delicate surfaces of the murals and the labor-intensive costs,",
            "content": "Figure 1: Disruption image and the restoration result of mural in Eastern Wall, Cave 320, Dunhuang with our DiffuMural model. the cultural heritage community has grown increasingly cautious about relying on hand-coloring methods. In contrast, digital restoration, utilizing digital high-resolution murals as medium, offers greater tolerance for error and presents broader array of solutions for the restoration process [44]. Thus, the exploration of effective digital techniques for restoring these murals in high-resolution is of paramount importance for large-scale restoration and the long-term preservation of these cultural treasures [39, 41]. Deep learning-based image inpainting and restoration methods have made considerable strides in recent years [49], yet they may encounter significant challenges when applied to the restoration of murals. This is due to the unique nature of mural painting, which involves the creation of smooth lines and evocative colors that differ from those typically found in natural images. As such, inpainting techniques designed for natural images may not be well-suited for restoring the distinct edge structures and intricate texture details inherent in murals. Furthermore, the varying degrees of damage to murals often alter the nature of the restoration task, and nearly all current digital restoration technologies are primarily focused on simpler restoration challenges, such as filling small defects and restoring localized color. Notably, heavily damaged murals (refer to Fig. 4a upper part) may have lost substantial amounts of information, rendering the inpainting process ineffective when simulated and learned through existing rectangular or arbitrary masks. Furthermore, we observed that the damaged areas contain wealth of vivid and meaningful content, which holds immense potential to provide crucial insights for the restoration process and should not be overlooked. Inspired by this, we integrate the spatial dynamics of damaged regions in high-resolution murals and propose generative framework, DiffuMural, for the task of restoring large areas of the missing mural. The main contributions can be summarized below: (1) Self-Guidance via Damage Contour: Taking segmentation masks of damaged regions as conditional guidance, ensuring stylistic and textural consistency between the restored and undamaged parts of the mural. (2) Feature Fusion Across Multi-Scale: Integrating information from both low-resolution and high-resolution layers, balancing global structure and local details to enhance the restoration quality and consistency, even with limited sample data. (3) Co-Diffusion Across Multi-Scale: Propagating high-confidence information across layers of different resolutions, resulting in more realistic and seamless integration of the restored areas with the original mural. (4) Humanistic Value Alignment Evaluation: Evaluating the restoration performance with mural restoration experts, ensuring that the restored sections seamlessly blended with the cultural and historical significance of the original artwork."
        },
        {
            "title": "2 Related work\nDigital restoration of ancient mural. AI-based mural restoration\ninvolves using deep learning methods to reconstruct damaged mu-\nrals while preserving their original style and detail. Key challenges\ninclude style fidelity, handling large missing areas, and balanc-\ning artistic authenticity with visual accuracy. GAN-based methods\n[3, 4, 17, 24, 32, 38, 42, 46] have demonstrated notable effective-\nness in generating visually coherent and stylistically consistent\nresults by leveraging adversarial training to recreate fine details\nand textures. Diffusion model-based methods [48, 50] have also\nbeen widely used in this task [14, 34, 45], showing a strong capabil-\nity to model complex structures and nuanced artistic patterns even\nwith substantial damage, making them well-suited for the intricate\ntextures often found in ancient murals. More effective design with\nthe generative model can enhance mural restoration by allowing\nprecise reconstruction of details and artistic elements.",
            "content": "Figure 2: The challenge of our restoration task. Figure 3: Types of mural damages: cracks, flaking, structural collapse, and fading colors Conditional guidance generation. Conditional generation is type of task for generative models that uses conditional inputs to control the features of generated outputs. This task offers significant flexibility and controllability, allowing the model to produce desired outputs based on varying input conditions, which has important applications in image restoration. The core challenge lies in maintaining high generation quality while ensuring strong correlation between the condition and the generated content. Attempts focus on introducing conditional information into generative models. VAE-based methods [12, 28, 36] have high stability, are easy to train, and can effectively integrate conditional information. GAN-based theories and applications [5, 79, 15, 20, 26, 51] feed conditional information to both the generator and discriminator, allowing the generator to produce data in specific style based on the given condition. Conditional diffusion-based models [1, 2, 6, 13, 27, 35, 52, 53] incorporate conditional information into the generation process, producing outputs under specific conditions by controlling diffusion steps. These models demonstrate powerful generative capabilities in tasks such as text and image generation, particularly excelling in high-resolution and detail-rich generation tasks. ControlNet [47] integrates additional guidance, such as edges, depth, or pose information, allowing the model to generate content with greater alignment to specified details, showing great success. Large-scale pre-training. Large-scale pre-training has become cornerstone in modern machine learning, especially in tasks like image generation. Large-scale pre-training enables models like CLIP [31] to learn deep semantic relationships between images and text by training on vast amounts of image-text pairs. Generative models like Stable Diffusion [33] can leverage the mapping provided by CLIP to create high-quality images based on textual descriptions. Large-scale pre-training has been widely used in image retortion and inpainting [10, 18, 19, 21, 23, 40, 43]. By learning from vast amounts of data, pre-trained models can easily understand the context and structure of images, which helps in tasks like inpainting or repairing damaged images, filling in missing parts of an image with realistic details, drawing from learned knowledge of how objects and scenes should appear, offering enhanced accuracy, creativity, and context-aware solutions for repairing and reconstructing images."
        },
        {
            "title": "3 Overview\nThe mural restoration task described in this paper was proposed\nby the Dunhuang Research Institute. In contrast to previous mu-\nral restoration tasks, this is an exploratory restoration of a large\narea lossing mural, which currently lacks adequate evaluation met-\nrics. Consequently, we have introduced quantitative indicators,\nincluding colour consistency, texture consistency, edge consistency,\nstructural similarity, and others, as well as a human value assess-\nment system comprising professional mural restorers to evaluate\nthe restoration results.",
            "content": "In section 4, we will give the restoration process and the corresponding theoretical basis for the mural paintings in Dunhuang Cave No. 320, and compare the results with other models to verify the validity of the MuralDiff and to explore the practical application value of the MuralDiff."
        },
        {
            "title": "4 Restoration processes and Methodology\nFor the restoration of large-area lossing murals, the existing al-\ngorithms are not applicable. The main challenges are listed: the\ninference of large-area missing regions requires the restoration\nalgorithms to have strong contextual understanding and to reason-\nably mine effective guidance information to assist in the inference.\nIn addition, with limited data samples, we need to consider how to\nmaximise the intake of effective training information to ensure the\nmodel generation capability.",
            "content": "For the first challenge, we also propose conditionally guided mural controllable generation framework, which enhances the controllability of mural generation by extracting the hidden and fuzzy contours of the damaged regions and introduces the mural spatial attention mechanism to improve the contextual reasoning ability of the model. For the second challenge, we propose the theory of multi-scale fusion under the synergistic diffusion mechanism, which effectively solves the problem of insufficient training samples and ensures that the restored part is highly consistent with the original image in terms of style, texture, and color. (a) Damaged Image (b) Extracted Contour Figure 4: Damaged area and its corresponding extracted contour"
        },
        {
            "title": "4.1 Contour Extraction\nThe missing sections of the mural are rarely entirely blank; instead,\nthey typically exhibit faint outline textures, as shown in Fig. 4a. In\nthe artificial restoration process, these textures serve as valuable\nreferences to guide the restorerâ€™s work. Similarly, these textures\ncan also play auxiliary roles in the process of automatic repair.",
            "content": "In this paper, we aim to extract contour information from the missing regions of the image, shown in Fig. 4b, and utilize it as conditional guidance to inform the restoration of the damaged areas. Specifically, this paper employs ğ¾-Means based approach to implement edge extraction of damaged region images. The damaged image, with dimensions â„ ğ‘¤, is first serialized, where each pixel is treated as an individual. This paper applies the ğ¾-Means clustering algorithm to classify the â„ğ‘¤ pixels into two categories, with ğ¾ = 2. The goal of ğ¾-means is to minimize the following objective function: ğ½ = ğ‘› ğ¾ ğ‘–=1 ğ‘˜= ğ‘Ÿğ‘–ğ‘˜ ğ‘¥ğ‘– ğœ‡ğ‘˜ 2 , ğ‘› = â„ğ‘¤, ğ¾ = 2 (1) where ğ‘Ÿğ‘–ğ‘˜ is binary indicator that equals 1 if specified pixel ğ‘¥ğ‘– belongs to cluster ğ‘˜, and 0 otherwise. ğœ‡ğ‘˜ is the center of cluster ğ‘˜, which is the mean of all pixels assigned to that cluster. In this case, all pixels are divided into two clusters, foreground and background, allowing for the extraction of the images contours."
        },
        {
            "title": "Generation",
            "content": "The diffusion model shows powerful performance in image processing, and we introduce the diffusion model as the main architecture for mural restoration.The diffusion model defines Markovian chain of diffusion forward process ğ‘ (ğ‘¥ğ‘¡ ğ‘¥0) by gradually adding noise to input data ğ‘¥0: ğ‘¥ğ‘¡ = ğ›¼ğ‘¡ ğ‘¥0 + 1 ğ›¼ğ‘¡ ğœ–, where ğœ– is noise map sampled, with ğ›¼ğ‘¡ 1 ğ›½ğ‘¡ .The diffusion training loss can be represented by: ğ‘ =0 ğ›¼ğ‘  and ğ›¼ğ‘¡ = ğœ– (0, ğ¼ ), := (cid:206)ğ‘¡ (2) ğ‘‡ (ğœ–ğœƒ ) = ğ‘¡ =1 Eğ‘¥0ğ‘ (ğ‘¥0 ),ğœ–N (0,ğ¼ ) (cid:20)(cid:13) ğœ–ğœƒ (cid:13) (cid:13) (cid:16) ğ›¼ğ‘¡ ğ‘¥0 + 1 ğ›¼ğ‘¡ ğœ– (cid:17) ğœ– 2 (cid:21) (cid:13) (cid:13) (cid:13) . (3) During the inference, given random noise ğ‘¥ğ‘¡ (0, ğ¼ ), we can predict final denoised image ğ‘¥0 with the step-by-step denoising Figure 5: An overview of DiffuMural model. process: ğ‘¥ğ‘¡ 1 = 1 ğ›¼ğ‘¡ (cid:18) ğ‘¥ğ‘¡ 1 ğ›¼ğ‘¡ 1 ğ›¼ğ‘¡ ğœ–ğœƒ (xğ‘¡ , ğ‘¡) (cid:19) + ğœğ‘¡ ğœ– In the context of controllable generation, given the image condition ğ‘ğ‘£ and the text prompt ğ‘ğ‘¡ , the diffusion training loss for time step ğ‘¡ can be rewritten as: Ltrain = Eğ‘¥0,ğ‘¡,ğ‘ğ‘¡ ,ğ‘ğ‘£,ğœ–N (0,1) (cid:2)ğœ–ğœƒ (ğ‘¥ğ‘¡ , ğ‘¡, ğ‘ğ‘¡ , ğ‘ğ‘£) ğœ– 2 2 (cid:3) . (5) By minimizing the loss of consistency between the input condition ğ‘ğ‘£ and the corresponding output condition Ë†ğ‘ğ‘£ of the generated image ğ‘¥ 0, the control of various conditions is optimized in unified way, and more controllable generation is realized. The reward consistency loss can be expressed as: Lreward = (ğ‘ğ‘£, Ë†ğ‘ğ‘£) = (cid:0)ğ‘ğ‘£, (cid:0)ğ‘¥ 0 (cid:1)(cid:1) ğ‘ğ‘£, (cid:104)Gğ‘‡ (ğ‘ğ‘¡ , ğ‘ğ‘£, ğ‘¥ğ‘‡ , ğ‘¡) (cid:16) = (6) (cid:105) (cid:17) is an abstract metric function in which is the cross-entropy loss per pixel in the context of the use of the segmentation mask as an input condition control in mural restoration task. Reward model also depends on the condition, and we use UperNet as the segmentation mask condition.In addition to the reward loss, we added the diffusion training loss to ensure that the original image generation ability is not affected. Ultimately, the total loss is combination ofLtrain and Lreward : (4) Ltotal = Ltrain + ğœ† Lreward , where ğœ† is the hyperparameter that adjusts the weight of the reward loss. The consistency loss-guided diffusion model samples at different time steps to obtain images that are consistent with the input control, thereby improving controllability. (7)"
        },
        {
            "title": "4.3 Murals spatial attention mechanism\nTo enhance the modelâ€™s contextual inference ability and focus on\nimportant regions, we integrate a mural-spatial attention mech-\nanism (MSA) into the U-Net framework. This process involves\ndownsampling the input features to expand the receptive field\nand reduce noise. The downsampled features are transformed into\nquery, key, and value tensors through linear projections. The scaled\ndot product attention is then applied to compute the attention\nweights, which are used to aggregate value tensors. Specifically, we\nuse a scaling factor 1/âˆšï¸ğ‘‘ğ‘˜ to prevent vanishing gradients during\nthe dot product computation. Finally, the multi-head self-attention\nmodule (MSA) computes attention across multiple subspaces by\ndividing the query, key, and value tensors into ğ‘› parts, processing",
            "content": "them in parallel, and concatenating the results. This mechanism enables the model to effectively capture interdependencies across input features, improving the overall feature extraction and mural restoration performance."
        },
        {
            "title": "4.4 Multi-scale convergence and co-diffusion\nFor the task of restoration an mural with a large missing area, we\nhope that the model has strong context modelling capability as well\nas richer sensory fields to achieve better repair results. Although\nmultiscale fusion can be a good solution to the problem of global\nlocal information connectivity, the difference in the processing of\nlocal and global information between context modelling and multi-\nscale fusion mechanisms may lead to the loss of local information\nor ineffective integration into the global context, and the combi-\nnation of the two often requires a more complex mechanism to\ncoordinate. Therefore, we propose a scale fusion method based on\nthe synergistic diffusion mechanism, which not only circumvents\nproblems such as incompatible information loss but also can more\nflexibly assign the weights of different scales of information on\nthe impact of the generated results to achieve better collaborative\neffects.",
            "content": "At the heart of the co-diffusion mechanism is the dynamic diffuser, which adaptively predicts influence functions to enhance and support multi-scale fusion generation. Given ğ‘ conditional diffusion models (cid:8)ğœ–ğœƒğ‘› (cid:9) pre-trained by single-scale samples which models the distribution ğ‘ (x0 ğ‘ğ‘›).where the modality index ğ‘› = 1, . . . , ğ‘ , we will sample from ğ‘ (x0 c) without changing the pretrained model,where = {ğ‘1, ğ‘2, , ğ‘ğ‘€ }. In the inverse process of diffusion modelling, noise needs to be predicted at each step, so it must be carefully determined when, where, and how each diffusion model contributes. Corresponding to mural restoration, this means that each step of inferential modelling requires judgement about which scale of visual information should be referred to, to ensure that each level of image detail is fully exploited and reconstructed.At every diffusion time step ğ‘¡ = ğ‘‡ , . . . , 1, the influence Iğ‘›,ğ‘¡ from every pre-trained diffusion (cid:9) is adaptively determined by dynamic diffuserDğœ™ğ‘› model (cid:8)ğœ–ğœƒğ‘› : (8) Iğ‘›,ğ‘¡ = Dğœ™ğ‘› (xğ‘¡ , ğ‘¡, ğ‘ğ‘›) , where ğ‘› = 1, . . . , ğ‘ is index of the modalities, Iğ‘›,ğ‘¡ Râ„ğ‘¤, xğ‘¡ is the noisy image at time ğ‘¡, ğ‘ğ‘› is the condition of the ğ‘›ğ‘¡â„ modality,where is the dynamic diffuserimplemented by UNet. To count Dğœ™ğ‘› the overall impact strength, we perform cross-modal maximum calculation for each pixel, which ultimately results in the impact function Ë†Iğ‘›,ğ‘¡ : Ë†Iğ‘›,ğ‘¡,ğ‘ = exp (cid:0)Iğ‘›,ğ‘¡,ğ‘ (cid:1) ğ‘—=1 exp (cid:0)Iğ‘—,ğ‘¡,ğ‘ (cid:1) (cid:205)ğ‘ . (9) We use the learned information function Ë†Iğ‘›,ğ‘¡ to control the contribution of each pre-trained diffusion model to accomplish multi-scale fusion and collaborative diffusion: ğ pred ,ğ‘¡ = ğ‘ ğ‘›= Ë†Iğ‘›,ğ‘¡ ğğœƒğ‘› (ğ‘¥ğ‘¡ , ğ‘¡, ğ‘ğ‘›) (10) where ğğœƒğ‘š cation. is the ğ‘šth collaborator, denotes pixel-wise multipli-"
        },
        {
            "title": "4.5 Generated image optimization\nTo optimize the stability of the generation phase of the model, we\nneed to further optimize the generated results for the problem of\nblurring in some regions due to the difference in information match-\ning in multi-scale fusion and context modeling. We propose the\nFDP module to which using the frequency domain to enhance the\ndetailed information of different modalities, including texture and\ncolor information. we introduce the learned filter with parameters\ninto the feature space and adjust the convolutional weights to ad-\njust the specific frequencies of some parts of the image, and finally\nremap the adjusted frequency features back to the explicit space to\nobtain the final optimized results. the FDP module is essentially an\noptimizer that fine-tunes the generated results in areas where the\nvisual display is lacking.",
            "content": "Overall, our method can be summarized in algorithm 1. Algorithm 1: DiffuMural: Multi-scale Collaborative Mural Restoration Input: Damaged mural image ğ‘¥0, contour condition ğ‘ğ‘£, text prompt ğ‘ğ‘¡ Output: Restored mural Ë†ğ‘¥0 1 Step 1: Contour Extraction; 2 Extract contour mask ğ‘ğ‘£ from ğ‘¥0 using K-Means clustering with ğ¾ = 2; 3 Step 2: Conditional Guidance Setup; 4 Encode multi-modal conditions (ğ‘ğ‘£, ğ‘ğ‘¡ ); 5 Prepare multi-scale noisy inputs ğ‘¥ (ğ‘›) ğ‘¡ (0, ğ¼ ) at resolutions {256, 512, 1024}; 6 Step 3: Multi-scale Collaborative Diffusion; 7 for each timestep ğ‘¡ = ğ‘‡ 1 do 8 for each scale ğ‘› {1, 2, 3} do Compute influence map ğ¼ğ‘›,ğ‘¡ = ğ·ğœ™ğ‘› (ğ‘¥ (ğ‘›) ; Normalize influence: Ë†ğ¼ğ‘›,ğ‘¡,ğ‘ = ğ‘¡ Fuse predictions: ğœ–pred,ğ‘¡ = (cid:205)ğ‘ Denoise: ğ‘¥ğ‘¡ 1 = 1 ğ›¼ğ‘¡ (cid:16) ğ‘¥ğ‘¡ ğ‘›=1 1ğ›¼ğ‘¡ 1ğ›¼ğ‘¡ , ğ‘¡, ğ‘ğ‘›); (cid:205)ğ‘ exp(ğ¼ğ‘›,ğ‘¡,ğ‘ ) ğ‘— =1 exp(ğ¼ ğ‘—,ğ‘¡,ğ‘ ) Ë†ğ¼ğ‘›,ğ‘¡ ğœ–ğœƒğ‘› (ğ‘¥ (ğ‘›) ğ‘¡ (cid:17) + ğœğ‘¡ ğœ–; ğœ–pred,ğ‘¡ , ğ‘¡, ğ‘ğ‘›); 9 10 11 13 Step 4: Generated Image Optimization; 14 Apply Frequency-Domain Processing (FDP) module to enhance Ë†ğ‘¥0 for texture and color refinement; 15 return Ë†ğ‘¥"
        },
        {
            "title": "5 Experiments\nIn this section, we will conduct experiments to assess the reason-\nableness and effectiveness of DiffuMural in the mural restoration\ntask.",
            "content": "Comparative experiments. We compare DiffuMural with some advanced mural restoration models and recent SOTA methods, including StyleGAN [16], MDT [11], DiT-XL/2 [29], LaMa [37], RePaint [22] and Muraldiff [45]. We did not select more specialised mural restoration models for comparison as the mural restoration tasks were different and could not be applied to our dataset. Figure 6: The qualitative results of the mural restoration experiments, with each row representing different restoration method. Data. With the assistance of the Dunhuang Academy, we used laser scanner with resolution of 0.5ğ‘šğ‘š to collect data from Cave 320 at Dunhuang, getting 27 murals from the Tang dynasty (618907) with resolution of 8K, averaging about 40 ğ‘š2 per mural. The murals to be restored are taken out as test set, and we repeatedly crop the remaining 26 murals according to the scales of 256, 512, and 1024 respectively, with overlapping pixels of 70% of the total pixels, and remove samples containing invalid regions such as black colour carried over from scanning and flaking of the murals themselves. We end up with valid training samples total 420K images. Training Detail. All experiments are conducted on an Ubuntu 16.04.1 server equipped with 8 Nvidia A100 GPUs. All codes were developed in Python 3.8.10, PyTorch 1.12.1, and CUDA 11.7 environments. We trained the three mask-driven diffusion models and the dynamic diffuser in DiffuMural using samples with scales of 256, 512, and 10240, respectively, and in particular we iterated the model 1000 times by training it from scratch with batch size of 64 (8 per GPU). The other models are trained on fixed 512-scale samples, with the training steps remaining consistent. In addition, during the generation process, since this model uses the technical framework of conditionally controllable generation for bootstrap repair, other models were tested twice during the testing phase under conditional and unconditional bootstrapping respectively to compare the results with DiffuMural."
        },
        {
            "title": "5.1 Quantitative index\nIn the task of mural restoration, traditional metrics like FID cannot\nbe directly applied due to the lack of real-value references. To\naddress this, we propose several quantitative metrics tailored to\nevaluate restoration quality:",
            "content": "structural similarity. Structural similarity (SSIM) assesses the similarity in structure, brightness, and contrast between the restored and reference regions. For mural restoration, it is computed by comparing the restored area with undamaged or blurred reference regions: (cid:33) (cid:33) (11) SSIM(ğ‘¥, ğ‘¦) = (cid:32) 2ğœğ‘¥ ğ‘¦ + ğ¶2 ğ‘¥ + ğœ2 ğœ2 ğ‘¦ + ğ¶2 (cid:32) 2ğœ‡ğ‘¥ ğœ‡ğ‘¦ + ğ¶1 ğ‘¥ + ğœ‡2 ğœ‡2 ğ‘¦ + ğ¶1 where ğ‘¥ and ğ‘¦ are local window of the repaired image and the undamaged image, ğœ‡ğ‘¥ and ğœ‡ğ‘¦ are the mean of ğ‘¥ and ğ‘¦ ,representative brightness,ğœ2 ğ‘¦ are the variance of ğ‘¥ and ğ‘¦, expressing contrast.ğœğ‘¥ ğ‘¦ is the covariance of ğ‘¥ and ğ‘¦,ğ¶1 and ğ¶2 are constants. Colour consistency. Colour consistency (CCON) evaluates the similarity in color distribution between the restored and surrounding regions. Using color histograms, we calculate the Chi-Square Distance: ğ‘¥ and ğœ2 ğœ’ 2 CCON = (cid:16) ğ»repair (ğ‘–) ğ»original (ğ‘–) ğ»repair (ğ‘–) + ğ»original (ğ‘–) (cid:17) ğ‘› ğ‘–=1 (12) where ğ»repair and ğ»original are the colour histograms of the repaired and undamaged regions.ğœ’ 2 CCON is Chi-Square Distance with CCON. The smaller ğœ’ 2 CCON is, the higher the colour consistency Texture consistency. Texture consistency (TCON) measures the similarity of texture features between the restored and reference areas. Using Local Binary Patterns (LBP), the LBP histograms of both regions are compared using the same method as color histograms. Higher similarity scores indicate better texture consistency. Edge consistency. Edge Consistency (ECON) evaluates the preservation of edge details by comparing the gradient information of the restored and reference edge maps: (cid:205)ğ‘–,ğ‘— (cid:12)ğ¸repaired (ğ‘–, ğ‘—) ğ¸original (ğ‘–, ğ‘—)(cid:12) (cid:12) (cid:12) (cid:12)ğ¸original (ğ‘–, ğ‘—)(cid:12) (cid:12) (cid:12) (cid:205)ğ‘–,ğ‘— ECON = (13) where ğ¸repaired and ğ¸original are the edge obtained by performing edge detection on the restored image and the original image respectively."
        },
        {
            "title": "5.2 Quantitative results\nWe compare our method with several state-of-the-art approaches,\nincluding StyleGAN [16], MDT [11], DiT-XL/2 [29], LaMa [37],\nRePaint [22], and Muraldiff [45]. The quantitative results for our\ndataset are presented in Tables 1 and 2 for task 1 and task 2, re-\nspectively. Our method achieves outstanding results across four\ndifferent evaluation metrics. Specifically, for both tasks, our method\noutperforms others on the SSIM and ECON indicators, achieving\nthe best performance. In SSIM, our method improves by 6.33% and\n45.45%, respectively, compared to the second-best results. Similarly,\non the ECON indicator, our method shows an enhancement of\n28.24% and 22.43% over the second-best methods. Furthermore, for",
            "content": "the CCON and TCON metrics, our method performs competitively, with an average gap of only 6.47% compared to the best results for each metric. Table 1: The models are trained on our dataset and tested on the real damaged mural dataset for Task 1. The best results are highlighted in bold, and the second-best results are marked with underline. The same format is applied to the results in Table 2. Model Condition Guidance Metrics SSIM CCON TCON ECON StyleGAN [16] MDT [11] DiT-XL/2 [29] LaMa [37] RePaint [22] Muraldiff [45] Ours 0.27 0.61 0.55 0.67 0.44 0.69 0.31 0. 0.42 0.79 0.44 0.76 0.84 0.86 0.81 0.89 0.97 0.72 0.77 0.84 0. 0.89 0.87 0.78 0.88 0.94 0.78 0.77 0.24 0.38 0.74 0. 0.62 0.74 0.54 0.65 0.91 0.77 0.86 15.78 12.75 9.27 10. 8.44 7.72 10.02 12.61 8.41 6.27 5.17 5.42 3.71 Table 2: The results on the real damaged mural dataset for task 2, and the performance of these models are trained on our dataset. Model Condition Guidance Metrics SSIM CCON TCON ECON StyleGAN [16] MDT [11] DiT-XL/2 [29] LaMa [37] RePaint [22] Muraldiff [45] Ours 0.18 0.42 0.39 0.44 0.37 0.49 0.18 0.47 0.30 0.55 0.27 0. 0.80 0.85 0.87 0.92 0.94 0.77 0.76 0.85 0.78 0.89 0.81 0.92 0. 0.89 0.82 0.65 0.44 0.57 0.55 0.68 0.51 0.77 0.92 0. 0.48 0.65 0.81 12.48 11.24 10.21 10.45 3.21 4.48 6.52 6. 5.58 7.41 4.38 4.15 2."
        },
        {
            "title": "6 Discussion: Significance and Social Impact\nThe preservation of cultural heritage, particularly ancient murals,\nis a task of immense historical, artistic, and societal importance.\nMurals such as those in the Dunhuang Mogao Grottoes are irre-\nplaceable artifacts that embody the spiritual and aesthetic legacy\nof past civilizations. However, these treasures face increasing risks\nof irreversible damage due to natural degradation and human ac-\ntivity. Traditional manual restoration methods, while effective, are\nconstrained by time, cost, and ethical concerns. In this context, our\nproposed DiffuMural framework offers a scalable, accurate, and\nculturally sensitive AI-based solution to aid the digital restoration\nof such murals.",
            "content": "By incorporating multi-scale collaborative diffusion mechanism, contour-guided conditioning, and frequency-aware optimization, DiffuMural significantly improves the realism, consistency, and controllability of mural restoration. Unlike generic inpainting models, our method is trained on curated subset of stylistically coherent murals, adhering to restoration ethics by avoiding the hallucination of historically inaccurate content. Furthermore, we introduce human-centric evaluation system, integrating expert feedback from professional restorers to align the generated results with artistic authenticity and cultural significance. The broader societal impact of this work lies in its potential to democratize heritage protection. Through responsible AI for Social Good, DiffuMural provides museums, researchers, and educators with powerful tool to virtually restore, preserve, and present damaged cultural assets. This not only safeguards endangered heritage sites but also fosters greater public engagement and cultural appreciation. As bridge between machine learning and humanistic values, our work represents step forward in using AI to preserve collective memory for future generations."
        },
        {
            "title": "7 Conclusion\nIn this study, addressing the challenge of high-resolution mural\nrestoration, particularly the extensive areas of missing murals and\nthe limited availability of samples, we propose an effective gener-\native AI-based solution, named DiffuMural. We depart from the\nconventional approach of utilizing vast collections of mural data\nfrom various dynasties and styles, which is prevalent in current\nrestoration practices. Instead, we adhere to the core principles and\nethics of traditional manual restoration by training the model ex-\nclusively on a curated collection of 23 murals from the same grotto.\nOn the other hand, we develop a multi-scale collaborative diffusion\nmodel, fine-tuning it for exploratory restoration. Specifically, we\nextract the contours of the damaged areas as inputs to guide the\ndiffusion model for generating inferences, while enhancing feature\nfusion and co-diffusion mechanisms across different scales. Simul-\ntaneously, we introduce quantitative criteria such as stylistic con-\nsistency, texture coherence, edge integrity, and structural similarity,\nestablishing a human value assessment system for the restoration\noutcomes, composed of professional mural restorers. Our results\ndemonstrate that this method achieves superior exploratory restora-\ntion results for large-scale missing Dunhuang murals after iterative\nrefinement, offering valuable reference solutions for the manual\nrestoration of murals faced with similar challenges.",
            "content": "Figure 7: Performance of various models in the subjective evaluation system by mural experts. In our evaluation, six qualitative metrics are employed to assess the performance of digital mural restoration, covering aspects such as detailed restoration, color fidelity, material texture, humanities and arts restoration, visual naturalness, and historical authenticity. Each metric is scored on 0-100 scale, with detailed scoring criteria and quality descriptions provided in the Appendix B."
        },
        {
            "title": "5.4 Qualitative results\nWe conducted comparative restoration experiments on real-world\nmurals. To ensure the professionalism of the restoration process\nand the objectivity of the evaluation, we construct a human value\nassessment system. Concretely, we enlisted 289 distinguished mu-\nral restoration experts from the Dunhuang Academy, the China\nAssociation for the Protection of Cultural Relics, and the Tencent\nSSV Digital Culture Laboratory. These experts assessed over one\nhundred restoration outcomes from seven different methods. The\nevaluations were based on a percentage system across six qualitative\nindicators, with the final performance of each model determined\nby the weighted opinions of the experts, as illustrated in Fig. 7.\nThe results demonstrate that our proposed DiffuMural excels in\nrestoring damaged areas, particularly in crucial visual metrics such\nas color consistency, structural integrity, and visual similarity.",
            "content": "Our method performed exceptionally well in the expert evaluation, maintaining well-balanced set of indicators. This suggests that the proposed approach harmoniously integrates with the cultural and historical significance of the original artwork. While some experts expressed concerns about the ethical and moral dimensions of mural restoration, the method was widely recognized and endorsed by the majority of professionals in the field. Notably, our approach not only represents an innovative solution for restoring large sections of missing murals but also offers solid foundation for making final restoration decisions in real-world mural conservation efforts. References [1] Lorenzo Baldassari, Ali Siahkoohi, Josselin Garnier, Knut Solna, and Maarten de Hoop. 2024. Conditional score-based diffusion models for Bayesian inference in infinite dimensions. Advances in Neural Information Processing Systems 36 (2024). [2] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane SchÃ¶nlieb, and Christian Etmann. 2021. Conditional image generation with score-based diffusion models. arXiv preprint arXiv:2111.13606 (2021). [3] Jianfang Cao, Yiming Jia, Minmin Yan, and Xiaodong Tian. 2021. Superresolution reconstruction method for ancient murals based on the stable enhanced generative adversarial network. EURASIP Journal on Image and Video Processing 2021 (2021), 123. [4] Jianfang Cao, Zibang Zhang, Aidi Zhao, Hongyan Cui, and Qi Zhang. 2020. Ancient mural restoration based on modified generative adversarial network. Heritage Science 8 (2020), 114. [5] Chen Chen, Shuai Mu, Wanpeng Xiao, Zexiong Ye, Liesi Wu, and Qi Ju. 2019. Improving image captioning with conditional generative adversarial nets. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 81428150. [6] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and Juan Helen Zhou. 2023. Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2271022720. [8] Remi Denton, Sam Gross, and Rob Fergus. 2016. [7] Grigorios Chrysos, Jean Kossaifi, and Stefanos Zafeiriou. 2018. Robust conditional generative adversarial networks. arXiv preprint arXiv:1805.08657 (2018). Semi-supervised learning with context-conditional generative adversarial networks. arXiv preprint arXiv:1611.06430 (2016). [9] Xin Ding, Yongwei Wang, Zuheng Xu, William Welch, and Jane Wang. 2021. Ccgan: Continuous conditional generative adversarial networks for image generation. In International conference on learning representations. [10] Akshay Dudhane, Omkar Thawakar, Syed Waqas Zamir, Salman Khan, Fahad Shahbaz Khan, and Ming-Hsuan Yang. 2024. Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image Restoration. arXiv preprint arXiv:2404.02154 (2024). [11] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. 2023. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2316423173. [12] William Harvey, Saeid Naderiparizi, and Frank Wood. 2021. Conditional imarXiv preprint age generation by conditioning variational auto-encoders. arXiv:2102.12037 (2021). [13] Rongjie Huang, Max WY Lam, Jun Wang, Dan Su, Dong Yu, Yi Ren, and Zhou Zhao. 2022. Fastdiff: fast conditional diffusion model for high-quality speech synthesis. arXiv preprint arXiv:2204.09934 (2022). [14] Shaozong Huang and Lan Hong. 2023. Diffusion model for mural image inpainting. In 2023 IEEE 7th Information Technology and Mechatronics Engineering Conference (ITOEC), Vol. 7. IEEE, 886890. [15] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. 2017. Image-toimage translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 11251134. [16] Tero Karras, Samuli Laine, and Timo Aila. 2019. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 44014410. [17] Jiao Li, Huan Wang, Zhiqin Deng, Mingtao Pan, and Honghai Chen. 2021. Restoration of non-structural damaged murals in Shenzhen Baoan based on generator discriminator network. Heritage Science 9 (2021), 114. [18] Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, and Jiaya Jia. 2022. Mat: Maskaware transformer for large hole image inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1075810768. [19] Yawei Li, Kai Zhang, Jingyun Liang, Jiezhang Cao, Ce Liu, Rui Gong, Yulun Zhang, Hao Tang, Yun Liu, Denis Demandolx, et al. 2023. Lsdir: large scale dataset for image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 17751787. [20] Jianxin Lin, Yingce Xia, Tao Qin, Zhibo Chen, and Tie-Yan Liu. 2018. Conditional image-to-image translation. In Proceedings of the IEEE conference on computer vision and pattern recognition. 55245532. [21] Lin Liu, Shanxin Yuan, Jianzhuang Liu, Xin Guo, Youliang Yan, and Qi Tian. 2022. Siamtrans: zero-shot multi-frame image restoration with pre-trained siamese transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 17471755. [22] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. 2022. RePaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1146111471. [23] Ziwei Luo, Fredrik Gustafsson, Zheng Zhao, Jens SjÃ¶lund, and Thomas SchÃ¶n. 2024. Photo-Realistic Image Restoration in the Wild with Controlled VisionLanguage Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 66416651. [24] Shang Ma, Jianfang Cao, Zhaoxia Li, Zeyu Chen, and Xiaohui Hu. 2022. An improved algorithm for superresolution reconstruction of ancient murals with generative adversarial network based on asymmetric pyramid modules. Heritage Science 10, 1 (2022), 58. [25] Jonathan McCormick and Neil Jarman. 2005. Death of Mural. Journal of Material [26] Mehdi Mirza. 2014. Conditional generative adversarial nets. arXiv preprint Culture 10, 1 (2005), 4971. arXiv:1411.1784 (2014). [27] Haomiao Ni, Changhao Shi, Kai Li, Sharon Huang, and Martin Renqiang Min. 2023. Conditional image-to-video generation with latent flow diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1844418455. [28] Artidoro Pagnoni, Kevin Liu, and Shangyan Li. 2018. Conditional variational autoencoder for neural machine translation. arXiv preprint arXiv:1812.04405 (2018). [29] William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 41954205. [30] Jianjun Qu, Shixiong Cao, Guoshuai Li, Qinghe Niu, and Qi Feng. 2014. Conservation of natural and cultural heritage in Dunhuang, China. Gondwana Research 26, 3-4 (2014), 12161221. [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. [32] Hui Ren, Ke Sun, Fanhua Zhao, and Xian Zhu. 2024. Dunhuang murals image restoration method based on generative adversarial network. Heritage Science 12, 1 (2024), 39. [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. [34] Huiyang Shao, Qianqian Xu, Peisong Wen, Peifeng Gao, Zhiyong Yang, and Qingming Huang. 2023. Building Bridge Across the Time: Disruption and Restoration of Murals In the Wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025920269. [35] Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021. D2c: Diffusion-decoding models for few-shot conditional generation. Advances in Neural Information Processing Systems 34 (2021), 1253312548. [36] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems 28 (2015). [37] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. 2022. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on applications of computer vision. 21492159. [38] Wang, Hou, and Lyu. 2021. Virtual Restoration of Missing Paint Loss of Mural Based on Generative Adversarial Network. The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences 46 (2021), 807811. [39] Shiru Wang, Ion Sandu, Yulia Ivashko, Michal Krupa, Anna KrukowieckaBrzeczek, Tetiana Yevdokimova, Serhii Stavroyany, Oksana Kravchuk, and Andrei Victor Sandu. 2024. Methods for the preservation and restoration of Dunhuang wall paintings: foreign experience. International Journal of Conservation Science 15, 1 (2024), 731748. [40] Siyang Wang, Jinghao Zhang, Jie Huang, and Feng Zhao. 2024. Image-free Pre-training for Low-Level Vision. In Proceedings of the 32nd ACM International Conference on Multimedia. 88258834. [41] Xiaoguang Wang, Ningyuan Song, Lu Zhang, and Yanyu Jiang. 2018. Understanding subjects contained in Dunhuang mural images for deep semantic annotation. Journal of documentation 74, 2 (2018), 333353. [42] Qiang Wu, Baixue Zhu, Binbin Yong, Yongqiang Wei, Xuetao Jiang, Rui Zhou, and Qingguo Zhou. 2021. ClothGAN: generation of fashionable Dunhuang clothes using generative adversarial networks. Connection Science 33, 2 (2021), 341358. [43] Xiaogang Xu, Shu Kong, Tao Hu, Zhe Liu, and Hujun Bao. 2024. Boosting Image Restoration via Priors from Pre-trained Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 29002909. [44] Zishan Xu, Yuqing Yang, Qianzhen Fang, Wei Chen, Tingting Xu, Jueting Liu, and Zehua Wang. 2024. comprehensive dataset for digital restoration of dunhuang murals. Scientific Data 11, 1 (2024), 955. [45] Zishan Xu, Xiaofeng Zhang, Wei Chen, Jueting Liu, Tingting Xu, and Zehua Wang. 2024. MuralDiff: Diffusion for Ancient Murals Restoration on Large-Scale Pre-Training. IEEE Transactions on Emerging Topics in Computational Intelligence (2024). [46] Yao Yan, Rui Zhang, Hao He, Tong Lei, Xusheng Zhang, and Chao Jiang. 2024. Image Restoration Technology of Tang Dynasty Tomb Murals Using Adversarial Edge Learning. ACM Journal on Computing and Cultural Heritage 17, 3 (2024), 111. [47] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 38363847. [48] Ruicheng Zhang, Kanghui Tian, Zeyu Zhang, Qixiang Liu, and Zhi Jin. 2025. FDG-Diff: Frequency-Domain-Guided Diffusion Framework for Compressed Hazy Image Restoration. arXiv preprint arXiv:2501.12832 (2025). [49] Xiaobo Zhang, Donghai Zhai, Tianrui Li, Yuxin Zhou, and Yang Lin. 2023. Image inpainting based on deep learning: review. Information Fusion 90 (2023), 7494. [50] Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. 2024. Motion mamba: Efficient and long sequence motion generation. In European Conference on Computer Vision. Springer, 265282. [51] Zhifei Zhang, Yang Song, and Hairong Qi. 2018. Decoupled learning for conditional adversarial networks. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 700708. [52] Zijian Zhang, Zhou Zhao, Jun Yu, and Qi Tian. 2023. ShiftDDPMs: exploring conditional diffusion models by shifting diffusion trajectories. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 35523560. [53] Yuanzhi Zhu, Zhaohai Li, Tianwei Wang, Mengchao He, and Cong Yao. 2023. Conditional text image generation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1423514245."
        }
    ],
    "affiliations": [
        "AI Geeks",
        "Beijing Normal University",
        "Hebei Guoyan Science and Technology Center",
        "Southern University of Science and Technology",
        "The Australian National University",
        "Wuhan University",
        "Zhejiang Lab",
        "Zhejiang University"
    ]
}