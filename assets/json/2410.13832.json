{
    "paper_title": "VidPanos: Generative Panoramic Videos from Casual Panning Videos",
    "authors": [
        "Jingwei Ma",
        "Erika Lu",
        "Roni Paiss",
        "Shiran Zada",
        "Aleksander Holynski",
        "Tali Dekel",
        "Brian Curless",
        "Michael Rubinstein",
        "Forrester Cole"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Panoramic image stitching provides a unified, wide-angle view of a scene that extends beyond the camera's field of view. Stitching frames of a panning video into a panoramic photograph is a well-understood problem for stationary scenes, but when objects are moving, a still panorama cannot capture the scene. We present a method for synthesizing a panoramic video from a casually-captured panning video, as if the original video were captured with a wide-angle camera. We pose panorama synthesis as a space-time outpainting problem, where we aim to create a full panoramic video of the same length as the input video. Consistent completion of the space-time volume requires a powerful, realistic prior over video content and motion, for which we adapt generative video models. Existing generative models do not, however, immediately extend to panorama completion, as we show. We instead apply video generation as a component of our panorama synthesis system, and demonstrate how to exploit the strengths of the models while minimizing their limitations. Our system can create video panoramas for a range of in-the-wild scenes including people, vehicles, and flowing water, as well as stationary background features."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 2 ] . [ 2 2 3 8 3 1 . 0 1 4 2 : r VidPanos: Generative Panoramic Videos from Casual Panning Videos JINGWEI MA, University of Washington, USA and Google DeepMind, USA ERIKA LU, Google DeepMind, USA RONI PAISS, Google DeepMind, Israel SHIRAN ZADA, Google DeepMind, Israel ALEKSANDER HOLYNSKI, UC Berkeley, USA and Google DeepMind, USA TALI DEKEL, Weitzmann Institute of Science, Israel and Google DeepMind, Israel BRIAN CURLESS, University of Washington, USA and Google DeepMind, USA MICHAEL RUBINSTEIN, Google DeepMind, USA FORRESTER COLE, Google DeepMind, USA Fig. 1. Given casually-captured panning video, our method synthesizes coherent panoramic video, depicting the full dynamic scene. Our framework projects the input video on top of panoramic canvas and harnesses generative video model to synthesize realistic and consistent dynamic content in the unknown regions. Note that the kayakers paddle moves realistically, even when it is out of frame in the input video. Panoramic image stitching provides unified, wide-angle view of scene that extends beyond the cameras field of view. Stitching frames of panning video into panoramic photograph is well-understood problem for stationary scenes, but when objects are moving, still panorama cannot capture the scene. We present method for synthesizing panoramic video from casually-captured panning video, as if the original video were captured with wide-angle camera. We pose panorama synthesis as space-time outpainting problem, where we aim to create full panoramic video of the same length as the input video. Consistent completion of the space-time volume requires powerful, realistic prior over video content and motion, for which we adapt generative video models. Existing generative models do not, however, immediately extend to panorama completion, as we show. We instead apply video generation as component of our panorama synthesis system, and demonstrate how to exploit the strengths of the models while minimizing their limitations. Our system can create video panoramas for range of in-the-wild scenes including people, vehicles, and flowing water, as well as stationary background features. Project page at: https://vidpanos.github.io. CCS Concepts: Computing methodologies Computer vision. Additional Key Words and Phrases: Video panorama, video completion, space-time outpainting, generative video models."
        },
        {
            "title": "1\nWhen visiting a place, for example while traveling, we often want\nto capture the moment, to help us remember what it was like to\nbe in that place. Most of us capture a handful of photos with our\nsmartphones for this reason, but the sense of immersion is lost when\nplayed back as a sequence of stills â€“ the sense of the scale of the\nspace is missing. We can create a single view that extends beyond\nthe field-of-view of the camera by stitching multiple exposures into\na panoramic image. Many video cameras can automatically stitch\nthe frames of a panning video into a panoramic still image, so long\nas the scene is static. While we can capture a space with an image\npanorama, however, the experience of the dynamic scene in the\nmoment, filled with moving people, cars, trees, water, etc., is lost.\nIn this paper, we propose to construct panoramic videos from\ncasually-captured panning video of general dynamic scenes, com-\npleting both the space and time spanned by the panoramic video\nvolume. We take as input a video that can include not just a single\npan, but multiple pans in one capture, e.g., panning from left to right\nand then back left again. In this more general multi-pan setting, we\nhave both an opportunity to ground the video with knowledge of\nâ€œwhat happened laterâ€ as we pan back to a spatial location at a later",
            "content": "1 Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, and Forrester Cole time, but also the challenge of consistently interpolating across gaps in time to answer the question what happened in between? Our approach is to register the input video frames into single video volume, leaving space-time regions outside of the input unknown, then complete the unknown volume regions. This task is challenging since in typical capture, the number of unknown pixels in this volume outnumbers the known pixels, and we cannot assume that the unknown regions are stationary. To solve this problem we need powerful and realistic prior model of video, and method to apply this prior to complete the video volume consistently. We demonstrate results with both diffusion-based model (Lumiere [Bar-Tal et al. 2024]) and token-based model (Phenaki [Villegas et al. 2022]). The main technical problem we address is how to constrain and condition these video generation models, which have limited context windows, to complete panoramic video of arbitrary length and width. To tackle this challenge we apply coarse-to-fine synthesis and spatial aggregation techniques to realistically and consistently complete unknown regions of the video volume. In summary, our contributions include: The first system for creating video panoramas from general, panning input videos that include moving people and objects. Adaptations of the base algorithm for diffusionand token-based video generation models, and an analysis of their relative strengths and weaknesses. new dataset of video panoramas (cropped from 360-degree videos) with synthetic panning camera motion."
        },
        {
            "title": "2.2 Video Panoramas\nThe limitation of image stitching in handling moving objects has\nbeen explored in a series of work expanding image panoramas to\nthe video domain [Agarwala et al. 2005; Couture et al. 2011; Rav-\nAcha et al. 2007]. Commonly referred to as panoramic video textures,\nthese methods use a graph-cut formulation to solve the panorama\nblending problem across both spatially and temporally varying\nobservations, and are able to produce panoramas with motion that\ncan even loop seamlessly [Liao et al. 2015, 2013]. Still, these methods\nare restricted to modeling textural motion, e.g. shaking trees and\nflowing water, and cannot resolve inconsistencies resulting from",
            "content": "2 transient objects, e.g. person walking across the scene. This is largely due to the fact that these methods do not generate novel observations of the scenerather, their goal is to consolidate existing observations into consistent representation."
        },
        {
            "title": "2.3 Video Completion\nVideo completion focuses on completing missing pixels given ob-\nserved pixels as context. Many methods retrieve visual features\n(e.g. pixels, patches, segments, templates, proposals) from the ob-\nserved regions to fill in the missing regions [Gao et al. 2020; Hu et al.\n2020; Huang et al. 2016; Ilan and Shamir 2015; Li et al. 2022; Wexler\net al. 2007; Zhou et al. 2023]. However, these methods tend to fail\nor produce low-fidelity results when the observation is sparse or\nincomplete (e.g. heavily-masked video, object insertion).",
            "content": "With recent advances in generative models [Bar-Tal et al. 2024; Blattmann et al. 2023a,b; Guo et al. 2023; Ho et al. 2022; Villegas et al. 2022; Zhou et al. 2022], many methods adapt pretrained generative models for the task of video completion [Bar-Tal et al. 2024; Zhang et al. 2023] or train versatile generative models on suite of tasks, where video completion is one of them [Kondratyuk et al. 2024; Yu et al. 2023]. While the generative completion methods succeed on broader range of scenarios (e.g. foreground/background replacement, transient motion), they fail on the task of panoramic video completion, which requires going beyond the models temporal and spatial context window, interpolating temporally-distant observations and completing regions under panning video mask."
        },
        {
            "title": "3 METHOD\nThe input to the method is a video captured with a panning camera\nthat sweeps over a scene containing moving people and objects.\nThe output is a complete panoramic video of the same duration as\nthe input, but spatially wide enough to capture the entire sweep of\nthe input camera (e.g., Fig. 1). The output panoramic video should\nmatch the input video in the known input regions, and should look\nrealistic and consistent. Since the extent of the unknown content\nmay span a significant portion of the video in both time and space,\nwe harness the power of a generative video prior to synthesize the\nmissing regions.",
            "content": "While existing text-to-video models encode powerful priors about our dynamic world, pivotal challenge in utilizing them for our task is their limited spatio-temporal context window. We overcome this restriction by adopting coarse-to-fine approach in the temporal dimension and mask-respecting aggregation in the spatial dimension  (Fig. 2)  . To ensure consistent motion across time, we first temporally downsample the video to the models context window length and complete base panoramic video by aggregating the model predictions in sliding spatial windows (Sec. 3.3). We then progressively restore the temporal details by temporal upsampling, merging with the input video, and resynthesizing pixels outside the input regions (Sec. 3.4). Optionally, the model may be finetuned at test-time to further improve fidelity of the completed video (Sec. 3.5)."
        },
        {
            "title": "3.1 Preliminaries: Video Generation Models\nTo illustrate the generality of the method, we employ two video\ngeneration models in our experiments: Lumiere [Bar-Tal et al. 2024]",
            "content": "VidPanos: Generative Panoramic Videos from Casual Panning Videos Fig. 2. Temporal coarse-to-fine. The input video (a) is projected on to unified panoramic canvas using estimated camera parameters. The reprojected input video (b) is temporally downsampled with temporal prefiltering. base panoramic video is synthesized at the coarsest temporal scale (top), then gradually refined by temporal upsampling, merging, and resynthesis (c). Finally, spatial super-resolution pass is applied and the original input pixels are merged with the result to produce the output video (d). and Phenaki [Villegas et al. 2022]. Lumiere is space-time, pixeldiffusion model with two-stage cascade: base model that produces 80 frames of 128 128 pixels, followed by an upsampling stage to 1024 1024 pixels. Phenaki is token-based model with an encoder/decoder pair to translate between pixels and the latent token space, as well as two-stage cascade: first 11 frames of 16096 pixels, then 320 192 pixels."
        },
        {
            "title": "3.2 Video Registration and Setup\nIn the remaining sections we use the following notation for the\nintermediate variables:",
            "content": "xğ‘˜ mğ‘˜ Ë†yğ‘˜ ğ‘¢ğ‘ Ë†yğ‘˜ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ yğ‘˜ input video at temporal scale mask at temporal scale 2x temporal upsampled Ë†yğ‘˜ ğ‘¢ğ‘ merged with xğ‘˜ Ë†yğ‘˜ completed video at temporal scale To prepare video for processing we project the input video onto panoramic canvas to produce input frames x0 and corresponding mask frames of valid pixels m0 in the panorama coordinate system. For videos with pure panning camera motion (rotation only), we can use fast, standard homography solver [Hartley and Zisserman 2004]. When camera parallax is present, we solve for full 3D camera path using more expensive, robust SLAM system [Zhang et al. 2022]. We ignore the translation of the camera and compute the elevation ğœƒ and azimuth ğœ™ of each input pixels ray relative to the first frames camera direction, then project each ray to an equirectangular canvas. We further prepare temporally-downsampled versions of the input {xğ‘˜ }ğ¾ ğ‘˜=1 such that the coarsest input xğ¾ fits exactly in the models context window (80 frames for Lumiere, 11 for Phenaki). To avoid temporal aliasing we apply simple temporal prefiltering with box blur before subsampling from x0."
        },
        {
            "title": "3.3 Base Panoramic Video Completion\nThe first step is to complete a panoramic video at the coarsest\ntemporal resolution by spatial outpainting. The input video xğ¾ is in\ngeneral wider than the modelâ€™s native aspect ratio. We downscale xğ¾\nto match the modelâ€™s native height and use multiple, overlapping\nspatial windows to span the panorama width. The distributions\npredicted by the model in each window are averaged, then a new\nsample is drawn from the average. This approach applies to both\ndiffusion and token-based models, as explained below.",
            "content": "3.3.1 Diffusion. For diffusion model, averaging overlapping windows is form of MultiDiffusion [Bar-Tal et al. 2023]. We crop the projected panoramic canvas to each input window, then outpaint any regions outside the valid pixels using the mask-conditioned version of Lumiere. The ğœ‡ and Î£ predictions are averaged, then new sample is drawn using DDPM [Ho et al. 2020]. We found that the shape and motion of the panorama masks caused boundary artifacts with the original mask-conditioned model, so we finetuned the model on dataset of natural videos masked by synthetic panorama masks designed to mimic real m. 3.3.2 Token-based. For token-based model like Phenaki, spatial aggregation can be performed by averaging the predicted probability distributions over the tokens before sampling. Fig. 4 illustrates spatial aggregation with simplified case of two overlapping spatial windows. The red patch represents one of the tokens to be generated and lies within both the left (purple) and the right (orange) windows. Each window of masked tokens can be input to the transformer network to predict probability distribution for the red token. These distributions are then averaged and predicted token is drawn from the averaged distribution. Note that Phenaki employs causal masking during training, which means the model cannot complete earlier frames based on later frames at test time. To work around this issue, the base panorama 3 Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, and Forrester Cole Fig. 3. Upsampling and outpainting. The completed panorama from the previous level yğ‘˜+1 (a) is temporally-upsampled and composited with the current level input video xğ‘˜ to form partially-completed input Ë†yğ‘˜ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ for context and resynthesizes content outside the input mask to complete the next level panorama yğ‘˜ (c). In the time dimension, the model is applied in sliding-window fashion with half-window overlap. In the spatial dimension, multiple overlapping predictions are computed in parallel, then aggregated and sample is drawn from the average (d). ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ (b, input pixels shown highlighted). The model uses the full Ë†yğ‘˜ xğ¾ is completed in two passes, one forward and one backward (timereversed), and the valid pixels of the forward and backward passes are merged (please see supplemental material for details). Since later temporal upsampling steps always start from complete panorama, this approach is only necessary for xğ¾ ."
        },
        {
            "title": "3.4 Temporal Coarse-to-fine\nTo restore the original temporal dimension of the video, we progres-\nsively complete panoramic videos at upsampled temporal resolu-\ntions. For each level ğ‘˜ âˆˆ [ğ¾ âˆ’ 1, 0], we take the input video xğ‘˜ and\nthe completed, coarser-level panoramic video yğ‘˜+1 and combine\nthem to produce a complete panoramic video yğ‘˜ (Fig. 3). Intuitively,\nwe want the video generation model to provide temporal details for\nyğ‘˜ that are consistent with the input pixels in xğ‘˜ and the coarse-level\ncontext in yğ‘˜+1. To achieve this, we apply three steps: (1) temporal\nupsampling, (2) merging with the input pixels, and (3) resynthesis.\nğ‘¢ğ‘ that is frame-\nğ‘¢ğ‘ to form a merged Ë†yğ‘˜\nrate matched, and composite xğ‘˜ over Ë†yğ‘˜\nğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ .\nOptionally, we may align xğ‘˜ to Ë†yğ‘˜\nğ‘¢ğ‘ prior to compositing using\ngrid-warp-based optical flow [Szeliski 2010] and color histogram\nmatching. We found spatial and color alignment useful when using\nPhenaki, but unnecessary when using Lumiere. The resulting Ë†yğ‘˜\nğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’\nmatches the input video inside mğ‘˜ but lacks temporal details outside.\nWe adapt the resynthesis algorithm to the base model type, as",
            "content": "We first temporally upsample yğ‘˜+1 to create Ë†yğ‘˜ follows: 3.4.1 Diffusion. Resynthesis using diffusion model is controlled by the mask conditioning signal. full-frame mask is applied to the odd-numbered frames of Ë†yğ‘˜ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ , and the original mask mğ‘˜ to the even-numbered frames. That is, we constrain the diffusion sampling to maintain the temporally-upsampled, generated pixels at the oddnumbered frames, and allow resynthesis of the generated pixels only at the even-numbered frames. For videos with fast motion (e.g., the kayak paddle in Fig. 1), the temporally-upsampled pixels at odd-number frames may also need to be resynthesized. In this case we maintain the full-frame masks at odd frames for the first 1/8 of the sampling schedule, then switch to the input mask mğ‘˜ for 4 Fig. 4. Spatial aggregation of predicted distributions. To generate sample in the overlap (red), we linearly interpolate the two predicted probability distributions (purple, orange) and sample from the aggregated distribution (brown). With token-based method the distribution is discrete distribution over the vocabulary. With diffusion, the distribution is Gaussian distribution over pixel values, represented by ğœ‡ and Î£. the entire window to allow the model to synthesize new temporal details. Resynthesis is applied over the entire sequence using sliding temporal windows with an overlap of half the window length (40 frames for Lumiere). 3.4.2 Token-based. With token-based model, the pixels outside mğ‘˜ are resynthesized by masking and regenerating the corresponding tokens: ğ‘˜ ğ‘˜ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ = ğ‘’ğ‘›ğ‘ ( Ë†y ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ ) Ë†z ğ‘˜ ğ‘˜ ğ‘˜ = ğ‘¥ ğ‘“ (Ë†z ğ‘§ ), ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ is the set of tokens encoded from Ë†yğ‘˜ where Ë†zğ‘˜ ğ‘§ is the token-level mask, zğ‘˜ is the set of resynthesized tokens, ğ‘’ğ‘›ğ‘ is the token encoder, and ğ‘¥ ğ‘“ is the token transformer network. The full sequence yğ‘˜ = ğ‘‘ğ‘’ğ‘ (zğ‘˜ ) is constructed using sliding temporal windows with an overlap of half the window length (5 frames for Phenaki, as in [Villegas et al. 2022]). ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ , mğ‘˜ VidPanos: Generative Panoramic Videos from Casual Panning Videos Inference-Time Finetuning"
        },
        {
            "title": "4.1 Baseline Methods\nWe include four baseline methods for comparison (Fig. 5): a simple\nlinear interpolation baseline, two flow-based video inpainting al-\ngorithms, and a recent video generation model MAGVIT [Yu et al.\n2023] that demonstrated video panorama outpainting.",
            "content": "Linear interpolation baseline. For the linear baseline, the output color at pixel is linear interpolation of the closest before and after frames, or the nearest neighbor frame if only before or after exists. The result matches the input video exactly and matches any stationary elements in the scene. Moving objects exhibit the expected ghosting artifacts, with extreme failures in the case of non-stationary camera. Flow-based baselines. We include two methods for video inpainting using optical flow: ProPainter [Zhou et al. 2023] and E2FGVI[Li et al. 2022]. These methods are tasked with completing the region outside the valid pixels m. Both methods internally estimate flow, so the only inputs are x0 and m0. MAGVIT baseline. For the MAGVIT baseline, we apply repeated horizontal outpainting to extend the input video to outside the panorama canvas, then crop. Due to the temporal window of MAGVIT being limited to 16 frames, we subsample portion of each video to obtain single-direction pan of the full scene from left to right, and evaluate only on this subset of frames."
        },
        {
            "title": "4.2 Quantitative Evaluation\nBesides typical similarity metrics (PSNR, LPIPS [Zhang et al. 2018])\nand single-video FID [Arora and Lee 2021], we also compute optical\nflow EPE (endpoint error) to measure the consistency of the gener-\nated motions. We compute flow between consecutive frames of the\ngroundtruth and the output video and measure their L2 difference.\nWhen evaluating small motions at low image resolution, we empiri-\ncally found grid-based flow [Szeliski 2010] to produce more reliable\nsub-pixel alignments than network-based flow (e.g. RAFT [Teed",
            "content": "5 and Deng 2020]). For pixel-level metrics (PSNR, EPE), we separately evaluate static and dynamic regions (see supplemental for details)."
        },
        {
            "title": "4.3 Synthetic Panning Videos\nTo create synthetic panning videos we center crop an input video\ncaptured with a 360-degree video camera and add a moving crop\nwindow over 88 frames at 15fps (Figure 7). We then apply our system\nto complete a new video panorama. We curated 12 360-degree videos\nlicensed under Creative Commons and plan to include these videos\nalong with our results upon publication.",
            "content": "Synthetic videos allow us to evaluate the models output directly against ground-truth video panorama. Note that our goal is to generate plausible completed panorama, not recreate an input panorama. However, our model should recreate stationary scene elements as closely as possible. Fig. 7 shows results on two stationary camera videos (scuba, Bangkok) and two moving camera videos (skate, ski). Our method recovers static regions faithfully, and renders moving objects in plausible positions (diver in scuba, skiers in ski) even under challenging moving-camera settings. Certain scenarios prove too difficult to resolve: for example, the person in skate is observed very briefly but undergoes large motion. The model realistically renders the missing person in the first frame, but it struggles to complete the middle frame. We show qualitative comparisons with baselines in Fig. 5. Linear interpolation produces obvious ghosting artifacts for moving objects (the diver in scuba, skier in ski), and fails completely for camera motion (skate). The MAGVIT prediction is reasonable near the observed region, but quickly degrades the greater the distance from the input window. Quantitative results are shown in Table 1. For all results on Ours, we generate four samples and manually select the best. Our diffusionbased method performs the best across all metrics except the static split of PSNR and EPE. The baseline interpolation method performs best on the static splits, which is expected given that it perfectly reproduces stationary parts of the synthetic videos. However, the interpolation method performs poorly on EPE dynamic split (1.92 vs. 1.67 and 1.25 for our token-based and diffusion-based methods, respectively). The flow-based methods ProPainter [Zhou et al. 2023] and E2FGVI [Li et al. 2022] both assume small mask regions and large frame-toframe overlap. Nevertheless, ProPainter [Zhou et al. 2023] handles videos with stationary camera (e.g. scuba, Bangkok) surprisingly well, even producing higher PSNR than our Phenaki-based result. Subjective quality is lower, however, especially for scenes with moving camera (Fig. 5, skate, ski). The video models allow our method to produce more realistic motion than baselines in the inpainted regions, with static/dynamic EPE of 0.05/1.25 (Lumiere) and 0.07/1.67 (Phenaki) compared to 0.12/1.70 for ProPainter."
        },
        {
            "title": "4.4 Real Panning Videos\nTo further evaluate performance on real-world panning captures,\nwe captured a set of 10 panning videos using a phone video camera.\nThree of these videos contain panning in both directions, while",
            "content": "Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, and Forrester Cole Fig. 5. Comparison with baseline methods. From top to bottom: linear interpolation between pixels based on time produces sharp results for stationary regions, but does not interpolate motion. ProPainter [Zhou et al. 2023] and E2FGVI [Li et al. 2022] are flow-based methods that can produce realistic results in stationary regions (scuba, Bangkok), but fail for moving cameras (skate, ski) or moving objects away from the input window (divers on left in scuba). MAGVIT [Yu et al. 2023] is video-generation method but does not generate on common panorama canvas, so it loses information away from the input window. Our results use coarse-to-fine approach to build consistent panoramic video and better match the ground-truth. Bottom: ground truth video with input window marked in yellow. See supplemental material for video results. Table 1. Quantitative results on synthetic panning videos, computed on the inpainted regions (further split into static and dynamic regions for pixellevel metrics). MAGVIT* is evaluated on subset of frames (Sec. 4.3). Method PSNR LPIPS VFID EPE Interpolate ProPainter E2FGVI MAGVIT* naive Phenaki Ours (Phenaki) naive Lumiere Ours (Lumiere) sta 29.4 24.7 18.2 12.9 18.3 23.2 18.5 28.5 dyn 19.1 19.6 16.6 12.4 16.4 18.4 18.3 20.8 0.10 0.19 0.36 0.41 0.23 0.20 0.24 0.09 sta 0.04 0.12 0.63 1.17 0.41 0.07 0.41 0. dyn 1.92 1.70 2.03 1.92 1.94 1.67 1.94 1.25 0.09 0.21 0.47 0.57 0.26 0.19 0.18 0.05 the rest pan in single direction. Additionally, 6 of the 10 videos are filmed with vertical aspect ratio. These videos have roughly stationary cameras that we stabilize onto the panoramic canvas using homographies [Hartley and Zisserman 2004]. Fig. 8 shows representative examples of real videos. We do not have groundtruth panoramic videos to compute quantitative results against, but we observe that overall quality is similar to the synthetic panning results. We additionally process and compare the waterfall video from the original panoramic video textures work [Agarwala et al. 2005]  (Fig. 6)  . Fig. 6. Comparison with Panoramic Video Textures [Agarwala et al. 2005]. PVT uses graph-cut formulation to create looping panoramic video. Our method can create similar videos, but can also include non-stationary features like the person walking behind the waterfall (boxed)."
        },
        {
            "title": "4.5 Ablations\nBesides the alternative baselines, we analyze two main ablations\nof our diffusion-based method: 1) â€œnaiveâ€ Lumiere without mask\nfinetuning, and 2) our method with temporal MultiDiffusion instead\nof temporal coarse-to-fine. Ablation results are shown in Fig. 9 and\nFig. 10. Additional ablations for our token-based method can be\nfound in supplemental.",
            "content": "6 VidPanos: Generative Panoramic Videos from Casual Panning Videos Fig. 7. Results on synthetic panning videos. Left: Phenaki model results. Middle: Lumiere model results. Right: ground-truth panoramic video captured with wide-angle camera. Darkened boxed area is the input window shown to the model. Please see supplemental material for full video results. 7 Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, and Forrester Cole Fig. 8. Results on real videos. Left: representative input frames. Middle: frames projected to panorama canvas. Right: our result. Our method synthesizes realistic motions for an unseen person entering the frame (top), ocean waves (middle), and for scenery around moving camera (bottom). See supplemental material for videos. 8 VidPanos: Generative Panoramic Videos from Casual Panning Videos Naive Lumiere. We use the baseline mask-conditioned Lumiere model from [Bar-Tal et al. 2024] along with spatio-temporal MultiDiffusion to complete the panorama videos. Since the mask-conditioned model was trained on static masks, it produces significant artifacts on the dynamic-mask panorama videos, including visible seams around the mask boundary, color issues, and large blobs  (Fig. 9)  . Quantitative comparisons are shown in Table 1. Removing temporal coarse-to-fine. We ablate the temporal coarseto-fine component and replace it with temporal MultiDiffusion with window sizes of 80 frames and stride of 40 frames. comparison is shown in Fig. 10. As the camera pans from left to right, the temporal MultiDiffusion result suffers from drift and is unable to propagate the appearance of the person from the earlier frames to the later frames (orange box). Temporal coarse-to-fine generates more plausible continuation of the persons appearance and motion due to an initial round of coarse completion where the model sees the full extent of the scene within single temporal window."
        },
        {
            "title": "4.6 Computational Cost\nFor the Lumiere model, base-resolution inference on a Google Cloud\nTPU v5p-4 configuration for a 84x128x512 size video (39 Lumiere\nforward passes, 256 ddpm steps per pass) takes 300 minutes. The\nsuper-resolution stage runs diffusion at 8x the base resolution and\ntakes 48 minutes (7 Lumiere super-res forward passes, 32 ddim\nsteps per pass). The one-time fine-tuning of the original Lumiere\ncheckpoint on panoramic masks takes 30 hours on a batch size of\n128 for 35K steps. For Phenaki, inference for a 172Ã—320Ã—96 video\n(31 Phenaki forward runs) on a TPU v3-8 is 20.4 min; finetuning the\nPhenaki decoder ğ‘‘ğ‘’ğ‘ takes âˆ¼25 min.",
            "content": "Inference for both models can be greatly optimized by parallelizing the spatial windows, reducing the runtime to its 1 ."
        },
        {
            "title": "5 DISCUSSION AND LIMITATIONS\nThe method presented in this paper can complete a panoramic\nvideo from a casually-captured panning video. This task would be\nimpossible without a strong prior on realistic videos and motion,\nwhich has only recently become available in the form of generative\nmodels of video. While panoramic videos from stationary (rotation-\nonly), panning cameras have been shown in limited settings, such\nas Panoramic Video Textures [Agarwala et al. 2005], constructing\npanoramic videos containing large object motions (Fig. 1) or the\nentirely shifting visual field of a moving camera (Fig. 8, â€œdynibarâ€)\nare capabilities unlocked only with a generative video prior.\nGiven the nascent capabilities of generative video models, however,\nour method has some limitations that could lead to future work:",
            "content": "Limited context window. Current video generation models process limited number of frames simultaneously (80 for Lumiere, 11 for Phenaki). This limited context window necessitates temporal coarse-to-fine approach to allow the model to fill in the entire panoramic video consistently  (Fig. 10)  . limited amount of temporal coarsening is possible, however, without completely blurring out fast motion or dropping small objects. For our experiments we used up to 5 levels of temporal coarsening for Phenaki and 2 for Lumiere, for maximum video length of 172 or 160 frames. Fig. 9. Naive Lumiere vs. Ours. Left: Lumiere without panorama mask finetuning or temporal coarse-to-fine. Right: our result. Compare with our full method and ground-truth in Fig. 5. Fig. 10. Ablation of Temporal Coarse-to-Fine. Coarse-to-Fine synthesis (right) generates more consistent results over long videos than temporal MultiDiffusion (middle). With temporal MultiDiffusion, later generations can drift from the input pixels (orange box), while coarse-to-fine generates plausible continuation of the pedestrian. Input pixels shown darkened. Synthesis quality. While the quality of the generated video is often convincing, neither model we tested consistently generates photorealistic results. Limitations on synthesis quality are especially noticeable for close-up human faces (e.g., Fig. 10, \"palace\"). Our diffusion-based results could potentially preserve the high frequencies of the input videos better by finetuning the Lumiere super-resolution module with mask-conditioning. However, this modification would require adding new conditioning input and computationally-intensive retraining, and was not done as part of this work. We expect this limitation to be removed in the future. Latent video diffusion models. Several recent video generation models [Blattmann et al. 2023a; Guo et al. 2023] use latent diffusion model (LDM) as backbone to reduce runtime cost. Since the latent space is lower resolution than the image space, applying our method to LDM-based models would likely require careful handling of masking in the encoder and the diffusion model, possibly through combination of panoramic-mask finetuning and separate masking in the encoder and diffusion model. We leave this exploration for future work. Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, and Forrester Cole REFERENCES Aseem Agarwala, Ke Colin Zheng, Chris Pal, Maneesh Agrawala, Michael Cohen, Brian Curless, David Salesin, and Richard Szeliski. 2005. Panoramic video textures. In ACM SIGGRAPH 2005 Papers. 821827. Rajat Arora and Yong Jae Lee. 2021. SinGAN-GIF: Learning Generative Video Model from Single GIF. In 2021 IEEE Winter Conference on Applications of Computer Vision (WACV). 13091318. https://doi.org/10.1109/WACV48630.2021.00135 Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. 2024. Lumiere: space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945 (2024). Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. 2023. MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation. arXiv preprint arXiv:2302.08113 (2023). Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. 2023a. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023). Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. 2023b. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2256322575. M. Brown and D.G. Lowe. 2007. Automatic Panoramic Image Stitching using Invariant Features. IJCV (2007). Peter Burt and Edward Adelson. 1987. The Laplacian pyramid as compact image code. In Readings in computer vision. Elsevier, 671679. Vincent Couture, Michael Langer, and SÃ©bastien Roy. 2011. Panoramic stereo video textures. In 2011 International Conference on Computer Vision. IEEE, 12511258. Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf. 2020. Flow-edge guided video completion. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XII 16. Springer, 713729. Nuno Gracias, Mohammad Mahoor, Shahriar Negahdaripour, and Arthur Gleason. 2009. Fast image blending using watersheds and graph cuts. Image and Vision Computing 27, 5 (2009), 597607. Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. 2023. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725 (2023). R. I. Hartley and A. Zisserman. 2004. Multiple View Geometry in Computer Vision (second ed.). Cambridge University Press, ISBN: 0521540518. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. 2022. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022). Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. Yuan-Ting Hu, Heng Wang, Nicolas Ballas, Kristen Grauman, and Alexander Schwing. 2020. Proposal-based video completion. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVII 16. Springer, 3854. Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, and Johannes Kopf. 2016. Temporally coherent completion of dynamic video. ACM Transactions on Graphics (ToG) 35, 6 (2016), 111. Shachar Ilan and Ariel Shamir. 2015. Survey on Data-Driven Video Completion. In Computer Graphics Forum, Vol. 34. Wiley Online Library, 6085. Diederik P. Kingma and Jimmy Ba. 2017. Adam: Method for Stochastic Optimization. arXiv:1412.6980 [cs.LG] Dan Kondratyuk, Lijun Yu, Xiuye Gu, JosÃ© Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Josh Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, and Lu Jiang. 2024. VideoPoet: Large Language Model for Zero-Shot Video Generation. arXiv:2312. Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and Ming-Ming Cheng. 2022. Towards An End-to-End Framework for Flow-Guided Video Inpainting. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Jing Liao, Mark Finch, and Hugues Hoppe. 2015. Fast computation of seamless video loops. ACM Transactions on Graphics (TOG) 34, 6 (2015), 110. Zicheng Liao, Neel Joshi, and Hugues Hoppe. 2013. Automated video looping with progressive dynamism. ACM Transactions on Graphics (TOG) 32, 4 (2013), 110. David Lowe. 2004. Distinctive image features from scale-invariant keypoints. International journal of computer vision 60 (2004), 91110. Patrick PÃ©rez, Michel Gangnet, and Andrew Blake. 2023. Poisson image editing. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2. 577582. Thomas K. Porter and Tom Duff. 1984. Compositing digital images. Proceedings of the 11th annual conference on Computer graphics and interactive techniques (1984). https://api.semanticscholar.org/CorpusID:18663039 Alex Rav-Acha, Yael Pritch, Dani Lischinski, and Shmuel Peleg. 2007. Dynamosaicing: Mosaicing of dynamic scenes. IEEE Transactions on Pattern Analysis and Machine Intelligence 29, 10 (2007), 17891801. Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. arXiv:1804.04235 [cs.LG] Heung-Yeung Shum and Richard Szeliski. 1997. Panoramic image mosaics. Technical Report. Citeseer. Drew Steedly, Chris Pal, and Richard Szeliski. 2005. Efficiently registering video into panoramic mosaics. In Tenth IEEE International Conference on Computer Vision (ICCV05) Volume 1, Vol. 2. IEEE, 13001307. Richard Szeliski. 2010. Computer Vision: Algorithms and Applications (1st ed.). SpringerVerlag, Berlin, Heidelberg. Richard Szeliski et al. 2007. Image alignment and stitching: tutorial. Foundations and Trends in Computer Graphics and Vision 2, 1 (2007), 1104. Richard Szeliski and Heung-Yeung Shum. 2000. Creating Full View Panoramic Image Mosaics and Environment Maps. Computer Graphics (Proceedings of Siggraph 97) (04 2000). https://doi.org/10.1145/258734.258861 Zachary Teed and Jia Deng. 2020. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16. Springer, 402419. Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. 2022. Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions. In International Conference on Learning Representations. Yonatan Wexler, Eli Shechtman, and Michal Irani. 2007. Space-time completion of video. IEEE Transactions on pattern analysis and machine intelligence 29, 3 (2007), 463476. Yingen Xiong and Kari Pulli. 2010. Fast panorama stitching for high-quality panoramic images on mobile phones. IEEE Transactions on Consumer Electronics 56, 2 (2010), 298306. https://doi.org/10.1109/TCE.2010.5505931 Lijun Yu, Yong Cheng, Kihyuk Sohn, JosÃ© Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. 2023. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1045910469. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The Unreasonable Effectiveness of Deep Features as Perceptual Metric. In CVPR. Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William Freeman. 2022. Structure and motion from casual videos. In European Conference on Computer Vision. Springer, 2037. Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris Metaxas, and Licheng Yu. 2023. AVID: Any-Length Video Inpainting with Diffusion Model. arXiv preprint arXiv:2312.03816 (2023). Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. 2022. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018 (2022). Shangchen Zhou, Chongyi Li, Kelvin C.K Chan, and Chen Change Loy. 2023. ProPainter: Improving Propagation and Transformer for Video Inpainting. In Proceedings of IEEE International Conference on Computer Vision (ICCV). 10 VidPanos: Generative Panoramic Videos from Casual Panning Videos"
        },
        {
            "title": "Supplementary Material",
            "content": "S-1 METHOD DETAILS Variable xğ‘˜ mğ‘˜ Ë†yğ‘˜ Ë†yğ‘˜ ğ‘¢ğ‘ Nğ‘˜ ğ‘’ğ‘›ğ‘ ğ‘‘ğ‘’ğ‘ Description input video at temporal scale mask at temporal scale completed video at temporal scale 2x temporal upsampled Ë†yğ‘˜ number of frames at temporal scale VQ video encoder VQ video decoder S-1.1 Panorama Registration We use simple homography-based registration to stabilize our real videos. Thus the resulting videos may contain imperfections from stabilization that we can adjust for by applying coarse warp at various stages of our pipeline, which we describe in Sec. S-1.3. S-1.2 Panorama Completion Forward-backward pass (Phenaki only, base level). Since its original application is to extend given video, Phenaki employs causal masking: the attention layers of ğ‘’ğ‘›ğ‘ are masked such that later frames attend to earlier frames in the window, but earlier frames are blocked from attending to later frames. This masking causes artifacts when we synthesize regions that are not seen until later frames. To complete those regions  (Fig. 11)  , we run Phenaki both forward and backward over the coarsest level clip xğ¾ , taking the result of the forward pass in regions seen previously by the camera (blue) and combining it with the backward pass in the rest of the regions (orange). The result is an 11-frame completed panoramic video yğ¾ . Temporal box filtering (Phenaki only). At each temporal level ğ‘˜, we apply box blur on the full framerate input x0 (N0 total frames) before frame subsampling. The box filter size for temporal level ğ‘˜ is N0/Nğ‘˜ and the temporal stride is N0/Nğ‘˜ , both rounded to the nearest integer. We then subsample 1 frame from the center of each temporal window (Nğ‘˜ total) to obtain Nğ‘˜ frames for the respective temporal level. Spatial windows. We downsample the input to fit the video models height dimension, and span the width dimension with multiple overlapping windows. We use stride of 32 pixels for Lumiere and stride of 80 pixels for Phenaki. S-1.3 Spatial/Color Alignment In our temporal coarse-to-fine pipeline (Sec. 3.4), we complete base panoramic video followed by multiple temporally-upsampled panoramic videos. An important subtlety for the upsampled tokenbased video completion is that since Ë†yğ‘˜ ğ‘¢ğ‘ is the output of video generation, spatial details and color may not align exactly between ğ‘¢ğ‘ and xğ‘˜ (this mis-alignment is particularly worse for real videos Ë†yğ‘˜ with imperfect stabilization; see Sec. S-1.1). We found improved results with our token-based method by aligning xğ‘˜ to Ë†yğ‘˜ ğ‘¢ğ‘ before merging. We align xğ‘˜ to Ë†yğ‘˜ ğ‘¢ğ‘ spatially by computing coarse flow field, obtaining xğ‘˜ ğ‘¤ğ‘ğ‘Ÿğ‘ . We then perform an adjustment in color Fig. 11. Base level completion. Given reprojected input video (a), we run the video generation model at the coarsest level forwards and backwards in time (b), with the forward pass used if an earlier frame contains data at that pixel (blue) and the backwards pass where only the later frames contain data (orange). We combine the two passes to get the completed regions (c). ğ‘¢ğ‘ and xğ‘˜ space by computing Gaussian pyramid for Ë†yğ‘˜ ğ‘¤ğ‘ğ‘Ÿğ‘ and constructing color-aligned xğ‘˜ ğ‘ğ‘™ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘ using the 2 finest pyramid levels of xğ‘˜ ğ‘¢ğ‘ . The final merged video is computed as Ë†yğ‘˜ , Ë†yğ‘˜ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ = ğ‘œğ‘£ğ‘’ğ‘Ÿ (xğ‘˜ ğ‘¢ğ‘ ), where ğ‘œğ‘£ğ‘’ğ‘Ÿ () is the conventional over-compositing operation [Porter and Duff 1984]. ğ‘¤ğ‘ğ‘Ÿğ‘ and the coarsest ğ‘› 2 levels of Ë†yğ‘˜ ğ‘ğ‘™ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘ S-1.4 Finetuning Lumiere on Dynamic Masks We finetune the mask-conditioned Lumiere model on dataset of 5 million videos of dimension 128x128, and on masks generated by randomly taking 128x128 crops and augmenting our set of synthetic and real panning video masks. Initializing from the original Lumiere inpainting model weights, we finetune for 35k steps, using batch size of 128 and the Adafactor optimizer [Shazeer and Stern 2018] with ğ›½1 = 0.9 and ğ›½2 = 0.999. We use constant learning rate of 1 105. The finetuning continues to optimize for the diffusion denoising objective (squared error loss), with our dynamic masks. S-1.5 Phenaki Decoder Finetuning As described in Sec. 3.5, we finetune the decoder ğ‘‘ğ‘’ğ‘ğœƒ to restore video details lost during the tokenization process. We finetune one model on all temporal scales, using batch size of 24 and the Adam optimizer [Kingma and Ba 2017] with ğ›½1 = 0.9 and ğ›½2 = 0.9. We use an initial learning rate of 1 104 and decay to final learning rate of 1 106 over 5000 steps with cosine schedule. The finetuning objective is: = ğ‘¥ ğ· (ğ‘‘ğ‘’ğ‘ğœƒ (ğ‘’ğ‘›ğ‘ (ğ‘¥)) ğ‘¥) ğ‘š2 where ğ· is the set of 11-frame clips sampled from different temporal scales in each batch. S-2 BASELINE/EVALUATION DETAILS S-2.1 MAGVIT For the MAGVIT baseline, we adopt the same panorama generation procedure as the original work: given the visible 64x128 center region, the model outpaints 32 pixels on both sides to obtain 11 Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, and Forrester Cole Fig. 12. MAGVIT baseline in two configurations. MAGVIT operates on context window of 16 frames. Here we show with an example two different ways we select the 16-frame subset from the synthetic videos. One way is to take 1 in 3 frames from the first 48 frames (left) to span the full scene and minimize hallucination. We compare this version with linear interpolation and our results in Fig. 5 and Table 1. Since the fast panning motion from subsampling might be challenging for MAGVIT, we additionally show version where we run on the first 16 frames of the videos, which is closer to the panorama outpainting setting in the original work (right). The camera pans slowly with large frame-to-frame overlap, at the expense of observing portion of the scene. In both settings, MAGVIT struggles to synthesize consistent and realistic content at spatial locations far from the observed input window. 128x128 result. This procedure is repeated multiple times on both sides until the desired width is achieved. We resize the synthetic input panning video to height of 128 pixels, preserving the aspect ratio, and outpaint on both sides. We then apply stabilization to the outpainted panning video by cropping accordingly. Since the MAGVIT model operates on 16-frame videos, we subsample every 3rd frame from the first 48 frames of each synthetic video to obtain 16 input frames. This sampling allows the model to observe every spatial location of the input panorama, as the camera pans from the leftmost window to the rightmost window within the first 48 frames. We report numbers in Table 1 on the 16-frame subset. Since the large camera motion from subsampling may pose challenge for the MAGVIT model, we additionally show MAGVIT outpainting results on the first 16 frames of each synthetic video in our supplementary webpage. This reduces camera motion at the expense of the models only being able to observe small portion of the scene. As seen in Fig. 12, MAGVIT still struggles to outpaint consistent and realistic content at spatial locations far from the observed input window. S-2.2 Linear interpolation In Fig. 13, we show comparison between linear interpolation and our results over sequence of frames. Linear interpolation has obvious motion artifacts, for example, the divers on the left crossfade between two observations, while in our result the divers are consistent with the ground-truth video and have plausible motion trajectories. 12 VidPanos: Generative Panoramic Videos from Casual Panning Videos Fig. 13. Comparison with linear interpolation baseline (88-frame video, showing 1 in 8 frames, 11 frames total). Given an input video (a) with left-right-left pan, the linear interpolation result (b) have degenerate motion, e.g. divers on the left cross-fade in the synthesized regions, while our result (c) have smoothly interpolated motion and look consistent with the ground-truth (d). S-2.3 Disentangled static/dynamic evaluation For pixel-level metrics (i.e. PSNR, EPE), we split the inpainted regions into static and dynamic regions for disentangled evaluation. We determine static/dynamic regions by calculating optical flow and thresholding by flow magnitude of 0.2 pixels. For video clips with moving camera, most pixels are categorized as dynamic. S-3 PHENAKI ABLATIONS We analyze two main ablations of our token-based method: naive Phenaki with no temporal coarse-to-fine synthesis, and Phenaki with temporal coarse-to-fine but no alignment or finetuning. Ablation results on the same videos as Fig. 9 are shown in Fig. 14. Naive Phenaki. The most straightforward way to complete video volume using Phenaki is to apply the model in 11-frame, 160 96 sliding windows from the beginning to end of the video. As seen in Fig. 14a, this baseline lacks temporal consistency as static 13 inpainted regions differ from later observations of these regions (e.g. the landscape in the right half of the panorama, for scuba and ski). Furthermore, due to Phenakis causal training (see Sec. S-1.2), severe artifacts are exhibited in regions that are unseen at the start of the video (denoted in orange in Fig. 11), particularly for Bangkok and skate. Phenaki with coarse-to-fine. This ablation applies the coarse-tofine synthesis and merging, but does not apply flow alignment or decoder finetuning. While an improvement over naive Phenaki, the results still contain large artifacts (e.g. Fig. 14b, Bangkok and skate), as well as landscape inconsistencies (ski). Our full model avoids these artifacts and produces results consistent with the input video. Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, and Forrester Cole Fig. 14. Ablation of the method. Left: naive Phenaki result of completing the entire panorama without coarse-to-fine, spatial aggregation, or decoder finetuning. Right: our method without spatial aggregation or finetuning. Compare with our full method and ground-truth in Fig. 5. Yellow dotted lines visualize the boundary of the visible input region."
        }
    ],
    "affiliations": [
        "Google DeepMind, Israel",
        "Google DeepMind, USA",
        "UC Berkeley, USA",
        "University of Washington, USA",
        "Weitzmann Institute of Science, Israel"
    ]
}