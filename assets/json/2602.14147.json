{
    "paper_title": "LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models",
    "authors": [
        "Shufan Li",
        "Yuchen Zhu",
        "Jiuxiang Gu",
        "Kangning Liu",
        "Zhe Lin",
        "Yongxin Chen",
        "Molei Tao",
        "Aditya Grover",
        "Jason Kuen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 1 ] . [ 1 7 4 1 4 1 . 2 0 6 2 : r LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models Shufan Li1,2,,, Yuchen Zhu1,3,,, Jiuxiang Gu1, Kangning Liu1, Zhe Lin1 Yongxin Chen3, Molei Tao3, Aditya Grover2, Jason Kuen1 1Adobe 2UCLA 3Georgia Tech * Equal Contribution Work done primarily during internship at Adobe Research Figure 1. We introduce LaViDa-R1, multimodal diffusion language model with strong reasoning capabilities across diverse tasks. LaViDa-R1incorporates novel unified post-training that significantly improves upon the base model LaViDa-O (Li et al., 2025b) and SFT baseline on visual math reasoning, visual question answering, image editing, and object grounding tasks."
        },
        {
            "title": "Abstract",
            "content": "Diffusion language models (dLLMs) recently emerged as promising alternative to autoregressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDaR1, multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in unified manner. In particular, LaViDa-R1 is built with novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary . Correspondence to: Shufan Li <jacklishufan@cs.ucla.edu>, Kuen Zhu <yzhu738@gatech.edu>, Jason Yuchen <kuen@adobe.com>. Preprint. February 17, 2026. 1 likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1s strong performance on wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing. 1. Introduction Unified Multimodal Large Language Models (MLLMs) such as GPT-4o (OpenAI, 2024) have demonstrated strong utility on diverse scenarios. Traditionally, these models are built as auto-regressive (AR) models that generate tokens sequentially. Recently, dLLMs have emerged as promising alternative to auto-regressive models in many language (Nie et al., 2025; Ye et al., 2025a) and multimodal tasks (Yang et al., 2025b; Li et al., 2025b;c; Swerdlow et al., 2025). Instead of generating tokens in left-to-right order, dLLMs start with fully masked sequence and gradually unmask it through multiple diffusion steps, decoding multiple tokens in parallel. Compared with AR models, dLLMs offer many attractive properties such as faster inference speed (Wu et al., LaViDa-R1 2025b), bi-directional context (Li et al., 2025c; Nie et al., 2025), and unified generation paradigm for visual and text tokens (Li et al., 2025b; Yang et al., 2025b). To improve the performance of AR models, common technique is to incorporate reasoning process (Guo et al., 2025), in which the model generates text-reasoning traces before producing the final output. This approach has shown to be highly effective on complex tasks such as math reasoning (Shao et al., 2024) and coding (Li et al., 2025a). Recent work (Shen et al., 2025; Deng et al., 2025a) further extends the reasoning process to support multimodal understanding and generation tasks. The reasoning capability of model is typically acquired via post-supervised finetuning on Chain-of-Thought (CoT) data (Wei et al., 2022) and reinforcement learning (RL) (Guo et al., 2025; Yang et al., 2025a). While these approaches were first developed for AR models, recent work has explored applying them to build reasoning dLLMs and multimodal dLLMs (Yang et al., 2025b; Zhao et al., 2025a; Zhu et al., 2025). While AR models suffer from linear error accumulation due to causal masking, reasoning dLLMs leverage global visibility. By jointly modeling reasoning and results, they enable holistic refinement, allowing the emerging answer to adjust the reasoning trace and providing unified framework effective for both spatial visual modeling and complex logical reasoning. While the reasoning dLLM literature has seen some progress, several key challenges remain unaddressed. First, existing work focuses on limited set of tasks, such as mathematical reasoning, and often requires dataset-specific finetuning. Extending RL to build general-purpose reasoning model that supports diverse, multimodal tasks such as image editing and reason-intensive object grounding remains largely unexplored. Second, training dLLM with reinforcement learning is prone to collapse even in the presence of KL divergence regularizer. Furthermore, incorporating the KL-regularizer hinders models exploration during training, thereby degrading task performance. Third, for complex or difficult tasks, the model may fail to generate high-quality samples during training, leading to low-quality training signal or, in the worst case, zero training signal due to diminishing returns. Finally, unlike AR models, which can evaluate sequence likelihood exactly and efficiently, computing sequence likelihood for dLLMs is intractable and is typically estimated via the Monte Carlo (MC) method. This approach poses unique challenges for training stability as it produces missing and imbalanced token gradients. To address these gaps, we propose LaViDa-R1, recipe for building strong-performing multimodal dLLMs. Compared with existing methods, LaViDa-R1 introduces several key innovations. First, it introduces unified framework that encompasses diverse range of visual and language tasks, 2 including mathematical reasoning, visual question answering, reason-intensive grounding, and image editing. Second, it introduces novel post-training objective that seamlessly integrates SFT and the RL paradigm. By replacing the KL divergence term with SFT regularization, LaViDa-R1 allows the model to sufficiently explore beyond the distribution of pretrained base model while also preventing collapse. Third, to address the lack of training signal when no high-quality samples are generated for difficult prompts, we incorporate two guided rollout generation mechanisms to construct highquality samples. When ground-truth answers are available, we employ an answer-forcing mechanism that leverages dLLMs inpainting capabilities to artificially construct highquality reasoning traces on the fly. When the ground-truth answer is unavailable, we employ tree-search algorithm that tailors the generated distribution towards higher-quality outputs. Finally, we propose complementary likelihood estimator that improves upon existing MC methods by addressing the missing-signal and imbalanced-gradient problems discussed above. To validate the effectiveness of LaViDa-R1, we conducted extensive experiments covering wide range of tasks. Results show that LaViDa-R1 achieves strong reasoning performances on multiple benchmarks such as MathVerse, ChartQA, Lisa-Grounding, and ImgEdit. 2. Background and Related Works 2.1. Discrete diffusion models Early works on discrete diffusion models (Austin et al., 2021; Sahoo et al., 2024; Lou et al., 2023; Ou et al., 2024; Shi et al., 2024) first developed principled frameworks for training and sampling from masked generative models (MGMs) by formalizing the unmasking process of MGMs as discrete diffusion process. Later works, such as Mercury and LLaDA (Khanna et al., 2025; Nie et al., 2025; Ye et al., 2025a), scaled discrete diffusion models to large-scale language modeling, achieving performance comparable to autoregressive LLMs while offering benefits such as bidirectional context and faster inference. Recent works such as LaViDa-O and MMaDa (Li et al., 2025c;b; Yu et al., 2025b; Shi et al., 2025; Yang et al., 2025b) further expanded dLLMs to multimodal understanding and generation tasks. Formally, given sequence y0 of length L, and conditional prompt x, the forward masked diffusion process q(ytys) progressively mask tokens over the time interval [0, 1], with 1 0. At = 0, no tokens are masked. At = 1, the sequence y1 = [M, M, , M] consists entirely of special mask token M. When 0 < < 1, yt contains mixture of clean and masked tokens. dLLM policy model πθ is trained to model the reverse process p(ysyt, x). The LaViDa-R masked diffusion objective is defined as: LdLLM(θ) = Et,y0,yt,x (cid:21) log πθ(y0yt, x) (1) (cid:20) 1 where πθ(y0yt, x) is factorized to the product of per-token distribution (cid:81)L i=1 πθ(y0[i]yt, x) (Sahoo et al., 2024). At inference, given prompt x, we initialize with fully masked sequence y1 and iteratively apply the learned reverse process πθ to progressively unmask tokens until clean sequence y0 is obtained. 2.2. Reinforcement Learning Reinforcement learning (Schulman et al., 2017) can effectively improve the reasoning capability of LLMs. GRPO (Shao et al., 2024) is one of the best-performing RL methods, whose objective has the following form Jgrpo(θ) = xiπold (cid:104) 1 (cid:88) i=1 min (cid:16) πθ(yix) πold(yix) AGRPO , clip (cid:0) πθ(yix) πold(yix) , 1 ε, 1 + ε(cid:1)AGRPO (cid:17) β kl(yi) (cid:105) , AGRPO = ri mean(r1, . . . , rN ) std(r1, . . . , rN ) (2) where y1, . . . , yN is group of responses sampled with prompt x, kl(yi) is per-sample reverse KL estimator, ri is the per-sample reward and AGRPO is the per-sample advantage, which is the normalized reward. It has been shown that in pure on-policy setting, GRPO advantages can be simplified as J(θ) = (cid:104) 1 (cid:88) i=1 AGRPO log πθ(yix) β kl(yi) (cid:105) (3) Interestingly, it is shown that by changing β, and the definition of Ai (Shao et al., 2024), we can use the same form to represent many other objectives such as SFT and DPO (Rafailov et al., 2023). Inspired by this view, our work proposes practical unified post-training method that combines SFT, RL, and self-distillation loss to improve reasoning capabilities. RL for dLLMs. Multiple works have also explored applying GRPO-style RL to dLLMs (Zhao et al., 2025a; Gong et al., 2025; Wang et al., 2025a; Tang et al., 2025; Zhu et al., 2025; Wang et al., 2025e; Ou et al., 2025; Zheng et al., 2025a), mostly focused on language-only tasks with task-specific training. Few works have explored applying RL to multimodal tasks. Uni-GRPO (Yang et al., 2025b) first explored extending RL to improve math reasoning, image captioning and text-to-image generation simultaneously. Our work extends RL to broader set of tasks such as reasonintensive object grounding and image editing. We provide more thorough review to these literature in Appendix D. RL for Multimodal tasks. For AR VLMs and unified MLLMs, many works explored enhancing reasoning with RL (Wang et al., 2025b; Shen et al., 2025; Meng et al., 2025; Zhou et al., 2025; Yang et al., 2025c; Deng et al., 2025b; Huang et al., 2025; Wang et al., 2025c; Yuan et al., 2025), achieving successes on wide range of visual understanding and generation tasks. We also note that while RL is commonly associated with reasoning, it can also be applied to improve visual generation tasks without reasoning elements. Multiple works explored applying RL in image-output-only setup (e.g. Stable Diffusion) to improve text-to-image generation and image-editing quality (Li et al., 2025d; Geng et al., 2025; Wei et al., 2025; Liu et al., 2025a; Zheng et al., 2025b; Wu et al., 2025a; Luo et al., 2025b). Most existing work focuses on applying RL techniques in task-specific manner, with few exceptions exploring unified reasoning for unified multimodal models (Tian et al., 2025; Xin et al., 2025; Yang et al., 2025b; Cui et al., 2025). Our work focuses specifically on improving multimodal task performance by enhancing reasoning capabilities using RL. It is more closely aligned with the VLM and MLLM reasoning literature than with the general RL literature on visual generation. 2.3. Improve reasoning with non-online-RL methods Other lines of work address the challenge of LLM alignment beyond GRPO-style policy-gradient methods. Direct Preference Optimization (DPO) (Rafailov et al., 2023; Azar et al., 2024; Zhao et al., 2023) aligns LLMs using off-line paired preference data, and Online-DPO (Guo et al., 2024b) optimizes models based on preference pairs obtained from model-generated responses and an external reward model as judge. Self-play (Chen et al., 2024; Wu et al., 2024; Rosset et al., 2024; Swamy et al., 2024) formulate the task of LLM alignment as two-player game. Self-distillation-type methods (Yang et al., 2024; Amini et al., 2024) progressively improve the model by distilling optimized self-generated rollouts back into the model. Specifically, BOND (Sessa et al., 2024) iteratively distilled the Best-of-N model output distribution into the model policy by minimizing the KL divergence between them, which is equivalent to performing SFT on the best sequence yj among group of responses y1, . . . , yN from the same prompt x, with the following objective Jdistill(θ) = log πθ(yjx), = argmaxi ri (4) Our work creatively combines the best-of-N distillation objective with the standard RL objective. We also explored alternatives like Online-DPO. Further discussions can be 3 LaViDa-R1 found in Appendix A.2. 3. Method In this section, we introduce the training framework for LaViDa-R1, which comprises three main components: unified post-training policy-gradient objective, guided rolloutgeneration algorithms that efficiently sample high-quality outputs during training, and novel, stable, complementarymasking-based likelihood estimator. 3.1. Unified Post-training As is first noted in (Shao et al., 2024), many post-training objectives, including online GRPO (Shao et al., 2024) (without KL regularization), Online DPO (Guo et al., 2024b), SFT, and self-distillation (Sessa et al., 2024), can be written in the same form of policy gradient objectives: JUnified(θ) = 1 (cid:88) i=1 Ai log πθ(yixi) (5) The design choices that differentiate these objectives are the sources of (yi, xi) pairs and the per-sample weights Ai. For example, When yi πθ(xi) is sampled form the policy model , J(θ) is the online GRPO during training, Ai = AGRPO objective with zero KL regularization from Eq. 3. When yi πθ(xi), Ai = Adistill = 1 if = arg max(r1, . . . , rN ) and 0 otherwise, J(θ) is equivalent to the best-of-N distillation (Eq. 4). , with Adistill When (yi, xi) comes from offline training dataset, Ai = Asft = 1, J(θ) is the SFT loss. This observation has several important implications. First, we can easily combine training batches of these objectives by concatenating lists of (yi, xi) and corresponding Ai. This pipeline is concretely shown in Figure 2. At each training step, generic data engine provides pairs of prompts and responses (yi, xi), as well as corresponding advantage values Ai. These can be obtained either by loading from an offline training dataset or by online generation, followed by reward and advantage calculation such as Adistill or AGRPO . The policy model is then used to compute the log-likelihood of each sequence log πθ(yixi). Finally, we optimize the unified objective in Eq. 5. This design is illustrated in Fig 2. Second, for on-policy objectives where yi πθ(xi) are sampled from the policy model, we can efficiently combine different objectives by simply aggregating the advantage values from each method using weighted average, without the need to resample rollouts across different losses. 4 For example, we can simultaneously perform online GRPO and best-of-N self-distillation by adopting new advantage Aaggr for each sample with barely any additional computational overhead. +(1γ)AGRPO = γAdistill In our final design, we combined SFT, online GRPO and online self-distillation objectives, with γ = 0.5. We also explored other objectives that can be written in this form such as online DPO and SLiC (Rafailov et al., 2023; Zhao et al., 2023). Further details are provided in Appendix A.2. Intuitively, adding the SFT objective can serve as substitute for KL regularization. It allows the model to sufficiently explore the action space without being constrained by suboptimal reference model, while preventing collapse. Furthermore, from computational-efficiency perspective, removing the need for reference model significantly reduces the cost of RL training, since we no longer need to load it into GPU memory or host it on separate server. On the other hand, incorporating self-distillation objective amplifies the training signal from the best sample in the group, leading to stronger training signals. 3.2. Guided Rollout Generation Online RL with group-based advantage computation is known to suffer from vanishing training signal when all generated responses receive low rewards, resulting in zero advantage for all responses and rendering the RL process ineffective. To effectively address this notorious issue, we propose using guided generation to create high-quality rollout samples during training. We consider two types of guided generation algorithms, each with different operating scenarios. Answer-forcing is applied when we have access to the ground-truth answers to the training questions (e.g. math reasoning). When answers are unavailable, we resort to tree search, which is applicable when real-valued reward function is available. 3.2.1. ANSWER FORCING We leverage dLLMs bidirectional generation capabilities to construct high-quality reasoning traces when ground truth answers are available. When the policy model fails to generate high-quality outputs in group (i.e., no correct math solution or no high-IoU bounding boxes), we manually insert the ground truth answer token to the end of fully masked sequence and leverage dLLMs text-infilling capabilities to inpaint intermediate reasoning traces that lead towards the final answer. We name this guided generation approach Answer Forcing. An example is shown in Fig. 3. Formally, given prompt x, we first sample group responses yi, , yN πθ(yx). Each yi typically starts with text reasoning trace enclosed by < think >...</ think > tags followed by the final answer enclosed in < answer LaViDa-R1 Figure 2. Unified Post Training Framework of LaViDa-R1. At each training step, generic data engine provides prompts-response pairs of (yi, xi), and sample weights Ai, either by loading from dataset or by online generation. The policy model is then used to compute the log-likelihood of each sequence log πθ(yixi). Finally, we optimize the proposed unified policy gradient objective. the generated trajectories of those samples rather than from fully noised sequences. This gives new samples. This process is repeated times, yielding final effective group size of k. This process is illustrated in Figure 4. , . . . , yN , . . . , yN Concretely, given prompt x, we generate sequences y1 0, . . . , yN 0 through diffusion steps. We also keep track of intermediate diffusion states y1 ti where 1 = t0 > ti t1 > ..tT = 0 are discretized diffusion timesteps. Notably, y1 , . . . , yN t0 are fully masked sequences and y1 tT tT t0 are final generated responses, which may contain both image and text tokens. After obtaining rewards r1, , rN for each response, we find an index = argmax(r1, ...rN ) with the highest rewards and retrieve its early diffusion states ym ts , which is partially masked sequence. The selection of the timestep ts {t0, ..., tT }, is controlled by hyperparameter. We then proceed to generate new samples yN +1 ts as the initialization as opposed to 0 fully masked sequence. To generate these samples, we only need to perform diffusion steps. This process is repeated times until all samples are obtained. using ym ..y2N 0 3.3. Complementary-Masking Likelihood Estimator One essential challenge in applying policy gradient methods to dLLMs is the estimation of the data log probability log πθ(yx). Unlike AR models whose likelihood has an exact computable form, dLLMs likelihoods are estimated via the ELBO surrogate. The ELBO for the sequence log probability is expressed as the following formula, log πθ(yx) = Et,yt (cid:104) w(t) (cid:88) (cid:105) log πθ(y[k]yt, x) kM (yt) Figure 3. Answer-Forcing. We initialize partially masked sequence with ground truth answer injected at the end, and use the diffusion unmasking process to obtain the reasoning trace. >...</ answer > tags. The final answer can be either text or image tokens, depending on the tasks. If all of the extracted answers have low rewards (e.g., incorrect for math reasoning tasks), and ground truth answer is accessible, we can initialize new sequence yN +1 = . . . < answer > z</ answer > where is the special mask token. We then employ the dLLM πθ to progressively unmask these tokens and effectively generate synthetic reasoning trace conditioned on the ground-truth answer. This sample is then added to the group. Additional details of answer-forcing are included in Appendix A.4 3.2.2. TREE SEARCH For tasks that do not provide ground-truth answers (such as image editing), we leverage Tree Search to obtain highreward rollouts. Given base group size of , we first generate samples and compute rewards as usual. We then find the samples in each group with the highest rewards and generate new samples, starting from an early state in where w(t) is weighting function and (y) = {ky[k] = } is the set of masked indices. The expectation is typically computed via Monte Carlo (MC) Estimator. Existing works on dLLM RL mostly distinguish themselves from others through the choice of w(t) and how they sample yt in each MC sample. For example, d1 (Zhao et al., 2025a) samples one MC sample at = 1 (i.e. fully-masked sequence), and 5 LaViDa-R1 Figure 4. Tree Search. Given base group size , we first sample i.i.d samples and evaluate the rewards. We then select the samples with the highest rewards and generate new samples from an early diffusion state of the best sample. This process is repeated times. In this example, = 4. adopts the weighting w(t) = 1/t; UniGRPO (Yang et al., 2025b) samples one MC sample at Uniform([0, 1]) with w(t) = 1/t. In our design, we use two samples with timestep t1 Uniform([0, 1]) and t2 = 1 t. We sample yt1 q(yt1y0) using the discrete forward diffusion process, and set yt2[i] = y[i] if yt1[i] = and yt2[i] = if yt1 [i] = . This design, known as complementary masking, was first proposed in LaViDa (Li et al., 2025c) for pretraining. For example, if the sequence is is there is dog and yt1 is [M] is [M] dog, yt2 will be there [M] [M]. We adopt w(t) = 1 instead of w(t) = 1 from LaViDa, giving the following estimator log πθ(yx) = 1 2 (cid:88) (cid:88) j{1,2} kM (ytj ) log πθ(y[k]ytj , x) Our estimation recipe has several advantages. First, compare with i.i.d MC samples, it masks all tokens once, ensuring the estimate accounts for all tokens in the sequence. This prevents important tokens from being disregarded during training. Second, compare with d1, which can also get estimates for all tokens by always masking every token, it has smaller training-inference gap. Finally, compared with naively applying complementary masking with w(t) = 1 , using w(t) = 1 avoids imbalanced token weighting caused by drastically different masking ratios. When w(t) = 1 , suppose t1 = 0.9 and t2 = 0.1, we have w(t2) w(t1) = 9, indicating that tokens in sample yt2 is 9 more important than those in sample yt1 , which is highly unideal since which tokens are masked in which sample is randomly determined. 4. Experiments 4.1. Setup We select LaViDa-O as our base model because of its strong multimodal performances and pre-existing reasoning capabilities (Li et al., 2025b). LaViDa-R1 involves two training stages: the first stage is supervised finetuning (SFT) on reasoning data, the second stage is unified post-training on mix of SFT and RL data using mix SFT, RL, and self-distillation loss under our unified framework. The RL datasets consist of math reasoning, visual question answering, reason-intensive object grounding and image editing. We use correctness reward for math and QA problems, IoU rewards for object grounding, and the EditScore (Luo et al., 2025b) reward model for image editing. We defer further details on the dataset composition, training schedule, and hyperparameter to Appendix B. 4.2. Image Understanding Results We report results on wide range of visual understanding tasks and language-only tasks in Table 1. We report results on MathVista (Lu et al., 2023) and MathVerse (Zhang et al., 2024) for visual math reasoning, ChartQA, AI2D and MMMU-Pro (Masry et al., 2022; Kembhavi et al., 2016; Yue et al., 2025) for visual QA and GSM8K and Math500 (Cobbe et al., 2021; Lightman et al., 2023) for languageonly tasks. For all datasets, we report the accuracy metric. Notably, we observe that LaViDa-R1 show improvements across all tasks, with the biggest gain coming from the language-only GSM8K and Math500 datasets. We hypothesize that this is because the base models pretraining dataset is vision-centric, leading to poor language performance and leaving considerable room for improvement. We provide additional qualitative results in Appendix B.5. 4.3. Image Editing Results We evaluate image editing performance on the ImgEdit benchmark (Ye et al., 2025b), and report the benchmark scores in Table 2. These scores measure both visual quality and prompt compliance via GPT-4 judge model. We note that the base model LaViDa-O already included some reasoning data in its training pipeline, and has reported image LaViDa-R Table 1. Performance comparison across visual reasoning, VQA, and language-only benchmarks. *per-dataset finetuning results. RL checkpoint for MMada is not open-sourced. Original authors only reported limited set of results. Model MathVista MathVerse ChartQA AI2D MMMU-Pro GSM8K MATH-500 Visual Reasoning Visual QA Text Only LLaDA-8B-Instruct (Nie et al., 2025) + DiffuGRPO (Zhao et al., 2025a) Dream-7B(Ye et al., 2025a) LaViDa-L (Li et al., 2025c) Dimple (Yu et al., 2025b) MMaDa-8B-Base (Yang et al., 2025b) +CoT SFT (Yang et al., 2025b) +UniGRPO (Yang et al., 2025b) LaViDa-O (Li et al., 2025b) +SFT LaViDa-R1 44.8 42.3 27.1 33.7 56.9 57.6 60. Language-Only dLLMs Visual-Understanding-Only dLLMs 27.2 64.6 63.4 70.0 74.4 Unified-Understanding-and-Generation dLLMs 56.1 66.6 76.7 78.6 78.9 9.6 9.8 80.0 80.8 81.7 13.4 13.5 36.9 36.6 38.7 - 27.1 3.2 8.4 31.2 31.9 32.8 78.2 82.1 77.2 17.4 65.2 73.4 47.4 70.6 81.5 36.2 40.2 39.6 4.2 26.5 36.0 23.4 31.0 38.6 Table 2. Per-Category and overall scores on ImgEdit benchmark. Model GPT-4o (OpenAI, 2024) Qwen2.5VL+Flux (Wang et al., 2025f) FluxKontext dev (Labs et al., 2025) OmniGen2 (Wu et al., 2025c) UniWorld-V1 (Lin et al., 2025) BAGEL (Deng et al., 2025a) Step1X-Edit (Liu et al., 2025c) OmniGen (Xiao et al., 2025) UltraEdit (Zhao et al., 2024) AnyEdit (Yu et al., 2025a) InstructAny2Pix(Li et al., 2023) MagicBrush (Zhang et al., 2023) Instruct-Pix2Pix(Brooks et al., 2023) LaViDa-O (Li et al., 2025b) + Reasoning + SFT LaViDa-R1 Add 4.61 4.07 3.76 3.57 3.82 3.56 3.88 3.47 3.44 3.18 2.55 2.84 2.45 4.04 4.11 4.11 4.25 Adjust 4.33 3.79 3.45 3.06 3.64 3.31 3.14 3.04 2.81 2.95 1.83 1.58 1.83 3.62 3.67 3.80 3.90 Extract 2.90 2.04 2.15 1.77 2.27 1.70 1.76 1.71 2.13 1.88 2.10 1.51 1.44 2.01 2.04 2.21 2. Replace 4.35 4.13 3.98 3.74 3.47 3.30 3.40 2.94 2.96 2.47 2.54 1.97 2.01 4.39 4.40 4.46 4.52 Remove 3.66 3.89 2.94 3.20 3.24 2.62 2.41 2.43 1.45 2.23 1.17 1.58 1.50 3.98 4.05 3.90 4.06 Background 4.57 3.90 3.78 3.57 2.99 3.24 3.16 3.21 2.83 2.24 2.01 1.75 1.44 4.06 4.00 3.86 3.86 Style 4.93 4.84 4.38 4.81 4.21 4.49 4.63 4.19 3.76 2.85 3.51 2.38 3.55 4.82 4.75 4.76 4.87 Hybrid 3.96 3.04 2.96 2.52 2.96 2.38 2.64 2.24 1.91 1.56 1.42 1.62 1.20 2.94 3.10 3.09 3.10 Action 4.89 4.52 4.26 4.68 2.74 4.17 2.52 3.38 2.98 2.65 1.98 1.22 1.46 3.54 4.04 4.14 4. Overall 4.20 3.80 3.52 3.44 3.26 3.20 3.06 2.96 2.70 2.45 2.12 1.90 1.88 3.71 3.80 3.81 3.90 editing performance with reasoning. While SFT leads to negligible improvement (+0.01) , indicating performance saturation characteristic of supervised scaling, LaViDa-R1 achieves significant boost (+0.10). This underscores that our unified RL framework successfully drives exploration beyond the modes learned during supervision. We provide additional qualitative results in Appendix B.5. 4.4. Reason-Intensive Grounding We evaluate reason-intensive grounding on Lisa-Grounding dataset(Lai et al., 2024), and report results in Table 3. We report precision@0.5 (P@0.5) and the mean IoU (mIoU) of bounding boxes. While the base model LaViDa-O exhibits strong grounding performance on simple queries, it performs poorly on Lisa-Grounding which requires complex visual reasoning. Compared with this baseline, SFT improves the performance by +10.8 mIoU and unified posttraining further improves the performance by an additional +22.1 mIoU. We provide additional qualitative results in Appendix B.5. Table 3. Performance comparison on Lisa-Grounding Dataset. Model P@0.5 mIoUbox Specialist Models SegLLM (Wang et al., 2025d) LISA-7B (Lai et al., 2024) 61.3 49.4 General-purpose VLMs Qwen2.5-VL-7B (Bai et al., 2025b) Qwen3-VL-8B(Bai et al., 2025a) 32.0 62.4 Reinforcement Learning VLM-R1 (Shen et al., 2025) LaViDa-O (Li et al., 2025b) +SFT LaViDa-R1 63.1 29.2 40.3 66.7 55.2 50.6 28.7 56. 26.1 36.9 60.0 5. Ablation Studies To verify the effectiveness of LaViDa-R1, we conduct additional ablation studies to assess several design choices. Answer Forcing. We investigate the effectiveness of answer forcing and report results in Table 4. We explored answer forcing randomly with probabilities of 0%, 10%, 7 LaViDa-R Table 4. Ablation Studies of Answer-Forcing. *Collapsed Table 6. Ablation Studies of Likelyhood Estimator M.Vista Lisa-Gnd. Math500 #MC Masking Lisa-Grounding ImgEdit Inject 0% Inject 10% Inject 50% Inject 100% 57.8 58.9 58.0 4.1* 63.1 65.0 64.2 5.1* 36.2 38.0 35.4 4.2* 1 1 2 2 i.i.d Full i.i.d Compl. 61.9 59.2 62.1 65. 3.82 3.77 3.86 3.88 Table 5. Ablation Studies of Tree search Tree Search Steps Group Size ImgEdit N/A N/A N/A [0, 8] [0, 8, 16, 32] 16 32 64 16 2 16 3.85 3.84 3.84 3.90 3.87 50%, and 100%. Results demonstrate that 10% forcing has the best overall performance. high inject ratio leads to collapse because answer-forced samples always receive high correctness reward, even when their reasoning traces are ill-formed, thereby producing potentially misleading learning signals. This problem is particularly severe when answer forcing always occurs, since it implies that most other samples will likely have negative advantage due to centering. Tree Search. The hyperparameter that controls the tree search behavior is called restart timestep indices, which is list of integers specifying the branching steps. For example, given group size of 16 samples per prompt and 64-step generation pipeline, tree search of [0,8] means we first sample 16 outputs independently, each run for 64 steps. We then identify the trajectory corresponding to the sample with the highest reward and branch from its 8th step to generate 16 additional samples. These 16 new samples are initialized from the 8th step of the best-performing sample previously generated, underwriting 56 steps each. Results are shown in Tab. 5. Steps [0,8] is good choice. Steps [0, 8, 16, 32] yield almost identical performance, because starting from later diffusion step introduces less uncertainty and does not contribute much. Likelihood Estimator. We investigate the effectiveness of our simple likelihood estimation recipe and report results in Table 6. We explored four approaches. In the first setup (row 1), we randomly mask subset of tokens and compute the likelihood only over masked positions. This is equivalent to UniGRPO with 1 MC sample. In the second setup (row 2), we mask all tokens and thus compute the likelihood over all tokens, which is equivalent to the d1 setup. In the third setup, 2 i.i.d. MC mask samples were explored. Finally, we report the results of our proposed estimation recipe (row 4). The results show that our method achieves the best performance. Self-Distillation Loss. We experimented with varying γ,the weight of self-distillation loss described in Section 3.1 and report results in Table 7. The results show that combining two loss functions yields better performance. Intuitively, this loss assigns greater importance to the best-generated samples than standard GRPO. These results highlight the flexibility of the proposed unified paradigm. Table 7. Ablation Studies of Self-Distillation Loss γ Effective Objective ImgEdit γ = 0 γ = 0.5 γ = 1.0 On-Policy GRPO Mixed Self-Distillation 3.86 3.90 3.84 Unified Loss. We finally investigated the effectiveness of the proposed unified loss that combines multiple objectives. We plot the average reward per sample during training. The results are shown in Figure 5. We compare with the standard online GRPO with and without the KL regularizer. Results show that the proposed unified loss with SFT as regularization term is more stable and yields higher reward. We observe that GRPO diverges even with strong KL regularization β = 0.1 because the KL estimators only compute divergences on sampled tokens and are not suitable for highentropy image distributions. Specifically, for most samples, the negative-log-likelihood is above 6 for visual generation and is less than 2 for text generation. The high NLL leads to high variance in the KL term. Figure 5. Ablation Studies of Unified Objective. 8 LaViDa-R1 6. Conclusion In this paper, we propose LaViDa-R1, novel training recipe to enhance reasoning ability in unified multimodal dLLMs. LaViDa-R1 introduces unified post-training paradigm via weighted policy-gradient objectives and simple yet effective likelihood estimator for stable training. LaViDa-R1 also adopts two new guided rollout generation algorithms to address the key issue of vanishing training signal in online RL. Through multi-task, multi-reward, cross-modality RL, LaViDa-R1 achieves superior performances across wide range of tasks, including text-only and multimodal reasoning, visual QA, image grounding, and editing."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work aimed at advancing the field of machine learning. It proposes unified multimodal model capable of generating text and images, thereby inheriting the full potential of LLMs and image generators. For example, it may be abused to create various harmful and offensive content. We strongly caution the community against such use cases."
        },
        {
            "title": "References",
            "content": "Amini, A., Vieira, T., Ash, E., and Cotterell, R. Variational best-of-n alignment. arXiv preprint arXiv:2407.06057, 2024. Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025a. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Bie, T., Cao, M., Chen, K., Du, L., Gong, M., Gong, Z., Gu, Y., Hu, J., Huang, Z., Lan, Z., et al. Llada2. 0: Scaling up diffusion language models to 100b. arXiv preprint arXiv:2512.15745, 2025. Brooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1839218402, 2023. Byeon, M., Park, B., Kim, H., Lee, S., Baek, Image-text pair https://github.com/kakaobrain/ Coyo-700m: W., and Kim, S. dataset. coyo-dataset, 2022. Chen, H., Razin, N., Narasimhan, K., and Chen, D. Retaining by doing: The role of on-policy data in mitigating forgetting. arXiv preprint arXiv:2510.18874, 2025a. Chen, J., Cai, Z., Chen, P., Chen, S., Ji, K., Wang, X., Yang, Y., and Wang, B. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation. arXiv preprint arXiv:2506.18095, 2025b. Chen, J., Xu, Z., Pan, X., Hu, Y., Qin, C., Goldstein, T., Huang, L., Zhou, T., Xie, S., Savarese, S., et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025c. Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Cui, Y., Chen, H., Deng, H., Huang, X., Li, X., Liu, J., Liu, Y., Luo, Z., Wang, J., Wang, W., et al. Emu3. 5: Native multimodal models are world learners. arXiv preprint arXiv:2510.26583, 2025. Deng, C., Zhu, D., Li, K., Gou, C., Li, F., Wang, Z., Zhong, S., Yu, W., Nie, X., Song, Z., et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025a. Deng, Y., Bansal, H., Yin, F., Peng, N., Wang, W., and Chang, K.-W. OpenVLThinker: Complex visionIn language reasoning via iterative SFT-RL cycles. The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025b. URL https:// openreview.net/forum?id=gfX1nqBKtu. 9 LaViDa-R1 Fang, R., Duan, C., Wang, K., Huang, L., Li, H., Yan, S., Tian, H., Zeng, X., Zhao, R., Dai, J., et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. Kazemzadeh, S., Ordonez, V., Matten, M., and Berg, T. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 787798, 2014. Fu, Y., Chen, T., Chai, J., Wang, X., Tu, S., Yin, G., Lin, W., Zhang, Q., Zhu, Y., and Zhao, D. Srft: single-stage method with supervised and reinforcement fine-tuning for reasoning. arXiv preprint arXiv:2506.19767, 2025. Gao, J., Xu, S., Ye, W., Liu, W., He, C., Fu, W., Mei, Z., Wang, G., and Wu, Y. On designing effective rl reward at training time for llm reasoning. arXiv preprint arXiv:2410.15115, 2024. Geng, Z., Wang, Y., Ma, Y., Li, C., Rao, Y., Gu, S., Zhong, Z., Lu, Q., Hu, H., Zhang, X., et al. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again. arXiv preprint arXiv:2507.22058, 2025. Gong, S., Zhang, R., Zheng, H., Gu, J., Jaitly, N., Kong, L., and Zhang, Y. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv preprint arXiv:2506.20639, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Guo, J., Zheng, T., Bai, Y., Li, B., Wang, Y., Zhu, K., Li, Y., Neubig, G., Chen, W., and Yue, X. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024a. Guo, S., Zhang, B., Liu, T., Liu, T., Khalman, M., Llinares, F., Rame, A., Mesnard, T., Zhao, Y., Piot, B., et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024b. Hu, M., Zheng, C., Zheng, H., Cham, T.-J., Wang, C., Yang, Z., Tao, D., and Suganthan, P. N. Unified discrete diffusion for simultaneous vision-language generation. 2022. URL https://arxiv.org/abs/2211.14842. Huang, W., Jia, B., Zhai, Z., Cao, S., Ye, Z., Zhao, F., Xu, Z., Hu, Y., and Lin, S. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Kembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., and Farhadi, A. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pp. 235251. Springer, 2016. Khanna, S., Kharbanda, S., Li, S., Varma, H., Wang, E., Birnbaum, S., Luo, Z., Miraoui, Y., Palrecha, A., Ermon, S., et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., and Levy, O. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:3665236663, 2023. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th symwith pagedattention. posium on operating systems principles, pp. 611626, 2023. Labs, B. F., Batifol, S., Blattmann, A., Boesel, F., Consul, S., Diagne, C., Dockhorn, T., English, J., English, Z., Esser, P., Kulal, S., Lacey, K., Levi, Y., Li, C., Lorenz, D., Muller, J., Podell, D., Rombach, R., Saini, H., Sauer, A., and Smith, L. Flux.1 kontext: Flow matching for incontext image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., and Jia, J. Lisa: Reasoning segmentation via large language In Proceedings of the IEEE/CVF Conference model. on Computer Vision and Pattern Recognition, pp. 9579 9589, 2024. Li, J., Li, G., Li, Y., and Jin, Z. Structured chain-ofthought prompting for code generation. ACM Transactions on Software Engineering and Methodology, 34(2): 123, 2025a. Li, S., Singh, H., and Grover, A. Instructany2pix: Flexible visual editing via multimodal instruction following. arXiv preprint arXiv:2312.06738, 2023. Jia, Y., Li, J., Yue, X., Li, B., Nie, P., Zou, K., and Chen, W. Visualwebinstruct: Scaling up multimodal instruction data through web search. arXiv preprint arXiv:2503.10582, 2025. Li, S., Gu, J., Liu, K., Lin, Z., Wei, Z., Grover, A., and Kuen, J. Lavida-o: Elastic masked diffusion models for unified multimodal understanding and generation. arXiv preprint arXiv:2509.19244, 2025b. 10 LaViDa-R1 Li, S., Kallidromitis, K., Bansal, H., Gokul, A., Kato, Y., Kozuka, K., Kuen, J., Lin, Z., Chang, K.-W., and Grover, A. Lavida: large diffusion language model for multimodal understanding. arXiv preprint arXiv:2505.16839, 2025c. Li, Z., Liu, Z., Zhang, Q., Lin, B., Wu, F., Yuan, S., Yan, Z., Ye, Y., Yu, W., Niu, Y., et al. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025d. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Lin, B., Li, Z., Cheng, X., Niu, Y., Ye, Y., He, X., Yuan, S., Yu, W., Wang, S., Ge, Y., et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Liu, J., Liu, G., Liang, J., Li, Y., Liu, J., Wang, X., Wan, P., Zhang, D., and Ouyang, W. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025a. Liu, M., Farina, G., and Ozdaglar, A. Uft: Unifying supervised and reinforcement fine-tuning. arXiv preprint arXiv:2505.16984, 2025b. Liu, S., Han, Y., Xing, P., Yin, F., Wang, R., Cheng, W., Liao, J., Wang, Y., Fu, H., Han, C., et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025c. Lou, A., Meng, C., and Ermon, S. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Luo, M., Tan, S., Wong, J., Shi, X., Tang, W. Y., Roongta, M., Cai, C., Luo, J., Li, E. L., Popa, R. A., and Stoica, I. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. Notion blog post, 2025a. Luo, X., Wang, J., Wu, C., Xiao, S., Jiang, X., Lian, D., Zhang, J., Liu, D., et al. Editscore: Unlocking online rl for image editing via high-fidelity reward modeling. arXiv preprint arXiv:2509.23909, 2025b. Lv, X., Zuo, Y., Sun, Y., Liu, H., Wei, Y., Chen, Z., He, L., Zhu, X., Zhang, K., Wang, B., et al. Towards unified view of large language model post-training. arXiv preprint arXiv:2509.04419, 2025. Ma, L., Liang, H., Qiang, M., Tang, L., Ma, X., Wong, Z. H., Niu, J., Shen, C., He, R., Li, Y., et al. Learning what reinforcement learning cant: Interleaved online fine-tuning for hardest questions. arXiv preprint arXiv:2506.07527, 2025. Masry, A., Long, D., Tan,"
        },
        {
            "title": "In Findings of",
            "content": "Joty, S., and J. Q., ChartQA: benchmark for question Hoque, E. answering about charts with visual and logical reathe Association for Comsoning. putational Linguistics: ACL 2022, pp. 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl. 177. URL https://aclanthology.org/2022. findings-acl.177. Meng, F., Du, L., Liu, Z., Zhou, Z., Lu, Q., Fu, D., Han, T., Shi, B., Wang, W., He, J., et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., Zhou, J., Lin, Y., Wen, J.-R., and Li, C. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. URL https://arxiv. org/abs/2410.21276. Ou, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li, C. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Ou, J., Han, J., Xu, M., Xu, S., Xie, J., Ermon, S., Wu, Y., and Li, C. Principled rl for diffusion llms emerges arXiv preprint from sequence-level perspective. arXiv:2512.03759, 2025. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PmLR, 2021. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023. Rasheed, H., Maaz, M., Shaji, S., Shaker, A., Khan, S., Cholakkal, H., Anwer, R. M., Xing, E., Yang, M.-H., and Khan, F. S. Glamm: Pixel grounding large multimodal LaViDa-R1 model. The IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Rosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadallah, A., and Xie, T. Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715, 2024. Sahoo, S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136 130184, 2024. Schuhmann, C. Laion-aesthetics. https://laion.ai/ blog/laion-aesthetics/, 2022. Accessed: 2024 - 03 - 06. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35: 2527825294, 2022. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Sessa, P. G., Dadashi, R., Hussenot, L., Ferret, J., Vieillard, N., Rame, A., Shariari, B., Perrin, S., Friesen, A., Cideron, G., et al. Bond: Aligning llms with best-of-n distillation. arXiv preprint arXiv:2407.14622, 2024. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shen, H., Liu, P., Li, J., Fang, C., Ma, Y., Liao, J., Shen, Q., Zhang, Z., Zhao, K., Zhang, Q., et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Shi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024. Shi, Q., Bai, J., Zhao, Z., Chai, W., Yu, K., Wu, J., Song, S., Tong, Y., Li, X., Li, X., et al. Muddit: Liberating generation beyond text-to-image with unified discrete diffusion model. arXiv preprint arXiv:2505.23606, 2025. Swamy, G., Dann, C., Kidambi, R., Wu, Z. S., and Agarwal, A. minimaximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056, 2024. Swerdlow, A., Prabhudesai, M., Gandhi, S., Pathak, D., and Fragkiadaki, K. Unified multimodal discrete diffusion. arXiv preprint arXiv:2503.20853, 2025. doi: 10.48550/ arXiv.2503.20853. Tang, X., Dolga, R., Yoon, S., and Bogunovic, I. wd1: Weighted policy optimization for reasoning in diffusion arXiv preprint arXiv:2507.08838, language models. 2025. Tian, R., Gao, M., Xu, M., Hu, J., Lu, J., Wu, Z., Yang, Y., and Dehghan, A. Unigen: Enhanced training & testtime strategies for unified multimodal understanding and generation. arXiv preprint arXiv:2505.14682, 2025. Wang, G., Schiff, Y., Turok, G., and Kuleshov, V. d2: Improved techniques for training reasoning diffusion language models. arXiv preprint arXiv:2509.21474, 2025a. Wang, H., Qu, C., Huang, Z., Chu, W., Lin, F., and Chen, W. Vl-rethinker: Incentivizing self-reflection of visionlanguage models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025b. Wang, X., Yang, Z., Feng, C., Lu, H., Li, L., Lin, C.-C., Lin, K., Huang, F., and Wang, L. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning selfimprovement. arXiv preprint arXiv:2504.07934, 2025c. Wang, X., Zhang, S., Li, S., Li, K., Kallidromitis, K., Kato, Y., Kozuka, K., and Darrell, T. Segllm: Multi-round reasoning segmentation with large language models. In The Thirteenth International Conference on Learning Representations, 2025d. Wang, Y., Yang, L., Li, B., Tian, Y., Shen, K., and Wang, M. Revolutionizing reinforcement learning framework for diffusion large language models. arXiv preprint arXiv:2509.06949, 2025e. Wang, Y., Yang, S., Zhao, B., Zhang, L., Liu, Q., Zhou, Y., and Xie, C. Gpt-image-edit-1.5 m: millionarXiv preprint scale, gpt-generated image dataset. arXiv:2507.21033, 2025f. Wang, Y., Zang, Y., Li, H., Jin, C., and Wang, J. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025g. Wei, H., Xu, B., Liu, H., Wu, C., Liu, J., Peng, Y., Wang, P., Liu, Z., He, J., Xietian, Y., et al. Skywork unipic 2.0: Building kontext model with online rl for unified multimodal model. arXiv preprint arXiv:2509.04548, 2025. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in 12 LaViDa-R1 neural information processing systems, 35:2482424837, 2022. Wu, C., Li, J., Zhou, J., Lin, J., Gao, K., Yan, K., Yin, S.-m., Bai, S., Xu, X., Chen, Y., et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Wu, C., Zhang, H., Xue, S., Liu, Z., Diao, S., Zhu, L., Luo, P., Han, S., and Xie, E. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025b. Wu, C., Zheng, P., Yan, R., Xiao, S., Luo, X., Wang, Y., Li, W., Jiang, X., Liu, Y., Zhou, J., et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025c. Wu, Y., Sun, Z., Yuan, H., Ji, K., Yang, Y., and Gu, Q. Self-play preference optimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024. Xiao, S., Wang, Y., Zhou, J., Yuan, H., Xing, X., Yan, R., Li, C., Wang, S., Huang, T., and Liu, Z. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1329413304, 2025. Xin, Y., Qin, Q., Luo, S., Zhu, K., Yan, J., Tai, Y., Lei, J., Cao, Y., Wang, K., Wang, Y., et al. Luminadimoo: An omni diffusion large language model for multimodal generation and understanding. arXiv preprint arXiv:2510.06308, 2025. Yan, J., Li, Y., Hu, Z., Wang, Z., Cui, G., Qu, X., Cheng, Y., and Zhang, Y. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Yang, L., Tian, Y., Li, B., Zhang, X., Shen, K., Tong, Y., and Wang, M. Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025b. Yang, S., Qu, T., Lai, X., Tian, Z., Peng, B., Liu, S., and Jia, J. Lisa++: An improved baseline for reasoning segmentation with large language model. arXiv preprint arXiv:2312.17240, 2023. Yang, T., Mei, J., Dai, H., Wen, Z., Cen, S., Schuurmans, D., Chi, Y., and Dai, B. Faster wind: Accelerating iterative best-of-n distillation for llm alignment. arXiv preprint arXiv:2410.20727, 2024. Yang, Y., He, X., Pan, H., Jiang, X., Deng, Y., Yang, X., Lu, H., Yin, D., Rao, F., Zhu, M., et al. R1-onevision: Advancing generalized multimodal reasoning through crossmodal formalization. arXiv preprint arXiv:2503.10615, 2025c. Ye, J., Xie, Z., Zheng, L., Gao, J., Wu, Z., Jiang, X., Li, Z., and Kong, L. Dream 7b, 2025a. URL https:// hkunlp.github.io/blog/2025/dream. Ye, Y., He, X., Li, Z., Lin, B., Yuan, S., Yan, Z., Hou, B., and Yuan, L. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025b. You, Z., Nie, S., Zhang, X., Hu, J., Zhou, J., Lu, Z., Wen, J.-R., and Li, C. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. Yu, Q., Chow, W., Yue, Z., Pan, K., Wu, Y., Wan, X., Li, J., Tang, S., Zhang, H., and Zhuang, Y. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2612526135, 2025a. Yu, R., Ma, X., and Wang, X. Dimple: Discrete diffusion multimodal large language model with parallel decoding. arXiv preprint arXiv:2505.16990, 2025b. Yuan, R., Xiao, C., Leng, S., Wang, J., Li, L., Xu, W., Chan, H. P., Zhao, D., Xu, T., Wei, Z., et al. Vl-cogito: Progressive curriculum reinforcement learning for advanced multimodal reasoning. arXiv preprint arXiv:2507.22607, 2025. Yue, X., Zheng, T., Ni, Y., Wang, Y., Zhang, K., Tong, S., Sun, Y., Yu, B., Zhang, G., Sun, H., et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1513415186, 2025. Zhang, K., Mo, L., Chen, W., Sun, H., and Su, Y. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.-W., Gao, P., et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. Zhao, H., Ma, X. S., Chen, L., Si, S., Wu, R., An, K., Yu, P., Zhang, M., Li, Q., and Chang, B. Ultraedit: Instructionbased fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. Zhao, S., Gupta, D., Zheng, Q., and Grover, A. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025a. 13 LaViDa-R1 Zhao, S., Liu, M., Huang, J., Liu, M., Wang, C., Liu, B., Tian, Y., Pang, G., Bell, S., Grover, A., et al. Inpaintingguided policy optimization for diffusion large language models. arXiv preprint arXiv:2509.10396, 2025b. Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025a. Zheng, K., Chen, H., Ye, H., Wang, H., Zhang, Q., Jiang, K., Su, H., Ermon, S., Zhu, J., and Liu, M.-Y. Diffusionnft: Online diffusion reinforcement with forward process. arXiv preprint arXiv:2509.16117, 2025b. Zhou, H., Li, X., Wang, R., Cheng, M., Zhou, T., and Hsieh, C.-J. R1-zeros aha moment in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. Zhu, Y., Guo, W., Choi, J., Molodyk, P., Yuan, B., Tao, M., and Chen, Y. Enhancing reasoning for diffusion llms via distribution matching policy optimization. arXiv preprint arXiv:2510.08233, 2025. A. Additional Technical Details A.1. Formulation of dLLM LaViDa-R1 In this section, we provide an overview of the standard formulation of dLLMs that are adopted by the literature (Ou et al., 2024; Shi et al., 2024; Sahoo et al., 2024; Lou et al., 2023; You et al., 2025; Li et al., 2025c;b). Notations are adapted from these references to be consistent with the ones used in the main paper to avoid potential confusion. Given sequence of discrete tokens y0 whose lengths is L, the forward discrete diffusion process q(ytys) gradually replace the original tokens in y0 to special mask token [M ] over the time interval [0, 1], with 1 0. At = 1, the sequence y1 is fully masked sequence. This forward process is formally defined as q(yi tyi s) = (cid:40) Cat(yt[i]; M), Cat(yt[i]; 1t 1s Ys[i] + ts 1s M), if ys[i] = [M ] if ys[i] = [M ], (6) where Cat() denotes discrete categorical distribution, and M, Ys[i] RV are probability vectors, and is the vocabulary size. Specifically, is one-hot vector corresponding to the special token [M ], and Ys[i] is one-hot vector corresponding to the token yi s. It has been shown that this forward process has the following marginal distribution: q(yt[i]y0[i]) = Cat(yt[i]; (1 t)Y0[i] + tM). MDLM (Sahoo et al., 2024) shows that the posterior of the reverse process p(ysyt, y0) has the following form: p(ys[i]yt[i], y0[i]) = (cid:40) Cat(ys[i]; Yt[i]), Cat(ys[i]; ts Y0[i] + M), if ys[i] = [M ] if ys[i] = [M ]. (7) (8) In inference, the clean sequence y0 is not known at start, so it is replaced with the prediction from policy network πθ(y0[i]yt), which gives the following empirical sampling process: pθ(ys[i]yt) = (cid:40) Cat(X Cat(X s; Yt[i]), s; ts πθ(y0[i]yt) + M), if ys[i] = [M ] if = [M ]. SFT training process. During SFT training, the following maximum likelihood estimation objective is adopted: LSFT = E(y,x)D[log πθ(yx)] (9) (10) where the likelihood is estimated via an MC estimator described in Sec. 3.3 of the main paper. We provide further discussion of estimating the likelihood in Appendix A.5. A.2. Unified paradigm for post-training In this section, we provide theoretical justification for unifying various post-training objectives into weighted policy gradient method. We will include the derivation for GRPO (Guo et al., 2025), Online DPO and its variants (Guo et al., 2024b; Zhao et al., 2023), SFT, Best-of-N self-distillation (Sessa et al., 2024). We will show that these objectives share the same gradients as certain policy-gradient objectives, with special weight. GRPO We consider the setup of fully-online GRPO with strength of KL regularization β = 0. In this case, the GRPO objective is simplified to Jgrpo(θ) = xiπold (cid:104) 1 (cid:88) i=1 min (cid:16) πθ(yix) πold(yix) AGRPO , clip (cid:0) πθ(yix) πold(yix) , 1 ε, 1 + ε(cid:1)AGRPO (cid:17)(cid:105) 15 In pure on-policy setup, πold(yix) = sg(πθ)(yix). Using the fact that πθ(yix) = sg(πθ)(yix) log πθ(yix), we can express the gradient of the GRPO objective as, LaViDa-R1 Jgrpo(θ) = xiπθ = xiπθ = xiπθ (cid:104) 1 (cid:104) 1 (cid:104) 1 N (cid:88) i=1 (cid:88) i=1 (cid:88) i= min (cid:16) sg(πθ(yix) sg(πθ(yix)"
        },
        {
            "title": "AGRPO\ni",
            "content": ", clip (cid:0) sg(πθ(yix) sg(πθ(yix) , 1 ε, 1 + ε(cid:1)AGRPO (cid:17) (cid:105) log πθ(yix) (cid:16) min"
        },
        {
            "title": "AGRPO\ni",
            "content": ", clip (cid:0)1, 1 ε, 1 + ε(cid:1)AGRPO (cid:17) log πθ(yix) (cid:105) (cid:105) log πθ(yix)"
        },
        {
            "title": "AGRPO",
            "content": "= xiπθ (cid:104) 1 (cid:88) i="
        },
        {
            "title": "AGRPO\ni",
            "content": "(cid:105) log πθ(yix) This is exactly policy gradient objectives with advantage Ai = AGRPO Online DPO For Online DPO (Guo et al., 2024b), and its variants, Online SiLC (Zhao et al., 2023), we generate the preference pairs from self-generated rollouts using the following protocol. After obtaining group of responses y1, . . . , yN , we re-order the responses so that the reward values r1, . . . , rN are monotonically decreasing as the response index grows. This is to say, we ensure r1 r2 rN . Then, we create preference pair data by matching y1 with yN , y2 with yN 1, etc, where we consider y1, y2 as the positive data and yN 1, yN as the negative data. Since the online DPO-type objectives are computed for pair of positive and negative data points, we illustrate their derivation using the notation y+, y. Note that the DPO objective is given as JDPO(θ) = Ey+,yπθ(x) (cid:104) log σ(cid:0)zθ(x, y+, y)(cid:1)(cid:105) , where σ is the sigmoid function that is defined as σ(x) = 1 1+ex , and it satsifies σ(x) = 1 σ(x), and zθ is define as zθ(x, y+, y) = β (cid:16) (cid:17) log πθ(y+ x) log πθ(y x) log πref (y+ x) + log πref (y x) . Therefore, computing the gradient of the DPO objective gives, JDPO(θ) = Ey+,yπθ(x) (cid:104) (cid:105) log σ(zθ) = Ey+,yπθ(x) = Ey+,yπθ(x) = Ey+,yπθ(x) = Ey+,yπθ(x) = Ey+,yπθ(x) = Ey+,yπθ(x) (cid:105) (cid:104) log σ(zθ) zθ zθ (cid:104)(cid:0)1 σ(zθ)(cid:1) zθ (cid:16) (cid:104)(cid:0)σ(zθ)(cid:1) β (cid:16) (cid:104) (cid:105) β σ(zθ) log πθ(y+ x) log πθ(y x) log πθ(y+ x) log πθ(y x) (cid:17)(cid:105) (cid:17)(cid:105) (cid:105) (cid:104) β σ(zθ) log πθ(y+x) + (β σ(zθ)) log πθ(yx) (cid:105) (cid:104) β σ(zθ) log πθ(y+x) + (β σ(zθ)) log πθ(yx) This is the same as weighted policy gradient objectives with advantage β σ(zθ) assigned to y+ and β σ(zθ) assigned to y. Online DPO-smooth We can also create smoothed version of Online DPO to alleviate label noise arising from inaccurate preference pairs, which stem from the inherent flaws of the reward models. Let ε be label smooth/noise parameter, indicating that with probability ε, the obtained preference is wrong. Then, after taking this into consideration, the correct gradient for Online DPO-smooth should be JDPO-smooth = Ey+,yπθ(x) (cid:104) β (cid:16)(cid:0)(1 ε) σ(zθ) εσ(zθ)(cid:1) log πθ(y+x) (cid:0)(1 ε) σ(zθ) εσ(zθ)(cid:1) log πθ(yx) (cid:17)(cid:105) 16 LaViDa-R This is the same as weighted policy gradient objectives with advantage (1 ε) σ(zθ) εσ(zθ) assigned to y+ and (cid:0)(1 ε) σ(zθ) εσ(zθ)(cid:1) assigned to y. Online SLiC Similarly, we can compute derive for SLiC (Zhao et al., 2023), JSLiC(θ) := Ey+,yπθ(x) (cid:104) max (cid:0)0, τ log πθ(y+ x) + log πθ(y x)(cid:1)(cid:105) . where τ is pre-defined threshold value. Define the margin violation indicator Iviol = 1(cid:2)τ log πθ(y+ x) + log πθ(y x) > 0(cid:3) . Therefore, the gradient of the objective JSLiC(θ) = Ey+,yπθ(x) = Ey+,yπθ(x) (cid:104) Iviol (cid:16) (cid:104) Iviol (cid:16) log πθ(y+ x) log πθ(y x) (cid:17)(cid:105) log πθ(y+ x) log πθ(y x) (cid:17)(cid:105) . This is the same as the gradient of the weighted policy gradient objective with advantage Iviol assigned to y+ and advantage Iviol assigned to SFT and Best-of-N self-distillation Its straightforward to see that, for SFT, the gradient is the same as policy gradient with constant advantage value 1 across all samples as the objectives are equivalent. For Best-of-N self-distillation (Sessa et al., 2024), the objective is given as Ldistill(θ) = KL(πBoNπθ) = EyiπBoN(x) (cid:104) log πBoN(yix) πθ(yix) (cid:105) Therefore, we can write its gradient as, Ldistill(θ) = EyiπBoN(x) (cid:104) log πBoN(yix) πθ(yix) (cid:105) = EyiπBoN(x) (cid:104) log πθ(yix) (cid:105) Since yi πBoN(x) represents that yi is the one with highest reward ri among r1, . . . , rN , the objective can be simplified to weighted policy gradient objective with advantage 1 assigned to the best sequence with highest reward yi and other wise 0. This is equivalent to performing SFT only on the self-generated best sequence. A.3. Answer Forcing In this section, we provide detailed account of the proposed answer-forcing algorithm. This technique is applicable to tasks with verifiable rewards, where the reward is computed by checking the generated answer against ground truth, such as the 0-1 correctness reward for math problem and IoU reward for object grounding. Given group size of , the naive implementation of answer-forcing described in Section 3.2.1 would first generate samples, evaluate the rewards, and then decide whether to generate an additional sample via injection. This is highly inefficient. Instead, we always generate + 1 samples in parallel for each group, with 1 sample containing ground-truth answer. However, depending on the rewards of the first samples, we optionally discard the extra sample from the loss computation when the remaining samples already include outputs with significantly effective training signal (e.g., high rewards as measured by accuracy or IoU). Concretely, during the online sampling process, we are given prompt x, ground truth answer and reward function R. When the desired group size is , we always generate + 1 samples with one extra answer-forced sample. Specifically, we initialize timestamps t1 = t2 = ...tN = 1 and initialize y1 = y2..yN using fully masked sequence ...M. We initialize yN +1 tN +1 = . . . <answer> </answer> with the answer section pre-filled. tN +1 is set to the value according to the mask ratio. For example, if the answer has 3 tokens and max generation length is 12, the timestep will be 0.75, since 9 12 of the tokens are masked. LaViDa-R1 yi 1 . . . ti 1 M . . . <answer> </answer> Algorithm 1 Answer Forcing with dLLMs Require: Prompt x, policy πθ, group size , ground-truth answer z, reward function R, threshold τ , injection ration β Ensure: Training group 1: Initialize forced sample: 2: for = 1 to do 3: 4: 5: end for 6: yN +1 7: tN +1 8: for = 1 to + 1 in parallel do 9: 10: end for 11: for = 1 to do 12: 13: end for 14: rmax maxi[N ] ri 15: if rmax < τ and Rand() < β then 16: 17: else 18: 19: end if 20: return Gloss {(x, yi)}N +1 i=2 ri R(x, yi, z) {(x, yi)}N yi πθ(yi 0 x, yi ti i=1 ) All of these sequences have equivalent length, which is set to 512 for math reasoning, 128 for object grounding and 256 for image editing based on the distribution of reasoning lengths in our SFT data. Ideally, we do not want the generated sequences to have exactly 512, 256, or 128 tokens to allow for some flexibility. In standard sampling, the model generates special [PAD] tokens at the end of the sentence if the reasoning length is less than the maximum sequence length. For the answer-forced sample, we adopted the Fill-in-the-Middle (FIM) design of LaViDa (Li et al., 2025c) and inserted random-length [S]...[S] sequences in the SFT data right before <answer>...</answer>, where [S] is special infilling token. This design allows flexible-length text infilling as the model can generate [S]...[S] if the reasoning length is less than the lengths of mask segment [M]...[M] in yN +1. ...yN +1 tN +1 , we perform diffusion sampling using the policy model πθ to obtain yi πθ(yi After obtaining y1 ) for t1 = 1, 2...N, + 1. Notably, the model forward computation of these samples can be performed in parallel. We then evaluate the rewards on these + 1 samples to obtain r1...rN +1. Finally, we check if the maximum rewards rmax among non-answer-forced samples exceeds threshold τ . In our setup, τ is set to 0.5 for both 0-1 math rewards and IoU rewards. We do not include any auxiliary rewards such as format rewards in this step. If rmax exceeds τ , we consider the original sample to have high-quality outputs and discard the answer-forced sample. Otherwise, we randomly replace one samples in y1...yN with yN +1. Since y1...yN are just i.i.d samples, we always discard y1 in our implementation. The answer-forcing algorithm is formally documented in Algorithm 1. 0 x, yi ti A.4. Tree Search In this section, we provide detailed descriptions of our tree-search algorithm. This technique is applicable to tasks without ground-truth answers but with real-valued reward function. It is not applicable to 0-1 rewards, since we cannot meaningfully identify best sample when all rewards are zero. Given prompt and reward function R, and base group size , the tree search process is controlled by the number of tree expansions and max diffusion steps , and restart timestep index s1...sk {0, 1..T }. In particular, the restart timestep index determines which point in the saved trajectories should serve as the branching point. s1 is always 0 since we always need to go through the full diffusion steps for the first samples y1...yN . The indices s1...sk {0, 1..T } directly correspond to diffusion timesteps t1...tN [0, 1] through the relation ti = 1 si . 18 Algorithm 2 Tree Search with dLLMs Require: Prompt x, policy πθ, reward function R, base group size , number of tree expansions k, diffusion steps , restart timestep index s1...sk {0, 1, ...T } with s1 = 0 LaViDa-R1 ti 1 si Ensure: Generated group of size 1: Initialize empty group 2: for = 1 to do 3: 4: end for 5: for = 1 to do yj t1 . . . 6: 7: end for 8: for = 1 to do 9: 10: for = + 1 to in parallel do 0 πθ(y0 x, yj yj ti) rj R(x, yj 0) Record unmasking order vj {1, . . . , }L end for Gather {rj, vj, yj}j=N iN +1,N iN +2,...N across processes {(x, yj 0)}j=N iN +1,N iN +2,...N arg maxj{1,2..N i} rj ym ti+1 for = + 1 to + do 0 [ℓ] if vm[ℓ] si+1, otherwise M, [ℓ] ym ℓ = 1, . . . , yj ti+1 ym ti+1 end for 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: end for 22: return We always have t1 = 1 s1 = 1 Fully masked sequence has samples after this point. Recover restart state Prepare next batch for restarting from partial state has samples. In particular, t1 = 1 always holds, indicating fully masked sequences. After the ith batch is generated, we will have samples. We find the index corresponding to the sample with the highest reward among all previously generated samples y1.yN i. In the (i + 1)th batch, we generate the (N + 1)th sample to the (N + ) th sample using ym ti+1 as the starting point. These samples will go through si+1 diffusion steps. This process is repeated until all samples are generated. In distributed training setup, samples in each batch are generated on multiple GPUs in parallel. Hence, we need to gather the generated trajectories and evaluated rewards. To reduce the cost of maintaining and synchronizing multiple trajectories in memory, we use more compact representation, leveraging the fact that once token is unmasked at diffusion step, it will not be modified in subsequent steps. Concretely, we store the final generated result yj 0 and an array vj {1, ..T }L which keeps track of when each token is unmasked, where is the sequence length. vj[i] = indicates the i-th token is unmasked at p-th diffusion step where 1 . To recover yj for arbitrary [0, 1], we can obtain the corresponding diffusion step index through the 0[i] if vj[i] and yj t[i] = yj relationship ti = 1 si t[i] = otherwise. This reduces the overhead from O(N LT ) to O(N L) at each batch. The tree search algorithm is formally described in Algorithm 2. and recover yj through yj A.5. Likelihood Estimator In this section, we discuss more technical details of likelihood estimation in dLLMs. Recall that we estimate the data log-likelihood using the ELBO, as follows. log πθ(yx) = Et,yt (cid:104) w(t) (cid:88) (cid:105) log πθ(y[k]yt, x) k{kyt[k]=M} We will list several ELBO estimation methods from the literature for comparison with ours. detailed visualization is presented in Figure 6. LaViDa-R1 Figure 6. Different Design Choices of Likelihood Estimators. a) d1 uses MC=1, w(t) = 1 = 1 and always sample = 1, equivalent to fully masked sequences. Hence, it can compute per-token likelihood at all positions. b) UniGPRO uses MC=1 and w(t) = 1 . It randomly masks subset of tokens and computes the likelihood with only masked tokens. c) We can improve UniGRPO by increasing the MC to 2 for broader token coverage. However, it leads to imbalanced gradients with w(t) = 1 . d) In vanilla complementary masking developed for pretraining, two MC samples are coupled to ensure 100% token coverage. However, the token imbalance issue persists because it uses w(t) = 1 . e) In our design, we set w(t) = 1, ensuring all-token coverage while balancing the importance of each token in uniform fashion. d1 (Zhao et al., 2025a) d1 estimates the likelihood by constantly sampling = 1 and y1 = [M, . . . , M], uses only one sample (MC = 1), with weight w(t) = 1 . This gives the estimation expression as, log πd1 θ (yx) = (cid:88) k=1 log πθ(y[k]y1, x) UniGRPO (Yang et al., 2025b) UniGRPO instead sample mask ratio from [0, 1] uniformly and corresponding binary mask associated with yt based on such mask ratio (MC = 1). It adopts the weight w(t) = 1 . This means that it computes the data loglikelihood with, log πunigrpo θ (yx) = 1 (cid:88) log πθ(y[k]yt, x) k{kyt[k]=M} We could improve the design by increasing the Monte Carlo sample size to 2 (MC=2) by sampling another masked sample with different mask rate, chosen also uniformly at random. log πunigrpo,2 θ (yx) = 1 2 (cid:104) 1 t1 (cid:88) log πθ(y[k]yt1, x) + k{kyt1 [k]=M} 1 (cid:88) (cid:105) log πθ(y[k]yt2, x) k{kyt2 [k]=M} Our recipe We adopt simple, effective approach for estimating the log probability using 2 samples with complementary masking (MC=2). We sample yt1 q(yt1 y0) using the discrete forward diffusion process, and set yt2[i] = (cid:40) y[i], M, if yt1 [i] = M, if yt1 [i] = M. (11) Finally, we compute the likelihood via 20 LaViDa-R log πours θ (yx) = 1 2 (cid:104) (cid:88) log πθ(y[k]yt1, x) + (cid:88) (cid:105) log πθ(y[k]yt2 , x) k{kyt1 [k]=M} k{kyt2 [k]=M} This approach differs from vanilla complementary masking used by few other works (Li et al., 2025c; Zhu et al., 2025; Bie et al., 2025) in that we adopt w(t) = 1 as opposed to w(t) = 1 . While this seems like small modification, it has profound impact on model performance, as confirmed by the ablation study in B.3. We visualize these likelihood estimators in Figure 6. B. Additional Experiment Details and Results B.1. Setup Training Dataset Our training consists of two stages. Stage 1 consists of only SFT objective. Stage 2 incorporates both online sampling and offline datasets in unified fashion. Stage 1 datasets can be generally categorized into two categories. The first category is subset of LaViDa-Os pretraining data, which includes: A: Text-to-Image Pairs. We source data from LAION-2B (Schuhmann et al., 2022), COYO-700M (Byeon et al., 2022), BLIP3o-60k (Chen et al., 2025c), ShareGPT4o-Image (Chen et al., 2025b). Each dataset is heavily filtered to remove NSFW prompts, low CLIP scores (Radford et al., 2021), low aesthetic scores (Schuhmann, 2022), and low-resolution images following LaViDa-Os pipeline. We include all data from BLIP3o-60k and ShareGPT4o-Image, and select highest-quality samples from LAION and COYO based on CLIP scores and aesthetic scores. This part consists of 20M images. B: Image-level Understanding Data. We include MAmmoth-VL (Guo et al., 2024a), and VisualWebInstruct (Jia et al., 2025). C: Region-level Understanding Data. We include GranD (Rasheed et al., 2024) and RefCOCO (Kazemzadeh et al., 2014). D: Image Editing Data. We include ShareGPT4o-Image (Chen et al., 2025b), GPT-Edit-1.5M (Wang et al., 2025f), and the image editing subset of UniWorld-V1 (Hu et al., 2022). The second category includes newly incorporated reasoning data not used in LaViDa-Os original training recipe. It includes E: Visual Understanding with Reasoning. We include Vision-R1-Cold(Huang et al., 2025), which consists of visual math problems and VQA problems. F: Pure Language-based Reasoning. We include DeepScalar (Luo et al., 2025a). G: Image Editing with Reasoning. We include GoT (Fang et al., 2025) data. Additionally, we run the data pipeline of GoT on GPT-Edit-1.5M (Wang et al., 2025f) to further expand the data. H: Reason-intensive Grounding. We include ReasonSeg (Lai et al., 2024) and Lisa-CoT (Yang et al., 2023). Given the substantial imbalance between the two categories, we design dataloader to prioritize the acquisition of reasoning capabilities. The dataset of the two categories is sampled with 3:7 ratio. Stage 2 training includes all datasets from Stage 1. It also incorporates additional RL datasets, including I: Visual Understanding with Reasoning. We include Vision-R1-Cold and Vision-R1-RL (Huang et al., 2025), which consists of visual math problems and VQA problems, and ViRL-39k (Wang et al., 2025b) LaViDa-R1 J: Language Based Reasoning. We include DeepScalar (Luo et al., 2025a), GSM8K (Cobbe et al., 2021), MATH (Lightman et al., 2023). K: Image Editing with Reasoning. We include input images and prompts from EditScore-RL (Luo et al., 2025b). L: Reason-intensive Grounding. We include ReasonSeg (Lai et al., 2024), Lisa-CoT (Yang et al., 2023), RefCOCO (Kazemzadeh et al., 2014). While some datasets such as DeepScalar (Luo et al., 2025a) appear both in the SFT data and RL data, they are incorporated in different manners. In RL data, the reasoning traces provided in these datasets are not used. Only the prompts and corresponding answers are used. Reward Functions For math problems and question answering, we use 0-1 correctness reward. For object grounding, we use IoU reward. For image editing, we use VLM-based reward model, Editscore(Luo et al., 2025b). Since Editscore requires considerable memory, it is hosted on separate server to the main training processes. In addition to these key rewards, we also incorporate auxiliary rewards such as tag-formatting rewards and repetition penalty, following existing work (Gao et al., 2024; Zhao et al., 2025a). We note that the formatting reward in existing works like d1 is not robust as it merely counts the <think> and <answer> tokens. We replace it with more robust formatting reward based on regular expressions. Training Hyperparameter We report the training hyperparameters, including the learning rate, number of training steps, optimizer setup, and image resolution for understanding and generation tasks in Table 8 for reference. The training is conducted of 64 GPUs. The main experiment is conducted on H100s. Some ablation studies use A100s. The total training time takes 5 days for Stage 1 and 3 days for Stage 2. We observe considerable bottleneck in reward evaluation. In particular, with global batch size of 256 images, the edit score evaluation takes 70-140 seconds. We will investigate how to further optimize this infrastructure bottleneck in future work by improving the network infrastructure and scaling reward servers. Table 8. Training configurations of LaViDa-R1. We report the relevant hyperparameters for training, including the learning rate, number of training steps, optimizer setup, image resolution for understanding and generation tasks. Learning Rate Steps β1 β2 optimizer Stage 1 5 106 100k 0.99 0.999 AdamW Stage 2 5 107 5k 0.99 0.999 AdamW Dataset Used Loaded Parameters Trainable Parameters Und. resolution Gen. resolution Loss A,B,C,D,E,F,G,H 10.4B 10.4B 384 {(1, 3), (2, 2)} 1024 SFT A,B,C,D,E,F,G,H,I,J,K,L 10.4B 10.4B 384 {(1, 3), (2, 2)} 1024 Unifed Evaluation Protocol. We use max sequence length of 128 for object grounding, 256 for image editing and 512 for all other tasks to match the training setup. All other sampling parameters remain unchanged following LaViDa-Os inference protocol. B.2. Additional Ablation Studies In this section, we report additional ablation study results to verify the design of LaViDa-R1. B.3. Timestep Weighting In Table 6, we report results of varying the timestep weighting discussed in Section A.5. Specifically, we compare results of complementary masking with = 1 and = 1, results show that = 1 works better. Specifically, we observe that 22 LaViDa-R1 Table 9. Ablation Studies of Per-token Likelihood Weighting Weighting Lisa-Grounding ImgEdit LaViDa-O + SFT w(t) = 1 w(t) = 1 40.3 64.3 65.0 3.81 3.82 3.90 Figure 7. Ablation Studies of Combining Different Losses in the Unified Framework = 1 significantly degrades performance on image-editing tasks. This may be attributed to the large number of tokens in visual generation tasks, which exacerbates the imbalance in per-token loss. In particular, image editing tasks produce 256 text tokens and 4096 image tokens per sample, significantly more than understanding-only tasks, which generate only 512 tokens. B.4. Alternative Losses in the Unified Framework B.5. Qualitative Results In Figure 7, we visualize the reward trajectories of combining RL loss with other post-training methods beyond best-of-N distillation discussed in Section A.2. Specifically, we start with the GRPO+SFT baseline and explore incorporating additional losses, including online DPO, online DPO-smooth, and online SLiC loss. Notably, all these can be achieved by simply modifying the weight Ai in Equation 5 without considerably changing the training pipeline. Results show that adding other losses is worse than the GRPO+SFT baseline, while incorporating Best-of-N distillation improves upon the baseline. Although most auxiliary losses did not improve performance, this experiment highlighted the flexibility of our proposed framework. We hope future work will further explore loss-weighting recipes beyond group advantages in GRPO. To further demonstrate the effectiveness of LaViDa-R1, we include qualitative results of model outputs on object grounding tasks in Figure 8, image editing tasks in Figure 9 and math reasoning tasks in Figure 10. These results demonstrated the strong reasoning capabilities of LaViDa-R1 on diverse multimodal tasks. C. Limitation Despite the strong results of LaViDa-R1, it has several key limitations. First, while LaViDa-R1 improves upon the base model LaViDa-O. There still exists considerable gap between the reasoning performance of multimodal dLLMs and state-of-the-art AR MLLMs such as Qwen3-VL (Bai et al., 2025a). We hope that future work on scaling and improving pretraining can close this gap for base models. Second, while AR LLMs can leverage many efficient inference frameworks LaViDa-R1 such as vLLM (Kwon et al., 2023), these low-level optimization frameworks have not yet been fully adapted to support dLLMs. This results in throughput bottleneck because our training process uses eager Python execution during online sampling. We hope future acceleration frameworks tailored to dLLMs will help address this issue. Lastly, while LaViDaR1 already included diverse number of tasks and significantly expanded the scope of dLLM RL literature, there are more tasks to explore for unified multimodal dLLMs, such as multi-turn website code generation with visual feedback, interleaved text-image reasoning, poster design, etc. In this work, we focused on single-round reasoning with visual inputs. We will explore more tasks in future work. Text-to-Image Generation.We also briefly explored extending reasoning to text-to-image (T2I) generation. However, we find that existing reward models are fundamentally misaligned with reasoning-centric objectives. Despite this limitation, LaViDa-R1 demonstrates non-trivial zero-shot reasoning ability on T2I tasks. An illustrative example is shown in Figure 11: given the prompt The light source that replaced candles in homes during the early 20th century, LaViDa-R1 correctly reasons about the historical context and generates light bulbs, whereas LaViDa-O produces images of candles. Unfortunately, current reward models fail to reliably distinguish such reasoning-grounded generations. Early approaches such as PickScore (Kirstain et al., 2023), which are built upon CLIP-based models, lack explicit reasoning capabilities. We further investigated more recent VLM-based reward models, but even the state-of-the-art UnifiedReward-Qwen-7B (Wang et al., 2025g) is unable to correctly rank the samples in this example. Upon inspecting the models justifications, we find that it hallucinates spurious criteria, assessing alignment with surface-level concepts such as candles (object) and replaced (activity), thereby misinterpreting the compositional and historical reasoning required by the prompt. Recent work (Wei et al., 2025) proposes using frontier VLM (GPT-4.1) as an online reward model, which may alleviate some of these issues. However, this approach is prohibitively expensive and difficult to scale. We believe that effective and scalable reward modeling remains key open challenge for reasoning-driven T2I generation. In light of these limitations, we leave reinforcement learning fine-tuning for T2I reasoning to future work. D. Additional Discussions for Related Works Reinforcement Learning with dLLMs. d1 (Zhao et al., 2025a) first explored adapting the GPRO algorithm through token log probability ratios estimated through one-sample ELBO, with many works exploring other improved designs of estimation (Gong et al., 2025; Yang et al., 2025b; Wang et al., 2025a). In another line of work, wd1 (Tang et al., 2025) and DMPO (Zhu et al., 2025) addressed the dLLM RL tasks from policy distribution-matching perspective, leading to new ways of computing advantage that differ from the aforementioned GRPO-style algorithms. TraceRL (Wang et al., 2025e) considers an actor-critic style method, and ESPO (Ou et al., 2025) adapts GSPO (Zheng et al., 2025a) to dLLMs. Unified SFT and RL. There have been multiple works in the LLM community that explored effective strategies to combine SFT and RL into single stage (Chen et al., 2025a; Lv et al., 2025; Liu et al., 2025b; Fu et al., 2025; Yan et al., 2025; Ma et al., 2025). These works have introduced approaches that dynamically balance off-policy SFT loss and on-policy RL loss, attempting to outperform single-stage RL or single-stage SFT for LLM post-training. Unlike these approaches, our loss formulation for unified SFT and RL is simple and was introduced to ensure the stability of the RL stage, rather than to make single-stage RL or single-stage SFT obsolete. Moreover, to the best of our knowledge, our work is the first to apply unified SFT-RL strategy to unified multimodal models, whereas prior work has largely focused on single-text modalities using LLMs. Connection between Answer Forcing and IGPO. IGPO (Zhao et al., 2025b) injects partially masked ground-truth reasoning traces as hint in the online sampling process of dLLMs. While this design and our proposed answer forcing algorithm both make use of inpainting capabilities of dLLMs, they are distinct from each other in that IGPO requires high-quality ground-truth reasoning traces, whereas Answer Forcing only requires access to the final answer and does not require any pre-existing reasoning traces. In general, high-quality reasoning traces are rare and difficult to obtain for tasks such as object grounding, making answer forcing more generalizable approach. 24 LaViDa-R1 Figure 8. Qualitative Results of Reason-Intensive Object Grounding. 25 LaViDa-R1 Figure 9. Qualitative Results of Reason-Based Image Editing. 26 LaViDa-R1 Figure 10. Qualitative Results on Visual Math Problems. 27 LaViDa-R1 Figure 11. Qualitative Reuslts on Text-to-Image Generation. While LaViDa-R1 demonstrated some zero-shot reasoning capabilities on text-to-image tasks, we find that existing VLM-based reward models fail to properly provide reward signal for reasoning-based tasks."
        }
    ],
    "affiliations": [
        "Adobe",
        "Georgia Tech",
        "UCLA"
    ]
}