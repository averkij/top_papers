{
    "paper_title": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality",
    "authors": [
        "Youngtaek Oh",
        "Jae Won Cho",
        "Dong-Jin Kim",
        "In So Kweon",
        "Junmo Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMs) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal capabilities, primarily due to the use of global hard negative (HN) loss, which contrasts global representations of images and texts. This global HN loss pushes HN texts that are highly similar to the original ones, damaging the model's multi-modal representations. To overcome this limitation, we propose Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard negative loss and selective calibrated regularization. These innovations provide fine-grained negative supervision while preserving the model's representational integrity. Our extensive evaluations across diverse benchmarks for both compositionality and multi-modal tasks show that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also retains strong multi-modal capabilities. Code is available at: https://github.com/ytaek-oh/fsc-clip."
        },
        {
            "title": "Start",
            "content": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality Youngtaek Oh1 Jae Won Cho2 Dong-Jin Kim3 1KAIST 2Sejong University In So Kweon1* 3Hanyang University Junmo Kim1* 1{youngtaek.oh, iskweon77, junmo.kim}@kaist.ac.kr 2chojw@sejong.ac.kr 3djdkim@hanyang.ac.kr 4 2 0 O 7 ] . [ 1 0 1 2 5 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we propose new method to enhance compositional understanding in pretrained vision and language models (VLMs) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal capabilities, primarily due to the use of global hard negative (HN) loss, which contrasts global representations of images and texts. This global HN loss pushes HN texts that are highly similar to the original ones, damaging the models multi-modal representations. To overcome this limitation, we propose Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard negative loss and selective calibrated regularization. These innovations provide fine-grained negative supervision while preserving the models representational integrity. Our extensive evaluations across diverse benchmarks for both compositionality and multi-modal tasks show that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also retains strong multi-modal capabilities. Code is available at: https://github.com/ytaek-oh/fsc-clip."
        },
        {
            "title": "Introduction",
            "content": "Humans naturally excel at multi-modal understanding, effortlessly perceiving and interpreting different modalities, such as images and text, and forming associations between them. This capability is evident in recognizing novel concepts (Fu et al., 2018), cross-modal retrieval (Kaur et al., 2021), and compositional reasoning (Levesque et al., 2012). To achieve this ability in artificial intelligence, foundational vision and language models (VLMs) have been trained on large-scale imagetext datasets (Schuhmann et al., 2022b), significantly bridging the gap between human and ma- *Corresponding authors Figure 1: holistic comparison of fine-tuning methods for vision-language compositionality. Enhancing compositionality often compromises multi-modal task performance in previous approaches. Our FSC-CLIP bridges this gap, minimizing these trade-offs. Full experimental results are provided in Tab. 1. chine capabilities in tasks like zero-shot recognition and image-text retrieval (Radford et al., 2021). Despite these advances, VLMs still face challenges in compositional reasoning (Yuksekgonul et al., 2023). Humans intuitively understand complex compositional language in combination with images, engaging in spatial reasoning (Kamath et al., 2023b), recognizing attributes and relationships in objects (Hsieh et al., 2023), and perceiving equivariance between image and text (Wang et al., 2023). In contrast, VLMs often fail to understand these nuanced relationships (Liu et al., 2023a; Ray et al., 2023). This shortfall is attributed to their reliance on global, single vector representations (Kamath et al., 2023a) and limited ability to match compositional knowledge (Wang et al., 2024). To improve compositionality in VLMs, both pretraining (Singh et al., 2023; Zheng et al., 2024) and fine-tuning (Zhang et al., 2024; Singh et al., 2024) methods have been proposed. In particular, finetuning, which leverages pre-trained knowledge and is cost-effective, is widely adopted in academia. Typically, this involves incorporating hard negative texts (Doveh et al., 2022, 2023; Herzig et al., 2023) into training. However, as shown in Fig. 1, this approach can result in trade-off, where gains in compositionality come at the expense of performance in the multi-modal tasks: zero-shot classification (ZS) and image to text retrieval (I2T Ret). Previously, hard negative (HN) losses are applied to global image and text representations. Since HN texts are encoded too similarly to the original ones (Kamath et al., 2023a), pushing them away with the HN loss can disrupt the multi-modal representations. To address this, we propose new fine-tuning framework for VLMs that enhances compositional reasoning while preserving performance in multi-modal tasks. Our approach mitigates the degradation caused by global hard negative loss on single vector representations, which struggles to capture subtle informational differences between hard negative texts and the original text. Our framework introduces two key innovations: (1) Local Hard Negative (LHN) Loss. We utilize dense alignments between image patches and text tokens to calculate the hard negative loss. This approach, inspired by the dense alignment for visionlanguage representation (Huang et al., 2021; Bica et al., 2024), aggregates local similarity scores to enhance compositional understanding without undermining multi-modal representations. (2) Selective Calibrated Regularization (SCR). To address the adverse effects of hard negative (HN) losses caused by similarly encoded HN and original texts, SCR is designed to better regulate HN supervision. It selectively focuses on challenging HN texts and applies slight positive margin, reducing confusion and improving calibration. The whole framework, dubbed Fine-grained Selective Calibrated CLIP, offers fine-grained supervision of hard negatives while preserving the integrity of multi-modal representations. As shown in Fig. 1, FSC-CLIP not only improves compositionality but also maintains high performance in multi-modal tasks. It outperforms DAC-LLM in ZS and I2T Ret scores, while achieving similar compositionality (Comp) across wide range of tasks. We summarize our contributions as follows: We propose novel fine-tuning methodology, FSC-CLIP, that aims to enhance vision-language compositionality in pre-trained VLMs while maintaining their multi-modal task capabilities. We design local hard negative (LHN) loss and selective calibrated regularization (SCR) mechanism, effectively capturing subtle differences in hard negative texts and preserving the integrity of multi-modal representations. We validate FSC-CLIP through an extensive range of experiments, covering 11 compositionality, 21 zero-shot recognition, and 3 image-text retrieval tasks, establishing comprehensive evaluation of VLMs multifaceted capabilities."
        },
        {
            "title": "2 Related Work",
            "content": "Contrastive Vision-Language Models. CLIP (Radford et al., 2021) has revolutionized multimodal domains through large-scale image-text pretraining, demonstrating remarkable zero-shot capabilities. Its dual encoder architecture has introduced versatility and driven advancements across wide range of existing vision (Zhou et al., 2022; Oh et al., 2022; Cho et al., 2022) and vision-language downstream tasks (Jang et al., 2022, 2023; Cho et al., 2023a,c,b; Kim et al., 2019, 2021a,b). CLIP also serves as the foundation for recognition (Liang et al., 2023), image captioning (Mokady et al., 2021; Lee et al., 2024; Kim et al., 2024a,b), large multi-modal models (Li et al., 2023; Liu et al., 2023b), and generative models (Podell et al., 2024). In addition, CLIP extends its utility to connecting 3D (Sun et al., 2024) or audio (Elizalde et al., 2023; Senocak et al., 2023) to language, highlighting its essential role in multi-modal and compositional tasks in practical applications. We aim to enhance CLIPs compositional understanding while preserving its multi-modal capabilities. Vision-Language Compositionality. Although vision and language models exhibit promising capabilities such as zero-shot classification and retrieval (Radford et al., 2021; Zeng et al., 2022), they still struggle with compositional reasoning, which requires fine-grained understanding between image and text (Peng et al., 2024). Numerous benchmarks have been proposed, testing various aspects like attributes, relationships and objects (Zhao et al., 2022; Yuksekgonul et al., 2023), spatial reasoning (Kamath et al., 2023b; Liu et al., 2023a) and linguistic phenomena (Parcalabescu et al., 2022). To enhance compositionality, incorporating hard negative captions during fine-tuning has become common approach (Zhang et al., 2024), with these captions being generated through rule-based methods (Doveh et al., 2022; Yuksekgonul et al., Figure 2: complete FSC-CLIP framework that incorporates Local Hard Negative (LHN) Loss with Selective Calibrated Regularization (SCR), alongside global HN loss. The LHN loss measures similarity between an image and text at the patch and token levels to more accurately identify subtle differences between original and HN texts. SCR combines focal loss with label smoothing to mitigate the adverse effects of using hard negative losses. 2023), large language model prompting (Doveh et al., 2023), or scene graphs (Singh et al., 2023; Herzig et al., 2023). We comprehensively evaluate the capabilities of VLMs across broad range of compositionality and multi-modal tasks."
        },
        {
            "title": "3 Methodology",
            "content": "We first outline the fine-tuning setup for CLIP in Sec. 3.1. Next, we introduce FSC-CLIP, which incorporates Local Hard Negative (LHN) Loss and Selective Calibrated Regularization (SCR) in Secs. 3.2 and 3.3. The training objective for FSC-CLIP is described in Sec. 3.4. The complete FSC-CLIP framework, integrating both global and local HN losses with SCR, is illustrated in Fig. 2."
        },
        {
            "title": "3.1 CLIP with Global Contrastive Loss",
            "content": "CLIP objective. Consider mini-batch = {(Ii, Ti)}B i=1 of size B, consisting of image and text pairs (Ii, Ti). Using CLIPs visual and language encoders, fv() (e.g., ViT (Dosovitskiy et al., 2021)) and ft() (e.g., Transformers (Vaswani et al., 2017)), each image Ii is encoded into sequence of visual tokens Vi = fv(Ii), and each text Ti into sequence of textual tokens Ti = ft(Ti). These sequences are represented in shared multi-modal space, with Vi = {vp,i}P p=1 comprising patch embeddings and Ti = {tw,i}W w=1 consisting of token embeddings. The global representations of image and text vi and ti Rd can be obtained by pooling the local representations: vi = Pool (Vi) and ti = Pool (Ti), respectively. For example, Pool() corresponds to avgpool and argmax for images and texts in Radford et al. (2021)."
        },
        {
            "title": "CLIP aligns the corresponding images and texts",
            "content": "by measuring the global-level similarity: Sg (Ii, Ti) = exp (cos (vi, ti) /τ ) , (1) where cos (v, t) = vT vt . The image to text loss Li2t of CLIP maximizes Sg (Ii, Ti), while minimizing Sg (Ii, Tj) for all non-matching texts = i: Li2t ="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) i=1 log Sg (Ii, Ti) j=1 Sg (Ii, Tj) (cid:80)B , (2) 2 (Li2t + Lt2i). and the text to image loss Lt2i is the reciprocal of Li2t which aligns the matching image per text. The final CLIP loss is Lclip = 1 Incorporating hard negative texts. To enhance the compositional reasoning of CLIP, hard negative (HN) texts are commonly incorporated into training, whether they are rule-based (Yuksekgonul et al., 2023) or generated by language models (Doveh et al., 2023). Consider set of different HN texts Ti = { i }K k=1 originated from Ti. We introduce separate hard negative loss added to Lclip, similar to Doveh et al. (2022). First, we compute similarity prediction probability pg , assigned to the original caption Ti as follows: pg = Sg (Ii, Ti) Sg (Ii, Ti) + (cid:80)K k=1 Sg (cid:17) . (3) (cid:16) Ii, i Here, represents the global representation, and the hard negative (HN) loss applied to this similarity assignment is formulated as cross entropy: Lg neg ="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) i=1 log pg . (4) However, incorporating such global HN loss can inadvertently harm the multi-modal representations due to the similarly encoded global text representations between original and HN texts."
        },
        {
            "title": "3.2 Local Hard Negative (LHN) Loss",
            "content": "To address this, we propose novel Local Hard Negative (LHN) loss that utilizes local similarity score Sl(I, ). Replacing the global similarity Sg with Sl, the LHN loss is formulated as follows: Ll neg = 1 (cid:88) i= log Sl (Ii, Ti) (cid:88) Sl Sl (Ii, Ti) + (cid:16) Ii, i , (cid:17) (cid:125) (cid:124) k=1 (cid:123)(cid:122) pl where pl (5) represents the local similarity prediction. Unlike Bica et al. (2024), which uses token-level contrast for image-text pairs, we introduce new HN loss based on local similarity Sl from tokenpatch representations, enabling the capture of subtle differences between the original and HN texts. Textual-aligned Visual Patches. Sl(I, ) is designed to measure the similarity between token and patch embeddings for each token in the given text . From the patch representations = {vp}P p=1, we first derive the textual-aligned patch embeddings ˆV = {ˆvw}W w=1, corresponding to each textual token feature tw in RW d. This is achieved by performing weighted average of patches using attention weights RW derived from normalizing the similarity map between token and patch embeddings. We denote the similarity map as = TT RW , where sw,p = tT wvp. To relate multiple similar patches for each token, we min-max normalize to obtain a: sw,p mink sw,k maxk sw,k mink sw,k aw,p = , (6) and use the attention weights to aggregate V, obtaining the textual-aligned patches ˆV = {ˆvw}W w=1: ˆvw = 1 p=1 aw,p (cid:80)P (cid:88) p=1 aw,p vp. (7) In Appendix B.1, we explore different normalization choices for the attention weights in Eq. (6). Token-level Similarity. After obtaining the textualaligned visual tokens ˆV, we aggregate the per-token similarities between ˆV and as follows: (cid:88) exp (cos (ˆvw, tw) /τ ) , (8) Sl (I, ) = w=1 Figure 3: conceptual illustration of the confidencebased weighting mechanism in HN loss. It reduces the adverse impact of HN supervision by lowering the signal from confident predictions while selectively focusing on challenging ones, crucial for learning compositionality. where ˆvw ˆV and tw T. Unlike Sg(I, ) which is based on global representations, Sl(I, ) focuses on the local alignment between image and text, better distinguishing features between correct and HN texts, thereby reducing the negative impact by the hard negative loss, as illustrated in Fig. 2."
        },
        {
            "title": "We observe that Ll",
            "content": "neg maintains multi-modal task performance close to the pre-trained representations while significantly enhancing compositionality. Notably, the order of aggregation, whether pooling first and then computing similarity (e.g., Sg), or computing token-level similarity before aggregation (e.g., Sl), proves to be important."
        },
        {
            "title": "3.3 Selective Calibrated Regularization (SCR)",
            "content": "Since hard negative (HN) texts are often encoded similarly to the original texts, HN losses can disrupt multi-modal representations. To counter this, we propose Selective Calibrated Regularization (SCR) to better regulate HN supervision, seamlessly applicable to both global and local HN losses. SCR has two components: one modulates the supervision signal based on image-text similarity, while the other adjusts label assignments to calibrate the positiveness of HN texts. As shown in Tab. 2, we confirm that both components are crucial for preserving the representation integrity. Focal Loss to Target Challenging HN Texts. To mitigate the negative impact of supervising HN texts, we reduce the supervision signal for confident similarity predictions to the original text. Instead, we focus more on challenging HN texts that exhibit higher similarity to the image and may be confused with the original texts. This confidencebased weighting aligns with the concept of focal loss (Lin et al., 2017), as shown in Fig. 3. Formally, let the similarity prediction for the i-th batch item, including generated HN texts, be represented as vector pi R1+K, where the first element corresponds to the original text. The HN loss can be re-formulated in vector representation with pi as CE(pi, yi) = (cid:80)K k=0 li,k, where li,k = yi,k log pi,k and yi = 1[k=0] R1+K indicates the assignment label between an image and all texts. To reduce the negative impact of the confident image-text similarity predictions, we apply confidence-based weighting to CE loss as follows: Focal (pi, yi) = (cid:88) k= (1 pi,k)γ li,k, (9) where γ is the modulation parameter. This strategy prioritizes challenging image-text associations, essential for learning compositionality, while effectively preventing degradation from the HN loss. Label Smoothing to Calibrate the Positiveness of HN Texts. Although hard negative (HN) texts share similar representations with the original text, previous methods have overlooked their potential positiveness in the HN loss design, assigning strict value of 0 to all HN texts in the label vector yi. Similar to the motivation in Zhang et al. (2024), but differing from their ranking loss approach, we acknowledge the potential correctness of HN texts by assigning slight positive margin rather than categorizing them as entirely negative. To this end, we apply label smoothing (Guo et al., 2017) to the label vector yi using smoothing parameter β to ensure positive margin for HN texts: yi,k = (1 β) yi,k + β 1 + , (10) where yi provides non-binary label for the HN losses. It helps preserve the models representations during training with HN losses."
        },
        {
            "title": "3.4 Overall Training Objective",
            "content": "Our FSC-CLIP incorporates two hard negative (HN) losses, Lg neg, representing global and local HN losses respectively, into CLIP loss Lclip: neg and Ll Ltotal = Lclip + λgLg neg + λlLl neg, (11) where λg and λl are the weighting factors for the respective losses. Selective Calibrated Regularization (SCR) is applied to both losses, incorporating label smoothing and focal loss. The global HN loss, Lg neg is computed as Focal (pg, y), while the LHN loss, Ll neg is derived similarly, by replacing pg with pl for the local representations."
        },
        {
            "title": "4 Experiments",
            "content": "Training Datasets. We consider three image-text datasets for fine-tuning: COCO captions (Chen et al., 2015), CC-3M (Sharma et al., 2018), and LAION-COCO (Schuhmann et al., 2022a). For COCO captions, we utilize 100K examples preprocessed by Yuksekgonul et al. (2023). As pointed out by Singh et al. (2023), COCO shares data with several evaluation benchmarks (e.g., SugarCrepe and retrieval tasks), which may inadvertently affect the results. To ensure broader evaluation and avoid such overlap, we additionally consider CC-3M and LAION-COCO for fine-tuning. For each dataset, we randomly sample 100K examples and, instead of using raw captions, we utilize synthetic captions paired with images. Specifically, for CC-3M, we generate captions using CoCa (Yu et al., 2022) with ViT-L/14, while for LAIONCOCO, we use captions generated by BLIP (Li et al., 2022b) with ViT-L/14, applied to the LAION2B dataset (Schuhmann et al., 2022b). Hard Negative (HN) Texts. We employ simple rule-based methods for generating hard negative (HN) texts, avoiding the need for external language models like Le Scao et al. (2023) used in Doveh et al. (2023). For each original caption, we apply three distinct operations: negclip, replace, and bi-gram shuffle. These operations are applied at every training step, ensuring variation in HN texts across iterations. As result, each batch item is paired with an image and four captions, as illustrated in Fig. 2. Further details and examples on these operations are provided in Appendix A.1. Training Setup. Consistent with previous methods (Yuksekgonul et al., 2023; Singh et al., 2023; Zhang et al., 2024), we trained our models during 5 epochs with batch size 256, using OpenCLIP repository (Ilharco et al., 2021). The learning rate is set to 5e-6 and decayed by cosine schedule, with warmup of 50 steps. Models are optimized using AdamW with weight decay of 0.1. We use single Quadro RTX 8000 GPU with 48GB memory for training. Images are re-scaled to 224, and the context length is 77 for texts. We set the weighting factors λg = 0.5 and λl = 0.2. For SCR, we set γ = 2.0 and β = 0.02 for focal loss and label smoothing, respectively. We also experiment with LoRA (Hu et al., 2022), which preserves the original model parameters. Consistent with Doveh et al. (2022, 2023), we set the rank to 4. Training our model takes less than one hour for 100K samples."
        },
        {
            "title": "Method",
            "content": "O LoRA E e e e I r g s r S A i e - n g W t C S o t 2 R 2 CLIP (ViT-B/32) 57.5 23.8 26.5 21.7 73. 84.1 67.5 70.8 41.5 8.8 31. 46.1 57.1 60.0 45.8 NegCLIP1 CE-CLIP2 GNM-CLIP3 MosaiCLIP,4 NegCLIP FSC-CLIP (Ours) FSC-CLIP (Ours) TSVLC5 (RB) TSVLC5 (RB+LLM) DAC-LLM6 DAC-SAM6 MosaiCLIP, NegCLIP FSC-CLIP (Ours) FSC-CLIP (Ours) 80.9 76.3 57.1 82.6 85.0 85.1 85. 30.3 34.7 17.4 - 34.7 42.2 42.9 Fine-tuned: MS-COCO, 100K Samples 74.9 76.9 70.6 76.8 90.8 90.1 89.2 90.7 83.7 85.7 78.7 - 26.4 24.5 25.0 - 73.7 76.7 71.1 - 30.3 26.8 28.3 - 29.8 29.8 29.7 26.2 26.3 26.5 84.5 85.1 82.1 90.6 90.9 90. 74.7 75.3 75.0 75.4 76.7 77.2 42.9 41.7 42.1 - 41.2 40.6 41.7 8.0 5.2 10.2 - 8.2 9.5 6. Fine-tuned: Conceptual Captions - 3M (CC-3M), 3M Samples 6.8 5.8 4.8 8.5 - 89.8 89.7 88.9 88.5 - 76.9 73.2 85.3 83.8 - 69.3 72.2 70.5 70.2 - 77.5 79.2 83.5 84.7 77.3 24.0 24.6 22.8 24.3 - 27.4 27.6 25.6 25.3 - 40.9 39.9 42.6 42.4 - 36.1 33.1 60.6 63.7 - 83.5 82.7 86.4 83.3 80.4 Fine-tuned: Conceptual Captions 3M (CC-3M), 100K Samples 7.0 6.8 5.0 43.2 42.6 42. 88.6 88.2 88.8 72.4 74.9 74.5 83.4 84.3 82.3 79.0 77.4 80.3 25.8 28.5 27.7 24.6 25.2 24. 50.5 44.0 50.6 86.5 78.8 84.4 34.6 33.0 33.1 - 34.2 34.2 33.2 31.6 31.4 30.8 29.9 - 32.8 34.2 32. 52.4 52.0 47.5 - 53.1 54.2 53.6 51.2 50.9 54.7 55.0 - 54.0 53.2 53.9 55.9 49.9 56.3 - 55.1 55.7 55. 54.9 55.4 51.1 51.9 53.5 52.6 53.5 53.6 66.8 59.2 66.1 - 66.1 66.3 65.3 54.9 55.1 36.9 41.1 - 51.8 55.8 56. 58.4 57.4 55.5 - 57.9 58.3 57.2 52.1 52.3 52.4 49.0 - 54.1 54.6 54.0 CLoVe7 83. 41.7 Fine-tuned: LAION-COCO, 600M Samples 26.9 84.6 87.9 71.8 25. 66.6 41.8 6.5 31.7 51.6 51. 53.1 56.0 NegCLIP FSC-CLIP (Ours) FSC-CLIP (Ours) Numbers taken from the original paper. Our implementation, without additional image batch. References: 1(Yuksekgonul et al., 2023) 2(Zhang et al., 2024) 3(Sahin et al., 2024) 4(Singh et al., 2023) 5,6(Doveh et al., 2022, 2023) 7(Castro et al., 2024) Fine-tuned: LAION-COCO, 100K Samples 27.2 29.1 29.1 43.0 42.4 42.8 25.3 24.7 24. 86.4 82.8 85.5 70.9 73.6 72.6 32.3 33.4 32.5 80.9 82.6 80.6 89.6 90.1 89.7 76.0 75.7 78. 48.7 46.8 54.4 53.5 53.5 54.2 54.1 55.3 55.9 7.8 6.8 5.8 52.3 58.2 57. 54.1 55.5 54.3 Table 1: holistic comparison of fine-tuning methods applied to the pre-trained CLIP ViT-B/32 model across 11 compositionality, 21 zero-shot classification, and 3 retrieval tasks, including their meta averages: Comp, ZS, and I2T/T2I Ret. FSC-CLIP achieves superior compositionality scores while maintaining strong multi-modal task performances. For each fine-tuning dataset, the best numbers are bold, and the second-best numbers are underlined. Evaluation Setup. We utilize an extensive range of benchmarks for comprehensive evaluation, exceeding the scope of previous works. Full details including references are provided in Appendix A.2. For compositionality, we employ 11 benchmarks in total: ARO, CREPE-Productivity, EqBen, ImageCoDe, SPEC, SugarCrepe, SVO Probes, VALSE, VL-Checklist, WhatsUp, and Winoground, testing different facets of compositional reasoning. For multi-modal tasks, we evaluate 21 zero-shot classification tasks using ELEVATER toolkit. In addition, we conduct image-text retrieval evaluations on COCO, Flickr30k, and COCO-Counterfactuals. All those evaluations are performed using the vl-compo package (Oh et al., 2024). We report single aggregated number, which is the average of sub-tasks for each compositionality benchmark. We also provide the meta-average across all compositionality benchmarks (Comp), the average performance over 21 zero-shot classification tasks (ZS), and the average Recall@1 for three image to text (I2T Ret) and text to image (T2I Ret) retrieval tasks, as shown in Tab. 1. For fair comparison, we consistently run evaluations for all the previous models across all the benchmarks."
        },
        {
            "title": "4.1 Main Results",
            "content": "We compare our FSC-CLIP to previous fine-tuning methods for compositionality. We report both compositionality and multi-modal task performance as shown in Tab. 1. In Fig. 4, we visualize the tradeoff trajectory between Comp and ZS through the robust fine-tuning method (Wortsman et al., 2022). Compositionality while Sacrificing Multi-Modal Tasks. We introduce our baseline, NegCLIP, serving as direct comparison to our FSC-CLIP. Unlike the original implementation of NegCLIP (Yuksekgonul et al., 2023), we utilize an online version of hard negatives generation (e.g., negclip) and omit the use of additional similar image batches. This baseline will be further used in our ablation study, with the symbol omitted for convenience. neg id Lg neg Ll - 1 2 3 - 4 5 7 8 - -"
        },
        {
            "title": "Focal LS Comp",
            "content": "ZS I2T Ret T2I Ret - - - - - - - 54.0 51.7 54.4 - 54.2 53.9 53.5 52.8 50.2 53.6 55.7 52.6 54.2 53.8 55.3 55.3 55. 47.4 61.6 46.9 53.1 51.7 58.2 57.1 63.2 53.7 54.5 53.8 54.8 54.9 55.5 55.6 55. Figure 4: Fine-tuning trajectories between compositionality (Comp) and zero-shot classification (ZS) via robust fine-tuning method (Wortsman et al., 2022). Each point represents the interpolated model between the pretrained and each fine-tuned version, at varying ratios. FSC-CLIP offers better trade-offs between Comp and ZS, maintaining ZS scores in the fully fine-tuned model. As shown in Tab. 1, we first compare our FSC-CLIP with previous models fine-tuned on COCO, aligning our results with those in the literature. CE-CLIP2 shows significant drop in ZS score to 49.9. Meanwhile, GNM-CLIP3 maintains ZS score close to that of the pre-trained model, but shows only modest increase in Comp. In contrast, our model achieves superior Comp scores while maintaining competitive ZS and retrieval performance. As note, we have grayed out the retrieval scores of models fine-tuned on COCO to account for the influence of overlapping data. When fine-tuned on datasets other than COCO, such as CC-3M and LAION-COCO, all baseline models show improvements in the Comp score, but this comes at the expense of their ZS and I2T Ret scores compared to the pre-trained CLIP. For example, NegCLIP demonstrates promising Comp scores compared to methods like TSVLC5 and CLoVe7, but still shows weaker ZS and I2T Ret scores relative to the pre-trained model. Similarly, DAC-LLM6, despite having the strongest Comp score supported by LLM-augmented captions, suffers notable declines in both ZS and I2T Ret, decreasing by 6.0 and 23.1 points, respectively. Although TSVLC5 preserves these scores better than other models, its Comp score improvements are relatively smaller. These methods apply hard negative (HN) loss to global-level representations, potentially causing the observed performance drops. Preserving Multi-Modal Tasks. FSC-CLIP stands out by achieving higher Comp scores than previous Table 2: Impact by individual component. The local HN loss preserves multi-modal task performance. In addition, focal loss and label smoothing (LS) in SCR complement each other, improving the decreased multimodal task performance caused by the HN losses. models, comparable to DAC-LLM, while maintaining strong performance in multi-modal tasks. As shown in Fig. 1, when fine-tuned on 100K subset of LAION-COCO, our model achieves Comp score of 53.5, significantly surpassing its pretrained counterpart, and ZS score of 55.9, nearly matching the pre-trained CLIP. Additionally, it attains an I2T Ret score of 58.2, the highest among models not fine-tuned on COCO. Further improvements are observed with using LoRA (Hu et al., 2022) for fine-tuning, which boosts the Comp score to 54.2 while maintaining the ZS score. Similar trends are evident when we fine-tune FSC-CLIP on 100K subset of CC3M. Remarkably, these results are achieved by our innovative Local HN loss and Selective Calibrated Regularization (SCR) design. We further analyze these contributions in Sec. 4.2. Robust Fine-tuning on Compositionality and Zero-shot Tasks. As depicted in Fig. 4, we utilize the weight-space ensembling technique, WiSEFT (Wortsman et al., 2022), to compare different fine-tuning methods and their trajectories, specifically in terms of Comp and ZS scores using LAIONCOCO for fine-tuning our model. We create intermediate models by interpolating between each finetuned model and the pre-trained one. The blending ratio increases from 0.0 (e.g., pre-trained) to 1.0 (e.g., fully fine-tuned), in increments of 0.1. FSC-CLIP with LoRA attains ZS score of 58 at the intermediate, surpassing the scores of other models, while improving Comp to 50. When fully fine-tuned, it attains superior Comp score and offers better trade-offs than CLoVe and CE-CLIP, without significant loss in ZS. In contrast, DAC-LLM sees significant drop in ZS, gaining only 0.5 point in Comp, as highlighted by the red marker. Meanwhile, FSC-CLIP not only matches but exceeds the ZS score by 4.9 in the fully fine-tuned model. id 1 2 3 4 λl - 0.1 0.2 0."
        },
        {
            "title": "Comp",
            "content": "ZS I2T Ret T2I Ret 52.9 55.8 53.0 53.5 53. 55.7 55.3 55.7 57.5 57.4 58.2 57.3 55.5 55.4 55.5 55.4 id 1 2 3 4 γ - 1.0 2.0 5."
        },
        {
            "title": "Comp",
            "content": "ZS I2T Ret T2I Ret 53.9 53.8 53.4 53.5 52. 54.9 55.3 55.6 51.7 54.7 58.2 60.2 54.9 55.1 55.5 55.5 id 1 2 3 4 β - 0.02 0.05 0."
        },
        {
            "title": "Comp",
            "content": "ZS I2T Ret T2I Ret 54.2 54.2 53.5 53.1 52. 55.3 55.2 55.2 53.1 58.2 59.0 58.7 54.8 55.5 55.1 55.3 (a) Sensitivity to the weighting factor λl of the local HN loss. (b) Sensitivity to the modulation factor γ of focal loss. (c) Sensitivity to the label smoothing factor β. Table 3: Sensitivity analysis of each component in our FSC-CLIP framework. (a): With the global HN loss applied, applying the local HN loss benefits the compositionality while preserving the multi-modal task scores. (b) and (c): Both focal loss and label smoothing, the two components of our Selective Calibrated Regularization (SCR), mutually enhance multi-modal task performance but may compromise compositionality when applied too strongly. We highlight the cells corresponding to our design choices in the final FSC-CLIP model. CLIP1 ViT-B/"
        },
        {
            "title": "LoRA Comp",
            "content": "ZS I2T Ret T2I Ret CLIP"
        },
        {
            "title": "LoRA Comp",
            "content": "ZS I2T Ret T2I Ret 46.2 60.3 62. 49.0 ViT-B/32 44.3 63.0 63.8 51. 58.1 + NegCLIP + FSC-CLIP 59.3 + FSC-CLIP 58.8 1Pre-trained: 400M OpenAI, Fine-tuned: LAION-COCO 100K subset. 53.8 59.7 59.9 55.9 57.0 57.4 54.1 54.1 54.6 + NegCLIP + FSC-CLIP + FSC-CLIP 2Pre-trained: DataComp-XL, Fine-tuned: LAION-COCO 100K subset. 52.1 56.8 56.8 52.3 53.8 53.1 59.2 61.1 60.7 53.5 52.9 54.0 Table 4: Fine-tuning results of CLIP with ViT-B/16 encoder, pre-trained on 400M samples of OpenAI data. Table 5: Fine-tuning results of CLIP with ViT-B/32 encoder, pre-trained on 12.8B DataComp-XL."
        },
        {
            "title": "4.2 Analysis",
            "content": "We further present an in-depth analysis on our FSC-CLIP, fine-tuned on LAION-COCO: Impact of Individual Components. From Tab. 2, we observe that applying the local HN loss alone (row 2) surprisingly preserves the multi-modal scores. However, when both global and local HN losses are activated (row 3), Comp is further boosted but at the cost of ZS and I2T Ret scores, likely due to the complicated adverse effects of the losses. The proposed SCR effectively addresses this degradation. Both focal loss (row 4) and label smoothing (row 5) are effective and, when combined, complementarily boost all the ZS, I2T Ret, and T2I Ret scores. Notably, I2T Ret increases by 11.3 (rows 3 to 6) with only relatively mild drop in Comp. We also note that comparing rows 7 and 8 with rows 1 and 2, SCR significantly boosts multi-modal task scores. Furthermore, as shown in row 6, applying both global and local HN losses is essential for achieving better Comp and I2T Ret scores. Sensitivity Analysis. We explore the impact of individually varying each components parameters in the final model, as detailed in Tab. 3. From Tab. 3a, we find that increasing the local HN loss parameter λl improves Comp score while preserving multimodal task scores. Tab. 3b shows that increasing the modulation parameter γ boosts multi-modal tasks; however, beyond certain point, we find that compositionality declines, as weakening the learning signal from HN texts. Similarly, Tab. 3c indicates that label smoothing benefits multi-modal tasks, particularly I2T Ret. Yet, assigning too much positive margin with β to negative samples can impede the learning of compositionality. Fine-tuning CLIP with ViT-B/16. We also finetuned CLIP with ViT-B/16 encoder from OpenAI for comparison, as detailed in Tab. 4. This model uses more image patches in training, showing better multi-modal capabilities. However, no gains are observed in Comp compared to the ViT-B/32 model from Tab. 1. After fine-tuning, NegCLIP decreases In contrast, FSC-CLIP ZS and I2T Ret scores. maintains its Comp score and significantly enhances multi-modal task performances. We also find that fine-tuning with LoRA yields improved ZS and I2T Ret scores, along with higher Comp score. Scaling Pre-training Data for Fine-tuning. We explore the effect of large-scale pre-training data when fine-tuned. From Tab. 5, we fine-tuned CLIP model with ViT-B/32 encoder, pre-trained on 12.8B DataComp-XL dataset (Gadre et al., 2023), far exceeding the 400M samples from OpenAI (Radford et al., 2021). Despite the larger scale pre-training yielding promising ZS score of 63.0, we find no improvement in compositionality compared to OpenAIs CLIP. For fine-tuning, NegCLIP results in notable drop in multi-modal task performance. In contrast, FSC-CLIP with LoRA not only counters this degradation but also achieves higher Comp score than NegCLIP. Figure 5: Image to text retrieval examples on COCO-CF dataset. CLIP and DAC-LLM often rank negative captions (marked with red crossmarks) as top-1, while FSC-CLIP consistently retrieves the correct caption (marked with green checkmarks), demonstrating superior fine-grained understanding and retrieval accuracy in challenging conditions. Qualitative Counterfactual Image to Text Retrieval Results. In Fig. 5, we compare image to text retrieval results on the COCO-Counterfactuals (COCO-CF) (Le et al., 2023) dataset for three models: pre-trained CLIP (Radford et al., 2021), DAC-LLM (Doveh et al., 2023), and our proposed FSC-CLIP. The figure displays the top-3 retrieved captions for each image, with correct captions indicated by green checkmarks and incorrect ones by red crossmarks. We observe that CLIP and DACLLM often fail to retrieve the correct caption associated with the image, ranking negative caption as top-1. In contrast, our FSC-CLIP consistently retrieves the correct caption as top-1, demonstrating superior retrieval capabilities along with stronger fine-grained compositional understanding, even in the presence of hard negative captions."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce Fine-grained Selective Calibrated CLIP (FSC-CLIP), new fine-tuning framework for vision-language compositionality. It aims to preserve multi-modal capabilities and address the limitations of existing methods relying on global representations. We achieve this by employing dense representations between images and texts and regularizing the hard negative losses to prevent degradation, thereby facilitating the introduction of Local Hard Negative Loss and Selective Calibrated Regularization. Our extensive validation shows improved compositional reasoning and promising performance in standard multi-modal tasks. Limitations. Our methodology, including all the prior approaches, relies on short captions for both training and evaluation benchmarks. This practice constrains the models exposure to and understanding of longer contexts, which are essential for achieving genuine vision-language compositional understanding. Longer and detailed captions involve more complex associations and contextual nuances (Onoe et al., 2024; Garg et al., 2024) that are essential for advanced compositionality in vision and language models. Moving forward, there is compelling need within the community to develop training and evaluation protocols that incorporate longer captions, better addressing the challenges of compositionality. Acknowledgements. This research was partially supported by Samsung Electronics Co., Ltd (G01200447), by the KAIST Cross-Generation Collaborative Lab Project, by the MSIT(Ministry of Science, ICT), Korea, under the Global Research Support Program in the Digital Field program(RS2024-00436680) supervised by the IITP(Institute for Information & Communications Technology Planning & Evaluation), and by the Institute of Information and Communications Technology Planning and Evaluation (IITP) grant funded by the Korea Government (MSIT) (Artificial Intelligence Innovation Hub) under Grant 2021-0-02068. Additionally, this project was supported in part by Microsoft Research Asia. Dong-Jin Kim was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. RS-2023-00245661)."
        },
        {
            "title": "References",
            "content": "Romain Beaumont. 2021. img2dataset: Easily turn large sets of image urls to an image dataset. https: //github.com/rom1504/img2dataset. Ioana Bica, Anastasija Ilic, Matthias Bauer, Goker Erdogan, Matko Bošnjak, Christos Kaplanis, Alexey Gritsenko, Matthias Minderer, Charles Blundell, RazImproving fine-grained van Pascanu, et al. 2024. arXiv understanding in image-text pre-training. preprint arXiv:2401.09865. Santiago Castro, Amir Ziai, Avneesh Saluja, Zhuoning Yuan, and Rada Mihalcea. 2024. Clove: Encoding compositional language in contrastive visionlanguage models. arXiv preprint arXiv:2402.15021. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325. Jae Won Cho, Dawit Mureja Argaw, Youngtaek Oh, Dong-Jin Kim, and In So Kweon. 2023a. Empirical study on using adapters for debiased visual question answering. Computer Vision and Image Understanding, 237:103842. Jae Won Cho, Dong-Jin Kim, Yunjae Jung, and In So Kweon. 2022. Mcdal: Maximum classifier discrepancy for active learning. IEEE transactions on neural networks and learning systems, 34(11):87538763. Jae Won Cho, Dong-Jin Kim, Yunjae Jung, and In So Kweon. 2023b. Counterfactual mix-up for visual question answering. IEEE Access, 11. Jae Won Cho, Dong-Jin Kim, Hyeonggon Ryu, and In So Kweon. 2023c. Generative bias for robust In Proceedings of the visual question answering. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1168111690. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations. Sivan Doveh, Assaf Arbelle, Sivan Harary, Roei Herzig, Donghyun Kim, Paola Cascante-Bonilla, Amit Alfassy, Rameswar Panda, Raja Giryes, Rogerio Feris, et al. 2023. Dense and aligned captions (dac) promote compositional reasoning in vl models. Advances in Neural Information Processing Systems, 36. Sivan Doveh, Assaf Arbelle, Sivan Harary, Rameswar Panda, Roei Herzig, Eli Schwartz, Donghyun Kim, Raja Giryes, Rogério Schmidt Feris, Shimon Ullman, et al. 2022. Teaching structured vision & language concepts to vision & language models. 2023 ieee. In CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 26572668. Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. 2023. Clap learning audio concepts from natural language supervision. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE. Christiane Fellbaum. 2010. Wordnet. In Theory and applications of ontology: computer applications, pages 231243. Springer. Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal, and Shaogang Gong. 2018. Recent advances in zero-shot recognition: Toward dataefficient understanding of visual content. IEEE Signal Processing Magazine, 35(1):112125. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. 2023. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36. Roopal Garg, Andrea Burns, Burcu Karagol Ayan, Yonatan Bitton, Ceslee Montgomery, Yasumasa Onoe, Andrew Bunner, Ranjay Krishna, Jason Baldridge, and Radu Soricut. 2024. Imageinwords: Unlocking hyper-detailed image descriptions. arXiv preprint arXiv:2405.02793. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. 2017. On calibration of modern neural networks. In International conference on machine learning, pages 13211330. PMLR. Lisa Anne Hendricks and Aida Nematzadeh. 2021. Probing image-language transformers for verb understanding. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 36353644, Online. Association for Computational Linguistics. Roei Herzig, Alon Mendelson, Leonid Karlinsky, Assaf Arbelle, Rogerio Feris, Trevor Darrell, and Amir Globerson. 2023. Incorporating structured representations into pretrained vision & language models using scene graphs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1407714098, Singapore. Association for Computational Linguistics. Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear. Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. 2023. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. Advances in Neural Information Processing Systems, 36. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Shih-Cheng Huang, Liyue Shen, Matthew Lungren, and Serena Yeung. 2021. Gloria: multimodal global-local representation learning framework for label-efficient medical image recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 39423951. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. 2021. Openclip. If you use this software, please cite it as below. Youngjoon Jang, Youngtaek Oh, Jae Won Cho, DongJin Kim, Joon Son Chung, and In So Kweon. 2022. Signing outside the studio: Benchmarking background robustness for continuous sign language recognition. In British Machine Vision Conference. Youngjoon Jang, Youngtaek Oh, Jae Won Cho, Myungchul Kim, Dong-Jin Kim, In So Kweon, and Joon Son Chung. 2023. Self-sufficient framework for continuous sign language recognition. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE. Amita Kamath, Jack Hessel, and Kai-Wei Chang. 2023a. Text encoders bottleneck compositionality in contrastive vision-language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 49334944, Singapore. Association for Computational Linguistics. Amita Kamath, Jack Hessel, and Kai-Wei Chang. 2023b. Whats up with vision-language models? investigating their struggle with spatial reasoning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9161 9175, Singapore. Association for Computational Linguistics. Parminder Kaur, Husanbir Singh Pannu, and Avleen Kaur Malhi. 2021. Comparative analysis on cross-modal information retrieval: review. Computer Science Review, 39:100336. Dong-Jin Kim, Jae Won Cho, Jinsoo Choi, Yunjae Jung, and In So Kweon. 2021a. Single-modal entropy based active learning for visual question answering. In British Machine Vision Conference. Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, and In So Kweon. 2019. Image captioning with very scarce supervised data: Adversarial semi-supervised learning approach. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 20122023, Hong Kong, China. Association for Computational Linguistics. Dong-Jin Kim, Tae-Hyun Oh, Jinsoo Choi, and In So Kweon. 2021b. Dense relational image captioning via multi-task triple-stream networks. IEEE Transactions on pattern analysis and machine intelligence, 44(11):73487362. Dong-Jin Kim, Tae-Hyun Oh, Jinsoo Choi, and In So Kweon. 2024a. Semi-supervised image captioning by adversarially propagating labeled data. IEEE Access. Taehoon Kim, Pyunghwan Ahn, Sangyun Kim, Sihaeng Lee, Mark Marsden, Alessandra Sala, Seung Hwan Kim, Honglak Lee, Kyounghoon Bae, Bohyung Han, Kyoung Mu Lee, Xiangyu Wu, Yi Gao, Hailiang Zhang, Yang Yang, Weili Guo, Jianfeng Lu, Youngtaek Oh, Jae Won Cho, Dong-Jin Kim, In So Kweon, Junmo Kim, Wooyoung Kang, Won Young Jhoo, Byungseok Roh, Jonghwan Mun, Solgil Oh, Kenan Emir Ak, Gwang-Gook Lee, Yan Xu, Mingwei Shen, Kyomin Hwang, Wonsik Shin, Kamin Lee, Wonhark Park, Dongkwan Lee, Nojun Kwak, Yujin Wang, Yimu Wang, Tiancheng Gu, Xingchang Lv, and Mingmao Sun. 2024b. Nice: Cvpr 2023 challenge on zero-shot image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73567365. Benno Krojer, Vaibhav Adlakha, Vibhav Vineet, Yash Goyal, Edoardo Ponti, and Siva Reddy. 2022. Image retrieval from contextual descriptions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 34263440, Dublin, Ireland. Association for Computational Linguistics. Tiep Le, Vasudev Lal, and Phillip Howard. 2023. Cococounterfactuals: Automatically constructed counterfactual examples for image-text pairs. Advances in Neural Information Processing Systems, 36. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2023. Bloom: 176bparameter open-access multilingual language model. arxiv preprint arXiv:2211.05100. Soeun Lee, Si-Woo Kim, Taewhan Kim, and DongJin Kim. 2024. Image-like retrieval and frequency-based entity filtering for zero-shot captioning. arXiv preprint arXiv:2409.18046. Ifcap: Hector Levesque, Ernest Davis, and Leora Morgenstern. In Thir2012. The winograd schema challenge. teenth international conference on the principles of knowledge representation and reasoning. Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, et al. 2022a. Elevater: benchmark and toolkit for evaluating language-augmented visual models. Advances in Neural Information Processing Systems, 35:9287 9301. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022b. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR. Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, et al. 2024. Docci: Descriptions of conarXiv preprint nected and contrasting images. arXiv:2404.19753. Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, and Albert Gatt. 2022. VALSE: task-independent benchmark for vision and language models centered on linguistic phenomena. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 82538280, Dublin, Ireland. Association for Computational Linguistics. Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, and Zuxuan Wu. 2024. Synthesize diagnose and optimize: Towards fine-grained vision-language understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1327913288. Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. 2023. Open-vocabulary semantic segmentation with mask-adapted clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 70617070. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2024. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 29802988. Fangyu Liu, Guy Emerson, and Nigel Collier. 2023a. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. Advances in neural information processing systems, 36. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Arijit Ray, Filip Radenovic, Abhimanyu Dubey, Bryan Plummer, Ranjay Krishna, and Kate Saenko. 2023. cola: benchmark for compositional text-to-image retrieval. Advances in Neural Information Processing Systems, 36. Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. 2023. Crepe: Can vision-language foundation models reason compositionally? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1091010921. Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, and Volker Tresp. 2024. Enhancing multimodal compositional reasoning of visual language models with generative negative mining. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 55635573. Ron Mokady, Amir Hertz, and Amit Bermano. 2021. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734. Youngtaek Oh, Pyunghwan Ahn, Jinhyung Kim, Gwangmo Song, Soonyoung Lee, In So Kweon, and Junmo Kim. 2024. Exploring the spectrum of visiolinguistic compositionality and recognition. arXiv preprint arXiv:2406.09388. Youngtaek Oh, Dong-Jin Kim, and In So Kweon. 2022. Daso: Distribution-aware semantics-oriented pseudolabel for imbalanced semi-supervised learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9786 9796. Christoph Schuhmann, Andreas , Köpf, Richard Vencu, Theo Coombes, and Romain Beaumont. 2022a. Laion coco: 600m synthetic captions from laion2ben. https://laion.ai/blog/laion-coco/. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022b. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294. Arda Senocak, Hyeonggon Ryu, Junsik Kim, Tae-Hyun Oh, Hanspeter Pfister, and Joon Son Chung. 2023. Sound source localization is all about cross-modal alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7777 7787. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, Melbourne, Australia. Association for Computational Linguistics. Harman Singh, Pengchuan Zhang, Qifan Wang, Mengjiao Wang, Wenhan Xiong, Jingfei Du, and Yu Chen. 2023. Coarse-to-fine contrastive learning in image-text-graph space for improved vision-language compositionality. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 869893, Singapore. Association for Computational Linguistics. Jaisidh Singh, Ishaan Shrivastava, Mayank Vatsa, Richa Singh, and Aparna Bharati. 2024. Learn\" no\" to say\" yes\" better: Improving vision-language models via negations. arXiv preprint arXiv:2403.20312. Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. 2024. Alpha-clip: clip model focusing on wherever you want. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1301913029. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. 2022. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5238 5248. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Jin Wang, Shichao Dong, Yapeng Zhu, Kelu Yao, Weidong Zhao, Chao Li, and Ping Luo. 2024. Diagnosing the compositional knowledge of vision language models from game-theoretic view. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 5033250352. PMLR. Tan Wang, Kevin Lin, Linjie Li, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. 2023. Equivariant similarity for visionlanguage foundation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1199812008. Farhadi, Hongseok Namkoong, et al. 2022. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 79597971. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. 2023. When and why vision-language models behave like bags-of-words, and what to do about it? In The Eleventh International Conference on Learning Representations. Yan Zeng, Xinsong Zhang, and Hang Li. 2022. Multigrained vision language pre-training: Aligning texts In Proceedings of the 39th with visual concepts. International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2599426009. PMLR. Le Zhang, Rabiul Awal, and Aishwarya Agrawal. 2024. Contrasting intra-modal and ranking crossmodal hard negatives to enhance visio-linguistic comIn Proceedings of the positional understanding. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1377413784. Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin. 2022. An explainable toolbox for evaluating pretrained vision-language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3037, Abu Dhabi, UAE. Association for Computational Linguistics. Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, and Ranjay Krishna. 2024. Iterated learning improves compositionality in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1378513795. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Learning to prompt for visionlanguage models. International Journal of Computer Vision, 130(9):23372348."
        },
        {
            "title": "A Additional Details",
            "content": "A.1 Rule-based Hard Negative Texts Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali We provide details in generating hard negative texts in our model. We employ three types of rule-based methods: negclip (Yuksekgonul et al., Figure 6: Example results of rule-based hard negative texts used for training our model. Image-text pairs were randomly sampled from LAION-COCO (Schuhmann et al., 2022a). For negclip (Yuksekgonul et al., 2023) and replace (Hsieh et al., 2023), differences from the original captions are highlighted in red. 2023), replace (Hsieh et al., 2023), and bi-gram shuffle. Each method is implemented in an online version and applied to the original text at every training step, resulting in total of four texts including the original caption for every batch as illustrated in Fig. 2. In the online augmentation process, some captions do not yield hard negative counterpart; these are masked out and excluded from the hard negative loss calculation. The negclip method rearranges words within captions by swapping similar phrase types such as nouns, verbs, or adjectives within the text. The replace method generates hard negative texts by replacing specific elements in the caption: entities, relations, or attributes, using antonyms or co-hyponyms from WordNet (Fellbaum, 2010). The bi-gram shuffle rearranges text by shuffling bi-grams (e.g., pairs of adjacent words), within sentence. It varies the sentence structure, ensuring the generated texts serve as challenging negatives to the original. All the augmentation methods above utilize the SpaCy (Honnibal and Montani, 2017) package. We implemented bi-gram shuffle, while for negclip and replace, we adopted the implementations from CLoVe (Castro et al., 2024). For illustrative purposes, we provide examples of each method applied to image-caption pairs, in Fig. 6. A.2 Details on Evaluation Benchmark Compositionality. VLMs are presented with either an image or text query and must identify the correct match from set of candidates, which includes subtly altered incorrect options of texts and images. If there are two candidates, including the original, the random chance accuracy becomes 0.5. Benchmarks are grouped into three categories based on the query modality. Tab. 6 provides list of benchmarks for each category, along with the corresponding dataset licenses. (1) Image-to-Text, where the objective is to choose the correct textual description for presented image: ARO (Yuksekgonul et al., 2023), CREPE-Productivity (Ma et al., 2023), SugarCrepe (Hsieh et al., 2023), VALSE (Parcalabescu et al., 2022), VL-Checklist (Zhao et al., 2022), and WhatsUp (Kamath et al., 2023b). (2) Text-to-Image, which requires the selection of the correct image that matches given text query: ImageCoDE (Krojer et al., 2022) and SVO Probes (Hendricks and Nematzadeh, 2021). (3) Group, which involves two counterfactual image-text pairs, the challenge is to match each image with its corresponding text and the vice versa: Winoground (Thrush et al., 2022), EqBen (Wang et al., 2023), and SPEC (Peng et al., 2024). For the Image-to-Text and Text-to-Image tasks, top-1 accuracy is used. For the Group tasks, group accuracy measures whether VLMs correctly match all the associated image-text pairs. VL-Checklist"
        },
        {
            "title": "MIT",
            "content": "COCO, Visual Genome, Flickr30k CREPE-Productivity SugarCrepe unspecified Visual Genome MIT"
        },
        {
            "title": "MIT",
            "content": "Visual7w, COCO, SWiG, VisDial_v1.0, FOIL-it VG_Relation, VG_Attribution, Flickr30k_Order, COCO_Order Atomic Foils, Negate, Swap Add_{object, attribute}, Replace_{object, attribute, relation}, Swap_{object, attribute} Actions_{swap, replacement}, Coreference_{hard, standard}, Counting_{adversarial, hard, small}, Existence, Foil-it, Plurals, Relations Object_Location_{center, margin, mid}, Object_Size_{large, medium, small}, Attribute_{action, color, material, state}, Relation_{action, spatial} Controlled_Images_{A, B}, COCO_QA_{One, Two}, VG_QA_{One, Two} size, Static (e.g., images), Video (e.g., videos) unspecified Visual Genome, SWiG, COCO, HAKE, HICO_Det, Pic, HCVRD, OpenImages"
        },
        {
            "title": "MIT",
            "content": "Controlled_Images (self-captured), COCO, GQA OpenImages, MSRVTT, VideoStorytelling, YouCook Apache-2.0 Google Image Search API Subject, Verb, Object"
        },
        {
            "title": "Getty Images",
            "content": "META IMAGES RESEARCH LICENSE Apache-2.0 Action Genome (AG), GEBC, YouCook2, Kubric, StableDiffusion (SD) Stable-Diffusion-XL 1.0 unspecified - EQ-AG, Kubric_{location, counting, attribute}, EQ-SD EQ-YouCook2, EQ-GEBC, EQAbsolute_size, Absolute_position, Count, Relative_size, Relative_position, Existence Table 6: comprehensive list of compositionality benchmarks used in our work, further subdivided based on the query types for each individual test: Image-to-Text, Text-to-Image, and Group, respectively. To elaborate on details in specific benchmarks, for EqBen, we cap the evaluation sample size at 20,000. This is because the sub-tasks eqbenag and eqbenyoucook2 contain 195,872 and 45,849 samples respectively, and evaluating all samples would be excessively time-consuming. Limiting the number of samples does not significantly alter the evaluation results. We do not use the official repositorys 10% evaluation split because it does not support sub-task-specific evaluations. For SVO-Probes, we have downloaded imagetext pairs using the img2dataset (Beaumont, 2021) package from the URL list1, as they are not available as physical files. Out of the original 36.8k samples, 22,162 were successfully downloaded, with 3,728 for the subj_neg, 13,523 for the verb_neg, and 4,911 for the obj_neg sub-tasks, respectively. For SPEC, unlike the other datasets in the Group category, we use the average of image to text and text to image accuracy, rather than group accuracy. Zero-shot Classification. We leverage ELEVATER toolkit (Li et al., 2022a) for 21 zero-shot classification tasks, including ImageNet (Deng et al., 2009), licensed under MIT License. Image-Text Retrieval. We utilize COCO captions (Chen et al., 2015), Flickr30k (Young et al., 2014), and COCO-Counterfactuals (Le et al., 2023) to evaluate the retrieval task. These datasets are licensed under BSD-3-Clause, CC0: Public Domain, and CC-BY-4.0, respectively. For COCOCounterfactuals, we randomly selected 30% of the total 17,410 samples for evaluation, resulting in 5,223 samples. Each example includes two counterfactual image-text pairs, so the total number of images and texts used for retrieval is 10,446; one for the original and one for the hard negatives. A.3 Train Dataset We used the pre-processed version of COCO captions (Chen et al., 2015) by Yuksekgonul et al. (2023), licensed under BSD 2-Clause. In addition, we utilized LAION-COCO (Schuhmann et al., 2022a), licensed under CC-BY-4.0, and CC3M (Sharma et al., 2018)2, with 100K randomly sampled examples from each dataset to match the size of COCO for fine-tuning. We downloaded both datasets using the img2dataset package. 1https://huggingface.co/datasets/MichiganNLP/ 2https://github.com/google-research-datasets/ svo_probes conceptual-captions/blob/master/LICENSE et al., 2024) and softmax, respectively. As in Tab. 2, id 2 only applies the LHN Loss without global HN loss and SCR, while id 6 represents the full objective. Our findings show that the effectiveness of LHN Loss is not significantly impacted by any particular normalization technique. In other words, general normalization of attention weights can be applied to LHN Loss, reducing reliance on techniques like those from Bica et al. (2024). This suggests that the unique design of LHN Loss is key to the improved performance. B.2 Multiple Runs In Tab. 8, we report the mean and standard deviation for our models across all tasks listed in Tab. 1, using three distinct seeds: 0, 1, and 2 for training each model. B.3 Zero-shot Classification We report the results for each benchmark within the 21 zero-shot classification tasks in Tab. 9. B.4 Image-Text Retrieval We present the results for each benchmark included in the three image-text retrieval tasks in Tab. 10. id Attn. Norm."
        },
        {
            "title": "Comp",
            "content": "ZS I2T Ret T2I Ret 2 minmax 2 minmax-sparse softmax 2 minmax 6 6 minmax-sparse softmax 6 51.7 51.6 52. 53.5 53.4 53.3 55.7 55.5 55.4 55.3 55.1 55.5 61.6 61.1 60.9 58.2 57.8 57.1 54.5 54.8 54. 55.5 55.4 55.7 Table 7: Ablation study on the normalization of attention weights in Eq. (6) for the LHN Loss. We found that no specific normalization method significantly impacted the results, highlighting the importance of the unique LHN loss design. A.4 Baseline Methods In the comparisons with previous methods in Tab. 1, we evaluated prior approaches using the same protocol as ours to ensure fair and consistent evaluation. We obtained the corresponding checkpoints from each official repository and loaded them using the open_clip package (Ilharco et al., 2021). When loading the checkpoints of previous models, we explicitly set quick_gelu to True in the open_clip implementation. While this setting was omitted in the implementations of NegCLIP (Yuksekgonul et al., 2023), CE-CLIP (Zhang et al., 2024), and GNM-CLIP (Sahin et al., 2024), the adjustment aligns with the original CLIP models from (Radford et al., 2021), which were pre-trained and also fine-tuned with this option activated. We list the previous methods with corresponding licenses. NegCLIP (Yuksekgonul et al., 2023): MIT License, CE-CLIP (Zhang et al., 2024): MIT License, GNM-CLIP (Sahin et al., 2024): Apache2.0 License, TSVLC3 and DAC4 (Doveh et al., 2022, 2023): unspecified, CLoVe (Castro et al., 2024): MIT License."
        },
        {
            "title": "B Additional Results",
            "content": "For thoroughness, we include additional results not featured in the main paper. Note that all models were fine-tuned using the CLIP ViT-B/32 encoder from OpenAI (Radford et al., 2021). B.1 Additional Analysis Normalization of attention weights. We present an ablation experiment on the normalization of attention weights in Eq. (6), in alignment with the ablation study in Tab. 2. We replace the current minmax normalization with minmax-sparse (Bica 3https://github.com/SivanDoveh/TSVLC 4https://github.com/SivanDoveh/DAC"
        },
        {
            "title": "LoRA",
            "content": "O P n E C m e a S b O S t k C - U h u o C S o t 2 R 2 FSC-CLIP FSC-CLIP 82.70.10 85.30.14 46.60.35 52.91.28 29.30.17 28.90.17 24.60.94 24.90. Fine-tuned: LAION-COCO, 100K Samples 73.50.15 72.40.17 90.10.03 89.70.05 75.70.33 78.70.20 82.60.14 80.50.11 42.10.25 42.90.05 6.20.63 5.40. 33.50.17 32.40.11 53.40.09 54.00.17 55.60.32 56.10.18 57.80.52 57.30.13 55.30.20 54.40.08 Table 8: Evaluation across three training runs of our model using different seeds. We report the mean and standard deviation obtained from the evaluation results of the models across three trials. 1 0 1 t 0 1 i 0 0 1 i 1 1 2 n p - o 3 1 0 2 f d Method 3 1 0 2 - r a - f 2 0 1 o fl 1 0 1 f t s m - e k 1 - e i a d - i i s - i - f o m c 2 - e r c - 5 4 s a o t i fi a 7 0 0 2 v a A CLIP-ViT-B/32 88.3 89.8 65.1 17. 44.4 45.5 42.3 19.7 66.7 84. 32.6 55.9 63.3 27.4 48.3 87. 60.6 58.6 60.0 59.7 82.6 57. NegCLIP CE-CLIP GNM-CLIP TSVLC (RB) TSVLC (RB+LLM) DAC-LLM DAC-SAM 88.2 82.2 86.8 83.7 84.6 82.6 81.3 88.9 85.9 88.4 92.3 92.0 90.4 89. 63.2 60.2 65.7 66.0 66.8 64.1 64.1 15.0 9.6 15.2 16.2 16.2 14.3 14.8 43.1 35.2 42.0 47.3 44.9 50. Fine-tuned: MS-COCO, 100K Samples 54.3 47.6 53.5 39.7 54.9 46.6 62.3 47.2 62.4 30.2 28.0 30.2 79.4 70.1 81.8 16.8 10.0 17.3 60.9 49.9 61. 27.6 34.6 25.2 Fine-tuned: Conceptual Captions 3M (CC-3M), 100K Samples 39.5 40.3 38.4 40.4 52.1 56.5 52.5 49.8 43.6 46.8 50.7 48.0 14.7 13.8 10.5 8. 58.2 58.5 49.7 48.9 81.2 81.6 74.1 72.3 24.2 27.1 24.2 24.9 57.8 56.9 56.3 55.7 58.5 59.7 51.0 52.3 30.4 27.8 16.3 18. Fine-tuned: LAION-COCO, 600M Samples 49.7 40.6 54.4 46.9 43.9 42.1 45.2 85.4 66.0 86.3 85.5 84.7 74.4 76.7 59.7 58.8 59. 50.0 50.5 50.0 58.9 58.8 61.1 58.5 59.8 60.1 54.5 60.0 56.9 51.5 58.7 58.6 59.5 52.2 54.7 54.0 35.3 53. 49.2 50.5 39.4 39.8 84.4 83.1 84.0 84.7 84.7 85.1 84.1 55.9 49.9 56.3 54.9 55.4 51.1 51.9 CLoVe 85.5 85.8 66.2 12.6 37.7 49. 38.0 9.0 44.6 71.9 22.6 54. 53.1 34.9 36.4 74.2 56.7 51. 55.2 48.7 81.9 51.0 FSC-CLIP (Ours) FSC-CLIP (Ours, LoRA) 86.5 85. 87.5 88.5 65.7 66.3 15.3 15.8 42.4 39.8 43.9 52.8 48.9 48. 14.9 14.2 55.5 57.0 80.5 81.0 31.6 27.9 55.9 56.3 58.1 57. 29.1 33.9 52.4 54.3 84.2 82.7 61.0 59.8 56.0 57.2 56.9 58. 52.0 52.6 83.6 83.7 55.3 55.9 Fine-tuned: LAION-COCO, 100K Samples Table 9: Expanded results for the 21 zero-shot classification tasks from ELEVATER (Li et al., 2022a). COCO Retrieval Flickr30k Retrieval COCO-Counterfactuals Retrieval Avg. Method Image to text (I2T) T2I R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@ Text to image (T2I) Text to image (T2I) Text to image (T2I) Image to text (I2T) Image to text (I2T) I2T CLIP-ViT-B-32 50.1 74.9 83.5 30.4 56. 66.8 78.8 94.9 98.3 58.7 83. 90.0 51.0 79.3 86.7 48.1 77. 85.9 60.0 45.8 NegCLIP CE-CLIP GNM-CLIP TSVLC (RB) TSVLC (RB+LLM) DAC-LLM DAC-SAM 59.3 56.0 58. 46.1 46.4 29.9 33.1 82.8 81.6 81.4 71.7 71.8 54.5 57.9 89.4 89.0 88.8 80.4 80.8 65.6 68.8 45.2 47.1 41. 36.3 36.6 37.3 34.0 72.1 74.1 67.5 81.7 83.1 77.8 Fine-tuned: MS-COCO, 100K Samples 91.8 89.6 89.9 85.7 75.3 82.9 96.4 93.2 96. 98.8 96.9 98.6 71.6 68.9 68.8 95.7 94.2 94.1 Fine-tuned: Conceptual Captions 3M (CC-3M), 100K Samples 62.0 62.2 63.5 59.7 72.4 72.7 73.8 70. 74.0 74.8 52.9 59.8 93.2 92.6 79.8 82.7 96.4 96.8 87.9 89.0 64.9 65.1 64.6 61.7 87.2 87.6 88.0 85.7 92.7 92.7 93.0 91. Fine-tuned: LAION-COCO, 600M Samples 55.3 46.3 57.2 44.6 44.1 28.1 30.4 82.5 75.7 84.5 72.0 71.5 53.6 55.2 89.2 84.5 90. 80.2 80.1 64.4 64.8 58.3 56.2 56.7 55.0 55.1 55.2 51.5 84.9 83.6 84.5 83.3 83.3 83.0 79.9 91.3 90.5 91. 90.0 90.4 90.0 87.3 66.8 59.2 66.1 54.9 55.1 36.9 41.1 58.4 57.4 55.5 52.1 52.3 52.4 49.0 CLoVe 48.3 73.9 82.8 42.7 68.7 78. 69.5 90.4 95.6 68.7 90.0 94. 41.5 69.1 78.3 56.5 84.2 90. 53.1 56.0 FSC-CLIP (Ours) FSC-CLIP (Ours, LoRA) 49.7 48.2 73.6 73.6 82.4 81. 40.4 39.0 66.4 64.9 76.4 75.0 75.6 75.1 93.3 93.2 97.4 96. 68.2 66.9 90.0 88.6 94.3 93.6 49.2 48.5 77.5 76.0 85.8 84. 57.9 57.1 85.4 84.7 91.4 91.0 58.2 57.3 55.5 54.3 Fine-tuned: LAION-COCO, 100K Samples Table 10: Expanded results for the three zero-shot image-text retrieval tasks, including COCO (Chen et al., 2015), Flickr30k (Young et al., 2014), and COCO-Counterfactuals (Le et al., 2023)."
        }
    ],
    "affiliations": [
        "Hanyang University",
        "KAIST",
        "Sejong University"
    ]
}