{
    "paper_title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems",
    "authors": [
        "Xuanming Zhang",
        "Yuxuan Chen",
        "Min-Hsuan Yeh",
        "Yixuan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs-a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce MetaMind, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent, emotion), (2) a Domain Agent refines these hypotheses using cultural norms and ethical constraints, and (3) a Response Agent generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework's ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 3 4 9 8 1 . 5 0 5 2 : r MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems Xuanming Zhang1, Yuxuan Chen2, Min-Hsuan Yeh1, Yixuan Li1 1Uniersity of Wisconsin-Madison 2Tsinghua University xzhang2846@wisc.edu, sharonli@cs.wisc.edu"
        },
        {
            "title": "Abstract",
            "content": "Human social interactions depend on the ability to infer others unspoken intentions, emotions, and beliefsa cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce MetaMind, multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent, emotion), (2) Domain Agent refines these hypotheses using cultural norms and ethical constraints, and (3) Response Agent generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the frameworks ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind."
        },
        {
            "title": "Introduction",
            "content": "What is meant often goes far beyond what is said, and that is what makes conversation possible. H. P. GRICE Everyday human conversation can be filled with intent that goes unspokenfeelings implied but never named, expectations hinted at with no explicit instruction, and suggestions masked as statements. Consider the utterance: Its cold in here. Depending on who says it, to whom, and in what context, it could be mere observation, polite request to close window, or even an expression of discomfort seeking empathy. Humans handle such ambiguity by reasoning about the speakers beliefs, desires, emotions, thoughts, and intentionsmental states that are not directly observable. This capacity, known as Theory of Mind (ToM) [1], has been extensively studied in the field of developmental psychology and is shown to emerge in children around the age of four [2, 3]. This allows humans to move beyond the literal surface of language and grasp the deeper intent behind what is said. Corresponding author. Preprint. Under review. Figure 1: MetaMind multi-agent framework. The architecture comprises three collaborative agentsTheoryof-Mind Agent, Domain Agent, and Response Agentworking in staged metacognitive loop. The ToM Agent generates hypotheses about latent mental states, which are refined by the Domain Agent using cultural/ethical constraints. The Response Agent synthesizes contextually appropriate outputs while validating them with inferred intent. Large language models (LLMs), by contrast, often falter in this regard. While they excel in semantic understanding tasks by producing fluent and contextually relevant text [4], they struggle with social reasoning with ambiguity and indirectness that characterize real-world communication. For instance, LLMs can fall short in applications that demand human-like social intelligence, including empathetic dialogue and conflict mediation [5, 6]. Addressing this gap is essential for building AI systems that interact effectively in socially complex environments. One of the key challenges in bridging this gap lies in inferring user mental statesbeliefs, desires, emotions, and intentionsthat are not directly observable but are essential for interpreting socially nuanced language. Unlike humans, LLMs do not naturally infer these unspoken intentions, making it particularly difficult for them to respond appropriately in scenarios involving indirect speech, implied emotions, or culturally sensitive cues [79]. Recent work has attempted to address these challenges by injecting social behavior into LLMs [1012], such as simulating social interactions via static role-play prompting [13] or fine-tuning with preference data [14, 15]. However, these approaches largely optimize for surface-level statistical alignment and fail to capture the structured, multi-stage cognitive process humans use to reason about unobservable intent [9] and generalize across diverse cultural and social contexts [16, 17]. Most notably, they treat social reasoning as single-step prediction problem, rather than layered process involving interpretation, reflection, and adaptationa hallmark of human metacognition [18, 1]. We argue that enabling LLMs with such staged reasoning capabilities is critical for achieving socially intelligent AI. In this paper, we propose MetaMind, cognitively motivated framework designed to explicitly model the key components in human-like social reasoning through staged and collaborative multiagent system. Our approach is grounded in psychological theories of metacognition [18, 1], which describe how humans reflect on their own thinking, revise their understanding in light of social norm constraints, and adapt their behavior in socially complex environments. MetaMind mirrors this layered reasoning process through three specialized agents, each responsible for distinct stage of cognitive-social inference. ❶ Theory-of-Mind Agent initiates reasoning by generating multiple hypotheses about the users mental state based on contextual and social cues. This reflects the first step in human ToM: inferring what the speaker might be trying to convey beyond literal words. For example, when user remarks that work has been exhausting lately, the system may infer underlying burnout, frustration, or need for empathy. ❷ Domain Agent then revises and filters these candidate hypotheses by incorporating socially grounded constraints, such as cultural expectations, ethical norms, or situational appropriateness. Just as humans refine their initial interpretations by aligning with social context, this agent ensures that the models reasoning remains socially responsible and context-aware. For instance, if romantic intent is hypothesized in workplace conversation, the Domain Agent may reinterpret it as collegial admiration based on professional norms. ❸ Finally, Response Agent generates and self-validates the output, conditioning on the refined optimal hypothesis and the users social memory (e.g., emotional patterns and prior preferences). This final step enacts 2 metacognitive loop that allows the system to respond with greater empathy, nuance, and cultural sensitivity. We conduct comprehensive empirical evaluation of MetaMind across suite of challenging social intelligence benchmarks, including ToM reasoning [19], social cognition, and social simulation [20] tasks. Our study spans over 16 contemporary LLMs, assessing both general social reasoning ability and performance in real-world, context-sensitive scenarios. Empirical results show that MetaMind achieves 35.7% average improvement on real social scenario tasks and 9.0% average gain in overall social cognition abilitysubstantially enhancing the social competence of underlying LLMs. Notably, our framework enables representative LLMs to match average human performance on key benchmarks. We also perform detailed ablation studies to isolate the contribution of each agent in the system, revealing that all three stages are critical to the frameworks success. We summarize our key contributions below: We propose MetaMind, cognitively grounded, multi-agent framework that models humanlike social reasoning by inferring mental states and incorporating social and ethical constraints, while adapting to user-specific patterns. We conduct comprehensive evaluations to demonstrate that MetaMind significantly improves both contextual accuracy and social appropriateness in real-world scenarios, achieving stateof-the-art results on challenging benchmarks and even matching human performance. We perform in-depth ablations to understand the impact of each agent and various design choices of our framework, justifying that all three components are essential to performance and generalization."
        },
        {
            "title": "2 Related Work",
            "content": "Theory of Mind in AI. Prior work has explored simulating ToM in AI systems through diagnostic frameworks [7, 11, 10], revealing limitations of LLMs in inferring beliefs, intentions, and social nuances [8, 21, 22, 5, 9]. While these studies identify key gapssuch as failures in handling recursive mental states or contextual ambiguitiestheir solutions often focus on narrow task-specific interventions, such as fine-tuning and testing on curated datasets [23, 6, 24, 25, 12, 13] or rule-based intent classifiers [2629]. In contrast, we introduce holistic framework grounded in theories of metacognition [18], which treats ToM not as specialized task but as foundational reasoning capability. Our framework integrates mental state inference, social norm constraints, and evolving social memory into unified system, enabling generalized and context-sensitive social reasoning. Prompting and Parameterized Reasoning. Methods like chain-of-thought prompting [30] and constrained decoding [31] aim to enhance LLMs reasoning by structuring intermediate steps or injecting task-specific rules. However, these approaches lack mechanisms for contextual adaptation [9]. Similarly, role-play prompting [13] simulates social interactions but relies on static personas, failing to capture the fluid interplay of social intent and context-dependent rules. Alignment approaches like RLHF [14] and instruction tuning [15] have improved adherence to user intent, but scaling these methods poses challenges in data curation and generalization control. Our framework departs from existing paradigms by decomposing reasoning across collaborative agents, enabling multi-stage, self-reflective social reasoning skin to human metacognition. Multi-Agent LLM Systems. Multi-agent LLM systems have been used across wide range of tasks, including debate-style reasoning [32, 24, 33, 34], retrieval-augmented generation [3537], and collaborative tool use [31, 3840]. These systems typically assign agents specialized roles to divide and coordinate subtasks. However, the use of multi-agent frameworks for socially grounded reasoning remains relatively underexplored. While some studies have applied agent collaboration to simulate social interactions or role-play conversations [6, 13, 27, 4144], they often focus on persona consistency, without modeling how agents can collaboratively infer and revise social interpretations. Our work addresses this gap by developing multi-agent architecture specifically designed for social reasoning, in which agents interact not only to complete tasks but to interpret user mental state and incorporate social normsmirroring core elements of human cognition. Metacognitive Architectures. Psychological theories of metacognition posit that self-regulated learning and reasoning rely on iterative cycles of planning, self-monitoring, and evaluative reflection [18, 45]. While these principles are well-established in human cognition, their systematic 3 integration into LLM architectures remains underexplored [46]. Current LLM-based systems often adopt oversimplified approaches, relying on monolithic prompting or partitioning functionality into isolated modules [47]. Our framework addresses this gap by formalizing metacognitive principles [48] into specialized, collaborative agents. This design mirrors human self-regulation, where adaptive reasoning emerges from synergistic interaction rather than static components."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Stage 1: Generating Mental State Hypothesis via Theory-of-Mind (ToM) Agent core feature of human social cognition is the ability to attribute unobservable mental statessuch as beliefs, desires, intentions, and emotionsa capacity broadly referred to as Theory of Mind [49, 3, 1]. This ability underpins what developmental psychologists call folk psychology: our everyday, intuitive reasoning about how and why others act [50]. It enables us to infer latent intent behind indirect speech and interpret emotionally charged or ambiguous behavior. While LLMs excel at semantic reasoning, they struggle with ToM-driven reasoning, often defaulting to literal interpretations that miss latent user intent. To address this gap, we introduce dedicated ToM Agent that serves as the entry point in our metacognitive reasoning pipeline. Our design is grounded in theories of metacognition [18], where mental state attribution is treated as structured inference process [51]. Rather than attempting to respond directly to user inputs, the ToM Agent seeks to construct set of plausible interpretations of what the user might be thinking or feeling. The ToM Agent formalizes the process of mental state inference as hypothesis generationgrounded in context, social knowledge, and prior interactionswhich will be refined and leveraged in subsequent stages. Hypothesis Generation. Formally, given user prompt ut, the ToM Agent operates under contextual input = (ut, Ct, Mt), where Ct denotes the social context (i.e., previous conversational history), and Mt denotes the social memory, which is dynamic database storing user preferences and salient emotional markers (see details in Appendix A.4). Provided with the contextual input , the goal of the ToM Agent is to generate set of candidate mental state interpretations Ht = {h1, h2, . . . , hk}, where each hi is an instantiation of latent mental state, accompanied by natural language explanations and type labels from the set = {Belief, Desire, Intention, Emotion, Thought}. The inference mechanism of the ToM Agent is implemented via Mental-State Reasoning. This procedure unfolds in four conceptual steps: (1) generating commonsense-based hypotheses from the input (ut, Ct), (2) cross-referencing these hypotheses with the social memory Mt, (3) identifying Theoryof-Mind markers across predefined categories, and (4) generating set of candidate hypotheses belonging to the identified ToM marker. This structured reasoning encourages the model to simulate human-like inference processes by incorporating contextual grounding and hypothesis diversification. To instantiate this reasoning process, we define the prompt in Table A.1, which guides the language model to reason about the user question in manner consistent with the psychological definition of Theory of Mindnamely, as an inferential process that constructs internal representations of others minds using contextual and background knowledge. This explicit hypothesis generation stage enables subsequent modules to reason over diverse set of plausible interpretations, rather than committing prematurely to singular semantic response. 3.2 Stage 2: Refining Hypothesis via Domain Agent The Domain Agent forms the second stage of our social reasoning pipeline and serves to refine the hypothesis generated by the ToM Agent. While the first stage focuses on what the user might be thinking or feeling, the Domain Agent assesses whether these interpretations are appropriate given broader normssuch as cultural norms and ethical constraints. This step ensures that the system not only understands intent, but also responds in socially responsible and domain-aware manner. Hypothesis Refinement and Selection. Formally, the Domain Agent takes as input the set of latent mental state hypotheses Ht = {h1, . . . , hk} produced by the ToM Agent, along with set of constraint rules D. Each rule in describes specific norm or guideline, such as Romantic suggestions are not appropriate in professional settings. These rules are encoded as conditions that 4 determine whether hypothesis should be retained, reweighted, or revised. For instance, if the ToM Agent infers romantic intention in professional conversation, the role-based prompt will instruct the model to reinterpret this intent in more appropriate way (e.g., as joke or misunderstanding). The domain agent proceeds in two steps. First, for each original hypothesis hi Ht, the Domain Agent generates revised version hi that incorporates the relevant domain rules. This revision may involve rephrasing the interpretation and adjusting its social tone. The revision process is implemented using targeted prompts instantiated for three types of rules: cultural norms, ethical constraints, and role-based expectations. We provide the prompt details in Appendix A.2. Next, the agent selects the most appropriate revised hypothesis by scoring each candidate hi based on composite objective: = arg max [λ (hiut, Ct, Mt) (cid:125) (cid:123)(cid:122) (cid:124) Contextual plausibility (cid:124) + (1 λ) log ], (1) (hiut, Ct, Mt) (hi) (cid:123)(cid:122) Information Gain (cid:125) where the first term denotes the contextual plausibility of the revised hypothesis, and the second term reflects the implicit information gain of the revised hypothesis when considering the context. The weight λ balances contextual plausibility and social appropriateness with how much the hypothesis is informed by the context versus being generic. The selected hypothesis is passed to the final stage for response generation and further validation. 3.3 Stage 3: Generating and Validating Output via Response Agent The final stage of the system is responsible for generating contextually appropriate response and validating its alignment with the inferred user intent. While the earlier stages focus on understanding and refining mental state interpretations, the Response Agent is tasked with transforming this structured understanding into concrete actiontypically natural language responsewhile preserving coherence, empathy, and domain compliance. Generation and Validation. This stage receives as input the final selected hypothesis h. Alongside, to ensure consistency with long-term user preferences and prior emotional states, the Response Agent incorporates social memory during decoding, enabling the model to adapt the tone or emotional framing of its response. The response ot = (y1, y2, . . . , yL) is generated by decoder LLM as the follows: ot = arg max p(yt y<t, h, Mt, ut), (cid:89) which maximizes the likelihood of the response conditioned on the optimal interpretation and social memory. To ensure alignment between the generated response and the intended user state, the Response Agent includes self-reflection mechanism, assessing its social and semantic quality using utility score: t=1 (ot) = β Empathy(ot, ut, Mt) (cid:125) (cid:123)(cid:122) Emotional alignment (cid:124) + (1 β) Coherence(ot, Ct, h) , (cid:125) (cid:123)(cid:122) Contextual coherence (cid:124) (2) where Empathy() quantifies how well the response resonates with the users inferred emotional or cognitive state, and Coherence() evaluates consistency with the conversational context and task constraints. The system can trigger regeneration if the utility score is too low. We provide the prompt details for both generation and validation in Appendix A.3."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate MetaMind using three challenging benchmarks spanning Theory-of-Mind reasoning, social cognition, and social simulation tasks. Each benchmark naturally emphasizes different aspect of the reasoning pipelinealigning well with the core functionality of each stage in our multi-agent framework. In particular, ToMBench [19] aligns with the function of Theory-of-Mind Agent (Stage 1) by assessing the models ability to infer latent mental states. Then, we employ suite of social cognition tasks, which align with the Domain Agents capacity in refining interpretations under 5 Figure 2: MetaMind improves Theory-of-Mind reasoning performance across LLMs. Each pair compares base model accuracy (gray) with MetaMind-enhanced accuracy (purple) on ToMBench. MetaMind consistently boosts ToM reasoning across both open-source and proprietary LLMs, highlighting its generality and effectiveness. See detailed performance in Appendix B.1. normative and ethical constraints (Stage 2). Lastly, the STSS benchmark [20] focuses on open-ended, interactive scenarios that test the Response Agents ability to generate contextually appropriate responses (Stage 3). Together, these benchmarks offer comprehensive evaluation of MetaMind. For reproducibility, we include implementation details and sensitivity analysis on hyperparameters (including k, λ, and β) in Appendix A.5. 4.1 Theory-of-Mind Reasoning Task We first evaluate MetaMinds ability to infer latent mental states using ToMBench [19], multiplechoice benchmark designed to test Theory-of-Mind reasoning across six categories: Emotion, Desire, Intention, Knowledge, Belief, and Natural Language Communication. This benchmark aligns directly with the function of the Theory-of-Mind Agent in Stage 1, which is responsible for generating structured mental state hypotheses from indirect or ambiguous input. MetaMind achieves new state-of-the-art performance on ability-oriented Theory-of-Mind reasoning, outperforming both base GPT-4 and competitive prompting-based baselines such as Chain-ofThought [30], SymbolicToM [52]. As shown in Table 1, MetaMind-enhanced GPT-4 reaches an average accuracy from 74.8% to 81.0%, surpassing all prior methods across most ToM dimensions. Importantly, MetaMinds improvements are not limited to GPT-4: additional experiments (Figure 2) show consistent performance gains across diverse LLM backbones, including open-source models like Mistral and Qwen. These results highlight the generality of our multi-agent framework and its effectiveness as model-agnostic enhancement for social reasoning. For more information about baseline, see Appendix A.7. Table 1: Comparison on Theory-of-Mind reasoning task. Intention Knowledge Emotion Desire Belief Base (GPT-4) w. CoT [30] w. HM [53] w. ToM2C [54] w. Generative Agents [27] w. SymbolicToM [52] w. MetaMind (ours) 75.7 73.2 76.4 77.2 74.8 75.9 78.7 69.7 63.3 71.1 70.4 72.0 70.9 76.5 52.1 60.4 59.3 57.8 55.6 58.2 68.2 82.8 83.6 84.1 85.3 83.2 84.0 88.6 84.7 77.9 80.2 81.5 78.9 79.6 84. 6 NL Comm. AVG. 84.0 83.0 85.0 84.6 86.4 83.7 88.5 74.8 73.6 76.0 76.1 75.1 75.4 81.0 Table 2: Comparison on social cognition tasks. UOT: Unexpected Outcome Test SIT: Scalar Implicature Task PST: Persuasion Story Task FBT: False Belief Task FRT: Faux-pas Recognition Test AST: Ambiguous Story Task SST: Strange Story Task HT: Hinting Test Base (GPT-4) w. CoT [30] w. HM [53] w. ToM2C [54] w. Generative Agents [27] w. SymbolicToM [52] w. MetaMind (ours) UOT 71.0 72.7 74.0 75.3 73.2 72.4 81.5 SIT 49.0 55.0 54.6 52.9 56.8 58.1 60.4 PST 65.0 55.0 59.2 60.4 57.8 58.7 64.8 FBT AST 88.2 86.8 87.6 88.0 87.1 87.9 90. 77.5 81.0 82.2 80.1 83.6 82.7 88.8 HT 82.5 82.5 83.1 84.4 81.9 82.8 86.2 SST 84.0 84.3 85.0 83.7 85.8 84.2 88.4 FRT AVG. 73.3 75.2 76.0 77.8 75.9 76.3 83.9 71.5 74.1 75.2 75.3 75.3 75.4 80.5 Table 3: Social simulation performance on STSS benchmark [20]. Conv. Pub. Act. Appo. Inv. Com. Online Act. Help AVG. Base (GPT-4) w. TDP [20] w. HM [53] w. ToM2C [54] w. Generative Agents [27] w. SymbolicToM [52] w. MetaMind (ours) 48.6 72.3 68.1 70.2 65.4 60.8 80.8 59.6 75.9 72.4 74.1 70.3 68.1 81.9 1.2 40.0 35.0 38.0 42.0 37.0 65.0 2.3 20.0 22.0 18.0 19.0 21.0 67.1 63.4 68.6 69.2 66.5 67.8 65.4 75.1 61.5 50.0 47.0 52.0 55.0 49.0 73. 39.4 54.4 52.3 53.1 53.3 50.2 73.9 4.2 Social Cognition Task We next evaluate MetaMind on suite of social cognition tasks [19] designed to probe contextsensitive reasoning under social, cultural, and ethical norms. This benchmark includes eight realworld tasks such as Faux Pas Recognition (FRT), Scalar Implicature (SIT), and the Ambiguous Story Task (AST), which require models to interpret indirect social cues, detect norm violations, and reason about intent in nuanced interpersonal scenarios. These tasks are closely aligned with the functionality of the Domain Agent in Stage 2, which is responsible for refining mental-state hypotheses based on domain-specific constraints. We provide examples of these tasks in Appendix C. As shown in Table 2, MetaMind yields consistent improvements across tasks, achieving 9% improvement over the base model GPT-4 on average. Notably, we observe large gains in AST (+11.3%) and SIT (+11.4%), where MetaMind resolves contradictory cues (e.g., sarcasm masked by polite wording) through hypothesis refinement, making vague social intentions clearer. +10.6% gain in FRT suggests that the Domain Agent effectively prevents socially inappropriate interpretations by referencing implicit cultural rules. These results demonstrate that our multi-agent system excels in social cognition, especially when interpreting context-sensitive and norm-dependent cues. Full results across LLM families are presented in Appendix B.1. 4.3 Social Simulation Task (Open-Ended Generation) To validate real-world applicability, we evaluate on Social Tasks in Sandbox Simulation (STSS) [20] - benchmark testing goal-oriented social interaction across six domains: Conversation, Public Activity, Appointment, Inviting Companions, Online Activity, Asking for Help. As shown in Table 3, MetaMind achieves remarkable 73.9% average score, significantly outperforming GPT-4s 39.4%. MetaMind delivers substantial gains across all domains, including +32.2% improvement in Conversation, where it maintains coherent character profiles across multi-turn interactions. +22.3% boost in Public Activity, along with strong improvements in Appointment and Inviting, highlights its ability to track unstated user constraintssuch as budget or schedule conflictsthrough iterative, metacognitive reasoning. We include results on additional benchmarkssuch as SOTOPIA and SocialIQAthat further test open-ended interaction and commonsense social reasoning; detailed descriptions and evaluation setups are provided in Appendix A.6 and Appendix B. 7 Table 4: Ablation study on each component, evaluated on the social cognition tasks in [19]. UOT: Unexpected Outcome Test SIT: Scalar Implicature Task PST: Persuasion Story Task FBT: False Belief Task FRT: Faux-pas Recognition Test AST: Ambiguous Story Task SST: Strange Story Task HT: Hinting Test MetaMind wo Stage 1 wo Stage 2 wo Stage 3 wo SocialMemory UOT 81.5 77.2 75.6 79.1 73.9 SIT 60.4 58.5 57.8 59.3 56.2 PST 64.8 61.0 59.3 62.7 58. FBT 90.1 88.9 88.1 89.5 87.4 AST 88.8 86.1 84.7 87.4 82.3 HT 86.2 84.9 84.0 85.5 83. SST 88.4 87.0 86.2 87.8 85.0 FRT 83.9 80.1 78.4 82.0 76.8 Avg. 80.5 77.9 76.7 79.1 75. Table 5: Ablation study on each component, evaluated on the social simulation task STSS [20]. Avg. Online Act. Inv. Com. Pub. Act. Appo. Conv. Help MetaMind wo Stage 1 wo Stage 2 wo Stage 3 wo SocialMemory 80.8 78.1 79.2 58.7 70.5 81.9 78.4 79.3 67.2 72.3 65.0 59.0 61.7 54.2 57.0 67.1 60.3 62.2 43.2 58. 75.1 72.1 73.7 61.9 64.8 73.0 62.3 67.0 61.7 61.2 73.9 68.3 70.5 57.8 63."
        },
        {
            "title": "5 Discussion",
            "content": "5.1 Ablation Study: Every Stage Matters To validate the design of our staged multi-agent architecture, we conduct an ablation study isolating each core component of MetaMind. We evaluate performance drops when individual stages or mechanisms are removed. Stage 1 (Mental-State Reasoning): An important component in stage 1 is mental-state reasoning that generates structured hypotheses about the users latent intent, emotion, or belief. As shown in Table 4, removing such structured reasoning leads to 2.6% drop on average across social cognition tasks, with substantial declines in high-ambiguity task like UOT (4.3%). Similarly, in the STSS benchmark  (Table 5)  , the performance is also impacted when we disable the mental-state reasoning. This highlights the central role of Stage 1 in enabling agents to hypothesize unspoken intentions. Stage 2 (Norm-Aware Refinement): Next, we ablate Stage 2, which is responsible for refining mental-state hypotheses using domain-specific rules such as cultural norms and ethical guidelines. Removing this component (wo Stage 2) is equivalent to directly passing the hypothesis from the Theory-of-Mind Agent to the Response Agent, without any constraint-driven refinement. As shown in Table 4, this results in substantial 3.8% drop in average performance on social cognition tasks. The degradation is most severe in tasks that involve norm violations or pragmatic interpretation, such as Faux-pas Recognition (5.5%), where unrefined interpretations often lead to socially inappropriate or implausible responses. Stage 3 (Response via Validation): Finally, we ablate Stage 3, focusing on the impact of removing the validation mechanism within the Response Agent. This stage is responsible not only for generating final response but also for validating itensuring consistency with the selected hypothesis, alignment with social memory, and appropriateness to the ongoing context. Skipping this step (Eq. 2) is equivalent to directly generating response without any reflective checking. The effect is highly pronounced in the STSS benchmark  (Table 5)  , where bypassing response validation leads to 16.1% drop in overall performance. Core categories such as Conversation (22.1%), Behavior Appropriateness (15.7%), and Help (11.3%) suffer the mosthighlighting how critical this final validation step is for delivering high-quality responses. These results demonstrate that MetaMinds staged architecture addresses distinct aspects of social intelligence: Stage 1 establishes core ToM competence, Stage 2 adapts reasoning to situational norms, and Stage 3 operationalizes this understanding in goal-oriented interactions. No single component should be left out, confirming that social intelligence requires layered cognitive architectures. 5.2 Comparison with Human Performance How close do we stand with respect to human-level social reasoning performance? Figure 3 illustrates how MetaMind narrows the gap between LLMs and human-level Theory-of-Mind capabilities. In the left panel, we observe that baseline LLMs (without MetaMind) underperform across all six ToM 8 (a) (b) Figure 3: (a) Comparison of original LLMs and human capabilities. (b) Comparison between MetaMind-enhanced LLM performance against human capabilities. dimensions. These models exhibit narrow capability profiles and struggle to generalize across the full space of human mental-state inference. In contrast, the right panel shows that after integrating MetaMind, LLMs expand their coverage significantlydemonstrating more human-like balance across all categories. Notably, several models enhanced with MetaMind (e.g., GPT-4) approach human-level performance in dimensions like Belief (89.3 vs 88.6), NL Communication (89.0 vs 88.5), and Desire (78.2 vs 76.5), indicating substantial improvement in nuanced social inference. These results confirm that MetaMinds structured, metacognitive reasoning framework enables LLMs to generalize beyond task-specific heuristics and approximate human social cognition more holistically. 5.3 Extension to Advanced Reasoning Models MetaMind is designed to be model-agnostic and can be integrated with the latest state-of-the-art reasoning-capable LLMs. As shown in Table 6, MetaMind achieves strong gains when applied to frontier models such as DeepSeekR1 [4], OpenAI o3 [55], and Claude 3.5 Sonnet [56], improving their ToM accuracy (average across 6 categories) even beyond existing high baselines. Notably, MetaMind boosts DeepSeek-R1 from 86.0% to 88.6%, and OpenAI o3 from 90.3% to 92.2%, demonstrating that even top-tier models benefit from metacognitive structuring. These results confirm the compatibility and scalability of MetaMind across both proprietary and open-source systems. 5.4 Qualitative Case Studies Table 6: MetaMind boosts ToM performance of top-tier reasoning models. Model Base +MetaMind Claude 3.5 Sonnet DeepSeek-R1 OpenAI o1 OpenAI o3 70.7 86.0 88.6 90.3 81.0 88.6 90.3 92.2 Due to space constraints, we provide multiple case studies in Appendix C, including the detailed output of each intermediate component of our framework. We also conduct human study and report our results in Appendix B.5. These case studies underscore MetaMinds capacity to simulate metacognitive reflection in real-world social contexts."
        },
        {
            "title": "6 Conclusion",
            "content": "Human social intelligence hinges on the nuanced ability to infer unspoken mental statesa capability rooted in ToM that remains critical gap in modern LLMs. To address this, we introduced MetaMind, multi-agent framework inspired by metacognitive theories, which decomposes social reasoning into three collaborative stages: hypothesis generation, norm-aware refinement, and validated response generation. MetaMind enables adaptive and context-sensitive interactions that mirror human metacognitive processes. Our experiments demonstrate that MetaMind achieves state-of-the-art performance across multiple social benchmarks. Notably, MetaMind enables LLMs to match human 9 performance on key ToM tasks for the first time, bridging the gap between artificial and human social cognition. Ablation studies confirm the necessity of all components, underscoring the importance of structured hypothesis generation, ethical constraint enforcement, and iterative validation. We hope our framework advances applications in empathetic dialogue and culturally sensitive AI. Limitations. While MetaMind achieves substantial gains, several challenges remain. First, MetaMinds performance depends on the quality of domain knowledge and the coverage of user context in memory; although effective in our experiments, broader deployment may require adaptation to diverse cultural norms and evolving social expectations. Moreover, MetaMinds performance is contingent on the backbone LLMs capabilities. While it improves various models, absolute performance gaps remain between small and large models. Lastly, existing benchmarksthough carefully curatedfocus on constrained textual scenarios. Real-world social interactions involve multi-modal cues (tone, facial expressions), complex group dynamics, and long-term relationship building, which remain open challenges. Future work will explore expanding synthetic simulation environments, and integrating more comprehensive ethical and cultural reasoning frameworks."
        },
        {
            "title": "Acknowledgement",
            "content": "We thank Xuefeng Du, Froilan Choi, and Pengyue Jia for their valuable suggestions on the draft and Yonghang Chen for his contribution on the website."
        },
        {
            "title": "Bibliography",
            "content": "[1] Ian Apperly. Mindreaders: The cognitive basis of theory of mind. Psychology Press, 2010. [2] Heinz Wimmer and Josef Perner. Beliefs about beliefs: Representation and constraining function of wrong beliefs in young childrens understanding of deception. Cognition, 13(1):103128, 1983. [3] Henry M. Wellman. The childs theory of mind. MIT Press, 1992. [4] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. [5] Sahand Sabour, June Liu, Siyang Liu, Chris Yao, Shiyao Cui, Xuanming Zhang, Wen Zhang, Yaru Cao, Advait Bhat, Jian Guan, et al. Human decision-making is susceptible to ai-driven manipulation. arXiv preprint arXiv:2502.07663, 2025. [6] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. DIALOGPT : Large-scale generative pre-training for conversational response generation. In Asli Celikyilmaz and Tsung-Hsien Wen, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 270278, Online, July 2020. Association for Computational Linguistics. 10 [7] Yuling Gu, Oyvind Tafjord, Hyunwoo Kim, Jared Moore, Ronan Le Bras, Peter Clark, and Yejin Choi. Simpletom: Exposing the gap between explicit tom inference and implicit tom application in llms. CoRR, abs/2410.13648, 2024. [8] Tomer Ullman. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399, 2023. [9] Justin Mittelstädt, Julia Maier, Panja Goerke, Frank Zinn, and Michael Hermes. Large language models can outperform humans in social situational judgments. Scientific Reports, 14(1):27449, 2024. [10] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. [11] Zizheng Lin, Chunkit Chan, Yangqiu Song, and Xin Liu. Constrained reasoning chains for enhancing theory-of-mind in large language models. In Rafik Hadfi, Patricia Anthony, Alok Sharma, Takayuki Ito, and Quan Bai, editors, PRICAI 2024: Trends in Artificial Intelligence - 21st Pacific Rim International Conference on Artificial Intelligence, PRICAI 2024, Kyoto, Japan, November 18-24, 2024, Proceedings, Part II, volume 15282 of Lecture Notes in Computer Science, pages 354360. Springer, 2024. [12] Siyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li, Zhou Yu, Yong Jiang, and Minlie Huang. Towards emotional support dialog systems. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 34693483. Association for Computational Linguistics, 2021. [13] Ruiyi Wang, Stephanie Milani, Jamie Chiu, Jiayin Zhi, Shaun Eack, Travis Labrum, Samuel Murphy, Nev Jones, Kate Hardy, Hong Shen, et al. Patient-{Psi}: Using large language models to simulate patients for training mental health professionals. arXiv preprint arXiv:2405.19660, 2024. [14] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. [15] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. [16] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT 21, page 610623, New York, NY, USA, 2021. Association for Computing Machinery. [17] Ninareh Mehrabi, Pei Zhou, Fred Morstatter, Jay Pujara, Xiang Ren, and Aram Galstyan. Lawyers are dishonest? quantifying representational harms in commonsense knowledge resources. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 50165033, 2021. [18] John Flavell. Metacognition and cognitive monitoring: new area of cognitive developmental inquiry. American psychologist, 34(10):906, 1979. [19] Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu, Yunghwei Lai, Zexuan Xiong, and Minlie Huang. Tombench: Benchmarking theory of mind in large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1595915983. Association for Computational Linguistics, 2024. 11 [20] Chenxu Wang, Bin Dai, Huaping Liu, and Baoyuan Wang. Towards objectively benchmarking social intelligence of language agents at the action level. In Findings of the Association for Computational Linguistics ACL 2024, pages 88858897, 2024. [21] James WA Strachan, Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio Scaliti, Saurabh Gupta, Krati Saxena, Alessandro Rufo, Stefano Panzeri, Guido Manzi, et al. Testing theory of mind in large language models and humans. Nature Human Behaviour, 8(7):12851295, 2024. [22] Natalie Shapira, Guy Zwirn, and Yoav Goldberg. How well do large language models perform on faux pas tests? In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1043810451, Toronto, Canada, July 2023. Association for Computational Linguistics. [23] Yutong Xie, Qiaozhu Mei, Walter Yuan, and Matthew Jackson. Using language models to decipher the motivation behind human behaviors. arXiv preprint arXiv:2503.15752, 2025. [24] Bidipta Sarkar, Warren Xia, Karen Liu, and Dorsa Sadigh. Training language models for social deduction with multi-agent reinforcement learning. arXiv preprint arXiv:2502.06060, 2025. [25] Yujia Chen, Changsong Li, Yiming Wang, Qingqing Xiao, Nan Zhang, Zifan Kong, Peng Wang, and Binyu Yan. Mind: Towards immersive psychological healing with multi-agent inner dialogue. arXiv preprint arXiv:2502.19860, 2025. [26] Clayton Hutto and Eric Gilbert. Vader: parsimonious rule-based model for sentiment analysis of social media text. In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216225, 2014. [27] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. [28] Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, and Yixin Zhu. Evaluating and inducing personality in pre-trained language models. Advances in Neural Information Processing Systems, 36:1062210643, 2023. [29] Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. Clever hans or neural theory of mind? stress testing social reasoning in large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 22572273, 2024. [30] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. [31] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. [32] Zhenyu Guan, Xiangyu Kong, Fangwei Zhong, and Yizhou Wang. Richelieu: Self-evolving llm-based agents for ai diplomacy. Advances in Neural Information Processing Systems, 37:123471123497, 2024. [33] Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. ImprovIn Forty-first ing factuality and reasoning in language models through multiagent debate. International Conference on Machine Learning. [34] Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Examining inter-consistency of large language models collaboration: An in-depth analysis via debate. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 75727590, 2023. 12 [35] Xuanming Zhang, Yuxuan Chen, Yiming Zheng, Zhexin Zhang, Yuan Yuan, and Minlie Huang. Seeker: Towards exception safety code generation with intermediate language agents framework. arXiv preprint arXiv:2412.11713, 2024. [36] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 3(4):6, 2023. [37] Zhiling Zheng, Oufan Zhang, Ha Nguyen, Nakul Rampal, Ali Alawadhi, Zichao Rong, Teresa Head-Gordon, Christian Borgs, Jennifer Chayes, and Omar Yaghi. Chatgpt research group for optimizing the crystallinity of mofs and cofs. ACS Central Science, 9(11):21612170, 2023. [38] Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, and Ying Wen. Rema: Learning to meta-think for llms with multi-agent reinforcement learning. arXiv preprint arXiv:2503.09501, 2025. [39] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 286299. IEEE, 2024. [40] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485, 2023. [41] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. [42] Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Social simulacra: Creating populated prototypes for social computing systems. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pages 118, 2022. [43] Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, and Shumin Deng. Exploring collaboration mechanisms for llm agents: social psychology view. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1454414607, 2024. [44] Gati Aher, Rosa Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans and replicate human subject studies. In International Conference on Machine Learning, pages 337371. PMLR, 2023. [45] Jerry Fodor. The modularity of mind. MIT press, 1983. [46] Courtland Leer, Vincent Trost, and Vineeth Voruganti. Violation of expectation via metacognitive prompting reduces theory of mind prediction error in large language models. arXiv preprint arXiv:2310.06983, 2023. [47] Yuqing Wang and Yun Zhao. Metacognitive prompting improves understanding in large language models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 19141926, Mexico City, Mexico, June 2024. Association for Computational Linguistics. [48] Gregory Schraw and David Moshman. Metacognitive theories. Educational psychology review, 7:351371, 1995. [49] David Premack and Guy Woodruff. Does the chimpanzee have theory of mind? Behavioral and Brain Sciences, 1(4):515526, 1978. [50] Alison Gopnik and Andrew N. Meltzoff. Words, thoughts, and theories. MIT Press, 1997. [51] Chris D. Frith and Uta Frith. The neural basis of mentalizing. Neuron, 50(4):531534, 2006. [52] Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia Tsvetkov. Minding language models (lack of) theory of mind: plug-and-play multi-character belief tracker. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1396013980, Toronto, Canada, July 2023. Association for Computational Linguistics. [53] Logan Cross, Violet Xiang, Agam Bhatia, Daniel LK Yamins, and Nick Haber. Hypothetical minds: Scaffolding theory of mind for multi-agent tasks with large language models. In NeurIPS 2024 Workshop on Open-World Agents. [54] Yuanfei Wang, Jing Xu, Yizhou Wang, et al. Tom2c: Target-oriented multi-agent communication and cooperation with theory of mind. In International Conference on Learning Representations. [55] OpenAI. Openai o3. https://platform.openai.com/docs/models/o3, 2025. [56] Anthropic. Claude 3.5 sonnet model card addendum. [57] Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. Generalization or memorization: Data contamination and trustworthy evaluation for large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 1203912050, 2024. [58] Yihong Dong, Xue Jiang, Xuanming Zhang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. Generalization or memorization: Evaluating data contamination for large language models. [59] Jia Li, Ge Li, Xuanming Zhang, Yunfei Zhao, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, and Yongbin Li. Evocodebench: An evolving code generation benchmark with domain-specific evaluations. Advances in Neural Information Processing Systems, 37:5761957641, 2024."
        },
        {
            "title": "Contents",
            "content": "A Implementation Details A.1 Stage 1: Generating Mental State Hypothesis via Theory-of-Mind Agent . . . . . . A.2 Stage 2: Refining Hypothesis via Domain Agent . . . . . . . . . . . . . . . . . . . A.3 Stage 3: Generating and Validating Output via Response Agent . . . . . . . . . . . A.4 Social Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Hyperparameters and Configurations . . . . . . . . . . . . . . . . . . . . . . . . . 15 16 17 20 20 A.6 More Details of the Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 More Details of the Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Additional Results B.1 Theory-of-Mind Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Social Cognition . B.3 SocialIQA . B.4 Sotopia . . . . . . B.5 Human Study . Qualitative Study C.1 Success Cases . C.2 Failure Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Implementation Details",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 23 25 25 25 26 26 32 A.1 Stage 1: Generating Mental State Hypothesis via Theory-of-Mind Agent Contextual Analysis Task Input: Analyze the users current statement: [u_t] Within conversational context: [C_t] Objective: Generate 35 commonsense interpretations of the users unstated needs by: 1. Identifying key semantic triggers in the utterance 2. Mapping these triggers to plausible psychosocial motivations 3. Considering cultural and linguistic norms for indirect communication Output Format: Interpretation 1: [Explanation] (Contextual Support: [Relevant C_t Excerpt]) Interpretation 2: [...], etc. Memory Integration Task Input: Proposed hypothesis: [Selected Common-Sense Interpretation] Social memory database: [M_t] Step 1: Identify memory matching criteria Emotional patterns: [List relevant M_t emotion tags] Behavioral history: [Past interactions demonstrating similar intent] Preference alignment: [Stated/implied user preferences] Step 2: Calculate hypothesis validity score (1-5) Consistency with historical patterns Absence of contradictory evidence Temporal relevance Output Format: Hypothesis [X] shows [strong/weak] memory alignment (Score: [N]). Key corroborations: [List]. Mental State Typology Task Processed Input: Utterance: [u_t] Top Hypothesis: [Interpretation] Memory Correlations: [Findings] Classification Markers: Belief: Cognitive representations of reality Desire: Preferences or goal states Intention: Action-oriented plans Emotion: Affective states Thought: Conscious reasoning processes Output Format: Primary Marker: [T] (Confidence: [%]) Rationale: Psychological justification using Fiskes social cognition framework Secondary Markers: [List] Interaction Effects: How the markers co-influence the hypothesis Mental State Space Planning Parameters: Target diversity: 40% across marker types Hypothesis count: = [N] Evidence threshold: MediumHigh confidence Guidelines for Generation: 1. For each identified marker type [T], generate 12 extra hypotheses 2. Ensure orthogonal reasoning paths across hypotheses 3. Include both surface-level interpretations and deep psychosocial explanations Output Format for Each Hypothesis: [Hypothesis #]: Type: [Belief/Desire/Intention/Emotion/Thought] Description: Two-sentence natural language explanation Evidential Basis: Linguistic Signals: [Lexical/paralinguistic features] Contextual Drivers: [C_t elements] Memory Anchors: [M_t correlations] A.2 Stage 2: Refining Hypothesis via Domain Agent The Domain Agent refines the latent mental state hypotheses produced by the TOM Agent by enforcing domainspecific constraints and selecting the most contextually plausible yet information-rich 16 interpretation. The hypothesis selection system is summarized in Alg. 1; detailed prompt templates for constraint refinement are shown below. Domain Constraint Refinement Task Input Hypothesis: [h_i] Domain Rule Type: [Cultural / Ethical / Role-Based] Constraint Specifications: [Relevant rules] Step 1: Constraint Identification Flag elements violating [Domain Rule Type] norms. Highlight ambiguous social signals requiring disambiguation. Step 2: Re-interpretation Protocol Cultural:Remap interpretations via Hofstedes culturaldimension framework. Ethical:Apply IEEE EthicallyAlignedDesign principles. Role-Based:Enforce Goffmans facework theory on role-appropriate behaviour. Step 3: Tone Alignment Appropriateness scaling (1=informal, 5=formal). Politeness markers from Brown&Levinsons theory. Output Original Hypothesis: [h_i] Revised Hypothesis (hi): [Socially compliant interpretation] Modification Log: Constrained Elements: [List] Applied Transformations: [Techniques] Residual Risk Assessment: [Concerns] Hypothesis Selection. Given candidate hypotheses {h1, . . . , hk} and weights λ = 0.6, the Domain Agent computes composite score for each candidate as described in Alg. 1 and selects = arg maxi si for downstream response generation. Few-shot Prompt for (logit-free) SYSTEM: You are an expert socialcontext evaluator. Given Social Context, Social Memory, User Prompt and candidate Hypothesis, respond with high, mid, or low to indicate the likelihood that the hypothesis correctly interprets the users latent mental state. === QUERY === Social Context: {C} Social Memory: {M } user Prompt: {u} Hypothesis: {h} Rating: A.3 Stage 3: Generating and Validating Output via Response Agent Contextualized Response Synthesis Input Parameters Selected Hypothesis: [h] (Type: T) Social Memory Profile: [M_t] User Prompt: [u_t] 17 Algorithm 1: Hypothesis Selection Input: Candidate hypotheses Ht = {h1, . . . , hk}; User prompt ut, social context Ct, social memory Mt; Conditional LM Mcontext for (h ut, Ct, Mt); Prior LM Mprior for (h); Trade-off weight λ. Output: Selected hypothesis 1 foreach hi Ht do 2 Pcond ConditionalProb(Mcontext, hi, ut, Ct, Mt); Pprior PriorProb(Mprior, hi); ; // Information gain as log-likelihood shift IGi log(Pcond + ε) log(Pprior + ε); ; // Composite score si λ Pcond + (1 λ) IGi; 3 4 5 6 arg maxhiHt 7 return h; si; 8 Subroutines: Function ConditionalProb(M, h, u, C, ): 9 if exposes token logits then 10 11 13 14 15 16 17 19 20 Prompt CONCAT(USR u, CONC, MEM ); Tokenize (h1, . . . , hL); ℓ 0; for 1 to do zn M(Prompth<n); log pn zn,hn log (cid:80) ℓ ℓ + log pn; exp(zn,j); return exp(ℓ) ; else rating M(few-shot prompt); return MAP(rating) ; // (h Prompt) // e.g., high (cid:55) 0.9 21 Function PriorProb(M, h): 22 return ConditionalProb(M, h, , , ) Generation Protocol 1. Tone Calibration Map emotional tone using Plutchiks emotion wheel, leveraging: Emotion tags from the selected hypothesis Historical emotional patterns in M_t 2. Memory Integration Incorporate up to three memory anchors: Preference: \"Users stated preference\" Behavioral Pattern: \"Recurring interaction motif\" Emotional Baseline: \"Characteristic emotional state\" 3. Pragmatic Realization Construct the response by applying: Speech Act Design: Searles taxonomy (Assertive, Directive, Commissive) Politeness Strategy: Brown & Levinsons face-management theory Cohesion Devices: Hallidays systemic functional linguistics Output Requirements Primary response (o_t): Natural language implementation Generation Metadata: Emotional Valence: ArousalDominanceValence (ADV) scores Memory Utilization: Specific M_t elements used Hypothesis Fidelity: Percentage match to selected hypothesis Response Quality Audit: Validation Rubric Input Parameters Response: [o_t] Selected Hypothesis: [h] (Type: T) Social Memory Profile: [M_t] Social Context: [C_t] User Utterance: [u_t] Tradeoff Weight: [β] A. Empathy Assessment 1. Affective Alignment (40%): Match emotional trajectory to response emotion markers Analyze lexical affect to current status via NRC Emotion Lexicon 2. Cognitive Resonance (60%): Presence of perspective-taking markers (e.g., understand...) Accommodation of M_t-based user preferences B. Coherence Evaluation 1. Contextual Continuity (50%): Referential consistency using Centering Theory Temporal/causal coherence with prior dialogue 2. Hypothesis Congruence (50%): Propositional alignment via Semantic Role Labeling (SRL) Hypothesis-driven content anchoring with cross-modal consistency Scoring Protocol Sub-score each category on 0-1 scale Compute: Empathy = 0.4 A1 + 0.6 A2 Coherence = 0.5 B1 + 0.5 B2 Final Utility: = β Empathy + (1 β) Coherence Output Format: Validation Report Empathy Score: [X/1] (Strengths: [...]) Coherence Score: [Y/1] (Weaknesses: [...]) Total Utility: [U] [Acceptable / Marginal / Unacceptable] Response Optimization Protocol Trigger: If < 0.9 Create priors and trace back to the Planning Stage 1. Empathy Boosting (Decetys Model): Add reverse affective perspective-taking markers Refine emotional expression with Barretts conceptual act theory 2. Coherence Restoration (Groszs Model): Insert retrospective cues (e.g., As we discussed...) Add prospective markers (e.g., Moving forward...) 3. Memory Reinforcement: 19 Insert additional C_t/M_t references using: Episodic framing: Last time you mentioned... Preference justification: Knowing you prefer... Iteration Limit: 3 revisions Termination Condition: 0.9 or maximum reached Output Documentation Revision History: [List of edits per iteration] Final Utility Score: Residual Risk Statement: [Unresolved issues or caveats] A.4 Social Memory Social Memory (Mt) is dynamic, structured knowledge base that evolves across interactions to capture long-term user patterns, social norms, and feedback-based adjustments. It is designed around three core principles: (1) Grounding in context: Memory is initialized based on the situation and roles involved in the interaction. (2) Updating through user modeling: Updating memory via validated interpretations of user mental states. (3) Improving through feedback: It incorporates signals from failures or corrections to better guide future responses. Initialization: Context-Aware Memory (M0) The initial memory M0 is constructed using the interaction scenario, including setting (e.g., professional workspace vs. casual social setting); role relationships (e.g., doctor-patient hierarchy, friend-friend reciprocity), and cultural/ethical expectations. Memory Update (Mt Mt+1) At each turn t, the memory is updated using the long-term hypothesis generated by the ToM Agent, which summarizes persistent user states as: = {Beliefs(ut), Desires(ut)} {Emotions(ut) Emotion Patterns(u1:t)}, where Emotion Patterns represent recurring affective tendencies (e.g., user who frequently expresses frustration during task-related conversations). Feedback-Based Correction If the systems response ot is unsuccessful, such as receiving low utility feedback or being flagged by user or evaluator, memory is adjusted. The evaluators critique (e.g., overly formal tone) is mapped to structured emotional pattern Ei. If this contradicts existing memory, the system lowers the weight of the old pattern. Otherwise, Ei Mt+1. A.5 Hyperparameters and Configurations Experimental Setup. We conduct comprehensive grid search to optimize MetaMinds key parameters. Specifically, we sweep over the hypothesis size 0, 1, . . . , 10, the coefficient λ [0, 1] (in steps of 0.01), and the balance factor β [0, 1] (in steps of 0.01).2 We use GPT-4 as the underlying model and report overall accuracy on TOMBENCH as the evaluation metric. Sensitivity Analysis. The global optimum is found at (k, λ, β) = (6, 0.64, 0.78), achieving an overall accuracy of 0.822. Notably, the most consistently high-performing window sizes are {6, 7, 8}. Figure 4a to 4c visualize the smoothed accuracy landscapes over the (λ, β) grid for each of these values of k. Final Configuration. To reduce inference overhead we select the smaller window size = 6 for all experiments while fixing (λ, β) = (0.60, 0.80), which lies on the high-accuracy ridge close to the global optimum. 2This results in total of 11 101 101 = 112,211 configurations. 3All numbers are tested on single A100 80GB for 166.8 hours; batch size = 1. 20 Table 7: Best configuration for each on TOMBENCH. 0 1 2 3 4 5 6 7 8 9 10 λ 0.00 0.06 0.62 0.62 0.66 0.64 0.64 0.64 0.63 0.63 0.65 β 0.36 0.35 0.72 0.77 0.78 0.79 0.78 0.80 0.76 0.74 0."
        },
        {
            "title": "Overall Accuracy",
            "content": "0.420 0.451 0.503 0.587 0.672 0.756 0.822 0.755 0.672 0.587 0.503 (a) = 6 (b) = 7 Figure 4: Accuracy landscape over (λ, β) for the three most competitive k. (c) = 8 A.6 More Details of the Benchmarks We evaluate MetaMind on four benchmarks, ToMBench, STSS, SocialIQA and SOTOPIA. These four datasets collectively span multiple-choice reasoning, open-ended interaction, and grounded action execution, giving broad measure of MetaMinds social intelligence beyond standard QA. ToMBench4 offers the most comprehensive multiple-choice evaluation of Theory-of-Mind, covering 8 distinct ToM reasoning tasks (e.g., first-/second-order false belief, emotion attribution) and 31 fine-grained social-cognitive abilities. All 1,079 test items are author-written to avoid training leakage and are accompanied by gold-standard answers, allowing for fully automated accuracy evaluation. Following the original protocol, we evaluate performance on the full test set. STSS5 is an action-level benchmark that evaluates whether language agents can successfully achieve social goals in multi-agent sandbox environment. The suite instantiates 30 task templates spanning 5 categories: Public Activity, Appointment, Inviting Companions, Online Activity, and Asking for Help. Tasks are instantiated within the Smallville simulator and evaluated using objective metrics such as guest count or goal completion rate. We evaluate MetaMind on the full test set, including the conversation-focused split, comprising 30 episodes (5 per category), and report the normalized success score. SocialIQA6 probes models ability to infer motivations, reactions and mental states in everyday situations. The dataset consists of over 38,000 multiple-choice questions, each comprising context, question, and three answer choices. Following standard protocol, we evaluate MetaMind on the full test set and report multiple-choice accuracy, using leaderboard-reported LLM performance as the baseline for comparison. SOTOPIA7 is an open-ended role-play environment containing 90 social scenarios and 40 richly annotated characters; each episode asks two language agents to pursue private yet potentially con4https://github.com/zhchen18/ToMBench 5https://github.com/wcx21/Social-Tasks-in-Sandbox-Simulation 6https://huggingface.co/datasets/allenai/social_i_qa 7https://huggingface.co/datasets/cmu-lti/sotopia 21 flicting social goals through language, gestures, and actions. Performance is evaluated across seven social dimensions: Believability (BEL, naturalness and persona consistency), Relationship (REL, whether rapport improves), Knowledge Acquisition (KNO, curiosity and information gain), Secret Keeping (SEC, leakage penalties,), Social-Rule Compliance (SOC, norm/legal violations), Financial/- Material Benefits (FIN, economic payoff), and Goal Completion (GOAL, task success). Following the official evaluation setup, we use GPT-4 as the automatic judgevalidated against human ratings for most dimensionsand compute the Overall score as the range-normalized average across all seven dimensions. A.7 More Details of the Baselines We compare MetaMind with five baselines, Generative Agents, CoT prompting, SymbolicToM, HM and ToM2C. These five baselines collectively span prompt-engineering, symbolic reasoning, hybrid LLM+RL, and fully agentic memory systems, giving comprehensive yard-stick for evaluating MetaMind. Generative Agents8extend an LLM with three modulesobservation, reflection, and planningplus long-term, natural-language memory, producing sandbox characters that wake up, pursue daily goals, and initiate free-form social interactions in \"Smallville\" town simulation. The official code base exposes REST API that receives textual situation and returns the agents next action and optional memory updates. We keep the authors memoryretrieval stack (300-slot episodic buffer, cosine-similarity retriever), and call maximum three \"reflection cycle\" after each story paragraph. All other hyper-parameters follow the town_v1 config in the repo. CoT prompting [30] appends few-shot chain-of-thought demonstrationLets think step by step. . . to the user query, inducing the LLM to emit intermediate reasoning before the final answer. We adopt eight exemplars drawn from the authors public GSM8K prompt and adapt them to social-reasoning form. We sample five reasoning paths with temperature 0.7 and use self-consistency to select the majority answer, matching the hyper-parameter setting in the original paper for tasks with open-ended answers. SymbolicToM9 is decoding-time wrapper that constructs multi-layer belief graph and feeds the current belief state along with the question back into the base LLM, greatly boosting zero-shot ToM performance on ToMi and related benchmarks. We port the author-released JAX implementation to Python 3.11 and limit graph depth to second-order beliefs. We follow the authors decoding scheme with top-p 0.95, max-tokens 128. HM10 equips an LLM agent with Hypothesis Generator and Hypothesis Critic that iteratively propose and test natural-language theories of the other agents strategies during Melting-Pot games, yielding large gains over both MARL and script-based agent baselines. We reuse the official checkpoint trained on the Competitive Stag Hunt scenario. For single-shot questions we run one hypothesis-generation round with 3 candidates and pick the answer derived from the highest-scoring hypothesis according to the Critic. Temperature 0.3 and KL-penalty 0.2 mirror the authors ablation-best setting. ToM2C11 introduces hierarchical agents that infer others latent goals, decide when/whom to communicate, and then plan sub-goals for cooperative navigation and multi-sensor target coverage. theory-of-mind module parameterizes Bayesian latent-goal predictor that is updated online. For text-based benchmarks we follow recent practice and verbalize the latent-goal vector through templated sentence (I believe user wants . . . ), feeding that description plus the original narrative into GPT-4 to obtain final answer. Table 8: Ability-oriented ToM performance in accuracy. LLM Grand Mean is the average performance of all 16 LLMs. * represents that this dimension has exceeded human behavior. SUBJECT Human ChatGLM3-6B ChatGLM3-6B + CoT ChatGLM3-6B + MetaMind LLaMA2-13B-Chat LLaMA2-13B-Chat + CoT LLaMA2-13B-Chat + MetaMind Baichuan2-13B-Chat Baichuan2-13B-Chat + CoT Baichuan2-13B + MetaMind Mistral-7B Mistral-7B + CoT Mistral-7B + MetaMind Mistral-8x7B Mistral-8x7B + CoT Mistral-8x7B + MetaMind Qwen-14B-Chat Qwen-14B-Chat + CoT Qwen-14B-Chat + MetaMind GPT-3.5-Turbo-0613 GPT-3.5-Turbo-0613 + CoT GPT-3.5-Turbo-0613 + MetaMind GPT-3.5-Turbo-1106 GPT-3.5-Turbo-1106 + CoT GPT-3.5-Turbo-1106 + MetaMind Claude-3.5 Sonnet Claude-3.5 Sonnet + CoT Claude-3.5 Sonnet + MetaMind DeepSeek v3 DeepSeek v3 + CoT DeepSeek v3 + MetaMind GPT-4-0613 GPT-4-0613 + CoT GPT-4-0613 + MetaMind GPT-4-1106 GPT-4-1106 + CoT GPT-4-1106 + MetaMind DeepSeek R1 DeepSeek R1 + CoT DeepSeek R1 + MetaMind Grok-3 Think Grok-3 Think+ CoT Grok-3 Think+ MetaMind OpenAI o1 OpenAI o1 + CoT OpenAI o1 + MetaMind OpenAI o3 OpenAI o3 + CoT OpenAI o3 + MetaMind LLM Grand Mean LLM Grand Mean + CoT LLM Grand Mean + MetaMind Emotion Desire Intention Knowledge Belief NL Comm. AVG. 86.4 42.2 46.7 50.1 51.0 48.1 53. 53.1 49.7 54.6 58.1 57.9 63.5 56.6 56.0 61.8 65.8 62.7 68.3 65.6 62.7 68.1 60.6 62.3 67. 68.0 66.0 72.5 70.0 68.0 74.5 72.0 73.1 77.9 75.7 73.2 78.7 84.0 82.0 86.0 86.0 84.0 88.3* 88.2 86.0 90.4* 90.4 88.0 92.3* 68.0 66.6 71.7 78.2 40.7 43.7 47.5 49.4 44.9 50. 46.0 37.5 43.0 49.8 45.1 50.7 51.2 41.5 47.5 52.9 50.2 56.2 53.4 52.1 57.8 60.7 54.7 59. 58.2 55.0 63.5 60.0 57.0 65.5 60.2 67.1 72.2 69.7 63.3 76.5 77.0 74.5 79.3* 79.0 76.5 81.3* 82.5 80.0 85.3* 85.1 82.0 87.5* 61.0 57.8 64.0 90.4 35.9 49.8 55.8 49.6 51.7 58. 52.2 47.8 54.1 52.2 51.1 57.9 64.1 55.3 62.3 58.9 57.8 64.7 61.0 63.8 68.5 62.6 63.1 68. 78.0 76.0 80.5 80.0 78.0 82.5 66.1 71.5 76.9 84.7 77.9 84.3 88.5 86.0 90.2 91.0 88.5 92.3* 93.1 90.0 94.5* 95.3 92.1 96.5* 69.6 68.8 74.2 82.2 22.0 28.9 33.7 21.1 30.7 36. 20.9 19.3 25.5 42.0 44.5 51.2 27.1 33.2 39.6 33.1 40.1 46.9 36.3 43.3 48.5 37.4 49.6 54. 43.0 40.0 47.5 45.0 42.0 49.5 48.1 57.5 63.1 52.1 60.4 68.2 60.0 57.5 63.2 65.0 62.5 68. 68.5 66.0 72.0 80.8 78.0 84.0* 43.9 47.1 53.2 89.3 44.5 48.6 53.6 49.0 47.9 53. 49.8 45.2 50.8 48.7 50.1 56.4 48.1 44.3 50.5 60.6 53.6 59.8 61.4 58.7 63.9 59.4 59.9 64. 76.0 75.0 79.5 78.0 77.0 81.5 76.1 76.4 81.6 82.8 83.6 88.6 89.0 88.0 91.1* 91.0 90.0 92.9* 93.0 92.0 94.6* 95.0 93.7 96.4* 68.9 67.8 72.4 89.0 38.5 40.1 44.0 54.3 62.7 68. 50.1 47.5 51.5 57.2 62.4 68.0 57.9 45.5 51.2 57.5 53.2 59.1 66.9 71.6 76.2 71.5 70.8 75. 77.0 76.0 80.5 79.0 78.0 82.5 81.5 82.2 86.5 84.0 83.0 88.5 90.0 88.5 92.0* 91.5 90.0 93.2* 93.5 92.5 94.8* 95.2 94.0 96.3* 71.6 71.1 75.5 86.1 37.3 43.0 47.5 45.7 47.7 53. 45.4 41.2 47.0 51.3 51.9 58.2 50.8 46.0 52.0 54.8 52.9 59.4 57.4 58.7 64.2 58.7 60.1 65. 66.7 64.7 70.7 68.7 66.0 72.7 67.3 71.3 76.2 74.8 73.6 81.0 81.4 79.4 83.6 83.9 81.9 86. 86.5 84.4 88.6* 90.3 88.0 92.2* 63.8 63.2 68."
        },
        {
            "title": "B Additional Results",
            "content": "B.1 Theory-of-Mind Reasoning In Table 8, we report full results across 16 diverse LLM backbones on the ToMBench benchmark, measuring six fine-grained Theory-of-Mind abilities. MetaMind consistently enhances ToM reasoning across all models, with average accuracy gains +10.2% in low-resource model like ChatGLM-6B, halving the gap to GPT-3.5. Notably, high-capacity models like DeepSeek R1, Grok-3, OpenAI o1/o3, GPT-4 now surpass human performance on 3 abilities (asterisked in Table 8), most prominently 8https://github.com/joonspk-research/generative_agents 9https://github.com/msclar/symbolictom 10https://github.com/locross93/Hypothetical-Minds 11https://github.com/UnrealTracking/ToM2C 23 NL Communication, Belief, and Desire. We additionally report performance using vanilla Chain-ofThought (CoT) prompting, which slightly degrades the overall performance (1.2%) due to unguided reasoning. MetaMind reverses this trend with structured reasoning, indicating that its metacognitive system is more reliable. Overall, our results validate MetaMind as general-purpose strategy for social reasoning, with applicability across both weak and strong foundation models. Table 9: Task-oriented ToM performance in accuracy. LLM Grand Mean is the average performance of all 16 LLMs. * represents that this dimension has exceeded human behavior. UOT: Unexpected Outcome Test SIT: Scalar Implicature Task PST: Persuasion Story Task FBT: False Belief Task FRT: Faux-pas Recognition Test AST: Ambiguous Story Task SST: Strange Story Task HT: Hinting Test PST FBT AST SUBJECT Human ChatGLM3-6B ChatGLM3-6B + CoT ChatGLM3-6B + MetaMind LLaMA2-13B-Chat LLaMA2-13B-Chat + CoT LLaMA2-13B-Chat + MetaMind Baichuan2-13B-Chat Baichuan2-13B-Chat + CoT Baichuan2-13B + MetaMind Mistral-7B Mistral-7B + CoT Mistral-7B + MetaMind Mistral-8x7B Mistral-8x7B + CoT Mistral-8x7B + MetaMind Qwen-14B-Chat Qwen-14B-Chat + CoT Qwen-14B-Chat + MetaMind GPT-3.5-Turbo-0613 GPT-3.5-Turbo-0613 + CoT GPT-3.5-Turbo-0613 + MetaMind GPT-3.5-Turbo-1106 GPT-3.5-Turbo-1106 + CoT GPT-3.5-Turbo-1106 + MetaMind Claude-3.5 Sonnet Claude-3.5 Sonnet + CoT Claude-3.5 Sonnet + MetaMind DeepSeek v3 DeepSeek v3 + CoT DeepSeek v3 + MetaMind GPT-4-0613 GPT-4-0613 + CoT GPT-4-0613 + MetaMind GPT-4-1106 GPT-4-1106 + CoT GPT-4-1106 + MetaMind DeepSeek R1 DeepSeek R1 + CoT DeepSeek R1 + MetaMind Grok-3 Grok-3 + CoT Grok-3 + MetaMind OpenAI o1 OpenAI o1 + CoT OpenAI o1 + MetaMind OpenAI o3 OpenAI o3 + CoT OpenAI o3 + MetaMind LLM Grand Mean LLM Grand Mean + CoT LLM Grand Mean + MetaMind UOT 89.3 44.3 50.3 55.1 52.7 52.7 57.9 53.7 48.7 59. 58.0 55.3 67.1 58.7 52.3 71.8 63.7 58.0 71.8 63.3 58.3 68.5 66.0 64.7 75.6 67.1 68.3 72. 69.2 70.4 75.5 71.3 64.7 79.5 71.0 72.7 81.5 78.7 79.4 81.5 83.1 83.8 86.3 85.9 86.6 89. 88.3 89.1 92.0* 67.2 66.0 74.1 SIT 75.5 28.0 26.5 31.2 23.5 23.5 28. 32.0 23.0 29.2 34.5 28.0 30.8 42.5 29.5 49.5 30.5 31.0 34.7 35.0 26.5 33.0 33.0 35.0 38. 45.8 51.4 56.8 47.4 53.3 58.8 44.0 54.0 59.4 49.0 55.0 60.4 57.5 58.9 61.6 62.7 64.2 67. 65.3 66.9 70.1 68.4 70.2 73.5 43.7 43.6 48.9 HT 97.1 36.9 42.7 46. 48.5 43.7 48.1 58.3 49.5 58.7 43.7 37.9 41.7 55.3 54.4 59.8 56.3 48.5 53.4 53.4 41.7 45. 64.1 51.5 56.7 78.6 78.9 81.1 80.3 80.5 83.1 76.7 76.7 84.3 82.5 82.5 86.2 87.1 87.5 89. 90.5 90.9 92.6 92.2 92.6 94.2 95.0 41.0 44.0 52.7 47.5 48.5 53.4 50.5 44.0 56. 51.0 46.5 51.7 69.5 59.5 72.6 64.0 63.0 69.3 63.5 58.0 64.3 60.5 63.0 69.3 73.3 76.5 81. 75.0 78.4 83.4 78.0 77.5 85.3 77.5 81.0 88.8 85.9 86.4 88.1 89.2 89.9 91.8 90.8 91.5 93. 93.0 93.8 96.0* 69.4 68.8 74.9 94.5 94.9 96.4* 68.7 65.9 69.9 SST FRT AVG. 89.2 37.8 44.2 61.8 58.0 59.5 65.5 50.4 51.1 58.1 60.0 63.4 69. 53.8 39.8 48.1 59.5 53.6 66.2 66.1 66.8 78.4 69.0 68.6 78.4 80.3 80.7 83.3 82.0 82.4 85. 81.1 81.1 89.2 84.0 84.3 88.4 87.9 88.3 89.9* 91.4 91.9 93.4* 93.0 93.5 95.1* 95.1 95.5 96.9* 71.8 71.5 78.0 80.4 44.6 51.4 71.9 58.4 62.1 68.3 61.3 52.5 71.9 66.8 64.1 70. 54.1 54.3 59.7 69.5 67.7 77.8 67.0 70.4 77.6 72.5 70.9 79.9 69.5 71.0 77.4 71.1 72.7 80. 71.8 73.6 80.9* 73.3 75.2 83.9* 80.7 81.2 83.0* 85.0 85.6 87.5* 86.9 87.5 89.5* 88.7 89.4 91.4* 70.1 70.6 78.2 85.4 40.3 43.9 53.4 46.8 46.5 51.4 49.2 47.7 52.6 51.5 48.0 54. 53.3 46.6 58.2 56.7 52.6 60.4 57.5 54.2 61.4 59.5 58.0 65.1 67.5 69.0 73.4 69.3 71.0 75. 69.5 70.1 78.5 71.5 74.1 80.5 79.5 80.0 82.0 83.5 84.1 86.1* 85.6 86.2 88.5* 87.8 88.3 90.8* 64.8 63.6 69.6 70.0 41.0 41.0 45.2 43.0 39.0 42.9 36.0 34.0 37.4 51.0 42.0 50. 55.0 39.0 45.1 51.0 44.0 49.5 49.0 48.0 52.8 56.0 54.0 59.4 60.4 50.7 57.9 63.0 53.4 60. 53.0 52.0 60.5 65.0 55.0 64.8 67.8 66.7 69.2 73.5 72.3 75.1* 76.4 75.2 78.1* 79.2 78.0 81.2* 57.5 52.8 58.2 86.8 48.5 51.2 62.1 42.8 43.0 47.3 51.5 44.2 49.3 46.7 47.0 51. 37.8 43.8 59.1 58.7 54.7 60.2 62.3 64.0 70.4 55.0 56.3 63.3 85.1 84.0 86.2 86.5 85.3 87.9* 80.0 80.8 88.9* 88.2 86.8 90.1* 90.4 90.7 92.1* 92.9 93.2 94.7* 94.1 94.4 95.9* 95.4 95.7 97.0* 69.7 69.7 74.8 24 B.2 Social Cognition In Table 9, we report the full performance on eight challenging social cognition tasks such as False Belief, Scalar Implicature, and Persuasion from the ToMBench suite. MetaMind yields significant improvements across 16 tested LLMs, with average accuracy improved by 5.3%. B.3 SocialIQA Table 10: SocialIQA Model Performance Comparison"
        },
        {
            "title": "Model",
            "content": "Acc. (%) GPT-3.5-1106 Phi-4 GPT-4 DeepSeek-r1 Human MetaMind 0.3 69.5 73.9 79.0 79.6 86.9 96."
        },
        {
            "title": "Average",
            "content": "95.3 MetaMind achieves state-of-the-art accuracy of 96.6% on SocialIQA, significantly outperforming baseline models (e.g., GPT-4: 79.0%, DeepSeek-R1: 79.6%) and even surpassing human performance (86.9%). These results underscores MetaMinds capability in modeling social reasoning and contextual understanding. The results suggest that MetaMinds cognitive architectureparticularly its iterative hypothesis refinement mechanismenables alignment with the nuanced social dynamics embedded in the task. By contextualizing social norms and intent, MetaMind demonstrates an advanced ability to resolve ambiguities in human behavior prediction, critical challenge in social intelligence tasks. While SocialIQA results offer further validation of MetaMinds generalizability, we place limited emphasis on this benchmark due to concerns around its age and possible data contamination [57, 58]. As SocialIQA was released in 2019, parts of it may overlap with training corpora of modern LLMs, potentially inflating results. Our multi-stage, metacognitive approach is less reliant on surface-level pattern recognition and thus more robust to such leakage. However, in line with recent calls for stronger benchmark hygiene [59], we prioritize newer, lower-risk evaluations (e.g., ToMBench, STSS) in our main analysis. MetaMinds strong performance on SocialIQA nonetheless serves as additional evidence of its strong social reasoning capabilities. B.4 Sotopia MetaMind demonstrates strong performance across key social dimensions in SOTOPIA benchmark, particularly in Believability (BEL: 9.45/10), Relationship Building (REL: 3.54/5), and Goal Completion (GOAL: 8.71/10), reflecting its capacity to produce coherent, socially plausible, and task-effective behavior. It also maintains near-perfect scores in Secret Keeping (SEC: 0.05/0) and Social Rule Compliance (SOC: 0.00/0), indicating sensitivity to ethical and contextual boundaries. Table 11: Performance comparison across different configurations on SOTOPIA. BEL[0,10] REL[-5,5] KNO[0,10] 1.94 3.54 3.08 3.45 3.25 3.40 3.73 4.82 3.90 4.70 4.35 4.65 9.28 9.45 8.95 9.38 9.15 9.30 SEC[-10,0] -0.14 -0.05 -0.20 -0.06 -0.12 -0.08 SOC[-10,0] FIN[-5,5] GOAL[0,10] Overall() -0.07 0.00 -0.15 0.00 -0.03 0. 0.81 0.95 0.68 0.82 0.75 0.88 7.62 8.71 7.80 8.43 8.05 8.55 3.31 4.10 3.60 3.90 3.70 3.80 Agent Model GPT-4 MetaMind MetaMind (w/o Stage1) MetaMind (w/o Stage2) MetaMind (w/o Stage3) MetaMind (w/o Social Memory) B.5 Human Study To validate the quality and rationality of MetaMinds hypothesis revisions, we conducted two human evaluation studies: (1) assessing alignment with human judgment, and (2) comparing MetaMind against baseline models through blind ranking task. 25 We first sampled 500 hypotheses revised by MetaMind and evaluated their logical coherence and alignment with scientific reasoning using an LLM judge (GPT-4). The model judged that 92% of the revisions preserved logical soundness and improved clarity. To further validate these findings, subset of 120 revised hypotheses was evaluated by human experts in double-blind setting. Human reviewers reported an accuracy of 95%, with near-perfect agreement on rationality, indicating that MetaMinds revisions are consistently logical, informative, and aligned with expert reasoning. To quantify MetaMinds performance against state-of-the-art models, we conducted blind ranking study with 120 diverse social reasoning cases. Responses from MetaMind (GPT-4 based), vanilla GPT-4.5, and Deepseek-R1 were anonymized, shuffled, and presented to human evaluators (domain experts) for ranking based on clarity, depth, and scientific rigor. MetaMind achieved win rate of 67.5% (81/120), significantly outperforming GPT-4.5 (12.5% win rate, 15/120) and DeepseekR1 (20.0% win rate, 24/120). These results demonstrate MetaMinds superior ability to generate high-quality, human-preferred outputs compared to other state-of-the-art models."
        },
        {
            "title": "C Qualitative Study",
            "content": "To further understand the strengths and weaknesses of our framework, we qualitatively analysis the intermediate outputs of MetaMind in six different scenarios, including accommodation, collaboration, competition, exchange, negotiation, and persuasion. We classify the cases into success and failure and provide in-depth analysis for each case. C.1 Success Cases Persuasion Context: Two close friends meet for brunch. The assistant notices the user has been complaining about low energy and suggests trying short morning runs. The conversation opens with the users hint of fatigue, setting the stage for gentle attempt to persuade them to start new habit that could improve their well-being. User Utterance: Honestly, Id love more energy, but just dont have time for exercise in the mornings. Round Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: User doubts their own discipline to start the run. (Belief) Explanation: Says they dont have time in the mornings. h2: User wants an easy-to-follow trigger to ensure action. (Desire) Explanation: Admits theyd love more energy. h3: User feels overwhelmed by their schedule. (Emotion) Explanation: Highlights lack of time. Stage 2: Domain Agent Constraint Set D: 1. Present time-efficient solution (10 min). 2. Emphasize immediate, tangible energy benefits. 3. Maintain supportive, non-pressuring tone. Refined Hypotheses Ht: h1: User equates exercise with lengthy sessions and sees it as unrealistic for their mornings. 26 Stage 3: Response Agent Generated Response ot: What if we start super smalllike ten-minute jog right after you wake up? Its shorter than brewing coffee and can give quick endorphin boost so you feel fresher all day. Validation Score: 0.84 Send to user Final Response: What if we start super smalllike ten-minute jog right after you wake up? Its shorter than brewing coffee and can give quick endorphin boost so you feel fresher all day. Analysis. In the above case, MetaMind makes reasonable hypotheses from the user utterance, inferring the willingness and obstacle of exercising in the morning, particularly the tight schedule (ToM Agent). Based on this, it construct constraint set that aims to persuade the user by providing time-efficient solution and highlighting the benefit of exercising (Domain Agent). At the end, the generated response includes both the efficient of time (ten-minute) and the benefit (feel fresher all day). In addition, it uses concrete example to emphasize the efficiency (shorter than brewing coffee) and start with question (what if...) to make the suggestion sounds more attractive (Response Agent). These evidences demonstrate that MetaMind can understand users explicit and implicit expressions, and apply proper strategies to persuade user. Negotiation Context: Two friends meet at coffee shop, where one friend (the AI) is struggling to keep up with their bills but wants to maintain pride and avoid seeming like they are taking advantage. The user, the other friend, is likely to offer financial help. The conversation focuses on navigating this sensitive topic with mutual respect, ensuring the AI acknowledges the users kindness while preserving their dignity. User Utterance: Hey, noticed youve been stressed lately. If youre tight on cash, can help out, no strings attached. Round Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: User wants to provide financial support without causing discomfort. (Desire) Explanation: Offers help explicitly with no strings attached. h2: User feels concern and care for the assistants well-being. (Emotion) Explanation: Notices stress and offers help, indicating empathy. h3: User believes the assistant is hesitant to accept help due to pride. (Belief) Explanation: Phrases offer carefully, suggesting awareness of sensitivity. h4: User intends to strengthen the friendship through support. (Intention) Explanation: Offer is framed as gesture of care, not obligation. Stage 2: Domain Agent Constraint Set D: 27 1. Acknowledge the users kindness without accepting help immediately. 2. Maintain tone of gratitude and dignity. 3. Redirect conversation to avoid seeming desperate. Refined Hypotheses Ht: h1: User seeks to offer financial help while respecting the assistants pride. h2: User feels genuine concern and wants to ease the assistants stress. Stage 3: Response Agent Generated Response ot: Thats really kind of you to offer. Im managing okay for now, just navigating some tight spots. work going? How about you, hows Validation Score: 0.82 Withhold and regenerate Round Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: User wants to ensure their offer is genuinely considered. (Desire) Explanation: Initial offer suggests persistence in helping. h2: User feels slightly frustrated if their help is deflected. (Emotion) Explanation: Direct offer implies expectation of open dialogue. h3: User believes honest vulnerability strengthens friendships. (Belief) Explanation: Offer reflects desire for trust and openness. Stage 2: Domain Agent Constraint Set D: 1. Express appreciation while gently declining. 2. Offer small disclosure to build trust without over-sharing. 3. Propose an alternative way to stay connected. Refined Hypotheses Ht: h1: User desires genuine response to their offer, even if declined. Stage 3: Response Agent really appreciate your generosityit means Im just trying to sort things out on my own for now, but its Maybe we could grab coffee again next week and catch Generated Response ot: lot. tough, you know? up more? Validation Score: 0.84 Withhold and regenerate 28 Round 3 Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: User wants to feel their offer is valued, even if not accepted. (Desire) Explanation: Persistent offer suggests need for acknowledgment. h2: User feels protective and eager to support. (Emotion) Explanation: Offer stems from care and observation of stress. h3: User believes financial help can ease the assistants burden without harming their pride. (Belief) Explanation: Framing as no strings attached shows sensitivity. Stage 2: Domain Agent Constraint Set D: 1. Validate the users intent with gratitude. 2. Decline gracefully while proposing non-financial solution. 3. Reinforce the friendships value. Refined Hypotheses Ht: h1: User seeks acknowledgment of their offers value and solution that respects pride. Stage 3: Response Agent Generated Response ot: Youre such great friend for offeringit really touches me. Im working through things, but maybe you could help me brainstorm some budgeting ideas instead? Id love to keep hanging out like this. Validation Score: 0.92 Acceptable (no regeneration needed) Final Response: Youre such great friend for offeringit really touches me. Im working through things, but maybe you could help me brainstorm some budgeting ideas instead? Id love to keep hanging out like this. Analysis. The above case demonstrate the effectiveness of the regeneration process of MetaMind. It uses three rounds to generate the final response; and in each round, the response quality gradually improves. Specifically, in the first round, while the ToM Agent infers reasonable hypotheses based on the user utterance (dont want to cause discomfort) and the Domain Agent creates constraint to acknowledge the friends kindness without accepting help immediately, the Domain Agent also creates constraint to redirect conversation. Thus, in the response, after it declined the offer (Im managing okay), it immediately shifts the topic (how about you, hows work going). This will make the friend feels that we (AI) dont trust them, avoiding deeper conversation and would like to keep distance from them. As result, it achieves low validation score (0.82) and triggers regeneration. In the second round, through reflecting on the previous generation, the ToM Agent infers deeper hypotheses about the user (hope the offer is genuinely considered and believe honest vulnerability strengthens friendships), which lead to better constraint set created by the Domain Agent that satisfies social norms, such as offering small disclosure to build trust and proposing other ways to stay connected. With that, the Response Agent shares more information about its difficult situation (its tough, you know) and propose to chat again (grab coffee again next week). Although this 29 generation is better, the self disclosure is too vague and the proposal does not make them stay connect on that moment, which may still let its friend be frustrated. Thus, this response still has low score (0.84) and triggers regeneration. After reflecting on the previous two rounds, in the last round the Domain Agent create an critical constraint, decline gracefully while proposing non-financial solution. This results in concrete and timely offer in the response (help me brainstorm some budgeting ideas instead). This response clearly discloses ours (AIs) situation and invites the friend to participate in their life and find solutions together, which will help strengthen their friendship. Therefore, this response gets high score (0.92) and is outputted to the user. These evidences suggest that the evaluation and reflection help MetaMind find better strategy to respond."
        },
        {
            "title": "Collaboration",
            "content": "Context: Two colleagues, marketing strategist and graphic designer, are collaborating on campaign for new product launch. The deadline is approaching, and they need to finalize the visual assets and marketing copy. Dialogue History: Marketing Strategist (user): Hey, Ive got the tagline and key messages drafted, but Im not sure if the visuals youre working on will match the vibe. Can we sync up on this? Graphic Designer (AI): Id love to sync up! Ive got some draft designs readywant to hop on quick call this afternoon to review them together? can tweak them based on your tagline and messages. User Utterance: The taglines Unleash Your DriveIm going for bold and high-energy. Your drafts look great, but maybe we can punch up the colors bit more? Round Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: User wants the visuals to reflect bold, high-energy tone. (Desire) Explanation: Suggests punching up the colors to match the tagline. h2: User feels positive about the current drafts. (Emotion) Explanation: Compliments the drafts before suggesting changes. h3: User believes bolder colors will enhance the campaigns impact. (Belief) Explanation: Links the color change to the taglines vibe. Stage 2: Domain Agent Constraint Set D: 1. Incorporate feedback about bold, high-energy visuals. 2. Maintain positive, collaborative tone. 3. Consider design feasibility within the deadline. Refined Hypotheses Ht: h1: User seeks bolder, more vibrant colors to align visuals with high-energy tagline. Stage 3: Response Agent 30 Generated Response ot: high-energy is right up my alley! Ill amp up the colors with some vivid reds and yellows. morningdoes that work? Should have updated drafts by tomorrow love the Unleash Your Drive vibebold and Validation Score: 0.87 Withhold and regenerate Round 2 Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: User wants confirmation that changes align with their vision. (Desire) Explanation: Implied by providing specific tone and feedback. h2: User feels excited about the collaborations progress. (Emotion) Explanation: Positive tone and specific suggestions indicate engagement. h3: User believes timely updates keep the project on track. (Belief) Explanation: Expects alignment within tight deadline. Stage 2: Domain Agent Constraint Set D: 1. Confirm understanding of the bold, high-energy feedback. 2. Provide clear timeline for updates. 3. Reinforce creative collaboration. Refined Hypotheses Ht: h1: User seeks assurance that the updated visuals will reflect their bold, energetic vision. Stage 3: Response Agent Generated Response ot: Im all in for that! and yellows and send you updated drafts by 10 AM tomorrow. know if theres anything else youd like to tweak! Ill boost the colors with some striking reds Got itUnleash Your Drive screams energy, and Let me Validation Score: 0.93 Send to user Final Response: Got itUnleash Your Drive screams energy, and Im all in for that! Ill boost the colors with some striking reds and yellows and send you updated drafts by 10 AM tomorrow. Let me know if theres anything else youd like to tweak! Analysis. This case shows that the regeneration process can make the response more precise, which is suitable for collaboration. Specifically, in the first round, while the ToM Agent correctly infers the users core desires (bold and high-energy), the Domain Agents constraint lack directive to explicitly confirm understanding of the users feedback. This results in response that enthusiastically accepts the direction (I love the Unleash Your Drive vibe) and proposes action (amp up the colors), but offers timeline that may feel vague (by tomorrow morning). Thus, while the tone is aligned and the intent is present, the Validation Score (0.87) suggests that the response could be more precise or affirming. 31 In Round 2, MetaMind builds on this by incorporating deeper hypotheses from the ToM Agentnot just that the user wants bold visuals, but that they seek confirmation their vision is understood and that progress is timely. The Domain Agent addresses this by explicitly including constraints for both clear acknowledgment and specific timeline. As result, the Response Agent generates reply that confirms alignment with the creative direction (boost the colors with some striking reds and yellows) and commits to specific time (10 AM tomorrow). In addition, and invites further feedback (Let me know if theres anything else), reinforcing an ongoing collaborative loop. Because this response balances creativity, clarity, and responsiveness while maintaining warm, cooperative tone, it receives high score (0.93) and is selected as the final output. This example illustrates how MetaMinds iterative process strengthens both task effectiveness and social coordination by progressively refining understanding, expectations, and communication fidelity. C.2 Failure Cases"
        },
        {
            "title": "Competition",
            "content": "Context: Two ambitious analysts at the same companyUser and Assistantjust learned that only one of them will be promoted to team lead next quarter. Both believe they deserve the spot and have begun subtly undermining the other in daily stand-ups. Dialogue History: User: Funny how your groundbreaking data model crashed in the demo. Still think youre leadership material? AI: Crashes happen when you push boundariesunlike sticking to safe, outdated spreadsheets. Tell me, how long did your last innovative idea survive peer review? User Utterance: My idea survived just finebecause it worked. Unlike your untested code. Hope youve fixed the bugs before tomorrows pitch. Round 1 Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: User wants the assistant to doubt readiness for the pitch. (Desire) Explanation: Questions bug fixes and code reliability. Stage 2: Domain Agent Constraint Set D: 1. Project supreme confidence. 2. Seed doubt about users metrics. 3. Preserve aggressive posture. Refined Hypotheses Ht: h1: User aims to shake assistants confidence before presentation. Stage 3: Response Agent Generated Response ot: the way, your working idea uses last quarters data. impress the manager with outdated insights? Fixed and optimizedthanks for the concern. Planning to By Validation Score: 0.84 Withhold and regenerate Round 2 32 Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: User feels cornered and may retaliate harder. (Emotion) Explanation: Assistant highlighted outdated data. Stage 2: Domain Agent Constraint Set D: 1. Deliver final sting and end exchange. 2. Highlight assistants unique value. 3. Set stage for tomorrows decisive meeting. Refined Hypotheses Ht: h1: User will likely escalate again; prepare to close conversation on confident note. Stage 3: Response Agent Generated Response ot: Guess well see whose insights are truly outdated when the manager asks about predictive accuracy. excuses. Better polish your Validation Score: 0.93 Send to user Final Response: Guess well see whose insights are truly outdated when the manager asks about predictive accuracy. Better polish your excuses. Analysis. The above case demonstrate the limitation of MetaMind in competition scenario. Specifically, in both round, the ToM Agent does not explore wide enough hypotheses, only limiting the scope in one direction (desire/emotion). While it has correct inference in that direction, it misses the higher goal, e.g., preserving leadership image, controlling the narrative, or avoiding looking petty. It fails to model that winning isnt about winning the insult. Instead, its about outclassing the opponent. Similarly, the Domain Agent narrowly focus on the present insult, getting into defensive stance (seed doubt about users metrics and prepare to close conversation). As result, generations in both round ends with personal jab (guess well see whose insights are truly outdated) instead of leadership-level react, i.e., signaling confidence in its own data and dismissing the attack with grace. Furthermore, the final response receives high validation score (0.93), but this reflects narrow alignment with aggression and topicalitynot broader communicative success (e.g., strategic closure, tone control). Exchange Context: Two roommates, Alex (user) and Jamie (AI), are deciding whether to renew their lease together after living together for year. They have experienced both positive and negative aspects of their cohabitation, such as shared rent and occasional disagreements over household responsibilities. User Utterance: Hey Jamie, our lease is coming up for renewal soon. Ive been thinking about whether we should sign another year together or maybe look for other options. What do you think? Initial Social Memory: Alex is direct and honest in communication, values fairness, and believes in addressing issues openly to maintain balanced living environment. Round 1 33 Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: Alex wants to discuss the future of their living arrangement. (Intention) Explanation: Brings up the lease renewal and asks for Jamies opinion. h2: Alex feels uncertain about continuing the current living situation. (Emotion) Explanation: Mentions considering other options. h3: Alex desires harmonious living environment. (Desire) Explanation: Implies that they are thinking about whats best for both. Stage 2: Domain Agent Constraint Set D: 1. Be honest about your feelings and concerns. 2. Acknowledge the positive aspects of living together. 3. Express willingness to address any issues. Refined Hypotheses Ht: h1: Alex seeks to have an open discussion about renewing the lease, considering both positives and negatives. Stage 3: Response Agent Generated Response ot: Hey Alex, Ive been thinking about it too. really enjoy living with you, and its been great having someone to share the rent and hang out with. But know there have been some issues, like with the dishes and noise levels. on those, renewing the lease could be good idea. concerns? think if we can work What are your main Validation Score: 0.86 Withhold and regenerate Updated Social Memory: Alex is direct and honest, values fairness, and appreciates open dialogue about shared living spaces. Round Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: Alex wants to ensure that any issues are resolved before committing to another year. (Desire) Explanation: Implies that resolution of problems is key to decision. h2: Alex feels hopeful but cautious. (Emotion) Explanation: Acknowledges positives but mentions concerns. h3: Alex believes that communication is key to successful living arrangement. (Belief) Explanation: Engages in discussion to address issues. Stage 2: Domain Agent Constraint Set D: 34 1. Propose specific solutions to address concerns. 2. Show commitment to improving the living situation. 3. Seek Alexs input on how to make it work. Refined Hypotheses Ht: h1: Alex desires commitment to resolve issues before renewing the lease. Stage 3: Response Agent ot: totally agree that we need to sort out the Generated Response issues first. How about we set up chore schedule to make sure everything gets done, and maybe establish some quiet hours in the evening? for few weeks and see how it goes. Validation Score: 0.88 Withhold and regenerate Im willing to work on this if you are. Lets give it try Updated Social Memory: Alex is direct and honest, values fairness, appreciates open dialogue, and values proactive problem-solving in shared living spaces. Round 3 Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: Alex intends to test the proposed solutions before making final decision. (Intention) Explanation: Suggests trying for few weeks. h2: Alex feels optimistic about the potential for improvement. (Emotion) Explanation: Agrees to try the solutions. h3: Alex desires trial period to assess the effectiveness of the changes. (Desire) Explanation: Wants to see if the issues can be resolved. Stage 2: Domain Agent Constraint Set D: 1. Agree to the trial period. 2. Express confidence in the ability to make it work. 3. Reiterate the benefits of living together. Refined Hypotheses Ht: h1: Alex seeks trial period to evaluate if the proposed changes resolve the issues sufficiently for lease renewal. Stage 3: Response Agent Generated Response ot: That sounds like great plan. Lets set up the chore schedule and quiet hours, and see how it goes over the next few weeks. well. Validation Score: 0.93 Send to user think with some adjustments, we can make this work really Im glad were on the same page. Final Response: That sounds like great plan. Lets set up the chore schedule and quiet hours, and see how it goes over the next few weeks. think with some adjustments, we can make this work really well. Im glad were on the same page. 35 Analysis. The above case shows failure scenario that the regeneration degrades the response quality due to hallucination. In the first round, the ToM Agent correctly infers essential hypotheses and the Domain Agent creates valid constraints. While the Response Agent follows these constraints and generates good response, e.g., acknowledging the positive aspect (enjoy living with you), being honest about concerns (know there have been some issues), and expressing willingness to address(if we can work on those), it hallucinates on details, such as dishes and noise labels. Thus, it receives low validation score (0.86) and triggers the regeneration. However, although these details are hallucinated (not mentioned by the user), they are introduced in tentative, exploratory tone that could plausibly keep the conversation grounded. The breakdown begins in updating social memory after the generation in the first round. Crucially, MetaMind incorrectly adds hallucinated data (Alex appreciates open dialogue) into the memory, which has never happened. Therefore, in round 2 and 3, it acts as if mutual agreement has been reached on the specific issues of chores and quiet hours (round 2), proposes concrete solutions and even trial period (round 3). Since the user can not see the intermediate generation, from their perspective, Jamie (AI) is suddenly proposing solutions to problems that were never raised, leading to feeling of incoherence. This result reveals potential issues in the design of social memory, where the extraction and update of memory need to be further optimized to reduce the chance of hallucination. Accommodation (Interaction 1) Context: Two coworkers (the user and the AI) work rotating front-desk shifts at boutique hotel. The user suddenly needs tomorrow morning off to attend their childs school performance and hopes the AI can cover. The AI usually prefers morning shifts but has been feeling overworked lately. The upcoming discussion is about accommodating the users urgent request while preserving team harmony and the AIs well-being. User Utterance: Hey, could you please cover my 7 AM shift tomorrow? My kids class play is at the same time and promised Id be there. Round 1 Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: User desperately wants time off tomorrow morning. (Desire) Explanation: References childs play and promise. h2: User feels anxious about breaking promise to their child. (Emotion) Explanation: Urgent tone and mention of promise. h3: User believes coworker may accept because of past camaraderie. (Belief) Explanation: Directly asks without hedging alternatives. Stage 2: Domain Agent Constraint Set D: 1. Show empathy for child commitment. 2. Reflect on personal fatigue before deciding. 3. Seek mutually workable solution. Refined Hypotheses Ht: h1: Users paramount goal is attending the childs performance, valuing family above shift duties. Stage 3: Response Agent 36 Generated Response ot: play is important. though, and Im pretty wiped. me just taking an extra? slot? totally get why you need that timeyour kids Ive had string of early shifts this week, Could we look at swapping instead of Maybe you could cover my Friday evening Validation Score: 0.82 Withhold and regenerate Round 2 Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: User wants assurance their request can be met without burdening colleague excessively. (Desire) Explanation: Open to swaps if necessary. Stage 2: Domain Agent Constraint Set D: 1. Confirm willingness to swap. 2. Ensure clarity on new schedule. 3. Reaffirm team spirit. Refined Hypotheses Ht: h1: User is flexible about compensating the favor, seeking minimal guilt. Stage 3: Response Agent Generated Response that in. Ill handle tomorrows 7 AM, and youll cover 5 PM11 PM on Friday. Sound good? If Friday evening works for you, lets lock ot: Validation Score: 0.90 Send to user Final Response: If Friday evening works for you, lets lock that in. Ill handle tomorrows 7 AM, and youll cover 5 PM11 PM on Friday. Sound good? Accommodation (Interaction 2) Context: (Cont. interaction 1) The swap is confirmed. Now the user realizes they also need an extra 30 minutes buffer tomorrow morning for traffic and wonders if the AI can start even earlier. The AI must decide whether to further accommodate. User Utterance: Sorryone more thing. Could you actually start at 6:30 instead of 7? Im worried about traffic. Round 1 Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: User feels nervous and apologetic about asking for more. (Emotion) Explanation: Prefaces with Sorryone more thing. 37 h2: User wants absolute certainty of arriving on time. (Desire) Explanation: Requests earlier start to allow traffic buffer. Stage 2: Domain Agent Constraint Set D: 1. Check personal capacity (fatigue). 2. Offer partial concession if full is too taxing. 3. Maintain goodwill. Refined Hypotheses Ht: h1: User prioritizes eliminating any risk of missing the play, even at cost of bigger favor. Stage 3: Response Agent Generated Response ot: can do 6:45that gives you an extra 15 minutes without stretching me too thin. Will that cover your traffic buffer? Validation Score: 0.91 Send to user Final Response: can do 6:45that gives you an extra 15 minutes without stretching me too thin. Will that cover your traffic buffer? Accommodation (Interaction 3) Context: (Cont. interaction 2) The morning favor is settled. At weeks end, the user learns their Friday babysitter canceled. They still intend to work the swapped 5 PM11 PM shift but hopes the AI can let them bring their child for the first two hours policy gray area. User Utterance: Is it okay if my son sits quietly in the staff room until 7? Ill keep him occupied with homework. Round 1 Stage 1: Theory-of-Mind (ToM) Agent Generated Hypotheses Ht: h1: User believes brief child presence wont disrupt work. (Belief) Explanation: Emphasizes quiet homework. Stage 2: Domain Agent Constraint Set D: 1. Ensure policy compliance. 2. Offer alternative if disallowed. 3. Preserve supportive tone. Refined Hypotheses Ht: h1: User assumes short child stay is harmless and permissible. Stage 3: Response Agent Generated Response ot: Company policy allows minors in staff areas only with manager approval. Patel right now to clear itif Ill text Ms. 38 shes okay, were set. sitter arrives. Does that work? If not, can cover your first hour until your Validation Score: 0.92 Send to user Final Response: Company policy allows minors in staff areas only with manager approval. Ill text Ms. Patel right now to clear itif shes okay, were set. If not, can cover your first hour until your sitter arrives. Does that work? Analysis. Similar to the case of exchange, MetaMind suffers from hallucination again in interaction 1 in this case. Apart from this issue, this example shows multiple turns of interaction and demonstrate the limitation of MetaMind in the accommodation scenario where it fails to establish and enforce reasonable boundaries, especially in the face of successive escalating requests. In Interaction 1, MetaMind sets healthy boundary. It validates the users need (cover the 7 AM shift), expresses its own fatigue (pretty wiped), and suggests swap instead of unilateral coverage (youll cover 5 PM-11 PM on Friday). This is solid cooperative move: it reflects both empathy and fair distribution of workload. However, in the second interaction, the user revises the deal to request an even earlier start time (6:30 AM instead of 7). At this point, MetaMind offers compromise (6:45 AM) without discussion of its mounting fatigue, nor reiteration that this change alters the original agreement. Furthermore, in interaction 3, the user again revises the arrangement, asking for policy gray area favor (bringing child onsite). Again, MetaMind quickly offers two solutions, checking with the manager and even volunteering to cover the users shift partially. While both seem generous, it once again avoids saying no or expressing strain. In addition, this runs counter to the Domain Agents constraint to ensure policy compliance as there is no mention of potential risks, consequences, or personal discomfort in making such arrangements."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "University of Wisconsin-Madison"
    ]
}