{
    "paper_title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation",
    "authors": [
        "Niclas Popp",
        "Kevin Alexander Laube",
        "Matthias Hein",
        "Lukas Schott"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large foundation models trained on extensive datasets demonstrate strong zero-shot capabilities in various domains. To replicate their success when data and model size are constrained, knowledge distillation has become an established tool for transferring knowledge from foundation models to small student networks. However, the effectiveness of distillation is critically limited by the available training data. This work addresses the common practical issue of covariate shift in knowledge distillation, where spurious features appear during training but not at test time. We ask the question: when these spurious features are unknown, yet a robust teacher is available, is it possible for a student to also become robust to them? We address this problem by introducing a novel diffusion-based data augmentation strategy that generates images by maximizing the disagreement between the teacher and the student, effectively creating challenging samples that the student struggles with. Experiments demonstrate that our approach significantly improves worst group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art diffusion-based data augmentation baselines"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 2 4 9 2 2 0 . 6 0 5 2 : r Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation Niclas Popp Bosch Center for Artificial Intelligence University of Tübingen niclas.popp@de.bosch.com Kevin Alexander Laube Bosch Center for Artificial Intelligence kevin.laube@de.bosch.com Matthias Hein University of Tübingen matthias.hein@uni-tuebingen.de Lukas Schott Bosch Center for Artificial Intelligence lukas.schott@de.bosch.com"
        },
        {
            "title": "Abstract",
            "content": "Large foundation models trained on extensive datasets demonstrate strong zeroshot capabilities in various domains. To replicate their success when data and model size are constrained, knowledge distillation has become an established tool for transferring knowledge from foundation models to small student networks. However, the effectiveness of distillation is critically limited by the available training data. This work addresses the common practical issue of covariate shift in knowledge distillation, where spurious features appear during training but not at test time. We ask the question: when these spurious features are unknown, yet robust teacher is available, is it possible for student to also become robust to them? We address this problem by introducing novel diffusion-based data augmentation strategy that generates images by maximizing the disagreement between the teacher and the student, effectively creating challenging samples that the student struggles with. Experiments demonstrate that our approach significantly improves worst group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art diffusion-based data augmentation baselines."
        },
        {
            "title": "Introduction",
            "content": "Foundation models have demonstrated strong zero-shot capabilities and distributional robustness across wide range of data domains [34, 15]. However, training these models demands massive datasets and computational resources, and their deployment in resource-constrained environments is often infeasible due to their scale. Achieving similar generalization and robustness with substantially smaller models and less training data remains an open research area. Knowledge distillation (KD) [13] offers way to transfer knowledge from pre-trained teacher model to smaller student model. Compared to supervised training with only the hard ground-truth labels, distillation has been shown to improve both performance and generalization of the student model [12, 27, 31]. The common reason why knowledge distillation is believed to improve generalization is that the output of the teacher contains more information than hard labels (referred to as dark knowledge). However, the availability of task-specific training data is often crucial limitation on the knowledge that can be transferred to the student. Especially in the limited-data regime, the training data might not sufficiently cover the data distribution that the student will face during deployment. Preprint. Under review. Figure 1: Knowledge Distillation under Covariate Shift with ConfiG Data Augmentations. Our goal is to maximize student performance even on groups that are fully absent from the training data. Left: Without having seen similar samples, the student is incapable of correctly classifying the unseen groups in the test set. Right: Leveraging the discrepancy between robust teacher and the biased student, ConfiG generates and adds challenging samples to the training set. Further training improves the students performance on previously missing groups by removing shortcuts. In this work, we investigate the challenge of covariate shift between the training and test data distributions in knowledge distillation. Covariate shift refers to situation where the distribution of the input features (covariates) changes between the training and test datasets while the conditional distribution of the output given the input remains the same. In particular, it can happen that correlated covariates, so called spurious features, enable correct decision making on the training but not the test data. Such features seemingly provide shortcut [9] over causal or discriminative features, which always enable correct decision making. An example for this setting is illustrated in Figure 1. The task is gender classification on training data subset of CelebA, which only contains young and blond females as well as non-blond males with glasses. At test time, however, the student also encounters non-blond females or blond males. Without training data from these groups, the student fails to learn the correct decision boundary but instead relies on spurious features such as hair color or age. Assuming the pre-trained teacher disentangles spuriously correlated covariates and discriminative features for the target classes, our goal is to distill student that is similarly capable. Several recent works improve model performance in the limited-data regime through diffusionbased data augmentations [50, 39, 52, 14]. These approaches can be interpreted as form of implicit distillation, where generative model is used to provide additional knowledge in the form of augmented training samples. However, by only looking at existing training samples or the student model, the augmented samples cannot effectively target the students reliance on spurious features. To this end we introduce ConfiG, Confidence-Guided diffusion-based data augmentation method that leverages the disagreement between teacher and student models to identify and correct student biases. As illustrated in Figure 1, ConfiG generates synthetic data augmentations in the regions of disagreement (dashed), such as women with brown hair or young men without glasses. Further training on the real training images together with the augmented samples then leads the student to better approximate the decision function of the teacher, reducing the reliance on spurious features. Our contributions are summarized as follows: 1. We demonstrate the problem that, while knowledge distillation from robust teacher improves overall student performance under unknown covariate shift, the student still performs poorly on test samples from unseen groups. 2. We introduce ConfiG, data augmentation framework for knowledge distillation that guides diffusion model to generate samples to explicitly target the shortcuts that the student has picked up from spurious features in the training data. 3. We provide empirical and theoretical motivation for ConfiG and demonstrate its superior performance compared to prior diffusion-based augmentations under covariate shift in knowledge distillation on CelebA [23], SpuCo Birds [16], and Spurious ImageNet [29]."
        },
        {
            "title": "2 Related Work",
            "content": "Knowledge distillation (KD) is widely used training framework [11, 25] that transfers knowledge from one model (teacher) to another (student). In contrast to empirical risk minimization, the student in pure response-based KD is trained to match the predictive distribution of the teacher [25]. If the teacher is sufficiently accurate, doing so provably improves the generalization of the student over 2 training with hard labels [27, 38, 32]. While there exists lot of KD variants, given sufficient compute and augmentations, simply matching the responses of student and teacher [13] has been shown to be highly effective for image classification [5, 12]. We adopt response-based knowledge distillation purely based on the soft labels of the teacher and investigate the influence of the training data on the generalization gap. Synthetic data augmentation is rising trend to mitigate data scarcity and bias. Unlike standard augmentations like CutMix [47] or MixUp [49], generative models like GANs [51, 19, 26] and diffusion models [14, 48, 46, 4, 39, 43, 20, 10] have the ability to augment semantics such as hair color, background, or weather conditions. This can be viewed as indirect knowledge distillation, where the generated data represents the implicit knowledge of the generative teacher. In our work, we combine this implicit knowledge distillation together with explicit knowledge distillation by guiding the augmentation process based on the confidences of teacher and student. The assumption of also having an explicit teacher for the classification task is usually not stronger in practice, as models like CLIP [34, 15] are trained on similar datasets as diffusion models. Group Robustness without Annotations (Fairness without Demographics) Common methods for achieving group robustness rely on labels for majority and minority groups in either the training [37, 17, 1] or validation data [22, 18]. Recent approaches improve group robustness without requiring explicit group labels [48, 40, 33], but require all relevant groups to be represented in the dataset. This assumption is still suboptimal in practice as relevant groups may be unknowingly absent. Our proposed method is designed to improve robustness even for groups that are not represented at all."
        },
        {
            "title": "3 Methods",
            "content": "We introduce ConfiG, Confidence-Guided data augmentation method for knowledge distillation. We first review the concept of empirical distilled risk minimization for knowledge distillation and subsequently describe the methodology of ConfiG based on guided diffusion. 3.1 Background: Empirical Risk and Empirical Distilled Risk We consider standard classification setting with training data Dtrain = {(xi, yi)}N i=1 sampled from distribution Ptrain and test data Dtest Ptest. Both share common discrete label space [L]. student model and teacher model produce predictions in form of probability distributions over the label space. For given input with ground-truth label y, we denote the predicted probability assigned to the correct class by the student and teacher as (x)y and t(x)y respectively. We refer to these values as the confidences. The true test risk is defined as RPtest(f) = E(x,y)Ptest[ℓ(f (x), y)] = ExPtest,x[Eyx[ℓ(f (x), y)]] = EPtest,x [p(x)l(f (x))] (1) where p(x) = [P(yx)]y[L] is the data-generating conditional distribution over the labels, and l(f (x)) = [ℓ(f (x), y)]y[L] denotes the vector of losses for each possible label. Similarly, the true train risk is Rtrain(f) = E(x,y)Ptrain [ℓ(f (x), y)] = EPtrain,x [p(x)ℓ(f (x))]. (2) In practice, the true data distribution is unknown, and models are instead trained by minimizing the empirical risk on the training data. In standard empirical risk minimization (ERM) the student model is trained to minimize the empirical approximation of Rtrain using the label of the train samples. The empirical risk on Dtrain is defined as Rtrain(f) = 1 (cid:88) i=1 ℓ(f (xi), yi) = 1 (cid:88) i=1 eT yi log(f (xi)), (3) where we choose ℓ(, ) to be the cross-entropy loss. In knowledge distillation, the student is trained by empirical distilled risk minimization (EDRM) which optimizes the cross-entropy loss between the teacher and student predictive probabilities: RD train(f) = 1 (cid:88) i=1 t(xi) log(f (xi)). (4) Here, t(xi) refers to the vector with the predicted probability for each class by the teacher at xi. The student is trained to match these soft targets provided by the teacher instead of the hard labels. The 3 generalization error of the student under this training scheme is defined as = Rtest(f) RD train(f). (5) Using teacher whose outputs approximate closely has been shown both theoretically and empirically to improve the generalization error if Ptrain = Ptest [27, 42, 41, 31]. This corresponds to the setting without covariate shift where the input distribution for the training and test data is the same. In the case of covariate shift, the input marginal distributions Ptrain,x and Ptest,x are different and only the data-generating conditional distribution is the same at train and test time. 3.2 Confidence-Guided Data Augmentations using Diffusion Models For knowledge distillation under covariate shift, we consider the case where Ptrain = Ptest. While the error between the distilled risk and the true train risk Rtrain RD train has been studied in prior works [27, 42], in the case of covariate shift this does not correspond to the true generalization error. Therefore, we first decompose into two terms = Ω + Ψ (6) where Ψ = Rtrain(f) RD train(f) and Ω = Rtest(f) Rtrain(f). While Ψ depends on how well the teacher approximates on Ptrain [27], Ω only depends on the distribution of the training and test data. The goal of our data augmentation scheme is to reduce Ω. We assume access to teacher that approximates the p(x) well on both Ptrain and Ptest. In Figure 2, we distill student on training data where certain groups from the test data are fully absent and analyze the agreement of teacher and student on test data, which refers to the fraction of images on which the teacher and student make the same predictions. For instance, in CelebA, where the task is gender classification (female/male), our training data only includes female samples that are young, blond, and do not wear glasses. The test data, however, also includes females who are older, nonblond, or wear glasses. We observe that the agreement between teacher and student is high for groups that are present in both the training and test data (left) but significantly lower for groups absent from the training data (right). This indicates that the student does not recover the teachers decision boundary in these regions but instead learns to rely on spurious correlations like blond hair, rather than the causal feature of gender (see Figure 1). Figure 2: The student relies on spurious features in the training data and disagrees with the teacher when tested in their absence. Distillation was performed with standard augmentations and EDRM as described in Section 4. ConfiG aims to generate augmented images that reduce reliance on spurious features or datasetspecific biases without requiring the explicit knowledge of what these features are. Therefore, we make use of the observation that teacher and student predictions tend to diverge on samples from groups that are absent from the training data. We use diffusion model and real training images to generate augmented samples on which the student has low confidence in the target class while the teachers confidence remains high. Maximizing the teachers confidence ensures that the causal, class-relevant features are preserved and the original image label remains correct. Minimizing the students confidence yields images that include non-causal (spurious) features which lead the student into making incorrect decisions. For this purpose, we use guided diffusion framework [3, 2] based on Stable Diffusion [35], which operates in the latent space of VAE. To simplify the notation below, we define Mϵθ as the application of noise prediction model ϵθ, iteratively mapping latent variable zt to its final representation zT . For an image with corresponding class label y, we first encode the image zT = E(x) and apply null-text inversion [28] to obtain latent code z0 suitable for reconstructing zT ˆzT = Mϵθ (z0). We further need to decode the image ˆx = D(ˆzT ) to obtain images as input for the teacher and student. We then maximize the loss: max z0 LConf iG(D(Mϵθ (z0))) = max z0 t(D(cid:0)Mϵθ (z0))(cid:1)γ + (cid:16) 1 f(cid:0)D(Mϵθ (z0))(cid:1) (cid:17)γ (7) As Equation 7 is differentiable with respect to z0, we can solve it using stochastic optimizer such as AdamW [24]. strong teacher is accurate on the training data and outputs high probability for 4 Figure 3: Illustration of ConfiG and other diffusion-based augmentation methods: Left: the task is gender prediction (male/female) where the men in the training data all have glasses, are old and not blond. Right: Existing methods just produce high confidence for male but do not take into account the student. In contrast, ConfiG maximizes the difference between the teacher and the student over the latent space and so targets spurious features that the student picked up from the training data. Added to the training set, these augmentations severely improve the students worst group accuracy. the correct class. student distilled from this teacher using the training data then also outputs high probability for the correct class on the training data. When the student is highly confident on the initial image, using linear confidence difference as in [3] requires many optimizer steps to decrease f()y. The nonlinear transform (1 f()y)γ amplifies the gradients for decreasing the student confidence which allows us to use fewer optimization steps. Applying the same nonlinear transform to t()γ balances the objective of retaining high teacher confidence. Empirically, we find γ = 2 to perform well. An example of the optimization process is visualized in Figure 3. For performing knowledge distillation with ConfiG augmentations in practice, we first distill an auxiliary student on the real images only. This student is supposed to explicitly learn the spurious features or biases and is only used to generate the augmented samples with Eq. 7. The final student model is distilled on the joint dataset with real images and synthetic data augmentations. The closest baseline is ActGen [14], which generates augmentations that minimize the cross-entropy loss of the student model given real image and target class. However, it lacks explicit class guidance apart from the text prompt and the real image. As we show in Figure 3, ActGen can thus modify the causal attribute (i.e., gender) rather than only the spurious features. We provide theoretical investigation of the effect of using ConfiG augmentations in Appendix A.1. Under realistic assumptions, using the real training images together with augmentations generated through ConfiG provably decreases Ω."
        },
        {
            "title": "4 Experiments",
            "content": "In our experiments, we empirically demonstrate the effectiveness of ConfiG to improve knowledge distillation under covariate shift. We first outline the experimental setup before discussing the results on datasets with group shift, spurious ImageNet and the ablations. 4.1 Experimental setup Datasets Our experiments are based on three datasets for image classification. We use SpuCo Birds [16] and CelebA [23] to investigate group shift as an example for covariate shift. CelebA contains images of celebrity faces. SpuCo Birds includes waterbirds or landbirds over water or land backgrounds, with bird type as the descriptive feature. Unlike the Waterbirds dataset [37], where the background is artificially altered, the images in SpuCo birds are natural and provide more realistic setting. We subsample these datasets to 500 training images with 250 samples per class, fully discarding certain groups, e.g., examples of landbirds with water background. Thus, for SpuCo Birds, we only retain training images of waterbirds with water background and landbirds with land background such that the background becomes binary spurious feature. The test split also contains waterbirds with land background and vice versa, which represents the covariate shift 5 resulting in 4 distinct test groups. For CelebA, we only retain training images of young, blond female celebrities without glasses as well as old, non-blond male celebrities with glasses. The descriptive feature is the gender, other features are spuriously correlated only in the training split. The test set spans all combinations of age (old/young), hair color (blond/non-blond), glasses (yes/no) and gender (female/male) which leads to 16 test groups. Our third dataset is subset of ImageNet [36]. We draw 250 training images for each of the 100 classes contained in Spurious ImageNet [29]. All diffusion-based augmentations are combined with the real training sets. We use 2 augmentations per real image, as supported by our ablation in Sec. 4.4. In Appendix A.5, we show that 2x augmentations from ConfiG even outperform the baselines with more synthetic samples. Models We use ViT-T [7] pre-trained on ImageNet 21k for our main experiments on CelebA and SpuCo Birds. Ablations with ResNet-18 and TinyViT student pre-trained on ImageNet-1k are in Appendix A.3. On CelebA, we use CLIP [34, 15] ViT-L/14 pretrained on DataComp-XL [8] as teacher. On SpuCo Birds, we find that CLIP models have poor zero-shot accuracy as the spurious features (water/land) are contained semantically in the class names (\"waterbird\"/ \"landbird\"). To create reliable teacher, we use ViT-B [7] pretrained on ImageNet and allow it access to balanced training set of SpuCo Birds for fine-tuning. Note that for all distillation methods, only the teacher is directly visible to the student and not the balanced training dataset itself. We calibrate the teachers on the validation sets and use the calibrated probabilities for computing the empirical distilled risk. On ImageNet, we follow the setup of VanillaKD [12]. We use ViT-T and ViT-S students initialized with the best performing checkpoints distilled from BeiTv2-L [30] model. We use the same teacher together with the same \"A1\" augmentations [45] for our experiments. All pre-trained model weights were downloaded from the timm library [44]. Hyperparameter Search and Early Stopping All experiments use the AdamW optimizer [24] with fixed batch size of 128 and 100 training epochs using cosine learning rate decay. The learning rate and weight decay are tuned for each dataset, except for Spurious ImageNet, where we follow VanillaKD [12] and only tune the learning rate. For CelebA and SpuCo Birds, the hyperparameter search is conducted on subset of the validation split which only contains the groups that are present in the training data. This reflects the assumption that the student model will only encounter samples from these groups at test time. We select the checkpoint with the highest validation accuracy on the same subset of the validation data and evaluate it on the test data to obtain the final results. Evaluation Metrics We consider three evaluation metrics for SpuCo Birds and CelebA. Given test set Dtest = {(xi, yi)}N test}, the sample mean accuracy is the average accuracy independent of groups: i=1 composed of disjoint groups {D test, ..., DK Acc(f )Sample ean = 1 Dtest (cid:88) 1(argmax f(xj) = yj) (8) (xj ,yj )Dtest while the group mean accuracy computes the average accuracy when groups are weighted equally: Acc(f )Group ean = 1 (cid:88) i= 1 Di test (cid:88) 1(argmax f(xj) = yj) (xj ,yj )Di test The worst group mean accuracy is the minimum accuracy on group: Acc(f )W orst Group = min Di test (cid:16) 1 Di test (cid:88) 1(argmax f(xj) = yj) (cid:17) . (xj ,yj )Di test (9) (10) On SpuCo Birds, the test groups are balanced so group and sample mean accuracies coincide. On Spurious ImageNet, we consider the sample mean as well as the mean spurious AUC (spuIN mAUC) introduced by [29]. The spuIN AUC the class-wise separation of images containing the spurious feature but not the class object versus validation set of images with the class object is computed. The spuIN mAUC averages these scores over all classes. Baselines We compare ConfiG to four state-of-the-art data augmentation methods: GIF [50], DistDiff [52], DAFusion [39] and ActGen [14]. All methods generate augmentations by applying diffusion model to augment real images. To ensure fair comparison, we follow the baselines and use Stable Diffusion 1.4 [35] as implicit teacher. We show in Appendix A.12 that naively sampling images from Stable Diffusion without the guidance from real images is outperformed by all baselines. ActGen and ConfiG use student model for generating data augmentations. For this purpose, we use the setup introduced in Sec. 3.2 and first distill an auxiliary student, which is supposed to learn the spurious features in the training data, on the real images only. We then use this student to generate 6 Table 1: Results on CelebA and SpuCo Birds. Using knowledge distillation together with CutMix and synthetic augmentations consistently improves results in contrast to the baseline setup without distillation. Tailored to this setting, ConfiG consistently outperforms all other methods. We report mean accuracy and standard deviation (in grey) for 3 trainings. CelebA SpuCo Birds Method Teacher Group Mean Worst Group Acc. (%) Acc. (%) Sample Mean Group Mean Worst Group Acc. (%) Acc. (%) Acc. (%) 94.6 66.7 98.9 95.9 95.1 With standard augmentations 68.06.0 ERM 73.71.6 EDRM 7.34.4 15.22.1 EDRM with stronger augmentations CutMix MixUp AutoAugment CutMixPick 38.93.4 82.21.6 29.43.2 78.61.7 11.43.8 69.01.5 28.113.1 79.21.2 Baseline: Diffusion-Based Aug. + CutMix + ERM 29.10.4 72.51.6 DAFusion 21.82.1 75.50.6 DistDiff 19.13.8 73.51.1 GIF 26.73.1 75.50.3 ActGen 33.10.1 78.30.6 ConfiG (Ours) Proposed: Diffusion-Based Aug. + CutMix + EDRM 85.10.5 DAFusion 82.91.2 DistDiff 85.01.6 GIF 87.11.7 ActGen 89.30.8 ConfiG (Ours) 38.97.7 36.60.5 33.92.1 54.03.8 66.10.5 88.32.0 90.40.5 93.40.9 91.40.7 87.42.2 91.30. 82.64.0 88.91.5 88.50.4 84.32.5 89.51.0 95.60.3 93.80.6 92.90.9 96.20.1 96.60.1 53.91.9 56.90.7 63.90.5 62.00.5 57.20.8 62.32.6 54.81.1 54.20.5 55.31.2 54.90.8 62.41.8 79.61.4 77.82.2 75.20.5 80.71.1 83.52. 5.63.8 12.51.1 29.40.9 23.72.5 8.12.8 19.75.1 9.72.7 7.41.8 11.62.7 20.51.6 31.20.6 52.71.8 48.12.9 48.22.6 52.73.3 62.70.9 augmentations with the purpose of removing the spurious features. Finally, we train new student on the joint dataset of real images and synthetic augmentations. Further experimental details on ConfiG and the baselines are listed in Appendix A.10 and A.11. 4.2 Group Shift Datasets Quantitative Results Table 1 summarizes results on CelebA and SpuCo Birds. We first compare ERM and EDRM using standard augmentations (horizontal flips and rotations as in [39]). While distillation improves all evaluation metrics, there remains substantial gap to the teacher in worst group accuracy - over 50% on CelebA and 80% on SpuCo Birds. Our goal is to close this gap with stronger augmentations. We investigate CutMix [47], MixUp [49], AutoAugment [6] and CutMixPick [42] as stronger augmentations. While prior work found CutMixPick to be most effective for knowledge distillation without covariate shift [42], CutMix yields the best results in our setting with covariate shift. To further narrow the gap to the teacher, we expand the training set using diffusion-based image augmentations. We make two key observations: Firstly, EDRM is crucial for improving both mean and worst-group accuracy, even if the teacher is not considered for the data augmentation. The difference in worst group accuracy of the baseline setting with ERM to our proposed setup with EDRM is up to 33%. Secondly, ConfiG consistently outperforms all other methods across all evaluation metrics. This is especially pronounced in the worst group accuracy, demonstrating the effectiveness of ConfiG against spurious correlations. For the worst group on CelebA, the distilled model is almost on par with its CLIP teacher. To further investigate the effectiveness of our method, we analyze how student trained only on real images performs on augmented samples in Table 2 (left). Low accuracy and high loss are preferable as this indicates that spurious features which the student relies on (due to missing groups in the training data) were successfully altered. We find that the samples generated by DistDiff, DAFusion, 7 Table 2: Left: Evaluation of augmented samples for the CelebA and SpuCo Birds datasets, using the student trained only on real images. Acc(S), R(x) and RD(x) Right: ERM and EDRM results on subset of ImageNet with the 100 classes of Spurious ImageNet. ConfiG improves the SpuIN mAUC in comparison to all baselines and yields the best or second best validation accuracy. Method Acc(S) RD(x) R(x) Method ViT-T ViT-S l DAFusion DistDiff GIF ActGen ConfiG DAFusion B p DistDiff GIF ActGen ConfiG 87.2 92.4 98.2 67.0 37.2 95.2 98.8 91.4 77.4 64.8 0.49 0.25 0.06 0.77 2.92 0.25 0.26 0.52 0.67 1.09 0.52 0.27 0.07 1.30 3. 0.42 0.26 0.02 0.78 1.44 SpuIN Val Val SpuIN Acc. mAUC Acc. mAUC (%) (%) (%) (%) ERM EDRM +DAFusion +DistDiff +GIF +ActGen +ConfiG 92.0 93.1 93.3 93.0 92.7 93.2 93.3 70.8 79.5 80.3 80.2 80.3 81.0 81.5 95.2 95.8 96.5 96.0 95.9 96.1 96.4 84.3 85.4 84.8 84.9 85.2 85.9 86.4 Figure 4: Qualitative examples of our method ConfiG (green) and the baselines (red) on CelebA and SpuCo birds. ConfiG successfully identifies spurious features and changes removes in the augmentation process. The augmented images from the baselines still contain the spurious features. and GIF are already correctly classified and have low empirical risk and empirical distilled risk on average. As ActGen augmentations are tailored towards the student, its generated samples are notably more difficult. When comparing to the student-agnostic baselines, the empirical risk is increased more than the empirical distilled risk as the augmentations do not consider the teacher output. Samples generated with ConfiG consistently yield the lowest student accuracy and highest losses for ERM and EDRM. This indicates that our samples are most informative for the student. Qualitative Results We show four further examples for ConfiG and the baselines in Figure 4. The augmentations generated by ConfiG retain the discriminative feature while changing the spurious features that we deliberately introduced to the training set. In contrast, the spurious features in the augmented images from the baselines remain present. Interestingly, ConfiG also altered the eye color 8 in the CelebA sample, which is actually not part of the dataset attributes and was not intended as spurious feature. Due to the natural correlation between blond hair (a deliberate spurious feature in the training data) and blue eyes [21], this is likely correlation we unknowingly introduced to the training data. This finding further highlights the ability of ConfiG to identify unknown biases. 4.3 Spurious ImageNet Table 2 (right) presents results for fine-tuning VanillaKD students on subset of ImageNet containing the classes of spurious ImageNet. EDRM improves over ERM in both validation accuracy and spurious score which confirms the commonly known benefit of incorporating explicit teacher supervision. Synthetic data augmentations generally improve the validation further, with ConfiG achieving the second-best performances after DAFusion. However, for the spurious score, ConfiG surpasses all baselines by at least 0.5% mAUC. These results indicate that ConfiG enhances knowledge distillation under covariate shift, not only in group-based settings but also for class-specific spurious features. 4.4 Ablations Number of Synthetic Augmentations: In Fig. 5, we ablate on the number of synthetic augmentations per image on the CelebA dataset (left). Adding two synthetic samples per real sample maximizes the worst and mean group performance. Adding more synthetically augmented images negatively affects the combined data distribution and decreases the worst group and group mean accuracy. potential cause could be that the training distribution becomes biased towards the augmented samples. This is reflected in our theoretical analysis in App. A.1. Data-Centric vs. Model-Centric Bias Mitigation: The goal of ConfiG is to use synthetic data augmentations to mitigate unknown spurious correlations and biases present in the training data, particularly in scenarios where certain subgroups are entirely absent. This approach complements existing strategies that focus on learning robust models from fixed dataset with potential biases. We demonstrate this by exemplarily combining TAB [48], state-of-the-art approach for training classification models on biased datasets, with ConfiG on the subset of CelebA used for Table 1 with the same task of gender classification. The results in Figure 5 (right) demonstrate that the performance of TAB can be further improved with synthetic data augmentations from ConfiG. Further ablations. We demonstrate that our findings also hold for different student models and larger training sets in App. A.3 and A.4. App. A.6 shows that the common combination of ERM with EDRM is detrimental to the worst group and group mean accuracies. The computational cost for the synthetic data augmentations is discussed in App. A.7."
        },
        {
            "title": "5 Conclusion",
            "content": "Figure 5: Two augmentations per real image maximizes worst and mean group accuracy. Table 3: Using ConfiG augmentations in addition to TAB [48] further improves the performance of the student model. Results are obtained with CutMix. Group Mean Worst Group Sample Mean Method Acc. (%) Acc. (%) Acc. (%) 93.40. 82.21.6 38.93.4 EDRM EDRM + TAB EDRM + ConfiG EDRM + TAB + ConfiG 86.70.4 46.69.5 94.51.1 89.30.8 66.10.5 96.60. 90.10.2 67.00.4 96.80.1 In this work, we introduce ConfiG, novel confidence-guided data augmentation method designed to enhance knowledge distillation under unknown covariate shift. Our approach effectively addresses the challenge of group shift or class-level spurious features in training data by generating targeted augmentations that maximize the disagreement between teacher and student models. Experimental results on datasets such as CelebA, SpuCo Birds and Spurious ImageNet demonstrate that ConfiG significantly improves both worst-case and mean accuracy in group shift setting and improves 9 the robustness against class-wise spurious features. On all datasets, ConfiG outperforms existing state-of-the-art diffusion-guided data augmentation methods. Limitations. The main limitation of ConfiG is the availability of suitable teacher models (for KD and data augmentation). Both models must be robust against covariate shift for the student to effectively learn these robustness properties. As the intended CLIP teacher demonstrated poor worst group performance of only 40.2% on the SpuCo Birds dataset, we decided to obtain stronger teacher by fine-tuning ViT-B which would not be possible in practice. On CelebA, our student was arguably limited by the CLIP teacher, almost reaching its worst group performance. However, as foundation models cover an ever greater variety of data, the severity of this limitation is expected to decrease."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Maximilian Augustin and Yannic Neuhaus for helpful discussions on DiG-IN. We also thank the European Laboratory for Learning and Intelligent Systems (ELLIS) for supporting Niclas Popp."
        },
        {
            "title": "References",
            "content": "[1] Carolyn Ashurst and Adrian Weller. Fairness Without Demographic Data: Survey of Approaches. In: EAAMO. 2023. [2] Maximilian Augustin, Yannic Neuhaus, and Matthias Hein. DASH: Detection and Assessment of Systematic Hallucinations of VLMs. In: arxiv 2503.23573 (2025). [3] Maximilian Augustin, Yannic Neuhaus, and Matthias Hein. DiG-IN: Diffusion Guidance for Investigating Networks - Uncovering Classifier Differences, Neuron Visualisations, and Visual Counterfactual Explanations. In: CVPR. 2024. [4] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J. Fleet. Synthetic Data from Diffusion Models Improves ImageNet Classification. In: TMLR (2023). [5] Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: good teacher is patient and consistent. In: CVPR. 2022. [6] Ekin D. Cubuk, Barret Zoph, Dandelion Mané, Vijay Vasudevan, and Quoc V. Le. AutoAugment: Learning Augmentation Strategies From Data. In: CVPR. 2019. [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In: ICLR. 2021. [8] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah M. Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander J. Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. DataComp: In search of the next generation of multimodal datasets. In: NeurIPS. 2023. [9] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard S. Zemel, Wieland Brendel, Matthias Bethge, and Felix A. Wichmann. Shortcut learning in deep neural networks. In: Nat. Mach. Intell. 2.11 (2020), pp. 665673. [10] Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Utkarsh Tyagi, S. Sakshi, Sanjoy Chowdhury, and Dinesh Manocha. ASPIRE: Language-Guided Data Augmentation for Improving Robustness Against Spurious Correlations. In: ACL. 2024, pp. 386406. Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. Knowledge Distillation: Survey. In: Int. J. Comput. Vis. 129.6 (2021), pp. 17891819. [11] [12] Zhiwei Hao, Jianyuan Guo, Kai Han, Han Hu, Chang Xu, and Yunhe Wang. Revisit the Power of Vanilla Knowledge Distillation: from Small Scale to Large Scale. In: NeurIPS. 2023. [13] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the Knowledge in Neural Network. In: arXiv 1503.02531 (2015). 10 [14] Tao Huang, Jiaqi Liu, Shan You, and Chang Xu. Active Generation for Image Classification. In: ECCV. 2024. [15] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. OpenCLIP. Version 0.1. July 2021. URL: https://doi. org/10.5281/zenodo.5143773. [16] Siddharth Joshi, Yu Yang, Yihao Xue, Wenhan Yang, and Baharan Mirzasoleiman. Challenges and Opportunities in Improving Worst-Group Generalization in Presence of Spurious Features. In: arXiv 2306.11957 (2025). [17] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. In: ICLR. 2023. [18] Tyler LaBonte, Vidya Muthukumar, and Abhishek Kumar. Towards Last-layer Retraining for Group Robustness with Fewer Annotations. In: NeurIPS. 2023. [19] Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Sanja Fidler, and Antonio Torralba. BigDatasetGAN: Synthesizing ImageNet with Pixel-wise Annotations. In: CVPR. 2022. [20] Yumeng Li, Margret Keuper, Dan Zhang, and Anna Khoreva. Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive. In: ICCV. 2024. [21] Bochao Lin, Gonneke Willemsen, Abdel Abdellaoui, Meike Bartels, Erik Ehli, Gareth Davies, Dorret Boomsma, and Jouke-Jan Hottenga. The Genetic Overlap Between Hair and Eye Color. In: Twin Research and Human Genetics 19.6 (2016), pp. 595599. [22] Evan Zheran Liu, Behzad Haghgoo, Annie S. Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just Train Twice: Improving Group Robustness without Training Group Information. In: ICML. 2021. [23] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild. In: ICCV. 2015. Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In: ICLR. 2019. [24] [25] Amir M. Mansourian, Rozhan Ahmadi, Masoud Ghafouri, Amir Mohammad Babaei, Elaheh Badali Golezani, Zeynab Yasamani Ghamchi, Vida Ramezanian, Alireza Taherian, Kimia Dinashi, Amirali Miri, and Shohreh Kasaei. Comprehensive Survey on Knowledge Distillation. In: arXiv 2503.12067 (2025). [26] Maximilian Menke, Thomas Wenzel, and Andreas Schwung. AWADA: Attention-Weighted Adversarial Domain Adaptation for Object Detection. In: arXiv 2208.14662 (2022). [27] Aditya Krishna Menon, Ankit Singh Rawat, Sashank J. Reddi, Seungyeon Kim, and Sanjiv Kumar. statistical perspective on distillation. In: ICML. Ed. by Marina Meila and Tong Zhang. 2021. [28] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text Inversion for Editing Real Images using Guided Diffusion Models. In: CVPR. 2023. [29] Yannic Neuhaus, Maximilian Augustin, Valentyn Boreiko, and Matthias Hein. Spurious Features Everywhere - Large-Scale Detection of Harmful Spurious Features in ImageNet. In: ICCV. 2023. [30] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers. In: arXiv 2208.06366 (2022). [31] Mary Phuong and Christoph Lampert. Towards Understanding Knowledge Distillation. In: ICML. 2019. [32] Niclas Popp, Jan Hendrik Metzen, and Matthias Hein. Feature Distillation Improves Zero-Shot Transfer from Synthetic Images. In: TMLR (2024). ISSN: 2835-8856. [33] Shikai Qiu, Andres Potapczynski, Pavel Izmailov, and Andrew Gordon Wilson. Simple and Fast Group Robustness by Automatic Feature Reweighting. In: ICML. 2023. [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In: ICML. Ed. by Marina Meila and Tong Zhang. 2021. [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In: CVPR. 2022. 11 [36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. In: Int. J. Comput. Vis. 115.3 (2015), pp. 211252. [37] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally Robust Neural Networks. In: ICLR. 2020. [38] Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander A. Alemi, and Andrew Gordon Wilson. Does Knowledge Distillation Really Work? In: NeurIPS. 2021, pp. 69066919. [39] Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective Data Augmentation With Diffusion Models. In: ICLR. 2024. [40] Christos Tsirigotis, João Monteiro, Pau Rodríguez, David Vázquez, and Aaron C. Courville. Group Robust Classification Without Any Group Information. In: NeurIPS. 2023. [41] Chaofei Wang, Qisen Yang, Rui Huang, Shiji Song, and Gao Huang. Efficient Knowledge Distillation from Model Checkpoints. In: NeurIPS. 2022. [42] Huan Wang, Suhas Lohit, Michael J. Jones, and Yun Fu. What Makes \"Good\" Data Augmentation in Knowledge Distillation - Statistical Perspective. In: NeurIPS. Ed. by Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh. 2022. Jiayi Wang, Kevin Alexander Laube, Yumeng Li, Jan Hendrik Metzen, Shin-I Cheng, Julio Borges, and Anna Khoreva. Label-Free Neural Semantic Image Synthesis. In: ECCV. Ed. by Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol. 2024. [43] [44] Ross Wightman. PyTorch Image Models. https://github.com/rwightman/pytorchimage-models. 2019. DOI: 10.5281/zenodo.4414861. [45] Ross Wightman, Hugo Touvron, and Herve Jegou. ResNet strikes back: An improved training procedure in timm. In: NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future. 2021. [46] Lihe Yang, Xiaogang Xu, Bingyi Kang, Yinghuan Shi, and Hengshuang Zhao. FreeMask: Synthetic Images with Dense Annotations Make Stronger Segmentation Models. In: NeurIPS. Ed. by Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine. 2023. [47] Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe. CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features. In: ICCV. 2019. [48] Mateo Espinosa Zarlenga, Swami Sankaranarayanan, Jerone T. A. Andrews, Zohreh Shams, Mateja Jamnik, and Alice Xiang. Efficient Bias Mitigation Without Privileged Information. In: ECCV. 2024. [49] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. Mixup: Beyond Empirical Risk Minimization. In: ICLR. 2018. [50] Yifan Zhang, Daquan Zhou, Bryan Hooi, Kai Wang, and Jiashi Feng. Expanding Small-Scale Datasets with Guided Imagination. In: NeurIPS. 2023. [51] Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, and Sanja Fidler. DatasetGAN: Efficient Labeled Data Factory With Minimal Human Effort. In: CVPR. 2021. [52] Haowei Zhu, Ling Yang, Jun-Hai Yong, Hongzhi Yin, Jiawei Jiang, Meng Xiao, Wentao Zhang, and Bin Wang. Distribution-Aware Data Expansion with Diffusion Models. In: NeurIPS. 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Theoretical Investigation In this section, we show that ConfiG data augmentations provably decrease Ω under realistic assumptions, reducing the generalization error of the student. The notation is consistent with Section 3.1. When augmenting the training dataset Dtrain with samples of the distribution of augmentations Q, the resulting distribution can be written as convex combination By RQ(f ) = E(x,y)Q[ℓ(f (x), y)] we denote the expected risk on the augmented data where Paug = (1 α)Ptrain + αQ Paug = (1 α) Ptrain + α Q, 0 < α < 1, (11) (12) is the distribution of the training data with the data augmentations. We define such that it has the following support: supp(Q) = {x ty(x) τ and fy(x) σ < τ } (13) The augmentations which we aim to generate through ConfiG with Equation 7 are precisely from this set. The following assumption demonstrates that under suitable assumptions, using data augmentations from this domain provably improves Ω. Proposition 1. (Decreasing Ω through confidence-guided augmentations) Let ℓ be the crossentropy loss and assume that under covariate shift the student has low train error but high test error Ω = Rtest(f ) Rtrain(f ) > 0. Let the teacher provide an accurate estimate of the true data-generating distribution t(x) p(x) δt for all supp(Ptest) supp(Ptrain) supp(Q) and the student approximate the teacher well on the support of Ptrain f(x) t(x) δs < 1 for all supp(Ptrain) (14) (15) (16) Assume that both δs and δt are small such that 1 ϵaug = (cid:12) (cid:12). If Θ = log( τ α min{1, Ω (cid:12)Rtrain(p) RQ(p)(cid:12) 2Θ }, then the augmented generalization error > δt + δs where is the number of classes. Define (cid:16) 1 (cid:1) > 0 and σ ) (cid:0)2 log + ϵaug δs 1 δt2δs 1δtL (cid:17) satisfies Ωaug = Rtest(f ) (cid:2)(1 α)Rtrain(f ) + αRQ(f )(cid:3) (cid:12) (cid:12)Ωaug (cid:12) (cid:12) min{Ω Θ, Ω 2 }. (17) (18) We first prove an auxiliary lemma that bounds the teacher-risk aug between train and test distribution under the cross-entropy loss. Lemma 1. Let ℓ be the cross-entropy loss. If and fulfill the assumptions from Proposition 1, then (cid:12) (cid:12)Rtrain(t) RQ(t)(cid:12) (cid:12) 2 log (cid:16) 1 (cid:17) 1δtL + ϵaug. Proof. First, we re-write the loss of the teacher ℓ(t(x), y) = ℓ(cid:0)p(x), y(cid:1) + (cid:104) ℓ(t(x), y) ℓ(cid:0)p(x), y(cid:1)(cid:105) . This allows us to decompose the difference in risk between and Ptrain Rtrain(t) RQ(t) = (cid:104) (cid:105) Rtrain(p) RQ(p) + E(x,y)Ptrain (cid:124) (19) (20) (21) (cid:2)ℓ(t(x), y) ℓ(p(x), y)(cid:3) EQ (cid:2)ℓ(t(x), y) ℓ(p(x), y)(cid:3) (cid:125) . (cid:123)(cid:122) =:T (22) By definition of ϵaug, so it remains to bound . (cid:12)Rtrain(p) RQ(p)(cid:12) (cid:12) (cid:12) ϵaug, (23) Under the assumptions stated in Proposition 1, for every in the support of Ptest, Ptrain or one has so in particular p(x)y"
        },
        {
            "title": "1\nL",
            "content": ", t(x) p(x) δt < 1 , ty(x) p(x)y δt 1 δt > 0. Thus, for the cross-entropy loss (cid:12)ℓ(t(x), y) ℓ(p(x), y)(cid:12) (cid:12) (cid:12) = (cid:111)(cid:17) (cid:12) (cid:12) (cid:12) (cid:110) y(x) ty(x) , ty(x) y(x) (cid:110) p y(x)+δt y(x) p y(x)δt y(x) (cid:17) , y(x) ty(x) (cid:12) (cid:12) (cid:12)log (cid:16) = log max (cid:16) = log = log log max (cid:16) y(x) y(x)δt (cid:17) (cid:16) 1 1δtL . (24) (25) (26) (27) (28) (29) (30) (cid:111)(cid:17) From Eq. 28 to 29 and from Eq. 29 to 30 we used the assumption 1 y(x) > 1 L > δt > 0. δt + δs such that In summary, we get Etrain (cid:12)ℓ(t(x), y) ℓ(p(x), y)(cid:12) (cid:2)(cid:12) (cid:12) (cid:16) 1 (cid:17) 2 log 1δtL . (cid:3) + EQ (cid:12)ℓ(t(x), y) ℓ(p(x), y)(cid:12) (cid:2)(cid:12) (cid:12) (cid:3) Combining with the ϵaug term completes the proof: (cid:12)Rtrain(t) RQ(t)(cid:12) (cid:12) (cid:12) ϵaug + 2 log (cid:16) 1 1δtL (cid:17) . (31) (32) (33) Next, we prove Proposition 1: Proof. First, decompose for any distribution D, R(x,y)D(f) = R(x,y)D(t) + (cid:0)R(x,y)D(f) R(x,y)D(t)(cid:1). (34) Thus, we can write the difference between the risk on Ptrain and as RQ(f) Rtrain(f) = (cid:0)RQ(t) Rtrain(t)(cid:1) + (cid:0)RQ(f) RQ(t)(cid:1) (Rtrain(f) Rtrain(t)). (35) By Lemma, RQ(t) Rtrain(t) (cid:0)2 log (cid:16) 1 (cid:17) 1δtL + ϵaug (cid:1). From assumptions we can directly bound the difference of the student and teacher risk on RQ(f) RQ(t) log(σ) + log(τ ) = log( τ σ ) > 0, On Ptrain we have Rtrain(f) Rtrain(t) = E(x,y)Ptrain (cid:19)(cid:21) (cid:20) log (cid:18) t(x)y f(x)y = E(x,y)Ptrain (cid:20) (cid:18) log 1 + t(x)y f(x)y f(x)y (cid:19)(cid:21) . 2 (36) (37) (38) (39) By assumptions on δs and δt we have (x)y t(x)y δs p(x)y δt δs"
        },
        {
            "title": "1\nL",
            "content": "δt δs > 0 (40) For > 1 it holds that log(1 + x) < x. Since (x)y > 0 and t(x)y > 0 it holds that t(x)y (x)y > (x)y t(x)y f(x)y f(x)y > Thus, we get E(x,y)Ptrain (cid:16) (cid:104) log 1 + t(x)yf (x)y (x)y (cid:17)(cid:105) < t(x)y f(x)y f(x)y < δs 1 δt δs (41) (42) In summary, we obtain RQ(f) Rtrain(f) (cid:0)2 log (cid:16) 1 (cid:17) 1δtL + ϵaug (cid:1) + log( τ σ ) δs 1 δt δs =: Θ. (43) Thus, Ωaug = Rtest(f) (cid:0)(1 α)Rtrain(f) + αRQ(f)(cid:1) = Rtest(f) Rtrain(f) α (cid:0)RQ(f) Rtrain(f)(cid:1) = Ω α (cid:0)RQ(f) Rtrain(f)(cid:1) Ω α Θ Since 0 < α min{1, Ω 2Θ } by assumption, we have (cid:12) (cid:12)Ωaug (cid:12) (cid:12) min{Ω Θ, Ω 2 }. (44) (45) (46) (47) (48) To underline the practical relevance of Proposition 1, we discuss its assumptions in more detail. Ω = Rtest Rtrain > 0 is realistic scenario under covariate shift as learning spurious features instead of causal features typically results in low training but high test risk. We assume t(x) p(x) δt on the support of Ptest, Ptrain, (x) t(x) δs on the support of Ptrain δt + δs < 1 . (49) (50) (51) The first bound reflects that the teacher approximates the data-generating distribution p. This requires the teacher to be both accurate and well-calibrated. These two properties have been identified as favorable characteristics for teacher model in knowledge distillation by prior work [27, 42]. The second bound requires the student accurately mimics the teacher output on the original training distribution which is the goal of EDRM-based distillation. δt + δs 1/L ensures that both the approximation errors between the teacher and and the student and teacher are sufficiently small. ϵaug measures how much risk of the true data-generating distribution changes under the augmentations. The condition on Θ ensures the the confidence gap is sufficiently large such that the augmented samples bring Ptrain closer to Ptest. This can be achieved through large τ and small σ. Importantly, both τ and σ are determined through the augmentations (i.e. the samples drawn from Q. With ConfiG we dedicatedly try to maximize τ and minimize σ. The final condition on α reflects that sampling too many augmented images can bias Paug towards Q, which in turn harms Ω. Both conditions on Θ and α can be controlled through the type and number of augmented images in practice such that the proposition does not apply to an empty set. Overall, we highlight that the individual assumptions are satisfied in common setting with being the cross-entropy loss, strong teacher which is accurate both on Ptrain and Ptest, well-trained student and after augmenting the samples with ConfiG by successfully minimizing LConf iG. 3 A.2 Combining ConfiG with Standard Augmentations To complement our results from Table 1, where we used CutMix in addition to the synthetic data augmentations, we combine ConfiG with AutoAugment [6], MixUp [49] and CutMixPick [42] in Table 4. We make two key observations. First, using ConfiG yields substantial improvements for all four augmentation types. Second, using CutMix as presented in Table 1 outperforms AutoAugment, MixUp and CutMixPick. Method Group Mean Acc. (%) Worst Group Acc. (%) Sample Mean Acc. (%) EDRM with stronger augmentations (no ConfiG) MixUp AutoAugment CutMixPick CutMix 78.61.7 69.01.5 79.21.2 82.21.6 29.43.2 11.43.8 28.113.1 38.93.4 EDRM with stronger augmentations and ConfiG MixUp AutoAugment CutMixPick CutMix 83.81.5 (+1.6) 78.87.5 (+9.8) 90.90.5 (+11.7) 90.90.6 (+8.7) 48.84.8 (+9.9) 32.814.8 (+21.4) 53.53.1 (+25.4) 66.70.3 (+27.8) 91.40.7 87.42.2 91.30.5 93.40.9 94.11.0 (+0.7) 90.76.9 (+3.3) 95.60.5 (+4.3) 96.40.2 (+3.0) Table 4: We combine ConfiG with different augmentation techniques that do not require generative model on CelebA in the same experimental setting as Table 1. Numbers in green state the improvement over training purely with real images and no ConfiG augmentations. Incorporating images from ConfiG improves all for augmentation types. The best results are obtained with CutMix. A.3 Different Student Models In Tables 5 and 6, we report the results for training TinyViT-11 and ResNet-18 students on CelebA in the setting of Table 1. We find that also for these two additional student architectures, ConfiG outperforms all baselines for synthetic data augmentation in group mean worst group and sample mean accuracy. The ViT-T model yields the best overall results. Method Group Mean (%) Worst Group (%) Sample Mean (%) Without Diffusion-Based Augmentations 60.70.4 ERM 73.04.5 EDRM 1.90.8 14.12.4 EDRM with Stronger Augmentations CutMix 82.90.6 33.30.0 Diffusion-Based Data Augmentations with CutMix and EDRM DAFusion DistDiff GIF ActGen ConfiG 84.40.5 82.22.4 80.50.4 83.31.1 86.61. 44.92.5 37.36.1 33.61.2 43.27.0 58.20.3 77.11.2 90.13.0 95.30.3 94.20.2 93.20.9 91.51.6 94.90.6 96.10.4 Table 5: Results for TinyViT 11M trained on CelebA in the same experimental setting as Table 1. We use the synthetic data augmentations together with CutMix and CLIP ViT-L/14 teacher. A.4 Larger Training Dataset In Table 7 we perform an ablation on the CelebA dataset where we select training data set with the same spurious features as for Table 1 (i.e. only young and blond females without glasses and 4 Method Group Mean (%) Worst Group (%) Sample Mean (%) Without Diffusion-Based Augmentations 58.02.0 ERM 61.41.4 EDRM 1.50.4 4.00.9 EDRM with Stronger Augmentations CutMix 73.11.1 15.43.0 Diffusion-Based Data Augmentations with CutMix and EDRM DAFusion DistDiff GIF ActGen ConfiG 25.84.7 15.51.5 16.62.0 28.32.2 31.21.1 76.42.1 76.42.0 72.70.7 77.00.3 79.80.6 77.41.0 78.61.2 86.21.4 88.63.1 91.80.8 88.00.6 91.00.3 92.80.1 Table 6: Results for ResNet-18 trained on CelebA in the same experimental setting as Table 1. We use the synthetic data augmentations together with CutMix and CLIP ViT-L/14 teacher. old non-blond males with glasses). However, for each class we include 750 images which results in 1500 training images in total. We again use 2x synthetic data augmentations and combine these with CutMix for training the student with EDRM. We observe that also in this setting ConfiG outperforms all baselines for synthetic data augmentations Group Mean Worst Group Method (%) (%) Sample Mean (%) Diffusion-Based Data Augmentations with CutMix and EDRM DAFusion DistDiff GIF ActGen ConfiG 88.80.8 86.41.6 85.30.4 90.70.8 90.90.3 55.94.8 48.20.8 43.92.0 60.14.2 66.70.3 95.70.1 94.70.7 93.70.8 96.90.4 97.10.2 Table 7: We ablate the results on CelebA from Table 1 with larger training dataset that features 750 instead of 250 images per class. The remaining experimental setup including class level spurious features and the student and teacher model architecture are consistent with Table 1 as described in Section 4.1. We observe that also for the larger training dataset ConfiG outperforms the baselines. A.5 Cost vs. Quality In Section 4.4 we find that 2x ablations yield the best worst group and group mean performances. Following this observation, we investigate whether more than 2x synthetic augmentations from the baselines can outperform the best results from ConfiG. In other words, we ask whether the lower quality of the augmentations from the baselines can be outweighed by quantity. For this purpose, we use CelebA with the same setup as in Table 1 but generate 5 or 10 synthetic augmentations per real image using the baselines. The results are shown in Table 8. The numbers in red indicate the difference to the results with 2x augmentations by ConfiG. Regarding computational cost, the baseline methods with 10x augmentations are as expensive or more so than ConfiG with 2x augmentations (see Section A.7). We find that even using more synthetic augmentations from the baselines perform worse than ConfiG. This highlights that data quality rather than quantity is crucial for improving knowledge distillation under covariate shift. 5 Method Group Mean (%) Worst Group (%) Sample Mean (%) 87.42.0 (-1.9) 85.21.2 (-4.1) 85.30.8 (-4.0) 88.51.8 (-0.8) 95.80.5 (-0.8) 94.90.8 (-1.7) 94.80.4 (-1.8) 96.30.5 (-0.3) 5 Expansion (Diffusion-Based Data Augmentations with CutMix and EDRM) DistDiff GIF DAFusion ActGen 57.13.1 (-9.1) 45.85.9 (-20.3) 50.50.7 (-15.6) 52.83.9 (-13.3) 10 Expansion (Diffusion-Based Data Augmentations with CutMix and EDRM) 54.07.0 (-12.1) DistDiff 47.92.1 (-18.2) GIF 48.12.7 (-18.0) DAFusion 53.76.4 (-12.4) ActGen Table 8: Performance of baseline augmentation methods with 5 and 10 synthetic expansions on CelebA with the same experimental setup as for Table 1. Red values indicate the difference in performance compared to ConfiG with 2 augmentations. Despite using up to five times more data, baseline methods consistently underperform ConfiG, particularly on worst-group accuracy, highlighting that augmentation quality is more critical than quantity. The computational cost of 10 expansions from the baselines is comparable to or higher than that of ConfiGs 2 setup (see Section A.7). 96.30.1 (-0.3) 95.40.2 (-1.2) 94.50.6 (-2.1) 96.50.4 (-0.1) 87.40.3 (-1.9) 86.11.1 (-3.2) 84.71.1 (-4.6) 88.30.3 (-1.0) Method DaFusion DistDiff GIF ActGen ConfiG Group Mean (%) 81.22.0 (-3.9) 77.81.1 (-5.1) 79.10.8 (-5.9) 81.60.2 (-5.5) 85.11.5 (-4.2) Worst Group (%) 33.30.0 (-5.6) 26.26.5 (-10.4) 23.81.6 (-10.1) 33.30.0 (-20.7) 49.80.4 (-16.3) Sample Mean (%) 93.71.2 (-1.9) 91.91.2 (-1.9) 92.61.0 (-0.3) 94.90.1 (-1.4) 95.01.2 (-1.6) Table 9: Combining ERM with EDRM results in worse student performance than only EDRM. The results are obtained in the same experimental setup as the last section of Table 1. We use synthetic data augmentations together with CutMix on the same training. The student and teacher model are as described in Section 4.1. A.6 Combining EDRM and ERM In Table 9 we investigate the combination of ERM and EDRM under covariate shift. We therefore distill student on the same training data from CelebA as used for Table 1 but optimize the sum of the ERM and EDRM loss during training. We observe that including ERM in addition to EDRM deteriorates the performance of the student in all three evaluation metrics when comparing to EDRM only. A.7 Computational Cost The stochastic optimization within the guided diffusion framework of ConfiG requires to decode the image after every optimization step. This is different to the baselines which require to decode the augmented image only once. For our experiments, we used 5 optimization steps which took 11.4 sec/image for the generation on NVIDIA v100 GPU. In comparison, creating an image with Stable Diffusion without any guidance took 2.1 seconds and ActGen took 3.3 sec/image. While ConfiG introduces additional computational overhead in comparison to the baselines, we emphasize that in our experiments this is outweighed by the quality of the augmentations. Even though with the same cost for generating 2x augmentations one can generate (nearly) 10x augmentations with the baselines, the results in Table 8 highlight that fewer augmentations with ConfiG still yields better performance. 6 A.8 Hyperparameter Details and Training Setup In this section, we report the hyperparameter and the search spaces for choosing them. As described in Section 4, we use validation set which only contains images from the groups in the training data to select the best performing hyperparameters. For all experiments we use the AdamW optimizer [24] for 100 epochs with cosine learning rate decay. For SpuCo Birds we search for the learning rate in {102, 103, 104, 105} and the weight decay in {100, 101, 103, 104, 105, 106, 107}. For ImageNet we only search for the learning rate in {103, 104, 5 105, 105, 106} and keep all other hyperparamters as VanillaKD [12]. The hyperparameter searches are performed for EDRM and ERM separately. The full hyperparameters are shown in Tabel 10. For performing EDRM, we optimize the KullbackLeibler divergence instead of the cross-entropy loss as this is the common approach. Since the Kullback-Leibler divergence and the cross-entropy loss only differ by constant factor independent of the student model, these two training approaches are in practice equivalent. Model Dataset ImageNet subset ViT-T ImageNet subset ViT-T ImageNet subset ViT-S ImageNet subset ViT-S ViT-T CelebA ViT-T CelebA ResNet-18 EDRM CelebA ResNet-18 ERM CelebA TinyViT CelebA TinyViT CelebA ViT-T SpuCo Birds ViT-T SpuCo Birds EDRM ERM EDRM ERM Training Type Learning Rate Weight Decay EDRM ERM EDRM ERM EDRM ERM 5 105 5 105 5 105 5 105 1 104 1 104 1 104 1 104 1 104 1 104 1 104 1 105 2 102 2 102 2 102 2 102 1 103 1 105 1 104 1 104 1 105 1 105 1 105 1 10 Table 10: Hyperparameters across datasets, training types and models. A.9 TAB with ERM In Section 4.4, we report the results when using TAB to balance the training data and train student model using EDRM on this data. In Figure 11 we use the same setup on CelebA including the same balancing with TAB but use ERM to train the student model as originally proposed by TAB [48]. We make two observations. First, when training with ERM, ConfiG also yields substantial improvement over training with the real images only. Second, when comparing to Table 3, we observe that combining TAB with EDRM yields performance across all metrics than TAB with ERM. A.10 Experimental Details for ConfiG The first step of ConfiG requires prompt to perform nulltext inversion. For this purpose we use \"a realistic photo of {classname}\". For SpuCo Birds the classnames are \"waterbird\" and \"landbird\". For CelebA we use \"female face of celebrity\" and \"male face of celebrity\". For ImageNet we use the classnames. Subsequently, we perform five optimization steps using AdamW [24] to optimize Eq. 7. We use learning rate of 0.01. A.11 Summary of the baselines DistDiff [52] constructs hierarchical prototypes to approximate the real data distribution. The latent data points within diffusion model are optimized to stay close to these prototypes with hierarchical energy guidance. DAFusion [39] performs data augmentation by introducing noise at random diffusion timestep. The decoding process is guided by class-specific prompts in addition to real images. 7 Group Mean Worst Group Sample Mean Method Acc. (%) Acc. (%) Acc. (%) 86.70.9 68.74.8 16.61."
        },
        {
            "title": "ERM",
            "content": "ERM + TAB ERM + ConfiG ERM + TAB + ConfiG 73.41.1 24.22.9 88.40.3 78.30.7 33.10. 89.51.0 80.72.6 37.11.9 91.41.5 Table 11: Combining ConfiG with TAB [48] improves the student performance in all three metrics. The experimental setup is the same as for Table 3 but the student is trained with ERM instead of EDRM. Consistent with our previous findings, using EDRM as in Table 3 yields the best performance. GIF [50] first extract the latent feature from the encoder of the generative model. Within the latent of the diffusion model, the framework introduces multiplicative random perturbations. In addition, the framework leverages custom prompts to enhance diversity. ActGen [14] is the only baseline that considers the outputs of the downstream to perform data augmentations. The generation process maximizes the cross-entropy loss of the model output with respect to the label while guiding with the attention maps of the student model evaluated on the real image and class specific prompt. SD [35] As simple baseline, we generate images using Stable Diffusion only with the prompt \"a realistic photo of {classname}\" without additional guidance by real images or model predictions. A.12 Full Results In Tables 12 we report the full results on CelebA and SpuCo birds. The training setup including data splits is the same as for Table 1. detailed description on the experimental setup can be found in Section 4.1. A.13 Further Examples In Figures 6, 7 and 8 we show further examples for augmentations with ConfiG and the baselines on all three datasets. 8 Group Mean Acc. (%) Worst Group Acc. (%) Sample Mean Acc. (%) Method Teacher 94. Without Diffusion-Based Data Augmentations ERM EDRM 68.06.0 73.71.6 EDRM with Stronger Augmentations 82.21.6 CutMix 78.61.7 MixUp 69.01.5 AutoAugment 79.21.2 CutMixPick 66.7 7.34.4 15.22.1 38.93.4 29.43.2 11.43.8 28.113.1 Baselines for Diffusion-Based Data Augmentations with CutMix and ERM SD DAFusion DistDiff GIF ActGen ConfiG (Ours) 18.12.1 29.10.4 21.82.1 19.13.8 26.73.1 33.10.1 78.50.3 72.51.6 75.50.6 73.51.1 75.50.3 78.30.6 15.55.1 20.46.3 23.92.6 16.91.0 43.22.7 51.21.8 Diffusion-Based Data Augmentations with EDRM SD DAFusion DistDiff GIF ActGen ConfiG (Ours) 78.90.3 77.01.7 78.90.5 74.40.3 82.81.5 84.81.1 Diffusion-Based Data Augmentations with CutMix and EDRM 85.71.2 SD 85.10.5 DAFusion 82.91.2 DistDiff 85.01.6 GIF 87.11.7 ActGen 89.30.8 ConfiG (Ours) 33.70.7 38.97.7 36.60.5 33.92.1 54.03.8 66.10. 98.9 88.32.0 90.40.5 93.40.9 91.40.7 87.42.2 91.30.5 90.20.7 82.64.0 88.91.5 88.50.4 84.32.5 89.50.1 90.52.6 89.62.7 93.80.6 89.91.0 94.20.2 94.30.1 95.50.3 95.60.3 93.80.6 92.90.9 96.20.1 96.60. Table 12: Full results with the same training set from CelebA as in Table 1. For all experiments the student model is ViT-T model and the teacher is CLIP ViT-L/14 model which was trained on DataComp XL. 9 Group Mean Acc. (%) Worst Group Acc. (%) Method Teacher 95. Without Diffusion-Based Augmentations ERM EDRM 53.91.9 56.90.7 EDRM with standard augmentations 63.90.5 CutMix 62.00.5 MixUp 57.20.8 AutoAugment 62.32.6 CutMixPick 95.1 5.63.8 12.51.1 29.40.9 23.72.5 8.12.8 19.75.1 Baselines for Diffusion-Based Data Augmentations with CutMix and ERM SD DAFusion DistDiff GIF ActGen ConfiG (Ours) 6.71.8 9.72.7 7.41.8 11.62.7 20.51.6 31.20.6 52.91.1 54.81.1 54.20.5 55.31.2 54.90.8 62.41.8 17.65.2 21.04.0 19.61.6 16.56.5 25.33.4 30.01.4 Diffusion-Based Data Augmentations with EDRM SD DAFusion DistDiff GIF ActGen ConfiG (Ours) 48.60.9 61.70.6 59.11.0 55.00.8 62.20.8 64.00.1 Diffusion-Based Data Augmentations with CutMix and EDRM 53.71.0 SD 79.61.4 DAFusion 77.82.2 DistDiff 75.20.5 GIF 80.71.1 ActGen 83.52.1 ConfiG (Ours) 36.32.0 52.71.8 48.12.9 48.22.6 52.73.3 62.70. Table 13: Full results using the same training set from SpuCo Birds as in Table 1. For all experiments the student model is ViT-T model and the teacher is ViT-B model which was fine-tuned on the full (balanced) SpuCo Birds dataset. The full training data is only used for fine-tuning the teacher. For distilling the student we use subset where certain groups are fully absent from the training data. For details see 4.1. 10 Figure 6: Further examples for diffusion-guided data augmentations for images from CelebA and SpuCo Birds 11 Figure 7: Further examples for diffusion-guided data augmentations for images from CelebA and SpuCo Birds. Figure 8: Further examples for diffusion-guided data augmentations for images from ImageNet."
        }
    ],
    "affiliations": [
        "Bosch Center for Artificial Intelligence",
        "University of Tübingen"
    ]
}