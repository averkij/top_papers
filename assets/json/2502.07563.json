{
    "paper_title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
    "authors": [
        "Weigao Sun",
        "Disen Lan",
        "Yiran Zhong",
        "Xiaoye Qu",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE."
        },
        {
            "title": "Start",
            "content": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid Weigao Sun 1 Disen Lan 1 2 Yiran Zhong 1 Xiaoye Qu 1 Yu Cheng"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 5 2 0 2 1 1 ] . [ 1 3 6 5 7 0 . 2 0 5 2 : r Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on Linear-Llama3 model, variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with sequence length of 2048K across 64 GPUs. The Code is released as part of: https://github.com/ OpenSparseLLMs/Linear-MoE. 1Shanghai AI Laboratory 2South China University of Technology 3The Chinese University of Hong Kong. Work was done during Disen Lans internship at Shanghai AI Laboratory. Correspondence to: Yu Cheng <chengyu@cse.cuhk.edu.hk>. Copyright 2025 by the author(s). 1 originally introduced by Vaswani Transformer, et al. (Vaswani et al., 2017), has become the backbone of modern models across wide range of domains, including language, vision, audio, video, graphs, and even time-series data (Achiam et al., 2023; Team, 2023; Qu et al., 2024). Although the Transformer dates back to 2017, its adaptability and robustness have made it indispensable for variety of tasks. Central to its success is the self-attention mechanism, which is highly effective for sequence modeling, but has quadratic complexity (w.r.t. sequence length), leading to significant computational costs during training. However, the ability to handle long-context sequences is crucial for large model applications, not only for language tasks but also for multi-modal tasks, where sequences naturally tend to be long (Xue et al., 2024). FlashAttention series (Dao et al., 2022; Dao, 2023; Shah et al., 2024) have provided substantial advancements in scaling attention to handle longer sequences by optimizing the CUDA-level computations for better hardware utilization. However, the theoretical complexity of FlashAttention remains quadratic. Moreover, the need to maintain the KV cache presents further difficulties in managing memory as the sequence length extends (Qin et al., 2024c). As result, long-sequence processing in Transformer models continues to be complex and resource-intensive problem. Recently, numerous variations of attention have been proposed, primarily aimed at addressing its quadratic computational and memory complexity, as well as the growing size of the KV cache (Peng et al., 2023; 2024). One promising approach line is linear attention (Katharopoulos et al., 2020), which replaces the exponential kernel in softmax attention with simpler dot product between key and query vectors. This shift allows linear attention to be structured as linear recurrent neural network (RNN) with matrixvalued hidden states, thereby eliminating the need for KV cache. In consequence, it supports constant-memory inference and reduces training complexity from quadratic to linear (Yang et al., 2023). parallel line of research focuses on State Space Models (SSMs), such as Mamba (Gu & Dao, 2023) and Mamba 2 (Dao & Gu, 2024), which draw upon concepts from control theory. Both linear attention and SSMs share common recurrent formulation, expressed as Ms = Ms1 + (cid:99)Ms, where (cid:99)Ms represents the increLASP-2 mental memory state of the s-th token (Yang et al., 2024). However, despite these advantages, they tend to perform poorly on recall-intensive tasks, such as in-context learning (e.g., five-shot MMLU (Hendrycks et al., 2020), Phone-book lookup (Jelassi et al., 2024), Needle In Haystack (Briakou et al., 2023)) and long-context reasoning. Empirical research (Lieber et al., 2024; Ren et al., 2024; Waleffe et al., 2024; Li et al., 2025) has shown that models relying solely on linear sequence modeling struggle to excel in these domains. However, hybrid architecture combining linear sequence modeling layers with standard transformer layers has been demonstrated to significantly enhance model performance on tasks that are recall-intensive. Sequence Parallelism (SP) techniques (Korthikanti et al., 2022; Jacobs et al., 2023; Liu et al., 2023) are commonly employed to partition long sequences into smaller subsequences, allowing them to be processed across multiple GPUs in parallel. Despite the advantages offered by SP for handling large sequences, current SP methods do not fully exploit the right-product-first feature of linear attention, which can lead to inefficiencies in parallelism and communication. LASP (Sun et al., 2024a) (referred to as LASP-1) introduced SP approach specifically tailored for linear attention, that uses point-to-point (P2P) communication strategy. In this method, intermediate states are transferred across GPUs in ring-style pattern within the distributed world. However, although such P2P ring-style communication offers certain benefits, part of its computation has to be executed sequentially, which leads low computation parallelism. In addition, too many small P2P operators make the overlapping of communication and computation difficult. In this paper, we introduce LASP-2 by rethinking the minimal communication requirement involved in SP of linear attention. Specifically, we innovatively reorganize the whole computation and communication workflow with an optimized execution mechanism. In this way, only single allgather collective communication is needed in the forward or backward of each iteration. These bring both communication and computation efficiency improvements: 1) the size of intermediate memory state tensor the all-gather operator works on is independent of the sequence length, making the communication burden insignificant in the context of long sequences. The communication parallelism and accessibility to overlap with computation are notably improved. 2) the refactored workflow improves both communication and computation parallelism over multiple devices. Additionally, we separately present LASP-2 with and without masking for autoregressive and bidirectional tasks, respectively, as the presence of mask matrix significantly impacts the design criterion of LASP-2. To extend LASP-2 to hybrid models with both linear and standard attention layers, we introduce LASP-2H. This extension employs the same allgather-based communication for standard attention layers, with similar designing philosophy on linear attention. We conduct experiments with up to sequence length of 2048K to verify the efficiency advantages of LASP-2 and LASP-2H. Our main contributions can be summarized as follows: We rethink the communication design for the current SP on linear attention, reorganize its whole communication & computation workflow with an optimized execution mechanism. This involves using single AllGather collective communication on intermediate memory states, whose sizes are independent of sequence length. The resulted LASP-2 improves both communication and computation parallelism for SP on linear attention, thus significantly enhances efficiency. We extend LASP-2 to LASP-2H, offering an efficient SP solution for hybrid models that blend both linear and standard attention layers, employing an unified all-gather-based communication design. We construct series of Linear-Llama3 models, including both purely linear and hybrid versions. Extensive experimental results on these models with up to sequence length of 2048K, validate the efficiency improvement and performance of LASP-2 and LASP-2H. 2. Preliminary Notation In this paper, we ensure the consistent use of notations to enhance clarity. Table 1 provides complete list of all the symbols utilized throughout, including indices, constants, vectors, and matrices. Vectors and matrices are represented in boldface. For simplicity, we have omitted the dimensions related to batch size and number of heads in tensor shapes. Linear Attention The term \"attention\" generally refers to computation that assigns scores to pairs of positions within sequence, enabling each element to \"attend\" to others. The most widely used and significant variant of this mechanism is softmax self-attention, which is central to standard transformer models (Vaswani et al., 2017). During training, with an assumption of single attention head for simplicity, softmax self-attention computes as follows: Q, K, = XWQ, XWK, XWV , = Softmax(QK)V. (1) The mechanism of pairwise comparisons (induced by materializing QK) leads to the characteristic quadratic training cost of softmax self-attention. Recently, Linear Attention (Katharopoulos et al., 2020; Shen et al., 2024; Qin et al., 2024a) has gained attention as potential alternative to softmax self-attention, with two key distinctions. First, it removes the Softmax() operation, incorporating it into kernel feature map. Second, it leverages the associativity of matrix multiplication to reformulate (QK)V = Q(KV). 2 Table 1: Notations. Indices, operations, constants, vectors and matrices used in the paper. LASPIndices Constants T Any indices Index of current token Index of chunk Hidden dimension World size Sequence length Total number of chunks Rdd Chunk length Operations (or omitted) Vectors and Matrices x, R1d q, k, R1d X, RN Q, K, RN Matrix multiplication Hadamard multiplication Input and output vectors Query, key, value vectors Input and output matrices Query, key, value matrices Memory state matrix WQ, WK , WV Rdd Weight matrices These adjustments reduce both the computation and memory complexity of attention calculation from O(N 2d) to O(N d2). This technique is often referred to as the rightproduct kernel trick because it prioritizes the multiplication on the right side first. During inference, both softmax self-attention and linear attention handle single token at each iteration. Given the s-th token xs R1d, softmax self-attention computes requiring the storage of an expanding set of keys {k1, , ks} and values {v1, , vs} i.e., the KV cache, which leads to significant memory burden when dealing with long input sequences. In linear attention, researchers have experimented with using various nonlinear kernels to replace the exp() function in Eq. 2. qs, ks, vs = xsWQ, xsWK, xsWV , os = (cid:80)s )vi i=1 exp(qski (cid:80)s i=1 exp(qsk ) . (2) However, recent studies (Sun et al., 2023; Yang et al., 2023; Qin et al., 2024c) have found that employing linear kernel (i.e., using the identity function) without normalizing denominator works effectively in practice. This results in an unnormalized linear attention form as below: os = (cid:88) i=1 qs(ki vi) = qs (cid:88) (ki i= vi) = qsMs, (3) i=1 ki vi is the prefix sum of ki where Ms = (cid:80)s vi from = 1 to s, which is also known as the memory state in linear attention. This reformulation leads to recurrent structure for linear attention, resembling the behavior of RNNs as Ms = Ms1 + s vs, os = qsMs. (4) 3. Method 3.1. LASP-2 without Masking SP methods work by dividing long input sequences into several smaller chunks, which are then distributed across multiple computational devices. Each device independently 3 processes the queries, keys, and values for its assigned chunk in parallel. To complete the attention computation for the entire sequence, necessary communication steps are performed to either gather the results from all devices or exchange information between them. LASP (Sun et al., 2024a) was introduced as sequence parallelism technique designed specifically for the linear attention module. Let us consider distributed computing setup where there are devices, and the input sequence is divided into chunks, referred to as the sequence parallel size. In the usual case, is evenly divisible by , and we often assume = . It means each chunk is assigned to single device, ensuring that every chunk is processed in parallel across the distributed system. This scenario exemplifies pure sequence parallelism. Additionally, in Sec.A.4.1, we will explore cases where = , representing hybrid approach that combines sequence parallelism with data parallelism. In LASP-2, the input sequence is divided into smaller chunks, represented as [Xt]T 1 , and each chunk is distributed across the devices in the distributed system. For each chunk Xt, its corresponding query, key, value, and the linear attention memory state can be computed in parallel across all chunks. This parallel computation is carried out as follows: Qt, Kt, Vt = XtWQ, XtWK, XtWV , Vt. Mt = (5) By performing this concurrent computation for each chunk, LASP-2 efficiently handles long input sequences in distributed setting. The query Qt, key Kt, value Vt, and the memory state Mt are calculated individually for every chunk of the sequence, ensuring that no single device is overburdened with processing the entire sequence at once. This distributed approach facilitates better memory management and computational efficiency, especially when dealing with extremely long sequences. Thus, LASP-2 leverages the power of sequence partitioning to optimize the calculation of linear attention in distributed framework. Notably, in LASP-2, only single all-gather collective communication operation is required during the forward pass. LASP-2 Algorithm 1 LASP-2 w/o Masking Algorithm 2 LASP-2 w/ Masking 1: Input: input sequence X, distributed world size , sequence parallel size = . 2: Distribute = [Xt]T 1 . 3: for chunk {1, , } on ranks {1, , } in parallel 1: Input: input sequence X, distributed world size , sequence parallel size = . 2: Distribute = [Xt]T 1 . 3: Initialize mask matrix Ψ, where Ψij = 1 if and Ψij = do 4: 5: 6: Calculate Qt = XtWQ, Kt = XtWK , Vt = XtWV . Compute Mt = Communicate Vt. [Mt]T 1 = AllGather([Mt]T 1 ). Compute M1:T = Sum([Mt]T Compute Ot = QtM1:T . 7: 8: 9: end for 10: return = [Ot]T 1 . 1 ). if < j. 4: for chunk {1, , } on ranks {1, , } in parallel do 1 = AllGather([Mt]T Calculate Qt = XtWQ, Kt = XtWK , Vt = XtWV . Compute Mt = (Kt)Vt. Communicate [Mt]T Compute Ot,intra = [(QtK Compute prefix sum M1:t1 = PrefixSum([Mt]t1 Compute Ot,inter = QtM1:t1. Compute Ot = Ot,intra + Ot,inter. 5: 6: 7: 8: 9: 10: 11: 12: end for 13: return = [Ot]T 1 . ) Ψ]Vt. 1 ). ). This all-gather operation acts on the memory states [Mt]T 1 associated with each sequence chunk, ensuring that every device in the system has access to the complete set of memory states [Mt]T 1 . Once the memory states from all chunks have been gathered, they are concurrently accumulated on all devices to compute the memory state corresponding to the entire input sequence. This process is expressed as follows: 1 = AllGather([Mt]T M1:T = Sum([Mt]T [Mt]T 1 ), 1 ). (6) Finally, the linear attention output corresponding to the local query Qt can be computed as: Ot = QtM1:T . Importantly, the accumulation step Sum([Mt]T 1 ) can be efficiently performed in recursive manner, by adding each memory state sequentially as M1:t1 +Mt. This eliminates the need to repeatedly calculate the sum of the memory states from earlier chunks, improving the efficiency of the computation. To further optimize performance, we cache the accumulated result M1:T in high-bandwidth memory (HBM). This caching strategy speeds up the backward pass by avoiding redundant recalculations of M1:T , which is necessary for computing gradients. This approach is akin to the concept of activation checkpointing, where intermediate activations are saved to avoid recomputation. It is important to point out that each memory state Mt has dimensions of d, which means the communication cost for the all-gather operation is independent of the sequence or chunk length. Instead, the cost scales linearly with the number of devices involved in the SP communication group. For clarity, we provide summary of the LASP-2 method, without considering the attention mask, in Algorithm 1. During the backward pass, similar all-gather communication operation on the gradients of memory states dMt is required. The details of this backward pass without masking, can be found in Algorithm 3 in Appendix A.1 for further reference. 4 Figure 1: Computation Decomposition in LASP-2 with masking. Colored chunks represent inter-chunks. 3.2. LASP-2 with Masking the mask matrix Ψ In autoregressive tasks, {, 1}N is typically lower triangular matrix, where Ψij = 1 for and Ψij = when < j. This structure enforces causal constraint during computation. Specifically, when calculating = Softmax(QK Ψ)V, it becomes impossible to leverage the associative property of matrix multiplication to reduce the computational complexity from quadratic to linear in parallel form. To address this challenge in linear attention with causal mask, we adopt the approach of computation decomposition, as proposed in earlier work (Yang et al., 2023; Sun et al., 2024a). Figure 1 provides an illustration that highlights the difference between intra-chunk and inter-chunk computations in linear attention. Inter-chunk calculations, which have no dependencies on other chunks across devices, can be treated as if they have no causal mask. As result, these computations can be parallelized across all devices in the distributed setup. In contrast, intra-chunk calculations account for the influence of previous chunks (1 to (t1)) on LASP-2 the t-th chunk. These intra-chunk operations are affected by the mask matrix, and therefore, require specialized handling to respect the causal constraints. For linear attention computation on intra-chunks, given the query, key, and value matrices Qt, Kt, and Vt corresponding to the chunk Xt, the output is computed as Ot,intra = [(QtK ) Ψ]Vt, (7) This formulation adheres to the standard left-product matrix multiplication. Although the computation can be executed in parallel across devices, it retains the quadratic complexity commonly associated with traditional attention mechanisms during training. This limitation arises from the element-wise masking operation (Ψ), which enforces causal constraints within the chunk, preventing the use of optimizations that would reduce the computational cost to linear. For linear attention computation across inter-chunks, we follow similar approach as the procedure outlined for LASP-2 without masking. First, the memory states for each chunk are computed concurrently across different devices as Mt = Vt. These memory states, corresponding to each chunk, are initially distributed across separate devices. To synchronize the results, an AllGather collective communication operation is performed. This step ensures that all devices hold the memory states for all chunks, enabling further parallel processing. Once the memory states have been gathered, we proceed with concurrent PrefixSum operation across all devices. This operation accumulates the memory states from the 1st chunk up to the (t 1)-th chunk, effectively building the necessary intermediate states. This can be expressed as: 1 = AllGather([Mt]T [Mt]T 1 ), M1:t1 = PrefixSum([Mt]t1 (8) ). The PrefixSum operation can be optimized by implementing it recursively, utilizing cached memory states stored on the HBM. Specifically, the accumulation of memory states is computed as: M1:t1 = M1:t2 + Mt1. (9) By caching M1:t1, the backward pass computation is facilitated since this cached value is necessary activation for gradient calculations. This approach not only speeds up the backward pass but also reduces the computational load, as the cached memory state eliminates the need for repeated re-computation. and inter-chunk outputs. Ot,inter = QtM1:t1, Ot = Ot,intra + Ot,inter. (10) We provide the complete algorithm for LASP-2 with masking in Algorithm 2, and its backward pass in Algorithm 4 in Appendix A.1. Note that, in Algorithm 2, the communication operation in line 7 (in magenta), along with the computation of Ot,intra in line 8 (in cyan), can be overlapped by executing them on separate threads. This concurrent execution helps improve overall efficiency, as it allows for the overlap of communication and computation. 3.3. LASP-1 vs LASP-2 LASP-2, as well as its previous version LASP-1, both aim on efficient SP on linear attention. Although, in theory, LASP-1 and LASP-2 share similarity on communicating the KV activation (d d), whose size is independent of the sequence or chunk length. They have fundamental distinctions where the key differences lie in their communication manners and the computational order reorganization, as elaborated as below: LASP-1 utilizes ring-style P2P communication, which needs to launch many send & receive operators between devices, to sequentially transfer the KV activation one-by-one among the devices. This makes the communication process relatively slow and hard to adequately overlap with intra-chunk computations. While LASP-2 uses single AllGather collective communication operator to exchange KV activation concurrently among all decices. This offers practical advantages: (1) Only one well-optimized collective communication operator needs to be launched, and the exchange of KV activation on all devices can be finished concurrently all at once; (2) the collective communication can be more easily overlapped with computations. Like in LASP-2 with masking, the AllGather communication is able to overlap with the intra-chunk output computations. And, in addition, LASP-2 reorganizes the whole computation order to make the AllGather based communication strategy feasible and efficiency. We also write down the Algorithms of LASP-1 (with and without masking) in identical mathematical symbols in Appendix A.2 for convenience to compare with LASP-2 on their algorithmic differences. 3.4. Theoretical Cost Analysis Following the calculation of the memory states, the outputs corresponding to the inter-chunks and the final output for the t-th token can be derived with ease. The overall output for the t-th token is obtained by summing both the intra-chunk For better understanding the superiorities of LASP-2, we provide theoretical cost analysis of both LASP-1 and LASP-2. We consider the pure SP scenario, i.e., the distributed world size is , and an input sequence with LASP-2 Figure 2: Visualization of LASP-2H on Linear Attention and Standard Attention hybrid model. We exemplify LASP2H on the hybrid layers of linear attention and standard attention modules with both TP and SP (both have dimension of 2). The communication operations colored in yellow and green are for TP and SP, respectively. AG/RS: all-gather in forward and reduce-scatter in backward, and vice versa. AG/No: all-gather in forward and no-op in backward, and vice versa. Note that the SP communication operations for linear attention operate on the memory state Mt Rdd, while for standard attention, they operate on states Kt, Vt RCd. length of is partitioned into = chunks, thus all devices in this world need to involve into the communication. Below denotes batch size, represents number of heads. Communication traffic in each communication step: LASP-1: BHd2, LASP-2: BHd2. This is because both LASP-1 and LASP-2 transfer linear attention memory states (not keys and values) among devices. The memory state corresponding to each chunk (located at each device) has tensor shape of [B, H, d, d]. Thus in each communication step, their communication traffic are both BHd2. For Linear-Llama3-1B model with = 16, = 16 and = 2048, each memory state will has BHd2 1.07B parameters, which takes around 2.14GB memory in FP16. For Linear-Llama3-8B model with = 16, = 32 and = 4096, each memory state has BHd2 8.59B parameters, which takes around 17.18GB memory in FP16. 1 = AllGather([Mt]T Number of communication steps in each iteration: LASP-1: 2(W 1), LASP-2: 2. This depends on the different communication manners of these two algorithms. During the forward of an iteration, LASP-2 launches single all-gather operation to gather all memory states Mt to all devices, i.e., [Mt]T 1 ). This collective operation is concurrently executed on all devices. While in backward, another all-gather is performed on the 1 = AllGather([dMt]T gradients of Mt, i.e., [dMt]T 1 ). Thus in each iteration, LASP-2 has 2 communication steps. While LASP-1 uses pair of send & receive operation to sequentially exchange the memory state from one device to another device. During forward, device sends its memory state to device + 1, and device + 1 receives the memory state from device i, and so on. Computations of Ot,inter, Ot and updates of Mt are followed behind each receive operation on that device. Thus in the process of forward, LASP-1 has 1 communication steps. In the backward, this process is repeated reversely from the last device to device 0. Thus in each iteration, LASP-1 have totally 2(W 1) communication steps. Given that both LASP-1 and LASP-2 perform total of iterations, their communication traffic models can be expressed as follows: LASP-1: 2(W 1)IBHd2 and LASP2: 2IBHd2. Ideally, the communication traffic of LASP2 would be reduced by factor of 1 compared to LASP-1. However, the actual communication cost depends on practical factors like communication bandwidth, which is typically faster within nodes and slower across nodes, and communication stability. As result, the benefits of LASP-2 become more evident in clusters with slower interconnects, and vice versa. It is important to note that this cost model only accounts for communication, excluding computation or data-loading. In practice, communication represents smaller portion of the total cost, thus the overall training speedup achieved by LASP-2 is less than 1 times. LASP-2 performs best in scenarios involving long sequences, large clusters, slow communication links, and efficient data-loading and computation. 3.5. Hybrid Model Sequence Parallelism The hybrid model, which combines linear transformer layers with standard transformer layers that utilize softmax self-attention, has been demonstrated to effectively enhance long-context capabilities, particularly in tasks such as reLASP-2 call and retrieval. To optimize SP in such hybrid models, we propose an extended version of LASP-2, referred to as LASP-2H. This approach introduces comprehensive solution by incorporating SP into both the linear attention and standard attention modules. The structure of LASP-2H is illustrated in Fig. 2. On Linear Attention Module. As outlined in Algorithm 1 and Algorithm 2, LASP-2H handles linear attention modules by performing single all-gather communication operation on the memory state Mt Rdd. The communication complexity remains independent of both sequence or chunk length, and only scales linearly with the SP size , making this method efficient in distributed clusters. On Standard Attention Module. Context Parallelism (CP) is SP technique in Megatron-LM that divides network inputs and all activations along the sequence dimension. This approach is specifically tailored for standard softmax attention. While traditional CP implementations in MegatronLM rely on overlapping communication and computation in ring-like structure (Liu et al., 2023), our LASP-2H adopts different method, following the best practice in Llama3 (Dubey et al., 2024). Instead of the ring-style strategy, LASP-2H employs AllGather-based communication on standard attention, where the Kt and Vt tensors are first gathered across devices, after which the attention output is computed locally for the Qt tensor chunk. Although the all-gather communication has higher latency compared to ring-based methods, it provides greater ease and flexibility in handling various types of attention masks, such as document-level masks. This flexibility is particularly beneficial in scenarios where different attention patterns are needed. Additionally, the all-gather latency is minimized because the Kt and Vt tensors are significantly smaller than the Qt tensor, especially when using Grouped Query Attention (GQA) (Ainslie et al., 2023). As result, the time complexity of computing the attention output far exceeds the complexity of all-gather operation. We present the description of AllGather-based Context Parallelism in Algorithm 7 in Appendix A.3. 4. Experiments We conducted an empirical evaluation of LASP-2 by applying it to model based on Llama3 (Dubey et al., 2024). We replaced the standard softmax attention with various linear attention modules, including the original basic linear attention (Katharopoulos et al., 2020), Lightning Attention (Qin et al., 2024b), Retention (Sun et al., 2023), Gated Linear Attention (GLA) (Yang et al., 2023), Based (Arora et al., 2024), and Rebased (Aksenov et al., 2024). This modified model, termed Linear-Llama3, comprises 16 (linear transformer) layers, with total of 1B parameters. Additionally, we created hybrid model by retaining transformer layers with standard softmax attention at every fourth layer of LinearLlama3, forming 1/4 hybrid architecture. All experiments were conducted on the SlimPajama dataset (Soboleva et al., 2023), utilizing the Llama3 tokenizer (Dubey et al., 2024). The full dataset contains 627B tokens, but for our experiments, we used 50B tokens subset derived from the first chunk of the training split. The experiments were performed using GPT-style autoregressive language modeling tasks with attention masks, as this setup mirrors many practical scenarios where such tasks are commonly applied. Note that the primary focus of these experiments is to assess the training efficiency of LASP-2 when handling very-long input sequences. Training large language model with optimal long-context capabilities falls outside the scope of this study. Besides the following results, we have provided more additional experiment results in Appendix A.5. 4.1. Experimental Setup Hardware and Software. Our experiments were conducted on configuration of up to 16 DGX-A100 servers, each equipped with 8 A100 GPUs. The GPUs are connected through NVSwitch, offering an inter-GPU bandwidth of 600 GBps. The experiments were implemented using PyTorch 2.3.1, with support from CUDA 12.1, cuDNN 8.9.2, and NCCL 2.20.5. The algorithm was developed on top of NVIDIAs Megatron-Core 0.9.0 (Shoeybi et al., 2019). We use Triton 2.3.1 (Tillet et al., 2019) to accelerate the linear attention computation on GPU, and take FlashAttention2 (Dao, 2023) as the standard attention implementation. When implement other SP methods (e.g., Ring Attentoin, Megatron-SP) on linear attention instances for the purpose of comparison, we do not incorporate the right-product kernel trick. We maintain the use of each methods original communication primitives and computational manners as they originally proposed for standard attention. Hyperparameters. For training the Linear-Llama3 model, we employed cosine learning rate schedule with linear warm-up phase (Sun et al., 2024b). The minimum learning rate was set to 1e6. We applied gradient clipping with value of 1.0 and weight decay at rate of 0.1. The Adam optimizer (Kingma & Ba, 2014) was used, configured with β1 = 0.9 and β2 = 0.95 (Zhang et al., 2019; Zhou et al., 2020). Additionally, the dropout rate in both attention and hidden layers was set to 0 (Tang et al., 2023). 4.2. Speed To assess the speed performance of our proposed LASP-2, we conducted comparison against existing SP methods, including Megatron-SP (Korthikanti et al., 2022), Ring Attention (Liu et al., 2023), and LASP-1 (Sun et al., 2024a). As depicted in Fig. 3, LASP-2 demonstrated superior throughput, particularly when sequence lengths exceeded 64K. This 7 Table 2: Convergence Performance Results. All experiments used 8 A100 GPUs, sequence length of 16K, and batch size of 8, trained on 50B tokens from the SlimPajama corpus. LASP-2 Model SP Method Attention Module Pure Model 1/4 Hybrid Model Thpt Loss Thpt Loss Llama3 Ring Attention Standard Attention 16549.5 2. Linear-Llama3 LASP-2(H) Basic Linear Attention Lightning Attention Retention GLA Based Rebased 17834.3 17926.1 17859.6 17785.3 17946.1 17896.2 2.892 2.862 2.867 2.845 2.754 2. 17394.7 17384.2 17352.5 17273.2 17462.5 17284.5 2.824 2.758 2.759 2.754 2.751 2.787 Figure 3: Speed Comparison (tokens/s). Experiments were carried out on pure Linear-Llama3-1B model, utilizing the basic linear attention module. total of 64 A100 GPUs were employed, and the SP size was also set to 64. To accommodate very-long sequence lengths, such as 2048K, the batch size was kept fixed at 1 throughout this experiment. performance advantage became increasingly prominent as sequence lengths grew longer. Specifically, at sequence length of 512K, LASP-2 outperformed Ring Attention by 17.8% and surpassed LASP-1 by 7.3%. This advantage became even more pronounced at sequence length of 2048K, where LASP-2 achieved throughput gains of 36.6% over Ring Attention and 15.2% over LASP-1. 4.3. Scalability We assessed the scalability of LASP-2 in terms of both GPU memory usage and throughput by adjusting the sequence length and the number of GPUs. The results were displayed in Figure 4. LASP-2 demonstrated the ability to scale linearly with the input sequence length by increasing the number of GPUs. For instance, while maintaining the same memory cost per GPU, using 8 GPUs allowed training on sequences up to 128K in length, whereas 128 GPUs (16 8 GPUs) enabled training on sequences as long as 2048K (16 128K). Additionally, we observed that increasing both sequence length and device numbers results in higher throughput, indicating improved communication efficiency and linear scalability. More detailed quantitative scalability outcomes are provided in Table 6 in Appendix A.5. 8 Figure 4: Scalability Results. Experiments were conducted on pure Linear-Llama3-1B model using the Basic Linear Attention module. SP size was always equal to number of GPUs. Batch size was fixed as 1 to accommodate very-long sequence lengths, e.g., 2048K. The sign \"\" with dotted line represented occurring an Out of Memory (OOM). 4.4. Convergence Performance We conducted additional experiments to assess the pretraining convergence performance of LASP-2 on Llama-3 with various attention modules, including standard softmax attention, basic linear attention, Lightning Attention, Retention, GLA, Based, Rebased, and their 1/4 hybrid models. All experiments were performed on the SlimPajama corpus (Soboleva et al., 2023), using 50B tokens, sequence length of 16K, and global batch size of 8, using 8 A100 GPUs. The results, as shown in Table 2, indicated that for pure Linear-Llama3 models with different linear attention modules, LASP-2 achieved comparable, though slightly higher, loss values while maintaining superior throughput. On the 1/4 hybrid Linear-Llama3 model, the loss results were generally better than those of the pure linear models, with Lightning Attention, Retention, and GLA even attaining equivalent or lower loss values compared to the baseline. The Based attention module shows strong throughput and LASPprove learning in long-context tasks. An enhanced version, HGRN2 (Qin et al., 2024d), expands on this approach by incorporating state expansion mechanism that utilizes outer product operations, which allows for greater scalability and improved modeling capabilities over extended sequences. Both RWKV and HGRN series seek to overcome weaknesses of RNNs for efficient long-sequence modeling. 4.5.2. SEQUENCE PARALLELISM SP (Li et al., 2022) is distributed technology designed for training language models more efficiently, which is implemented by dividing long sequence into multiple shorter subsequences and processing these subsequences in parallel on multiple computing devices. Existing SP methods (Korthikanti et al., 2022; Jacobs et al., 2023) whose parallelism degree cannot exceed the number of attention heads, which limits their scalability. Ring Attention (Liu et al., 2023) is proposed to address high memory cost in long sequence modeling by distributing subsequences across different devices and overlapping the communication of KV blocks. LASP (Sun et al., 2024a) proposes new linear attentiontailored SP strategy based on GPU friendly implementation by utilizing P2P ring-style communication strategy, but still lacks of optimizations for hybrid model architecture. 5. Conclusion This paper presents LASP-2, new SP method that addresses the inefficiencies of existing SP approaches for linear sequence modeling. By redesigning the whole algorithm workflow and leveraging single all-gather communication strategy, LASP-2 significantly enhances both the communication and computation parallelism, and enables easier communication-computation overlapping, comparing with preceding work LASP-1. Our results demonstrate that LASP-2 offers significant improvements in speed and scalability, especially in the context of very-long sequence length. Furthermore, the extension to LASP-2H enables efficient SP in hybrid models that integrate both linear and standard attention modules, both utilize an unified all-gatherbased communication primitive. Experimental evaluations on the Linear-Llama3 models validate these advancements, with LASP-2 outperforming previous methods like LASP-1 and Ring Attention by substantial margins, particularly at extreme sequence lengths. These findings confirm the practical utility of LASP-2 for large-scale distributed systems, making it promising approach for future applications in long-sequence linear transformer models. loss performance, since its original design uses mix of (Taylor) linear attention and sliding window attention. The 1/4 hybrid model striked balance between throughput and convergence performance, performing competitively when compared to both the baseline and its pure linear version. 4.5. Related Work 4.5.1. LINEAR SEQUENCE MODELING Linear Attention. Vanilla linear attention (Katharopoulos et al., 2020) introduces the use of kernel methods as replacement for the Softmax attention (Vaswani et al., 2017), thereby reducing the computational complexity to linear in sequence length. Following this, several variants of linear attention have been proposed. TransNormerLLM (Qin et al., 2023b;a) proposes Lightning Attention, refined linear attention mechanism that accelerates processing by optimizing IO interactions. Lightning Attention-2 (Qin et al., 2024b) further realizes the theoretical advantages of linear attention by separately handling interand intra-block computations. RetNet (Sun et al., 2023) introduces retention mechanism that combines recurrence with attention, benefiting from both parallel training and linear inference. Gated Linear Attention (GLA) (Yang et al., 2023) incorporates data-independent gating mechanism into the linear attention framework, and presents an efficient algorithm for training. DeltaNet (Schlag et al., 2021) and its parallelized version (Yang et al., 2024) use delta rule-like update to enhance linear attention performance in long-context scenarios. Finally, Gated Slot Attention (GSA) (Zhang et al., 2024), inspired by GLA, introduces gated linear attention mechanism with bounded-memory slot control to further improve efficiency. State Space Modeling. The SSM serves as powerful framework for representing the behavior of sequences within dynamic systems, and it has shown considerable promise in the realm of linear sequence modeling. Mamba (Gu & Dao, 2023) incorporates mechanism for selecting states, thereby facilitating the scaling of linear sequence lengths. This architecture has been further enhanced in Mamba-2 (Dao & Gu, 2024), where the introduction of the state space duality (SSD) framework optimizes its performance. Linear RNN. Traditional RNNs face significant challenges in handling long-context sequence modeling, primarily due to their inherent sequence dependency during training, which prevents them from fully capitalizing on scaling laws (Sun et al., 2023). To address these limitations, RWKV (Peng et al., 2023; 2024) was introduced as linear RNN-based large language model that aims to efficiently manage long-term dependencies. Additionally, HGRN (Qin et al., 2024e) highlights the critical role of data-dependent decay mechanisms in enhancing linear RNN performance, demonstrating how adjustments to decay parameters can imLASP-"
        },
        {
            "title": "Impact Statement",
            "content": "This work represents notable advancement in artificial intelligence and machine learning, particularly in improving the efficiency and scalability of linear attention-based models. LASP-2 enables the processing of much longer sequences compared to existing methods while significantly accelerating computation, making it highly beneficial for tasks like natural language understanding, genomic sequence analysis, and time-series forecasting. However, the enhanced capabilities and efficiency introduced by LASP-2 also raise ethical and societal considerations, such as the potential for misuse in generating persuasive but misleading content or in surveillance applications. Nevertheless, the contributions of LASP-2 to reducing computational overhead and energy consumption in training large models may also bring positive environmental impacts."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebrón, F., and Sanghai, S. GQA: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Aksenov, Y., Balagansky, N., Vaina, S. M. L. C., Shaposhnikov, B., Gorbatovski, A., and Gavrilov, D. Linear transformers with learnable kernel functions are better incontext models. arXiv preprint arXiv:2402.10644, 2024. Arora, S., Eyuboglu, S., Zhang, M., Timalsina, A., Alberti, S., Dylan Zinsley, J. Z., Rudra, A., and Ré, C. Simple linear attention language models balance the recallthroughput tradeoff. arXiv preprint arXiv:2402.18668, 2024. Ding, H., Wang, Z., Paolini, G., Kumar, V., Deoras, A., Roth, D., and Soatto, S. Fewer truncations improve language modeling. arXiv preprint arXiv:2404.10830, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Jacobs, S. A., Tanaka, M., Zhang, C., Zhang, M., Song, S. L., Rajbhandari, S., and He, Y. Deepspeed Ulysses: System optimizations for enabling training of extreme long sequence transformer models, 2023. Jelassi, S., Brandfonbrener, D., Kakade, S. M., and Malach, E. Repeat after me: Transformers are better than state space models at copying. arXiv preprint arXiv:2402.01032, 2024. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast autoregressive transformers In International Conference on with linear attention. Machine Learning, pp. 51565165. PMLR, 2020. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Korthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., and Catanzaro, B. Reducing activation recomputation in large transformer models, 2022. Briakou, E., Cherry, C., and Foster, G. Searching for needles in haystack: On the role of incidental bilingualism in palms translation capability. arXiv preprint arXiv:2305.10266, 2023. Li, A., Gong, B., Yang, B., Shan, B., Liu, C., Zhu, C., Zhang, C., Guo, C., Chen, D., Li, D., et al. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313, 2025. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Dao, T. and Gu, A. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Dao, T., Fu, D., Ermon, S., Rudra, A., and Ré, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. Li, S., Xue, F., Baranwal, C., Li, Y., and You, Y. Sequence parallelism: Long sequence training from system perspective, 2022. Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos, I., Safahi, E., Meirom, S., Belinkov, Y., ShalevShwartz, S., et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. Liu, H., Zaharia, M., and Abbeel, P. Ring attention with blockwise transformers for near-infinite context, 2023. 10 LASPPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Biderman, S., Cao, H., Cheng, X., Chung, M., Derczynski, L., Du, X., Grella, M., Gv, K., He, X., Hou, H., Kazienko, P., Kocon, J., Kong, J., Koptyra, B., Lau, H., Lin, J., Mantri, K. S. I., Mom, F., Saito, A., Song, G., Tang, X., Wind, J., Wozniak, S., Zhang, Z., Zhou, Q., Zhu, J., and Zhu, R.-J. RWKV: Reinventing RNNs for the transformer era. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1404814077, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp. 936. URL https://aclanthology.org/2023. findings-emnlp.936. Peng, B., Goldstein, D., Anthony, Q., Albalak, A., Alcaide, E., Biderman, S., Cheah, E., Ferdinan, T., Hou, H., Kazienko, P., et al. Eagle and Finch: RWKV with matrixvalued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. Pouransari, H., Li, C.-L., Chang, J.-H. R., Vasu, P. K. A., Koc, C., Shankar, V., and Tuzel, O. Dataset decomposition: Faster llm training with variable sequence length curriculum. arXiv preprint arXiv:2405.13226, 2024. Qin, Z., Li, D., Sun, W., Sun, W., Shen, X., Han, X., Wei, Y., Lv, B., Luo, X., Qiao, Y., et al. TransNormerLLM: faster and better large language model with improved transnormer. 2023a. Qin, Z., Li, D., Sun, W., Sun, W., Shen, X., Han, X., Wei, Y., Lv, B., Yuan, F., Luo, X., et al. Scaling transnormer to 175 billion parameters. arXiv preprint arXiv:2307.14995, 2023b. Qin, Z., Shen, X., Li, D., Sun, W., Birchfield, S., Hartley, R., and Zhong, Y. Unlocking the secrets of linear complexity sequence model from unified perspective. arXiv preprint arXiv:2405.17383, 2024a. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., and Zhong, Y. Lightning Attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024b. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., and Zhong, Y. Various lengths, constant speed: Efficient language modeling with lightning attention. arXiv preprint arXiv:2405.17381, 2024c. Qin, Z., Yang, S., Sun, W., Shen, X., Li, D., Sun, W., and Zhong, Y. HGRN2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024d. Qin, Z., Yang, S., and Zhong, Y. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024e. Qu, X., Dong, D., Hu, X., Zhu, T., Sun, W., and Cheng, Y. LLaMA-MoE v2: Exploring sparsity of llama from perspective of mixture-of-experts with post-training. arXiv preprint arXiv:2411.15708, 2024. Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models, 2020. Ren, L., Liu, Y., Lu, Y., Shen, Y., Liang, C., and Chen, W. Samba: Simple hybrid state space models for efficient unlimited context language modeling. arXiv preprint arXiv:2406.07522, 2024. Schlag, I., Irie, K., and Schmidhuber, J. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, 2021. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. arXiv preprint arXiv:2407.08608, 2024. Shen, X., Li, D., Leng, R., Qin, Z., Sun, W., and Zhong, Y. Scaling laws for linear complexity language models. arXiv preprint arXiv:2406.16690, 2024. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-LM: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R., Hestness, J., and Dey, N. SlimPajama: 627B token cleaned and deduplicated version of RedPajama, 2023. URL https://huggingface.co/ datasets/cerebras/SlimPajama-627B. Sun, W., Qin, Z., Li, D., Shen, X., Qiao, Y., and Zhong, Y. Linear attention sequence parallelism. arXiv preprint arXiv:2404.02882, 2024a. Sun, W., Qin, Z., Sun, W., Li, S., Li, D., Shen, X., Qiao, Y., and Zhong, Y. CO2: Efficient distributed training with full communication-computation overlap. arXiv preprint arXiv:2401.16265, 2024b. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Tang, X., Sun, W., Hu, S., Sun, Y., and Guo, Y. MS-Net: multi-path sparse model for motion prediction in multiscenes. IEEE Robotics and Automation Letters, 2023. Team, I. InternLM: multilingual language model with progressively enhanced capabilities, 2023. 11 LASP-2 Zhou, B., Liu, J., Sun, W., Chen, R., Tomlin, C. J., and Yuan, Y. pbSGD: Powered stochastic gradient descent methods for accelerated non-convex optimization. In IJCAI, pp. 32583266, 2020. Tillet, P., Kung, H.-T., and Cox, D. D. Triton: an intermediate language and compiler for tiled neural network computations. Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, 2019. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Waleffe, R., Byeon, W., Riach, D., Norick, B., Korthikanti, V., Dao, T., Gu, A., Hatamizadeh, A., Singh, S., Narayanan, D., et al. An empirical study of mambabased language models. arXiv preprint arXiv:2406.07887, 2024. Xue, F., Chen, Y., Li, D., Hu, Q., Zhu, L., Li, X., Fang, Y., Tang, H., Yang, S., Liu, Z., et al. LongVILA: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Yang, S., Wang, B., Zhang, Y., Shen, Y., and Kim, Y. Parallelizing linear transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484, 2024. Zeng, J., Li, M., Wu, Z., Liu, J., Liu, Y., Yu, D., and Ma, Y. Boosting distributed training performance of the unpadded bert model. arXiv preprint arXiv:2208.08124, 2022. Zhai, Y., Jiang, C., Wang, L., Jia, X., Zhang, S., Chen, Z., Liu, X., and Zhu, Y. ByteTransformer: highperformance transformer boosted for variable-length inputs. In 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS), pp. 344355. IEEE, 2023. Zhang, H.-T., Sun, W., Li, Y., Fu, D., and Yuan, Y. fast optimal power flow algorithm using powerball method. IEEE Transactions on Industrial Informatics, 16(11): 69937003, 2019. Zhang, Y., Yang, S., Zhu, R., Zhang, Y., Cui, L., Wang, Y., Wang, B., Freda Shi, Bailin Wang, W. B., Zhou, P., and Fu, G. Gated slot attention for efficient linear-time sequence modeling. arXiv preprint arXiv:2409.07146, 2024. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch FSDP: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. 12 LASP-2 A. Appendix A.1. LASP-2 Algorithms (Backward Pass) See Algorithm 3 and Algorithm 4. Algorithm 3 LASP-2 w/o Masking (Backward Pass) 1: Input: distributed world size , sequence parallel size = , Qt, Kt, Vt, Ot, dOt RCd for chunk {1, , }. 2: for chunk {1, , } on ranks {1, , } in parallel do 3: 4: 5: 6: 7: 8: 9: end for 10: return dQ = [dQt]T Compute dMt = (Qt)dOt. Communicate [dM]T Compute dM1:T = Sum([dM]T Compute dQt = dOtM 1:T . Compute dKt = VtdM 1:T . Compute dVt = KtdM1:T . 1 = AllGather([dM]T t+1). 1 , dV = [dVt]T 1 . 1 , dK = [dKt]T 1 ). 1 = AllGather([dM]T Algorithm 4 LASP-2 w/ Masking (Backward Pass) 1: Input: distributed world size , sequence parallel size = , Qt, Kt, Vt, Ot, dOt RCd for chunk {1, , }. 2: for chunk {1, , } on ranks {1, , } in parallel do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: Compute dMt = (Qt)dOt. Communicate [dM]T Compute dQt,intra = [(dOtV Compute dKt,intra = [(dOtV Compute dVt,intra = [(QtK Compute dQt,inter = dOtM Compute suffix sum dMt+1:T = SuffixSum([dM]T Compute dKt,inter = VtdM t+1:T . Compute dVt,inter = KtdMt+1:T . Combine intraand inter-chunk parts of dQt, dKt, dVt ) Ψ]Kt. ) Ψ]Qt. ) Ψ]dOt. 1:t1. t+1). 1 ). dQt = dQt,intra + dQt,inter, dKt = dKt,intra + dKt,inter, dVt = dVt,intra + dVt,inter. 13: end for 14: return dQ = [dQt]T 1 , dK = [dKt]T 1 , dV = [dVt]T 1 . A.2. LASP-1 Algorithms See Algorithm 5 and Algorithm 6. A.3. AllGather-based Context Parallelism See Algorithm 7. A.4. Compatibility A.4.1. HYBRID PARALLELISM LASP-2 enables the selection of sequence parallel size that is smaller and divisible by the distributed world size. This setup splits the input data along both the batch and sequence dimensions, parallelization strategy known as data-sequence hybrid parallelism. The ZeRO-series optimizers (Rajbhandari et al., 2020) and FSDP (Zhao et al., 2023) offer methods for distributing model states such as optimizer states, gradients, and model parameters across all GPUs in the distributed system. As these techniques are variants of data parallelism, they integrate seamlessly with LASP. Their primary objective of minimizing the memory footprint of model states complements LASP-2s specific focus on reducing activation memory on each GPU, making the training of large-scale models that handle long sequence lengths significantly more manageable. 13 Algorithm 5 LASP-1 w/o Masking LASP-2 Vt. 1: Input: input sequence X, distributed world size , sequence parallel size = . 2: Distribute input = [Xt]T 1 . 3: for chunk {1, , } at rank {1, , } in parallel do Compute Qt = XtWQ, Kt = XtWK, Vt = XtWV . 4: Compute Mt = 5: 6: end for 7: for chunk {1, , } at rank {1, , } sequentially do 8: 9: 10: 11: 12: end for 13: return = [Ot] with {1, , }. Recv activation Mt1 from rank (i 1). Save Mt1 in memory for backward computation. Compute Ot = QtMt1. Update Mt = Mt1 + Send activation Mt to rank (i + 1). Vt. Algorithm 6 LASP-1 w/ Masking 1: Input: input sequence X, distributed world size , sequence parallel size = . 2: Distribute input = [Xt]T 1 . 3: Initialize mask matrix Ψ, where Ψij = 1 if j, and Ψij = if < j. 4: for chunk {1, , } at rank {1, , } in parallel do Compute Qt = XtWQ, Kt = XtWK, Vt = XtWV . 5: Compute Mt = (Kt)Vt. 6: Compute Ot,intra = [(QtK 7: 8: end for 9: for chunk {1, , } at rank {1, , } sequentially do 10: 11: 12: 13: 14: 15: end for 16: return = [Ot] with {1, , }. Recv activation Mt1 from rank (i 1). Save Mt1 in memory for backward computation. Compute Ot,inter = QtMt1. Compute Ot = Ot,intra + Ot,inter. Update Mt = Mt1 + Send activation Mt to rank (i + 1). ) Ψ]Vt. Vt. LASP-2 also offers support for both tensor parallelism (TP) and pipeline parallelism (PP). In the case of TP, its integration with LASP-2 is straightforward and efficient. Linear attention layers apply TP to break down matrix operations across both intra-chunk and inter-chunk computations. At the same time, the MLP layers are processed as usual under TP, without any modification. When LASP-2 is paired with PP, instead of using traditional micro-batches, it substitutes them with sub-sequences extracted from the mini-batch. One key difference from standard PP is that each device locally and specifically stores the intermediate states, Mt during the forward pass and dMt during the backward pass without communicating these states to other devices. A.4.2. VARIABLE LENGTH During pretraining, the batch typically contains sequences of uniform length. However, when finetuning or during inference, the model might encounter input sequences of varying lengths. straightforward solution to address this is to right-pad all sequences in batch to match the length of the longest sequence. Unfortunately, this method can be inefficient, especially when the lengths differ significantly across sequences. For standard transformers, more sophisticated approaches have been developed to handle this challenge. These include techniques like load-balancing across GPUs without padding (Zeng et al., 2022; Zhai et al., 2023) or packing multiple sequences into single batch and adjusting the attention mask accordingly (Ding et al., 2024; Pouransari et al., 2024). LASP-2 can manage variable sequence lengths efficiently by treating the entire batch as single long sequence, streamlining the process without requiring padding. 14 Algorithm 7 AllGather-based Context Parallelism LASP1: Input: input sequence X, distributed world size , sequence parallel size = . 2: Distribute = [Xt]T 1 . 3: for chunk {1, , } on ranks {1, , } in parallel do Calculate Qt = XtWQ, Kt = XtWK, Vt = XtWV . 4: Communicate [Kt]T 1 ) and [Vt]T 5: Concatenate = Concat([Kt]T 6: Compute Ot = Softmax(QtK/ 7: 8: end for 9: return = [Ot]T 1 . 1 ) and = Concat([Vt]T 1 = AllGather([Kt]T 1 = AllGather([Vt]T d)V. 1 ). 1 ). A.5. Additional Experiment Results A.5.1. BIDIRECTIONAL LANGUAGE MODELING TASK To evaluate on the bidirectional language modeling task, we take RoBERTa as the base model and replace its standard attention modules with Basic Linear Attention, train it on 4 A100 GPUs for 50K iterations with total input sequence length of 2048. As the results shown in Table 3, LASP-2 with Basic Linear Attention is able to reach an approximate convergence performance with Ring Attention on the standard attention based model. Table 3: Convergence Performance on Bidirectional Language Modeling Task. Both training and validation loss values are reported. Model Training Loss Validation Loss RoBERTa Baseline (Ring Attention) RoBERTa with Basic Linear Attention (LASP-2) 1.815 1. 1.957 1.957 A.5.2. ABLATION STUDY ON HYBRID RATIO We provide ablation results on the hybrid ratio of hybrid models. Let \"L\" denotes linear Transformer layers and \"N\" denotes normal Transformer layers. The hybrid models evaluated here have architectures of: 0 Hybrid: \"LLLL LLLL LLLL LLLL\"; 1/8 Hybrid: \"LLLL LLLN LLLL LLLN\"; 1/4 Hybrid: \"LLLN LLLN LLLN LLLN\"; 1/2 Hybrid: \"LNLN LNLN LNLN LNLN\". Comparing with the Llama3-1B baseline using standard attention, whose loss value is 2.759, it shows that higher hybrid ratios tend to lead better convergence performance, but sometimes, moderate hybrid ratio may reach better result. Table 4: Ablation Study on Hybrid Ratio in Hybrid Models. Loss values are reported in the Table. Note that pure linear models use LASP-2, while hybrid models use LASP-2H. Linear Sequence Modeling Module 0 Hybrid (Pure Linear Model) 1/8 Hybrid 1/4 Hybrid 1/2 Hybrid Basic Linear Attention Lightning Attention Retention GLA 2.892 2.848 2.855 2. 2.826 2.756 2.757 2.751 2.824 2. 2.758 2.754 2.775 2.742 2.748 2. A.5.3. ABLATION STUDY ON VARYING SIZES OF GATHERING We have conducted ablation study on varying sizes of gathering memory states. Considering batch size of 1, in the Linear-Llama3-1B model (with 16 heads and hidden dimension of 2048), the tensor shape of each memory state is [1, 16, 2048, 2048]. We use 64 GPUs and sequence lenghth of 1024K, repeat each test 10 times and report their mean values. We change the split size of gathering memory states and present the LASP-2 throughput results in Table 5. It can be seen that smaller split size (i.e., more number of splits) tends to lead lightly slower throughput. The results show that the utilization of all-gather operation is not the only reason of efficiency enhancement. The communication manner as well as 15 the computational workflow reorganization plays an important role. LASP-2 Table 5: Throughput Results (tokens/sec) on Varying Split Sizes of Gathering. Linear-Llama3-1B model (with 16 heads and hidden dimension of 2048) is used. Split Size of Gathering 2048 Number of Splits 1 512 128 16 32 64 Throughput 486166 486169 486158 A.5.4. QUANTITATIVE SCALABILITY RESULTS See Table 6 in next page. LASP-2 Table 6: Quantitative Scalability Results of LASP-2 on Throughput (tokens/sec) and Memory Usage Per GPU (GB). Experiments are performed on Linear-Llama3-1B, scaling sequence length from 2K to 4096K. Sequence Length Number of GPUs Throughput Memory Usage Per GPU 2K 4K 8K 16K 32K 64K 128K 256K 512K 1024K 2048K 4096K 25.6 25.6 25.6 25.6 25.6 25.6 25.6 25.6 25.6 25.6 25.6 25. 25.6 25.6 25.6 25.6 28.7 25.6 25.6 25.6 33.8 28.7 25.6 25.6 40.2 33.8 28.7 25.6 57.8 40.2 33.8 28.7 OOM 57.8 40.2 33. OOM OOM 57.8 40.2 OOM OOM OOM 57.8 OOM OOM OOM OOM 16 32 64 128 16 32 64 128 16 32 64 16 32 64 128 16 32 64 128 16 32 64 128 16 32 64 128 16 32 64 128 16 32 64 16 32 64 128 16 32 64 128 16 32 64 128 1254 1209 1285 1205 2478 2446 2327 2344 4835 4784 4693 9530 9494 9305 9313 18105 17755 17835 17807 35507 34240 34118 33344 68406 68545 67344 66811 135635 132605 130215 131550 OOM 250586 245353 OOM OOM 442221 416465 OOM OOM OOM 769030 OOM OOM OOM OOM"
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "South China University of Technology",
        "The Chinese University of Hong Kong"
    ]
}