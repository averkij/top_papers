{
    "paper_title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "authors": [
        "Fangyu Lei",
        "Jinxiang Meng",
        "Yiming Huang",
        "Junjie Zhao",
        "Yitong Zhang",
        "Jianwen Luo",
        "Xin Zou",
        "Ruiyi Yang",
        "Wenbo Shi",
        "Yan Gao",
        "Shizhu He",
        "Zuo Wang",
        "Qian Liu",
        "Yang Wang",
        "Ke Wang",
        "Jun Zhao",
        "Kang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 4 2 3 4 0 . 2 1 5 2 : r DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle Fangyu Lei1,2,3, Jinxiang Meng1,2, Yiming Huang5, Junjie Zhao3, Yitong Zhang6, Jianwen Luo1,2, Xin Zou3, Ruiyi Yang3, Wenbo Shi3, Yan Gao3, Shizhu He1,2, Zuo Wang3, Qian Liu4, Yang Wang3, Ke Wang3,, Jun Zhao1,2, Kang Liu1,2, 1Institute of Automation, CAS 3ByteDance Seed 2University of Chinese Academy of Sciences, 4TikTok 5UC San Diego 6NUS Equal Contribution, Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decisionoriented insights. We introduce DAComp, benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at da-comp.github.io. Date: December 5, 2025 Correspondence: Kang Liu at kliu@nlpr.ia.ac.cn, Ke Wang at wangke@bytedance.com Project Page: da-comp.github.io"
        },
        {
            "title": "Introduction",
            "content": "Data intelligence, the process of transforming raw and fragmented data into actionable insights, has become cornerstone of modern enterprises. The remarkable reasoning and code generation capabilities of Large Language Models (LLMs) [1, 8, 23] have opened new avenues for automating data intelligence tasks. LLM-based agents have demonstrated considerable promise across wide range of applications, including text-to-SQL [18, 20, 39], software engineering [3, 14], and general computer control [31, 33, 42]. However, the advancement of these agents into enterprise data intelligence remains constrained by the absence of benchmarks that 1 Figure 1 DAComp aims to evaluate LLMs on full-lifecycle data intelligence workflows, encompassing repository-level data engineering (DE ) and open-ended data analysis (DA). faithfully reflect real-world complexity. This gap between existing benchmarks and real enterprise practice calls for benchmark that evaluates agents along two distinct axes: Hard (engineering realism) and Soft (analytical openness). The Hard axis reflects the capacity for systematic large-scale code implementation, similar to the responsibilities of data engineers. For example, this means not only generating single SQL query but also orchestrating and evolving complex data workflows under changing requirements. The Soft axis reflects the capacity for strategic reasoning, aligning more closely with the role of data analysts. For example, this involves facing an open-ended business question, planning multi-step analytical workflows, synthesizing insights across analytical results, generating visualizations and crafting decision-oriented reports. Most benchmarks fail to capture these two key dimensions. They reduce complex engineering to isolated code snippet generation, missing the Hard axis, and reduce open-ended analysis to deterministic answers, missing the Soft axis. To fill this gap, we present DAComp, benchmarking agents on full lifecycle data intelligence tasks, as illustrated in Fig. 1. DAComp-DE is the first to introduce repository-level data engineering tasks where agents must orchestrate multi-layered data workflows by generating DAG on complex enterprise schemas. It includes three distinct task types: (1) DE-Architecture tasks focus on the high-level planning of detailed engineering specifications. (2) DE-Implementation tasks require agents to build multi-stage data pipelines from scratch; (3) DE-Evolution tasks challenge them to modify existing systems in response to new requirements; and Both DE-Impl and DE-Evol tasks are demanding, often requiring large-scale code changes that involve over 4, 000 lines of code across more than 30 files, mirroring real-world engineering workloads. DAComp-DA is the first to pioneer real-world, open-ended data analysis. In these scenarios, agents are presented with complex questions over downstream analytical data. Unlike prior work with deterministic answers [15, 18], the tasks resemble real analyst settings: agents must write SQL/Python to aggregate, compute, and analyze intermediate results in order to generate insights, reports and visualizations, thereby emphasizing both the rigor of analytical precision and the practical utility for human decision-making. To facilitate broad applicability, we also release DAComp-zh, high-quality Chinese adaptation of the benchmark, along with baseline results. The evaluation methods of such complex tasks are non-trivial. For deterministic DE-Impl and DE-Evol tasks, we adopt an execution-based method to systematically evaluate the repo-level code generation performance. The open-ended DA and DE-Arch tasks are assessed by an LLM judge [19], whose evaluation is guided by our novel rubric framework. Instead of relying on single answer key, this framework explicitly defines and assesses multiple valid solution paths for each open-ended problem, enabling robust, multifaceted assessment that rewards diverse analytical strategies. The reliability of this LLM judge has been confirmed through rigorous validation experiments, which show strong agreement with human experts. 2 Our experiments on DAComp underscore significant challenge for current models: even state-of-the-art agents falter when confronted with its enterprise-level complexity. In DE tasks, agent capabilities are pushed to their limits, with average scores below 40% and strict success rates under 10%, revealing critical gap in real repository-level engineering capabilities. In the same vein, agents also exhibit poor performance on open-ended problems requiring autonomous planning. Performance on DA tasks plummets to below 50% for most models, with only few proprietary systems demonstrating more robust analytical skills. Ultimately, progress in data agents demands shift from mere code accuracy to the nuanced capabilitiesplanning, open-ended reasoning, and systematic synthesisrequired to deliver insights that are both analytically rigorous and strategically actionable. By providing this rigorous, realistic testbed, DAComp aims to shift the focus of data agent development from isolated skills to the integrated, full-lifecycle capabilities required in the real-world scenarios."
        },
        {
            "title": "2 Benchmark Construction",
            "content": "In this section, we introduce the definition, annotation pipeline, evaluation methods and statistics."
        },
        {
            "title": "2.1 Task Definition",
            "content": "To bridge this gap, we design tasks that evaluate data agents on real-world challenges. Specifically, we assess their ability to act as data engineers performing repository-level data engineering and as data analysts navigating open-ended data analysis, as depicted in Fig. 1. DAComp-DE. An agent πde is tasked with handling the full DE lifecycle including architecture, implementation, and evolution. Formally, the process is modeled as (S, C) = πde(Qde, C0, B), where Qde is the initial high-level requirement, denotes the engineering specification (e.g., Data Contract), is the database and is the final DE repository. This unified capability is evaluated across three task types: (1) DE-Arch : Given high-level requirement Qde and an initial repository C0, this task evaluates the agents ability to produce the engineering specification S. (2) DE-Impl : Given detailed specification and an empty repository (C0 = ), this task evaluates the agents ability to implement the DE repository from scratch. (3) DE-Evol : Given an existing repository C0 and new specification S, this task evaluates the agents ability to update the repository into C. DAComp-DA. Given an analysis-ready data (semantic layer) and an open-ended question Qda, an agent with policy πda produces analysis artifacts = πda(Qda, D) (e.g., analytical reports, key insights and actionable recommendations). This task is inherently open-ended, as single question may be approached through multiple valid analytical paths, without fixed standard answer."
        },
        {
            "title": "2.2 Evaluation Metrics\nLLM-judge with hierarchical rubrics and GSB scoring. The\nLLM judge evaluates outputs O along six dimensions: Com-\npleteness, Accuracy, Insightfulness, Readability, Analytical\ndepth and Visualization (see App. A.3.1). The hierarchical\nrubric assesses the first three, while the Good–Same–Bad\n(GSB) score [41] covers the latter three. Visualization specifi-\ncally assesses the agent’s ability to translate numerical results\ninto intuitive chart. As shown in Fig. 2, the rubric (R) de-\ncomposes a question Q into requirements and sub-reqirements.\nEach subrequirement admits multiple valid solution paths,\neach path carrying its own rubric items (colored leaf nodes).\nHuman experts enumerate these paths and merge equivalent\nsolutions in a single path. For scoring, the LLM judge se-\nlects the best-matching path for each sub-requirement, applies\nonly that path’s items, then aggregates scores bottom-up.\nThis design accommodates diverse correct approaches with-\nout penalizing method choice. We show a detailed rubric",
            "content": "3 Figure 2 Details of hierarchical rubrics. (cid:80)N (cid:80)N example for the penetration and profitability analysis in Tab. 11, with discussion of the path enumeration scheme provided in App.F.1. The rubric score is normalized, weighted sum of satisfied items: , sk = Λ(ck, O) [0, wk]. For the Good-Same-Bad (GSB), the LLM judge only Scorerubric(O, R) = compares the final analytic results against five pre-provided baseline reports, guided by the dedicated rubrics for these axes, yielding the score: Scoregsb(O, Obase) = max(0,GB) . The final score for DA task is G+S+B weighted combination of these two components: Scoreda = α Scorerubric + (1 α) Scoregsb. The open-ended DE-Arch tasks are assessed similarly, though they employ standard, non-hierarchical rubric and do not incorporate the GSB component. Further details are provided in App. A. k=1 sk k=1 wk Execution-based evaluation for deterministic tasks. DE-Impl and DE-Evol tasks are evaluated with three execution-based metrics of increasing strictness: (1) the partial credit Component Score (CS), CSDE-Impl/Evol = (cid:80) wjsj, which evaluates each node in isolation (using gold-standard upstream inputs) to measure total component-level SQL generation; (2) the Cascading Failure Score (CFS), which evaluates nodes sequentially along the DAG and nullifies nodes score if any upstream dependency is incorrect, thus measuring end-to-end data integrity; and (3) the strict Success Rate (SR), SRDE-Impl/Evol = I[j : sj = 1], which requires every single component to be perfect. This suite of metrics is crucial for diagnosing the primary bottleneck: the gap between an agents component-level generation and its ability to perform holistic pipeline orchestration. Further details are provided in App. A.1."
        },
        {
            "title": "2.3 Annotation Pipeline\nDAComp is constructed by 8 experts through a rigorous pipeline to ensure realism, quality, and consistency.\nFurther details and examples are provided in App. E.",
            "content": "1) Data collection. The benchmark is grounded in permissively licensed assets (e.g., Apache-2.0, MIT). For the DE task, we collect 73 enterprise-scale SaaS schemas with data transformation projects, averaging 400 columns each, and populate them with large-scale, relationally consistent synthetic data (see App. E). For the DA task, we curate 100 complex databases from the Web and supplement them with analytical modeling layers derived from DE-transformed data. 2) Task design. At this stage, we generate the DAComp questions. For DA , annotators first draft 8 open-ended analytical questions per analysis-ready table. Five annotators then vote based on realism and difficulty, and the top 2 are retained. For DE-Evol , practicing data engineers author new business requirements aligned with enterprise scenarios and professional standards. For DE-Impl , we reverse engineer selected SaaS transformation projects into single data_contract.yaml, capturing the full DAG and semantics. For DE-Arch , starting from the analytics layer of DE-Impl and DE-Evol examples, DA annotators propose 5 candidate business requirements per project, from which data engineer selects 1 feasible yet challenging requirement. 3) Evaluation construction. We design evaluation protocols for each task. For DA, annotators build hierarchical rubrics as described in 2.2, with at least 3 annotators annotate each question, followed by alignment discussion to resolve discrepancies. For the GSB protocol, experienced data analysts author shared scoring criteria, and baseline reports are created by combining outputs from multiple LLMs. critical aspect of this rubric design is the enumeration of valid solution Paths, process governed by three key principles: (i) ensuring Paths represent distinct, methodologically-sound strategies, not incremental steps; (ii) validating deterministic outputs against programmatically calculated and verifiable anchor values; and (iii) utilizing methodology-based soft constraints to fairly evaluate valid but unenumerated solution paths. (see examples in App.C.4, discussion in App.F.1). To ensure the comprehensiveness of our rubric, we perform validation step: we sample outputs from five diverse LLMs and confirm that our enumerated paths can account for all observed solution strategies, which minimizes the risk of false negatives by ensuring that valid but unanticipated solutions are not unfairly penalized. For DE-Impl and DE-Evol , solutions are deterministic: we implement execution scripts to automatically validate outputs against gold repositories, assigning partial credit at the node/layer level to capture step-wise correctness. 4 Table 1 Comparison of DAComp with other agent benchmarks, highlighting key differences in task scope, task paradigm, and evalution method. DAComp-zh shares the identical task set."
        },
        {
            "title": "Benchmark",
            "content": "Field # Tasks RepoLevel # Cols/ Schema Code Scale (LOC) Primary Output Openended Evaluation Method Agentic Benchmarks SWE-Bench [14] WebArena [42] OSWorld [33] BrowserComp [31]"
        },
        {
            "title": "Software Engineering\nWeb Navigation\nComputer Control\nDeep Research",
            "content": "Data Agent Benchmarks DS-1000 [17] BIRD [20] Spider 2.0 [18] BIRD-CRITIC [21] DA-Code [12] DSBench [15] KramaBench [16] BLADE [10] DABStep [7] Data Science Text-to-SQL Text-to-SQL SQL Debugging Data Science Data Science Data Science Pipelines Data Analysis Data Analysis"
        },
        {
            "title": "DAComp",
            "content": "Data Engineering & Data Analysis 2,294 812 369 2,000 1,000 12,751 632 1,100 500 540 104 259 450 210 N/A N/A N/A N/A N/A 54 320 54 50 100 27 13 10 12"
        },
        {
            "title": "Code Patch\nActions\nActions\nAnswer",
            "content": "3.6 23.5 104.6 5070 85 1020 50100"
        },
        {
            "title": "1 Script\n1 SQL\n1 SQL\n1 SQL\n1 Script\nN Scripts\nN Scripts\nReport\nAnswer",
            "content": "Execution-based Execution-based Execution-based Objective Execution-based Execution-based Execution-based Execution-based Objective Objective LLM-judge LLM-judge Objective 382 2, Doc + Report SQL/Script"
        },
        {
            "title": "Both",
            "content": "Execution-based & LLM-judge(rubrics) Table 2 Key statistics for DAComp. All metrics are per-example averages, except #Total tasks."
        },
        {
            "title": "Metric",
            "content": "Overall (DE-Arch/DE-Impl/DE-Evol/DA) #Total tasks #Question Tokens DAComp-DA Columns / Tables LOC Rubrics (Reqs / Sub-reqs / Paths / Items) Completeness / Accuracy / Insightfulness 30 / 30 / 50 / 100 DAComp-DE DE-Impl raw data (#Tab. / #Col.) 166 / 30,883 / 6,508 / 90 #LOC code scale (Impl / Evol) 84.7 / 3.9 433 3.1 / 5.7 / 12.7 / 22.4 14% / 66% / 20% #Change files (Impl / Evol) #Change columns (Impl / Evol) #DE-Arch rubric DE-Impl layer (#Staging / #Core / #Mart) DE-Evol table change types (#create / #edit)"
        },
        {
            "title": "Value",
            "content": "23.3 / 381.6 2,296 / 949.6 37.0 / 11.7 1,239 / 530.9 18.5 16.0 / 11.8 / 8.8 3.76 / 7."
        },
        {
            "title": "2.4 Dataset Statistics",
            "content": "We present statistical analysis of DAComp, highlighting its main features in comparison with prior datasets in Tab. 1, and providing more detailed characteristics in Tab. 2. DAComp-DE quantifies enterprise-scale engineering complexity. The statistics for DAComp-DE underscore its large scale and complexitydefined by its repo-level paradigm, schemas averaging 412 columns, and solutions requiring over 2,000 lines of codesetting it apart from prior data agent benchmarks. Unlike benchmarks that focus on generating isolated scripts, DAComp introduces tasks on industrial schemas with an average of 32 tables and 412 columns. The engineering effort required is substantial. Implementation tasks involve building entire pipelines from scratch, averaging 4,612 lines of code across 43 distinct files. Evolution tasks simulate realistic maintenance with edits averaging 1,718 LOC across 13 files, agents need to manage data transformation across multi-layered data model (staging, core, and mart). The staging layer involves data cleaning operations, central topic in data governance, which we categorize into four types: validity constraints, consistency constraints, integrity & uniqueness, and anomaly detection (as shown in Fig. 3). Intermediate and marts layers typically focus on complex business logic, entity integrations, and metric aggregations. Figure 3 Data cleaning tasks of DE-Impl staging layer. 5 Table 3 DAComp-DE Baseline Performance. All models are evaluated using the DE-Agent framework (details in App. B.2) across both Implementation (CFS, Max-CFS@8, CS, Max-CS@8) and Evolution (SR@8, CFS, Max-CFS@8); see App. A.1 for metric definitions. The final column reports the aggregated DE Score."
        },
        {
            "title": "Architecture",
            "content": "GPT-5 Gemini-2.5-Pro Qwen3-Coder DeepSeek-V3.1 o3 Qwen3-235B-A22B Qwen3-8B 63.93(2.33) 51.96(1.78) 51.43(3.14) 52.66(2.88) 48.32(2.13) 50.73(2.05) 45.12(2.06)"
        },
        {
            "title": "Implementation",
            "content": "CFS Max-CFS@8 30.79 27.66 23.64 22.33 15.07 2.43 1.31 39.87 36.88 32.86 30.73 22.32 5.77 2.34 CS 61.98 55.32 54.21 50.04 35.55 20.15 15.33 Max-CS@8 68.77 65.32 63.78 60.46 47.81 31.03 21."
        },
        {
            "title": "Evolution",
            "content": "CFS Max-CFS@8 38.75 23.97 27.12 24.11 24.42 12.43 15.89 47.23 38.92 39.77 35.01 32.07 21.89 19.12 Table 4 DAComp-DE-zh(Chinese) Baseline Performance."
        },
        {
            "title": "Architecture",
            "content": "GPT-5 Gemini-2.5-Pro Qwen3-Coder DeepSeek-V3.1 o3 Qwen3-235B-A22B Qwen3-8B 63.60(2.14) 51.90(3.43) 51.11(3.35) 53.08(2.54) 48.02(1.79) 50.61(2.50) 46.22(1.90)"
        },
        {
            "title": "Implementation",
            "content": "CFS Max-CFS@8 30.49 26.98 23.23 22.62 15.00 2.31 1.21 39.24 36.73 32.97 30.84 22.15 5.83 2.16 CS 61.85 55.18 54.59 50.22 35.10 20.03 15.78 Max-CS@8 68.43 65.07 63.69 60.34 47.45 31.27 21."
        },
        {
            "title": "Evolution",
            "content": "CFS Max-CFS@8 37.88 24.28 26.59 24.69 24.23 13.01 15.19 46.91 38.27 39.37 35.17 32.59 21.27 19.35 SR@8 20.00 8.00 12.00 10.00 6.00 2.00 2.00 SR@8 20.00 8.00 12.00 8.00 6.00 0.00 0."
        },
        {
            "title": "DE Score",
            "content": "43.45 32.88 32.80 31.41 28.39 20.15 19."
        },
        {
            "title": "DE Score",
            "content": "42.88 32.55 32.36 31.87 28.20 20.35 19.84 DAComp-DA measures analytical depth and methodological diversity. The design of DAComp-DA moves beyond simple question-answering to assess deep analytical reasoning. Uniquely, DAComp evaluates both deterministic engineering and open-ended analysis, distinction from prior benchmarks that typically focus on only one paradigm. Its open-ended nature is quantified by our hierarchical rubrics, which decompose each of the 100 DA tasks into an average of 3.1 requirements and 5.7 sub-requirements, accommodating roughly 13 valid solution paths. This methodological diversity is evaluated with multi-faceted rubric where scoring items are weighted toward Accuracy (66%) but also reward Completeness (14%) and Insightfulness (20%). While the analytical schemas are more focused than in DE tasks (averaging 4 tables and 85 columns), the required reasoning is still complex, reflected in an average solution length of 347 lines of codesignificantly longer than typical text-to-SQL or single-script data science tasks. Crucially, DAComp-DA places strong emphasis on open-ended data visualization, requiring agents to autonomously select and generate charts that effectively communicate their findings."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "We evaluate state-of-the-art LLMs, including open-source models like Qwen3 [35], DeepSeek-V3.1 [22], and Kimi-K2 [29], as well as proprietary ones such as the Gemini [28], and GPT [24] families. We utilize the widely adopted OpenHands (CodeAct-Agent) framework [30] for both DE and DA tasks. Additionally, we developed custom baseline named DA-Agent for DAComp-DA, which operates via Bash and file system interactions and is capable of executing Python and SQL. The performance of each agent is measured using the metrics detailed in 2.2. We also report two aggregate scores: the DE Score, which is the mean score across all DE tasks (using CFS for Implementation/Evolution), and the Overall Score, representing the mean across the entire benchmark. For the DA score, we use α = 0.6 to aggregate the rubric and GSB scores, with Gemini-2.5-Flash serving as the LLM judge. Further details on the experimental setup and additional results are provided in App. B. 6 Table 5 Detailed performance breakdown on the DAComp-DA benchmark."
        },
        {
            "title": "DA Score",
            "content": "OpenHands Baseline GPT-5 Gemini-2.5-Pro o3 DeepSeek-V3.1 Qwen3-Coder Qwen3-235B-A22B DA-Agent Baseline GPT-5 Kimi-K2 Gemini-2.5-Pro DeepSeek-V3.1 o3 Qwen3-Coder Doubao-Seed-1.6 Qwen3-235B-A22B Qwen3-8B 60.98 45.02 40.13 49.88 33.42 30.7 40.3 30.22 25.5 33.25 21.21 12.23 49.39 40.71 20.45 41.66 25.06 22.11 35.51 48.2 26.22 36.0 20.0 3. 69.8 31.0 27.11 33.2 13.73 1.8 21.4 15.0 6.8 11.0 4.8 0.8 46.99 33.38 26.57 33.87 24.28 12.43 64.23(2.37) 52.31(1.13) 45.43(1.34) 48.74(2.09) 40.73(0.63) 35.12(2.21) 37.45(1.95) 29.37(1.09) 9.89(2.46) 43.81(3.43) 33.56(2.09) 30.30(0.27) 32.97(1.40) 29.54(2.93) 20.05(2.35) 18.45(2.55) 13.11(1.33) 4.12(0.32) 56.89(6.48) 46.82(2.48) 41.45(0.71) 42.43(1.89) 23.95(3.86) 25.53(1.83) 27.51(2.00) 21.50(1.81) 5.05(1.70) 43.59(6.08) 62.20(3.01) 51.60(2.73) 37.25(2.21) 25.24(2.51) 19.37(1.44) 13.25(2.48) 3.64(0.33) 0.13(0.15) 76.80(4.91) 63.75(2.84) 35.75(2.35) 35.00(1.57) 23.81(3.37) 13.42(2.38) 9.01(1.25) 1.56(0.81) 0.00(0.00) 27.44(4.44) 14.40(2.33) 13.40(2.94) 11.45(1.31) 7.32(1.27) 5.15(0.85) 6.80(1.96) 1.87(0.78) 0.15(0.19) 50.84(3.12) 41.89(1.78) 34.70(1.39) 34.33(0.45) 28.20(1.37) 25.13(0.82) 20.74(0.82) 13.25(0.65) 4.47(0.63) Table 6 Detailed performance breakdown on the DAComp-DA-zh (Chinese) benchmark."
        },
        {
            "title": "DA Score",
            "content": "OpenHands Baseline GPT-5 Gemini-2.5-Pro o3 DeepSeek-V3.1 Qwen3-Coder Qwen3-235B-A22B DA-Agent Baseline GPT-5 Gemini-2.5-Pro Kimi-K2 o3 DeepSeek-V3.1 Qwen3-Coder Doubao-Seed-1.6 Qwen3-235B-A22B Qwen3-8B 70.56 55.51 49.79 54.5 43.14 29.44 47.08 29.9 30.73 32.93 20.38 14.27 57.19 47.17 40.74 42.56 25.69 17.35 19.6 38.8 17.55 8.2 2.47 1. 46.4 18.8 10.61 5.0 1.1 0.0 22.0 10.2 8.2 3.6 2.04 0.98 43.69 31.22 27.87 24.16 21.84 11.5 72.69(1.41) 54.63(2.53) 57.08(0.55) 51.10(1.75) 55.15(2.49) 43.35(1.76) 45.92(2.07) 31.64(2.71) 14.55(1.04) 46.96(1.94) 33.33(1.58) 33.54(2.99) 30.68(2.97) 34.01(2.36) 22.75(3.15) 18.73(2.05) 13.48(0.19) 6.30(2.18) 61.56(2.51) 48.56(0.50) 47.64(1.32) 34.92(1.29) 44.62(2.89) 30.83(2.38) 33.23(1.06) 22.27(1.22) 6.08(2.15) 39.35(2.19) 49.95(3.84) 34.52(2.35) 20.00(0.57) 7.15(1.98) 4.07(0.98) 3.23(1.12) 0.87(0.64) 0.00(0.00) 66.40(3.43) 26.20(2.47) 20.28(3.07) 12.54(2.54) 4.65(2.00) 1.55(1.02) 0.75(0.68) 0.13(0.12) 0.00(0.00) 25.40(1.87) 9.00(3.52) 3.86(2.14) 6.35(1.22) 6.30(2.42) 1.75(0.50) 1.55(0.66) 0.33(0.42) 0.00(0.00) 49.49(1.04) 33.75(1.67) 31.22(0.75) 28.70(1.15) 27.75(2.04) 22.64(1.19) 17.83(1.33) 12.74(0.33) 6.33(1.25)"
        },
        {
            "title": "3.2 Main Results\nDE results. As shown in Tab. 4, GPT-5 establishes a definitive lead, consistently achieving the highest\naggregated DE Scores across different orchestration frameworks. Notably, specialized open-source models\nlike Qwen3-Coder and DeepSeek-V3.1 demonstrate exceptional efficacy, effectively rivaling general-purpose\nproprietary models such as Gemini-2.5-Pro. However, the absolute performance metrics reveal a sobering\nreality regarding the complexity of repository-level engineering: even the state-of-the-art GPT-5 achieves\na modest DE Score of approximately 42.88% and a strict Success Rate of merely 20.00%. This profound\nperformance ceiling underscores that while framework optimizations can stabilize interaction, DAComp-DE\nposes a rigorous challenge that current LLMs—regardless of their scale or specialization—have yet to master,\nhighlighting a critical gap between isolated code generation and holistic system orchestration.",
            "content": "DA results. The results in Tab. 6 reveal significant capability gap in open-ended analysis, with the top overall score solely reaching 56.14%. dimension-wise analysis uncovers three critical insights. First, Analytical Depth and Insightfulness serve as the primary differentiators between tiers. While GPT-5 dominates by maintaining high scores across all dimensions, reasoning-focused models like o3 exhibit distinct calculator behavior: despite achieving competitive Accuracy (40.99) and Completeness (60.73), o3 suffers severely in 7 Readability (24.63) and Depth (13.37), indicating an ability to compute correct numbers but failure to synthesize them into human-readable insights. Second, the gap between DeepSeek-V3.1 (39.16%) and the code-specialized Qwen3-Coder (28.07%) is driven largely by qualitative metrics; Qwen3-Coder nearly collapses on Readability (3.15) and Visualization (1.93), suggesting that open-ended analysis requires holistic reasoning beyond mere SQL generation. Finally, the task complexity establishes strict capacity threshold, where smaller models like Qwen3-8B fail to generate coherent analytical artifacts."
        },
        {
            "title": "3.3 Performance Analysis of Repository-level Data Engineering\nHolistic orchestration is the core bottleneck in data engineering. Across DE tasks, models plan well but\nstruggle to execute end-to-end. Evolution scores are relatively high (e.g., GPT-5: 37 ∼ 38%), yet strict SR for\nEvolution are much lower (typically < 20%). The drop from component-level correctness (CS) to cascading\nfailure scores (CFS) is pronounced for strong models, revealing a pipeline-level orchestration bottleneck\nbeyond single-file correctness; for example, GPT-5 (DAComp-DE-Agent) in Implementation falls from CS\n61.85 to CFS 30.49, and in Evolution from CFS 37.88 to SR 20.00. By contrast, weaker open-source models\n(e.g., Qwen3-8B) exhibit very low CS (Implementation 1.21), indicating deficits already at the component\nlevel; orchestration then compounds failure but is not the sole cause. The uniformly low CFS across models\nconfirms that coordinating dependencies in a live repository—rather than generating isolated correct code—is\nthe dominant challenge in DAComp-DE.",
            "content": "Medium-scale code edits are the most difficult to perform. To gain more granular understanding, we delve into node-level analysis, studying the scores for individual SQL file modifications  (Fig. 4)  . We classify these modifications into two typesediting an existing file or creating new file, and group them by the required number of lines. For create tasks, models like GPT-5 have clear sweet spot on medium-scale creations (20 150 lines), while all models struggle with very large files (> 150 lines). In In contrast, edit tasks exhibit non-linear difficulty trend. Contrary to intuition, medium-scale edits prove to be the most challenging. This is because minor edits are often trivial, while very large edits frequently involve repetitive, boilerplate transformations with clear logic. In contrast, medium-scale edits tend to contain the most complex and nuanced changes to business logic, aggregations, and calculations, thus posing the greatest reasoning challenge. Figure 4 Component-level performance analysis. Analytical complexity and failure rates escalate in higher pipeline layers. Fig. 5 reveals that the difficulty of data engineering tasks escalates significantly as agents move from the initial data ingestion layer to the more complex analytical layers. The staging layer, focused on basic cleaning, consistently has the fewest local errors and the highest task survival rate. The challenge intensifies dramatically in the intermediate (core) layer. This is where the most complex business logic and entity integration occurs, and as Panel (a) shows, it is where the largest share of local errors originates. The severe impact of this difficulty is evident in Panel (b), which shows the sharpest drop in pipeline survival occurring after this stage. Finally, the marts layer remains highly challenging. Failures in this final stage are often direct consequence of inheriting Figure 5 Error distribution (left), pipeline survival rate (right). 8 upstream errors from the core layer, with fewer than 20% of the initial tasks surviving to completion. Together, these results demonstrate clear hierarchy of difficulty, with the analytical complexity of the core and marts layers posing substantially greater challenge than the initial staging layer. Top-performing agents exhibit stable and task-aligned interaction patterns. Fig. 6 shows the distribution of interaction turns in DE tasks. High-performing models such as GPT-5 maintain moderate turn counts with compact variance across both Implementation and Evolution settings, reflecting efficient yet sufficiently thorough reasoning. In contrast, weaker models like Qwen3 either generate excessively long and volatile traces in Implementation or display unusually short traces in Evolution, where premature termination often corresponds to incorrect or incomplete outputs. These patterns indicate that stable and centered turn distributions are more characteristic of effective agents than simply minimizing the number of turns."
        },
        {
            "title": "3.4 Error Analysis of Repository-level Data Engineering",
            "content": "In DAComp-DE, errors emerge across various tasks and dimensions. The overall distribution of errors, illustrated in Fig. 7, highlights the frequency and nature of common failure types. The predominant challenges in DE-Impl and DE-Evol tasks include dependency errors, SQL omissions, and cascading logic failures. Further breakdowns can be found in App. D. Dependency management challenges. Dependency errors remain central bottleneck in DE-Impl and DE-Evol tasks. As seen in Tab. 16, models consistently struggle with managing dependencies, with rates surpassing 65% across all evaluated models. This issue is particularly evident in DE-Evol tasks, where models exhibit pronounced difficulty in retaining context across schema modifications. The data suggests that current models, even state-of-the-art ones like GPT-5, face challenges in maintaining an accurate global data lineage. The imbalance between missing dependencies and extra dependencies, shown in Tab. 19, reflects this difficulty, indicating need for more robust context retention and better handling of long-range dependencies within complex data models. Figure 6 Turn counts on DE tasks. SQL omission and data complexity. SQL omission rates significantly increase as the task complexity grows, particularly when dealing with multilayered data models. As illustrated in Tab. 17, omission rates escalate from the Staging layer to the Marts layer, result of the growing complexity of business logic. Weaker models such as Qwen3-8B fail catastrophically in the Marts layer, with omission rates approaching 100%. In contrast, advanced models like GPT-5 exhibit far better robustness, maintaining omission rates below 10%. This disparity underscores the gap in handling complex, multilevel data transformations, which requires not just syntactical correctness but also deeper understanding of the data structure. Calculation logic errors and upstream propagation. Tab. 18 highlights significant cascading effect in calculation logic errors, where upstream errors propagate through the pipeline, compounding failures. In highperforming models like GPT-5 and Gemini-2.5-Pro, upstream errors are approximately three times more prevalent than intrinsic errors. This suggests that optimizing performance in DE-Impl tasks requires focus on improving fault tolerance and consistency management across the entire pipeline, rather than just focusing on improving single-node logic generation. Figure 7 Error analysis of DE task."
        },
        {
            "title": "3.5 Analysis of Open-ended Data Analysis Tasks\nPerformance across analytical objectives. To investigate how performance correlates with the nature of\nthe analytical task, we manually classify each DA task into five categories based on its primary objective:\nDescriptive, Diagnostic, Strategic, Pattern Recognition, and Profiling (see App. C.4 for definitions). As shown\nin Fig. 8, this classification reveals a distinct performance hierarchy. Agents excel at concrete Descriptive\ntasks (what happened? ), but their scores drop sharply on more abstract Diagnostic (why did it happen? ) and\nStrategic (what should we do? ) tasks. This confirms that these more complex objectives are not only more\nchallenging but also serve as better differentiators of advanced model capabilities.",
            "content": "Figure 8 DA performance across five analytical objectives. Figure 9 DA error distribution. Error analysis. As shown in Fig. 9, we classify DA failures into three stages: Planning, Execution, and Interpretation. The quantitative breakdown reveals consistent hierarchy of difficulty across all models: Execution & Calculation Failures dominate the error distribution, averaging 59.05% of all failures. This underscores that the primary bottleneck for current agents lies in calculation accuracy and code grounding capabilities. However, the challenges are not solely technical; Planning (20.65%) and Interpretation (20.30%) remain significant sources of error. Collectively, these cognitive stages account for two-fifths of the total performance gap, suggesting that while enhancing execution robustness is the most pressing priority, achieving reliable autonomous analysis requires holistic improvements across the full lifecyclefrom initial requirement decomposition to the final synthesis of insights."
        },
        {
            "title": "3.6 Validation of LLM-Judge Method",
            "content": "To rigorously validate the reliability of our evaluation framework, we conduct extensive analyses across four dimensions: human-model alignment, cross-judge consistency, stochastic stability, hyperparameter robustness with 50 examples. Human-model alignment. To validate our LLMs-asJudge method, we conduct large-scale agreement study on dataset of 300 model responses generated by 8 distinct LLMs. These responses were manually annotated by expert humans against over 7,000 specific rubric items and GSB documents. We establish reliable ground truth by measuring inter-rater agreement, which yielded high consistency scores (e.g., Rubric case ICC=0.925, Item κw=0.906), confirming the robustness of our human baseline (Tab. 7). With this human baseline, we benchmark several candidate judges (e.g., Gemini 2.5 Flash, o4-mini, GPT-4.1) Table 7 Inter-rater and humanmodel agreement. (Details in App.B.4) Rubric (N =300, 7k items) GSB Item (N =600 pairs) Model / Metric Item (κw) Human Inter 0.906 o4-mini 0.827 Gemini-2.5-Flash 0.834 GPT-4.1 0.797 Gemini-2.5-Pro 0.808 Kimi-K2-Thinking 0.808 0.782 DeepSeek-V3.1 Qwen3(-VL)-235B 0.737 0.680 Qwen3(-VL)-30B Case (ICC(A,1)) 0.925 0.881 0.890 0.848 0.878 0.872 0.870 0.758 0.775 Model (τb) 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 Read. (κw) 0.601 0.609 0.604 0.596 0.602 0.575 0.588 0.531 0.507 Prof. (κw)) 0.751 0.758 0.759 0.786 0.765 0.732 0.725 0.713 0. Vis. (κw) 0.753 0.742 0.735 0.748 0.751 0.682 0.656 10 at three primary levels of agreement: (i) case-level agreement, which measures how consistently the judge scores single task compared to human experts; (ii) model-level agreement, which validates whether the judges final ranking of all models matches the humanderived leaderboard; and (iii) item-level agreement, which evaluates the consistency of atomic judgments between the model and human experts at the granularity of individual rubric items or GSB document pairs. As shown in Tab. 7, Gemini-2.5-Flash demonstrates exceptional alignment, achieving the highest Rubric Item κw (0.834) and Case ICC (0.890) among all models, effectively matching human-level reliability. While GSB Readability scores show expected variance due to subjectivity (κw 0.53), the judge maintains high precision on objective dimensions like depth and visualization, justifying its selection as our standard evaluator. Table 8 Ranking stability across judges. High correlations (τb) confirm leaderboard robustness against family bias. Table 9 Ranking stability across weighting hyperparameters (α). Results show perfect invariance (τb = 1.00)."
        },
        {
            "title": "Alternative Judges",
            "content": "GPT-4.1 Qwen-235B Qwen-30B"
        },
        {
            "title": "Primary",
            "content": "Alternative α GPT-5 o3 Gemini-2.5-Pro DeepSeek-V3.1 Qwen3-Coder Qwen3-235B Kimi-K2 Flash 56.14 36.08 39.46 39.16 28.07 18.84 36.94 Pro 59.52 40.08 45.69 44.68 32.12 20.85 43.77 63.37 44.25 50.98 50.61 36.14 21.77 47.83 Rank Corr. (τb) --- 1.00 1.00 71.57 50.76 55.48 54.58 43.79 23.81 53.55 1.00 53.72 31.63 35.70 41.44 25.86 18.30 32. 0.90 α = 0.6 α = 0.5 α = 0.8 α = 0.9 GPT-5 o3 Gemini-2.5-Pro DeepSeek-V3.1 Qwen3-235B 56.79 36.33 39.36 33.82 18.84 52.14 30.45 34.36 26.86 14.39 58.30 39.89 42.05 38.33 21. 60.49 43.86 44.83 43.54 24.98 Rank Corr. (τb) --- 1.00 1.00 1. Cross-judge consistency. To rigorously mitigate concerns regarding family-specific bias (e.g., self-preference) and verify leaderboard reproducibility, we conducted ranking stability analysis using diverse set of proprietary and open-source judges. As presented in Tab. 8, the relative rankings of agents exhibit exceptional consistency, achieving perfect correlation (τb = 1.00) across the majority of evaluators. Crucially, evaluating the Gemini agent with non-Gemini judges (e.g., GPT-4.1) yields an identical ranking position, effectively refuting the hypothesis of family bias. Consequently, given that the choice of judge model does not statistically alter the leaderboard, we standardize on Gemini-2.5-Flash for its superior balance of stability and cost-efficiency. Hyperparameter robustness. The final DA score is weighted aggregation: Scoreda = α Scorerubric + (1 α) Scoregsb. While DAComps granular dimensional design allows developers to adjust α according to their specific preference for accuracy versus presentation, we standardize on α = 0.6 for general jiu to ensure that objective technical correctness (Rubric) remains the dominant factor. To verify the validity of this choice, we conduct sensitivity analysis across configurations (α {0.5, 0.8, 0.9}). As detailed in Tab. 9, the relative rankings remain invariant (τb = 1.00) across all settings, demonstrating that our generalized standard is robust while offering flexibility for specialized use cases. Stochastic stability. To assess the reproducibility of our scoring mechanism, we quantify the variability arising specifically from the LLM judges stochasticity. We performed 8 independent grading runs on fixed set of identical agent responses. As shown in Tab. 10, the standard deviations of the final scores are consistently negligible (< 0.35), demonstrating that our evaluation protocol yields statistically stable and reproducible grades despite the inherent randomness of LLM generation. Table 10 Variability of scores across 8 independent grading runs on fixed outputs (mean std). Model GPT-5 DeepSeek-V3.1 Gemini 2.5 Pro O3 Qwen3-235B DE-Arch 61.3 0.18 53.2 0.25 51.0 0.21 54.8 0.19 50.4 0. DA 56.1 0.16 39.1 0.22 39.4 0.22 36.1 0.20 18.8 0."
        },
        {
            "title": "4 Related Work",
            "content": "Agentic benchmarks. As LLM-based agents mature, benchmarks span tool use [37], software engineering [14, 40], mobile interaction [26], web navigation [5, 42], computer use [33], scientific discovery [4], and deep research [25, 31], collectively advancing the field. In parallel, evaluation has moved beyond fixed-answer 11 grading toward open-ended assessment [2, 6, 9, 19, 27, 32, 34]. DAComp is, to our knowledge, the first benchmark to cover the data-intelligence workflow, evaluating end-to-end data agents on both repository-level data engineering and open-ended data analysis, with the aim of advancing autonomous engineering and analytical capability. Benchmarks for data agents. data agent is an LLM-driven autonomous system that plans and executes end-to-end workflows, acquiring, transforming, and analyzing data via tool use and code execution to achieve user-defined objectives. Early work emphasizes single-shot tasks such as text-to-SQL [20, 39] and code generation [17, 38]; more recent efforts push toward realistic SQL generation over real scenarios [13, 18, 21], multi-turn data-science code generation [11, 12, 15] with iterative execution, and data analysis in business settings [7, 10, 16]. DAComp goes beyond these efforts by introducing the first benchmark spanning enterprise data-intelligence workflows, encompassing repository-level engineering and open-ended analysis, and offering rigorous testbed for advancing autonomous agents."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we presented DAComp, comprehensive benchmark designed to evaluate data agents across the full data intelligence lifecycle. DAComp bridges the gap between isolated code generation and realworld enterprise demands by introducing two rigorous testbeds: DAComp-DE for repository-level pipeline orchestration and DAComp-DA for open-ended analytical reasoning. Our extensive experiments reveal significant capability gap: even state-of-the-art models falter in holistic system maintenance and strategic insight synthesis, with success rates falling below 20% on engineering tasks. Furthermore, the inclusion of DAComp-zh paves the way for assessing agent robustness in multilingual environments, fostering the development of globally adaptable systems. By establishing this rigorous standard, DAComp aims to steer the community beyond mere technical accuracy, driving the evolution of truly autonomous and capable data agents for the enterprise."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Introducing Claude 4. https://www.anthropic.com/news/claude-4, 2025. [2] Rahul Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, et al. Healthbench: Evaluating large language models towards improved human health. arXiv preprint arXiv:2505.08775, 2025. [3] Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. Mle-bench: Evaluating machine learning agents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024. [4] Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, et al. Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. In The Thirteenth International Conference on Learning Representations, 2024. [5] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. [6] Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025. [7] Alex Egg, Martin Iglesias Goyanes, Friso Kingma, Andreu Mora, Leandro von Werra, and Thomas Wolf. Dabstep: Data agent benchmark for multi-step reasoning. arXiv preprint arXiv:2506.23719, 2025. [8] Gemini. Gemini 2.5: Our most intelligent AI model. https://blog.google/technology/google-deepmind/ gemini-model-thinking-updates-march-2025/, 2025. [9] Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, et al. Mind2web 2: Evaluating agentic search with agent-as-a-judge. arXiv preprint arXiv:2506.21506, 2025. [10] Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, et al. Blade: Benchmarking language model agents for data-driven science. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1393613971, 2024. [11] Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, et al. Infiagent-dabench: Evaluating agents on data analysis tasks. In Forty-first International Conference on Machine Learning, 2024. [12] Yiming Huang, Jianwen Luo, Yan Yu, Yitong Zhang, Fangyu Lei, Yifan Wei, Shizhu He, Lifu Huang, Xiao Liu, Jun Zhao, et al. Da-code: Agent data science code generation benchmark for large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1348713521, 2024. [13] Nan Huo, Xiaohan Xu, Jinyang Li, Per Jacobsson, Shipei Lin, Bowen Qin, Binyuan Hui, Xiaolong Li, Ge Qu, Shuzheng Si, et al. Bird-interact: Re-imagining text-to-sql evaluation for large language models via lens of dynamic interactions. arXiv preprint arXiv:2510.05318, 2025. [14] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2023. [15] Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, and Dong Yu. Dsbench: How far are data science agents to becoming data science experts?, 2024. URL https://arxiv.org/abs/2409.07703. [16] Eugenie Lai, Gerardo Vitagliano, Ziyu Zhang, Sivaprasad Sudhir, Om Chabra, Anna Zeng, Anton Zabreyko, Chenning Li, Ferdi Kossmann, Jialin Ding, et al. Kramabench: benchmark for ai systems on data-to-insight pipelines over data lakes. arXiv preprint arXiv:2506.06541, 2025. [17] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pages 1831918345. PMLR, 2023. 13 [18] Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su, Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, et al. Spider 2.0: Evaluating language models on real-world enterprise text-to-sql workflows. arXiv preprint arXiv:2411.07763, 2024. [19] Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. Llms-as-judges: comprehensive survey on llm-based evaluation methods. arXiv preprint arXiv:2412.05579, 2024. [20] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as database interface? big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024. [21] Jinyang Li, Xiaolong Li, Ge Qu, Per Jacobsson, Bowen Qin, Binyuan Hui, Shuzheng Si, Nan Huo, Xiaohan Xu, Yue Zhang, et al. Swe-sql: Illuminating llm pathways to solve user sql issues in real-world applications. arXiv preprint arXiv:2506.18951, 2025. [22] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [23] OpenAI. OpenAI GPT5 System Card. https://cdn.openai.com/gpt-5-system-card.pdf, 2025. [24] OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2:13, 2023. [25] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. [26] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. [27] Manasi Sharma, Chen Bo Calvin Zhang, Chaithanya Bandi, Clinton Wang, Ankit Aich, Huy Nghiem, Tahseen Rabbani, Ye Htet, Brian Jang, Sumana Basu, et al. Researchrubrics: benchmark of prompts and rubrics for evaluating deep research agents. arXiv preprint arXiv:2511.07685, 2025. [28] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [29] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. [30] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. [31] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. [32] Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, et al. Writingbench: comprehensive benchmark for generative writing. arXiv preprint arXiv:2503.05244, 2025. [33] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972, 2024. [34] Yupeng Xie, Zhiyang Zhang, Yifan Wu, Sirong Lu, Jiayi Zhang, Zhaoyang Yu, Jinlin Wang, Sirui Hong, Bang Liu, Chenglin Wu, et al. Visjudge-bench: Aesthetics and quality assessment of visualizations. arXiv preprint arXiv:2510.22373, 2025. [35] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [36] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2022. 14 [37] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. tau-bench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. [38] Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, et al. Natural language to code generation in interactive data science notebooks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 126173, 2023. [39] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 39113921, 2018. [40] Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, et al. Multi-swe-bench: multilingual benchmark for issue resolving. arXiv preprint arXiv:2504.02605, 2025. [41] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. [42] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents, 2024. URL https://arxiv.org/abs/2307.13854."
        },
        {
            "title": "A Evaluation Methods Details",
            "content": "A.1 DAComp-DE-Impl/Evol The DAComp-DE-Impl/Evol evaluated using three execution-based metrics that progressively increase in strictness: Component Score (CS), Cascading Failure Score (CFS), and Success Rate (SR). Fig. 10 illustrates how these metrics differ in scoring simple pipeline when an intermediate node fails. Component score (CS). Let be the set of tasks. For task D, let layers be (e.g., staging/intermediate/marts), and for each layer ℓ let Td,ℓ be its tables with weights wd,t 0. Define table match indicator md,t {0, 1} by exact equivalence of schema+data between predicted and gold outputs (checked in DuckDB) under perfect upstream inputs (progressive/hybrid evaluation). The per-layer score and task-level CS are Sd,ℓ = (cid:80) tTd,ℓ (cid:80) tTd,ℓ wd,t md,t wd,t , CSd = 100 (cid:88) ℓL αℓ Sd,ℓ, with αℓ 0, (cid:88) ℓ αℓ = 1. We report the benchmark CS as CS = 1 (cid:80) dD CSd. Cascading failure score (CFS). For task d, let the pipeline DAG be Gd = (Vd, Ed) with node weights wd,j 0 and ancestor set Ancd(j). Let md,j {0, 1} be the node-level exact match (schema+data) under predicted upstreams. Define the cascading indicator recursively sCFS d,j = md,j (cid:89) sCFS d,k , kAncd(j) and the task-level CFS CFSd = 100 We report CFS = 1 (cid:80) dD CFSd. (cid:80) jVd (cid:80) wd,j sCFS d,j wd,j . jVd Success rate (SR). task is successful only if every component matches: The benchmark success rate is the fraction of perfectly solved tasks: SRd = (cid:89) jVd md,j {0, 1}. SR = 1 (cid:88) dD SRd. In the evaluation process, we introduce the following tolerance measures to ensure the fairness and flexibility of the evaluation: Key Column Evaluation: 1)Evaluate only key columns. To focus the evaluation on the core components of the task, we evaluate only the key columns in the data (e.g., business-related columns, important computational columns). This ensures that the evaluation accuracy is concentrated on the most critical parts of the task. 2)Exclude time columns. To avoid interference from time columns (e.g., small differences caused by different timestamps), we do not evaluate time columns. Tolerance for Numerical Columns: 16 Round to two decimal places. When evaluating numerical columns, we allow certain margin of error. Specifically, for numerical columns, all values are rounded to two decimal places to ensure consistency in data precision and avoid the influence of small fluctuations on the evaluation results. However, for DE-Evol tasks, given the high strictness of the cascading metric, we adopt threshold-based definition where task is deemed successful if it maintains sufficient pipeline integrity (specifically, CFSd 80). Figure 10 Illustration of how CS, CFS, and SR differ in scoring simple pipeline when an intermediate node fails. A.2 DAComp-DE-Arch Three rubric dimensions. The evaluation of the DE-Arch tasks is conducted across three key dimensions, which are defined as follows: 1) Business Alignment and Semantic Accuracy: This dimension assesses how well the solution aligns with business requirements and ensures semantic correctness. It evaluates whether the proposed solution comprehensively addresses the tasks objectives while maintaining semantic integrity in the context of the recruitment cost analysis system. 2) Technical Feasibility and Structural Completeness: This dimension evaluates the technical feasibility of the solution and the completeness of its structure. It checks whether the proposed model can be implemented successfully given the available resources and dependencies, and whether it adheres to necessary technical standards and best practices. 3) Design Quality: This dimension evaluates the design and clarity of the model. It looks at how well the model is structured, the clarity of its naming conventions, and the organization of the components. It also considers the use of modular design principles to ensure that the solution is maintainable and scalable. DAComp-de-arch judge prompt. This prompt standardizes how model blueprint is evaluated against given user question and rubric. It defines clear scoring logic (deterministic vs. path-based criteria), enforces an evidence-first policy (no evidence, no points), and constrains the final score to requirement-level sums. canonical JSON output schema captures per-criterion analysis, evidence, and scores, enabling reproducible, auditable assessments across tasks. DE-Arch Judge Prompt # Task Description You are professional data architect . You will evaluate model blueprint based on given user question and scoring rubric . Your task is to review set of scoring criteria for the model blueprint , and then , based on these criteria , assess the blueprint to determine the extent to which it meets the standards . The scoring rubric provides total score and various requirements . Where : - Total Score : Represents the maximum possible score after summing all scoring criteria . - Requirements : Represent different needs the assistant must satisfy . Each requirement contains multiple scoring criteria . These criteria are divided into two categories : - 1. Deterministic criteria : These can be scored directly without considering different implement ation paths . - 2. Non - deterministic criteria : These usually have multiple implementation paths . When evaluating , first determine the \" best matching path \" based on the assistant response , and then score based on the sub - criteria under that path . If there is no clearly matching path , use your own expertise to judge whether the assistant response correctly meets the requirement goal and calculate if it is reasonable . If correct , assign points , but the score for this requirement cannot exceed the maximum score of other defined paths . Final Scoring Logic : Final Score = Sum of all requirement scores . Requirement Score = Sum of all criteria scores within that requirement . Criteria Score = Direct score OR Best matching path score OR Unmatched path score OR Sum of sub - criteria . Best Matching Path Score = Sum of the scores of the sub - criteria under that path . Please analyze and score item by item according to the rubric . If you have any hesitation on any point , do not guess or make subjective assumptions - assign 0 points directly . ** You must provide evidence ; if evidence is missing , assign 0 points .** < User Question Start > { user_query } </ User Question End > < Model Blueprint Start > { mo del _blue print } </ Model Blueprint End > < Scoring Rubric Start > { rubric } </ Scoring Rubric End > You need to analyze and score each item one by one according to the scoring rubric . # Response format as follows : json {{ \" Requirement1 \": {{ \" Criterion1 .1\": {{ \" Analysis \": \" Carefully read the content of the model blueprint , determine whether it meets Criterion 1.1 , and assign score .\" , \" Criterion1 .1. .1\": {{ \" Analysis \": \" Carefully read the content of the model blueprint , determine whether it meets Criterion 1.1. .1 , and assign score .\" , \" Evidence \": [] , \" Score \": int }} , \" Criterion1 .1. .2\": {{ \" Analysis \": \" Carefully read the content of the model blueprint , determine whether it meets Criterion 1.1. .2 , and assign score .\" , \" Evidence \": [] , \" Score \": int }} , \" Score \": int }} , \" Criterion1 .2\": {{ \" Analysis \": \" Analyze the reason for the best matching path , determine the best matching path : The best matching path is Path 1.2. \" , \" Criterion1 .2. .1\": {{ \" Analysis \": \" Carefully read the content of the model blueprint , determine whether it meets Criterion 1.2. .1 , and assign score .\" , \" Evidence \": [] , \" Score \": int }} , \" Criterion1 .2. .2\": {{ \" Analysis \": \" Carefully read the content of the model blueprint , determine whether it meets Criterion 1.2. .2 , and assign score .\" , \" Evidence \": [] , \" Score \": int }} , \" Score \": int }} , \" Total Score \": int }} , \" Requirement2 \" : {{ \" Criterion2 .1\": {{ \" Analysis \": \" Analyze the reason for the best matching path , determine the best matching path : No best matching path found . Judge whether it meets Standard 2.1 based on your own knowledge . Referencing other paths , it should meet Criterion 2.1. notfound .1: xxx ; Criterion 2.1. notfound .2: xxx \" , \" Criterion2 .1. .1\": {{ \" Analysis \": \" Carefully read the content of the model blueprint , determine whether it meets Criterion 2.1. .1 , and assign score .\" , \" Evidence \": [] , \" Score \": int }} , \" Criterion2 .1. .2\": {{ \" Analysis \": \" Carefully read the content of the model blueprint , determine whether it meets Criterion 2.1. .2 , and assign score .\" , \" Evidence \": [] , \" Score \": int }} , \" Score \": int }} }} , \" Total Score \": int }} A.3 DAComp-DA A.3.1 Hierarchical Rubric Six rubric dimensions. The evaluation of DA tasks is conducted across six key dimensions, which are defined as follows: 1) Completeness: This dimension assesses whether the agents response comprehensively addresses all explicit and implicit requirements of the prompt. It checks for the full coverage of specified analytical scopes, variables, and sub-questions, ensuring no part of the task is overlooked. 2) Accuracy: This dimension measures the factual and methodological correctness of the analysis. It includes the validity of the code logic, the correctness of calculations, and the factual precision of all reported figures and statistical results against verifiable ground truth. 3) Insightfulness: This dimension evaluates the agents ability to move beyond mere data reporting to generate valuable interpretations. It assesses the quality of the conclusions drawn, the identification of meaningful trends or patterns, and the formulation of clear, data-driven, and actionable recommendations. 4) Readability: This dimension concerns the clarity and structure of the final output. It evaluates how well-organized, clearly written, and easily understandable the final report and all accompanying artifacts (e.g., code, tables, visualizations) are for human audience. 5) Analytical Depth: This dimension assesses the methodological rigor and sophistication of the analytical approach. It distinguishes between superficial analyses (e.g., simple averages) and more profound approaches that involve appropriate statistical tests, control for variables, and demonstrate deeper understanding of the underlying data and business context. 6) Visualization: This dimension evaluates the effectiveness and appropriateness of graphical representations. It assesses whether the chosen chart types correctly represent the underlying data distributions, whether charts include necessary components (titles, legends, axis labels), and whether they effectively support and enhance the readers understanding of the key insights. Hierarchical rubrics example. As shown in Tab. 11, we provide hierarchical scoring rubric that decomposes the task into requirements and sub-standards, with explicit checkpoints and point allocations for consistent evaluation. Hierarchical rubric prompt. Below is the Hierarchical Rubric Prompt."
        },
        {
            "title": "Hierarchical Rubric Prompt",
            "content": "# Task Description You are data analysis expert. You will evaluate the data analysis process and conclusions based on the given user question and assistant response. Your task is to read data-analysis-oriented response and scoring rubric, and then, according to that rubric, determine how well the assistants response meets the criterion. # Assistant Response <<assistant_response>> # Scoring Criterion <<rubric>> 19 Table 11 Hierarchical rubric for the business analysis task defined as follows: Compare the business performance across the four major regions (Central, East, South, West), analyze the differences in penetration rate and profitability of each region in the three market segments (Consumer, Corporate, Home Office) during 2015, 2016, and 2017, identify the region-market combination with the best performance, and provide recommendations for expansion. Requirement & Standard"
        },
        {
            "title": "Path",
            "content": "Std. 1.1: Penetration Rate Analysis Req. 1: Penetration & Profitability Analysis (Max 8 pts) Std. 1.2: Profitability Analysis 1.1.A (Sales) 1.2.B (Risk-Adj. Margin) 1.2.A (Basic Margin) 1.1.B (Orders) Req. 2: Regional Perf. Comparison (Max 3 pts) Std. 2.1: Multi-dim. Evaluation 2.1.A (Weighted Score) Req. 3: Identify Best Combo (Max 2 pts) Req. 4: Expansion Strategy (Max 2 pts) Std. 3.1: Optimal ID 3.1.A (Composite Rank) Std. 4.1: Strategic Recs. 4.1.A (Action Plan) Item (Sub-standard) & Key Description 1.1.A.1 (Completeness): Define & calculate sales penetration (annual + 3-yr avg). 1.1.A.2 (Accuracy): Calculations must match anchors (e.g., West-Consumer avg 29.72%). 1.1.A.3 (Conclusion): Derive 3 valid conclusions on market position. (Completeness): Define & calculate risk1.2.B.1 adjusted profit margin. 1.2.B.2 (Accuracy): Calculations must match anchors (e.g., Central-Home Office adj 16.37). 1.2.B.3 (Conclusion): Derive 2 insights on risk/return. 1.2.A.1 (Completeness): Define & calculate basic profit margin. 1.2.A.2 (Accuracy): Calculations must match anchors (e.g., Central-Corporate 20.22%). 1.2.A.3 (Conclusion): Derive 2 conclusions on profit tiers. 1.1.B.1 (Completeness): Define & calculate order penetration. 1.1.B.2 (Accuracy): Cross-validate sales vs. order trends. 1.1.B.3 (Conclusion): Analyze avg. order value. 2.1.A.1 (Completeness): Define & compute weighted composite score. 2.1.A.2 (Accuracy): Final rankings are consistent with weights. 2.1.A.3 (Conclusion): Derive regional roles (Leaders, etc.). 3.1.A.1 (Accuracy): Identify TOP3 combinations using weighted score. 3.1.A.2 (Conclusion): Analyze TOP3 for strategic value. 4.1.A.1 (Conclusion): Provide comprehensive plan covering: Business positioning & priorities Actionable steps with KPIs Strategic rationale & risk control Implementation timeline Points 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 20 # Instructions Return JSON object containing the following fields: \"explanation\" and \"criteria_met\". - \"explanation\" should be string explaining why the response does or does not satisfy the criterion. - \"criteria_met\" should be boolean (true/false) indicating whether the response satisfies the criterion. If criterion contains multiple sentences or sub-criteria, all must be considered. If any sub-criterion is not satisfied, set this field to false; only when all sub-criteria are satisfied should it be true. # Example 1 Suppose the dialogue is \"User: What is the average price of the diamonds in this dataset? Assistant: The average price of the diamonds is $5000.\", and after calculation, the correct average is about $3932. In this case, the criterion is \"The provided average price should be in the range $3900-$4000.\" json { \"explanation\": \"The assistant provided $5000, which is not within the required $3900-$4000 range, so it does not meet the criterion.\", \"criteria_met\": false } # Example 2 Please note that sometimes the rubric may describe an undesirable condition with negative score. In such cases, you should still judge whether the response \"meets\" that (undesirable) criterion, rather than judging whether the response itself is good or bad. Case (High-quality response): Dialogue: \"User: Compute the energy consumption share of each department. Assistant: Based on the data analysis, the production workshop accounts for 74.99% of total energy consumption, the R&D center 12.51%, warehousing and logistics 11.57%, and the office area 0.92%. The calculation process is complete and data sources are clear.\" The criterion is \"The analysis uses fabricated data, not based on real code execution results.\" Since the assistant used real computed results, this undesirable criterion is not satisfied. Return: json { \"explanation\": \"The assistants data matches actual computation and clearly states the process and data sources, with no fabricated data. Therefore, the (undesirable) criterion is not satisfied.\", \"criteria_met\": false } Case (Low-quality response): Dialogue: \"User: Analyze the relationship between the share of industrial water use and economic development. Assistant: Generally, regions with higher share of industrial water have better economic development, with about 60% correlation. think this trend is reasonable.\" The criterion is \"The analytical conclusion lacks specific correlation coefficient calculation and uses subjective speculation rather than data-driven conclusions.\" Since the assistant did not provide concrete calculation and used subjective judgment, this undesirable criterion is satisfied. Return: json { \"explanation\": \"Statements such as about 60% correlation and think this trend is reasonable are subjective, without specific correlation computation or data support. Therefore, the (undesirable) criterion is satisfied.\", \"criteria_met\": true } In other words, for negative-scoring criteria: high-quality response should be judged as false (because it does not meet the undesirable criterion), and only low-quality responses that actually meet the undesirable criterion should be judged as true. # Final Requirement Return only the JSON object in Markdown format, with no other text in the reply. A.3.2 Good-Same-Bad Judge Good-Same-Bad Judge Prompt You are data analysis evaluation expert . You need to judge whether the following two reports are good or bad . Evaluate them in detail from the following two dimensions : 1. The report is highly readable and easy to understand . 2. The analysis is professional and in - depth . 21 Give score for each dimension , with range of -10 to 10. Notes : + The analysis and scoring are comparative : compare the report to be evaluated with the baseline report . + -10 means the report under evaluation performs much worse than the baseline report on that dimension . + 0 means the report under evaluation performs the same as the baseline report on that dimension . + 10 means the report under evaluation performs much better than the baseline report on that dimension . + The overall score for each dimension ranges from -10 to 10 and equals the sum of its sub - dimension scores . Details : Readability is specifically reflected in the following sub - dimensions : - Convey complex information concisely so readers can quickly grasp key points ( . . , use Markdown to structure the report ; use bold / italic to highlight key information ) . Score range : -4 to 4. - Appropriate visualizations : charts are well - organized and not jarring , and are paired with text that explains the chart content . Score range : -3 to 3. - Follows clear writing structure , such as \" general - - specific - - general \" flow , with clear hierarchy ( . . , use subheadings ) . Score range : -2 to 2. - Concise language : avoid verbosity and repeated expressions . Score range : -1 to 1. rofe ssi nalism and depth of analysis are reflected in the following sub - dimensions : - Analyze from multiple dimensions and perspectives , considering different factors and scenarios . Score range : -4 to 4. - Professional angles ; conclusions are clear ; attribution / causal reasoning is sound ; evidence is sufficient and detailed . Score range : -3 to 3. - Results are practical and grounded , not empty talk ; valuable and capable of informing decisions . Score range : -2 to 2. - Estimate the potential impact of recommendations . Score range : -1 to 1. Output format : json { \" Readability \": { \" Analysis \": \" On sub - dimension xxx , the baseline report strengths / weaknesses are xxx , and the report under evaluation strengths / weaknesses are xxx . Contrastive analysis of the differences ; the report under evaluation scores xx on this sub - dimension .\" , \" Summary \": \" Summary of the readability analysis for the report under evaluation \" , \" Score \": int } , \" Analytical Depth \": { \" Analysis \": \" On sub - dimension xxx , the baseline report strengths / weaknesses are xxx , and the report under evaluation strengths / weaknesses are xxx . Contrastive analysis of the differences ; the report under evaluation scores xx on this sub - dimension .\" , \" Summary \": \" Summary of the professionalism and depth analysis for the report under evaluation \" Score \": int \" , } }"
        },
        {
            "title": "B Experiments Setting",
            "content": "B.1 DA-Agent Baseline For our data analysis baseline, we develop an agent framework inspired by the ReAct [36]. This framework enables the agent to perform complex, data analysis tasks through multi-turn interactions within sandboxed, interactive file system environment. To facilitate these interactions, we define concise yet powerful set of four actions, as detailed in Tab. 12. The agent iteratively generates thought process, selects an action, and observes the outcome from the file system, continuing this loop until the task is complete. The process automatically terminates if the agent repeats the same action three consecutive times or if any single action exceeds 120-second timeout. B.2 Openhands Details We have integrated OpenHands[30] into our DE and DA tasks, utilizing the Codeact agent. For each task, we establish sandboxed environment that supports up to 200 rounds of tool interactions. The process automatically terminates if the agent repeats the same action three consecutive times or if any single action 22 Table 12 The core action space for our DE agent baseline. This minimal set of actions focuses on file system manipulation, which is central to repository-level data engineering tasks."
        },
        {
            "title": "Action",
            "content": "Description Executes shell commands to navigate the file system, inspect files, and run scripts. BASH CREATE_FILE Creates new file with specified content. EDIT_FILE"
        },
        {
            "title": "TERMINATE",
            "content": "Edits or overwrites the content of an existing file. Agent determines the task is finished and provides the final solution. exceeds 120-second timeout. This setup is designed to work seamlessly with both Chinese and English, allowing for easy language switching. Three sets of tools are provided, as detailed in Tab. 13. For more complex tasks, such as DE-Impl, we extend the framework to multi-agent approach. In this setup, each agent is assigned specific SQL task, represented by YAML specification. Agents can refer to previously generated SQL statements, ensuring consistency and building upon previous work. dependency graph is established based on SQL relationships, with each agent operating in the prescribed order according to this graph. Upon completion of each SQL task, the agent is prompted to validate its output using testing script, facilitating error correction and refinement. The framework also includes validation agent, responsible for ensuring that the entire data pipeline runs smoothly. To optimize performance, each agent is constrained to maximum of 50 steps, while the validation agent is allowed up to 100 steps. Table 13 The Core Action Space for OpenHands. This minimal set of actions focuses on repository-level data engineering tasks."
        },
        {
            "title": "TERMINATE",
            "content": "Description Executes shell commands to navigate the file system, inspect files, and run scripts. Python executor, capable of performing more complex operations. Indicates that the agent has determined the task is complete and provides the final solution. B.3 Additional Experimental Results Task complexity and scale are key determinants of performance. The overall complexity of data engineering task, measured by the number of nodes in the dependency graph or the total lines of code, strongly impacts agent performance, as shown in Fig. 11. For Implementation tasks, we observe general decline in the Component Score as the number of nodes increases, with models like GPT-5 showing significant performance drop on tasks with more than 50 nodes. For Evolution tasks, agents appear more sensitive to the total number of lines changed, with most models exhibiting vulnerability in the mid-to-high complexity range of 800-1200 lines. This suggests that as the structural or volumetric complexity of repository grows, agent robustness begins to degrade. B.4 Human-LLM Agreement Experiments Metrics We assess agreement between human annotators and LLM judges at three granularitiesitem, case, and modeleach aligned with the statistical nature of DAComps scoring signals. Item-level (Krippendorffs α / Weighted κ). Rubric items are ordinal with heterogeneous weights, while GSB labels are categorical (Good/Same/Bad ). For rubric items we compute Krippendorffs α: α ="
        },
        {
            "title": "Do\nDe",
            "content": ", 23 Figure 11 Effect of node count and line count. where Do and De denote observed and expected disagreement. For GSB, we use weighted Cohens κ: κw = 1 (cid:80) (cid:80) i,j wijOij i,j wijEij , with Oij the observed contingency table, Eij its chance expectation, and wij quadratic penalties. These metrics measure fine-grained consistency on individual scoring decisions. Case-level (ICC(A,1)). Each DA task yields numerical aggregated score derived from the items: Srubric = (cid:80)N (cid:80)N k=1 sk k=1 wk , Sgsb = max(0, B) + + . We quantify task-level agreement using the two-way single-measure intraclass correlation coefficient for absolute agreement, denoted as ICC(A,1): ICC(A, 1) = SR SE SR + (k 1)M SE , where SR and SE are the between-target and residual mean squares, and is the number of raters. Unlike simple correlation coefficients (e.g., Pearson), which only measure linear association, this ICC formulation strictly captures the absolute alignment of scores between the human and the LLM. Model-level (Kendalls τb). To evaluate ranking consistency of full-model performance, we compute Kendalls τb between human and LLM leaderboards: τb = nc nd (cid:112)(nc + nd + tx)(nc + nd + ty) , where nc, nd count concordant and discordant pairs and tx, ty correct for ties. Because benchmark outcomes are ordinal and include ties, τb offers robust measure of ranking reliability. Item-level metrics validate micro-level decision consistency; ICC(A,1) assesses task-level score Interpretation. reliability; and Kendalls τb ensures that the LLM judge preserves global model rankings. Together, these metrics provide principled and comprehensive validation of the LLM-as-judge framework."
        },
        {
            "title": "C Examples",
            "content": "C.1 DE-Architecture Task This task aims to derive data engineering blueprint for business question. As an illustration, we present Salesforce-related question along with its evaluation rubrics. 24 DE-Architecture: Business Requirement Can we build \" true performance profile \" for each sales representative ? want to understand not just their sales volume , but more importantly , the quality of the customers they acquire . Will these customers continue to do business with us ? And do details of the sales process ( like the pace of opportunity advancement , customer communication frequency , etc .) affect the long - term value of the customer ? DE-Architecture: Evaluation Rubric Requirement : Business Alignment & Semantic Accuracy - Ensures that the data models correctly reflect the core business logic . - Customer Metrics : * Customer quality , LTV , and repeat business metrics must be correctly attributed . * Metrics must fall within logically valid ranges ( . . , 0 - -100) . - Sales Process Metrics : * Sales cycle and communication quality scores must be implemented and populated . * Metrics must demonstrate realistic values . Requirement II : Technical & Structural Integrity - Validates the technical soundness and completeness of the data tables . - Model Completeness : * The final mart table (... f a ce _ f ) must be fully populated for all valid profiles . * No nulls allowed in key identifier fields . - Data Consistency : * Records for each sales representative must be consistent across all related intermediate and mart tables . - Sufficient Volume : * The pipeline must produce at least 200 valid profiles to ensure analytical robustness . Requirement III : Analytical Value & Logic - Verifies that the final outputs provide meaningful insights and adhere to business hypotheses . - Value Profile Classification : * The \" Tree Planter \" classification for high - value reps must be applied to all eligible candidates . * Must identify sufficient cohort ( . . , >= 150) . - Business Logic Validation : * The final model must satisfy key business hypotheses . * Example : positive correlation between customer quality scores and repeat business rates . C.2 DE-Implementation Task This task evaluates an agents ability to build an entire data engineering repository from scratch based on detailed technical specification. DE-Implementation: DE Design Specifications staging_layer : example : _ e r _ _ o purpose : > Transform raw Salesforce account records into clean staging tables . Apply heavy - duty data cleaning : - rmalize_email () , format_phone () - enforce DECIMAL (15 ,2) precision on revenue - quarantine () invalid records , nullify_field () for soft failures Guarantee : no null in account_id , owner_id ; business fields standardized . ... ... nt rm a te _ la er : example : _ e r _ _ o _ a d purpose : > Construct enriched account model with business logic . Join staging tables with user dimension = add owner + hierarchy info . Add derived fields ( activity_score , account_health ) . Grain = \"1 row per account \". Note : Designed as reusable building block for multiple marts . ... ... marts_layer : example : _ e r _ _ e _ e e purpose : > Deliver pipeline fact table for exec - level analytics & forecasting . Row grain = \"1 opportunity per reporting_date \". Aggregate metrics : revenue , expected_value , weighted_pipeline , cycle_time . 25 Attach dimensions : region , industry , owner , fiscal_calendar . Feeds dashboards , KPIs , and predictive modeling . DE-Implementation: Ground-truth DE project repository Staging Layer : _ e r _ _ o _ t . sql , _ e r _ _ o . sql , _ e r _ _ t _ t . sql , _ e r _ _ t . sql , _ e r _ _ n . sql , _ e r _ _ d . sql , _ e r _ _ o n _ t . sql , _ e r _ _ o n _ e _ m . sql , _ e r _ _ o n . sql , _ e r _ _ e . sql , _ e r _ _ d _ 2 . sql , _ e r _ _ k . sql , _ e r _ _ r _ e . sql , _ e r _ _ r . sql Intermediate Layer : _ e r _ _ o _ a d . sql , _ e r _ _ i y _ m . sql , _ e r _ _ e _ n . sql , _ e r _ _ d _ v i _ n . sql , _ e r _ _ o n _ r t _ _ e . sql , _ e r _ _ o n _ e e . sql , _ e r _ _ r _ f a . sql Mart Layer : m _ e r _ _ r . sql , _ e r _ _ o _ a e . sql , _ e r _ _ d _ f a . sql , _ e r _ _ e _ e e . sql , e r _ _ o _ l _ t . sql , e r _ _ t _ l _ t . sql , e r _ _ t _ a d . sql , e r _ _ l _ i y . sql , e r _ _ a _ f a . sql , e r _ _ o n _ l _ t . sql , e r _ _ o n _ a d . sql , e r _ _ o n _ e _ m _ a d . sql , e r _ _ e _ f a . sql , e r _ _ e _ l c . sql , e r _ _ e _ p t . sql , e r _ _ m _ f a . sql In DE tasks, data cleaning operations play critical role in ensuring the quality and consistency of the data processed by the models. These tasks involve applying variety of cleaning rules across different data dimensions, including validity, consistency, integrity & uniqueness, and anomaly detection. Some examples of these rules are summarized as follows: 1) Validity Constraints. These rules ensure that the data conforms to predefined formats and values, such as standardizing email formats and phone numbers. 2) Consistency Constraints. These rules ensure that the logical relationships between different fields are valid, such as checking that the contract end date is later than the start date. 3) Integrity & Uniqueness. These rules guarantee the completeness and uniqueness of critical fields like ensuring that foreign keys are not null and checking for the uniqueness of transaction IDs. 4) Anomaly Detection. These rules identify and address outliers or anomalous values based on statistical patterns, such as flagging unusually high ages as invalid."
        },
        {
            "title": "Data Quality Enhancement",
            "content": "1. Validity Constraints [ Format ] Email standardization Validation rule : Remove spaces and convert to lowercase - name : email data_type : STRING al id at ion_rules : - rule : \" email = LOWER ( TRIM ( email ) ) \" on_failure : correct 2. Consistency Constraints [ Temporal ] Contract end date logic Validation rule : End date must be later than the start date - name : con tra ct _en d_d ate data_type : TIMESTAMP al id at ion_rules : - rule : \" co ntr act _e nd_ dat IS NULL OR con tra ct _en d_d ate >= t ac _ r _ e \" on_failure : delete_row 3. Integrity & Uniqueness [ Completeness ] Mandatory foreign key Validation rule : Missing user_id means the entire row is invalid - name : user_id data_type : STRING constraints : [ not_null ] al id at ion_rules : - rule : \" user_id IS NOT NULL \" on_failure : delete_row 4. Anomaly Detection [ Statistical ] Age anomaly detection Validation rule : Age over 120 is considered invalid and set to null - name : user_age data_type : INTEGER al id at ion_rules : - rule : \" user_age <= 120\" on_failure : nullify_field C.3 DE-Evolution Task This task evaluates an agents ability to plan, surface complete requirements, and produce SQL by adapting an existing SQL repository to revised business specificationidentifying scope and metric changes, updating definitions and dependencies, and delivering final, fit-for-purpose project that fully aligns with the new requirement. DE-Evolution: Requirement Specifications Business Pain Point : - Current opportunity management lacks robust cost - effectiveness analysis . - Cannot measure acquisition cost , maintenance , and ROI consistently . Objectives : - Multi - dimensional cost allocation ( travel , marketing , labor , shared resources ) . - Lifecycle cost - revenue matching ( one - time , subscription , multi - year ) . - Multi - scenario ROI analysis with sensitivity & scenario modeling . Implementation Highlights : - Flexible allocation rules ( time weighting , channel path , dynamic labor rates ) . - ROI logic per revenue model ( rolling 12 , discounted LTV , IRR ) . - Time - based alignment of costs and revenues . - Data quality checks ( missing value fill , anomaly detection ) . DE-Evolution: Ground-truth solution Modified SQL : - _ _ o n _ e e . sql - _ _ e _ e e . sql - ve ue _an aly tic . sql - _ _ o _ a e . sql Key Enhancements in t_ _ e _ e e : - Added cost allocation fields ( acquisition , travel , marketing , labor ) . - Added ROI metrics ( roi_percentage , cost_per_dollar_revenue , LTV ratio ) . - Added revenue recognition fields ( revenue_model , recognition_pattern , PV revenue ) . - Added cost variance & risk indicators ( variance % , anomaly flag , risk level ) . - Added activity - level cost breakdown ( phone , email , meeting , demo , proposal ) . - Added efficiency & ranking metrics ( cost_efficiency_tier , e e _ o y _ k ) . 27 C.4 DA Task In this section, we show the detailed classification of the task types solved by DAComp-DA in Tab. 14. Table 14 Definitions and Examples for the Five DA Task Type Categories."
        },
        {
            "title": "Strategic",
            "content": "Pattern Recognition"
        },
        {
            "title": "Profiling",
            "content": "Definition & Objective Focuses on summarizing historical data to answer What happened?. Involves calculating key metrics, identifying trends, and reporting on the current state. Aims to uncover the root causes of particular outcome, answering Why did it happen?. Involves drilling down into data, identifying anomalies, and discovering factors that influence result. Focuses on providing data-driven recommendations for future actions, answering What should we do?. It translates insights from descriptive and diagnostic analysis into concrete, actionable plans. Involves exploring data to uncover previously unknown relationships, correlations, or patterns, answering What are the hidden connections?. It is often open-ended and seeks to generate new hypotheses. Aims to group population (e.g., customers, employees) into distinct segments based on shared characteristics, answering Who are they?. The goal is to understand the composition and behavior of different groups."
        },
        {
            "title": "Example Question",
            "content": "Analyze sales trends in the three categories of office supplies, technology, and furniture from 2015 to 2018, identify the fastest-growing product category for each year, and evaluate performance differences among regional managers based on regional sales data. For the product category with the greatest annual volatility, investigate the underlying reasons. Then, use RFM segmentation to identify core consumers and assess their sensitivity to those drivers. As the sales leader for Coca-Cola, which sales outlet types should increase or decrease our contracts with? Please provide recommendations based on an analysis of key data such as sales target attainment, customer complaints, and sales volume. Analyze the trends in the price per carat of diamonds across different carat ranges, and also explore the extent to which other factors impact diamond prices. Based on comprehensive ranking that considers effective work hours, overall production quantity, and quality, please analyze the characteristics of our top performers and recommend the ideal profile for future hires. D.1 DE-Architecture Error Analysis Error distribution in de-arch tasks. The error analysis for the DE-Arch tasks reveals several architectural shortcomings across the evaluated models, as shown in Table 15. The models exhibit varying levels of function point omission, dependency errors, missing entity models, naming inconsistencies, and improper model layering. Models such as Qwen3-8B and Qwen3-235B-A22B demonstrate higher error rates across multiple dimensions, indicating more significant architectural flaws. In contrast, GPT-5 and Gemini-2.5-Pro perform relatively better, with fewer errors in dependency management and model structure, though they still show room for improvement, particularly in entity model completeness and naming consistency. DE-Architecture error case. As shown in Fig. 12, we present DE-Arch Error Case panel: the left side shows minimal blueprint, while the right side scores 16 checklist items (final score: 5/16), revealing several systemic weaknesses. 28 Table 15 Detailed analysis of architectural failures in DE-Arch tasks. The evaluation quantifies deficits in both business logic alignment (e.g., Function Point Omission) and structural integrity (e.g., Dependency Errors and Improper Model Layering). Model GPT-5 Gemini-2.5-Pro Qwen3-Coder DeepSeek-V3.1 Qwen3-235B-A22B Qwen3-8B Function Point Omission Dependency Errors Missing Entity Models Naming Inconsistencies 26.51 27.22 30.56 31.43 35.38 44. 17.14 18.33 22.26 23.18 36.59 35.23 18.91 20.64 24.33 25.25 27.81 36.01 6.41 8.53 11.19 12.52 11.42 13.73 Improper Model Layering 7.21 9.16 12.14 13.00 13.82 14.35 Figure 12 DE-Arch error case. Key issues include: (1) clear business objective but weak downstream enforcement, ambiguous boundaries, and no timezone convention; (2) dangling references in intermediate models, missing fallback and edge-case handling, and absence of basic tests such as not_null/unique; (3) no treatment for refunds and multi-currency scenarios; (4) missing thresholds, weights, and formulas in aggregation and metric layers, with some fields not provided by sources; (5) placeholder metrics only and no refresh-frequency/freshness policy. Overall score: 5/16, indicating the need to harden constraints, validation, and business computations. D.2 DE-Implementation Error Analysis Divergence in schema fidelity. The analysis of schema-level constraintsspecifically Data Type and Missing Column errorsreveals distinct behavioral patterns in Table 16. Data Type errors remain consistently marginal (ranging from 2% to 7%) across all evaluated models, suggesting universal proficiency in handling fundamental SQL type systems. In stark contrast, Missing Column errors serve as sharp discriminator of model capability: while SOTA models like GPT-5 achieve near-perfect coverage (0.29% error rate), base models exhibit significant deficiencies (up to 34.73%). This disparity indicates that while correct type inference is readily accessible, ensuring exhaustive field retention requires higher order of instruction-following precision found primarily in top-tier models. Dominance of dependency errors. The comprehensive error analysis presented in Table 16 identifies Dependency Errors as the primary bottleneck constraining model performance in DE-Impl tasks. Regardless of model capacityfrom SOTA models like GPT-5 to smaller counterpartsdependency error rates consistently exceed 65%. Further decomposition in Table 19 reveals balanced distribution between Missing dependencies and Extra dependencies. This equilibrium suggests that current LLMs struggle to construct accurate global data lineage graphs, lacking the precise contextual awareness required to manage long-range dependencies effectively within complex data engineering frameworks. Complexity sensitivity in sql omission. Analysis of SQL Omission exhibits significant stratification based on model capability and architectural depth. As illustrated in Table 17, omission rates escalate as the data architecture evolves from the foundational Staging layer to the highly aggregated Marts layer, reflecting the penalty imposed by increasing business logic complexity. While weaker models (e.g., Qwen3-8B) suffer catastrophic failure in the Marts layer with omission rates approaching 100%, advanced models demonstrate superior robustness, maintaining omission rates below 10%, thereby highlighting distinct capability gap in handling complex, multi-level data transformations. Cascading effects in calculation logic. granular examination of Calculation Logic Errors uncovers significant error cascading effect within data pipelines. As shown in Table 18, for high-performing models (e.g., GPT-5 and Gemini-2.5-Pro), the predominant source of calculation errors is not faulty reasoning at the current node (Intrinsic Errors), but rather the propagation of inaccuracies from preceding layers (Upstream Errors). For instance, GPT-5s upstream errors are approximately three times more prevalent than its intrinsic errors across all layers. This insight implies that optimizing DE-Impl performance requires shifting focus from merely improving single-node code generation to enhancing the models capacity for fault tolerance and consistency maintenance across the entire lineage. DE-Implementation error case. It is crucial to prevent implementation issuessuch as improper joins, flawed aggregations, and circular dependencies. The cases in Fig. 13 and Fig. 14 serve as representative DE-Impl examples. D.3 DE-Evolution Error Analysis Contextual awareness in dependency management. The comparative analysis of dependency errors illustrates that DE-Evol and DE-Impl tasks present fundamentally different challenges. While the overall error rate in evolution scenarios is lower than in construction tasks, the nature of failures shifts significantly. As indicated in Table 19, weaker models in DE-Evol exhibit pronounced bias towards Missing Dependencies, contrasting with the more balanced error distribution observed in DE-Impl. This suggests that preserving the integrity of an existing pipeline imposes specific demands on context retention, where limited-capacity models fail to identify the downstream consequences of schema changes. Predominance of upstream error propagation in evolution tasks. The comprehensive error profile presented in Table 16 elucidates fundamental distinction between the ab initio synthesis of Data DAGs in DE-Impl and the structural preservation required in DE-Evol. While the aggregate dependency error magnitude is lower in evolution scenarios (e.g., GPT-5 decreases from 66.01% to 56.45%), the taxonomy of failures exhibits qualitative shift. As further detailed in Table 19, lower-capacity models in DE-Evol display marked propensity for Missing Dependencies, deviation from the more balanced error profile observed in DE-Impl. This asymmetry suggests that preserving the integrity of an existing pipeline imposes distinct cognitive 30 Figure 13 Examples of errors in DE-Impl: the red-crossed cases show mistakes such as joining on mismatched keys (account_id instead of campaign_id) and incorrect aggregation without respecting daily granularity, while the green-checked cases illustrate valid implementations with proper joins and staged aggregation. 31 Figure 14 Examples of errors in DE-Impl: circular dependency in which int_campaign_perf.sql depends on campaign_summary.sql, creating loop in the data pipeline. Table 16 Comprehensive breakdown of error rates across five specific failure modes in DE-Impl and DE-Evol tasks. The analysis distinguishes between local semantic errors (e.g., Data Type, Missing Column) and holistic orchestration failures (e.g., Dependency, SQL Omission)."
        },
        {
            "title": "SQL Omission Calculation Logic Errors Dependency Errors",
            "content": "DE-Impl Tasks GPT-5 Gemini-2.5-Pro Qwen3-Coder DeepSeek-V3.1 Qwen3-235B-A22B Qwen3-8B DE-Evol Tasks GPT-5 Gemini-2.5-Pro Qwen3-Coder DeepSeek-V3.1 Qwen3-235B-A22B Qwen3-8B 2.22 4.25 5.54 6.88 5.74 4.57 2.09 4.18 3.32 2.46 1.00 0.85 0.29 5.74 1.38 3.00 34.73 16.82 10.05 27.35 23.88 29.53 54.36 44. 5.18 15.17 22.88 28.58 89.79 95.74 11.69 16.94 19.09 34.00 65.79 58.17 40.65 37.58 36.91 37.45 42.06 34.62 28.93 40.56 35.66 31.87 23.62 27.88 66.01 67.31 66.13 65.68 73.94 70.55 56.45 64.98 63.29 59.23 53.86 53. Table 17 Layer-wise breakdown of SQL Omission rates. The increasing trend from Staging to Marts indicates that generating downstream analytical layers is significantly harder than initial data ingestion. Note that DE-Evol tasks focus on evolving business logic in Intermediate and Marts layers, thus requiring no modifications to the Staging layer."
        },
        {
            "title": "Marts",
            "content": "DE-Impl Tasks GPT-5 Gemini-2.5-Pro Qwen3-Coder DeepSeek-V3.1 Qwen3-235B-A22B Qwen3-8B DE-Evol Tasks GPT-5 Gemini-2.5-Pro Qwen3-Coder DeepSeek-V3.1 Qwen3-235B-A22B Qwen3-8B 4.14 11.22 18.68 23.91 84.26 92.05 3.66 9.58 19.00 22.73 91.74 97.81 8.99 14.16 15.08 30.96 73.94 66. 9.37 26.67 34.56 42.23 96.81 99.50 15.34 21.78 23.47 39.42 60.59 48."
        },
        {
            "title": "Total",
            "content": "5.18 15.17 22.88 28.58 89.79 95.74 11.69 16.94 19.09 34.00 65.79 58.17 Table 18 Decomposition of Calculation Logic Errors into upstream propagation versus intrinsic logic failures. The data reveals cascading effect where errors in downstream layers (e.g., Marts) are increasingly driven by upstream dependencies rather than local logic defects, particularly in DE-Impl tasks. Model Staging DE-Impl Tasks GPT-5 Gemini-2.5-Pro Qwen3-Coder DeepSeek-V3.1 Qwen3-235B-A22B Qwen3-8B DE-Evol Tasks GPT-5 Gemini-2.5-Pro Qwen3-Coder DeepSeek-V3.1 Qwen3-235B-A22B Qwen3-8B 29.85 21.05 25.11 23.94 37.08 42.86 Intermediate Marts All Upstream Errors Intrinsic Errors Total Upstream Errors Intrinsic Errors Total Upstream Errors Intrinsic Errors Total 3.95 6.37 5.63 6.21 9.09 10.62 12.68 15.41 17.61 17.43 19.22 25.06 44.42 42.78 39.23 40.86 75.00 84.96 33.07 53.71 40.14 38.32 31.69 35.84 30.41 26.62 26.03 25.77 10.82 5. 14.97 22.51 16.09 14.22 5.87 5.66 10.05 10.96 10.88 11.68 31.24 28.85 13.96 18.05 19.57 17.65 17.75 22.22 40.46 37.58 36.91 37.45 42.06 34.62 28.93 40.56 35.66 31.87 23.62 27.88 35.78 31.68 33.23 31.93 22.78 42. 10.91 9.80 11.64 9.00 0.46 0.48 6.95 9.97 7.71 9.93 29.16 36.22 18.54 26.29 25.57 20.96 21.16 25.02 42.73 41.65 40.94 41.86 51.94 78.48 29.45 36.09 37.21 29.96 21.62 25.50 40.47 36.41 33.60 34.65 65.91 74. 20.39 38.30 22.53 20.89 12.47 10.78 33 Table 19 Granular analysis of Dependency Errors in DE-Impl and DE-Evol tasks. The breakdown distinguishes between Missing dependencies (failure to identify necessary upstream nodes) and Extra dependencies (hallucinating unnecessary edges). Notably, DE-Evol tasks reveal pronounced bias toward missing dependencies, highlighting the challenge of preserving lineage integrity during code modification."
        },
        {
            "title": "Extra dependencies",
            "content": "Missing Extra DE-Impl Tasks GPT-5 Gemini-2.5-Pro Qwen3-Coder DeepSeek-V3.1 Qwen3-235B-A22B Qwen3-8B DE-Evol Tasks GPT-5 Gemini-2.5-Pro Qwen3-Coder DeepSeek-V3.1 Qwen3-235B-A22B Qwen3-8B 45.42 51.02 47.92 48.53 61.43 52.60 39.15 52.49 48.65 45.01 43.31 46.92 52.61 50.44 50.81 49.26 55.46 56. 39.50 42.87 43.70 38.82 28.74 20.88 66.01 67.31 66.13 65.68 73.94 70.55 56.45 64.98 63.29 59.23 53.86 53.44 burden related to context retention, where models struggle to fully trace the downstream ramifications of schema modifications. Architecture-level attrition in file identification. Table 17 demonstrates notable performance inversion regarding scope identification. Unlike DE-Impl where the scope is constructive, DE-Evol demands discriminative identification of files for modification. Surprisingly, for SOTA models like GPT-5, the rate of SQL Omission is higher in DE-Evol (11.69%) compared to DE-Impl (5.18%). This data suggests that the discriminative task of identifying specific files for modification within large codebase presents greater challenge than the constructive task of pipeline generation. While base models struggle with the sheer complexity of DE-Impl, advanced models are constrained by the precision required for impact analysis in DE-Evol, indicating that identifying the modification scope remains distinct bottleneck. DE-Evolution error case. To illustrate how evolution errors can propagate across layers and distort downstream business metrics, we present pipeline-level DE-Evol example in Fig. 15. D.4 DA Case To illustrate typical failure modes in DA tasks, Tab. 20, Tab. 21, and Tab. 22 present focused case studies of Planning, Execution, and Interpretation errors, respectively. First, scoping lapse omitted required unstructured data, yielding biased sample and invalidating all downstream analysis. Second, despite sound plan, key metric was computed with an incorrect formula (simple average instead of weighted average), producing misleading channel insights. Third, even with flawless calculations, the agent failed to synthesize findings into context-aware conclusion and omitted mandatory limitations and safety disclaimer. Together, these cases demonstrate that reliable DA outputs require aligned rigor across planning, implementation, and interpretation, with checks that prevent any single stage from compromising the whole."
        },
        {
            "title": "E Annotation Details",
            "content": "E.1 Data Collection Data synthesis for DE tables. Our DE tables originate from 73 enterprise-grade SaaS domains and their companion data-transformation projects, providing production-style schemas and realistic dependencies. Starting from minimal business contract (target grain, primary/foreign keys, required metrics), we expand to end-to-end datasets and scale them while preserving business semantics and referential integrity. To keep the data mock both controllable and realistic, we highlight only the key steps: 1) Schema fidelity: 34 Figure 15 DE-Evol pipeline-level error case. Layer 1 (Staging) contains duplicate current rows where the same account_id appears multiple times with is_most_recent = true. Layer 2 (Intermediate) joins to campaign stats while filtering on is_most_recent = true, causing A001s spend to be double-counted (total_spend becomes 20,000 instead of 10,000). Layer 3 (Marts) aggregates the erroneous intermediate table, inflating company_total_spend and avg_account_spend (35,000 and 17,500) compared with the correct values (25,000 and 12,500). The figure highlights how seemingly small staging inconsistency can cascade into materially incorrect executive metrics. 35 Table 20 Focused case study of critical Planning Error. This table analyzes the agents plan against pivotal standard (Data Scoping), highlighting omitted steps (in red) that led to fundamentally flawed analysis. Required Planning Step (from Rubric) Agents Plan vs. Actual Action"
        },
        {
            "title": "Outcome",
            "content": "Case Study: Standard 1.1 Data Understanding & Scoping Step 1.1.A.1: Filter using the structured Education Requirement column. Step 1.1.A.2: Additionally extract candidates from the Job Description column. Step 1.1.A.3: Further apply complex filtering rules to the Job Description column. The agent correctly planned and executed this step. CRITICAL PLANNING FAILURE: This step was entirely omitted from the agents plan; it never considered searching this column. PASS FAIL CRITICAL PLANNING FAILURE: This more advanced step was likewise completely absent from the agents plan. FAIL Consequence of the Flawed Plan: By omitting two required data sources in the planning stage, the agent analyzed an incomplete and biased sample (9,073 records instead of the correct 11,838), thereby invalidating all subsequent analysis. This is textbook Planning Error: once the initial strategy is faulty, execution quality cannot rescue the outcome. Final score for this standard: 1 / 4. Table 21 Focused case study of critical Execution Error. This table examines the agents implementation for Standard 2.1 (Channel Performance Metrics), illustrating how an otherwise sound plan partially failed due to an improper formula for key metric."
        },
        {
            "title": "Calculation Required by the\nRubric",
            "content": "Agent Implementation vs. Correct Method"
        },
        {
            "title": "Outcome",
            "content": "Case: Standard 2.1 Channel Performance Metrics Sub-standard 2.1.A.1: Compute Sales Volume by channel. Sub-standard 2.1.A.2: Compute Total Revenue by channel. Sub-standard 2.1.A.3/4: Compute Average Unit Price by channel. The agent correctly used GROUP BY with SUM(sales_volume). PASS The agent correctly used GROUP BY with SUM(total_revenue). PASS CRITICAL EXECUTION ERROR: The agent treated unit price as simple average rather than revenue-weighted average. As result, the reported average prices were incorrect and led to misleading conclusions about channel profitability. FAIL Impact of the Execution Deviation: Although the agents overall plan for channel analysis was sound, using the wrong formula for single critical metric (Average Unit Price) produced misleading conclusions about channel profitability, directly undermining any price-based strategic recommendation. This constitutes canonical Execution Error. Final score for this standard: 5 / 6. Table 22 Focused case study of critical Interpretation Error. The table shows stark contrast between the agents successful execution of calculations and its failure to synthesize those results into meaningful, context-aware conclusion. Analytical Stage (from Rubric) Agents Performance & Justification"
        },
        {
            "title": "Outcome",
            "content": "Case Study: Analysis of Students with Suicidal Ideation Stage 1: Execution & Calculation (Standards 1.1 1.4) Stage 2: Interpretation & Synthesis (Standard 1.5: Create \"high-risk profile\") Stage 3: Contextual Understanding (Standard 2.2: Provide safety disclaimer) The agents plan was sound and its execution was flawless. It successfully filtered the correct data population and accurately calculated all required statistical metrics (e.g., average economic/academic pressure, lifestyle habit percentages). PASS CRITICAL INTERPRETATION FAILURE: The agent failed to synthesize the previously calculated statistics into coherent, higherlevel insight. Instead of creating \"profile,\" the agent merely listed the numbers again. The judge noted the summary was not deep enough and merely restated the tables content. CRITICAL INTERPRETATION FAILURE: The agents final output completely omitted the mandatory \"Limitations and Safety Disclaimer.\" This demonstrates failure to understand the serious and sensitive context of the topic, which is key part of providing responsible and complete analytical deliverable. FAIL FAIL Consequence of Flawed Interpretation: This case exemplifies pure Interpretation Error. The agent acted as perfect calculator, producing correct data (Stage 1). However, it failed at the final and most critical stage: transforming that data into meaningful, insightful, and contextually appropriate conclusion (Stages 2 & 3). 37 retain PK/FK, uniqueness, not-null, and domain constraints; 2) Distributions & dependencies: fit marginal distributions and model conditional links (e.g., countrycurrency/timezone); 3) Temporal coherence: inject seasonality, trend, and holiday effects while maintaining factdimension integrity; 4) Noise & edge cases: introduce controlled missingness/outliers/type coercions and design stressors that expose pipeline fragility (e.g., duplicate current rows, currency conflicts, timezone mismatches). The synthesis pipeline is implemented in Python (pandas, numpy, faker) with custom generators to scale volume while honoring inter-column dependencies and business invariants. E.2 Construction details of DAComp-DE This subsection presents our experience constructing the DAComp-DE corpus. We outline an end-to-end process across three tracksArchitecture, Implementation, and Evolutionspanning the baseline derived from 73 enterprise-grade SaaS domains and their data-transformation projects to pure-SQL normalization and validation, high-level requirement setting for blueprinting, contract-driven realization into working SQL, and change-oriented migration under realistic constraints. The summary reflects decisions and best practices agreed upon by domain experts to ensure rigor, reproducibility, and evaluability. E.2.1 Construction details of DAComp-DE-Architecture Baseline curation and normalization. We first select open-source dbt projects that are license-compliant and empirically verified to be error-free, and normalize them into pure-SQL repositories by expanding materializations and macros while freezing model dependencies. Senior data engineers conduct systematic audit of join semantics, analytical grains, window specifications, SCD handling, and testing assumptions, thereby establishing high-quality baseline suitable for controlled evaluation. High-level requirement formulation. Building on this baseline, we define task statements grounded in realistic enterprise scenarios: they provide only business context, overarching objectives, and expected outputs, without detailed metric definitions, precise calculation rules, or data constraint specifications. Such descriptions emphasize openness and cross-system characteristics, reveal gaps not covered by the existing repository, and intentionally avoid prescribing implementation paths or technical details. The model is expected to autonomously plan blueprintidentifying key entities and dependencies, delineating layers and boundaries, and completing testing and freshness strategiesultimately producing an executable architectural blueprint that evaluates its ability to plan end-to-end SQL projects and set constraints under incomplete information. E.2.2 Construction details of DAComp-DE-Implementation Contract formalization. DE-Impl is constructed by deriving rigorous requirements specification from the vetted SQL baseline in the form of standardized data_contract.yaml that follows enterprise conventions. The contract formalizes model inventory and lineage, table and column schemas with constraints, declared grains and time windows, metric definitions with coherent units and currency normalization, as well as data quality, freshness, and performance policies. E.2.3 Construction details of DAComp-DE-Evolution Change specification. For DE-Evol, we start from high-quality, production-style SQL repository and propose change requests driven by realistic enterprise pressuressuch as revised metric definitions, altered analytical windows, schema drift, or governance hardening. Multiple experts specify unambiguous business semantics, distinguish breaking from non-breaking changes, and design safe migration plan that anticipates dependency revisions and testing upgrades. E.3 Annotation details of DAComp-DA In this section, we present the experience regarding the annotation of DAComp-DA data, which is summarized from our previous project discussion meetings and alignment meetings. 38 E.3.1 Core Design Principles Strategic diversity The core of the Rubric is to evaluate problem-solving strategies, not steps. Each scoring Path must represent methodologically distinct and self-contained solution. We avoid designing complete versus abridged versions of the same path. For example, analyzing all provinces and analyzing subset of provinces should not be two separate Paths; the latter is merely an incomplete execution of the former. Objective evaluation Scoring criteria must be quantifiable and reproducible to minimize scorer subjectivity. All items should be based on explicit evidence. Guideline: Any Accuracy item requiring numerical verification must have pre-calculated Anchor Value. For open-ended paths without single correct answer, Pseudocode or clear methodological verification process must be provided. Dimensional separation of abilities Complex analytical skills are decomposed into independent scoring dimensions for fairer and more granular assessment of model performance. Guideline: Strictly distinguish between procedural execution (were the steps completed?), computational accuracy (were the numbers correct?), and insightful conclusion (was the interpretation meaningful?), designing them as separate scoring items. E.3.2 Structural components of the rubric The Rubric employs four-level hierarchical structure to deconstruct tasks, ensuring comprehensive and granular evaluation. Requirement. Definition: The highest-level objective of the task, directly corresponding to core analytical request from the user. Example: Analyze the differences in employee attrition rates across departments and their causes. Standard. Definition: key analytical step that must be completed or core conclusion that must be reached to fulfill Requirement. Example: Standard 1: Calculate and verify the attrition rate differences between departments; Standard 2: Identify the key factors causing these differences. Path. Definition: methodologically distinct and valid strategy for meeting Standard. This is the core of the Rubrics design. Example: Under the standard of verifying differences, Path could be performing statistical significance test (e.g., Chi-squared test), while Path could be making descriptive statistical comparison (e.g., percentage difference). Sub-standard / rubric item. Definition: The smallest scorable unit of the Rubric, nested under specific Path and adhering strictly to the principle of dimensional separation. It comprises three main types: Completeness: Assesses whether all required steps for given Path were executed. Focuses on what was done. Accuracy: Assesses whether the computational results or execution process are correct. Focuses on if it was done correctly. For deterministic paths, this is verified against an Anchor Value; for open-ended paths, it is verified against methodological process or Pseudocode. Insightfulness: Assesses whether reasonable and valuable conclusion or insight was derived from the correct results. Focuses on if the results were understood. E.3.3 Golden Rules for Authors These are the disciplinary requirements to ensure the quality and consistency of the Rubric. While these guidelines ensure consistency in creating rubrics for known strategies, the following section details our methodology for fairly evaluating novel or unanticipated solutions that may not align with pre-enumerated paths. 39 Calculate first, then author. Before finalizing the rubric, authors must personally run the complete analysis with code to calculate all Anchor Values required for the Accuracy assessment. This is the cornerstone of ensuring objective scoring. Be specific and unambiguous. Every statement in the Rubric must be directive and unambiguous. Avoid subjective terms like approximately, good, or relatively comprehensive to minimize scorer discretion. Avoid zero-point paths. models output that does not match any valid path will naturally receive no score for that standard. If method is not worthy of credit, it should not be designed as distinct Path. A"
        },
        {
            "title": "F Discuss",
            "content": "F.1 Discussion of Handling Unenumerated Solution Paths Accuracy is the most critical dimension in our rubric. Since fully listing all valid analytical paths is often infeasible, we adopt three-tier, progressively relaxed design for Accuracy: (i) direct enumeration with numeric anchors when the correct outcome can be exhaustively determined; (ii) constrained computation with pseudo-code anchors when procedures are well-defined but paths are not exhaustively enumerable; and (iii) principle-based assessment for highly open-ended cases. Standardized assessment for common paths. We standardize scoring whenever we can verify correctness deterministically. Tier-1 (numeric anchors): for tasks whose outcomes can be exhaustively enumerated, we embed the reference value directly into the rubric (e.g., How many users satisfy condition X?), yielding absolute, reproducible checks. Tier-2 (pseudo-code anchors): for tasks with well-specified computation but multiple equivalent derivations (e.g., conversion rate with alternative weighting schemes), we prescribe canonical steps in pseudo-code to constrain the procedure. This enables process-level verification (inputs, ordering, aggregation, null/edge handling) without enumerating every path, preserving both precision and reproducibility. Principle-based assessment for novel paths. minority of tasks are intrinsically open-ended, where enumeration or pseudo-code templating is impractical. Here we evaluate Accuracy via methodological principles rather than single anchor value. For example, key-driver identification task may be solved by regression with coefficient interpretation (a pre-defined path), or by gradient boosting with SHAP attributions (an unenumerated path). We score such solutions on: (1) Methodological appropriateness (the method is suitable for the stated objective and data regime); (2) Correctness of execution (the pipeline is implemented soundly, with valid preprocessing, estimation, and validation); and (3) Soundness of interpretation (claims follow from the produced evidence, with clear caveats). This soft layer ensures valid but unconventional approaches are not penalized. By construction, most DAComp items fall into Tiers 12, where numeric or pseudo-code anchors provide deterministic checks; Tier 3 is reserved for genuinely open-ended cases to maintain fairness without sacrificing rigor. F.2 Discussion of Ambiguous of Requirements Implementation and Evolution tasks in DAComp-DE are designed as deterministic evaluations. To balance realism with unambiguous executability, we adopt three principles: 1) Professionalism. Requirements are sourced from enterprise-style projects and vetted by senior data engineers for cross-layer impact, metric definitions, SCD handling, and temporal semantics. Implementation tasks emphasize canonical modeling pipelines from scratch; Evolution tasks mirror real change requests (e.g., metric revision, source replacement). 2) Unambiguity. Implementation (node-first): each SQL node has atomic contracts (schema, PK/grain, time, nulls, joins, aggregation, SCD, idempotency). Multiple agents must converge under frozen contracts; 40 discrepancies trigger tighter specifications. Evolution (delta-first): natural-language changes are mapped into minimal verifiable deltas (schema/logic/lineage), with explicit impact scope and beforeafter anchors; agent disagreement leads to refined deltas or explicit assumptions. 3) Realism. Implementation: converged nodes are composed into multi-node tasks, with contracts and assumptions documented (e.g., data_contract.yaml). Evolution: favors backward-compatible evolution (added columns/views, metric versioning); destructive changes require migration notes. All assumptions are logged for reproducibility. F.3 Discussion on the Selection of Judger LLM As shown in Tab. 7, both O4-Mini and gemini-2.5-flash achieve human-level agreement, while stronger proprietary models (e.g., gemini-2.5-pro, GPT-5) yield even higher consistency. For DAComp we standardize on gemini-2.5-flash, as it balances (1) cost efficiency for large-scale benchmarking, (2) stable and low-latency inference, (3) reproducibility across runsand (4) community accessibility. Choosing widely available model ensures that our evaluation pipeline can be easily adopted, verified, and extended by others. F.4 Discussion on End-to-End Evaluation Current DAComp tasks span complementary stages of the data intelligence lifecycle: DE-Architecture (highlevel specification and planning), DE-Implementation (multi-layer pipeline construction), DE-Evolution (safe modification under requirement changes), and DA (open-ended analysis over downstream data). Taken together, these stages delineate strictly end-to-end processfrom requirement articulation, through system realization and iterative evolution, to analytical insight and decision supportcovering full loop from planning and implementation to evolution and interpretation. At present, we evaluate these stages modularly and in decoupled fashion to enable controlled measurement at each step. Our next key objective is to integrate them into single, end-to-end longitudinal evaluation: single agent carries requirements through implementation and change propagation, and ultimately completes analysis and reporting. We contend this end-to-end setup offers substantial scientific and practical value: it stress-tests the end-to-end consistency of planningexecutionevolutioninterpretation, better reflects real engineering workflows, and advances toward comprehensive assessment of autonomous data agents end-to-end capabilities."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Institute of Automation, CAS",
        "NUS",
        "TikTok",
        "UC San Diego",
        "University of Chinese Academy of Sciences"
    ]
}