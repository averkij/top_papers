{
    "paper_title": "Token-based Audio Inpainting via Discrete Diffusion",
    "authors": [
        "Tali Dror",
        "Iftach Shoham",
        "Moshe Buchris",
        "Oren Gal",
        "Haim Permuter",
        "Gilad Katz",
        "Eliya Nachmani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio inpainting refers to the task of reconstructing missing segments in corrupted audio recordings. While prior approaches-including waveform and spectrogram-based diffusion models-have shown promising results for short gaps, they often degrade in quality when gaps exceed 100 milliseconds (ms). In this work, we introduce a novel inpainting method based on discrete diffusion modeling, which operates over tokenized audio representations produced by a pre-trained audio tokenizer. Our approach models the generative process directly in the discrete latent space, enabling stable and semantically coherent reconstruction of missing audio. We evaluate the method on the MusicNet dataset using both objective and perceptual metrics across gap durations up to 300 ms. We further evaluated our approach on the MTG dataset, extending the gap duration to 500 ms. Experimental results demonstrate that our method achieves competitive or superior performance compared to existing baselines, particularly for longer gaps, offering a robust solution for restoring degraded musical recordings. Audio examples of our proposed method can be found at https://iftach21.github.io/"
        },
        {
            "title": "Start",
            "content": "Token-based Audio Inpainting via Discrete Diffusion Tali Dror, Iftach Shoham, Moshe Buchris, Oren Gal, Haim Permuter, Gilad Katz, Eliya Nachmani 5 2 0 2 4 ] . [ 2 3 3 3 8 0 . 7 0 5 2 : r Abstract Audio inpainting refers to the task of reconstructing missing segments in corrupted audio recordings. While prior approachesincluding waveform and spectrogram-based diffusion modelshave shown promising results for short gaps, they often degrade in quality when gaps exceed 100 milliseconds (ms). In this work, we introduce novel inpainting method based on discrete diffusion modeling, which operates over tokenized audio representations produced by pre-trained audio tokenizer. Our approach models the generative process directly in the discrete latent space, enabling stable and semantically coherent reconstruction of missing audio. We evaluate the method on the MusicNet dataset using both objective and perceptual metrics across gap durations up to 300 ms. We further evaluated our approach on the MTG dataset, extending the gap duration to 500 ms. Experimental results demonstrate that our method achieves competitive or superior performance compared to existing baselines, particularly for longer gaps, offering robust solution for restoring degraded musical recordings. Audio examples of our proposed method can be found at https://iftach21.github.io/"
        },
        {
            "title": "Introduction",
            "content": "Audio inpainting refers to the task of reconstructing missing or corrupted segments of an audio It is fundamental inverse problem in audio processing, with applications ranging signal [1]. from restoring damaged recordings and removing artifacts, to filling in gaps caused by data loss in transmission or editing operations [24]. Traditional approaches to this problem often relied on signal modeling techniques such as autoregressive models [1], sparse representations [19], or linear predictive coding [1]. While effective under certain assumptions (e.g., local stationarity), these methods typically perform well only on short gaps and may struggle with long-range dependencies or semantic coherence [24, 37]. In recent years, deep generative models have significantly advanced the state of audio inpainting. Models such as Variational Autoencoders (VAEs) [20] and diffusion probabilistic models [15, 17] have demonstrated the ability to learn expressive priors directly from large-scale audio datasets. Notably, diffusion models have emerged as particularly powerful tools for solving ill-posed inverse problems due to their iterative denoising process and strong generative capacity. Approaches like DiffWave [15] apply diffusion directly on waveform samples, while others such as MAID [17] and CQT-Diff+ [24] operate in the continuous time-frequency domain using spectrograms or ConstantQ Transform (CQT) representations [24, 37]. In this work, we propose novel approach to audio inpainting based on discrete diffusion modeling inspired from the work of [18]. Unlike prior methods that operate in the continuous domain [15,17, *Equal contribution 1 24, 37], our method applies the diffusion process to discrete latent space. Specifically, we employ the WavTokenizer [11] to quantize audio signals into compact sequences of discrete tokens, and perform the diffusion process entirely in this categorical space. This formulation allows the model to capture high-level semantic structures in audio, while avoiding the challenges of modeling raw waveforms or spectrograms directly. Our discrete approach provides several key benefits. First, the token representation reduces the sequence length compared to raw time-domain audio, and filters out low-level noise, simplifying the generative task. Second, discrete diffusion enables stable and coherent long-range modeling, making it well-suited for reconstructing larger gaps. Finally, operating in token space mitigates artifacts such as oversmoothing and spectral blurring, which are common in continuous-domain inpainting models [24, 37]. This paper presents the first study of Discrete Diffusion Models (DDMs) for audio inpainting. Our method sets new direction by unifying discrete generative modeling with inpainting tasks, and offers promising alternative to waveform and spectrogram based diffusion approaches. Experimental results show that our model achieves high perceptual quality across diverse musical inputs and gap durations, even in challenging inpainting scenarios."
        },
        {
            "title": "2.1 Methods for Audio Inpainting",
            "content": "The term audio inpainting was used by Adler et al. [1] to describe the restoration of missing or corrupted segments in audio signals. Although the terminology is relatively recent, the problem itself has been studied under various names, including audio interpolation [19], audio extrapolation [16], missing sample reconstruction [1], waveform substitution [1], and imputation [16]. Early methods primarily focused on short-gap restoration, typically below 100 ms, by assuming local stationarity in the signal. major class of such methods is based on Auto-Regressive (AR) modeling [1], where each sample is predicted from previous samples. These techniques are effective for short segments, but performance deteriorates with increasing gap length. Another classical approach exploits the sparsity of audio in timefrequency representations, such as the Short-Time Fourier Transform (STFT) or Gabor transforms [1, 16]. Sparse reconstruction algorithms like Orthogonal Matching Pursuit (OMP) [34] are used to find sparse coefficient vectors that match the known parts of the signal. Later improvements employed adaptive dictionary learning instead of fixed transforms. For example, Taubock et al. [32] learned Gabor-like atoms from the neighborhood of the gap, improving reconstruction fidelity. Nonnegative Matrix Factorization (NMF) based methods [22] represent the spectrogram as product of low-rank matrices, allowing inference of missing timefrequency bins. Probabilistic extensions of NMF further enhance robustness in noisy settings. Beyond linear and sparse priors, more structured models have also been explored. Sinusoidal modeling leverages harmonic regularities in musical or speech signals [1], while graph-based methods use audio self-similarity to propagate known content into missing regions. Notably, Perraudin et al. [27] introduced graph Laplacian regularization on self-similarity matrices to infer missing parts from acoustically similar, uncorrupted regions. Although these classical methods are effective for short-duration gaps, they typically struggle with gaps exceeding 100 ms due to the breakdown of their underlying assumptions. To address longer and more complex gaps, deep learning approaches have emerged. Convolutional neural networks were first applied to spectrogram-based inpainting. Marafioti et al. [20] proposed context encoder, using U-Netstyle architecture to restore tens-of-milliseconds gaps from 2 timefrequency representations. Generative Adversarial Networks (GANs) were then introduced to increase generative diversity and temporal coherence. Ebner and Eltelt [8] employed Wasserstein GAN (WGAN) with multi-context conditioning to inpaint gaps up to 500 ms, while Marafioti et al. [19] extended this idea with GACELA, multi-scale GAN that leverages latent conditioning to handle gaps as long as 11.5 seconds. These models outperform traditional methods on long gaps, especially when rich temporal dependencies or musical structures must be captured. Overall, the evolution from linear and sparse signal models to modern deep learning approaches has significantly advanced the state of audio inpainting, particularly for long and structured signal gaps."
        },
        {
            "title": "2.2 Audio Inpainting using Diffusion Models",
            "content": "Diffusion models, also known as score-based generative models, have recently revolutionized generative modeling in vision and audio. These models define forward process that gradually adds noise to data and learn to reverse it via denoising steps (e.g., DDPMs [9] and score-SDEs [29]). In the audio domain, diffusion models have been applied to waveform generation and spectrogram generation, often conditioned on context such as mel-spectrograms. For example, DiffWave [15] is convolutional diffusion model that synthesizes raw-waveform audio (e.g., neural vocoding) with high fidelity. Other works use diffusion in latent spaces or time-frequency representations (e.g., using STFT or constant-Q transforms) to leverage signal structure [23, 24]. Continuousus state diffusion is the norm, but discrete-diffusion approaches on tokenized audio have also been explored (e.g., DiffSound [36] uses discrete diffusion model on quantized spectrogram tokens). Conceptually, discrete diffusion iteratively masks or replaces tokens (suitable for categorical data), whereas continuous diffusion perturbs continuous values (e.g., waveforms or spectrogram magnitudes) [9,29]. This distinction can affect inpainting: continuous models yield smooth interpolations, while discrete models may introduce quantization artifacts but can model rich multimodal structures [36]. Recently, diffusion-based methods have been applied to audio inpainting. Moliner and Valimaki [24] propose an unconditional diffusion model trained on clean audio that is conditioned in zero-shot manner during inference to fill missing segments. Their model operates on CQT spectrogram, exploiting pitch-equivariant structure via log-frequency representation. During sampling, dataconsistency step is applied so that known samples remain fixed, effectively solving posterior sampling problem [4, 12]. This approach regenerates gaps of arbitrary length: experiments show it matches baselines on short gaps and significantly outperforms them on longer gaps (up to 300 ms). related model, CQT-Diff [23], similarly uses an invertible CQT front-end for diffusion to solve general audio inverse problems (bandwidth extension, declipping, inpainting) and achieves state-ofthe-art restoration results. In contrast, Liu et al. [17] propose MAID, conditional diffusion model specifically trained for long music inpainting; it conditions on surrounding audio and additional signals (e.g., pitch or latent cues) to guide generation. MAID was shown to handle gaps longer than those tackled by zero-shot methods, at the cost of requiring supervised training for the inpainting task. Key distinctions in architecture and conditioning arise between these approaches. Diffusion inpainting models may operate on raw waveforms or on time-frequency representations. Waveform diffusers produce audio directly, while spectrogram-based models interpolate in log-frequency domain [24] or in latent spaces. Conditioning mechanisms also vary: Moliner et al. use an unconditioned diffusion with posterior sampling from known context [24], whereas GANs or conditional diffusers explicitly input the masked audio or latent noise as extra channels during training. Masking strategies typically involve contiguous spans: most studies consider gaps of fixed or random lengths. For example, Moliner et al. evaluate on randomly placed gaps from 25 ms to 300 ms, whereas GAN works have 3 In practice, diffusion models show graceful degradation tackled up to seconds-long gaps [8, 19]. with gap length: short gaps are easily interpolated by all methods, but diffusions strong generative prior yields more realistic content when extrapolating beyond 100 ms, compared to sparsity or autoregressive methods. In summary, diffusion-based audio inpainting leverages the flexibility of score-based generation to fill long gaps with high-fidelity content. These methods contrast with conventional inpainting (which often assumes linear predictability or uses hand-crafted sparsity priors). Ongoing distinctions include whether to use continuous or discrete diffusion (continuous models on audio samples/spectra vs. discrete tokenized models [36]), and how to incorporate context (zero-shot posterior sampling vs. trained conditional networks)."
        },
        {
            "title": "3.1 Discrete Diffusion Models",
            "content": "Continues Diffusion Models (CDMs) [9, 29] have achieved remarkable success across various generative modeling tasks, particularly in the image domain [7]. The high-dimensional nature of raw audio data adds to the difficulty of directly modeling it within the diffusion framework. One effective strategy to address this issue is to first compress continuous audio signals into compact, discrete representations, such as quantized tokens derived from learned codebook [5]. DDMs, which operate in token space, have recently demonstrated strong performance in the domain of natural language generation [18, 25, 28]. To the best of our knowledge, our method Audio Inpainting using Discrete Diffusion Model (AIDD) is the first approach to leverage discrete diffusion model for the task of audio inpainting. Similar to CDMs, DDMs comprise forward noising process that progressively corrupts an initial sample x0 into maximally noisy (fully masked) version xT , and reverse denoising process trained to reconstruct x0 from xT . Forward process. Given the tokenized audio data x0 pdata over finite alphabet = {1, . . . , }, the forward process is defined by transition matrix Qt that indicates how xt1 transits to xt for each step in the forward process. The behavior of the process is given by: P(xt+t = xt = x) = δxy + Qt(y, x)t + O(t2) where Qt(y, x) denotes the rate of transitioning from state to state at time t. The marginal probability distribution pt over states evolves according to the Kolmogorov forward equation [2, 3]: (1) dpt dt = Qtpt (2) In practice, the transition matrix is often written as Qt = σ(t)Q, where σ(t) is scalar noise schedule and is fixed matrix defining the corruption structure. As , the process converges to simple stationary distribution pbase, such as uniform or absorbing. Following [18] we use an absorbing design of Q, where all tokens eventually transition into terminal mask symbol. Reverse Process. The reverse process is described by new rate matrix Qt, yielding the following time-reversed differential equation [13, 31]: dpT dt = QT pT t, (3) with the reverse transition matrix defined as: Qt(y, x) = pt(y) pt(x) (cid:88) Qt(x, x) = Qt(x, y), Qt(y, x) (4) To simulate the reverse process, it is sufficient to estimate the ratio pt(y) score [18, 21]. This ratio can be approximated by neural score function: pt(x) , referred to as the concretey=x sθ(x, t) (cid:21) (cid:20) pt(y) pt(x) yX , y=x (5) Training. To train this score function, recent work has proposed the Diffusion Weighted Denoising Score Entropy (DWDSE) objective [18], which is defined as: (cid:90) 0 Extpt0(x0) (cid:88) y=xt (cid:18) Qt(xt, y) sθ(xt, t)y pt0(y x0) pt0(xt x0) (cid:19) log sθ(xt, t)y + dt, (6) (cid:16) pt0(yx0) pt0(xtx0) (cid:17) where = , and K(a) := log is convex regularization term. Once trained, the model can generate samples by stepping through the reverse-time equation (3) and replacing the concrete-score with the trained network (5)."
        },
        {
            "title": "3.2 Audio Tokenization",
            "content": "WavTokenizer [11] is recent state-of-the-art acoustic codec introduced to provide efficient, semantically rich audio tokenization, optimized for audio language modeling. The model is structured around classical encoder-quantizer-decoder pipeline, yet introduces several innovations that make it suitable for extreme audio compression and downstream generative tasks, including audio inpainting. The encoder consists of stack of convolutional layers followed by an Long-Short-TermMemory model (LSTM) and final convolution, designed to reduce the temporal resolution while preserving critical acoustic features. key novelty of WavTokenizer is its use of single vector quantizer with an expanded codebook space, instead of multi-stage residual quantization stack. This simplifies the token sequence while maintaining strong representational fidelity. The quantized latent vectors are then passed to decoder that notably incorporates an inverse STFT upsampling structure, avoiding aliasing artifacts common in traditional transposed convolutions. The decoder also integrates an attention module and ConvNeXt blocks to enhance semantic modeling and reconstruction quality."
        },
        {
            "title": "4 Audio Inpainting using Discrete Diffusion Model",
            "content": "To address the task of audio inpainting, we propose novel token-based methodAIDD, that leverages recent advancements in audio compression and discrete generative modeling. Our method comprises three main components: (1) audio tokenization using WavTokenizer [11], (2) generative modeling using DDM [18], and (3) waveform reconstruction via token decoding [11]. In this section, we present our method for implementing audio inpainting using DDM."
        },
        {
            "title": "4.1 Method Overview",
            "content": "Our method consists of WavTokenizer [11] that transforms high-dimensional raw audio signals into compact sequences of discrete tokens and reconstructs it back to raw audio. Diffusion Transformer (DiT) architecture [26] which learns to predict masked tokens through the reverse diffusion process. The lustration of our method is shown in Figure 1. Figure 1: Our method operates on audio signals with missing (silent) segments. First, the input waveformcontaining silence gapis processed by the WavTokenizer encoder, which converts the audio into discrete sequence of tokens. Next, DiT performs inpainting by iteratively predicting the masked tokens, resulting in reconstructed token sequence. Finally, the reconstructed tokens are passed through the WavTokenizers decoder to synthesize the output audio waveform in the masked part."
        },
        {
            "title": "4.2 Audio Tokenization",
            "content": "The first step in our method is to convert the raw audio waveform into discrete audio tokens that can be efficiently processed and trained by the DDM. We utilize pre-trained audio tokenizer WavTokenizer [11], which compresses high-resolution audio into compact sequence of discrete tokens using single quantizer. Despite its extreme compression, WavTokenizer preserves high reconstruction fidelity and rich semantic content. Unlike conventional frameworks for audio inpainting that operate directly on the audio waveform or the spectrogram [24], audio tokenization allows us to formulate audio inpainting as discrete sequence completion task."
        },
        {
            "title": "4.3 Discrete Diffusion Model",
            "content": "At the core of our method is DiT architecture [26], which integrates time conditioning into standard encoder-only transformer [6, 35] and following the work of [18], we incorporate rotary positional encoding [30]. To operate effectively in the discrete token space, we use the denoising score entropy formulation, guiding the reverse diffusion dynamics as seen in 6."
        },
        {
            "title": "4.4 Inference Pipeline",
            "content": "At inference time, the input waveform containing masked audio segment is tokenized using the pretrained WavTokenizer encoder. The corrupted region is then inpainted by the trained DDM, which applies reverse diffusion to predict the missing tokens conditioned on the surrounding context. 6 The resulting token sequence is decoded back into waveform space to reconstruct the missing audio. To minimize unnecessary reconstruction loss, we only replace the inpainted segment in the original waveform, preserving the unmasked regions in their original form. This avoids re-encoding and decoding the intact audio, whicheven with the high fidelity of WavTokenizer can introduce minor artifacts. To ensure smooth transitions between the original waveform and generated audio, we apply short crossfade at the boundaries of each inpainted segment, linearly blending the waveforms over 10 ms window. An overview of the inference pipeline is shown in Figure 1."
        },
        {
            "title": "5 Experiments",
            "content": "To assess the performance of our audio inpainting framework, we adopt combination of metrics for evaluations, capturing both the technical fidelity and perceptual plausibility of the reconstructed audio."
        },
        {
            "title": "5.1 Objective Metrics",
            "content": "Frechet Audio Distance (FAD). We utilize the FAD , an established perceptual metric that measures the distance between distributions of real and generated audio features [14]. FAD assesses how similar the inpainted audio is to real music in terms of high-level perceptual attributes, such as timbre and texture, offering robust indication of audio realism. In our evaluation we used VGG model. Objective Difference Grade (ODG). We evaluate perceptual audio quality using the ODG computed with the PEMO-Q model [10]. PEMO-Q simulates the human auditory system to compare internal representations of the original and reconstructed signals. The resulting Perceptual Similarity Measure (PSM) is mapped to an ODG score ranging from 0 (imperceptible distortion) to 4 (very annoying artifacts). As reference-based metric aligned with subjective listening tests, ODG complements waveform-based metrics by capturing perceptual fidelity. Log Spectral Distance (LSD). We also adopt the LSD, following prior work such as [24], to quantify spectral differences between original and reconstructed audio. LSD is defined as: LSD ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 (cid:18) log Xt,k2 log (cid:12) ˆXt,k (cid:12) (cid:12) 2(cid:19)2 (cid:12) (cid:12) (cid:12) (7) where Xt,k = STFT(x0) and ˆXt,k = STFT(ˆx0) are the STFT of the original and reconstructed signals, respectively. For our experiments, we use window size of = 2048 samples and hop length of 512 samples. This metric captures fine-grained spectral distortions introduced during inpainting."
        },
        {
            "title": "5.2 Experimental setup",
            "content": "We trained our model on the MusicNet dataset [33], which provides predefined split of training and test sets. The training set comprises 330 freely-licensed classical music recordings. During training, raw audio signals are first tokenized and truncated to fixed length of 1024 tokens (approximately 13.6 seconds). random timestep is selected, and noise (in the form of token masking) is applied to the corresponding segment. The DDM is then trained to reconstruct the 7 original token sequence from this corrupted input. Through reverse diffusion steps, the model gradually denoises the token sequence, effectively learning to recover clean representations from noisy ones in token space, illustrated in Figure 2. Figure 2: Training pipeline illustrating the application of token corruption and backpropagation guided by the DWDSE loss Training was conducted using the AdamW optimizer with learning rate of 106. We used batch size of 128, with each sample containing 1024 tokens. The model was trained for approximately 400,000 steps over the course of two days on single NVIDIA A6000 GPU."
        },
        {
            "title": "6 Results",
            "content": "To evaluate the effectiveness of our proposed audio inpainting method, we followed the experimental protocol of [24] and selected 60 previously unseen music segments from the MusicNet test set [33], each with duration of 4.17 seconds. For each segment, we introduced four synthetic gaps at fixed, evenly spaced locations. The gap durations varied across experiments, ranging from 50 ms to 300 ms. These corrupted inputs were then processed using our model (see Section 4.4), which reconstructed the missing audio to produce complete, inpainted waveforms. We evaluated each reconstructed sample against its corresponding ground truth using the objective metrics described in Section 5.1. Table 1 summarizes the average performance across different durations of the gaps. In Figure 3, we compare our method with multiple similar methods on MusicNet [33] dataset. Our modelbased on DDM - demonstrates strong inpainting performance across range of gap durations. When comparing to the diffusion-based approach of [24], we observe comparable or superior performance, particularly in terms of the FAD. While [24] reported FAD of approximately 4.9 for 300 ms gap, our method achieves significantly lower FAD of 3.81, indicating more realistic reconstructions. In general, our method AIDD out performs CQT-Diff+ across all metrics. 8 Gap (ms) FAD () LSD () ODG () 50 100 200 300 1.1953 1.6655 3.0489 3.8067 0.0824 0.1283 0.2161 0. -3.011 -3.191 -3.277 -3.300 Table 1: Performance of our method (AIDD) across different gap lengths using FAD, LSD, and ODG metrics. Down-arrows () meaning the lower the score the better. Up-arrows () meaning the higher the score the better. Although our method will benefit from larger corpus of music, even small dataset such as MusicNet [33] was enough to allow the DDM to learn effectively. Figure 3: Average objective metricsincluding Log-Spectral Distance (LSD), Objective Difference Grade (ODG), and Frechet Audio Distance (FAD)computed across gap lengths ranging from 25 to 300 ms, following the evaluations of [24]."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we proposed novel approach to audio inpainting by combining DDM with WavTokenizer-based encoder-decoder architecture [11]. Rather than applying diffusion directly to raw waveformswhich are noisy and difficult to modelwe operate in discrete token space, enabling the DDM to effectively learn the underlying distribution of musical structure. This allows for more stable training and more coherent reconstruction during inference. Our method not only advances the task of audio inpainting, but also lays the foundation for additional generative capabilities, including unconditional music generation and music continuation based on given context. Experimentally, we demonstrated that our method achieves competitive or superior results compared to existing state-of-the-art approaches, particularly in challenging settings with longer gaps. While many previous methods focus on short gaps of up to 300 ms, our model maintains strong performance even on 500 ms gapshighlighting its potential applicability to real-world scenarios where such gaps can significantly affect the listening experience. Overall, our method introduces 9 robust and extensible paradigm for music-based generative modeling in the token domain, with promising applications in both restoration and creative audio generation tasks."
        },
        {
            "title": "References",
            "content": "[1] Amir Adler, Valentin Emiya, Maria Jafari, Michael Elad, Remi Gribonval, and Mark Plumbley. Audio inpainting. IEEE Transactions on Audio, Speech, and Language Processing, 20(3):922932, 2011. [2] William Anderson. Continuous-time Markov chains: An applications-oriented approach. Springer Science & Business Media, 2012. [3] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. [4] Hyungjin Chung, Jeongsol Kim, Michael Mccann, Marc Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022. [5] Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022. [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training In Proceedings of the 2019 of deep bidirectional transformers for language understanding. conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [8] Pirmin Ebner and Amr Eltelt. Audio inpainting with generative adversarial network. arXiv preprint arXiv:2003.07704, 2020. [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [10] Rainer Huber and Birger Kollmeier. Pemo-qa new method for objective audio quality assessment using model of auditory perception. IEEE Transactions on audio, speech, and language processing, 14(6):19021911, 2006. [11] Shengpeng Ji, Ziyue Jiang, Wen Wang, Yifu Chen, Minghui Fang, Jialong Zuo, Qian Yang, Xize Cheng, Zehan Wang, Ruiqi Li, et al. Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling. arXiv preprint arXiv:2408.16532, 2024. [12] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. Advances in Neural Information Processing Systems, 35:2359323606, 2022. [13] Frank Kelly. Reversibility and stochastic networks. PF KellyNew York: Willy, 1981. [14] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Frechet auarXiv preprint dio distance: metric for evaluating music enhancement algorithms. arXiv:1812.08466, 2018. 10 [15] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020. [16] Florian Lieb and Hans-Georg Stark. Audio inpainting: Evaluation of time-frequency representations and structured sparsity approaches. Signal Processing, 153:291299, 2018. [17] Kaiyang Liu, Wendong Gan, and Chenchen Yuan. Maid: conditional diffusion model for long music audio inpainting. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. [18] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution, 2024. URL https://arxiv. org/abs/2310.16834. [19] Andres Marafioti, Piotr Majdak, Nicki Holighaus, and Nathanael Perraudin. Gacela: generative adversarial context encoder for long audio inpainting of music. IEEE Journal of Selected Topics in Signal Processing, 15(1):120131, 2020. [20] Andres Marafioti, Nathanael Perraudin, Nicki Holighaus, and Piotr Majdak. context encoder for audio inpainting. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 27(12):23622372, 2019. [21] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. Advances in Neural Information Processing Systems, 35:3453234545, 2022. [22] Ondˇrej Mokr`y, Paul Magron, Thomas Oberlin, and Cedric Fevotte. Algorithms for audio inpainting based on probabilistic nonnegative matrix factorization. Signal Processing, 206:108905, 2023. [23] Eloi Moliner, Jaakko Lehtinen, and Vesa Valimaki. Solving audio inverse problems with diffusion model. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. [24] Eloi Moliner and Vesa Valimaki. Diffusion-based audio inpainting. arXiv preprint arXiv:2305.15266, 2023. [25] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [26] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [27] Nathanael Perraudin, Nicki Holighaus, Piotr Majdak, and Peter Balazs. Inpainting of long audio segments with similarity graphs. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(6):10831094, 2018. [28] Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. [29] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 11 [30] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [31] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuoustime discrete diffusion models. arXiv preprint arXiv:2211.16750, 2022. [32] Georg Taubock, Shristi Rajbamshi, and Peter Balazs. Dictionary learning for sparse audio inpainting. IEEE Journal of Selected Topics in Signal Processing, 15(1):104119, 2020. [33] John Thickstun, Zaid Harchaoui, and Sham Kakade. Learning features of music from scratch. arXiv preprint arXiv:1611.09827, 2016. [34] Joel Tropp and Anna Gilbert. Signal recovery from random measurements via orthogonal matching pursuit. IEEE Transactions on information theory, 53(12):46554666, 2007. [35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, (cid:32)Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [36] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:17201733, 2023. [37] Hang Zhou, Ziwei Liu, Xudong Xu, Ping Luo, and Xiaogang Wang. Vision-infused deep audio inpainting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 283292, 2019."
        }
    ],
    "affiliations": []
}