{
    "paper_title": "Struct-Bench: A Benchmark for Differentially Private Structured Text Generation",
    "authors": [
        "Shuaiqi Wang",
        "Vikas Raunak",
        "Arturs Backurs",
        "Victor Reis",
        "Pei Zhou",
        "Sihao Chen",
        "Longqi Yang",
        "Zinan Lin",
        "Sergey Yekhanin",
        "Giulia Fanti"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Differentially private (DP) synthetic data generation is a promising technique for utilizing private datasets that otherwise cannot be exposed for model training or other analytics. While much research literature has focused on generating private unstructured text and image data, in enterprise settings, structured data (e.g., tabular) is more common, often including natural language fields or components. Existing synthetic data evaluation techniques (e.g., FID) struggle to capture the structural properties and correlations of such datasets. In this work, we propose Struct-Bench, a framework and benchmark for evaluating synthetic datasets derived from structured datasets that contain natural language data. The Struct-Bench framework requires users to provide a representation of their dataset structure as a Context-Free Grammar (CFG). Our benchmark comprises 5 real-world and 2 synthetically generated datasets, each annotated with CFGs. We show that these datasets demonstrably present a great challenge even for state-of-the-art DP synthetic data generation methods. Struct-Bench also includes reference implementations of different metrics and a leaderboard, thereby providing researchers a standardized evaluation platform to benchmark and investigate privacy-preserving synthetic data generation methods. Further, we also present a case study showing how to use Struct-Bench to improve the synthetic data quality of Private Evolution (PE) on structured data. The benchmark and the leaderboard have been publicly made available at https://struct-bench.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 6 9 6 0 1 . 9 0 5 2 : r Struct-Bench: Benchmark for Differentially Private Structured Text Generation Shuaiqi Wang Carnegie Mellon University Vikas Raunak Microsoft Corporation"
        },
        {
            "title": "Arturs Backurs\nMicrosoft Research",
            "content": "shuaiqiw@andrew.cmu.edu viraunak@microsoft.com arturs.backurs@microsoft.com"
        },
        {
            "title": "Victor Reis\nMicrosoft Research",
            "content": "Pei Zhou Microsoft Corporation"
        },
        {
            "title": "Sihao Chen\nMicrosoft Corporation",
            "content": "victorol@microsoft.com pei.zhou@microsoft.com sihaochen@microsoft.com"
        },
        {
            "title": "Longqi Yang\nMicrosoft Corporation",
            "content": "Zinan Lin Microsoft Research"
        },
        {
            "title": "Sergey Yekhanin\nMicrosoft Research",
            "content": "Longqi.Yang@microsoft.com zinanlin@microsoft.com yekhanin@microsoft.com"
        },
        {
            "title": "Giulia Fanti\nCarnegie Mellon University",
            "content": "gfanti@andrew.cmu.edu Abstract Differentially private (DP) synthetic data generation is promising technique for utilizing private datasets that otherwise cannot be exposed for model training or other analytics. While much research literature has focused on generating private unstructured text and image data, in enterprise settings, structured data (e.g., tabular) is more common, often including natural language fields or components. Existing synthetic data evaluation techniques (e.g., FID) struggle to capture the structural properties and correlations of such datasets. In this work, we propose Struct-Bench, framework and benchmark for evaluating synthetic datasets derived from structured datasets that contain natural language data. The Struct-Bench framework requires users to provide representation of their dataset structure as Context-Free Grammar (CFG). Our benchmark comprises 5 real-world and 2 synthetically generated datasets, each annotated with CFGs. We show that these datasets demonstrably present great challenge even for state-of-the-art DP synthetic data generation methods. Struct-Bench also includes reference implementations of different metrics and leaderboard, thereby providing researchers standardized evaluation platform to benchmark and investigate privacy-preserving synthetic data generation methods. Further, we also present case study showing how to use Struct-Bench to improve the synthetic data quality of Private Evolution (PE) on structured data. The benchmark and the leaderboard have been publicly made available at https://struct-bench.github.io."
        },
        {
            "title": "Introduction",
            "content": "Enterprise settings often feature datasets that include both structured relationships between fields or objects, and fields that contain natural language data. For example, consider dataset of user queries to search engine, and the corresponding results. The dataset is structured in question-and-response structure, and both the query and the response contain natural language. Another example consists of patients medical records, which can include multiple events over time, visits with different providers for different ailments, These authors contributed equally to this work. This work was partially done while an intern at Microsoft. Now at Google DeepMind. Primary internship mentors: Pei Zhou, Zinan Lin. 1 (a) Struct-Bench evaluation pipeline. The synthetic dataset can be generated via DP generation methods with private access to the private dataset. Struct-Bench evaluates synthetic dataset by parsing samples and extracting nodes and their attributes using context-free grammar (CFG) with full access to both private and synthetic datasets. (b) Sample level view of the Struct-Bench. As an example, multi-round conversation is parsed by CFG into several nodes with types Query and Response. Struct-Bench extracts its sample-level and node-level attributes for evaluation. Figure 1: Dataset level and sample level views into the Struct-Bench framework. and associated (natural language) doctors notes. Such datasets are valuable for downstream use cases (e.g., training predictive models, understanding preferences), but they cannot always be used directly, due to privacy or data use restrictions. Differentially private (DP) synthetic data generation is an increasingly important technique for making use of such sensitive datasets [66, 42, 60]. Evaluating the quality of such synthetic data (private or not) is an active research area, with many proposals tailored to unstructured data like images or text [54, 27, 60, 16] and tabular data [36, 55, 66]. Notably, existing synthetic data evaluation frameworks do not naturally capture the salient properties of datasets that feature both general structural properties and natural language elements. Evaluation metrics for unstructured data, like Fréchet Inception Distance (FID) [20], or precision and recall [25, 43], do not capture the structural properties of data. In fact, as we will show in our results, synthetic data that completely fails to capture structural constraints can still achieve high precision and recall scores. For example, the ShareGPT dataset [1], which consists of multi-round conversations between human and an AI agent, requires the format tokens HUMAN: before queries and GPT: before responses. Consider the synthetic sample: \"How are you? Im doing well.\" Although semantically reasonableand therefore scoring high on precisionit violates the required format. This underscores the importance of structure-aware evaluation. On the other hand, evaluation frameworks for tabular data are designed primarily for datasets with categorical or numeric fields [19, 62, 8], and do not naturally extend to natural language fields. For instance, these frameworks often compare k-way marginal distributions in the real and synthetic data, which is not meaningful for natural language. In this work, drawing inspiration from Natural Language Generation (NLG) benchmarks such as GEM [12], we present Struct-Bench: composite benchmark and an automatic evaluation protocol that measures the quality of structured, natural language-based synthetic data relative to their corresponding real (possibly private) datasets. In the Struct-Bench framework, user first selects one or more real datasets for which they want to evaluate corresponding synthetic dataset. For each dataset, the user provides set of production rules under the generative grammar formalism (i.e., context-free grammar [21]); the user then selects key nodes, which will be programmatically extracted from the parse tree of dataset samples; key nodes are used to measure important correlations and properties of the synthetic data. Based on this dataset representation, Struct-Bench measures an array of syntactic (i.e., structural) and semantic properties of the synthetic dataset. We illustrate the dataset level and sample level views into Struct-Bench in Fig. 1. Our contributions are as follows: 1. Benchmark: We propose Struct-Bench, novel evaluation framework and benchmark to evaluate synthetic data quality relative to real dataset, where the real dataset features complex inter-field structural relations, and at least some fields contain natural language. key observation is that many structural properties (and even semantic properties) can be modeled and evaluated with the help of context-free grammar. We also provide public leaderboard and reference, extensible implementation of the Struct-Bench framework, which can be used to benchmark new DP synthetic data generation methods on other datasets in standardized manner. 2. Findings: We use Struct-Bench to benchmark and analyze state-of-the-art (SOTA) DP synthetic data generation techniques on seven diverse datasets, including real-world textual and tabular datasets, as well as synthetic datasets with controllable data attributes. Our main findings are (1) no single metric fully describes synthetic data quality; (2) none of the existing SOTA DP synthetic data generation techniques are able to reliably capture the structural properties of data without sacrificing semantic performance. Finding (1) highlights the importance of using multiple metrics to evaluate synthetic data, which is key contribution of our work, and (2) underscores the need for further research in synthetic structured data generation. We also conduct case study to show how to algorithmically improve on SOTA DP synthetic data generation methods, Private Evolution (PE) [27, 60, 28], using the insights from Struct-Bench. These improvements achieve nearly 100% compliance with dataset structural constraints, while also improving semantic and statistic metrics."
        },
        {
            "title": "2 Struct-Bench Framework and Evaluation Protocol",
            "content": "We aim to design an evaluation framework that measures how closely real, private dataset matches synthetic dataset D. The real dataset features (1) an inherent structure, and (2) natural language fields. Our evaluation framework must therefore quantify how well has acquired the structure and content of the private dataset. As these are not well-defined quantities, we define framework for representing real dataset D, as well as suite of metrics to capture how well the synthetic dataset matches the syntax and semantics of the real dataset. with samples. As concrete running example, suppose is Notation Consider dataset = (Di)m i=1 the ShareGPT dataset [1], which contains multi-round conversations between human and an AI agent. We illustrate sample of ShareGPT in B.1. dataset is set of samples; in ShareGPT, each sample Di is one full conversation (e.g., few iterations of conversation between the human and the agent). Each sample (the number of nodes can vary across samples). In the ShareGPT Di contains set of nodes O1, O2, . . . , Oni example, each node Oj is one text snippeteither single query from the human or response from the AI agent, as illustrated in Fig. 1b. Samples and nodes can have attributes, which are either numeric or categorical. Each sample Di is associated with sample-level attributes a1, . . . , am. These can be any derived property of the whole samples, such as token length of the conversation in ShareGPT. node Oj can also have node-level attributes . In ShareGPT, attributes could include the token length of node, the identity of the speaker v1, . . . , vnj (agent or human), and the topic of the query or response (obtained through human or LLM-based labeling). Dataset Representation The nodes of dataset may satisfy complex structural relations. For example, in ShareGPT, each node can only have human or agent as its speaker attribute, and successive nodes should always alternate speaker between human and agent. This information is considered public (to the synthetic data holder); for example, if an enterprise is training DP synthetic dataset to model private dataset of search engine queries, the enterprise is likely to know the schema of the data, even if they do not know the contents. Such relations are captured in Struct-Bench by context-free grammar (CFG), which specifies different categories of nodes and the relations between them. As an example, we provide the CFG of ShareGPT in B.1. To add new dataset to Struct-Bench, user must write structural dependencies that should be enforced in the form of CFG.1 For each dataset (real or synthetic), Struct-Bench then uses the user-provided CFG to construct parse tree for each sample. Remark: An alternate design choice could be to specify each dataset under the formalism of contextsensitive grammars (CSGs), which are more expressive than CFGs and can capture semantic dependencies. Specifying CSG, however, requires significantly more domain knowledge and detail, making it more burdensomeand potentially error-proneprocess than specifying CFG. Hence, instead of encoding 1The CFG specification only needs to be performed manually once when the dataset is onboarded, and it may be possible to leverage LLMs to summarize the structure and generate the CFG automatically [53]. 3 semantic dependencies as hard constraints via CSG, we empirically assess them by introducing metric Key Node Dependency (KND) in 2.1. KND is simple to implement, requires less domain expertise than full CSG specification, and statistically captures correlations in the data. Once the CFG is defined, the user also specifies set of key nodes. For instance, if the CFG defines set of node types = {Query, Response, Follow-Up}, the set of key nodes can be any subset of S. Key nodes are expected to exhibit strong dependencies in the data. For example, in ShareGPT, we would define each query-response as pair of key nodes. We will assess whether the correlations among these key nodes are preserved in the synthetic data, as detailed later in 2.1. user can optionally not specify key nodes (in which case all nodes are treated as key nodes). In this work, our goal is to evaluate differentially private synthetic data generation Privacy Constraint algorithms. data generator is (ϵ, δ)-differentially-private if for any neighboring datasets D0 and D1 (i.e., D0 and D1 differ one sample), and any set range (M), we have (M (D0) S) eϵ (M (D1) S) + δ . In other words, the output synthetic data distribution should not depend too much on any single sample in the input dataset. Today, there exist many algorithms for generating DP synthetic data (some of which can accommodate text and/or structured data) [63, 60, 22, 49, 51]. Struct-Bench provides systematic way of comparing DP synthetic data generators. Struct-Bench can also be applied to other forms of privacy-preserving synthetic data, such as those generated under frameworks like quantitative information flow [46], statistical maximal leakage [31, 58], and distribution privacy [48], among others. To ensure fair comparison, all synthetic data generation baselines should be compared under the same privacy framework. Remark: Private DP data synthesis and non-private data synthesis are two problems with different applications and objectives. In the private setting, the goal is typically to match synthetic data to private dataset as closely as possible under DP constraint [51, 42, 29, 22, 60]. On the other hand, if there is no privacy constraint and if one wants to match the real data as closely as possible, one should just use the real data. Indeed, in the non-private setting, synthetic data is typically designed to deviate from the real dataset (e.g., conditional generation of specific class of data) [18, 35, 9, 10, 11, 30]. In our design and evaluation, we focus on DP synthetic data, and therefore Struct-Bench is designed to measure the similarity between the synthetic dataset and real dataset. However, the metrics in Struct-Bench could be helpful for benchmarking non-private synthetic data as well, and we leave it to future work. In summary, the Struct-Bench framework takes as input: (1) real dataset D, 2 (2) Summary of Inputs synthetic dataset D, (3) CFG that represents the structural characteristics of the data, (4) set of key nodes (optional) from the CFG, which represent the types of nodes whose correlations are important. Given these inputs, Struct-Bench automatically calculates the following suite of metrics."
        },
        {
            "title": "2.1 Struct-Bench Metrics",
            "content": "We report three types of metrics: structural, non-structural, and downstream task accuracy. Structural Metrics These are metrics that depend on the CFG in some way. Within sample, structure can be defined at the level of the whole sample (CFG Pass Rate, Attribute Match), groups of nodes (Key Node Dependency), and individual nodes (Attribute Match). (1) CFG Pass Rate (CFG-PR): This measures the fraction of samples in the synthetic dataset that parse correctly under the CFG. (2) Key Node Dependency (KND): This metric measures the semantic dependencies between key node pairs, which are pairs of nodes believed to have meaningful relation;3 for example, in question-and-answer dataset, we would expect that each associated question and answer pair should have strong correlation. Programmatically, users 2The input data should be in string format. For structured data with categorical or numerical values, we convert it to JSON object where the attribute of each column is the key, and the actual value/text is its value. 3We measure dependencies between node pairs and do not consider higher-order relationships, as they are more computationally intensive and the dependency functions can vary across different scenarios. 4 specify pairs of key nodes with Tregex [26], tool for matching regular expressions on trees. Typically, we can measure the dependencies by cosine similarity of the node embeddings, while one can also adopt LLM as judge or other dependency functions defined by the users. For dataset, we can construct distribution of dependencies of node pairs in the same pattern. To evaluate the similarity of the node dependencies captured by the private and synthetic dataset, we then calculate the distributional distance, e.g., Wasserstein-2 distance, between the private and synthetic dependency distributions. (3) Attribute Match (AM): This metric measures the distributional distance of sample-level attributes (e.g., number of nodes) or node-level attributes (e.g., node token length) between the private and synthetic datasets. The attribute can be statistical property or semantic property, and it can be derived either from an explicit attribute function or through human annotation or LLM-based labeling. The distributional distance can be Wasserstein-2 distance if the attribute is numeric or total variation distance if the attribute is categorical. The precise definitions and instantiation guidelines for KND and AM are provided in A. For our structural metrics, higher CFG-PR indicates better performance, while KND and AM are the opposite. Non-Structural Metrics Following prior precedent [51, 5], non-structural metrics are per-sample metrics that do not rely on the CFG, i.e. they are unrelated to the structure of the data. These metrics quantify the similarity between the content of the generated synthetic data and the real/private data. Moreoever, as in prior works on unstructured DP synthetic data [25, 60], we report the precision (KNN-Precision) and recall (KNN-Recall ) for each synthetic dataset. Roughly, KNN-Precision (resp. KNN-Recall) calculates the proportion of synthetic (resp. private) samples whose embedding distance to private (resp. synthetic) sample is smaller than this samples k-th nearest neighbor within its own dataset. KNN-Precision evaluates the average semantic quality of the synthetic samples, and KNN-Recall assesses the semantic diversity of the samples [25]. Downstream Evaluations (DE) Based on Label Prediction The eventual goal of synthetic data is typically downstream task, e.g., training machine learning (ML) model. Struct-Bench allows users to design their own downstream label prediction tasks. Our pipeline includes label generation, downstream model training, and evaluation. (1) Label Generation: The evaluation pipeline requires labels both for the synthetic and real data. Labels can either be generated by human or we can use large language models (LLMs) to simulate human labeler. In our evaluation, we adopted GPT-4o to label the samples, but the Struct-Bench codebase gives users flexibility to choose different LLM. We provide guidelines for the label generation process in A.3. (2) Downstream Model Training: To conduct label prediction, we fine-tune language model based on the synthetic dataset and its label. Since the samples may have long text, we adopt Longformer [7] in this paper. (3) Evaluation: We evaluate the downstream task by calculating the prediction accuracy (Acc) on held-out test set from the real data. This is commonly done in synthetic data evaluations, and is known as the train-synthetic-test-real (TSTR) framework [56, 39, 30]."
        },
        {
            "title": "3 Benchmarking Differentially Private Synthetic Data",
            "content": "To demonstrate the utility of the Struct-Bench framework, we instantiate and implement it on set of seven datasets and four DP synthetic data generation methods."
        },
        {
            "title": "3.1 Struct-Bench Datasets",
            "content": "We include three types of datasets in Struct-Bench (details in B.2). Real-World Datasets with Graph-Structured Dependencies We use ShareGPT [1] and ICLR 2024 paper reviews [2], which represent two typical real-world examples of graph-structured datasets with significant differences: (1) Content: ShareGPT consists of multi-round conversations between user and AI agents (GPT), whereas the ICLR 2024 paper review dataset contains reviews, rebuttals, and comments 4Some evaluation frameworks compare TSTR with train-real-test-real (TRTR) for self-contained evaluation [64]. However, since we are comparing different synthetic data generation algorithms against each other, we compute only TSTR in this case, which is slightly more interpretable. 5 Table 1: List of datasets with descriptions, key nodes, and sizes of real and generated data. Dataset Description Key Nodes Number of Samples Real Generated ShareGPT HumanGPT conversations ICLR Adult Water Arena Reviews Grounding ICLR paper reviews & rebuttals Census dataset Water-bottle reviews Chatbot-Arena conversations Synthetic product reviews Synthetic grounded QA query, response review, rebuttal, comment native country, workclass title, review conversation 1 & 2 review, rating query, response 3 000 3 000 50 000 25 000 25 000 2 000 2 500 600 300 31 561 20 000 20 000 2 000 2 000 of the papers submitted to ICLR 2024 between the authors and multiple reviewers; (2) Topic: ShareGPT conversations cover wide range of open-domain topics, while the ICLR 2024 paper review dataset is specific to AI research areas; (3) Structure: ShareGPT conversations follow linear structure each user query is followed by GPT response, continuing sequentially; while ICLR posts exhibit tree structure, in which paper receives multiple reviews, followed by corresponding rebuttals and subsequent discussions between authors and reviewers. Both datasets have diverse semantics and logic and clear data structures with at least two types of nodes. Notably, the ICLR 2024 paper review dataset was released after the training data cut-off date for the LLMs we evaluate. Real-World Tabular Datasets Although there exist benchmarks for tabular data [5, 51], they do not naturally extend to natural language fields. We evaluate Struct-Bench on three tabular datasets. Two of them (Water and Arena) contain textual fields while the third dataset (Adult) contains only numerical and categorical values. We include the Adult dataset to demonstrate that Struct-Bench can be applied to non-textual data as well, but note that this data type is not main focus of our work. The Water [52] dataset contains reviews of water bottles, the Arena [67] dataset contains pairs of human-model conversations, and the Adult [6] dataset contains census data. Synthetic Datasets with Controllable Data Attributes To control the complexity of the data structure and semantics more explicitly, we construct two synthetic datasets named Synthetic Reviews and the Synthetic Grounding Dataset. The Synthetic Reviews dataset contains reviews with varying sentiments and review scores, and the Synthetic Grounding dataset contains source documents and question-answering based on them. We show the key nodes of each dataset and the sizes of the real and generated data in Table 1, and defer the detailed data modeling to B.3."
        },
        {
            "title": "3.2 Struct-Bench Synthetic Data Generation Baselines",
            "content": "Since we focus on structured datasets that contain natural language, we select LLM-based DP synthetic data generators as our baselines, specifically, Private Evolution (PE) [27, 60, 28], DP model fine-tuning [63, 59, 65], and some variants. While several methods have been proposed to generate DP tabular data using LLMs [4, 55], they are typically limited to handling only numerical or categorical values and cannot generate structured data that incorporates natural language. Private Evolution (PE) [27, 60, 28] PE is leading training-free DP synthetic data generation algorithm that makes use of foundation models pre-trained on public data [27, 60, 28, 22, 16, 57]. PE first uses Random API to generate initial samples from the foundation language model. Then, it iteratively: (1) constructs differentially private (noisy) voting histogram based on private samples voting for their nearest synthetic counterparts; (2) draws samples according to this histogram; and (3) creates new samples with Variation API that generates perturbed versions of the original samples. We use the PE variant, Augmented Private Evolution (Aug-PE) [60], provided by the Private Evolution library;5 our only modification is to 5https://github.com/microsoft/DPSDA 6 choose the prompt for the Random and Variation APIs. IF uses foundation language models to generate samples based on prompt Instruction Following (IF) that outlines the required data structure, without incorporating any signals or data from the private dataset, and therefore ϵ = 0. This can be viewed as zero-shot version of PE, i.e., it is equivalent to using only the Random API in PE to generate samples. DP Fine-Tuning (DP-FT) [63, 59, 65] DP-FT adopts DP stochastic gradient descent (DP-SGD) [3] to fine-tune the language model on the next token prediction task. We generate synthetic data unconditionally from the fine-tuned model. This simple baseline remains competitive when training is allowed [16]. Since PE generates synthetic data based on the instructions in prompt in the Random and Variation APIs, we also include variant of DP-FT that conditionally generates samples according to the same instructions after DP fine-tuning, which we refer to as Instruct DP-FT. We provide details of this instruction fine-tuning in C. Real Data Fine-Tuning (FT) FT directly fine-tunes the language model without any privacy guarantees, and therefore ϵ = . This is best-case scenario for DP-FT. We also include Instruct FT, which utilizes instruction-guided conditional generation. Since DP-FT and FT require model training, we are limited to open-source models and thus use only GPT-2. In contrast, PE and IF do not require access to model weights, making them compatible with models available exclusively through APIs; therefore, we evaluate them on both GPT-2 and the state-of-the-art GPT-4o. We further evaluate our baselines on additional foundation models in C.3. PE is run for 10 iterations, and DP-FT and FT are run for 20 epochs. We vary the privacy budget ϵ as 1, 2, 4 for PE and DP-FT, and set δ = 0."
        },
        {
            "title": "3.3 Experimental Results",
            "content": "We present the results of benchmarking the DP synthetic data generation methods under ShareGPT and ICLR datasets with ϵ = 4 in Table 2, and defer the results on other datasets to C.1. We illustrate the performance of the baselines on CFG-PR and KNN-Recall across all datasets in radar plots shown in Fig. 2. We specify the metrics reported and the whole set of evaluation metrics we adopt in B.4; additional results for ϵ = 2 and ϵ = 1 the on the ShareGPT and ICLR datasets can be found in C.2. Table 2 and Fig. 2 highlight several main takeaways: No single metric fully describes synthetic data quality. For single algorithm and dataset, some metrics can be high, while others remain low (e.g., see CFG-PR and KNN-Recall in Fig. 2). This further motivates the need for Struct-Bench, which aggregates many diverse metrics. Existing DP synthetic data generators struggle to learn complicated data structures. All baselines achieve CFG-PR score below 0.2 on the ICLR dataset, which features more node types and significantly more intricate graph structure than ShareGPT. DP fine-tuning alone cannot learn structure. At ϵ = 4, it achieves CFG-PR of 0 on all of our datasets. Even at ϵ = , it fails to learn structural information on all datasets except ShareGPT, where it achieves CFG-PR of 0.53; this is likely because ShareGPT contains fewer formatting tokens compared to other datasets (e.g., JSON tags in tabular datasets). PE and IF learn structure at the expense of semantic performance. Although PE and IF reliably capture the data structure with SOTA models, they suffer from poor semantic performance (low KNN-Recall). The performance gap between PE and DP-FT may arise from the foundation models they employ and the use of instruction-guided generation. Unlike DP-FT, PE does not require model training, which allows us to leverage SOTA models in the Random and Variation APIs. In contrast, DP-FT relies on fine-tuning, and we thus need to use smaller, open-source models due to computational restrictions and the weight access requirement. Nevertheless, with instruction-guided conditional generation, Instruct DP-FT achieves performance comparable to PE across most metrics on both ShareGPT and ICLR, when both use the same foundation model (GPT-2). 7 Table 2: DP synthetic data generation benchmarking results on Struct-Bench with ϵ = 4. All baselines use GPT-2 unless otherwise specified. Structural Metrics Non-Structural Metrics DE Dataset Baseline CFG-PR KND AM KNN-Precision KNN-Recall Acc 0.28 41.86 0.64 0.31 ShareGPT IF (ϵ = 0) IF (ϵ = 0) (GPT-4o) FT (ϵ = ) Instruct FT (ϵ = ) DP-FT Instruct DP-FT PE PE (GPT-4o) IF (ϵ = 0) IF (ϵ = 0) (GPT-4o) FT (ϵ = ) Instruct FT (ϵ = ) DP-FT Instruct DP-FT PE PE (GPT-4o) ICLR 0.03 0.87 0.53 0.59 0 0.55 0.57 0. 0.09 0.17 0 0.09 0 0.08 0.10 0.19 0. 0.06 0.03 0.04 - 0.18 0.12 0.07 0.11 0. - 43.85 52.70 30.70 - 32.59 34.51 38.17 207. 204.80 - 0.16 208.37 - 0.22 0.20 0.26 - 237.52 248.73 240. 0.72 0.76 0.80 0.02 0.77 0.78 0.81 0.66 0. 0.71 0.77 0 0.49 0.49 0.98 0.26 0.66 0. 0 0.31 0.33 0.15 0.28 0.03 0.47 0.29 0 0.18 0.20 0. 0.38 0.37 0.37 - 0.35 0.38 0.39 0.39 0. 0.46 0.51 0.18 0.40 0.41 0.52 (a) CFG-PR (b) KNN-Recall Figure 2: CFG-PR and KNN-Recall of baselines on Struct-Bench with different datasets. While frontier models can capture the syntactic structure of many of our datasets (i.e., CFG-PR is high), existing DP synthetic data generation techniques do not capture the semantic diversities (KNN-Recall). With GPT-4o, both IF and PE achieve perfect CFG-PR on tabular and synthetic datasets, while FT with GPT-2 fails to learn the correct data format. However, KNN-Recall of all baselines are near 0, indicating poor semantic performance on tabular and synthetic datasets. We exclude DP-FT as it attains score of zero on both metrics for all datasets. Note that Instruct DP-FT and Instruct FT may achieve stronger results than the DP-FT and FT variants shown here, as reported in Table 2."
        },
        {
            "title": "4 Case Studies",
            "content": "In this section, we demonstrate how users can leverage Struct-Bench to better understand and improve PE. We focus on PE because, unlike DP-FT, it does not require training on private data, offering both efficiency and qualitative advantages [60, 22]. We use the the ShareGPT dataset for this case study as it is real-world dataset that is semantically diverse and has larger token length for each node relative to our tabular or synthetic datasets. While GPT-4o might achieve stronger performance, we adopt Llama3-8b as the foundation model in this section since it is more cost-efficient for our experiments and, importantly, more affordable and accessible to end users. We mainly focus on improving structural validity and semantic diversity of the PE synthetic data in this section, and defer more thorough analysis as well as methods on improving node dependency (KND) to D. Problem 1: Structural Validity (CFG-PR) is low. Structural validity, i.e., CFG-PR, is critical metric, as many downstream applications on structured datasets expect data to be formatted in particular way for compatibility with utilities and dataset-specific pipelines. Fig. 3 shows that the CFG-PR of vanilla PE is below 60% when ϵ 4, and only achieves 63% in the non-private setting (i.e., we run PE with no added noise). This suggests that with smaller foundation models (e.g., Llama38B), vanilla PE inherently cannot capture even simple structural constraints. Solution 1: LLM-Assisted reformatting can improve CFG compliance. To improve structural validity, we introduce reformatting feature to the Random and Variation APIs by prompting LLMs to explicitly check and reformat CFG-invalid samples (see D.2). For example, if PE generates HUMAN: How are you?, the model detects the missing response and reformats it to HUMAN: How are you? GPT: Im fine. Figure 3: CFG-PR of vanilla PE on ShareGPT. Each data point is averaged over three independent trials. CFG-PR is low for all ϵ. As described in 3.2, PE generates set of samples and then select those with high qualities by private voting in each iteration. Sample reformatting can happen before or after the PE private voting process. We compare both reformatting methods with vanilla PE at ϵ = 4 in Fig. 4. We see that the CFG-PR of both our methods increases by over 20% compared with vanilla PE, and reformatting after the PE private voting procedure does the best. Reformatting can help enforce structural correctness but may come at the cost of semantic integrity. For example, as illustrated in Fig. 5, if PE generates How are you? Im fine. Thanks., the model may detect missing format tokens and reformat it to HUMAN: How are you? Im fine. GPT: Thanks! While the reformatted version follows valid structure, its semantics are flawedthe users query includes part of the response. As result, in the reformatting-before-voting setting, the voting process may become biased against structurally valid but semantically distorted samples. In contrast, reformatting-after-voting directly reformats only the most highly-voted samples, which are either used as final output (at the last iteration of PE) or utilized as seeds in the next PE iteration without further selection, resulting in higher CFG-PR. Figure 4: CFG-PR of vanilla PE and PE with CFG reformat under ϵ = 4. Problem 2: Semantic diversity (KNN-Recall) is low. As described in 2.1, KNN-Precision measures the semantic quality of the generated samples, and the KNN-Recall measures how well the se9 Figure 5: Illustration of reformatting-before-voting on the ShareGPT dataset. The syntactically-correctly reformatted sample follows valid structure but has flawed semanticsthe users query includes part of the response. In contrast, the incorrectly reformatted sample preserves semantic integrity. In these cases, the voting process can become biased toward samples that are semantically consistent but structurally invalid. mantic diversity of the private dataset is captured in the synthetic data. As we see in Fig. 6, as ϵ increases, i.e., with looser privacy constraints, the KNN-Precision of vanilla PE increases from 0.56 to 0.69, increases from 0.56 to 0.69, while KNN-Recall remains very low, around 0.35. This suggests that vanilla PE focuses on semantic quality while sacrificing diversity. Solution 2: Node extraction & auto-generation can In the Variation API, PE improve semantic diversity. generates new samples by first masking subset of the original text and then using the LLM to fill in the blanks based on the remaining context. This process largely preserves the original meanings, which limits semantic diversity. For example, if conversation is about weather, and its masked version retains keywords like cloudy or rainy, the blank-filled new sample will likely still be about weather, rather than an unrelated topic like dogs. To improve semantic diversity, we propose variant of PE that extracts specific nodes for blank-filling and then allows the language model to auto-generate the remaining nodes with fewer semantic constraints in the Variation API. We call this pipeline Node extraction and auto-generation\", and illustrate an example on the ShareGPT dataset in Fig. 7. The user must specify which nodes are to be extractedfor instance, these could be roots in the parse tree of the CFG. The remaining nodes are auto-generated by feeding the extracted nodes (with blank-filled variations generated) into an LLM and asking it to generate the remaining nodes in the sample. Due to the post-processing property of DP, this pipeline does not incur any additional privacy cost. Figure 6: KNN-Precision and KNN-Recall of vanilla PE on ShareGPT under different privacy guarantees. Both are low. We compare the performance of vanilla PE and PE with node extraction in Fig. 8. Recall that the ShareGPT dataset has only two types of nodes: query and response nodes. In Fig. 8, we consider two variants of the node extraction method: (1) extract all queries and auto-generate all responses (listed as Extract Query), and (2) extract all responses and auto-generate all queries (shown as Extract Response). We show three metrics: KNN-Recall, KNN-Precision, and CFG-PR. Fig. 8 shows that Extract Query not only improves the KNN-Recall but also the KNN-Precision, (i.e., semantic quality) as auto-generation also ensures the semantic meaning is more consistent and natural across nodes. However, Extract Response does not improve KNN-Recall as the semantic diversity depends mainly on responses, while queries are fairly constrained for given response. This indicates that the type of nodes extracted is crucial to the performance of semantic diversity. Additionally, since the formatting tokens around the extracted critical nodes will not be accidentally modified by Variation API, the CFG-PR of PE with node extraction is also higher than vanilla PE. 10 Figure 7: Example executions of the Variation API of vanilla PE and PE with node extraction and autogeneration on the ShareGPT dataset. Vanilla PE generates new samples by first masking subset of the original text and then using the LLM to fill in the blanks based on the remaining context. In contrast, PE with query extraction and auto-generation first extracts node(s) from the conversation; in this example, the extracted node is the Query\". It then conducts blank-filling only on the extracted Query node. The language model then generates responses conditioned on the query. This produces fewer semantic constraints, allowing this variant of PE to generate more semantically-diverse samples. Figure 8: Performance of vanilla PE and PE with node extraction on KNN-Precision & KNN-Recall and CFG-PR. 11 Combination of our solutions achieves the best performance on most metrics. We finally compare the performance of our proposed methods according to Struct-Bench. Specifically, in Fig. 9, we illustrate the performance of vanilla PE, PE + Reformat, PE + Extract Query, and PE + Reformat + Extract Query. To better visualize differences in the performance of different methods, we scale the metrics in these radar plots as follows: We assign score of 0 if CFG-PR=0 or structure-related metric is not applicable for the dataset, and rescale the values of other metrics from 20 to 100, where 20 indicates the worst performance among all methods, and 100 indicates the performance upper bound the synthetic data can achieve (e.g., CFG-PR=1 or AM=0). We observe that the combination of our proposed methods (orange curve) significantly improves in CFG-PR relative to the other methods, achieving up to 94%, and it also outperforms or performs comparably to other methods in most of the metrics, including semantic metrics such as KNN-Precision and KNN-Recall, and statistic metrics such as AM on response token length and number of nodes"
        },
        {
            "title": "5 Related Work",
            "content": "Figure 9: Performance of Different Methods on ShareGPT with ϵ = 4. Our benchmark is tied to the key problems of synthetic data generation and its evaluation, under the constraints of differential privacy. Differentially Private (DP) Synthetic Data Generation DP synthetic data generation for arbitrary tasks has developed as an effective tool in the machine learning model development pipeline [66, 42, 27, 60, 28, 55], especially with the advent of instruction-following LLMs generating natural and fluent text [4, 60, 38, 17, 50]. Prior methods, such as Private Evolution (PE) [27, 60, 28, 22, 68, 23, 57], leverage pretrained models (e.g., large language models) or non-neural approaches (e.g., computer graphics tools) for the task of DP synthetic data generation. [27, 60, 28] show that PE could be competitive with DP fine-tuning baselines [63, 59, 65, 13] while does not require training on the private data. DP Synthetic Data Evaluation Evaluating DP synthetic data presents unique challenge, namely of quantifying adherence of the synthetic data to arbitrary private datasets [51, 44]. Several benchmarks have been proposed to evaluate DP synthetic data in image [16, 24, 32, 61], text [40, 45, 64], tabular [42, 14, 5, 37, 34, 33], time series [47], and graph data [15]. However, these benchmarks either do not explicitly consider data structure, or their evaluation is confined to numerical or categorical data types, thus limiting their scope. Fundamentally, DP synthetic data generation methods need to solve for both structure and content acquisition through noisy signals; unified evaluation framework can spur further research in the field by providing fine-grained signals on algorithmic progress."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we proposed new benchmark for DP synthetic data generation named Struct-Bench. To the best of our knowledge, Struct-Bench is the first benchmark to comprehensively evaluate DP synthetic data derived from structured datasets that contain natural language data. Struct-Bench also has the strength of being composite benchmark, wherein diverse collection of datasets might preclude algorithmic research to overfit to only few data types. Through our evaluations, we also characterize the limitations of existing 12 SOTA DP synthetic data generation methods and conduct case study to show how to improve on SOTA methods using the insights from Struct-Bench."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors would like to thank Sivakanth Gopi for his helpful suggestions. This work was supported in part by the National Science Foundation under grants CCF-2338772 and CNS-2148359."
        },
        {
            "title": "References",
            "content": "[1] ShareGPT_Vicuna_unfiltered Dataset. Hugging Face Datasets https:// huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/ bcd32a724d8460ebe14e1d05b0195e30e9a46cb1, apr 2023. [2] The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [3] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308318, 2016. [4] Tejumade Afonja, Hui-Po Wang, Raouf Kerkouche, and Mario Fritz. Dp-2stage: Adapting language models as differentially private tabular data generators. arXiv preprint arXiv:2412.02467, 2024. [5] Christian Arnold and Marcel Neunhoeffer. Really useful synthetic dataa framework to evaluate the quality of differentially private synthetic data. arXiv preprint arXiv:2004.07740, 2020. [6] Barry Becker and Ronny Kohavi. Adult https://doi.org/10.24432/C5XW20. UCI Machine Learning Repository, 1996. [7] Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [8] Vikram Chundawat, Ayush Tarun, Murari Mandal, Mukund Lahoti, and Pratik Narang. universal metric for robust evaluation of synthetic tabular data. IEEE Transactions on Artificial Intelligence, 5(1):300309, 2022. [9] Hari Prasanna Das, Ryan Tran, Japjot Singh, Xiangyu Yue, Geoffrey Tison, Alberto SangiovanniVincentelli, and Costas Spanos. Conditional synthetic data generation for robust machine learning applications with limited pandemic data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 1179211800, 2022. [10] Georgios Douzas and Fernando Bacao. Effective data generation for imbalanced learning using conditional generative adversarial networks. Expert Systems with applications, 91:464471, 2018. [11] Justin Engelmann and Stefan Lessmann. Conditional wasserstein gan-based oversampling of tabular data for imbalanced learning. arXiv preprint arXiv:2008.09202, 2020. [12] Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillanMajor, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, 13 Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. The GEM benchmark: Natural language generation, its evaluation and metrics. In Antoine Bosselut, Esin Durmus, Varun Prashant Gangal, Sebastian Gehrmann, Yacine Jernite, Laura Perez-Beltrachini, Samira Shaikh, and Wei Xu, editors, Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96120, Online, August 2021. Association for Computational Linguistics. [13] Sahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira Ktena, Robert Stanforth, Jamie Hayes, Soham De, Samuel Smith, Olivia Wiles, and Borja Balle. Differentially private diffusion models generate useful synthetic images. arXiv preprint arXiv:2302.13861, 2023. [14] Matteo Giomi, Franziska Boenisch, Christoph Wehmeyer, and Borbála Tasnádi. unified framework for quantifying privacy risk in synthetic data. arXiv preprint arXiv:2211.10459, 2022. [15] Alexander Goldberg, Giulia Fanti, Nihar Shah, and Steven Wu. Benchmarking fraud detectors on private graph data. KDD, 2025. [16] Chen Gong, Kecen Li, Zinan Lin, and Tianhao Wang. Dpimagebench: unified benchmark for differentially private image synthesis. arXiv preprint arXiv:2503.14681, 2025. [17] Mandeep Goyal and Qusay Mahmoud. An llm-based framework for synthetic data generation. In 2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC), pages 0034000346. IEEE, 2025. [18] Begüm Hattatoğlu, Abdulhakim Qahtan, Heysem Kaya, and Yannis Velegrakis. Synthfair: Ensuring subgroup fairness in classification via synthetic data generation. In World Congress in Computer Science, Computer Engineering & Applied Computing, pages 347363. Springer, 2024. [19] Mikel Hernadez, Gorka Epelde, Ane Alberdi, Rodrigo Cilla, and Debbie Rankin. Synthetic tabular data evaluation in the health domain covering resemblance, utility, and privacy dimensions. Methods of information in medicine, 62(S 01):e19e38, 2023. [20] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by two time-scale update rule converge to local nash equilibrium. In Advances in neural information processing systems, volume 30, 2017. [21] John Hopcroft, Rajeev Motwani, and Jeffrey Ullman. Introduction to automata theory, languages, and computation. Acm Sigact News, 32(1):6065, 2001. [22] Charlie Hou, Akshat Shrivastava, Hongyuan Zhan, Rylan Conway, Trang Le, Adithya Sagar, Giulia Fanti, and Daniel Lazar. Pre-text: training language models on private federated data in the age of llms. In Proceedings of the 41st International Conference on Machine Learning, pages 1904319061, 2024. [23] Charlie Hou, Mei-Yu Wang, Yige Zhu, Daniel Lazar, and Giulia Fanti. Private federated learning using preference-optimized synthetic data. arXiv preprint arXiv:2504.16438, 2025. [24] Yuzheng Hu, Fan Wu, Qinbin Li, Yunhui Long, Gonzalo Munilla Garrido, Chang Ge, Bolin Ding, David Forsyth, Bo Li, and Dawn Song. Sok: Privacy-preserving data synthesis. In 2024 IEEE Symposium on Security and Privacy (SP), pages 46964713. IEEE, 2024. [25] T. Kynkäänniemi, T. Karras, S. Laine, J. Lehtinen, and T. Aila. Improved precision and recall metric for assessing generative models. In Advances in Neural Information Processing Systems, volume 32, 2019. [26] Roger Levy and Galen Andrew. Tregex and tsurgeon: Tools for querying and manipulating tree data structures. In LREC, pages 22312234. Genoa, 2006. [27] Z. Lin, S. Gopi, J. Kulkarni, H. Nori, and S. Yekhanin. Differentially private synthetic data via foundation model APIs 1: Images. In International Conference on Learning Representations (ICLR), 2024. 14 [28] Zinan Lin, Tadas Baltrusaitis, Wenyu Wang, and Sergey Yekhanin. Differentially private synthetic data via apis 3: Using simulators instead of foundation model. arXiv preprint arXiv:2502.05505, 2025. [29] Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Harsha Nori, and Sergey Yekhanin. Differentially private synthetic data via foundation model apis 1: Images. In ICLR, 2024. [30] Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar. Using gans for sharing networked time series data: Challenges, initial promise, and open questions. In Proceedings of the ACM internet measurement conference, pages 464483, 2020. [31] Zinan Lin, Shuaiqi Wang, Vyas Sekar, and Giulia Fanti. Summary statistic privacy in data sharing. IEEE Journal on Selected Areas in Information Theory, 5:369384, 2024. [32] Yintong Liu, Rajendra Acharya, and Jen Hong Tan. Preserving privacy in healthcare: systematic review of deep learning approaches for synthetic data generation. Computer Methods and Programs in Biomedicine, page 108571, 2024. [33] Ioannis Livieris, Nikos Alimpertis, George Domalis, and Dimitris Tsakalidis. An evaluation framework In IFIP International Conference on Artificial Intelligence for synthetic data generation models. Applications and Innovations, pages 320335. Springer, 2024. [34] Yunbo Long, Liming Xu, and Alexandra Brintrup. Evaluating inter-column logical relationships in synthetic tabular data generation. arXiv preprint arXiv:2502.04055, 2025. [35] Maria Antonietta Longo. Synthetic Data Generation Approach for Subgroup-Based Bias Mitigation in Structured Data. PhD thesis, Politecnico di Torino, 2025. [36] Ryan McKenna, Brett Mullins, Daniel Sheldon, and Gerome Miklau. Aim: An adaptive and iterative mechanism for differentially private synthetic data. arXiv preprint arXiv:2201.12677, 2022. [37] Parisa Movahedi, Valtteri Nieminen, Ileana Montoya Perez, Hiba Daafane, Dishant Sukhwal, Tapio Pahikkala, and Antti Airola. Benchmarking evaluation protocols for classifiers trained on differentially private synthetic data. IEEE Access, 2024. [38] Md Mahadi Hasan Nahid and Sadid Bin Hasan. Safesynthdp: Leveraging large language models for privacy-preserving synthetic data generation using differential privacy. arXiv preprint arXiv:2412.20641, 2024. [39] Zhaozhi Qian, Thomas Callender, Bogdan Cebere, Sam Janes, Neal Navani, and Mihaela van der Schaar. Synthetic data for privacy-preserving clinical risk prediction. Scientific Reports, 14(1):25676, 2024. [40] Krithika Ramesh, Nupoor Gandhi, Pulkit Madaan, Lisa Bauer, Charith Peris, and Anjalie Field. Evaluating differentially private synthetic data generation in high-stakes domains. arXiv preprint arXiv:2410.08327, 2024. [41] Brian Richards. Type/token ratios: What do they really tell us? Journal of child language, 14(2):201209, 1987. [42] Lucas Rosenblatt, Xiaoyan Liu, Samira Pouyanfar, Eduardo de Leon, Anuj Desai, and Joshua Allen. Differentially private synthetic data: Applied evaluations and enhancements. arXiv preprint arXiv:2011.05537, 2020. [43] Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing generative models via precision and recall. Advances in neural information processing systems, 31, 2018. [44] Viktor Schlegel, Anil Bharath, Zilong Zhao, and Kevin Yee. Generating synthetic data with formal privacy guarantees: State of the art and the road ahead. arXiv preprint arXiv:2503.20846, 2025. 15 [45] Viktor Schlegel, Yuping Wu, Warren Del-Pinto, Goran Nenadic, and Anil Anthony Bharath. Ai for data science: benchmark for differentially private text dataset generators. In AI4X 2025 International Conference. [46] Geoffrey Smith. On the foundations of quantitative information flow. In International Conference on Foundations of Software Science and Computational Structures, pages 288302. Springer, 2009. [47] Michael Stenger, Robert Leppich, Ian Foster, Samuel Kounev, and André Bauer. Evaluation is key: survey on evaluation measures for synthetic time series. Journal of Big Data, 11(1):66, 2024. [48] Anshuman Suri and David Evans. Formalizing and estimating distribution inference risks. arXiv preprint arXiv:2109.06024, 2021. [49] Bowen Tan, Zheng Xu, Eric Xing, Zhiting Hu, and Shanshan Wu. Synthesizing privacy-preserving text data via finetuning without finetuning billion-scale llms. arXiv preprint arXiv:2503.12347, 2025. [50] Xinyu Tang, Richard Shin, Huseyin Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, and Robert Sim. Privacy-preserving in-context learning with differentially private few-shot generation. arXiv preprint arXiv:2309.11765, 2023. [51] Yuchao Tao, Ryan McKenna, Michael Hay, Ashwin Machanavajjhala, and Gerome Miklau. Benchmarking differentially private synthetic data generation algorithms. arXiv preprint arXiv:2112.09238, 2021. [52] Tharunmss. Water Bottle Dataset - Flipkart https://www.kaggle.com/datasets/tharunmss/ water-bottle-dataset-flipkart. Kaggle, 2024. [53] Mohammad Jalili Torkamani. Kajal: Extracting grammar of source code using large language models. arXiv preprint arXiv:2412.08842, 2024. [54] Reihaneh Torkzadehmahani, Peter Kairouz, and Benedict Paten. Dp-cgan: Differentially private synthetic data and label generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 00, 2019. [55] Toan Tran and Li Xiong. Differentially private tabular data synthesis using large language models. arXiv preprint arXiv:2406.01457, 2024. [56] Boris Van Breugel, Zhaozhi Qian, and Mihaela Van Der Schaar. Synthetic data, real errors: how (not) to publish and use synthetic data. In International Conference on Machine Learning, pages 3479334808. PMLR, 2023. [57] Haoxiang Wang, Zinan Lin, Da Yu, and Huishuai Zhang. Synthesize privacy-preserving high-resolution images via private textual intermediaries. arXiv preprint arXiv:2506.07555, 2025. [58] Shuaiqi Wang, Zinan Lin, and Giulia Fanti. Statistic maximal leakage. In 2024 IEEE International Symposium on Information Theory (ISIT), pages 27422747. IEEE, 2024. [59] Lukas Wutschitz, Huseyin Inan, and Andre Manoel. dp-transformers: Training transformer models with differential privacy, 2022. [60] Chulin Xie, Zinan Lin, Arturs Backurs, Sivakanth Gopi, Da Yu, Huseyin Inan, Harsha Nori, Haotian Jiang, Huishuai Zhang, Yin Tat Lee, et al. Differentially private synthetic data via foundation model apis 2: Text. In International Conference on Machine Learning, pages 5453154560. PMLR, 2024. [61] Andrew Yale, Saloni Dash, Ritik Dutta, Isabelle Guyon, Adrien Pavao, and Kristin Bennett. Generation and evaluation of privacy preserving synthetic health data. Neurocomputing, 416:244255, 2020. [62] Scott Cheng-Hsin Yang, Baxter Eaves, Michael Schmidt, Ken Swanson, and Patrick Shafto. Structured evaluation of synthetic tabular data. arXiv preprint arXiv:2403.10424, 2024. 16 [63] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500, 2021. [64] Yefeng Yuan, Yuhong Liu, and Liang Cheng. multi-faceted evaluation framework for assessing synthetic data generated by large language models. arXiv preprint arXiv:2404.14445, 2024. [65] X. Yue, H. A. Inan, X. Li, G. Kumar, J. McAnallen, H. Sun, D. Levitan, and R. Sim. Synthetic text generation with differential privacy: simple and practical recipe. In ACL, 2023. [66] Zhikun Zhang, Tianhao Wang, Ninghui Li, Jean Honorio, Michael Backes, Shibo He, Jiming Chen, and Yang Zhang. {PrivSyn}: Differentially private data synthesis. In 30th USENIX Security Symposium (USENIX Security 21), pages 929946, 2021. [67] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging LLM-as-a-judge with MT-bench and chatbot arena, 2023. [68] Tianyuan Zou, Yang Liu, Peng Li, Yufei Xiong, Jianqing Zhang, Jingjing Liu, Xiaozhou Ye, Ye Ouyang, and Ya-Qin Zhang. Contrastive private data synthesis via weighted multi-plm fusion. arXiv preprint arXiv:2502.00245, 2025."
        },
        {
            "title": "A Metric Definitions and Instantiation Guidelines",
            "content": "A.1 Key Node Dependency (KND) Definition KND measures the distributional distance of node pair dependencies between the synthetic and original data. For key node pair (Oi, Oj), let Ci,j be the cosine similarity between their embeddings, and let ωCi,j be the distributions of these similarities in the original and synthetic data, respectively. and ω Then, KND is defined as: Ci,j where Dis is the Wasserstein-2 distance. KND(Oi, Oj) = Dis(ωCi,j , ω Ci,j ), Instantiation Guideline We allow the user to specify key nodes. If not specified, all nodes parsed by CFG are treated as key nodes by default. To instantiate key nodes, we recommend users ask the question Which nodes are central to our downstream tasks, and which nodes are semantically related to them?. For example, key node pairs could be query and response in conversation dataset, or review and its rating in product review dataset. Weve specified the key nodes of our datasets in Table 1. A.2 Attribute Match (AM) Definition AM calculates the distributional distance of given attribute between the synthetic and original data. For attribute a, let ωa and ω denote its distributions in the original and synthetic data, respectively. Then, AM is defined as: AM(a) = Dis(ωa, ω For distributional distance Dis, we use Wasserstein-2 distance for numeric attributes and total variation distance for categorical attributes. a). Instantiation Guideline Users can specify semantic or statistical attributes. guiding question is: Which data properties matter for our downstream tasks? Common semantic attributes include topic, intent, and sentiment; statistical attributes include token length (overall or per node). Original categorical/numerical values are also often relevant. The selected attributes for our datasets are detailed in B.4. A.3 Downstream Evaluations (DE) Label Generation Guideline To instantiate labels for downstream evaluation, we recommend that users first determine whether the label should be extracted directly from the sample or generated. Extracted labels typically include original categorical or numerical valuessuch as income level in our Adult dataset or paper decision in the ICLR dataset. If the label is to be generated, we suggest considering the question: What is data property that is present and can be inferred from each node in sample and is relevant to our downstream tasks? Generated labels are usually semantic attributes of the samples or nodes, such as topic, intent, or sentiment. To discourage prompt gaming by data uploaders, we provide suggested prompt template for labeling: For each sample in ____ scenarios, label the ____ property. Possible answers include {____, ____, ____, ...}. The specific labels used for our datasets are detailed in B.4."
        },
        {
            "title": "B Data Modeling and Evaluation Items of Each Dataset",
            "content": "B.1 Examples on ShareGPT To support the running example of data modeling and representation discussed in 2, we first show sample of the ShareGPT dataset, the data modeling of it, and the context-free grammar (CFG) according to its data structure. sample of ShareGPT 1 1 3 4 5 6 1 3 4 5 6 7 9 10 11 12 HUMAN : Pretend you are successful and very thorough real estate investor and financial analyst GPT : Sure , can help you with that . As successful real estate investor and financial analyst , have deep understanding of the real estate market and the financial factors that drive its performance . Here are some key things that would keep in mind when analyzing investment opportunities :... HUMAN : Assume the debt down payment is 50% GPT : Sure , can adjust the calculations to assume 50% HUMAN : What is the IRR from the perspective of the equity GPT : To calculate the IRR from the perspective of the equity , we need to adjust the cash flows to reflect the equity portion of the profits . Here is how we can calculate the equity IRR :... Illustration of the data modeling of ShareGPT We illustrate the data modeling of ShareGPT in Fig. 10. Figure 10: Illustration of the data modeling of ShareGPT. CFG of ShareGPT ShareGPT : conversation ( conversation ) * // ShareGPT contains one or more conversation rounds conversation : query response // Each conversation round contains query and response query : \" HUMAN : \" query_text // The query starts with \" HUMAN : \" response : \" GPT : \" response_text // The response starts with \" GPT : \" query_text : /(? ) .+?(?=(?: GPT : $ ) ) / // The query text ends before \" GPT : \" or the end of the string response_text : /(? ) .+?(?=(?: HUMAN : $ ) ) / // The response text ends before \" HUMAN : \" or the end of the string B.2 Dataset Descriptions ShareGPT [1] The ShareGPT dataset contains multi-round conversations between users and GPT. We structure each conversation such that each users query starts with HUMAN: and each GPTs response starts with GPT: . The downstream task we conduct is to predict the users intent and conversation topic based on user queries. ICLR [2] The ICLR dataset contains the reviews, author rebuttals, follow-up discussions, and final decisions of the papers submitted to ICLR 2024 [2]. Each review or reviewers comment starts with Reviewer where represents the reviewers identity, and each author rebuttal or discussion starts with Response. The downstream task is to predict the research area of the paper based on the review and rebuttals. Water [52] The Water dataset contains reviews of water bottles. The columns are product_name, overall_rating, title, cleaned_review and the goal is to predict the current rating (column \"rating\") of the bottle, which takes values 1, 2, 3, 4, 5. Arena [67] The Arena dataset contains pairs of human-model conversations. The columns are conversation_a, conversation_b and the goal is to predict which of the conversations are better (column \"winner\"), which takes values model_a, model_b, tie, \"tie (bothbad)\". Adult [6] The Adult dataset contains census data. The columns are age, workclass, fnlwgt, education, education-num, marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week, native-country and the goal is to predict income (column \"income\"), which takes values <= 50k or > 50k. Synthetic Datasets with Controllable Data Attributes We include two synthetic datasets6 named Synthetic Reviews and the Synthetic Grounding Dataset. The reviews dataset has 4 fields, namely text, sentiment, emotion, and rating. The grounding dataset has 4 fields including two source documents, query, and response. We generate these datasets through multi-step synthetic data generation process with GPT-4o wherein we verify whether the fields satisfy certain conditions, e.g., the reviews dataset is 1:1 split of extreme negative and extreme positive reviews about products and the grounding dataset is 1:1:1:1 split of relevant/irrelevant queries and consistent/inconsistent source documents. In particular, the reviews dataset is composed on only extreme reviews, either very positive or very negative. This differs from typical review distribution and is unique to this particular dataset. Similarly, for the grounding dataset, we vary the samples along two axes, first on the consistency of the information between the sources and second, on the relevancy of the query to the sources. Each of the synthetic datasets is balanced in both their training and (downstream) test sets on these variations. B.3 Data Modeling of Each Dataset Tables 3 and 4 shows the data modeling and structure rules of each dataset. B.4 Evaluation Metrics of Each Dataset The evaluation items of each dataset are summarized in Table 5. For ShareGPT, in our experimental results, we show the semantic similarity of the node pair (query, response) as KND, show the distributional distance of the queries token lengths as AM, and present the prediction accuracy of the conversation topics in downstream task performance. For ICLR, we show the semantic similarity of the node pair (review, rebuttal) as KND, show the distributional distance of the reviews token lengths as AM, and present the prediction accuracy of the papers research area in downstream task performance. Additional Results on Struct-Bench Resource Costs All baselines are implemented and performed on server with eight H100 GPUs. Running experiments took approximately 400 GPU hours. 6https://www.kaggle.com/datasets/structpedataset/structpe-synthetic-datasets 3 Table 3: Data modeling of each dataset"
        },
        {
            "title": "Node O",
            "content": "ShareGPT conversation ICLR Water Arena reviews & rebuttals of paper water bottle review 2 conversations to compare Adult census information of an adult Reviews annotated product review Grounding 2 sources and QA pair a1: number of nodes a2: token length a1: number of nodes a2: token length a3: topic a4: final decision a1: number of nodes a2: attitude a1: number of nodes a2: winner a1: age a2: workclass a1: number of nodes a2: rating a1: number of nodes a2: answer query/response post from the reviewer/author column of the tabular data column of the tabular data column of the tabular data review text grounded response Node Attributes v1: token length v2: topic v3: intent v4: speaker v1: token length v2: writer v3: review score v1: token length v2: review score v1: token length v2: winner v1: token length v2: income v1: token length v2: rating v1: token length v2: answer Dataset ShareGPT ICLR Water Arena Adult Reviews Grounding Table 4: Structure rules of each dataset Rules [Alternate Speakers] Oi, Oi+1 : Oi[Speaker] = Oi+1[Speaker]. [Format] O[Speaker] {User, AI Agent}. If O[Speaker] = User, the text starts with HUMAN: ; If O[Speaker] = AI Agent, the text starts with GPT: . [Format] O[Writer] {Author, Reviewer 1-9, Meta Reviewer}. If O[Writer] = Author, the text starts with Response:; if O[Writer] = Reviewer n, the text starts with Reviewer n: (1 9). [Format] O[Review Score] {1, 3, 5, 6, 8, 10}. [Format] D[Final Decision] {Reject, Accept:poster, Accept:top5%, Accept:top25%, }. [Format]O[Overall_rating] {1.0, 1.1, 1.2, ..., 4.9, 5.0}. [Format]O[Rating] {1, 2, 3, 4, 5} [Format]O[Winner] {model_a, model_b, tie, tie (bothbad)}. O[Conversation_a] starts with \"Question:\" and has \"Answer:\" before somewhere in the following text. O[Conversation_b] starts with \"Question:\" and has \"Answer:\" before somewhere in the following text. [Format]O[income] {<= 50k, > 50k}. [Format] Some of the columns are categorical (e.g. workclass, native-country). [Format] Some of the columns are numerical (e.g. age, capital-gain). [Format]O[Rating] {1, 2, 3, 4, 5} [Format]O[Consistency] {1, 2, 3, 4, 5}. [Format]O[relevancy] {1, 2, 3, 4, 5} 4 Table 5: Metrics for Different Datasets."
        },
        {
            "title": "Dataset",
            "content": "ShareGPT ICLR Arena Water Adult Reviews Grounding Structural Metrics CFG-PR KND 1. (query, response) pair 2. (response, query) pair AM 1. number of nodes 2.query token length 3. response token length 4. topic 5. intent CFG-PR KND 1. (review, rebuttal) pair 2. (rebuttal, comment) pair 3. (review, review) pair from different reviewers AM 1. number of nodes 2. review token length 3. rebuttal token length 4. Recommendation 5. final decision 6. topic CFG-PR KND 1. (conversation_a, conversation_b) pair AM 1. winner CFG-PR KND 1. (title, cleaned_review) pair AM 1. attitude CFG-PR KND 1. (native country, workclass) pair AM 1. income CFG-PR KND 1. (text, sentiment) pair AM 1. review token length CFG-PR KND 1. (source1, source2) pair AM 1. query relevancy 5 Non-structural Metrics"
        },
        {
            "title": "Downstream Task",
            "content": "1. KNN-Precision 2. KNN-Recall 1. topic prediction 2. intent prediction 1. KNN-Precision 2. KNN-Recall topic prediction 1. KNN-Precision 2. KNN-Recall winner prediction 1. KNN-Precision 2. KNN-Recall rating prediction 1. KNN-Precision 2. KNN-Recall income prediction 1. KNN-Precision 2. KNN-Recall review label prediction 1. KNN-Precision 2. KNN-Recall query relevancy prediction Implementation Details on Instruction Fine-tuning For both Instruct DP-FT and Instruct FT, we use the same instructions as those in the Random API of PE. We prepend the instructions to each training sample and fine-tune the foundation model for 20 epochs with batch size 32, weight decay 0.01, and learning rate 104. The fine-tuned model then generates new samples conditioned on the given instructions. C.1 Benchmarking DP Synthetic Data Generation Across Datasets We present the results of benchmarking the DP synthetic data generation methods under different datasets with ϵ = 4 in Table 6. We use GPT-2 for FT and DP-FT, and use GPT-4o for IF and PE. Table 6: DP synthetic data generation benchmarking results on Struct-Bench with ϵ = Structural Metrics Non-Structural Metrics Dataset Baseline ShareGPT IF (ϵ = 0) ICLR Water Arena Adult Reviews Grounding FT (ϵ = ) DP-FT PE IF (ϵ = 0) FT (ϵ = ) DP-FT PE IF (ϵ = 0) FT (ϵ = ) DP-FT PE IF (ϵ = 0) FT (ϵ = ) DP-FT PE IF (ϵ = 0) FT (ϵ = ) DP-FT PE IF (ϵ = 0) FT (ϵ = ) DP-FT PE IF (ϵ = 0) FT (ϵ = ) DP-FT PE CFG-PR KND 0.0635 0.0315 0.8700 0.5378 0 0.8633 0.1733 0 0 0.1900 1.0000 0 1.0000 1.0000 0 0 1.0000 1.0000 0 0 1.0000 1.0000 0 1.0000 1.0000 0 0 1.0000 - 0.0660 0.2582 - - 0. 0.4222 - - 0.2877 0.1257 - - 0.1054 0.0290 - - 0. 0.3510 - 0.0020 0.2495 0.5800 - - 0.1435 AM 43.8514 52.6984 - 38. 204.7997 - - 240.9434 0.1574 - - 0.0236 0.9395 - - 0. 0.0332 - - 0.0000 0.4010 - 0.0060 0.0770 0.6006 - - 0. KNN-Precision KNN-Recall 0.7217 0.7594 0.0161 0.8050 0.8400 0.7056 0.0000 0.9800 0.0000 0. 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0030 0.0030 0.0030 0.0030 0.0334 0. 0.0020 0.0290 0.0500 0.0290 0.0430 0.0300 0.2627 0.6588 0.0000 0.1528 0.0257 0. 0.0000 0.0207 0.0060 0.0060 0.0060 0.0070 0.0090 0.0060 0.0060 0.0070 0.0030 0. 0.0030 0.0060 0.0344 0.0900 0.0900 0.0900 0.0600 0.0900 0.0900 0.0600 DE Acc 0.3754 0.3718 - 0.3816 0.4715 0.4584 0.1806 0.5218 0.5485 - - 0. 0.3607 - - 0.3510 0.7920 - - 0.8017 0.6000 0.5400 0.5600 0. 0.6400 0.4000 0.4000 0.6000 C.2 Benchmarking DP Synthetic Data Generation with Varying Privacy Budget We illustrate the performance of PE and DP-FT on all metrics under different privacy budgets ϵ {1, 2, 4, } on ShareGPT and ICLR datasets by radar plots in Figs. 11 and 12. Similar to C.1, we use GPT-2 for FT 6 and DP-FT, and use GPT-4o for IF and PE. (a) ϵ = (b) ϵ = 4 (c) ϵ = 2 (d) ϵ = 1 Figure 11: Performance of PE and DP-FT on all metrics under different privacy budgets on ShareGPT. C.3 Benchmarking DP Synthetic Data Generation on ShareGPT using Llama27b We illustrate the performance of PE, DP-FT, and Instruct DP-FT in Fig. 13 , where each dimension corresponds to different metric from Struct-Bench. To better visualize differences in the performance of different methods, we scale the metrics in these radar plots as follows: We assign score of 0 if CFG-PR=0 or structure-related metric is not applicable for the dataset, and rescale the values of other metrics from 20 to 100, where 20 indicates the worst performance among all methods, and 100 indicates the performance upper bound the synthetic data can achieve (e.g., CFG-PR=1 or AM=0). Fig. 13 shows that (1) DP-FT does not learn any structural information (CFG-PR); and (2) with instruction-guided conditional generation, Instruct DP-FT achieves similar performance to PE on most metrics and has slight edge in terms of structure learning CFG-PR."
        },
        {
            "title": "D Detailed analysis of the case study on PE",
            "content": "Resource Costs All baselines are implemented and performed on server with eight H100 GPUs. Running experiments took approximately 1000 GPU hours. 7 (a) ϵ = (b) ϵ = 4 (c) ϵ = 2 (d) ϵ = Figure 12: Performance of PE and DP-FT on all metrics under different privacy budgets on ICLR. Figure 13: Performance of different baselines on ShareGPT using Llama2-7b with ϵ = 4. With instructionguided conditional generation, Instruct DP-FT achieves similar performance to PE on most metrics and has slight edge in terms of CFG-PR. 8 D.1 Analyzing Vanilla PE on ShareGPT Dataset In this section, we analyze the performance of PE under the ShareGPT dataset according to our proposed benchmark. We further divide the metrics into semantic and statistic metrics, and the evaluation items for ShareGPT can be categorized in Table 7. Table 7: Metrics for ShareGPT."
        },
        {
            "title": "Statistic Metrics",
            "content": "AM: 1. number of statements 2. query token length 3. response token length - - Semantic Metrics KND: 1. (query, response) pair 2. (response, query) pair AM: 1. topic 2. intent 1. KNN-Precision 2. KNN-Recall 1. topic prediction 2. intent prediction CFG-PR CFG-PR - -"
        },
        {
            "title": "Structural\nMetrics",
            "content": "Non-structural Metrics Downstream Tasks We illustrate and compare the performance of PE with privacy parameter ϵ {1, 2, 4, } under structural semantic and statistic metrics in Figs. 14c and 14d respectively, and plot the CFG-PR and KNN-Precision & KNN-Recall in Figs. 14a and 14b. We do not include PE with ϵ = 0 (that is, IF) as its CFG-PR is only 2% and thus its performance under structural metrics is unreliable. As we can observe, only CFG-PR and KNN-Precision improve with the increase of ϵ, while the value of KNN-Recall always keep around 0.35 and the performance under other semantic metrics and all statistic metrics does not necessarily increase with more relaxed privacy constraints. Additionally, CFG-PR drops below 60% when ϵ 4. Since downstream tasks also depend on structural information, we can conclude that PE mainly focuses on non-structural semantic quality of the synthetic samples, while suffers from poor performance on semantic diversity and structure-based properties. D.2 CFG Reformat Prompt You are required to REFORMAT the provided conversation between user and an AI agent in ChatGPT . - User prompt must start with \" HUMAN : \" , and ChatGPT response must start with \" The format should be : GPT : \" . - The conversation may contain one or multiple rounds . Each round includes ONE user prompt and ONE ChatGPT response . - User prompts and ChatGPT responses appear alternately . - The conversation begins with user prompts . The reformatted conversation follows the following context - free grammar : sharegpt : round ( round ) * round : request response request : \" HUMAN : \" user_string response : \" GPT : \" gpt_string user_string : /(? ) .+?(?=(?: GPT : HUMAN : $ ) ) / gpt_string : /(? ) .+?(?=(?: GPT : HUMAN : $ ) ) / Do NOT change the content of the conversation . For example : If the input conversation is : \" How are you ? fine . \" You should reformat it as \" HUMAN : How are you ? GPT : fine . \" D.3 Further Analysis on CFG Reformat as Self-debugging We compare the performance of vanilla PE and PE with CFG reformat on all evaluation items in Fig. 15. Self-debugging after voting directly reformats voted samples, which are taken as output or utilized as seeds 1 2 3 4 5 7 8 9 10 11 13 14 (a) CFG-PR (b) KNN-Precision & KNN-Recall (c) Performance on structural semantic metrics (d) Performance on statistic metrics Figure 14: Performance of Vanilla PE with different privacy guarantees under ShareGPT dataset 10 in the next PE iteration without further selection, resulting in higher CFG-PR while lower performance on semantic and statistic properties. Figure 15: Performance of PE with CFG Reformat on ShareGPT with ϵ = 4 D.4 Improving Node Dependency (KND): Fix Format Token in Variation API Key node dependency (i.e., KND) is an important semantic metric that measures the similarity of the node pair dependencies between the private and synthetic datasets. To improve KND, we fix the format tokens during blank-filling in variation API. Since nodes are recognized and separated by format tokens in textual datasets, fixing the format tokens ensures that multiple nodes will not be mistakenly merged into one and thus helps to remain the original semantic meaning of each node, and therefore the node semantic dependencies. We compare the performance of vanilla PE and PE with fixed format token on KND on (query, response) and (response, query) pairs and CFG-PR in Fig. 16, where we consider two variants of our method: fix all the format tokens (shown as Fixed Token) and randomly fix 65% of the format tokens (shown as Fixed Selected Token). We can observe that our methods achieve better semantic performance on KND compared to vanilla PE, and Fixed Token outperforms since it keeps more node structures than Fixed Selected Token. Additionally, as fixing format tokens avoids node merging, it also improves the structural validity, i.e., CFG-PR. We then compare the performance of vanilla PE and our methods on all metrics in Fig. 17. Since Fixed Token fixes all format tokens, the blank-filling process becomes less flexible, e.g., the number of nodes after blank-filling will never decrease, which is ensured by existing format tokens. Therefore, its performance in most statistic properties is worse than that of vanilla PE and Fixed Selected Token. D.5 Further Analysis on Node extraction & Auto-generation To further examine the semantic diversity of the dataset, we adopt another metric Type to Token Ratio (TTR) [41] to provide auxiliary information. TTR measures diversity in the tokens used in the dataset by dividing the number of unique tokens by the total number of tokens in the dataset. higher TTR suggests more diverse vocabulary. Fig. 18 shows that Extract Query has higher TTR than vanilla PE. To illustrate the semantic quality and diversity of the synthetic dataset, we then focus on the embeddings of the generated sample, and draw them in 2-dimensional plot after principal component analysis (PCA). As shown in Figs. 19a and 19b, the embeddings of vanilla PE and PE with query node extraction (blue 11 Figure 16: Performance of Vanilla PE and PE with fix token on CFG-PR and KND Figure 17: Performance of PE with Fix Format Token on ShareGPT with ϵ = 4 Figure 18: Performance of vanilla PE and PE with node extraction on Type to Token Ratio (TTR). 12 dots) are drawn together with the embeddings of private data (yellow dots). We can easily observe that the embeddings of PE with query node extraction have more overlaps with the private data embeddings, indicating higher sample semantic quality and diversity. (a) Embeddings of Vanilla PE (b) Embeddings of PE with node extraction Figure 19: Embedding distributions of Vanilla PE and PE with node extraction. We them compare the performance of vanilla PE and PE with node extraction on all metrics in Fig. 20, where we consider several variants of our method: extract all query nodes and auto-generate all response nodes (shown as Extract Query); combination of query node extraction, reformat before voting, and fix format token (Extract Query & Reformat & Fixed Token); combination of query node extraction, reformat before voting, and fix 65% format token (Extract Query & Reformat & Fixed Selected Token); combination of response node extraction, reformat before voting, and fix 65% format token (Extract Response & Reformat & Fixed Selected Token). We can observe that (1) Extract Query outperforms vanilla PE across most statistic properties, CFG-PR, and semantic properties including KNN-Precision, KNN-Recall, and KND on (response, query) pair. (2) Extract Query & Reformat & Fixed Selected Token outperforms or achieves similar performance to other node extraction variants on CFG-PR, most statistic and semantic metrics. This indicates that the combination of reformat and fix selected format tokens to node extraction improves CFG-PR and structural semantic performance without degrading statistic performance. (3) Extracting query nodes outperforms extracting response nodes, indicating that the type of nodes extracted significantly influences the synthetic data performance. D.6 Performance Comparison between Different Methods We compare the performance of our proposed methods and some combinations of them according to our benchmark. Specifically, in Fig. 21, we illustrate the performance of vanilla PE; PE with CFG reformat; PE with fixed format token; combination of CFG reformat and fix format token (Fixed Token & Reformat); and combination of CFG reformat, fix partial format token, and query node extraction (Extract Query & Reformat & Fixed Selected Token). As we can observe, Extract Query & Reformat & Fixed Selected Token outperforms on structural validity CFG-PR, semantic properties KNN-Precision and KNN-Recall, and statistic properties AM on conversation round and response token length; while Fixed Token & Reformat outperforms mainly on semantic properties KND on (query, response) and (response, query) pair. As different methods focus on different aspects of the synthetic data, users can choose the method according to their practical needs. The algorithm design and analysis based on our benchmark also pave the way to propose method that outperforms on all evaluation metrics, which we leave as future work. 13 Figure 20: Performance of PE with Node Extraction on ShareGPT with ϵ = Figure 21: Performance of Different Methods on ShareGPT with ϵ ="
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Microsoft Corporation",
        "Microsoft Research"
    ]
}