{
    "paper_title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
    "authors": [
        "Eldad Matmon",
        "Amit Bracha",
        "Noam Rotstein",
        "Ron Kimmel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars."
        },
        {
            "title": "Start",
            "content": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces with Gaussian Curvature"
        },
        {
            "title": "Eldad Matmon Amit Bracha Noam Rotstein Ron Kimmel",
            "content": "Technion Israel Institute of Technology, Haifa, Israel 6 2 0 2 6 ] . [ 1 9 1 3 3 0 . 1 0 6 2 : r Figure 1. Photorealistic 3D caricature avatars produced by our method."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce oversmoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given multiview sequence, we extract FLAME mesh, solve curvatureweighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudogroundtruth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise training scheme that alternates real and synthesized supervision, enabling single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometrycontrolled caricature avatars. Project page: https://c4ricaturegs.github.io Face caricaturization refers to the action of exaggerating distinctive facial features while preserving identity. Despite its promise for lifelike, immersive avatars, producing such exaggerations in controllable, photorealistic 3D remains an open challenge. Successful mesh-based approaches are based on geometric deformations with curvature-based methods, such as the scale-aware Poisson framework [31]. When such deformed surfaces are rendered through traditional mesh-centric pipelines, such as texture mapping, the results often appear unnatural [31]. Recently, 3D Gaussian Splatting (3DGS) [18] has emerged as potential multiview representation that provides state-of-the-art real-time photorealism by optimizing Gaussian primitives directly from given set of images taken from various directions. This raises the following question. Can we combine curvature-based geometric fidelity with 3DGS to generate photorealistic caricatures? To address this, we start with multiview video of subject and its extracted FLAME mesh [23]. From this, solving the weighted Poisson equation gives us the deformed caricature mesh. We rig Gaussians to the original undeformed surface and train them following framework previously proposed for facial expressions [21]. Later, at inference, we deform the original mesh and its rigged Gaussians according to the caricature mesh, stretching, shearing, and rotating them. However, modeling these deformations as merely an additional expression, using Gaussians optimized only on 1 the input sequence, leads to low fidelity (see Fig. 5), revealing domain gap in which caricatures lie outside the distribution of natural expression dynamics. To bridge this gap and in the absence of real caricature training data, we synthesize pseudoground truth (GT) by warping each input frame with Local Affine Transformations (LAT) induced by the correspondence from the original mesh to its curvature-exaggerated counterpart, producing photorealistic supervision (see Sec. 3.2). During training, we stochastically alternate between real views and GTviews so that single Gaussian set jointly models both natural and caricatured deformations, allowing the Gaussians to benefit from real ground truth while adapting to GT. To mitigate occlusion-related artifacts and protect fine structures (e.g. hair and mesh boundaries), we apply spatial mask that freezes the affected Gaussians during GTsteps  (Fig. 7)  . These Gaussians are updated only from real frames, allowing consistent appearance to accumulate in their attributes. Although trained only on the two sets of views, the optimized model offers additional flexibility and control at inference. First, it generalizes across continuous range of caricature intensities, with the exaggeration level controlled by an efficient linear interpolation as an approximation of the solution to the weighted Poisson equation, property that we demonstrate both theoretically and empirically. Moreover, this representation is robust to both global and local deformations, enabling controlled localized edits, such as exaggerating the nose size, while leaving unrelated regions unchanged. The new 3DGS animatable representation is the first, to our knowledge, to enable photorealistic caricature rendering while faithfully retaining identity under caricature deformations. We compare it to the current state-of-the-art dynamic facial reconstruction model [21], which consistently achieves higher scores and qualitative results in terms of image fidelity, structural consistency, and identity preservation metrics. Our contributions include, novel 3DGS training scheme that uses GTgenerated with local Affine transformations that represent real and caricature avatars. Curvature-weighted deformation with rigged 3DGS for identity-preserving photorealistic caricatures. Real-time avatars supporting variable exaggeration levels and fine-grained local control of facial features. 2. Related Work 2.1. Representation for 3D Head Avatars Neural implicit representations have become dominant approach for high-fidelity 3D head avatars, enabling photorealistic view synthesis from sparse multiview observations. IMAvatar [45] combines 3D morphable-model parameters for pose and expression control using neural blendshapes and skinning fields to produce animatable head avatars. ImFace [43] disentangles identity and expression using two deformation fields applied to signed distance function (SDF) template. ImFace++ [44] extends this approach with two-stage refinement framework that improves detail preservation. NeRFs [24] map spatial coordinates and viewing directions to radiance and density and render images via volumetric integration. For head avatars, Wang et al. [37] encode sparse views into 3D structure-aware grid of animation codes refined by an MLP. Gafni et al. [9] integrate low-dimensional morphable face model with neural scene representation to obtain photorealistic, controllable avatars from monocular video. Gao et al. [11] employ multilevel voxel fields with low-dimensional expression coefficients to capture elements beyond mesh blendshapes (e.g. hair and accessories). INSTA [47] accelerates dynamic NeRF by embedding it around surface representation to obtain animatable avatars from short monocular video and AvatarMAV [39] decouples appearance from motion via motionaware neural voxel grids. 3D Gaussian splatting [18] represents 3D scenes as anisotropic Gaussian primitives, and renders them via difIn the context of head avatars, ferentiable splatting. Rig3DGS [28] reconstructed scenes in canonical Gaussian space and learned 3DMM-guided deformations for efficient and photorealistic animation, while HeadGaS [7] extended the representation with blendable Gaussians whose attributes adapt to expression coefficients. MeGA [35] introduced hybrid meshGaussian design, combining splats with mesh geometry for high-fidelity rendering and editable head avatars. GaussianAvatars [27] bound deformable 3D Gaussians to parametric face mesh via binding inheritance strategy, and SurFhead [21] replaced the 3D Gaussians with 2D Gaussian surfels [16], applying Jacobian Blend Skinning and polar decomposition, achieving stateof-the-art results in dynamic head reconstruction. 2.2. Mesh Deformation and Exaggeration Classical mesh-based approaches realize deformations using geometry processing, e.g., Poisson/Laplacian editing and related curvature-driven deformations [19, 32, 33, 42]. For faces, mesh-based deformation and caricaturization have been explored through both geometry-driven and datadriven approaches, evolving from early parametric face models to modern neural deformation networks. Early work by Blanz and Vetter [2] introduced the 3D Morphable Face Model (3DMM), representing shape and texture as linear combinations of example faces, enabling identity and expression manipulation. In the caricature domain, Brennan [3] developed an interactive system for producing lineFigure 2. CaricatureGS generation framework. (1) From subjects multi-view video, we extract FLAME mesh and compute curvature-driven caricature based on it. Combined with subject-specific FLAME parameters, this yields the subjects caricature mesh. (2) Per-triangle 2D affine transforms map the neutral mesh projection to its caricatured counterpart, warping each frame to generate pseudoground-truth image pairs. (3) Anisotropic 3D Gaussians primitives are bound to the original mesh and transformed to the caricature mesh via the corresponding 3D triangle transforms. Rendered neutral and caricature views are alternated and compared to their pseudoground-truth counterparts in joint optimization. drawn caricatures by exaggerating the vector differences between the features of subject and an average face. Eigensatz [8] used curvature maps to enhance, smooth, and transfer characteristics while preserving global structure. Later, Sela et al. [31] proposed scale-aware Poisson-based curvature framework for surface caricaturization, exaggerating geometric features while maintaining spatial and temporal coherence. Data-driven methods have enabled for more expressive and automated mesh exaggerations. Wu et al. [38] learned deformation patterns from artist-created examples to generate 3D caricatures from single 2D portrait while preserving identity. Han et al. [12] introduced DeepSketch2Face, where CNN infers and refines 3D face or caricature meshes from 2D sketches, while their later work CaricatureShop [13] combined vertex-wise Laplacian scaling with deep learning to produce photorealistic, personalized 2D caricatures from reconstructed 3D faces. Jung et al. [17] advanced this idea by using an MLP to map latent codes to 3D displacements, supporting controlled and diverse exaggerations. More recent approaches focus on style adaptation and broader correspondences. Yan et al. [40] presented an alignment-aware 3D face morphing framework with controller-based mapping for cross-species correspondence. Olivier et al. [25] explored GAN-based style transfer from scans to caricatures. Yoon et al. [41] proposed LeGO, one-shot method that fine-tunes surface deformation network to replicate target style. An additional line of work that can be adapted to facial exaggeration is the generative line, exemplified by Diffusionand GAN-based 3DGS editors [6, 22, 36], which operate primarily on appearance while leaving the underlying geometry unchanged. 3. Method Here, we introduce method for creating controllable photorealistic caricaturizations of human faces with 3DGS. Our pipeline, illustrated in Fig. 2, begins with multiview video of subject, from which we extract FLAME-fitted mesh. In Sec. 3.1, we describe how we deform the geometry to obtain caricaturized mesh. To supervise 3DGS training, we generate pseudoground-truth caricature images (GT) using 2D warping scheme (Sec. 3.2). The Gaussian primitives are then rigged to both the neutral and caricatured meshes and optimized by minimizing alternating photometric losses between their renders, the original frames, and the corresponding GT images (Sec. 3.3). Finally, we demonstrate that this single shared Gaussian set, although trained 3 only on these two image domains, supports real-time rendering across continuous range of exaggeration levels via surface interpolation and enables region-specific edits (Sec. 3.4). 3.1. Surface Caricaturization Starting from the temporally consistent FLAME mesh obtained by fitting the landmarks [34], we apply curvaturedriven deformation that exaggerates facial geometry. Since the mesh maintains consistent vertex correspondences across frames, these deformations preserve temporal coherence. To implement this deformation, we formulate it as weighted Poisson equation on the surface. Let R3 be surface with metric and Gaussian curvature K(p) for S. For γ [0, γf ], we define the weighted Poisson equation GSγ = (cid:0)w(γ)GS(cid:1). (1) We adopt the curvature-driven deformation model introduced by [30], whose weights are given by w(γ) = Kγ. This gives, for each γ, the following family of Poisson equations : GSγ = (cid:0)KγGS(cid:1). (2) In order to derive the deformed surface we solve the PDE by the following least-squares: min Lx b2 A. (3) is the discrete LaplaceBeltrami operator, defined as = A1W , is diagonal area matrix, is the classic cotangent weight matrix and = (cid:0)KγG(x)(cid:1). The = trace(F AF ). We weighted norm is defined as 2 denote by Sγ the solution of the weighted Poisson equation in equation 2. To accommodate open surfaces, where the Gaussian curvature may be ill defined on or to allow precise usercontrolled exaggerations as discussed in Sec. 3.4, we impose boundary conditions on the selected vertices, namely: min xRn Lx A s.t. = x, (4) where {0, 1}mn selects the rows corresponding to the set of vertices and are the prescribed boundary positions. The same constrained system is solved independently for the and coordinates. An example of the resulting mesh deformation is illustrated in part (1) of Fig. 2. 3.2. GT Generation via Local Affine Transforms With these deformed surfaces, the avatars geometry is represented in caricatured form. For photorealistic rendering, we employ mesh-rigged 3DGS, detailed in Sec. 3.3. Since using 3DGS without caricature optimization yields poor results (Sec. 4.2), training requires ground-truth supervision images. As real caricature images do not exist, we generate pseudoground truth (GT): photorealistic caricature images that preserve identity while ensuring multiview consistency. One possible way to obtain such supervision is one-shot stylization (e.g., Zhou et al. [46]), which narrows the naturalcaricature gap using single exemplar image. However, it fails to disentangle style from pose and identity, often transferring both instead of style alone (see supplementary). We therefore propose an alternative: Local Affine Transformations (LAT), illustrated in part (2) of Fig. 2. LAT exploits the shared connectivity of the neutral and deformed meshes, implying per-triangle correspondence. Consider corresponding 3D triangles = {X1, X2, X3} R3 and = {Y1, Y2, Y3} R3. Let π : R3 R2 denote the image-plane projection, with xi = π(Xi) and yi = π(Yi) R2. Assuming {x1, x2, x3} are noncollinear, there exists unique affine map, Φ(x) = Ax + b, R22, R2, (5) such that Φ(x) = y. We then used these per-triangle 2D affine transformations to map color from the original image In practice, to the 2D projection of the deformed mesh. we apply an inverse warp from each target pixel back to the original image and use bilinear interpolation to avoid empty regions. Caricature deformation can reveal regions previously self-occluded in the neutral pose or occlude regions that were visible, leaving some pixels in GTwithout valid correspondences. To address this, we generate 2D trianglelevel mask for occluded regions. In addition, because hair strays fall outside the mesh limits and cannot be warped reliably, we add the hair boundary to the mask. The final output is pseudoground truth (GT): high-quality caricature images that preserve identity, ensure multiview consistency, and provide effective supervision for 3DGS, together with masks indicating per-pixel validity (see appendix for further details). 3.3. CaricatureGS Training We model the avatars appearance photorealistically using the 3D Gaussian Splatting framework [18]. Each Gaussian gi stores local attributes: position µi, scale si, rotation ri, opacity σi, and view-dependent color ci. At each time frame [0, ], the FLAME mesh R3 is represented by triangles {Tj[k]}M j=1, where is the number of mesh faces. To ensure spatialtemporal coherence, each Gaussian Gi is linked [27] to specific triangle Tj by binding index bi, converting its local attributes to world space. Building on this rigged Gaussian setup, SurFhead [21] used 2D Gaussian surfels [16], which represent surfaces as oriented planar Gaussian disks, and replaced Linear Blend Skinning (LBS) with Jacobian Blend Skinning (JBS) for Gaussians deformations, namely, Σ1/2 = Jbrisi, µ = Jbµi + where Jb = exp (cid:88) vi log(Ui) iadj (cid:88) iadj viPi, (6) where vi are learned weights and is the triangles barycentric center. Ui and Pi are the rotations and stretches from decomposing the Jacobian gradient via polar decomposition. Polar decomposition separates rotation and stretch, ensuring geometrically accurate Gaussian deformations (see [21] for further details). We show that setup originally designed for natural facial expressions can be adapted to caricature modeling by applying the deformed caricature mesh for Gaussian deformation and using GTfor 3DGS optimization. Nevertheless, training exclusively on GTintroduces occlusioninduced artifacts and limits the model to single expression level. To overcome these limitations, we propose joint optimization procedure that alternates supervision randomly between real video frames and their caricatured GTcounterparts, while maintaining single shared set of Gaussians, whose rigging ensures consistent kinematics across both supervision domains. The masks introduced in Sec. 3.2 prevent supervision of Gaussians corresponding to caricature GTpixels that cannot be reliably warped. The joint optimization scheme allows the caricatured 3DGS to learn beyond GTby simultaneously filling occlusioninduced holes using supervision from the original frames. As further demonstrated in Sec. 5.2, this strategy effectively captures hair details for our caricature avatar, despite hair pixels being excluded from direct GTsupervision. Moreover, as explained in Sec. 3.4, it also enables the generation of intermediate caricatures at any level, at inference, without additional capture. 3.4. CaricatureGS Features The joint optimization not only complements the caricature Gaussians with information absent from GTbut present in the original frames, it also provides controllability advantages during inference. Controlling Caricature Level. After joint training at the target exaggeration level γf , we empirically observe that the single-rigged Gaussian set generalizes seamlessly, rendering avatars from meshes deformed for any γ [0, γf ] without additional optimization. However, obtaining the deformed mesh for each γ requires solving curvatureweighted Poisson problem, which poses runtime bottleFigure 3. Parametric trend of the error with respect to γ. The error, normalized by the bounding-box diagonal of the mesh, increases from both ends of γ, reaching negligible maximum at γf 2 , where γf = 0.25. neck and makes interactive control of caricature levels impractical. This motivates the need for representation that can be efficiently derived from the original mesh S0 and the precomputed caricatured mesh Sγf . We define this representation as vertex-wise blend: Sblend(γ) = (1 α) S0 + α Sγf , α γ γf . (7) We define the residual between the approximation Sblend(γ) and the exact solution S(γ) as δS(γ) = Sblend(γ) S(γ). (8) In the supplementary material, we show that the L2 energy of this residual can be bounded using Poincaré inequality together with the Lax-Milgram theorem given by δS(γ)L2 γ(γf γ) GS0L2 , = CP (ln K)2 emax{0,γf ln K}, (9) with CP constant. This bound is zero at the end points γ = 0, γf , which means there is no error, as expected from (7) and maximized near γ = γf 2 , where it remains small in practice. Empirically, we evaluate the maximal deformation error between Sblend(γ) and Sγ on varying γ and different subjects, normalized by the mesh bounding-box diagonal. As shown in Fig. 3, the worst-case deviation is negligible, supporting the fidelity of the interpolation and confirming that it lies near the theoretical midpoint of the exaggeration, as predicted. This implies that, with this approximation, no additional Poisson equations need to be solved when inferring new γ values, thereby enabling full interactive control of caricaIn Fig. 5, we illustrate that this interpolation ture levels. scheme enables single set of Gaussians to smoothly represent shape deformations across the full range of γ."
        },
        {
            "title": "Method",
            "content": "CLIP-I CLIP-D CLIP-C DINO SD"
        },
        {
            "title": "SurFhead\nOurs",
            "content": "0.67 0.73 0.0006 0.014 0.944 0.945 0.757 0.888 0.460 0.539 Table 1. Quantitative comparison for caricature avatar. Higher is better for all reported metrics. 4.2. Baseline To the best of our knowledge, there are no explicit methods that construct dynamic 3D photorealistic model from an input multi-view video. To this end, we compare with SurFhead [21] using the authors official implementation. SurFhead achieves state-of-the-art performance in head reconstruction and reenactment and, in principle, can handle mesh deformations through JBS, making it the most suitable baseline for comparison. We train the SurFhead on the original input sequence and, at inference, we exaggerate the underlying mesh using γf , as elaborated in Sec. 2.2, thereby driving the Gaussians to represent caricaturized avatar. Figure 4. Visualizations of localized, semantically controlled facial exaggerations. 4.3. Metrics Localized Caricature Control. Our curvature-weighted model uses the local curvature to generate globally consistent caricature by solving the unconstrained Poisson equation. To target specific regions, we solve the constrained least-squares system in Eq. (4), whereby only the chosen region of interest undergoes curvature deformations, producing smooth and localized exaggerations that blend harmonically with the rest of the face. Coupled with the training scheme in Sec. 3.3, the 3DGS, rigged to the mesh, faithfully tracks these deformations, so the same Gaussian set realizes semantically controlled exaggerations while preserving identity and global shape (see Fig. 4). 4. Experiments We evaluate our caricaturized avatars along two main axes: (i) photorealistic rendering, (ii) identity preservation. All experiments are conducted on the NeRSemble dataset [20] and compared against the recent state-of-the-art 4D avatar reconstruction method of SurFhead [21]. Unless noted otherwise, we apply an unconstrained exaggeration with γf = 0.25. 4.1. Dataset The NeRSemble dataset [20] provides multi-view facial performance dataset captured by 16 spatially arIt comranged, synchronized high-resolution cameras. prises 10 scripted sequences, 4 emotion-driven (EMO) and 6 expression-driven (EXP), plus an additional free selfreenactment sequence. For fair comparison, we adopt the same train/validation/test partition as in [21] with 120, 000 training iterations. Further implementation details are provided in the supplementary. Quantitative evaluation of caricature models is inherently challenging due to their under-constrained nature and the lack of ground-truth images. We use the following metrics for evaluation: CLIP-I (ImagePrompt Similarity) [15]: Cosine similarity between the rendered image and text in CLIP space. CLIP-D (Directional Similarity) [10]: Measures the change between source and edited images against the change between source and edited prompts. CLIP-C (Spatial Consistency): Following [14], we report CLIP image alignment between adjacent novel views of image embeddings along novel trajectory. DINO (Identity/Structure Consistency): Following [46], we extract DINO [5] features from the renders and the corresponding original test frames and compute the cosine similarity of the embeddings. SD (Score Distillation): Inspired by DreamFusion [26], we define the reference-free metric as, SD = 1 (cid:16) (cid:13) (cid:13) (cid:13)ϵθ 1 BT B,T,N (cid:88) b,t,n x(b,t,n) (cid:17) , ϵb,t,n2 2 ϵb,t,n (cid:13) 2 (cid:13) (cid:13) 2 . (10) where ϵθ(xt, t) is the noise predicted by the diffusion model [29] at time step t, ϵ is the true noise, and B, T, refer to the image count, time step, and seed number, respectively. Higher SD indicates that the rendered image is more consistent with the training distribution of the diffusion model, which is intended to approximate the natural image distribution. Text prompts are provided in the appendix. Together, these metrics evaluate: (i) how well the renders reflect the carica6 Figure 5. Rendering results from our pipeline [21]. SURFHEAD: Caricature generation by first reconstructing an avatar with the state-of-the-art SURFHEAD model [21], followed by mesh exaggeration. Ours: Renderings across different caricature intensities. Our approximation-based control interpolates smoothly along the caricature intensity axis while preserving visual fidelity. ture intent (CLIP-I, CLIP-D, SD), (ii) identity preservation and the extent to which exaggerations remain localized to caricaturization (DINO, CLIP-D), and (iii) consistency of generated views across novel trajectories (CLIP-C). 4.4. Results Fig. 5 presents side-by-side renderings at the target exaggeration level γf for our method and the baseline. Our approach maintains subject identity while delivering natural, visually pleasing exaggerations that remain consistent across views, and reduces the distortions visible in the baseline. The figure further illustrates caricature-level controllability by varying γ from 0 to γf , demonstrating continuous control and showing that the approximation in Sec. 3.4 successfully supports intermediate exaggeration levels. For quantitative evaluation, we conduct comprehensive comparison using the metrics in Sec. 4.3. As summarized in Tab. 1, our method consistently surpasses the baseline across all measures, demonstrating that the learned edits faithfully capture the intended caricature while preserving both identity and view-consistency. 4.5. Diffusion Based Editing As an additional baseline, we adapt diffusion-driven, textguided, mesh-free 3DGS editor [6] for caricaturization. Using the authors implementation, we run 5,000 optimization steps per prompt on multiview images of subject, guided by ControlNet-Pix2Pix. Fig. 6a presents global edit, while Fig. 6b shows local edit, manually masked for face and nose, respectively. While the edits appear visually plausible in individual views, it is evident that, unlike our method, this baseline suffers from (i) geometry drift, (ii) unstable, view-dependent specularities, and (iii) poor multi-view coherence. 5. Ablations 5.1. Alternated Training In this subsection, we demonstrate that training with GT, generated using LAT, is essential for controlling the caricaturization level. As discussed in Sec. 4.4, training only on input images fails to generalize: rendering with caricatured mesh yields heavily degraded outputs. In the supple7 (a) Edit instruction: Turn him into realistic caricature. The result exhibits skin-tone shifts and specular degradation. (b) Edit instruction: Make his nose bigger. The geometry falls apart and color inconsistencies appear across views. (a) Figure 6. GaussianEditor [36] caricaturization attempts. Global edit. (b) Local semantic edit. Both reveal degraded geometry and appearance fidelity, particularly in novel views. mentary, we show that training solely with GTalso fails: neutral renders appear unrealistic, with distorted Gaussian structures. These complementary failures underscore the necessity of alternating both forms of supervision for effective caricaturization control. 5.2. Mask Due to the nature of GTgeneration, certain fine details, most notably hair, are often misrepresented during the caricature stage. To address this, we identify hair regions of the mesh and freeze the corresponding Gaussian parameters with suitable mask during GTsupervision iterations, thereby preventing updates in those regions when the caricature is rendered (see Sec. 3.2). Fig. 7 illustrates the effect: on the left, hair regions are masked and remain frozen, whereas on the right they are unfrozen and allowed to train freely, resulting in unnaturally plastic-looking hair. 6. Limitations While our method provides powerful framework for photorealistic 3D caricaturization, several limitations remain. 8 Figure 7. Ablation on hair masking. Without masking, GTintroduces visible artifacts in hair regions. Masking and freezing Gaussians associated with hair during GTsupervision effectively prevents these artifacts. Although our approach improves upon the baseline, residual specularity artifacts persist, and small eyelid inaccuraciesamplified by over-stretching in LAT, become visually noticeable. This effect also extends to hair: training Caricature 3DGS hair with input-view supervision alone (without GT) substantially alleviates the issue. However, in some cases, we observe slight over-smoothing of the hair. Qualitative examples of these effects are provided in the supplementary material. Finally, the deformed FLAME mesh does not fully span the space of facial expressions. For instance, eyelid closure in caricatured results is imperfect: eyes that should be completely shut under certain expressions often remain slightly open, leading to misrepresentations of eyelid geometry in the final caricature. 7. Discussion This work demonstrates that curvature-driven geometric deformation and mesh-rigged 3D Gaussian Splatting (3DGS) can be combined into single, controllable avatar model that remains photorealistic under large exaggerations. The key is training scheme that alternates supervision between real views and generated pseudoground-truth caricature views, produced using per-triangle Local Affine Transformations (LAT) with reliability masks. One Gaussian set is capable of jointly learning both natural and caricatured appearance while retaining identity and expression. Prior work indicates that deliberate shape exaggeration can amplify discriminative geometric cues for recognition [30]. Looking ahead, we hypothesize that integrating our controllable exaggeration as plug-in augmentation within face-recognition pipelines could improve robustness to pose and expression variability. Finally, coupling our geometrygrounded deformations with diffusion-based editors may enable semantically guided edits that are both photorealistic and extend beyond appearance-only changes to joint control of shape and appearance."
        },
        {
            "title": "References",
            "content": "[1] Kendall E. Atkinson and Weimin Han. Theoretical Numerical Analysis: Functional Analysis Framework. Springer, 3rd edition, 2009. [2] Volker Blanz and Thomas Vetter. Morphable Model For The Synthesis Of 3D Faces. [3] Susan E. Brennan. Caricature Generator: The Dynamic Exaggeration of Faces by Computer. Leonardo, 18(3):170178, 1985. Publisher: The MIT Press. [4] Richard L. Burden and J. Douglas Faires. Numerical Analysis. Brooks/Cole, 10th edition, 2010. [5] Mathieu Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. EmergIn ing properties in self-supervised vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [6] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. GaussianEditor: Swift and Controllable In 2024 IEEE/CVF 3D Editing with Gaussian Splatting. Conference on Computer Vision and Pattern Recognition (CVPR), pages 2147621485, Seattle, WA, USA, 2024. IEEE. [7] Helisa Dhamo, Yinyu Nie, Arthur Moreau, Jifei Song, Richard Shaw, Yiren Zhou, and Eduardo Pérez-Pellitero. HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting. arXiv e-prints, art. arXiv:2312.02902, 2023. [8] Michael Eigensatz, Robert W. Sumner, Curvature-Domain Shape Processing. Pauly. puter Graphics Forum, 27(2):241250, 2008. https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.14678659.2008.01121.x. and Mark Com_eprint: [9] Guy Gafni, Justus Thies, Michael Zollhöfer, and Matthias Nießner. Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction. arXiv e-prints, art. arXiv:2012.03065, 2020. [10] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, Stylegan-nada: Clip-guided doarXiv preprint and Daniel Cohen-Or. main adaptation of image generators. arXiv:2108.00946, 2021. [11] Xuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong, Yudong Guo, and Juyong Zhang. Reconstructing Personalized Semantic Facial NeRF Models From Monocular Video. arXiv e-prints, art. arXiv:2210.06108, 2022. [12] Xiaoguang Han, Chang Gao, and Yizhou Yu. DeepSketch2Face: deep learning based sketching system for 3D face and caricature modeling. ACM Transactions on Graphics, 36(4):112, 2017. [13] Xiaoguang Han, Kangcheng Hou, Dong Du, Yuda Qiu, Yizhou Yu, Kun Zhou, and Shuguang Cui. CaricatureShop: Personalized and Photorealistic Caricature Sketching, 2018. arXiv:1807.09064 [cs]. [14] Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Instruct-NeRF2NeRF: Holynski, and Angjoo Kanazawa. Editing 3D Scenes with Instructions. arXiv e-prints, art. arXiv:2303.12789, 2023. 9 [15] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: Reference-free Evaluation Metric for Image Captioning. arXiv e-prints, art. arXiv:2104.08718, 2021. [16] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2D Gaussian Splatting for Geometrically Accurate Radiance Fields. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers 24, pages 111, 2024. arXiv:2403.17888 [cs]. [17] Yucheol Jung, Wonjong Jang, Soongjin Kim, Jiaolong Yang, Xin Tong, and Seungyong Lee. Deep Deformable 3D Caricatures with Learned Shape Control. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings, pages 19, 2022. arXiv:2207.14593 [cs]. [18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions on Graphics, 42(4):114, 2023. [19] ByungMoon Kim and Jarek Rossignac. Geofilter: Geometric selection of mesh filter parameters. Comput. Graph. Forum, 24:295302, 2005. [20] Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, and Matthias Nießner. NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads. ACM Transactions on Graphics, 42(4):114, 2023. arXiv:2305.03027 [cs]. [21] Jaeseong Lee, Taewoong Kang, Marcel C. Bühler, Min-Jung Kim, Sungwon Hwang, Junha Hyung, Hyojin Jang, and Jaegul Choo. SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars, 2024. arXiv:2410.11682 version: 1. [22] Guohao Li, Hongyu Yang, Yifang Men, Di Huang, Weixin Li, Ruijie Yang, and Yunhong Wang. Generating Editable Head Avatars with 3D Gaussian GANs, 2024. arXiv:2412.19149 [cs]. [23] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. Learning model of facial shape and expression from 4D scans. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia), 36(6):194:1194:17, 2017. [24] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. CoRR, abs/2003.08934, 2020. [25] Nicolas Olivier, Glenn Kerbiriou, Ferran Argelaguet Sanz, Quentin Avril, Fabien Danieau, Philippe Guillotel, Ludovic Hoyet, and Franck Multon. Study on Automatic 3D Facial Caricaturization: From Rules to Deep Learning. Frontiers in Virtual Reality, 2:115, 2022. [26] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D Diffusion, 2022. arXiv:2209.14988 [cs]. [27] Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, and Matthias Nießner. GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians, 2024. arXiv:2312.02069 [cs]. [28] Alfredo Rivero, ShahRukh Athar, Zhixin Shu, and Dimitris Samaras. Rig3DGS: Creating Controllable Portraits [43] Mingwu Zheng, Hongyu Yang, Di Huang, and Liming Chen. ImFace: Nonlinear 3D Morphable Face Model with Implicit Neural Representations. arXiv e-prints, art. arXiv:2203.14510, 2022. [44] Mingwu Zheng, Haiyu Zhang, Hongyu Yang, Liming Chen, ImFace++: Sophisticated Nonlinear 3D and Di Huang. Morphable Face Model with Implicit Neural Representations. arXiv e-prints, art. arXiv:2312.04028, 2023. [45] Yufeng Zheng, Victoria Fernández Abrevaya, Marcel C. Bühler, Xu Chen, Michael J. Black, and Otmar Hilliges. Avatar: Implicit Morphable Head Avatars from Videos. arXiv e-prints, art. arXiv:2112.07471, 2021. [46] Yang Zhou, Zichong Chen, and Hui Huang. Deformable One-shot Face Stylization via DINO Semantic Guidance, 2024. arXiv:2403.00459 [cs]. [47] Wojciech Zielonka, Timo Bolkart, and Justus Thies. InarXiv e-prints, art. stant Volumetric Head Avatars. arXiv:2211.12499, 2022. from Casual Monocular Videos. arXiv:2402.03723, 2024. arXiv e-prints, art. [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. [30] Matan Sela, Yonathan Aflalo, and Ron Kimmel. Computational caricaturization of surfaces. Computer Vision and Image Understanding, 141:117, 2015. [31] Matan Sela, Yonathan Aflalo, and Ron Kimmel. Computational caricaturization of surfaces. Computer Vision and Image Understanding, 141:117, 2015. [32] Olga Sorkine and Marc Alexa. As-rigid-as-possible surface modeling. In Proceedings of EUROGRAPHICS/ACM SIGGRAPH Symposium on Geometry Processing, pages 109 116, 2007. [33] Olga Sorkine, Daniel Cohen-Or, Yaron Lipman, Marc Alexa, Christian Rössl, and Hans-Peter Seidel. Laplacian surface editing. In Proceedings of the EUROGRAPHICS/ACM SIGGRAPH Symposium on Geometry Processing, pages 179 188. ACM Press, 2004. [34] Justus Thies, Michael Zollhöfer, Marc Stamminger, Christian Theobalt, and Matthias Nießner. Face2Face: Real-time Face Capture and Reenactment of RGB Videos. arXiv eprints, art. arXiv:2007.14808, 2020. [35] Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, and Song-Hai Zhang. MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and Head Editing. arXiv e-prints, art. arXiv:2404.19026, 2024. [36] Junjie Wang, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, and Qi Tian. GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions, 2024. arXiv:2311.16037 [cs]. [37] Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, and Michael Zollhöfer. Learning Compositional Radiance Fields of Dynamic Human Heads. arXiv e-prints, art. arXiv:2012.09955, 2020. [38] Qianyi Wu, Juyong Zhang, Yu-Kun Lai, Jianmin Zheng, and Jianfei Cai. Alive Caricature from 2D to 3D, 2018. arXiv:1803.06802 [cs]. [39] Yuelang Xu, Lizhen Wang, Xiaochen Zhao, Hongwen Zhang, and Yebin Liu. AvatarMAV: Fast 3D Head Avatar Reconstruction Using Motion-Aware Neural Voxels. arXiv e-prints, art. arXiv:2211.13206, 2022. [40] Xirui Yan, Zhenbo Yu, Bingbing Ni, and Hang Wang. CrossSpecies 3D Face Morphing via Alignment-Aware Controller. Proceedings of the AAAI Conference on Artificial Intelligence, 36(3):30183026, 2022. [41] Soyeon Yoon, Kwan Yun, Kwanggyoon Seo, Sihun Cha, Jung Eun Yoo, and Junyong Noh. LeGO: Leveraging Surface Deformation Network for Animatable Stylized Face Generation with One Example, 2024. arXiv:2403.15227 [cs] version: 1. [42] Yizhou Yu, Kun Zhou, Dong Xu, Xiaohan Shi, Hujun Bao, Baining Guo, and Heung-Yeung Shum. Mesh editing with In ACM SIGpoisson-based gradient field manipulation. GRAPH 2004 Papers, pages 644651. 2004. 10 CaricatureGS: Exaggerating 3D Gaussian Splatting Faces with Gaussian Curvature"
        },
        {
            "title": "Supplementary Material",
            "content": "8. Implementation considerations Unless stated otherwise, we optimize each subjects 3D Gaussian Splatting model for 120,000 iterations, adhering to SurFheads training protocol and evaluation split [21]. All experiments are run on single NVIDIA RTX 3090 (24 GB VRAM). The optimization time per subject is 4 hours (this is offline training time, not rendering runtime.) We used the NeRSemble dataset [34] with 10 subjects, 4 emotions (EMO), and 6 expressions (EXP). Expression EXP2 is held for testing and Camera 8 serves as the validation view during training. Caricaturization is performed once at the beginning of the training by solving the unconstrained Poisson equation, deforming the FLAME base template with γ = 0.25 ( 1 min). Because FLAME uses shared template across subjects, the deformed surface is saved and reused for all subjects. Unless stated otherwise, we report metrics over 256 frames from the rendered test sequence, aggregated across all camera viewpoints. CLIP configuration. For textimage alignment, we use OpenAI CLIP with the ViT-B/32 backbone and the librarys default preprocessing. Prompts are: Source: realistic neutral head with natural lighting. Edit: photorealistic caricature of head with highly exaggerated nose and large ears, under natural lighting. degree of Defaults inherited. The optimizer, learning rate schedule, and Gaussian spherical harmonics, growth/pruning follow the SurFhead [21] configuration unless otherwise specified. 9. Linear Model and Error Analysis Notation. Let S(u, v) be parametric surface, where (u, v) R2, with metric and denotes the Gaussian curvature at each point of the surface S, and w(γ) = Kγ = eγL, ln K. (11) For γ [0, γf ], denote by Sγ the solution of the weighted Poisson problem with Dirichlet boundary condition on S. To avoid degeneracies at = 0, we use ϵ to stabilize the magnitude. Note, for convenience we refer to as Kϵ = 1 2 + ϵ2 with fixed ϵ > 0. For brevity we write to denote this stabilized quantities. 1) Poisson equation with secant weights. The original family is defined by GSγ = (cid:0)w(γ) GS(cid:1). (12) Note, that S0 and Sγf refer to γ = 0 and γ = γf , respectively. Define the vertex blend, Sblend(γ) = (1 α) S0 + α Sγf , α γ γf .(13) By linearity of and Equation (13) GSblend(γ) = (1 α) GS0 + α GSγf (cid:16) wsec(γ) GS (cid:17) , = where secant weight is wsec(γ) = 1 + (cid:0)Kγf 1(cid:1). γ γf (14) (15) Thus Sblend(γ) solves the exact Poisson equation at level γ with w(γ) replaced by wsec(γ), and SinterpS = (see (4) for x). 2) Remainder and properties The secant wsec is the linear interpolant of in [0, γf ]. By the classical interpolation remainder for 2 functions on closed interval (e.g., [4, Thm. 3.1], [1, 3.3]), for every γ [0, γf ] there exists ξ(γ) (0, γf ) such that wsec(γ) w(γ) = w(ξ) 2 γ(γf γ). (16) Since w(γ) = L2eγL, we get wsec(γ) w(γ) = L2 eξL γ(γf γ). (17) The secant model is exact at both endpoints (where α = 0 and α = 1, yielding analytic expression in [0, γf ] preserving the convexity-induced non-negativity. Since 0, γ (cid:55) w(γ) is convex, hence wsec is nonnegative on [0, γf ] and vanishes at the endpoints. In particular, at γ = γf /2, (cid:12) (cid:12)wsec( γf 2 ) w( γf 2 )(cid:12) (cid:12) γ2 8 L2 max (1, eγf ).(18) The maximum of this upper bound occurs at γf /2 because γ(γf γ) is maximized there. Let Sblend(γ) = (1 α)S0 + αSγf with α = γ/γf , and define 3) Poincaré and LaxMilgram for residual bound. Throughout, we approximate the γdependent weight w(γ) = Kγ by its secant wsec(γ) to enable cheap vertex blend instead of solving new Poisson problem for each γ. To justify this alternative, we should quantify how the weight error propagates to geometric residual δS(γ) S(γ) Sblend(γ). The goal here is to derive norm bound on δS that depends only on: (i) ellipticity and Poincaré constants of the domain, (ii) the magnitude of GS0, and (iii) the scalar secant remainder from Appendix Eq. (18). This yields mesh and metric agnostic error budget for the blend. Setting (frozen operator). Let (S, G) be compact Riemannian surface with Lipschitz boundary S. We impose Dirichlet conditions u(cid:12) (cid:12)S = 0. We fix the differential operators on the surface S, namely, the gradient and the divergence w.r.t metric G. Let 1 0 (S) and define (cid:90) a(u, v) = Gu, GvG dAG uV GuL2(S). We also define the dual norm by V sup vV {0} (v) vV . Using Poincaré inequality, there exists CP > 0 such that, for all 1 0 (S), uL2(S) CP GuL2(S) = CP uV . (21) Hence uV is true norm on 1 the standard 1-norm on 1 0 (S). By CauchySchwarz, 0 (S) and is equivalent to a(u, v) uV vV (boundedness), a(v, v) = v2 (coercivity with α = 1) (22) where coercivity means that there exists α > 0 such that a(v, v) α v2 V. LaxMilgram. If is bounded and coercive on the Hilbert space and is bounded, then, there exists unique solution , solving a(u, v) = (v) for all , with estimate uV 1 α V (22)= V . (23) For each γ, we solve the weighted Poisson PDE given by (cid:0)w(γ) GS(cid:1), GSγ = (cid:12) (cid:12)S = x. (24) Sγ ψ(γ) wsec(γ) w(γ) (cid:0)ψ GS(cid:1). R(γ) (25) Define (weak residual functional) by (v) = R, (cid:90) = (cid:0) G(ψ GS)(cid:1) dAG (cid:90) = ψ GS, GvG dAG, (26) with v(cid:12) (cid:12)S = 0. Using the dual norm and by CauchySchwarz and ψL -bound, we readily have (v) ψL(S) GSL2(S) GvL2(S) = ψL GSL2(S) vV , (27) and using (20) we get (19) (20) V ψL GSL2(S). (28) Let δS Sblend Sγ. Subtract the weak forms for Sblend and Sγ to obtain a(δS, v) = a(Sblend, v) a(Sγ, v) (cid:90) (cid:90) = = wsec GS, GvG dAG (cid:90) w(γ) GS, GvG dAG ψ GS, GvG dAG (cid:90) = (cid:0)ψ GS(cid:1) dAG () (v). (29) Where in (*) we use integration by parts and Dirichlet boundary conditions on S. Testing with = δS and using coercivity and duality, δS2 = a(δS, δS) = (δS) V δSV δSV V . (30) Combining with the bound on V yields the energy estimate δSV ψL(S) GSL2(S) δSV wsec wL GSL2(S). (31) 2 Optional L2 bound. By Poincaré on 1 0 (S), δSL2(S) CP δSV CP wsec wL GSL2(S).(32) In summary, the secant error bound yields the energy bound for the residual δS by δS(γ)L2 CP (ln K)2emax(0,γf ln K) γ(γf γ) GSL2(S). (33) which depends on geometric constants of the domain (CP ). The curvature in (33) is evaluated at its global maximum = = max sS K(s) (34) We note that S0 = (for γ = 0 by definition since there is no deformation done to S), hence (33) can be written using either terms. 10. Caricature GTvia one-shot stylization As discussed in Sec. 3, one-shot stylization methods (e.g., Deformable StyleGAN [46]) address the natural-caricature domain gap by aligning DINO features and adapting pretrained GAN to single caricature exemplar. Given target style image (Fig. 8a), they synthesize stylized outputs for arbitrary inputs. In practice, we observe pronounced identityexpression entanglement, which degrades both identity fidelity and expression accuracy  (Fig. 8)  . Moreover, the outputs are not consistent across viewpoints or expressions: under view changes or when transferring expressions from the source, the method exhibits structural drift and collapse toward the reference style (Figs. 8b and 8c), limiting its suitability for our 3DGS reconstruction setting. Protocol. We ran [46] using the official implementation, employing Style1, Style2, and Style3 as target style exemplars and EMO3, EMO4 for expression prompts. 11. Masking and GT As noted in Sec. 3.2, GT supervision is constructed by projecting the FLAME mesh, fitted to each original frame, onto the image. Consequently, the quality of GT inherits any meshimage misregistration. In practice, small fitting errors that are negligible at γ=0 are amplified as the caricature strength increases, with the most visible drift around delicate geometry such as the eyelids and eyeballs; see Fig. 9. In addition, the deformation can reveal triangles that were occluded in the original projection (e.g., along the eyelid crease), creating pixels with no reliable photometric support. (a) Deformable StyleGAN [46]: stylization conditioned on target style exemplar. (b) View variation induces identity drift and structural artifacts (e.g. neck geometry). (c) Expressions are not preserved, outputs bias toward the style exemplar (e.g. persistent smile, forward gaze). Figure 8. Limitations of one-shot stylization for caricature. Identityexpression entanglement and lack of view/expression consistency hinder 3DGS supervision. To prevent these failure modes, we build visibilityaware GT mask. We (i) suppress supervision on triangles that become newly visible at nonzero γ relative to the original projection, and (ii) mask anatomically fragile regions prone to amplified alignment error (eyelids, ear tips). 3 Conclusions. Alternating supervision is necessary to obtain single 3DGS that is faithful at γ=0 and γ=γf and stable along the interpolation path, while training on either domain alone leads to domain-specific overfitting and characteristic failure modes. Figure 9. FLAMEimage misregistration under increasing caricature strength γ. Projection drift concentrates on thin, highcurvature structures (eyelids/iris rim) and grows with γ, introducing erroneous supervision if used unfiltered. This filtering removes inconsistent labels before they reach Gaussians anchored to those areas, yielding cleaner gradients and more stable appearance/geometry during training. The resulting GT thus preserves the benefits of deformation-aware supervision while avoiding artifacts introduced by projection drift and occlusions. 12. Ablation: Alternating Supervision Setup. As motivated in Sec. 5.1, we seek single 3DGS model that renders both the original avatar (γ=0) and its caricatured counterpart (γ=γf ). We compare three training schedules using identical budgets: (i) Original-only: supervision from original frames only. (ii) GT-only: supervision from caricatured (GT) frames only. (iii) Alternating (ours): alternating mini-batches from both sources. We set the target exaggeration to γf =0.25 and evaluate along the interpolation path γ {0, 0.10, 0.15, 0.20, 0.25}. Findings. Original-only (i) fits the undeformed scene well but fails to generalize to caricatured geometry Fig. 10, yielding visible distortions under nonzero γ. Conversely, GT-only (ii) represents the caricatured avatar but degrades markedly at γ=0. In addition, GT-only exhibits systematic artifacts around hair and other structures that extend beyond the tracked mesh support (e.g. holes or under-coverage), because those pixels are never directly supervised in the warped domain, see Fig. 11. Our alternate schedule (iii) maintains high fidelity at both endpoints and produces smooth interpolation across γ (see Fig. 12), avoiding the hair/occlusion failures seen in (ii). Practically, alternating acts as simple multi-domain regularizer, as it preserves appearance outside the mesh support (from original frames) while learning the exaggerated geometry and view-dependent effects required by GT. 4 Figure 10. Training on original frames only 5 Figure 11. Training on GTframes only. 6 Figure 12. Training on both original and GTframes interleaved"
        }
    ],
    "affiliations": [
        "Technion Israel Institute of Technology, Haifa, Israel"
    ]
}