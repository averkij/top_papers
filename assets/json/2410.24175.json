{
    "paper_title": "Constraint Back-translation Improves Complex Instruction Following of Large Language Models",
    "authors": [
        "Yunjia Qi",
        "Hao Peng",
        "Xiaozhi Wang",
        "Bin Xu",
        "Lei Hou",
        "Juanzi Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research."
        },
        {
            "title": "Start",
            "content": "Constraint Back-translation Improves Complex Instruction Following of Large Language Models Yunjia Qi*, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li Department of Computer Science and Technology, BNRist, Tsinghua University {qyj23, peng-h24}@mails.tsinghua.edu.cn 4 2 0 2 1 3 ] . [ 1 5 7 1 4 2 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instructionresponse pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to backtranslate constraints and create high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have achieved remarkable performance in numerous natural language processing tasks (Zhao et al., 2023; OpenAI, 2024; Yang et al., 2024; Dubey et al., 2024; Team et al., 2024). However, they still fall short in following instructions with complex constraints (Zhou et al., 2023; Jiang et al., 2024; Qin et al., 2024), such as length constraints shown in Figure 1, which limits their effectiveness and usability. *Equal contribution. 1https://github.com/THU-KEG/Crab 1 Figure 1: Existing datasets inherently include implicit satisfied complex constraints in the responses. To enhance the instruction-following ability of LLMs, the standard practice is to post-train the targeted LLM on large set of instruction-response data pairs. For the complex instruction-following with multiple constraints, existing efforts (Sun et al., 2024; He et al., 2024) synthesize complex datasets by adding multiple constraints to existing instructions and generating responses with advanced LLMs like GPT-4 (OpenAI, 2024). While this data generation pipeline is straightforward and widely adopted, even the most capable LLMs cannot follow complex instructions well (Jiang et al., 2024; Qin et al., 2024), which limits the quality of generated data and necessites laborious filtering. The status quo urges the development of automatic data generation methods relying less on existing LLMs complex instruction-following abilities. Our key observation is that existing datasets inherently include implicit complex constraints so that we can reuse the widely-available high-quality instruction-following datasets (Xu et al., 2023; Taori et al., 2023; Mukherjee et al., 2023; Köpf et al., 2024) to synthesize complex instructionresponse pairs. As shown in Figure 1, although the original concise instruction does not explicitly specify constraints like writing style or length, the response already satisfies some constraints in multiple dimensions. Therefore, we can efficiently create high-quality complex instruction-response pairs from existing datasets by generating constraints from responses and adding them to instructions. We dub this data generation method as constraint back-translation. It only requires discovering the constraints already met by responses rather than following the complex instructions with multiple constraints, which significantly reduces requirements for model capability. As result, it is both costeffective and capable of producing high-quality data with limited noise. We also find that constraint back-translation can serve as useful auxiliary training objective in post-training, dubbed as the reverse training technique. Specifically, we use instructions and responses as inputs to train the model to output constraints in post-training. The intuition is that reverse training may enhance the models understanding of constraints and improve its efficacy (Golovneva et al., 2024). We adopt Llama3-70B-Instruct (Dubey et al., 2024) to back-translate constraints from collection of existing data, generating large-scale complex instruction-following dataset, named CRAB. Specifically, we sample total of 13, 500 instances from existing high-quality instruction-following datasets (Peng et al., 2023; Es, 2023; Xu et al., 2023; Köpf et al., 2024) as the seed data, and manually define scope of common constraints. We then use the original instruction, response, and constraint scope as inputs to Llama3-70B-Instruct to generate the corresponding implicitly satisfied constraints. Following previous works (Sun et al., 2024; He et al., 2024), we train the LLMs using the mixture of CRAB and ShareGPT dataset (Chiang et al., 2023), and we jointly adopt standard supervised fine-tuning and reverse-training on CRAB. In the experiments, we select the capable opensource LLMs Llama3 8B (Dubey et al., 2024) and Mistral 7B (Jiang et al., 2023) as backbone models and evaluate the complex instruction-following abilities of our models against various baselines on IFEval (Zhou et al., 2023) and FollowBench (Jiang et al., 2024). The results demonstrate that training on CRAB significantly enhances LLM performance in complex instruction following. We also conduct evaluation for general instruction-following abilities on AlpacaEval (Li et al., 2023b) and find that our models achieve even larger improvements to previous works focusing on enhancing complex instruction-following abilities like Conifer (Sun et al., 2024). This indicates that constraint backtranslation yields higher general data quality than previous techniques relying on the ability of advanced LLMs. Ablation studies further validate the efficacy of our CRAB dataset and reverse training approach. Finally, we discuss the advantages and challenging scenarios for our constraint backtranslation method with experiments In summary, our contributions are threefold: (1) We propose constraint back-translation, costeffective and high-quality data construction method for complex instruction following. (2) We construct CRAB, high-quality complex instructionfollowing dataset, and design reverse training method for developing Llama3CRAB, MistralCRAB models with better complex instruction-following abilities. (3) We conduct extensive experiments to demonstrate the efficacy of CRAB and discuss key design choices and potential improvement opportunities to inspire future research on following complex instructions with multiple constraints."
        },
        {
            "title": "2 Method",
            "content": "This section introduces the construction process of CRAB ( 2.1) and the training method ( 2.2)."
        },
        {
            "title": "2.1 Constructing CRAB",
            "content": "We begin by introducing the notions. Given an instruction x, which typically defines specific task, such as Write blog on French cuisine, set of constraints c, which specify conditions for the response, such as length restrictions, and response that satisfies both the constraints and the instruction x, our goal is to construct high-quality dataset of (x, c, y) triples. We first collect set of high-quality (x, y) pairs from existing datasets and then apply constraint back-translation to generate the constraints for each (x, y) pair. The data construction process is illustrated in Figure 2, which consists of three steps: data collection, constraint back-translation, and constraint combination. In the data collection process, we collect comprehensive set of high-quality (x, y) pairs from existing datasets. We then back-translate the corresponding for each (x, y) using Llama3-70B-Instruct and Python scripts automatically. Finally, we perform 2 Figure 2: The framework of constructing the proposed alignment training dataset. filtering and combination of the constraints to construct CRAB. More details of the data construction process are shown in appendix A. Data Collection We first collect comprehensive set of (x, y) as seed data from four existing widelyused high-quality supervised fine-tuning datasets, including Alpaca GPT4 (Peng et al., 2023), Orca Chat (Es, 2023), Evol Instruct (Xu et al., 2023), and OpenAssitant (Köpf et al., 2024). For Alpaca GPT4 and OpenAssistant, which contain the human annotated quality score for each instance, we use the instances with the highest quality. Moreover, to ensure that the responses include diverse constraints implicitly, we only consider instances where the response exceeds 300 words. We randomly sample the qualified instances using an examples-proportional mixture (Wei et al., 2022), resulting in total of 4, 500 raw instances. Constraint Back-translation We then adopt the Llama3-70B-Instruct LLM and Python scripts to back-translate constraints for the seed data. First, we define set of 19 types of common constraints, such as length and style constraints. Considering some constraints, e.g., length constraint, cannot be effectively followed by LLMs (Sun et al., 2024), leading to noisy back-translation, and some constraints can be easily generated using Python scripts, we choose to adopt Python scripts for 6 types of constraints, including length, keyword, and punctuation constraints. Specifically, we write and paraphrase several templates for each of these 6 types of constraints. For example, for length constraint, one template is Please generate response with fewer than <placeholder> words but more than <placeholder> words. We then use Python scripts to automatically identify the value for this constraint in the response and fill the templates to construct constraint. For the length constraint, we randomly sample range that includes the value to fill the template. For keyword and punctuation constraints, we randomly select corresponding items present in the response to fill the templates of constraints. For the remaining 13 types of constraints, such as style constraints, we use Llama3-70BInstruct for constraint back-translation. Specifically, we include these 13 types of constraints and corresponding descriptions, along with the instruction and response, as the prompt to query Llama370B-Instruct to generate the constraints implicitly satisfied by the response. We then use Llama370B-Instruct to re-verify whether the response satisfies the generated constraints and exclude the constraints that are not met. We adopt ROUGE-L (Lin, 2004) with threshold of 0.6 to exclude constraints that are similar to the instruction and other constraints. Finally, we randomly sample and manually review 50 instruction-response pairs and their generated constraints, finding minimal noise and high compliance between constraints and response. Constraint Combination Finally, we combine individual constraints to form the final constraint for each instruction. Previous studies have shown that increasing the number of constraints in the training data leads to better model performance (He et al., 2024). Therefore, we enhance each instruction with combination of multiple constraints. 3 Specifically, we randomly sample 6 to 8 constraints from each instructions constraint set generated in the previous step, shuffle their order, and recombine them into the final constraint c. Similar to previous work (Sun et al., 2024; Qi et al., 2024), we add 1 to 3 in-context demonstrations for 50% of the data. Finally, we construct CRAB with 13, 500 instances, containing an average of 7.1 constraints."
        },
        {
            "title": "2.2 Model Training",
            "content": "To further enhance LLMs understanding of complex constraints, we propose reverse training method that takes the instruction-response pair (x, y) as input to teach LLMs to generate the constraints c. The intuition is that correctly generating constraints requires sufficient comprehension first. Formally, the reverse training objective is to minimize Lr, where Lr = log Pθ(cx, y) and the LLM is parameterized by θ. We also adopt the standard supervised fine-tuning (SFT; Ouyang et al., 2022), named forward training, to minimize Lf, where Lf = log Pθ(yx, c). The final training objective is combination of Lf and Lr: = αLf + (1 α)Lr. We set α to 0 during 70% of the training process, and to 1 for the remaining time. We train the LLM using mixture of ShareGPT (Chiang et al., 2023) and CRAB. We adopt Lf when training on ShareGPT (Chiang et al., 2023) and adopt on CRAB. We train base LLM on this data and obtain the SFT version of our model. Based on the SFT trained model, we continue training using the Direct Preference Optimization objective (DPO; Rafailov et al., 2023). Specifically, same as the DPO phrase by Sun et al. (2024), we use the high-quality DPO dataset UltraFeedback (Cui et al., 2023) to conduct further training and obtain the DPO version of our model. More training details are presented in appendix B."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we introduce the experimental setup ( 3.1), experimental results ( 3.2), and further analyses on our model ( 3.3 to 3.5)."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Backbone Models We adopt two widely-used open-source base models, Mistral 7B (Jiang et al., 2023) and Llama 3 8B (Dubey et al., 2024), as our backbone models for developing Llama3CRAB and MistralCRAB. Specifically, we employ Mistral-7B-v0.3 and Meta-Llama-3-8B, downloaded from Hugging Face (Wolf et al., 2019). During the SFT stage, we adopt 5 106 learning rate, 256 batch size, and train the Mistral for 4 epochs and Llama 3 for 3 epochs. During the DPO optimization stage, we adopt 5 107 learning rate, 64 batch size, and 1 training epoch. Baselines Our baselines include popular opensource and proprietary LLMs, divided into three main categories for comparison: (1) Proprietary LLMs, including GPT-3.5 (OpenAI, 2022) and GPT-4 (OpenAI, 2024). (2) General instructiontuning LLMs, including Vicuna-V1.5 13B (Chiang et al., 2023), trained on the 125k ShareGPT dataset, WizardLM-V1.2 13B (Xu et al., 2023), trained on the 196k Evol-Instruct dataset, and Zephyr beta 7B (Tunstall et al., 2023), trained with the UltraFeedback (Cui et al., 2023) dataset using the DPO objective (Rafailov et al., 2023), which achieves leading performance on chat benchmarks based on Mistral 7B. (3) Models specifically optimized for complex instruction-following tasks. We use the Conifer series (Sun et al., 2024), which first generates constraints based on seed instructions and then generates corresponding responses based on instructions and constraints using GPT-4, and achieves the state-of-the-art performance in complex instruction-following at the 7B model scale. Evaluation Datasets We use two widely-used and challenging complex instruction-following datasets IFEval (Zhou et al., 2023) and FollowBench (Jiang et al., 2024) for evaluation. IFEval consists of 541 instructions that can be automatically validated using Python scripts. Each instruction contains 1 to 3 constraints, primarily focusing on strict lexical and formatting constraints. FollowBench is fine-grained, multi-constraint instruction-following benchmark and it categorizes the difficulty into five levels (L1 to L5) based on the number of constraints of an instruction, where L1 represents the simplest level with only one constraint, while L5 is the most difficult, with combination of five constraints. It also includes five constraint categories, including content, situation, style, format, and example, along with mixed constraint category that combines various categories of constraints. FollowBench contains total of 820 instructions across more than 50 different NLP tasks, and it is automatically evaluated using either Python scripts or GPT-4. Please refer to the original paper for more details (Jiang et al., 2024)."
        },
        {
            "title": "Backbone",
            "content": "GPT-3.5* GPT-"
        },
        {
            "title": "GPT\nGPT",
            "content": "Vicuna-v1.5-13B Llama2 WizardLM-v1.2-13B Llama2 ConiferSFT-13B Llama2 Zephyr-beta-7B Mistral ConiferSFT-7B Mistral ConiferDPO-7B Mistral Llama3 8B Llama3CRAB Llama3CRAB + DPO Mistral 7B MistralCRAB MistralCRAB + DPO Llama3 Llama3 Llama3 Mistral Mistral Mistral"
        },
        {
            "title": "IFEval",
            "content": "[L]P 64.0 79.3 46.6 48.4 47.5 44.9 50.8 52.3 28.1 43.8 47.7 19.6 51.6 57.7 [S]I 68.5 83. 53.6 54.4 53.0 46.8 57.1 59.1 36.8 50.2 52.0 30.8 57.3 61.5 [S]P 59.0 76.9 43.1 43.6 42.9 32.0 45.8 48.1 25.7 39.4 40.3 18.5 47.9 49. FollowBench (HSR) [L]I AVG 73.6 85.4 58.0 59.1 57.4 58.0 62.0 63.3 35.1 54.2 58.9 31.9 61.2 68.5 66.3 81. 50.3 51.4 50.2 45.4 53.9 55.7 31.4 46.9 49.7 25.2 54.5 59.3 L1 80.3 84.7 71.2 61.3 60.5 57.6 54.3 60.3 4.8 57.5 64.6 14.3 63.9 66. L2 68.0 76.1 61.3 51.6 53.6 51.9 49.5 53.6 8.7 44.9 49.0 16.6 54.4 53.6 L3 68.6 71. 48.3 43.3 48.4 41.9 49.3 48.0 8.8 34.9 41.6 8.3 40.1 53.4 L4 61.1 74.5 38.0 37.5 40.7 41.4 40.8 47.1 6.0 25.2 35.8 5.8 30.4 42. L5 AVG 53.2 62.4 33.1 29.9 31.7 31.4 30.5 41.0 9.8 20.0 36.8 5.5 27.9 31.7 66.2 73.8 50.4 44.7 47.0 44.8 44.9 50. 7.6 36.5 45.5 10.1 43.3 49."
        },
        {
            "title": "AVG",
            "content": "66.3 77.6 50.4 48.1 48.6 45.1 49.4 52.9 19.5 41.7 47.6 17.7 48.9 54.4 Table 1: Experimental results (%) of the LLMs on IFEval and FollowBench. In IFEval, [S] and [L] denote strict and loose accuracy, and indicate the prompt and instruction level. In FollowBench, L1 (simplest) to L5 (hardest) denote different difficulty levels. We highlight the highest and second-highest scores of open-source LLMs using bold font and underline. and * means the results are from Sun et al. (2024) and He et al. (2024)."
        },
        {
            "title": "LC WinRate WinRate",
            "content": "GPT-3.5-turbo-0613 GPT-4-0613 WizardLM-70B WizardLM-v1.2-13B Vicuna-v1.5-13B Zephyr-beta-7B ConiferDPO-7B MistralCRAB MistralCRAB + DPO (vs.) ConiferDPO 22.4 30.2 17.6 14.5 10. 13.2 17.1 13.3 18.1 60.6 14.1 15.8 14.4 12.0 6.7 11.0 11.3 7.9 17.6 63.5 Table 2: Winning rate (%) of the investigated LLMs on Alpaca-Eval 2.0 (Li et al., 2023b). LC denotes length-controlled (Dubois et al., 2024). means the results are sourced from the original leaderboard."
        },
        {
            "title": "3.2 Experimental Results",
            "content": "The experimental results are presented in Table 1. Our observations are as follows: (1) After training on the CRAB dataset, our models significantly outperform the corresponding base models and the open-source models trained through SFT on general instruction-following datasets. Our DPO version of models achieves the best performance among the compared models. It demonstrates the effectiveness of our data and training approach. (2) Our models surpass Conifer Sun et al. (2024), which is specifically trained for complex instruction-following, on IFEval. It suggests that our model performs better in following lexical and format constraints. However, our models slightly lag behind Conifer on FollowBench. We provide an in-depth discussion on the performance across different constraint categories in FollowBench in 3.5. We observe that the performance decline is primarily due to the style constraint, where our models significantly underperform in this constraint category compared to Conifer. Nonetheless, Our models achieve significant improvements in real-world scenarios, i.e., the mixed constraint in FollowBench (Jiang et al., 2024), compared to Conifer. (3) Training with the DPO objective consistently improves model performance on both evaluation datasets. In this paper, we focus on constructing high-quality SFT data by constraint backtranslation, we leave the development of DPO data for complex instruction-following as future work."
        },
        {
            "title": "3.3 Analysis on General Instruction Following",
            "content": "The complex instruction-following ability not only involves following complex constraints but also encompasses the basic ability to follow instructions themselves, e.g., Write blog on French cuisine, named as general instruction following. In this section, we further evaluate our models general instruction-following capability. Given that IFEval and FollowBench primarily focus on evaluating the ability to follow constraints, we adopt another widely-used dataset, AlpacaEval (Li et al., 2023b), which serves as an easy-to-use and high-quality automatic evaluator for instructionfollowing ability. Specifically, we use AlpacaEval 2.0, which contains 805 instructions, and use gpt-4-1106-preview as the evaluator to get the final weighted win rate. The evaluation results are presented in Table 2, where the LC WinRate 5 Figure 3: An example of responses generated with and without constraints by Llama3-70B-Instruct. The evaluator is gpt-4o-0806. For better visualization, we present only subset of the responses generated without constraints. contrast, Conifer uses instruction and corresponding constraints to generate the response. We hypothesize that possible reason is that the response quality in Conifer is lower than CRAB, that is, generating response conditioned on both instruction and constraints may result in lower content quality, such as lower coherence, compared to the response directly generated from instruction without constraints. Intuitively, incorporating constraints may limit the models capacity when generating responses. To validate this intuition, we conduct controlled analytical experiment. Specifically, we sample 100 instructions and their corresponding constraints from IFEval and FollowBench. We first use Llama3-70B-Instruct to generate responses based only on the instructions (w/o constraints). Then, we include the additional constraints and generate corresponding responses (w/ constraints). Following previous work on automated evaluation using advanced LLMs (Bai et al., 2024b; Chan et al., 2024), we employ gpt-4o-0806 as the evaluator, assessing the responses on four dimensions: engagingness, understandability, fluency, and coherence, with scores ranging from 1 to 5. We report the full-mark (score 5) rate for each dimension. The results are shown in Figure 4. We can observe that responses generated with constraints significantly underperform those generated without constraints, which suggests that involving constraints when generation may reduce the content quality of the final response. We further conduct case study, illustrated in Figure 3, showing an example of responses generated with and without constraints. We can Figure 4: Full-mark rates (%) of the responses generated with and without constraints. The evaluator is gpt-4o-0806, focusing on four widely-used dimensions: Engagingness (Eng.), Understandability (Und.), Fluency (Flu.), and Coherence (Coh.). represents the length-controlled win rate (Dubois et al., 2024). The default reference model is gpt-4-1106-preview. We can observe that our model significantly outperforms the baseline model Conifer and even exceeds the performance of the 70B version of WizardLM. We also conduct head-to-head comparison between our model and Conifer. The LC win rate of our model reaches 60.6, significantly outperforming Conifer, which demonstrates that our model possesses superior general instruction-following capability. We conduct further analysis to explore the potential reasons why our model outperforms Conifer in general instruction-following. The primary difference between our model and Conifer is the data construction process. We utilize constraint backtranslation, where the response is generated directly from the instruction without constraints. In 6 Model MistralCRAB (-) Reverse training (-) Forward training (-) In-Context Demons InstBackTSFT IFEval FollowBench AVG L1-L2 L3-L5 54. 52.1 53.9 53.6 52.7 59.1 56.2 57.1 55.8 55.4 32. 33.5 32.1 30.0 29.3 AVG 48.9 47.3 48.0 47.0 46. Table 3: Experimental results (%) of the ablation study. In-Context Demons denotes in-context demonstrations. find that when involving constraints, the response includes vague terms, such as [database name], and lacks sufficient details and depth. While previous work on complex instruction-following mainly focuses on enhancing the ability to follow multiple constraints, we encourage future work to prioritize response content quality, and constraint backtranslation can serve as potential solution. Assessment details of this analysis are in appendix C."
        },
        {
            "title": "3.4 Ablation Study",
            "content": "We conduct an ablation study to analyze the key factors influencing model performance. Specifically, we investigate three key factors in developing our model: reverse training, forward training, i.e., standard supervised fine-tuning, and in-context demonstrations. We exclude each factor and keep all other conditions identical, to the model separately. When excluding reverse and forward training, we set the loss ratio α in 2.2 to 1 and 0, respectively. The backbone model is Mistral. The results are presented in Table 3, where L1-L2 in FollowBench represent simpler constraints and L3-L5 denote more complex constraints. We can observe that removing any of these factors leads to decline in model performance, which demonstrates the effectiveness of these factors in developing our model. For more complex constraints following, adding incontext demonstrations during training is effective, as excluding in-context demonstrations leads to significant performance drop in L3-L5. The reason may be that in-context demonstrations enhance the models ability to understand multiple in-context instructions and complex constraints. We further compare with competitive baseline model, InstBackTSFT, which is trained on the data generated by instruction back-translation (Li et al., 2024). The key difference between instruction and constraint back-translation is that the former uses advanced LLMs to generate both instructions and constraints from responses, while the latter focuses Figure 5: Experimental results on different categories of constraints in FollowBench of MistralCRAB and ConiferSFT. on generating constraints from instruction and response pairs. The results are presented in Table 3. We can observe that InstBackTSFT significantly underperforms compared to MistralCRAB, which suggests that instruction back-translation may produce lower-quality data for complex instruction following. The possible reason is that generating both instructions and constraints simultaneously is more challenging than generating constraints alone. It further demonstrates the efficacy of the constraint back-translation method in creating high-quality training data for complex instruction following."
        },
        {
            "title": "3.5 Analysis on Constraint Category",
            "content": "We further investigate our models performance across different constraint categories to analyze its strengths and potential limitations. Specifically, we analyze the results on FollowBench, which includes five categories of constraints, including example, content, situation, style, and format. Please refer to the original paper (Jiang et al., 2024) for the detailed definitions for each constraint category. FollowBench also includes mixed category which is designed for simulating real-world scenarios (Jiang et al., 2024), where various types of constraints are combined to form the final constraint. We compare our model MistralCRAB with the Conifer model, which is trained on the data generated using the standard pipeline: generating the constraints first and then generating the response based on the instruction and constraints. The results on different constraint categories of FollowBench are shown in Figure 5. We can observe that our model significantly outperforms Conifer on the mixed constraint, which represents real-world scenarios, suggesting that our model is more effective in handling complex instruction-following scenarios. However, in the style constraint category, e.g., Write in the style of Shakespeare, our model per7 forms significantly worse than Conifer. The possible reason is that the style constraints in our dataset CRAB may be not sufficiently diverse. The data for style constraints requires deliberate construction, and the pipeline that generates constraints first and then responses is more effective at generating diverse style constraints, but the responses in our seed data have limited style diversity. It suggests limitation of constraint back-translation, as it relies on diverse responses to generate specific categories of constraints, such as style constraint. Combining the constraint back-translation method with other data generation methods to produce higher-quality data for those specific constraints can further enhance the models complex instruction-following ability, and we leave this exploration as future work."
        },
        {
            "title": "4 Related Work",
            "content": "4."
        },
        {
            "title": "Instruction Following",
            "content": "Instruction following involves following user intentions to generate helpful responses, which is fundamental to modern LLMs (Zhang et al., 2023). Ouyang et al. (2022) first propose the practice of aligning LLMs to follow human instructions, using SFT and RLHF to train models, which is the key factor in the success of ChatGPT (OpenAI, 2022). Subsequently, numerous studies focus on enhancing the instruction-following capabilities of LLMs, particularly for open-source models, which can be summarized in two main aspects: (1) data-driven approaches, which design an automated pipeline or use human annotation to produce high-quality training data (Xu et al., 2023; Taori et al., 2023; Chiang et al., 2023; Peng et al., 2023; Kim et al., 2023; Cui et al., 2023; Mukherjee et al., 2023; Ivison et al., 2023; Köpf et al., 2024; Qi et al., 2024; Liu et al., 2024; Li et al., 2024; Bai et al., 2024a). (2) new training methods, including novel objectives (Rafailov et al., 2023; Gallego, 2024; Zhou et al., 2024; Hejna and Sadigh, 2024; Meng et al., 2024) or training pipelines (Tunstall et al., 2023; Li et al., 2024; Yuan et al., 2024; Chen et al., 2024). more challenging instruction following scenario is complex constrained instruction following, where the responses should further satisfy specific constraints, such as length. Previous studies have shown that LLMs struggle to follow these instructions (Jiang et al., 2024; Qin et al., 2024). Recent efforts focus on enhancing this ability by constructing high-quality training data (Sun et al., 2024; He et al., 2024). This process typically involves collecting set of seed instructions, constructing constraints, and then generating responses based on the instructions and constraints using advanced LLMs. In this work, we introduce constraint backtranslation, which directly generates constraints from existing instruction and response pairs, which reduces the generation costs and noise."
        },
        {
            "title": "4.2 Back-translation",
            "content": "Back-translation is first proposed in the field of machine translation (Sennrich, 2015; Hoang et al., 2018), which mainly is used for data augmentation. It first trains model to back-translate the target language into the source language, then uses this model to generate parallel training data from large amount of monolingual target language data, which sufficiently saves human translation efforts. Considering its simplicity and efficacy, back-translation has also been widely applied to various tasks, such as style transfer (Prabhumoye et al., 2018; Toshevska and Gievska, 2021) and paraphrase generation (Wieting et al., 2017; Mallinson et al., 2017). Recently, several studies have explored applying back-translation to the field of large language models to efficiently generate high-quality data automatically (Li et al., 2023a; Pham et al., 2024; Köksal et al., 2023). Li et al. (2023a) proposed reversing the training objective to automatically generate corresponding instructions for existing unsupervised corpora, while Pham et al. (2024) and Köksal et al. (2023) leveraged the powerful general capabilities of LLMs to generate instructions from the corpus directly. These works usually focus on instruction back-translation, which generate instructions from existing text corpora. In this work, we propose constraint back-translation, which generates high-quality constraints based on instructions and responses. It can effectively generate constraints without altering the original responses, and reduces data noise and the cost of data generation."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we aim to enhance large language models capability for complex constrained instruction following. We propose constraint backtranslation data generation method, which can reduce data noise and generation costs, resulting in high-quality complex instruction-following dataset CRAB. We also propose reverse training method and develop Llama3CRAB and MistralCRAB based on CRAB. Extensive experiments demonstrate the 8 effectiveness of our data generation and training methods. We further conduct extensive analytical experiments and discuss the key factors, advantages, and potential limitations of our model."
        },
        {
            "title": "Limitations",
            "content": "As discussed in 3.5, for certain types of constraints, such as style constraint, the constraints generated through constraint back-translation may lack sufficient diversity if the original response data itself is not diverse enough. We leave further improvements to constraint back-translation as future work. Another limitation of our study is that we do not use larger base model due to computational constraints. We believe that using larger base model could develop more advanced LLM in following complex constraints, but this does not affect our overall experimental conclusions."
        },
        {
            "title": "Ethical Considerations",
            "content": "We discuss potential ethical concerns related to this work: (1) Intellectual property. Our research leverages several widely used SFT datasets, and we strictly comply with their licensing terms. We will share CRAB under the CC BY-SA 4.0 license2. (2) Intended use and Potential risk control. The goal of this paper is to introduce CRAB, designed to enhance the performance of LLMs on complex instruction tasks. CRAB is built using widely available public datasets. We trust that the original publishers have anonymized and sanitized these datasets appropriately. Additionally, we randomly sampled 100 instances and found no sensitive information. (3) AI assistance. We used GPT-4 to paraphrase some sentences and check grammar."
        },
        {
            "title": "References",
            "content": "Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. 2024a. Longalign: recipe for long context alignment of large language models. arXiv preprint arXiv:2401.18058. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. 2024b. Benchmarking foundation models with language-model-as-an-examiner. Advances in NeurlPS, 36. Ricardo Campos, Vítor Mangaravite, Arian Pasquali, Alípio Jorge, Célia Nunes, and Adam Jatowt. 2020. Yake! keyword extraction from single documents 2https://creativecommons.org/licenses/by-sa/4. 0/ using multiple local features. Information Sciences, 509:257289. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2024. Chateval: Towards better llm-based evaluators through multi-agent debate. In ICLR. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. 2024. Self-play fine-tuning converts weak language models to strong language models. In ICML. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475. Shahul Es. 2023. Orca-chat: high-quality explanation-style chat dataset. Víctor Gallego. 2024. Refined direct preference optimization with synthetic data for behavioral alignment of llms. arXiv preprint arXiv:2402.08005. Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, and Sainbayar Sukhbaatar. 2024. Reverse trainarXiv preprint ing to nurse the reversal curse. arXiv:2403.13799. Qianyu He, Jie Zeng, Qianxi He, Jiaqing Liang, and Yanghua Xiao. 2024. From complex to simple: Enhancing multi-constraint complex instruction following ability of large language models. Preprint, arXiv:2404.15846. Joey Hejna and Dorsa Sadigh. 2024. Inverse preference learning: Preference-based rl without reward function. Advances in Neural Information Processing Systems, 36. Cong Duy Vu Hoang, Philipp Koehn, Gholamreza Haffari, and Trevor Cohn. 2018. Iterative backtranslation for neural machine translation. In 2nd Workshop on Neural Machine Translation and Generation, pages 1824. ACL. 9 Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah Smith, Iz Beltagy, et al. 2023. Camels in changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. 2024. FollowBench: multi-level fine-grained constraints following benchmark for large language models. In Proceedings of ACL, pages 46674688. Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Yoo, and Minjoon Seo. 2023. Aligning large language models through synthetic feedback. In Proceedings of EMNLP, pages 1367713700. Abdullatif Köksal, Timo Schick, Anna Korhonen, and Hinrich Schütze. 2023. Longform: Effective instruction tuning with reverse instructions. arXiv preprint arXiv:2304.08460. Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. 2024. Openassistant conversations-democratizing large language model alignment. Advances in NeurlPS, 36. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis. 2023a. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis. 2024. Self-alignment with instruction backtranslation. In ICLR. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023b. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval. Chin-Yew Lin. 2004. Rouge: package for automatic In Text summarization evaluation of summaries. branches out, pages 7481. Yantao Liu, Zhao Zhang, Zijun Yao, Shulin Cao, Lei Hou, and Juanzi Li. 2024. Aligning teacher with student preferences for tailored training data generation. arXiv preprint arXiv:2406.19227. Edward Loper and Steven Bird. 2002. Nltk: The natural language toolkit. arXiv preprint cs/0205028. Jonathan Mallinson, Rico Sennrich, and Mirella Lapata. 2017. Paraphrasing revisited with neural machine translation. In Proceedings of ACL, pages 881893. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization arXiv preprint 2024. with reference-free reward. arXiv:2405.14734. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707. OpenAI. 2022. Introducing ChatGPT. OpenAI. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in NeurlPS, 35:2773027744. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277. Chau Minh Pham, Simeng Sun, and Mohit Iyyer. 2024. Suri: Multi-constraint instruction following for long-form text generation. arXiv preprint arXiv:2406.19371. Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan SalakhutStyle transarXiv preprint dinov, and Alan Black. 2018. fer through back-translation. arXiv:1804.09000. Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, and Juanzi Li. 2024. Adelie: Aligning large language models on information extraction. arXiv preprint arXiv:2405.05008. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Advances in NeurIPs. Rico Sennrich. 2015. Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709. Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, and Ruohui Huang. 2024. Conifer: Improving complex constrained instructionfollowing ability of large language models. arxiv preprint arXiv:2404.02823. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models., 3(6):7. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911. Zhanhui Zhou, Jie Liu, Jing Shao, Xiangyu Yue, Chao Yang, Wanli Ouyang, and Yu Qiao. 2024. Beyond one-preference-fits-all alignment: Multi-objective direct preference optimization. In Findings of ACL, pages 1058610613. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Martina Toshevska and Sonja Gievska. 2021. review of text style transfer using deep learning. IEEE Transactions on Artificial Intelligence, 3(5):669684. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. 2023. Zephyr: Direct distillation of lm alignment. Preprint, arXiv:2310.16944. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners. In Proceedings of ICLR. John Wieting, Jonathan Mallinson, and Kevin Gimpel. 2017. Learning paraphrastic sentence embeddings from back-translated bitext. arXiv preprint arXiv:1706.01847. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingfaces transformers: State-of-theart natural language processing. arxiv. arXiv preprint arXiv:1910.03771. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. In ICML. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792."
        },
        {
            "title": "A Data Collection",
            "content": "In this section, we provide detailed explanation of our data construction process, divided into two parts: the details of constraint construction (appendix A.1) and the data distribution of CRAB (appendix A.2). A.1 Details of Constraints Construction Table 4 presents all 19 types of constraints we defined. It is important to note that for the \"Define circumstances\", clarifying the subject or object, or defining the circumstances under which the instruction applies, we observed that generating this constraint independently often results in this additional constraint being too similar to the original instruction. Therefore, we integrate it directly with the original interaction to develop refined instruction. If selected during the combination process, instead of being added to the instruction like other constraints, it replaces the original instruction. Since not all constraints are applicable to every output text (e.g., Special Output Format refers to the special format of the output, such as Python, tables, JSON, HTML, or LaTeX), we apply weighted sampling during the combination process to ensure more balanced sampling of all constraints. Among the constraints calculated using Python scripts, two categories are particularly unique: (1) Number-related categories: such as Length and Words Per Sentence, where we used NLTK (Loper and Bird, 2002) for calculation. (2) Keyword: We applied the lightweight, unsupervised keyword extraction method Yake (Campos et al., 2020) to extract the top 3 most significant keywords from the output text. A.2 Dataset Distrubution Figure 6 shows the distribution of 13, 500 instances in the CRAB. The left chart categorizes data by the number of constraints after combination, while the right chart categorizes data by the source dataset. To enhance data diversity during the combination stage, we randomly introduced 25% of data with constraint count outside the 68 range, with the maximum number of constraints being 14."
        },
        {
            "title": "B Model Training",
            "content": "For model training, we utilize the repository The Alignment Handbook (Tunstall et al., 2023) to Figure 6: Proportion (%) of data in the CRAB by the number of constraints and the source dataset. train Mistral 7B based models, and use OpenInstruct (Ivison et al., 2023) to train LLaMA 3 8B based models. The implementation of the ratio α between reverse training and forward training is achieved by segregating the dataset into two parts, since the model may memorize the data during the forward process. All experiments in the paper are done using 8 NVIDIA A100 80GB GPUs. We adopt DeepSpeed ZeRO stage 2 for SFT and DPO training. The training of Mistral took approximately 48 GPU hours in total, while the training of LLaMA3 took around 72 GPU hours. In the SFT stage, we set the learning rate to 5 106, with per-device batch size of 4 and 8 gradient accumulation steps. The warm-up ratio is set to 0.1. The Mistral-7B experiments are trained for 4 epochs with maximum sequence length of 2048, while the LLaMA 3 8B experiments are trained for 3 epochs with maximum sequence length of 4096. For DPO training, the learning rate is set to 5 107, with per-device batch size of 4, 2 gradient accumulation steps, and maximum sequence length of 2048. The beta value for Mistral 7B experiments is set to 0.01, trained for 1 epoch with cosine learning rate schedule, while for LLaMA 3 8B, the beta is 0.1, trained for 3 epochs with linear learning rate scheduler following the setting in Ivison et al. (2023)."
        },
        {
            "title": "C Details on the impact of constraints on",
            "content": "output quality To explore the impact of constraints on output quality, we sampled 100 instruction pairs from FollowBench and IFEval, with each pair consisting of instruction without constraints and its corresponding multi-constraint version (with over 3 constraints). Since IFEval does not provide instruction without constraints, we randomly selected 50 instances and manually removed the constraints. For FollowBench, we selected level 0 instructions along 12 consistency of ideas, ensuring the texts structure is logical and ideas are connected. Components: Logical Flow (clear progression of ideas), Transitions (smooth movement between topics or sentences), and Consistency (absence of contradictions or disjointed thoughts)."
        },
        {
            "title": "Generator Weighted",
            "content": "Define Circumstances Desired Writing Style Semantic Elements Morphological Constraints Multi-lingual Constraints Specific Literary Devices Specific Grammatical Structure Hierarchical Instructions Special Output Format Paragraphs Constraints Specific Sentence Keyword Formatting Item Listing Details Length Constraint Words Per Sentence Sentences Per Paragraph Characters Per Word Keyword Constraint Punctuation Limitation"
        },
        {
            "title": "LLM\nLLM\nLLM\nLLM\nLLM\nLLM\nLLM\nLLM\nLLM\nLLM\nLLM\nLLM\nLLM\nPython\nPython\nPython\nPython\nPython\nPython",
            "content": "0.7 0.7 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 1.0 1.0 0.5 0.5 0.3 0.3 0.5 0.3 Table 4: Constraint types defined during the backtranslation process. \"Weighted\" indicates the sampling probability of each constraint type during combination. with their corresponding level 5 counterparts. To ensure fair comparison, we only retained instruction pairs where the core meaning of the instruction pairs remained consistent, such that the output generated from the complex instructions would still satisfy the simple versions."
        },
        {
            "title": "We evaluate the quality of model output along",
            "content": "the following four dimensions. Engagingness: Evaluate how captivating and interesting the text is, based on its ability to hold attention and evoke interest. Components: Interest (ability to sustain attention), Appeal (suitability for the audience), and Emotional/Intellectual Impact. Understandability: Evaluate the clarity and ease with which the text can be understood by the target audience. Components: Simplicity (absence of unnecessary complexity), Accessibility (use of language suitable for the audience), and Clarity. Fluency: Evaluate the smoothness of the writing, focusing on grammar, sentence structure, and the natural flow of language. Components: Grammar (correct use of language rules), Sentence Structure (variety and complexity), and Naturalness (how easily the text flows). Coherence: Evaluate the logical flow and 13 Instruction What is an easy Perm PLL algorithm that wouldnt be too complicated for beginner at speed solving? Output One easy Perm PLL algorithm that is not too complicated for beginner at speed solving is the following: R R R J U2 This algorithm only has 10 moves and does not require any complicated finger tricks. Here is breakdown of the algorithm: 1. R - This is common setup move for Perm PLL algorithms. It sets up the shape in the corner of the cube. 2. U U - This is the Perm algorithm itself. It swaps the two adjacent edges and the two adjacent corners in the shape. 3. U2 - This is the inverse of the setup move and completes the PLL algorithm. Remember to practice this algorithm slowly at first, and then gradually increase your speed. With enough practice, you should be able to execute this algorithm quickly and efficiently."
        },
        {
            "title": "Additional Constraints",
            "content": "Define Circumstance: What is simple, 10-move or fewer Perm PLL algorithm for speed solving that beginner can learn and execute efficiently, without requiring advanced finger tricks or complex sequences? Length Constraint: Submit an response that contains at least 160 words. Word Per Sentence: Restrict each sentence to 25 words maximum. Characters Per Word: Each word in the response must consist of at least 15 characters. Keyword Constraint: In your reply, include the terms complicated finger tricks, Perm PLL, Perm PLL algorithmto emphasize the critical components and provide clarity. Punctuation Limitation: Avoid using question marks in your response. Desired Writing Style: The response should be written in clear and concise manner, with tone that is informative and helpful, suitable for beginner in speed solving. Semantic Elements: The main theme of the response should focus on providing simple Perm PLL algorithm for beginners, highlighting its ease of use and simplicity. Morphological Constraints: Avoid using overly technical jargon or complex mathematical notation, and refrain from using formatting styles that may confuse the reader. Multi-lingual Constraints: The response should be written in English. Specific Literary Devices: Use step-by-step breakdown of the algorithm to enhance clarity and facilitate understanding. Specific Grammatical Structure: The response should be written primarily in simple sentences, with occasional use of compound sentences to provide additional explanations. Hierarchical Instructions: The response should prioritize explaining the algorithm, followed by breakdown of the algorithm, and finally providing practice tips. Paragraphs Constraints: The response should consist of three paragraphs, with blank line separating each paragraph. Specific Sentence: The response should start with sentence that introduces the algorithm, and end with sentence that encourages practice. Item Listing Details: The breakdown of the algorithm should use numbers (1., 2., 3., etc.) to list the steps, with hyphen and space before each step description. Table 5: An example from OpenAssistant of CRAB after constraint back-translation and before combination."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Technology, BNRist, Tsinghua University"
    ]
}