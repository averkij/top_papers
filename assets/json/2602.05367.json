{
    "paper_title": "RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs",
    "authors": [
        "Youngcheon You",
        "Banseok Lee",
        "Minseop Choi",
        "Seonyoung Kim",
        "Hyochan Chong",
        "Changdong Kim",
        "Youngmin Kim",
        "Dongkyu Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary ($\\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a $4.49\\times$ inference speed-up over full-precision models on an RTX 4090."
        },
        {
            "title": "Start",
            "content": "RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs Youngcheon You * 1 Banseok Lee * 1 Minseop Choi 1 Seonyoung Kim 1 Hyochan Chong 1 Changdong Kim 1 Youngmin Kim 1 Dongkyu Kim 1 6 2 0 2 5 ] A . [ 1 7 6 3 5 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Efficient deployment of large language models (LLMs) requires extreme quantization, forcing critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary (1) layers, but is plagued by pathological feature co-adaptation. We identify key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, novel quantization framework that resolves co-adaptation by algoIts rithmically enforcing residual hierarchy. core mechanism sequentially derives each binary path from single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers 4.49 inference speed-up over full-precision models on an RTX 4090. 1. Introduction Model compression is essential for efficient deployment of large language models (LLMs). While 4-bit quantization methods (Frantar et al., 2022; Lin et al., 2024) have emerged as successful industry standard (Kwon et al., 2023; Zheng *Equal contribution 1Samsung Research, Seoul, Korea. Correspondence to: Dongkyu Kim <dongkyu.k@samsung.com>, Youngmin Kim <ym1012.kim@samsung.com>. Preprint. February 6, 2026. et al., 2024), the relentless pursuit of greater efficiency is pushing the research frontier towards the 2-bit regime. This transition to lower bit compression introduces architectural trade-offs that characterize the current domain. At the 2-bit frontier, two primary strategies present choice between representational fidelity and hardware efficiency. Vector Quantization (VQ) methods typically yield high accuracy but incur hardware overhead due to lookup tables or complex rotations (Tseng et al., 2024a;b; Egiazarian et al., 2024). Conversely, residual binarizationstacking multiple binary layersfacilitates exceptional, matmul-free efficiency. However, this approach has struggled to preserve performance, due to fundamental training instabilities that prevent full potential realization (Bulat et al., 2024; Wang et al., 2024a; Tran & Nguyen, 2025). The core promise of residual architecturethat subsequent paths compensate for the errors of preceding onesis fundamentally undermined by feature co-adaptation (Hinton et al., 2012), pathological training dynamic where parallel components learn redundant features. In residual binarization, we identify critical manifestation of this phenomenon, which we term inter-path adaptation. During standard quantization-aware training (QAT) (Bengio et al., 2013; Hubara et al., 2018), structurally agnostic global gradient is simultaneously applied to all paths. This forces each path to learn redundant features in race to minimize the global objective, overriding their intended compensatory roles. The result is breakdown of the residual hierarchy that severely limits the models expressive power. Previous efforts have employed heuristic constraints, such as path freezing (Bulat et al., 2024; Tran & Nguyen, 2025), which inherently limit the models capacity to derive an optimal joint solution. To address this, we propose ResidualAware Binarization Training (RaBiT), QAT framework that resolves inter-path adaptation by design, as depicted in Figure 1. Instead of using independent latent weights, RaBiT maintains single shared full-precision weight from which binary paths are sequentially derived on-the-fly, guided by learnable scales. This algorithmically enforces residual hierarchy, training each path to correct RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs (a) (b) (c) Figure 1. Overview of the RaBiT training framework. (a) Dynamic Compute Process: During training, binary paths are dynamically derived from shared weight WFP to enforce residual hierarchy. (b) Forward Pass: For inference, these paths execute in parallel for matmul-free efficiency. (c) Backward Pass: Gradients from the loss update both the learnable scales (gi, hi) and the shared WFP. its predecessors error. Combined with robust, functionaware initialization, RaBiT achieves state-of-the-art accuracy while delivering up to 4.49 inference speed-up and halving the training memory footprint. Our contributions can be summarized as follows: We identify and analyze inter-path adaptation, critical manifestation of feature co-adaptation in residual binarization, where the intended error-compensation structure breaks down during Standard QAT, as parallel paths become functionally redundant. We propose RaBiT, novel QAT framework that resolves inter-path adaptation by enforcing residual coupling onthe-fly. The mechanism inherently halves the training memory footprint and is stabilized by robust functionaware initialization strategy to directly address the unstable dynamics of extreme QAT. We demonstrate that RaBiT achieves state-of-the-art accuracy at 2-bit precision, delivering up to 4.49 inference speed-up while maintaining competitive performance against hardware-intensive VQ methods through matmul-free operations. 2. Related Works The Shift to QAT in Extreme Quantization. PostTraining Quantization (PTQ) methods, such as GPTQ (Frantar et al., 2022) and AWQ (Lin et al., 2024), have proven to be highly successful for compressing large language models to 3or 4-bit precision by focusing on weight approximation. However, these methods encounter substantial performance degradation at lower bit-widths (e.g., 2-bit) (Wang et al., 2023; Huang et al., 2024; Li et al., 2024), as the information loss inherent to coarse quantization cannot be mitigated solely by weight approximation. Consequently, more recent works target preserving global model functionality (Liu et al., 2025) via quantization-aware training (QAT) (Hubara et al., 2018; Krishnamoorthi, 2018). QAT integrates low-precision arithmetic simulations into the finetuning process, enabling parameter adaptation to the target bit-width constraints. Although the non-differentiable nature of quantization presents challengestypically managed via the Straight-Through Estimator (STE) (Bengio et al., 2013)modern frameworks for binary models have achieved stability by updating latent full-precision weight via surrogate gradients (Wang et al., 2023; Xu et al., 2024; Jo et al., 2024; Lee et al., 2025). This work builds upon this methodology to address the specific complexities of residual binary architectures. Co-adaptation in Residual Binary Architectures. To enhance the limited expressive capacity of single low-bit layers, residual binarization stacks multiple low-bit paths (W ˆWi), thereby achieving higher effective precision while retaining matmul-free efficiency (Wang et al., 2024a). Nevertheless, this parallel structure is prone to feature co-adaptation (Hinton et al., 2012), where components learn redundant representations. This phenomenon previously motivated regularization techniques, such as Dropout (Srivastava et al., 2014). 2 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs Table 1. Detailed Decomposition of MSE Loss across Representative Layers of Llama2-7B. The table breaks down the total MSE into its core components. While the base error (C ) and path amplitude (2σ1σ2) are comparable, RaBiT consistently generates strong negative correlation, creating significant loss-reducing bonus. In contrast, Standard QATs weak correlation provides negligible benefit, demonstrating RaBiTs structural advantage in error correction. Layer Method Layer 5 (Early) Layer 15 (Mid) Layer 25 (Late) Standard QAT RaBiT (Ours) Standard QAT RaBiT (Ours) Standard QAT RaBiT (Ours) Base Error (C ) Path Amp. (2σ1σ2) Path Corr. (Corr) Covariance (Amp. Corr) Total MSE (C + Cov.) 0.0019 0. 0.0182 0.0163 0.0575 0.0609 0.0030 0.0028 0.0214 0.0200 0.0728 0.0801 -0.0752 -0. -0.1240 -0.3418 -0.1279 -0.3535 -0.0002 -0.0014 -0.0026 -0.0068 -0.0093 -0.0283 0.0017 0. 0.0156 0.0094 0.0482 0.0327 We identify critical form of feature co-adaptation in residual binarization, termed inter-path adaptation, wherein shared QAT gradient compels parallel paths to acquire redundant features, compromising the error-compensation hierarchy. Unlike prior works that depend on suboptimal heuristics like path freezing (Bulat et al., 2024; Tran & Nguyen, 2025), which restrict the solution space, RaBiT resolves this issue structurally, facilitating true joint optimization while algorithmically enforcing the hierarchy. 3. Motivation The goal of quantization-aware training (QAT) is to make quantized student model, Ys, functionally mimic its fullprecision teacher, Yt. This is typically achieved by optimizing an objective that combines the final task loss with an intermediate knowledge distillation loss, often formulated as the mean squared error (MSE) (Hinton et al., 2015; Liu et al., 2024). While our full training objective also includes the final KL divergence-based task loss, we initially focus our analysis on the MSE component for its analytical tractability. We formally prove that this error-decomposition logic and the optimality of RaBiTs residual coupling extend rigorously to the KL divergence objective (see Appendix B). The additive structure of the MSE provides clear window into how parallel paths interact. In 2-bit residual architecture, the MSE between the teacher output yt and the student output ys = y1 + y2 can be decomposed. Using the Pearson correlation coefficient,1 this decomposition is: MSE(yt, ys) = (E[y ] + E[y2 2] 2E[ytys]) } + 2σ1σ2 {z } athAmp. 1] + E[y2 {z Corr(y1, y2) , } {z athCorr. where represents the sum of correlation-independent er1The relationship E[y1y2] σ1σ2Corr(y1, y2) relies on zero-mean assumption for the path outputs. We empirically verify this, finding the omitted E[y1]E[y2] term is less than 1% of the covariance term and thus negligible. 3 ror terms. This reveals core principle: to minimize the MSE, the paths must be strongly negatively correlated. negative correlation transforms the interaction term into substantial bonus that actively reduces the total loss, signifying effective error-cancellation. However, Standard QATwhich optimizes independent latent weights for parallel paths simultaneously via shared gradientstructurally fails to achieve this. The shared global gradient induces inter-path adaptation, forcing both paths to learn redundant features instead of their intended compensatory roles (see Appendix A). To provide concrete analysis, we decompose the MSE loss for representative layers of Llama2-7B, selected to show the characteristics across the early, middle, and late stages of the network, in Table 1. The analysis in Table 1 is definitive. Across early, middle, and late stages of the network, the base error term and the path amplitude 2σ1σ2 remain comparable between both methods. The critical difference lies in the correlation. Standard QAT yields correlation close to zero, resulting in negligible interaction term that fails to meaningfully reduce the total error. In contrast, RaBiT structurally enforces strong negative correlation (e.g.,-0.50 in layer 5) (see Appendix A). This transforms the interaction term into significant loss-reducing bonus, systematically lowering the total MSE (see Appendix G). This principled enforcement of anti-correlation creates more stable optimization landscape, leading to better generalization and superior performance. (1) 4. Method We introduce RaBiT, novel QAT framework that prevents interference between the parallel paths of stacked binary architectures. To achieve this, RaBiT enforces clear errorcorrection role for each path using novel coupled training loop and stabilizes the process with function-aware initialization strategy. An overview of the RaBiT training framework is illustrated in Figure 1. RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs 4.1. The Residual Binarization Architecture To efficiently achieve low-bit precision (e.g., 2-bit), we adopt residual architecture built upon highly efficient binary building blocks. Binary Building Blocks. The fundamental component is the dual-scale binarization framework. We define the approximation of weight matrix ˆW using notation that highlights the underlying element-wise scaling operations: ˆW = h. (2) ( ˆW)ij is computed as giBijhj. Here, {1, +1}doutdin represents the binary core matrix, while Rdout and Rdin are full-precision, per-channel scaling vectors. This formulation facilitates matmul-free efficiency. For an input vector Rdin, the output Rdout is computed as = (B(h x)), implementable via additions and subtractions without costly matrix multiplications. Multi-bit Approximation via Stacking. To increase representational capacity (e.g., to 2-bit) while maintaining efficiency, = 2 binary paths are stacked in parallel. The effective weight is the sum of binarized terms: ˆW(k) = i=1 ˆWi = i= gi Bi hi. (3) This architecture preserves matmul-free execution, as the forward pass accumulates outputs from each path. 4.2. Coupled Training for Co-Adaptation Mitigation To prevent inter-path co-adaptation, RaBiT diverges from standard methods that train independent latent weights for each binary path. Instead, single shared full-precision (FP) weight WFP is used as an anchor for the entire residual structure. The Coupled Forward Pass. The mechanism relies on dynamic forward pass. In 2-bit configuration (k = 2), binary core matrices B1 and B2 are not explicitly stored. Rather, they are re-computed during each pass from the shared weight WFP (Figure 1a). This procedure algorithmically enforces an error-compensation hierarchy. While binary cores are dynamically derived, scaling vectors {gi, hi} remain as independent learnable parameters to capture pathspecific magnitudes. The forward pass consists of 3 steps: 1. Path 1 Derivation: The first binary core, B1, is determined by directly binarizing the shared weight: B1 = sign(WFP). This core is combined with corresponding learnable scaling vectors, g1 and h1, to reconstruct the first-path approximation ˆW1 = g1 B1 h1. 4 2. Residual Calculation: The residual error, R1, is computed by subtracting the reconstructed first path from the shared weight: R1 = WFP ˆW1. 3. Path 2 Derivation: The second binary core, B2, is determined by binarizing this calculated residual error: B2 = sign(R1). The final effective weight for the forward pass is the sum of the two reconstructed paths: ˆW(2) = ˆW1 + (g2 B2 h2). critical distinction is made between dynamically deriving binary cores Bi := sign(Ri1) and treating scaling vectors {gi, hi} as independent parameters. This separation ensures computational feasibility and training stability. Recalculating optimal scales at every stepe.g., via Singular Value Decomposition (SVD)would impose prohibitive computational costs. By maintaining scales as learnable parameters, the optimizer applies state accumulation (e.g., momentum) to fine-tune initialized values effectively (Section 4.3). Such data-adaptive tuning enables the error-compensation hierarchy to learn optimal magnitudes for each path. Backward Pass and Parameter Updates. The backward pass facilitates stable parameter updates. Gradients from the loss propagate to both the independent scaling vectors {gi, hi} and the shared weight WFP, as depicted in Figure 1c. Gradient for the Shared Weight: To update the single shared weight WFP, RaBiT employs an effective-weight gradient. This functions as Straight-Through Estimator (STE) for the entire coupled derivation process. The gradient is computed with respect to the final effective weight ˆW(k) = ˆWi and is directly passed to update WFP. To ensure dimensional consistency for general cases (din = dout), we formulate the update as: WFP ˆW(k) = Y X. (4) In this context, denotes the task loss, while RdinN and RdoutN represent the full input and output matrices for the mini-batch. By recomputing binary paths from the updated WFP at every step, the system forces each path to correct the most recent residual error, thereby preserving the error-compensation hierarchy. Gradient for Learnable Scales: The scaling vectors {gi, hi} function as standard learnable parameters and receive gradients via the chain rule. For mini-batch of size , gradients are accumulated over each sample while RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs treating the dynamic binary cores (Bi) as constants: gi = hi = n=1 n=1 (cid:0)Bi(hi xn)(cid:1), (cid:0)B (n gi)(cid:1) xn. (5) Here, xn denotes the input vector corresponding to the n-th column of X, and = (L/yn) represents the upstream gradient from the layers output vector yn. During inference, the final binary cores {Bi} are derived from the trained WFP and frozen, allowing WFP to be discarded. This yields highly efficient architecture where independent paths execute in fully parallel, matmul-free manner. Furthermore, the single-weight design reduces memory usage for optimizer states by 50%, addressing primary bottleneck in LLM fine-tuning. The complete training procedure is detailed in Algorithm 2. 4.3. Stable Initialization for Functional Preservation Quantization-aware training (QAT) in the 2-bit regime is sensitive to initialization, often trapping models in suboptimal minima (Nagel et al., 2020; Liu et al., 2024). To address this, we implement two-stage initialization process designed to preserve model functionality rather than simply approximating weight values. 1. Iterative Residual SVID. The primary objective is to identify binary paths that jointly approximate the target matrix. Standard greedy decomposition is suboptimal as early choices irreversibly bias subsequent paths. Therefore, we employ Iterative Residual Sign-Value-Independent Decomposition (SVID), Gauss-Seidel style iteration facilitating path co-adaptation. The process refines scales {gi, hi} and binary cores {Bi} for each path = 1, . . . , over iterations = 1, . . . , as follows: := WFP R(t) j<i B(t) , g(t) , h(t) ˆW(t) j>i := SVID(R(t) B(t) := g(t) ), h(t) . ˆW(t) ˆW(t1) , (6) SVID() (Xu et al., 2024) extracts optimal per-channel scales via rank-1 SVD approximation on the magnitudes. In practice, this iterative process is applied to preconditioned target matrix rather than the raw weights WFP. 2. I/O Channel Importance-Scaled Preconditioning. To focus decomposition on functionally critical weight components, the raw weights WFP are not used directly. Drawing on recent methods for functional saliency preservation (Boža & Macko, 2025), the matrix is preconditioned to generate target W. Using calibration dataset, input activation magnitudes (sin) and output gradient magnitudes (sout) are computed as the maximum absolute values and normalized to re-weight the full-precision matrix: = sαout out WFP sαin in . (7) Following Iterative Residual SVID, the resulting scales are mapped back to the original domain: gi = sαout and hi = sαin i. This approach reduces initial task loss and stabilizes the QAT startup phase (see Algorithm 1, Table 7, and Figure 5). out in 5. Experiments 5.1. Experimental Settings Setup. The RaBiT framework is evaluated using the Llama2, Llama3, and Gemma3 model families (Touvron et al., 2023; Grattafiori et al., 2024; Team et al., 2025). For quantization-aware training (QAT), calibration dataset of 200 million tokens is derived from the combined WikiText-2 and C4 datasets (Jo et al., 2024). Performance assessment relies primarily on perplexity (PPL) calculated on validation sets with context length of 4096. Evaluation Benchmarks. We report the average zeroshot accuracy (QA Avg.) across five standard reasoning benchmarks, including HellaSwag, PIQA, WinoGrande, ARC-e and ARC-c (Zellers et al., 2019; Bisk et al., 2020; Sakaguchi et al., 2021; Clark et al., 2018). Detailed breakdowns for these standard benchmarks are provided in Appendix D. Furthermore, to test the models robustness on harder tasks involving complex reasoning and instruction following, we include evaluation on Big Bench Hard (BBH), GPQA, MMLU-Pro, and IFEval (Suzgun et al., 2023; Rein et al., 2024; Wang et al., 2024b; Zhou et al., 2023). These challenging benchmarks serve to highlight the functional preservation of the quantized models beyond basic language modeling metrics. Training Details. The training process uses QAT framework integrated with Knowledge Distillation (KD) (Hinton et al., 2015; Liu et al., 2024), employing the full-precision model as the teacher. The objective function combines the KullbackLeibler (KL) divergence loss on output logits with intermediate mean squared error (MSE) losses: Ltotal = Lkl + γ Linter,i. We set γ = 100 for Llama models and γ = 0 for Gemma3 to mitigate instability caused by large activation ranges (Han & Han, 2025). Models are trained for 6 epochs using the Muon optimizer (Jordan et al., 2024) and our proposed function-aware initialization. We provide comprehensive hyperparameter configurations in Appendix F. RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs Table 2. Comparison with state-of-the-art 2-3-bit methods on Llama models. We report perplexity (PPL ) and zero-shot QA Average (). For the 2-bit results, the best and runner-up are marked in bold and underlined, respectively. RaBiT achieves state-of-the-art (SOTA) performance on Llama2-7B and Llama3-8B, while showing highly competitive results on Llama2-13B. Methods Llama2-7B Llama2-13B Llama3-8B Bit Wiki2 C4 QA Avg Bit Wiki2 C4 QA Avg Bit Wiki2 C4 QA Avg 16 Baseline GPTQ 2.1 EfficientQAT 2.1 AQLM QuIP# QTIP BitStack DB-LLM MBOK DBF RaBiT (Ours) 2.3 2 2 3 2 2 3 2 2.3 2 3 2 5.12 50.75 6. 6.29 6.19 5.86 6.91 29.97 7.23 6.13 6.99 5.81 6.10 5.36 5.78 6.63 36.76 8.34 8.56 8.16 7.73 9.10 34.91 9.62 8.13 9.38 7.69 8. 7.06 7.64 62.26 39.16 57.75 58.57 58.23 58.97 56.54 40.12 55.12 54.63 53.63 59.84 58.42 63.05 61.51 16 2.1 2. 2.2 2 2 3 2 2 3 2 2.3 2 3 2 4.57 43.84 5.58 5.41 5.35 5.11 5.90 67.98 6.19 5.14 5.76 5.15 5. 4.84 5.15 6.05 23.07 7.40 7.20 7.20 6.85 7.86 72.60 8.38 6.94 7.89 6.85 7.13 6.51 6.95 65.46 43.72 62. 61.58 61.96 62.92 61.06 39.38 59.41 62.73 60.58 62.53 61.53 64.09 62.10 16 2 2.1 2.3 2 2 3 2 2 3 2 2.3 3 2 5.75 1.21e3 8.75 7.23 8.70 7.52 12.38 2.75e3 12.08 7.81 10.74 7.22 7.78 6.58 7.34 8.32 4.97e2 12. 10.32 12.04 10.76 17.51 1.93e3 16.80 11.29 14.61 10.34 10.99 9.54 10.52 68.66 35.59 60.63 64.12 63.89 63.88 58.41 36.21 50.92 61.08 54.41 64.84 62. 65.61 64.13 Table 3. Comparison with state-of-the-art 2-bit methods on Gemma3 models. We report perplexity (PPL ) and zero-shot QA Average (). The context length is 4096. RaBiT consistently achieves SOTA or highly competitive performance, demonstrating its robustness across diverse model architectures. Methods Baseline DBF QTIP Gemma3-1B Gemma3-4B Gemma3-12B Bit Wiki2 C4 QA Avg Bit Wiki2 C4 QA Avg Bit Wiki2 C4 QA Avg 16 2 2 9.80 13.28 13.14 13.69 17.57 17.36 57.82 51.98 50. 16 2 2 2 6.88 8.72 8.31 8.09 10.44 12.71 12.21 67.60 60.91 63. 11.91 62.21 16 2 2 2 5.50 6.97 6.65 6. 9.28 10.60 10.25 73.45 68.37 69.69 10.18 68.85 RaBiT (Ours) 11.27 15.54 53.18 Baselines. We compare RaBiT against broad spectrum of state-of-the-art 2to 3-bit quantization methods. These include: (1) standard post-training quantization methods such as GPTQ and EfficientQAT (Frantar et al., 2022; Chen et al., 2025); (2) high-accuracy Vector Quantization (VQ) approaches including AQLM, QuIP#, and QTIP (Egiazarian et al., 2024; Tseng et al., 2024a;b), which typically incur high hardware overhead; and (3) hardware-efficient binary and residual methods such as BitStack, DB-LLM, MBOK, and DBF (Wang et al., 2024a; Chen et al., 2024; Tran & Nguyen, 2025; Boža & Macko, 2025)2, which serve as the most direct architectural comparisons. 5.2. Main Results The empirical results, summarized in Tables 2 to 4, demonstrate that RaBiT consistently redefines the state-of-the2We rely on our re-implementation for DB-LLM and MBOK. art for 2-bit quantization, exhibiting superior performance across all tested model architectures and datasets. Dominance over Hardware-Efficient Methods. RaBiT significantly outperforms existing matmul-free binary and residual methods, highlighting the efficacy of the proposed optimization strategy. On the Llama2-7B benchmark, RaBiT achieves WikiText-2 perplexity (PPL) of 5.78. This represents substantial improvement over direct competitors such as MBOK (6.99 PPL) and DBF (6.10 PPL). The performance gap becomes even more pronounced on larger models and more complex datasets, which underscores the severe performance penalty incurred by the inter-path adaptation issues that prior methods fail to address. Notably, we observed that methods lacking our principled coupled design, such as BitStack, suffer from catastrophic instability on newer architectures like Llama3-8B (degrading to 2.75e3 PPL), whereas RaBiT successfully maintains robust convergence and high fidelity. 6 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs Table 4. Zero-shot evaluation on challenging benchmarks. We report BBH, GPQA, MMLU-Pro, and IFEval results for Llama213B and Llama3-8B. Table 5. Ablation on RaBiT (Llama2-7B PPL). The analysis isolates the impact of Iterative Residual SVID (I) and I/O Channel Importance-Scaled Preconditioning (S). Llama2-13B Llama3-8B Training Method WikiText-2 Methods Baseline QTIP RaBiT Baseline QTIP RaBiT Bit BBH GPQA MMLU-Pro IFEval 16 2 2 16 2 Standard QAT 40.99 27.45 24.81 23. 33.36 25.75 16.69 25.74 37.72 26. 19.44 24.63 45.84 30.85 14.91 32. 36.27 24.56 19.24 15.60 36.78 28. 19.65 15.42 Coupled QAT (RaBiT) 6.55 6.21 6.31 6.18 5.84 5.80 5.81 5.78 Average 29.27 25. 27.14 31.03 23.92 25.12 Achieving VQ-Level Accuracy with Binary Efficiency. Perhaps most impressively, RaBiT effectively closes the gap withand frequently surpasseshardware-intensive Vector Quantization (VQ) methods, thereby resolving the historical trade-off between accuracy and efficiency. On Llama27B, RaBiTs perplexity of 5.78 edges out the leading VQ method, QTIP (5.86 PPL). This trend holds for downstream reasoning tasks as well; RaBiT achieves 61.51% average zero-shot accuracy on Llama2-7B, surpassing QTIPs 58.97% and demonstrating superior functional preservation. This robustness is further corroborated by results on Llama38B, where RaBiT maintains strong performance (7.34 PPL) while other VQ methods like QuIP# suffer from severe degradation (8.70 PPL). As shown in Table 3, this advantage generalizes beyond the Llama family. On the Gemma3 suite (1B/4B/12B), RaBiT consistently delivers competitive perplexity and robust zero-shot QA accuracy. Performance on Harder Tasks. To further validate the models capabilities in complex scenarios, we evaluated performance on challenging benchmarks including BBH, GPQA, MMLU-Pro, and IFEval. As presented in Table 4, RaBiT outperforms QTIP on average (27.14 vs. 25.38 on Llama2-13B) and retains substantially more capability relative to the full-precision baseline than prior quantization techniques. This indicates that RaBiTs coupled training strategy preserves the delicate internal representations required for advanced reasoning and instruction following, which are often lost in standard binary quantization. (a) Inter-Path Correlation (b) Training Loss Figure 2. Visualization of Coupled Training Dynamics. (a) Inter-Path Correlation: RaBiT enforces negative inter-path correlation, indicating effective error-correction, whereas Standard QAT leads to positive correlation (co-adaptation). (b) Training Loss: This structural advantage directly translates to lower and more stable training loss for RaBiT, demonstrating its superior optimization path. 5.3. Ablation Studies 5.3.1. COMPONENT-WISE CONTRIBUTION ANALYSIS We performed an ablation study to analyze the contributions of RaBiTs core components Coupled QAT, Iterative SVID (I), and I/O Channel Importance-Scaled Preconditioning (S), with results in Table 5. The analysis clearly shows that Coupled QAT is the most critical performance factor. Simply switching from Standard QAT (6.55 PPL) to Coupled QAT reduces the perplexity to 5.84, confirming that resolving inter-path adaptation yields the largest gain. Our initialization methods (I and S) provide further essential improvements. While they offer significant boost to the baseline Standard QAT, their role within the powerful Coupled QAT framework is to provide the final, crucial finetuning needed to reach the optimal 5.78 PPL. This synergy between robust training method and function-aware initialization is key to state-of-the-art performance of RaBiT. 7 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs Table 6. Inference Performance Analysis on NVIDIA RTX 4090. Kernel latency for key Llama2-7B/13B layers and Llama2-7B decoding throughput for 256-token generation. RaBiT shows superior efficiency at both the kernel and system levels. Method FP16 DBF QTIP RaBiT (Ours) Bit 2.3 2 3 2 3 2 Kernel-Level Latency (µs) End-to-End Decoding 40964096 (q_proj, 7B) 110084096 (gate_proj, 7B) 51205120 (q_proj, 13B) 138245120 (gate_proj, 13B) Throughput (tok/s) 17.15 (1.00) 70.37 (1.00) 17.85 (1.00) 122.90 (1.00) 64.96 (1.00) 12.66 (1.35) 11.47 (1.50) 28.43 (2.48) 20.90 (3.37) 14.72 (1.21) 14.08 (1.27) 24.04 (0.71) 23.40 (0.73) 37.08 (1.90) 42.40 (1.66) 36.22 (0.49) 37.46 (0.48) 8.15 (2.10) 7.72 (2.22) 17.13 (4.11) 15.71 (4.48) 9.90 (1.80) 8.33 (2.14) 31.87 (3.86) 29.58 (4.15) 49.97 (2.46) 59.22 (2.08) 22.36 (5.50) 17.50 (7.02) 157.66 (2.43) 175.21 (2.70) 153.59 (2.36) 171.74 (2.64) 191.63 (2.95) 291.88 (4.49) 5.3.2. ANALYSIS OF COUPLED TRAINING DYNAMICS To empirically validate that coupled training resolves coadaptation, we conducted controlled experiment comparing RaBiT to four variants: (1) Standard QAT (independent latent weights), (2) MBOK (frozen primary binary core, mimicking the path-freezing heuristic of Tran & Nguyen (2025) within our training loop), (3) Scale-only (frozen binary cores), and (4) Scale-frozen (RaBiT with frozen scales). To strictly isolate architectural dynamics from optimizerspecific effects, we standardize the initialization and optimizer (Muon) across all variants. Figure 2 reveals the resulting training dynamics. As theorized, RaBiT successfully maintains stable negative interpath correlation, enforcing the error-correction hierarchy (Figure 2a). In contrast, Standard QAT develops strong positive correlation, confirming that shared global gradient induces harmful redundancy. The constrained variants (MBOK, Scale-frozen) fail to establish strong anticorrelation, limiting their optimization potential. This structural advantage directly translates to model functionality, as shown by the training loss curves (Figure 2b). RaBiT achieves the lowest and most stable loss, while the coadaptation in Standard QAT and the incomplete optimization of the other variants lead to significantly higher loss. This analysis confirms that sequential optimization of all parameters algorithmically prevents co-adaptation and results in its superior performance. 5.4. Inference Performance RaBiT not only achieves state-of-the-art accuracy but also delivers exceptional inference speed by leveraging its parallelizable matmul-free binary architecture. As shown in Table 6, 2-bit RaBiT achieves 4.49 speed-up in endto-end decoding throughput over the FP16 baseline, on an NVIDIA RTX 4090. This performance gain stems from two key advantages. First, the 8 reduction in model size (2-bit vs. 16-bit) dra8 matically lowers memory bandwidth requirements, which is the primary bottleneck in the autoregressive decoding phase. Second, unlike VQ methods, RaBiT avoids hardwareunfriendly overheads like lookup tables or rotations. Its simple architecture of additions and element-wise scaling translates to higher hardware utilization. This is evident in our kernel-level benchmarks, where RaBiTs specialized kernels exhibit consistently lower latency than both the FP16 baseline and QTIPs VQ kernels. By eliminating computational complexity, RaBiT ensures that theoretical memory savings translate directly into real-world speed, delivering solution that is both accurate and genuinely efficient. Further details on our kernel design and additional performance benchmarks are provided in Appendix E.1 and Appendix E.2, respectively. 6. Conclusion This study addresses the trade-off between accuracy and hardware efficiency in 2-bit large language model (LLM) quantization through the introduction of RaBiT. We identified inter-path adaptation as primary bottleneck that compromises the error-compensation structure in residual binarization. To mitigate this, the proposed framework employs on-the-fly residual coupling, mechanism that prevents structural breakdown during training and ensures that the expressive capacity of the model is effectively used. Furthermore, to address the instability inherent in extreme quantization, we introduced function-aware initialization strategy that facilitates stable convergence. Experimental results indicate that RaBiT achieves state-of-the-art performance at 2-bit precision, outperforming both existing binary methods and hardware-intensive Vector Quantization (VQ) approaches. By establishing these capabilities, this work offers viable pathway for the efficient deployment of high-performance low-bit LLMs and serves as scalable foundation for future research. RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents RaBiT, framework for the extreme 2-bit quantization of LLMs. By drastically reducing computational requirements, our work is environmentally friendly and democratizes access, enabling high-performance models to run on consumer hardware (e.g., RTX 4090). This efficiency facilitates local deployment, thereby enhancing user privacy and data sovereignty compared to cloud-based solutions. However, such accessibility also introduces dual-use risks by potentially enabling malicious applications outside controlled environments. Additionally, while our method preserves general capabilities, the specific effects of extreme compression on safety alignment remain an important area for future investigation."
        },
        {
            "title": "References",
            "content": "Bengio, Y., Léonard, N., and Courville, A. Estimating or Propagating Gradients Through Stochastic NeuarXiv preprint rons for Conditional Computation. arXiv:1308.3432, 2013. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. PIQA: Reasoning About Physical Commonsense in Natural Language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Boža, V. and Macko, V. Addition Is Almost All You Need: Compressing Neural Networks with Double Binary Factorization. arXiv preprint arXiv:2505.11076, 2025. Bulat, A., Ouali, Y., and Tzimiropoulos, G. QBB: Quantization with Binary Bases for LLMs. Advances in Neural Information Processing Systems, 37:32093228, 2024. Chen, H., Lv, C., Ding, L., Qin, H., Zhou, X., Ding, Y., Liu, X., Zhang, M., Guo, J., Liu, X., et al. DB-LLM: Accurate Dual-Binarization for Efficient LLMs. arXiv preprint arXiv:2402.11960, 2024. Chen, M., Shao, W., Xu, P., Wang, J., Gao, P., Zhang, K., and Luo, P. EfficientQAT: Efficient Quantization-Aware Training for Large Language Models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10081 10100, 2025. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think You Have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457, 2018. Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar, E., Babenko, A., and Alistarh, D. Extreme Compression of Large Language Models via Additive Quantization. arXiv preprint arXiv:2401.06118, 2024. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. arXiv preprint arXiv:2210.17323, 2022. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783, 2024. Han, D. and Han, M. Fine-tune gemma 3 with unsloth, March 2025. URL https://unsloth.ai/blog/ gemma3. Accessed: 2026-01-29. Hinton, G., Vinyals, O., and Dean, J. Distilling the arXiv preprint Knowledge in Neural Network. arXiv:1503.02531, 2015. Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. R. Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors. arXiv preprint arXiv:1207.0580, 2012. Huang, W., Liu, Y., Qin, H., Li, Y., Zhang, S., Liu, X., Magno, M., and Qi, X. BiLLM: Pushing the Limit of Post-Training Quantization for LLMs. arXiv preprint arXiv:2402.04291, 2024. Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y. Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations. Journal of Machine Learning Research, 18(187):130, 2018. Jo, D., Kim, T., Kim, Y., et al. Mixture of Scales: MemoryEfficient Token-Adaptive Binarization for Large Language Models. Advances in Neural Information Processing Systems, 37:137474137494, 2024. Jordan, K., Jin, Y., Boza, V., Jiacheng, Y., Cesista, F., Newhouse, L., and Bernstein, J. Muon: An Optimizer for Hidden Layers in Neural Networks, 2024. URL https: //kellerjordan.github.io/posts/muon/. Kim, T., Oh, J., Kim, N. Y., Cho, S., and Yun, S.-Y. Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, 2021. Krishnamoorthi, R. Quantizing Deep Convolutional Networks for Efficient Inference: Whitepaper. arXiv preprint arXiv:1806.08342, 2018. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient Memory Management for Large Language Model 9 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lee, B., Kim, D., You, Y., and Kim, Y. LittleBit: Ultra Low-Bit Quantization via Latent Factorization. arXiv preprint arXiv:2506.13771, 2025. Li, Z., Yan, X., Zhang, T., Qin, H., Xie, D., Tian, J., Kong, L., Zhang, Y., Yang, X., et al. ARB-LLM: Alternating Refined Binarizations for Large Language Models. arXiv preprint arXiv:2410.03129, 2024. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. AWQ: Activation-Aware Weight Quantization for On-Device LLM Compression and Acceleration. Proceedings of machine learning and systems, 6:87100, 2024. Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. LLMQAT: Data-Free Quantization Aware Training for Large Language Models. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 467484, 2024. Liu, Z., Zhao, C., Huang, H., Chen, S., Zhang, J., Zhao, J., Roy, S., Jin, L., Xiong, Y., Shi, Y., et al. ParetoQ: Scaling Laws in Extremely Low-Bit LLM Quantization. arXiv preprint arXiv:2502.02631, 2025. Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or Down? Adaptive Rounding for Post-Training Quantization. In International Conference on Machine Learning, pp. 71977206. PMLR, 2020. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. GPQA: Graduate-Level Google-Proof Q&A Benchmark. In First Conference on Language Modeling, 2024. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. Communications of the ACM, 64(9):99 106, 2021. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: Simple Way to Prevent Neural Networks from Overfitting. The journal of machine learning research, 15(1):19291958, 2014. Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q., Chi, E., Zhou, D., et al. Challenging Big-Bench Tasks and Whether Chain-of-Thought Can Solve Them. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, 2023. Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Ramé, A., Rivière, M., et al. Gemma 3 Technical Report. arXiv preprint arXiv:2503.19786, 2025. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open Foundation and FineTuned Chat Models. arXiv preprint arXiv:2307.09288, 2023. Tran, B.-H. and Nguyen, V. M. Highly Efficient and Effective LLMs with Multi-Boolean Architectures. arXiv preprint arXiv:2505.22811, 2025. Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and De Sa, C. QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks. arXiv preprint arXiv:2402.04396, 2024a. Tseng, A., Sun, Q., Hou, D., and De Sa, C. M. QTIP: Quantization with Trellises and Incoherence Processing. Advances in Neural Information Processing Systems, 37: 5959759620, 2024b. Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., Yang, F., Wang, R., Wu, Y., and Wei, F. BitNet: Scaling 1-Bit Transformers for Large Language Models. arXiv preprint arXiv:2310.11453, 2023. Wang, X., Wang, P., Wang, B., Zhang, D., Zhou, Y., and Qiu, X. BitStack: Any-Size Compression of Large Language Models in Variable Memory Environments. arXiv preprint arXiv:2410.23918, 2024a. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. MMLU-Pro: More Robust and Challenging Multi-Task Language Understanding Benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024b. Xu, Y., Han, X., Yang, Z., Wang, S., Zhu, Q., Liu, Z., Liu, W., and Che, W. OneBit: Towards Extremely Low-Bit Large Language Models. Advances in Neural Information Processing Systems, 37:6635766382, 2024. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can Machine Really Finish Your Sentence? arXiv preprint arXiv:1905.07830, 2019. Zhang, F., Liu, Y., Li, W., Lv, J., Wang, X., and Bai, Q. Towards Superior Quantization Accuracy: LayerSensitive Approach. arXiv preprint arXiv:2503.06518, 2025. Zhang, Y., Dong, Y., and Kawaguchi, K. Investigating Layer Importance in Large Language Models. arXiv preprint arXiv:2409.14381, 2024. 10 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs Zheng, L., Yin, L., Xie, Z., Sun, C. L., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., et al. SGLang: Efficient Execution of Structured Language Model Programs. Advances in Neural Information Processing Systems, 37:6255762583, 2024. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-Following EvalarXiv preprint uation for Large Language Models. arXiv:2311.07911, 2023. 11 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs A. Mathematical Analysis of Training Dynamics This section provides mathematical analysis of the training dynamics for residual binary architectures. We demonstrate why Standard QAT is prone to inter-path adaptation, where paths become redundant. In contrast, we show how RaBiTs coupled training mechanism structurally enforces an error-correcting hierarchy and is superior to other heuristic solutions. Proposition 1 (Inter-Path Adaptation in Standard QAT). In Standard QAT scheme where two paths ( ˆW1, ˆW2) are updated from their respective latent weights (W1, W2) using shared global gradient = ˆW1+ ˆW2 L, the paths have persistent tendency to become positively correlated, leading to redundancy. Proof. Let the latent weights be W1 and W2. After single update step with learning rate η and shared gradient G, the new weights are 1 and 2: 1 := W1 ηG and 2 := W2 ηG The change in the Frobenius inner product between the weights, which reflects their correlation, is: , := 1, 2F W1, W2F Expanding this gives: , = η (W1, GF + W2, GF ) + η2G2 While the linear terms depend on the alignment between the current weights and the gradient, the quadratic term η2G2 is always non-negative. This term acts as systematic force, constantly pushing the two paths in the same direction defined by the global gradient G. This dynamic, the underlying mechanism of inter-path adaptation, compels both paths to learn redundant, dominant features in order to minimize the global loss. This ultimately leads to breakdown of the intended residual hierarchy and compromises the models expressive capacity. Proposition 2 (Structurally Enforced Error Correction in RaBiT). RaBiTs coupled training mechanism resolves the redundancy drift by fundamentally changing the optimization objective. Instead of independent updates, RaBiTs on-the-fly derivation structurally forces the second path ( ˆW2) to align with the true residual of the first path (R1 = WFP ˆW1), thereby enforcing an error-correcting relationship. Analysis. The optimization objectives of the two paths are implicitly different in RaBiT versus the naïve approach. Standard QAT Objective: Both paths are driven by the same structurally-agnostic global gradient G. Their implicit goal is to align with to reduce the global loss. Since both ˆW1 and ˆW2 are incentivized to align with the same vector G, they inevitably learn to align with each other, leading to redundancy as shown in Proposition 1. ˆW1 and ˆW2 = ˆW1, ˆW2F > 0 RaBiTs Enforced Objective: RaBiT maintains single shared blueprint, WFP. The on-the-fly derivation process, R1 := WFP ˆW1 followed by the binarization of R1 to create ˆW2, explicitly defines the optimization target for the second path. The goal for ˆW2 is no longer to align with the global gradient g, but to be the best possible low-rank approximation of the current residual R1. Objective for ˆW2 : min R1 ˆW22 = ˆW2 R1 This structural constraint forces high Residual Alignment. In the context of extreme low-bit quantization, the first approximation ˆW1 often \"overshoots\" the target WFP in certain directions. To correct this, the residual R1 = WFP ˆW1 will point in the opposite direction of the overshoot. By aligning with R1, ˆW2 naturally becomes anti-correlated with ˆW1, implementing an efficient active cancellation mechanism rather than degenerating into redundancy. Proposition 3 (Superior Optimization Dynamics of Coupled vs. Iterative Training). Iterative training (e.g., freezing one path while training the other) avoids adaptation but at the cost of optimization efficiency. In contrast, RaBiT resolves adaptation while permitting full parameter co-adaptation, resulting in superior optimization trajectory. 12 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs Proof. Following Proposition 1, the problem of Standard QAT is the simultaneous update of both paths in the same direction. An alternative solution is to update them iteratively, which prevents this simultaneous push and thus avoids adaptation. However, this introduces new problem of inefficiency. The optimal direction to reduce the loss is the steepest descent direction in the joint parameter space of (W1, W2), which is = (G, G). When training iteratively, one path is frozen, so the update is restricted to an axis-aligned direction, e.g., diter = (0, G). The cosine similarity between the iterative update and the optimal update direction is: cos(θ) = = = diter, dF diterF dF G2 GF pG2 + G2 (0, G), (G, G)F (0, G)F (G, G)F 1 2 = This fixed 45 misalignment forces the optimization to follow an inefficient zig-zag trajectory. While it solves adaptation, it sacrifices optimization efficiency. RaBiT, through its coupled derivation described in Proposition 2, resolves this trade-off. By updating single shared weight WFP with the full gradient G, it allows both paths to co-adapt simultaneously in coordinated manner that is not restricted to an inefficient path. Thus, RaBiT resolves adaptation without compromising optimization efficiency, leading to superior dynamics. Corollary 1 (to Proposition 2). Negative Correlation Induction. RaBiTs coupled training mechanism, by forcing the second path ( ˆW2) to approximate the residual of the first path (R1), inherently promotes negative correlation between their respective outputs (y1, y2). Analysis. From Proposition 2, we established that RaBiT trains the second path to approximate the residual of the first: ˆW2 R1 = WFP ˆW1 Let us consider the outputs for given input x. The outputs of the full-precision teacher, the first path, and the second path are yt = WFPx, y1 = ˆW1x, and y2 = ˆW2x, respectively. Based on the weight approximation, the output of the second path is: y2 R1x = (WFP ˆW1)x = yt y1 Now, we can analyze the covariance between the outputs y1 and y2. Assuming the outputs are centered for simplicity, the covariance is proportional to the expected value of their dot product, E[y E[y 1 y2] E[y 1 (yt y1)] = E[y 1 yt] E[y 1 yt] E[y12] 1 y2]. 1 y1] = E[y Let us analyze the two terms: 1. E[y 1 yt]: The first path ˆW1 is the primary, albeit coarse, approximation of WFP. Its purpose is to capture the main features of the teacher, so y1 and yt are expected to be positively correlated. However, in the extreme 1-bit regime, the approximation is directionally coarse: non-negligible portion of y1 lies in directions not well-aligned with yt. This misaligned component limits how large the alignment term 1 yt can become, making the positive correlation typically not strong. 2. E[y12]: This is the expected squared norm of the first paths output. Binarization is an aggressive quantization that often leads to an overshoot in magnitude. single binary path must represent wide range of continuous values, so its effective scaling factor often results in an output magnitude y1 that exceeds the projection of y1 onto yt. Consequently, it is often the case that y12 > 1 yt, making E[y12] larger positive term than E[y 1 yt]. Combining these points, the covariance is approximately the difference between positive term and larger positive term: Cov(y1, y2) E[y 1 yt] {z } Positive Alignment E[y12] {z } Larger Magnitude Term < 0 Thus, RaBiTs mechanism of forcing the second path to correct the error of the first path structurally drives the covariance, and therefore the correlation Corr(y1, y2), toward negative, consistent with our empirical observations. 13 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs B. Extended Analysis: Inter-Path Adaptation under KL Divergence While Section 3 and Corollary 1 motivate RaBiT using the Mean Squared Error (MSE) decomposition, modern LLM training often relies on the Kullback-Leibler (KL) divergence loss. In this section, we rigorously demonstrate that the principle of inter-path adaptation and RaBiTs solution remain valid under the KL divergence objective. We leverage local quadratic approximation and the findings from (Kim et al., 2021) regarding the decomposition of KL loss. Proposition 4 (Optimality of Residual Coupling under KL Divergence). RaBiTs residual coupling mechanism structurally eliminates the optimization bias inherent in the KL divergence loss by enforcing negative Hessian-weighted path correlation, property that Standard QAT fails to satisfy. Analysis. We analyze the optimization dynamics at specific linear layer ℓ where RaBiT is applied. Let yℓ the output feature of the teacher model, and yℓ students output is the sum of its binary paths: yℓ = WFPxℓ be be that of the student. Since the operations within the layer are linear, the = h1 + h2. Local Quadratic Approximation. The global KL divergence loss LKL can be approximated locally around the teachers output yℓ using second-order Taylor expansion: LKL(yℓ s) LKL(yℓ t) + L(yℓ t)y + 1 2 yHℓy (8) Assuming the teacher is optimal locally (L 0), the optimization objective reduces to minimizing Hessian-weighted MSE: Jlocal yℓ yℓ t2 Hℓ = (h1 + h2 yℓ t)Hℓ(h1 + h2 yℓ t) (9) 1 2 1 2 where Hℓ is the Hessian matrix representing the local curvature. Decomposition with Hessian-weighted Path Correlation. Similar to Equation (1) in the main text, we decompose this local objective. By treating (h1 yℓ t) as the residual error of the first path, we expand the quadratic term: Jlocal h1 yℓ t2 Hℓ {z } Base Error + h22 Hℓ {z } Path 2 Amp. t)Hℓh2 + 2(h1 yℓ } {z Interaction Term (10) We define the Hessian-weighted Path Correlation, denoted as PathCorrH, based on the generalized cosine similarity in the inner product space defined by Hℓ: Interaction Term = 2 h1 yℓ tHℓ h2Hℓ PathCorrH(h1 yℓ t, h2) (11) This formulation reveals that minimizing the local loss requires maximizing the negative magnitude of PathCorrH. Bridging Interaction to Bias Cancellation. To understand the implication of this interaction term in the context of KL divergence, we refer to (Kim et al., 2021), which proved that minimizing LKL is equivalent to minimizing MSE plus negative Bias Term (δ). This bias term acts as destabilizing force that pushes the students logit sum to diverge from the teachers. Crucially, effectively maximizing the negative interaction term (i.e., enforcing error correction) is the key to neutralizing this bias. Failure of Standard QAT: In Standard QAT, h1 and h2 are updated independently using the same gradient signal. This leads to h2 aligning with yℓ h1). Consequently, PathCorrH becomes positive (redundancy) or near zero. This failure to exploit the interaction bonus leaves the destabilizing Bias Term (δ) unchecked, causing the optimization drift described in (Kim et al., 2021). rather than the residual (yℓ Success of RaBiT: RaBiT enforces ˆW2 = sign(WFP ˆW1), structurally guaranteeing: h2 yℓ h1 = (h1 yℓ t) (12) This forces the second path vector to be anti-parallel to the first paths error vector in the feature space. As result: 1. Maximized Interaction: PathCorrH approaches its theoretical minimum of 1 (perfect anti-correlation). 14 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs 2. Bias Cancellation: By strictly adhering to the residual h2 yℓ h1, the total student output yℓ without the scale divergence issues. The Bias Term (δ) is effectively cancelled out locally ((P yℓ approximates yℓ yℓ t)2 0). Therefore, RaBiTs residual coupling is the optimal strategy for minimizing LKL as it structurally enforces the necessary negative correlation that Standard QAT fails to learn. C. Initialization Analysis: Functionality vs. Approximation Table 7. Initialization Analysis on Llama2-7B. Trade-off between weight reconstruction error (Avg. MAE/MSE) and model functionality (Initial KL Divergence Loss), for the first q_proj layer. I/O Channel Importance Scaling dramatically reduces KL Divergence Loss despite increasing MSE. Initialization Method Avg. MAE Avg. MSE KL Loss Greedy SVID Iterative Residual SVID + I/O Ch. Importance Scaling 0.359 0.370 0.632 0.150 0.122 0.302 17,152 13,760 2,672 (a) WFP (b) Greedy SVID Init. (c) Iterative SVID Init. (d) Iter + I/O Scaling (e) Diff. (Greedy) (f) Diff. (Iterative) (g) Diff. (Iter + I/O) Figure 3. Visual analysis of weight initialization for the first layers q_proj matrix of the Llama2-7B model. The top row displays the original full-precision weight (WFP) alongside its initial approximations from three methods: (b) Greedy SVID, (c) Iterative SVID, and (d) Iterative SVID with I/O Channel Importance Scaling. The bottom row shows the corresponding difference matrices (WFP ˆWinit), illustrating the initial error structure. Our function-aware initialization produces qualitatively different structure compared to the others, which is reflected in its distinct error pattern. Stable initialization is paramount in the low-bit regime, as the initial quantization error spike can destabilize QAT. On the Llama2-7b model, we evaluate our proposed techniquesIterative Residual SVID and I/O Channel Importance Scaling (Section 4.3)by measuring both the weight reconstruction error (Avg. MAE, MSE) and the initial task loss (Knowledge Distillation (KD) loss) before the first training step. Table 7 details the results for the first q_proj layer and reveals crucial insight. As our baseline, Greedy SVID is non-iterative decomposition that finalizes each path sequentially without the co-adaptation enabled by our iterative approach. First, regarding Iterative Refinement, moving from Greedy SVID to Iterative Residual SVID consistently improves weight reconstruction (e.g., Avg. MSE drops 0.150 0.122) and substantially reduces the initial KL divergence loss (17,152 15 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs 13,760), confirming mitigation of scheduling bias. Second, adding I/O Channel Importance Scaling to the iterative process yields striking result: while reconstruction error increases significantly (Avg. MSE 0.122 0.302), the KL divergence loss plummets dramatically (13,760 2,672, an 81% reduction). This confirms that extreme quantization should prioritize preserving functionality over merely approximating weights. I/O Channel Importance Scaling allocates the limited 2-bit capacity to critical channels based on activation and gradient statistics (Section 4.3), sacrificing the reconstruction of less important weights. This trade-off is visually stark in Figure 3. While Iterative SVID produces lower-error approximation than Greedy SVID (comparing Figure 3f to Figure 3e), the function-aware I/O Scaling method yields visibly larger reconstruction error (Figure 3g). Despite this higher weight-level discrepancy, its focus on functional saliency provides far superior starting point for QAT, as evidenced by the dramatic reduction in initial task loss. D. Extended Results Detailed Zero-Shot Reasoning Accuracy. Table 8 and Table 9 provide detailed breakdown of the zero-shot reasoning accuracy across five common benchmarks, complementing the average scores reported in the main text. Table 8. Detailed Zero-Shot Reasoning Accuracy on Llama Models (%). Comparison of FP16 against leading 2-bit methods on five common benchmarks. Models Method WinoGrande HellaSwag ARC-e ARC-c PIQA Average Llama2-7B Llama2-13B Llama3-8B FullPrecision QTIP DBF RaBiT (Ours) FullPrecision QTIP DBF RaBiT (Ours) FullPrecision QTIP DBF RaBiT (Ours) 67.80 64.64 63.61 67.80 69.93 67.56 67.09 67.56 72.93 70.24 68.90 69.37 56.71 53.09 52.44 53.52 59.64 57.4 56.6 56.71 60.08 55.53 54.49 55. 69.28 65.57 64.73 72.43 73.19 70.8 69.02 69.06 80.30 75.29 74.62 75.37 39.93 35.67 35.58 37.88 45.73 41.46 38.74 39.76 50.17 41.64 39.76 42. 78.29 75.90 75.84 75.90 78.67 77.37 78.18 77.42 79.76 76.71 76.44 77.96 62.40 58.97 58.44 61.51 65.43 62.92 61.93 62.10 67.80 63.88 62.84 64. Table 9. Detailed Zero-Shot Reasoning Accuracy on Gemma Models (%). Comparison of FP16 against leading 2-bit methods on five common benchmarks. Models Method WinoGrande HellaSwag ARC-e ARC-c PIQA Average Gemma3-1B Gemma3-4B Gemma3-12B FullPrecision QTIP DBF RaBiT (Ours) FullPrecision QTIP DBF RaBiT (Ours) FullPrecision QTIP DBF RaBiT (Ours) 59.59 54.62 58.01 56.59 69.22 66.85 63.69 65. 75.45 72.69 72.14 72.30 72.22 63.93 62.92 64.52 81.52 77.53 74.74 75.04 87.08 84.09 82.49 82.41 35.32 25.85 28.41 29.44 51.45 44.62 40.87 41. 61.60 54.95 52.05 52.13 74.65 68.88 70.18 72.42 79.05 76.12 75.08 76.88 81.12 78.73 77.97 78.95 57.82 50.30 51.98 53.18 67.60 63.47 60.91 62. 73.45 69.69 68.37 68.85 47.30 38.24 40.37 42.94 56.77 52.25 50.15 52.57 61.98 57.99 57.20 58.45 16 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs E. Inference Performance Analysis E.1. Kernel Design Our CUDA kernels implement binary GEMV operations tailored to the memory-bound regime typical of the decoding phase in LLM inference. The design centers on bit-packing to reduce global memory traffic, with latency-tolerant and matmul-free compute pipeline that leverages register-level staging. Weight Packing. To reduce memory traffic, each group of 32 columns is mapped to uint32_t, with +1 7 0 and 1 7 1. We then group the 32-bit words into uint2 or PackedBits3 (3 uint32_t weights with padding), for 2-bit (2 binary weights) and 3-bit (3 binary weights) models, respectively. Rows are interleaved into warp-sized groups, ensuring that warp issues full coalesced memory transactions when loading weights. Our efficient packing reduces the raw footprint of weights by factor of 32, compared to full-precision weights. Compute Pipeline. Each warp is assigned set of output rows to avoid inter-warp synchronization. Input activations (x) and column scales (g) are read as vectorized uint4 chunks. Binary signs are applied via lane-local bit shifts and XOR masks, instead of matrix multiplication. The kernel uses simple yet effective pipelining: while one tile of data is consumed, the subsequent tile is prefetched into registers. Accumulation proceeds using half2 fused multiply-add intrinsics (__hfma2), which increase arithmetic throughput without resorting to shared memory. Finally, reductions across threads in warp are performed with shuffle operations, and output scale factors (h) are applied in fp16 precision. Notably, our architecture enables per-path parallelizable computation - instead of an n-bit weight, we parallelize with 1-bit operations. Efficient weight packing and pipelining reduce global memory access and raise the utilization of execution units on the GPU. The kernel therefore shifts the limiting factor from raw memory bandwidth toward register throughput, yielding measurable efficiency gains during the decoding stage of LLM inference, showing remarkable performance. E.2. More Comparisons To provide more detailed analysis of the end-to-end inference speed, we benchmarked RaBiT against several key baselines: the full-precision (FP16) model, QTIP as the state-of-the-art Vector Quantization (VQ) method, and DBF, which features similar stacked binary architecture. For QTIP, we utilized the publicly available CUDA kernels from the official implementation3. For DBF, which also uses stacked binary design but executes its two paths sequentially, we developed 3https://github.com/Cornell-RelaxML/qtip Figure 4. End-to-end decoding throughput (tokens/second) for Llama2-7B on an NVIDIA RTX 4090 across various generated token lengths. RaBiTs parallel architecture consistently delivers superior performance over other 2-bit methods. 17 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs an optimized CUDA kernel that runs approximately 21% faster than their public Triton-based implementation to ensure fair and robust comparison. All evaluations were conducted on an NVIDIA RTX 4090 with torch.compile using the Llama2-7B model. The results, depicted in Figure 4, were benchmarked across range of generated token lengths (64, 128, 256, 512, and 1024) for comprehensive analysis. As expected, all 2-bit methods significantly outperform the FP16 baseline due to the 8 reduction in memory bandwidth requirements. More importantly, RaBiT demonstrates substantial performance advantage, achieving nearly twice the decoding throughput of the other 2-bit quantization methods. This speed-up stems directly from the efficiency of our parallel, matmul-free architecture. Unlike DBF, which is bottlenecked by its sequential computation of two binary paths, RaBiTs fully parallel design allows it to maximally leverage the benefits of its efficient binary cores. While the absolute tokens/second rate naturally decreases with longer generation sequences, the relative performance gap between the methods remains consistent, confirming the robustness of RaBiTs architectural advantage. F. Hyperparameters F.1. Training Details We detail the hyperparameters used for our Quantization-aware training (QAT) experiments in Table 10. All models were trained for 6 epochs using the Muon optimizer (Jordan et al., 2024) with cosine learning rate decay schedule. The models were initialized using our proposed function-aware strategy, with fixed SVID iteration count of Tmax = 20. Key hyperparameters, such as the learning rate and the I/O Channel Importance Scaling intensities (αin, αout), were fine-tuned for each specific model to achieve the best performance. All experiments were conducted on single node equipped with four NVIDIA H100 GPUs. Table 10. Hyperparameter Configuration. We detail the training settings including learning rates, intensities (αin, αout), and batch information for the Llama and Gemma model families. Training Setup Llama2 Llama3 Gemma3 Bit Target 7B 13B 8B 1B 4B 12B 3 Intensities (αin, αout) Iteration (Tmax) Learning Rate Epoch # GPUs # Training Hours Intensities (αin, αout) Iteration (Tmax) Learning Rate Epoch # GPUs # Training Hours (0.8, 0.65) 20 12e-6 6 1 4 39 (0.8, 0.65) 20 1e-5 6 1 4 46 (0.95, 0.45) 20 1e-5 6 1 4 (0.95, 0.45) 20 1e-5 6 1 4 88 (0.85, 0.7) 20 1e-5 6 1 4 38 (0.85, 0.7) 20 1e-5 6 1 4 44 (0.85, 0.7) 20 1e-5 6 1 4 8 (0.95, 0.7) 20 1e-5 6 1 4 23 (0.75, 0.6) 20 5e-6 6 1 4 - - - - - - - - - - - - - - - - - - F.2. Grid Search for I/O Channel Importance Scaling Intensities To determine the optimal intensity hyperparameters for our I/O Channel Importance Scaling (Section 4.3), we performed comprehensive grid search. The objective was to identify the values of αin and αout that minimized the initial Knowledge Distillation (KD) loss post-initialization. This process utilized calibration dataset of 128 samples randomly selected from the training data to measure the loss. The example results of this search on the Llama2-7B model are detailed in Table 11. We observed clear optimum, with the minimum initial KL divergence loss of 2,672 achieved at the configuration of αin = 0.80 and αout = 0.65. This finding underscores the importance of balanced preconditioning strategy that considers both input activation statistics and output gradient magnitudes. We repeated this grid search process for all other models to find their optimal alpha values. 18 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs Table 11. Grid search results for I/O Channel Importance Scaling Intensities (αin, αout) on Llama2-7B. The metric is the Initial KL Divergence Loss (Lower is better). The optimal configuration is highlighted in bold. αout 0.55 0.60 0.65 0.70 αin 0.75 0.80 0.85 0.90 3,100 2,932 3,167 3,083 2,932 2,938 2,672 2, 2,984 2,971 2,697 2,983 3,108 3,143 3,063 3,462 F.3. SVID Iteration Convergence Analysis The Iterative Residual SVID initialization (Section 4.3) aims to mitigate the scheduling bias inherent in standard greedy initialization. We analyzed the required number of iterations (Tmax) for convergence on the Llama2-7B model. We measured the Initial KL divergence loss as the iterations progressed from 1 (equivalent to Greedy SVID) up to 35. The results, shown in Figure 5, indicate that the initialization quality improves rapidly in the initial phase. The loss stabilizes significantly around 15 iterations, and the optimum is reached at 20 iterations. Beyond this point, further iterations do not provide additional benefits. Based on this analysis, we selected Tmax = 20 as the default setting for RaBiT initialization, providing an optimal balance between initialization quality and computational cost. Figure 5. Convergence analysis of Iterative Residual SVID on Llama2-7B. The metric is the Initial KL Divergence Loss (Lower is better). Convergence stabilizes around 20 iterations. G. Extended Analysis of Inter-Path Adaptation To provide more granular view of the training dynamics, we conduct layer-wise analysis of the Mean Squared Error (MSE) decomposition for the Llama2-7B model, visualized in Figure 6. This analysis offers two key insights into RaBiTs structural advantages over Standard QAT. First, the results empirically confirm our central hypothesis across the networks depth. For most layers, RaBiT consistently generates substantial negative covariance (the red-dashed component), which acts as significant loss-reducing bonus, thereby lowering the total MSE. In contrast, Standard QAT fails to establish this effective error-cancellation, exhibiting much smaller covariance term that provides negligible benefit. This provides strong visual evidence that RaBiTs coupled training successfully enforces the intended error-correction hierarchy, while Standard QAT suffers from the performance degradation of inter-path adaptation. Second, and more strikingly, the analysis reveals RaBiTs ability to overcome critical optimization challenge in extreme quantization: layer sensitivity. The Standard QAT baseline exhibits an exceptionally high MSE in the initial layers, phenomenon consistent with the known sensitivity of early network layers to input distributions and quantization errors, as also observed in other LLMs by (Zhang et al., 2025; 2024). RaBiT, however, dramatically suppresses this MSE peak. This suggests its benefits extend beyond merely enforcing anti-correlation. The fact that RaBiT tames this instability indicates that our method may resolve more fundamental bottleneck in MSE-based QAT that has historically hindered extreme quantization in conventional architectures. While we designed RaBiT to foster negative correlation, its success in stabilizing 19 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs Figure 6. Layer-wise MSE Decomposition in Llama2-7Bs down_proj layers. The bars compare the total Mean Squared Error (MSE) for Standard QAT (blue) and RaBiT (orange). The top of each bar represents the base error term (C ), while the red-dashed component visualizes twice the covariance (2 Cov). RaBiT consistently generates large negative covariance, which actively reduces the total MSE, demonstrating effective error cancellation. Notably, RaBiT also suppresses the extremely high MSE peak observed in the early layers of the Standard QAT baseline, indicating its robustness against layer sensitivity. these sensitive layers points to deeper robustness. full investigation into how residual coupling imparts this stability is compelling direction for future research. 20 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs H. Generated Samples Sample 1: Newton Prompt: Newton discovered the law of universal gravitation. Universal gravitation is"
        },
        {
            "title": "Generated Output",
            "content": "Newton discovered the law of universal gravitation. Universal gravitation is the force of gravity which draws objects towards the earth and other massive bodies. Gravity is the universal attraction between two objects and is proportional to the product of the masses of the two objects and inversely proportional to the square of the distance between them. Newton explained the force of gravity as being caused by the presence of matter in the universe. He stated that all matter attracted all other matter and that the tendency to move towards massive body was proportional to the product of the masses of the objects. Newton developed the law of universal gravitation so that the attraction between two bodies could be calculated. Sample 2: Adam Smith Prompt: Adam Smith wrote The Wealth of Nations. The Wealth of Nations is Generated Output Adam Smith wrote The Wealth of Nations. The Wealth of Nations is the first book to outline the modern principles of capitalism and to make the case for it. Smiths work is widely considered to be the first comprehensive system of political economy and one of the most important works of political philosophy. The Wealth of Nations is the foundation of modern capitalism. The Wealth of Nations is book by Adam Smith, first published in 1776, which describes the nature of economic systems. Adam Smith is regarded as the father of modern economics. The Wealth of Nations is widely considered to be the first comprehensive system of political economy and one of the most important works of political philosophy. It is also one of the most influential books on economics. Adam Smith is regarded as the father of modern economics. Sample 3: DNA double helix Prompt: Watson and Crick discovered the DNA double helix. The DNA double helix is Generated Output Watson and Crick discovered the DNA double helix. The DNA double helix is structure found in the nucleus of every living cell. The double helix is the most fundamental structure in DNA. The structure is made of two single strands of DNA. The double helix was discovered in 1953 by James Watson and Francis Crick. In 1953, James Watson and Francis Crick discovered the double helix structure of DNA. The double helix structure is the most fundamental structure in DNA. The double helix structure is made of two single strands of DNA. The double helix is found in the nucleus of every living cell. 21 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs I. Algorithms Algorithm 1 RaBiT Initialization: Calibrated Iterative Residual SVID 1: Require: Pretrained weight WFP, Number of paths k, Max iterations Tmax 2: Require: Calibration stats sin, sout and intensities αin, αout 3: Output: Initialized scales {(gi, hi)}k i=1 4: // Step 1: I/O Channel Importance-Calibrated Preconditioning 5: Normalize: sin sin/ max(sin), 6: Precondition: sαout sout sout/ max(sout) out WFP sαin in 7: // Step 2: Iterative Residual SVID 8: Initialize ˆW(0) 9: for = 1 to Tmax do 10: 11: for = 1 to do 0 for = 1, . . . , j<i (cid:16)P + i // Calculate target residual (Gauss-Seidel style update) R(t) ˆW(t) // Apply SVID to find the best rank-1 approximation ) SVID(R(t) (B(t) h(t) ˆW(t) end for , g(t) g(t) , h(t) B(t) ˆW(t1) j>i (cid:17) ) 12: 13: 14: 15: 16: 17: end for 18: // Step 3: Map scales back to the original weight domain 19: for = 1 to do 20: gi sαout hi sαin out g(Tmax) in h(Tmax) 21: 22: end for i=1. // Effective weight for the entire layer // Sequentially derive the i-th binary path // Initialize residual with the shared weight ˆWi gi Bi hi. ˆW(k) ˆW(k) + ˆWi. // Update residual for the next path Algorithm 2 RaBiT: Residual-Aware Binarization Training (One Step) 1: Parameters: Shared full-precision weight WFP; Scales {(gi, hi)}k 2: Input: Minibatch Input X, Targets T. 3: // 1. Forward Pass: On-the-fly Residual Coupling 4: R0 WFP. 5: ˆW(k) 0. 6: for = 1 to do 7: 8: Bi sign(Ri1). 9: 10: 11: 12: Ri Ri1 ˆWi. 13: end for 14: ˆW(k)X. 15: Calculate Loss L(Y, T). 16: // 2. Backward Pass 17: L/Y. 18: // Surrogate gradient for the shared weight WFP 19: WFP X. 20: // Gradients for scales (treating Bi as constant) 21: for = 1 to do 22: 23: end for 24: // 3. Parameter Update 25: Update {WFP, (gi, hi)k Compute gi and hi using , Bi, X, and other scales. // Compute layer output (Output gradient) i=1} using an optimizer with the computed gradients. RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs J. Core Kernel Code do { do { } while (0) CHECK_CUDA(x); CHECK_CONTIGUOUS(x); CHECK_F16(x); CHECK_CUDA(x); CHECK_CONTIGUOUS(x); CHECK_INT32(x); 1 #include <ATen/cuda/CUDAContext.h> 2 #include <c10/cuda/CUDAException.h> 3 #include <c10/cuda/CUDAGuard.h> 4 #include <c10/macros/Macros.h> 5 #include <cuda_fp16.h> 6 #include <cuda_runtime.h> 7 #include <torch/extension.h> 8 9 #include <sstream> 10 #include <string> 11 #include <type_traits> 12 13 // ======================================================================== 14 // === Macros & Constants === 15 // ======================================================================== 16 17 #define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be CUDA tensor\") 18 #define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\") 19 #define CHECK_F16(x) TORCH_CHECK(x.scalar_type() == torch::kFloat16, #x \" must be float16 tensor\") 20 #define CHECK_INT32(x) TORCH_CHECK(x.scalar_type() == torch::kInt32, #x \" tensor dtype must be int32\") 21 22 #define CHECK_CUDA_CONT_F16(x) 23 24 25 26 27 28 29 #define CHECK_CUDA_CONT_INT32(x) 30 31 32 33 34 35 36 constexpr int WARP_SIZE = 32; 37 constexpr uint32_t SIGN_MASK_U32 = 0x80008000u; 38 constexpr unsigned FULL_MASK = 0xffffffff; 39 40 // ======================================================================== 41 // === Device Helper Functions === 42 // ======================================================================== 43 44 static __device__ __forceinline__ uint32_t half2_to_uint32(const half2& h) { 45 46 } 47 48 static __device__ __forceinline__ half2 uint32_to_half2(uint32_t v) { 49 50 } 51 52 // Sign-flipping utility. 53 // Logic: bits determine if we flip the sign of the float16 values packed in uint4. 54 __device__ __forceinline__ uint4 apply_sign(uint4 x4, uint32_t bits, int shift_base) { 55 56 57 58 59 60 61 } 62 63 // ======================================================================== 64 // === Kernel Implementation === 65 // ======================================================================== 66 67 template <unsigned NUM_THREAD, unsigned NUM_ROW_PER_WARP = 2, unsigned NUM_ACC = 4, unsigned uint32_t shifted = bits << shift_base; x4.x ^= shifted & SIGN_MASK_U32; x4.y ^= (shifted << 1) & SIGN_MASK_U32; x4.z ^= (shifted << 2) & SIGN_MASK_U32; x4.w ^= (shifted << 3) & SIGN_MASK_U32; return x4; return reinterpret_cast<const uint32_t&>(h); return reinterpret_cast<const half2&>(v); } while (0) NUM_PIPELINE_STAGE = 2> 68 __forceinline__ __device__ void rabit_2bit_half_impl( 69 70 71 72 73 74 75 76 77 78 const half* __restrict__ x, const half* __restrict__ scale_g_0, const uint32_t* __restrict__ Wbits, const half* __restrict__ scale_h_0, const half* __restrict__ scale_g_1, const half* __restrict__ scale_h_1, half* __restrict__ y, int N, int M) { // Constraints 23 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs static_assert(NUM_ROW_PER_WARP == 2, \"Only 2 rows per warp supported currently\"); static_assert(NUM_THREAD % WARP_SIZE == 0, \"Threads must be multiple of warp size\"); constexpr unsigned NUM_WARP = NUM_THREAD / WARP_SIZE; constexpr unsigned NUM_ROW_PER_BLOCK = NUM_ROW_PER_WARP * NUM_WARP; // Identifiers const int tid = threadIdx.x % WARP_SIZE; const int wid = threadIdx.x / WARP_SIZE; // Vectorized pointers (loading 128-bit chunks / 8 halves at time) const uint4* x8s = reinterpret_cast<const uint4*>(x); const uint4* sg8s_0 = reinterpret_cast<const uint4*>(scale_g_0); const uint4* sg8s_1 = reinterpret_cast<const uint4*>(scale_g_1); // Data Structures for Pipelining struct RowAccum { half2 plane[2][NUM_ACC]; }; struct ColumnScalePair { uint4 g0; uint4 g1; }; struct StageTile { uint4 x; ColumnScalePair scale; uint32_t bits; }; struct RowContext { int index; RowAccum acc; }; RowContext rows[NUM_ROW_PER_WARP] = {}; StageTile pipe[NUM_PIPELINE_STAGE]; // Setup Row Indices const int block_row_base = blockIdx.x * NUM_ROW_PER_BLOCK; #pragma unroll for (unsigned irow = 0; irow < NUM_ROW_PER_WARP; ++irow) { rows[irow].index = block_row_base + static_cast<int>(wid) * NUM_ROW_PER_WARP + static_cast<int>(irow); } // Weight Bit Indexing // N8 = Number of 8-half chunks (128-bit words) const int N8 = >> 3; const int wordsN = N8; const int tile_stride = wordsN; const int tile_idx = blockIdx.x * NUM_WARP + wid; const uint32_t* tile_ptr = Wbits + tile_idx * tile_stride; // --- Helper Lambdas --- auto load_to_reg = [&](int ireg, int idx8) { StageTile& stage = pipe[ireg]; stage.x = x8s[idx8]; stage.scale.g0 = sg8s_0[idx8]; stage.scale.g1 = sg8s_1[idx8]; stage.bits = *(tile_ptr + idx8); }; // Accumulator indices for circular buffer usage or simple unrolling const int MASK = NUM_ACC - 1; auto fma = [&](const uint4& scale, const uint4& x, half2 acc[NUM_ACC]) { acc[0] = __hfma2(uint32_to_half2(scale.x), uint32_to_half2(x.x), acc[0]); acc[1 & MASK] = __hfma2(uint32_to_half2(scale.y), uint32_to_half2(x.y), acc[1 & MASK]); acc[2 & MASK] = __hfma2(uint32_to_half2(scale.z), uint32_to_half2(x.z), acc[2 & MASK]); acc[3 & MASK] = __hfma2(uint32_to_half2(scale.w), uint32_to_half2(x.w), acc[3 & MASK]); }; auto calc_main = [&](int ireg) { StageTile& stage = pipe[ireg]; #pragma unroll for (unsigned irow = 0; irow < NUM_ROW_PER_WARP; ++irow) { RowAccum& acc = rows[irow].acc; // Apply sign logic based on bits (0 or 1 selects path) // 0 + 8*irow and 4 + 8*irow are specific bit shifts for the packed format fma(stage.scale.g0, apply_sign(stage.x, stage.bits, 0 + 8 * irow), acc.plane[0]); fma(stage.scale.g1, apply_sign(stage.x, stage.bits, 4 + 8 * irow), acc.plane[1]); } }; 24 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs // --- Main Loop (Pipelined) --- int idx_load = tid; int idx_calc = tid; // Prologue: Fill pipeline #pragma unroll for (int istg = 0; istg < NUM_PIPELINE_STAGE; ++istg) { if (idx_load < N8) { load_to_reg(istg, idx_load); } idx_load += WARP_SIZE; } // Steady state const int bound_mainloop = N8 - (NUM_PIPELINE_STAGE * WARP_SIZE); while (idx_calc < bound_mainloop) { #pragma unroll for (int istg = 0; istg < NUM_PIPELINE_STAGE; ++istg) { calc_main(istg); } #pragma unroll for (int istg = 0; istg < NUM_PIPELINE_STAGE; ++istg) { if (idx_load < N8) { load_to_reg(istg, idx_load); } idx_load += WARP_SIZE; idx_calc += WARP_SIZE; } } // Epilogue: Drain pipeline #pragma unroll for (int istg = 0; istg < NUM_PIPELINE_STAGE; ++istg) { if (idx_calc < N8) { calc_main(istg); idx_calc += WARP_SIZE; } if (idx_load < N8) { load_to_reg(istg, idx_load); idx_load += WARP_SIZE; } } // --- Reduction & Output --- auto warp_sum_half = [](half v) { half2 h2 = __halves2half2(v, __float2half(0.0f)); #pragma unroll for (int off = WARP_SIZE >> 1; off > 0; off >>= 1) { half2 other = __shfl_xor_sync(FULL_MASK, h2, off); h2 = __hadd2(h2, other); } return __low2half(h2); }; #pragma unroll for (unsigned irow = 0; irow < NUM_ROW_PER_WARP; ++irow) { RowContext& row = rows[irow]; const int out_i = row.index; RowAccum& acc = row.acc; half2 lane_acc_0 = __float2half2_rn(0.0f); half2 lane_acc_1 = __float2half2_rn(0.0f); #pragma unroll for (unsigned iacc = 0; iacc < NUM_ACC; ++iacc) { lane_acc_0 = __hadd2(lane_acc_0, acc.plane[0][iacc]); lane_acc_1 = __hadd2(lane_acc_1, acc.plane[1][iacc]); } // Horizontal sum within registers (half2 -> half) half sum0_h = __hadd(__low2half(lane_acc_0), __high2half(lane_acc_0)); half sum1_h = __hadd(__low2half(lane_acc_1), __high2half(lane_acc_1)); // Apply final scaling factors (h0, h1) and combine paths half lane_total_h = __hfma(sum0_h, scale_h_0[out_i], __hmul(sum1_h, scale_h_1[out_i])); // Cross-lane reduction half warp_sum_h = warp_sum_half(lane_total_h); 25 RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs if (tid == 0) { y[out_i] = warp_sum_h; 241 242 243 244 245 246 } 247 248 template <unsigned NUM_THREAD, unsigned NUM_ROW_PER_WARP = 2, unsigned NUM_ACC = 4, unsigned } } NUM_PIPELINE_STAGE = 2> rabit_2bit_half_impl<NUM_THREAD, NUM_ROW_PER_WARP, NUM_ACC, NUM_PIPELINE_STAGE>( x_ptr_for_this_seq, scale_g_0, Wbits, scale_h_0, scale_g_1, scale_h_1, y_ptr_for_this_seq, N, M); [&]() { if (!found && runtime_value == values) { template <typename F, typename U> void operator()(const char* name, runtime_value, F&& func) const { bool found = false; // Fold expression to iterate over template values ( const int = blockIdx.y; // Offset for batch size (sequence length) const half* x_ptr_for_this_seq = + * N; half* y_ptr_for_this_seq = + * M; const half* __restrict__ x, const half* __restrict__ scale_g_0, const uint32_t* __restrict__ Wbits, const half* __restrict__ scale_h_0, const half* __restrict__ scale_g_1, const half* __restrict__ scale_h_1, half* __restrict__ y, int N, int M) { 249 __launch_bounds__(NUM_THREAD) __global__ void rabit_2bit_half_dyn_kernel( 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 } 267 268 // ======================================================================== 269 // === Template Dispatcher === 270 // ======================================================================== 271 272 // Helper to sweep through compile-time list of values. 273 template <typename T, T... values> 274 struct parameter_sweep { 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 }; 296 297 // ======================================================================== 298 // === C++ Interface === 299 // ======================================================================== 300 301 torch::Tensor rabit_2bit_half_dyn_forward( 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 const int seqlen = x.dim() == 1 ? 1 : static_cast<int>(x.size(0)); const int = static_cast<int>(x.size(-1)); const int = static_cast<int>(scale_h_0.size(0)); CHECK_CUDA_CONT_F16(x); CHECK_CUDA_CONT_F16(scale_g_0); CHECK_CUDA_CONT_INT32(Wbits); CHECK_CUDA_CONT_F16(scale_h_0); CHECK_CUDA_CONT_F16(scale_g_1); CHECK_CUDA_CONT_F16(scale_h_1); // Validations if (!found) { }(), ...); } } } TORCH_CHECK(x.dim() == 1 x.dim() == 2, \"Dynamic Half: must be 1D or 2D\"); torch::Tensor x, torch::Tensor scale_g_0, torch::Tensor Wbits, torch::Tensor scale_h_0, torch::Tensor scale_g_1, torch::Tensor scale_h_1, int64_t num_thread, int64_t num_row_per_warp, int64_t num_acc, int64_t num_pipeline_stage) { std::forward<decltype(func)>(func)(std::integral_constant<T, values>{}); found = true; std::stringstream ss; bool is_first = true; (((is_first ? ss : (ss << \", \")) << values, is_first = false), ...); TORCH_CHECK(false, name, \" should be one of [\", ss.str(), \"], but \", runtime_value, \" was given\"); RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs TORCH_CHECK(scale_g_0.size(0) == N, \"Dynamic Half: scale_g_0 size mismatch\"); TORCH_CHECK(scale_h_0.size(0) == M, \"Dynamic Half: scale_h_0 size mismatch\"); TORCH_CHECK(scale_g_1.size(0) == N, \"Dynamic Half: scale_g_1 size mismatch\"); TORCH_CHECK(scale_h_1.size(0) == M, \"Dynamic Half: scale_h_1 size mismatch\"); TORCH_CHECK((N % 32 == 0), \"Dynamic Half: must be multiple of 32\"); TORCH_CHECK(num_row_per_warp > 0, \"Dynamic Half: num_row_per_warp must be positive\"); const int rows_per_warp = static_cast<int>(num_row_per_warp); TORCH_CHECK(M % rows_per_warp == 0, \"Dynamic Half: rows_per_warp must divide output size\"); TORCH_CHECK(Wbits.dim() == 2 && Wbits.size(0) == / rows_per_warp && Wbits.size(1) == / 8, \"Dynamic Half: Wbits shape mismatch\"); auto = torch::empty({(long long)seqlen, (long long)M}, x.options()); at::cuda::CUDAGuard guard(x.device()); auto stream = at::cuda::getCurrentCUDAStream(); // Template Sweepers parameter_sweep<unsigned, 32u, 64u, 128u, 256u, 512u, 1024u> sweep_num_thread; parameter_sweep<unsigned, 2> sweep_row_per_warp; parameter_sweep<unsigned, 1, 2, 4> sweep_num_acc; parameter_sweep<unsigned, 1, 2, 3, 4> sweep_pipeline_stage; // Dispatcher auto kernel_func = [=](auto num_thread_c, auto num_row_per_warp_c, auto num_acc_c, auto num_pipeline_stage_c) { constexpr unsigned num_thread_v = num_thread_c.value; constexpr unsigned num_row_per_warp_v = num_row_per_warp_c.value; constexpr unsigned num_acc_v = num_acc_c.value; constexpr unsigned pipeline_stage_v = num_pipeline_stage_c.value; // Runtime check against compile time constant wrapper if (num_row_per_warp_v != static_cast<unsigned>(rows_per_warp)) return; constexpr unsigned NUM_ROW_PER_BLOCK = (num_thread_v / WARP_SIZE) * num_row_per_warp_v; TORCH_CHECK(M % NUM_ROW_PER_BLOCK == 0, \"Output size \", M, \" must be div by block rows \", NUM_ROW_PER_BLOCK); dim3 grid_dim(M / NUM_ROW_PER_BLOCK, seqlen); rabit_2bit_half_dyn_kernel<num_thread_v, num_row_per_warp_v, num_acc_v, pipeline_stage_v> <<<grid_dim, num_thread_v, 0, stream>>>( reinterpret_cast<const half*>(x.data_ptr<at::Half>()), reinterpret_cast<const half*>(scale_g_0.data_ptr<at::Half>()), reinterpret_cast<const uint32_t*>(Wbits.data_ptr<int32_t>()), reinterpret_cast<const half*>(scale_h_0.data_ptr<at::Half>()), reinterpret_cast<const half*>(scale_g_1.data_ptr<at::Half>()), reinterpret_cast<const half*>(scale_h_1.data_ptr<at::Half>()), reinterpret_cast<half*>(y.data_ptr<at::Half>()), N, M); C10_CUDA_KERNEL_LAUNCH_CHECK(); }; // Execute Sweep sweep_num_thread(\"num_thread\", num_thread, [&](auto num_thread_c) { sweep_row_per_warp(\"num_row_per_warp\", num_row_per_warp, [&](auto num_row_per_warp_c) { sweep_num_acc(\"num_acc\", num_acc, [&](auto num_acc_c) { sweep_pipeline_stage(\"num_pipeline_stage\", num_pipeline_stage, [&](auto num_pipeline_stage_c) { kernel_func(num_thread_c, num_row_per_warp_c, num_acc_c, num_pipeline_stage_c); }); }); }); }); if (x.dim() == 1) { y.squeeze_(0); } return y; 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 }"
        }
    ],
    "affiliations": [
        "Samsung Research, Seoul, Korea"
    ]
}