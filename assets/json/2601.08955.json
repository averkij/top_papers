{
    "paper_title": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models",
    "authors": [
        "Youwei Liu",
        "Jian Wang",
        "Hanlin Wang",
        "Beichen Guo",
        "Wenjie Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (\\texttt{ITP}), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially \\textit{observable} and \\textit{imaginable} Markov decision process to guide policy learning. We instantiate \\texttt{ITP} with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that \\texttt{ITP} significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks."
        },
        {
            "title": "Start",
            "content": "Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models Youwei Liu2,1*, Jian Wang1, Hanlin Wang1, Beichen Guo1, Wenjie Li1 1 The Hong Kong Polytechnic University 2 Central South University loyiv5477@gmail.com jian51.wang@polyu.edu.hk {hanlin-henry.wang,beichen.guo}@connect.polyu.hk cswjli@comp.polyu.edu.hk 6 2 0 J 3 1 ] . [ 1 5 5 9 8 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (ITP), unified framework for agent learning via lookahead imagination, where an agents policy model interacts with the learned world model, yielding multi-step imagined trajectories. Since the imagination horizon may vary by tasks and stages, we introduce novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating partially observable and imaginable Markov decision process to guide policy learning. We instantiate ITP with both training-free and reinforcementtrained variants. Extensive experiments across representative agent benchmarks demonstrate that ITP significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents reasoning capability, providing valuable insights into addressing broader, complex tasks. Our code and data will be released."
        },
        {
            "title": "Introduction",
            "content": "The emergence of Large Language Models (LLMs) has sparked paradigm shift in autonomous agents, enabling them to reason and interact across wide range of digital and physical environments (Li et al., 2023; Wang et al., 2024a; Fung et al., 2025). LLM-based agents primarily leverage immediate * This work was conducted while Youwei Liu was remote research assistant at the Hong Kong Polytechnic University. Corresponding author. 1 Figure 1: Comparison between our ITP framework and conventional agent learning frameworks. observations and historical interaction traces to facilitate decision-making (Yao et al., 2023; Shinn et al., 2023). Despite the impressive performance achieved, most agents remain constrained by shallow grounding, state where they perceive the environment but lack deep, causal understanding of how their current actions will ultimately reshape the environment. Without the ability to project into the future, agents are prone to catastrophic failures, discovering erroneous actions or state conflicts only after they have been irreversibly executed. To bridge this grounding gap, world models (LeCun, 2022; Hafner et al., 2023) that focus on future state modeling have emerged as promising solution, enabling agents to simulate environmental dynamics and rehearse actions in mental sandbox (Wang et al., 2025d). Contemporary methods typically employ world models for single-step verification or with fixed-horizon rollouts (Hao et al., 2023; Qiao et al., 2024; Zhang et al., 2025). Such rigid strategies are often suboptimal, as they fail to capture long-term dependencies in complex tasks (e.g., household embodied tasks). Furthermore, they are prone to suffer from high computational costs with unnecessary deep rollouts for trivial actions. truly intelligent agent should be deliberative, allocating deep foresight adaptively to resolve potential state conflicts and account for long-term dependencies in pivotal, high-stakes decisions. In this paper, we propose Imagine-then-Plan (ITP), framework that empowers agents to perform task planning with world model-based foresight. The core of ITP is to move beyond passive observation and perform proactive, deliberative rehearsal phase, where decisions are conditioned on both present observations and potential futures. This requires the agent to internally imagine multi-step future trajectories by looking ahead. Unlike previous rigid approaches, ITP introduces an adaptive lookahead mechanism that dynamically scales the imagination horizon by trading off the ultimate goal and estimated task progress. More importantly, this shift necessitates new conceptualization of the agents decision-making process. We move beyond the Partially Observable Markov Decision Process (POMDP) (Åström, 1965; Song et al., 2024; Wang et al., 2025b) toward Partially Observable and Imaginable MDP (POIMDP). As illustrated in Figure 1, the agents action policy is optimized over dual-stream representation: the concrete present (observable) and the foresighted future (imaginable). These imagined trajectories provide rich signals, such as anticipating goal progress or detecting potential bottlenecks, allowing the agent to close the loop between planning action sequences and estimating their possible consequences. This implicit feedback enables the agent to perform self-correction when necessary, significantly reducing the need for expensive interactions with the real environment. We instantiate ITP in two variants: trainingfree (ITPI) variant that uses reflection as the adaptive lookahead for plug-and-play enhancement of LLM agents, and reinforcement-trained (ITPR) variant that leverages imagined futures to optimize the agent policy more effectively and more efficiently. Extensive experiments demonstrate that both variants significantly improve task success rates. Further analyses validate the vital role of our adaptive lookahead mechanism. Our contributions are summarized as follows: 1) We conceptualize the partially observable and imaginable Markov decision process (POIMDP), laying solid foundation for integrating imagined futures and historical interactions into agent decisionmaking. 2) We propose Imagine-then-Plan (ITP), framework that incorporates world model-based imagination with an adaptive lookahead mechanism, which provides deliberative guidance for action policy planning. 3) We demonstrate through training-free and reinforcement-trained variants that ITP significantly improves the success rates of LLM-based agents, providing valuable insights into addressing complex, long-horizon tasks."
        },
        {
            "title": "2 Preliminaries",
            "content": "Problem Formulation. The reasoning process of LLM agents is often formulated as Partially Observable Markov Decision Process (POMDP), defined by (S, A, O, , R). Here, denotes the environment state space, the action space, and the observation space. : represents the state transition function, and : [0, 1] denotes the reward function that evaluates task performance. At each time step t, the agent receives an observation ot and selects an action at πθ(ht), where ht = (o1, a1, . . . , ot) denotes the interaction history. Executing at induces transition to new latent state st+1 (st, at), from which the environment emits the next observation ot+1. The interaction terminates when the agent reaches terminal state or exceeds predefined maximum number of steps. LLMs as World Models. world model is predictive model of environment dynamics that estimates future states conditioned on actions (LeCun, 2022). In text-based environments, the environment state is typically represented as text and observable to the agent. We treat textual observations as state representations, denoted as st at time step t. Under this formulation, LLMs can be interpreted as world models, as they capture transition regularities by predicting the next state given the current interaction context (Zhang et al., 2025; Li et al., 2025). Concretely, textual world model parameterized by an autoregressive LLM defines the conditional distribution pϕ(st+1st, at). The distribution is factorized at the token level, and the model generates the next state sequentially. Such world models serve as powerful proxies for environment dynamics and enable planning in language-based agents. 2 Figure 2: Overview of the proposed Imageine-then-Plan (ITP) framework. It consists of two variants: (a) ITPI, which is training-free and enables LLM agents to learn from the imagination at inference time. (b) ITPR, which leverages imagined futures to optimize the action policy more effectively and more efficiently."
        },
        {
            "title": "3 Method",
            "content": "We propose Imagine-then-Plan (ITP), framework that equips LLM-based agents with adaptive lookahead via learned world models. As illustrated in Figure 2, ITP enables agents to condition decisions on both the observable present and imagined future trajectories."
        },
        {
            "title": "3.1 World Model Training",
            "content": "As shown in Figure 2, we first fine-tune base LLM on expert demonstrations Dexp to obtain an initial agent policy πθ0. This warm-up establishes basic capability to produce executable actions, serving as the foundation for the agents exploration. We ask the agent to perform rollouts in the environment, obtaining the rollout trajectories Droll. As introduced in 2, we learn LLM-based world model Mϕ that approximates the environment dynamics pϕ(ss, a), where and denote the current state and action, respectively. represents the next-step state. To ensure the world model is grounded and robust to out-of-distribution actions, we train it on joint dataset DWM = Dexp Droll. The world model Mϕ is optimized by minimizing the negative log-likelihood as follows: LWM(ϕ) = E(s,s,a)DWM (cid:2)log pϕ(ss, a)(cid:3). (1)"
        },
        {
            "title": "3.2 Lookahead Imagination and POIMDP",
            "content": "To integrate world-model foresight into decision making, we extend the standard POMDP (as introduced in 2) to Partially Observable and Imaginable Markov Decision Process (POIMDP). Under this formulation, the agent is endowed with lookahead imagination operator induced by the learned world model. Formally, given the current observed state st and lookahead horizon Kt {0, 1, . . . , Kmax} at each time step t, the agent policy πθ and the world model Mϕ interact for Kt steps within mental sandbox. This imagination process is given by: πθ ˆat πθ . . . Mϕ ˆst+ πθ ˆat+Kt1 Mϕ ˆst+Kt. (2) This yields an imagined future trajectory ˆτ (Kt) = i=0 , where ˆat+i πθ(st, ˆτ (i) {(ˆat+i, ˆst+i+1)}Kt1 ) and ˆst+i+1 Mϕ(ˆst+i, ˆat+i). The objective of the agent policy is to yield an appropriate action at conditioned on both the observable state st and the imagined future ˆτ (Kt) . As such, our POIMDP formulates the policy decision as follows: at πθ( st, ˆτ (Kt) ). (3) This allows the agent to anticipate goal progress or detect potential bottlenecks before generating the next action, enabling the agent to perform selfcorrection when necessary."
        },
        {
            "title": "3.3 Planning with Adaptive Lookahead",
            "content": "To achieve effective task planning, key challenge is determining how far the agent should imagine. Short-horizon imagination may miss long-term dependencies, while excessive rollouts can amplify model errors and incur unnecessary computation. To resolve this, we aim to adaptively select the imagination horizon Kt based on the estimated task progress against the ultimate goal. We instantiate ITP via two distinct variants to provide both flexibility and optimization: (i) ITPI, which is an 3 inference-time method that learns from the imagination, and (ii) ITPR, which is reinforcementtrained method that jointly optimizes the lookahead horizon selection and action policy. In-Imagination Learning (ITPI) 3.3.1 ITPI is training-free variant designed for improving LLM agents during inference time. In this mode, both the agent policy and the world model remain frozen, and the agent relies on its inherent capabilities to perform deliberative reasoning. At each step t, the agent receives the current state st and executes three-stage Imagine-then-Plan procedure: 1) Adaptive horizon selection: The agent first assesses the task instruction and the current state st to determine an appropriate imagination horizon Kt {0, 1, . . . , Kmax}. This allows the agent to allocate deeper foresight to highstakes decisions while maintaining computational efficiency for trivial actions. 2) World-model imagination: The agent invokes its world model to perform mental rehearsal. By interacting with Mϕ for Kt steps, the agent obtains multi-step future trajectory ˆτ (Kt) . 3) Reflective policy generation: Rather than directly taking the first action of the imagined sequence, the agent uses ˆτ (Kt) as implicit feedback for reflective self-refinement. Specifically, the agent is prompted to self-reflect on the imagined trajectory to evaluate task progress, determine if the predicted states move closer to the ultimate task goal, and detect potential conflicts, bottlenecks, or catastrophic failures before they are irreversibly executed in the real environment. The agent is asked to perform self-refinement and then select the optimal next action at, given by: at πθ( st, Reflect(ˆτ (Kt) )). (4) By grounding decisions in these imagined futures, ITPI transforms passive observation into proactive deliberation, significantly improving task success rates without additional training."
        },
        {
            "title": "3.3.2 Reinforced Training (ITPR)\nCompared to ITPI that relies on inference-time rea-\nsoning, ITPR aims to explicitly learn when and\nhow long to imagine. We augment the agent with\na lightweight K-head predictor Pθ(Kt|st), which\nis a linear layer built on top of a backbone LLM\nthat predicts distributions over imagination hori-\nzons. The action policy and predictor are optimized\njointly with the following three stages.",
            "content": "4 Stage 1: Pseudo-Labeling Lookahead Horizon. key obstacle for learning adaptive lookahead is that expert trajectories provide only (st, ) pairs but do not specify the right lookahead horizon. We therefore construct pseudo labels using the world model Mϕ and the initial agent policy πθ0. Specifically, we use teacher-forced expert actions to rollout on the frozen world model Mϕ by looking ahead one step and obtain future states, from which we derive lookahead-conditioned trajectories {ˆτ (k) k=0 . We then score each candidate step by the log-likelihood of expert actions under πθ0, and select the optimal lookahead step by: }Kmax Kt = arg max 0kKmax (cid:104) log pθ0(a st, ˆτ (k) )λK (cid:105) , (5) where λK is hyperparameter controlling the lookahead penalty. Based on the selection criteria in Eq. (5), we obtain dataset DK containing the pseudo-labels of the optimal lookahead step for each action in the expert trajectory. Stage 2: Warm-Up Training. Starting from the initial agent policy πθ0, we further fine-tune the agent to jointly (i) act under lookahead-conditioned pseudo-trajectories and (ii) predict the required lookahead step. Specifically, given labeled tut , Kt)}, we condition the agent polples {(st, icy on ˆτ ( Kt) and train πθ(atst, ˆτ ( Kt) ) to imitate the expert action , with standard negative loglikelihood loss Lπ(θ). Meanwhile, we train the K-head predictor to estimate the pseudo label Kt, with similar negative log-likelihood loss LK(θ). Our warm-up training is given by: LWT(θ) = Lπ(θ) + η LK(θ). (6) where η is weighted coefficient. This yields (i) competent agent policy that can generate reliable actions and (ii) an adaptive lookahead horizon predictor that approximates lookahead steps, providing stable initialization for the subsequent online reinforcement optimization. Stage 3: Online Optimization. To balance task performance and imagination cost, we further refine the agent policy through online reinforcement learning. At each step t, the agent samples lookahead step Kt from the K-head predictor, invokes the frozen world model Mϕ to generate Kt-step imagined trajectory ˆτ (Kt) , and subsequently samples an action at πθ( st, ˆτ (Kt) ). As illustrated in Figure 2, we employ reward function that augments the environment reward renv with explicit penalties for computational and interaction overhead, given by: rt+1 = renv λKKt λstep, (7) (cid:16) where λK is the lookahead penalty coefficient, and λstep is factor discouraging reasoning redundancy. Specifically, we utilize the Advantage ActorCritic (A2C) algorithm (Mnih et al., 2016) to jointly optimize the action policy and the K-head parameters. The objective is decomposed into three components: (i) an actor term Lact(θ) (cid:17)(cid:105) (cid:104) log pθ(Kt st) + log πθ(at st, ˆτ (Kt) At ) jointly updates the lookahead predictor that (ii) Critic regression and the agent policy; (Vθ(st) ˆVt)2(cid:105) term Lvalue(θ) that trains Value-head to match the TD learning target; and (iii) an entropy regularizer Lent(θ) E[H(pθ(Kt st))] that encourages sufficient exploration over the lookahead steps to prevent premature convergence to sub-optimal horizons. The final training objective is: (cid:104) LA2C(θ) = Lact(θ)+αLvalue(θ)+βLent(θ), (8) where α and β are hyperparameters balancing value estimation and exploration. Algorithm 1 shows the overview of ITPRs training process. At inference time, ITPR utilizes the learned Khead to perform adaptive lookahead, following similar deliberative procedure as ITPI."
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "Benchmarks. We evaluate ITP on two representative agent benchmarks: ALFWorld (Shridhar et al., 2020) for embodied household tasks and ScienceWorld (Wang et al., 2022) for simulated science experiments. All tasks in these two environments can be formally described as POMDPs. Please refer to Appendix for more details of the two environments. Algorithm 1 ITPR: Reinforced Training with Adaptive Lookahead Input: Dataset = {(st, )}; Agent policy πθ0 ; World model Mϕ; Parameters Kmax, λK , η, α, β. Output: Agent policy πθ; Lookahead predictor Pθ. // Stage 1: Pseudo-Labeling Lookahead Horizon k=0 via Mϕ & teacher-forced . 1: for each episode in do Cache {ˆτ (k) }Kmax 2: for each step do 3: 4: 5: 6: 7: 8: end for end for st, ˆτ (k) St(k) log pθ0 (a ). Kt arg max (cid:2)St(k) λK k(cid:3). Store (st, , Kt) in DK . // Stage 2: Warm-Up Training with Lookahead 9: for each episode in DK do Sample (st, 10: Update θ via Eq. (6). 11: 12: end for , Kt) DK . // Stage 3: Online ActorCritic Optimization Sample Kt Pθ(K st), query Mϕ for ˆτ (Kt) Update θ via Eq. (8). 13: for each episode in do 14: 15: 16: end for 17: return πθ and Pθ. . other backbone series as world models in 4.4. All models are prompted using their official chat templates to eliminate performance variances caused by formatting inconsistencies. Baseline Methods. We compare ITP against two categories of baselines. (1) Prompting-based methods: CoT (Wei et al., 2022) elicits reasoning capabilities by prompting the agent with step-bystep rationales. ReAct (Yao et al., 2023) interleaves reasoning and action to solve interactive tasks. RAP (Hao et al., 2023) leverages the LLM as both world model and policy model, employing Monte Carlo Tree Search to perform planning. (2) Training-based methods: SFT (Chen et al., 2023) conducts behavioral cloning on expert trajectories. WKM (Qiao et al., 2024) trains parametric world knowledge model that provides global task priors and local dynamic state knowledge to guide planning. IWM (Zhang et al., 2025) augments imitation learning with an implicit world-modeling objective to encourage the policy to internalize environment dynamics. Backbone Models. To ensure fair comparison, all methods are instantiated using the same suite of backbone models: Qwen2.5-7B (Yang et al., 2024), Qwen3-8B (Yang et al., 2025), and Llama-3-8BInstruct (Dubey et al., 2024). For our main experiments, we employ Qwen3-8B as the backbone for training the world model. We further evaluate Evaluation Metrics. We adopt success rate (SR) as the evaluation metric, defined as the percentage of episodes that successfully achieve the task goal (Feng et al., 2025; Wang et al., 2025a). For ALFWorld, we report SR on each task, including Pick & Place (PICK), Examine in Light (LOOK), Clean & Place (CLEAN), Heat & Place (HEAT), Backbone Type Method PICK CLEAN HEAT COOL LOOK PICK2 Overall Seen Unseen ALFWorld ScienceWorld Prompting CoT ReAct RAP Qwen2.5-7B ITPI (Ours) Training SFT WKM IWM 17.14 20.00 40.00 65.71 85.71 77.14 90.60 ITPR (Ours) 94. Prompting CoT ReAct RAP Qwen3-8B ITPI (Ours) Training SFT WKM IWM 14.29 25.71 42.86 82.86 71.43 80.00 85.71 ITPR (Ours) 97.14 CoT ReAct RAP 17.14 22.86 25.71 Prompting 18.52 22.22 33.33 25.93 66.67 77.78 85.20 88. 14.81 22.22 37.04 25.93 70.37 77.78 85.19 88.88 14.81 22.22 25.93 18.75 18.75 6. 16.00 20.00 32.00 15.38 23.08 15.38 0.00 0.00 20.83 14.29 17.14 27.86 3.09 8.24 10.30 4.63 9.93 16. 25.00 24.00 30.77 25.00 35.71 16. 17.88 56.25 75.00 88.20 68.00 76.00 84.20 38.46 76.92 42.90 87.50 53. 76.00 12.50 12.50 37.50 12.00 12.00 16.00 15.38 7.69 15.38 66.67 75.00 76.90 91. 12.50 25.00 4.17 67.86 76.43 82.80 55.67 54.12 60.82 49.00 56.29 57.61 85.07 62. 58.94 13.57 19.29 28.57 2.44 9.79 15.46 1.99 8.61 27.14 12.50 16. 23.08 54.17 41.43 20.61 19.86 68.75 81.25 87. 72.00 80.00 84.00 69.23 76.92 46.15 93.75 88.00 76.92 18.75 6.25 6. 16.00 24.00 32.00 15.38 23.08 7.69 70.83 79.17 87.50 79.17 8.33 25.00 25.00 70.71 79.29 82. 56.70 60.31 59.27 49.67 47.68 54.30 88.57 61.85 56.95 15.00 21.43 22. 3.09 9.27 11.34 3.31 13.24 17.21 Llama3.1-8B ITPI (Ours) 57.14 37. 31.25 28.00 23.08 33.33 37.86 19. 19.20 Training"
        },
        {
            "title": "SFT\nWKM\nIWM",
            "content": "85.71 85.71 87.50 ITPR (Ours) 88.57 85.20 85.19 88.90 92.59 82.40 75.00 82. 89.50 80.00 94.70 85.70 38.46 85.90 53.80 79.17 84.60 79.28 77.86 85.90 57.21 61.34 57.56 50.33 54.96 56. 93.75 92.00 46.15 91.67 87.14 63. 57.61 Table 1: Evaluation of task success rates (%) across ALFWorld and ScienceWorld benchmarks. Bold and underlined values represent the best and second-best performance within each backbone model group, respectively. Cool & Place (COOL), and Pick Two & Place (PICK2), as well as the overall SR. For ScienceWorld, we report SR on both the seen and unseen test splits. Implementation Details. To ensure rigorous and fair comparison, we maintain consistent training protocol across ITP and all baseline methods. We train distinct world models tailored to the two benchmarks. The maximum lookahead horizon is configured based on the typical complexity and average horizon of each environment, with Kmax = 5 for ALFWorld and Kmax = 8 for ScienceWorld. During inference, we employ decoding temperature of 0.7 to balance generation diversity and stability. For hyperparameter settings and training procedures, please refer to Appendix B."
        },
        {
            "title": "4.2 Main Results",
            "content": "How does ITP perform across different benchmarks and backbone models? Table 1 demonstrates that ITP achieves consistent and significant improvements across all tested backbone models on both ALFWorld and ScienceWorld benchmarks. Figure 3: Ablation results of ITPR on ALFWorld and ScienceWorld benchmarks. Specifically, without any additional training, ITPI substantially enhances zero-shot planning performance compared to strong prompting baselines like ReAct and RAP. For instance, on the Qwen2.5-7B backbone, ITPI achieves an overall success rate of 35.71%, nearly doubling the performance of ReAct (17.14%). Furthermore, our trained model ITPR consistently outperforms all training-based baselines, reaching the highest success rates in every backbone group (e.g., 88.57% with Qwen3-8B on ALFWorld and 63.91% with Llama3.1-8B on ScienceWorld). These results underscore that ITP provides robust advantage that scales effectively 6 across different model architectures. Do ITP performance gains stem from lookahead with world models? The effectiveness of our approach is twofold. First, ITPI utilizes the same backbone LLM as the prompting baselines, meaning its performance gains stem solely from conditioning actions on world-model rollouts at inference time. This isolates the intrinsic value of explicit lookahead. Second, ITPR significantly surpasses training-based alternatives such as IWM and WKM. Unlike IWM, which internalizes dynamics implicitly, or WKM, which relies on static task priors, ITPR learns an adaptive lookahead to determine when to invoke the world model. The fact that ITPR achieves the best results (e.g., 63.91% on ScienceWorld test-seen with Llama3.1-8B) suggests that combining policy learning with selective, adaptive lookahead provides complementary benefits that static or implicit methods lack. Moreover, as detailed in our ablation study, the reinforcement optimization of the lookahead horizon is also crucial to this final performance leap. Ablation Study. To identify the contribution of the reinforced training variant ITPR, we ablate ITPR by removing the online reinforcement optimization (w/o RT). In this setting, the model relies solely on initial warm-up training without the adaptive refinement of when to invoke the world model. We use Qwen-3-8B as the backbone for both the agent policy and the world model, evaluating performance across ALFWorld and ScienceWorld benchmarks. As shown in Figure 3, removing RT consistently and significantly degrades performance: The success rate on ALFWorld drops from 88.57% to 71.42%, while the performance on ScienceWorld decreases from 59.70% to 46.00%. These results confirm that the online reinforcement stage in ITPR is critical contributor rather than an incidental training detail. The substantial performance gap suggests that while supervised learning provides baseline capability, the reinforced optimization is essential for the agent to master the strategic timing of imagination, allowing it to efficiently leverage the world model only when necessary to solve complex tasks."
        },
        {
            "title": "4.3 Benefits of Adaptive Lookahead",
            "content": "Superior performance-efficiency trade-off over Fixed Lookahead. We first compare ITP against fixed-k lookahead, where the agent always imagines constant horizon at every step. We meaFigure 4: Comparison between our adaptive lookahead mechanism and baselines with fixed lookahead steps, with success rate (left) and computational cost (right). sure compute by the total tokens consumed over (cid:0)T (t)(πθ) + (t)(Mϕ)(cid:1), and an episode, = (cid:80) report Normalized Budget (NB) defined by: NB(k) = (k) (0) (Kmax) (0) . This calculation of the computational budget transforms the average episode tokens (k) to the [0, 1] range. In subsequent experiments, we set Kmax=5 on the ALFWorld and Kmax=8 on the ScienceWorld benchmark, respectively (see Appendix B). As shown in Figure 4 (left), fixed horizons are brittle: SR peaks at moderate and then drops. In contrast, ITPs adaptive lookahead preserves significantly higher success rate, removing the need to tune global horizon and avoiding the diminishing-returns regime of large k. As shown in Figure 4 (right), the fixed-k lookahead strategy remains cost-inefficient, where its normalized budget grows steeply as increases. In comparison, our adaptive lookahead attains significantly lower budget, eliminating complex tuning and avoiding the high cost of large lookahead steps. Higher success rates with reduced computational cost over Random Lookahead. While Figure 4 compares ITP to fixed-horizon lookahead, it may also vary the horizon over time. To disentangle the benefit of adaptive horizon selection from merely using non-constant horizon, we additionally compare against random lookahead, which samples Kt independently at each step. We evaluate the SR vs. NB trade-off on ALFWorld using Qwen3-8B for both the policy and the world model. We run 140 tasks grouped into 14 folds and report fold-averaged SR and NB, where each point in Figure 5 corresponds to one fold. We observe that our adaptive lookahead consistently dominates the random strategy, achieving higher SR under lower and more stable budgets. This confirms that the im7 (a) Our ITPI vs. ReAct + Random Lookahead (a) ITPI (Ours) (b) ITPR (Ours) Figure 6: Impact of different world-model backbones. We report success rates of (a) ITPI and (b) ITPR across six different tasks on ALFWorld. instructions and partial observations to executable actions. significant line of work formulates agent learning as trajectoryor step-level optimization. For example, ETO (Song et al., 2024) frames learning as exploration-based trajectory optimization, while IPR (Xiong et al., 2024) and E2CL (Wang et al., 2024b) refine agent behavior via iterative revision and correction signals. More recent posttraining methods further improve robustness and generalization through reflective updates, such as Agent-R (Yuan et al., 2025), STeCa (Wang et al., 2025b), and AgentRefine (Fu et al., 2025). However, these methods primarily focus on learning from historical traces, often leaving the agent in state of shallow grounding where it lacks the proactive foresight to anticipate future environmental shifts before execution. World Models for Planning. World models provide mental sandbox for agents, enabling model-based decision-making by predicting environment dynamics (LeCun, 2022; Xiang et al., 2024; Wang et al., 2025d). Recent research has increasingly positioned LLMs as either implicit or explicit world models for task planning. For instance, RAP (Hao et al., 2023) treats LLM reasoning as planning process over an implicit state space, while other works explicitly construct world models to simulate future states and verify plan feasibility (Guan et al., 2023). To improve simulation fidelity, methods like WKM (Qiao et al., 2024) and IWM (Zhang et al., 2025) incorporate world knowledge or historical experience to guide rollouts, while D2PO (Wang et al., 2025d) and internalizing strategies (Chen et al., 2025) use preference signals or reinforcement learning to align planning behavior with environmental feedback. (b) Our ITPR vs. SFT + Random Lookahead Figure 5: Comparison between our adaptive lookahead mechanism (both ITPI and ITPR) and baselines with random lookahead strategy (ReAct and SFT). provements stem from state-conditioned allocation of lookahead, rather than stochasticity or horizon variability per step. 4."
        },
        {
            "title": "Impact of World Models",
            "content": "In this section, we study how the choice of worldmodel backbones affects ultimate performance. We fix the agent policy to Qwen3-8B, and instantiate the world model with Qwen3-8B, Llama3.1-8B, and the large-scale DeepSeek-V3.2 (DeepSeek-AI et al., 2025). Here, DeepSeek-V3.2 is not trained with world-modeling objective. We evaluate ITPI and ITPR on the ALFWorld benchmark. Figure 6 shows that world-model backbone choice matters most in the training-free setting. With ITPI, using DeepSeek-V3.2 (without worldmodel training) yields markedly weaker performance, whereas Qwen and Llama world models maintain high success rates across task families. However, after training with ITPR, DeepSeek-V3.2 becomes competitive, reaching success rates comparable to or outperforming other world models."
        },
        {
            "title": "5 Related Work",
            "content": "LLM-based Agents. LLM-based agents typically use language models as policies that map 8 Further specialized architectures, such as WebEvolver (Fang et al., 2025) and DMWM (Wang et al., 2025c), explore world-model-driven agents in complex, high-stakes domains. Unlike existing methods that rely on rigid, fixed-horizon planning, our work formalizes the POIMDP to integrate imagined futures directly into the agents decision state. We propose an adaptive lookahead mechanism that dynamically scales the imagination horizon. This allows the agent to optimize the balance between foresight reliability and computational efficiency."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose Imagine-then-Plan (ITP), an agent learning framework that equips LLMbased agents with adaptive lookahead with world models. By extending the conventional Partially Observable MDP to Partially Observable and Imaginable MDP, ITP enables agents to explicitly reason over both the observable present and imaginable future trajectories, addressing the shallow grounding limitation of reactive decision making. We instantiate ITP into both an inference-time variant and reinforcement-trained variant. Extensive experiments demonstrate that our approach significantly improves task success and robustness across domains, and further analyses validate the critical role of the adaptive control of imagination horizon. We believe that ITP provides principled advancement toward more deliberative utilization of world models for autonomous agent learning."
        },
        {
            "title": "Limitations",
            "content": "While our approach demonstrates superior performance compared to baseline methods, it is important to acknowledge the limitations of our current work as follows: (1) Current evaluation primarily focuses on interactive text-based benchmarks. While these environments provide rigorous test of long-horizon reasoning, they do not fully capture the challenges of multimodal environments, open-world tool-use, or real-world robotic control. The transition from linguistic state descriptions to visual or sensorimotor observations may introduce additional noise that could affect the stability of the adaptive lookahead mechanism. (2) Although our adaptive lookahead mechanism is designed to optimize efficiency by scaling the imagination horizon, the use of world models inherently introduces higher inference-time overhead compared to purely reactive agents. While higher success rates in highstakes tasks often justify this trade-off, further optimization (e.g., via speculative decoding or distilled world models) is needed for real-time applications. We will leave these directions as our future work."
        },
        {
            "title": "Ethics Statement",
            "content": "We strictly follow the protocols governing the academic use of all LLMs. Our study is conducted in simulated, text-based benchmarks and involves no human subjects or personally identifying information. We cite and comply with the licenses of all models, datasets, and software used. We acknowledge that world-model-based lookahead may introduce potential risks if transferred beyond our simulated evaluation settings: prediction errors and hallucinated rollouts could lead to unsafe or unintended actions in open-world tool-use systems or robots, and the generation of imagined trajectories increases inference-time computation and energy cost. Additionally, while AI assistants (e.g., Cursor and ChatGPT) were partially utilized for coding and linguistic refinement, we affirm that all core content and findings in this paper are the original work of the authors."
        },
        {
            "title": "References",
            "content": "Karl Johan Åström. 1965. Optimal control of markov processes with incomplete state information i. Journal of Mathematical Analysis and Applications, 10:174205. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. 2023. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915. Shiqi Chen, Tongyao Zhu, Zian Wang, Jinghan Zhang, Kangrui Wang, Siyang Gao, Teng Xiao, Yee Whye Teh, Junxian He, and Manling Li. 2025. Internalizing world models via self-play finetuning for agentic rl. arXiv preprint arXiv:2510.15047. DeepSeek-AI, Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, and 1 others. 2025. Deepseek-v3.2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, and Dong Yu. 9 2025. WebEvolver: Enhancing web agent selfimprovement with co-evolving world model. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 8970 8986, Suzhou, China. Association for Computational Linguistics. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. 2025. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978. Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma GongQue, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, and Weiran Xu. 2025. Agentrefine: Enhancing agent generalization through refinement tuning. In The Thirteenth International Conference on Learning Representations (ICLR). Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, Hervé Jégou, Alessandro Lazaric, and 1 others. 2025. Embodied ai agents: Modeling the world. arXiv preprint arXiv:2506.22355. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. 2023. Leveraging pretrained large language models to construct and utilize world models for model-based task planning. In Advances in Neural Information Processing Systems, volume 36, pages 7908179094. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. 2023. Mastering diverse doarXiv preprint mains through world models. arXiv:2301.04104. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, Singapore. Association for Computational Linguistics. Yann LeCun. 2022. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. Camel: Communicative agents for\" mind\" exploration of large language model society. In Advances in Neural Information Processing Systems, volume 36, pages 5199152008. Yixia Li, Hongru Wang, Jiahao Qiu, Zhenfei Yin, Dongdong Zhang, Cheng Qian, Zeping Li, Pony Ma, Guanhua Chen, Heng Ji, and 1 others. 2025. From word to world: Can large language models be imarXiv preprint plicit text-based world models? arXiv:2512.18832. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 19281937. PmLR. Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. 2024. Agent planning with world knowledge model. In Advances in Neural Information Processing Systems, volume 37, pages 114843114871. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems, volume 36, pages 86348652. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020. ALFRED: benchmark for interpreting grounded instructions for everyday tasks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 1073710746. IEEE. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. 2024. Trial and error: Exploration-based trajectory optimization of LLM agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 75847600, Bangkok, Thailand. Association for Computational Linguistics. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2024a. Voyager: An open-ended embodied agent with large language models. Transactions on Machine Learning Research. Hanlin Wang, Chak Tou Leong, Jian Wang, and Wenjie Li. 2024b. E2CL: Exploration-based error correction In Findings of the learning for embodied agents. Association for Computational Linguistics: EMNLP 2024, pages 76267639, Miami, Florida, USA. Association for Computational Linguistics. Hanlin Wang, Chak Tou Leong, Jiashuo Wang, Jian Wang, and Wenjie Li. 2025a. Spa-rl: Reinforcing llm agents via stepwise progress attribution. arXiv preprint arXiv:2505.20732. Hanlin Wang, Jian Wang, Chak Tou Leong, and Wenjie Li. 2025b. STeCa: Step-level trajectory calibration for LLM agent learning. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1159711614, Vienna, Austria. Association for Computational Linguistics. Lingyi Wang, Rashed Shelim, Walid Saad, and Naren Ramakrishnan. 2025c. Dmwm: Dual-mind world model with long-term imagination. In The Thirtyninth Annual Conference on Neural Information Processing Systems. 10 Agent learning via early experience. arXiv preprint arXiv:2510.08558. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. 2022. ScienceWorld: Is your agent smarter than 5th grader? In EMNLP. Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai, Jinlan Fu, and Xipeng Qiu. 2025d. World modeling makes better planner: Dual preference optimization for embodied task planning. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2151821537, Vienna, Austria. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. 2024. Language models meet world models: Embodied experiences enhance language models. Advances in neural information processing systems, 36. Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. 2024. Watch every step! LLM agent learning via iterative step-level process refinement. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 15561572, Miami, Florida, USA. Association for Computational Linguistics. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 22 others. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. 2025. Agent-r: Training language model agents to reflect via iterative selftraining. arXiv preprint arXiv:2501.11425. Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, and 1 others. 2025."
        },
        {
            "title": "Dataset Statistics",
            "content": "A.1 Datasets We evaluate our method on two representative agent benchmarks: ALFWorld1 (Shridhar et al., 2020) and ScienceWorld2 (Wang et al., 2022). Table 2 presents task descriptions and data statistics. Household, text-based embodied tasks ALFWorld: ALFWorld is text-based household embodied benchmark where an agent interacts with simulated environment via natural-language observations and admissible text actions. Each episode specifies goal instruction that can be instantiated from compositional templates, and success requires planning over multi-step action sequences under partial observability. PICK: to find an object of the desired type, pick it up, navigate to the correct location/receptacle, and place it there. CLEAN: to find the target object, pick it up, go to sink/basin, wash it by turning on the faucet, then navigate to the target receptacle and place it. HEAT: to find the target object, pick it up, go to microwave, heat it by turning on the microwave, then place it at the specified location. COOL: to find the target object, pick it up, go to fridge, cool it by placing it inside the fridge, then return and place it at the specified location. LOOK: to find the target object, locate light source, turn it on, and examine the object with the light while holding it. PICK2: to find the first target object and place it at the destination, then find second object of the same type, return to the destination, and place it together with the first one. ScienceWorld: ScienceWorld is text-based interactive science environment that evaluates an agents ability to solve procedural and reasoningintensive tasks grounded in everyday scientific phenomena. Compared to household tasks, ScienceWorld typically involves longer horizons and requires the agent to combine information gathering, tool use, and multi-step experimentation. 1https://github.com/alfworld/alfworld 2https://github.com/allenai/ScienceWorld Training: 3,119 Test: 140 Training: 1,483 Test-Seen: 194 Test-Unseen:"
        },
        {
            "title": "ALFWorld",
            "content": "Six compositional task families: PICK, CLEAN, HEAT, COOL, LOOK, PICK2."
        },
        {
            "title": "ScienceWorld",
            "content": "30 subtasks with many variations (entities, initial conditions, distractors, room layouts), partitioned following the benchmark protocol."
        },
        {
            "title": "Elementary\nscience\ncurriculum in\nan interactive\ntext\nenvironment",
            "content": "Table 2: Dataset statistics. We report dataset splits following the standard benchmark protocol."
        },
        {
            "title": "ALFWorld\nScienceWorld",
            "content": "3 8 8 15 Table 3: Average lookahead horizon step (K) and episode length (steps) on different benchmarks. A.2 Data Preprocessing We use the provided expert trajectories as supervision to warm-start the agent policy. For the world model, we repurpose the same expert rollouts into transition-level supervision for text world model by emitting one record per environment step, containing: (i) compact state string (goal + current observation + optional inventory), (ii) the executed expert action, and (iii) the next observation, optionally augmented with scalar signals such as reward and done. All records are stored in JSONL format and serialized as dialogue-style causal-LM input output pairs, following standard SFT practice for text-based world models. To annotate the number of lookahead steps (i.e., K) in ITPR, we precompute imagined rollouts using the trained world model for small discrete set of horizons (k_candidates) at each expert step. Each imagined rollout is summarized into short lookahead text snippet (lookahead_summary). We then compute per-horizon score that balances improved expert-action likelihood against deeper rollout cost, and store the resulting pseudo label (k_label) along with all scores. We conducted descriptive statistics on the average lookahead steps (Avg. K) and average interac12 B.2 Parameter Settings Table 4 summarizes the hyperparameters used for training and inference. Unless otherwise specified, we apply the same configuration across different backbone models. During exploration, the agent samples actions with temperature 0.7. Due to the difference in task trajectory lengths between ALFWorld and ScienceWorld (ScienceWorld has an average of 15 steps, while ALFWorld has an average of 8 steps), during ITPR training, we set Kmax to 5 for ALFWorld and to 8 for ScienceWorld."
        },
        {
            "title": "Name",
            "content": "Warm-Up Training cutoff_len epochs per_device_train_batch_size gradient_accumulation_steps learning_rate warmup_ratio lr_scheduler fp16 / bf16 gradient_checkpointing lora_r lora_alpha lora_dropout merge_lora Online A2C Optimization γ (discount) rl_learning_rate λK (lookahead penalty) λstep (step cost) success_bonus invalid_action_penalty η α β max_grad_norm"
        },
        {
            "title": "Value",
            "content": "2048 3 1 16 2 105 0.03 cosine True / False False 8 16 0.05 True 0.99 5 106 0.2 0.01 0.01 -0.1 0.5 1.0 0.01 1.0 do_sample (exploration) temperature (exploration) top_p (exploration) action_max_new_tokens imagine_action_max_new_tokens wm_max_new_tokens Kmax (ALFWorld) Kmax (ScienceWorld) True 0.7 0.9 16 12 192 5 8 Table 4: Hyperparameter setup. tion steps (Avg. steps) during the adaptive planning process across different benchmark environments. As shown in the table 3, models on ALFWorld tend to adopt shorter lookahead planning with an average = 3, while the average steps required to complete tasks is 8. This reflects relatively compact task chains and decision space that is more easily covered within shorter lookaheads. In contrast, on ScienceWorld, the average increases to 8 and the average steps reach 15. This indicates that this environment typically requires longer-range reasoning and more complex state evolution modeling; the adaptive strategy more frequently selects larger lookahead depths to maintain planning stability and completion rates."
        },
        {
            "title": "B Additional Implementation Details",
            "content": "For fine-tuning, we employed several open-source models, including Qwen3-8B (Yang et al., 2025)3, Qwen2.5-7B (Yang et al., 2024)4, and Llama3.18B (Dubey et al., 2024)5. All experiments were conducted on computational cluster equipped with 2 NVIDIA A100 80GB GPUs. We report the prompting templates used and the parameter settings as follows. B.1 Prompting Templates We provide the prompt templates for the agent policy and for the adaptive K-step lookahead inference procedure of ITPI. Figure 7 shows the prompting template for the ReAct baseline. Figure 8 illustrates the adaptive horizon selector, where the policy model maps the task instruction and the current textual state st to single discrete lookahead depth [0, Kmax]. Figure 9 presents the world-model foresight generator, which conditions only on st and produces concise K-step imagined trajectory enclosed by <Foresight>...</Foresight>. Figure 10 depicts the foresight-conditioned agent prompt, where the policy model consumes the task, the current state st, and the generated foresight to produce ReAct-style response, and outputs an admissible environment action via exact copying. 3https://huggingface.co/Qwen/Qwen3-8B/blob/ main/LICENSE 4https://huggingface.co/Qwen/Qwen2. 5-7B-Instruct/blob/main/LICENSE 5https://huggingface.co/meta-llama/Llama-3. 1-8B-Instruct/blob/main/LICENSE"
        },
        {
            "title": "Prompt Template for Base Agent Policy",
            "content": "Interact with household to solve task. You are an intelligent agent in household environment. Your goal is to perform valid actions to complete the task goal. At each step t, you will be given the task goal, the current state (observation and optional inventory), and the previous step context (the previous action and its resulting environment feedback/observation). You must follow the ReAct paradigm: Reason Thought Action. You need to process the information in specific order: 1. Reason: Briefly interpret the current state and the latest environment feedback. Identify what has been achieved and what remains. 2. Thought: Plan the next few steps to make progress toward the task goal based on the reasoning. 3. Action: Output exactly one next action that is valid in environment. After each turn, the environment will provide immediate feedback (a new observation, and optionally inventory updates). If the environment outputs Nothing happened, the previous action is invalid; revise your plan and try different valid action. Your response must use the following format: Reason: <brief interpretation of state/feedback> Thought: <short plan for next steps> Action: <EXACTLY ONE valid action line> Inputs at step t: Task goal: {task_goal} Current state (observation + optional inventory): {state} Previous action (optional): {prev_action} Latest environment feedback / observation: {feedback} Figure 7: Prompt template used for base agent policy on ALFWorld and ScienceWorld benchmarks. Adaptive Selection of (PolicyModel.decide_k) System prompt: You are planning assistant. Your job is to decide how many steps of look-ahead are needed right now. Given task instruction and the dialogue/action history, output single integer in the range [0, Kmax] Task instruction: task History trajectory (thoughts, actions, observations so far): history Question: Output single integer in [0, Kmax]. Your response should be: Figure 8: Prompt used to adaptively select the lookahead horizon from the task instruction and trajectory history. The output must be single integer within the range [0, Kmax]. K-Step Foresight Generation (WorldModel.imagine) System prompt: You are world model for the ALFWorld environment. Given an action/observation history, imagine the next few steps, describing likely observations and key objects. User prompt: Predict the next step(s). Return concise plan inside <Foresight>...</Foresight> with numbered steps. User prompt: History so far: <history> Your response should be: Foresight state: <Foresight>...</Foresight> Figure 9: Prompt used by the world model to generate K-step foresight trajectory. Foresight-Conditioned Action (PolicyModel.reflect_and_act) You are an agent that first imagines and then acts . At each step, you will be given the task instruction, the current state, and K-step foresight trajectory imagined by world model. Use the foresight to reflect on progress and bottlenecks, then decide the next admissible action. You will be given: 1. Task instruction ({task}) 2. Current state ({state}) 3. K-step foresight trajectory from the world model ({foresight}) 4. Admissible actions (instance-level) ({admissible_actions_joined_by_newlines}) After each turn, the environment will give you immediate feedback based on which you plan your next few steps. If the environment output Nothing happened, that means the previous action is invalid and you should try more options. You must follow this order: 1. Reflection: Briefly assess whether the foresight indicates progress, contradictions, or missing subgoals. 2. Thought: Provide short plan for the next few steps based on the current state and foresight. 3. Action: Output exactly one admissible action by exact copying. Hard constraint on the action: You MUST choose the Action by copying EXACTLY one line from the provided admissible actions list. Do NOT paraphrase. Do NOT add extra tokens. User prompt: Task instruction: {task} Current state: {state} K-step foresight trajectory from the world model: {foresight} Admissible actions (copy exactly one line for Action): {admissible_actions_joined_by_newlines} Your response should use the following format: Reflection: <Reflection> ... </Reflection> Thought: <Thought> ... </Thought> Action: <Action> ... </Action> Figure 10: Prompt used to generate ITP-style response action conditioned on world-model foresight on ALFWorld and ScienceWorld benchmarks."
        }
    ],
    "affiliations": [
        "Central South University",
        "The Hong Kong Polytechnic University"
    ]
}