{
    "paper_title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models",
    "authors": [
        "Siqiao Huang",
        "Jialong Wu",
        "Qixing Zhou",
        "Shangchen Miao",
        "Mingsheng Long"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse predictions, limiting their applicability in complex environments. In contrast, video diffusion models trained on large, internet-scale datasets have demonstrated impressive capabilities in generating high-quality videos that capture diverse real-world dynamics. In this work, we present Vid2World, a general approach for leveraging and transferring pre-trained video diffusion models into interactive world models. To bridge the gap, Vid2World performs casualization of a pre-trained video diffusion model by crafting its architecture and training objective to enable autoregressive generation. Furthermore, it introduces a causal action guidance mechanism to enhance action controllability in the resulting interactive world model. Extensive experiments in robot manipulation and game simulation domains show that our method offers a scalable and effective approach for repurposing highly capable video diffusion models to interactive world models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 7 5 3 4 1 . 5 0 5 2 : r Vid2World: Crafting Video Diffusion Models to Interactive World Models Siqiao Huang1 , Jialong Wu1 , Qixing Zhou2, Shangchen Miao1, Mingsheng Long1(cid:0) 1Tsinghua University 2Chongqing University huang-sq23@mails.tsinghua.edu.cn, wujialong0229@gmail.com mingsheng@tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domainspecific training and still produce low-fidelity, coarse predictions, limiting their applicability in complex environments. In contrast, video diffusion models trained on large, internet-scale datasets have demonstrated impressive capabilities in generating high-quality videos that capture diverse real-world dynamics. In this work, we present Vid2World, general approach for leveraging and transferring pre-trained video diffusion models into interactive world models. To bridge the gap, Vid2World performs casualization of pre-trained video diffusion model by crafting its architecture and training objective to enable autoregressive generation. Furthermore, it introduces causal action guidance mechanism to enhance action controllability in the resulting interactive world model. Extensive experiments in robot manipulation and game simulation domains show that our method offers scalable and effective approach for repurposing highly capable video diffusion models to interactive world models. Project page: http://knightnemo.github.io/vid2world/."
        },
        {
            "title": "Introduction",
            "content": "World models [20, 11] have emerged as pivotal components for sequential decision making, enabling agents to predict future states and plan actions by simulating environment dynamics. Despite their success in numerous domains, including game simulation [1, 22], autonomous driving [49, 35], and robotics [61, 54, 60], these world models often rely on extensive domain-specific training, necessitating large amounts of carefully curated and task-specific interaction data. Moreover, despite these substantial data requirements, current world models [51, 1] still generate relatively coarse predictions with limited fidelity and physical realism, thereby constraining their utility in complex decision-making scenarios. Concurrently, video diffusion models [28] have achieved remarkable success in generating highfidelity video sequences by leveraging large, internet-scale datasets. Recent video diffusion models, such as Sora [38], Veo-2 [13], Stable Video Diffusion [4], inherently capture rich world knowledge and physical dynamics priors from diverse visual experiences, allowing them to produce content that adheresat least partiallyto physical laws and real-world constraints. Hence, leveraging these pre-trained video diffusion models for world modeling holds great potential to overcome longstanding challenges of coarse generation quality and excessive data requirements in existing world models. Despite their potential, significant challenges arise in bridging the gap between video diffusion models and interactive world models. One key challenge lies in enabling causal generation. Standard video diffusion models are designed to denoise entire sequences simultaneously, leveraging bidirectional Equal Contribution Preprint. Under review. Figure 1: Transforming video diffusion models into interactive world models involves two key challenges: (1) Causal generation: converting full-sequence diffusion models into causal diffusion models; (2) Action conditioning: adapting causal diffusion models into an interactive world model. context across time. This full-sequence formulation inherently introduces temporal dependencies that span both past and future frames, making it unsuitable for causal rollouts where predictions only depend on past information. Even in cases predicting frame chunks simultaneously, bidirectional dependencies can allow noise or errors from future reconstructions to propagate backward, corrupting the generation of the current frame and leading to temporal inconsistencies or degraded performance. Another key challenge is enforcing action conditioning. While transformation into causal architecture is essential to enable autoregressive rollout, such causal video diffusion models do not satisfy the requirements of interactive world models. Specifically, world models must be capable of counterfactual reasoning, namely predicting how different actions influence future states, which necessitates injecting frame-level action signals into the generation process. Especially in diffusion models, where classifier-free guidance (CFG) [27] offers the freedom of balancing mode coverage and sample fidelity, effective action guidance mechanisms for video diffusion models under interactive settings require careful algorithmic designs as well as architectural choices. An illustration of this two-step transformation from video diffusion models to interactive world models is shown in Figure 1. In this paper, we propose Vid2World, general approach to effectively transform internet-level pre-trained video diffusion models into interactive world models capable of autoregressive, actionconditioned generation. For causalization of video diffusion models, we discover better weight transfer schemes that adapt temporal attention and convolution layers to their causal counterparts, enabling fine-tuning with training objective [8] for causal generation. For action conditioning of causal video diffusion, we inject action signals into model inputs at corresponding frames, and principledly design an extended training objective by independently dropping actions in the sequence. This design enables classifier-free action guidance during causal generation. We evaluate Vid2World by transferring an extensively pre-trained model [52] with 1.4B parameters to both robot manipulation [6, 10] and game simulation [40, 1] domains. Experimental results demonstrate significant improvements over existing transfer methods as well as state-of-the-art world models. To summarize, our contributions are: (1) To the best of our knowledge, we are the first to systematically explore the problem of transferring full-sequence, non-causal, passive video diffusion models into autoregressive, interactive, action-conditioned world models. (2) We propose Vid2World, general and effective approach for this problem, featuring novel techniques for the causalization and action conditioning of video diffusion models. (3) State-of-the-art performance of Vid2World across domains establishes new benchmarks for this critical problem and facilitating future research."
        },
        {
            "title": "2 Related Works",
            "content": "Diffusion for World Modeling. Due to the high fidelity offered by diffusion models in image and video generation, utilizing diffusion for world modeling has garnered growing interest. Prior works fall primarily into two categories. The first treats world modeling as conditional image generation problem, where history observation and action sequences serve entirely as conditions. While these approaches follow an autoregressive framework and have shown promise in domains such as game simulation [1, 12] and navigation [3], they typically rely on fixed-length context window, limiting their applicability in environments that demand long-term temporal reasoning. The second category formulates the problem as full-sequence video generation task [54, 56, 61], often achieving better temporal coherence between frames. Yet, these models operate on full video segments, precluding autoregressive rollout, and thus hindering their use in interactive environments. 2 Leveraging Foundation Models to World Models. Foundation models [5], trained on large-scale and diverse data, have shown revolutionary potential across modalities such as text [15, 39, 14], image [45, 37], and video [38, 13]. In the text domain, large language models are prompted to act as world models for spatiotemporal reasoning in agentic tasks [23, 17, 30]. In the video domain, adapting pre-trained generative models into world models typically involves architectural modifications. For instance, He et al. [24] integrate an action-conditioning module into the generative backbone, while Rigter et al. [43] introduce an action-aware adapter to modulate the output of frozen video model. However, these approaches often overlook the critical need for interactivity and temporal causality, limiting their applicability in sequential decision-making and interactive environments."
        },
        {
            "title": "3 Preliminaries",
            "content": "World Models. world model is an internal model learned by an agent to model the dynamics of its environment. This environment is typically formalized as (discrete-time) Partially Observable Markov Decision Process (POMDP) [32], defined over tuple (S, O, ϕ, A, p, r, γ). At each time step t, the agent receives an observation ot = ϕ(st), where st is the underlying state that satisfies the Markov property. Upon taking an action at A, the next state is sampled from the transition distribution : (S), i.e. st+1 p( st, at). In the context of world models, the agent learns to estimate this transition function through history observation and action sequence: pθ(ot+1o t). While world models can be applied to wide range of observation modalities, including proprioceptive signals [55], text [48], 3D meshes [57] and pixel-based inputs [51, 62], here we focus on learning world models in the pixel space, where observations are defined over = RH t, 3. Diffusion Models. Diffusion models [26, 46] are highly expressive generative models that learn to approximate target data distribution q(x), where Rd, by progressively denoising Gaussian noise. The process begins from noise sample xK (0, I) , and iteratively transforms it into clean data sample x0 q(x) through learned reverse process over timesteps = K, ..., 1. The forward (noising) process defines fixed Markov chain that gradually adds Gaussian noise to the data: 1) = (xk; (cid:112)1 βk xk q(xkxk 1, βkI), where {βk}K as Markov chain: k=0 is pre-defined variance schedule. The reverse (denoising) process is also modeled pθ(xk 1xk) = (xk 1; µθ(xk, k), γkI), where µθ is learned through training. In practice, it is common to reparameterize the objective in terms of noise prediction, i.e., learning to predict ϵk = ( αkµ. This simplifies to minimizing the mean square loss: 1 αk) 1xk L(θ) = Ek,ϵ,x0[ϵk ϵθ(xk, k)2], where xk = iterative denoising through Langevin dynamics: xk αkx0 + 1 αkϵk and ϵk (0, I). At test time, sampling is performed via 1 1 αk (xk 1 1 αk αk ϵθ(xk, k) + σkw). }T Video Diffusion Models. In video diffusion models [28], the sample is typically represented as sequence of frames {xkt t=0, where denotes the frame index and kt indicates the noise level at that frame. Conventional approaches [28, 4] apply uniform noise level across all frames during both training and inference, treating each frame identically within the denoising process. To relax this constraint, Chen et al. [8] propose sampling noise levels independently for each frame, i.e., kt U[0, K] during training. This formulation intuitively captures all noise level combinations of history frames during training, opening up the potential of auto-regressive generation at inference time. At inference time, we follow denoising schedule RM , where is the number of denoising steps and each row Km = {km0, ..., kmT } specifies the per-frame noise levels at step . This design supports autoregressive rollout by setting kt = 0 for clean history frames (t < τ ), and kt = for masked future frames (t > τ ), and progressively denoising the current frame xkτ τ from kτ = to 0. When combined with causal architectures, this schedule can be reduced to including only frames up to the current timestep τ , opening up potentials of infinite-horizon rollouts and significantly reducing computational overhead induced in denoising future frames."
        },
        {
            "title": "4 Methods",
            "content": "While video diffusion models excel at generating high-fidelity, physically plausible sequences, their default formulation is fundamentally incompatible with interactive world modeling. Concretely, two key transformation barriers stand out: 1. Inability of causal generation: Typical video diffusion models generate frames using bidirectional temporal context, allowing future frames to influence the past; 2. Lack of action conditioning: These models are typically conditioned on coarse, video-level inputs (e.g., text prompts) and lack mechanisms for fine-grained, frame-level action conditioning. While effective for open-ended video synthesis, these design choices are misaligned with the needs of interactive world modeling, which requires that predictions must depend solely on past observations and actions, and the model must respond sensitively to frame-level actions, accurately capturing their influence on future predictions. To overcome these barriers, we propose Vid2World, general approach for leveraging and transferring pre-trained video diffusion models into interactive world models. Vid2World introduces two key modifications to enable autoregressive and action-conditioned generation, respectively. In Section 4.1, we present the strategy of video diffusion causalization, which converts non-causal architectures into temporally causal variants compatible with the post-training objective [9], while maximally preserving pre-trained weights. In Section 4.2, we introduce causal action guidance, to principledly enable classifier-free action guidance during inference for step-wise, interactive rollouts. This is achieved by injecting action signals through lightweight embedding layers and extending the posttraining objective with independent action dropout. An overview of our training and inference method can be viewed in Figure 3. 4.1 Video Diffusion Causalization In order to enable causal generation with video diffusion models, both architectural and training objectives modifications are necessary. From an architectural standpoint, standard video diffusion models include bidirectional temporal modules, such as temporal attention [21] or noncausal convolutions [4, 19] that enable information flow across all timesteps within denoising step. While effective for full-sequence generation, such modules are fundamentally incompatible with autoregressive world modeling, where current observation ot should not depend on future observations or actions. This necessitates transformations to enforce temporal causality. Figure 2: Weight transfer mechanisms in temporal convolution layers. Shift weight transfer introduces temporal misalignment, whereas Mixed weight transfer perserves temporally aligned pretrained weight to the maximum extent. Temporal Attention Layers. Converting non-causal temporal attention layers into their causal counterparts can be achieved by applying causal masks. Since attention operates through dot products between queries and keys, inherently adaptive to variable sequence lengths, therefore restricting the receptive field to exclude future frames does not alter the underlying computation of inter-token relationships. As result, this does not mandate parametric modifications. Temporal Convolution Layers. Temporal convolution layers use symmetric kernels centered around the current timestep, allowing feature flow from both past and future. Unlike temporal attention layers, simple adaptation of convolution layers to temporally-causal convolution layers fails to utilize the convolution kernels weights fully. The naive approach, which we refer to as shift weight transfer, directly reuses the full pre-trained kernel (denote the kernel size on temporal dimension as m) by shifting it m/2 steps into the past, so that the kernels receptive field lies before the current timestep. While this preserves all kernel weights, it causes temporal misalignment: the kernels i-th position, now aggregates information at timestep m/2, giving no guarantees for producing similar representations. 4 Figure 3: Training and Sampling of Vid2World. (a.) During training, we add independently sampled noise levels to each frame, as well as randomly droping out each action with fixed probability. (b.) For auto-regressive rollout, we denoise the latest frame while setting history clean. Action guidance is added for the current action. See Appendix for details. In contrast, we introduce mixed weight transfer, where instead of shifting the kernel weights by 2 , we preserve the corresponding positions weights and initialize the unseen positions weights to be the average weight of kernel weights along temporal dimension, mixing temporal information. Specifically, the transferred weight w, at position in the temporal dimension satisfies: (cid:40) wi = wi 2 µ = 1 , (cid:80)m 1 i=0 wi, o/w. if 2 , . An illustration of this convolutional architectural adaptation mechanism is provided in Figure 2. While architectural adaptations are necessary for causal generation in world modeling, they are not sufficient on their own. To support causal generation in interactive settings, where future frames are predicted step by step conditioned on previously fully denoised frames and actions, i.e. setting noise [T ] = [0, 0, ..., 0, k], = 0, ..., K, the training procedure must be levels of denoising to be {kt}t capable of capturing such inference-time noise level distributions. In standard video diffusion models, the training procedure follows an homogeneous noise schedule, where all frames in video share the same noise level. This limited subset of noise level combinations makes them naturally incompetent of capturing such noise levels of at inference time. Therefore, to enable the fore-mentioned auto-regressive generation capabilities, it becomes vital to train the model with different noise levels across frames. Here, we adopt Diffusion Forcing [8], where we sample noise levels to be independent and uniform in different frames, i.e. kt U(0, K), t. Through such training procedure transformation, the model is trained with all possible combination of noise levels in history frames, therefore enabling the flexibility of causal rollout at inference time. 4.2 Causal Action Guidance While enabling causal generation is vital to the transformation to interactive world models, these transformed causal diffusion models still fall short of providing action-conditioned generation results. Extensive works [1, 3, 63] have been done by integrating action condition through video-level condition, where the entire action sequence is encoded to single embedding, similar to text embeddings in text-to-video generative models. Aside from lacking the ability to perform frame-level fine-grained action-conditioned predictions, such global conditioning is also fundamentally incompatible with autoregressive generation, where actions arrive incrementally and must be processed in an online fashion during inference. Here, we opt to enforce frame-level condition through architectural modifications, where each action is encoded independently before injected into the corresponding frame. Specifically, when predicting the t-th frame ot, the embedding of at 1 is added to the models representation at temporal position t, allowing each frame to be conditioned directly on its preceding action. In practice, we inject action inputs into the denoising network using lightweight multi-layer perceptron. Through such frame-wise conditioning scheme, the action inputs are temporally aligned with the generative target at frame level, opening up potential for precise fine-grained control in interactive settings. To add flexibility to the level of control in the generated frames, we extend classifier-free guidance [27] to action-conditioned generation in auto-regressive settings, introducing Causal Action Guidance. 5 Algorithm 1 Vid2World Training 1: Input: Model θ, Trajectory dataset D. 2: loop 3: 4: 5: Sample trajectory [xgt for = 0, ..., do q( xgt xkt , kt), kt [0, K] ]0:T from , agt (cid:40), w.p. p, agt , o/w. kt xt αkt 1 αkt τ ]τ t, [aτ ]τ <t, [kτ ]τ t) ˆϵt = ϵθ([xkτ end for =MSELoss([ˆϵ1, ..., ˆϵn] , [ϵ1, ..., ϵn]) Backprop with and update θ 6: 7: at = ϵt = 8: 9: 10: 11: 12: end loop Algorithm 2 Auto-Regressive Sampling 1: Input: Model θ, Initial observation x0, Action sequence [at]0:T 1, Action guidance scale λ. I), 1, ..., . for = K, ..., 0 do 2: Initialize xt (0, σ2 3: for = 1, . . . , do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end for 14: Return x0:T . end if (0, I) xt 1 αk end for ˆϵ = ϵθ([xτ ]τ t, [aτ ]τ <t, [0, ..., 0, k]) if λ = 1 then ϵuc = ϵθ([xτ ]τ t, [aτ <t1, ], [0, ..., 0, k]) ˆϵ (1 + λ)ˆϵ λϵuc (xt 1αk 1 αk ˆϵ) + σkw τ ], [aτ ], [kτ ]) takes in tuple of noised observations [xk In classifier-free guidance, the model learns both the score function for conditional distribution as well as its unconditional counterpart, hence at sampling time one can guide the generation to be more controlled through amplifying the difference between two score functions. In this setup, the score function ϵθ([xkτ τ ], actions [aτ ], noise levels [kτ ] as input, and the conditioned variable is the most recent action. Therefore, the model should be capable of capturing the score function of the conditional distribution: ϵcond = ϵθ([xkτ t), as well as its unconditional counterpart with the most recent action t, [aτ ]τ <t, [kτ ]τ obscured: ϵucond = ϵθ([xkτ As result, the model should be capable of denoising the current frame with the latest action dropped out. Therefore, to open up the possibility of adding action guidance in auto-regressive settings, we propose lightweight yet principled approach. During training, for each timestep t, the action input at is independently dropped with fixed probability p: 1, ], [kτ ]τ t, [aτ <t τ ]τ τ ]τ t). at = (cid:26), with probability p, otherwise. at, This dropout mechanism intuitively encourages the model to learn the score functions conditioned on all possible subsets of the action sequences. As result, the model is forced to learn the effect of the immediate action on the predicted transition, enabling classifier-free guidance at test time. At inference time, we can guide the models generation results by ϵguided θ = (1 + λ) ϵcond λ ϵucond, where λ R+ is the guidance scale. This formulation offers test-time flexibility on the responsiveness to fine-grained action variations. Through such transformation, the model is better aligned with the core objective of world modelingnot merely to capture average behavioral trends, but to support counterfactual reasoning conditioned by the most recent action the agent takes. In summary, we propose Vid2World, general approach for transforming full-sequence, non-causal, passive video diffusion models into autoregressive, interactive, action-conditioned world models. Through video diffusion causalization, we open up the models capability to perform causal generation, and through causal action guidance, we incorporate action guidance for interactive settings. We provide the pseudocode for our approach in Algorithm 1 and Algorithm 2."
        },
        {
            "title": "5 Experiments",
            "content": "As proof of concept for Vid2World, we leverage DynamiCrafter [53], state-of-the-art U-Net based latent video diffusion model pre-trained on extensive internet-level video datasets with 1.1B trainable parameters, as our base model. We demonstrate our approachs effectiveness in multiple domains, spanning real-world robot manipulation [6, 33] and high-dynamic 3D scenes in game simulation [1, 40]. Through video prediction results, as well as downstream tasks of offline policy evaluation, we showcase that through Vid2World, we not only acquire models with high fidelity and similarity 6 Table 1: Quantitative results for RT1 dataset. For baseline methods, the models are trained for 7 days on 4 A100 GPUs, for our models, we train for 100k gradient steps on 4 A100 GPUs, which takes less than 7 days. Baseline results are taken from AVID [43]. Model FVD FID2 SSIM LPIPS PSNR Pre-trained Base Model Classifier Guidance ControlNet Action-Conditioned Fine-tuning Language-Conditioned Fine-tuning Vid2World-NAR Vid2World 237.6 213.1 27.1 24.2 33.7 18.5 21.0 5.432 6.005 3.248 2.965 3.511 5.921 5.985 0.712 0.683 0.836 0.852 0.812 0.857 0.847 0.228 0.250 0.148 0.134 0.177 0.132 0.148 20.6 19.8 24.5 25.6 22.1 25.7 24. to ground truth, but also models capable of aiding downstream tasks in sequential decision-making. Further details regarding the model implementation can be viewed in Appendix A."
        },
        {
            "title": "5.1 Vid2World for Robot Manipulation",
            "content": "Robot manipulation is an ideal testbed for world models, demanding temporally coherent, actionconditioned predictions that are both visually realistic and causally faithful under real-world physical constraints. These stringent requirements make it rigorous and practically relevant benchmark for evaluating models controllability and fidelity. Setup. We utilize the RT-1 dataset [6], collection of real-world robotic experiences spanning multiple manipulation tasks, including picking, placing, operating drawers, etc. For our proposed method, we consider two inference setups, (1) Vid2World-NAR: where similar to conventional video diffusion models and baseline methods, we denoise using homogenous noise levels across frames, generating the whole sequence simultaneously in non-auto-regressive manner; (2) Vid2World: where we denoise each frame in an auto-regressive manner, incorporated with action-guidance. Following Diffusion Forcing [8], during auto-regressive rollout, we add uniform small noise to the history frames during generation. Further implementation details can be found in Appendix B.3.1. Baselines. To validate the effectiveness of our approach as transfer method, we adopt variety of baselines that build upon the same base model but utilize different transformation approaches, including Action-Conditioned Fine-tuning, Language-Conditioned Fine-tuning, ControlNet [58], and Classifier Guidance. We train our model following the train and validation set split of baseline implementations. Details are shown in Appendix B.3.2. Metrics. For evaluation metrics, we adopt commonly used video generation metrics for measuring the pixel-level or semantic similarity between the models generation results and the ground truth frame sequence. These include Fréchet Video Distance (FVD) [47], Fréchet Image Distance (FID) [25], Structural Similarity Index Measure (SSIM) [50], Learned Perceptual Image Patch Similarity (LPIPS) [59], and Peak Signal-to-Noise Ratio (PSNR) [29]. See Appendix B.2 for details. Results. As shown in Table 1, Vid2World demonstrates strong quantitative performance across both non-autoregressive and autoregressive settings, showing stronger or comparable performance against other transfer methods. Under the non-autoregressive sampling setting, Vid2World outperforms all prior methods by significant margin. Even under the autoregressive setup, where other baselines are not capable of doing, Vid2World holds comparable or superior performance to these methods, showcasing its strong capabilities in video prediction. 2In the publicly released code of AVID [43] (https://github.com/microsoft/causica/blob/main/ research_experiments/avid/libs/avid_utils/avid_utils/metrics.py), the FID scores are computed without setting the model to evaluation mode, causing incorrect results. These results are shown in gray accordingly. 7 Application: Real2Sim Policy Evaluation. To demonstrate our methods capabilities to aid downstream tasks in interactive environments, we conduct Real2Sim Policy Evaluation experiments, following SIMPLER [33]. In this setup, we want to acquire the performance given policy by interacting with the world model instead of the real world. This setup necessitates the world models ability to perform auto-regressive rollout, and well-performing world model should be able to discriminate failure cases from success cases by auto-regressively rolling out the actions given by different policies. The procedure is summarized in Algorithm 3. Here we adopt three policy models: RT-1 (Begin), RT-1 (15%) and RT-1 (Converged), which are checkpoints taken from RT-1[10, 6] at different stages of training. We evaluate on the task of closing drawers, where we roll out for horizon of = 40 steps, with sliding window length of = 10. For simplicity, we use human evaluation as the verifier ψ to annotate whether trajectory is successful. As shown in Figure 4, Vid2World reliably reflects the performance gap among different policies, closely tracking their real-world success trends. Further details can be found in Appendix B.4. Figure 4: Vid2World for Real2Sim Policy Evaluation. We report the success rate of Vid2World in the task of closing drawers, compared with realworld evaluation results, denoted as GT. 5.2 Vid2World for Game Simulation Game simulation constitutes key application domain for world modeling and has attracted growing intellectual attention in recent literature [7, 12, 18, 1]. In this setting, the interactiveness of world models is especially critical, as it opens the door to building neural game engines [2, 1]models capable of supporting real-time user interaction through learned dynamics. This is particularly challenging domain due to its inherently complex temporal dynamics and the strong influence of actions on visual transitions, including rapid and discontinuous viewpoint changes, contact-rich object interactions, and fine-grained motion patterns, requiring the model to reason over sophisticated, causally entangled visual-temporal cues. Setup. To explore the capabilities of our approach in highly dynamic and visually complex 3D environments, we apply Vid2World to the celebrated video game Counter-Strike: Global Offensive (CS:GO). We utilize the online dataset proposed by Pearce et al. [40], which contains 5.5M frames (95 hours) of online human gameplay from the map Dust II. To make our method comparable to the baselines, we follow DIAMOND [1], using exactly the same holdout set of 0.5M frames (corresponding to 500 episodes, or 8 hours) for testing. DIAMOND [1] is state-of-the-art autoregressive world model, which generates the next frame conditioned on fixed number of previous observations and actions. We adopt the evaluation metrics in Section 5.1, measuring the visual and semantic similarity between the generated videos and the ground truth videos. As DIAMOND [1] requires 4 frames as condition, we initialize with four history frames, and auto-regressively generate frames until sequence length of 16 is reached. The metrics are calculated only on the predicted frames, excluding frames used for conditioning. More details are listed in Appendix B.5. Results. As shown in Table 2, Vid2World outperforms both configurations of DIAMOND across all evaluation metrics with significant margin, including 81.8% relative performance improvement Table 2: Quantitative results for CS:GO dataset. For DIAMOND [1], we test its performance under two configurations, fast and high-quality (HQ). Here we use validation set of 500 video segments, each consisting of 16 frames. Model FVD FID SSIM LPIPS PSNR DIAMOND-Fast DIAMOND-HQ Vid2World 903.9 562.2 102.6 115.6 87.2 19.8 0.449 0.447 0.488 0.547 0.510 0.407 18.2 18.3 18.6 Table 3: Ablation Study. Here we ablate two components of our proposed method: Mixed Weight Transfer (MWT) and Action Guidance. Model FVD FID SSIM LPIPS PSNR Vid2World w/o Action Guidance Vid2World w/o MWT Vid2World 25.4 27.9 22. 6.88 6.99 6.76 0.837 0.806 0.833 0.158 0.185 0.151 23.8 21.9 23.9 Figure 5: Video Prediction Results of Vid2World on RT-1 and CS:GO. Zoom in for details. Extended examples can be found in Appendix C. in FVD and 77.2% performance gain in FID compared to the best baseline configuration. These results demonstrate the superior visual fidelity and semantic consistency of our method, showcasing potential for leveraging video diffusion models to interactive neural game engines. 5.3 Ablation Study To verify the effectiveness of our proposed methods, we ablate our methods in auto-regressive settings. Here we are interested in two questions: (1) How important is the models ability to perform action guidance in interactive video prediction settings? (2) Does the proposed mixed weight transfer induce better transfer in video prediction tasks? Here we ablate using the RT-1 [6] dataset, where all models are trained for 30k gradient steps, due to limited computational budgets. We consider two variants of our model: (a.) Vid2World w/o Action Guidance: this variant is trained without action dropout, resulting in model only capable of generation with the full action sequence provided as deterministic condition; (b.) Vid2World w/o Mixed weight transfer (MWT): this variant is trained with temporal convolution weights initialized using shift weight transfer. As shown in Table 3, both techniques play dominant role in the superior performance of Vid2World."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we transform passive video diffusion models into interactive world models. We propose Vid2World, introducing two key mechanismsvideo diffusion causalization and causal action guidanceto support autoregressive, action-conditioned generation. Extensive experiments demonstrate that Vid2World achieves state-of-the-art performance in video prediction tasks as well as effectively supporting downstream decision-making. While this work marks successful first attempt, it leaves plentiful space for further exploration. First, due to computational resource constraints, we are limited to employing relatively lightweight video diffusion model as the base model, and we envision that exploring larger-scale models [36, 41] may lead to even better performance. Second, the training process remains relatively time-consuming. We look forward to future methods that can achieve comparable or even superior performance with fewer training steps."
        },
        {
            "title": "References",
            "content": "[1] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and François Fleuret. Diffusion for world modeling: Visual details matter in atari, 2024. [2] Chris Bamford and S. Lucas. Neural game engine: Accurate learning of generalizable forward models from pixels. IEEE Conference on Games (CoG), 2020. [3] Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. arXiv preprint arXiv:2412.03572, 2024. [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. None, 2023. [5] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. arXiv preprint arXiv: 2108.07258, 2021. [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv: 2212.06817, 2022. [7] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. [8] Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. arXiv preprint arXiv: 2407.01392, 2024. [9] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 10 [10] Open X-Embodiment Collaboration, Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben BurgessLimerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi \"Jim\" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag Sanketi, Patrick \"Tree\" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martin-Martin, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv: 2310.08864, 2023. [11] Anna Dawid and Yann LeCun. Introduction to latent variable energy-based models: path towards autonomous machine intelligence. Journal of Statistical Mechanics: Theory and Experiment, 2023. [12] Decart, Julian Quevedo, Quinn McIntyre, Spruce Campbell, Xinlei Chen, and Robert Wachen. Oasis: universe in transformer, 2024. [13] Google Deepmind. Veo 2: Our state-of-the-art video generation model. https://deepmind. google/technologies/veo/veo-2/, 2024. 11 [14] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv: 2501.12948, 2025. [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. North American Chapter of the Association for Computational Linguistics, 2019. [16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [17] John Gkountouras, Matthias Lindemann, Phillip Lippe, Efstratios Gavves, and Ivan Titov. Language agents meet causality - bridging llms and causal world models. arXiv preprint arXiv: 2410.19923, 2024. [18] Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, and Jiang Bian. Mineworld: real-time and open-source interactive world model on minecraft. arXiv preprint arXiv: 2504.08388, 2025. [19] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Y. Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. International Conference on Learning Representations, 2023. [20] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. arXiv preprint arXiv: 1809.01999, 2018. [21] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv: 2501.00103, 2024. [22] Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. 12 [23] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv: 2305.14992, 2023. [24] Haoran He, Yang Zhang, Liang Lin, Zhongwen Xu, and Ling Pan. Pre-trained video generative models as world simulators. arXiv preprint arXiv: 2502.07825, 2025. [25] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [26] Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion probabilistic models. Neural Information Processing Systems, 2020. [27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv: 2207.12598, 2022. [28] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. Neural Information Processing Systems, 2022. [29] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 23662369. IEEE, 2010. [30] Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Yao Mu, Hongyuan Zhang, Wenqi Shao, and Ping Luo. Text2world: Benchmarking large language models for symbolic world model generation. arXiv preprint arXiv: 2502.13092, 2025. [31] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [32] Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially observable stochastic domains. Artif. Intell., 101(12):99134, May 1998. [33] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv: 2405.05941, 2024. [34] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. arXiv preprint arXiv: 2305.08891, 2023. [35] Asen Nachkov, Danda Pani Paudel, Jan-Nico Zaech, Davide Scaramuzza, and Luc Van Gool. Dream to drive: Model-based vehicle control using analytic world models. arXiv preprint arXiv: 2502.10012, 2025. [36] NVIDIA, :, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klár, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv: 2501.03575, 2025. [37] OpenAI. Improving image generation with better captions, 2023. Accessed: 2025-05-04. [38] OpenAI. Video generation models as world simulators. https://openai.com/index/ video-generation-models-as-world-simulators, 2024. [39] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report. arXiv preprint arXiv: 2303.08774, 2023. [40] Tim Pearce and Jun Zhu. Counter-strike deathmatch with large-scale behavioural cloning. IEEE Conference on Games (CoG), 2021. [41] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, Yuhui Wang, Anbang Ye, Gang Ren, Qianran Ma, Wanying Liang, Xiang Lian, Xiwen Wu, Yuting Zhong, Zhuangyan Li, Chaoyu Gong, Guojun Lei, Leijun Cheng, Limin Zhang, Minghao Li, Ruijie Zhang, Silan Hu, Shijie Huang, Xiaokang 14 Wang, Yuanheng Zhao, Yuqi Wang, Ziang Wei, and Yang You. Open-sora 2.0: Training commercial-level video generation model in $ 200k. arXiv preprint arXiv: 2503.09642, 2025. [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [43] Marc Rigter, Tarun Gupta, Agrin Hilmkil, and Chao Ma. Avid: Adapting video diffusion models to world models. arXiv preprint arXiv: 2410.12822, 2024. [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. arXiv preprint arXiv: 2112.10752, 2021. [46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [47] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. [48] Ruoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan, Marc-Alexandre Côté, Peter Clark, and Peter Jansen. Can language models serve as text-based world simulators? arXiv preprint arXiv: 2406.06485, 2024. [49] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. arXiv preprint arXiv:2311.17918, 2023. [50] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. [51] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. ivideogpt: Interactive videogpts are scalable world models. arXiv preprint arXiv: 2405.15223, 2024. [52] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399417. Springer, 2024. [53] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors, 2023. [54] Sherry Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Leslie Kaelbling, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv: 2310.06114, 2023. [55] Shaofeng Yin, Jialong Wu, Siqiao Huang, Xingjian Su, Xu He, Jianye Hao, and Mingsheng Long. Trajectory world models for heterogeneous environments. arXiv preprint arXiv: 2502.01366, 2025. [56] Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Gamefactory: Creating new games with generative interactive videos. arXiv preprint arXiv: 2501.08325, 2025. 15 [57] Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, and Raquel Urtasun. Copilot4d: Learning unsupervised world models for autonomous driving via discrete diffusion. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [58] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [59] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [60] Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, and Chuang Gan. Tesseract: Learning 4d embodied world models. arXiv preprint arXiv: 2504.20995, 2025. [61] Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan. Robodreamer: Learning compositional world models for robot imagination. arXiv preprint arXiv: 2404.12377, 2024. [62] Chuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, and Abhishek Gupta. Unified world models: Coupling video and action diffusion for pretraining on large robotic datasets. arXiv preprint arXiv: 2504.02792, 2025. [63] Fangqi Zhu, Hongtao Wu, Song Guo, Yuxiao Liu, Chilam Cheang, and Tao Kong. Irasim: Learning interactive real-robot action simulators. arXiv preprint arXiv: 2406.14540, 2024."
        },
        {
            "title": "A Model Implementation Details",
            "content": "A.1 Model Details Base Model Details. The pre-trained model DynamiCrafter [52] is state of the art latent video diffusion model conditioned on text and image, with its full-sized version ranking high on the VBench leaderboard [31]. It builds on the Stable Diffusion autoencoder [44] and trains 3D U-Net for video generation using web-scale video data. Specifically, starting from the pre-trained VideoCrafter T2V model [9], DynamiCrafter introduces dual-stream conditional image injection paradigm: in one stream, CLIP [42] image encoder embeddings are fed into the U-Net via cross-attention; in the other, images are encoded into VAE latents, which are then replicated along the channel dimension for the full video length and concatenated with the initial noise latents. This mechanism simultaneously injects text-aligned semantic representations and fine-grained visual details, improving video quality. For the noise level k, the model injects such information into the diffusion network by firstly using sinusoidal embedding to transform it into vector, which is subsequentially fed into two-layer MLP, obtaining learned embedding. The embedding is then added into the convolutional features to provide the noise level condition. Image Preprocessing. For all of our experiments, we use the publicly released DynamiCrafter model at 320 512 resolution, which has 1.1B trainable parameters. During data preprocessing we resize the shorter side to 320 px while preserving aspect ratio. After resizing, if the longer side remains below 512 px, we pad with black borders up to 512 px; otherwise, we take the other approach: resizing the longer edge to 512 px, and pad with black borders on the height dimension. This setup is used in both training and inference. For evaluation metrics calculation, we resize the model input to baseline methods resolution. For instance, in CS:GO, we calculate the metrics by firstly cropping out the black paddings in the model output, followed by resizing to 150 280 resolution. Noise level Conditioning. The structure of noise level embedding layers naturally supports the transformation to different noise scales at different frames. Specifically, instead of broadcasting the indentical noise level sinosudual embedding along the temporal axis, we use the independently sampled noise level at each frame, stacking it in the temporal dimension. Action Conditioning. For action conditioning, we inject frame-level action conditions into the base model similar to the injection of noise levels. For cases where actions are discrete, we alter the first layer of the noise conditioning network into learned embedding layer. For cases where the action space is continuous, we simply switch the first layer to linear projection. The embedding obtained through action conditioning is later integrated with the noise conditioning through element-wise addition. A.2 Training Details We use the 320 512 version of DynamiCrafter [52] as base model for all experiments. For robot manipulation as well as the game simulation task, we train for 100k gradient steps; for ablation studies, all models are trained for 30k steps. The training is conducted using 4 40GB NVIDIA A100 GPUs. A.3 Inference Details During auto-regressive rollout, we denoise the current frame by fixing noise levels at the history frames to be zero, whereas denoising the current frame using DDIM [46]. In practice, following diffusion forcing [8], we add small noise ksmall uniformly to history frames. Under all settings in this paper, concerning action guidance, we apply guidance scale of 2.5 for our experiments, as well as guidance rescale factor [34] of 0.7. We believe that the optimal values of these hyperparameters are related to domains, and an extensive hyperparameter search can lead to even better performance. detailed list of hyperparameters regarding the model architecture, training, and inference process in shown Table 4. 17 Table 4: Hyperparameters for Vid2World Hyperparameter Value Architecture Base Model: Resolution Latent Diffusion Downsample Ratio z-shape U-Net Chaneels Noise level Conditioning: Embedding dimension Action Conditioning: Embedding dimension Other Conditioning: Language condition FPS condition Image condition 320 512 True 8 32 32 4 320 1024 1024 Empty Sequence 3 First frame Training Learning rate # training steps Batch size per GPU # GPUs Accumulate gradient batches GPU-type Diffusion Setup: Diffusion steps Noise schedule β0 βK Noise level along Temporal Axis 1.0 105 100k 2 4 2 A100-40GB 1000 Linear 0.00085 0.0120 iid. samples Data Processing: Input video length Normalize Input resize Brightness Contrast Saturation Hue 16 [-1,1] Resize, Center-Crop [0.9,1.1] [0.9,1.1] [0.9,1.1] [-0.05,0.05] Causalization: Mixed weight transfer Causal Mask for Temporal attention True True Action Conditioning: Dropout rate Sampling along Temporal Axis 0.2 iid. samples Sampling Sampler Steps Timestep spacing Action Guidance scale Guidance rescale ksmall DDIM 50 Uniform trailing 2.5 0.7"
        },
        {
            "title": "B Experimental Details",
            "content": "B.1 Dataset Details RT-1 Details. RT-1 [6] is widely used dataset consisting of real-world robot experiences, spanning multiple robot manipulation tasks, including opening drawers, closing drawers, picking and placing. Each episode is sampled at fps=3, with the embodiment, robot arm, performing some certain tasks. In addition to video frames, it also records action sequences as well as annotated language prompts. In our setup, we use the observations obtained by RGB cameras, as well as the action sequence. CS:GO Details. We use the publicly released dataset collected by Pearce and Zhu [40]. It contains different subsets of human players interacting with the CS:GO maps, spanning from expert-level to novice players. Here, we use the largest subset in their dataset, dataset_dm_scraped_dust2, which contains 5.5M frames (95 hours) of online human gameplay from the map Dust II. The dataset is created by scraping user behaviors on online servers, offering diverse set of interactions from policies of all sorts. For each timestep, the actions are represented as an array of discrete values. B.2 Metrics for Video Prediction For both Robot Manipulation and Game Simulation tasks, we adopt commonly used video prediction metrics for image or video generation tasks. These metrics measure either the pixel level or semantic level similarity between the generated videos and ground truth videos. For metrics calculated on each image, the values are obtained by extracting all frames and treat them as independent images for feature extraction and statistical estimation. Next, we provide description for each metric: FID. We compute the Fréchet Inception Distance (FID) introduced by Heusel et al. [25]. FID measures the Fréchet distance between two multivariate Gaussians fitted to Inception-v3 activations of real and generated frames. Specifically, let µr, Σr and µg, Σg denote the empirical means and covariances of these activations for real and generated frames, respectively. FID is defined as: FID(Pr, Pg) = µr µg2 2 + Tr (cid:16) Σr + Σg 2 (ΣrΣg)1/2(cid:17) . FVD. Fréchet Video Distance (FVD), introduced by Unterthiner et al. [47], generalizes FID by embedding entire video clips via pre-trained Inflated 3D ConvNet (I3D) and computing the Fréchet distance between the resulting feature distributions of real and generated videos. Concretely, let Pr and Pg be the distributions of I3D activations for real and generated videos, respectively, with empirical means µr, µg and covariances Σr, Σg. FVD is then defined as: FVD(Pr, Pg) = µr µg2 2 + Tr (cid:16) Σr + Σg 2 (ΣrΣg)1/2(cid:17) . SSIM. Structural Similarity Index Measure (SSIM) [50] quantifies perceptual similarity by jointly comparing the luminance, contrast, and structural information between two image patches. Given pair of patches and y, let µx, µy be their mean intensities, σ2 their variances, and σxy their covariance. The SSIM index is calculated using: x, σ2 SSIM(x, y) = (2µxµy + C1) (2σxy + C2) + µ2 + C1) (σ2 + σ + C2) (µ2 , where C1 = (K1L)2 and C2 = (K2L)2 are stability constants with the pixel dynamic range. For our purpose, we compute SSIM over an 11 11 Gaussian-weighted sliding window and average the local SSIM values to obtain mean SSIM (MSSIM) per frame; the final video-level SSIM score is the average MSSIM across all sampled frames. LPIPS. Learned Perceptual Image Patch Similarity (LPIPS) [59] measures perceptual similarity by comparing deep feature activations of real and generated frames across multiple layers of pre-trained network. Specifically, let ˆf l(x) and ˆf l(y) be the unitnormalized activations at layer for inputs and y, and wl the learned channel-wise weights. LPIPS is computed via: LPIPS(x, y) = (cid:88) 1 HlWl Hl(cid:88) Wl(cid:88) h= w=1 (cid:13) (cid:13) (cid:13)wl 19 (cid:16) ˆf h,w(x) ˆf h,w(y) (cid:17)(cid:13) 2 (cid:13) (cid:13) 2 . (1) PSNR. Peak Signal-to-Noise Ratio (PSNR) [29] quantifies pixel-level fidelity by comparing the maximum possible pixel intensity to the mean squared error (MSE) between two frames. PSNR is defined as: PSNR(x, y) = 10 log10 L2 MSE(x, y) , where is the maximum pixel value (e.g. 255 for 8-bit images). B.3 Details of Vid2World for Robot Manipulation. B.3.1 Implementation The RT-1 [6] dataset is large-scale dataset of real-world robots performing manipulation tasks using robot arms. Some typical tasks in this dataset include: picking, placing, moving near some object, opening drawers and closing drawers. To align with the baseline evaluation methods, we randomly split 4361 episodes as the holdout set, using to rest 82851 episode as the training set. In this case, since the action space is continuous, we use linear layer as the first layer to add the action condition, as described in Appendix A. Following baseline [43], we train the model for up to 100k gradient steps on 4 A100, which takes less time (6.4 days), than the seven days reported for training baseline methods. During training, the model inputs are video and action sequence segments of length 16. At test time, we randomly sample 1024 epsiodes from the evaluation set, and sample segment of 16 frames for each episode. The model is provided the first frame of the segment as well as the action sequence, and the metric is calculated on all 16 frames, same as baseline methods. B.3.2 Baselines We compare Vid2World with several baselines, all utilizing the same base model (Dynamicrafter [53], resolution 320 512), while being different in the ways of transformation. It is worth noting that for all baselines methods in this setting, the model is transformed neglecting the need for interactiveness, i.e. the models are still trained and sampled with homogenous noise levels in all frames, therefore the transformed models are unable to perform auto-regressive rollout. During test time, the models generate videos in non-autoregressive way, aligned with the Vid2World-NAR setting. Next, we provide brief introduction to each baseline method: Action-Conditioned Fine-tuning. In this approach, all parameters of the pre-trained model are finetuned on the action-conditioned dataset. For each timestep of the noisy video x, the corresponding action at is embedded to compute the action embedding et using an embedding table for discrete actions or linear layer for continuous actions. For RT1, action embeddings are both concatenated with and added to the corresponding timestep embeddings. Language-Conditioned Fine-tuning. Language-Conditioned Fine-tuning fine-tunes the pre-trained model using textual description of each video. Each description is embedded via CLIP [42] and incorporated through cross-attention following the approach of the original model. ControlNet [58]. ControlNet freezes the parameters of the pre-trained model and creates trainable copy of its UNet encoder. The trainable branch is conditioned on the action signal and connected to the original decoder via zero-initialized convolutions. In this work, ControlNet is employed with the aim of incorporate action-conditioning into the diffusion process. Classifier Guidance[16] classifier fϕ(a xi) is trained on noisy images xi to predict actions. With weight w, this classifier steers the diffusion sampling process toward samples that are consistent with the specified actions. The resulting noise prediction is ϵfinal(xi, a, i, x0) = ϵpre(xi, i, x0) B.4 Details of Real2Sim Policy Evaluation 1 αtwxi log fϕ(axi). Real2Sim Policy Evaluation [33] aims at evaluating policies using simulation as the surrogate of the real world, serving as an indicator of different policies performance. Through interacting between the policy and the simulation environment, this requires the world model to generate images in an interactive manner. well-performing model should be capable of distinguishing successful trajectories from failure cases by auto-regressively simulating the outcomes of different policy actions. We employ Vid2World as the world model to evaluate three different policies: RT-1 (Begin), RT-1 (15%), RT-1 (Converged), taken for different stages of RT-1 [6] training. Specifically, we sample trajectories from the RT-1 dataset for the given task and extract their initial frames. These frames are provided to each RT-1 policy to generate actions, which are then fed into the world model to simulate the next frame. The policy continues to act on these imagined frames in an iterative manner. For the first frames, new frames are generated autoregressively based on all previously observed frames. Beyond this point, each subsequent frame is generated based on sliding window of the most recent frames. This process continues until sequence of length is produced. We then employ verifier to determine whether each trajectory is successful, and compute the overall success rate accordingly. In our experiments, we sample trajectories from \"close drawer\" task in RT-1 dataset. For each policy, we use sample number = 80, sliding window length = 10, and rollout horizon = 40. The complete procedure is described in Algorithm 3. Algorithm 3 Real2Sim Policy Evaluation Require: World model (ot+1ot, at), policy π(atot), task κ, initial frame set Dκ, trajectory success ot+1 (ot+1ot, ot) Sample at π( ot) if < then Sample initial frame o0 from Dκ for = 0, ..., do verifier ψ(o0:H ) 0, 1 1: Init success_count 0 2: for = 0, ..., do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end for 14: Return success_rate = 1 We provide the instruction for human verification below: end if end for success_count success_count + ψ(o0:H ) ot+1 (ot+1otL:t, atL:t) success_count else Instruction for Human Verification Watch each clip of the robot attempting to close drawer and decide if the attempt succeeds or fails: label Success when, by the final frame, the drawer face sits flush with the cabinet frame (no visible gap and no rebound); label Failure when any gap remains, the drawer re-opens after contact, the robot jams or stops short, or the view prevents you from confirming full closure B.5 Details of Vid2World for Game Simulation B.5.1 Implementation We utilize the CS:GO dataset proposed by Pearce and Zhu [40]. Specifically, we utilize the largest subset of the dataset, which contains human interactions of various levels scraped from online game servers. It contains 5.5M frames (95 hours) of online human gameplay from the map Dust II. Following DIAMOND [1], we utilize exactly the same holdout set of 0.5M frames (corresponding to 500 episodes, or 8 hours) for testing. As actions are discrete values in this domain, the first layer in the action injection world is learned embedding layer. For training and evaluation purposes, we use segments of 16 frames. For evaluation, following DIAMONDs [1] required history frames number of 4, we auto-regressively generate frames from four consecutive history frames, until sequence length of 16 is reached. In this experiment, the metrics are calculated only on the predicted frames, excluding frames used for conditioning. Since the output of the baseline method, DIAMOND, is in resolution 150 280, we downsampled our generated image to this corresponding resolution. For our model, we train for 100k steps. 21 B.5.2 Baselines We use DIAMOND [1], state-of-the-art auto-regressive world model as baseline. It treats the world modeling task as an image generation problem, which learns image diffusion model based on the previous four observations and actions. In practice, the input image diffusion model is downsampled, and separate upsampler is learned to upsample the diffusion models output to higher resolutions. Here, we use the publicly released checkpoints of DIAMOND, which contains both the diffusion model and the upsampler. We evaluated both sampling configurations provided by the authors, namely: 1. DIAMOND-Fast: Under this configuration, the model generates images with lower fidelity in exchange for faster inference speed, necessary for interactive gaming. 2. DIAMOND-HQ: This is the configuration where the generated images have higher fidelity, coming at the cost of slower inference speed. We test our models performance with baseline performances using exactly the same test set. Additional generation results can be viewed in Appendix C.2."
        },
        {
            "title": "C Additional Visualization Results",
            "content": "In this section, we provide additional visualization results for our proposed Vid2World model. Generated results from our model are obtained by auto-regressive roll-out. In Section C.1, we include visual results for the RT-1 dataset in the video prediction task. In Section C.2, we provide generated results in the CS:GO environment under video prediction task. In Section C.3, we provide some generated examples for the Real2Sim Policy Evaluation experiments. C.1 Generation Results of RT-1 We provide additional visualization results for Vid2World on the RT-1 Dataset in Figure 7. As shown in the figure, our model makes video predictions that accurately represent the environment dynamics. Our world model generates physically plausible frame sequence with high fidelity, offering great potential in video prediction tasks. However, limitations still exist. We provide two examples of such limitations in Figure 6. These fall into two categories: 1. Failing to predict fine-grained control: In the upper case of Figure 6, the model predicts the moving directions of the robot arm successfully, but fails to capture the grippers control over the green bag. 2. Regressing to more familiar scenes: In the lower case of Figure 6, although the robot movement is mostly correct, the object grasped changes to more often seen object. Figure 6: Failure cases for RT-1 dataset 22 Figure 7: Comparison between Ground truth and Generated videos by Vid2World 23 C.2 Generation Results of CS:GO We provide generation results of Vid2World compared to baseline methods (DIAMOND [1]) in the CS:GO environment. We observe several interesting phenomena, demonstrating the characteristics, both in strength and in limitations, of our model. We provide the discussion below. Error Accumulation. common challenge for autoregressive models (for example the baseline model DIAMOND) in multi-step prediction is performance degradation due to error accumulation, which is especially pronounced when consecutive frames exhibit large variation. In Figure 8, we compare the qualitative predictions of Vid2World and Diamond under rapid viewpoint changes. By contrast Diamonds frames become progressively blurred, Vid2World maintains sharpness and closely follows the ground truth trajectory. Figure 8: Error Accumulation Action Alignment. The reliability of world model, to large extent, depends on how well its predictions align with the input actions. As shown in Figure 9, Vid2World accurately reflects the aim-down-sights action in its predicted video, whereas Diamond fails to manifest this action. Figure 9: Action alignment. Failure Cases. Despite substantially reducing the accumulated error and preserving action alignment, In this figure, neither Vid2World still encounters failure cases, as demonstrated in Figure 10. Vid2World nor Diamond matches the ground truth. Although the models capability is one important factor leading to failure, the environments randomness, in this case the place for players respawn also adds to the difficulty. 24 Figure 10: Failure Cases in CS:GO environments. Action influence on generated sequence. For world models, it is important to do so-called counterfactual reasoning with the current action, instead of predicting trends based solely on past observations. In Figure 11, we showcase the capability of our model to perform generation based on action sequences. All trajectories start from the same observation, but leading to completely different generated frame sequences in cause of different action sequences. Figure 11: Effect of different actions on generated videos in CS:GO for Vid2World. Trajectories start with the same initial observation, diverging drastically as the result of different action sequence. 25 C.3 Generation Results of Real2Sim Policy Evaluation We provide generation results of Vid2World in the Real2Sim Policy Evaluation task, as shown in Figure 12. Figure 12: Generation Results for Vid2World in Real2Sim Policy Evaluation experiments."
        }
    ],
    "affiliations": [
        "Chongqing University",
        "Tsinghua University"
    ]
}