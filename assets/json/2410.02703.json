{
    "paper_title": "Selective Attention Improves Transformer",
    "authors": [
        "Yaniv Leviathan",
        "Matan Kalman",
        "Yossi Matias"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention improves language modeling performance in a variety of model sizes and context lengths. For example, a range of transformers trained with the language modeling objective on C4 with selective attention perform equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 3 0 7 2 0 . 0 1 4 2 : r a"
        },
        {
            "title": "SELECTIVE ATTENTION IMPROVES TRANSFORMER",
            "content": "Yaniv Leviathan Google Research leviathan@google.com Matan Kalman Google Research matank@google.com Yossi Matias Google Research yossi@google.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Unneeded elements in the attentions context degrade performance. We introduce Selective Attention, simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention improves language modeling performance in variety of model sizes and context lengths. For example, range of transformers trained with the language modeling objective on C4 with selective attention perform equivalently to standard transformers with 2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attentions context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity."
        },
        {
            "title": "INTRODUCTION",
            "content": "Different tasks have different memory requirements. On one extreme, copying an arbitrary sequence requires retaining all sequence elements in memory. On the other extreme, determining whether specific element appeared at least once, only requires persisting constant amount of memory. Transformers (Vaswani et al., 2017) keep the entire history in their context buffers, allowing them to solve tasks such as copying, while famously leading to their squared attention cost. RNNs (Rumelhart et al., 1986) and their modern structured state space variants (Gu et al., 2022; Gu & Dao, 2024) keep only constant-sized sketch of the history, making inference cost linear, but rendering them incapable of solving tasks such as arbitrary string copying. Can we design model that persists just the right amount of context? Several works (see Section 8) aim to improve costs by compressing or otherwise reducing the context size with minimal impact to quality. We take different approach, focusing instead on quality improvement, and treating cost reductions as side benefit. Specifically, it has been demonstrated (Leviathan, 2022) that for some tasks removing unneeded elements from the context buffer enables more efficient transformer programs. Indeed, in the attentions differentiable memory, all memory cells contribute to the data read, and circuitry is needed to filter out the noise generated by irrelevant memories. Reducing the amount of circuitry needed should improve performance. In this work we propose Selective Attention, simple extension to the standard attention mechanism which allows token to decide that another token is no longer needed, reducing the attention that future tokens will pay to it. Selective attention adds no new parameters and only negligible amount of computation, yet yields meaningful improvements in synthetic tasks and natural language modeling for range of model and context sizes. Additionally, we show that elements that are selected to be forgotten by selective attention can be safely removed from the attentions context, leading to substantial reductions in the memory and computation requirements during inference, without penalizing quality. We name our method after the related neuroscience concept of selective attention. Quoting Plebanek & Sloutsky (2017): Selective attention allows adults to focus on task-relevant information, while ignoring task-irrelevant information. This in turn leads to superior processing of task-relevant information. Figure 1: visualization of the soft-masking by selective attention (red strike-through) and attention strength (averaged across heads, blue highlight) for different tasks (see Section 2)."
        },
        {
            "title": "2 MOTIVATING EXAMPLES",
            "content": "Consider transformer processing an input sequence with three tokens: a, b, c. In given layer with the standard attention module, token can decide how much to read from token a, and token can decide how much to read from token a, but token cannot affect how much token reads from token a. Specifically, if token has determined that token is irrelevant or even misleading to future tokens such as c, there is nothing it can do in the given layer to correct for this. Even in subsequent layers, masking token is not trivial. Selective attention enables exactly such masking. To illustrate its usefulness, lets consider the Variable Assignment problem, as well as natural language modeling. In Variable Assignment the input consists of set of repeated assignments to named variables, followed by query for the latest value for one of the variables which the model needs to output. For example, for the input: y=7; x=1; x=3; z=5; x=? the output is 3. Note that Variable Assignment can be seen as generalization of the Search problem (Leviathan, 2022), where repeated occurrences of the query pattern are allowed, and we are tasked with finding the most recent occurrence. It is well known that the Search problem is easily solvable by standard transformers, via induction heads (Olsson et al., 2022). Selective attention facilitates simple reduction from Variable Assignment to Search, whereby every assignment to variable masks out all previous assignments to the same variable. In Figure 1 (top) we see that this is indeed the case for transformer trained with selective attention. In Appendix A.1 we show that transformer with selective attention easily learns general solution to Variable Assignment while standard transformer does not. To motivate selective attention for natural language modeling, we first note that Variable Assignment is common sub-task, e.g. when persisting state. For further motivation, consider the common case where part of the input is ambiguous, and the ambiguity is only resolved at later token. For example, in the sequence: Bar, ##ack, Obama, the first token Bar encodes several competing meanings, but the later tokens ##ack and Obama resolve it to the entity for the ex-president. For many tasks that are mostly concerned with the semantic meaning, later tokens might not want to read the ambiguous meaning from the earlier tokens, so masking them, as with selective attention, might be useful. In Figure 1 (bottom) we see that this is indeed the case for transformer trained with selective attention. In the visualized layer, the last token in multi-token expressions masks out the earlier tokens. For example, ##ack masks out bar; obama masks out both bar and ##ack; ##bm masks out i; and ##la masks out both and ##c. We also observe additional masking, e.g. the token after masks out the tokens and day, perhaps because the token after absorbed the meaning from the tokens and day, or perhaps because the model deems the extra detail is not helpful at this point. Finally, Figure 1 also shows that for the trivial task of Parity*, where intermediate results are stored every other token, so that the models output is only function of the last two tokens, everything but the last two tokens is masked. For the Copy task, selective attention persists the entirety of the string to be copied until copying starts, and then masks out every element as it is copied."
        },
        {
            "title": "3 SELECTIVE ATTENTION",
            "content": "Selective attention is simple modification on top of standard attention. For context size , we produce real-valued soft-mask matrix where Si,j denotes how much token xi masks token xj (see Section 3.1). We then constrain S, e.g. to be causal and non-negative (see Section 3.2). We finally accumulate the information in matrix into new matrix , taking into account masking by all previous tokens (see Section 3.3). The matrix = Accumulate(Constrain(S)) is then simply subtracted from the attention logits before applying the softmax: SelectiveAttention(Q, K, ) = softmax( QK dk )V (1) Figure 2 illustrates sketch implementation."
        },
        {
            "title": "3.1 SELECTION FUNCTION",
            "content": "Computing the selection matrix is akin to computing compatibility score, and many functions can be used, for example, separate bilinear form. Following an observation from Leviathan (2022) that common case for token wanting to mask another is after absorbing its contents (i.e. after attending to it), we instead simply reuse the result of one of the existing heads of the attention module. This means that the selection function adds no new parameters or computation. Note that the head still contributes to attention as usual. We also experimented with separate bilinear form, and in spite of adding additional parameters and computation, this resulted in the same or slightly worse results (see Appendix A.2)."
        },
        {
            "title": "3.2 CONSTRAINTS",
            "content": "Following observations from Leviathan (2022), we apply the following constraints to S: 1. Zero out negative values (i.e. applying ReLU), only reducing attention, never boosting it. 2. Zero out the first column, so as not to mask the <BOS> token. 3. Zero out the diagonal, so as not to let token mask itself. We observe that all three constraints improve performance (see Appendix A.3). 3.3 ACCUMULATION In this work we focus on transformer decoders, so selective attention cannot influence the attention operation by past tokens. We chose cumulative summation as our accumulation function. We observe some improvement by only applying the masking for future tokens (i.e. the masking by token would not affect its own attention operation), so Fi,j = (cid:80) ki1 Sk,j (see ablation in Appendix A.4). . . . attn_logits = einsum(\"bhnd,bhmd->bhnm\", Q, K) / sqrt(dk) attn_logits = where(causal_mask, attn_logits, float(\"-inf\")) = attn_logits[:, 0] = relu(S) S[..., 0] = 0 = (1 - eye(n)) * = roll(S, 1, -2); S[..., 0, :] = 0 = np.cumsum(S, axis=-2) attn_logits -= F[:, None] attn_weights = softmax(attn_logits) . . . # Select head 0. # Only positive selection. # Do not mask <BOS>. # Do not mask self. # Mask strictly in the future. # Accumulate. Figure 2: sketch implementation of selective attention. The colored lines are the additions to standard attention. Note that selective attention adds negligible amount of computation, however care should be taken in practice in order to not interfere with compiler optimizations."
        },
        {
            "title": "4 CONTEXT PRUNING",
            "content": "As presented in Section 3, while beneficial to model quality, selective attention has negligible impact on inference efficiency1. However, an additional modification can improve inference efficiency substantially. Specifically, selective attention can reduce the memory and computation requirements of the attention module, via pruning elements from the context buffer. To see how, note that once token is sufficiently masked by selective attention, it will not contribute meaningfully to any future attention operations. Such tokens can be safely evicted from the context buffer. We could pick fixed threshold, and prune all elements whose soft masking is higher than 1The additional computation of O(hn2) is negligible compared to the O(dn2) of standard attention. 4 the threshold (i.e. Fi,j > τ ), but that would make the memory savings hard to take advantage of (e.g. due to fragmentation and variable number of dropped tokens each iteration). Instead, we observe that the sparsity (i.e. the magnitude of the soft-masking) per layer is stable across samples (see Section 7). To that effect we set fixed memory budget for each layer, which directly translates to memory and compute savings. Since when token is dropped it remains dropped for all future tokens, given memory budget of = K1, . . . , KL tokens for each layer, when processing the first Kl tokens in layer we drop nothing, and for each following token we drop the single notyet-dropped past token with the highest value. This maintains no more than Kl tokens in layer l. Given an overall memory budget, we allocate it between the layers, using greedy iterative approach. For context size , we initialize 0 = . In each iteration t, we set for all = m, and for constant (we use 8 in our experiments), where = argmini L(K t1 Ci), where Ci = (0, . . . , 0, C, 0, . . . , 0) contains at the ith position. In other words, we iteratively reduce the memory budget of the layer that impacts model performance the least. We stop when model performance reaches predefined threshold, in our experiments, the performance of standard transformer without selective attention. 1 = N, . . . , = t1 = t1 Note that with low memory budget there might be some discrepancy between training and inference. Fine tuning the model after the budgets have been set (or even better, in each iteration) might be advantageous and lead to larger reductions in memory budgets, but we havent experimented with this setup yet. As selective attentions masking is beneficial for the model, we observe significant reductions in context sizes without any auxiliary losses (see Section 6.2). Nevertheless, we can further encourage the model to mask out more elements by adding an explicit term to the loss, like so: Lmem = Lppl + ϵ (cid:80)L l=1 maxi L n=pad (2) k=1 min(F Where Lppl is the standard log-perplexity loss, ϵ is small weight factor (in our experiments we set ϵ = 0.1 without further tuning), is the number of layers, n=pad is the number of non-pad tokens, = (cid:80)i and i,k, τ )/τ is our approximation for the memory requirements at the ith token for layer (0 i,k from above by τ so as not to reward increasing it indefinitely (F is already clamped from below to 0). We set τ = 1 without further tuning. Since the memory required for given layer is the maximum memory required for each of the tokens, the loss only considers the maximum among the s. We observe further reduction in context sizes with this explicit loss term (see Section 6.2). i). We clamp l"
        },
        {
            "title": "5 EXPERIMENTAL SETUP",
            "content": "In all of our experiments we use decoder-only transformer with multi-head attention, as in Vaswani et al. (2017), with the following modifications: we use Pre-LN instead of Post-LN (Xiong et al., 2020), learned position encoding, SwiGLU gates instead of MLPs (Shazeer, 2020), normalize the and projections (Dehghani et al., 2023)2, remove the biases (Raffel et al., 2023), and replace the LayerNorms with RMSNorm (Zhang & Sennrich, 2019). Note that we tested other variants, including vanilla decoder-only transformer exactly as in (Radford et al., 2019) and observed similar results. We trained our models with the AdamW optimizer with β1 = 0.9 and β2 = 0.999 for total of 524,288 steps. We used cosine decay and 1,000 linear warmup steps and learning rate of 0.005. We repeated some of the experiments with different learning rates and obtained similar results. We used batch size of 256 and fixed context size of 512 for all training runs except for the context size experiments (Figure 3 left) where we used batch of 128. We follow Esser et al. (2024), and parameterize model size by parameter such that dmodel = 64d and nheads = nlayers = (see Table 7 in Appendix A.9). We trained all of our models on TPUv4s. For the language modeling experiments, we used the C4 (Raffel et al., 2023) dataset with vocabulary of size 8K tokens built with the SentencePiece tokenizer (Kudo & Richardson, 2018). We repeated some of the experiments with vocabulary of size 32K and observed similar results. We also ran experiments with WikiText (Merity et al., 2016), and lm1b (Chelba et al., 2014) and observed similar results. 2For larger models, we observed more cases of divergence when not normalizing the and projections."
        },
        {
            "title": "6.1 GENERATION QUALITY",
            "content": "Transformers with selective attention perform consistently better, as measured by perplexity on the validation set, across model and context sizes. We also observe consistent improvement on downstream task, HellaSwag (Zellers et al., 2019). Figure 3 (left) compares the validation perplexity for causal language modeling on the C4 dataset of decoder-only = 12 transformer models with and without selective attention, for varying context sizes. Likewise Figure 3 (right) compares validation perplexity with and without selective attention, for varying model sizes with context length of 512. We observe improvements across model sizes, and that the improvements grow with the size of the context. Figure 3: (Left) The validation perplexity of = 12 transformer, with (blue) and without (orange) selective attention, for varying context sizes. (Right) The validation perplexity of transformers of various sizes, with (blue) and without (orange) selective attention, for context size of 512. Figure 4 shows that even when equipped with additional attention heads (and increasing the parameters of the attention module proportionally, so that the size of each head remains constant), transformers with standard attention only become comparable to those with selective attention, when they have about double the number of heads (and parameters) as their selective attention counterparts. In addition to perplexity on the validation set, we also measure model accuracy on the HellaSwag benchmark (Zellers et al., 2019), with and without selective attention, for models of various sizes  (Table 1)  . We observe consistent improvements across all model sizes with selective attention. Table 1: Comparing the accuracy of transformers of different sizes with and without selective attention on the HellaSwag benchmark. Accuracy (baseline) Accuracy (selective attention) = 16 = 18 = 20 = 22 = 24 = 26 = 28 38.8% 41.8% 44.0% 46.0% 48.7% 51.0% 53.5% 40.0% 42.6% 45.1% 47.7% 50.3% 52.2% 53.8% 6 Figure 4: Perplexity on the C4 validation set after 524,288 training steps of transformers with various sizes (parameterized by as per Section 5) with and without selective attention. For the cases without selective attention we add additional attention heads with their respective parameters (i.e. we increase the sizes of all projection matrices). Transformers with selective attention perform equivalently to those with standard attention modules with 2X as many heads and parameters. 6.2 INFERENCE EFFICIENCY Efficiency improvements via selective attention stem from reduction in the attention modules context size when using pruning as in Section 4. Specifically, smaller context translates directly to more efficient inference in common scenarios. Indeed, note that during inference with large context and batch size (bn >> d), loading the KV-cache (linear in the size of the context) dominates the memory bandwidth (Pope et al., 2022), which is often the bottleneck for generation (Shazeer, 2019). In addition, for very large context sizes (n >> d), taking the dot product of the query and the keys in the cache and calculating the weighted average of the values in the cache both dominate compute, i.e., in this setup, smaller context directly translates to similar gains in FLOPs. When pruning the context with selective attention, we measure substantial improvements in the memory requirements for the attention module, at the same or better perplexity than the baseline without selective attention. Figure 6 in Appendix A.5 illustrates the trade-off between perplexity and efficiency. For example, for = 12 transformers with context sizes of 512, 1,024, and 2,048, we see that with selective attention we can maintain the baselines perplexity while reducing the memory requirements of the attention module by factors of 5X, 7X, and 8X respectively, without any explicit losses (i.e. using only the standard language modeling objective). When training with the Lmem loss (Equation 2) and an ϵ value of 0.1, the improvements grow to 16X, 25X, and 47X respectively. We also measure the memory savings when considering only very long examples by filtering C4 to only include examples that are at least 90% the size of the context buffer. In that settings we get memory savings of 12X, 18X, and 24X, while maintaining the perplexity of the baseline without selective attention. Finally, to maintain the perplexity gains of selective attention, instead of matching the perplexity of the baseline (i.e. taking the rightmost point on the flat part of the blue graphs of Figure 6), we need 3X, 4X and 4X less memory, for the context sizes above. In all cases we optimized the per-layer budgets on training set, while the reported results are on separate unseen test set."
        },
        {
            "title": "7 SELECTION PATTERNS",
            "content": "It is interesting to question which elements from the context are being masked by selective attention for language modeling. Figure 5 illustrates the values of the matrix for specific example (see Appendix A.6 for the full example text). We observe that some layers (e.g. 6) are mostly dense (i.e. low values), while other layers (e.g. 2) are sparse (i.e. high values). As expected, all layers persist some of the most recent elements, but several of the sparse layers (e.g. layers 1, 4, and 9) also persist elements for long time periods, as can be seen by the vertical lines. This suggests that 7 simply limiting the attention module to local window would not result in the same quality gains as those achieved by selective attention, which we confirm in Appendix A.7. Figure 7 from Appendix A.8 illustrates the values of the matrix averaged over 1,000 examples, and demonstrates that the sparsity patterns are stable across examples. Interestingly, we observe that these sparsity patterns are sometimes stable across different training runs, hinting at some general properties of language modeling (see Figure 8 in Appendix A.8). Figure 9 from Appendix A.8 depicts the items remaining in the context buffer after pruning (see Section 4), with budget set to match the perplexity of standard transformer. We observe that the per-layer budgets correspond well to the values of the matrix as seen in Figures 5 and 7. For example, layer 6 which gets the highest budget also has the lowest values. This might indicate that the scales of the values are consistent across layers. Figure 10 in Appendix A.8 illustrates the value of the last row of (i.e. the masking for the 512th token) for each of the layers. We observe some interesting patterns, for example, layer 4 persists the end-of-sentence periods (.). Figure 5: Visualization of the matrix (greener is lower, i.e. less masking) for = 12 transformer for the text in Appendix A.6."
        },
        {
            "title": "8 RELATED WORKS",
            "content": "Transformer Improvements. Our work aims at improving the transformer architecture. Since its introduction in Vaswani et al. (2017), large volume of research proposed architecture modifications towards an improved model. Some notable works here include Pre-LN instead of Post-LN (Xiong et al., 2020), gated units instead of MLPs (Shazeer, 2020), removing the biases (Raffel et al., 2023), using RMSNorm instead of LayerNorm (Zhang & Sennrich, 2019), normalizing the and projections (Dehghani et al., 2023), and multi-query and group-query attention (Shazeer, 2019; Ainslie et al., 2023). 8 Attention Modifications. Our work focuses on modifying the attention module. Most of the existing research work around modifying attention focuses on different goal than ours, specifically on devising more efficient attention variants. Some such variants are based on approximations to the attention operations and include sparse attention approximations (Child et al., 2019; Ding et al., 2023), and linear attention approximations (Shen et al., 2024; Katharopoulos et al., 2020; Schlag et al., 2021). Some works focus on hardware-aware optimizations instead, such as FlashAttention (Dao et al., 2022) and Ring Attention (Liu et al., 2023a). Context Pruning. part of our work (Section 4) consists of removing elements from the context buffer. Several works employ this mechanism in order to increase inference efficiency. Among those are compression methods, that aim to learn compressed representation for tokens in the context buffer, with or without auxiliary losses, in order to replace several tokens with their compressed form (Munkhdalai et al., 2024; Ren et al., 2023; Yun et al., 2023; Mu et al., 2024; Rae et al., 2019). Another line of work tries to simply remove elements from the context buffer without replacing them with new compressed forms, with minimal negative impact to model quality. The simplest and most widely adopted of these is using local attention windows in some of the layers (Wang et al., 2019; Beltagy et al., 2020; Zaheer et al., 2021). More sophisticated variants employ heuristics to evict less useful tokens from the context buffer instead of just the earliest ones (Oren et al., 2024; Zhang et al., 2023; Liu et al., 2023b; Berchansky et al., 2023; Ge et al., 2024). Dynamic context pruning (DCP) (Anagnostidis et al., 2024) proposed mechanism for fine tuning existing models to make inference more efficient. They learn which tokens to prune from the context buffer via mechanism which shares similarities with selective attention. DCP is more involved (e.g. needs root solving for evaluating the α-sigmoid), introduces new parameters, requires an auxiliary loss, and produces binary prune decisions. Notably, DCP didnt observe the perplexity gains we see from selective attention3. Inference Efficiency. part of our work (Section 4) consists of making inference from transformers more efficient. Many approaches aim to speed up inference from transformers, including distillation (Hinton et al., 2015), sparcification (Jaszczur et al., 2021), quantization (Hubara et al., 2016), architecture modification (So et al., 2022; Shazeer, 2019), and algorithmic optimization (Dao et al., 2022; Leviathan et al., 2022). Finally, we note that the importance of learning to forget has been shown repeatedly in many works, more generally beyond transformers. One of many notable examples are the forget-gates in LSTMs (Hochreiter & Schmidhuber, 1997)."
        },
        {
            "title": "9 DISCUSSION",
            "content": "In this work we introduced Selective Attention, simple parameter-free change to the standard attention mechanism which consistently improves language modeling performance across model sizes and context lengths, and can lead to substantial inference efficiency improvements. Given that it adds no new parameters, only negligible amount of compute, and provides consistent improvements, selective attention might be good default for transformer decoders. Limitations and future directions. We applied selective attention to decoder-only transformers. It could be interesting to investigate its applicability to encoders as well; Reducing the size of the context as in Section 4 improves inference efficiency but not training efficiency. It might be interesting to explore iteratively reducing the size of the context buffer during training; We did not further train the models after removing elements as per Section 4. It seems conceivable that further improvements could be achieved with some additional training after context reduction; We only experimented with pre-training models with selective attention. It is interesting to investigate how it could be applied in fine-tuning step to existing models; While we observed similar results with selective attention in several setups (Section 5), there are still important variants we did not test, notably transformers with multi-query (Shazeer, 2019) and grouped-query (Ainslie et al., 2023) attention, as well as models much larger than 1B parameters; Finally, it would be interesting to implement selective attention in GPU-aware way, similar to Flash Attention (Dao et al., 2022). 3We only became aware of DCP after this work was concluded. It is interesting that both works share some important similarities, although developed from very different perspectives (see Section 10)."
        },
        {
            "title": "IMPROVING NEURAL ARCHITECTURES",
            "content": "In The Art of Transformer Programming, Leviathan (2022) selected set of foundational problems (sorting, searching, addition, etc.) and manually implemented transformers to solve them (i.e. by manually setting the models weights). They showed that several programs become much easier, especially for small transformers, when equipped with mechanism allowing to selectively mask items in the context buffer, similar to selective attention. They further hypothesized that such mechanism will have similar positive effects on language modeling, which motivated our work. Zhou et al. (2023) proposed the RASP-Generalization Conjecture, that Transformers tend to length generalize on task if the task can be solved by short RASP program which works for all input lengths, i.e. that problems that are easily solved by transformers are those that are easily solved by human programmers using RASP. It follows that problems that are not easily solved by humans using RASP are hard for transformers as well, and if we made those easier, by changing the transformer architecture (and respectively the capabilities of RASP) we could meaningfully improve transformers. Similarly, when constructing transformer programs by hand, Leviathan (2022) notes that . . . the most interesting cases are those that are hard for us humans and are hard for the optimizer or the architecture, and understanding these better might be key to creating better AI systems. We are very strong advocates for this method, and believe that finding basic problems for which we cannot program general solution by hand on neural model, is an extremely fertile approach for producing further architecture improvements. ACKNOWLEDGMENTS Wed like to extend huge thank you to Raya Leviathan, Blake Hechtman, Asaf Aharoni, Uri Mendlovic, Danny Lumen, Avinatan Hassidim, Dani Valevski, Eyal Segalis, Molad Eyal, the Theta Labs and Google Research teams, and our families for insightful feedback, ideas, suggestions, and support."
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. URL https://arxiv.org/abs/2305.13245. Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, and Thomas Hofmann. Dynamic context pruning for efficient and interpretable autoregressive transformers, 2024. URL https://arxiv.org/abs/2305.15805. Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020. URL https://arxiv.org/abs/2004.05150. Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat. Optimizing retrieval-augmented reader models via token elimination, 2023. URL https://arxiv.org/ abs/2310.13682. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling, 2014. URL https://arxiv.org/abs/1312.3005. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019. URL https://arxiv.org/abs/1904.10509. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. URL https://arxiv.org/abs/ 2205.14135. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, 10 Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Luˇcic, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters, 2023. URL https://arxiv.org/abs/2302.05442. Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens, 2023. URL https://arxiv.org/abs/2307.02486. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv.org/abs/ 2403.03206. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. URL https://arxiv.org/ abs/2310.01801. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces, 2022. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network, 2015. URL https://arxiv.org/abs/1503.02531. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 17351780, 1997. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations, 2016. URL https://arxiv.org/abs/1609.07061. Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Łukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, and Jonni Kanerva. Sparse is enough in scaling transformers, 2021. URL https://arxiv.org/abs/2111.12763. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention, 2020. URL https://arxiv. org/abs/2006.16236. Taku Kudo and John Richardson. Sentencepiece: simple and language independent subword tokenizer and detokenizer for neural text processing, 2018. URL https://arxiv.org/abs/ 1808.06226. Yaniv Leviathan. The art of transformer programming, 2022. URL https://yanivle. github.io/taotp.html. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding, 2022. URL https://arxiv.org/abs/2211.17192. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for nearinfinite context, 2023a. URL https://arxiv.org/abs/2310.01889. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time, 2023b. URL https://arxiv.org/ abs/2305.17118. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. URL https://arxiv.org/abs/1609.07843. Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens, 2024. URL https://arxiv.org/abs/2304.08467. Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention, 2024. URL https://arxiv.org/abs/ 2404.07143. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 2022. URL https://arxiv.org/ abs/2209.11895. Matanel Oren, Michael Hassid, Yossi Adi, and Roy Schwartz. Transformers are multi-state rnns, 2024. Daniel J. Plebanek and Vladimir M. Sloutsky. Costs of selective attention: When children notice what adults miss. Psychological Science, 28:723 732, 2017. URL https://api. semanticscholar.org/CorpusID:30610756. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference, 2022. URL https://arxiv.org/abs/2211.05102. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://cdn.openai. com/better-language-models/language_models_are_unsupervised_ multitask_learners.pdf. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling, 2019. URL https://arxiv.org/abs/ 1911.05507. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. URL https://arxiv.org/abs/1910.10683. Siyu Ren, Qi Jia, and Kenny Q. Zhu. Context compression for auto-regressive transformers with sentinel tokens, 2023. URL https://arxiv.org/abs/2310.08152. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning internal representations by error propagation, 1986. URL https://api.semanticscholar.org/CorpusID: 62245742. Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers, 2021. URL https://arxiv.org/abs/2102.11174. Noam Shazeer. Fast transformer decoding: One write-head is all you need, 2019. URL https: //arxiv.org/abs/1911.02150. Noam Shazeer. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/ 2002.05202. Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities, 2024. URL https://arxiv.org/abs/1812.01243. David R. So, Wojciech Manke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le. Primer: Searching for efficient transformers for language modeling, 2022. URL https://arxiv. org/abs/2109.08668. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017. 12 Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang. Multi-passage BERT: globally normalized BERT model for open-domain question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 58785882, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1599. URL https://aclanthology.org/D19-1599. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture, 2020. URL https://arxiv.org/abs/2002.04745. Jungmin Yun, Mihyeon Kim, and Youngbin Kim. Focus on the core: Efficient attention via In Findings of the Association for pruned token compression for document classification. Computational Linguistics: EMNLP 2023. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.findings-emnlp.909. URL http://dx.doi.org/10.18653/v1/ 2023.findings-emnlp.909. Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences, 2021. URL https://arxiv.org/abs/2007.14062. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830. Biao Zhang and Rico Sennrich. Root mean square layer normalization, 2019. URL https:// arxiv.org/abs/1910.07467. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavyhitter oracle for efficient generative inference of large language models, 2023. Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? study in length generalization, 2023. URL https://arxiv.org/abs/2310.16028."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 TRANSFORMERS WITH SELECTIVE ATTENTION LEARN GENERAL SOLUTION TO VA B AS N T Transformers with selective attention reach close to 0 validation loss and 100% precision extremely fast when trained on Variable Assignment and they generalize well to out of distribution cases, unlike transformers without selective attention. Setup. We train small transformers (d = 3), with and without selective attention, to solve the Variable Assignment problem with 3 variables, 1,000 possible values, and 128 assignments. We train with batch size of 2,048 for 65,536 steps. In distribution. The transformer with selective attention reaches validation loss of 0.002 (and 100% accuracy) after less than 1,000 training steps. The transformer without selective attention only achieves validation log-perplexity loss of 3.18 and 26% accuracy after 1,000 step. At the end of training (65,536 steps) the transformer with selective attention obtains loss of 2.2e-8, whereas the transformer with standard attention is at 0.01. Both transformers reach 100% accuracy at the end of training. Out of distribution. We observe much stronger generalization capabilities for the transformer with selective attention. When we run on an out of distribution test set with the same 3 variables but only 2 possible values, the transformer with standard attentions accuracy drops substantially to 70% (loss of 3.64). Meanwhile the transformer with selective attention maintains 100% accuracy (with loss of 2.4e-8). We observed similar results in other settings (e.g. 10 variables and 10 possible values). We also repeated the experiments with somewhat larger transformers (d = 8) and observed similar results. A.2 SEPARATE BILINEAR FORM We experiment with using separate bilinear form instead of reusing the output of an existing attention head. We compare transformers (for = 8 and = 12) trained with selective attention on C4 for 524,288 steps, to similarly trained transformers where the selection function is implemented via separate bilinear form (adding additional parameters and computation). The transformers with standard selective attention (i.e. sharing the outputs of an existing attention head) achieve the same or slightly better log-perplexities at the end of training (see Table 2). Table 2: The log-perplexity on the validation set after 524,288 training steps for (1) standard attention, (2) selective attention with separate bilinear form for the selection module (more parameters than the baseline), and (3) selective attention. = 8 = 12 Additional parameters Standard attention Selective attention (separate) Selective attention 2.96 2.91 2.90 2.68 2.63 2.63 No Yes No A.3 ABLATING THE CONSTRAINTS We ablate the 3 constraints selective attention applies to (see Section 3.2). Negative selection. While with selective attention token can decide to reduce attention to another token by all future attention operations, allowing token to strengthen another tokens contribution to all future attention operations does not make sense. Indeed, when removing this constraint (dropping the ReLU, so that can contain negative values) the training does not converge. Masking the <BOS> token. Since several algorithms can benefit from the existence of the sentinel <BOS> token (Leviathan, 2022), it is plausible that masking it is detrimental. When we allow 14 selective attention to mask the <BOS> token, we observe neutral to slightly worse results compared to standard selective attention where the <BOS> token is forced to never be selected, see Table 3. Self-masking. Since selective attention reuses an existing attention head as the selection function, motivated by the absorption observation (see Section 3.1), it seems plausible that token should never mask itself. Indeed, when we allow tokens to mask themselves (i.e. we stop zeroing out the diagonal of S) we observe worse results, see Table 4. Table 3: The log-perplexity on the validation set after 524,288 training steps for (1) selective attention without the <BOS> constraint and (2) selective attention. Selective Attention (w/o <BOS> constraint)"
        },
        {
            "title": "Selective Attention",
            "content": "12 14 16 18 20 24 2.6409 2.5486 2.4750 2.4153 2.3674 2.2909 2.6373 2.5483 2.4741 2.4156 2.3673 2.2865 Table 4: The log-perplexity on the validation set after 524,288 training steps for (1) selective attention without the self-masking constraint and (2) selective attention. 12 14 Selective Attention (w/o self-masking constraint) Selective Attention 2.7348 2.6510 2.5261 2.7251 2.6423 2.5209 A.4 SELF-IMPACT Table 5 compares the results of forbidding self-impact (i.e. not allowing token to affect its own attention operation, as in Section 3.3) to those when allowing it (i.e. not shifting the matrix S). As can be seen, the shifting provides small but consistent improvement. Table 5: The average log-perplexity on the validation set of 3 training runs after 65,536 training steps for selective attention vs selective attention without shifting, for various model sizes. = 10 = 12 = 14 = 16 = = 26 Selective attention (no shift) Selective attention 2.927 2.923 2.832 2.831 2.753 2.750 2.692 2. 2.641 2.639 2.516 2.511 15 A.5 PERPLEXITY-EFFICIENCY TRADE-OFF Figure 6 illustrates the trade-off between perplexity gains and efficiency gains when pruning as in Section 4. See Section 6 for details. Figure 6: The trade-off between perplexity and KV-cache size for = 12 transformers with context sizes of 512, 1,024, and 2,048. Note that in all cases the perplexity with selective attention is better or equal to that of the baseline without selective attention (the dotted lines). Selective attention transformers trained with the Lmem loss and ϵ = 0.1 (Equation 2) match the perplexity of the baseline with 16X, 25X, and 47X less memory, while those with the standard loss match the perplexity of the baseline with 5X, 7X, and 8X less memory, respectively. 16 A.6 EXAMPLE DETAILS The following text from the C4 validation set was used in Figures 5, 9, and 10: the carving their who determines what fashion magazines offer photoshopped the real problem with traditional dental veneers has little to do with how they function or their performance . normal , aesthetically pleasing smile looks like is the real issue . america struggled for decades with defining image as marketplace bent on exploiting peoples flaws for economic gain . the danger of this national obsession has become systemic since the days of twiggy . perfection as the standard to which we should aspire . effects of this insidious marketing made their way into breast implants and the definition of hollywood smile . bodies and their teeth , people use their resources to chase false picture of their perfect self . people make investments in the tens of thousands of dollars at their dentist office to get the perfect smile . this message has become so endemic that people with nice smiles are convinced only perfect hollywood smile is acceptable . one particularly relevant example of this is highlighted in june 2015 article entitled saving jane smile . gary nankin , dds discusses how he saved the smile of patient who was not content with her first set of # porcelain veneers . 1 . endodontic referral for treatment of tooth number 15 , followed by composite core build - up . 2 . to achieve optimal tissue health . teeth and placement of permanent restorations . of dental implant by the periodontist followed by preparation of mandibular teeth and placement of permanent restorations . restore the now fully - healed and osseointegrated implant in the position of tooth number 30 . regarding person smile , the strong link to self - esteem and self - worth make an imperfect set of teeth concern . however , the picture in the article clearly illustrates what appears to be well - constructed and healthy - looking smile . does dentist promote saving smile that 97 % of the people in america would love to show off ? periodontal therapy in both the anterior region and upper left preparation of maxillary the entire premise is puzzling . placement 4 . 6 . 5 . how . 17 A.7 COMPARISON WITH LOCAL ATTENTION Table 6 compares the validation perplexity of = 12 transformers with various local attention patterns (all-local and alternating), to that of standard transformer and to that of transformer with selective attention. For the all-local attention transformers we set all layers to be sliding window attention layers with fixed sized window. For example, All-local 32 denotes transformer where all tokens can only attend up to 32 tokens back. We also include transformers with alternating local and global layers, where we have 3 local attention layers followed by 1 global attention layer, in repeated fashion. For example, Local-global 32 denotes transformer with 3 local attention layers where tokens can only attend up to 32 tokens back, followed by global layer where tokens can attend to all past tokens, and this 4-layer structure is repeated for the 12 layers of the = 12 transformer. We report the perplexity numbers after 524,288 training steps. We observe that all local attention patterns perform worse than the dense baseline, which in turn performs worse than transformer with selective attention. Table 6: Validation log-perplexity of transformers with different local attention patterns."
        },
        {
            "title": "Model Type",
            "content": "All-local 32 All-local 64 All-local 128 All-local 256 All-local 384 All-local 448 All-local 480 Local-global 32 Local-global 64 Local-global 128 Local-global 256 Local-global 384 Local-global 448 Local-global 480 Baseline (standard attention) Selective attention Validation Log-Perplexity 2.7860 2.7386 2.7154 2.6981 2.6873 2.6849 2. 2.7046 2.7105 2.7154 2.6993 2.6895 2.6870 2.6861 2.6815 2.6372 18 A.8 ADDITIONAL FIGURES FOR CONTEXT PRUNING Figure 7 illustrates the values of the matrix averaged across 1,000 examples of length at least 512 from the C4 dataset. Figure 8 compares the values from Figure 7 to those obtained from different training run (different random initialization and different data shuffle). While this isnt always the case, we sometimes observe stable sparsity patterns like those in this example, hinting at some general properties of language modeling on C4. Figure 9 illustrates the tokens that remain in the context buffer after pruning (as in Section 4) for the example text (see Appendix A.6), for = 12 model with selective attention, trained with the Lmem loss (Equation 2, ϵ = 0.1), for memory budget where the validation perplexity matches transformer without selective attention. The per-layer memory budgets chosen by the pruning algorithm for this model are: [8, 48, 8, 8, 24, 8, 168, 16, 8, 64, 8, 8], leading to memory saving factor of 16X. Figure 10 shows which tokens are pruned for the example text from Appendix A.6. Figure 7: Visualization of the matrix (greener is lower, i.e. less masking) for = 12 transformer averaged across 1,000 examples. 19 Figure 8: Visualization of the matrix (greener/bluer is lower, i.e. less masking) for = 12 transformer averaged across 1,000 examples for two training runs (different random initialization, and different shuffle of the training data). While we only have anecdotal evidence, it is interesting that we sometimes observe these stable sparsity patterns across training runs. Figure 9: Visualization of the persisted elements for = 12 transformer for the text in Appendix A.6. The white pixels denote tokens removed from the context buffer as per Section 4. 20 Figure 10: visualizing of the elements that are masked for the last (512th) token, for = 12 transformer for the text in Appendix A.6. We observe some interesting patterns, for example, layer 4 persists the end-of-sentence periods (.). 21 A.9 PARAMETER COUNTS Table 7 shows the actual parameter counts for models with different ds (see Section 5). Note that selective attention does not add any extra parameters. Table 7: The number of model parameters for different values of as in Section 5. nlayers nheads dmodel Number of parameters 8 10 12 14 16 18 20 22 24 26 8 10 12 14 16 18 20 22 24 26 28 8 10 12 14 16 18 20 22 24 26 28 512 640 768 896 1024 1152 1280 1408 1536 1664 1792 33,603,584 59,699,200 97,615,872 149,666,048 218,226,688 305,717,760 414,387,200 546,641,920 704,950,272 891,468,032 1,108,645,"
        }
    ],
    "affiliations": [
        "Google Research"
    ]
}