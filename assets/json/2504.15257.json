{
    "paper_title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
    "authors": [
        "Hongcheng Gao",
        "Yue Liu",
        "Yufei He",
        "Longxu Dou",
        "Chao Du",
        "Zhijie Deng",
        "Bryan Hooi",
        "Min Lin",
        "Tianyu Pang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at https://github.com/sail-sg/FlowReasoner."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 7 5 2 5 1 . 4 0 5 2 : r FlowReasoner: Reinforcing Query-Level Meta-Agents Hongcheng Gao1,2, Yue Liu3, Yufei He3, Longxu Dou1, Chao Du1 Zhijie Deng4, Bryan Hooi3, Min Lin1, Tianyu Pang1 1Sea AI Lab, Singapore 2University of Chinese Academy of Sciences 3National University of Singapore 4Shanghai Jiao Tong University gaohongcheng23@mails.ucas.ac.cn, yliu@u.nus.edu"
        },
        {
            "title": "Abstract",
            "content": "This paper proposes query-level meta-agent named FLOWREASONER to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FLOWREASONER. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FLOWREASONER is enabled to generate personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FLOWREASONER. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at https://github.com/sail-sg/FlowReasoner."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) [1, 40, 43, 52, 30] have exhibited remarkable power in various meaningful yet challenging domains, like chatbots [35], code [9], math [36], robotics [23], etc. LLM-based multi-agent systems [17, 51, 26], which are characterized by planning, reasoning, tool use, and memory, become the foundation of these LLM-driven applications.1 While effective, most of them are manually designed, increasing human resource costs and limiting scalability. To mitigate this challenge, early automatic methods are proposed to optimize the prompts [55, 22, 61, 53] or hyper-parameters [41]. But they still rely on the fixed workflow of the multi-agent system, which requires human effort to manually design workflows for each new scenario. From this motivation, various graph-based methods [64, 33, 56, 11] formulate the workflows as graphs or networks and automate the workflow designs. However, the structural complexity of graphs limits their scalability [18]. To overcome this limitation, state-of-the-art methods represent the multi-agent systems as programming codes [18] and prompt performant LLM, e.g., GPT-4o, as meta-agent to optimize workflows via complex search algorithms on carefully designed search sets [58, 42, 57]. These previous methods focus on task-level meta-agents, generating merely single task-specific multi-agent system that applies to one kind of task, e.g., code generation task, as in Figure 1 (a). However, for individual user queries, these one-size-fits-all systems lack the capability for automatic adaptation. To enhance the adaptability of multi-agent systems for individual user queries, this paper aims to design query-level meta-agent to generate query-specific multi-agent system for each user query, e.g., build 2048 game, as shown in Figure 1 (b). Equal contribution. Corresponding author. 1This paper defines multi-agent system as system consisting of multiple agents operating under workflow. Preprint. Under review. Figure 1: Task-Level Meta-Agents vs. Query-Level Meta-Agents at Inference Time. denotes user query, e.g., build 2048 game. (q) denotes one kind of task, e.g., code generation task, which is distribution of user queries. Given t, previous task-level meta-agent Ameta_task aims to search task-specific multi-agent system Stask to solve all queries sampled from t, i.e., one system per task. Differently, given one user query q(i), our query-level meta-agent Ameta_query conducts reasoning and output query-specific multi-agent system (i) query for q(i), i.e., one system per query. We first identify that the success of task-level meta-agents largely depends on carefully designed search sets, as they rely on complex search algorithms. However, such search sets are unavailable in the setting of query-specific multi-agent systems. To address this issue, instead of relying on search algorithms, we propose to integrate external execution feedback of the generated multi-agent system, based on which reasoning-driven meta-agent is leveraged to polish the system. We dub such meta-agent as FLOWREASONER. We first synthesize thousands of warm-up SFT data using DeepSeek R1-671B [44] as the meta-agent to generate multi-agent systems and process user queries individually. These synthetic data are then used to finetune DeepSeek-R1-Distill-Qwen-7B, enabling basic reasoning for multi-agent system generation. Furthermore, we enhance its reasoning capabilities for generating novel query-level multi-agent systems through reinforcement learning (RL), incorporating external execution feedback. multi-purpose reward is designed to guide RL training, focusing on performance, complexity, and efficiency. During inference, FLOWREASONER leverages deliberative reasoning to generate novel query-level multi-agent system for each user query, achieving one system per user query. The main contributions are summarized as follows: We propose query-level meta-agent termed FLOWREASONER to automate the designs of querylevel multi-agent systems, improving their adaptability in real-world scenarios. We train FLOWREASONER to reason from external execution feedback via RL, guided by multi-purpose reward considering performance, complexity, and efficiency. We demonstrate the superiority of FLOWREASONER via extensive experiments and open-source it."
        },
        {
            "title": "2 Related Work",
            "content": "LLM-Based Multi-Agent Systems. LLM-based multi-agent systems [15, 10, 7, 38, 19] serve as the foundation of various LLM-powered real-world applications, e.g., code intelligence [9], computer 2 use [2], and deep research [37]. LLM-based agents are equipped with planning capabilities, database access, and tool function invocation. These agents collaborate within multi-agent system, leading to promising performance. Most multi-agent systems are manually designed, increasing the costs of human resources and limiting the scalability. To address this issue, researchers propose automation methods to automate the design of multi-agent systems. Early methods [53, 22, 61] are proposed to automate the agentic designs via optimizing prompts [55] or hyper-parameters [41]. For example, [5, 54, 12] adopt the evolution algorithms to automate the agent profiling. Although effective, they merely optimize the agents but keep the workflow of the multi-agent system fixed, which still requires human effort to manually design for each new scenario. To address this problem, several methods [27, 62, 59, 42, 63] are proposed to automate the design of the entire agentic workflow. For example, researchers [64, 22, 33, 56, 11] formulate the workflows as graphs or networks and then optimize the connections between nodes. To improve the efficiency, ADAS [18] proposes to use programming codes to represent both agents and workflows. It also introduces meta-agent to generate these workflows and presents the meta-agent search to optimize the designs of multi-agent systems. At the same time, AFLOW [58] also adopts the code representation but proposes Monte Carlo Tree Search (MCTS) to optimize it. In addition, MaAS [57] presents the agentic supernet and then conducts the multi-agent architecture search. Differently, ScoreFlow [49] trains workflow generator to generate better workflows via direct preference optimization (DPO) [39]. This paper argues that the previous methods are task-level meta-agents. As shown in Figure 1 (a), they merely generate single task-specific multi-agent system for solving one kind of task. However, these one-size-fits-all systems are rigid and unable to automatically adapt or customize to individual user queries within task. From this motivation, we aim to propose query-level meta-agent to generate query-specific multi-agent system for each user query as shown in Figure 1 (b). Reasoning in LLMs. The ability to reason is essential for LLMs, enabling them to emulate human thinking patterns. Pioneering work [50, 24] has facilitated this by prompting LLMs to think step by step. Beyond this approach, reasoning capabilities are further enhanced through frameworks such as self-correction [25], self-critique [21], debate [29, 10], and plan-and-solve [48]. Additionally, efforts like [16, 14] seek to transition LLMs reasoning processes into the latent space. OpenAI has advanced reasoning in LLMs by developing the o1 model, demonstrating the potential for improvement through test-time scaling. Inspired by this, models such as QwQ [47], QvQ [46], DeepSeek [44], and Kimi [45] have followed suit, developing o1-like reasoning architectures. Moreover, OpenAIs o3 model has been announced to achieve promising results on the ARG-AGI benchmark [3]. LLMs progressively shift from intuitive processing (System 1) to deliberative reasoning (System 2) [28]. Besides, researchers demonstrate that reasoning can improve safety [31] and alleviate hallucination [13]. However, [8] examines the overthinking problem observed in o1-like models. To alleviate this problem, token efficiency methods [32] are proposed to reduce the token costs while maintaining the reasoning quality. This paper develops an o1-like reasoning model to serve as query-level meta-agent, getting rid of complex search algorithms and the carefully designed search set."
        },
        {
            "title": "3 Problem Definition",
            "content": "We denote user query as q, e.g., build 2048 game. Then user task is defined as distribution of user queries, denoted as = (q), e.g., code generation task. multi-agent system is denoted as = {A, W}, where = {A1, ..., An} denotes the agents in the system and is the workflow of collaboration among the agents. As shown in Figure 2 (a), in traditional multi-agent systems [17, 51, 26], the agents and the workflows are designed manually according to one kind of task = (q), as formulated Stask = H(t), where denotes human experts, and Stask denotes the task-level multi-agent system which is fixed for all queries in one task. This kind of manually designed multi-agent system leads to extensive human costs. Besides, such one-size-fits-all system fails to allocate inference resources dynamically for different user queries within the task. As illustrated in Figure 2 (b), search-based automatic multi-agent systems [58, 18, 5] are proposed to reduce human effort. Specifically, these approaches first prompt an LLM, such as GPT-4o, to act as meta-agent, generating multiple candidate multi-agent system designs. Subsequently, complex search algorithms are employed on the carefully designed search set to identify the optimal system for completing the task. We denote this kind of meta-agent as task-level meta-agent Ameta_task since they 3 Figure 2: Architectural Comparison of Three Multi-Agent Systems. (a) Manually-designed Multi-agent System, (b) Search-based Automatic Multi-agent System, and (c) Reasoning-based Automatic Multi-agent System. can merely generate one task-level multi-agent system Stask to solve one kind of task t. Although effective, they are still one-size-fits-all systems. Besides, the search algorithm is time-consuming and relies on the search set, which is absent in one user query."
        },
        {
            "title": "4 Meta-Agent FLOWREASONER",
            "content": "To solve these problems, we develop reasoning-based automatic multi-agent system shown in Figure 2 (c). Concretely, by guiding the model to reason from external execution feedback, we train query-level meta-agent denoted as Ameta_query, which can automatically propose novel query-level multi-agent system for each user query q, as formulated Squery = Ameta_query(q). Then, Squery accomplishes the specific user query and obtain the result a, i.e., = Squery(q). Subsequently, the evaluator evaluates the performance of the proposed multi-agents system Squery by comparing the result with ground truth agt as formulated E(a, agt). Our proposed method is more practical in real-world scenarios, as it can design an optimal multi-agent system for each specific query. The training pipeline of our method is demonstrated in Figure 3. 4 Figure 3: Training Pipeline of FLOWREASONER. It consists of (1) Reasoning Data Distillation, (2) Reasoning SFT Warmup, (3) Reinforce Reasoning from external execution feedback. 4.1 Learn to Reason Reasoning Data Syntheses. To enable our model to learn how to reason workflows based on external execution feedback, we first generate multi-round reasoning data using R1-671B[44]. For given input query q, the R1 model generates rounds of reasoning Rquery and multi-agent system Squery with external execution feedback at each round as follows: {{Rquery, Squery}(1), ..., {Rquery, Squery}(l)} = Ameta_query(q). (1) Then, we concatenate the models rounds of reasoning and multi-agent system to form reasoning process ˆR and final multi-agent system ˆS, and pair it with the instruction and query to construct our training sample {I, q, ˆR, ˆS}. Based on this, we construct warmup SFT dataset D. Reasoning SFT Warmup. After creating the reasoning training dataset D, we proceed to perform reasoning SFT for warmup. We input the instruction and query q, then guide DeepSeek-R1-DistillQwen-7B to output reasoning process ˆR and final multi-agent system ˆS. It can be formulated as follows: LSFT = (I,q, ˆR, ˆS)D log Pθ( ˆR, ˆS I, q), (2) where θ denotes the model parameters. Through SFT, we unlock the models reasoning ability regarding workflow generation. 4.2 Reinforce Reasoning from External Execution Feedback After the SFT stage, we use reinforcement learning phase to further enhance the models reasoning capabilities through workflows built on external execution feedback. We aims to leverage feedback to improve performance on complex tasks in multi-round multi-agent systems. Following the DeepSeekR1-Zero [44], we adopt standard GRPO (Grouped Relative Policy Optimization) as our training method. GRPO works by sampling multiple outputs for each query and computing relative advantages based on the rewards these outputs receive. The policy is then updated by maximizing an objective function incorporating these relative advantages. The GRPO objective function can be expressed as follows: LGRPO(θ) = qt,{oi}G i=1πθold (Oq) 1 G (cid:88) i=1 (cid:111) (cid:110) min [rratio, clip(rratio, 1 ε, 1 + ε)] ˆEi,m βDKL(πθπref ) , (3) where represents the number of sampled trajectories in each group, oi denotes the i-th trajectory, oi is the length of trajectory oi, rratio = πθ(oi,mq,oi,<m) πθold (oi,mq,oi,<m) represents the probability ratio between new and old policies, ˆEi,m is the estimated advantage for the m-th token in the i-th trajectory, ε is small positive clipping parameter that prevents excessively large update steps, and the KL regularization term constrains policy drift to maintain stability during training. 5 (a) Ablation of Meta-agent (b) Ablation of Workers Figure 4: Ablation of Meta-agent and Workers. (a) Accuracy of different meta-agents with o1-mini as workers. (B) Accuracy of the generated workflow with different workers. Since we can obtain reward from the external environment in every round, we use process reward supervision. The reward for each reasoning round is normalized, and the advantage function is calculated as the sum of subsequent normalized rewards. r(i) . ˆEi,m = (cid:88) (4) Here, the normalized reward for the j-th round of the i-th candidate output is defined as r(i) = jT r(i) mean(R) std(R) kj , where is scaling factor and is threshold used to exclude the first items from the calculation. The set represents the list of scores for each round across all candidates, and r(i) is calculated by the performance of the proposed solution (i.e. pass rate), algorithm complexity (i.e. complexity score of abstract syntax tree) and diversity (i.e. distinctness ratio followed by Chen et al. [8]) of workflow. is the score of candidate in round j. Each score r(i) 4.3 Generate Multi-Agent Systems with FLOWREASONER Constructing multi-agent system is essentially an optimization problem with the goal of designing an optimized system, query, that responds to user queries. When user submits query q, the system produces an answer = Squery(q). The performance of the system in each round is evaluated using an external feedback function E(a, agt), where agt represents the ground truth answer. In this framework, meta-agent, Ameta_query, is responsible for optimizing the workflow with external execution feedback E(a, agt) in every round. The pass rate of the proposed solution for the given query, as the key performance indicator of the system, serves as the external execution feedback. The optimization space is defined by all possible configurations of nodes and edges. Here, nodes represent various parameters (such as language models, prompts, temperature, and output formats), and edges capture the interactions or data flows between these nodes. By representing both nodes and edges in code and employing predefined operators (such as Ensemble, Review, and Revise) along with custom operator to combine these elements, FLOWREASONER utilizes an l-round optimization process same as Aflow [58] to arrive at the final multi-agent system: (5) E(Squery(q), agt), query = arg max SqueryS where query is the optimal multi-agent system refined through optimization. By optimizing multiagent system through iterative external execution feedback, FLOWREASONER can construct highly adaptive system that maximizes the accuracy and performance of solution for query."
        },
        {
            "title": "5 Experiments",
            "content": "Datasets. Given our focus on creating workflows tailored to individual user queries rather than the general task, we restrict our scope to code generation tasks, as they can provide test cases as external execution feedback for the workflow construction process. Among various benchmarks, we consider three 6 Table 1: Performance Evaluation. Accuracy comparison across three code benchmarks for three categories of baselines - individual models, manual workflows, and automated workflow methods - alongside our FLOWREASONER-14B. For manual methods, model names in parentheses indicate the worker model used. Type Vanilla Manual Auto Method o1-mini GPT-4o-mini Self-Refine (4o-mini) [34] LLM-Debate (4o-mini) [10] LLM-Blender (4o-mini) [20] Self-Refine (o1-mini) [34] LLM-Debate (o1-mini) [10] LLM-Blender (o1-mini) [20] AutoAgents [5] ADAS [18] Aflow [58] MaAS [57] Ours FLOWREASONER-14B BigCodeBench HumanEval MBPP Overall 57.67 56.33 54.78 56.88 57.46 56.68 57.25 59.51 56.65 53.87 59.83 60.33 63. 95.42 88.55 89.83 91.64 89.44 94.74 95.83 96.37 88.91 84.26 94.15 95.42 97.26 74.19 71.73 69.64 70.28 76.39 73.64 74.28 78. 72.03 68.47 82.40 84.16 92.15 71.37 68.60 67.29 68.69 71.25 70.63 71.33 74.22 68.92 65.48 75.63 76.81 81. Table 2: Ablation Study on Model Sizes and Training Stages. Accuracy(%) comparison across three code benchmarks for models of different sizes (7B/14B) at both the Supervised Fine-Tuning (SFT) stage and the combined SFT with Reinforcement Learning (SFT+RL) stage. Stage Size BigCodeBench HumanEval MBPP Overall SFT SFT+RL SFT SFT+RL 7B 7B 14B 14B 61.79 62.78 62.83 63. 96.38 96.95 97.18 97.26 87.22 89.86 91.91 92.15 78.89 80.53 81.50 81.89 representative datasets: BigCodeBench[65], which emphasizes engineering-oriented tasks, and two algorithmically focused benchmarks, HumanEval[6] and MBPP [4]. This selection enables us to comprehensively evaluate workflow discovery across diverse spectrum of code generation challenges. Baselines. In line with prior work [18, 58], we evaluate FLOWREASONER against three categories of baselines: (1) single-model direct invocation, where single LLM is prompted to solve the problem without additional structure; (2) manually designed workflows, including Self-Refine [34], LLM-Debate [10], and LLM-Blender [20], which incorporate human-crafted reasoning strategies; and (3) automated workflow optimization methods, such as Aflow [58], ADAS [18], and MaAS [57], which construct workflows through search or optimization over possible reasoning structures. This comparison enables comprehensive assessment of FLOWREASONERs effectiveness relative to both static and adaptive baselines. Implementation Details. For the manually designed workflow baselines, we employed both o1-mini and GPT-4o-mini as worker models for each method. For the automated workflow optimization baselines, we adopted the original configurations as described in MaAS [57]. In our proposed method, FLOWREASONER, we trained two variants of DeepSeek-R1-Distill-Qwen with 7B and 14B parameters, respectively, and used o1-mini as the worker model. We fixed the number of workflow iterations to 10. To assess performance, we used the standard pass@1 metric for code accuracy, consistent with prior work [6]. More details can be found in Appendix A. 5.1 Experiment Results Performance Comparison. Table 1 presents the performance comparison between our proposed method and the baselines. FLOWREASONER-14B consistently outperforms all competing approaches across the three benchmark datasets. Notably, it achieves an overall improvement of 5 percentage points over the strongest baseline, MaAS, and exceeds the performance of its underlying worker 7 max_retries = 3 solutions = [] 1 async def __call__ ( self , problem : str , test_cases : list ) : 2 3 4 5 6 7 8 9 10 11 12 for _ in range ( max_retries ) : for attempt in range (3) : # Try generating code up to 3 times solution = await self . t _ e _ e e ( test_result = await self . test ( problem = problem , instruction = prompt_custom . E E _ E _ M ) problem = problem , solution = solution [ response ] , test_cases = test_cases ) if test_result [ result ]: solutions . append ( solution [ response ]) break # Break the inner loop if correct solution is found elif attempt < 2: # If it not the last attempt , try to improve the solution error_analysis = await self . custom ( input = \" Problem : { problem } nFailed solution : { solution [ response ]} nError : { test_result [ solution ]} \" , instruction = prompt_custom . O _ L S _ M ) prompt_custom . E E _ E _ M = \" { prompt_custom . E E _ E _ M } { error_analysis [ response ]} \" if not solutions : # If no correct solution was found after 3 attempts solutions . append ( solution [ response ]) # Add the last generated solution best_solution = await self . sc_ensemble ( solutions = solutions , problem = problem ) return best_solution [ response ] , self . llm . cost_manager . total_cost (a) Workflow generated by FLOWREASONER-14B for the BigCodeBench task of generating traffic data for various vehicle types over specified number of hours, saving the data to CSV file with columns, and plotting it in line chart. async def __call__ ( self , problem : str , test_cases : list ) : solution = await self . t _ e _ e e ( problem = problem , entry_point = entry_point , instruction = prompt_custom . E _ E E _ M ) # Add review to check the solution review = await self . custom ( input = \" Problem : { problem } nGenerated solution : { solution [ response ]} \" , instruction = prompt_custom . REVIEW_PROMPT ) # Improve the solution based on the review information im pro ve d_s olu tio = await self . t _ e _ e e ( problem = problem , instruction = \" { prompt_custom . DE _ R _ M } nConsider this review : { review [ response ]} \" ) return im pr ove d_s olu tio [ response ] , self . llm . cost_manager . total_cost 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 1 3 4 5 6 7 8 9 10 11 12 13 14 (b) Workflow generated by FLOWREASONER-14B for the HumanEval task of spliting the string into words and returning an array of the words. Figure 5: Cases of Workflows generated by FLOWREASONER-14B for the tasks of BigCodeBench and HumanEval. model, o1-mini, by substantial margin of 10%. These results highlight the effectiveness of our workflow-based reasoning framework in enhancing code generation accuracy. Ablation on Model Size and Training Stages. To investigate the impact of model size and training stages on performance, we conducted an ablation study comparing different configurations of FLOWREASONER in Table 2. We observed that the 14B variant consistently outperformed the 7B counterpart across all benchmarks, indicating positive correlation between model scale and reasoning effectiveness. Furthermore, within each model size, versions trained with both SFT and RL exhibited notable improvements over those trained with SFT alone, demonstrating the complementary benefits of incorporating RL in enhancing workflow-guided code generation. 8 Table 3: Generalization Evaluation. Accuracy of our trained meta-agent FLOWREASONER-7B/14B when paired with alternative workers including Qwen2.5 Coder, Claude, and GPT4o-mini. Worker/Meta-agent o1-mini Qwen2.5Coder-32b Claude3.5 GPT-4o-mini FLOWREASONER-7B (SFT+RL) FLOWREASONER-14B (SFT+RL) BigCodeBench HumanEval MBPP BigCodeBench HumanEval MBPP 62.77 50.17 60.67 59.18 96. 92.89 96.07 94.24 89.86 80.40 87.63 82.19 63.53 52.67 61.12 59.75 97. 93.69 96.52 94.52 92.15 78.90 89.82 82.27 Ablation of Meta-agents and Workers. To analyze the impact of meta-agent and worker selection, we conducted an ablation study on the BigCodeBench dataset. Figure 1 presents performance comparisons under various meta-agent and worker configurations. As shown in Figure 4 (a), opensource models exhibited poor performance when paired with o1-mini as the worker and no initial workflow, frequently generating error-prone workflows. This highlights limitation of current open-source models, which struggle to produce reliable workflows solely based on instruction prompts and are heavily reliant on predefined, manually crafted workflows. In contrast, API-based models demonstrated stronger performance, likely attributable to their superior instruction-following capabilities. Figure 4 (b) further examines worker effectiveness when using high-performing metaagent (Claude 3.5). We compared the open-source model Qwen2.5-Coder-32B and three API-based models, including Claude 3.5, GPT-4o-mini, and o1-mini. Among these, o1-mini achieved the best overall performance, suggesting its suitability as worker model in the FLOWREASONER framework. Generalization Capability. To evaluate whether FLOWREASONER, trained with o1-mini as the worker, can generalize its planning capabilities to alternative workers, we conducted experiments by substituting the worker with Qwen2.5-Coder, Claude, and GPT-4o-mini, while keeping the meta-agent fixed as either FLOWREASONER-7B or FLOWREASONER-14B. As shown in Table 3, FLOWREASONER exhibits notable degree of transferability, maintaining consistent performance across different worker models on the same tasks. These results suggest that the planner is not tightly bound to specific worker and can adapt its strategies effectively across diverse execution agents. Case Study. Figure 5 illustrates two example workflows generated by FLOWREASONER-14B for representative tasks from BigCodeBench and HumanEval, respectively. The workflow corresponding to the BigCodeBench task exhibits greater complexity, reflecting the more challenging and engineering-oriented nature of the task. In contrast, the HumanEval workflow is substantially more concise, aligning with the relative simplicity and algorithmic focus of the task. These examples demonstrate FLOWREASONER ability to adapt the structure and granularity of workflows based on task complexity. More cases can be found in Appendix B."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present FLOWREASONER, query-level meta-agent designed to automate the creation of personalized multi-agent systems for individual user queries. Unlike previous task-level approaches that create fixed, one-size-fits-all systems, FLOWREASONER dynamically generates tailored workflows for each specific query through reasoning-based optimization. The system leverages external execution feedback and reinforcement learning with multi-purpose rewards focusing on performance, complexity, and efficiency to generate optimized workflows without relying on complex search algorithms or carefully designed search sets. Experimental results demonstrate that FLOWREASONER-14B outperforms both manually designed workflows and existing automated methods across multiple code generation benchmarks, notably improving o1-minis performance by overall 10.52% on three benchmarks, thus proving the effectiveness and adaptability of reasoning-driven workflow generation. Besides, the observation when pairing FLOWREASONER with different worker models further confirms its generalization capabilities. Our approach reduces human resource costs while enhancing scalability by enabling more adaptive and efficient multi-agent systems that dynamically optimize their structure based on specific user queries rather than relying on fixed workflows for entire task categories."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku. https://www.anthropic.com/news/3-5-models-and-computer-use, 2024. [3] ARC-AGI."
        },
        {
            "title": "Abstraction and reasoning corpus",
            "content": "for artificial general intelligence. https://github.com/fchollet/ARC-AGI/, 2024. [4] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [5] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje Karlsson, Jie Fu, and Yemin Shi. Autoagents: framework for automatic agent generation. arXiv preprint arXiv:2309.17288, 2023. [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [7] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2(4):6, 2023. [8] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. [9] CognitionAI. the https://www.cognition.ai/blog/introducing-devin/, 2024. Introducing devin, first ai software engineer. [10] Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate, 2023. URL https://arxiv. org/abs/2305.14325, 3, 2023. [11] Shangbin Feng, Zifeng Wang, Palash Goyal, Yike Wang, Weijia Shi, Huang Xia, Hamid Palangi, Luke Zettlemoyer, Yulia Tsvetkov, Chen-Yu Lee, et al. Heterogeneous swarms: Jointly optimizing model roles and weights for multi-llm systems. arXiv preprint arXiv:2502.04510, 2025. [12] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023. [13] Hongcheng Gao, Jiashu Qu, Jingyi Tang, Baolong Bi, Yue Liu, Hongyu Chen, Li Liang, Li Su, and Qingming Huang. Exploring hallucination of large multimodal models in video understanding: Benchmark, analysis and mitigation. arXiv preprint arXiv:2503.19622, 2025. [14] Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023. [15] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024. [16] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [17] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 3(4):6, 2023. [18] Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint arXiv:2408.08435, 2024. [19] Yichong Huang, Xiaocheng Feng, Baohang Li, Yang Xiang, Hui Wang, Ting Liu, and Bing Qin. Ensemble learning for heterogeneous large language models with deep parallel collaboration. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 119838119860. Curran Associates, Inc., 2024. [20] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023. [21] Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, et al. Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation. arXiv preprint arXiv:2311.18702, 2023. [22] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Saiful Haq, Ashutosh Sharma, Thomas Joshi, Hanna Moazam, Heather Miller, et al. Dspy: Compiling declarative language model calls into state-of-the-art pipelines. In The Twelfth International Conference on Learning Representations, 2024. [23] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [24] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [25] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. [26] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. [27] Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, and Yongfeng Zhang. Autoflow: Automated workflow generation for large language model agents. arXiv preprint arXiv:2407.12821, 2024. [28] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025. [29] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multiagent debate. arXiv preprint arXiv:2305.19118, 2023. [30] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [31] Yue Liu, Hongcheng Gao, Shengfang Zhai, Jun Xia, Tianyi Wu, Zhiwei Xue, Yulin Chen, Kenji Kawaguchi, Jiaheng Zhang, and Bryan Hooi. Guardreasoner: Towards reasoning-based llm safeguards. arXiv preprint arXiv:2501.18492, 2025. 11 [32] Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi Huang, and Bryan Hooi. Efficient inference for large reasoning models: survey. arXiv preprint arXiv:2503.23077, 2025. [33] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. arXiv preprint arXiv:2310.02170, 2023. [34] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [35] OpenAI. Introducing chatgpt. https://openai.com/index/chatgpt/, 2022. [36] OpenAI. Learning to reason with llms. https://openai.com/index/learning-to-reason-with-llms/, 2024. [37] OpenAI. Introducing deep research. https://openai.com/index/introducing-deep-research/, 2025. [38] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. [39] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [40] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [41] Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha, Kelly Buchanan, Mayee Chen, Neel Guha, Christopher Ré, et al. Archon: An architecture search framework for inference-time techniques. arXiv preprint arXiv:2409.15254, 2024. [42] Yu Shang, Yu Li, Keyu Zhao, Likai Ma, Jiahe Liu, Fengli Xu, and Yong Li. Agentsquare: Automatic llm agent search in modular design space. arXiv preprint arXiv:2410.06153, 2024. [43] Anthropic Team. https://wwwcdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, 2024. The claude 3 model family: Opus, sonnet, haiku. [44] Deepseek Team. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [45] Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint 2501.12599v1, 2025. [46] Qwen Team. Qvq: To see the world with wisdom. https://qwenlm.github.io/blog/qvq-72bpreview/, 2024. [47] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. https://qwenlm.github.io/blog/qwq-32b-preview/, 2024. [48] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023. [49] Yinjie Wang, Ling Yang, Guohao Li, Mengdi Wang, and Bryon Aragam. Scoreflow: Mastering llm agent workflows via score-based preference optimization. arXiv preprint arXiv:2502.04306, 2025. 12 [50] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [51] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. arXiv preprint arXiv:2308.08155, 2023. [52] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [53] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023. [54] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, and Deqing Yang. Evoagent: Towards automatic multi-agent generation via evolutionary algorithms. arXiv preprint arXiv:2406.14228, 2024. [55] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic\" differentiation\" via text. arXiv preprint arXiv:2406.07496, 2024. [56] Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, and Dawei Cheng. G-designer: Architecting multi-agent communication topologies via graph neural networks. arXiv preprint arXiv:2410.11782, 2024. [57] Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, and Xiang Wang. Multi-agent architecture search via agentic supernet. arXiv preprint arXiv:2502.04180, 2025. [58] Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. arXiv preprint arXiv:2410.10762, 2024. [59] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. Offline training of language model agents with functions as learnable weights. In Forty-first International Conference on Machine Learning, 2024. [60] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. [61] Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc Le, Ed Chi, Denny Zhou, Swaroop Mishra, and Huaixiu Steven Zheng. Self-discover: Large language models self-compose reasoning structures. Advances in Neural Information Processing Systems, 37: 126032126058, 2024. [62] Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532, 2024. [63] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan Ashley, Róbert Csordás, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. Mindstorms in natural language-based societies of mind. arXiv preprint arXiv:2305.17066, 2023. [64] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber. Gptswarm: Language agents as optimizable graphs. In Forty-first International Conference on Machine Learning, 2024. [65] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024."
        },
        {
            "title": "A More Implementation Details",
            "content": "In the workflow optimization process of our method, we evaluate each workflow for 3 times as external execution feedback. We use the following 6 operators based on Aflow [58] as our base operators: Code Generator: generates code solutions for given problem. Format Generator: produces formatted answers for given problem. Ensemble Operator: combines multiple solutions or approaches to create more robust final result. Review Operator: evaluates solutions for correctness, efficiency, and adherence to requirements. Revise Operator: refines solutions based on feedback from the review process. Code Test Operator: executes and validates code solutions against test cases to ensure functionality. For our supervised fine-tuning (SFT), we utilized approximately 1,400 items sourced from three datasets generated by R1 through our optimization process. We conducted the training using LLaMAFactory [60], with per-device training batch size of 1, gradient accumulation over 2 steps, learning rate of 1e-5, and max train epochs of 3. For the reinforcement learning (RL) phase, we set the scaling factor to 1.1, the threshold to 3, rollout number to 5, and max episodes to 5."
        },
        {
            "title": "B More Cases of Workflow",
            "content": "In this section, we present additional cases of both successful and failed results generated by FLOWREASONER. B.1 Successful Cases Fig. 6, Fig. 8 and Fig. 7 demonstrate three successful cases generated by FLOWREASONER for BigCodeBench, HumanEval and MBPP. B.2 Failure Cases Fig. 10, Fig. 11 and Fig. 9 demonstrate three failure cases generated by FLOWREASONER for BigCodeBench, HumanEval and MBPP. input = problem , instruction = prompt_custom . P _ L _ M # Extract key problem requirements and constraints problem_analysis = await self . custom ( 1 async def __call__ ( self , problem : str , test_cases : list ) : 2 3 4 5 6 7 8 9 10 11 12 # Generate initial solution with problem analysis context solution = await self . t _ e _ e e ( ) problem = problem , entry_point = entry_point , instruction = \" { prompt_custom . P _ E E _ M } nProblem Analysis : { 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 problem_analysis [ response ]} \" ) current_solution = solution [ response ] # Loop for max_iterations to improve the solution if tests fail for iteration in range (3) : # Test current solution test_results = await self . custom ( input = \" Solution : { current_solution } nTest Cases : { test_cases } \" , instruction = prompt_custom . GRA PH_ TES T_P ROM PT ) # If tests pass , return the solution if \" failed \" not in test_results [ response ]. lower () : return current_solution , self . llm . cost_manager . total_cost # Improve solution with test feedback and problem analysis im pro ved _so lu tio = await self . t _ e _ e e ( problem = problem , entry_point = entry_point , instruction = \" { prompt_custom . P _ R _ M } nProblem Analysis : { problem_analysis [ response ]} nCurrent Solution : { current_solution } nTest Results : { test_results [ response ]} nIteration : { iteration + 1}/{ max_iterations } \" ) # Update current solution current_solution = mpr ove d_ sol uti on [ response ] # Return the last solution attempt return current_solution , self . llm . cost_manager . total_cost Figure 6: Successful workflow generated by FLOWREASONER-14B for the BigCodeBench task of Generating and plot weather data for specified date range. 1 async def __call__ ( self , problem : str , test_cases : list ) : 2 3 # Generate solution solution = await self . t _ e _ e e ( problem = problem , entry_point = entry_point , instruction = prompt_custom . P _ E E _ M ) 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Test solution test_results = await self . custom ( input = \" Solution : { solution [ response ]} nTest Cases : { test_cases } \" , instruction = prompt_custom . GRA PH_ TES T_P ROM PT ) # Optimize solution if tests fail if \" failed \" in test_results [ response ]. lower () : im pro ved _so lu tio = await self . t _ e _ e e ( problem = problem , instruction = \" { prompt_custom . P _ R _ M } nTest Results : { test_results [ response ]} \" ) return mpr ove d_s olu tio [ response ] , self . llm . cost_manager . total_cost return solution [ response ] , self . llm . cost_manager . total_cost Figure 7: Successful workflow generated by FLOWREASONER-14B for the MBPP task of Writing function to reverse words in given string. 15 ] ( await self . t _ e _ e e ( # 1. Prepare generation prompts gen_prompts = [ # 2. Generate three candidate solutions sols = [ prompt_custom . CODE_GENERATE_PROMPT_A , prompt_custom . CODE_GENERATE_PROMPT_B , prompt_custom . CODE_GENERATE_PROMPT_C , 1 async def __call__ ( self , problem : str , test_cases : list ) : 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 4. Return the best solution and total cost return best , self . llm . cost_manager . total_cost input = ensemble_input , instruction = prompt_custom . ENSEMBLE_PROMPT # 3. Ensemble : pick or merge the best solution ensemble_input = \" Solutions : \" + \" - - - \" . join ( sols ) ) ) [ response ] for inst in gen_prompts problem = problem , instruction = inst best = ( await self . custom ( ) ) [ response ] ] Figure 8: Successful workflow generated by FLOWREASONER-14B for the HumanEval task of Returning list of prime factors of given integer in the order from smallest to largest. ) ) # 1. Analyze the problem analysis = await self . custom ( input = problem , instruction = prompt_custom . ANALYZE_PROMPT # 2. Generate initial solution solution = await self . t _ e _ e e ( problem = problem , instruction = prompt_custom . GENERATE_PROMPT , context = analysis [ response ] # 3. Test with subset of test cases sample_tests = test_cases [: min (3 , len ( test_cases ) ) ] test_result = await self . custom ( 1 async def __call__ ( self , problem : str , test_cases : list ) : 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 problem = problem , instruction = prompt_custom . OPTIMIZE_PROMPT , context = \" Solution : { solution [ response ]} \" # 4. Handle based on test results if \" all passed \" in test_result [ response ]. lower () : # Optimize if all tests pass final = await self . t _ e _ e e ( # Fix issues if tests fail fixed = await self . t _ e _ e e ( else : ) ) return final [ response ] , self . llm . cost_manager . total_cost input = \" Solution : { solution [ response ]} nTests : { sample_tests } \" , instruction = prompt_custom . TEST_PROMPT problem = problem , instruction = prompt_custom . FIX_PROMPT , context = \" Solution : { solution [ response ]} nTest results : { test_result [ 37 38 ) return fixed [ response ] , self . llm . cost_manager . total_cost response ]} \" Figure 9: Failed workflow generated by FLOWREASONER-14B for the MBPP task of Writing function to count those characters which have vowels as their neighbors in the given string. 16 async def __call__ ( self , problem : str , test_cases : list ) : # Extract key problem requirements and constraints with detailed specifications problem_analysis = await self . custom ( ) input = problem , instruction = prompt_custom . P _ L _ M 1 async def __call__ ( self , problem : str , test_cases : list ) : 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ) # Generate solution with problem analysis and algorithm selection solution = await self . t _ e _ e e ( problem = problem , entry_point = entry_point , instruction = \" { prompt_custom . P _ E E _ M } nProblem Analysis : { # Initial approach planning with appropriate algorithm selection o h m_ e o = await self . custom ( input = \" Problem : { problem } nAnalysis : { problem_analysis [ response ]} \" , instruction = prompt_custom . P _ O H _ E O _ M problem_analysis [ response ]} nAlgorithm Selection : { o h _ e io [ response ]} \" ) # Self - review for logical errors and optimization opportunities code_review = await self . custom ( input = \" Code : { solution [ response ]} \" , instruction = prompt_custom . P _ E _ I _ M ) # Generate improved solution with comprehensive context im pro ved _so lu tio = await self . t _ e _ e e ( problem = problem , entry_point = entry_point , instruction = \" { prompt_custom . P _ R _ M } nProblem Analysis : { problem_analysis [ response ]} nAlgorithm Selection : { o h _ e io [ response ]} nCode Review : { code_review [ response ]} \" ) # Add explanation to the solution u n _ l t = await self . custom ( input = \" Solution : { imp rov ed_ sol ut ion [ response ]} nProblem : { problem } \" , instruction = prompt_custom . P _ U N _ L T _ M ) final_solution = \" # Solution { mp rov ed_ sol uti on [ response ]} # Explanation { u n _ l t [ response ]} \" return final_solution , self . llm . cost_manager . total_cost 20 21 22 23 24 25 26 27 28 29 30 31 33 34 35 36 37 38 39 40 41 42 Figure 10: Failed workflow generated by FLOWREASONER-14B for the BigCodeBench task of Extracting the text and href attributes of all anchor tags from given URLs HTML content. ) ) [ response ] # 1. Generate solution solution = ( await self . t _ e _ e e ( problem = problem , instruction = prompt_custom . E _ E E _ M 1 async def __call__ ( self , problem : str , test_cases : list ) : 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 2. Test and refine solution refined_solution = ( await self . t _ e _ e e ( # 3. Return the solution and total cost return refined_solution , self . llm . cost_manager . total_cost ) ) [ response ] problem = problem , instruction = prompt_custom . REFINE_PROMPT , context = \" Initial solution : { solution } nTest cases : { test_cases } \" Figure 11: Failed workflow generated by FLOWREASONER-14B for the HumanEval task of Evaluating whether the given number can be written as the sum of exactly 4 positive even numbers."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Sea AI Lab, Singapore",
        "Shanghai Jiao Tong University",
        "University of Chinese Academy of Sciences"
    ]
}