{
    "paper_title": "Clinical ModernBERT: An efficient and long context encoder for biomedical text",
    "authors": [
        "Simon A. Lee",
        "Anthony Wu",
        "Jeffrey N. Chiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on large scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with their textual descriptions. Building on ModernBERT the current state of the art natural language text encoder featuring architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and extended context length up to 8,192 tokens our model adapts these innovations specifically for biomedical and clinical domains. Clinical ModernBERT excels at producing semantically rich representations tailored for long context tasks. We validate this both by analyzing its pretrained weights and through empirical evaluation on a comprehensive suite of clinical NLP benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 4 6 9 3 0 . 4 0 5 2 : r Clinical ModernBERT: An efficient and long context encoder for biomedical text Simon A. Lee Department of Computational Medicine UCLA simonlee711@g.ucla.edu Anthony Wu Department of Computational Medicine UCLA anthonytkwu@g.ucla.edu Jeffrey N. Chiang Department of Computational Medicine & Neurosurgery UCLA njchiang@g.ucla.edu"
        },
        {
            "title": "Abstract",
            "content": "We introduce Clinical ModernBERT, transformer-based encoder pretrained on large-scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC-IV clinical data, and medical codes with their textual descriptions. Building on ModernBERT [Warner et al., 2024]the current state-of-the-art natural language text encoder featuring architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and extended context length up to 8,192 tokensour model adapts these innovations specifically for biomedical and clinical domains. Clinical ModernBERT excels at producing semantically rich representations tailored for long-context tasks. We validate this both by analyzing its pretrained weights and through empirical evaluation on comprehensive suite of clinical NLP benchmarks."
        },
        {
            "title": "Introduction",
            "content": "Since the introduction of BERT (Bidirectional Encoder Representations from Transformers) in 2018, encoder-only transformer architectures have remained foundational to both industry-scale and research-driven natural language processing (NLP) [Devlin et al., 2019]. Although recent advances have centered around large-scale decoder-only models such as GPT [Radford et al., 2018, Achiam et al., 2023], LLaMA [Touvron et al., 2023], and DeepSeek [Guo et al., 2025], prized for their generative capabilities, BERT and its derivatives continue to play central role in real-world applications. Its sustained popularity can be attributed to its versatility and effectiveness across critical tasks, including dense retrieval [Gao et al., 2023], content moderation and classification [Kowsari et al., 2019], and the extraction of sensitive or regulated information in compliance-driven environments [Nadeau and Sekine, 2007]. While decoder-based models dominate applications requiring coherent generation and fluent language synthesis, encoder-only transformers offer unique advantages rooted in their bidirectional attention mechanisms. Unlike causal decoders, BERT-style models allow each token to attend to both preceding and succeeding context, yielding semantically enriched embeddings. This bidirectional encoding has proven especially valuable in scenarios that depend on fine-grained semantic discrimination. Furthermore, architectural advances in recent years have significantly modernized the encoder stack, with innovations in computational efficiency (e.g., Flash Attention [Dao et al., 2022]), extended sequence modeling, and parameter optimization, reaffirming the relevance of the encoder paradigm. Preprint. Under review. Table 1: Architecture differences: Comparison of encoder architectures across domains and design innovations. Clinical ModernBERT inherits ModernBERTs architectural advances while adapting to biomedical and clinical corpora. Feature Domain Adaptation Tokenizer Sequence Length Positional Encoding Attention Mechanism Activation Function Bias Parameters Pre-Training Corpus Application Focus BERT WordPiece 512 Sinusoidal (Learned) Standard GELU Present ModernBERT BPE 8192 Rotary (RoPE) Flash Attention GeGLU Removed BooksCorpus, Wikipedia Natural Language + Code General NLP General NLP + Code Clinical ModernBERT (Ours) BPE (Additional Clinical Tokens) 8192 Rotary (RoPE) Flash Attention GeGLU Removed 40 million PubMed + MIMIC-IV Notes Biomedical, Clinical NLP, Medical Codes Table 2: Result Reporting: Comparison of Clinical ModernBERT against prior works across pretraining supervision, ontology integration, task coverage. Clinical ModernBERT uniquely contributes comprehensive pretraining analysis and structured code ontology integration, along with strong performance across both short and long-context clinical tasks. Model Pretraining MLM Performance Reported Medical Code Ontology Integration Biomedical NLP Tasks Long-Context Tasks (i2b2 Benchmarks) BioClinicalBERT [Alsentzer et al., 2019] BioBERT [Lee et al., 2020] Clinical Longformer [Li et al., 2022] Clinical ModernBERT (Ours) ModernBERT [Warner et al., 2024] exemplifies this next-generation design, achieving notable Pareto improvement over the original BERT across speed, memory footprint, and representational fidelity. The architecture incorporates rotary positional embeddings (RoPE) [Su et al., 2024], GeGLU activation functions [Shazeer, 2020], bias-free linear transformations for parameter efficiency [Dayma et al., 2021, Xie and Lukasiewicz, 2023], and Flash Attention [Dao et al., 2022] for high-throughput inference. Critically, it supports context lengths up to 8,192 tokens, facilitating rich encoding of long-form documents. Unlike prior encoders, ModernBERT also includes source code in its training corpus, extending its utility to tasks such as code search and intelligent development assistance. Building on these architectural improvements, we present Clinical ModernBERT, domainspecialized encoder pretrained on biomedical literature, clinical narratives, and structured medical ontologies. Inspired by the design philosophy of BioClinicalBERT [Alsentzer et al., 2019] but grounded in the methodological advances of ModernBERT, our model is tailored for high-accuracy understanding in long-context biomedical and clinical NLP tasks. These include clinical information retrieval, narrative classification, and domain-specific entity and relation extraction. Clinical ModernBERT reaffirms the relevance of encoder-only architectures in the age of large generative models, offering performant, efficient, and scalable foundation for language understanding in high-stakes medical settings. Contributions: We introduce Clinical ModernBERT, domain-specialized transformer encoder that integrates the architectural advancements of ModernBERT with domain-adaptive pretraining over biomedical literature, clinical notes, and structured medical ontologies. Through targeted pretraining on 13 billion tokens, Clinical ModernBERT captures both granular medical terminology and the global discourse structure of clinical documentation. token-aware masking strategy further emphasizes semantic learning over high-value biomedical spans. summary of model features is provided in Table 1 with summary of results provided in Table 2. Empirically, we demonstrate that Clinical ModernBERT is competitive or outperforms domain baselinesincluding BioBERT, BioClinicalBERT, and Clinical Longformeracross suite of downstream clinical NLP benchmarks, including semantic retrieval, classification, and entity recognition. It also achieves state-of-the-art performance on long-context tasks such as i2b2 concept extraction. Latent space visualizations confirm improved alignment with clinical ontologies, showcasing its capacity to internalize medical taxonomies. By making Clinical ModernBERT and its tokenizer publicly available, we provide scalable and high-fidelity encoder backbone for clinical NLP and biomedical research applications."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Biomedical and Clinical Adaptations of BERT The introduction of BERT in 2018 revolutionized NLP by providing powerful general-purpose bidirectional encoder [Devlin et al., 2019]. This milestone led to wave of domain-adapted models, particularly in the biomedical and clinical NLP communities. Among the earliest was BioBERT, which continued BERTs pretraining on large-scale biomedical corpora, including PubMed abstracts and PMC articles [Lee et al., 2020]. BioBERT significantly outperformed vanilla BERT on core biomedical tasks such as named entity recognition (NER), relation extraction, and question answering. Another notable variant is SciBERT, trained on large corpus of scientific publications spanning both computer science and biomedical domains [Beltagy et al., 2019]. Unlike BioBERT, SciBERT introduced new domain-specific vocabulary, further boosting performance on scientific NLP benchmarks. These models demonstrated that pretraining on domain-specific corpora yields stronger performance across wide range of biomedical tasks, particularly when encoding specialized terminology and complex discourse structures. In parallel to biomedical literature-focused models, there has been considerable interest in adapting BERT to clinical narrative data. ClinicalBERT was one of the first models explicitly trained on clinical notes from the MIMIC-III dataset [Huang et al., 2019]. It captured the unique language patterns found in real-world clinical documentation, such as discharge summaries. An effective strategy that gained traction was sequential domain adaptationstarting with biomedical model and continuing pretraining on clinical notes. BioClinicalBERT exemplifies this approach, extending BioBERT by further pretraining on MIMIC-III, thereby bridging the linguistic gap between formal biomedical writing and informal, abbreviation-heavy clinical narratives [Alsentzer et al., 2019]. Similarly, BlueBERT combined PubMed and MIMIC-III corpora in its pretraining pipeline and was benchmarked on the BLUE evaluation suite [Peng et al., 2019], highlighting the benefits of cross-domain fusion in biomedical understanding. 2.2 Ontology-Enriched and Scaling Context Length BERT Variants Another line of work incorporates structured medical knowledge into the pretraining objective. SapBERT leverages synonym mappings from the Unified Medical Language System (UMLS) to optimize entity embeddings via contrastive learning [Liu et al., 2021]. This enables both better within-language medical entity representations and improved cross-lingual alignment, providing value for multilingual biomedical NLP systems. Similarly DK-BEHRT [An et al., 2025] found that including medical codes and their descriptions and introducing them during pre-training resulted in learning robust latent space of ICD-9 disease codes. Although BERT and its biomedical derivatives are limited to 512 tokens, clinical documents often exceed this threshold. To address this, Clinical Longformer was introduced with support for sequences up to 4,096 tokens [Li et al., 2022] extending the work of the longformer [Beltagy et al., 2020], and bigbird [Zaheer et al., 2020]. By incorporating sparse attention mechanisms [Tay et al., 2020], it enabled long-context processing across patient narratives, longitudinal EHR entries, and radiology reports. Recent work has emphasized rethinking the BERT architecture itself to enhance scalability and pretraining throughput, as exemplified by MosaicBERT [Portes et al., 2023]. Soon after, ModernBERT was introduced, as refreshed encoder-only design that incorporates rotary positional embeddings (RoPE), GeGLU activations, Flash Attention, and support for extended context lengths of up to 8,192 tokens [Warner et al., 2024]. Additional architectural and training details are provided in the Methods section. Building on this emerging paradigm of modernized BERT encoders, we integrate effective strategies from prior models to develop long-context, compute-efficient transformer that serves as drop-in replacement for BioClinicalBERT [Alsentzer et al., 2019], which has long been the de facto standard in biomedical NLP. Our model overcomes BioClinicalBERTs 512-token limitation, and is far more efficient enabling robust encoding of longer clinical and biomedical documents at scale."
        },
        {
            "title": "3 Methods",
            "content": "3.1 Summarizing ModernBERT ModernBERT diverges from the original BERT paradigm through several innovative architectural and methodological additions. These enhancements serve as the backbone for Clinical ModernBERT and ensure improved performance and scalability. The Table 1 summarizes the key distinctions between BERT, ModernBERT, and our proposed Clinical ModernBERT. Rotary Positional Embeddings (RoPE) Traditional BERT embeddings incorporate sinusoidal positional information, where position vectors E() are added to token embeddings. In contrast, ModernBERT uses RoPE [Su et al., 2024], which applies rotation to pairs of embedding dimensions. Specifically, the self-attention computation is modified by rotation matrices Rθ: Attention(Q, K, ) = softmax (cid:18) (Q Rθ) (K Rθ)T (cid:19) V. dk This formulation embeds relative positional information directly into the inner product of queries and keys, preserving the order-sensitive structure of input sequences without explicit position embeddings. Unlike absolute or additive positional embeddingswhich often struggle to extrapolate beyond their training context lengthRoPE enables the attention mechanism to capture token relationships in position-invariant and extrapolatable way. Mathematically, RoPE encodes position via complex rotation: each dimension pair (x2i, x2i+1) is rotated by phase proportional to position p, such that the inner product Q, reflects relative position differences. This encoding preserves inner product symmetries and relative distances, which becomes crucial when dealing with long sequences where the absolute position values may lie far outside the training regime. Empirically, RoPE has been shown to allow better generalization to longer contexts and reduced degradation in attention signal at distant token pairs. GeGLU Activation Layers Whereas standard BERT uses ReLU-based feedforward transformations, FFN(x) = max(0, xW1 + b1) W2 + b2, ModernBERT uses GeGLU [Shazeer, 2020]: GeGLU(x, W, V, b, c) = (cid:0)GeLU(xW + b)(cid:1) (xV + c), where denotes element-wise multiplication. This gating mechanism improves representational capacity and model stability, allowing for richer feature encodings across pre-training corpora. Compared to ReLU, GeGLU introduces learned multiplicative interaction between nonlinear and linear projections, enabling finer control over information flow through the network. Additionally, the smooth curvature of GeLU avoids the harsh saturation behavior of ReLU, mitigating gradient sparsity and improving convergence dynamics during optimization. Bias Removal and Parameter Efficiency Bias terms are removed throughout the ModernBERT architecture, inspired by findings in Dayma et al. [2021], which advocate for architectural minimalism by eliminating parameters that contribute marginally to performance. In standard transformer layers, bias terms appear in every linear transformation (e.g., + b), but empirical studies have shown that these terms often have negligible impact when layer normalization is appliedespecially in large-scale pretraining regimes. By removing these bias terms, the model reduces the total number of trainable parameters without affecting expressive capacity. More importantly, this simplification leads to tighter optimization landscape with fewer degrees of freedom, thereby improving gradient signal consistency and enabling faster convergence. From an efficiency standpoint, bias removal yields small but compounding reductions in memory footprint and compute cost, which become non-trivial when scaled across billions of tokens. Flash Attention One of the chief computational bottlenecks in self-attention is its quadratic memory and compute cost with respect to input sequence length, due to the dense attention matrix formed between all token pairs. ModernBERT mitigates this via Flash Attention [Dao et al., 2022], which rewrites the attention computation to be both memory-efficient and hardware-aware. Rather than materializing the full attention matrix in memory, Flash Attention computes attention scores and their softmax-normalized outputs blockwise using tiling scheme that fits into GPU on-chip SRAM (shared memory). Specifically, attention is computed as: Attentionblockwise(Q, K, ) (cid:77) softmax (cid:16) QiK dk (cid:17) Vi, where Qi, Ki, and Vi are local blocks of the query, key, and value matrices, and denotes concatenation across segments. This approach leverages two key insights. First, attention can be computed in numerically stable, streaming fashion by fusing softmax and matrix multiplication into single pass, eliminating the need to store intermediate attention weights. Second, by structuring computation to align with memory hierarchies (e.g., keeping blocks in registers and shared memory), Flash Attention maximizes throughput while reducing reliance on high-latency global memory. The result is near-linear scaling in memory usage with respect to sequence length, without compromising exactness of the attention result. Extended Sequence Length and Diverse Data As result of the optimizations described, ModernBERT extends the standard BERT context length from 512 to 8,192 tokens, drastically expanding its ability to parse and understand longer contexts which have proven to be beneficial in clinical applications [Wornow et al., 2024]. These features allow the model to have advantages over previous iterations of encoder based models with longformer [Beltagy et al., 2020] having the next largest context length 50% shorter than modernBERT. 3.2 Pretraining Data Sources To facilitate robust domain adaptation of our Clinical ModernBERT model, we curated composite corpus spanning unstructured biomedical literature, clinical free-text, and structured medical terminologies. This multi-source dataset is designed to maximize coverage across both academic and operational aspects of biomedical language. This pretraining setup combines the data sources proposed in Alsentzer et al. [2019] and An et al. [2025], integrating both clinical narratives and structured medical ontologies to enhance domain-specific language modeling. PubMed Abstracts We leveraged approximately 40 million PubMed abstracts, encompassing biomedical publications through 2025 [Lu, 2011]. These abstracts reflect the linguistic and conceptual heterogeneity of peer-reviewed biomedical literature, providing high-coverage substrate for encoding domain-relevant semantics. The inclusion of this corpus enables the model to internalize terminology, syntax, and semantic associations prevalent in formal scientific writing across biomedical subjects. Models like sci-bert have found that including scientific literature helps learn domain-specific concepts very well [Beltagy et al., 2019]. MIMIC-IV Clinical Notes We incorporated deidentified clinical notes from the MIMIC-IV dataset [Johnson et al., 2023], drawn from real-world inpatient encounters at the Beth Israel Deaconess Medical Center. Our pretraining corpus includes discharge summaries and radiology reports. Discharge summaries encode high-density longitudinal narratives covering diagnostic reasoning, therapeutic trajectories, and patient-specific contextualization. Radiology reports introduce specialized modality of medical interpretation characterized by compressed syntax, diagnostic speculation, and anatomical specificity. Structured Medical Ontologies To complement the free-text sources, we integrated comprehensive collection of standardized medical codes and descriptions, encompassing multiple revisions of the ICD taxonomy (ICD-9 through ICD-12), medication codes, and procedural terminologies (e.g. CPT). This is in light of both realizing and learning that coded language can be learned by providing natural language descriptions in conjunction with the code [An et al., 2025, Lee et al., 2024c]. This process is demonstrated in Figure 1. Each code-description pair is treated as linguistic signal representing 5 Figure 1: Medical Code Ontologies Construction: An illustration of structured ontology construction across multiple ICD code versions. Each row represents distinct medical concept identified by its version-specific code and description, which is then converted into standardized, descriptive natural language representation. This process facilitates alignment and interoperability across evolving coding schemes. This setup is inspired by methods like [Hegselmann et al., 2023, Ono and Lee, 2024] which use text templates to serialize tabular data. formalized medical knowledge. This structured input scaffolds the models understanding of discrete clinical concepts, reinforces terminology normalization, and enhances interoperability with evolving coding systems. These ontological alignments are essential for downstream tasks involving entity normalization, coding prediction, and concept linking [Lee and Lindsey, 2024, Soroush et al., 2024]. 3.3 Pre-training Optimization Clinical ModernBERT was pre-trained using masked language modeling (MLM) framework designed to enhance its capacity to encode biomedical and clinical language. Initialization from the ModernBERT-base checkpoint conferred architectural advantages, including Flash Attention, rotary positional embeddings (RoPE), and GeGLU activation layers. Building on the approach of Alsentzer et al. [2019], the goal was to enrich the model with specialized domain knowledge while preserving strong contextual reasoning capabilities. The training corpus was constructed by aggregating (i) discharge summaries and radiology reports from MIMIC-IV [Johnson et al., 2023], (ii) approximately 40 million PubMed abstracts published through 2025, and (iii) structured medical ontologies such as ICD and CPT codes. After normalization and preprocessing, the final dataset encompassed 13 billion tokens. To prioritize semantically dense content, samples with coherent discoursesuch as detailed clinical narrativeswere upsampled, providing richer training signal. This composition enabled the model to span broad spectrum of clinical language, from fine-grained medical terminology to higher-level documentation across radiology and discharge summaries. Tokenization used byte-pair encoding (BPE) scheme initialized from ModernBERT-base. To reinforce learning of clinically relevant semantics, custom data collator was introduced that applied token-aware masking to biomedical entities during training. This collator implemented dynamic corruption schedule, linearly decaying the masking rate from 30% to 15% over training. Early phases emphasized challenging and informative termssuch as medication names, procedure codes, and morphological descriptorswhile later stages aimed to stabilize representation learning. This targeted perturbation strategy encouraged the model to develop nuanced contextual embeddings grounded in clinical discourse. Training was conducted for 150,000 steps. While initial plans considered step counts derived from dataset size and average sample length, empirical evaluation showed MLM performance plateauing beyond this point. Checkpoints were saved every 10,000 steps to capture model evolution prior to any degradation. Consistent with observations from Alsentzer et al. [2019], we found that convergence 6 Table 3: Dataset Statistics: Statistics for downstream NLP tasks spanning both short and long context settings. Short context tasks typically fall within the standard input limits of models like BERT (512 tokens), whereas long context tasks significantly exceed this threshold, necessitating architectures capable of extended sequence modeling. Short Context Tasks Dataset Task Data Source Sample Size Avg. Seq. Length Max Seq. Length EHR-Prediction Classification MedNER Pubmed-NCT PMC-Retrieval NER Multiclass Classif. Retrieval MIMIC-IV ED Custom PubMed PMC 400,019 3,655 221,186 167,034 278.6 17.7 26.2 37. 2,684.0 125.0 260.0 2,728.0 Long Context Tasks Dataset Task Data Source Sample Size Avg. Seq. Length Max Seq. Length i2b2 2006 NER i2b2 i2b2 2010 NER i2b2 i2b2 2012 NER i2b2 i2b2 2014 NER i2b 66,034 43,947 13,108 83,466 867.0 1,459.3 793.6 5,133.5 3,986.0 6,052.0 2,900.0 14,370.0 occurred relatively early in training (Appendix Figure 4).Druing pre-training we measured top-1, 5, 10, and 25 MLM accuracies as well as tracked loss to find the optimal model checkpoint. Training Procedure The pre-training schedule began with sequence length of 128 tokens, leveraging large batch sizes and elevated learning rates to efficiently model short-range dependencies. StableAdamW [Wortsman et al., 2023] was used for gradient clipping and stabilization, proving effective for large-batch optimization on GPU clusters. cosine learning rate schedule guided dynamic adjustment of learning rates, while mixed-precision training improved memory utilization and throughput. Checkpoints were saved regularly to enable inspection and reuse of intermediate representations. Training was distributed across NVIDIA A100 GPUs using multi-node orchestration. The final model weights and tokenizer artifacts were saved for reproducibility and future adaptation on huggingface [Wolf et al., 2020, Simon Lee, 2025] 1. Clinical ModernBERT is publicly released to support the broader community in downstream fine-tuning, domain-specific pretraining, and exploration of new clinical NLP benchmarks."
        },
        {
            "title": "4 Experimental Setup",
            "content": "4.1 Assessing Pre-training Masked Language Modeling (MLM) is self-supervised objective in which random tokens within an input sequence are replaced with special [MASK] token, and the model is trained to reconstruct the original tokens based solely on the surrounding context. Given sequence = (x1, . . . , xn), subset of positions {1, . . . , n} is selected for masking. The model receives x, where xi = [MASK] for M, and is trained to minimize the cross-entropy loss: LMLM = (cid:88) iM log pθ(xi x), where pθ is the models predicted distribution over the vocabulary. This objective encourages the model to capture syntactic and semantic dependencies in the data. To quantify performance, we compute top-k accuracy over masked positions, where prediction is considered correct if the ground truth token appears in the top-k most probable tokens predicted by the model. Specifically, for each masked token xi, we sort the predicted distribution and check whether xi lies in the top logits. These metrics provide granular view of the models ability 1https://huggingface.co/Simonlee711/Clinical_ModernBERT 7 to recover meaningful clinical vocabulary under varying levels of strictness, with top-1 reflecting precision and top-25 capturing broader lexical recall. 4.2 Benchmarking Models We compare Clinical ModernBERT against range of baseline and state-of-the-art models to ensure broad coverage across both context regimes. This includes BERT-base [Devlin et al., 2019] as general-purpose baseline, BioBERT [Lee et al., 2020] and BioClinicalBERT [Alsentzer et al., 2019] as domain-adapted encoders, and Clinical Longformer [Li et al., 2022] and ModernBERT [Warner et al., 2024], which extend BERT to support longer contexts. These baselines collectively allow us to assess the benefits of domain-specific adaptation, architectural modernization, and sequence length scaling in clinical NLP. 4.3 Benchmarking Datasets To comprehensively evaluate Clinical ModernBERT, we design experiments spanning both standard biomedical NLP benchmarks and long-context clinical tasks. The former comprise widely-used short-context datasets for clinical note classification, biomedical named entity recognition (NER), and scientific trial labeling, where input sequences remain well within the 512-token constraint. These benchmarksoriginally introduced in prior studies such as BioBERT [Lee et al., 2020] and BioClinicalBERT [Alsentzer et al., 2019]primarily assess local representational fidelity, focusing on tasks where short-range dependencies dominate. We additionally include sentence-level biomedical retrieval benchmark [Zhao et al., 2023], which serves as further probe of the models embedding quality. To complement this, we evaluate on long-context benchmarks designed to test Clinical ModernBERTs ability to reason over extended sequences, core architectural advantage of the model. These tasks include full-document NER and long-range semantic retrieval, sourced from the i2b2 shared task suite and other publicly available clinical corpora, following prior protocols from Li et al. [2022] and Zhao et al. [2023]. This dual benchmark strategy allows us to disentangle the contributions of local versus global context modeling, and to isolate the gains attributable to architectural extensions such as Flash Attention and rotary position embeddings. Summary statistics for each dataset, including sample size and token length distributions, are provided in Table 3. Detailed dataset descriptions appear in Appendix A. 4.4 Medical Codes Visualization Protocol To probe the representational impact of incorporating structured medical codes during pretraining, we constructed an embedding visualization pipeline centered on ICD-9 diagnosis codes. Since both the codes and their textual descriptions were included in the pretraining corpus, this analysis serves as targeted lens into how effectively Clinical ModernBERT captures the semantics of coded clinical language. To enable qualitative analysis of the resulting latent spaces, we projected the high-dimensional embeddings into two dimensions using t-SNE. This visualization allows us to assess whether models pretrained on structured ontological data exhibit stronger alignment with the clinical taxonomyreflected, for instance, in the degree to which embeddings cluster by ICD chapter or semantic relatedness. By comparing Clinical ModernBERT against general-domain baseline (ModernBERT), this setup isolates the contribution of ontology-aware pretraining to the organization and separability of medical code representations. 4.5 Model Efficiency Benchmarking Protocol To assess runtime efficiency under increasing computational load, we measured the forward pass latency of Distil-BERT, BioClinicalBERT, and Clinical ModernBERT across input batches of varying sizes. Synthetic clinical text inputs were generated and standardized to fixed sequence length of 512 tokens to eliminate confounding variability due to input length. Each model was executed in inference mode using PyTorch with automatic mixed precision (AMP) enabled. Benchmarks were conducted on single NVIDIA A100 GPU, and total wall-clock time was measured from input tokenization through to the final hidden state output, excluding any disk or network I/O. Each experiment was repeated three times to ensure stability, and the reported timing reflects the mean runtime across runs. 8 This benchmarking framework was designed to isolate transformer-level inference cost, enabling direct comparison of architectural efficiency between the baseline and our proposed model."
        },
        {
            "title": "5 Results",
            "content": "5.1 Effectiveness of Pre-training Table 4: MLM Top-k accuracies: Masked language modeling (MLM) top-k accuracies after pretraining on clinical and biomedical corpora."
        },
        {
            "title": "Metric",
            "content": "Top-1 Accuracy Top-5 Accuracy Top-10 Accuracy Top-25 Accuracy Value (%) 63.31 79.67 83.33 88. Qualitative Analysis. The top-k accuracy metrics reflected in Table 4 indicate robust convergence and high recall over clinical token prediction, especially at larger k. The top-1 accuracy of 63.31% demonstrates strong discriminative capacity, even under high entropy masking. Moreover, the sharp increases in top-5 through top-25 accuraciesapproaching 88.10%suggest that the model consistently ranks clinically appropriate tokens among its top candidates. This behavior is indicative of successful semantic alignment with domain-specific medical terminology. Full Wandb metrics can be seen in the Appendix Section B. The concurrent decline in MLM loss over training steps and the plateauing of accuracy metrics further support the conclusion that the model learns stable, high-fidelity representations of biomedical and clinical language (Figure 4). Table 5: MLM Top-k accuracies: Masked language modeling (MLM) top-k accuracies after pretraining on clinical and biomedical corpora under various architectural and training ablations. Configuration Top-1 Accuracy Top-5 Accuracy Top-10 Accuracy Top-25 Accuracy Baseline (Ours) w/o token-aware masking w/ 15% Masking (over 30%) 63.31 48.84 58. 79.67 53.01 73.87 83.33 56.10 76.90 88.10 58.79 80.57 Ablation We conduct series of ablations to isolate the impact of masking strategies on the effectiveness of Clinical ModernBERTs pretraining. Removing token-aware maskinga strategy that prioritizes clinically salient tokens for maskingresults in steep decline across all top-k accuracy metrics. The top-1 accuracy drops from 63.31% to 48.84%, and even top-25 accuracy falls below 59%, indicating that naïvely masking uniformly degrades the models ability to predict meaningful biomedical content. This highlights the importance of directing the models learning signal toward domain-relevant vocabulary. Separately, we reduce the masking ratio from 30% to 15%, mirroring the original BERT setup. While this results in only moderate decline, it still substantially underperforms the baseline, with top-1 accuracy falling to 58.22%. This suggests that higher masking rates, when coupled with token-aware selection, enhance the density of the supervision signal and encourage the model to learn more informative contextual representations. Taken together, these results affirm that both what is masked and how much is masked matter critically in domain-specific pretraining regimes. 5.2 Benchmark Across Multiple Standard Biomedical NLP Tasks Across all tasks, models pretrained on biomedical or clinical corpora consistently outperform generaldomain baselines. On EHR classification, clinical modernbert achieves the highest AUROC (0.9769), outperforming both biobert and bioclinicalbert, and demonstrating strong generalization to structured EHR narratives. On the PubMed-NCT classification task, biobert yields the best F1 score (0.9187), though clinical modernbert surpasses it in accuracy, suggesting improved calibration. For MedNER, biobert again leads in F1 (0.794), while clinical modernbert remains competitive (0.766), outperforming all other models except biobert. 9 Table 6: Performance Across Short Context Clinical and Biomedical NLP Benchmarks. Benchmark results for EHR Classification, PubMed-NCT, and MedNER. We report AUROC for EHR Classification and Accuracy and F1 Score for PubMed-NCT and MedNER following metrics used in prior studies."
        },
        {
            "title": "Model",
            "content": "bert-base-uncased biobert bioclinicalbert clinical longformer modernbert Clinical modernbert EHR Classification AUROC PubMed-NCT Acc F1 MedNER F"
        },
        {
            "title": "Acc",
            "content": "0.9503 0.9680 0.9678 0.9640 0.9677 0.9769 0.8754 0.9179 0.9145 0.8950 0.9104 0.9209 0.8706 0.9187 0.8285 0.8250 0.7602 0.8654 0.842 0.909 0.849 0.820 0.695 0.829 0.691 0.794 0.710 0.720 0.517 0.766 Table 7: Retrieval Performance on PMC-Patients Benchmark. PMC-Patients Retrieval performance. We report NDCG@10, Precision@10, Recall@10, and Mean Average Precision (MAP) over 128 patient queries. Model NDCG@10 Precision@10 Recall@10 MAP bert-base-uncased biobert bioclinicalbert clinical longformer modernbert Clinical modernbert 0.0600 0.1956 0.1512 0.1700 0.1865 0.2167 0.0079 0.0489 0.0291 0.0420 0.0463 0. 0.0604 0.1866 0.1466 0.2100 0.2256 0.2791 0.0589 0.1827 0.1441 0.1600 0.1839 0.1982 In the retrieval setting (PMC Patients), clinical modernbert achieves the best performance across all metrics, including NDCG@10 (0.2167) and MAP (0.1982), indicating that clinically informed pretraining confers advantages for semantic matching. Overall, clinical modernbert exhibits strong generalization across diverse biomedical NLP tasks, spanning classification, named entity recognition, and retrieval. 5.3 Long Context Biomedical NLP Tasks On long-context clinical NER benchmarks, performance generally tracks the models ability to process extended sequences while preserving token-level precision. Clinical longformer achieves the highest F1 on i2b2 2006 (0.973), 2010 (0.886), and 2014 (0.960), demonstrating that extending the context length optimized for long sequences yield measurable gains when context exceeds standard transformer limits. Clinical modernbert, however, shows consistently competitive performance across all datasets, achieving the best result on i2b2 2012 (0.804) and the second-best on all other benchmarks. In contrast, biobert and bioclinicalbert perform strongly on shorter variants of the i2b2 tasks (e.g., 2006 and 2010), but show diminishing returns as document length and entity density increase. Modernbert outperforms these biomedical baselines on all four datasets, suggesting that long-range architectural modifications, prove better than domain tuning outperfomring modes like biobert and bioclinial bert on these long-context baselines. Overall, these results indicate that both domainspecific pretraining and architectural adaptation are necessary for robust performance on long-context clinical NER tasks. 5.4 Latent Space Visualizations for medical codes To qualitatively assess the clinical semantic structure captured by our proposed model, we visualize t-SNE projections of diagnosis code representations derived from ModernBERT (left) and our Clinical modernBERT (right) (Figure 2). Each point represents an ICD code embedding, color-coded by its corresponding high-level ICD category. Embeddings are extracted by feeding tokenized ICD 10 Table 8: Performance on i2b2 Long Context Benchmarks. F1 scores for different models across i2b2 benchmark datasets. Bold indicates the best performance per column, underline indicates the second-best. Model (all F1) i2b2 (2006) i2b2 (2010) i2b2 (2012) i2b2 (2014) bert-base-uncased biobert bioclinicalbert clinical longformer modernbert Clinical modernbert 0.938 0.947 0.950 0.973 0.957 0.965 0.834 0.866 0.860 0.886 0.875 0. 0.758 0.791 0.772 0.799 0.782 0.804 0.927 0.929 0.928 0.960 0.948 0.966 Figure 2: ICD-9 tSNE Latent Space Visualization: tSNE visualization of the ICD 9 Diagnoses codes using modernBERT versus Clinical ModernBERT. This visualization provides the added use of adding the medical code ontologies as pre-training source to encode coded language seen frequently in clinical practice. descriptions through each model and taking the classification token ([CLS]) embeddings. We then project these high-dimensional embeddings into 2D using t-SNE, preserving local similarity structure. The comparison reveals stark differences. ModernBERT, pretrained primarily on general domain corpora, fails to form well-defined clusters among diagnostic categories, particularly for ontologically proximate conditions such as respiratory and circulatory disorders. In contrast, Clinical modernBERTaugmented with structured medical code ontologies during pretrainingproduces significantly cleaner separations and tighter intra-category clustering by their ICD chapters. Notably, disease categories such as neoplasms, nervous system disorders, and congenital anomalies are distinctly separated, reflecting improved semantic alignment with clinical taxonomies. 5.5 Efficiency and Scalability of Encoding Embeddings Figure 3 presents the comparative processing times of Distil-BERT, BioClinicalBERT, and Clinical ModernBERT across increasing data volumes. Clinical ModernBERT demonstrates clear computational advantage, maintaining the lowest runtime footprint throughout, even at scales of 100,000 data points. In contrast, BioClinicalBERT exhibits the steepest growth in processing time, with final runtime nearly 60% higher than Clinical ModernBERT. While Distil-BERT, by design, remains lightweight, it still trails Clinical ModernBERT in efficiency, particularly at higher loads. This suggests that architectural optimizations in Clinical ModernBERTsuch as the integration of Flash Attention and linear-time positional encodingconfer measurable speedups without sacrificing model capacity. These results underscore Clinical ModernBERTs suitability for scalable deployment in clinical pipelines where latency and throughput are critical and are consistent with previous findings in the literature [Yamagishi et al., 2025]. 11 Figure 3: Comparative Performance Analysis of BERT Models: This figure demonstrates the processing time requirements across three BERT variants (Distil-BERT, BioClinicalBERT, and Clinical ModernBERT) as data volume increases from 10,000 to 100,000 points. BioClinicalBERT consistently shows the highest computational demand, requiring approximately 1.4x the processing time of Distil-BERT and 1.6x that of Clinical ModernBERT at maximum load. Clinical ModernBERT demonstrates superior efficiency, maintaining the lowest processing times across all data volumes, making it optimal for resource-constrained environments."
        },
        {
            "title": "6 Discussion",
            "content": "6.1 Domain Adaptation and Representational Quality The empirical findings across masked language modeling, downstream benchmarks, and latent space analyses offer converging evidence that Clinical ModernBERT achieves superior domain adaptation in the clinical and biomedical space. The elevated top-1 MLM accuracy of 63.31%, and especially the marked increase in top-5 to top-25 accuracies, highlights the models capacity not only to disambiguate masked tokens precisely but also to consistently narrow in on semantically proximate candidates. This predictive precision suggests strong inductive bias for clinical syntax and terminology, likely result of both the corpus composition and our token-aware masking regime. Clinical ModernBERTs strong showing in EHR classification and PMC retrieval further validates this pretraining protocol. The model attains state-of-the-art AUROC in EHR classification and consistently outperforms baselines on all retrieval metrics. This dualityhigh discriminative capacity in structured tasks and nuanced semantic alignment in open-ended retrievalindicates that the learned representations are both fine-grained and semantically rich. We also saw considerable performance gains on long-context tasks on the i2b2 datasets (2006, 2010, 2012, and 2014) where clinical longformer and clinical modernBERT had switched off between best and second-best models in that benchmark. 6.2 Ablation Findings and Pretraining Design The ablation studies underscore the critical role of pretraining design decisions. Removing tokenaware masking led to catastrophic drop in performance across all top-k thresholds, indicating that uniformly masking tokens fails to guide the model towards domain-salient lexical patterns. Similarly, reverting to 15% masking ratio degrades model efficacy, reinforcing the intuition that denser supervisionwhen targeted effectivelyfacilitates richer context modeling. These findings contribute to growing body of evidence suggesting that in specialized domains, indiscriminate adoption of generic pretraining heuristics can be suboptimal. Instead, task and domain-specific curriculum construction plays determinative role in representational quality. 6.3 Latent Structure and Qualitative Insights The latent space visualizations provide compelling qualitative evidence that our pretraining approach captures clinically meaningful structure. The improved clustering by ICD category in Clinical ModernBERT suggests that the model internalizes not just surface-level co-occurrence statistics but also latent ontological relationships. This aligns with theoretical intuitions from distributional semantics, where models trained on structured knowledge tend to reflect the graph topology of their source corpora in their embedding geometries. On the efficiency side, we also saw that modernBERT has the ability to generate dense embeddings at scalable efficiency outperforming BioClinical BERT as well as distil-BERT which is smaller and faster version of the original BERT architecture reinforcing the modifciations made by Portes et al. [2023]. 6.4 Future Directions Looking ahead, we envision several promising directions. One avenue is to investigate scaling laws within the clinical pretraining regimeexamining whether the predictable log-linear improvement observed in general language models extends to specialized corpora or if new inflection points emerge. We also foresee opportunities to identify the limitations of these models through comprehensive error analysis, as proposed in previous studies [Lee et al., 2024a, Soroush et al., 2024]. Moreover, integrating these approaches with clinical benchmarking frameworks such as MEDS, which facilitates the transformation and summarization of tabular data using the MEDS schema [Arnrich et al., 2024, Kolo et al., 2024], represents another compelling direction. Finally, extending Clinical ModernBERT to multimodal settingslinking textual information with imaging or waveform datacould unlock novel capabilities in clinical decision support. In summary, Clinical ModernBERT demonstrates that thoughtful adaptation of general language modeling principles to the clinical domainthrough targeted masking, structured knowledge integration, and dense supervisionyields models that are not only competitive but, in many cases, state-of-the-art across wide range of biomedical NLP tasks."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Emily Alsentzer, John Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, arXiv preprint Publicly available clinical bert embeddings. and Matthew McDermott. arXiv:1904.03323, 2019. Ulzee An, Simon Lee, Moonseong Jeong, Aditya Gorla, Jeffrey Chiang, and Sriram Sankararaman. Dk-behrt: Teaching language models international classification of disease (icd) codes using known disease descriptions. In AI for Medicine and Healthcare AAAI Bridge Program 2025, 2025. Bert Arnrich, Edward Choi, Jason Alan Fries, Matthew BA McDermott, Jungwoo Oh, Tom Pollard, Nigam Shah, Ethan Steinberg, Michael Wornow, and Robin van de Water. Medical event data standard (meds): Facilitating machine learning for health. In ICLR 2024 Workshop on Learning from Time Series For Health, pages 0308, 2024. Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: pretrained language model for scientific text. arXiv preprint arXiv:1903.10676, 2019. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in neural information processing systems, 35: 1634416359, 2022. Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phuc Le Khac, Luke Melas, and Ritobrata Ghosh. Dall mini. HuggingFace. com. https://huggingface. co/spaces/dallemini/dalle-mini (accessed Sep. 29, 2022), 2021. 13 Franck Dernoncourt and Ji Young Lee. Pubmed 200k rct: dataset for sequential sentence classification in medical abstracts, 2017. URL https://arxiv.org/abs/1710.06071. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. Tabllm: Few-shot classification of tabular data with large language models. In International Conference on Artificial Intelligence and Statistics, pages 55495581. PMLR, 2023. Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes and predicting hospital readmission. arXiv preprint arXiv:1904.05342, 2019. Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. Mimic-iv, freely accessible electronic health record dataset. Scientific data, 10(1):1, 2023. Aleksia Kolo, Chao Pang, Edward Choi, Ethan Steinberg, Hyewon Jeong, Jack Gallifant, Jason Fries, Jeffrey Chiang, Jungwoo Oh, Justin Xu, et al. Meds decentralized, extensible validation (meds-dev) benchmark: Establishing reproducibility and comparability in ml for health. Machine Learning For Health Confernce 2024 Demo Track, 2024. Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana Mendu, Laura Barnes, and Donald Brown. Text classification algorithms: survey. Information, 10(4):150, 2019. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):12341240, 2020. Simon Lee and Timothy Lindsey. Can large language models abstract medical coded language? arXiv preprint arXiv:2403.10822, 2024. Simon Lee, Trevor Brokowski, and Jeffrey Chiang. Enhancing antibiotic stewardship using natural language approach for better feature representation. arXiv preprint arXiv:2405.20419, 2024a. Simon Lee, Sujay Jain, Alex Chen, Arabdha Biswas, Jennifer Fang, Akos Rudas, and Jeffrey Chiang. Multimodal clinical pseudo-notes for emergency department prediction tasks using multiple embedding model for ehr (meme). arXiv e-prints, pages arXiv2402, 2024b. Simon Lee, Sujay Jain, Alex Chen, Kyoka Ono, Jennifer Fang, Akos Rudas, and Jeffrey Chiang. Emergency department decision support using clinical pseudo-notes. arXiv preprint arXiv:2402.00160, 2024c. Yikuan Li, Ramsey M. Wehbe, Faraz S. Ahmad, Hanyin Wang, and Yuan Luo. Clinical-longformer and clinical-bigbird: Transformers for long clinical sequences, 2022. URL https://arxiv.org/ abs/2201.11838. Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier. Self-alignment pretraining for biomedical entity representations. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 42284238, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 334. URL https://aclanthology.org/2021.naacl-main.334/. 14 Zhiyong Lu. Pubmed and beyond: survey of web tools for searching biomedical literature. Database, 2011:baq036, 2011. Shawn Murphy, Griffin Weber, Michael Mendis, Vivian Gainer, Henry Chueh, Susanne Churchill, and Isaac Kohane. Serving the enterprise and beyond with informatics for integrating biology and the bedside (i2b2). Journal of the American Medical Informatics Association, 17(2):124130, 2010. David Nadeau and Satoshi Sekine. survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):326, 2007. Kyoka Ono and Simon Lee. Text serialization and their relationship with the conventional paradigms of tabular machine learning. arXiv preprint arXiv:2406.13846, 2024. Yifan Peng, Shankai Yan, and Zhiyong Lu. Transfer learning in biomedical natural language processing: an evaluation of bert and elmo on ten benchmarking datasets. arXiv preprint arXiv:1906.05474, 2019. Jacob Portes, Alexander Trott, Sam Havens, Daniel King, Abhinav Venigalla, Moin Nadeem, Nikhil Sardana, Daya Khudia, and Jonathan Frankle. Mosaicbert: bidirectional encoder optimized for fast pretraining. Advances in Neural Information Processing Systems, 36:31063130, 2023. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Simon Lee. Clinical_modernbert, 2025. URL https://huggingface.co/Simonlee711/ Clinical_ModernBERT. Ali Soroush, Benjamin Glicksberg, Eyal Zimlichman, Yiftach Barash, Robert Freeman, Alexander Charney, Girish Nadkarni, and Eyal Klang. Large language models are poor medical codersbenchmarking of medical code querying. NEJM AI, 1(5):AIdbp2300040, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In International conference on machine learning, pages 94389447. PMLR, 2020. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, et al. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. arXiv preprint arXiv:2412.13663, 2024. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingfaces transformers: State-of-the-art natural language processing, 2020. URL https://arxiv.org/abs/1910.03771. Michael Wornow, Suhana Bedi, Miguel Angel Fuentes Hernandez, Ethan Steinberg, Jason Alan Fries, Christopher Ré, Sanmi Koyejo, and Nigam Shah. Context clues: Evaluating long context models for clinical prediction tasks on ehrs. arXiv preprint arXiv:2412.16178, 2024. Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt. Stable and low-precision training for large-scale vision-language models, 2023. URL https: //arxiv.org/abs/2304.13013. Zhongbin Xie and Thomas Lukasiewicz. An empirical analysis of parameter-efficient methods for debiasing pre-trained language models. arXiv preprint arXiv:2306.04067, 2023. 15 Yosuke Yamagishi, Tomohiro Kikuchi, Shouhei Hanaoka, Takeharu Yoshikawa, and Osamu Abe. Modernbert is more efficient than conventional bert for chest ct findings classification in japanese radiology reports, 2025. URL https://arxiv.org/abs/2503.05060. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. Zhengyun Zhao, Qiao Jin, Fangyuan Chen, Tuorui Peng, and Sheng Yu. large-scale dataset of patient summaries for retrieval-based clinical decision support systems. Scientific data, 10(1):909, 2023."
        },
        {
            "title": "A Dataset Details",
            "content": "Biomedical NLP tasks (short-context) EHR Prediction: Text Classification To evaluate representation quality on structured clinical data, we include an emergency department (ED) disposition prediction task based on the dataset introduced in Lee et al. [2024b]. The dataset comprises structured EHR records from emergency visits, including demographic variables, triage vitals, chief complaints, procedures, labs, and medication events recorded during the first hour of patients ED stay. These features are transformed into textual pseudo-notes, which emulate the style and structure of clinical documentation, making the data compatible with language models. The prediction target is the patients disposition outcomei.e., whether the patient is admitted or discharged. This task serves as proxy for real-time decision support and reflects high-stakes, operationally critical setting in clinical care. PubMed-200k-RCT: Text Classification The PubMed-200k-RCT dataset is sentence-level classification benchmark derived from structured text in the PubMed corpus [Dernoncourt and Lee, 2017]. It contains approximately 200,000 sentences, each labeled according to its rhetorical function within scientific abstract: Background, Objective, Methods, Results, or Conclusions. The dataset is formatted with fields for abstract ID, sentence ID, label, and sentence text. Medical Entity Recongition: NER: Named Entity Recognition at token level The MedNER dataset provides supervised benchmark for named entity recognition (NER) in clinical and biomedical text 2. We frame this dataset as resource for token-level medical entity recognition under constrained supervision. The MedNER corpus comprises tokenized clinical and biomedical text annotated for binary entity presence, where each token is labeled as either part of named medical entity or not. This reductionist framing positions the dataset as testbed for probing entity boundary detection and semantic salience in the absence of hierarchical supervision. It is particularly well-suited for studying low-resource NER, binary token classification, and early-stage entity bootstrapping in noisy medical corpora, and can serve as diagnostic task within broader pretraining or adaptation pipelines. PMC-Patients: Retrieval The PMC-Patients dataset provides large-scale benchmark for retrievalbased clinical decision support tasks, leveraging real-world case reports from PubMed Central (PMC) [Zhao et al., 2023]. It comprises 167,000 patient summaries paired with over 3.1 million patientarticle relevance annotations and 293,000 patient-patient similarity links, derived from the citation graph of biomedical literature. The dataset supports two primary retrieval tasks: Patient-to-Article Retrieval (PAR), where the goal is to retrieve relevant scientific literature given patient summary, and Patient-to-Patient Retrieval (PPR), which aims to identify clinically similar patient cases. Its scale, annotation quality, and diversity make it valuable testbed for models that integrate semantic understanding and clinical relevance. PMC-Patients includes predefined training, validation, and test splits to facilitate reproducibility and standard evaluation across retrieval methods in the clinical domain. Long Context Tasks i2b2: NER The i2b2 datasets are suite of de-identified clinical text corpora released through shared tasks organized by the Informatics for Integrating Biology and the Bedside (i2b2) initiative [Murphy et al., 2010]. These datasets span multiple years (e.g., 2006, 2010, 2012, 2014) and cover range of clinical named entity recognition (NER) challenges, such as extracting problems, treatments, tests, and temporal expressions from patient narratives. Unlike synthetic or abstracted biomedical corpora, i2b2 datasets consist of real-world clinical notes, making them uniquely valuable for benchmarking models in practical clinical NLP settings. Due to their rich annotation schemas and long-form input structure, they are particularly well-suited for evaluating models on long-context NER and contextual disambiguation tasks. In our setup, we preserve the original document structure and avoid chunking, allowing Clinical ModernBERT to leverage its extended context capacity to model entity boundaries across sentences and paragraphs. 2https://www.kaggle.com/datasets/arunagirirajan/medical-entity-recognition-ner"
        },
        {
            "title": "B Model Optimizations",
            "content": "B.1 MLM Accuracies and MLM Loss Figure 4: Masked Language Modeling (MLM) Top-K Accuracies and Loss: We report top-K accuracies for = 1, 5, 10, 25 alongside MLM loss across three pre-training runs initialized with different learning rates (top to bottom: 3 103, 5 104, 1 105). Higher learning rates yielded more stable convergence and avoided shallow local minima, suggesting improved exploration of the loss landscape. As expected, larger learning rates also introduced noisier gradient updates, which aligns with standard intuitions in stochastic optimization. 18 Pre-training Code and Model Weights C.1 Source Code and Model Weights The full source code for pretraining, finetuning, and evaluation of Clinical ModernBERT is available at: https://github.com/Simonlee711/Clinical_ModernBERT. Model weights for the pretrained Clinical ModernBERT checkpoint can be accessed via the Hugging Face Hub: https://huggingface.co/Simonlee711/Clinical_ModernBERT."
        }
    ],
    "affiliations": [
        "Department of Computational Medicine & Neurosurgery UCLA",
        "Department of Computational Medicine UCLA"
    ]
}