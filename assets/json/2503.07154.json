{
    "paper_title": "Ideas in Inference-time Scaling can Benefit Generative Pre-training Algorithms",
    "authors": [
        "Jiaming Song",
        "Linqi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent years have seen significant advancements in foundation models through generative pre-training, yet algorithmic innovation in this space has largely stagnated around autoregressive models for discrete signals and diffusion models for continuous signals. This stagnation creates a bottleneck that prevents us from fully unlocking the potential of rich multi-modal data, which in turn limits the progress on multimodal intelligence. We argue that an inference-first perspective, which prioritizes scaling efficiency during inference time across sequence length and refinement steps, can inspire novel generative pre-training algorithms. Using Inductive Moment Matching (IMM) as a concrete example, we demonstrate how addressing limitations in diffusion models' inference process through targeted modifications yields a stable, single-stage algorithm that achieves superior sample quality with over an order of magnitude greater inference efficiency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 2 4 5 1 7 0 . 3 0 5 2 : r Ideas in Inference-time Scaling can Benefit Generative Pre-training Algorithms Jiaming Song, Linqi Zhou"
        },
        {
            "title": "Luma AI",
            "content": "Abstract Recent years have seen significant advancements in foundation models through generative pre-training, yet algorithmic innovation in this space has largely stagnated around autoregressive models for discrete signals and diffusion models for continuous signals. This stagnation creates bottleneck that prevents us from fully unlocking the potential of rich multi-modal data, which in turn limits the progress on multimodal intelligence. We argue that an inference-first perspective, which prioritizes scaling efficiency during inference time across sequence length and refinement steps, can inspire novel generative pre-training algorithms. Using Inductive Moment Matching (IMM) as concrete example, we demonstrate how addressing limitations in diffusion models inference process through targeted modifications yields stable, single-stage algorithm that achieves superior sample quality with over an order of magnitude greater inference efficiency."
        },
        {
            "title": "Introduction",
            "content": "Despite the significant advances in applications from generative pre-training of foundation models, algorithmic developments in this area have stagnated. The generative pre-training field is currently dominated by two paradigms, both popularized around 2020 autoregressive models for discrete signals [BMR+20] and diffusion models [HJA20] for continuous signals. This stagnation has created bottleneck over generative pre-training and led to more explorations in inference-time scaling [SLXK24, BJE+24, MTJ+25]. In this position paper, we argue that ideas in inference-time scaling can benefit generative pre-training algorithms. We consider two axes of inference-time scaling, sequence length and refinement steps, which apply to autoregressive models and diffusion models respectively. Based on this unified perspective, we believe that in order to develop novel and practical generative pre-training algorithms, one should (1) develop algorithms that scale efficiently in the two axes during inference, and (2) design the inference algorithm before the training algorithm to optimally utilize the model capacity at inference time. We show that Inductive Moment Matching (IMM, [ZES25]) is concrete example supporting these beliefs in the continuous space. In particular, we examine the one-step iterative process of DDIM [SME20, LCBH+22, LGL22] and show that it has limited capacity with respect to the target timestep under the current denoising network design. This can be addressed by adding the target timestep to the inputs of the denoising network [KLL+23]. 1 Interestingly, this one fix, plus proper moment matching objective [GBR+12] leads to stable, single-stage algorithm that surpasses diffusion models in sample quality while being over an order of magnitude more efficient at inference [ZES25]. Notably, these ideas do not rely on denoising score matching [Vin11] or the score-based stochastic differential equations [SSDK+20] on which the foundations of diffusion models are built. We hope that these ideas can bring us out of the existing paradigms, inspire further research on effective pre-training algorithms on high-dimensional mixed-modality data, and ultimately breaking the current barriers around pre-training."
        },
        {
            "title": "2 Two axes of inference-time scaling",
            "content": "Sequence length defines the number of tokens. This is seen in standard autoregressive large language models (LLMs), which have recently demonstrated strong chain-ofthought and reasoning capabilities from inference-time scaling. Refinement steps defines the number of iterative steps that improve existing tokens without changing the sequence length. This is seen in standard score-based diffusion models, where more steps indicate fewer discretization errors in the numerical solver. We note that the refinement process is not restricted to denoising alone; it is considered valid as long as the sequence length does not increase. Under this definition, autoregressive models scale over sequence length and diffusion models scale over refinement steps. We list few notable examples that cover most of the techniques surrounding discrete and continuous signals. Methods that are not scalable in either sequence length or refinement steps: VAE [KW13], GAN [GPAM+14], Normalizing Flows [RM15]. Methods that are scalable in sequence length but not refinement steps: GPT [BMR+20], PixelCNN [vdOKK16], MaskGiT [CZJ+22], VAR [TJY+25]. Methods that are scalable in refinement steps but not in sequence length: Diffusion models [HJA20], Energy-based models [DM19], Consistency models [SDCS23]. Parallel non-linear equation solving for autoregressive models [SMLE21]; note that this uses an iterative approach to sample from all the tokens in parallel, despite being trained with autoregressive objectives. Methods that are scalable in both, with sequence length in the outer loop: AR-Diffusion [WFL+23], Rolling diffusion [RHSH24], MAR [LTL+25]. Blockwise parallel decoding [SSU18], which applies predict, verify, accept as part of the refinement process. 2 Methods that are scalable in both, with refinement steps in the outer loop: Autoregressive distribution smoothing [MSS+21], which performs an iterative denoising process with an autoregressive model as the inner loop."
        },
        {
            "title": "3 Designing algorithms that can scale efficiently",
            "content": "The paper takes the following positions. 1. Pre-training algorithms for generative AI should have inference-time scalability in sequence length and refinement steps. 2. These algorithms should also scale efficiently, e.g., with as few number of model steps as possible. 3. Before developing the training method, it should be verified whether the model has enough capacity to represent the target distribution during inference. For our first position, we note that in the visual generative domain, even very recently, various efforts have been made to revive traditional ideas such as GANs [KZZ+23, ZYZ+23, HGKT25] and Normalizing Flows [KPT24, ZZN+24], despite the overwhelming popularity of diffusion models in visual generation. We do not dismiss or promote the training designs (such as the advesarial training procedure, the transformer-based normalizing flow, or the denoising autoencoder objective), but rather attributes the popularity of diffusion models to its ability to leverage inference-time scaling. In the case of LLMs, scaling over sequence length may already be self-evident for practitioners in the field, but methods scaling over refinement steps [LME23, SHW+24, SAS+25, GRS+24] have not received widespread adoption. This is not to say that scaling over refinement steps is wrong path to take, but rather, as discussed in the third position, there lacks attention to proper inference algorithms that optimally utilize model capacity. If these limitations are resolved, scaling over refinement steps can potentially provide additional performance boost over the current autoregressive paradigms on text. The second position considers the practical scenario where fewer inference steps in either dimension are desired. Motivated by this, many works have explored diffusion distillation in the continous domain to reduce the number of refinement steps [SH22], as well as diffusion langauge models [LME23, SHW+24, ONX+24, SAS+25] in the discrete space that have chance to reduce the number of steps needed to represent the same sequence length. Advancing in these directions can lower the latency at inference time. The third position provides necessary condition to analyze whether particular training approach is sound based on its potential behavior at inference. In particular, we need to make sure that the inference process can represent any distribution that we are considering (whether it is the discrete or continuous space), assuming that the universal approximation theorem [HSW89] holds under the model that we are working with. If the model does not have enough capacity to represent the distribution, then no training algorithm can solve the problem perfectly. Unless we are willing to live with these limitations, we need to rethink 3 (a) DDIM (b) Improved DDIM Figure 1: Left shows traditional one-step DDIM sampling under Flow Matching construction. Given xt and t, xs is produced by following the prediced velocity field vθ(xt, t). However, the model does not have enough capacity to land on the ODE flow result at in one step because vθ(xt, t) is unaware of and thus cannot approximate any function over s. practical fix, on the right, simply injects into our network and now the model has enough capacity to approximate direct jump towards the correct solution. the design of the pre-training algorithm. Together, we believe that this inference-first perspective can reveal potential design flaws of current methods and hope that the new generation of pre-training algorithms can place proper attention on resolving such issues during inference. In the remainder of the section, we illustrate two examples that cover both continuous and discrete distributions, as well as both axes of inference-time scaling. In the continuous case, we show that the DDIM sampler is flawed for efficient few-step sampling because of missing arguments and propose practical fix. In the discrete case, we show that current multi-token prediction (MTP) models make naïve Bayes assumption that forces alternative sampling methods to be used."
        },
        {
            "title": "3.1 A limitation of DDIM and its improvement\nWe use the DDIM sampler [SME20] under rectified flow formulation [LGL22, GHH+24] as\nan example1. In rectified flows, the sampling procedure in each iteration from the current\nsample xt and source timestep t to the target timestep s is defined as:",
            "content": "xs := DDIM(xt, t, s) = xt + (s t)vθ(xt, t), (1) which transforms the sample xt to xs via the velocity network vθ. Note that in the typical diffusion model formulation, vθ takes in only xt and as arguments. It is clear that the denoising autoencoder objective [Vin11] will not recover desirable onestep (or few-step) sampler from DDIM, but how about other training objective functions, especially ones based on diffusion distillation? 1A similar argument would work for any other first-order Euler ODE solver for diffusion models as well. We believe that the answer is no. To see this, let us take the following partial derivative: DDIM(xt, t, s) = vθ(xt, t), (2) which does not depend on at all. This means that even though DDIM has enough capacity to represent any function over xt and (via universal function approximation), the same does not apply to s. We show an illustration of this problem in Figure 1. We further note that this criticism applies to the inference process, not how the model is trained. In fact, even if the model is learned via consistency training [SDCS23] and can generate accurate samples in single step, it will not necessarily generate accurate samples under multi-step DDIM. This partially explains why consistency models have restart sampling process [XDC+23] that injects additional Gaussian noise at each sampling step. natural fix to the above problem is simply to add also to the input of the velocity network [KLL+23], so that vθ takes three arguments: xt, t, and s. Now the improved DDIM has enough model capacity to represent any functions over and thus can learn the proper one-step jump to solution. Recently, Zhou et al. [ZES25] demonstrated that single, stable pre-training procedure is possible with Inductive Moment Matching (IMM). IMM offers promising single-stage alternative to the current two-stage diffusion then distillation paradigm in visual foundational models [LTH+23, SBD+24, YGZ+24, ZLL24, ZZG+24, LXR+25]. The paper argues that given the simplicity of moment matching and its relative low popularity in the community of visual generative modeling [LSZ15], there can be other promising alternatives to IMM. However, it is imperative to keep the networks dependence over if one wishes to employ DDIM-style sampler."
        },
        {
            "title": "3.2 Multi-token prediction\nMulti-token prediction (MTP) is of great interest to the language modeling community\nbecause of its potential to achieve faster inference [GIR+24], which allows efficient inference-\ntime scaling. However, the current multi-token prediction models often predict the softmax\nvalues of multiple tokens in parallel, which is a naïve conditional independence assumption\n(i.e., naïve Bayes). We argue that this inference design greatly limits the capacity of the\nmodel distribution and more efforts should be spent resolving this fundamental issue.",
            "content": "Consider the example of trying to predict the next two words/tokens2 in the sentence: The list of poker hands that consist of two English words are: .... As the list can be arbitrarily ordered, the immediate next two words can be any hand, such as high card, two pair, full house, or straight flush; there is correlation between the two words that makes valid hand. The multi-token prediction LLM first produce the corresponding softmax weights, and then sample the tokens independently, which may lead to unwanted combinations such as high house. Current MTP inference algorithms heuristically bypass this model capacity limitation. For example, Gloeckle et al. [GIR+24] uses self-speculative decoding whereas Deepseek2For simplicity of the argument, lets assume that token is word here. 5 V3 [LFX+24] simply discards the multi-token prediction heads and use regular next-token prediction during inference. Therefore, even though the model is misspecified in representing the joint distribution of the multi-token outcomes, many inference algorithms can be cleverly designed to correct this inherent capacity problem. Notably, MTP is also relevant to diffusion-based language models [LME23, SHW+24, SAS+25, NZY+25] for certain model design choices. To address the model capacity issue while realising latency gains, diffusion language models can adopt procedure where the initial sampling steps produce few tokens in parallel, making it closer to an any-order autoregressive model [SSE22], and then sampling more tokens in parallel when conditional independence between the remaining masked tokens is more likely to hold [SHW+24]. Despite the current corrections to the inference procedure, it is interesting to see if we can train well-specified multi-token prediction model without the naïve Bayes assumption, so that we can directly sample from the model and yield multiple tokens during inference without rejection sampling. One challenge here is to optimize through the discrete token sampling processes [JGP16, GCW+17]. Resolving this issue can truely unlock the scaling potential over refinement steps for LLMs."
        },
        {
            "title": "4 Conclusions",
            "content": "We believe the future of multi-modal pre-training belongs to algorithms that can scale on both inference axes. By reflecting inference-time scaling behavior before training, we can make proper trade-offs between model tractability and scaling efficiency. In the continuous domain, this has lead to simple, stable, and scalable algorithms with state-of-the-art performance that do not rely on key ideas of denoising diffusion models. We hope that these ideas can mend the Autoregressive-Diffusion Schism in the research community and unlock greater potentials in generative pre-training."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Jiaxin Shi, Qinsheng Zhang, Chongxuan Li, as well as members of the Luma AI research team for helpful discussions."
        },
        {
            "title": "References",
            "content": "[BJE+24] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, 6 Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are FewShot learners. arXiv preprint arXiv:2005.14165, May 2020. [CZJ+22] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. [DM19] Yilun Du and Igor Mordatch. Implicit generation and generalization in EnergyBased models. arXiv preprint arXiv:1903.08689, March 2019. [GBR+12] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alexander Smola. kernel two-sample test. The Journal of Machine Learning Research, 13(1):723773, 2012. [GCW+17] Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation through the void: Optimizing control variates for black-box gradient estimation. arXiv preprint arXiv:1711.00123, October 2017. [GHH+24] Ruiqi Gao, Emiel Hoogeboom, Jonathan Heek, Valentin De Bortoli, Kevin P. Murphy, and Tim Salimans. Diffusion meets flow matching: Two sides of the same coin. The Internet, 2024. [GIR+24] Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024. [GPAM+14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David WardeFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Ghahramani, Welling, Cortes, Lawrence, and Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 26722680. Curran Associates, Inc., 2014. [GRS+24] Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching. Advances in Neural Information Processing Systems, 37:133345133385, 2024. [HGKT25] Nick Huang, Aaron Gokaslan, Volodymyr Kuleshov, and James Tompkin. The gan is dead; long live the gan! modern gan baseline. Advances in Neural Information Processing Systems, 37:4417744215, 2025. [HJA20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arXiv:2006.11239, June 2020. [HSW89] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359366, 1989. [JGP16] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, November 2016. [KLL+23] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. 7 [KPT24] Alexander Kolesnikov, André Susano Pinto, and Michael Tschannen. Jet: modern transformer-based normalizing flow. arXiv preprint arXiv:2412.15129, 2024. [KW13] Diederik Kingma and Max Welling. Auto-Encoding variational bayes. arXiv preprint arXiv:1312.6114v10, December 2013. [KZZ+23] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1012410134, 2023. [LCBH+22] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [LFX+24] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [LGL22] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [LME23] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. [LSZ15] Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In International Conference on Machine Learning, pages 17181727. PMLR, 2015. [LTH+23] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. [LTL+25] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2025. [LXR+25] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. arXiv Diffusion adversarial post-training for one-step video generation. preprint arXiv:2501.08316, 2025. [MSS+21] Chenlin Meng, Jiaming Song, Yang Song, Shengjia Zhao, and Stefano Ermon. Improved autoregressive modeling with distribution smoothing. arXiv preprint arXiv:2103.15089, 2021. [MTJ+25] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inferencetime scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. 8 [NZY+25] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [ONX+24] Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. [RHSH24] David Ruhe, Jonathan Heek, Tim Salimans, and Emiel Hoogeboom. Rolling diffusion models. arXiv preprint arXiv:2402.09470, 2024. [RM15] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, May 2015. [SAS+25] Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2025. [SBD+24] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [SDCS23] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. [SH22] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. [SHW+24] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024. [SLXK24] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [SME20] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, April 2020. [SMLE21] Yang Song, Chenlin Meng, Renjie Liao, and Stefano Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In International Conference on Machine Learning, pages 97919800. PMLR, 2021. [SSDK+20] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [SSE22] Andy Shih, Dorsa Sadigh, and Stefano Ermon. Training and inference on any-order autoregressive models the right way. Advances in Neural Information Processing Systems, 35:27622775, 2022. 9 [SSU18] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018. [TJY+25] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2025. [vdOKK16] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, January 2016. [Vin11] Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):16611674, 2011. [WFL+23] Tong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, Jian Jiao, Juntao Li, Jian Guo, Nan Duan, Weizhu Chen, et al. Ar-diffusion: Auto-regressive diffusion model for text generation. Advances in Neural Information Processing Systems, 36:3995739974, 2023. [XDC+23] Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian, Ziming Liu, and Tommi Jaakkola. Restart sampling for improving generative processes. Advances in Neural Information Processing Systems, 36:7680676838, 2023. [YGZ+24] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution In Proceedings of the IEEE/CVF conference on computer matching distillation. vision and pattern recognition, pages 66136623, 2024. [ZES25] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. arXiv preprint, 2025. [ZLL24] Yuanzhi Zhu, Xingchao Liu, and Qiang Liu. Slimflow: Training smaller onestep diffusion models with rectified flow. In European Conference on Computer Vision, pages 342359. Springer, 2024. [ZYZ+23] Jiapeng Zhu, Ceyuan Yang, Kecheng Zheng, Yinghao Xu, Zifan Shi, and Yujun Shen. Exploring sparse moe in gans for text-conditioned image synthesis. arXiv preprint arXiv:2309.03904, 2023. [ZZG+24] Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, and Hai Huang. Adversarial score identity distillation: Rapidly surpassing the teacher in one step. arXiv preprint arXiv:2410.14919, 2024. [ZZN+24] Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Josh Susskind. Normalizing flows are capable generative models. arXiv preprint arXiv:2412.06329, 2024."
        }
    ],
    "affiliations": []
}