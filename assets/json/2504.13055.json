{
    "paper_title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation",
    "authors": [
        "Xiangyan Liu",
        "Jinjie Ni",
        "Zijian Wu",
        "Chao Du",
        "Longxu Dou",
        "Haonan Wang",
        "Tianyu Pang",
        "Michael Qizhe Shieh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in reinforcement learning (RL) have strengthened the reasoning capabilities of vision-language models (VLMs). However, enhancing policy exploration to more effectively scale test-time compute remains underexplored in VLMs. In addition, VLMs continue to struggle with imperfect visual perception, which in turn affects the subsequent reasoning process. To this end, we propose NoisyRollout, a simple yet effective RL approach that mixes trajectories from both clean and moderately distorted images to introduce targeted diversity in visual perception and the resulting reasoning patterns. Without additional training cost, NoisyRollout enhances the exploration capabilities of VLMs by incorporating a vision-oriented inductive bias. Furthermore, NoisyRollout employs a noise annealing schedule that gradually reduces distortion strength over training, ensuring benefit from noisy signals early while maintaining training stability and scalability in later stages. With just 2.1K training samples, NoisyRollout achieves state-of-the-art performance among open-source RL-tuned models on 5 out-of-domain benchmarks spanning both reasoning and perception tasks, while preserving comparable or even better in-domain performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 5 5 0 3 1 . 4 0 5 2 : r NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation Xiangyan Liu1 Jinjie Ni1 Zijian Wu1 Chao Du2 Longxu Dou2 Haonan Wang1 Tianyu Pang2 Michael Qizhe Shieh1 1National University of Singapore 2Sea AI Lab, Singapore Code github.com/John-AI-Lab/NoisyRollout Model & Dataset hf.co/collections/xyliu6/noisyrollout"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in reinforcement learning (RL) have strengthened the reasoning capabilities of vision-language models (VLMs). However, enhancing policy exploration to more effectively scale test-time compute remains underexplored in VLMs. In addition, VLMs continue to struggle with imperfect visual perception, which in turn affects the subsequent reasoning process. To this end, we propose NoisyRollout, simple yet effective RL approach that mixes trajectories from both clean and moderately distorted images to introduce targeted diversity in visual perception and the resulting reasoning patterns. Without additional training cost, NoisyRollout enhances the exploration capabilities of VLMs by incorporating vision-oriented inductive bias. Furthermore, NoisyRollout employs noise annealing schedule that gradually reduces distortion strength over training, ensuring benefit from noisy signals early while maintaining training stability and scalability in later stages. With just 2.1K training samples, NoisyRollout achieves state-ofthe-art performance among open-source RL-tuned models on 5 out-of-domain benchmarks spanning both reasoning and perception tasks, while preserving comparable or even better in-domain performance. Figure 1: Accuracy improvement over the baseline model (Qwen2.5-VL-7B-Instruct) on out-ofdomain benchmarks covering both reasoning and perception tasks. Both Qwen2.5-VL-GRPO-7B and NoisyRollout-7B are fine-tuned by ourselves (denoted with ) using GRPO with 2.1K training samples from Geometry3K. The exact accuracy of NoisyRollout-7B is annotated above each corresponding bar in parentheses. Equal contribution. Corresponding author. Technical Report"
        },
        {
            "title": "Introduction",
            "content": "Test-time compute scaling (often referred to as reasoning) through reinforcement learning (RL) has emerged as promising axis for scaling model intelligence (Jaech et al., 2024; Cui et al., 2025). While this idea has been primarily explored in the context of large language models (LLMs), the vision-language model (VLM) community is also actively investigating this direction (Liu et al., 2025b; Meng et al., 2025). As natural extensions of LLMs for visual understanding, VLMs could intuitively benefit from similar scaling behaviors via RL (Huang et al., 2025; Wang et al., 2025a; Lu et al., 2025; Yu et al., 2025a). However, scaling test-time compute through RL requires more than sheerly generating longer outputs (Liu et al., 2025a). key challenge is effective policy exploration, which enables models to discover behaviors that generalize well beyond training data (Yu et al., 2025b), which remains largely underexplored in VLM studies. Traditional practices, such as increasing rollout temperature to promote decoding diversity (Zeng et al., 2024), often introduce superficial variability without meaningfully guiding the policy toward more informative behaviors. Meanwhile, VLMs frequently struggle with imperfect visual perception due to the inherent complexity of visual information (Liu et al., 2024a; Wei et al., 2024). These perceptual limitations result in significant bottleneck, especially for reasoning tasks that seriously depend on accurate interpretations of visual inputs (Zhang et al., 2024a; Jiang et al., 2025). Tackling the challenges of policy exploration and perceptual limitations in VLMs, we propose NoisyRollout, simple yet effective RL fine-tuning approach for VLMs that introduces meaningful rollout diversity by incorporating noisy trajectories generated from image-text pairs with distorted visual inputs, enabling policy models to perform more targeted exploration during training. The core process happpens at the rollout collection phase: the old policy (πθold ) generates reasoning trajectories using both original clean images and moderately distorted versions of the same images in response to identical text queries. While the current policy (πθ) is updated using only clean image inputs, trajectories from both clean and distorted images contribute to calculating the reward baseline and normalized advantage in Group Relative Policy Optimization (GRPO) (Shao et al., 2024).1 This hybrid rollout strategy strengthens learning through two key mechanisms. First, successful reasoning trajectories from distorted images reveal alternative, potentially more robust reasoning strategies, providing diverse explorations that improves generalization to harder or out-of-domain visual conditions. Second, when the same query leads to diverging outcomes between clean and distorted inputs, the resulting reward differences highlight perceptual discrepancies that affect reasoning. These discrepancies serve as implicit contrastive signals that refine the models perception during the reasoning process via squeezing the negative perceptual exploration space. To further improve scalability and stabilize training, we employ noise annealing schedule that gradually reduces the strength of input distortions over training, avoiding significant distributional mismatch caused by noisy trajectories (which easily occurs in later training stages) while preserving the benefits of noisy signals in early training stages. Extensive experiments validate the effectiveness of NoisyRollout. With just 2.1K RL training samples from Geometry3K, NoisyRollout achieves state-of-the-art performance across 5 challenging out-ofdomain visual reasoning and perception benchmarks (Lu et al., 2023; Guan et al., 2024; Zhang et al., 2024a; Wang et al., 2024; Qiao et al., 2024) compared to other open-source RL-tuned models (Meng et al., 2025; Gui & Ren, 2025; Wang et al., 2025b) and even those leveraging large-scale SFT before RL training (Yang et al., 2025; Zhang et al., 2025a; Deng et al., 2025), with notable results including MathVerse (53.2%), MathVision (28.5%) and HallusionBench (72.1%). When compared to its direct baseline (vanilla GRPO), NoisyRollout consistently outperforms it on both in-domain and out-ofdomain benchmarks. The improvements generalize to other training corpora as well, showing similar gains when trained on K12 (Meng et al., 2025). Overall, NoisyRollout offers simple and effective strategy to enhance visual reasoning with noisy reinforcement leanring, improving model generalization capabilities without extra training overhead or modifications to the underlying RL objective. 1We adopt GRPO due to its natural compatibility with our method; demonstrating generality across other RL objectives is left for future work. 2 Figure 2: Illustration of our NoisyRollout workflow. Solid lines denote the standard GRPO process, while dashed lines depict the generation and use of noisy rollouts from distorted images. KL divergence loss is omitted as discussed in Section 2. The terms {oi}n1+n2 , and {Ai}n1+n2 represent mixed trajectories, rewards, and normalized advantages, respectively, with n1 from clean inputs and n2 from noisy inputs. (I, q) and ( I, q) denote clean and noisy inputs where = Tαt(I) applies distortion with strength αt. The distortion level αt is determined by noise annealing schedule η(), which gradually reduces distortion over the training process. Only clean inputs are used for policy optimization in RL. , {ri}n1+n2 i=1 i=1 i="
        },
        {
            "title": "2 NoisyRollout: A Free-Lunch with Noisy Reinforcement Learning",
            "content": "We introduce NoisyRollout, an RL fine-tuning method designed to enhance visual reasoning in VLMs, particularly improving the rollout diversity for effective policy exploration. NoisyRollout achieves this by incorporating hybrid rollout strategy that leverages reasoning trajectories form both clean and distorted images, and noise annealing schedule that progressively reduces distortion strength. These adjustments require no additional training cost and can be seamlessly applied to the standard implementation of GRPO. simplified overview is provided in Figure 2 and Algorithm 1. GRPO. Group Relative Policy Optimization (GRPO) (Shao et al., 2024) was originally developed to improve mathematical reasoning in LLMs but can be effectively adapted to enhance visual reasoning in VLMs. For given input pair (I, q) of image and text query from the training set pD, to avoid reward hacking, rule-based reward function R(I, q, o) is typically adopted such that R(I, q, o) = 1 if the generated response correctly addresses the query (as evaluated by verifiable parser) and R(I, q, o) = 0 otherwise. The old policy πθold generates response rollouts for this input, and the baseline reward is computed as = 1 i=1 Ri. The normalized advantage for the i-th rollout is given by ˆAi = Ri σ(R) , where σ(R) denotes the standard deviation of rewards within the group. Derived from PPO (Schulman et al., 2017), the GRPO objective function is: (cid:80)n LGRPO(θ) = E(I,q)pD,oπθold (I,q) 1 (cid:88) min (cid:32) (cid:34) πθ(oi I, q) πθold (oi I, q) i=1 ˆAi, clip (cid:16) πθ(oi I, q) πθold (oi I, q) , 1 ϵ, 1 + ϵ (cid:33)(cid:35) , (cid:17) ˆAi (1) where πθ is the current policy, ϵ > 0 sets the clipping range. We omit the KL divergence constraint DKL[πθπθref ] following recent practices in Meng et al. (2025) and Liu et al. (2025a). Hybrid rollout strategy. Building upon GRPO, NoisyRollout introduces hybrid rollout strategy to specifically enhance the rollout diversity from the visual perspective. The core idea is to leverage rollouts generated from both clean and distorted images, thereby enriching the signals used for policy updates. For each input pair (I, q), we generate corresponding noisy image using noise transformation function parameterized by noise strength α, denoted as = Tα(I). As illustrated in Figure 2, the old policy πθold then produces n1 rollouts conditioned on the clean image and n2 rollouts conditioned on the noisy image I. These responses are grouped together and collectively used to compute common baseline reward, = 1 i=1 Ri, as well as corresponding normalized advantages for each rollout. Crucially, although the noisy rollouts contribute to the baseline and advantage calculation, the policy update step remains conditioned solely on the clean image (and (cid:80)n1+n2 n1+n 3 Algorithm 1 NoisyRollout: Noisy Reinforcement Fine-Tuning 1: Input: Current policy πθ, old policy πθold , dataset D, training steps Tmax, clean rollout number n1, noisy rollout number n2, clip parameter ϵ, initial noise strength α0, noise scheduler η(), noise transformation function () 2: for = 1 to Tmax do 3: 4: 5: 6: j=1 from πθold(o I, q) k=n1+1 from πθold(o I, q) Sample batch (I, q) Set noise strength αt = η(α0, t, Tmax) Generate distorted images = Tαt(I) Sample {oj}n1 Sample {ok}n1+n2 Compute rewards Ri = R(I, q, oi) for all {1, . . . , n1 + n2} Compute baseline = 1 Compute advantages ˆAi = Ri Update policy using: (cid:104) L(θ) = 1 n1+n2 θ θ θL(θ) θold θ ˆAi, clip(cid:0) πθ(oiI,q) (cid:16) πθ(oiI,q) πθold (oiI,q) i=1 min i=1 Ri (cid:80)n1+n2 (cid:80)n1+n2 n1+n σ(R) 7: 8: 9: 10: 11: 12: 13: 14: 15: end for πθold (oiI,q) , 1 ϵ, 1 + ϵ(cid:1) ˆAi Update conditioned on clean images only Update old policy parameters (cid:17)(cid:105) Annealing schedule Clean rollouts Noisy rollouts query q). The NoisyRollout objective function is defined as: L(θ) = j=1πθold (I,q),{ok}n1+n2 k=n1+1πθold ( I,q) (cid:34) (I,q)pD,{oj }n1 n1+n2(cid:88) 1 n1 + n2 i=1 (cid:32) min πθ(oi I, q) πθold (oi I, q) ˆAi, clip (cid:16) πθ(oi I, q) πθold (oi I, q) , 1 ϵ, 1 + ϵ (cid:17) ˆAi (cid:33)(cid:35) . (2) Noise annealing schedule. Applying fixed-strength distortions throughout training often leads to training instability, primarily due to distributional mismatch between noisy rollouts and the evolving policy. To mitigate this, we introduce noise annealing schedule η() that gradually reduces the distortion strength over time. Specifically, at training step t, the noise level is defined as αt = η(α0, t, Tmax), where α0 is the initial noise strength and Tmax denotes the total number of training steps. The distorted input is then generated as = Tαt(I). This schedule provides diverse and informative supervision signals in the early stages of training, when the policy is constrained by its perceptual capacity. As training progresses, the noise level is gradually reduced, narrowing the gap between noisy rollouts and the outputs the policy would produce on clean images. This helps prevent abrupt distribution shifts after policy updates and allows the noisy rollouts to become increasingly on-policy over time, contributing to more stable training dynamics and better exploitation in the later stages of training. Summary. NoisyRollout aims to improve the visual reasoning abilities of VLMs by enhancing rollout diversity to enable more effective exploration during RL training. Built on top of GRPO, it introduces hybrid rollout strategy and noise annealing schedule. These additions require no extra training cost and preserve the original RL objective. This design offers several benefits: Robust reasoning: Positive trajectories2 from distorted inputs offer alternative, and potentially more robust reasoning paths, improving generalization to challenging or out-of-domain visual conditions. Contrastive perceptual signals: When clean and distorted inputs yield divergent outcomes for the same text query, the resulting reward differences shape better perceptual exploration space, serving as implicit contrastive signals that refine the models perceptual behaviors during reasoning. Stable training dynamics for better exploitation: The noise annealing schedule enables smooth transition from early-stage noisy signals to fully on-policy learning, mitigating distributional mismatch and ensuring stable convergence as the model gradually improves its perception and reasoning. This provides solid foundation for further exploitation in the later stages of RL training. 2We regard trajectory as positive if it receives reward of 1. Table 1: Performance comparison of various VLMs on suite of out-of-domain benchmarks. We report accuracy scores (%) for all benchmarks for clarity. Avg. denotes the average accuracy across the five benchmarks. Models marked with are evaluated using our evaluation scripts implemented with vLLM (Kwon et al., 2023), and indicates results evaluated using the direct-answer template. For models in the R1-related category, dataset sizes used for SFT and RL are annotated in red and blue, respectively. Among R1-related models, best value per column is bold, second best is underlined. #Data MathVerse MathVision MathVista WeMath HallusionBench Avg. Model GPT-4o (Hurst et al., 2024) Claude-3.5-Sonnet (Anthropic, 2024) Kimi1.5 (Kimi Team, 2025) InternVL-2.5-8B-Instruct (Chen et al., 2024) InternVL-2.5-78B-Instruct (Chen et al., 2024) QVQ-72B-Preview (Qwen, 2024) Qwen2.5-VL-72B-Instruct (Bai et al., 2025) URSA-8B (Luo et al., 2025) Mulberry-7B (Yao et al., 2024) Close-source - - - 50.8 26.5 - Open-source - - - - - - 39.5 51.7 - - 45.7 - 30.4 38.0 38.6 19.7 32.2 35.9 38.1 26.2 - R1-related (GRPO-tuned with rule-based reward) R1-VL-7B (Zhang et al., 2025a) Vision-R1-7B (Huang et al., 2025) R1-OneVision-7B (Yang et al., 2025) OpenVLThinker-7B (Deng et al., 2025) MM-Eureka-Qwen-7B (Meng et al., 2025) ADORA-7B (Gui & Ren, 2025) ThinkLite-7B-VL (Wang et al., 2025b) VLAA-Thinker-Qwen2.5-7B (Chen et al., 2025a) Qwen2.5-VL-7B-Instruct (Bai et al., 2025) + Vanilla GRPO (n=12) + NoisyRollout (n1=6, n2=6) + Vanilla GRPO (n=12) + NoisyRollout (n1=6, n2=6) 260K+10K 200K+10K 155K+10K 35K+15K 15K 2.1K 11K 126K+25K - Geo3K (2.1K) Geo3K (2.1K) K12 (2.1K) K12 (2.1K) 40.0 52.4 46.1 48.0 50.5 50.1 50.2 49.9 46.2 50.8 53.2 50.7 52.8 24.7 - 22.5 25.0 28.3 27.6 27.6 26. 25.0 27.3 28.5 28.5 28.9 63.8 67.7 74.9 64.4 72.3 71.4 74.8 59.8 63.1 63.5 73.5 63.9 71.5 71.5 71.1 72.7 68.8 67.5 70.5 72.6 71.7 72.9 69.0 - - - - - - - - - - 62.1 67.8 65.5 67.1 69.2 67.9 63.1 67.4 69.6 68.6 71.9 - - - - - - - - - - - 65.6 70.8 68.3 53.1 71.0 68.6 64.6 (71.2) 69.8 72.1 69.8 70. - - - - - - - - - - - 52.0 56.5 56.8 53.8 58.1 56.4 53.3 57.2 59.2 57.9 59."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Main Results Dataset & Evaluation. We use EasyR1 (Zheng et al., 2025) as our reinforcement learning training framework. Built on top of veRL (Sheng et al., 2024), EasyR1 is specifically designed to support VLMs. For our main experiments, we adopt the processed Geometry3K dataset (Lu et al., 2021) and the K12 dataset (Meng et al., 2025), each containing 2.1K training samples.3 To mitigate issues like reward hacking and model guessing, we convert all questions from multiple choice to free form formats.4 We evaluate model performance along two dimensions. First, we assess out-of-domain generalization across five benchmarks: four visual reasoning benchmarks, including MathVerse (Zhang et al., 2024a), MathVision (Wang et al., 2024), MathVista (Lu et al., 2023), and WeMath (Qiao et al., 2024), as well as one visual perception benchmark, HallusionBench (Guan et al., 2024). Second, we evaluate the in-domain performance of NoisyRollout by comparing it with the vanilla GRPO baseline on the Geometry3K test set. Moreover, we develop an evaluation suite for consistent assessment of our trained checkpoints and most open-source R1-related checkpoints using vLLM (Kwon et al., 2023) for accelerated inference (marked with ), while adopting reported results for others. We employ greedy decoding with Gemini-2.0-Flash-001 (Gemini Team et al., 2023) as the judge to parse generated responses. While we carefully adopt the system prompts from respective codebases or papers, minor inconsistencies with reported results remain, which we deem acceptable given possible differences in judge models and evaluation implementations. Implementation details. Following prior work (Yang et al., 2025; Huang et al., 2025; Deng et al., 2025), we initialize our policy model with Qwen2.5-VL-7B-Instruct, which exhibits strong foundational capabilities well-suited for subsequent RL training. In line with prior studies, we remove the KL divergence constraint with the reference model in GRPO to encourage exploration (Liu et al., 2025a), while keeping the vision encoder frozen for training stability and parameter efficiency (Meng 3K12 originally contains approximately 8K examples; we randomly sample 2.1K to ensure consistency with Geometry3K in terms of training scale. 4Frequently observed in our preliminary experiments, these issues might disrupt accurate reward assignment. 5 Figure 3: Comparison of NoisyRollout and vanilla GRPO across in-domain and out-of-domain scenarios with the same total rollout number (12). The X-axis in all plots represents RL training steps. First column: Reward comparison on the in-domain dataset during training. Second and third columns: Comparison on four out-of-domain visual reasoning benchmarks. Last column: Evaluation of perception capabilities, where the upper subplot directly compares their perception performance on HallusionBench and the lower subplot presents the model-ranked BradleyTerry win rates w.r.t. the perception qualities of their reasoning traces during training. et al., 2025). For other RL-related hyperparameters, we adopt the default settings from EasyR1, including global batch size of 128, rollout batch size of 512, rollout temperature of 1.0, and learning rate of 1e6. Training is conducted over 15 episodes and 60 optimization steps (Tmax) on both datasets using 8 A100-40G GPUs. For NoisyRollout-specific configurations, we implement Gaussian noise as our image distortion strategy. The initial Gaussian noise step α0 is set to 500 for Geometry3K and 450 for K12. We empirically set the rollout parameters to n1 = n2 = 6, and apply sigmoid-shaped noise annealing schedule: αt = η(α0, t, Tmax) = α0 1 (cid:18) 1 1 + eλ(t/Tmaxγ) (cid:19) , (3) where γ = 2/3 controls the midpoint of the annealing curve, while λ = 30 determines its steepness. This annealing schedule gradually reduces noise during training, helping to prevent convergence issues while preserving beneficial diversity in the early stages. Figure 6 visualizes the effect of applying different Gaussian noise steps to clean image. Result 1: Out-of-domain generalization. When trained on the Geometry3K dataset, NoisyRollout not only improves in-domain performance (Figure 3, lower left subplot), but more importantly, demonstrates strong out-of-domain generalization. As shown in Table 1, NoisyRollout achieves 59.2% average accuracy across five visual reasoning and perception benchmarks, consistently outperforming the GRPO baseline in every case. This advantage is further illustrated in Figure 3, which presents detailed comparisons across benchmarks as training progresses. Specifically, NoisyRollout achieves 53.2% on MathVerse, 28.5% on MathVision, and 69.6% on WeMath, surpassing all R1-related models and even outperforming GPT-4o, which scores 50.8% on MathVerse and 69.0% on WeMath. Moreover, NoisyRollout effectively mitigates perception degradation when reasoning templates are used: while Qwen2.5-7B-VL-Instructs perception accuracy drops from 71.2% to 64.6%, NoisyRollout not only avoids this degradation but improves performance to 72.1%. The final subplot in Figure 3 confirms NoisyRollout delivers improved perception quality during reasoning,5 suggesting our hybrid rollout strategy enhances perceptual alignment despite training solely on clean images. 5Gemini-2.0-Flash is used as the judge to assess perception quality, following carefully designed instructions and evaluation pipelines, based on 300 samples per benchmark. See Appendix A.3 for details. 6 Table 2: Performance comparison between NoisyRollout and vanilla GRPO on both out-of-domain and in-domain benchmarks under different rollout temperature settings. The total number of rollouts in all experiments is fixed at 12. In vanilla GRPO, n(6) : 1.0, n(6) : 1.2 indicates 6 rollouts with temperature 1.0 and another 6 with temperature 1.2. In NoisyRollout, n1(6) : 1.0 denotes 6 rollouts per sample generated from clean input (I, q) with temperature 1.0, while n2(6) : 1.0 denotes 6 rollouts per sample from noisy input ( I, q) with temperature 1.0. Geo3K represents the test set of Geometry3K dataset. The best value in each column is bold, and the second best is underlined. Geo3K MathVerse MathVision MathVista WeMath HallusionBench Avg. Temperature Method Vanilla GRPO n(12) : 0.8 n(12) : 1.0 n(12) : 1.1 n(12) : 1.2 n(12) : 1.4 n(6) : 1.0, n(6) : 1.2 NoisyRollout n1(6) : 1.0, n2(6) : 1.0 n1(6) : 1.2, n2(6) : 1. 50.1 51.4 50.4 53.2 51.4 50.8 54.9 53.4 50.5 50.8 50.2 51.2 50.6 50.7 53.2 52.6 26.7 27.3 27.7 27.1 25.8 26.8 28.5 28. 69.9 70.5 70.4 69.3 70.1 70.1 72.6 72.9 65.8 67.4 68.1 68.3 69.0 67.4 69.6 70.9 70.1 69.8 69.4 70.9 69.6 68.2 72.1 70. 55.5 56.2 56.0 56.7 56.1 55.7 58.6 58.2 Reasoning Template from EasyR1 SYSTEM: You FIRST think about the reasoning process as an internal monologue and then provide the final answer.The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in boxed{}. USER: {question} SYSTEM: You are helpful assistant USER: {question}. Answer yes or no directly. Direct-Answer Template Result 2: Sample efficiency. NoisyRollout demonstrates exceptional data efficiency by generalizing with only 2.1K training samples, whereas comparable models require significantly more data and additional SFT as warm-up training. For example, MM-Eureka-Qwen-7B uses 15K RL samples yet achieves only 56.8% average score, while R1-OneVision-7B needs 155K SFT samples and 10K RL samples but reaches just 52.0%. This efficiency stems from NoisyRollouts use of noisy training signals that foster broader exploration during RL, enabling effective generalization from limited samples. Result 3: Robustness across training datasets. NoisyRollout maintains strong generalization performance across diverse training sources. When trained on the K12 dataset, it achieves 59.5% average score, again surpassing the GRPO baseline (57.9%). These results demonstrate NoisyRollouts robustness across varying training sources and evaluation settings, confirming that its approach to reinforcement learning produces models with superior generalization capabilities for visual reasoning tasks. Result 4: Compatible with Dr. GRPO. Liu et al. (2025a) recently identified an optimization bias in GRPO that artificially increases response length. We evaluate NoisyRollout with their proposed Dr. GRPOs unbiased objective and, as shown in Table 3, NoisyRollout maintains consistent improvements over the baseline, demonstrating that our hybrid rollout strategys benefits extend beyond the standard GRPO method. Table 3: Performance comparison when using Dr. GRPO for optimization. OOD Avg. represents average accuracy (%) across five out-of-domain visual reasoning and perception benchmarks. Method Geometry3K OOD Avg. Qwen2.5-VL-7B-Instruct + Dr. GRPO + NoisyRollout (Dr. GRPO) 39.4 51.3 56.1 53.3 57.0 58.9 3.2 Ablation Study: More Effective Rollout Diversity with Noisy Trajectories Setup. We conduct ablation studies using Geometry3K as our training dataset (also applicable to the next subsection). In this part, we aim to examine the effectiveness of our NoisyRollout from the perspective of rollout diversity, key factor for effective policy exploration in RL training. We define rollout diversity as the average pairwise cosine distance between trajectory embeddings, where higher values indicate greater diversity. We randomly sample 256 instances from the Geometry3K training set. For each sample, we generate either = 12 trajectories in vanilla GRPO or combination of Figure 4: Comparison of accuracy and diversity metrics (%) across RL training steps (0 to 40). The left two subfigures contrast NoisyRollout versus vanilla GRPO (both with temperature 1.0), while the right two demonstrate the effects of different temperature settings (0.8, 1.0, 1.2) on vanilla GRPO. Table 4: Ablation study on total rollouts and noise steps. Left: Vanilla GRPO with total rollout versus our NoisyRollout with split rollouts n1 and n2. Right: Impact of initial noise steps on NoisyRollout performance. OOD Avg. reports average accuracy (%) across five out-of-domain visual reasoning and perception benchmarks. Best value per column is bold, second best is underlined. Rollout Geometry3K OOD Avg. Noise Step Geometry3K OOD Avg. = 8 = 12 = 16 = n1 = n2 = 4 n1 = n2 = 6 n1 = n2 = 8 49.6 51.4 54.7 53.6 51.7 54.9 54.7 56.8 57.2 57.5 57.3 58.3 59.2 58.6 0 100 300 400 500 550 51.4 52.7 53.4 54.6 54.9 39."
        },
        {
            "title": "Diverged",
            "content": "57.2 57.4 57.7 58.1 59.2 57.7 n1 = 6 and n2 = 6 trajectories in NoisyRollout, then encode them with an embedding model6. We track both diversity and accuracy across training steps (Figure 4) and evaluate final performance on in-domain and out-of-domain benchmarks  (Table 2)  . Results. We use vanilla GRPO with rollout temperature of 1.0 as the control group. As shown in Figure 4, introducing noisy trajectories in NoisyRollout produces similar effect to increasing the rollout temperature in vanilla GRPO (e.g., from 1.0 to 1.2): initial prediction accuracy on training samples decreases, but rollout diversity increases, ultimately leading to higher training accuracy in later stages. Moreover, both NoisyRollout and vanilla GRPO with higher temperature (1.2) exhibit similar diversity dynamics compared to the control group diversity is higher in the early stages of training, but drops below that of the control group later on. However, benchmark results in Table 2 show that NoisyRollout with temperature 1.0 consistently outperforms vanilla GRPO across all tested temperatures (0.8 to 1.4), as well as the mixed-temperature variant (using 6 rollouts at temperature 1.0 and 6 at 1.2). These results suggest that NoisyRollout introduces more effective rollout diversity from visual perspective than simply adjusting temperature parameters, which increases diversity in broad and less targeted way. 3.3 Ablation Study: Impact of Hyperparameters and Module Design Table 5: Ablation study on the noise annealing strategy. Noise annealing. As shown in Figure 5, removing noise annealing causes the in-domain performance of our method to drop sharply around training step 45. This drop is due to divergence caused by distributional mismatchan issue discussed in Section 2 and further illustrated by the training dynamics in the same figure. Additionally, Table 5 shows that disabling noise annealing leads Qwen2.5-VL-7B-Instruct + Vaninlla GRPO + NoisyRollout w.o. Noise Annealing + NoisyRollout Geometry3K OOD Avg. 53.3 57.2 58.0 59.2 39.4 51.4 43.9 54.9 Method 6https://huggingface.co/sentence-transformers/all-MiniLM-L6-v 8 Figure 5: Comparison of NoisyRollout w. and w.o. noise annealing, and vanilla GRPO in terms of training dynamics (policy clip fraction and training reward) and accuracy on the in-domain test set. to lower performance in both in-domain and out-of-domain settings (43.9% and 58.0%, respectively), compared to our standard setting with noise annealing (54.9% and 59.2%). These results further highlight the effectiveness of noise annealing. We defer the discussion of other annealing strategies (e.g., power, exponential) to Appendix A.1. Total rollout number. We analyze the impact of total rollout number by comparing vanilla GRPO and NoisyRollout under varying rollout budgets. As shown in Table 4 (left), increasing the number of rollouts in vanilla GRPO from = 8 to = 16 improves in-domain performance (from 49.6% to 54.7%), but only marginally benefits out-of-domain generalization (from 56.8% to 57.5%). In contrast, NoisyRollout consistently outperforms vanilla GRPO even when the total number of rollouts is held constant. Notably, NoisyRollout with n1 = n2 = 6 (total 12) achieves both higher in-domain (54.9%) and out-of-domain (59.2%) accuracy than vanilla GRPO with 16 rollouts. Initial noise step. We evaluate the impact of noise strength by varying the initial Gaussian noise step, as shown in Table 4 (right). Gradually increasing the initial noise step α0 from 0 to 500 consistently improves performance across all evaluation categories, suggesting that moderate noise promotes exploration and enriches the training signal. However, exceeding this threshold leads to performance degradation, as overly distorted images (see Figure 6) yield noisy rollouts with average near-zero rewards. These excessively noisy samples introduce harmful distribution shifts, ultimately destabilizing the learning process. 3.4 Unsuccessful Attempts Figure 6: Illustration of visual degradation under increasing Gaussian noise steps. During the early development of NoisyRollout, we encountered several unsuccessful trials that are worth documenting. Due to limited computational resources, we could only explore limited set of hyperparameter combinations heuristically. Some of these approaches might prove effective with further hyperparameter optimization and better design. Noise image condition for both πθold and πθ. We attempted to optimize noisy rollouts based on their corresponding distorted images (rather than conditioning all policy updates on the clean image). This approach failed to show significant improvements over vanilla GRPO. We hypothesize that this strategy contradicts with the GRPOs design, harming the information gain from the group advantage calculation, thus reducing their effectiveness for exploration during RL. Image data augmentation. We explored range of image distortions beyond Gaussian noise, including cropping and rotation. However, these augmentations often led to critical information loss, resulting in rollouts with consistently zero rewards. This caused unreliable policy gradient estimates and ultimately led to training instability and divergence. We also experimented with randomized augmentation, where one distortion (crop, rotation, or Gaussian noise) was randomly selected at each training step. However, this approach failed to improve stability. In contrast, Gaussian noise alone preserved essential visual information while introducing moderate perturbations. We find that such noise can act as beneficial regularizer, injecting useful variability into the inputs. 9 Reward penalty on noisy subgroup. We experimented with applying explicit reward penalties (e.g., 0.1) to all noisy rollouts, aiming to encourage the model to better capture contrastive learning signals. However, this approach quickly led to training divergence. Rather than improving its core reasoning and perception abilities, the policy model learned to distinguish between clean and noisy rollouts. As result, the noisy rollouts rapidly became highly off-policy, since the model could easily identify. This distributional mismatch destabilized training and undermined the learning."
        },
        {
            "title": "4 Related Work",
            "content": "4.1 Large Vision-Language Models VLMs have rapidly evolved to understand and reason with both visual and textual information. These models combine visual encoders with large language models to enable comprehension and inference across modalities. Early VLMs like Flamingo (Alayrac et al., 2022) and BLIP-2 (Li et al., 2023) established foundational integration techniques between vision and language components. The LLaVA series (Liu et al., 2023, 2024b; Li et al., 2024b,a) introduced effective visual instruction tuning methodologies that significantly advanced multimodal capabilities. For mathematical reasoning, specialized approaches like Math-LLaVA (Shi et al., 2024) and MAVIS (Zhang et al., 2024b) have employed mathematical visual instruction tuning to enhance VLMs abilities to interpret and solve mathematical problems in multimodal contexts. Advanced VLMs including GPT-4o (Hurst et al., 2024) and Gemini (Gemini Team et al., 2023) have demonstrated unprecedented general visual understanding through massive pretraining. Mixture-ofExperts approaches in DeepSeek-VL2 (Wu et al., 2024b), Uni-MoE (Li et al., 2025), and MoVA (Zong et al., 2024) improved computational efficiency by selectively activating specialized components based on input characteristics. Meanwhile, unified models like SEED-X (Ge et al., 2024), Chameleon (Team, 2024), Show-O (Xie et al., 2024), and Janus series (Wu et al., 2024a; Ma et al., 2024; Chen et al., 2025c) integrated visual understanding and generation capabilities within single architectures. However, most existing VLMs still lack robust visual reasoning capabilities, especially for tasks requiring sophisticated analysis of visual information combined with complex reasoning. 4.2 Reinforcement Learning in LLMs and VLMs RL has emerged as key methodology for enhancing the capabilities of LLMs and VLMs. Early research primarily focused on Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), which aligned model outputs with human preferences (Achiam et al., 2023). Recent advancements have further demonstrated that RL-based techniques can significantly enhance reasoning abilities. For instance, DeepSeek-R1 (Guo et al., 2025) utilizes rule-based rewards combined with Group Relative Policy Optimization (GRPO) (Shao et al., 2024), whereas Kimi-1.5 (Kimi Team, 2025) employs variant of online policy mirror descent, both methods showing notable improvements in reasoning performance. In the multimodal domain, research on leveraging RL to enhance VLMs reasoning capabilities remains in early stages. Some approaches explore using generative reward models (Zhang et al., 2025b) to enhance VLMs general capability, but these typically require powerful closed-source models for training data generation. Recent work including LMM-R1 (Peng et al., 2025), Vision-R1 (Huang et al., 2025), R1-V (Chen et al., 2025b) and OpenVLThinker (Deng et al., 2025) has applied R1-style reinforcement learning to VLMs in specific subdomains like geometry problems and object counting tasks. MM-Eureka (Meng et al., 2025) further extends large-scale rule-based RL to broader multimodal mathematical reasoning, demonstrating significant performance gains without relying on in-domain training data."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we investigate scaling test-time compute in VLMs via RL. We propose NoisyRollout, simple rollout strategy that introduces rollout diversity through clean and distorted inputs from the visual perspective, enhancing policy exploration during RL training. Without modifying the RL objective, NoisyRollout improves generalization and robustness, achieving state-of-the-art results on multiple benchmarks with minimal training data. In future work, we plan to evaluate NoisyRollout across diverse RL objectives and datasets, and explore adaptive noise scheduling to better accommodate varying training conditions."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. Anthropic. Claude 3.5 sonnet. https://www.anthropic.com, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. https://github.com/UCSC-VLAA/VLAA-Thinking, 2025a. Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/ R1-V, 2025b. Accessed: 2025-02-02. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025c. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards, 2025. URL https://arxiv.org/ abs/2502.01456. Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement, 2025. URL https://arxiv.org/abs/2503.17352. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1437514385, 2024. Lujun Gui and Qingnan Ren. Training reasoning model with dynamic advantage estimation on reinforcement learning. https://github.com/ShadeCloak/ADORA, 2025. Notion Blog. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 11 Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv. org/abs/2501.12599. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024a. URL https://llava-vl.github.io/blog/ 2024-05-10-llava-next-stronger-llms/. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024b. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, and Min Zhang. Uni-moe: Scaling unified multimodal llms with mixture of experts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253, 2024a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024b. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025a. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025b. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. 12 Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, and Yujiu Yang. Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics. arXiv preprint arXiv:2501.04686, 2025. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation, 2024. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. Qwen. Qvq: To see the world with wisdom. https://qwenlm.github.io/blog/ qvq-72b-preview/, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Haonan Wang, Chao Du, and Tianyu Pang. V1: Toward multimodal reasoning by designing auxiliary tasks, 2025a. URL https://v1-videoreasoning.notion.site. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement, 2025b. URL https://arxiv.org/abs/2504.07934. 13 Haoran Wei, Youyang Yin, Yumeng Li, Jia Wang, Liang Zhao, Jianjian Sun, Zheng Ge, Xiangyu Zhang, and Daxin Jiang. Slow perception: Lets perceive geometric figures step-by-step. arXiv preprint arXiv:2412.20631, 2024. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024a. Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024b. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Jingyu Wang, and Wenbing Tao. Perception-r1: Pioneering perception policy with reinforcement learning, 2025a. URL https: //arxiv.org/abs/2504.07954. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025b. Weihao Zeng, Yuzhen Huang, Lulu Zhao, Yijun Wang, Zifei Shan, and Junxian He. B-star: Monitoring and balancing exploration and exploitation in self-taught reasoners. arXiv preprint arXiv:2412.17256, 2024. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025a. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024a. Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv e-prints, pp. arXiv2407, 2024b. Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, et al. Mm-rlhf: The next step forward in multimodal llm alignment. arXiv preprint arXiv:2502.10391, 2025b. Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/ hiyouga/EasyR1, 2025. Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Additional Ablation Study on the Strategies of Noise Annealing On the Geometry3K training dataset, we further examine the impact of different noise annealing schedules on NoisyRollouts performance by comparing our default sigmoid strategy with power (αt = α0 (1 t/Tmax)p, = 3.0) and exponential (αt = α0 γt/Tmax, γ = 0.98) decay functions. As shown in Table 6, all three strategies enable NoisyRollout to outperform the vanilla GRPO baseline. However, the sigmoid schedule consistently achieves the highest performance, reaching an average score of 58.6%, compared to 57.1% for power decay and 57.0% for exponential decay. The superior performance of the sigmoid schedule likely stems from its characteristic slow-fast-slow decay profile. This shape maintains higher noise levels early on to encourage exploration, rapidly decreases noise during the main convergence phase. This contrasts with the monotonic nature of power and exponential decays, suggesting that the specific profile of noise reduction significantly influences the balance between exploration and stability, thereby impacting the final model effectiveness. Table 6: Ablation study on the strategies of noise annealing. Avg. denotes the average accuracy across all six visual reasoning benchmarks. Best value per column is bold, second best is underlined. Geo3K MathVerse MathVision MathVista WeMath HalluBench Avg. Method Qwen2.5-VL-7B-Instruct + GRPO + NoisyRollout (Power) + NoisyRollout (Exponential) + NoisyRollout (Sigmoid) 39.4 52. 52.2 51.9 54.9 46.2 50.8 52.1 52.6 53.2 25.0 27.3 26.4 27.7 28.5 67.5 70. 72.0 70.5 72.6 63.1 67.4 68.7 70.1 69.6 64.6 69.8 71.0 69.1 72.1 51.0 56. 57.1 57.0 58.6 A.2 Sensitivity to Training Data Variations To evaluate the generalization of our findings, we first examine the performance consistency of NoisyRollout when using different random seeds for data sampling during training on the Geometry3K dataset. We compare these results against vanilla GRPO trained with the same seeds to verify that the improvements offered by NoisyRollout are robust and not merely an artifact of specific data ordering or subset selection  (Table 7)  . We then assess the generalizability of NoisyRollout by applying it to different training dataset, K12. This experiment tests whether the benefits of our noisy training strategy extend beyond the Geometry3K domain. Furthermore, within this K12 training setup, we investigate the models sensitivity to varying initial noise steps to understand how the optimal noise level might depend on the dataset characteristics  (Table 8)  . Table 7: Performance comparision of NoisyRollout and vanilla GRPO across different data seeds. Avg. denotes the average accuracy across all six benchmarks. Method Seed Geo3K MathVerse MathVision MathVista WeMath HalluBench Avg. GRPO NoisyRollout GRPO NoisyRollout GRPO NoisyRollout 1 2 52.0 54.9 51.9 54.1 51.9 54.1 50.1 53.2 51.5 53.6 50.5 52. 27.6 28.5 27.8 26.6 27.2 27.3 69.4 72.6 70.7 72.9 71.3 72. 66.6 69.6 68.1 69.7 68.6 69.6 69.4 72.1 69.4 71.4 68.6 70. 55.9 58.6 56.6 58.1 56.4 57.7 Table 8: Performance of NoisyRollout on the K12 dataset across different distortion levels. Avg. denotes the average accuracy across all six benchmarks Method Noise Step Geo3K MathVerse MathVision MathVista WeMath HalluBench Avg. Qwen2.5-VL-7B-Instruct N/A NoisyRollout 0 300 400 450 500 39.4 39.6 39.9 39.8 43.9 41. 46.2 50.7 51.4 50.3 52.8 50.2 25.3 28.5 28.4 28.2 28.9 27.4 67.5 71.7 72.5 71.8 72.9 73. 63.0 68.6 69.5 70.3 71.9 71.4 64.6 69.8 70.5 70.8 70.7 71.3 51.0 54.8 55.4 55.2 56.9 55. 15 A.3 Evaluating the Perception Quality of Reasoning Traces Visual Information Extraction Prompt Extract all visual perception and information recognition components from the following reasoning trace. Original question: {question} Reasoning trace: {reasoning} Your task is to extract and summarize ONLY the parts that relate to visual perception, information extraction, and understanding of visual elements from the image. This includes: 1. Any measurements, dimensions, or numerical values extracted from the image 2. Description of visual elements like shapes, objects, positions, or spatial relationships 3. Recognition of text, symbols, diagrams, or graphs from the image 4. Any visual features mentioned or used in the reasoning Format your response with the tag: <visual perception> [Extracted visual information here] </visual perception> Include ONLY visual perception elements, not mathematical reasoning that happens after the information is extracted. If there are no clear visual perception elements, respond with No clear visual perception elements identified. Visual Perception Comparison Prompt Compare the quality of visual perception between two models based on the image and the original question. Original question: {question} Visual perception from Model A: {visual A} Visual perception from Model B: {visual B} Your task is to determine which model better captures and correctly extracts visual information from the image. Compare their visual perception quality based on: Accuracy of visual information extraction (measurements, shapes, relationships) Complete identification of all relevant visual elements Proper recognition of visual information required to solve the problem Score both models and determine the winner: If Model demonstrates significantly better perception than Model B, respond: <result>A</result>; if Model demonstrates significantly better perception than Model A, respond: <result>B</result>; if both models show similar quality of visual perception, respond: <result>tie</result>. Now: 1. identify what visual information is required to solve this problem. 2. analyze how each model perceives this information. 3. provide your comparative judgment with specific reasons. 4. provide your <result> tag with exactly A, B, or tie. To further evaluate the perception quality of models trained with NoisyRollout and vanilla GRPO during reasoning, we perform paired comparison using strong vision-language model7. We sample 300 reasoning traces from the evaluation logs of the models performing visual reasoning on the MathVerse and MathVista benchmarks, forming paired comparisons between NoisyRollout and vanilla GRPO. To isolate visual perception, we extract only the visual components from each reasoning trace using specialized prompt, removing any influence from mathematical reasoning or final answers. To reduce potential position bias in the comparisons, each pair of traces is evaluated twice: once with the NoisyRollout trace shown first and the vanilla GRPO trace second, and once 7Specifically, we use Gemini-Flash-2.0-001 in this experiment. 16 with the order reversed. We combine the results using the Bradley-Terry model to compute win rates. This methodology offers reliable measure focused specifically on visual perception quality during reasoning. The results are presented in Figure 3 (the 8th subfigure). The extraction and evaluation prompts are shown above. A.4 Case Study We present two case studies to demonstrate the improved perception and reasoning capabilities of our NoisyRollout compared to vanilla GRPO, as illustrated in Figure 7 and Figure 8, respectively. Figure 7: Case study showcasing improved perception capability of NoisyRollout compared to vanilla GRPO. 17 Figure 8: Case study illustrating enhanced reasoning capability of NoisyRollout over vanilla GRPO."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Sea AI Lab, Singapore"
    ]
}