{
    "paper_title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
    "authors": [
        "Kanishk Gandhi",
        "Michael Y. Li",
        "Lyle Goodyear",
        "Louise Li",
        "Aditi Bhaskar",
        "Mohammed Zaman",
        "Noah D. Goodman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research. Proposing theories, designing experiments to test them, and then revising them based on data are fundamental to scientific discovery. Despite the significant promise of LLM-based scientific agents, no benchmarks systematically test LLM's ability to propose scientific models, collect experimental data, and revise them in light of new data. We introduce BoxingGym, a benchmark with 10 environments for systematically evaluating both experimental design (e.g. collecting data to test a scientific theory) and model discovery (e.g. proposing and revising scientific theories). To enable tractable and quantitative evaluation, we implement each environment as a generative probabilistic model with which a scientific agent can run interactive experiments. These probabilistic models are drawn from various real-world scientific domains ranging from psychology to ecology. To quantitatively evaluate a scientific agent's ability to collect informative experimental data, we compute the expected information gain (EIG), an information-theoretic quantity which measures how much an experiment reduces uncertainty about the parameters of a generative model. A good scientific theory is a concise and predictive explanation. Therefore, to quantitatively evaluate model discovery, we ask a scientific agent to explain their model and then assess whether this explanation enables another scientific agent to make reliable predictions about this environment. In addition to this explanation-based evaluation, we compute standard model evaluation metrics such as prediction errors. We find that current LLMs, such as GPT-4o, struggle with both experimental design and model discovery. We find that augmenting the LLM-based agent with an explicit statistical model does not reliably improve these results."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 0 4 5 1 0 . 1 0 5 2 : r BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery Kanishk Gandhi Michael Y. Li Lyle Goodyear Louise Li Aditi Bhaskar Mohammed Zaman Noah D. Goodman Stanford University Abstract Understanding the world and explaining it with scientific theories is central aspiration of artificial intelligence research. Proposing theories, designing experiments to test them, and then revising them based on data are fundamental to scientific discovery. Despite the significant promise of LLM-based scientific agents, no benchmarks systematically test LLMs ability to propose scientific models, collect experimental data, and revise them in light of new data. We introduce BoxingGym , benchmark with 10 environments for systematically evaluating both experimental design (e.g., collecting data to test scientific theory) and model discovery (e.g., proposing and revising scientific theories). To enable tractable and quantitative evaluation, we implement each environment as generative probabilistic model with which scientific agent can run interactive experiments. These probabilistic models are drawn from various real-world scientific domains ranging from psychology to ecology. To quantitatively evaluate scientific agents ability to collect informative experimental data, we compute the expected information gain (EIG), an information-theoretic quantity which measures how much an experiment reduces uncertainty about the parameters of generative model. good scientific theory is concise and predictive explanation. Therefore, to quantitatively evaluate model discovery, we ask scientific agent to explain their model and then assess whether this explanation enables another scientific agent to make reliable predictions about this environment. In addition to this explanation-based evaluation, we compute standard model evaluation metrics such as prediction errors. We find that current LLMs, such as GPT-4o, struggle with both experimental design and model discovery. We find that augmenting the LLM-based agent with an explicit statistical model does not reliably improve these results.1 To understand system, you must perturb it. George Box (ad sensum)"
        },
        {
            "title": "1 Introduction",
            "content": "Helping humans understand the world (and themselves) by discovering scientific theories has been foundational goal of artificial intelligence research [25]. Proposing theories about the world, conducting experiments to test them, and revising them based on evidence is central to this process [6]. Recent advances in artificial intelligence, particularly with large language models (LLMs), have shown promising potential for accelerating scientific discovery. LLMs have extensive scientific knowledge [2], strong inductive reasoning capabilities [38, 49], and the ability to write statistical models of data [21, 22]. These promising results suggest that LLMs, functioning as autonomous agents, could be well-suited for two key components within the scientific discovery pipeline: Equal Contribution. Corresponding author: kanishk.gandhi@stanford.edu 1Project: https://github.com/kanishkg/boxing-gym/tree/v0.1.0-beta 1 Figure 1: Overview of BoxingGym. The BoxingGym Framework is designed to holistically evaluate experimental design and model discovery capabilities in the spirit of George Box [6]. 1) The process starts with user defining goal for the scientist agent. 2) The scientist formulates theory. 3) This theory guides the experimental design, where the scientist interacts with simulated world to gather new data. 4) The scientist then analyzes the new and old data to propose and refine theories. This iterative process continues for several iterations. 5) The scientist is then asked to explain the findings to novice. 6) We evaluate the novice and the scientist by casting the goal as prediction problem. experimental design (i.e., collecting informative experiments to test scientific theories) and model discovery (i.e., developing interpretable, parsimonious models based on experimental data). Previous work has evaluated automated experimental design and model discovery in isolation [13, 14, 15, 21]. However, in realistic settings, they are tightly coupled: scientists collect experimental data to build better models and better models help scientists design more informative experiments. While autonomous scientific agents are an exciting prospect, there is currently no systematic way to evaluate an agents ability to propose scientific models, collect experimental data, and revise them in light of new data. This motivates the need for benchmark that evaluates an agents capabilities within an integrated scientific discovery pipeline. We now outline the key desiderata for framework for evaluating experimental design and model discovery: (1) Our framework should enable the agent to actively experiment with the environment without requiring the agent to perform time-consuming and resource-intensive real-world lab experiments. (2) Since scientific theories come in different forms, our framework should flexibly accommodate different representations of scientific theories. (3) Our framework should evaluate experimental design and model discovery in an integrated way. (4) Science is often goal-directed or driven by particular inquiry. For example, biologist might perform experiments with the goal of identifying cellular mechanisms underlying circadian rhythm in mammals. Our framework should allow users to specify high-level goals to guide the agents discovery process. Our desiderata are inspired by the conceptual framework for scientific modeling introduced by George Box [7, 8], which emphasizes an iterative process of building models, designing experiments to test them, and revising models based on the results. To achieve these desiderata, we introduce BoxingGym  (Fig. 1)  flexible framework for evaluating experimental design and model discovery with autonomous agents. Our benchmark consists of 10 environments grounded in real-world scientific models. To enable agents to actively experiment, we implement each environment as generative model. This key design choice makes simulating active experimentation tractable because it corresponds to sampling from the underlying generative model, conditioned on the experimental interventions. To accommodate various representations of scientific theories, all environments are designed with flexible language based interface  (Fig. 2)  . Finally, our environments can be instantiated with different goals, or intents for inquiry, that encourage the agent to adapt their experimentation towards accomplishing the goal (e.g., understand the parameters underlying participant behavior in psychology study) by specifying the goal in language. We introduce principled evaluation metrics that measure the quality of experiments and discovered models. To evaluate experimental design, we draw from Bayesian optimal experimental (BOED) design [39] and use expected information gain (EIG) to measure the informativeness of an experiment. EIG captures how much an experiment reduces uncertainty in the parameters of generative model and, importantly, this measure complements our decision to implement environments as generative models. To evaluate model discovery, we take inspiration from the fact that science is communicative endeavor. We propose communication-based evaluation strategy: we ask scientist agent to distill their experiments into natural language explanation and evaluate how much that explanation empowers novice agent, who does not have access to the experiments conducted by the scientist, to make accurate predictions about the environment. Finally, as initial steps, we implement and evaluate two baseline agents: (1) purely language-based agent (2) language agent that builds statistical models of the data and uses the model to aid their predictions [21, 22]. These tasks are challenging for both agents and highlight fertile ground for further research."
        },
        {
            "title": "2 Related Works",
            "content": "Optimal Experimental Design. Bayesian optimal experimental design (BOED) is principled framework for designing maximally informative experiments with successful applications in wide variety of disciplines [10, 31, 45]. Although the framework is appealing, implementing BOED in practice is challenging because it involves computing information gain metrics, such as expected information gain (EIG), which are often intractable. There are number of approaches to approximating the EIG of design. Rainforth et al. [39] propose nested Monte Carlo estimator and Foster et al. [14] estimate the EIG using variational approximation. Subsequent work has scaled BOED to sequential settings by learning an amortized design network [15]. While these methods are significant technical innovations, they typically assume that the data is well-approximated by an assumed generative model. However, in many practical settings, we also want to revise our model as we collect new data. Automated Model Discovery. Automated model discovery from data has been long-standing goal in AI, with the aim of building interpretable and explainable models that capture underlying patterns and relationships in data. Researchers have developed systems for discovering physical laws [5, 26], performing nonparametric regression [13], and conducting unsupervised learning [18]. Recently, Li et al. [21, 22] proposed integrating language models into an automated statistical model discovery pipeline that involves both proposing candidate models and critiquing them, leveraging 3 Figure 2: Python pseudocode examples. (left) BoxingGym is instantiated as modular classes and methods for the environment (WorldEnv), goals (Goal), and agents (Agent). (center) Pseudocode illustrating the workflow of setting goals, performing experiments, predicting outcomes, and providing explanations. (right) An example, hyperbolic temporal discounting, where the agent predicts participants choice between immediate and delayed rewards and explains the concept to novice. their ability to generate symbolic expressions and critique their own reasoning. This work highlights the potential of using language models as powerful tool for model discovery. Reasoning with LLMs. Experimental design and model discovery require both deductive reasoning (e.g., what does given hypothesis entail) and inductive reasoning, (e.g., what might be the underlying hypothesis that explains the data) (c.f., Liu et al. [23]). Qiu et al. [38], Wang et al. [49] show how LLMs can reason inductively to solve abstract pattern based problems from few examples. Other work in deductive reasoning has shown how LLMs [37, 42, 43] can perform logical deduction and identify inconsistencies or contradictions in set of statements. However, these works have primarily been applied to deterministic, rule-based systems rather than the noisy, stochastic data typically seen in scientific discovery. In-context Exploration. The challenge of building accurate models through experimental design requires efficient exploration and information-seeking strategies. Recent research has tried to study this through in-context exploration Nie et al. [32] and Min et al. [27] demonstrate how to optimize in-context exploration, while Monea et al. [29] shows how transformers can learn reinforcement learning policies directly in sequence modelling framework. This work on exploration in language has been complemented by recent research on teaching language models how to search effectively in language [16, 17, 20, 44]. Interactive Environments. BoxingGym takes inspiration from the long-line of research in designing RL environments [9, 28]. In particular, we adapt the modularity and simplicity of these benchmarks. However, the focus of our environments is on evaluation and not training agents. More recently, Jimenez et al. [19] presented an interactive benchmark to test capabilities of languageagents on debugging software issues. Our work builds on this line of research by introducing 4 benchmark for evaluating the ability of language agents to perform iterative model discovery through combination of reasoning and experimentation."
        },
        {
            "title": "3.1 Problem Formulation.",
            "content": "We formalize the experimental design and model discovery problem within the framework of probabilistic modelling and Bayesian optimal experimental design (BOED). In BoxingGym , each environment is implemented as generative model defining joint distribution over the experimental outcome y, experimental design d, and unobserved parameters θ. This joint distribution is characterized in terms of prior distribution over θ, p(θ) and simulator p(yθ, d) which is model of the experimental outcome given parameters θ and design d. For example, θ could be the parameters of some behavioral model of participants in psychology experiment, could be the questions posed to the participants, and could be the participants response to d. Within this framework, running an experiment corresponds to choosing design and observing sample from the marginal predictive distribution conditioned on that design, i.e., p(yd) = Ep(θ)[p(yθ, d)]) 2."
        },
        {
            "title": "3.2 Evaluation",
            "content": "We now discuss how we quantitatively evaluate scientific agents capabilities in 1) experimental design and 2) model discovery. 3.2.1 Evaluating experimental design via Expected Information Gain To quantitatively evaluate experimental design, we take inspiration from the Bayesian OED literature [14, 15]. Crucially, our design choice to implement environments as generative models enables us to leverage this literature. For each domain, we have an underlying predictive model p(yθ, d). We quantify the informativeness of design through the expected information gain (EIG), an information-theoretic quantity measuring the (expected) reduction in uncertainty about the model parameters θ after running an experiment d. EIG(d) = Ep(yd) [H[p(θ)] H[p(θy, d)]] Intuitively, the EIG measures the average informativeness of running design over different possible experimental outcomes y. Unfortunately, the EIG is doubly intractable [30]. The expectation involves marginalizing over θ and, in general, the posterior over the parameters p(θy, d) is also intractable. To approximate the EIG, we therefore use Nested Monte Carlo estimator (cid:33) (cid:32) where θn,m i.i.d. p(θ), yn p(yθ = θn,0, d) ˆµNMC(d) = 1 (cid:88) n= log 1 p(ynθn,0, d) m=1 p(ynθn,m, d) (cid:80)M We chose this estimator because it is consistent (as N, ) estimator of the true EIG with theoretical properties explored by [39] and is straightforward to implement. Crucially, EIG measures the value of an experiment under an assumption about the form of the underlying predictive model. In particular, we assume that the true distribution of experimental outcomes given design is modeled by p(yd). In general, this assumption is not true and EIG may not always be an accurate measure under model misspecification [40]. However, in our benchmark, since we generate data from an underlying model, EIG is still useful measure of informativeness. 2In the sequential setting, we replace the prior p(θ) with the posterior p(θy, d). 5 3.2.2 Evaluating model discovery via communication To evaluate the quality of model, we employ both number of standard model evaluation metrics (e.g., prediction MSE) as well as metric loosely inspired by communication through noisy channel, that takes advantage of the natural language interface. In particular, scientist agent interacts with an environment via experiments. After series of these experiments, we ask the scientist agent to synthesize their findings through an explanation. We then evaluate the extent to which that explanation empowers novice agent to make accurate predictions about the environment without any interaction. Since good explanation is both predictive and parsimonious, we set token limit on the explanation. Crucially, this evaluation method can accommodate different forms of scientific theories. For example, in our experiments, we ask the scientist agent to produce statistical model and then interpret and distill the model into natural language explanation to guide the novice agent. 3.2.3 Evaluating goals via prediction To evaluate success at achieving specific goal (e.g., how do the populations of predator and prey change with time) we employ prediction target (e.g., predict the population of predators at particular time) and calculate standardized prediction error. First, we compute the error between the predicted and true values. Then, we standardize this error with respect to the prior predictive mean, which is obtained by assuming uniform prior over the design space. Specifically, for each domain, we sample design uniformly from the design space and parameter θ from the prior distribution p(θ). We then generate samples from the predictive model p(yθ, d) and average over multiple and θ to obtain the prior predictive mean µ0 and variance σ0. Let {yi}n i=1 be the ground truth outputs for inputs {xi}n i=1 be the predictions of the agent. The standardized prediction error is then calculated using these quantities, providing measure of the agents performance relative to the prior predictive mean. We use domain-specific function computing the discrepancy between prediction ˆyi and ground truth value yi (e.g., MSE). We compute the errors ϵi = ( ˆyi, yi) and ϵµ0 = (µ0, yi). Finally, we compute the standardized error as ϵiϵµ0 σ0 i=1. and let { ˆyi}n ."
        },
        {
            "title": "3.3 Design Decisions in Constructing BoxingGym",
            "content": "We outline the key design decisions of BoxingGym that allow it to capture key aspects of scientific discovery within flexible, simulated, and extensible environment. Discovery via active experimentation. The agent actively interacts with the environment by conducting experiments, reflecting the real-world coupling of experimentation and model discovery. This approach assesses the agents ability to gather relevant data and refine its models based on experimental results. Real-world scientific models. Our environments are grounded in real-world scientific models from various domains, ensuring the benchmark tests the agents ability to handle realistic scenarios. We implement these environment as pymc generative models to make active experimentation tractable. Goal-driven discovery. Each environment has specific goal or an intent of inquiry, mirroring the nature of human scientific research. This encourages the agent to engage in targeted experimentation. 6 Language-based interface for experiments. We use language-based interface for our experiments which enjoys several compelling advantages. First, language-based interface is flexible and general, since scientific domains can be described using natural language. Second, language is natural choice, given that our work is motivated by the promise of LLM-based systems. Third, languagebased interface is interpretable to humans. Emphasis on Measuring Discovery with Explanations. BoxingGym places strong emphasis on measuring the quality of the agents discoveries through the explanations it provides (3.2.2). This design decision is motivated by two considerations. From theoretical perspective, science is fundamentally about developing better theories, and scientific theories are explanations of observed phenomena. From practical perspective, communicating findings to the broader scientific community is an essential aspect of scientific research. By using language, we can avoid committing to particular representation of scientific theory. We illustrate this flexibility, by showing how different representations can be easily integrated within our method for measuring natural language explanations. Extensible/modular environments for benchmarking agents. BoxingGym is easily extensible and modular, enabling researchers to integrate new environments and test different agents with minimal effort. We illustrate this in Fig. 2 which provides pseudo-code example of how to implement new environment and goal in BoxingGym ."
        },
        {
            "title": "3.4 Domains",
            "content": "BoxingGym consists of 10 environments (see App. for full details) that cover range of scientific domains and test different aspects of experimental design and model discovery. Some environments are designed to test optimal experiment design, while others focus on model discovery or involve simulated neuro-symbolic human participants. [15] In an n-dimensional space with signal-emitting sources, the scientist Location finding. measure signals at any grid location. Goals include predicting the signal at any point or locating the sources. Hyperbolic temporal discounting. [15] The scientist observes participants choices for different immediate rewards (ir), delayed rewards (dr), and delay periods (D days) Fig. 2 (right). Goals include predicting choices of participant or discount factors. Death process. [15] disease spreads at certain infection rate. The scientist can measure the number of infected individuals at different points of time to predict future infections or the infection rate. Item Response Theory (IRT). [41] In this environment, there is set of students and set of questions. The experimenter can observe the correctness of students response to particular question. The goal is to discover the underlying model that relates student ability and question difficulty to the probability of correct response. Animal growth curves. age. The goal is to discover the underlying growth model of dugongs. [24] An experimenter can observe the length of dugong at particular 7 Table 1: Performance of GPT-4o Across Different Tasks. Numbers shown are standardized errors. Errors are averaged across 5 runs. Environment Goal Before Experiments After After 10 Experiments Communication Hyperbolic Discounting Location Finding Death Process IRT Dugongs Peregrines Mastectomy Predator-Prey Emotions Choice Discount Signal Source Location Num Infected Infection Rate Correctness Length Population Survival Population Prediction Moral Machines Judgement 0.31 -0. 0.96 1.29 1.19 0.13 0.00 0.06 2.29 0. 0.08 0.74 0.32 0.74 -0.06 1.24 -0.15 0.46 1. 0.00 -0.09 -0.65 0.27 -0.45 0. 0.44 0.74 - 0.97 - 0.75 - -0.28 -0. -0.63 1.00 -0.26 0.87 0.60 Population growth dynamics. [24] An experimenter can observe the population of peregrines at particular point in time. The goal is to discover the underlying population dynamics model. This is tested by asking the experimenter to predict population dynamics at particular point in time. Mastectomy Survival analysis. [11] The experimenter can observe if patient is alive after mastectomy, including metastasis status and time since surgery. The goal is to predict survival probabilities for new patients. [48] This simulates predator-prey populations over time. The goal is to Predator-Prey dynamics. discover models like the Lotka-Volterra equations to predict future populations. Emotion from outcome. [33] Participants guess players emotions after gambling games outcome. The experimenter designs games with varied probabilities and prizes to model how participants judge the emotions of player from outcomes. Human participants are simulated using probabilistic model translated into natural language by language model. Moral Machines. [4] Participants face moral dilemmas, choosing which group an autonomous car should save. Experimenters manipulate group compositions and required actions to model moral decision-making. Human participants are simulated with probabilistic model, and their actions are translated into natural language by language model."
        },
        {
            "title": "4 Experiments with Baseline Agents",
            "content": "We conduct experiments to evaluate the performance of two baseline agents on BoxingGym . Our goal is to assess their ability to perform experimental design and theory building across diverse set of environments. We benchmark two types of agents: standard language model (GPT-4o, OpenAI [34]) and language model augmented with symbolic reasoning capabilities (Boxs Apprentice). 8 Table 2: Performance of Boxs Apprentice Across Different Tasks. Standardized errors shown here. Errors are averaged across 5 runs. Environment Goal Before Experiments After After 10 Experiments Communication Hyperbolic Discounting Location Finding Choice Signal Death Process Num Infected IRT Dugongs Peregrines Mastectomy Correctness Length Population Survival Moral Machines Judgement 0.66 0.99 3.79 0.44 0. 2.71 0.14 0.97 1.17 1.45 -1. -0.12 -0.08 0.04 0.55 0.89 0. 1.18 0.58 -0.08 -0.09 0.97 0. 0.56 LLM Agent. We prompt an LLM, GPT-4o, to interact with our environment directly, purely through natural language, without any additional functionalities (see Fig. 2, see App. for details). Boxs Apprentice. Boxs Apprentice is an agent that augments the statistical modelling capabilities of the language model (e.g., GPT-4o) by allowing it to write and fit generative model to observed data and use them to make predictions [21] (see App. for implementation details). This allows Boxs Apprentice to build explicit representations of the underlying phenomena in each environment. In particular, for the model discovery experiments, we ask the LM agent to write pymc program given the collected datapoints after 10 experiments. We then fit this generative model and provide the implementation of the model and its inferred parameters to the scientist producing the explanation for the novice agent. For the experimental design experiments, we ask the LM agent to write pymc program given the collected datapoints so far. The agent then uses this model to guide its subsequent experiments. Experiment Setup. For each environment, we run the agents for 5 independent trials. At each step, the agent chooses to perform an experiment, by specifying design, and observes the outcome. After fixed number of steps (0, 1, 3, 5, 7, 10), we evaluate the agents performance using the metrics described earlier 3.2. The values reported in Tab. 1 and Tab. 2 are averaged across the 5 runs and over 10 evaluation points. We also explore prior vs no prior conditions. In the prior condition, we give the LM full context about the problem domain (e.g., you are observing how participants balance delayed vs immediate rewards) . In the no prior condition, we remove this context and describe the setting in domain-agnostic way (e.g., you receive tuple of three values)."
        },
        {
            "title": "4.1 Experimental Design Evaluation",
            "content": "To evaluate the agents performance, we first assess their ability to gather valuable information through their experiment selection and then measure how effectively they use this information to predict the environment. The Expected Information Regret (EI Regret), key metric, compares the Expected Information Gain (EIG) (3.2.1) of the agents chosen experiments to the maximum EIG achievable from 100 random experiments. Lower EI Regret indicates more informative experiment selection. To verify if agents can use observational data to make predictions, we measure their standardized prediction error about the environment before and after experimentation. 9 Figure 3: Standardized errors compared. We plot the standardized errors for the two agents, gpt-4o (solid line) and Boxs Apprentice (dashed line) across three domains: Peregrines (left), Hyperbolic Discounting (center) and IRT (right). Error bars are 95% CIs across 5 runs. We present our findings3 in Tab. 1, Tab. 2 and Fig. 3 (see Tab. 3 for EI regrets). We observe that in few environments, the LLM agent possesses accurate prior knowledge, and in these cases, experimentation can occasionally lead to decline in performance as the language model overcorrects based on limited number of observations (see hyperbolic discounting; Tab. 1). For instance, in the hyperbolic discounting environment (Fig. 3, center), the error after experimentation is higher than before conducting experiments. The LM agent excels at integrating observations for the Peregrines and Predator-Prey environments, as indicated by the negative standardized error. However, for most other environments, its performance does not significantly improve with experimentation and remains suboptimal, as evidenced by the positive standardized error. Although Boxs Apprentice can propose and fit pymc model to the observed data, it fails to systematically improve its predictions consistently after additional observations. For example, it outperforms the pure LLM agent in the death process environment (-1.02 compared to 0.46 for the LLM agent), but is worse at Peregrines (0.04 compared to -0.65 for the LLM agent). We observe that one common failure mode is that, because the number of datapoints is relatively small, Boxs Apprentice has consistent bias towards simple functional forms. For example, to model the population count of Peregrines, Boxs Apprentice starts with simple linear model, and after collecting more datapoints, opts to parameterize the Poisson rate as linear function of the year, which is restrictive model. In some cases, Boxs Apprentice identifies program that is very close to the ground truth program; for example, in the IRT domain, Boxs Apprentice proposes logistic model that incorporates student abilities and question difficulties as features. This is also the case for the Location Finding domain; Boxs Apprentice comes up with inverse square functional form (App. Fig. 8). However, as its high prediction error indicates, it is not able to strategically collect datapoints that are useful for fitting an accurate probabilistic model in the low data regime. This is consistent with our EIG results in App. Tab. 3 showing that the scientist agents experiments attain high regret relative to random baseline."
        },
        {
            "title": "4.2 Model Discovery Evaluation via Communication",
            "content": "Next, we evaluate the agents ability to build and communicate models that capture the underlying phenomena in each environment. To test this, we have the agents interact with the environment for 10 steps (scientist phase) and then generate natural language explanation of their findings. We then provide this explanation to novice agent, which must make predictions about the environment 3We omit the predatory-prey and Emotions domains for Boxs Apprentice, since GPT-4o could not reliably produce pymc programs 10 without any direct interaction (novice phase by using the explanation from the scientist; 3.2.2). We consider two conditions: 1) Both the scientist and novice agents receive brief prior description of the environment. 2) Neither the scientist nor the novice receives any prior information. The scientists prediction after 10 observations (Error After Experiments) acts as weak positive control, and the scientists predictions without any observations (Error Before Experiments) acts as negative control. Ideally, if the scientists explanation is effective, the novices error should be lower than the negative control and approach the positive control. For Boxs Apprentice, we provide the scientist with the statistical model and ask it to use both the statistical model and its experiments to produce an explanation. The results in Tab. 1 and Tab. 2 show that the explanations generated by both agents generally improve the novices predictions, but both agents struggle across different environments. Despite having access to an explicit generative model, Boxs apprentice does not always produce better explanation. In some cases, this is because generative model does not enable the scientist to draw any additional useful conclusions as the agents prior knowledge for the domain is already substantial. For example, in the IRT domain, the agent summarizes the parameters of the explicit statistical model as revealing that question difficulty and student ability are both related to students likelihood of answering question correctly (see App. Fig. 10). We find that purely natural language based agent already understands this about the environment and can also produce similar explanation, as indicated by the language agents performance on IRT compared to Boxs Apprentice (App. Fig. 9). Surprisingly, for IRT, we find that having to communicate an explanation to the novice agent forces the LLM agent to generalize its findings, allowing the novice agent to achieve significantly lower error compared to the scientist (-0.28 compared to 0.00 for the LLM agent). In general, there is gap between the novices performance and the positive control, showing that learning from an agent-generated explanation is not as informative as performing interactive experiments directly with the environment. As surprising exception, for IRT, we find that having to communicate an explanation to the novice agent forces the LLM agent to generalize its findings, allowing the novice agent to achieve significantly lower error compared to the scientist (-0.28 compared to 0.00)."
        },
        {
            "title": "5 Discussion",
            "content": "We introduced BoxingGym , benchmark for measuring experimental design coupled with model discovery with language interface. BoxingGym includes 10 environments, each based on realworld generative models, that allow us to systematically compare different language-based agents. To evaluate experimental design capabilities, we used information gain metrics to approximate the effectiveness of an agents active experimentation. For model discovery and communication, we introduced novel metric based on the agents ability to explain its discovered model to novice agent, who then makes predictions about the environment. Our experiments with two types of agentsa standard language model and language model augmented with statistical modelingdemonstrate this frameworks potential to drive progress. Results with our baseline agents reveal significant challenges: proposing hypotheses from few observations, designing experiments from hypotheses, and incorporating new observations with prior knowledge to revise them. While BoxingGym is valuable testbed, it has limitations. The environments in BoxingGym offer pre-defined experimental paradigms, whereas real-world science involves designing the experimental method itself; deciding variables to measure, conditions to compare, and controlling for confounds. Future research could develop environments requiring agents to design experiments from scratch [12]. Additionally, BoxingGym does not consider the time and cost of experiments. 11 However, scientists work within resource constraints and decide the number of experiments based on time, cost, and information gain. Future versions could incorporate these costs, allowing agents to choose the number of experiments or evaluate informativeness normalized by cost (e.g., [47]). Lastly, while BoxingGym covers various scientific domains, it is still limited compared to the full breadth of scientific inquiry. Expanding the benchmark to include more diverse fields, such as biology, chemistry, and social science, would provide more comprehensive test. One unique aspect of BoxingGym is the inclusion of environments that involve human behavior, such as the Moral Machines and Emotions environments. These domains require the agent to reason about psychological factors and social dynamics, which are crucial for many real-world scientific applications. Future work could incorporate more sophisticated simulations of human participants [1, 3, 35, 36, 46], to create more realistic simulations of experiments in psychology and the social sciences. Our experiments showcased the potential of interfaces, such as access to statistical modelling libraries, that augment language models reasoning capabilities for experimental design and discovery. However, there is still much room for improvement in the design of interfaces that help model understand an environment and the data produced better. Future research in designing agents could explore new interfaces that leverage data visualization techniques, strategic simulations based on prior theories, model validation [22], and even web-based research strategies, to enhance the models ability to help guide experimentation and discovery [21, 51]."
        },
        {
            "title": "References",
            "content": "[1] G. V. Aher, R. I. Arriaga, and A. T. Kalai. Using large language models to simulate multiple humans and replicate human subject studies. In International Conference on Machine Learning, pages 337371. PMLR, 2023. [2] M. R. AI4Science and M. A. Quantum. The impact of large language models on scientific discovery: preliminary study using gpt-4, 2023. [3] L. P. Argyle, E. C. Busby, N. Fulda, J. R. Gubler, C. Rytting, and D. Wingate. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3):337351, 2023. [4] E. Awad, S. Dsouza, R. Kim, J. Schulz, J. Henrich, A. Shariff, J.-F. Bonnefon, and I. Rahwan. The moral machine experiment. Nature, 563(7729):5964, 2018. [5] J. C. Bongard and H. Lipson. Automated reverse engineering of nonlinear dynamical systems. Proceedings of the National Academy of Sciences, 104:9943 9948, 2007. [6] G. E. Box. Science and statistics. Journal of the American Statistical Association, 71(356):791799, 1976. [7] G. E. P. Box. Sampling and Bayes Inference in Scientific Modelling and Robustness. Journal of the Royal Statistical Society. Series (General), 143(4):383430, 1980. ISSN 00359238. [8] G. E. P. Box and W. G. Hunter. Useful Method for Model-Building. Technometrics, 4:301318, 1962. [9] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. 12 [10] K. Chaloner and I. Verdinelli. Bayesian Experimental Design: Review. Statistical Science, 10(3):273 304, 1995. doi: 10.1214/ss/1177009939. URL https://doi.org/10.1214/ss/ 1177009939. [11] D. R. Cox. Analysis of survival data. Chapman and Hall/CRC, 2018. [12] M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch, and S. Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. Advances in neural information processing systems, 33:1304913061, 2020. [13] D. Duvenaud, J. Lloyd, R. Grosse, J. Tenenbaum, and G. Zoubin. Structure discovery in nonparametric regression through compositional kernel search. In S. Dasgupta and D. McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 11661174, Atlanta, Georgia, USA, 1719 Jun 2013. PMLR. [14] A. Foster, M. Jankowiak, E. Bingham, P. Horsfall, Y. W. Teh, T. Rainforth, and N. Goodman. Variational bayesian optimal experimental design. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [15] A. Foster, D. R. Ivanova, I. Malik, and T. Rainforth. Deep adaptive design: Amortizing sequential bayesian experimental design. In Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 1824 Jul 2021. [16] K. Gandhi, D. Sadigh, and N. D. Goodman. Strategic reasoning with language models. arXiv preprint arXiv:2305.19165, 2023. [17] K. Gandhi, D. Lee, G. Grand, M. Liu, W. Cheng, A. Sharma, and N. D. Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024. [18] R. B. Grosse. Model selection in compositional spaces. 2014. [19] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. R. Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66. [20] L. Lehnert, S. Sukhbaatar, D. Su, Q. Zheng, P. Mcvay, M. Rabbat, and Y. Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping. arXiv preprint arXiv:2402.14083, 2024. [21] M. Y. Li, E. B. Fox, and N. D. Goodman. Automated Statistical Model Discovery with Language Models. In International Conference on Machine Learning (ICML), 2024. [22] M. Y. Li, V. Vajipey, N. D. Goodman, and E. B. Fox. Critical: Critic automation with language models, 2024. URL https://arxiv.org/abs/2411.06590. [23] E. Liu, G. Neubig, and J. Andreas. An incomplete loop: Deductive, inductive, and abductive learning in large language models. arXiv preprint arXiv:2404.03028, 2024. [24] M. Magnusson, P. Bürkner, and A. Vehtari. posteriordb: set of posteriors for Bayesian inference and probabilistic programming, Oct. 2023. 13 [25] J. McCarthy, M. L. Minsky, N. Rochester, and C. E. Shannon. proposal for the dartmouth summer research project on artificial intelligence, august 31, 1955. AI magazine, 27(4):1212, 1955. [26] B. A. McKinney, J. E. Crowe, H. U. Voss, P. S. Crooke, N. Barney, and J. H. Moore. Hybrid grammar-based approach to nonlinear dynamical system identification from biological time series. Phys. Rev. E, 73:021912, Feb 2006. doi: 10.1103/PhysRevE.73.021912. [27] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943, 2021. [28] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. [29] G. Monea, A. Bosselut, K. Brantley, and Y. Artzi. Llms are in-context reinforcement learners. 2024. [30] I. Murray, Z. Ghahramani, and D. J. C. MacKay. Mcmc for doubly-intractable distributions. In Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, UAI06, page 359366, Arlington, Virginia, USA, 2006. AUAI Press. ISBN 0974903922. [31] J. I. Myung, D. R. Cavagnaro, and M. A. Pitt. tutorial on adaptive design optimization. Journal of Mathematical Psychology, 57(3):5367, 2013. ISSN 0022-2496. doi: https://doi.org/ 10.1016/j.jmp.2013.05.005. URL https://www.sciencedirect.com/science/article/ pii/S0022249613000503. [32] A. Nie, Y. Su, B. Chang, J. N. Lee, E. H. Chi, Q. V. Le, and M. Chen. Evolve: Evaluating and optimizing llms for exploration. arXiv preprint arXiv:2410.06238, 2024. [33] D. C. Ong, J. Zaki, and N. D. Goodman. Affective cognition: Exploring lay theories of emotion. Cognition, 143:141162, 2015. [34] OpenAI. Hello, GPT-4. https://openai.com/index/hello-gpt-4o/, 2024. Accessed: 2024-06-04. [35] J. S. Park, L. Popowski, C. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Social simulacra: Creating populated prototypes for social computing systems. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pages 118, 2022. [36] J. S. Park, J. OBrien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 122, 2023. [37] G. Poesia, K. Gandhi, E. Zelikman, and N. D. Goodman. Certified deductive reasoning with language models. arXiv preprint arXiv:2306.04031, 2023. [38] L. Qiu, L. Jiang, X. Lu, M. Sclar, V. Pyatkin, C. Bhagavatula, B. Wang, Y. Kim, Y. Choi, N. Dziri, and X. Ren. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. In The Twelfth International Conference on Learning Representations, 2024. [39] T. Rainforth, R. Cornish, H. Yang, A. Warrington, and F. Wood. On Nesting Monte Carlo Estimators. International Conference on Machine Learning (ICML), 2018. 14 [40] T. Rainforth, A. Foster, D. R. Ivanova, and F. B. Smith. Modern Bayesian Experimental Design. Statistical Science, 39(1):100 114, 2024. doi: 10.1214/23-STS915. URL https://doi.org/10. 1214/23-STS915. [41] G. Rasch. Probabilistic models for some intelligence and attainment tests. ERIC, 1993. [42] A. Saparov and H. He. Language models are greedy reasoners: systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022. [43] A. Saparov, R. Y. Pang, V. Padmakumar, N. Joshi, M. Kazemi, N. Kim, and H. He. Testing the general deductive reasoning capacity of large language models using ood examples. Advances in Neural Information Processing Systems, 36, 2024. [44] J. Schultz, J. Adamek, M. Jusup, M. Lanctot, M. Kaisers, S. Perrin, D. Hennes, J. Shar, C. Lewis, A. Ruoss, et al. Mastering board games by external and internal planning with language models. arXiv preprint arXiv:2412.12119, 2024. [45] B. Shababo, B. Paige, A. Pakman, and L. Paninski. Bayesian inference and online experimental design for mapping neural microcircuits. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/ 2013/file/17c276c8e723eb46aef576537e9d56d0-Paper.pdf. [46] O. Shaikh, V. Chai, M. J. Gelfand, D. Yang, and M. S. Bernstein. Rehearsal: Simulating conflict to teach conflict resolution. arXiv preprint arXiv:2309.12309, 2023. [47] J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning algorithms. In F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012. URL https://proceedings.neurips.cc/paper_files/paper/2012/file/ 05311655a15b75fab86956663e1819cd-Paper.pdf. [48] V. Volterra. Variations and fluctuations of the number of individuals in animal species living together. ICES Journal of Marine Science, 3(1):351, 1928. [49] R. Wang, E. Zelikman, G. Poesia, Y. Pu, N. Haber, and N. D. Goodman. Hypothesis search: Inductive reasoning with language models. In The Twelfth International Conference on Learning Representations, 2024. [50] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [51] J. Yang, C. E. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan, and O. Press. Sweagent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024."
        },
        {
            "title": "A Full Results",
            "content": "See Fig. 4 for the performance of the two baseline agents on peregrines, hyperbolic discounting, and IRT environments for different number of experiments. See Tab. 3 for the EI regrets of the two agents. See Tab. 4, and Tab. 5 for prediction errors across all environments. Figure 4: EIG regrets and standardized errors compared. We plot the standardized errors (top row) and the EIG regrets (bottom row) for the two agents, gpt-4o (solid line) and Boxs Apprentice (dashed line) across three domains: Peregrines (left), Hyperbolic Discounting (center) and IRT (right). Error bars are 95% CIs across 5 runs. LLM Agent The LLM agent provides an easy way for large language model (LLM) to interact with BoxingGym . By tailoring the system message to the specific environment, we can clearly define goals for the LLM, elicit experimental designs from it, make accurate predictions for queries, and generate explanations for novice. This agent class also incorporates simple retry mechanism that allows the LLM to correct its designs if they are initially invalid. Boxs Apprentice We closely follow Li et al. [21]. In particular, to generate candidate, we sample single probabilistic program from the proposal LM, qLM(). For the model discovery experiments, we perform this once after 10 experiments. For the OED experiments, we perform this three times over the course of 10 experiments. In all experiments, we use GPT-4o (gpt-4o-2024-05-13). The proposal LM qLM conditions on ht Σ, natural language instruction synthesizing previous modeling approaches and suggesting new approaches, the previous program zt1, and textual representation of the dataset D. zt qLM(zt1, ht1, D). We run this at temperature of 0.0. Chain-of-thought reasoning, or generating intermediate reasoning steps, improves the performance of LMs [50]. Motivated by this, we instruct qLM to reflect on the properties of the dataset, sketch high-level modeling approach, state the hypotheses that it will address before writing program, and add comments to code. See the system prompt in Figure 5. 16 Env Goal EI Regret (gpt-4o) EI Regret (boxs apprentice) Hyperbolic Discounting Location Finding Death Process Choice Discount Signal Source Location 0.57 / 0.61 0.69 / - 15.3 / 11.8 16.8 / - Num Infected Infection Rate 0.037 / 0.042 0.108 / - IRT Correctness 0.035 / 0.031 Dugongs Peregrines Length Population 0.20 / 0.17 0.26 / 0.38 0.55 / 0.62 - / - 12.6 / 15.3 - / - 0.029 / 0.019 - / - 0.031 / 0.033 0.19 / 0.20 0.25 / 0.66 Mastectomy Survival 0.084 / 0.082 0.079 / 0.075 Predator-Prey Emotions"
        },
        {
            "title": "Population",
            "content": "Prediction Moral Machines Judgement - / - 0.538 / - 0.046 / - - / - - / - 0.045 / - Table 3: EI Regrets for GPT-4o and Boxs Apprentice Across Different Tasks. EI regrets for prior and no prior conditinos are separated by /."
        },
        {
            "title": "D Domains",
            "content": "D.1 Location Finding The location finding environment has hidden signal sources that emit signal. The scientist can makeg measurements of the superimposed signal at various points. The experiment is directly taken from Foster et al. [14]. In table 6, we describe the inputs and outputs of the experiment. We define = 3 signal sources in Rd = R2 space with locations at θk. The number of sources is predefined and is known to the agent. Each source emits signal strength αk. In our implementation, we choose αk to be fixed for all sources. The signal strength decays according to the inverse square lawif an agent measures at point ξ, then the noisy superimposed signal observed will be distributed according to (µ(θ, ξ), σ) where σ is the signal noise, µ(θ, ξ) is the total intensity at point ξ, µ(θ, ξ) = + (cid:88) k=1 αk m+ θk ξ (1) and b, > 0 are constants governing background and maximum signal. Note that unlike Foster et al. [15], we observe the total intensity, not the log total intensity. D.2 Hyperbolic Discounting The hyperbolic discounting domain has two hidden variables (k, α) to describe participants behavior, where each participant is asked to choose between an immediate reward $iR or delayed reward $dR in days. The experiment is outlined in table 7 below. 17 Env Goal Error@0 Error@10 Discovery@10 Hyperbolic Discounting Choice Hyperbolic Discounting Discount 0.310.18 0.960.14 0.740.21 0.950.07 0.740.14 1.00.00 -0.060.00 - -0.060.00 - - - Location Finding Signal Location Finding Source Location Death Process Num Infected Death Process Infection Rate IRT Correctness Dugongs Length Peregrines Population Mastectomy Survival Predator-Prey Population Emotions Prediction"
        },
        {
            "title": "Judgement",
            "content": "0.960.58 1.170.60 1.291.3 - 1.191.09 0.190.96 0.130.37 - 0.000.00 -0.160.26 1.240.96 0.50. -0.15 0.4 - 0.460.76 0.741.14 1.641.12 - 00.11 0.080.32 0.060.12 -0.020.04 -0.090.00 -0.080. 2.291.20 2.211.57 0.180.37 0.000.28 0.080.09 0.730.05 0.740.29 - 0.320.26 - -0.650.03 -0.670. 0.270.19 0.360.27 -0.450.02 -0.430.02 0.820.34 - 0.440.16 - 0.970.72 0.630.71 - - 0.750.75 1.611.60 - - -0.280.26 -0.160.20 -0.080.01 -0.080.01 -0.630.06 -0.660.02 1.000.27 0.210. -0.260.16 -0.400.03 0.870.35 - 0.600.13 - Table 4: Performance of GPT-4o Across Different Tasks. Numbers shown are standardized errors. Errors with prior (top line) and without prior (bottom line) appear on different lines. Errors are averaged across 5 runs. 18 Boxs Apprentice system prompt 1 You are brilliant statistician modeling dataset. 2 Your job is to come up with generative model that explains the true data by writing pymc probabilistic program. 3 Here is description of the dataset {dataset_description} 4 {dataset_text_representation} 5 Here is description of the columns {column_description} 6 If you are in the first round, you will not receive any additional information. 7 However, for the second round and beyond, will give you the model you proposed previously. 8 Please import pymc NOT pymc3! 9 Note that there are differences in the arguments pymc expects. 10 IMPORTANT: do not use sd as an argument use sigma instead! 11 It is crucial that you pass the idata_kwargs argument to pm.sample!! 12 IMPORTANT: Use the variable name \"y_obs\" for the observations when you define it! 13 IMPORTANT: Use the variable name \"y_obs\" for the observations when you define it! 14 IMPORTANT: Index the appropriate column names when grabbing data from observed_data. These column names are indicated in the column description. 15 16 Your answer should follow the template in the following order. 17 1. First, sketch high-level probabilistic program for the data. 18 You will go through multiple rounds of revision. If there's previous program in your context window and list of hypotheses, revise based on this information! Explicitly cite the hypotheses (if there are any) that you address in your sketch. 21 2. After coming up with plan, write your program and add comments to lines of code that address certain hypotheses. 22 ```python 23 import pymc as pm import numpy as np def gen_model(observed_data): # convert observed_data columns to numpy arrays # index the appropriate column names .... rng1 = np.random.default_rng(42) rng2 = np.random.default_rng(314) with pm.Model as model(): # create pm.MutableData object for each non-observation column ...Your code here... # Copy the rest of this code verbatim but remember to have this indented in scope of model()! trace = pm.sample(1000, tune=500, target_accept=0.90, chains=3, cores=1, random_seed=rng1) posterior_predictive = pm.sample_posterior_predictive(trace, random_seed =rng2, return_inferencedata=False) return model, posterior_predictive, trace 38 39 ``` Figure 5: BoxLM system prompt The system prompt for the proposal pLM . We also include some additional instructions on pymc syntax such as wrapping features in MutableData container. 19 19 20 25 26 27 28 29 31 32 33 34 35 37 Env Goal Error@0 Error@10 Discovery@ Hyperbolic Discounting Choice Location Finding Signal Death Process Num Infected IRT Correctness Dugongs Length Peregrines Population Mastectomy Survival 0.66 0.25 0.66 0.25 0.99 0.58 1.18 0.64 1.17 0.14 0.91 0.09 1.45 1.60 0.83 0. 3.79 1.68 -0.90 0.05 -1.02 0.05 -0.61 0.30 0.44 0.36 0.12 0.24 0.12 0.14 0.12 0.14 0.66 0.30 0.74 0.42 1.18 1.12 -0.01 0. 0.58 0.85 0.50 1.26 -0.08 0.39 0.2 0.40 0.26 0.12 0.05 0.10 2.71 0.60 1.62 0.47 0.14 0.41 0.73 0.15 -0.08 0.02 -0.09 0. 0.09 0.005 0.08 0.004 0.04 0.21 0.95 0.86 0.55 0.24 0.64 0.15 0.97 1.38 -0.19 0.79 0.91 0.28 0.27 0.23 Moral Machines Judgement 0.97 0.33 0.89 0.21 0.56 0.18 Table 5: Performance of Boxs Apprentice Across Different Tasks. Standardized errors shown here. Errors with prior (top line) and without prior (bottom line) appear on different lines. Errors are averaged across 5 runs. Parameter Description Superposition of signal sources in d-dim space Model Setup Parameters Num signal sources K, dim of space d, base signal b, max signal m, noise σ Observations Goals Total noisy signal at point of measurement Predicting signal intensity at new points and source locations Table 6: Location Finding In each measurement, we require iR is strictly smaller than dR and all three values have to be positive, because we assume rational participant would always choose higher immediate reward over lower delayed reward. We follow the prior distribution of the latent variables given by Foster et al. [14]: log (4.25, 1.5), α Half ormal(0, 2) (2) where the HalfNormal distribution is normal distribution truncated at 0. For each test, there are three variables in design: iR, dR, and D. We give values to each choice: receiving the immediate reward $iR has value Vi = iR, while receiving the delayed reward $dR in days has value Vd = dR 1+kD . Then, whether each participants chooses the delayed reward in each scenario is characterized as Bernoulli random variable Bernoulli(p) where the probability of choosing the delayed reward is given by 20 Parameter Description Human decision-making in temporal discounting of rewards Model Setup Parameters Params of the discount function (ϵ, mean and std for log k, scale for α) Observations Goals Choice between immediate iR and delayed reward dR at delay Predicting choices and the value of the discount factor Table 7: Hyperbolic Discounting p(X = 1k, α, iR, dR, D) = ϵ + (1 2ϵ)Φ( Vd Vi α ) (3) where Φ is the cumulative distribution function of the standard normal distribution. In our implementation, we set ϵ = 0.01 for all scenarios. D.3 Death Process The death process environment models an infection spreading among healthy population of individuals. The infection rate θ determines how the probability of infection increases over time. The environment is outlined in table 8 below. Parameter Description The spread of an infection over time Model Setup Parameters Pop size , params of the infetion rate (µ, σ, upper and lower bounds) Number of infected individuals at observation time Observations Predicting the number of infected individuals at time and the infection rate Goals Table 8: Death Process In our model, θ is given by the prior distribution outlined in Foster et al. [15]. θ TruncatedNormal(µ = 1, σ = 1, min = 0, max = ) The number of infected individuals at time is distributed as binomial random variable: θ, Binomial(N, η) (4) (5) where η = 1 eθt, and is the population size. We ask the agent to make observations sequentially by giving time > 0 at each step. D."
        },
        {
            "title": "IRT",
            "content": "1PL IRT Model The one parameter IRT (or Rasch) domain models the performance of multiple students on multi-question exams. The binary outcome (whether the student is correct) of studentquestion pair is determined by latent variables governing the students proficiency and the questions difficulty (Figure 2). The agents goal is to predict the outcome of particular student-question pair. 21 Param Description Student performance on multi-question exams Model Setup Parameters Number of students , number of questions Q, student-question pair to predict Observations Goals Outcomes of various student-question pairs Predicting the correctness of student responses to questions Table 9: IRT Model The agent may observe other student-question pairs to view their outcome. Table 9 below details the inputs, outputs, and target for every variation of the IRT model. We define the ability αj of student and the difficulty βk of question k. In our implementation, α and β are standard normals. The outcome Ojk of student on question is determined by Bernoulli trial where the probability of success pjk is determined by the logit function of zjk = αj βk. pjk = 1 1 + ezjk (6) In summary, for given student-question pair, we compute the probability of the student getting the question correct and return the result of the corresponding Bernoulli trial. 2PL IRT Model The two parameter IRT model is identical to the 1PL variant with an additional variable governing the discriminability γk of question k. The discriminability models how sensitive the question is to incorrect answers. For higher values of γ, the probability of students answer being correct is higher. Thus the outcome Ojk of student on question is determined by Bernoulli trial where the probability of success pjk is determined by the logit function of zjk = γk(αj βk). 3PL IRT Model The three parameter IRT model is identical to the 2PL variant with an additional variable modeling how susceptible question is to guessing. For question k, ck determines the probability that student gets the question right by guessing. Thus the outcome Ojk of student on question is determined by Bernoulli trial where the probability of success pjk is determined by pjk = ck + (1 ck) 1 1 + ezjk (7) where zjk = γk(αj βk) as in 2PL. We use the 2PL model in BoxingGym . D.5 Dugongs The dugongs environment has the ages and lengths of dugongs (sea cows)[24]. The goal is to model the length of dugong based on its age. The following table describes the inputs and outputs of the experiment: In this environment, the length of dugong at age is modeled using hierarchical Bayesian model with parameters α, β, and λ. The age values range between 0 and 5. The observed length at given age is generated from normal distribution with mean that is function of and the 22 Parameter Description Model Setup Parameters Observations Goals Bayesian hierarchical model alpha, beta, lambda, lower limit, upper limit Length of dugong at given age Predicting the length of dugongs at different ages Table 10: Dugongs Environment parameters α, β, and λ, and fixed standard deviation. The function representing the mean length is defined as: The observed lengths are then drawn from normal distribution: = α β λx where σ is the noise in the observed lengths, set to fixed value (e.g., 0.25). (m, σ) D.6 Peregrines (8) (9) The peregrine environment models the population count of peregrine falcons at different times [24]. The goal is to understand how the population changes over time. The following table describes the inputs and outputs of the experiment: Parameter Description Poisson regression model Model Setup Parameters Regression params: α, β1, β2, and β3 Observations Goals Population count of peregrine falcons at given time Predicting the population of peregrines at different times Table 11: Peregrine Environment In this environment, the population count of peregrine falcons at time is modeled using Poisson regression model with parameters α, β1, β2, and β3 . The time values range between 0 and 5. The population count at given time is generated from Poisson distribution with mean that is function of and the parameters α, β1, β2, and β3. The function representing the log of the mean population count λ is defined as: The observed population counts are then drawn from Poisson distribution: log λ = α + β1t + β2t2 + β3t3 This model allows for capturing the non-linear trends in the population data over time. Poisson(exp(log λ)) (10) (11) 23 D.7 Survival Analysis: Mastectomy The survival analysis environment models the outcomes of breast cancer patients based on the time since surgery and the metastasized status. The following table describes the inputs and outputs of the experiment: Parameter Description Survival analysis using Bayesian approach Model Setup Parameters num_patients, time_upper_bound, lambda, beta Observations Goals Whether selected patient is alive or dead Predict survival based on time since surgery and if the cancer had metastasized Table 12: Survival Analysis Environment In this environment, the outcome (alive or dead) of patient is modeled based on the time since surgery and whether the cancer metastasized [11]. The outcomes are generated using Bayesian model with parameters λ0 and β. The number of patients and the upper bound of the time since surgery are configurable. At the start of an episode, we sample set of patients that have undergone mastectomy, with varying times since they had surgery and if their cancer had metastasized or not. The experimenter can then choose to observe specific patients to see if they are alive or dead. The probability of death is calculated using the following model: λ = exp(β metastasized) λ0µ = time_since_surgery λ The probability of death for patient is given by the logistic function: p(death) = 1 1 + exp(µ) (12) (13) Each patients outcome is simulated from Bernoulli distribution with the calculated death probability. The observed data consists of tuples indicating whether the patient died, the time since surgery, and the metastasized status. For example, for patient with given time since surgery and metastasized status, the death outcome is sampled as follows: death_outcome Bernoulli(p(death)) (14) D.8 Predator-Prey Dynamics The predator-prey environment models the interaction between populations of predators and prey over time using the Lotka-Volterra equations [48]. The following table describes the inputs and outputs of the experiment: In this environment, the populations of prey and predators at time are modeled using the LotkaVolterra equations. The initial populations of prey and predators are given by the parameters prey_init and predator_init, respectively. The interaction between the populations is governed by the parameters α, β, γ, and δ. The time values range between 0 and 50. The Lotka-Volterra system of differential equations is defined as follows: 24 Parameter Description Model Setup Parameters Observations Goals Lotka-Volterra equations Initial prey population, initial predator population, α, β, γ, and δ Populations of prey and predators at given time Predicting populations Table 13: Predator-Prey Environment dprey dt dpredator dt = α prey β prey predator = δ prey predator γ predator (15) (16) The populations of prey and predators at any given time are obtained by solving these differential equations. The observed data consists of tuples indicating the time and the populations of prey and predators at that time. For example, for given time t, the populations of prey and predators are computed by solving the Lotka-Volterra equations with the specified parameters and initial populations. The resulting populations are nonnegative integers representing realistic population counts. D.9 Emotions from Outcomes The Emotions from Outcomes environment models participants predictions of players emotions after spinning wheel with three possible monetary outcomes [33]. The model considers the actual outcome, the expected outcome, and the absolute difference between the actual and expected outcomes. The following table describes the inputs and outputs of the experiment: Parameter Description Forward regression model with priors for emotional response Model Setup Parameters Prize values, probabilities, outcome, LLM Observations Goals Prediction in natural language of how player feels and why Predicting what participant thinks player feels on likert scale of 8 emotions. Table 14: Emotions From Outcomes Environment In this environment, the participants predictions of players emotions are modelled after observing the outcome of the player spinning wheel with three possible prizes. Each outcome has known probability and monetary value. The emotion predictions are influenced by the actual outcome, the difference between the actual outcome and the expected outcome, and the absolute difference between the actual outcome and the expected outcome. The model uses the following parameters: 1. Prize values: The monetary values of the three possible outcomes. 2. Probabilities: The probabilities of each outcome occurring. 25 3. Outcome: The actual outcome of the wheel spin. The emotions are measured on Likert scale from 1 to 9 for the following eight emotions: Happiness, Sadness, Anger, Surprise, Fear, Disgust, Contentment, Disappointment The emotional response is generated based on the following model: mean = α + βwin win + βPE PE + βabsPE absPE (17) where: α are the intercepts for each emotion. βwin are the coefficients for the actual outcome. βPE are the coefficients for the prediction error (PE). βabsPE are the coefficients for the absolute prediction error (absPE). For each emotion, the value is sampled from normal distribution with the computed mean and predefined standard deviation. The generative model produces Likert scale ratings for the 8 emotions for the participants predictions of what player would feel. These predictions are translated into free-form natural language observations by language model with the prompt shown in Fig. 6. For example, an observation when the prizes are $50, $20, $10 with probabilities 0.1, 0.4, 0.5, and the player wins $50, the simulated participant responds with The player might be feeling quite happy and content because they landed on the highest possible outcome, which was unexpected given its low probability. D.10 Moral Machines The Moral Machine environment Awad et al. [4] models participants decisions in moral dilemmas involving autonomous vehicles. Participants are presented with scenarios where the vehicle must decide between two outcomes, each involving the death of different group of characters. The following table describes the inputs and outputs of the experiment: Parameter Description Logistic regression model with priors for moral decision-making Model Setup Parameters Character attributes, intervention type, LLM Observations Goals Prediction in natural language of which group to save and why Predicting which group participants choose to save Table 15: Moral Machines Environment In this environment, participants must decide which group of characters to save in moral dilemma involving autonomous vehicles. The characters in each group can be any of the following: stroller, boy, girl, pregnant_woman, male_doctor, female_doctor, female_athlete, male_athlete, female_executive, male_executive, large_woman, large_man, homeless, old_man, old_woman, criminal, dog, cat. The model uses the following parameters: 1. Character attributes: gender, age, social status, fitness, species (human or pet). 26 LLM prompt to translate predictions from the generative model to observations 1 You are observing user play game where they spin wheel. 2 The wheel has three possible outcomes (monetary values), and the probabilities of landing on each are known to you and the player. 3 You are observing the player play the game and the outcomes. 4 You are asked to predict how the player feels after each spin of the wheel. 5 Translate the values for emotions to sentence that describes the player. 6 The decisions are based on the following model and features: 7 - Your predition of the player's happiness, sadness, anger, surprise, fear, disgust, contentment, and disappointment are influenced by few factors. 8 - The player's emotions are influenced by the actual outcome of the spin. 9 - The player's emotions are influenced by the difference between the actual outcome and the expected outcome. 10 - The player's emotions are influenced by the absolute difference between the actual outcome and the expected outcome. 11 The wheel has three possible outcomes with the following probabilities: 12 {v1:0.2f}: {p1:0.2f} 13 {v2:0.2f}: {p2:0.2f} 14 {v3:0.2f}: {p3:0.2f} 15 The player has spun the wheel and landed on {outcome}. 16 This is how you think the player feels: 17 Happiness: {happiness}/9 18 Sadness: {sadness}/9 19 Anger: {anger}/9 20 Surprise: {surprise}/9 21 Fear: {fear}/9 22 Disgust: {disgust}/9 23 Contentment: {contentment}/9 24 Disappointment: {disappointment}/9 25 Translate the values for emotions to sentence that describes the player. 26 1: Not at all, 9: Very much 27 This sentence should be concise and describe the player's emotions after the spin. 28 The sentence should be few words long and should not contain any numbers or refer to the numbers directly. 29 Only talk about the most salient emotions. 30 Start with \"The player might be feeling...because...\" and provide description of the player's emotions and reason. Figure 6: LLM prompt for simulated participant. LLM prompt to translate predictions from the generative model to observations in free-form natural language. 2. Intervention type: swerve or stay. The decision to save group is influenced by the difference in attributes between the two groups and the intervention required. The logistic regression model considers the following coefficients: βintervention: Preference for inaction. βgroup: Preference for group 1 (passengers). βgender: Preference for sparing females. βfitness: Preference for sparing the fit. βsocial_status: Preference for sparing higher status individuals. βage: Preference for sparing the young. 27 βhuman_count: Preference for sparing more characters. βspecies: Preference for sparing humans over pets. The logistic regression model generates choice for which group to save based on the computed attributes and intervention. These predictions are translated into free-form natural language observations by language model with the prompt shown in Fig. 7. For example, in scenario where group 1 consists of boy and girl, and group 2 consists of an elderly man and an elderly woman, with the intervention being swerve, the simulated participant responds with choose to save group 1 because they are younger and have more potential life ahead of them. LLM prompt to translate predictions from the logistic regression model to observations 1 You have to make choices in moral dilemma involving autonomous vehicles. In each dilemma, the vehicle must decide between two outcomes, each involving the death of different group of characters. 2 There are two groups of characters, and the vehicle must decide which group to save. You can query the participant for their decision by providing the descriptions of the characters in each group and the intervention the car would take ('swerve' or 'stay' to save group 1). 3 The characters can be any of the following: {characters} 4 Translate the decision that is shown to you in natural language and provide reasons for it. 5 The importance that you place on the different attributes of the characters affects your decision. Values range from -1 to 1, with 0 being neutral. 6 Your preferences are as follows: 7 - Preference for human count: {beta_human_count} 8 - Preference for group 1 (passengers): {beta_group} 9 - Preference for males: {beta_gender} 10 - Preference for young: {beta_age} 11 - Social status: {beta_social_status} 12 - Preference for Fitness: {beta_fitness} 13 - Preference for humans over pets: {beta_species} 14 - Preference for having to take an action: {beta_intervention} 15 DO NOT talk about the values of preferences directly in your response. Only talk about the most salient reasons for the decision. Be concise, write one sentence only. 16 Translate the decision that is shown to you in natural language and provide reasons for it. 17 The scenario presented to you was as follows: 18 Group 1: {group1} 19 Group 2: {group2} 20 Intervention: {intervention} 21 Choice made: {choice} 22 Translate the decision that is shown to you in natural language and provide reasons for it. 23 Only talk about the most salient reasons for the decision. 24 The sentence should be concise and should not refer to the importance factors/ values directly. 25 Start with \"I choose to save group 1/group 2 because...\". Be concise and faithful to the importance weights. Figure 7: LLM prompt for simulated participant. LLM prompt to translate predictions from the logistic regression model to observations in free-form natural language."
        },
        {
            "title": "E Qualitative Examples",
            "content": "Boxs Apprentice proposed programs 1 with pm.Model() as model: 2 # Priors for student abilities and question difficulties student_ability = pm.Normal(\"student_ability\", mu=0, sigma=1, shape=6) question_difficulty = pm.Normal(\"question_difficulty\", mu=0, sigma=1, shape=6) # Linear combination of student ability and question difficulty logit_p = student_ability[student_id] - question_difficulty[question_id] # Likelihood y_obs = pm.Bernoulli(\"y_obs\", logit_p=logit_p, observed=correctness, dims =\"obs_id\") 1 with pm.Model() as model: 2 # Define priors for the parameters of the linear model intercept = pm.Normal(\"intercept\", mu=0, sigma=10) slope = pm.Normal(\"slope\", mu=0, sigma=10) # Define the linear model for the mean of the Poisson distribution lambda_ = pm.math.exp(intercept + slope * year_data) # Define the likelihood y_obs = pm.Poisson(\"y_obs\", mu=lambda_, observed=C, dims=\"obs_id\") 1 with pm.Model() as model: # Define the locations of the three sources as latent variables source_x1 = pm.Uniform(\"source_x1\", lower=0, upper=1, shape=3) source_x2 = pm.Uniform(\"source_x2\", lower=0, upper=1, shape=3) # Calculate the distances from each source to each observation point distances = pm.math.sqrt((x1_data[:, None] - source_x1[None, :])**2 + (x2_data[:, None] - source_x2[None, :])**2) # Model the signal strength as function of the distances signal_strength_mu = pm.math.sum(1 / distances, axis=1) # Define the likelihood of the observations y_obs = pm.Normal(\"y_obs\", mu=signal_strength_mu, sigma=1, observed= signal_strength, dims=\"obs_id\") 3 4 5 6 7 9 10 3 4 5 7 8 9 10 3 5 6 7 8 9 11 12 13 14 Figure 8: BoxLM proposed programs. (top) IRT (middle) Peregrines (bottom) Location finding Example of an explanation produced by the LLM Agent for the IRT Environment 1 Based on the observations made, we can infer some patterns about the students and questions: 2 3 1. **Question 1**: Students 0, 1, 2, and 3 answered question 1 correctly, while student 4 answered it incorrectly. This suggests that question 1 is generally easier for most students, but not universally so. 4 5 2. **Question 0**: Students 2, 3, and 4 answered question 0 correctly. This indicates that question 0 might also be relatively easy for students, though we lack data for students 0, 1, and 5. 6 7 3. **Student 1**: Answered question 1 correctly but answered question 2 incorrectly. This suggests that student 1's performance may vary depending on the question. 8 9 4. **Student 4**: Answered question 0 correctly but answered question 1 incorrectly. This indicates that student 4's performance also varies by question. 10 11 From these observations, it appears that some questions (like question 1 and question 0) might be easier for majority of students, but individual performance can vary. Without more data, it's challenging to make definitive predictions for all student-question pairs, but these patterns provide starting point for understanding the environment. Figure 9: Example Explanation. Example of an explanation produced by the LLM Agent for the IRT Environment. 30 Example of an explanation produced by Boxs Apprentice for the IRT Environment 1 Based on the observations and the statistical model, we can infer that each student's ability and each question's difficulty play significant roles in determining the correctness of answers. The model uses these factors to predict the probability of correct response. 2 3 **Student Abilities:** 4 - Students have varying abilities, with some being more likely to answer correctly than others. For instance, student 2 has higher ability (0.544) compared to student 0 (-0.405). 5 6 **Question Difficulties:** 7 - Questions also vary in difficulty. For example, question 1 is relatively easier (-0.873) compared to question 3 (0.492). 8 9 **Predicted Probabilities:** 10 - The model calculates the probability of correct answer by combining student ability and question difficulty. For example, student 2 answering question 1 has high probability of correctness (0.708), while student 0 answering question 3 has lower probability (0.414). 11 12 **General Patterns:** 13 - Students with higher abilities are more likely to answer correctly across various questions. 14 - Easier questions are more likely to be answered correctly by most students. 15 16 To predict if student will answer question correctly, consider both the student's ability and the question's difficulty. Higher student ability and lower question difficulty increase the likelihood of correct answer. Figure 10: Example Explanation. Example of an explanation produced by the Boxs Apprentice for the IRT Environment."
        }
    ],
    "affiliations": [
        "Stanford University"
    ]
}