{
    "paper_title": "WebNovelBench: Placing LLM Novelists on the Web Novel Distribution",
    "authors": [
        "Leon Lin",
        "Jun Zheng",
        "Haidong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed for evaluating long-form novel generation. WebNovelBench leverages a large-scale dataset of over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story generation task. We propose a multi-faceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to a percentile rank against human-authored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide a comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides a scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 8 1 8 4 1 . 5 0 5 2 : r WebNovelBench: Placing LLM Novelists on the Web Novel Distribution Leon Lin1, Jun Zheng2, Haidong Wang2 1Nanyang Technological University, 2Sun Yat-Sen University liangtao.lin@ntu.edu.sg, {zhengj98, wanghd7}@mail2.sysu.edu.cn https://github.com/OedonLestrange42/webnovelbench https://huggingface.co/datasets/Oedon42/webnovelbench"
        },
        {
            "title": "Abstract",
            "content": "Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, novel benchmark specifically designed for evaluating longform novel generation. WebNovelBench leverages large-scale dataset of over 4,000 Chinese web novels, framing evaluation as synopsisto-story generation task. We propose multifaceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to percentile rank against humanauthored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide comprehensive analysis of 24 stateof-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation."
        },
        {
            "title": "Introduction",
            "content": "Can Large Language Models (LLMs) generate stories that surpass human-written ones? Recent breakthroughs, exemplified by models like GPT4o and Deepseek-R1 (DeepSeek-AI et al., 2025a), underscore their remarkable ability to produce coherent, imaginative, and contextually nuanced narratives. This raises intriguing questions: How proficient are todays LLMs in story generation, and how do their outputs compare to human-authored works? Evaluating LLM performance in this open-ended domain remains significant challenge. While prior research has explored story generation evaluation (Guan et al., 2021; Liu et al., 2024; Paech, 2024; Ismayilzada et al., 2025), these efforts often face limitations such as small dataset sizes or insufficient story diversity, hindering widespread adoption. This contrasts with fields like code generation and mathematical reasoning, where benchmarks such as CodeForces Rating (Quan et al., 2025) and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024) serve as widely accepted standards. Inspired by such successes, we propose WebNovelBench, comprehensive and intuitive benchmark for story generation guided by three key principles: Broad Data Foundation: Utilizes diverse, popular human-authored works. Representative Tasks: Covers diverse storytelling styles, themes, and complexities (details in Section 3.1). Automated and Objective Evaluation: Minimizes subjectivity via robust, consistent automated methods. We leverage 4,000+ popular Chinese web novels (>10,000 readers each) for synopsis-to-story generation task. An LLM-as-Judge approach evaluates stories across eight narrative dimensions. Validation with Mao Dun Literature Prize novels, which scored high (Figure 1), confirms alignment with human judgment. WebNovelBench thus offers automatic assessment of LLM storytelling capabilities without extensive manual intervention, establishing standardized framework for comparison against human-authored content. In summary, our main contributions are: We introduce WebNovelBench, largescale, data-driven evaluation framework for story generation, accurately ranking humanauthored and LLM-generated stories via distribution analysis. Figure 1: Web Novel Dataset Distribution and LLM Placement. Our web novel datasets quality distribution, with Low, Medium, and High zones (95% CIs). The red curve (classic literary works) validates the high-quality zone. Positions of 24 LLMs indicate their performance relative to this corpus. We define eight evaluation dimensions for Chinese story quality, employing validated LLM-as-Judge mechanism for robust automated evaluation. We conduct comprehensive evaluation of 24 state-of-the-art LLMs, ranking their storytelling abilities relative to human-authored works and offering insights for future development."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 LLM General Benchmark General LLM benchmarks like MMLU (Hendrycks et al., 2021) and its variants (Wang et al., 2024; Yue et al., 2024), or dynamic benchmarks like MixEval (Ni et al., 2024), are invaluable for assessing broad capabilities such as reasoning and knowledge recall. However, they often lack the specific, nuanced criteria required to effectively evaluate creative, open-ended tasks like long-form story generation, particularly regarding narrative quality, coherence, and creativity. This highlights the need for specialized benchmarks in creative domains, akin to CodeForces Rating (Quan et al., 2025) and SWE-Bench Verified (Jimenez et al., 2024) or AIME 2024 (MAA, 2024) for coding and mathematics. 2.2 Story Generation Benchmark Previous work on evaluating LLM-generated stories has explored creativity (Ismayilzada et al., 2025; Paech, 2024) and the correlation of automatic metrics with human judgment (Guan et al., 2021). For instance, Ismayilzada et al. (2025) assessed LLM creativity in short story generation but used only four samples per LLM. EQ-Bench (Paech, 2024) offers creative writing score based on limited set of thirty-two prompts. OpenMEVA (Guan et al., 2021) provided framework for evaluating automatic metrics but did not focus on broad dataset diversity. These existing efforts often suffer from limitations such as small dataset sizes or insufficient story diversity, hindering the establishment of universally accepted standard. Our work aims to address this gap by emphasizing large, diverse dataset derived from widely-read web novels and robust, automated evaluation protocol. 2.3 LLM-as-a-Judge Automated evaluation for open-ended text generation increasingly employs the LLM-as-a-Judge paradigm (Zheng et al., 2023), where LLMs assess outputs without reference texts (Li et al., 2024; Kasner and Dušek, 2024). While promising for aligning with human preferences, LLM-as-Judge reliability is concern, with research focusing on fairness and potential biases (Shi et al., 2025; Zhang et al., 2023; Ye et al., 2024). For instance, Ye et al. (2024) introduced CALM to identify and quantify biases in LLM judges. This body of work underscores the importance of careful design and validation of the LLM-as-Judge component, which we address in WebNovelBench to ensure reliable and fair story evaluation based on multi-dimensional criteria."
        },
        {
            "title": "3 Benchmark Construction",
            "content": "3.1 Dataset We curated dataset from over 10,000 Chinese web novels (published 2013-2020) via several preprocessing steps: Deduplication. Some text files may contain highly similar content under different titles, indicating that they represent the same novel. Directly comparing exact character matches is insufficient for identifying duplicates. To address this, we employed difflib1 to compute pairwise similarity scores and removed novels with similarity score greater than 0.9. Chapter Parsing. Web novels are typically long-form narratives presented in serialchapter format. However, the raw crawled texts lacked explicit chapter delimiters. To extract chapters, we designed regular expression patterns tailored to common chapter formatting styles in Chinese web novels. Novels containing fewer than ten chapters were excluded, as they did not meet our definition of long-form or suggested incomplete parsing. Tail Author Removal. It is commonly observed that successful authors tend to publish multiple novels over time. Based on this observation, we compiled the list of all authors and excluded novels written by those with the fewest works. After this filtering step, we retained final set of 4,000 web novels. Fantasy: 670, Historical: 234, plus Sci-Fi, Suspense, Romance with varied subthemes like Original World Setting). This breadth reflects popular web fiction complexities. For the synopsis-to-story task, we utilize Doubao-pro-32k2 to generate synopsis (main characters, key plot points, important scenes) for ten random consecutive chapters from each novel. This yielded ten <chapter content, synopsis> pairs per novel (details in Appendix A). 3.2 Metric We evaluate narrative quality using eight key dimensions  (Table 1)  covering stylistic and structural elements (e.g., literary devices, character consistency). This provides nuanced assessment beyond surface-level fluency. 3.3 Scoring Method To rank LLM works against our 4,000 humanauthored web novels, we combine Principal Component Analysis (PCA) (Hotelling, 1933) for multidimensional score aggregation and Empirical Cumulative Distribution Function (ECDF) (Conover, 1999) for percentile ranking. Score Aggregation via PCA. Given dataset of samples, where each sample is evaluated along dimensions, we first normalize all dimension scores using z-score standardization: zij = xij µj σj (i [1, ], [1, d]), (1) where xij is the raw score of the i-th sample in the j-th dimension, and µj and σj are the mean and standard deviation of the j-th dimension across all samples. We then perform PCA on the standardized data to extract the first principal component, whose normalized loading vector = (w1, w2, . . . , wd) represents the relative importance of each dimension. The aggregated composite score si for the i-th sample is then computed as weighted sum: si = (cid:88) j=1 wjzij (2) The curated 4,000 novels ensure Representative Tasks by covering diverse genres (e.g., Eastern Fantasy: 1281, General Realism: 1255, Western 1https://docs.python.org/3/library/difflib. html Percentile Ranking via ECDF. To translate raw scores into interpretable relative rankings, we apply the ECDF over all aggregated scores {s1, s2, . . . , sN }: 2https://www.volcengine.com/product/doubao Figure 2: Framework of Our Method. Our benchmark framework consists of four major components: (1) Data Preparation Phase: We collect and curate large web novel dataset, and use Doubao for story-to-synopsis extraction to build 4,000 novels synopsis-to-story dataset. (2) Distribution Construction: Each story is scored across eight quality dimensions using LLM-as-judge, followed by PCA+ECDF to form quality distribution benchmark. Classic literary works are used to validate the high end of the distribution. (3) Model Evaluation: LLMs generate stories from selected subsets of the dataset. Their outputs are scored and mapped onto the distribution to assess model performance. (4) Ad Hoc Evaluation: New data can be scored and aligned with the benchmark for measuring data quality and supporting further applications. ECDF(x) = 1 (cid:88) i=1 I(si x), (3) where I() is the indicator function. The ECDF provides percentile score in the range [0, 1], representing the proportion of samples with scores less than or equal to x. Given new LLM-generated sample with aggregated score snew, its percentile rank is Pnew = ECDF(snew). This percentile reflects the models performance relative to the full empirical distribution of the reference datasetindicating the level at which stories generated by LLM are comparable to those written by humans. By evaluating the model across batches of test samples, we can estimate the LLMs overall writing ability. LLM, s(2) LLM} be set of aggregated scores for batch of LLM-generated samples. The estimated writing level is then defined as the average percentile: LLM, . . . , s(M ) Let = {s(1) ˆPLLM = 1 (cid:88) m=1 ECDF (cid:17) (cid:16) s(m) LLM (4) The value ˆPLLM [0, 1] represents the expected percentile rank of the LLMs output relative to the distribution of human-written texts in the reference dataset."
        },
        {
            "title": "4 Experiments and Results",
            "content": "4.1 Experimental Setup Due to resource limits, our evaluation dataset uses 100 web novels (one per percentile from the 4,000novel distribution), with 10 synopsis-to-story pairs per novel (1,000 test samples). An LLMs overall rank is its average percentile across these 100 books. LLMs (open-source and proprietary) were accessed via APIs with standardized system prompt and constant generation settings (4096 max tokens, temperature 0.6, eight evaluation criteria in prompt; see Appendix A). Outputs were evaluated by Deepseek-V3 using consistent evaluation Metric D1: Use of Literary Devices D2: Richness of Sensory Detail D3: Balance of Character Presence 评估维度 修辞手法 Explanation Based on the quantity and quality of rhetorical devices like metaphor and symbolism 感官描述丰富度 Frequency of visual, auditory, and other sensory descriptions 角色平衡度 Frequency, dialogue proportion, and psychological depth of each character D4: Distinctiveness of Character Dialogue 角色对白独特性 Whether dialogue reflects distinct personalities D5: Consistency of Characterisation D6: Atmospheric and Thematic Alignment D7: Contextual Appropriateness D8: Scene-to-Scene Coherence 角色一致性 意境匹配度 语境适配度 跨场景衔接度 Whether language and actions align with character identity Whether scenes support the overall atmosphere and themes Whether settings match time/place/cultural background Whether scene transitions are smooth and natural Weight 0. 0.1160 0.1152 0.1171 0.1377 0.1290 0. 0.1263 Table 1: Narrative Evaluation Metrics and PCA-Derived Weights. This table lists the eight dimensions used to evaluate narrative quality. Each weight reflects the metrics relative importance, derived through PCA on web novels scores. prompt. For details on the robustness analysis, see Section 5.3 below. 4.2 Main Experiments We evaluate total of 13 open source models and 11 closed source models on our benchmark. Figure 3 illustrates the performance of these frontier models across eight distinct narrative evaluation dimensions and overall effect. Top models (Qwen3-235B-A22B (Yang et al., 2025), DeepSeek-R1 (DeepSeek-AI et al., 2025a), Gemini-2.5-Pro) score high (3.5-4.6) across dimensions; Qwen3-235B-A22B achieves 5.21 norm score, indicating strong alignment with highquality human writing. Mid-tier models (e.g., GPT4o, DeepSeek-V3 (DeepSeek-AI et al., 2025b)) show varied performance (scores 2.5-3.8), highlighting areas like sensory detail and literary device use for improvement. Lower-ranked models (e.g., GLM-4-9B-chat (GLM et al., 2024), LLaMA-38B (Grattafiori et al., 2024)) perform consistently poorly (less than 2.0 norm score), especially in literary devices and character dialogue, indicating significant room for improvement, particularly for open-source or smaller LLMs. An intriguing observation from our analysis is the relatively narrow performance gap between top closed-source models (such as Claude-3-7-Sonnet and GPT-4.1) and leading open-source models (Qwen series and DeepSeek models), suggesting that open-source communities are rapidly bridging the performance divide traditionally held by proprietary models. Overall, this benchmark effectively captures distinct strengths and weaknesses among current LLMs. While the most advanced models achieve near-perfect scores within our distribution, demonstrating their strong performance on the story-tosynopsis dataset, this does not diminish the value of the benchmark. On the contrary, it validates our original intuition. Our primary goal is to evaluate the story generation capabilities of contemporary LLMs, and the results suggest that leading models have reached level comparable to toptier works in web novels. This study primarily presents methodological framework; for more fine-grained evaluation, future work may involve collecting higher-quality reference texts or designing more nuanced evaluation dimensions. 4.3 Comparison with Other Benchmarks To contextualize our contribution, we compare WebNovelBench with existing benchmarks for story generation evaluation, as summarized in Table 2. OpenMEVA (Guan et al., 2021) provides framework for evaluating automatic metrics using an existing dataset of 400 stories. It aligns with human preference and utilizes 8 evaluation dimenFigure 3: LLM Performance Heatmap Across Narrative Dimensions. Shows average scores (1-5 scale) for 24 LLMs on eight dimensions, sorted by percentile rank. Final column is PCA-derived weighted norm score. Higher scores indicate better alignment with quality human writing. Benchmark Dataset Information Evaluation Method Data Source Testing Samples Human Preference Alignment Dimension Weights Evaluation Dimension Existing dataset OpenMEVA (Guan et al., 2021) AlignBench Writing Ability (Liu et al., 2024) Self-constructed EQ-Bench Longform Creative Writing (Paech, 2024) Self-constructed Existing dataset Ismayilzada et al. (2025) Web novels WebNovelBench (ours) 400 stories 75 stories 12 stories, each with 8 chapters 4 stories 100 stories, each with 10 chapters 8 5 14 4 Table 2: Comparison of Other Benchmarks sions. However, it does not employ weighted dimensions for score aggregation, potentially treating all aspects of narrative quality as equally important, which might not reflect nuanced human judgment. AlignBench Writing Ability (Liu et al., 2024) uses self-constructed dataset of 75 stories. While it considers human preference and evaluates across 5 dimensions, its dataset size is relatively small, which might limit the diversity of narrative styles and scenarios covered. Similar to OpenMEVA, it does not incorporate dimension weights in its scoring. EQ-Bench Longform Creative Writing (Paech, 2024) focuses on long-form creative writing with self-constructed dataset of 12 stories, each comprising 8 chapters (totaling 96 evaluation instances). It employs comprehensive set of 14 dimensions. However, according to the available information, it does not explicitly align its dataset construction with broad human preference (e.g., via popularity metrics of source texts) and also lacks weighted approach to aggregating its numerous dimensional scores. The work by Ismayilzada et al. (2025) evaluates creative short story generation using an existing dataset. While it aligns with human preference and uses 4 evaluation dimensions, its very small test set of only 4 stories per LLM significantly limits the robustness and generalizability of its findings. It also does not use weighted dimensions. In contrast, WebNovelBench offers several key advantages: Scale and Diversity: Built on 4,000+ web novels, testing with 100 distinct stories (10 chapters each, 1,000 instances), ensuring wide genre/style coverage. Inherent Human Preference Alignment: By using popular web novels (each with over 10,000 readers) as the source, our benchmark inherently captures broad human literary preferences. Data-Driven Dimension Weighting: PCA derives weights for 8 narrative dimensions, providing nuanced, objective assessment reflecting their relative importance in humanauthored works. Comprehensive and Focused Evaluation: Eight carefully defined dimensions provide thorough yet focused assessment of key storytelling elements. These features make WebNovelBench robust, scalable, and replicable solution for assessing and advancing LLM-driven narrative generation, particularly for long-form stories in the Chinese web novel domain."
        },
        {
            "title": "5 Rationality Analysis",
            "content": "5.1 Metric Analysis To assess the rationality and effectiveness of the proposed eight evaluation metrics, we conducted detailed statistical analysis combining principal component analysis (PCA) and distributional visualization. Principal Component Analysis. PCA shows the first component explains 75.6% of variance (first three greater than 90%), indicating the metrics capture dominant quality factor. Derived weights (11.5%-13.8%) are balanced, with Consistency of Characterisation  (Table 1)  highest, reflecting its discriminative power.3 Distributional Characteristics. The probability density function (PDF) of each individual metric, obtained via kernel density estimation (KDE) (Parzen, 1962), is plotted against its bestfit normal distribution to evaluate shape characteristics (Figure 4). The majority of metrics (e.g., D2, D3, D4, and D6) display near-Gaussian behavior, implying smooth and well-behaved scoring distributions conducive to comparative assessment. 3See Appendix for further details. Figure 4: Distributions of Narrative Metrics and Fitted Normal Curves. Each subplot shows the empirical distribution (solid line) of narrative evaluation dimension across the web novel dataset, alongside the corresponding fitted normal distribution (dashed line). The comparison illustrates the varying shapes of real-world data and highlights where distributions deviate from normality. The bottom-right panel presents the overall distribution of mean scores. Metrics such as D1 (Use of Literary Devices) and D7 (Contextual Appropriateness) exhibit mild deviations from normality, with signs of skewness or slight multimodality. These deviations likely reflect the existence of content subgroups, for instance, differences in stylistic density between human and LLM-generated texts or variations in how explicitly context is embedded. Importantly, the averaged score across all dimensions yields an aggregate distribution that closely aligns with the Gaussian, further validating the integration of the metrics into coherent composite score. Implication. PCA and distributional analyses confirm our metrics are well-structured, diverse, complementary, and robust, suitable for large-scale evaluation of human and LLM narratives. 5.2 Classic Literature Comparison We validated our benchmark against 25 Mao Dun Literature Prize-winning novels (first 10 chapters each). As shown in Figure 1, these classics consistently scored in the high range, confirming our frameworks ability to capture acknowledged literary merit. This comparative analysis not only aligns with human evaluative judgments but also affirms the anticipated quality hierarchy among the three text categories under study: classic works, web novels, and LLM-generated content. The findings validate the benchmarks sensitivity to nuanced differences in textual quality and its robustness in reflecting the relative literary value of diverse sources. Moreover, the stratification of LLM-generated outputs into three distinct quality tiers based on this distribution appears both credible and well-justified. 5.3 LLM-as-Judge To eliminate human involvement in automatic evaluation, we adopt the LLM-as-Judge paradigm, employing Deepseek-V3 as the evaluator, which currently is one of the most advanced Chinese language models. Based on our intuition and empirical observations, models without explicit chain-ofthought reasoning tend to perform more efficiently and effectively on classification tasks of this nature. To mitigate position bias and context-length bias, issues shown to significantly impact pairwise comparison methods as demonstrated in Ye et al. (2024), we adopt direct scoring approach, where the LLM evaluates each generated output independently. This method not only reduces systematic biases but also enhances flexibility and scalability. Notably, it allows for the direct assessment of single-generation outputs, which is particularly valuable for tasks such as dataset cleaning and screening, critical processes in the development of high-quality LLMs. Figure 5: Robustness Assessment of LLM-as-Judge. Boxplot of normalized scores for selected classic Chinese novels, based on 11 repeated evaluations using the LLM-as-judge framework. Each box shows the interquartile range (IQR) with the median (solid line) and mean (dashed line) marked. The majority of works demonstrate consistently high scores with narrow IQRs and minimal outliers, indicating the robustness and stability of model evaluations. Deepseek-V3 configurations. As shown in Figure 5, the boxplot exhibit high consistency, with an interquartile range (IQR) below 0.05 and score variance within the 0.001 range, confirming the stability of both the model and the evaluation prompt design. These findings reinforce the reliability and credibility of our benchmark framework. 5.4 Length Analysis To avoid introducing bias, we conducted an analysis of the length of model-generated outputs. The results indicate that most models closely adhered to the requested length or context window, producing outputs averaging between 800 and 1200 words. Notable exceptions include Claude 3.7 Sonnet and Gemini 2.5 Pro, which consistently generated significantly longer texts. Overall, output length remained relatively stable across models and did not emerge as major differentiating factor under the 4096-token constraint. In future work, scoring regularization term based on output length may be introduced to enhance robustness."
        },
        {
            "title": "6 Conclusion",
            "content": "WebNovelBench addresses challenges in evaluating LLM long-form storytelling. It uses 4,000+ Chinese web novels for synopsis-to-story task. An automated pipeline with eight LLM-judged narrative dimensions and PCA+ECDF scoring provides percentile rankings against human content. Experiments show WebNovelBench effectively distinguishes classic literature, web fiction, and LLM outputs, providing stable rankings for 24 SOTA LLMs. WebNovelBench is valuable tool for benchmarking progress and guiding LLM development in creative narrative generation. While focused on Chinese web novels, its principles are extendable. Future work includes diverse judge models, genre expansion, and fostering more engaging LLM storytellers, catalyzing innovation in machine-generated narratives."
        },
        {
            "title": "Limitations",
            "content": "To demonstrate the robustness of the LLM-asJudge approach, which is concern of broad relevance, we conducted repeated experiments using subset of classic works, which fall outside the distribution of our web novels benchmark. Specifically, we performed eleven independent evaluation rounds on the same dataset using identical Here we outline several limitations of our work. First, our benchmark relies exclusively on Chinese web novels as the evaluation dataset. While this provides rich, diverse, and representative corpus for our purposes, future work should extend the benchmark to other languages and literary forms 4See Appendix for further details. to improve its generalizability. Second, due to resource and time constraints, our experimental scale is limited: we evaluated performance only on the subset using single LLM-as-Judge model. Although our results demonstrate robustness, evaluating additional subsets with multiple judge models in future studies would further strengthen and validate our conclusions. Lastly, while we propose that our benchmark can directly assess data quality, we have not yet explored its broader applications. Future research will investigate how these benchmark datasets can be leveraged to enhance model performance and other downstream tasks."
        },
        {
            "title": "References",
            "content": "William Jay Conover. 1999. Practical nonparametric statistics. john wiley & sons. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 181 others. 2025b. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Team GLM, :, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, and 40 others. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. Preprint, arXiv:2406.12793. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie Huang. 2021. Openmeva: benchmark for evaluating open-ended story generation metrics. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR). Harold Hotelling. 1933. Analysis of complex of statistical variables into principal components. Journal of Educational Psychology, 24:498520. Mete Ismayilzada, Claire Stevenson, and Lonneke van der Plas. 2025. Evaluating creative short story generation in humans and large language models. Preprint, arXiv:2411.02316. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. Zdenˇek Kasner and Ondˇrej Dušek. 2024. Beyond traditional benchmarks: Analyzing behaviors of open llms on data-to-text generation. Preprint, arXiv:2401.10186. Ruosen Li, Teerth Patel, and Xinya Du. 2024. Prd: Peer rank and discussion improve large language model based evaluations. Preprint, arXiv:2307.02762. Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Xiaotao Gu, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang. 2024. Alignbench: Benchmarking chinese alignment of large language models. Preprint, arXiv:2311.18743. MAA. 2024. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024. Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and Yang You. 2024. Mixeval: Deriving wisdom of the crowd from llm benchmark mixtures. arXiv preprint arXiv:2406.06565. Samuel J. Paech. 2024. Eq-bench: An emotional intelligence benchmark for large language models. Preprint, arXiv:2312.06281. Emanuel Parzen. 1962. On estimation of probability density function and mode. The annals of mathematical statistics, 33(3):10651076. Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, and 1 others. 2025. Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. arXiv preprint arXiv:2501.01257. Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, and Neil Zhenqiang Gong. 2025. Optimization-based prompt injection attack to llmas-a-judge. Preprint, arXiv:2403.17710. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, and 1 others. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh Chawla, and Justice or prejudice? Xiangliang Zhang. 2024. Preprint, quantifying biases in llm-as-a-judge. arXiv:2410.02736. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. 2024. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813. Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023. Wider and deeper llm networks are fairer llm evaluators. Preprint, arXiv:2308.01862. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685."
        },
        {
            "title": "A System Prompt",
            "content": "To ensure consistency and clarity in our evaluation pipeline, we standardised the prompts used across all stages of the benchmark. Figure 9 shows the system prompt used to guide story generation, while Figure 10 presents the evaluation prompt employed by the LLM-as-judge to assess narrative quality. Additionally, Figure 8 illustrates the system prompt used for Story-to-Synopsis extraction, which serves as foundation for constructing our generation dataset. These prompts were carefully designed to minimise ambiguity and ensure the reproducibility of our benchmark results."
        },
        {
            "title": "B Data Samples",
            "content": "Story-to-Synopsis Extraction Sample, generated by Doubao-pro-32k: Story-to-Synopsis Extraction Input 第1498章不知好歹萧炎哥哥这药 万归乃是四星初期的斗圣强者实力 极强对付起来可并不容易 ...... 废物一般的东西也敢在我老师面 前叫嚣这一巴掌就当是个小小教 训 Story-to-Synopsis Extraction Output <主要人物> 萧炎. 薰儿, 药万归, 药老, 药天</主要人物> <主要情节> (1) 薰儿提醒萧炎药万归 是四星初期斗圣强者对付不易萧 炎表示自有分寸只要得到并炼化净 莲妖火灭杀药万归不在话下...... (5) 药天愤怒让药万归杀萧炎萧炎 灵魂分身出现打了药天一巴掌并教 训他</主要情节> <重要场景> 天空众人聚集在此发 生冲突的地点</重要场景> LLMs synopsis-to-story generation samples, using the above extraction result as input example: Qwen3-235B-A22B Generation 浮空岛的云海翻涌如沸腾的熔岩萧 炎负手立于青石栏前玄衣翻卷间露 出腰间漆黑的陨落心炎薰儿忽然按 住他的手腕指尖微颤\"那老者眉 心的七色丹纹...\"少女的呢喃被罡风 撕碎却在萧炎耳畔凝成清晰的警 告\"药族三长老药万归四星斗圣 的威压能压弯古树年轮\" ...... 药天 的衣袍被狂风撕裂露出心口狰狞 的\"罪丹\"疤痕分身降临的萧炎踩着 虚空踏出八道残影每记响亮的耳光 都让云层塌陷成莲瓣\"你可知你祖 父当年偷炼化骨丹时药老替他背 下了多少罪行\"最后一掌落下时 浮空岛深处传来妖火的长吟净莲虚 影在所有人额头绽开血色印记 GPT-4.1 Generation 天空如同被烈焰炙烤的铁幕高远而 幽深浮云仿佛被无形之力撕裂成碎 絮斑驳地掩映着一场即将爆发的风 暴浩渺的天际下人影浮现气息 如潮水般搅动空气斗气波澜层层激 荡薰儿身着淡青长裙立于萧炎 身侧眉宇间藏着一抹忧色她低 声道萧炎哥哥药万归乃四星初 期斗圣强者不可大意她声音纤 细却带着隐隐的紧迫如同清泉流 淌过心头石隙萧炎目光沉静如潭 水嘴角勾起自信的弧度放心 只要夺得并炼化净莲妖火区区药万 归自然不在话下 ...... 药天面色 铁青怒吼道杀了他我要亲手 取他性命 话音未落虚空骤然扭 曲一道灵魂分身悄然凝聚于药天身 侧只听啪的一声脆响灵魂分 身挥手给了药天一个响亮耳光天地 间仿佛静止了一瞬只剩下风声和众 人的倒吸冷气萧炎分身俯瞰着愣神 的药天语气冰冷自以为是只 会辱骂前辈这就是你所谓的尊严 若再挑衅我让你永远闭嘴 GLM-4-9B Generation"
        },
        {
            "title": "C Ethical Considerations",
            "content": "在广袤的天空之下人声鼎沸众人 的目光都被那三道身影牢牢锁定薰 儿紧紧地站在萧炎身旁她的眼中闪 烁着坚定与担忧仿佛能感受到萧炎 内心的决心药万归你不是四星 初期斗圣吗对付你我萧炎自有分 寸萧炎的声音在空中回荡带着 一丝不容置疑的威严...... 话音刚 落药天便释放出强大的灵魂力量 向萧炎发动攻击然而萧炎的反应 却出乎所有人的意料他轻易地捏碎 了药天的灵魂力量你药天愤怒 地看向药万归杀了他萧炎的灵 魂分身出现在众人面前他毫不犹豫 地给了药天一巴掌教训你一顿 LLM-as-Judge evaluation sample, using Deepseek-V3 as critic: Deepseek-V3 Evaluation <提取结果> <主要人物及其对白> 1. **萧炎** - 放心只要夺得并炼化 净莲妖火区区药万归自然不在话 下... 1. **场景铺垫**天空被烈 焰炙烤风暴即将爆发萧炎与薰儿 在紧张的氛围中对话薰儿提醒萧炎 药万归的强大... </主要情节> <重 要场景> 1. **天空下的对峙场景** 天空被烈焰炙烤云海翻滚萧炎与 药万归双方针锋相对... </重要场 景> </提取结果> <评 分 结 果> <修 辞 手 法 评 分>4</修 辞 手 法 评 分> <感 官 描 述 丰 富 度 评 分>4</感 官 描 述 丰 富 度 评 分> <角 色 平 衡 度 评 分>3</角 色 平 衡 度 评 分> <角色对白独特性评分>4</角色 对 白 独 特 性 评 分> <角 色 一 致 性 评 分>4</角色一致性评分> <意境匹配 度评分>4</意境匹配度评分> <语境 适 配 度 评 分>4</语 境 适 配 度 评 分> <跨场景衔接度评分>4</跨场景衔接 度评分> </评分结果> Our WebNovelBench dataset comprises over 4,000 publicly available Chinese web novels freely sourced from the internet. For each novel, we extract only 10 chapters, which is only very small fraction of the full content, to minimize data usage while enabling robust evaluation. These texts contain no personally identifiable information or private data and are used solely for statistical evaluation and benchmarking purposes. They are not employed for model training or direct fine-tuning. We strictly adhere to the license or terms for use of each LLM during inference testing. The models generate scientific artefacts based on our synopsisto-story dataset, and these outputs are used exclusively for research evaluation and capability assessment. No generated content is repurposed for other uses. Our use of these materials aligns with fair use principles for research. We conducted checks to ensure no offensive or sensitive personal content is included. While our benchmark specifically addresses Chinese web novels, potentially limiting generalizability, it poses minimal risks related to misuse, privacy, or bias."
        },
        {
            "title": "D Principal Component Analysis Details",
            "content": "To determine the relative importance of each evaluation dimension, we applied Principal Component Analysis (PCA) to the score distributions across our web novel dataset. The scree plot in Figure 6 shows that the first principal component accounts for over 75% of the total variance. This indicates that while each of our eight evaluation dimensions captures distinct and meaningful aspect of narrative quality, they also collectively reflect strong underlying evaluative signal. The high explained variance supports the internal coherence of our metric design and justifies the use of PCA-derived weights for aggregating narrative quality scores. This balance suggests that the dimensions are complementary rather than redundant, each contributing uniquely to the overall narrative assessment. The radar chart embedded in the same figure visualises the PCA-derived weights assigned to each of the eight narrative dimensions (D1D8). These weights, used throughout our benchmark scoring, reflect each dimensions contribution to the primary variance component and therefore represent their relative importance in the overall evaluation framework. Figure 6: PCA analysis of evaluation metrics. The bar chart shows the explained variance ratio for each principal component. The radar chart visualises the relative weights of the eight narrative dimensions used in our benchmark."
        },
        {
            "title": "E Length Analysis Details",
            "content": "To support the main findings in Section 5.4, we provide visual summary of the mean output lengths across all evaluated models in Figure 7. The green and red dashed lines represent the expected length bounds (8001200 words). As shown, the vast majority of models generated outputs that fall within or near this range, indicating consistent adherence to the specified context length. Given this overall consistency, we do not delve into detailed length-based comparisons in the main text. Notable outliers such as Claude 3.7 Sonnet (around 2,700) and Gemini 2.5 Pro produced (around 2,000) significantly longer outputs, while models like LLaMA 3.3 and GLM-4-9B-chat tended to under-generate. These deviations are exceptions rather than the norm and had limited impact on the overall evaluation results. While length was not found to be major differentiating factor in narrative quality, future iterations of the benchmark may consider applying soft constraints or regularisation mechanisms to penalise excessively long or short outputs. Figure 7: Mean length of generated outputs by model. Story-to-Synopsis Extraction System Prompt 你的任务是从给定的小说片段中提取主要人物情节和场景等信息以便生成该小 说的知识图谱和百科信息请仔细阅读以下小说文本 <小说> {text} </小说> 在提取信息时请遵循以下步骤 1. 仔细通读整个小说文本 2. 识别出主要人物主要人物是在小说中起到关键作用有较多情节围绕的角色 3. 梳理主要情节主要情节是推动故事发展的核心事件和关键转折 4. 确定重要场景重要场景是故事发生的关键地点和环境 5. 检查提取的信息是否准确和完整 请在<提取结果>标签内输出你的提取结果格式如下 <主要人物> [列出主要人物的名字并且用逗号分隔] </主要人物> <主要情节> [详细描述主要情节按照事件发展顺序如(1)...(2)...情节之间使用换 行符分隔] </主要情节> <重要场景> [列出重要场景的名称并且用逗号分隔] </重要场景> 请确保提取的信息丰富全面且准确 Your task is to extract key informationsuch as main characters, major plot points, and important settingsfrom the given novel excerpt. This information will be used to construct knowledge graph and encyclopaedic entry for the novel. Please read the following text carefully: <Novel> {text} </Novel> When extracting the information, follow these steps: 1. Carefully read through the entire novel excerpt. 2. Identify the main characters, i.e., the characters who play central role and around whom significant parts of the plot revolve. 3. Outline the major plot points, which refer to the core events and pivotal turns that drive the story forward. 4. Determine the important settings, i.e., the key locations and environments where significant story developments occur. 5. Check the extracted information for accuracy and completeness. Output your extracted results within the <Extraction> tags using the following format: <Main Characters> [List the names of the main characters, separated by commas] </Main Characters> <Main Plots> [Describe the main plot points in detail, following the chronological order of events. Use line breaks between different events, e.g., (1)...(2)...] </Main Plots> <Important Scenes> [List the names of the important settings, separated by commas] </Important Scenes> Please ensure that the extracted information is rich, comprehensive, and accurate. Figure 8: System Prompt Used for Story-to-Synopsis Extraction Generation System Prompt 你是一个中文小说作家你需要根据用户提供的信息进行扩写创作创作需要满足 下列条件 1. 用户会用下面的格式给出长篇小说的主要人物主要情节和主要场景请仔细阅 读用户提供的信息 <主要人物>[主要人物的名字]</主要人物> <主要情节>[主要情节按照事件发展顺序]</主要情节> <重要场景>[重要场景的名称]</重要场景> 2. 评论家会根据下列标准打分 根据复杂修辞隐喻/象征/悖论的数量与质量提炼度对修辞手法评分 根据文本中的视觉听觉嗅觉等描写数量对感官描述丰富度评分 统计每个角色在生成内容中的出现频率对话占比心理描写和评估人物描述的平 衡度对角色平衡度评分 查看角色台词是否能反映本身个性遮住名字后是否有区分度对角色对白独特性 评分 分析角色语言动作是否匹配其身份和背景对角色一致性评分 通过情感色谱分析检查场景描写是否服务于整体氛围对意境匹配度评分 通过分析环境细节是否适应时代/地域背景对语境适配度评分 评估生成内容是否自然衔接不同场景从而避免场景割裂对跨场景衔接度评分 3. 只需按照指定格式返回生成的小说 <text>你生成的小说内容</text> You are Chinese fiction writer. Your task is to expand and create narrative based on the information provided by the user. Your writing must adhere to the following guidelines: 1. The user will provide the key information for long-form novel using the following format. Please read the information carefully: <Main Characters>[Names of the main characters]</Main Characters> <Main Plots>[Main plot points in chronological order]</Main Plots> <Important Scenes>[Names of important scenes or locations]</Important Scenes> 2. critic will evaluate your writing according to the following criteria: Use of Literary Devices: Scored based on the quantity and refinement of complex rhetorical devices such as metaphor, symbolism, and paradox. Richness of Sensory Detail: Scored based on the frequency of visual, auditory, olfactory, and other sensory descriptions. Balance of Character Presence: Scored based on the frequency of each characters appearance, proportion of dialogue, psychological depiction, and overall balance of character portrayal. Distinctiveness of Character Dialogue: Scored based on whether each characters dialogue reflects individual personality and remains distinguishable even if names are hidden. Consistency of Characterisation: Scored based on whether the characters language and actions align with their identities and backgrounds. Atmospheric and Thematic Alignment: Scored based on whether scene descriptions support the emotional tone and thematic coherence of the narrative. Contextual Appropriateness: Scored based on whether the setting details are appropriate for the time period and regional background. Scene-to-Scene Coherence: Scored based on whether the narrative transitions naturally between scenes, avoiding abrupt or disjointed shifts. 3. Return only the generated novel in the following format: <text>Your generated story content</text> Figure 9: System Prompt Used for Generation Evaluation System Prompt 你的任务是根据给定的指标规则对小说进行评分(1-5)请仔细阅读以下小说文 本<小说> {chapter} </小说> 在提取信息时请遵循以下步骤 1. 仔细通读整个小说文本 2. 识别出主要人物主要人物是在小说中起到关键作用有较多情节围绕的角色 3. 梳理主要情节主要情节是推动故事发展的核心事件和关键转折 4. 确定重要场景重要场景是故事发生的关键地点和环境 5. 检查提取的信息是否准确和完整 请在<提取结果>标签内输出你的提取结果格式如下 <主要人物及其对白>[列出主要人物的名字和对白]</主要人物及其对白> <主要情节>[详细描述主要情节按照事件发展顺序]</主要情节> <重要场景>[列出重要场景的名称]</重要场景> 请确保提取的信息丰富全面且准确在评分时请遵循以下步骤 1. 根据复杂修辞隐喻/象征/悖论的数量与质量提炼度给出修辞手法评分 2. 根据文本中的视觉听觉嗅觉等描写数量给出感官描述丰富度评分 3. 统计每个角色在生成内容中的出现频率对话占比心理描写和评估人物描述的 平衡度给出角色平衡度评分 4. 查看角色台词是否能反映本身个性遮住名字后是否有区分度给出角色对白独 特性评分 5. 分析角色语言动作是否匹配其身份和背景给出角色一致性评分 6. 通过情感色谱分析检查场景描写是否服务于整体氛围给出意境匹配度评分 7. 通过分析环境细节是否适应时代/地域背景给出语境适配度评分 8. 评估生成内容是否自然衔接不同场景从而避免场景割裂给出跨场景衔接度评 分 请在<评分结果>标签内输出你的评分结果格式如下 <修辞手法评分>1</修辞手法评分> <感官描述丰富度评分>1</感官描述丰富度评分> <角色平衡度评分>1</角色平衡度评分> <角色对白独特性评分>1</角色对白独特性评 分> <角色一致性评分>1</角色一致性评分> <意境匹配度评分>1</意境匹配度评分> <语境适配度评分>1</语境适配度评分> <跨场景衔接度评分>1</跨场景衔接度评分> 请确保评分全面且准确符合要求 Figure 10: System Prompt Used for Evaluation"
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Sun Yat-Sen University"
    ]
}